Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 31?40,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Modeling Annotators:
A Generative Approach to Learning from Annotator Rationales?
Omar F. Zaidan and Jason Eisner
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,jason}@cs.jhu.edu
Abstract
A human annotator can provide hints to a machine learner
by highlighting contextual ?rationales? for each of his
or her annotations (Zaidan et al, 2007). How can one
exploit this side information to better learn the desired
parameters ?? We present a generative model of how
a given annotator, knowing the true ?, stochastically
chooses rationales. Thus, observing the rationales helps
us infer the true ?. We collect substring rationales for
a sentiment classification task (Pang and Lee, 2004) and
use them to obtain significant accuracy improvements for
each annotator. Our new generative approach exploits the
rationales more effectively than our previous ?masking
SVM? approach. It is also more principled, and could be
adapted to help learn other kinds of probabilistic classi-
fiers for quite different tasks.
1 Background
Many recent papers aim to reduce the amount of an-
notated data needed to train the parameters of a sta-
tistical model. Well-known paradigms include ac-
tive learning, semi-supervised learning, and either
domain adaptation or cross-lingual transfer from ex-
isting annotated data.
A rather different paradigm is to change the ac-
tual task that is given to annotators, giving them a
greater hand in shaping the learned classifier. Af-
ter all, human annotators themselves are more than
just black-box classifiers to be run on training data.
They possess some introspective knowledge about
their own classification procedure. The hope is to
mine this knowledge rapidly via appropriate ques-
tions and use it to help train a machine classifier.
How to do this, however, is still being explored.
1.1 Hand-crafted rules
An obvious option is to have the annotators directly
express their knowledge by hand-crafting rules. This
?This work was supported by National Science Foundation
grant No. 0347822 and the JHU WSE/APL Partnership Fund.
Special thanks to Christine Piatko for many useful discussions.
approach remains ?data-driven? if the annotators re-
peatedly refine their system against a corpus of la-
beled or unlabeled examples. This achieves high
performance in some domains, such as NP chunk-
ing (Brill and Ngai, 1999), but requires more analyt-
ical skill from the annotators. One empirical study
(Ngai and Yarowsky, 2000) found that it also re-
quired more annotation time than active learning.
1.2 Feature selection by humans
More recent work has focused on statistical classi-
fiers. Training such classifiers faces the ?credit as-
signment problem.? Given a training example xwith
many features, which features are responsible for its
annotated class y? It may take many training exam-
ples to distinguish useful vs. irrelevant features.1
To reduce the number of training examples
needed, one can ask annotators to examine or pro-
pose some candidate features. This is possible even
for the very large feature sets that are typically used
in NLP. In document classification, Raghavan et al
(2006) show that feature selection by an oracle could
be helpful, and that humans are both rapid and rea-
sonably good at distinguishing highly useful n-gram
features from randomly chosen ones, even when
viewing these n-grams out of context.
Druck et al (2008) show annotators some features
f from a fixed feature set, and ask them to choose a
class label y such that p(y | f) is as high as possible.
Haghighi and Klein (2006) do the reverse: for each
class label y, they ask the annotators to propose a
few ?prototypical? features f such that p(y | f) is as
high as possible.
1.3 Feature selection in context
The above methods consider features out of context.
An annotator might have an easier time examining
1Most NLP systems use thousands or millions of features,
because it is helpful to include lexical features over a large vo-
cabulary, often conjoined with lexical or non-lexical context.
31
features in context to recognize whether they appear
relevant. This is particularly true for features that
are only modestly or only sometimes helpful, which
may be abundant in NLP tasks.
Thus, Raghavan et al (2006) propose an active
learning method in which, while classifying a train-
ing document, the annotator also identifies some fea-
tures of that document as particularly relevant. E.g.,
the annotator might highlight particular unigrams as
he or she reads the document. In their proposal, a
feature that is highlighted in any document is as-
sumed to be globally more relevant. Its dimension
in feature space is scaled by a factor of 10 so that
this feature has more influence on distances or inner
products, and hence on the learned classifier.
1.4 Concerns about marking features
Despite the success of the above work, we have
several concerns about asking annotators to identify
globally relevant features.
First, a feature in isolation really does not have a
well-defined worth. A feature may be useful only in
conjunction with other features,2 or be useful only
to the extent that other correlated features are not
selected to do the same work.
Second, it is not clear how an annotator would
easily view and highlight features in context, ex-
cept for the simplest feature sets. In the phrase
Apple shares up 3%, there may be several fea-
tures that fire on the substring Apple?responding
to the string Apple, its case-invariant form apple,
its lemma apple- (which would also respond to ap-
ples), its context-dependent sense Apple2, its part
of speech noun, etc. How does the annotator indi-
cate which of these features are relevant?
Third, annotating features is only appropriate
when the feature set can be easily understood by a
human. This is not always the case. It would be hard
for annotators to read, write, or evaluate a descrip-
tion of a complex syntactic configuration in NLP or
a convolution filter in machine vision.
Fourth, traditional annotation efforts usually try to
remain agnostic about the machine learning methods
2For example, a linear classifier can learn that most training
examples satisfyA? B by setting ?A = ?5 and ?A?B = +5,
but this solution requires selecting bothA andA?B as features.
More simply, a polynomial kernel can consider the conjunction
A ?B only if both A and B are selected as features.
and features to be used. The project?s cost is justi-
fied by saying that the annotations will be reused by
many researchers (perhaps in a ?shared task?), who
are free to compete on how they tackle the learning
problem. Unfortunately, feature annotation commits
to a particular feature set at annotation time. Subse-
quent research cannot easily adjust the definition of
the features, or obtain annotation of new features.
2 Annotating Rationales
To solve these problems, we propose that annotators
should not select features but rather mark relevant
portions of the example. In earlier work (Zaidan et
al., 2007), we called these markings ?rationales.?
For example, when classifying a movie review as
positive or negative, the annotator would also high-
light phrases that supported that judgment. Figure 1
shows two such rationales.
A multi-annotator timing study (Zaidan et al,
2007) found that highlighting rationale phrases
while reading movie reviews only doubled annota-
tion time, although annotators marked 5?11 ratio-
nale substrings in addition to the simple binary class.
The benefit justified the extra time. Furthermore,
much of the benefit could have been obtained by giv-
ing rationales for only a fraction of the reviews.
In the visual domain, when classifying an im-
age as containing a zoo, the annotator might circle
some animals or cages and the sign reading ?Zoo.?
The Peekaboom game (von Ahn et al, 2006) was in
fact built to elicit such approximate yet relevant re-
gions of images. Further scenarios were discussed in
(Zaidan et al, 2007): rationale annotation for named
entities, linguistic relations, or handwritten digits.
Annotating rationales does not require the anno-
tator to think about the feature space, nor even to
know anything about it. Arguably this makes an-
notation easier and more flexible. It also preserves
the reusability of the annotated data. Anyone is free
to reuse our collected rationales (section 4) to aid
in learning a classifier with richer features, or a dif-
ferent kind of classifier altogether, using either our
procedures or novel procedures.
3 Modeling Rationale Annotations
As rationales are more indirect than explicit features,
they present a trickier machine learning problem.
32
We wish to learn the parameters ? of some classi-
fier. How can the annotator?s rationales help us to
do this without many training examples? We will
have to exploit a presumed relationship between the
rationales and the optimal value of ? (i.e., the value
that we would learn on an infinite training set).
This paper exploits an explicit, parametric model
of that relationship. The model?s parameters ? are
intended to capture what that annotator is doing
when he or she marks rationales. Most importantly,
they capture how he or she is influenced by the true
?. Given this, our learning method will prefer values
of ? that would adequately explain the rationales (as
well as the training classifications).
3.1 A generative approach
For concreteness, we will assume that the task is
document classification. Our training data consists
of n triples {(x1, y1, r1), ..., (xn, yn, rn)}), where xi
is a document, yi is its annotated class, and ri is its
rationale markup. At test time we will have to pre-
dict yn+1 from xn+1, without any rn+1.
We propose to jointly choose parameter vectors ?
and ? to maximize the following regularized condi-
tional likelihood:3
n?
i=1
p(yi, ri | xi, ?, ?) ? pprior(?, ?) (1)
def
=
n?
i=1
p?(yi | xi) ? p?(ri | xi, yi, ?) ? pprior(?, ?)
Here we are trying to model all the annotations, both
yi and ri. The first factor predicts yi using an ordi-
nary probabilistic classifier p?, while the novel sec-
ond factor predicts ri using a model p? of how an-
notators generate the rationale annotations.
The crucial point is that the second factor depends
on ? (since ri is supposed to reflect the relation be-
tween xi and yi that is modeled by ?). As a result,
the learner has an incentive to modify ? in a way
that increases the second factor, even if this some-
what decreases the first factor on training data.4
3It would be preferable to integrate out ? (and even ?), but
more difficult.
4Interestingly, even examples where the annotation yi is
wrong or unhelpful can provide useful information about ? via
the pair (yi, ri). Two annotators marking the same movie re-
view might disagree on whether it is overall a positive or nega-
After training, one should simply use the first fac-
tor p?(y | x) to classify test documents x. The sec-
ond factor is irrelevant for test documents, since they
have not been annotated with rationales r.
The second factor may likewise be omitted for any
training documents i that have not been annotated
with rationales, as there is no ri to predict in those
cases. In the extreme case where no documents are
annotated with rationales, equation (1) reduces to
the standard training procedure.
3.2 Noisy channel design of rationale models
Like ordinary class annotations, rationale annota-
tions present us with a ?credit assignment problem,?
albeit a smaller one that is limited to features that fire
?in the vicinity? of the rationale r. Some of these
?-features were likely responsible for the classifica-
tion y and hence triggered the rationale. Other such
?-features were just innocent bystanders.
Thus, the interesting part of our model is p?(r |
x, y, ?), which models the rationale annotation pro-
cess. The rationales r reflect ?, but in noisy ways.
Taking this noisy channel idea seriously, p?(r |
x, y, ?) should consider two questions when assess-
ing whether r is a plausible set of rationales given
?. First, it needs a ?language model? of rationales:
does r consist of rationales that are well-formed a
priori, i.e., before ? is considered? Second, it needs
a ?channel model?: does r faithfully signal the fea-
tures of ? that strongly support classifying x as y?
If a feature contributes heavily to the classification
of document x as class y, then the channel model
should tell us which parts of document x tend to be
highlighted as a result.
The channel model must know about the partic-
ular kinds of features that are extracted by f and
scored by ?. Suppose the feature not . . . gripping,5
with weight ?h, is predictive of the annotated class y.
This raises the probabilities of the annotator?s high-
lighting each of various words, or combinations of
words, in a phrase like not the most gripping ban-
quet on film. The channel model parameters in ?
tive review?but the second factor still allows learning positive
features from the first annotator?s positive rationales, and nega-
tive features from the second annotator?s negative rationales.
5Our current experiments use only unigram features, to
match past work, but we use this example to outline how our
approach generalizes to complex linguistic (or visual) features.
33
should specify how much each of these probabilities
is raised, based on the magnitude of ?h ? R, the
class y, and the fact that the feature is an instance
of the template <Neg> . . .<Adjective>. (Thus, ?
has no parameters specific to the word gripping; it
is a low-dimensional vector that only describes the
annotator?s general style in translating ? into r.)
The language model, however, is independent of
the feature set ?. It models what rationales tend to
look like in the input domain?e.g., documents or
images. In the document case, ? should describe:
How frequent and how long are typical rationales?
Do their edges tend to align with punctuation or ma-
jor syntactic boundaries in x? Are they rarer in the
middle of a document, or in certain documents?6
Thanks to the language model, we do not need to
posit high ? features to explain every word in a ratio-
nale. The language model can ?explain away? some
words as having been highlighted only because this
annotator prefers not to end a rationale in mid-
phrase, or prefers to sweep up close-together fea-
tures with a single long rationale rather than many
short ones. Similarly, the language model can help
explain why some words, though important, might
not have been included in any rationale of r.
If there are multiple annotators, one can learn dif-
ferent ? parameters for each annotator, reflecting
their different annotation styles.7 We found this to
be useful (section 8.2).
We remark that our generative modeling approach
(equation (1)) would also apply if r were not ratio-
nale markup, but some other kind of so-called ?side
information,? such as the feature annotations dis-
cussed in section 1. For example, Raghavan et al
(2006) assume that if feature h is relevant?a bi-
6Our current experiments do not model this last point. How-
ever, we imagine that if the document only has a few ?-features
that support the classification, the annotator will probably mark
most of them, whereas if such features are abundant, the anno-
tator may lazily mark only a few of the strongest ones. A simple
approach would equip ? with a different ?bias? or ?threshold?
parameter ?x for each rationale training document x, to mod-
ulate the a priori probability of marking a rationale in x. By
fitting this bias parameter, we deduce how lazy the annotator
was (for whatever reason) on document x. If desired, a prior
on ?x could consider whether x has many strong ?-features,
whether the annotator has recently had a coffee break, etc.
7Given insufficient rationale data to recover some annota-
tor?s ?well, one could smooth using data from other annotators.
But in our situation, ? had relatively few parameters to learn.
nary distinction?iff it was selected in at least one
document. But it might be more informative to ob-
serve that h was selected in 3 of the 10 documents
where it appeared, and to predict this via a model
p?(3 of 10 | ?h), where ? describes (e.g.) how to de-
rive a binomial parameter nonlinearly from ?h. This
approach would not how often h was marked and in-
fer how relevant is feature h (i.e., infer ?h). In this
case, p? is a simple channel that transforms relevant
features into direct indicators of the feature. Our
side information merely requires a more complex
transformation?from relevant features into well-
formed rationales, modulated by documents.
4 Experimental Data: Movie Reviews
In Zaidan et al (2007), we introduced the ?Movie
Review Polarity Dataset Enriched with Annotator
Rationales.?8 It is based on the dataset of Pang and
Lee (2004),9 which consists of 1000 positive and
1000 negative movie reviews, tokenized and divided
into 10 folds (F0?F9). All our experiments use F9
as their final blind test set.
The enriched dataset adds rationale annotations
produced by an annotator A0, who annotated folds
F0?F8 of the movie review set with rationales (in the
form of textual substrings) that supported the gold-
standard classifications. We will use A0?s data to
determine the improvement of our method over a
(log-linear) baseline model without rationales. We
also use A0 to compare against the ?masking SVM?
method and SVM baseline of Zaidan et al (2007).
Since ? can be tuned to a particular annotator, we
would also like to know how well this works with
data from annotators other than A0. We randomly
selected 100 reviews (50 positive and 50 negative)
and collected both class and rationale annotation
data from each of six new annotators A3?A8,10 fol-
lowing the same procedures as (Zaidan et al, 2007).
We report results using only data from A3?A5, since
we used the data from A6?A8 as development data
in the early stages of our work.
We use this new rationale-enriched dataset8 to de-
termine if our method works well across annotators.
We will only be able to carry out that comparison
8Available at http://cs.jhu.edu/?ozaidan/rationales.
9Polarity dataset version 2.0.
10We avoid annotator names A1?A2, which were already
used in (Zaidan et al, 2007).
34
Figure 1: Rationales as sequence an-
notation: the annotator highlighted
two textual segments as rationales for
a positive class. Highlighted words in
~x are tagged I in ~r, and other words
are tagged O. The figure also shows
some ?-features. For instance, gO(,)-I
is a count of O-I transitions that occur
with a comma as the left word. Notice
also that grel is the sum of the under-
lined values.
at small training set sizes, due to limited data from
A3?A8. The larger A0 dataset will still allow us to
evaluate our method on a range of training set sizes.
5 Detailed Models
5.1 Modeling class annotations with p?
We define the basic classifier p? in equation (1) to be
a standard conditional log-linear model:
p?(y | x)
def
=
exp(~? ? ~f(x, y))
Z?(x)
def
=
u(x, y)
Z?(x)
(2)
where ~f(?) extracts a feature vector from a classified
document, ~? are the corresponding weights of those
features, and Z?(x)
def
=
?
y u(x, y) is a normalizer.
We use the same set of binary features as in pre-
vious work on this dataset (Pang et al, 2002; Pang
and Lee, 2004; Zaidan et al, 2007). Specifically, let
V = {v1, ..., v17744} be the set of word types with
count ? 4 in the full 2000-document corpus. Define
fh(x, y) to be y if vh appears at least once in x, and
0 otherwise. Thus ? ? R17744, and positive weights
in ? favor class label y = +1 and equally discourage
y = ?1, while negative weights do the opposite.
This standard unigram feature set is linguistically
impoverished, but serves as a good starting point for
studying rationales. Future work should consider
more complex features and how they are signaled by
rationales, as discussed in section 3.2.
5.2 Modeling rationale annotations with p?
The rationales collected in this task are textual seg-
ments of a document to be classified. The docu-
ment itself is a word token sequence ~x = x1, ..., xM .
We encode its rationales as a corresponding tag se-
quence ~r = r1, ..., rM , as illustrated in Figure 1.
Here rm ? {I, O} according to whether the token
xm is in a rationale (i.e., xm was at least partly high-
lighted) or outside all rationales. x1 and xM are
special boundary symbols, tagged with O.
We predict the full tag sequence ~r at once using
a conditional random field (Lafferty et al, 2001). A
CRF is just another conditional log-linear model:
p?(r |x, y, ~?)
def
=
exp(~? ? ~g(r, x, y, ~?))
Z?(x, y, ~?)
def
=
u(r, x, y, ~?)
Z?(x, y, ~?)
where ~g(?) extracts a feature vector, ~? are the
corresponding weights of those features, and
Z?(x, y, ~?)
def
=
?
r u(r, x, y,
~?) is a normalizer.
As usual for linear-chain CRFs, ~g(?) extracts two
kinds of features: first-order ?emission? features that
relate rm to (xm, y, ?), and second-order ?transi-
tion? features that relate rm to rm?1 (although some
of these also look at x).
These two kinds of features respectively capture
the ?channel model? and ?language model? of sec-
tion 3.2. The former says rm is I because xm is
associated with a relevant ?-feature. The latter says
rm is I simply because it is next to another I.
5.3 Emission ?-features (?channel model?)
Recall that our ?-features (at present) correspond to
unigrams. Given (~x, y, ~?), let us say that a unigram
w ? ~x is relevant, irrelevant, or anti-relevant if
y ? ?w is respectively 0, ? 0, or 0. That is, w
is relevant if its presence in x strongly supports the
annotated class y, and anti-relevant if its presence
strongly supports the opposite class ?y.
35
Figure 2: The
function family Bs
in equation (3),
shown for s ?
{10, 2,?2,?10}.
We would like to learn the extent ?rel to which
annotators try to include relevant unigrams in their
rationales, and the (usually lesser) extent ?antirel to
which they try to exclude anti-relevant unigrams.
This will help us infer ~? from the rationales.
The details are as follows. ?rel and ?antirel are the
weights of two emission features extracted by ~g:
grel(~x, y, ~r, ~?)
def
=
M?
m=1
I(rm = I) ?B10(y ? ?xm)
gantirel(~x, y, ~r, ~?)
def
=
M?
m=1
I(rm = I) ?B?10(y ? ?xm)
Here I(?) denotes the indicator function, returning
1 or 0 according to whether its argument is true or
false. Relevance and negated anti-relevance are re-
spectively measured by the differentiable nonlinear
functions B10 and B?10, which are defined by
Bs(a) = (log(1 + exp(a ? s))? log(2))/s (3)
and graphed in Figure 2. Sample values of B10 and
grel are shown in Figure 1.
How does this work? The grel feature is a sum
over all unigrams in the document ~x. It does not fire
strongly on the irrelevant or anti-relevant unigrams,
since B10 is close to zero there.11 But it fires posi-
tively on relevant unigrams w if they are tagged with
I, and the strength of such firing increases approxi-
mately linearly with ?w. Since the weight ?rel > 0 in
practice, this means that raising a relevant unigram?s
?w (if y = +1) will proportionately raise its log-
odds of being tagged with I. Symmetrically, since
?antirel > 0 in practice, lowering an anti-relevant un-
igram?s ?w (if y = +1) will proportionately lower
11B10 sets the threshold for relevance to be about 0. One
could also include versions of the grel feature that set a higher
threshold, using B10(y ? ?xm ? threshold).
its log-odds of being tagged with I, though not nec-
essarily at the same rate as for relevant unigrams.12
Should ? also include traditional CRF emis-
sion features, which would recognize that particular
words like great tend to be tagged as I? No! Such
features would undoubtedly do a better job predict-
ing the rationales and hence increasing equation (1).
However, crucially, our true goal is not to predict
the rationales but to recover the classifier parame-
ters ?. Thus, if great tends to be highlighted, then
the model should not be permitted to explain this
directly by increasing some feature ?great, but only
indirectly by increasing ?great. We therefore permit
our rationale prediction model to consider only the
two emission features grel and gantirel, which see the
words in ~x only through their ?-values.
5.4 Transition ?-features (?language model?)
Annotators highlight more than just the relevant un-
igrams. (After all, they aren?t told that our current
?-features are unigrams.) They tend to mark full
phrases, though perhaps taking care to exclude anti-
relevant portions. ?models these phrases? shape, via
weights for several ?language model? features.
Most important are the 4 traditional CRF tag tran-
sition features gO-O, gO-I, gI-I, gI-O. For example,
gO-I counts the number of O-to-I transitions in ~r
(see Figure 1). Other things equal, an annotator with
high ?O-I is predicted to have many rationales per
1000 words. And if ?I-I is high, rationales are pre-
dicted to be long phrases (including more irrelevant
unigrams around or between the relevant ones).
We also learn more refined versions of these fea-
tures, which consider how the transition probabil-
ities are influenced by the punctuation and syntax
of the document ~x (independent of ~?). These re-
fined features are more specific and hence more
sparsely trained. Their weights reflect deviations
from the simpler, ?backed-off? transition features
such as gO-I. (Again, see Figure 1 for examples.)
Conditioning on left word. A feature of the form
gt1(v)-t2 is specified by a pair of tag types t1, t2 ?
{I,O} and a vocabulary word type v. It counts the
12If the two rates are equal (?rel = ?antirel), we get a simpler
model in which the log-odds change exactly linearly with ?w for
each w, regardless of w?s relevance/irrelevance/anti-relevance.
This follows from the fact thatBs(a)+B?s(a) simplifies to a.
36
number of times an t1?t2 transition occurs in ~r con-
ditioned on v appearing as the first of the two word
tokens where the transition occurs. Our experiments
include gt1(v)-t2 features that tie I-O and O-I tran-
sitions to the 4 most frequent punctuation marks v
(comma, period, ?, !).
Conditioning on right word. A feature gt1-t2(v)
is similar, but v must appear as the second of the
two word tokens where the transition occurs. Again
here, we use gt1-t2(v) features that tie I-O and O-I
transitions to the four punctuation marks mentioned
above. We also include five features that tie O-I
transitions to the words no, not, so, very, and quite,
since in our development data, those words were
more likely than others to start rationales.13
Conditioning on syntactic boundary. We parsed
each rationale-annotated training document (no
parsing is needed at test time).14 We then marked
each word bigram x1-x2 with three nonterminals:
NEnd is the nonterminal of the largest constituent
that contains x1 and not x2, NStart is the nontermi-
nal of the largest constituent that contains x2 and
not x1, and NCross is the nonterminal of the smallest
constituent that contains both x1 and x2.
For a nonterminalN and pair of tag types (t1, t2),
we define three features, gt1-t2/E=N , gt1-t2/S=N ,
and gt1-t2/C=N , which count the number of times
a t1-t2 transition occurs in ~r with N matching the
NEnd, NStart, or NCross nonterminal, respectively.
Our experiments include these features for 11 com-
mon nonterminal types N (DOC, TOP, S, SBAR,
FRAG, PRN, NP, VP, PP, ADJP, QP).
6 Training: Joint Optimization of ? and ?
To train our model, we use L-BFGS to locally max-
imize the log of the objective function (1):15
13These are the function words with count ? 40 in a random
sample of 100 documents, and which were associated with the
O-I tag transition at more than twice the average rate. We do
not use any other lexical ?-features that reference ~x, for fear that
they would enable the learner to explain the rationales without
changing ? as desired (see the end of section 5.3).
14We parse each sentence with the Collins parser (Collins,
1999). Then the document has one big parse tree, whose root is
DOC, with each sentence being a child of DOC.
15One might expect this function to be convex because p? and
p? are both log-linear models with no hidden variables. How-
ever, log p?(ri | xi, yi, ?) is not necessarily convex in ?.
n?
i=1
log p?(yi | xi)?
1
2?2?
???2
+C(
n?
i=1
log p?(ri | xi, yi, ?))?
1
2?2?
???2 (4)
This defines pprior from (1) to be a standard diago-
nal Gaussian prior, with variances ?2? and ?
2
? for the
two sets of parameters. We optimize ?2? in our ex-
periments. As for ?2?, different values did not affect
the results, since we have a large number of {I,O}
rationale tags to train relatively few ? weights; so
we simply use ?2? = 1 in all of our experiments.
Note the new C factor in equation (4). Our ini-
tial experiments showed that optimizing equation (4)
without C led to an increase in the likelihood of the
rationale data at the expense of classification accu-
racy, which degraded noticeably. This is because
the second sum in (4) has a much larger magnitude
than the first: in a set of 100 documents, it predicts
around 74,000 binary {I,O} tags, versus the one
hundred binary class labels. While we are willing
to reduce the log-likelihood of the training classifi-
cations (the first sum) to a certain extent, focusing
too much on modeling rationales (the second sum)
is clearly not our ultimate goal, and so we optimize
C on development data to achieve some balance be-
tween the two terms of equation (4). Typical values
of C range from 1300 to
1
50 .
16
We perform alternating optimization on ? and ?:
1. Initialize ? to maximize equation (4) but with
C = 0 (i.e. based only on class data).
2. Fix ?, and find ? that maximizes equation (4).
3. Fix ?, and find ? that maximizes equation (4).
4. Repeat 2 and 3 until convergence.
The L-BFGS method requires calculating the gra-
dient of the objective function (4). The partial
derivatives with respect to components of ? and ?
involve calculating expectations of the feature func-
tions, which can be computed in linear time (with
respect to the size of the training set) using the
forward-backward algorithm for CRFs. The par-
tial derivatives also involve the derivative of (3),
to determine how changing ? will affect the firing
strength of the emission features grel and gantirel.
16C also balances our confidence in the classifications y
against our confidence in the rationales r; either may be noisy.
37
7 Experimental Procedures
We report on two sets of experiments. In the first
set, we use the annotation data that A3?A5 provided
for the small set of 100 documents (as well as the
data from A0 on those same 100 documents). In
the second set, we used A0?s abundant annotation
data to evaluate our method with training set sizes up
to 1600 documents, and compare it with three other
methods: log-linear baseline, SVM baseline, and the
SVM masking method of (Zaidan et al, 2007).
7.1 Learning curves
The learning curves reported in section 8.1 are gen-
erated exactly as in (Zaidan et al, 2007). Each curve
shows classification accuracy at training set sizes
T = 1, 2, ..., 9 folds (i.e. 200, 400, ..., 1600 training
documents). For a given size T , the reported accu-
racy is an average of 9 experiments with different
subsets of the entire training set, each of size T :
1
9
8?
i=0
acc(F9 | Fi+1 ? . . . ? Fi+T ) (5)
where Fj denotes the fold numbered j mod 9, and
acc(F9 | Y ) means classification accuracy on the
held-out test set F9 after training on set Y .
We use an appropriate paired permutation test, de-
tailed in (Zaidan et al, 2007), to test differences in
(5). We call a difference significant at p < 0.05.
7.2 Comparison to ?masking SVM? method
We compare our method to the ?masking SVM?
method of (Zaidan et al, 2007). Briefly, that method
used rationales to construct several so-called con-
trast examples from every training example. A con-
trast example is obtained by ?masking out? one of
the rationales highlighted to support the training ex-
ample?s class. A good classifier should have more
trouble on this modified example. Hence, Zaidan et
al. (2007) required the learned SVM to classify each
contrast example with a smaller margin than the cor-
responding original example (and did not require it
to be classified correctly).
The masking SVM learner relies on a simple geo-
metric principle; is trivial to implement on top of an
existing SVM learner; and works well. However, we
believe that the generative method we present here is
more interesting and should apply more broadly.
Figure 3: Classification accuracy curves for the 4 meth-
ods: the two baseline learners that only utilize class data,
and the two learners that also utilize rationale annota-
tions. The SVM curves are from (Zaidan et al, 2007).
First, the masking method is specific to improving
an SVM learner, whereas our method can be used to
improve any classifier by adding a rationale-based
regularizer (the second half of equation (4)) to its
objective function during training.
More important, there are tasks where it is unclear
how to generate contrast examples. For the movie
review task, it was natural to mask out a rationale
by pretending its words never occurred in the doc-
ument. After all, most word types do not appear in
most documents, so it is natural to consider the non-
presence of a word as a ?default? state to which we
can revert. But in an image classification task, how
should one modify the image?s features to ignore
some spatial region marked as a rationale? There is
usually no natural ?default? value to which we could
set the pixels. Our method, on the other hand, elim-
inates contrast examples altogether.
8 Experimental Results and Analysis
8.1 The added benefit of rationales
Fig. 3 shows learning curves for four methods. A
log-linear model shows large and significant im-
provements, at all training sizes, when we incor-
porate rationales into its training via equation (4).
Moreover, the resulting classifier consistently out-
performs17 prior work, the masking SVM, which
starts with a slightly better baseline classifier (an
SVM) but incorporates the rationales more crudely.
17Differences are not significant at sizes 200, 1000, and 1600.
38
size A0 A3 A4 A5
SVM baseline 100 72.0 72.0 72.0 70.0
SVM+contrasts 100 75.0 73.0 74.0 72.0
Log-linear baseline 100 71.0 73.0 71.0 70.0
Log-linear+rats 100 76.0 76.0 77.0 74.0
SVM baseline 20 63.4 62.2 60.4 62.6
SVM+contrasts 20 65.4 63.4 62.4 64.8
Log-linear baseline 20 63.0 62.2 60.2 62.4
Log-linear+rats 20 65.8 63.6 63.4 64.8
Table 1: Accuracy rates using each annotator?s data. In a
given column, a value in italics is not significantly differ-
ent from the highest value in that column, which is bold-
faced. The size=20 results average over 5 experiments.
To confirm that we could successfully model an-
notators other than A0, we performed the same
comparison for annotators A3?A5; each had pro-
vided class and rationale annotations on a small 100-
document training set. We trained a separate ? for
each annotator. Table 1 shows improvements over
baseline, usually significant, at 2 training set sizes.
8.2 Analysis
Examining the learned weights ~? gives insight into
annotator behavior. High weights include I-O and
O-I transitions conditioned on punctuation, e.g.,
?I(.)-O = 3.55,
18 as well as rationales ending at the
end of a major phrase, e.g., ?I-O/E=VP = 1.88.
The large emission feature weights, e.g., ?rel =
14.68 and ?antirel = 15.30, tie rationales closely to
? values, as hoped. For example, in Figure 1, the
word w = succeeds, with ?w = 0.13, drives up
p(I)/p(O) by a factor of 7 (in a positive document)
relative to a word with ?w = 0.
In fact, feature ablation experiments showed that
almost all the classification benefit from rationales
can be obtained by using only these 2 emission
?-features and the 4 unconditioned transition ?-
features. Our full ? (115 features) merely improves
our ability to predict the rationales (whose likeli-
hood does increase significantly with more features).
We also checked that annotators? styles differ
enough that it helps to tune ? to the ?target? annota-
torAwho gave the rationales. Table 3 shows that a ?
model trained onA?s own rationales does best at pre-
dicting new rationales fromA. Table 2 shows that as
18When trained on folds F4?F8 with A0?s rationales.
?A0 ?A3 ?A4 ?A5 Baseline
?A0 76.0 73.0 74.0 73.0 71.0
?A3 73.0 76.0 74.0 73.0 73.0
?A4 75.0 73.0 77.0 74.0 71.0
?A5 74.0 71.0 72.0 74.0 70.0
Table 2: Accuracy rate for an annotator?s ? (rows) ob-
tained when using some other annotator?s ? (columns).
Notice that the diagonal entries and the baseline column
are taken from rows of Table 1 (size=100).
Trivial
?A0 ?A3 ?A4 ?A5 model
?L(rA0) 0.073 0.086 0.077 0.088 0.135
?L(rA3) 0.084 0.068 0.071 0.068 0.130
?L(rA4) 0.088 0.084 0.075 0.085 0.153
?L(rA5) 0.058 0.044 0.047 0.044 0.111
Table 3: Cross-entropy per tag of rationale annotations
~r for each annotator (rows), when predicted from that
annotator?s ~x and ~? via a possibly different annotator?s
? (columns). For comparison, the trivial model is a bi-
gram model of ~r, which is trained on the target annotator
but ignores ~x and ~?. 5-fold cross-validation on the 100-
document set was used to prevent testing on training data.
a result, classification performance on the test set is
usually best if it wasA?s own ? that was used to help
learn ? from A?s rationales. In both cases, however,
a different annotator?s ? is better than nothing.
9 Conclusions
We have demonstrated a effective method for elic-
iting extra knowledge from naive annotators, in
the form of lightweight ?rationales? for their an-
notations. By explicitly modeling the annotator?s
rationale-marking process, we are able to infer a bet-
ter model of the original annotations.
We showed that our method performs signifi-
cantly better than two strong baseline classifiers,
and also outperforms our previous discriminative
method for exploiting rationales (Zaidan et al,
2007). We also saw that it worked across four anno-
tators who have different rationale-marking styles.
In future, we are interested in new domains that
can adaptively solicit rationales for some or all
training examples. Our new method, being essen-
tially Bayesian inference, is potentially extensible to
many other situations?other tasks, classifier archi-
tectures, and more complex features.
39
References
Eric Brill and Grace Ngai. 1999. Man [and woman] vs.
machine: A case study in base noun phrase learning.
In Proceedings of the 37th ACL Conference.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of ACM Special Interest
Group on Information Retrieval, (SIGIR).
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Proceedings of the Hu-
man Language Technology Conference of the NAACL,
Main Conference, pages 320?327, New York City,
USA, June. Association for Computational Linguis-
tics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning.
Grace Ngai and David Yarowsky. 2000. Rule writing
or annotation: Cost-efficient resource usage for base
noun phrase chunking. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 117?125, Hong Kong.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. of ACL, pages 271?
278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proc. of EMNLP, pages 79?86.
Hema Raghavan and James Allan. 2007. An interactive
algorithm for asking and incorporating feature feed-
back into support vector machines. In Proceedings of
SIGIR.
Hema Raghavan, Omid Madani, and Rosie Jones. 2006.
Active learning on both features and instances. Jour-
nal of Machine Learning Research, 7:1655?1686,
Aug.
Luis von Ahn, Ruoran Liu, and Manuel Blum. 2006.
Peekaboom: A game for locating objects. In CHI
?06: Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems, pages 55?64.
Omar Zaidan, Jason Eisner, and Christine Piatko. 2007.
Using ?annotator rationales? to improve machine
learning for text categorization. In NAACL HLT 2007;
Proceedings of the Main Conference, pages 260?267,
April.
40
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 52?61,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Feasibility of Human-in-the-loop Minimum Error Rate Training
Omar F. Zaidan and Chris Callison-Burch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,ccb}@cs.jhu.edu
Abstract
Minimum error rate training (MERT) in-
volves choosing parameter values for a
machine translation (MT) system that
maximize performance on a tuning set as
measured by an automatic evaluation met-
ric, such as BLEU. The method is best
when the system will eventually be eval-
uated using the same metric, but in reality,
most MT evaluations have a human-based
component. Although performing MERT
with a human-based metric seems like a
daunting task, we describe a new metric,
RYPT, which takes human judgments into
account, but only requires human input to
build a database that can be reused over
and over again, hence eliminating the need
for human input at tuning time. In this
investigative study, we analyze the diver-
sity (or lack thereof) of the candidates pro-
duced during MERT, we describe how this
redundancy can be used to our advantage,
and show that RYPT is a better predictor of
translation quality than BLEU.
1 Introduction
Many state-of-the-art machine translation (MT)
systems over the past few years (Och and Ney,
2002; Koehn et al, 2003; Chiang, 2007; Koehn
et al, 2007; Li et al, 2009) rely on several mod-
els to evaluate the ?goodness? of a given candidate
translation in the target language. The MT system
proceeds by searching for the highest-scoring can-
didate translation, as scored by the different model
components, and returns that candidate as the hy-
pothesis translation. Each of these models need
not be a probabilistic model, and instead corre-
sponds to a feature that is a function of a (can-
didate translation,foreign sentence) pair.
Treated as a log-linear model, we need to as-
sign a weight for each of the features. Och (2003)
shows that setting those weights should take into
account the evaluation metric by which the MT
system will eventually be judged. This is achieved
by choosing the weights so as to maximize the per-
formance of the MT system on a development set,
as measured by that evaluation metric. The other
insight of Och?s work is that there exists an ef-
ficient algorithm to find such weights. This pro-
cess has come to be known as the MERT phase
(for Minimum Error Rate Training) in training
pipelines of MT systems.
A problem arises if the performance of the sys-
tem is not judged by an automatic evaluation met-
ric such as BLEU or TER, but instead through
an evaluation process involving a human. The
GALE evaluation, for instance, judges the quality
of systems as measured by human-targeted TER
(HTER), which computes the edit distance be-
tween the system?s output and a version of the
output post-edited by a human. The IWSLT and
WMT workshops also have a manual evaluation
component, as does the NIST Evaluation, in the
form of adequacy and fluency (LDC, 2005).
In theory, one could imagine trying to optimize
a metric like HTER during the MERT phase, but
that would require the availability of an HTER au-
tomatic scorer, which, by definition, does not ex-
ist. If done manually, the scoring of thousands of
candidates produced during MERT would literally
take weeks, and cost a large sum of money. For
these reasons, researchers resort to optimizing an
automatic metric (almost always BLEU) as a proxy
for human judgment.
As daunting as such a task seems for any
human-based metric, we describe a new metric,
RYPT, that takes human judgment into accout
when scoring candidates, but takes advantage of
the redundancy in the candidates produced dur-
ing MERT. In this investigative study, we describe
how this redundancy can be used to our advantage
to eliminate the need to involve a human at any
52
time except when building a database of reusable
judgments, and furthermore show that RYPT is a
better predictor of translation quality than BLEU,
making it an excellent candidate for MERT tun-
ing.
The paper is organized as follows. We start by
describing the core idea of MERT before intro-
ducing our new metric, RYPT, and describing the
data collection effort we undertook to collect the
needed human judgments. We analyze a MERT
run optimizing BLEU to quantify the level of re-
dundancy in the candidate set, and also provide
an extensive analysis of the collected judgments,
before describing a set of experiments showing
RYPT is a better predictor of translation quality
than BLEU. Following a discussion of our findings,
we briefly review related work, before pointing out
future directions and summarizing.
2 Och?s Line Search Method
A common approach to translating a source sen-
tence f in a foreign language is to select the can-
didate translation e that maximizes the posterior
probability:
Pr(e | f)
def
=
exp(s
?
(e, f))
?
e
?
exp(s
?
(e
?
, f))
.
This defines Pr(e | f) using a log-linear model
that associates a sentence pair (e, f) with a fea-
ture vector ?(e, f) = {?
1
(e, f), ..., ?
M
(e, f)},
and assigns a score
s
?
(e, f)
def
= ? ? ?(e, f) =
M
?
m=1
?
m
?
m
(e, f)
for that sentence pair, with the feature weights
? = {?
1
, ..., ?
M
} being the parameters of the
model. Therefore, the system selects the transla-
tion e?:
e? = argmax
e
Pr(e | f) = argmax
e
s
?
(e, f). (1)
Och (2003) provides evidence that ? should be
chosen by optimizing an objective function basd
on the evaluation metric of interest, rather than
likelihood. Since the error surface is not smooth,
and a grid search is too expensive, Och suggests an
alternative, efficient, line optimization approach.
Assume we are performing a line optimiza-
tion along the d
th
dimension. Consider a for-
eign sentence f , and let the candidate set for f
be {e
1
, ..., e
K
}. Recall from (1) that the 1-best
candidate at a given ? is the one with maxi-
mum
?
M
m=1
?
m
?
m
(e
k
, f). We can rewrite the
sum as ?
d
?
d
(e
k
, f) +
?
m 6=d
?
m
?
m
(e
k
, f). The
second term is constant with respect to ?
d
, and
so is ?
d
(e
k
, f). Renaming those two quantities
offest
?
(e
k
) and slope(e
k
), we get
s
?
(e
k
, f) = slope(e
k
)?
d
+ offset
?
(e
k
).
Therefore, if we plot the score for a candidate
translation vs. ?
d
, that candidate will be repre-
sented by a line. If we plot the lines for all candi-
dates (Figure 1), then the upper envelope of these
lines indicates the best candidate at any value for
?
d
.
Therefore, the objective function is piece-wise
linear across any of the M dimensions
1
, mean-
ing we only need to evaluate it at the ?critical?
points corresponding to line intersection points.
Furthermore, we only need to calculate the suffi-
cient statistics once, at the smallest critical point,
and then simply adjust the sufficient statistics to
reflect changes in the set of 1-best candidates.
2.1 The BLEU Metric
The metric most often used with MERT is BLEU
(Papineni et al, 2002), where the score of a candi-
date c against a reference translation r is:
BLEU = BP (len(c), len(r))?exp(
4
?
n=1
1
4
log p
n
),
where p
n
is the n-gram precision
2
and BP is a
brevity penalty meant to penalize short outputs, to
discourage improving precision at the expense of
recall.
There are several compelling reasons to opti-
mize to BLEU. It is the most widely reported met-
ric in MT research, and has been shown to cor-
relate well with human judgment (Papineni et al,
2002; Coughlin, 2003). But BLEU is also partic-
ularly suitable for MERT, because it can be com-
puted quite efficiently, and its sufficient statistics
are decomposable, as required by MERT.
3,4
1
Or, in fact, along any linear combination of the M di-
mensions.
2
Modifed precision, to be precise, based on clipped n-
gram counts.
3
Note that for the sufficient statistics to be decomposable,
the metric itself need not be ? this is in fact the case with
BLEU.
4
Strictly speaking, the sufficient statistics need not be de-
53
e1 e21 2
e1 e21 4
e1 e22 4
e1 e23 1
e1 2
e2 1
e1 3
e1 1 e1 4 e2 2
e2 3
e2 4
1: 
[6,
10
]
2: 
[1,
10
]
3: 
[3,
10
]
4: 
[4,
10
]
e1 e1 e1 e1
1: 
[3,
15
]
2: 
[8,
15
]
3: 
[9,
15
]
4: 
[2,
15
]
e2 e2 e2 e2
e1 e23 3
score(e,f
1
) TERscore(e,f
2
)
0.4
8
[12
,25
]
0.2
4
[6,
25
]
0.5
6
[14
,25
] 0
.32[8,
25
]
0.1
2
[3,
25
]
? d ? d? d
? ???
+
TE
R 
su
ff.
 st
at
s f
or
 
ca
nd
ida
te
s. 
Th
e 
SS
 
fo
r 
m
ea
n 
6 
ed
its
 
ar
e 
ne
ed
ed
 to
 m
at
ch
 
a 
10
-w
or
d 
re
fe
re
nc
e.
e1 1
Oc
h?s
 m
et
ho
d 
ap
pli
ed
 t
o 
a 
se
t 
of
 t
wo
 
fo
re
ign
 s
en
te
nc
es
. C
an
did
at
es
 c
or
re
sp
on
d 
to
 li
ne
s, 
an
d 
en
ve
lop
es
 o
f t
op
-m
os
t l
in
es
co
rre
sp
on
d 
to
 a
rg
m
ax
in 
Eq
. 1
. T
he
 s
et
 o
f 
1-
be
st 
ca
nd
ida
te
s 
an
d 
th
e 
er
ro
r 
m
et
ric
 
(T
ER
) 
ch
an
ge
 
on
ly 
at
 
fo
ur
 
cr
itic
al 
? d
va
lue
s. 
Nu
m
be
rs
 (
)
 in
 s
qu
ar
e 
br
ac
ke
ts 
ar
e 
th
e 
ov
er
all
 s
uf
fic
ien
t s
ta
tis
tic
s 
(S
S)
 fo
r 
TE
R,
 a
nd
 a
re
 th
e 
su
m
 o
f S
S 
fo
r i
nd
ivi
du
al 
1-
be
st 
ca
nd
ida
te
s 
(
). 
Th
is 
su
m
 is
 o
nly
 
do
ne
 o
nc
e 
to
 o
bt
ain
 [
14
,2
5]
, 
an
d 
th
en
 
sim
ply
 a
dju
ste
d 
ap
pr
op
ria
te
ly 
to
 r
ef
lec
t 
ch
an
ge
(s
) i
n 
1-
be
st 
ca
nd
ida
te
s.
Figure 1: Och?s method applied to a set of two foreign sentences. This figure is essentially a visualization
of equation (1). We show here sufficient statistics for TER for simplicity, since there are only 2 of them,
but the metric optimized in MERT is usually BLEU.
In spite of these advantages, recent work has
pointed out a number of problematic aspects of
BLEU that should cause one to pause and recon-
sider the reliance on it. Chiang et al (2008) in-
vestigate several weaknesses in BLEU and show
there are realistic scenraios where the BLEU score
should not be trusted, and in fact behaves in a
counter-intuitive manner. Furthermore, Callison-
Burch et al (2006) point out that it is not always
appropriate to use BLEU to compare systems to
each other. In particular, the quality of rule-based
systems is usually underestimated by BLEU.
All this raises doubts regarding BLEU?s ade-
quacy as a proxy for human judgment, which is
a particularly important issue in the context of set-
ting parameters during the MERT phase. But what
is the alternative?
2.2 (Non-)Applicability of Och?s Method to
Human Metrics
In principle, MERT is applicable to any evalua-
tion metric, including HTER, as long as its suffi-
cient statistics are decomposable.
4
In practice, of
course, the method requires the evaluation of thou-
sands of candidate translations. Whereas this is
composable in MERT, as they can be recalculated at each crit-
ical point. However, this would slow down the optimization
process quite a bit, since one cannot traverse the dimension
by simply adjusting the sufficient statistics to reflect changes
in 1-best candidates.
not a problem with a metric like BLEU, for which
automatic (and fast) scorers are available, such an
evaluation with a human metric would require a
large amount of effort and money, meaning that
a single MERT run would take weeks to com-
plete, and would cost thousands of dollars. As-
sume a single candidate string takes 10 seconds
to post-edit, at a cost of $0.10. Even with such
an (overly) optimistic estimate, scoring 100 candi-
dates for each of 1000 sentences would take 35 8-
hour work days and cost $10,000. The cost would
further grow linearly with the number of MERT it-
erations and the n-best list size. On the other hand,
optimizing for BLEU takes on the order of minutes
per iteration, and costs nothing.
2.3 The RYPT Metric
We suggest here a new metric that combines the
best of both worlds, in that it is based on human
judgment, but that is a viable metric to be used in
the MERT phase. The key to the feasiblity is the
reliance on a database of human judgment rather
than immendiate feedback for each candidate, and
so human feedback is only needed once, and the
collected human judgments can be reused over and
over again by an automatic scorer.
The basic idea is to reward syntactic con-
stituents in the source sentence that get algned
to ?acceptable? translations in the candidate sen-
54
S{0-11}
S{0-10} X{11-11}
S{0-7} X{8-10}
X{8-8}
X{10-10}
X{0-7}
X{4-6}X{0-2}
X{1-1} X{4-4}
official forecasts    are  based on only 3 per cent reported   ,   bloomberg    .
offizielle prognosen sind von nur 3 prozent ausgegangen  ,  meldete bloomberg .
Y
N
Y
Y
Y Y Y
N
N
Y Y Y
N
Y
ROOT
Y NY
Y
Label Y indicates 
forecasts deemed 
acceptable translation 
of prognosen.
Range 0-2 indicates 
coverage of the first 
3 words in the 
source sentence.
Figure 2: The source parse tree (top) and the can-
didate derivation tree (bottom). Nodes in the parse
tree with a thick border correspond to the frontier
node set with maxLen = 4. The human annota-
tor only sees the portion surrounded by the dashed
rectangle, including the highlighting (though ex-
cluding the word alignment links).
tence, and penalize constituents that do not. For
instance, consider the source-candidate sentence
pair of Figure 2. To evaluate the candidate transla-
tion, the source parse tree is first obtained (Dubey,
2005), and each subtree is matched with a sub-
string in the candidate string. If the source sub-
string covered by this subtree is translated into an
acceptable substring in the candidate, that node
gets a YES label. Otherwise, the node gets a NO
label.
The metric we propose is taken to be the ratio of
YES nodes in the parse tree (or RYPT). The candi-
date in Figure 2, for instance, would get a RYPT
score of 13/18 = 0.72.
To justify its use as a proxy for HTER-like met-
rics, we need to demonstrate that this metric corre-
lates well with human judgment. But it is also im-
portant to show that we can obtain the YES/NO la-
bel assignments in an efficient and affordable man-
ner. At first glance, this seems to require a human
to provide judgments for each candidate, much
like with HTER. But we describe in the next sec-
tion strategies that minimize the number of judg-
ments we need to actually collect.
3 Collecting Human Judgments
The first assumption we make to minimize the
number of human judgments, is that once we
have a judgment for a source-candidate substring
pair, that same judgment can be used across all
candidates for this source sentence. In other
words, we build a database for each source sen-
tence, which consists of <source substring,target
substring,judgment> entries. For a given source
substring, multiple entries exist, each with a dif-
ferent target candidate substring. The judgment
field is one of YES, NO, and NOT SURE.
Note that the entries do not store the full can-
didate string, since we reuse a judgment across
all the candidates of that source sentence. For in-
stance, if we collect the judgment:
<der patient,the patient,YES>
from the sentence pair:
der patient wurde isoliert .
the patient was isolated .
then this would apply to any candidate translation
of this source sentence. And so all of the following
substrings are labeled YES as well:
the patient isolated .
the patient was in isolation .
the patient has been isolated .
Similarly, if we collect the judgment:
<der patient,of the patient,NO>
from the sentence pair:
der patient wurde isoliert .
of the patient was isolated .
then this would apply to any candidate translation
of the source, and the following substrings are la-
beled NO as well:
of the patient isolated .
of the patient was in isolation .
of the patient has been isolated .
The strategy of using judgments across candi-
dates reduces the amount of labels we need to col-
lect, but evaluating a candidate translation for the
source sentence of Figure 2 would still require ob-
taining 18 labels, one for each node in the parse
tree. Instead of querying a human for each one
55
of those nodes, it is quite reasonable to percolate
existing labels up and down the parse tree: if a
node is labeled NO, this likely means that all its
ancestors would also be labeled NO, and if a node
is labeled YES, this likely means that all its de-
scendents whould also be labeled YES.
While those two strategies (using judgments
across candidates, and percolating labels up and
down the tree) are only approximations for the true
labels, employing them considerably reduces the
amount of data we need to collect.
3.1 Obtaining Source-to-Candidate
Alignments
How do we determine which segment of the can-
didate sentence aligns to a given source segment?
Given a word alignment between the source and
the candidate, we take the target substring to con-
tain any word aligned with at least one word in
the source segment. One could run an aligner (e.g.
GIZA++) on the two sentences to obtain the word
alignment, but we take a different approach.
We use Joshua (Li et al, 2009), in our experi-
ments. Joshua is a hierarchical parsing-based MT
system, and it can be instructed to produce deriva-
tion trees instead of the candidate sentence string
itself. Furthermore, each node in the derivation
tree is associated with the two indices in the source
sentence that indicate the segment corresponding
to this derivation subtree (the numbers indicated
in curly brackets in Figure 2).
Using this information, we are able to recover
most of the phrasal alignments. There are other
phrasal alignments that can be deduced from
the structure of the tree indirectly, by system-
atically discarding source words that are part
of another phrasal alignment. For instance,
in Figure 2, one can observe the alignment
(offizielle,prognosen,sind)?(official,forecasts,are)
and the alignment (prognosen)?(forecasts) to
deduce (offizielle,sind)?(official,are).
Although some of the phrasal alignment are
one-to-one mappings, many of them are many-
to-many. By construction, any deduced many-to-
many mapping has occurred in the training paral-
lel corpus at least once. And so we recover the
individual word alignments by consulting the par-
allel corpus from which the grammar rules were
extracted (which requires maintaining the word
alignments obtained prior to rule extraction).
5
5
We incorporated our implementation of the source-
We emphasize here that our recovery of word
alignment from phrasal alignment is independent
from the hierarchical and parsing-based nature of
the Joshua system. And so the alignment approach
we suggest here can be applied to a different MT
system as well, as long as that system provides
phrasal alignment along with the output. In partic-
ular, a phrase-based system such as Moses can be
modified in a straightforward manner to provide
phrasal alignments, and then apply our method.
4 Data Collection
We chose the WMT08 German-English news
dataset to work with, and since this is an investiga-
tive study of a novel approach, we collected judg-
ments for a subset of 250 source sentences from
the development set for the set of candidate sen-
tences produced in the last iteration of a MERT
run optimizing BLEU on the full 2051-sentence de-
velopment set. The MT system we used is Joshua
(Li et al, 2009), a software package that comes
complete with a grammar extraction module and a
MERT module, in addition to the decoder itself.
What segments of the source should be chosen
to be judged? We already indicated that we limit
ourselves, by definition of RYPT, to segments that
are covered exactly by a subtree in the source parse
tree. This has a couple of nice advantages: it al-
lows us to present an annotator with a high num-
ber of alternatives judged simulataneously (since
the annotator is shown a source segment and sev-
eral candidates, not just one), and this probably
also makes judging them easier ? it is reasonable
to assume that strings corresponding to syntactic
constituents are easier to process by a human.
Our query selection strategy attempts to max-
imize the amount of YES/NO percolation that
would take place. We therefore ensure that for any
2 queries, the corresponding source segments do
not overlap: such overlap indicates that one sub-
tree is completely contained within the other. Hav-
ing both queries (in the same batch) might be re-
dundant if we use the above percolation procedure.
The idea is to select source segments so that
they fully cover the entire source sentence, but
have no overlap amongst them. In one extreme,
each query would correspond to an entire parse
tree. This is not ideal since the overwhelming ma-
jority of the judgments will most likely be NO,
candidate aligner into the Joshua software as a new
aligner package.
56
which does not help identify where the problem
is. In the other extreme, each query would corre-
spond to a subtree rooted at a preterminal. This is
also not ideal, since it would place too much em-
phasis on translations of unigrams.
So we need a middle ground. We select a
maximum-source-length maxLen to indicate how
long we?re willing to let source segments be. Then
we start at the root of the parse tree, and prop-
agate a ?frontier? node set down the parse tree,
to end up with a set of nodes that fully cover the
source sentence, have no overlap amongst them,
and with each covering no more than maxLen
source words. For instance, with maxLen set to
4, the frontier set of Figure 2 are the nodes with
a thick border. An algorithmic description is pro-
vided in Algorithm 1.
Algorithm 1 Constructing the frontier node set for
a parse tree.
Input: A source parse tree T rooted at ROOT, and
a maximum source length maxLen.
Return: A nonempty set frontierSet, con-
taining a subset of the nodes in T .
1. Initialize frontierSet to the empty set.
2. Initialize currNodes to {ROOT}.
3. while currNodes is not empty do
4. Initialize newNodes to the empty set.
5. for each node N in currNodes do
6. if N covers ? maxLen source words
then
7. Add N to frontierSet.
8. else
9. Add children of N to newNodes.
10. end if
11. end for
12. Set currNodes = newNodes
13. end while
14. Return frontierSet.
This would ensure that our queries cover be-
tween 1 and maxLen source words, and ensures
they do not overlap, which would allow us to take
full advantage of the downward-YES and upward-
NO percolation. We set maxLen = 4 based on a
pilot study of 10 source sentences and their candi-
dates, having observed that longer segments tend
to always be labeled as NO, and shorter segments
tend to be so deep down the parse tree.
4.1 Amazon Mechanical Turk
We use the infrastructure of Amazon?s Mechan-
ical Turk (AMT)
6
to collect the labels. AMT is
a virtual marketplace that allows ?requesters? to
create and post tasks to be completed by ?work-
ers? around the world. To create the tasks (called
Human Intelligence Tasks, or HITs), a requester
supplies an HTML template along with a comma-
separated-values database, and AMT automati-
cally creates the HITs and makes them available to
workers. The queries are displayed as an HTML
page (based on the provided HTML template),
with the user indicating the label (YES, NO, or NOT
SURE) by selecting the appropriate radio button.
The instructions read, in part:
7
You are shown a ?source? German
sentence with a highlighted segment,
followed by several candidate trans-
lations with corresponding highlighted
segments. Your task is to decide if each
highlighted English segment is an ac-
ceptable translation of the highlighted
German segment.
In each HIT, the worker is shown up to 10 al-
ternative translations of a highlighted source seg-
ment, with each itself highlighted within a full
candidate string in which it appears. To aid the
worker in the task, they are also shown the ref-
erence translation, with a highlighted portion that
corresponds to the source segment, deduced using
word alignments obtained with GIZA++.
8
4.2 Cost of Data Collection
The total number of HITs created was 3873,
with the reward for completing a HIT depend-
ing on how many alternative translations are being
judged. On average, each HIT cost 2.1 cents and
involved judging 3.39 alternatives. 115 distinct
workers put in a total of 30.82 hours over a pe-
riod of about 4 days. On average, a label required
8.4 seconds to determine (i.e. at a rate of 426 la-
bels per hour). The total cost was $81.44: $21.43
for Amazon?s commission, $53.47 for wages, and
6
AMT?s website: http://www.mturk.com.
7
Template and full instructions can be viewed at http:
//cs.jhu.edu/
?
ozaidan/hmert.
8
These alignments are not always precise, and we do note
that fact in the instructions. We also deliberately highlight the
reference substring in a different color to make it clear that
workers should judge a candidate substring primarily based
on the source substring, not the reference substring.
57
$6.54 for bonuses
9
, for a cost per label of 0.62
cents (i.e. at a rate of 161.32 labels per dol-
lar). Excluding Amazon?s commission, the effec-
tive hourly ?wage? was $1.95.
5 Experimental Results and Analysis
By limiting our queries to source segments corre-
sponding to frontier nodes with maxLen = 4, we
obtain a total of 3601 subtrees across the 250 sen-
tences, for an average of 14.4 per sentence. On
average, each subtree has 3.65 alternative trans-
lations. Only about 4.8% of the judgments were
returned as NOT SURE (or, occasionally, blank),
with the rest split into 35.1% YES judgments and
60.1% NO judgments.
The coverage we get before percolating labels
up and down the trees is 39.4% of the nodes, in-
creasing to a coverage of 72.9% after percolation.
This is quite good, considering we only do a sin-
gle data collection pass, and considering that about
10% of the subtrees do not align to candidate sub-
strings to begin with (e.g. single source words that
lack a word alignment into the candidate string).
The main question, of course, is whether or not
those labels allow us to calculate a RYPT score
that is reliably correlated with human judgment.
We designed an experiment to compare the predic-
tive power of RYPT vs. BLEU. Given the candidate
set of a source sentence, we rerank the candidate
set according to RYPT and extract the top-1 can-
didate, and we rerank the candidate set according
to BLEU, and extract the top-1 candidate. We then
present the two candidates to human judges, and
ask them to choose the one that is a more adequate
translation. For reliability, we collect 3 judgments
per sentence pair comparison, instead of just 1.
The results show that RYPT significantly outper-
forms BLEU when it comes to predicting human
preference, with its choice prevailing in 46.1%
of judgments vs. 36.0% for BLEU, with 17.9%
judged to be of equal quality (left half of Ta-
ble 1). This advantage is especially true when the
judgments are grouped by sentence, and we ex-
amine cases of strong agreement among the three
annotators (Table 2): whereas BLEU?s candidate
is strongly preferred in 32 of the candidate pairs
(bottom 2 rows), RYPT?s candidate is strongly pre-
ferred in about double that number: 60 candidate
9
We would review the collected labels and give a 20%
reward for good workers to encourage them to come back
and complete more HITs.
pairs (top 2 rows).
This is quite a remarkable result, given that
BLEU, by definition, selects a candidate that has
significant overlap with the reference shown to the
annotators to aid in their decision-making. This
means that BLEU has an inherent advantage in
comparisons where both candidates are more or
less of equal quality, since annotators are encour-
aged (in the instructions) to make a choice even if
the two candidates seem of be of equal quality at
first glance. Pressed to make such a choice, the
annotator is likely to select the candidate that su-
perficially ?looks? more like the reference to be the
?better? of the two candidates. That candidate will
most likely be the BLEU-selected one.
To test this hypothesis, we repeated the experi-
ment without showing the annotators the reference
translations, and limited data collection to work-
ers living in Germany, making judgments based
only on the source sentences. (We only collected
one judgment per source sentence, since German
workers on AMT are in short supply.)
As expected, the difference is even more pro-
nounced: human judges prefer the RYPT-selected
candidate 45.2% of the time, while BLEU?s can-
didate is preferred only 29.2% of the time, with
25.6% judged to be of equal quality (right half
of Table 1). Our hypothesis is further supported
by the fact that most of the gain of the ?equal-
quality? category comes from BLEU, which loses
6.8 percentage points, whereas RYPT?s share re-
mains largely intact, losing less than a single per-
centage point.
5.1 Analysis of Data Collection
Recall that we minimize data collection by per-
forming label percolation and by employing a
frontier node set selection strategy. While the re-
sults just presented indicate those strategies pro-
vide a good approximation of some ?true? RYPT
score, label percolation was a strategy based pri-
marily on intuition, and choosing maxLen = 4
for frontier set construction was based on examin-
ing a limited amount of preliminary data.
Therefore, and in addition to encouraging em-
pricial results, we felt a more rigorous quantitative
analysis was in order, especially with future, more
ambitious annotation projects on the horizon. To
this end, we collected a complete set of judgments
for 50 source sentences and their candidates. That
is, we generated a query for each and every node
58
References shown; References not shown;
unrestricted restricted to DE workers
Preferred candidate # judgments % judgments # judgments % judgments
Top-1 by RYPT 346 46.1 113 45.2
Top-1 by BLEU 270 36.0 73 29.2
Neither 134 17.9 64 25.6
Total 750 100.0 250 100.0
Table 1: Ranking comparison results. The left half corresponds to the experiment (open to all workers)
where the English reference was shown, whereas the right half corresponds to the experiment (open only
to workers living in Germany) where the English reference was not shown.
Aggregate # sentences % sentences Aggregate # sentences % sentences
RYPT +3 45 18.0
RYPT +2 15 6.0 RYPT +any 120 48.0
RYPT +1 60 24.0
? 0 42 16.8 ? 0 42 16.8
BLEU +1 55 22.0
BLEU +2 5 2.0 BLEU +any 88 35.2
BLEU +3 28 11.2
Total 250 100.0 Total 250 100.0
Table 2: Ranking comparison results, grouped by sentence. This table corresponds to the left half of
Table 1. 3 judgments were collected for each comparison, with the ?aggregate? for a comparison calcu-
lated from these 3 judgments. For instance, an aggregate of ?RYPT +3? means all 3 judgments favored
RYPT?s choice, and ?RYPT +1? means one more judgment favored RYPT than did BLEU.
in the source parse tree, instead of limiting our-
selves to a frontier node set. (Though we did limit
the length of a source segment to be ? 7 words.)
This would allow us to judge the validity of label
percolation, and under different maxLen values.
Furthermore, we collected multiple judgments
for each query in order to minimize the effet of
bad/random annotations. For each of 5580 gen-
erated queries, we collected five judgments, for a
total of 27,900 judgments.
10
As before, the anno-
tator would pick one of YES, NO, and NOT SURE.
First, collecting multiple judgments allowed us
to investigate inter-annotator agreement. In 68.9%
of the queries, at least 4 of the 5 annotators chose
the same label, signifying a high degree of inter-
annotator agreement. This is especially encourag-
ing considering that we identified about 15% of
the HITs as being of poor quality, and blocked the
respective annotators from doing further HITs.
11
We then examine the applicability and validity
10
For a given query, the five collected judgments are from
five different annotators, since AMT ensures an annotator is
never shown the same HIT twice.
11
It is especially easy to identify (and then block) such an-
notators when they submit a relatively large number of HITs,
since inspecting some of their annotations would indicate
they are answering randomly and/or inconsistently.
of label percolation. For each of 7 different values
for Algorithm 1?s maxLen, we ignore all but la-
bels that would be requested under that maxLen
value, and percolate the labels up and down the
tree. In Figure 3 we plot the coverage before and
after percolation (middle two curves), and observe
expansion in coverage across different values of
maxLen, peaking at about +33% for maxLen= 4
and 5, with most of the benefit coming from YES
percolation (bottom two curves).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
0 1 2 3 4 5 6 7
maxLen
Coverage before percolation
Coverage after percolation
Percolation accuracy
? due to perc. of YES
? due to perc. of NO
Figure 3: Label percolation under different
maxLen values. The bottom two curves are the
breakdown of the difference between the middle
two. Accuracy is measured against majority votes.
59
We also measure the accuracy of labels deduced
from percolation (top curve of Figure 3). We de-
fine a percolated label to be correct if it matches
the label given by a majority vote over the col-
lected labels for that particular node. We find that
accuracy at low maxLen values is significantly
lower than at higer values (e.g. 72.6% vs. 84.1%
for 1 vs. 4). This means a middle value such as 3
or 4 is optimal. Higher values could be suitable if
we wish to emphasize translation fluency.
6 Related Work
Nie?en et al (2000) is an early work that also con-
structs a database of translations and judgments.
There, a source sentence is stored along with all
the translations that have already been manually
judged, along with their scores. They utilize this
database to carry out ?semi-automatic? evaluation
in a fast and convenient fashion thanks to tool they
developed with a user-friendly GUI.
In their annual evaluation, the WMT work-
shop has effectively conducted manual evaluation
of submitted systems over the past few years by
distributing the work across tens of volunteers,
though they relied on a self-designed online por-
tal. On the other hand, Snow et al (2008) illus-
trate how AMT can be used to collect data in a
?fast and cheap? fashion, for a number of NLP
tasks, such as word sense disambiguation. They
go a step further and model the behavior of their
annotators to reduce annotator bias. This was pos-
sible as they collect multiple judgments for each
query from multiple annotators.
The question of how to design an automatic
metric that best approximates human judgment
has received a lot of attention lately. NIST started
organizing the Metrics for Machine Translation
Challenge (MetricsMATR) in 2008, with the aim
of developing automatic evaluation metrics that
correlate highly with human judgment of transla-
tion quality. The latest WMT workshop (Callison-
Burch et al, 2009) also conducted a full assess-
ment of how well a suite of automatic metrics cor-
relate with human judgment.
7 Future Work
This pilot study has demonstrated the feasibility
of collecting a large number of human judgments,
and has shown that the RYPT metric is better than
BLEU at picking out the best translation. The
next step is to run a complete MERT run. This
will involve collecting data for thousands of al-
ternative translations for several hundreds source
sentences. Based on our analysis, this it should
be cost-effective to solicit these judgments using
AMT. After training MERT using RYPT as an ob-
jective function the, the next logical step would be
to compare two outputs of a system. One output
would have parameters optimized to BLEU and the
other to RYPT. The hope is that the RYPT-trained
system would be better under the final HTER eval-
uation than the BLEU-trained system.
We are also investigating a probabilistic ap-
proach to percolating the labels up and down the
tree, whereby the label of a node is treated as a
random variable, and inference is performed based
on values of the other observed nodes, as well as
properties of the source/candidate segment. Cast
this way, a probabilistic approach is actually quite
appealing, and one could use collected data to
train a prediction model (such as a Markov ran-
dom field).
8 Summary
We propose a human-based metric, RYPT, that is
quite feasible to optimize using MERT, relying on
the redundancy in the candidate set, and collect-
ing judgments using Amazon?s Mechanical Turk
infrastructure. We show this could be done in a
quite cost-effective manner, and produces data of
good quality. We show the effectiveness of the
metric by illustrating that it is a better predictor of
human judgment of translation quality than BLEU,
the most commonly used metric in MT. We show
this is the case even with a modest amount of data
that does not cover the entirety of all parse trees,
on which the metric is dependent. The collected
data represents a database that can be reused over
and over again, hence limiting human feedback to
the initial phase only.
Acknowledgments
This research was supported by the EuroMatrix-
Plus project funded by the European Commission
(7th Framework Programme), by the Defense Ad-
vanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001, and
the US National Science Foundation under grant
IIS-0713448. The views and findings are the au-
thors? alone.
60
References
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of Bleu in ma-
chine translation research. In Proceedings of EACL,
pages 249?256.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008. Decomposability of trans-
lation metrics for improved evaluation and efficient
algorithms. In Proceedings of EMNLP, pages 610?
619.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proceedings of MT Summit IX.
Amit Dubey. 2005. What to do when lexicaliza-
tion fails: parsing German with suffix analysis and
smoothing. In Proceedings of ACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of ACL, Demo and Poster Ses-
sions, pages 177?180.
LDC. 2005. Linguistic data annotation specification:
Assessment of fluency and adequacy in translations.
Revision 1.5.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139.
Sonja Nie?en, Franz Josef Och, Gregor Leusch, and
Hermann Ney. 2000. An evaluation tool for ma-
chine translation: Fast evaluation for mt research.
In Proceedings of LREC.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of ACL,
pages 295?302.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160?167.
Kishore Papineni, Salim Poukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL, pages 311?318.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good? evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP, pages
254?263.
61
Proceedings of NAACL HLT 2007, pages 260?267,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Using ?Annotator Rationales? to Improve
Machine Learning for Text Categorization?
Omar F. Zaidan and Jason Eisner
Department of Computer Science
Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,jason}@cs.jhu.edu
Christine D. Piatko
JHU Applied Physics Laboratory
11100 Johns Hopkins Road
Laurel, MD 20723 USA
christine.piatko@jhuapl.edu
Abstract
We propose a new framework for supervised ma-
chine learning. Our goal is to learn from smaller
amounts of supervised training data, by collecting a
richer kind of training data: annotations with ?ra-
tionales.? When annotating an example, the hu-
man teacher will also highlight evidence support-
ing this annotation?thereby teaching the machine
learner why the example belongs to the category. We
provide some rationale-annotated data and present a
learning method that exploits the rationales during
training to boost performance significantly on a sam-
ple task, namely sentiment classification of movie
reviews. We hypothesize that in some situations,
providing rationales is a more fruitful use of an an-
notator?s time than annotating more examples.
1 Introduction
Annotation cost is a bottleneck for many natural lan-
guage processing applications. While supervised
machine learning systems are effective, it is labor-
intensive and expensive to construct the many train-
ing examples needed. Previous research has ex-
plored active or semi-supervised learning as possible
ways to lessen this burden.
We propose a new way of breaking this annotation
bottleneck. Annotators currently indicate what the
correct answers are on training data. We propose
that they should also indicate why, at least by coarse
hints. We suggest new machine learning approaches
that can benefit from this ?why? information.
For example, an annotator who is categorizing
phrases or documents might also be asked to high-
light a few substrings that significantly influenced
her judgment. We call such clues ?rationales.? They
need not correspond to machine learning features.
?This work was supported by the JHU WSE/APL Partner-
ship Fund; National Science Foundation grant No. 0347822 to
the second author; and an APL Hafstad Fellowship to the third.
In some circumstances, rationales should not be
too expensive or time-consuming to collect. As long
as the annotator is spending the time to study exam-
ple xi and classify it, it may not require much extra
effort for her to mark reasons for her classification.
2 Using Rationales to Aid Learning
We will not rely exclusively on the rationales, but
use them only as an added source of information.
The idea is to help direct the learning algorithm?s
attention?helping it tease apart signal from noise.
Machine learning algorithms face a well-known
?credit assignment? problem. Given a complex da-
tum xi and the desired response yi, many features of
xi could be responsible for the choice of yi. The
learning algorithm must tease out which features
were actually responsible. This requires a lot of
training data, and often a lot of computation as well.
Our rationales offer a shortcut to solving this
?credit assignment? problem, by providing the
learning algorithm with hints as to which features
of xi were relevant. Rationales should help guide
the learning algorithm toward the correct classifica-
tion function, by pushing it toward a function that
correctly pays attention to each example?s relevant
features. This should help the algorithm learn from
less data and avoid getting trapped in local maxima.1
In this paper, we demonstrate the ?annotator ra-
tionales? technique on a text categorization problem
previously studied by others.
1To understand the local maximum issue, consider the hard
problem of training a standard 3-layer feed-forward neural net-
work. If the activations of the ?hidden? layer?s features (nodes)
were observed at training time, then the network would de-
compose into a pair of independent 2-layer perceptrons. This
turns an NP-hard problem with local maxima (Blum and Rivest,
1992) to a polytime-solvable convex problem. Although ratio-
nales might only provide indirect evidence of the hidden layer,
this would still modify the objective function (see section 8) in
a way that tended to make the correct weights easier to discover.
260
3 Discriminative Approach
One popular approach for text categorization is to
use a discriminative model such as a Support Vec-
tor Machine (SVM) (e.g. (Joachims, 1998; Dumais,
1998)). We propose that SVM training can in gen-
eral incorporate annotator rationales as follows.
From the rationale annotations on a positive ex-
ample ??xi , we will construct one or more ?not-quite-
as-positive? contrast examples ??vij . In our text cat-
egorization experiments below, each contrast docu-
ment ??vij was obtained by starting with the original
and ?masking out? one or all of the several rationale
substrings that the annotator had highlighted (rij).
The intuition is that the correct model should be less
sure of a positive classification on the contrast exam-
ple ??vij than on the original example ~xi, because
??vij
lacks evidence that the annotator found significant.
We can translate this intuition into additional con-
straints on the correct model, i.e., on the weight vec-
tor ~w. In addition to the usual SVM constraint on
positive examples that ~w ? ??xi ? 1, we also want (for
each j) that ~w ? ~xi ? ~w ?
??vij ? ?, where ? ? 0 con-
trols the size of the desired margin between original
and contrast examples.
An ordinary soft-margin SVM chooses ~w and ~? to
minimize
1
2
?~w?2 + C(
?
i
?i) (1)
subject to the constraints
(?i) ~w ? ??xi ? yi ? 1? ?i (2)
(?i) ?i ? 0 (3)
where ??xi is a training example, yi ? {?1,+1} is
its desired classification, and ?i is a slack variable
that allows training example ??xi to miss satisfying
the margin constraint if necessary. The parameter
C > 0 controls the cost of taking such slack, and
should generally be lower for noisier or less linearly
separable datasets. We add the contrast constraints
(?i, j) ~w ? (??xi ?
??vij) ? yi ? ?(1? ?ij), (4)
where ??vij is one of the contrast examples con-
structed from example ??xi , and ?ij ? 0 is an asso-
ciated slack variable. Just as these extra constraints
have their own margin ?, their slack variables have
their own cost, so the objective function (1) becomes
1
2
?~w?2 + C(
?
i
?i) + Ccontrast(
?
i,j
?ij) (5)
The parameter Ccontrast ? 0 determines the impor-
tance of satisfying the contrast constraints. It should
generally be less than C if the contrasts are noisier
than the training examples.2
In practice, it is possible to solve this optimization
using a standard soft-margin SVM learner. Dividing
equation (4) through by ?, it becomes
(?i, j) ~w ? ??xij ? yi ? 1? ?ij , (6)
where ??xij
def
=
??xi?
??vij
? . Since equation (6) takes
the same form as equation (2), we simply add the
pairs (??xij , yi) to the training set as pseudoexam-
ples, weighted by Ccontrast rather than C so that the
learner will use the objective function (5).
There is one subtlety. To allow a biased hyper-
plane, we use the usual trick of prepending a 1 el-
ement to each training example. Thus we require
~w ? (1,??xi) ? 1 ? ?i (which makes w0 play the
role of a bias term). This means, however, that we
must prepend a 0 element to each pseudoexample:
~w ? (1,~xi)?(1,
??vij)
? = ~w ? (0,
??xij) ? 1? ?ij .
In our experiments, we optimize ?, C, and
Ccontrast on held-out data (see section 5.2).
4 Rationale Annotation for Movie Reviews
In order to demonstrate that annotator rationales
help machine learning, we needed annotated data
that included rationales for the annotations.
We chose a dataset that would be enjoyable to re-
annotate: the movie review dataset of (Pang et al,
2002; Pang and Lee, 2004).3 The dataset consists
of 1000 positive and 1000 negative movie reviews
obtained from the Internet Movie Database (IMDb)
review archive, all written before 2002 by a total of
312 authors, with a cap of 20 reviews per author per
2Taking Ccontrast to be constant means that all rationales
are equally valuable. One might instead choose, for example,
to reduce Ccontrast for examples xi that have many rationales,
to prevent xi?s contrast examples vij from together dominating
the optimization. However, in this paper we assume that an xi
with more rationales really does provide more evidence about
the true classifier ~w.
3Polarity dataset version 2.0.
261
category. Pang and Lee have divided the 2000 docu-
ments into 10 folds, each consisting of 100 positive
reviews and 100 negative reviews.
The dataset is arguably artificial in that it keeps
only reviews where the reviewer provided a rather
high or rather low numerical rating, allowing Pang
and Lee to designate the review as positive or neg-
ative. Nonetheless, most reviews contain a difficult
mix of praise, criticism, and factual description. In
fact, it is possible for a mostly critical review to give
a positive overall recommendation, or vice versa.
4.1 Annotation procedure
Rationale annotators were given guidelines4 that
read, in part:
Each review was intended to give either a positive or a neg-
ative overall recommendation. You will be asked to justify why
a review is positive or negative. To justify why a review is posi-
tive, highlight the most important words and phrases that would
tell someone to see the movie. To justify why a review is nega-
tive, highlight words and phrases that would tell someone not to
see the movie. These words and phrases are called rationales.
You can highlight the rationales as you notice them, which
should result in several rationales per review. Do your best to
mark enough rationales to provide convincing support for the
class of interest.
You do not need to go out of your way to mark everything.
You are probably doing too much work if you find yourself go-
ing back to a paragraph to look for even more rationales in it.
Furthermore, it is perfectly acceptable to skim through sections
that you feel would not contain many rationales, such as a re-
viewer?s plot summary, even if that might cause you to miss a
rationale here and there.
The last two paragraphs were intended to provide
some guidance on how many rationales to annotate.
Even so, as section 4.2 shows, some annotators were
considerably more thorough (and slower).
Annotators were also shown the following exam-
ples5 of positive rationales:
? you will enjoy the hell out of American Pie.
? fortunately, they managed to do it in an interesting and
funny way.
? he is one of the most exciting martial artists on the big
screen, continuing to perform his own stunts and daz-
zling audiences with his flashy kicks and punches.
? the romance was enchanting.
and the following examples5 of negative rationales:
4Available at http://cs.jhu.edu/?ozaidan/rationales.
5For our controlled study of annotation time (section 4.2),
different examples were given with full document context.
Figure 1: Histograms of rationale counts per document (A0?s
annotations). The overall mean of 8.55 is close to that of the
four annotators in Table 1. The median and mode are 8 and 7.
? A woman in peril. A confrontation. An explosion. The
end. Yawn. Yawn. Yawn.
? when a film makes watching Eddie Murphy a tedious ex-
perience, you know something is terribly wrong.
? the movie is so badly put together that even the most
casual viewer may notice themiserable pacing and stray
plot threads.
? don?t go see this movie
The annotation involves boldfacing the rationale
phrases using an HTML editor. Note that a fancier
annotation tool would be necessary for a task like
named entity tagging, where an annotator must mark
many named entities in a single document. At any
given moment, such a tool should allow the annota-
tor to highlight, view, and edit only the several ra-
tionales for the ?current? annotated entity (the one
most recently annotated or re-selected).
One of the authors (A0) annotated folds 0?8 of
the movie review set (1,800 documents) with ra-
tionales that supported the gold-standard classifica-
tions. This training/development set was used for
all of the learning experiments in sections 5?6. A
histogram of rationale counts is shown in Figure 1.
As mentioned in section 3, the rationale annotations
were just textual substrings. The annotator did not
require knowledge of the classifier features. Thus,
our rationale dataset is a new resource4 that could
also be used to study exploitation of rationales un-
der feature sets or learning methods other than those
considered here (see section 8).
4.2 Inter-annotator agreement
To study the annotation process, we randomly se-
lected 150 documents from the dataset. The doc-
262
Rationales % rationales also % rationales also % rationales also % rationales also % rationales also
per document annotated by A1 annotated by A2 annotated by AX annotated by AY ann. by anyone else
A1 5.02 (100) 69.6 63.0 80.1 91.4
A2 10.14 42.3 (100) 50.2 67.8 80.9
AX 6.52 49.0 68.0 (100) 79.9 90.9
AY 11.36 39.7 56.2 49.3 (100) 75.5
Table 1: Average number of rationales and inter-annotator agreement for Tasks 2 and 3. A rationale by Ai (?I think this is a great
movie!?) is considered to have been annotated also by Aj if at least one of Aj?s rationales overlaps it (?I think this is a great
movie!?). In computing pairwise agreement on rationales, we ignored documents where Ai and Aj disagreed on the class. Notice
that the most thorough annotatorAY caught most rationales marked by the others (exhibiting high ?recall?), and that most rationales
enjoyed some degree of consensus, especially those marked by the least thorough annotator A1 (exhibiting high ?precision?).
uments were split into three groups, each consisting
of 50 documents (25 positive and 25 negative). Each
subset was used for one of three tasks:6
? Task 1: Given the document, annotate only the
class (positive/negative).
? Task 2: Given the document and its class, an-
notate some rationales for that class.
? Task 3: Given the document, annotate both the
class and some rationales for it.
We carried out a pilot study (annotators AX and
AY: two of the authors) and a later, more controlled
study (annotators A1 and A2: paid students). The
latter was conducted in a more controlled environ-
ment where both annotators used the same annota-
tion tool and annotation setup as each other. Their
guidelines were also more detailed (see section 4.1).
In addition, the documents for the different tasks
were interleaved to avoid any practice effect.
The annotators? classification accuracies in Tasks
1 and 3 (against Pang & Lee?s labels) ranged from
92%?97%, with 4-way agreement on the class for
89% of the documents, and pairwise agreement also
ranging from 92%?97%. Table 1 shows how many
rationales the annotators provided and how well
their rationales agreed.
Interestingly, in Task 3, four of AX?s ratio-
nales for a positive class were also partially
highlighted by AY as support for AY?s (incorrect)
negative classifications, such as:
6Each task also had a ?warmup? set of 10 documents to be
annotated before that tasks?s 50 documents. Documents for
Tasks 2 and 3 would automatically open in an HTML editor
while Task 1 documents opened in an HTML viewer with no
editing option. The annotators recorded their classifications for
Tasks 1 and 3 on a spreadsheet.
min./KB A1 time A2 time AX time AY time
Task 1 0.252 0.112 0.150 0.422
Task 2 0.396 0.537 0.242 0.626
Task 3 0.399 0.505 0.288 1.01
min./doc. A1 time A2 time AX time AY time
Task 1 1.04 0.460 0.612 1.73
min./rat. A1 time A2 time AX time AY time
Task 2 0.340 0.239 0.179 0.298
Task 3 0.333 0.198 0.166 0.302
Table 2: Average annotation rates on each task.
? Even with its numerous flaws, the movie all comes to-
gether, if only for those who . . .
? ?Beloved? acts like an incredibly difficult chamber
drama paired with a ghost story.
4.3 Annotation time
Average annotation times are in Table 2. As hoped,
rationales did not take too much extra time for most
annotators to provide. For each annotator except
A2, providing rationales only took roughly twice the
time (Task 3 vs. Task 1), even though it meant mark-
ing an average of 5?11 rationales in addition to the
class.
Why this low overhead? Because marking the
class already required the Task 1 annotator to read
the document and find some rationales, even if s/he
did not mark them. The only extra work in Task 3
is in making them explicit. This synergy between
class annotation and rationale annotation is demon-
strated by the fact that doing both at once (Task 3)
was faster than doing them separately (Tasks 1+2).
We remark that this task?binary classification on
full documents?seems to be almost a worst-case
scenario for the annotation of rationales. At a purely
mechanical level, it was rather heroic of A0 to at-
tach 8?9 new rationale phrases rij to every bit yi
of ordinary annotation. Imagine by contrast a more
local task of identifying entities or relations. Each
263
lower-level annotation yi will tend to have fewer ra-
tionales rij , while yi itself will be more complex and
hence more difficult to mark. Thus, we expect that
the overhead of collecting rationales will be less in
many scenarios than the factor of 2 we measured.
Annotation overhead could be further reduced.
For a multi-class problem like relation detection, one
could ask the annotator to provide rationales only for
the rarer classes. This small amount of extra time
where the data is sparsest would provide extra guid-
ance where it was most needed. Another possibility
is passive collection of rationales via eye tracking.
5 Experimental Procedures
5.1 Feature extraction
Although this dataset seems to demand discourse-
level features that contextualize bits of praise and
criticism, we exactly follow Pang et al (2002) and
Pang and Lee (2004) in merely using binary uni-
gram features, corresponding to the 17,744 un-
stemmed word or punctuation types with count ? 4
in the full 2000-document corpus. Thus, each docu-
ment is reduced to a 0-1 vector with 17,744 dimen-
sions, which is then normalized to unit length.7
We used the method of section 3 to place addi-
tional constraints on a linear classifier. Given a train-
ing document, we create several contrast documents,
each by deleting exactly one rationale substring
from the training document. Converting documents
to feature vectors, we obtained an original exam-
ple ??xi and several contrast examples
??vi1,
??vi2, . . ..8
Again, our training method required each original
document to be classified more confidently (by a
margin ?) than its contrast documents.
If we were using more than unigram features, then
simply deleting a rationale substring would not al-
ways be the best way to create a contrast document,
as the resulting ungrammatical sentences might
cause deep feature extraction to behave strangely
(e.g., parse errors during preprocessing). The goal in
creating the contrast document is merely to suppress
7The vectors are normalized before prepending the 1 corre-
sponding to the bias term feature (mentioned in section 3).
8The contrast examples were not normalized to precisely
unit length, but instead were normalized by the same factor used
to normalize ??xi . This conveniently ensured that the pseudoex-
amples ??xij
def
=
~xi?
??vij
? were sparse vectors, with 0 coordinates
for all words not in the jth rationale.
features (n-grams, parts of speech, syntactic depen-
dencies . . . ) that depend in part on material in one
or more rationales. This could be done directly by
modifying the feature extractors, or if one prefers to
use existing feature extractors, by ?masking? rather
than deleting the rationale substring?e.g., replacing
each of its word tokens with a special MASK token
that is treated as an out-of-vocabulary word.
5.2 Training and testing procedures
We transformed this problem to an SVM problem
(see section 3) and applied SVMlight for training and
testing, using the default linear kernel. We used only
A0?s rationales and the true classifications.
Fold 9 was reserved as a test set. All accuracy
results reported in the paper are the result of testing
on fold 9, after training on subsets of folds 0?8.
Our learning curves show accuracy after training
on T < 9 folds (i.e., 200T documents), for various
T . To reduce the noise in these results, the accuracy
we report for training on T folds is actually the aver-
age of 9 different experiments with different (albeit
overlapping) training sets that cover folds 0?8:
1
9
8?
i=0
acc(F9 | ?
?, Fi+1 ? . . . ? Fi+T ) (7)
where Fj denotes the fold numbered j mod 9, and
acc(Z | ?, Y ) means classification accuracy on the
set Z after training on Y with hyperparameters ?.
To evaluate whether two different training meth-
ods A and B gave significantly different average-
accuracy values, we used a paired permutation test
(generalizing a sign test). The test assumes in-
dependence among the 200 test examples but not
among the 9 overlapping training sets. For each
of the 200 test examples in fold 9, we measured
(ai, bi), where ai (respectively bi) is the number
of the 9 training sets under which A (respectively
B) classified the example correctly. The p value
is the probability that the absolute difference be-
tween the average-accuracy values would reach or
exceed the observed absolute difference, namely
| 1200
?200
i=1
ai?bi
9 |, if each (ai, bi) had an independent
1/2 chance of being replaced with (bi, ai), as per the
null hypothesis that A and B are indistinguishable.
For any given value of T and any given train-
ing method, we chose hyperparameters ?? =
264
Figure 2: Classification accuracy under five different experi-
mental setups (S1?S5). At each training size, the 5 accura-
cies are pairwise significantly different (paired permutation test,
p < 0.02; see section 5.2), except for {S3,S4} or {S4,S5} at
some sizes.
(C, ?,Ccontrast) to maximize the following cross-
validation performance:9
?? = argmax
?
8?
i=0
acc(Fi | ?, Fi+1 ? . . . ? Fi+T )
(8)
We used a simple alternating optimization procedure
that begins at ?0 = (1.0, 1.0, 1.0) and cycles repeat-
edly through the three dimensions, optimizing along
each dimension by a local grid search with resolu-
tion 0.1.10 Of course, when training without ratio-
nales, we did not have to optimize ? or Ccontrast.
6 Experimental Results
6.1 The value of rationales
The top curve (S1) in Figure 2 shows that perfor-
mance does increase when we introduce rationales
for the training examples as contrast examples (sec-
tion 3). S1 is significantly higher than the baseline
curve (S2) immediately below it, which trains an or-
dinary SVM classifier without using rationales. At
the largest training set size, rationales raise the accu-
racy from 88.5% to 92.2%, a 32% error reduction.
9One might obtain better performance (across all methods
being compared) by choosing a separate ?? for each of the 9
training sets. However, to simulate real limited-data training
conditions, one should then find the ?? for each {i, ..., j} us-
ing a separate cross-validation within {i, ..., j} only; this would
slow down the experiments considerably.
10For optimizing along the C dimension, one could use the
efficient method of Beineke et al (2004), but not in SVMlight.
The lower three curves (S3?S5) show that learn-
ing is separately helped by the rationale and the
non-rationale portions of the documents. S3?S5
are degraded versions of the baseline S2: they are
ordinary SVM classifiers that perform significantly
worse than S2 (p < 0.001).
Removing the rationale phrases from the train-
ing documents (S3) made the test documents much
harder to discriminate (compared to S2). This sug-
gests that annotator A0?s rationales often covered
most of the usable evidence for the true class.
However, the pieces to solving the classification
puzzle cannot be found solely in the short rationale
phrases. Removing all non-rationale text from the
training documents (S5) was even worse than re-
moving the rationales (S3). In other words, we can-
not hope to do well simply by training on just the
rationales (S5), although that approach is improved
somewhat in S4 by treating each rationale (similarly
to S1) as a separate SVM training example.
This presents some insight into why our method
gives the best performance. The classifier in S1
is able to extract subtle patterns from the corpus,
like S2, S3, or any other standard machine learn-
ing method, but it is also able to learn from a human
annotator?s decision-making strategy.
6.2 Using fewer rationales
In practice, one might annotate rationales for only
some training documents?either when annotating a
new corpus or when adding rationales post hoc to
an existing corpus. Thus, a range of options can be
found between curves S2 and S1 of Figure 2.
Figure 3 explores this space, showing how far the
learning curve S2 moves upward if one has time to
annotate rationales for a fixed number of documents
R. The key useful discovery is that much of the ben-
efit can actually be obtained with relatively few ra-
tionales. For example, with 800 training documents,
annotating (0%, 50%, 100%) of themwith rationales
gives accuracies of (86.9%, 89.2%, 89.3%). With
the maximum of 1600 training documents, annotat-
ing (0%, 50%, 100%) with rationales gives (88.5%,
91.7%, 92.2%).
To make this point more broadly, we find that the
R = 200 curve is significantly above the R = 0
curve (p < 0.05) at all T ? 1200. By contrast, the
R = 800, R = 1000, . . . R = 1600 points at each T
265
Figure 3: Classification accuracy for T ? {200, 400, ..., 1600}
training documents (x-axis) when only R ? {0, 200, ..., T} of
them are annotated with rationales (different curves). The R =
0 curve above corresponds to the baseline S2 from Figure 2.
S1?s points are found above as the leftmost points on the other
curves, where R = T .
value are all-pairs statistically indistinguishable.
The figure also suggests that rationales and docu-
ments may be somewhat orthogonal in their benefit.
When one has many documents and few rationales,
there is no longer much benefit in adding more doc-
uments (the curve is flattening out), but adding more
rationales seems to provide a fresh benefit: ratio-
nales have not yet reached their point of diminishing
returns. (While this fresh benefit was often statisti-
cally significant, and greater than the benefit from
more documents, our experiments did not establish
that it was significantly greater.)
The above experiments keep all of A0?s rationales
on a fraction of training documents. We also exper-
imented with keeping a fraction of A0?s rationales
(chosen randomly with randomized rounding) on all
training documents. This yielded no noteworthy or
statistically significant differences from Figure 3.
These latter experiments simulate a ?lazy annota-
tor? who is less assiduous than A0. Such annotators
may be common in the real world. We also suspect
that they will be more desirable. First, they should
be able to add more rationales per hour than the A0-
style annotator from Figure 3: some rationales are
simply more noticeable than others, and a lazy anno-
tator will quickly find the most noticeable ones with-
out wasting time tracking down the rest. Second, the
?most noticeable? rationales that they mark may be
the most effective ones for learning, although our
random simulation of laziness could not test that.
7 Related Work
Our rationales resemble ?side information? in ma-
chine learning?supplementary information about
the target function that is available at training time.
Side information is sometimes encoded as ?virtual
examples? like our contrast examples or pseudoex-
amples. However, past work generates these by
automatically transforming the training examples
in ways that are expected to preserve or alter the
classification (Abu-Mostafa, 1995). In another for-
mulation, virtual examples are automatically gener-
ated but must be manually annotated (Kuusela and
Ocone, 2004). Our approach differs because a hu-
man helps to generate the virtual examples. Enforc-
ing a margin between ordinary examples and con-
trast examples also appears new.
Other researchers have considered how to reduce
annotation effort. In active learning, the annotator
classifies only documents where the system so far is
less confident (Lewis and Gale, 1994), or in an in-
formation extraction setting, incrementally corrects
details of the system?s less confident entity segmen-
tations and labelings (Culotta andMcCallum, 2005).
Raghavan et al (2005) asked annotators to iden-
tify globally ?relevant? features. In contrast, our ap-
proach does not force the annotator to evaluate the
importance of features individually, nor in a global
context outside any specific document, nor even to
know the learner?s feature space. Annotators only
mark text that supports their classification decision.
Our methods then consider the combined effect of
this text on the feature vector, which may include
complex features not known to the annotator.
8 Future Work: Generative models
Our SVM contrast method (section 3) is not the only
possible way to use rationales. We would like to ex-
plicitly model rationale annotation as a noisy pro-
cess that reflects, imperfectly and incompletely, the
annotator?s internal decision procedure.
A natural approach would start with log-linear
models in place of SVMs. We can define a proba-
bilistic classifier
p?(y | x)
def
=
1
Z(x)
exp
k?
h=1
?hfh(x, y) (9)
266
where ~f(?) extracts a feature vector from a classified
document.
A standard training method would be to choose ?
to maximize the conditional likelihood of the train-
ing classifications:
argmax
~?
n?
i=1
p?(yi | xi) (10)
When a rationale ri is also available for each
(xi, yi), we propose to maximize a likelihood that
tries to predict these rationale data as well:
argmax
~?
n?
i=1
p?(yi | xi) ? p??(ri | xi, yi, ?) (11)
Notice that a given guess of ? might make equa-
tion (10) large, yet accord badly with the annotator?s
rationales. In that case, the second term of equa-
tion (11) will exert pressure on ? to change to some-
thing that conforms more closely to the rationales.
If the annotator is correct, such a ? will generalize
better beyond the training data.
In equation (11), p?? models the stochastic process
of rationale annotation. What is an annotator actu-
ally doing when she annotates rationales? In par-
ticular, how do her rationales derive from the true
value of ? and thereby tell us about ?? Building a
good model p?? of rationale annotation will require
some exploratory data analysis. Roughly, we expect
that if ?hfh(xi, y) is much higher for y = yi than
for other values of y, then the annotator?s ri is corre-
spondingly more likely to indicate in some way that
feature fh strongly influenced annotation yi. How-
ever, we must also model the annotator?s limited pa-
tience (she may not annotate all important features),
sloppiness (she may indicate only indirectly that fh
is important), and bias (tendency to annotate some
kinds of features at the expense of others).
One advantage of this generative approach is that
it eliminates the need for contrast examples. Con-
sider a non-textual example in which an annotator
highlights the line crossing in a digital image of the
digit ?8? to mark the rationale that distinguishes it
from ?0.? In this case it is not clear how to mask out
that highlighted rationale to create a contrast exam-
ple in which relevant features would not fire.11
11One cannot simply flip those highlighted pixels to white
9 Conclusions
We have proposed a quite simple approach to im-
proving machine learning by exploiting the clever-
ness of annotators, asking them to provide enriched
annotations for training. We developed and tested
a particular discriminative method that can use ?an-
notator rationales??even on a fraction of the train-
ing set?to significantly improve sentiment classifi-
cation of movie reviews.
We found fairly good annotator agreement on the
rationales themselves. Most annotators provided
several rationales per classification without taking
too much extra time, even in our text classification
scenario, where the rationales greatly outweigh the
classifications in number and complexity. Greater
speed might be possible through an improved user
interface or passive feedback (e.g., eye tracking).
In principle, many machine learning methods
might be modified to exploit rationale data. While
our experiments in this paper used a discriminative
SVM, we plan to explore generative approaches.
References
Y. S. Abu-Mostafa. 1995. Hints. Neural Computation, 7:639?
671, July.
P. Beineke, T. Hastie, and S. Vaithyanathan. 2004. The sen-
timental factor: Improving review classification via human-
provided information. In Proc. of ACL, pages 263?270.
A. L. Blum and R. L. Rivest. 1992. Training a 3-node neural
network is NP-complete. Neural Networks, 5(1):117?127.
A. Culotta and A. McCallum. 2005. Reducing labeling effort
for structured prediction tasks. In AAAI, pages 746?751.
S. Dumais. 1998. Using SVMs for text categorization. IEEE
Intelligent Systems Magazine, 13(4), July/August.
T. Joachims. 1998. Text categorization with support vector
machines: Learning with many relevant features. In Proc. of
the European Conf. on Machine Learning, pages 137?142.
P. Kuusela and D. Ocone. 2004. Learning with side informa-
tion: PAC learning bounds. J. of Computer and System Sci-
ences, 68(3):521?545, May.
D. D. Lewis and W. A. Gale. 1994. A sequential algorithm for
training text classifiers. In Proc. of ACM-SIGIR, pages 3?12.
B. Pang and L. Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proc. of ACL, pages 271?278.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up?
Sentiment classification using machine learning techniques.
In Proc. of EMNLP, pages 79?86.
H. Raghavan, O. Madani, and R. Jones. 2005. Interactive fea-
ture selection. In Proc. of IJCAI, pages 41?46.
or black, since that would cause new features to fire. Possibly
one could simply suppress any feature that depends in any way
on the highlighted pixels, but this would take away too many
important features, including global features.
267
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 135?139,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Joshua: An Open Source Toolkit for Parsing-based Machine Translation
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,+ Sanjeev Khudanpur,
Lane Schwartz,? Wren N. G. Thornton, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University, Germany
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe Joshua, an open source
toolkit for statistical machine transla-
tion. Joshua implements all of the algo-
rithms required for synchronous context
free grammars (SCFGs): chart-parsing, n-
gram language model integration, beam-
and cube-pruning, and k-best extraction.
The toolkit also implements suffix-array
grammar extraction and minimum error
rate training. It uses parallel and dis-
tributed computing techniques for scala-
bility. We demonstrate that the toolkit
achieves state of the art translation per-
formance on the WMT09 French-English
translation task.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high bar-
rier to entry for other researchers, and makes ex-
periments difficult to duplicate and compare. In
this paper, we describe Joshua, a general-purpose
open source toolkit for parsing-based machine
translation, serving the same role as Moses (Koehn
et al, 2007) does for regular phrase-based ma-
chine translation.
Our toolkit is written in Java and implements
all the essential algorithms described in Chiang
(2007): chart-parsing, n-gram language model in-
tegration, beam- and cube-pruning, and k-best ex-
traction. The toolkit also implements suffix-array
grammar extraction (Lopez, 2007) and minimum
error rate training (Och, 2003). Additionally, par-
allel and distributed computing techniques are ex-
ploited to make it scalable (Li and Khudanpur,
2008b). We have also made great effort to ensure
that our toolkit is easy to use and to extend.
The toolkit has been used to translate roughly
a million sentences in a parallel corpus for large-
scale discriminative training experiments (Li and
Khudanpur, 2008a). We hope the release of the
toolkit will greatly contribute the progress of the
syntax-based machine translation research.1
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: The Joshua code is organized
into separate packages for each major aspect of
functionality. In this way it is clear which files
contribute to a given functionality and researchers
can focus on a single package without worrying
about the rest of the system. Moreover, to mini-
mize the problems of unintended interactions and
unseen dependencies, which is common hinder-
ance to extensibility in large projects, all exten-
sible components are defined by Java interfaces.
Where there is a clear point of departure for re-
search, a basic implementation of each interface is
provided as an abstract class to minimize the work
necessary for new extensions.
End-to-end Cohesion: There are many compo-
nents to a machine translation pipeline. One of the
great difficulties with current MT pipelines is that
these diverse components are often designed by
separate groups and have different file format and
interaction requirements. This leads to a large in-
vestment in scripts to convert formats and connect
the different components, and often leads to unten-
able and non-portable projects as well as hinder-
1The toolkit can be downloaded at http://www.
sourceforge.net/projects/joshua, and the in-
structions in using the toolkit are at http://cs.jhu.
edu/?ccb/joshua.
135
ing repeatability of experiments. To combat these
issues, the Joshua toolkit integrates most critical
components of the machine translation pipeline.
Moreover, each component can be treated as a
stand-alone tool and does not rely on the rest of
the toolkit we provide.
Scalability: Our third design goal was to en-
sure that the decoder is scalable to large models
and data sets. The parsing and pruning algorithms
are carefully implemented with dynamic program-
ming strategies, and efficient data structures are
used to minimize overhead. Other techniques con-
tributing to scalability includes suffix-array gram-
mar extraction, parallel and distributed decoding,
and bloom filter language models.
Below we give a short description about the
main functions implemented in our Joshua toolkit.
2.1 Training Corpus Sub-sampling
Rather than inducing a grammar from the full par-
allel training data, we made use of a method pro-
posed by Kishore Papineni (personal communica-
tion) to select the subset of the training data con-
sisting of sentences useful for inducing a gram-
mar to translate a particular test set. This method
works as follows: for the development and test
sets that will be translated, every n-gram (up to
length 10) is gathered into a map W and asso-
ciated with an initial count of zero. Proceeding
in order through the training data, for each sen-
tence pair whose source-to-target length ratio is
within one standard deviation of the average, if
any n-gram found in the source sentence is also
found in W with a count of less than k, the sen-
tence is selected. When a sentence is selected, the
count of every n-gram in W that is found in the
source sentence is incremented by the number of
its occurrences in the source sentence. For our
submission, we used k = 20, which resulted in
1.5 million (out of 23 million) sentence pairs be-
ing selected for use as training data. There were
30,037,600 English words and 30,083,927 French
words in the subsampled training corpus.
2.2 Suffix-array Grammar Extraction
Hierarchical phrase-based translation requires a
translation grammar extracted from a parallel cor-
pus, where grammar rules include associated fea-
ture values. In real translation tasks, the grammars
extracted from large training corpora are often far
too large to fit into available memory.
In such tasks, feature calculation is also very ex-
pensive in terms of time required; huge sets of
extracted rules must be sorted in two directions
for relative frequency calculation of such features
as the translation probability p(f |e) and reverse
translation probability p(e|f) (Koehn et al, 2003).
Since the extraction steps must be re-run if any
change is made to the input training data, the time
required can be a major hindrance to researchers,
especially those investigating the effects of tok-
enization or word segmentation.
To alleviate these issues, we extract only a sub-
set of all available rules. Specifically, we follow
Callison-Burch et al (2005; Lopez (2007) and use
a source language suffix array to extract only those
rules which will actually be used in translating a
particular set of test sentences. This results in a
vastly smaller rule set than techniques which ex-
tract all rules from the training set.
The current code requires suffix array rule ex-
traction to be run as a pre-processing step to ex-
tract the rules needed to translate a particular test
set. However, we are currently extending the de-
coder to directly access the suffix array. This will
allow the decoder at runtime to efficiently extract
exactly those rules needed to translate a particu-
lar sentence, without the need for a rule extraction
pre-processing step.
2.3 Decoding Algorithms2
Grammar formalism: Our decoder assumes a
probabilistic synchronous context-free grammar
(SCFG). Currently, it only handles SCFGs of the
kind extracted by Heiro (Chiang, 2007), but is eas-
ily extensible to more general SCFGs (e.g., (Gal-
ley et al, 2006)) and closely related formalisms
like synchronous tree substitution grammars (Eis-
ner, 2003).
Chart parsing: Given a source sentence to de-
code, the decoder generates a one-best or k-best
translations using a CKY algorithm. Specifically,
the decoding algorithm maintains a chart, which
contains an array of cells. Each cell in turn main-
tains a list of proven items. The parsing process
starts with the axioms, and proceeds by applying
the inference rules repeatedly to prove new items
until proving a goal item. Whenever the parser
proves a new item, it adds the item to the appro-
priate chart cell. The item also maintains back-
2More details on the decoding algorithms are provided in
(Li et al, 2009a).
136
pointers to antecedent items, which are used for
k-best extraction.
Pruning: Severe pruning is needed in order to
make the decoding computationally feasible for
SCFGs with large target-language vocabularies.
In our decoder, we incorporate two pruning tech-
niques: beam and cube pruning (Chiang, 2007).
Hypergraphs and k-best extraction: For each
source-language sentence, the chart-parsing algo-
rithm produces a hypergraph, which represents
an exponential set of likely derivation hypotheses.
Using the k-best extraction algorithm (Huang and
Chiang, 2005), we extract the k most likely deriva-
tions from the hypergraph.
Parallel and distributed decoding: We also
implement parallel decoding and a distributed
language model by exploiting multi-core and
multi-processor architectures and distributed com-
puting techniques. More details on these two fea-
tures are provided by Li and Khudanpur (2008b).
2.4 Language Models
In addition to the distributed LM mentioned
above, we implement three local n-gram language
models. Specifically, we first provide a straightfor-
ward implementation of the n-gram scoring func-
tion in Java. This Java implementation is able to
read the standard ARPA backoff n-gram models,
and thus the decoder can be used independently
from the SRILM toolkit.3 We also provide a na-
tive code bridge that allows the decoder to use the
SRILM toolkit to read and score n-grams. This
native implementation is more scalable than the
basic Java LM implementation. We have also im-
plemented a Bloom Filter LM in Joshua, following
Talbot and Osborne (2007).
2.5 Minimum Error Rate Training
Johsua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu. The optimization
consists of a series of line-optimizations along
the dimensions corresponding to the parameters.
The search across a dimension uses the efficient
method of Och (2003). Each iteration of our
MERT implementation consists of multiple weight
3This feature allows users to easily try the Joshua toolkit
without installing the SRILM toolkit and compiling the native
bridge code. However, users should note that the basic Java
LM implementation is not as scalable as the native bridge
code.
updates, each reflecting a greedy selection of the
dimension giving the most gain. Each iteration
also optimizes several random ?intermediate ini-
tial? points in addition to the one surviving from
the previous iteration, as an approximation to per-
forming multiple random restarts. More details on
the MERT method and the implementation can be
found in Zaidan (2009).4
3 WMT-09 Translation Task Results
3.1 Training and Development Data
We assembled a very large French-English train-
ing corpus (Callison-Burch, 2009) by conducting
a web crawl that targted bilingual web sites from
the Canadian government, the European Union,
and various international organizations like the
Amnesty International and the Olympic Commit-
tee. The crawl gathered approximately 40 million
files, consisting of over 1TB of data. We converted
pdf, doc, html, asp, php, etc. files into text, and
preserved the directory structure of the web crawl.
We wrote set of simple heuristics to transform
French URLs onto English URLs, and considered
matching documents to be translations of each
other. This yielded 2 million French documents
paired with their English equivalents. We split the
sentences and paragraphs in these documents, per-
formed sentence-aligned them using software that
IBM Model 1 probabilities into account (Moore,
2002). We filtered and de-duplcated the result-
ing parallel corpus. After discarding 630 thousand
sentence pairs which had more than 100 words,
our final corpus had 21.9 million sentence pairs
with 587,867,024 English words and 714,137,609
French words.
We distributed the corpus to the other WMT09
participants to use in addition to the Europarl
v4 French-English parallel corpus (Koehn, 2005),
which consists of approximately 1.4 million sen-
tence pairs with 39 million English words and 44
million French words. Our translation model was
trained on these corpora using the subsampling de-
scried in Section 2.1.
For language model training, we used the
monolingual news and blog data that was as-
sembled by the University of Edinburgh and dis-
tributed as part of WMT09. This data consisted
4The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
137
of 21.2 million English sentences with half a bil-
lion words. We used SRILM to train a 5-gram
language model using a vocabulary containing the
500,000 most frequent words in this corpus. Note
that we did not use the English side of the parallel
corpus as language model training data.
To tune the system parameters we used News
Test Set from WMT08 (Callison-Burch et al,
2008), which consists of 2,051 sentence pairs
with 43 thousand English words and 46 thou-
sand French words. This is in-domain data that
was gathered from the same news sources as the
WMT09 test set.
3.2 Translation Scores
The translation scores for four different systems
are reported in Table 1.5
Baseline: In this system, we use the GIZA++
toolkit (Och and Ney, 2003), a suffix-array archi-
tecture (Lopez, 2007), the SRILM toolkit (Stol-
cke, 2002), and minimum error rate training (Och,
2003) to obtain word-alignments, a translation
model, language models, and the optimal weights
for combining these models, respectively.
Minimum Bayes Risk Rescoring: In this sys-
tem, we re-ranked the n-best output of our base-
line system using Minimum Bayes Risk (Kumar
and Byrne, 2004). We re-score the top 300 trans-
lations to minimize expected loss under the Bleu
metric.
Deterministic Annealing: In this system, in-
stead of using the regular MERT (Och, 2003)
whose training objective is to minimize the one-
best error, we use the deterministic annealing
training procedure described in Smith and Eisner
(2006), whose objective is to minimize the ex-
pected error (together with the entropy regulariza-
tion technique).
Variational Decoding: Statistical models in
machine translation exhibit spurious ambiguity.
That is, the probability of an output string is split
among many distinct derivations (e.g., trees or
segmentations). In principle, the goodness of a
string is measured by the total probability of its
many derivations. However, finding the best string
(e.g., during decoding) is then computationally in-
tractable. Therefore, most systems use a simple
Viterbi approximation that measures the goodness
5Note that the implementation of the novel techniques
used to produce the non-baseline results is not part of the cur-
rent Joshua release, though we plan to incorporate it in the
next release.
System BLEU-4
Joshua Baseline 25.92
Minimum Bayes Risk Rescoring 26.16
Deterministic Annealing 25.98
Variational Decoding 26.52
Table 1: The uncased BLEU scores on WMT-09
French-English Task. The test set consists of 2525
segments, each with one reference translation.
of a string using only its most probable deriva-
tion. Instead, we develop a variational approxima-
tion, which considers all the derivations but still
allows tractable decoding. More details will be
provided in Li et al (2009b). In this system, we
have used both deterministic annealing (for train-
ing) and variational decoding (for decoding).
4 Conclusions
We have described a scalable toolkit for parsing-
based machine translation. It is written in Java
and implements all the essential algorithms de-
scribed in Chiang (2007) and Li and Khudanpur
(2008b): chart-parsing, n-gram language model
integration, beam- and cube-pruning, and k-best
extraction. The toolkit also implements suffix-
array grammar extraction (Callison-Burch et al,
2005; Lopez, 2007) and minimum error rate train-
ing (Och, 2003). Additionally, parallel and dis-
tributed computing techniques are exploited to
make it scalable. The decoder achieves state of
the art translation performance.
Acknowledgments
This research was supported in part by the Defense
Advanced Research Projects Agency?s GALE pro-
gram under Contract No. HR0011-06-2-0001 and
the National Science Foundation under grants
No. 0713448 and 0840112. The views and find-
ings are the authors? alone.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08).
138
Chris Callison-Burch. 2009. A 109 word parallel cor-
pus. In preparation.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, , and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Philipp Koehn. 2005. A parallel corpus for statistical
machine translation. In Proceedings of MT-Summit,
Phuket, Thailand.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT/NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008a. Large-scale
discriminative n-gram language models for statisti-
cal machine translation. In Proceedings of AMTA.
Zhifei Li and Sanjeev Khudanpur. 2008b. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li, Chris Callison-Burch, Sanjeev Khudanpur,
and Wren Thornton. 2009a. Decoding in joshua:
Open source, parsing-based machine translation.
The Prague Bulletin of Mathematical Linguistics,
91:47?56.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In preparation.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Robert C. Moore. 2002. Fast and accurate sentence
alignment of bilingual corpora. In Proceedings of
AMTA.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Pro-
ceedings of the ACL/Coling.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken Language Process-
ing, Denver, Colorado, September.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
139
Arabic Dialect Identification
Omar F. Zaidan?
Microsoft Research
Chris Callison-Burch??
University of Pennsylvania
The written form of the Arabic language, Modern Standard Arabic (MSA), differs in a non-
trivial manner from the various spoken regional dialects of Arabic?the true ?native languages?
of Arabic speakers. Those dialects, in turn, differ quite a bit from each other. However, due to
MSA?s prevalence in written form, almost all Arabic data sets have predominantly MSA content.
In this article, we describe the creation of a novel Arabic resource with dialect annotations. We
have created a large monolingual data set rich in dialectal Arabic content called the Arabic On-
line Commentary Data set (Zaidan and Callison-Burch 2011). We describe our annotation
effort to identify the dialect level (and dialect itself) in each of more than 100,000 sentences from
the data set by crowdsourcing the annotation task, and delve into interesting annotator behaviors
(like over-identification of one?s own dialect). Using this new annotated data set, we consider
the task of Arabic dialect identification: Given the word sequence forming an Arabic sentence,
determine the variety of Arabic in which it is written. We use the data to train and evaluate
automatic classifiers for dialect identification, and establish that classifiers using dialectal data
significantly and dramatically outperform baselines that use MSA-only data, achieving near-
human classification accuracy. Finally, we apply our classifiers to discover dialectical data from
a large Web crawl consisting of 3.5 million pages mined from on-line Arabic newspapers.
1. Introduction
The Arabic language is a loose term that refers to the many existing varieties of Arabic.
Those varieties include one ?written? form, Modern Standard Arabic (MSA), and many
?spoken? forms, each of which is a regional dialect. MSA is the only variety that
is standardized, regulated, and taught in schools, necessitated by its use in written
communication and formal venues. The regional dialects, used primarily for day-to-
day dealings and spoken communication, remain somewhat absent from written com-
munication compared with MSA. That said, it is certainly possible to produce dialectal
Arabic text, by using the same letters used in MSA and the same (mostly phonetic)
spelling rules of MSA.
? E-mail: ozaidan@gmail.com.
?? Computer and Information Science Department University of Pennsylvania, Levine Hall, room 506,
3330 Walnut Street, Philadelphia, PA 19104. E-mail: ccb@cis.upenn.edu.
Submission received: 12 March 2012; revised version received: 14 March 2012; accepted for publication:
17 April 2013.
doi:10.1162/COLI a 00169
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 1
One domain of written communication in which both MSA and dialectal Arabic are
commonly used is the on-line domain: Dialectal Arabic has a strong presence in blogs,
forums, chatrooms, and user/reader commentary. Harvesting data from such sources
is a viable option for computational linguists to create large data sets to be used in
statistical learning setups. However, because all Arabic varieties use the same character
set, and furthermore much of the vocabulary is shared among different varieties, it is
not a trivial matter to distinguish and separate the dialects from each other.
In this article, we focus on the problem of Arabic dialect identification. We describe
a large data set that we created by harvesting a large amount of reader commentary
on on-line newspaper content, and describe our annotation effort on a subset of the
harvested data. We crowdsourced an annotation task to obtain sentence-level labels
indicating what proportion of the sentence is dialectal, and which dialect the sentence
is written in. Analysis of the collected labels reveals interesting annotator behavior
patterns and biases, and the data are used to train and evaluate automatic classifiers for
dialect detection and identification. Our approach, which relies on training language
models for the different Arabic varieties, greatly outperforms baselines that use (much
more) MSA-only data: On one of the classification tasks we considered, where human
annotators achieve 88.0% classification accuracy, our approach achieves 85.7% accuracy,
compared with only 66.6% accuracy by a system using MSA-only data.
The article is structured as follows. In Section 2, we provide an introduction to
the various Arabic varieties and corresponding data resources. In Section 3, we intro-
duce the dialect identification problem for Arabic, discussing what makes it a difficult
problem, and what applications would benefit from it. Section 4 provides details about
our annotation set-up, which relied on crowdsourcing the annotation to workers on
Amazon?s Mechanical Turk. By examining the collected labels and their distribution,
we characterize annotator behavior and observe several types of human annotator
biases. We introduce our technique for automatic dialect identification in Section 5.
The technique relies on training separate language models for the different Arabic
varieties, and scoring sentences using these models. In Section 6, we report on a large-
scale Web crawl that we performed to gather a large amount of Arabic text from on-line
newspapers, and apply our classifier on the gathered data. Before concluding, we give
an overview of related work in Section 7.
2. Background: The MSA/Dialect Distinction in Arabic
Although the Arabic language has an official status in over 20 countries and is spoken
by more than 250 million people, the term itself is used rather loosely and refers to
different varieties of the language. Arabic is characterized by an interesting linguistic
dichotomy: the written form of the language, MSA, differs in a non-trivial fashion from
the various spoken varieties of Arabic, each of which is a regional dialect (or a lahjah,
lit. ?accent?; also darjah, lit. ?modern?). MSA is the only variety that is standardized,
regulated, and taught in schools. This is necessitated because of its use in written
communication in formal venues.1 The regional dialects, used primarily for day-to-day
dealings and spoken communication, are not taught formally in schools, and remain
somewhat absent from traditional, and certainly official, written communication.
1 The term MSA is used primarily by linguists and in educational settings. For example, constitutions
of countries where Arabic is an official language simply refer to The Arabic Language, the reference to
the standard form of Arabic being implicit.
172
Zaidan and Callison-Burch Arabic Dialect Identification
Unlike MSA, a regional dialect does not have an explicit written set of grammar
rules regulated by an authoritative organization, but there is certainly a concept of
grammatical and ungrammatical.2 Furthermore, even though they are ?spoken? varieties,
it is certainly possible to produce dialectal Arabic text, by spelling out words using the
same spelling rules used in MSA, which are mostly phonetic.3
There is a reasonable level of mutual intelligibility across the dialects, but the
extent to which a particular individual is able to understand other dialects depends
heavily on that person?s own dialect and their exposure to Arab culture and literature
from outside of their own country. For example, the typical Arabic speaker has little
trouble understanding the Egyptian dialect, thanks in no small part to Egypt?s history
in movie-making and television show production, and their popularity across the Arab
world. On the other hand, the Moroccan dialect, especially in its spoken form, is quite
difficult to understand by a Levantine speaker. Therefore, from a scientific point of
view, the dialects can be considered separate languages in their own right, much like
North Germanic languages (Norwegian/Swedish/Danish) and West Slavic languages
(Czech/Slovak/Polish).4
2.1 The Dialectal Varieties of Arabic
One possible breakdown of regional dialects into main groups is as follows (see
Figure 1):
 Egyptian: The most widely understood dialect, due to a thriving Egyptian
television and movie industry, and Egypt?s highly influential role in the
region for much of the 20th century (Haeri 2003).
 Levantine: A set of dialects that differ somewhat in pronunciation and
intonation, but are largely equivalent in written form; closely related to
Aramaic (Bassiouney 2009).
 Gulf: Folk wisdom holds that Gulf is the closest of the regional dialect to
MSA, perhaps because the current form of MSA evolved from an Arabic
variety originating in the Gulf region. Although there are major differences
between Gulf and MSA, Gulf has notably preserved more of MSA?s verb
conjugation than other varieties have (Versteegh 2001).
2 There exist resources that describe grammars and dictionaries of many Arabic dialects (e.g.,
Abdel-Massih, Abdel-Malek, and Badawi 1979; Badawi and Hinds 1986; Cowell 1964; Erwin 1963;
Ingham 1994; Holes 2004), but these are compiled by individual linguists as one-off efforts, rather
than updated regularly by central regulatory organizations, as is the case with MSA and many
other world languages.
3 Arabic speakers writing in dialectal Arabic mostly follow MSA spelling rules in cases where MSA
is not strictly phonetic as well (e.g., the pronunciation of the definite article Al). Habash, Diab, and
Rambow (2012) have proposed CODA, a Conventional Orthography for Dialectal Arabic, to
standardize the spelling of Arabic dialect computational models.
4 Note that such a view is not widely accepted by Arabic speakers, who hold MSA in high regard. They
consider dialects, including their own, to be simply imperfect, even ?corrupted,? versions of MSA,
rather than separate languages (Suleiman 1994). One exception might be the Egyptian dialect, where a
nationalistic movement gave rise to such phenomena as the Egyptian Wikipedia, with articles written
exclusively in Egyptian, and little, if any, MSA. Another notable exception is the Lebanese poet Said Akl,
who spearheaded an effort to recognize Lebanese as an independent language, and even proposed a
Latin-based Lebanese alphabet.
173
Computational Linguistics Volume 40, Number 1
Figure 1
One possible breakdown of spoken Arabic into dialect groups: Maghrebi, Egyptian, Levantine,
Gulf, and Iraqi. Habash (2010) and Versteegh (2001) give a breakdown along mostly the same
lines. Note that this is a relatively coarse breakdown, and further division of the dialect groups
is possible, especially in large regions such as the Maghreb.
 Iraqi: Sometimes considered to be one of the Gulf dialects, though it has
distinctive features of its own in terms of prepositions, verb conjugation,
and pronunciation (Mitchell 1990).
 Maghrebi: Heavily influenced by the French and Berber languages. The
Western-most varieties could be unintelligible by speakers from other
regions in the Middle East, especially in spoken form. The Maghreb is a
large region with more variation than is seen in other regions such as the
Levant and the Gulf, and could be subdivided further (Mohand 1999).
There are a large number of linguistic differences between MSA and the regional
dialects. Some of those differences do not appear in written form if they are on the level
of short vowels, which are omitted in Arabic text anyway. That said, many differences
manifest themselves textually as well:
 MSA?s morphology is richer than dialects? along some dimensions such
as case and mood. For instance, MSA has a dual form in addition to the
singular and plural forms, whereas the dialects mostly lack the dual
form. Also, MSA has two plural forms, one masculine and one feminine,
whereas many (though not all) dialects often make no such gendered
distinction.5 On the other hand, dialects have a more complex cliticization
system than MSA, allowing for circumfix negation, and for attached
pronouns to act as indirect objects.
 Dialects lack grammatical case, whereas MSA has a complex case system.
In MSA, most cases are expressed with diacritics that are rarely explicitly
written, with the accusative case being a notable exception, as it is
expressed using a suffix (+A) in addition to a diacritic (e.g., on objects
and adverbs).
5 Dialects may preserve the dual form for nouns, but often lack it in verb conjugation and pronouns, using
plural forms instead. The same is true for the gendered plural forms, which exist for many nouns (e.g.,
?teachers? is either m?lmyn [male] or m?lmAt [female]), but not used otherwise as frequently as in MSA.
174
Zaidan and Callison-Burch Arabic Dialect Identification
 There are lexical choice differences in the vocabulary itself. Table 1
gives several examples. Note that these differences go beyond a lack
of orthography standardization.
 Differences in verb conjugation, even when the triliteral root is preserved.
See the lower part of Table 1 for some conjugations of the root s?-r-b
(to drink).
This list, and Table 1, deal with differences that are expressed at the inidividual-
word level. It is important to note that Arabic varieties differ markedly in style and
sentence composition as well. For instance, all varieties of Arabic, MSA, and otherwise,
allow both SVO and VSO word orders, but MSA has a higher incidence of VSO sen-
tences than dialects do (Aoun, Benmamoun, and Sportiche 1994; Shlonsky 1997).
2.2 Existing Arabic Data Sources
Despite the fact that speakers are usually less comfortable communicating in MSA than
in their own dialect, MSA content significantly dominates dialectal content, as MSA
is the variant of choice for formal and official communication. Relatively little printed
material exists in local dialects, such as folkloric literature and some modern poetry,
but the vast majority of published Arabic is in MSA. As a result, MSA?s dominance is
also apparent in data sets available for linguistic research. The problem is somewhat
mitigated in the speech domain, since dialectal data exists in the form of phone conver-
sations and television program recordings, but, in general, dialectal Arabic data sets are
hard to come by.
Table 1
A few examples illustrating similarities and differences across MSA and three Arabic dialects:
Levantine, Gulf, and Egyptian. Even when a word is spelled the same across two or more
varieties, the pronunciation might differ due to differences in short vowels (which are not
spelled out). Also, due to the lack of orthography standardization, and variance in pronunciation
even within a single dialect, some dialectal words could have more than one spelling (e.g.,
Egyptian ?I drink? could be bAs?rb, Levantine ?He drinks? could be bys?rb). (We use the
Habash-Soudi-Buckwalter transliteration scheme to represent Arabic orthography, which maps
each Arabic letter to a single, distinct character. We provide a table with the character mapping
in Appendix A.)
English MSA LEV GLF EGY
Book ktAb ktAb ktAb ktAb
Year sn sn sn sn
Money nqwd mSAry flws flws
Come on! hyA! ylA! ylA! ylA!
I want Aryd bdy Ab?y? ?Ayz
Now AlA?n hlq AlHyn dlwqt
When? mty?? Aymty?? mty?? Amty??
What? mAA? Ays?? ws?? Ayh?
I drink A?s?rb bs?rb As?rb bs?rb
He drinks ys?rb bs?rb ys?rb bys?rb
We drink ns?rb bns?rb ns?rb bns?rb
175
Computational Linguistics Volume 40, Number 1
Figure 2
Two roughly equivalent Arabic sentences, one in MSA and one in Levantine Arabic, translated
by the same MT system (Google Translate) into English. An acceptable translation would be
When will we see this group of criminals undergo trial (or tried)?. The MSA variant is handled well,
whereas the dialectal variant is mostly transliterated.
Figure 3
Two roughly equivalent Arabic sentences, one in MSA and one in Egyptian Arabic, translated
by the same MT system (Google Translate) into English. An acceptable translation would be
What is this that is happening? What is this that I?m seeing?. As in Figure 2, the dialectal variant
is handled quite poorly.
The abundance of MSA data has greatly aided research on computational meth-
ods applied to Arabic, but only the MSA variant of it. For example, a state-of-the-art
Arabic-to-English machine translation system performs quite well when translating
MSA source sentences, but often produces incomprehensible output when the input
is dialectal. For example, most words of the dialectal sentence shown in Figure 2 are
transliterated, whereas an equivalent MSA sentence is handled quite well. The high
transliteration rate is somewhat alarming, as the first two words of the dialectal sentence
are relatively frequent function words: Aymty? means ?when? and rH corresponds to the
modal ?will?.
Figure 3 shows another dialectal sentence, this time in Egyptian, which again causes
the system to produce a poor translation even for frequent words. Case in point, the
system is unable to consistently handle any of Ayh (?what?), Ally (the conjunction ?that?),
or dh (?this?). Granted, it is conceivable that processing dialectal content is more difficult
than MSA, but the main problem is the lack of dialectal training data.6
This is an important point to take into consideration, because the dialects differ to
a large enough extent to warrant treating them as more or less different languages. The
behavior of machine translation systems translating dialectal Arabic when the system
6 In the context of machine translation in particular, additional factors make translating dialectal content
difficult, such as a general mismatch between available training data and the topics that are usually
discussed dialectally.
176
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 4
The output of a Spanish-to-English system when given a Portuguese sentence as input,
compared with the output of a Portuguese-to-English system, which performs well.
The behavior is very similar to that in Figures 2 and 3, namely, the failure to translate
out-of-vocabulary words when there is a language mismatch.
has been trained exclusively on MSA data is similar to the behavior of a Spanish-
to-English MT system when a user inputs a Portuguese sentence. Figure 4 illustrates
how MT systems behave (the analogy is not intended to draw a parallel between the
linguistic differences MSA-dialect and Spanish-Portuguese). The MT system?s behavior
is similar to the Arabic example, in that words that are shared in common between
Spanish and Portuguese are translated, while the Portuguese words that were never
observed in the Spanish training data are left untranslated.
This example illustrates the need for dialectal data to train MT systems to handle
dialectal content properly. A similar scenario would arise with many other NLP tasks,
such as parsing or speech recognition, where dialectal content would be needed in large
quantities for adequate training. A robust dialect identifier could sift through immense
volumes of Arabic text, and separate out dialectal content from MSA content.
2.3 Harvesting Dialect Data from On-line Social Media
One domain of written communication in which MSA and dialectal Arabic are both
commonly used is the on-line domain, because it is more individual-driven and less
institutionalized than other venues. This makes a dialect much more likely to be the
user?s language of choice, and dialectal Arabic has a strong presence in blogs, fo-
rums, chatrooms, and user/reader commentary. Therefore, on-line data is a valuable
resource of dialectal Arabic text, and harvesting this data is a viable option for com-
putational linguists for purposes of creating large data sets to be used in statistical
learning.
We created the Arabic On-line Commentary Data Set (AOC) (Zaidan and Callison-
Burch 2011) a 52M-word monolingual data set by harvesting reader commentary from
the on-line versions of three Arabic newspapers (Table 2). The data is characterized
by the prevalence of dialectal Arabic, alongside MSA, mainly in Levantine, Gulf, and
Egyptian. These correspond to the countries in which the three newspapers are pub-
lished: Al-Ghad is from Jordan, Al-Riyadh is from Saudi Arabia, and Al-Youm Al-Sabe? is
from Egypt.7
Although a significant portion of the AOC?s content is dialectal, there is still a very
large portion of it that is in MSA. (Later analysis in Section 4.2.1 shows dialectal content
is roughly 40%.) In order to take full advantage of the AOC (and other Arabic data sets
7 URLs: www.alghad.com, www.alriyadh.com, and www.youm7.com.
177
Computational Linguistics Volume 40, Number 1
Table 2
A summary of the different components of the AOC data set. Overall, 1.4M comments were
harvested from 86.1K articles, corresponding to 52.1M words.
News Source Al-Ghad Al-Riyadh Al-Youm Al-Sabe? ALL
# articles 6.30K 34.2K 45.7K 86.1K
# comments 26.6K 805K 565K 1.4M
# sentences 63.3K 1,686K 1,384K 3.1M
# words 1.24M 18.8M 32.1M 52.1M
comments/article 4.23 23.56 12.37 16.21
sentences/comment 2.38 2.09 2.45 2.24
words/sentence 19.51 11.14 23.22 16.65
with at least some dialectal content), it is desirable to separate dialectal content from
non-dialectal content automatically. The task of dialect identification (and its automa-
tion) is the focus for the remainder of this article. We next present the task of Arabic
dialect identification, and discuss our effort to create a data set of Arabic sentences with
their dialectal labels. Our annotation effort relied on crowdsourcing the annotation task
to Arabic-speakers on Amazon?s Mechanical Turk service (Section 3).
3. Arabic Dialect Identification
The discussion of the varieties of Arabic and the differences between them gives rise
to the task of automatic dialect identification (DID). In its simplest form, the task is to
build a learner that can, given an Arabic sentence S, determine whether or not S contains
dialectal content. Another form of the task would be to determine in which dialect S was
written, which requires identification at a more fine-grained level.
In many ways, DID is equivalent to language identification. Although language
identification is often considered to be a ?solved problem,? DID is most similar to a
particularly difficult case of language ID, where it is applied to a group of closely related
languages that share a common character set. Given the parallels between DID and
language identification, we investigate standard statistical methods to establish how
difficult the task is. We discuss prior efforts for Arabic DID in Section 7.
3.1 The Difficulty of Arabic DID
Despite the differences illustrated in the previous section, in which we justify treating
the different dialects as separate languages, it is not a trivial matter to automatically
distinguish and separate the dialects from each other. Because all Arabic varieties
use the same character set, and because much of the vocabulary is shared among
different varieties, identifying dialect in a sentence is not simply a matter of, say,
compiling a dialectal dictionary and detecting whether or not a given sentence contains
dialectal words.
This word-level source ambiguity is caused by several factors:
 A dialectal sentence might consist entirely of words that are used across all
Arabic varieties, including MSA. Each of the sentences in Figure 5 consists
178
Zaidan and Callison-Burch Arabic Dialect Identification
of words that are used both in MSA and dialectally, and an MSA-based
dictionary would not (and should not) recognize those words as
out of vocabulary (OOV). Nevertheless, the sentences are heavily
dialectal.
 Some words are used across the varieties with different functions. For
example, Tyb is used dialectally as an interjection, but is an adjective
in MSA. (This is similar to the English usage of okay.)
 Primarily due to the omission of short vowels, a dialectal word might
have the same spelling as an MSA word with an entirely different
meaning, forming pairs of heteronyms. This includes strongly dialectal
words such as dwl and nby: dwl is either Egyptian for these (pronounced
dowl) or the MSA for countries (pronounced duwal); nby is either
the Gulf for we want (pronounced nibi) or the MSA for prophet
(pronounced nabi).
It might not be clear for a non-Arabic speaker what makes certain sentences, such
as those of Figure 5, dialectal, even when none of the individual words are. The answer
lies in the structure of such sentences and the particular word order within them, rather
than the individual words themselves taken in isolation. Figure 6 shows MSA sentences
that express the same meaning as the dialectal sentences from Figure 5. As one could
see, the two versions of any given sentence could share much of the vocabulary, but
in ways that are noticeably different to an Arabic speaker. Furthermore, the differences
would be starker still if the MSA sentences were composed from scratch, rather than
by modifying the dialectal sentences, since the tone might differ substantially when
composing sentences in MSA.
Figure 5
Three sentences that were identified by our annotators as dialectical, even thought they do
not contain individually dialectal words. A word-based OOV-detection approach would
fail to classify these sentences as being dialectal, because all these words could appear
in an MSA corpus. One might argue that a distinction should be drawn between informal
uses of MSA versus dialectical sentences, but annotators consistently classify these sentences
as dialect.
179
Computational Linguistics Volume 40, Number 1
Figure 6
The dialectal sentences of Figure 5, with MSA equivalents.
3.2 Applications of Dialect Identification
Being able to perform automatic DID is interesting from a purely linguistic and experi-
mental point of view. In addition, automatic DID has several useful applications:
 Distinguishing dialectal data from non-dialectal data would aid in creating
a large monolingual dialectal data set, exactly as we would hope to do
with the AOC data set. Such a data set would aid many NLP systems that
deal with dialectal content, for instance, to train a language model for
an Arabic dialect speech recognition system (Novotney, Schwartz, and
Khudanpur 2011). Identifying dialectal content can also aid in creating
parallel data sets for machine translation, with a dialectal source side.
 A user might be interested in content of a specific dialect, or, conversely,
in strictly non-dialectal content. This would be particularly relevant
in fine-tuning and personalizing search engine results, and could
allow for better user-targeted advertising. In the same vein, being
able to recognize dialectal content in user-generated text could aid in
characterizing communicants and their biographic attributes (Garera
and Yarowsky 2009).
180
Zaidan and Callison-Burch Arabic Dialect Identification
 In the context of an application such as machine translation (MT),
identifying dialectal content could be quite helpful. Most MT systems,
when faced with OOV words, either discard the words or make an effort
to transliterate them. If a segment is identified as being dialectal first, the
MT system might instead attempt to find equivalent MSA words, which
are presumably easier to process correctly (e.g., as in Salloum and Habash
[2011] and, to some degree, Habash [2008]). Even for non-OOV words,
identifying dialectal content before translating could be critical to resolve
the heteronym ambiguity of the kind mentioned in Section 3.1.
4. Crowdsourcing Arabic Dialect Annotation
In this section, we discuss crowdsourcing Arabic dialect annotation. We discuss how
we built a data set of Arabic sentences, each of which is labeled with whether or not
it contains dialectal content. The labels include additional details about the level of
dialectal content (i.e., how much dialect there is), and of which type of dialect it is. The
sentences themselves are sampled from the AOC data set, and we observe that about
40% of sentences contain dialectal content, with that percentage varying between 37%
and 48%, depending on the news source.
Collecting annotated data for speech and language applications requires careful
quality control (Callison-Burch and Dredze 2010). We present the annotation interface
and discuss an effective way for quality control that can detect spamming behavior. We
then examine the collected data itself, analyzing annotator behavior, measuring agree-
ment among annotators, and identifying interesting biases exhibited by the annotators.
In Section 5, we use the collected data to train and evaluate statistical models for several
dialect identification tasks.
4.1 Annotation Interface
The annotation interface displayed a group of Arabic sentences, randomly selected from
the AOC. For each sentence, the annotator was instructed to examine the sentence and
make two judgments about its dialectal content: the level of dialectal content, and its
type, if any. The instructions were kept short and simple:
This task is for Arabic speakers who understand the different local Arabic dialects,
and can distinguish them from Fusha8 Arabic.
Below, you will see several Arabic sentences. For each sentence:
1. Tell us how much dialect is in the sentence, and then
2. Tell us which Arabic dialect the writer intends.
The instructions were accompanied by the map of Figure 1, to visually illustrate
the dialect breakdown. Figure 7 shows the annotator interface populated with some
actual examples, with labeling in progress. We also collected self-reported information
such as native Arabic dialect and age (or number of years speaking Arabic for non-
native speakers). The interface also had built-in functionality to detect each annotator?s
geographic location based on their IP address.
8 Fusha is the Arabic word for MSA, pronounced foss-ha.
181
Computational Linguistics Volume 40, Number 1
Figure 7
The interface for the dialect identification task. This example, and the full interface, can be
viewed at the http://bit.ly/eUtiO3.
Of the 3.1M sentences in the AOC, we randomly9 selected a ?small? subset of about
110,000 sentences to be annotated for dialect.
For each sentence shown in the interface, we asked annotators to label which dialect
the segment is written in and the level of dialect in the segment. The dialect labels were
Egyptian, Gulf, Iraqi, Levantine, Maghrebi, other dialect, general dialect (for segments
that could be classified as multiple dialects), dialect but unfamiliar (for sentences that
are clearly dialect, but are written in a dialect that the annotator is not familiar with), no
dialect (for MSA), or not Arabic (for segments written in English or other languages).
Options for the level of dialect included no dialect (for MSA), a small amount of dialect,
an even mix of dialect and MSA, mostly dialect, and not Arabic. For this article we
use only the dialect labels, and not the level of dialect. Zaidan (2012) incorporates
finer-grained labels into an ?annotator rationales? model (Zaidan, Eisner, and Piatko
2007).
The sentences were randomly grouped into sets of 10 sentences each, and when
Workers performed our task, they were shown the 10 sentences of a randomly selected
set on a single HTML page. As a result, each screen contained a mix of sentences across
the three newspapers presented in random order. As control items, each screen had two
additional sentences that were randomly sampled from the article bodies. Such sentences
are almost always in MSA Arabic, and so their expected label is MSA. Any worker
who frequently mislabeled the control sentences with a non-MSA label was considered
a spammer, and their work was rejected. Hence, each screen had twelve sentences in
total.
9 There are far fewer sentences available from Al-Ghad commentary than the other two sources over any
given period of time (third line of Table 2). We have taken this imbalance into account and heavily
oversampled Al-Ghad sentences when choosing sentences to be labeled, to obtain a subset that is more
balanced across the three sources.
182
Zaidan and Callison-Burch Arabic Dialect Identification
We offered a reward of $0.05 per screen (later raised to $0.10), and had each set
redundantly completed by three distinct Workers. The data collection lasted about
4.5 months, during which 33,093 Human Intelligence Task (HIT) Assignments were
completed, corresponding to 330,930 collected labels (excluding control items). The
total cost of annotation was $3,050.52 ($2,773.20 for rewards, and $277.32 for Amazon?s
commission).
4.2 Annotator Behavior
With the aid of the embedded control segments (taken from article bodies) and expected
dialect label distribution, it was possible to spot spamming behavior and reject it. Table 3
shows three examples of workers whose work was rejected on this basis, having clearly
demonstrated they are unable or unwilling to perform the task faithfully. In total,
11.4% of the assignments were rejected on this basis. In the approved assignments,
the embedded MSA control sentence was annotated with the MSA label 94.4% of
the time. In the remainder of this article, we analyze only data from the approved
assignments.
We note here that we only rejected assignments where the annotator?s behavior
was clearly problematic, opting to approve assignments from workers mentioned later in
Section 4.2.3, who exhibit systematic biases in their labels. Although these annotators?
behavior is non-ideal, we cannot assume that they are not working faithfully, and
therefore rejecting their work might not be fully justified. Furthermore, such behavior
might be quite common, and it is worth investigating these biases to benefit future
research.
Table 3
Some statistics over the labels provided by three spammers. Compared with the typical worker
(right-most column), all workers perform terribly on the MSA control items, and also usually fail
to recognize dialectal content in commentary sentences. Other red flags, such as geographic
location and ?identifying? unrepresented dialects, are further proof of the spammy behavior.
A
29
V
7O
G
M
2C
62
05
A
3S
Z
L
M
2N
K
8N
U
O
G
A
8E
F1
I6
C
O
7T
C
U
Typical
MSA in control items 0% 14% 33% >90%
LEV in Al-Ghad 0% 0% 15% 25%
GLF in Al-Riyadh 8% 0% 14% 20%
EGY in Al-Youm Al-Sabe? 5% 0% 27% 33%
Other dialects 56% 0% 28% <1%
Incomplete answers 13% 6% 1% <2%
Worker location Romania Philippines Jordan Middle East
Claimed native dialect Gulf ?Other? Unanswered (Various)
183
Computational Linguistics Volume 40, Number 1
4.2.1 Label Distribution. Overall, 454 annotators participated in the task, 138 of whom
completed at least 10 HITs. Upon examination of the provided labels for the com-
mentary sentences, 40.7% of them indicate some level of dialect, and 57.1% indicate
no dialectal content (Figure 8a). Note that 2.14% of the labels identify a sentence as
being non-Arabic, non-textual, or as being left unanswered. The label breakdown is a
strong confirmation of our initial motivation, which is that a large portion of reader
commentary contains dialectal content.10
Figure 8 also illustrates the following:
 The most common dialectal label within a given news source matches
the dialect of the country of publication. This is not surprising, since the
readership for any newspaper is likely to mostly consist of the local
population of that country. Also, given the newspapers? countries of
publication, there is almost no content that is in a dialect other than
Levantine, Gulf, or Egyptian. For this reason, other dialects such as
Iraqi and Maghrebi, all combined, correspond to less than 0.01% of our
data, and we mostly drop them from further discussion.
 The three news sources vary in the prevalence of dialectal content. The
Egyptian newspaper has a markedly larger percentage of dialectal content
(46.6% of labels) compared with the Saudi newspaper (40.1%) and the
Jordanian newspaper (36.8%).
 A nontrivial amount of labels (5?8%) indicate General dialectal content.
The General label was meant to indicate a sentence that is dialectal but
lacks a strong indication of a particular dialect. Although many of the
provided General labels seem to reflect an intent to express this fact,
there is evidence that some annotators used this category in cases where
choosing the label Not sure would have been more appropriate but
was ignored (see Section 4.2.3).
 Non-Arabic content, although infrequent, is not a rare occurrence in the
Jordanian and Egyptian newspapers, at around 3%. The percentage is
much lower in the Saudi newspaper, at 0.8%. This might reflect the deeper
penetration of the English language (and English-only keyboards) in
Jordan and Egypt compared with Saudi Arabia.
We can associate a label with each segment based on the majority vote over the three
provided labels for that segment. If a sentence has at least two annotators choosing
a dialectal label, we label it as dialect. If it has at least two annotators choosing
the MSA label, we label it as MSA.11 In the remainder of the article, we will report
classification accuracy rates that assume the presence of gold-standard class labels.
Unless otherwise noted, this majority-vote label set is used as the gold-standard in such
experiments.
10 Later analysis in Section 4.2.3 shows that a non-trivial portion of the labels were provided by MSA-biased
annotators, indicating that dialectal content could be even more prevalent than what is initially suggested
by the MSA/dialect label breakdown.
11 A very small percentage of sentences (2%) do not have such agreement; upon inspection these are
typically found to be sentences that are in English, e-mail addresses, romanized Arabic, or simply
random symbols.
184
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 8
The distribution of labels provided by the workers for the dialect identification task, over all
three news sources (a) and over each individual news source (b?d). Al-Ghad is published in
Jordan, Al-Riyadh in Saudi Arabia, and Al-Youm Al-Sabe? in Egypt. Their local readerships are
reflected in the higher proportion of corresponding dialects. Note that this is not a breakdown
on the sentence level, and does not reflect any kind of majority voting. For example, most of
the LEV labels on sentences from the Saudi newspaper are trumped by GLF labels when taking
a majority vote, making the proportion of LEV-majority sentences smaller than what might be
deduced by looking at the label distribution in (c).
In experiments where the dialectal label set is more fine-grained (i.e., LEV, GLF, and
EGY instead of simply dialect), we assign to the dialectal sentence the label corre-
sponding to the news source?s country of publication. That is, dialectal sentences in the
Jordanian (respectively, Saudi, Egyptian) are given the label LEV (respectively, GLF, EGY).
We could have used dialect labels provided by the annotators, but chose to override
those using the likely dialect of the newspaper instead. It turns out that sentences with
an EGY majority, for instance, are extremely unlikely to appear in either the Jordanian or
Saudi newspaper?only around 1% of those sentences have an EGY majority. In the case
of the Saudi newspaper, 9% of all dialectal sentences were originally annotated as LEV
but were transformed to GLF. Our rationales for performing the transformation is that
no context was given for the sentences when they were annotated, and annotators had a
bias towards their own dialect. We provide the original annotations for other researchers
to re-analyze if they wish.
185
Computational Linguistics Volume 40, Number 1
Table 4
The specific-dialect label distribution (given that a dialect label was provided), shown for each
speaker group.
Group size % LEV % GLF % EGY % GNRL % Other dialects
All speakers 454 26.1 27.1 28.8 15.4 2.6
Levantine speakers 181 35.9 28.4 21.2 12.9 1.6
Gulf speakers 32 21.7 29.4 25.6 21.8 1.4
Egyptian speakers 121 25.9 19.1 38.0 10.9 6.1
Iraqi speakers 16 18.9 29.0 23.9 18.2 10.1
Maghrebi speakers 67 20.5 28.0 34.5 12.7 4.3
Other/Unknown 37 17.9 18.8 27.8 31.4 4.1
Even when a sentence would receive a majority-vote label that differs from
the news source?s primary dialect, inspection of such sentences reveals that the
classification was usually unjustified, and reflected a bias towards the annotator?s
native dialect. Case in point: Gulf-speaking annotators were in relatively short supply,
whereas a plurality of annotators spoke Levantine (see Table 4). Later in Section 4.2.3,
we point out that annotators have a native-dialect bias, whereby they are likely to
label a sentence with their native dialect even when the sentence has no evidence of
being written in that particular dialect. This explains why a non-trivial number of LEV
labels were given by annotators to sentences from the Saudi newspaper (Figure 8). In
reality, most of these labels were given by Levantine speakers over-identifying their
own dialect. Even if we were to assign dialect labels based on the (Levantine-biased)
majority votes, Levantine would only cover 3.6% of the sentences from the Saudi
newspaper.12
Therefore, for simplicity, we assume that a dialectal sentence is written in the
dialect corresponding to the sentence?s news source, without having to inspect the
specific dialect labels provided by the annotators. This not only serves to simplify our
experimental set-up, but also contributes to partially reversing the native dialect bias
that we observed.
4.2.2 Annotator Agreement and Performance. The annotators exhibit a decent level of
agreement with regard to whether a segment is dialectal or not, with full agreement (i.e.,
across all three annotators) on 72.2% of the segments regarding this binary dialect/MSA
decision. This corresponds to a kappa value of 0.619 (using the definition of Fleiss
[1971] for multi-rater scenarios), indicating very high agreement.13 The full-agreement
percentage decreases to 56.2% when expanding the classification from a binary decision
to a fine-grained scale that includes individual dialect labels as well. This is still quite
a reasonable result, since the criterion is somewhat strict: It does not include a segment
labeled, say, {Levantine, Levantine, General}, though there is good reason to consider
that annotators are in ?agreement? in such a case.
12 Note that the distributions in Figure 8 are on the label level, not on the sentence level.
13 Although it is difficult to determine the significance of a given kappa value, Landis and Koch (1977)
characterize kappa values above 0.6 to indicate ?substantial agreement? between annotators.
186
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 9
A bubble chart showing workers? MSA and dialect recall. Each data point (or bubble) in
the graph represents one annotator, with the bubble size corresponding to the number of
assignments completed by that annotator.
So how good are humans at the classification task? We examine their classifica-
tion accuracy, dialect recall, and MSA recall. The classification accuracy is measured
over all sentences, both MSA and dialectal. We define dialect (MSA) recall to be the
number of sentences labeled as being dialectal (MSA), over the total number of sen-
tences that have dialectal (MSA) labels based on the majority vote. Overall, human
annotators have a classification accuracy of 90.3%, with dialect recall at 89.0%, and
MSA recall at 91.5%. Those recall rates do vary across annotators, as shown in Fig-
ure 9, causing some accuracy rates to drop as low as 80% or 75%. Of the annota-
tors performing at least five HITs, 89.4% have accuracy rates greater than or equal
to 80%.
Most annotators have both high MSA recall and high dialect recall, with about 70%
of them achieving at least 80% in both MSA and dialect recall. Combined with the
general agreement rate measure, this is indicative that the task is well-defined?it is
unlikely that many people would agree on something that is incorrect.
We note here that the accuracy rate (90.3%) is a slight overestimate of the human
annotators? accuracy rate, by virtue of the construction of the gold labels. Because the
correct labels are based on a majority vote of the annotators? labels themselves, the two
sets are not independent, and an annotator is inherently likely to be correct. A more
informative accuracy rate disregards the case where only two of the three annotators
agreed and the annotator whose accuracy was being evaluated contributed one of those
two votes. In other words, an annotator?s label would be judged against a majority vote
that is independent of that annotator?s label. Under this evaluation set-up, the human
accuracy rate slightly decreases, to 88.0%.
4.2.3 Annotator Bias Types. Examining the submitted labels of individual workers reveals
interesting annotation patterns, and indicates that annotators are quite diverse in their
187
Computational Linguistics Volume 40, Number 1
Table 5
Two annotators with a General label bias, one who uses the label liberally, and one who is more
conservative. Note that in both cases, there is a noticeably smaller percentage of General labels
in the Egyptian newspaper than in the Jordanian and Saudi newspapers.
A
ll
w
or
ke
rs
A
1M
50
U
V
37
A
M
B
Z
3
A
2Z
N
K
1P
Z
O
V
IE
C
D
% General 6.3 12.0 2.3
% General in Al-Ghad 5.2 14.2 3.1
% General in Al-Riyadh 7.7 13.1 2.6
% General in Al-Youm Al-Sabe? 4.9 7.6 1.0
Native dialect (Various) Maghrebi Egyptian
behavior. An annotator can be observed to have one or more of the following bias
types:14
 MSA bias/dialect bias: Figure 9 shows that annotators vary in how willing
they are to label a sentence as being dialectal. Whereas most workers (top
right) exhibit both high MSA and high dialect recall, other annotators have
either a MSA bias (top left) or a dialect bias (bottom right).
 Dialect-specific bias: Many annotators over-identify a particular dialect,
usually their native one. If we group the annotators by their native dialect
and examine their label breakdown (Table 4), we find that Levantine
speakers over-identify sentences as being Levantine, Gulf speakers
over-identify Gulf, and Egyptian speakers over-identify Egyptian. This
holds for speakers of other dialects as well, as they over-identify other
dialects more often than most speakers. Another telling observation
is that Iraqi speakers have a bias for the Gulf dialect, which is quite
similar to Iraqi. Maghrebi speakers have a bias for Egyptian, reflecting
their unfamiliarity with the geographically distant Levantine and
Gulf dialects.
 The General bias: The General label is meant to signify sentences that
cannot be definitively classified as one dialect over another. This is the
case when enough evidence exists that the sentence is not in MSA, but
contains no evidence for a specific dialect. In practice, some annotators
make very little use of this label, even though many sentences warrant
its use, whereas other annotators make extensive use of this label (see,
for example, Table 5). One interesting case is that of annotators whose
General label seems to mean they are unable to identify the dialect,
14 These biases should be differentiated from spammy behavior, which we already can deal with quite
effectively, as explained in Section 4.2.
188
Zaidan and Callison-Burch Arabic Dialect Identification
and a label like Not sure might have been more appropriate. Take the
case of the Maghrebi worker in Table 5, whose General bias is much
more pronounced in the Jordanian and Saudi newspapers. This is
an indication she might have been having difficulty distinguishing
Levantine and Gulf from each other, but that she is familiar with the
Egyptian dialect.
5. Automatic Dialect Identification
From a computational point of view, we can think of dialect identification as language
identification, though with finer-grained distinctions that make it more difficult than
typical language ID. Even languages that share a common character set can be distin-
guished from each other at high accuracy rates using methods as simple as examining
character histograms (Cavnar and Trenkle 1994; Dunning 1994; Souter et al. 1994), and,
as a largely solved problem, the one challenge becomes whether languages can be
identified for very short segments.
Due to the nature and characteristics and high overlap across Arabic dialects, rely-
ing on character histograms alone is ineffective (see Section 5.3.1), and more context
is needed. We will explore higher-order letter models as well as word models, and
determine what factors determine which model is best.
5.1 Smoothed n-Gram Models
Given a sentence S to classify into one of k classes C1, C2, . . . , Ck, we will choose the class
with the maximum conditional probability:
C? = argmax
Ci
P(Ci|S) = argmax
Ci
P(S|Ci) ? P(Ci) (1)
Note that the decision process takes into account the prior distribution of the
classes, which is estimated from the training set. The training set is also used to train
probabilistic models to estimate the probability of S given a particular class. We rely
on training n-gram language models to compute such probabilities, and apply Kneser-
Ney smoothing to these probabilities and also use that technique to assign probability
mass to unseen or OOV items (Chen and Goodman 1998). In language model scoring, a
sentence is typically split into words. We will also consider letter-based models, where
the sentence is split into sequences of characters. Note that letter-based models would
be able to take advantage of clues in the sentence that are not complete words, such as
prefixes or suffixes. This would be useful if the amount of training data is very small,
or if we expect a large domain shift between training and testing, in which case content
words indicative of MSA or dialect might not still be valuable in the new domain.
Although our classification method is based only on language model scoring, and
is thus relatively simple, it is nevertheless very effective. Experimental results in Sec-
tion 5.3 (e.g., Figure 10) indicate that this method yields accuracy rates above 85%,
only slightly behind the human accuracy rate of 88.0% reported in Section 4.2.2.
5.2 Baselines
To properly evaluate classification performance trained on dialectal data, we compare
the language-model classifiers to two baselines that do not use the newly collected data.
189
Computational Linguistics Volume 40, Number 1
Figure 10
Learning curves for the general MSA vs. dialect task, with all three news sources pooled
together. Learning curves for the individual news sources can be found in Figure 11.
The 83% line has no significance, and is provided to ease comparison with Figure 11.
Rather, they use available MSA-only data and attempt to determine how MSA-like a
sentence is.
The first baseline is based on the assumption that a dialectal sentence would contain
a higher percentage of ?non-MSA? words that cannot be found in a large MSA corpus.
To this end, we extracted a vocabulary list from the Arabic Gigaword Corpus, producing
a list of 2.9M word types. Each sentence is given a score that equals the OOV percentage,
and if this percentage exceeds a certain threshold, the sentence is classified as being
dialectal. For each of the cross validation runs in Section 5.3.1, we use the threshold
that yields the optimal accuracy rate on the test set (hence giving this baseline as
much a boost as possible). In our experiments, we found this threshold to be usually
around 10%.
The second approach uses a more fine-grained approach. We train a language model
using MSA-only data, and use it to score a test sentence. Again, if the perplexity exceeds
a certain threshold, the sentence is classified as being dialectal. To take advantage of
domain knowledge, we train this MSA model on the sentences extracted from the article
bodies of the AOC, which corresponds to 43M words of highly relevant content.
5.3 Experimental Results
In this section, we explore using the collected labels to train word- and letter-based
DID systems, and show that they outperform other baselines that do not utilize the
annotated data.
5.3.1 Two-Way, MSA vs. Dialect Classification. We measure classification accuracy at vari-
ous training set sizes, using 10-fold cross validation, for several classification tasks. We
examine the task both as a general MSA vs. dialect task, as well as when restricted
within a particular news source. We train unigram, bigram, and trigram (word-based)
models, as well as unigraph, trigraph, and 5-graph (letter-based) models. Table 6 sum-
marizes the accuracy rates for these models, and includes rates for the baselines that do
not utilize the dialect-annotated data.
Generally, we find that a unigram word model performs best, with a 5-graph model
slightly behind (Figure 11). Bigram and trigram word models seem to suffer from the
sparseness of the data and lag behind, given the large number of parameters they
190
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 11
Learning curves for the MSA vs. dialect task, for each of the three news sources. The 83% line
has no significance, and is provided to ease comparison across the three components, and
with Figure 10.
would need to estimate (and instead resort to smoothing heavily). The letter-based
models, with a significantly smaller vocabulary size, do not suffer from this problem,
and perform well. This is a double-edged sword though, especially for the trigraph
model, as it means the model is less expressive and converges faster.
Overall though, the experiments show a clear superiority of a supervised method,
be it word- or letter-based, over baselines that use existing MSA-only data. Whichever
model we choose (with the exception of the unigraph model), the obtained accuracy
rates show a significant dominance over the baselines.
It is worth noting that a classification error becomes less likely to occur as the length
of the sentence increases (Figure 12). This is not surprising given prior work on the
language identification problem (R?ehu?r?ek and Kolkus 2009; Verma, Lee, and Zakos
2009), which points out that the only ?interesting? aspect of the problem is performance
on short segments. The same is true in the case of dialect identification: a short sentence
191
Computational Linguistics Volume 40, Number 1
Table 6
Accuracy rates (%) on several two-way classification tasks (MSA vs. dialect) for various models.
Models in the top part of the table do not utilize the dialect-annotated data, whereas models in
the bottom part do. (For the latter kind of models, the accuracy rates reported are based on a
training set size of 90% of the available data.)
Model MS
A
vs
.d
ia
le
ct
A
l-
G
ha
d
MS
A
vs
.d
ia
le
ct
(L
ev
an
ti
ne
)
A
l-
R
iy
ad
h
MS
A
vs
.d
ia
le
ct
(G
u
lf
)
A
l-
Yo
um
A
l-
Sa
be
?M
SA
vs
.d
ia
le
ct
(E
gy
p
ti
an
)
Majority Class 58.8 62.5 60.0 51.9
OOV % vs. Gigaword 65.5 65.1 65.3 66.7
MSA LM-scoring 66.6 67.8 66.8 65.2
Letter-based, 1-graph 68.1 69.9 68.0 70.4
Letter-based, 3-graph 83.5 85.1 81.9 86.0
Letter-based, 5-graph 85.0 85.7 81.4 87.0
Word-based, 1-gram 85.7 87.2 83.3 87.9
Word-based, 2-gram 82.8 84.1 80.6 85.9
Word-based, 3-gram 82.5 83.7 80.4 85.6
that contains even a single misleading feature is prone to misclassification, whereas a
long sentence is likely to have other features that help identify the correct class label.15
One could also observe that distinguishing MSA from dialect is a more difficult
task in the Saudi newspaper than in the Jordanian paper, which in turn is harder than
in the Egyptian newspaper. This might be considered evidence that the Gulf dialect
is the closest of the dialects to MSA, and Egyptian is the farthest, in agreement with
the conventional wisdom. Note also that this is not due to the fact that the Saudi
sentences tend to be significantly shorter?the ease of distinguishing Egyptian holds
even at higher sentence lengths, as shown by Figure 12.
5.3.2 Multi-Way, Fine-Grained Classification. The experiments reported earlier focused
on distinguishing MSA from dialect when the news source is known, making it
straightforward to determine which of the Arabic dialects a sentence is written in (once
15 The accuracy curve for the Egyptian newspaper has an outlier for sentence lengths 10?12. Upon
inspection, we found that over 10% of the sentences in that particular length subset were actually
repetitions of a single 12-word sentence. (A disgruntled reader, angry about perceived referee corruption,
essentially bombarded the reader commentary section of several articles with that single sentence.)
This created an artificial overlap between the training and test sets, hence increasing the accuracy
rate beyond what would be reasonably expected due to increased sentence length alone.
192
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 12
Accuracy rates vs. sentence length in the general MSA vs. dialect task. Accuracy rates shown
are for the unigram word model trained on 90% of the data.
the sentence is determined to be dialectal). If the news source is not known, we do not
have the luxury of such a strong prior on the specific Arabic dialect. It is therefore
important to evaluate our approach in a multi-way classifiation scenario, where the
class set is expanded from {MSA, dialect} to {MSA, LEV, GLF, EGY}.
Under this classification set-up, the classification accuracy decreases from 85.7% to
81.0%.16 The drop in performance is not at all surprising, since four-way classification is
inherently more difficult than two-way classification. (Note that the classifier is trained
on exactly the same training data in both scenarios, but with more fine-grained dialectal
labels in the four-way set-up.)
Table 7 is the classifier?s confusion matrix for this four-way set-up, illustrating
when the classifier tends to make mistakes. We note here that most classification errors
on dialectal sentences occur when these sentences are mislabeled as being MSA, not
when they are misidentified as being in some other incorrect dialect. In other words,
dialect?dialect confusion constitutes a smaller proportion of errors than dialect?MSA
confusion. Indeed, if we consider a three-way classification setup on dialectal sentences
alone (LEV vs. GLF vs. EGY), the classifier?s accuracy rate shoots up to 88.4%. This is a
higher accuracy rate than for the general two-way MSA vs. dialect classification (85.7%),
despite involving more classes (three instead of two), and being trained on less data
(0.77M words instead of 1.78M words). This indicates that the dialects deviate from
MSA in various ways, and therefore distinguishing dialects from each other can be done
even more effectively than distinguishing dialect from MSA.
5.3.3 Word and Letter Dialectness. Examining the letter and word distribution in the
corpus provides valuable insight into what features of a sentence are most dialectal.
Let DF(w) denote the dialectness factor of a word w, defined as:
DF(w) def=
f (w|D)
f (w|MSA) =
countD(w)/countD(.)
countMSA(w)/countMSA(.)
(2)
16 For clarity, we report accuracy rates only for the unigram classifier. The patterns from Section 5.3.1 mostly
hold here as well, in terms of how the different n-gram models perform relative to each other.
193
Computational Linguistics Volume 40, Number 1
Table 7
Confusion matrix in the four-way classification setup. Rows correspond to actual labels, and
columns correspond to predicted labels. For instance, 6.7% of MSA sentences were given a GLF
label (first row, third column). Note that entries within a single row sum to 100%.
Class label MSA LEV GLF EGY
MSA Sentences 86.5% 4.2% 6.7% 2.6%
LEV Sentences 20.6% 69.1% 8.6% 1.8%
GLF Sentences 24.2% 2.4% 72.0% 1.4%
EGY Sentences 14.4% 2.2% 4.6% 78.8%
where countD(w) (respectively, countMSA(w)) is the number of times w appeared in the
dialectal (respectively, MSA) sentences, and countD(.) is the total number of words in
those sentences. Hence, DF(w) is simply a ratio measuring how much more likely w is to
appear in a dialectal sentence than in an MSA sentence. Note that the dialectness factor
can be easily computed for letters as well, and can be computed for bigrams/bigraphs,
trigrams/trigraphs, and so forth.
Figure 13 lists, for each news source, the word types with the highest and lowest
dialectness factor. The most dialectal words tend to be function words, and they also
tend to be strong indicators of dialect, judging by their very high DF. On the other hand,
the MSA word group contains several content words, relating mainly to politics and
religion.
One must also take into account the actual frequency of a word, as DF only captures
relative frequencies of dialect/MSA, but does not capture how often the word occurs in
the first place. Figure 14 plots both measures for the words of Al-Ghad newspaper. The
Figure 13
Words with the highest and lowest dialectness factor values in each of the three news sources.
194
Zaidan and Callison-Burch Arabic Dialect Identification
Figure 14
A plot of the most common words in the Al-Ghad sentences, showing each word?s DF and
corpus frequency. The right- and left-most words here also appear in Figure 13. Not every
word from that list appears here though, since some words have counts below 100. For clarity,
not all points display the words they represent.
plot illustrates which words are most important to the classifier: the words that are
farthest away from the point of origin, along both dimensions.
As for letter-based features, many of the longer ones (e.g., 5-graph features) are
essentially the same words important to the unigram word model. The letter-based
models are, however, able to capture some linguistic phenomenon that the word model
is unable to: the suffixes +s? (not in Levantine) and +wn (plural conjugation in Gulf),
and the prefixes H+ (will in Egyptian), bt+ (present tense conjugation in Levantine and
Egyptian), and y+ (present tense conjugation in Gulf).
Figure 15 sheds some light on why even the unigraph model outperforms the
baselines. It picks up on subtle properties of the MSA writing style that are lacking
Figure 15
A plot of the most common letters in the Al-Ghad sentences, showing each letter?s DF and
corpus frequency.
195
Computational Linguistics Volume 40, Number 1
when using dialect. Namely, there is closer attention to following hamza rules (distin-
guishing A, A?, and A? from each other, rather than mapping them all to A), and better
adherence to (properly) using + instead of +h at the end of many words. There is also
a higher tendency to use words containing the letters that are most susceptible to being
transformed when pronounced dialectally: ? (usually pronounced as z), D? (pronounced
as D), and ? (pronounced as t).
On the topic of spelling variation, one might wonder if nomalizing the Arabic
text before training language models might enhance coverage and therefore improve
performance. For instance, would it help to map all forms of the alef hamza to a single
letter, and all instances of  to h, and so on? Our pilot experiments indicated that such
normalization tends to slightly but consistently hurt performance, so we opted to leave
the Arabic text as is. The only type of preprocessing we performed was more on the
?cleanup? side of things rather than computationally motivated normalization, such as
proper conversion of HTML entities (e.g., &quot; to ") and mapping Eastern Arabic
numerals to their European equivalents.
6. Applying DID to a Large-Scale Arabic Web Crawl
We conducted a large-scale Web crawl to gather Arabic text from the on-line versions of
newspapers from various Arabic-speaking countries. The first batch contained 319 on-
line Arabic-language newspapers published in 24 countries. This list was compiled from
http://newspapermap.com/ and http://www.onlinenewspapers.com/, which are Web
sites that show the location and language of newspapers published around the world.
The list contained 55 newspapers from Lebanon, 42 from Egypt, 40 from Saudi Arabia,
26 from Yemen, 26 from Iraq, 18 from Kuwait, 17 from Morocco, 15 from Algeria, 12
from Jordan, and 10 from Syria. The data were gathered from July?Sept 2011.
We mirrored the 319 Web sites using wget, resulting in 20 million individual files
and directories. We identified 3,485,241 files that were likely to contain text by selecting
the extensions htm, html, cmff, asp, pdf, rtf, doc, and docx. We converted these files
to text using xpdf?s pdftotext for PDFs and Apple?s textutil for HTML and Doc files.
When concatenated together, the text files contained 438,940,861 lines (3,452,404,197
words). We performed de-duplication to remove identical lines, after which 18,219,348
lines (1,393,010,506 words) remained.
We used the dialect-annotated data to train a language model for each of the four
Arabic varieties (MSA, LEV, GLF, EGY), as described in the previous section. We used these
models to classify the crawled data, assigning a given sentence the label corresponding
to the language model under which that sentence received the highest score. Table 8
gives the resulting label breakdown. We see that the overwhelming majority of the
sentences are classified as MSA, which comes as no surprise, given the prevalence of
MSA in the newspaper genre. Figure 16 shows some sentences that were given non-
MSA labels by our classifier.
7. Related Work
Habash et al. (2008) presented annotation guidelines for the identification of dialec-
tal content in Arabic content, paying particular attention to cases of code switching.
They present pilot annotation results on a small set of around 1,600 Arabic sentences
(19k words), with both sentence- and word-level dialectness annotations.
196
Zaidan and Callison-Burch Arabic Dialect Identification
Table 8
Predicted label breakdown for the crawled data, over the four varieties of Arabic. All varieties
were given equal priors.
Variety Sentence Count Percentage
MSA 13,102,427 71.9%
LEV 3,636,525 20.0%
GLF 630,726 3.5%
EGY 849,670 4.7%
ALL 18,219,348 100.0%
The Cross Lingual Arabic Blog Alerts (COLABA) project (Diab et al. 2010) is another
large-scale effort to create dialectal Arabic resources (and tools). They too focus on on-
line sources such as blogs and forums, and use information retrieval tasks to measure
their ability to properly process dialectal Arabic content. The COLABA project demon-
strates the importance of using dialectal content when training and designing tools that
deal with dialectal Arabic, and deal quite extensively with resource creation and data
harvesting for dialectal Arabic.
Figure 16
Example sentences from the crawled data set that were predicted to be dialectal, two in each of
the three Arabic dialects.
197
Computational Linguistics Volume 40, Number 1
Chiang et al. (2006) investigate building a parser for Levantine Arabic, without
using any significant amount of dialectal data. They utilize an available Levantine?MSA
lexicon, but no parses of Levantine sentences. Their work illustrates the difficulty of
adapting MSA resources for use in a dialectal domain.
Zbib et al. (2012) show that incorporating dialect training data into a statistical
machine translation system vastly improves the quality of the translation of dialect
sentences when compared to a system trained solely on an MSA-English parallel cor-
pus. When translating Egyptian and Levantine test sets, a dialect Arabic MT system
outperforms a Modern Standard Arabic MT system trained on a 150 million word
Arabic?English parallel corpus?over 100 times the amount of data as their dialect
parallel corpus.
As far as we can tell, no prior dialect identification work exists that is applied to Ara-
bic text. However, Lei and Hansen (2011) and Biadsy, Hirschberg, and Habash (2009) in-
vestigate Arabic dialect identification in the speech domain. Lei and Hansen (2011) build
Gaussian mixture models to identify the same three dialects we consider, and are able
to achieve an accuracy rate of 71.7% using about 10 hours of speech data for training.
Biadsy, Hirschberg, and Habash (2009) utilize a much larger data set (170 hours of
speech data) and take a phone recognition and language modeling approach (Zissman
1996). In a four-way classification task (with Iraqi as a fourth dialect), they achieve a
78.5% accuracy rate. It must be noted that both works use speech data, and that dialect
identification is done on the speaker level, not the sentence level as we do.
8. Conclusion
Social media, like reader commentary on on-line newspapers, is a rich source of dialectal
Arabic that has previously not been studied in detail. We have harvested this type of
resource to create a large data set of informal Arabic that is rich in dialectal content. We
selected a large subset of this data set, and had the sentences in it manually annotated
for dialect. We used the collected labels to train and evaluate automatic classifiers for
dialect identification, and observed interesting linguistic aspects about the task and
annotators? behavior. Using an approach based on language model scoring, we develop
classifiers that significantly outperform baselines that use large amounts of MSA data,
and we approach the accuracy rates exhibited by human annotators.
In addition to n-gram features, one could imagine benefiting from morphological
features of the Arabic text, by incorporating analyses given by automatic analyzers such
as BAMA (Buckwalter 2004), MAGEAD (Habash and Rambow 2006), ADAM (Salloum
and Habash 2011), or CALIMA (Habash, Eskander, and Hawwari 2012). Although the
difference between our presented approach and human annotators was found to be
relatively small, incorporating additional linguistically motivated features might be
pivotal in bridging that final gap.
In future annotation efforts, we hope to solicit more detailed labels about dialectal
content, such as specific annotation for why a certain sentence is dialectal and not MSA:
Is it due to structural differences, dialectal terms, and so forth? We also hope to expand
beyond the three dialects discussed in this article, by including sources from a larger
number of countries.
Given the recent political unrest in the Middle East (2011), another rich source of
dialectal Arabic are Twitter posts (e.g., with the #Egypt tag) and discussions on various
political Facebook groups. Here again, given the topic at hand and the individualistic
nature of the posts, they are very likely to contain a high degree of dialectal data.
198
Zaidan and Callison-Burch Arabic Dialect Identification
Appendix A
The Arabic transliteration scheme used in the article is the Habash-Soudi-Buckwalter
transliteration (HSBT) mapping (Habash, Soudi, and Buckwalter 2007), which extends
the scheme designed by Buckwalter in the 1990s (Buckwalter 2002). Buckwalter?s origi-
nal scheme represents Arabic orthography by designating a single, distinct ASCII char-
acter for each Arabic letter. HSBT uses some non-ASCII characters for better readibility,
but maintains the distinct 1-to-1 mapping.
Figure 17 lists the character mapping used in HSBT. We divide the list into four
sections: vowels, forms of the hamzah (glottal stop), consonants, and pharyngealized
Figure 17
The character mapping used in the HBST scheme. Most mappings are straightforward; a
few non-obvious mappings are highlighted with an arrow (?) next to them. For brevity, the
mappings for short vowels and other diacritics are omitted. Note that we take the view that
? is a pharyngealized glottal stop, which is supported by Gairdner (1925), Al-Ani (1970),
Ka?stner (1981), Thelwall and Sa?Adeddin (1990), and Newman (2002). For completeness,
we indicate its IPA name as well.
199
Computational Linguistics Volume 40, Number 1
consonants. Pharyngealized consonants are ?thickened? versions of other, more familiar
consonants, voiced such that the pharynx or epiglottis is constricted during the articula-
tion of the sound. Those consonants are present in very few languages and are therefore
likely to be unfamiliar to most readers, which is why we place them in a separate
section?there is no real distinction in Arabic between them and other consonants.
HSBT also allows for the expression of short vowels and other Arabic diacritics, but
because those diacritics are only rarely expressed in written (and typed) form, we omit
them for brevity.
Acknowledgments
This research was supported in part by
the DARPA GALE program under contract
no. HR0011-06-2-0001, the DARPA BOLT
program contract no. HR0011-12-C-0014,
the EuroMatrixPlus project funded by the
European Commission (7th Framework
Programme), the Human Language
Technology Center of Excellence, and by
gifts from Google and Microsoft. The views
and findings are the authors? alone. They
do not reflect the official policies or positions
of the Department of Defense or the
U.S. Government. The authors would like
to thank the anonymous reviewers for their
extremely valuable comments on earlier
drafts of this article, and for suggesting
future work ideas.
References
Abdel-Massih, Ernest T., Zaki N.
Abdel-Malek, and El-Said M. Badawi.
1979. A Reference Grammar of Egyptian
Arabic. Georgetown University Press.
Al-Ani, Salman H. 1970. Arabic Phonology:
An Acoustical and Physiological Investigation.
Mouton.
Aoun, Joseph, Elabbas Benmamoun, and
Dominique Sportiche. 1994. Agreement,
word order, and conjunction in some
varieties of Arabic. Linguistic Inquiry,
25(2):195?220.
Badawi, El-Said and Martin Hinds. 1986.
A Dictionary of Egyptian Arabic. Librairie
du Liban.
Bassiouney, Reem. 2009. Arabic
Sociolinguistics. Edinburgh
University Press.
Biadsy, Fadi, Julia Hirschberg, and
Nizar Habash. 2009. Spoken Arabic dialect
identification using phonotactic modeling.
In Proceedings of the EACL Workshop on
Computational Approaches to Semitic
Languages, pages 53?61, Athens.
Buckwalter, Tim. 2002. Buckwalter Arabic
transliteration. http://www.qamus.org/
transliteration.htm.
Buckwalter, Tim. 2004. Buckwalter Arabic
morphological analyzer version 2.0.
Linguistic Data Consortium,
Philadelphia, PA.
Callison-Burch, Chris and Mark Dredze.
2010. Creating speech and language
data with Amazon?s Mechanical Turk.
In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language
Data with Amazon?s Mechanical Turk,
pages 1?12, Los Angeles, CA.
Cavnar, William B. and John M. Trenkle.
1994. N-gram-based text categorization.
In Proceedings of SDAIR-94, pages 161?175,
Vilnius.
Chen, Stanley F. and Joshua T. Goodman.
1998. An empirical study of smoothing
techniques for language modeling.
Technical Report TR-10-98, Computer
Science Group, Harvard University.
Chiang, David, Mona Diab, Nizar Habash,
Owen Rambow, and Safiullah Shareef.
2006. Parsing Arabic dialects.
In Proceedings of EACL, pages 369?376,
Trento.
Cowell, Mark W. 1964. A Reference
Grammar of Syrian Arabic. Georgetown
University Press.
Diab, Mona, Nizar Habash, Owen Rambow,
Mohamed Altantawy, and Yassine
Benajiba. 2010. COLABA: Arabic dialect
annotation and processing. In Proceedings
of the LREC Workshop on Semitic Language
Processing, pages 66?74.
Dunning, T. 1994. Statistical identification of
language. Technical Report MCCS 94-273,
New Mexico State University.
Erwin, Wallace. 1963. A Short Reference
Grammar of Iraqi Arabic. Georgetown
University Press.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Gairdner, William Henry Temple. 1925.
The Phonetics of Arabic. Oxford
University Press.
Garera, Nikesh and David Yarowsky. 2009.
Modeling latent biographic attributes in
200
Zaidan and Callison-Burch Arabic Dialect Identification
conversational genres. In Proceedings of
ACL, pages 710?718, Singapore.
Habash, Nizar. 2008. Four techniques for
online handling of out-of-vocabulary
words in Arabic-English statistical
machine translation. In Proceedings
of ACL, Short Papers, pages 57?60,
Columbus, OH.
Habash, Nizar, Mona Diab, and Owen
Rambow. 2012. Conventional orthography
for dialectal Arabic. In Proceedings of
the Language Resources and Evaluation
Conference (LREC), pages 711?718,
Istanbul.
Habash, Nizar, Ramy Eskander, and Abdelati
Hawwari. 2012. A morphological analyzer
for Egyptian Arabic. In Proceedings of the
Twelfth Meeting of the Special Interest Group
on Computational Morphology and Phonology,
pages 1?9, Montre?al.
Habash, Nizar and Owen Rambow. 2006.
MAGEAD: A morphological analyzer
and generator for the Arabic dialects.
In Proceedings of the 21st International
Conference on Computational Linguistics and
44th Annual Meeting of the Association for
Computational Linguistics, pages 681?688,
Sydney.
Habash, Nizar, Owen Rambow, Mona Diab,
and Reem Kanjawi-Faraj. 2008. Guidelines
for annotation of Arabic dialectness.
In Proceedings of the LREC Workshop on
HLT & NLP within the Arabic World,
pages 49?53, Marrakech.
Habash, Nizar, Abdelhadi Soudi, and
Tim Buckwalter. 2007. On Arabic
transliteration. In Antal van den Bosch,
Abdelhadi Soudi, and Gu?nter Neumann,
editors, Arabic Computational Morphology:
Knowledge-based and Empirical Methods.
Kluwer/Springer Publications, chapter 2.
Habash, Nizar Y. 2010. Introduction to
Arabic Natural Language Processing.
Morgan & Claypool.
Haeri, Niloofar. 2003. Sacred Language,
Ordinary People: Dilemmas of Culture and
Politics in Egypt. Palgrave Macmillan.
Holes, Clive. 2004. Modern Arabic: Structures,
Functions, and Varieties. Georgetown
Classics in Arabic Language and
Linguistics. Georgetown University Press.
Ingham, Bruce. 1994. Najdi Arabic: Central
Arabian. John Benjamins.
Ka?stner, Hartmut. 1981. Phonetik und
Phonologie des modernen Hocharabisch.
Verlag Enzyklopa?die.
Landis, J. Richard and Gary G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics, 33:159?174.
Lei, Yun and John H. L. Hansen. 2011. Dialect
classification via text-independent training
and testing for Arabic, Spanish, and
Chinese. IEEE Transactions on Audio, Speech,
and Language Processing, 19(1):85?96.
Mitchell, Terence Frederick. 1990.
Pronouncing Arabic. Clarendon Press.
Mohand, Tilmatine. 1999. Substrat et
convergences: Le berbe?re et l?arabe
nord-africain. Estudios de Dialectologia?
Norteaafricana y andalus??, 4:99?119.
Newman, Daniel L. 2002. The phonetic status
of Arabic within the world?s languages.
Antwerp Papers in Linguistics, 100:63?75.
Novotney, Scott, Rich Schwartz, and Sanjeev
Khudanpur. 2011. Unsupervised Arabic
dialect adaptation with self-training.
In Interspeech, pages 541?544, Florence.
R?ehu?r?ek, Radim and Milan Kolkus. 2009.
Language Identification on the Web:
Extending the Dictionary Method,
volume 5449 of Lecture Notes in Computer
Science, pages 357?368. SpringerLink.
Salloum, Wael and Nizar Habash. 2011.
Dialectal to standard Arabic paraphrasing
to improve Arabic-English statistical
machine translation. In Proceedings
of the EMNLP Workshop on Algorithms
and Resources for Modelling of Dialects
and Language Varieties, pages 10?21,
Edinburgh.
Shlonsky, Ur. 1997. Clause Structure and
Word Order in Hebrew and Arabic:
An Essay in Comparative Semitic Syntax.
Oxford University Press.
Souter, Clive, Gavin Churcher, Judith Hayes,
John Hughes, and Stephen Johnson. 1994.
Natural language identification using
corpus-based models. Hermes Journal of
Linguistics, 13:183?203.
Suleiman, Yasir. 1994. Nationalism and
the Arabic language: A historical
overview. In Yasir Suleiman, editor,
Arabic Sociolinguistics. Curzon Press.
Thelwall, Robin and M. Akram Sa?Adeddin.
1990. Arabic. Journal of the International
Phonetic Association, 20(2):37?39.
Verma, Brijesh, Hong Lee, and John Zakos.
2009. An Automatic Intelligent Language
Classifier, volume 5507 of Lecture Notes
in Computer Science, pages 639?646.
SpringerLink.
Versteegh, Kees. 2001. The Arabic Language.
Edinburgh University Press.
Zaidan, Omar, Jason Eisner, and Christine
Piatko. 2007. Using ?annotator rationales?
to improve machine learning for text
categorization. In Human Language
Technologies 2007: The Conference of the
201
Computational Linguistics Volume 40, Number 1
North American Chapter of the Association
for Computational Linguistics; Proceedings
of the Main Conference, pages 260?267,
Rochester, NY.
Zaidan, Omar F. 2012. Crowdsourcing
Annotation for Machine Learning in Natural
Language Processing Tasks. Ph.D. thesis,
Johns Hopkins University, Baltimore, MD.
Zaidan, Omar F. and Chris Callison-Burch. 2011.
The Arabic Online Commentary Dataset:
An annotated dataset of informal Arabic
with high dialectal content. In Proceedings
of ACL, pages 37?41, Portland, OR.
Zbib, Rabih, Erika Malchiodi, Jacob Devlin,
David Stallard, Spyros Matsoukas, Richard
Schwartz, John Makhoul, Omar F. Zaidan,
and Chris Callison-Burch. 2012. Machine
translation of Arabic dialects. In the 2012
Conference of the North American Chapter of
the Association for Computational Linguistics,
pages 49?59, Montreal.
Zissman, Marc A. 1996. Comparison of
four approaches to automatic language
identification of telephone speech. IEEE
Transactions on Speech and Audio Processing,
4(1):31?44.
202
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 369?372,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Predicting Human-Targeted Translation Edit Rate 
via Untrained Human Annotators 
 
Omar F. Zaidan  and  Chris Callison-Burch 
Dept. of Computer Science, Johns Hopkins University 
Baltimore, MD 21218, USA 
{ozaidan,ccb}@cs.jhu.edu 
 
 
 
Abstract 
In the field of machine translation, automatic 
metrics have proven quite valuable in system 
development for tracking progress and meas-
uring the impact of incremental changes. 
However, human judgment still plays a large 
role in the context of evaluating MT systems. 
For example, the GALE project uses human-
targeted translation edit rate (HTER), wherein 
the MT output is scored against a post-edited 
version of itself (as opposed to being scored 
against an existing human reference). This 
poses a problem for MT researchers, since 
HTER is not an easy metric to calculate, and 
would require hiring and training human an-
notators to perform the editing task. In this 
work, we explore soliciting those edits from 
untrained human annotators, via the online 
service Amazon Mechanical Turk. We show 
that the collected data allows us to predict 
HTER-ranking of documents at a significantly 
higher level than the ranking obtained using 
automatic metrics. 
1 Introduction 
In the early days of machine translation (MT), it 
was typical to evaluate MT output by soliciting 
judgments from human subjects, such as evaluat-
ing the fluency and adequacy of MT output (LDC, 
2005). While this approach was appropriate (in-
deed desired) for evaluating a system, it was not a 
practical means of tracking the progress of a sys-
tem during its development, since collecting hu-
man judgments is both costly and time-consuming. 
The introduction of automatic metrics like BLEU 
contributed greatly to MT research, for instance 
allowing researchers to measure and evaluate the 
impact of small modifications to an MT system. 
However, manual evaluation remains a core 
component of system evaluation. Teams on the 
GALE project, a DARPA-sponsored MT research 
program, are evaluated using the HTER metric, 
which is a version of TER whereby the output is 
scored against a post-edited version of itself, in-
stead of a preexisting reference. Moreover, empha-
sis is placed on performing well across all 
documents and across all genres. Therefore, it is 
important for a research team to be able to evaluate 
their system using HTER, or at least determine the 
ranking of the documents according to HTER, for 
purposes of error analysis. Instead of hiring a 
human translator and training them, we propose 
moving the task to the virtual world of Amazon?s 
Mechanical Turk (AMT), hiring workers to edit the 
MT output and predict HTER from those edits. We 
show that edits collected this way are better at 
predicting document ranking than automatic 
metrics, and furthermore that it can be done at a 
low cost, both in terms of time and money. 
The paper is organized as follows. We first 
discuss options available to predict HTER, such as 
automatic metrics. We then discuss the possibility 
of relying on human annotators, and the inherent 
difficulty in training them, before discussing the 
concept of soliciting edits over AMT. We detail 
the task given to the workers and summarize the 
data that we collected, then show how we can 
combine their data to obtain significanly better 
rank predictions of documents. 
2 Human-Targeted TER 
Translation edit rate (TER) measures the number 
of edits required to transform a hypothesis into an 
appropriate sentence in terms of grammaticality 
and meaning (Snover et al, 2006). While TER 
usually scores a hypothesis against an existing ref-
erence sentence, human-targeted TER scores a 
hypothesis against a post-edited version of itself. 
369
While HTER has been shown to correlate quite 
well with human judgment of MT quality, it is 
quite challenging to obtain HTER scores for MT 
output, since this would require hiring and training 
human subjects to perform the editing task. There-
fore, other metrics such as BLEU or TER are used 
as proxies for HTER. 
2.1 Amazon?s Mechanical Turk 
The high cost associated with hiring and training a 
human editor makes it difficult to imagine an alter-
native to automatic metrics. However, we propose 
soliciting edits from workers on Amazon?s Me-
chanical Turk (AMT). AMT is a virtual market-
place where ?requesters? can post tasks to be 
completed by ?workers? (aka Turkers) around the 
world. Two main advantages of AMT are the pre-
existing infrastructure, and the low cost of com-
pleting tasks, both in terms of time and money. 
Data collected over AMT has already been used in 
several papers such as Snow et al (2008) and Cal-
lison-Burch (2009). 
When a requester creates a task to be completed 
over AMT, it is typical to have completed by more 
than one worker. The reason is that the use of 
AMT for data collection has an inherent problem 
with data quality. A requester has fewer tools at 
their disposal to ensure workers are doing the task 
properly (via training, feedback, etc) when com-
pared to hiring annotators in the ?real? world. 
Those redundant annotations are therefore col-
lected to increase the likelihood of at least one 
submission from a faithful (and competent) 
worker. 
2.2 AMT for HTER 
The main idea it to mimic the real-world HTER 
setup by supplying workers with the original MT 
output that needs to be edited. The worker is also 
given a human reference, produced independently 
from the MT output. The instructions ask the 
worker to modify the MT output, using as few ed-
its as possible, to match the human reference in 
meaning and grammaticality. 
The submitted edited hypothesis can then be 
used as the reference for calculating HTER. The 
idea is that, with this setup, a competent worker 
would be able to closely match the editing behav-
ior of the professionally trained editor. 
3 The Datasets 
We solicited edits of the output from one of 
GALE?s teams on the Arabic-to-English task. This 
MT output was submitted by this team and HTER-
scored by LDC-hired human translators. Therefore, 
we already had the edits produced by a 
professional translator. These edits were used as 
the ?gold-standard? to evaluate the edits solicited 
from AMT and to evaluate our methods of 
combining Turkers? submissions. 
The MT output is a translation of more than 
2,153 Arabic segments spread across 195 docu-
ments in 4 different genres: broadcast conversa-
tions (BC), broadcast news (BN), newswire (NW), 
and blogs (WB). Table 1 gives a summary of each 
genre?s dataset. 
 
Genre # docs Segs/doc Words/seg 
BC 40 15.8 28.3 
BN 48 9.6 36.1 
NW 54 8.7 39.5 
WB 53 11.1 31.6 
Table 1:  The 4 genres of the dataset. 
 
For each of the 2,153 MT output segments, we 
collected edits from 5 distinct workers on AMT, 
for a total of 10,765 post-edited segments by a total 
of about 500 distinct workers.1 The segments were 
presented in 1,210 groups of up to 15 segments 
each, with a reward of $0.25 per group. Hence the 
total rewards to workers was around $300, at a rate 
of 36 post-edited segments per dollar (or 2.8 pen-
nies per segment). 
4 What are we measuring? 
We are interested in predicting the ranking the 
documents according to HTER, not necessarily 
predicting the HTER itself (though of course at-
tempting to predict the latter accurately is the cor-
nerstone of our approach to predict the former). To 
measure the quality of a predicted ranking, we use 
Spearman?s rank correlation coefficient, ?, where 
we first convert the raw scores into ranks and then 
use the following formula to measure correlation: 
)1(
))()((6
1),( 2
1
2
?
?
?=
?
=
nn
yrankxrank
YX
n
i
ii
?  
                                                           
1 Data available at http://cs.jhu.edu/~ozaidan/hter. 
370
 
where n is the number of documents, and each of X 
and Y is a vector of n HTER scores. 
Notice that values for ? range from ?1 to 1, with 
+1 indicating perfect rank correlation, ?1 perfect 
inverse correlation, and 0 no correlation. That is, 
for a fixed X, the best-correlated Y is that for which 
),( YX?  is highest. 
5 Combining Tukers? Edits 
Once we have collected edits from the human 
workers, how should we attempt to predict HTER 
from them? If we could assume that all Turkers are 
doing the task faithfully (and doing it adequately), 
we should use the annotations of the worker per-
forming the least amount of editing, since that 
would mirror the real-life scenario. 
However, data collected from AMT should be 
treated with caution, since a non-trivial portion of 
the collected data is of poor quality. Note that this 
does not necessarily indicate a ?cheating? worker, 
for even if a worker is acting in good faith, they 
might not be able to perform the task adequately, 
due to misunderstanding the task, or neglecting to 
attempt to use a small number of edits. 
And so we need to combine the redundant edits in 
an intelligent manner. Recall that, given a segment, 
we collected edits from multiple workers. Some 
baseline methods include taking the minimum over 
the edits, taking the median, and taking the aver-
age. 
Once we start thinking of averages, we should 
consider taking a weighted average of the edits for 
a segment. The weight associated with a worker 
should reflect our confidence in the quality of that 
worker?s edits. But how can we evaluate a worker 
in the first place? 
5.1 Self Verification of Turkers 
We have available ?gold-standard? editing behav-
ior for the segments, and we treat a small portion 
of the segments edited by a Turker as a verification 
dataset. On that portion, we evaluate how closely 
the Turker matches the LDC editor, and weight 
them accordingly when predicting the number of 
edits of the rest of that group?s segments. Specifi-
cally, the Turker?s weight is the absolute difference 
between the Turker?s edit count and the profes-
sional editor?s edit count. 
Notice that we are not simply interested in a 
worker whose edited submission closely matches 
the edited submission of the professional transla-
tor. Rather, we are interested in mirroring the pro-
fessional translator?s edit rate. That is, the closer a 
Turker?s edit rate is to the LDC editor?s, the more 
we should prefer the worker. This is a subtle point, 
but it is indeed possible for a Turker to have simi-
lar edit rate as the LDC editor but still require a 
large number of edits to get the LDC editor?s sub-
mission itself. 
6 Experiments 
We examine the effectiveness of any of the above 
methods by comparing the resulting document 
ranking versus the desired ranking by HTER. In 
addition to the above methods, we use a baseline a 
ranking predicted by TER to a human reference. 
(For clarity, we omit discussion with other metrics 
such as BLEU and (TER?BLEU)/2, since those 
baselines are not as strong as the TER baseline. 
6.1 Experimental Setup 
We examine each genre individually, since genres 
vary quite a bit in difficulty, and, more impor-
tantly, we care about the internal ranking within 
each genre, to mirror the GALE evaluation proce-
dure. 
We examine the effect of varying the amount of 
data by which we judge a Turker?s data quality. 
The amount of this ?verification? data is varied as 
a percentage of the total available segments. Those 
segments are chosen at random, and we perform 
100 trials for each point. 
6.2 Experimental Results 
Figure 1 shows the rank correlations for various 
methods across different sizes of verification sub-
sets. Notice that some methods, such as the TER 
baseline, have horizontal lines, since these do not 
rate a Turker based on a verification subset. 
It is worth noting that the oracle performs very 
well. This is an indication that predicting HTER 
accurately is mostly a matter of identifying the best 
worker. While oracle scenarios usually represent 
unachievable upper bounds, keep in mind that 
there are only a very small number of editors per 
segment (five, as opposed to oracle scenarios deal-
ing with 100-best lists, etc). 
371
Other than that, in general, it is possible to 
achieve very high rank correlation using Turkers? 
data, significantly outperforming the TER ranking, 
even with a small verification subset. The genres 
do vary quite a bit in difficulty for Turkers, with 
BC and especially NW being quite difficult, 
though in the case of NW for instance, this is due 
to the human reference doing quite well to begin 
with, rather than Turkers performing poorly. 
7 Conclusions and Future Work 
We proposed soliciting edits of MT output via 
Amazon?s Mechanical Turk and showed we can 
predict ranking significantly better than an auto-
matic metric. The next step is to explicitly identify 
undesired worker behavior, such as not editing the 
MT output at all, or using the human reference as 
is instead of editing the MT output. This can be 
detected by not limiting our verification to compar-
ing behavior to the professional editor?s, but also 
by comparing submitted edits to the MT output 
itself and to the human reference. In other words, a 
worker?s submission could be characterized in 
terms of its distance to the MT output and to the 
human reference, thus building a complete ?pro-
file? of the worker, and adding another component 
to guard against poor data quality and to reward 
the desired behavior. 
Acknowledgments 
This work was supported by the EuroMatrixPlus 
Project (funded by the European Commission), and 
by the DARPA GALE program under Contract No. 
HR0011-06-2-0001. The views and findings are 
the authors' alone. 
References  
Chris Callison-Burch. 2009. Fast, Cheap, and Creative: 
Evaluating Translation Quality Using Amazon's Me-
chanical Turk. In Proceedings of EMNLP. 
LDC. 2005. Linguistic data annotation specification: 
Assessment of fluency and adequacy in translations. 
Revision 1.5. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz. 
2006. A Study of Translation Edit Rate with Targeted 
Human Annotation. Proceedings of AMTA. 
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and 
Andrew Y. Ng. 2008. Cheap and fast ? but is it 
good? Evaluating non-expert annotations for natural 
language tasks. In Proceedings of EMNLP. 
 
 
 
 
 
 
 
 
Figure 1: Rank correlation between predicted rank-
ing and HTER ranking for different prediction 
schemes, across the four genres, and across various 
sizes of the worker verification set. 
372
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49?59,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Machine Translation of Arabic Dialects
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas,
Richard Schwartz, John Makhoul, Omar F. Zaidan?, Chris Callison-Burch?
Raytheon BBN Technologies, Cambridge MA
?Microsoft Research, Redmond WA
?Johns Hopkins University, Baltimore MD
Abstract
Arabic Dialects present many challenges for
machine translation, not least of which is the
lack of data resources. We use crowdsourc-
ing to cheaply and quickly build Levantine-
English and Egyptian-English parallel cor-
pora, consisting of 1.1M words and 380k
words, respectively. The dialectal sentences
are selected from a large corpus of Arabic web
text, and translated using Amazon?s Mechan-
ical Turk. We use this data to build Dialec-
tal Arabic MT systems, and find that small
amounts of dialectal data have a dramatic im-
pact on translation quality. When translating
Egyptian and Levantine test sets, our Dialec-
tal Arabic MT system performs 6.3 and 7.0
BLEU points higher than a Modern Standard
Arabic MT system trained on a 150M-word
Arabic-English parallel corpus.
1 Introduction
The Arabic language is a well-known example of
diglossia (Ferguson, 1959), where the formal vari-
ety of the language, which is taught in schools and
used in written communication and formal speech
(religion, politics, etc.) differs significantly in its
grammatical properties from the informal varieties
that are acquired natively, which are used mostly for
verbal communication. The spoken varieties of the
Arabic language (which we refer to collectively as
Dialectal Arabic) differ widely among themselves,
depending on the geographic distribution and the
socio-economic conditions of the speakers, and they
diverge from the formal variety known as Mod-
ern Standard Arabic (MSA) (Embarki and Ennaji,
2011). Significant differences in the phonology,
morphology, lexicon and even syntax render some
of these varieties mutually incomprehensible.
The use of Dialectal Arabic has traditionally been
confined to informal personal speech, while writ-
ing has been done almost exclusively using MSA
(or its ancestor Classical Arabic). This situation is
quickly changing, however, with the rapid prolifer-
ation of social media in the Arabic-speaking part
of the world, where much of the communication
is composed in dialect. The focus of the Arabic
NLP research community, which has been mostly on
MSA, is turning towards dealing with informal com-
munication, with the introduction of the DARPA
BOLT program. This new focus presents new chal-
lenges, the most obvious of which is the lack of di-
alectal linguistic resources. Dialectal text, which is
usually user-generated, is also noisy, and the lack
of standardized orthography means that users often
improvise spelling. Dialectal data also includes a
wider range of topics than formal data genres, such
as newswire, due to its informal nature. These chal-
lenges require innovative solutions if NLP applica-
tions are to deal with Dialectal Arabic effectively.
In this paper:
? We describe a process for cheaply and quickly
developing parallel corpora for Levantine-
English and Egyptian-English using Amazon?s
Mechanical Turk crowdsourcing service (?3).
? We use the data to perform a variety of machine
translation experiments showing the impact of
morphological analysis, the limited value of
adding MSA parallel data, the usefulness of
cross-dialect training, and the effects of trans-
lating from dialect to MSA to English (?4).
We find that collecting dialect translations has a low
cost ($0.03/word) and that relatively small amounts
of data has a dramatic impact on translation quality.
When trained on 1.5M words of dialectal data, our
system performs 6.3 to 7.0 BLEU points higher than
when it is trained on 100 times more MSA data from
a mismatching domain.
49
2 Previous Work
Existing work on natural language processing of Di-
alectal Arabic text, including machine translation, is
somewhat limited. Previous research on Dialectal
Arabic MT has focused on normalizing dialectal in-
put words into MSA equivalents before translating
to English, and they deal with inputs that contain
a limited fraction of dialectal words. Sawaf (2010)
normalized the dialectal words in a hybrid (rule-
based and statistical) MT system, by performing a
combination of character- and morpheme-level map-
pings. They then translated the normalized source
to English using a hybrid MT or alternatively a
Statistical MT system. They tested their method
on proprietary test sets, observing about 1 BLEU
point (Papineni et al, 2002) increase on broadcast
news/conversation and about 2 points on web text.
Salloum and Habash (2011) reduced the proportion
of dialectal out-of-vocabulary (OOV) words also by
mapping their affixed morphemes to MSA equiva-
lents (but did not perform lexical mapping on the
word stems). They allowed for multiple morpho-
logical analyses, passing them on to the MT system
in the form of a lattice. They tested on a subset of
broadcast news and broadcast conversation data sets
consisting of sentences that contain at least one re-
gion marked as non-MSA, with an initial OOV rate
against an MSA training corpus of 1.51%. They
obtained a 0.62 BLEU point gain. Abo Bakr et
al. (2008) suggested another hybrid system to map
Egyptian Arabic to MSA, using morphological anal-
ysis on the input and an Egyptian-MSA lexicon.
Other work that has focused on tasks besides MT
includes that of Chiang et al (2006), who built a
parser for spoken Levantine Arabic (LA) transcripts
using an MSA treebank. They used an LA-MSA
lexicon in addition to morphological and syntac-
tic rules to map the LA sentences to MSA. Riesa
and Yarowsky (2006) built a statistical morphologi-
cal segmenter for Iraqi and Levantine speech tran-
scripts, and showed that they outperformed rule-
based segmentation with small amounts of training.
Some tools exist for preprocessing and tokenizing
Arabic text with a focus on Dialectal Arabic. For ex-
ample, MAGEAD (Habash and Rambow, 2006) is a
morphological analyzer and generator that can ana-
lyze the surface form of MSA and dialect words into
their root/pattern and affixed morphemes, or gener-
ate the surface form in the opposite direction.
Amazon?s Mechanical Turk (MTurk) is becom-
ing an essential tool for creating annotated resources
for computational linguistics. Callison-Burch and
Dredze (2010) provide an overview of various tasks
for which MTurk has been used, and offer a set of
best practices for ensuring high-quality data.
Zaidan and Callison-Burch (2011a) studied the
quality of crowdsourced translations, by quantifying
the quality of non-professional English translations
of 2,000 Urdu sentences that were originally trans-
lated by the LDC. They demonstrated a variety of
mechanisms that increase the translation quality of
crowdsourced translations to near professional lev-
els, with a total cost that is less than one tenth the
cost of professional translation.
Zaidan and Callison-Burch (2011b) created the
Arabic Online Commentary (AOC) dataset, a 52M-
word monolingual dataset rich in dialectal content.
Over 100k sentences from the AOC were annotated
by native Arabic speakers on MTurk to identify the
dialect level (and dialect itself) in each, and the col-
lected labels were used to train automatic dialect
identification systems. Although a large number
of dialectal sentences were identified (41% of sen-
tences), none were passed on to a translation phase.
3 Data Collection and Annotation
Following Zaidan and Callison-Burch (2011a,b), we
use MTurk to identify Dialectal Arabic data and to
create a parallel corpus by hiring non-professional
translators to translate the sentences that were la-
beled as being dialectal. We had Turkers perform
three steps for us: dialect classification, sentence
segmentation, and translation.
Since Dialectal Arabic is much less common in
written form than in spoken form, the first challenge
is to simply find instances of written Dialectal Ara-
bic. We draw from a large corpus of monolingual
Arabic text (approximately 350M words) that was
harvested from the web by the LDC, largely from
weblog and online user groups.1 Before present-
ing our data to annotators, we filter it to identify
1Corpora: LDC2006E32, LDC2006E77, LDC2006E90,
LDC2007E04, LDC2007E44, LDC2007E102, LDC2008E41,
LDC2008E54, LDC2009E14, LDC2009E93.
50
M
ag
hr
eb
i
E
gy
Ir
aq
i
G
ul
f
Ot
he
r
L
ev
Figure 1: One possible breakdown of spoken Arabic into
dialect groups: Maghrebi, Egyptian, Levantine, Gulf and
Iraqi. Habash (2010) gives a breakdown along mostly
the same lines. We used this map as an illustration for
annotators in our dialect classification task (Section 3.1),
with Arabic names for the dialects instead of English.
segments most likely to be dialectal (unlike Zaidan
and Callison-Burch (2011b), who did no such pre-
filtering). We eliminate documents with a large per-
centage of non-Arabic or MSA words. We then
retain documents that contain some number of di-
alectal words, using a set of manually selected di-
alectal words that was assembled by culling through
the transcripts of the Levantine Fisher and Egyp-
tian CallHome speech corpora. After filtering, the
dataset contained around 4M words, which we used
as a starting point for creating our Dialectal Arabic-
English parallel corpus.
3.1 Dialect Classification
To refine the document set beyond our keyword fil-
tering heuristic and to label which dialect each doc-
ument is written in, we hire Arabic annotators on
MTurk to perform classification similar to Zaidan
and Callison-Burch (2011b). Annotators were asked
to classify the filtered documents for being in MSA
or in one of four regional dialects: Egyptian, Lev-
antine, Gulf/Iraqi or Maghrebi, and were shown the
map in Figure 1 to explain what regions each of the
dialect labels corresponded to. We allowed an addi-
tional ?General? dialect option for ambiguous docu-
ments. Unlike Zaidan and Callison-Burch, our clas-
sification was applied to whole documents (corre-
sponding to a user online posting) instead of individ-
ual sentences. To perform quality control, we used
a set of documents for which correct labels were
known. We presented these 20% of the time, and
Dialect Classification HIT $10,064
Sentence Segmentation HIT $1,940
Translation HIT $32,061
Total cost $44,065
Num words translated 1,516,856
Cost per word 2.9 cents/word
Table 1: The total costs for the three MTurk subtasks in-
volved with the creation of our Dialectal Arabic-English
parallel corpus.
eliminated workers who did not correctly classify
them (2% of labels).
Identifying the dialect of a text snippet can be
challenging in the absence of phonetic cues. We
therefore required 3 classifications from different
workers for every document, and accepted a dialect
label if at least two of them agreed. The dialect dis-
tribution of the final output was: 43% Gulf/Iraqi,
28% Levantine, 11% Egyptian, and 16% could not
be classified. MSA and the other labels accounted
for 2%. We decided to translate only the Levantine
and Egyptian documents, since the pool of MTurk
workers contained virtually no workers from Iraq or
the Gulf region.
3.2 Sentence Segmentation
Since the data we annotated was mostly user-
generated informal web content, the existing punc-
tuation was often insufficient to determine sentence
boundaries. Since sentence boundaries are impor-
tant for correct translation, we segmented passages
into individual sentences using MTurk. We only re-
quired sentences longer than 15 words to be seg-
mented, and allowed Turkers to split and rejoin at
any point between the tokens. The instructions were
simply to ?divide the Arabic text into individual sen-
tences, where you believe it would be appropriate
to insert a period.? We also used a set of correctly
segmented passages for quality control, and scored
Turkers using a metric based on the precision and
recall of correct segmentation points. The rejection
rate was 1.2%.
3.3 Translation to English
Following Zaidan and Callison-Burch (2011a), we
hired non-professional translators on MTurk to
translate the Levantine and Egyptian sentences into
51
Sentence Arabic English
Data Set Pairs Tokens Tokens
MSA-150MW 8.0M 151.4M 204.4M
Dialect-1500KW 180k 1,545,053 2,257,041
MSA-1300KW 71k 1,292,384 1,752,724
MSA-Web-Tune 6,163 145,260 184,185
MSA-Web-Test 5,454 136,396 172,357
Lev-Web-Tune 2,600 20,940 27,399
Lev-Web-Test 2,600 21,092 27,793
Egy-Web-Test 2,600 23,671 33,565
E-Facebook-Tune 3,351 25,130 34,753
E-Facebook-Test 3,188 25,011 34,244
Table 2: Statistics about the training/tuning/test datasets
used in our experiments. The token counts are calculated
before MADA segmentation.
English. Among several quality control measures,
we rendered the Arabic sentences as images to pre-
vent Turkers from simply copying the Arabic text
into translation software. We still spot checked the
translations against the output of Google Translate
and Bing Translator. We also rejected gobbledygook
garbage translations that have a high percentage of
words not found in an English lexicon.
We quantified the quality of an individual Turker?s
translations in two ways: first by asking native Ara-
bic speaker judges to score a sample of the Turker?s
translations, and second by inserting control sen-
tences for which we have good reference translations
and measuring the Turker?s METEOR (Banerjee and
Lavie, 2005) and BLEU-1 scores (Papineni et al,
2002).2 The rejection rate of translation assignments
was 5%. We promoted good translators to a re-
stricted access ?preferred worker queue?. They were
paid at a higher rate, and were required to translate
control passages only 10% of the time as opposed
to 20% for general Turkers, thus providing us with a
higher translation yield for unseen data.
Worker turnout was initially slow, but increased
quickly as our reputation for being reliable payers
was established; workers started translating larger
volumes and referring their acquaintances. We had
121 workers who each completed 20 or more trans-
lation assignments. We eventually reached and sus-
tained a rate of 200k words of acceptable quality
2BLEU-1 provided a more reliable correlation with human
judgment in this case that the regular BLEU score (which uses
n-gram orders 1, . . . , 4), given the limited size of the sample
measured.
translated per week. Unlike Zaidan and Callison-
Burch (2011a), who only translated 2,000 Urdu sen-
tences, we translated sufficient volumes of Dialectal
Arabic to train machine translation systems. In total,
we had 1.1M words of Levantine and 380k words of
Egyptian translated into English, corresponding to
about 2.3M words on the English side.
Table 1 outlines the costs involved with creating
our parallel corpus. The total cost was $44k, or
$0.03/word ? an order of magnitude cheaper than
professional translation.
4 Experiments in Dialectal Arabic-English
Machine Translation
We performed a set of experiments to contrast sys-
tems trained using our dialectal parallel corpus with
systems trained on a (much larger) MSA-English
parallel corpus. All experiments use the same meth-
ods for training, decoding and parameter tuning, and
we only varied the corpora used for training, tun-
ing and testing. The MT system we used is based
on a phrase-based hierarchical model similar to that
of Shen et al (2008). We used GIZA++ (Och and
Ney, 2003) to align sentences and extract hierar-
chical rules. The decoder used a log-linear model
that combines the scores of multiple feature scores,
including translation probabilities, smoothed lexi-
cal probabilities, a dependency tree language model,
in addition to a trigram English language model.
Additionally, we used 50,000 sparse, binary-valued
source and target features based on Chiang et al
(2009). The English language model was trained on
7 billion words from the Gigaword and from a web
crawl. The feature weights were tuned to maximize
the BLEU score on a tuning set using the Expected-
BLEU optimization procedure (Devlin, 2009).
The Dialectal Arabic side of our corpus consisted
of 1.5M words (1.1M Levantine and 380k Egyp-
tian). Table 2 gives statistics about the various
train/tune/test splits we used in our experiments.
Since the Egyptian set was so small, we split it only
to training/test sets, opting not to have a tuning set.
The MSA training data we used consisted of Arabic-
English corpora totaling 150M tokens (Arabic side).
The MSA train/tune/test sets were constructed for
the DARPA GALE program.
We report translation quality in terms of BLEU
52
Simple Segment MADA Segment
Training Tuning BLEU OOV BLEU OOV ?BLEU ?OOV
MSA-Web-Test
MSA-150MW MSA-Web 26.21 1.69% 27.85 0.48% +1.64 -1.21%
MSA-1300KW 21.24 7.20% 25.23 1.95% +3.99 -5.25%
Egyptian-Web-Test
Dialect-1500KW Levantine-Web 18.55 6.31% 20.66 2.85% +2.11 -3.46%
Levantine-Web-Test
Dialect-1500KW Levantine-Web 17.00 6.22% 19.29 2.96% +2.29 -3.26%
Table 3: Comparison of the effect of morphological segmentation when translating MSA web text and Dialectal
Arabic web text. The morphological segmentation uniformly improves translation quality, but the improvements are
more dramatic for MSA than for Dialectal Arabic when comparing similarly-sized training corpora.
Training Tuning BLEU OOV BLEU OOV BLEU OOV
Egyptian-Web-Test Levantine-Web-Test MSA-Web-Test
MSA-150MW MSA-Web 14.76 4.42% 11.83 5.53% 27.85 0.48%
MSA-150MW Lev-Web 14.34 4.42% 12.29 5.53% 24.63 0.48%
MSA-150MW+Dial-1500KW 20.09 2.04% 19.11 2.27% 24.30 0.45%
Dialect-1500KW 20.66 2.85% 19.29 2.96% 15.53 3.70%
Egyptian-360KW 19.04 4.62% 11.21 9.00% - -
Levantine-360KW 14.05 7.11% 16.36 5.24% - -
Levantine-1100KW 17.79 4.83% 19.29 3.31% - -
Table 4: A comparison of translation quality of Egyptian, Levantine, andMSAweb text, using various training corpora.
The highest BLEU scores are achieved using the full set of dialectal data (which combines Levantine and Egyptian),
since the Egyptian alone is sparse. For Levantine, adding Egyptian has no effect. In both cases, adding MSA to the
dialectal data results in marginally worse translations.
score.3 In addition, we also report the OOV rate of
the test set relative to the training corpus in each ex-
perimental setups.
4.1 Morphological Decomposition
Arabic has a complex morphology compared to En-
glish. Preprocessing the Arabic source by morpho-
logical segmentation has been shown to improve the
performance of Arabic MT (Lee, 2004; Habash and
Sadat, 2006) by decreasing the size of the source vo-
cabulary, and improving the quality of word align-
ments. The morphological analyzers that underlie
most segmenters were developed for MSA, but the
different dialects of Arabic share many of the mor-
phological affixes of MSA, and it is therefore not
unreasonable to expect MSA segmentation to also
improve Dialect Arabic to English MT. To test this,
3We also computed TER (Snover et al, 2006) andMETEOR
scores, but omit them because they demonstrated similar trends.
we ran experiments using the MADA morpholog-
ical analyzer (Habash and Rambow, 2005). Table
3 shows the effect of applying segmentation to the
text, for both MSA and Dialectal Arabic. The BLEU
score improves uniformly, although the improve-
ments are most dramatic for smaller datasets, which
is consistent with previous work (Habash and Sadat,
2006). Morphological segmentation gives a smaller
gain on dialectal input, which could be due to two
factors: the segmentation accuracy likely decreases
since we are using an unmodified MSA segmenter,
and there is higher variability in the written form of
dialect compared to MSA. Given the significant, al-
beit smaller gain on dialectal input, we use MADA
segmentation in all our experiments.
4.2 Effect of Dialectal Training Data Size
We next examine how the size of the dialectal train-
ing data affects MT performance, and whether it is
useful to combine it with MSA training data. We
53
oh
 ti
me
 (s
pa
ce
 om
itt
ed
). 
Ap
pe
are
d w
ith
in
 a 
po
em
.
11
yA
zm
n

?
lik
e y
ou
 (c
or
ru
pti
on
 of
 M
SA
 m
vl
k)
.
10
m
tlk
"#
$
by
 m
uc
h (
co
rru
pti
on
 of
 M
SA
 bk
vy
r).
11
bk
ty
r
&'$
()
I m
iss
 yo
u (
sp
ok
en
 to
 a 
fe
ma
le)
 ?
Eg
yp
tia
n.
14
w
H
$t
yn
y
/0
'$1
2?
Th
e l
as
t n
am
e (
Al
-N
a'o
om
) o
f a
 fo
ru
m 
ad
mi
n.
16
A
ln
E
w
m
?:;
0<?
a l
oo
ot 
(c
or
ru
pti
on
 of
 M
SA
 kv
yr
A
).
17
kt
yy
yr
&''
'$?
rea
lly
/fo
r r
ea
l ?
Le
va
nti
ne
.
31
E
nj
d
DE
0F
En
gli
sh
 E
qu
iva
len
t
Co
un
t
TL
Ar
ab
ic
Table 5: The most frequent OOV?s (with counts ? 10) of the dialectal test sets against the MSA training data.
Source (EGY):  ? ? ??	
?   ? ! !
Transliteration: Ant btEml lh AElAn wlA Ayh?!!
MSA-Sys. Output: You are working for a declarationand not?
Dial-Sys. Output: You are making the advertisementfor him or what?
Reference: Are you promoting it or what?!!
Source (EGY):  01?. ??78 6 35 34? ?
 9:;? <=>
Transliteration: nfsY Atm}n Elyh bEd mA $Af
AlSwrh dy
MSA-Sys. Output: Myself feel to see this image.
Dial-Sys. Output: I wish to check on him afterhe saw this picture.
Reference: I wish to be sure that he is fineafter he saw this images
Source (LEV):  ?0??? E7770 ?F? G7H
Transliteration: lhyk Aljw ktyyyr kwwwl
MSA-Sys. Output: God you the atmosphere.
Dial-Sys. Output: this is why the weather is so cool
Reference: This is why the weather is so cool
Source (LEV):  ?L M
 G3 0?;
Transliteration: Twl bAlk Em nmzH
MSA-Sys. Output: Do you think about a joke long.
Dial-Sys. Output: Calm down we are kidding
Reference: calm down, we are kidding
Figure 2: Examples of improvement in MT output when
training on our Dialectal Arabic-English parallel corpus
instead of an MSA-English parallel corpus.
Source (EGY):   	
 	  ? 
Transliteration: qAltlp Tb tEAlY nEd ,
MSA-Sys. Output: Medicine almighty promise.
Dial-Sys. Output: She said, OK, come and then
Reference: She told him, OK, lets count them ,
Source (LEV):  "#$%& 
#'01 ?-%. ! -,%+? ?? ?2 
Transliteration: fbqrA w>HyAnA bqDyhA Em
>tslY mE rfqAty
MSA-Sys. Output: I read and sometimes with gowith my uncle.
Dial-Sys. Output: So I read, and sometimes I spendtrying to make my self comfortwith my friends
Reference: So i study and sometimes I spendthe time having fun with my friends
Source (LEV):  ?@ ?< ??' => +? &#:9? B:C12D E?
?? %$?+G 
Transliteration: Allh ysAmHkn hlq kl wAHd TAlb
qrb bykwn bdw Erws
MSA-Sys. Output: God now each student near the
Bedouin bride.
Dial-Sys. Output: God forgive you, each one is aclose student would want the bride
Reference: God forgive you. Is every oneasking to be close, want a bride!
Figure 3: Examples of ambiguous words that are trans-
lated incorrectly by the MSA-English system, but cor-
rectly by the Dialectal Arabic-English system.
54
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
%
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Egyptian web test
!"
!#
!$
!%
"&
""
&' "&&' #&&' %&&' !(&&'
!
"#
$
!"#$%&'()*#"+"+,(-./0(/1(2/*345
)*+,-./0123
-./0123
Levantine web test
Figure 4: Learning curves showing the effects of increas-
ing the size of dialectal training data, when combined
with the 150M-word MSA parallel corpus, and when
used alone. Adding the MSA training data is only use-
ful when the dialectal data is scarce (200k words).
started with a baseline system trained on the 150M-
word MSA parallel corpus, and added various sized
portions of the dialect parallel corpus to it. Figure 4
shows the resulting learning curve, and compares it
to the learning curve for a system trained solely on
the dialectal parallel corpus. When only 200k words
of dialectal data are available, combining it with the
150M-word MSA corpus results in improved BLEU
scores, adding 0.8?1.5 BLEU points. When 400k
words or more of dialectal data are available, the
MSA training data ceases to provide any gain, and
in fact starts to hurt the performance.
The performance of a system trained on the 1.5M-
word dialectal data is dramatically superior to a sys-
tem that uses only the 150M-word MSA data: +6.32
BLEU points on the Egyptian test set, or 44% rela-
tive gain, and +7.00 BLEU points on the Levantine
test set, or 57% relative gain (fourth line vs. second
line of Table 4). In Section 4.4, we show that those
gains are not an artifact of the similarity between test
and training datasets, or of using the same translator
pool to translate both sets.
Inspecting the difference in the outputs of the Di-
alectal vs. MSA systems, we see that the improve-
ment in score is a reflection of a significant improve-
ment in the quality of translations. Figure 2 shows
a few examples of sentences whose translations im-
prove significantly using the Dialectal system. Fig-
ure 3 shows a particularly interesting category of ex-
amples. Many words are homographs, with different
meanings (and usually different pronunciations) in
MSA vs. one or more dialects. The bolded tokens
in the sentences in Figure 3 are examples of such
words. They are translated incorrectly by the MSA
system, while the dialect system translates them cor-
rectly.4 If we examine the most frequent OOVwords
against the MSA training data (Table 5), we find a
number of corrupted MSA words and names, but
that a majority of OOVs are dialect words.
4.3 Cross-Dialect Training
Since MSA training data appeared to have little ef-
fect when translating dialectal input, we next inves-
tigated the effect of training data from one dialect on
translating the input of another dialect. We trained a
system with the 360k-word Egyptian training subset
of our dialectal parallel corpus, and another system
with a similar amount of Levantine training data. We
used each system to translate the test set of the other
dialect. As expected, a system performs better when
it translates a test set in the same dialect that it was
trained on (Table 4).
That said, since the Egyptian training set is so
small, adding the (full) Levantine training data im-
proves performance (on the Egyptian test set) by
1.62 BLEU points, compared to using only Egyp-
tian training data. In fact, using the Levantine
training data by itself outperforms the MSA-trained
system on the Egyptian test set by more than 3
BLEU points. (For the Levantine test set, adding
the Egyptian training data has no affect, possibly
due to the small amount of Egyptian data.) This
may suggest that the mismatch between dialects is
less severe than the mismatch between MSA and
dialects. Alternatively, the differences may be due
to the changes in genre from the MSA parallel cor-
pus (which is mainly formal newswire) to the news-
groups and weblogs that mainly comprise the dialec-
tal corpus.
4The word nfsY of Figure 2 (first word of second example)
is also a homograph, as it means myself in MSA and I wish in
Dialectal Arabic.
55
Training Tuning BLEU OOV
MSA-150MW Levantine-Web 13.80 4.16%
MSA-150MW+Dialect-1500KW 16.71 2.43%
Dialect-1500KW 15.75 3.79%
MSA-150MW Egyptian-Facebook 15.80 4.16%
MSA-150MW+Dialect-1500KW 18.50 2.43%
Dialect-1500KW 17.90 3.79%
Dialect-1000KW (random selection) Egyptian-Facebook 17.09 4.64%
Dialect-1000KW (no Turker overlap) 17.10 4.60%
Table 6: Results on a truly independent test set, consisting of data harvested from Egyptian Facebook pages that are
entirely distinct from the our dialectal training set. The improvements over the MSA baseline are still considerable:
+2.9 BLEU points when no Facebook data is available for tuning and +2.7 with a Facebook tuning set.
4.4 Validation on Independent Test Data
To eliminate the possibility that the gains are solely
due to similarity between the test/training sets in the
dialectal data, we ran experiments using the same
dialectal training data, but using truly independent
test/tuning data sets selected at random from a larger
set of monolingual data that we collected from pub-
lic Egyptian Facebook pages. This data consists of
a set of original user postings and the subsequent
comments on each, giving the data a more conversa-
tional style than our other test sets. The postings
deal with current Egyptian political affairs, sports
and other topics. The test set we selected consisted
of 25,011 words (3,188 comments and 427 postings
from 86 pages), and the tuning set contained 25,130
words (3,351 comments and 415 conversations from
58 pages). We obtained reference translations for
those using MTurk as well.
Table 6 shows that using the 1.5M-word dialect
parallel corpus for training yields a 2 point BLEU
improvement over using the 150M-word MSA cor-
pus. Adding the MSA training data does yield an
improvement, though of less than a single BLEU
point. It remains true that training on 1.5M words
of dialectal data is better than training on 100 times
more MSA parallel data. The system performance
is sensitive to the tuning set choice, and improves
when it matches the test set in genre and origin.
To eliminate another potential source of artificial
bias, we also performed an experiment where we
removed any training translation contributed by a
Turker who translated any sentence in the Egyptian
Facebook set, to eliminate translator bias. For this,
we were left with 1M words of dialect training data.
This gave the same BLEU score as when training
with a randomly selected subset of the same size
(bottom part of Table 6).
4.5 Mapping from Dialectal Arabic to MSA
Before Translating to English
Given the large amount of linguistic resources that
have been developed for MSA over the past years,
and the extensive research that was conducted on
machine translation from MSA to English and other
languages, an obvious research question is whether
Dialectal Arabic is best translated to English by first
pivoting through MSA, rather than directly. The
proximity of Dialectal Arabic to MSA makes the
mapping in principle easier than general machine
translation, and a number of researchers have ex-
plored this direction (Salloum and Habash, 2011).
In this scenario, the dialectal source would first be
automatically transformed to MSA, using either a
rule-based or statistical mapping module.
The Dialectal Arabic-English parallel corpus we
created presents a unique opportunity to compare
the MSA-pivoting approach against direct transla-
tion. First, we collected equivalent MSA data for
the Levantine Web test and tuning sets, by asking
Turkers to transform dialectal passages to valid and
fluent MSA. Turkers were shown example transfor-
mations, and we encouraged fewer changes where
applicable (e.g. morphological rather than lexical
mapping), but allowed any editing operation in gen-
eral (deletion, substitution, reordering). Sample sub-
missions were independently shown to native Ara-
bic speaking judges, who confirmed they were valid
MSA. A lowOOV rate also indicated the correctness
of the mappings. By manually transforming the test
56
Training BLEU OOV BLEU OOV ?BLEU ?OOV
Direct dialect trans Map to MSA then trans
MSA-150MW 12.29 5.53% 14.59 1.53% +2.30 -4.00%
MSA-150MW+Dialect-200KW 15.37 3.59% 15.53 1.22% +0.16 -2.37%
MSA-150MW+Dialect-400KW 16.62 3.06% 16.25 1.13% -0.37 -1.93%
MSA-150MW+Dialect-800KW 17.83 2.63% 16.69 1.04% -1.14 -1.59%
MSA-150MW+Dialect-1500KW 19.11 2.27% 17.20 0.98% -1.91 -1.29%
Table 7: A comparison of the effectiveness of performing Levantine-to-MSA mapping before translating into English,
versus translating directly from Levantine into English. The mapping from Levantine to MSA was done manually, so it
is an optimistic estimate of what might be done automatically. Although initially helpful to the MSA baseline system,
the usefulness of pivoting through MSA drops as more dialectal data is added, eventually hurting performance.
dialectal sentence into MSA, we establish an opti-
mistic estimate of what could be done automatically.
Table 7 compares direct translation versus piv-
oting to MSA before translating, using the base-
line MSA-English MT system.5 The performance
of the system improves by 2.3 BLEU points with
dialect-to-MSA pivoting, compared to attempting to
translate the untransformed dialectal input directly.
As we add more dialectal training data, the BLEU
score when translating the untransformed dialect
test set improves rapidly (as seen previously in the
MSA+Dialect learning curve in Figure 4), while the
improvement is less rapid when the text is first trans-
formed to MSA. Direct translation becomes a better
option than mapping to MSA once 400k words of di-
alectal data are added, despite the significantly lower
OOV rate with MSA-mapping. This indicates that
simple vocabulary coverage is not sufficient, and
data domain mismatch, quantified by more complex
matching patterns, is more important.
5 Conclusion
We have described a process for building a Dialec-
tal Arabic-English parallel corpus, by selecting pas-
sages with a relatively high percentage of non-MSA
words from a monolingual Arabic web text corpus,
then using crowdsourcing to classify them by di-
alect, segment them into individual sentences and
translate them to English. The process was success-
fully scaled to the point of reaching and sustaining a
rate of 200k translated words per week, at 1/10 the
cost of professional translation. Our parallel corpus,
consisting of 1.5M words, was produced at a total
5The systems in each column of the table are tuned consis-
tently, using their corresponding tuning sets.
cost of $40k, or roughly $0.03/word.
We used the parallel corpus we constructed to
analyze the behavior of a Dialectal Arabic-English
MT system as a function of the size of the dialec-
tal training corpus. We showed that relatively small
amounts of training data render larger MSA corpora
from different data genres largely ineffective for this
test data. In practice, a system trained on the com-
bined Dialectal-MSA data is likely to give the best
performance, since informal Arabic data is usually
a mixture of Dialectal Arabic and MSA. An area of
future research is using the output of a dialect clas-
sifier, or other features to bias the translation model
towards the Dialectal or the MSA parts of the data.
We also validated the models built from the di-
alectal corpus by using them to translate an inde-
pendent data set collected from Egyptian Facebook
public pages. We finally investigated using MSA
as a ?pivot language? for Dialectal Arabic-English
translation, by simulating automatic dialect-to-MSA
mapping using MTurk. We obtained limited gains
from mapping the input to MSA, even when the
mapping is of good quality, and only at lower train-
ing set sizes. This suggests that the mismatch be-
tween training and test data is an important aspect of
the problem, beyond simple vocabulary coverage.
The aim of this paper is to contribute to setting
the direction of future research on Dialectal Arabic
MT. The gains we observed from using MSA mor-
phological segmentation can be further increased
with dialect-specific segmenters. Input preprocess-
ing can also be used to decrease the noise of the
user-generated data. Topic adaptation is another im-
portant problem to tackle if the large MSA linguistic
resources already developed are to be leveraged for
Dialectal Arabic-English MT.
57
Acknowledgments
This work was supported in part by DARPA/IPTO
Contract No. HR0011-12-C-0014 under the BOLT
Program, and in part by the EuroMatrixPlus project
funded by the European Commission (7th Frame-
work Programme). The views expressed are those
of the authors and do not reflect the official policy
or position of the Department of Defense or the U.S.
Government. Distribution Statement A (Approved
for Public Release, Distribution Unlimited).
References
Hitham M. Abo Bakr, Khaled Shaalan, and Ibrahim
Ziedan. 2008. A hybrid approach for converting writ-
ten Egyptian colloquial dialect into diacritized Arabic.
In The 6th International Conference on Informatics
and Systems, INFOS2008, Cairo, Egypt.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor:
An automatic metric for MT evaluation with improved
correlation with human judgments. In In Proc. of ACL
2005 Workshop on Intrinsic and Extrinsic Evaluation
Measures for MT and/or Summarization, Ann Arbor,
Michigan.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June.
David Chiang, Mona Diab, Nizar Habash, Owen Ram-
bow, and Safiullah Shareef. 2006. Parsing Arabic di-
alects. In Proceedings of the Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, Trento, Italy.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In NAACL ?09: Proceedings of the 2009 Human Lan-
guage Technology Conference of the North American
Chapter of the Association for Computational Linguis-
tics, Boulder, Colorado.
Jacob Devlin. 2009. Lexical features for statistical ma-
chine translation. Master?s thesis, University of Mary-
land, December.
Mohamed Embarki and Moha Ennaji, editors. 2011.
Modern Trends in Arabic Dialectology. The Red Sea
Press.
Charles A. Ferguson. 1959. Diglossia. Word, 15:325?
340.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
the 43th Annual Meeting of the Association for Com-
putational Linguistics (ACL), Ann Arbor, Michigan.
Nizar Habash and Owen Rambow. 2006. MAGEAD: A
morphological analyzer and generator for the Arabic
dialects. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics (ACL),
Sydney, Australia.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
Proceedings of the 2006 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics, New York,
New York.
Nizar Y. Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool.
Young-Suk Lee. 2004. Morphological analysis for
statistical machine translation. In HLT-NAACL ?04:
Proceedings of HLT-NAACL 2004, Boston, Mas-
sachusetts.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), Philadelphia, PA.
Jason Riesa and David Yarowsky. 2006. Minimally
supervised morphological segmentation with applica-
tions to machine translation. In Proceedings of the 7th
Conf. of the Association for Machine Translation in the
Americas (AMTA 2006), Cambridge, MA.
Wael Salloum and Nizar Habash. 2011. Dialectal to stan-
dard Arabic paraphrasing to improve Arabic-English
statistical machine translation. In Proceedings of the
2011 Conference of Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the 9th Conf. of
the Association for Machine Translation in the Ameri-
cas (AMTA 2010), Denver, Colorado.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
577?585, Columbus, Ohio.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
In Proceedings of the 7th Conf. of the Association for
Machine Translation in the Americas (AMTA 2006),
pages 223?231, Cambridge, MA.
58
Omar F. Zaidan and Chris Callison-Burch. 2011a.
The Arabic online commentary dataset: an annotated
dataset of informal Arabic with high dialectal content.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 37?41, Portland, Oregon,
June.
Omar F. Zaidan and Chris Callison-Burch. 2011b.
Crowdsourcing translation: Professional quality from
non-professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, June.
59
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1220?1229,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Crowdsourcing Translation: Professional Quality from Non-Professionals
Omar F. Zaidan and Chris Callison-Burch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,ccb}@cs.jhu.edu
Abstract
Naively collecting translations by crowd-
sourcing the task to non-professional trans-
lators yields disfluent, low-quality results if
no quality control is exercised. We demon-
strate a variety of mechanisms that increase
the translation quality to near professional lev-
els. Specifically, we solicit redundant transla-
tions and edits to them, and automatically se-
lect the best output among them. We propose a
set of features that model both the translations
and the translators, such as country of resi-
dence, LM perplexity of the translation, edit
rate from the other translations, and (option-
ally) calibration against professional transla-
tors. Using these features to score the col-
lected translations, we are able to discriminate
between acceptable and unacceptable transla-
tions. We recreate the NIST 2009 Urdu-to-
English evaluation set with Mechanical Turk,
and quantitatively show that our models are
able to select translations within the range of
quality that we expect from professional trans-
lators. The total cost is more than an order of
magnitude lower than professional translation.
1 Introduction
In natural language processing research, translations
are most often used in statistical machine translation
(SMT), where systems are trained using bilingual
sentence-aligned parallel corpora. SMT owes its ex-
istence to data like the Canadian Hansards (which by
law must be published in both French and English).
SMT can be applied to any language pair for which
there is sufficient data, and it has been shown to pro-
duce state-of-the-art results for language pairs like
Arabic?English, where there is ample data. How-
ever, large bilingual parallel corpora exist for rela-
tively few languages pairs.
There are various options for creating new train-
ing resources for new language pairs. These include
harvesting the web for translations or comparable
corpora (Resnik and Smith, 2003; Munteanu and
Marcu, 2005; Smith et al, 2010; Uszkoreit et al,
2010), improving SMT models so that they are bet-
ter suited to the low resource setting (Al-Onaizan
et al, 2002; Probst et al, 2002; Oard et al, 2003;
Niessen and Ney, 2004), or designing models that
are capable of learning translations from monolin-
gual corpora (Rapp, 1995; Fung and Yee, 1998;
Schafer and Yarowsky, 2002; Haghighi et al, 2008).
Relatively little consideration is given to the idea of
simply hiring translators to create parallel data, be-
cause it would seem to be prohibitively expensive.
For example, Germann (2001) estimated the cost
of hiring professional translators to create a Tamil-
English corpus at $0.36/word. At that rate, translat-
ing enough data to build even a small parallel corpus
like the LDC?s 1.5 million word Urdu?English cor-
pus would exceed half a million dollars.
In this paper we examine the idea of creating low
cost translations via crowdscouring. We use Ama-
zon?s Mechanical Turk to hire a large group of non-
professional translators, and have them recreate an
Urdu?English evaluation set at a fraction of the cost
of professional translators. The original dataset al
ready has professionally-produced reference trans-
lations, which allows us to objectively and quantita-
tively compare the quality of professional and non-
professional translations. Although many of the in-
dividual non-expert translators produce low-quality,
disfluent translations, we show that it is possible to
1220
Signs of human livings have been found in many caves 
in   Attapure. In 1994, the remains of pre-historic man, 
which are believed to be 800,000 years old were 
discovered and they were named `Home Antecessor' 
meaning `The Founding Man'. Prior to that 6 lac years 
old humans, named as   Homogenisens in scientific 
terms,were believed to be the   oldest dwellers of this 
area. Archaeological experts say that evidence is found 
that proves that the inhabitants of this area used 
molded tools. The ground where these digs took place 
has been claimed to be the oldest known European 
discovery of civilization, as announced by the French 
News Agency.
 !"#$"% &' ()*"+*, &-,./%, 0#1 234 5, 0#1 1994
 67"89: ;2< &="> &*"1 &*,?@ A"B C'D 8 E"FG8?H= )>
 ?I"+*, &*"%? &JK8 ?+#B &LJ8, )1)< 0#MJ> 0#NO &'
P"#O "8: Q"* "'
 &+J-"B 0#MJ> I"+*, 2*,?@ C'D 6 RG$ 2B 5,
 5, ;2< "="> "M' S+J#>?GTU#< )1)< 0#1 VW3X,
P2Y= 2="> 2*"1 &Z-"<9 [8?= \8.$ 2' 234
 2+8, 0#M*, ]' 2< "JM' "' [8?<"1 2' ]^8.$ _9"`a
 2' 234 5, ]' 2< "/bc ]/@ 2B [> 0#< 2b1 .<,)d
P2Y= 2=?' A"^K/B, &Y% 9,ef, 2-)< 2#' &-Wgh i)T
Signs of human life of ancient people have been 
discovered in several caves of Atapuerca. In 1994, 
several homo antecessor fossils i.e. pioneer human 
were uncovered in this region, which are supposed to 
be 800,000 years old. Previously, 600,000 years old 
ancestors, called homo hudlabar [sic] in scientific 
term, were supposed to be the most ancient 
inhabitants of the region.Archeologists are of the view 
that they have gathered evidence that the people of 
this region had also been using fabricated tools.
On the basis of the level at which this excavation was 
carried out, the French news agency [AFP] has termed 
it the oldest European discovery.
Urdu source Professional LDC Translation Non-Professional Mechanical Turk Translation
Figure 1: A comparison of professional translations provided by the LDC to non-professional translations created on
Mechanical Turk.
get high quality translations in aggregate by solicit-
ing multiple translations, redundantly editing them,
and then selecting the best of the bunch.
To select the best translation, we use a machine-
learning-inspired approach that assigns a score to
each translation we collect. The scores discrimi-
nate acceptable translations from those that are not
(and competent translators from those who are not).
The scoring is based on a set of informative, intu-
itive, and easy-to-compute features. These include
country of residence, number of years speaking En-
glish, LM perplexity of the translation, edit rate from
the other translations, and (optionally) calibration
against professional translators, with the weights set
using a small set of gold standard data from profes-
sional translators.
2 Crowdsourcing Translation to
Non-Professionals
To collect crowdsourced translations, we use Ama-
zon?s Mechanical Turk (MTurk), an online market-
place designed to pay people small sums of money
to complete Human Intelligence Tasks (or HITs) ?
tasks that are difficult for computers but easy for
people. Example HITs range from labeling images
to moderating blog comments to providing feedback
on relevance of results for search queries. Anyone
with an Amazon account can either submit HITs or
work on HITs that were submitted by others. Work-
ers are referred to as ?Turkers?, and designers of
HITs as ?Requesters.? A Requester specifies the re-
ward to be paid for each completed item, sometimes
as low as $0.01. Turkers are free to select whichever
HITs interest them, and to bypass HITs they find un-
interesting or which they deem pay too little.
The advantages of Mechanical Turk include:
? zero overhead for hiring workers
? a large, low-cost labor force
? easy micropayment system
? short turnaround time, as tasks get completed
in parallel by many individuals
? access to foreign markets with native speakers
of many rare languages
One downside is that Amazon does not provide
any personal information about Turkers. (Each
Turker is identifiable only through an anonymous
ID like A23KO2TP7I4KK2.) In particular, no in-
formation is available about a worker?s educational
background, skills, or even native language(s). This
makes it difficult to determine if a Turker is qualified
to complete a translation task.
Therefore, soliciting translations from anony-
mous non-professionals carries a significant risk of
poor translation quality. Whereas hiring a profes-
sional translator ensures a degree of quality and
care, it is not very difficult to find bad translations
provided by Turkers. One Urdu headline, profes-
sionally translated as Barack Obama: America Will
Adopt a New Iran Strategy, was rendered disfluently
by a Turker as Barak Obam will do a new policy
with Iran. Another translated it with snarky sar-
casm: Barak Obama and America weave new evil
strategies against Iran. Figure 1 gives more typical
translation examples. The translations often reflect
non-native English, but are generally done conscien-
tiously (in spite of the relatively small payment).
To improve the accuracy of noisy labels from non-
experts, most existing quality control mechanisms
1221
employ some form of voting, assuming a discrete
set of possible labels. This is not the case for trans-
lations, where the ?labels? are full sentences. When
dealing with such a structured output, the space of
possible outputs is diverse and complex. We there-
fore need a different approach for quality control.
That is precisely the focus of this work: to propose,
and evaluate, such quality control mechanisms.
In the next section, we discuss reproducing the
Urdu-to-English 2009 NIST evaluation set. We then
describe a principled approach to discriminate good
translations from bad ones, given a set of redundant
translations for the same source sentence.
3 Datasets
3.1 The Urdu-to-English 2009 NIST
Evaluation Set
We translated the Urdu side of the Urdu?English test
set of the 2009 NIST MT Evaluation Workshop. The
set consists of 1,792 Urdu sentences from a vari-
ety of news and online sources. The set includes
four different reference translations for each source
sentence, produced by professional translation agen-
cies. NIST contracted the LDC to oversee the trans-
lation process and perform quality control.
This particular dataset, with its multiple reference
translations, is very useful because we can measure
the quality range for professional translators, which
gives us an idea of whether or not the crowdsourced
translations approach the quality of a professional
translator.
3.2 Translation HIT design
We solicited English translations for the Urdu sen-
tences in the NIST dataset. Amazon has enabled
payments in rupees, which has attracted a large de-
mographic of workers from India (Ipeirotis, 2010).
Although it does not yet have s direct payment in
Pakistan?s local currency, we found that a large con-
tingent of our workers are located in Pakistan.
Our HIT involved showing the worker a sequence
of Urdu sentences, and asking them to provide an
English translation for each one. The screen also
included a brief set of instructions, and a short ques-
tionnaire section. The reward was set at $0.10 per
translation, or roughly $0.005 per word.
In our first collection effort, we solicited only one
translation per Urdu sentence. After confirming that
the task is feasible due to the large pool of work-
ers willing and able to provide translations, we car-
ried out a second collection effort, this time solicit-
ing three translations per Urdu sentence (from three
distinct translators). The interface was also slightly
modified, in the following ways:
? Instead of asking Turkers to translate a full doc-
ument (as in our first pass), we instead split the
data set into groups of 10 sentences per HIT.
? We converted the Urdu sentences into images
so that Turkers could not cheat by copying-and-
pasting the Urdu text into an MT system.
? We collected information about each worker?s
geographic location, using a JavaScript plugin.
The translations from the first pass were of notice-
ably low quality, most likely due to Turkers using
automatic translation systems. That is why we used
images instead of text in our second pass, which
yielded significant improvements. That said, we do
not discard the translations from the first pass, and
we do include them in our experiments.
3.3 Post-editing and Ranking HITs
In addition to collecting four translations per source
sentence, we also collected post-edited versions
of the translations, as well as ranking judgments
about their quality.
Figure 2 gives examples of the unedited transla-
tions that we collected in the translation pass. These
typically contain many simple mistakes like mis-
spellings, typos, and awkward word choice. We
posted another MTurk task where we asked workers
to edit the translations into more fluent and gram-
matical sentences. We restrict the task to US-based
workers to increase the likelihood that they would be
native English speakers.
We also asked US-based Turkers to rank the trans-
lations. We presented the translations in groups of
four, and the annotator?s task was to rank the sen-
tences by fluency, from best to worst (allowing ties).
We collected redundant annotations in these two
tasks as well. Each translation is edited three times
(by three distinct editors). We solicited only one edit
per translation from our first pass translation effort.
So, in total, we had 10 post-edited translations for
1222
Avoiding dieting to prevent 
from flu
abstention from dieting in 
order to avoid Flu
Abstain from decrease eating in 
order to escape from flue
In order to be safer from flu 
quit dieting
This research of American 
scientists came in front after 
experimenting on mice.
This research from the 
American Scientists have 
come up after the 
experiments on rats.
This research of American 
scientists was shown after 
many experiments on mouses.
According to the American 
Scientist this research has come 
out after much 
experimentations on rats.
Experiments proved that mice 
on a lower calorie diet had 
comparatively less ability to 
fight the flu virus.
in has been proven from 
experiments that rats put on 
diet with less calories had less 
ability to resist the Flu virus.
It was proved by experiments 
the low calories eaters 
mouses had low defending 
power for flue in ratio.
Experimentaions have proved 
that those rats on less calories 
diet have developed a tendency 
of not overcoming the flu virus.
research has proven this old 
myth wrong that its better to 
fast during fever.
Research disproved the old 
axiom that " It is better to 
fast during fever"
The research proved this old 
talk that decrease eating is 
useful in fever.
This Research has proved the 
very old saying wrong that it is 
good to starve while in fever.
Figure 2: We redundantly translate each source sentence by soliciting multiple translations from different Turkers.
These translations are put through a subsequent editing set, where multiple edited versions are produced. We select
the best translation from the set using features that predict the quality of each translation and each translator.
each source sentence (plus the four original transla-
tions). In the ranking task, we collected judgments
from five distinct workers for each translation group.
3.4 Data Collection Cost
We paid a reward of $0.10 to translate a sentence,
$0.25 to edit a set of ten sentences, and $0.06 to rank
a set of four translation groups. Therefore, we had
the following costs:
? Translation cost: $716.80
? Editing cost: $447.50
? Ranking cost: $134.40
(If not done redundantly, those values would be
$179.20, $44.75, and $26.88, respectively.)
Adding Amazon?s 10% fee, this brings the grand
total to under $1,500, spent to collect 7,000+ transla-
tions, 17,000+ edited translations, and 35,000+ rank
labels.1 We also use about 10% of the existing pro-
fessional references in most of our experiments (see
4.2 and 4.3). If we estimate the cost at $0.30/word,
that would roughly be an additional $1,000.
3.5 MTurk Participation
52 different Turkers took part in the translation task,
each translating 138 sentences on average. In the
editing task, 320 Turkers participated, averaging 56
sentences each. In the ranking task, 245 Turkers par-
ticipated, averaging 9.1 HITs each, or 146 rank la-
bels (since each ranking HIT involved judging 16
translations, in groups of four).
1Data URL: www.cs.jhu.edu/?ozaidan/RCLMT.
4 Quality Control Model
Our approach to building a translation set from
the available data is to select, for each Urdu sen-
tence, the one translation that our model believes
to be the best out of the available translations. We
evaluate various selection techniques by compar-
ing the selected Turker translations against existing
professionally-produced translations. The more the
selected translations resemble the professional trans-
lations, the higher the quality.
4.1 Features Used to Select Best Translations
Our model selects one of the 14 English options gen-
erated by Turkers. For a source sentence si, our
model assigns a score to each sentence in the set
of available translations {ti,1, ...ti,14}. The chosen
translation is the highest scoring translation:
tr(si) = tri,j? s.t. j
? = argmax
j
score(ti,j) (1)
where score(.) is the dot product:
score(ti,j)
def
= ~w ? ~f(ti,j) (2)
Here, ~w is the model?s weight vector (tuned as
described below in 4.2), and ~f is a translation?s cor-
responding feature vector. Each feature is a function
computed from the English sentence string, the Urdu
sentence string, the workers (translators, editors, and
rankers), and/or the rank labels. We use 21 features,
categorized into the following three sets.
1223
Sentence-level (6 features). Most of the Turk-
ers performing our task were native Urdu speakers
whose second language was English, and they do not
always produce natural-sounding English sentences.
Therefore, the first set of features attempt to discrim-
inate good English sentences from bad ones.
? Language model features: each sentence is
assigned a log probability and per-word per-
plexity score, using a 5-gram language model
trained on the English Gigaword corpus.
? Sentence length features: a good translation
tends to be comparable in length to the source
sentence, whereas an overly short or long trans-
lation is probably bad. We add two features that
are the ratios of the two lengths (one penalizes
short sentences and one penalizes long ones).
? Web n-gram match percentage: we assign a
score to each sentence based on the percentage
of the n-grams (up to length 5) in the transla-
tion that exist in the Google N-Gram Database.
? Web n-gram geometric average: we calculate
the average over the different n-gram match
percentages (similar to the way BLEU is com-
puted). We add three features corresponding to
max n-gram lengths of 3, 4, and 5.
? Edit rate to other translations: a bad translation
is likely not to be very similar to other transla-
tions, since there are many more ways a trans-
lation can be bad than for it to be good. So, we
compute the average edit rate distance from the
other translations (using the TER metric).
Worker-level (12 features). We add worker-level
features that evaluate a translation based on who pro-
vided it.
? Aggregate features: for each sentence-level
feature above, we have a corresponding feature
computed over all of that worker?s translations.
? Language abilities: we ask workers to provide
information about their language abilities. We
have a binary feature indicating whether Urdu
is their native language, and a feature for how
long they have spoken it. We add a pair of
equivalent features for English.
? Worker location: two binary features reflect a
worker?s location, one to indicate if they are lo-
cated in Pakistan, and one to indicate if they are
located in India.
Ranking (3 features). The third set of features is
based on the ranking labels we collected (see 3.3).
? Average rank: the average of the five rank la-
bels provided for this translation.
? Is-Best percentage: how often the translation
was top-ranked among the four translations.
? Is-Better percentage: how often the translation
was judged as the better translation, over all
pairwise comparisons extracted from the ranks.
Other features (not investigated here) could in-
clude source-target information, such as translation
model scores or the number of source words trans-
lated correctly according to a bilingual dictionary.
4.2 Parameter Tuning
Once features are computed for the sentences, we
must set the model?s weight vector ~w. Naturally, the
weights should be chosen so that good translations
get high scores, and bad translations get low scores.
We optimize translation quality against a small sub-
set (10%) of reference (professional) translations.
To tune the weight vector, we use the linear search
method of Och (2003), which is the basis of Min-
imum Error Rate Training (MERT). MERT is an
iterative algorithm used to tune parameters of an
MT system, which operates by iteratively generating
new candidate translations and adjusting the weights
to give good translations a high score, then regener-
ating new candidates based on the updated weights,
etc. In our work, the set of candidate translations is
fixed (the 14 English sentences for each source sen-
tence), and therefore iterating the procedure is not
applicable. We use the Z-MERT software package
(Zaidan, 2009) to perform the search.
4.3 The Worker Calibration Feature
Since we use a small portion of the reference trans-
lations to perform weight tuning, we can also use
that data to compute another worker-specific fea-
ture. Namely, we can evaluate the competency of
each worker by scoring their translations against the
reference translations. We then use that feature for
every translation given by that worker. The intuition
1224
is that workers known to produce good translations
are likely to continue to produce good translations,
and the opposite is likely true as well.
4.4 Evaluation Strategy
To measure the quality of the translations, we make
use of the existing professional translations. Since
we have four professional translation sets, we can
calculate the BLEU score (Papineni et al, 2002) for
one professional translator P1 using the other three
P2,3,4 as a reference set. We repeat the process four
times, scoring each professional translator against
the others, to calculate the expected range of profes-
sional quality translation. We can see how a trans-
lation set T (chosen by our model) compares to this
range by calculating T ?s BLEU scores against the
same four sets of three reference translations. We
will evaluate different strategies for selecting such
a set T , and see how much each improves on the
BLEU score, compared to randomly picking from
among the Turker translations.
We also evaluate Turker translation quality by us-
ing them as reference sets to score various submis-
sions to the NIST MT evaluation. Specifically, we
measure the correlation (using Pearson?s r) between
BLEU scores of MT systems measured against non-
professional translations, and BLEU scores mea-
sured against professional translations. Since the
main purpose of the NIST dataset was to compare
MT systems against each other, this is a more di-
rect fitness-for-task measure. We chose the middle 6
systems (in terms of performance) submitted to the
NIST evaluation, out of 12, as those systems were
fairly close to each other, with less than 2 BLEU
points separating them.2
5 Experimental Results
We establish the performance of professional trans-
lators, calculate oracle upper bounds on Turker
translation quality, and carry out a set of experiments
that demonstrate the effectiveness of our model and
that determine which features are most helpful.
Each number reported in this section is an average
of four numbers, corresponding to the four possible
2Using all 12 systems artificially inflates correlation, due to
the vast differences between the systems. For instance, the top
system outperforms the bottom system by 15 BLEU points!
ways of choosing 3 of the 4 reference sets. Further-
more, each of those 4 numbers is itself based on a
five-fold cross validation, where 80% of the data is
used to compute feature values, and 20% used for
evaluation. The 80% portion is used to compute the
aggregate worker-level features. For the worker cal-
ibration feature, we utilize the references for 10% of
the data (which is within the 80% portion).
5.1 Translation Quality: BLEU Scores
Compared to Professionals
We first evaluated the reference sets against each
other, in order to quantify the concept of ?profes-
sional quality?. On average, evaluating one refer-
ence set against the other three gives a BLEU score
of 42.38 (Figure 3). A Turker set of translations
scores 28.13 on average, which highlights the loss in
quality when collecting translations from amateurs.
To make the gap clearer, the output of a state-of-
the-art machine translation system (the syntax-based
variant of Joshua; Li et al (2010)) achieves a score
of 26.91, a mere 1.22 worse than the Turkers.
We perform two oracle experiments to determine
if there exist high-quality Turker translations in the
first place. The first oracle operates on the segment
level: for each source segment, choose from the four
translations the one that scores highest against the
reference sentence. The second oracle operates on
the worker level: for each source segment, choose
from the four translations the one provided by the
worker whose translations (over all sentences) score
the highest. The two oracles achieve BLEU scores
of 43.75 and 40.64, respectively ? well within the
range of professional translators.
We examined two voting-inspired methods, since
taking a majority vote usually works well when deal-
ing with MTurk data. The first selects the translation
with the minimum average TER (Snover et al, 2006)
against the other three translations, since that would
be a ?consensus? translation. The second method se-
lects the translation that received the best average
rank, using the rank labels assigned by other Turkers
(see 3.3). These approaches achieve BLEU scores of
34.41 and 36.64, respectively.
The main set of experiments evaluated the fea-
tures from 4.1 and 4.3. We applied our approach
using each of the four feature types: sentence fea-
tures, Turker features, rank features, and the cali-
1225
26.91 28.13 43.75 40.64 34.41 36.6442.38 34.95 35.79 37.14 37.82 39.06
20
25
30
35
40
45
Reference
(ave.)
Joshua
(syntax)
Turker
(ave.)
Oracle
(segment)
Oracle
(Turker)
Lowest
TER
Best
rank
Sentence
features
Turker
features
Rank
features
Calibration
feature
All
features
B
L
E
U
Figure 3: BLEU scores for different selection methods, measured against the reference sets. Each score is an average
of four BLEU scores, each calculated against three LDC reference translations. The five right-most bars are colored
in orange to indicate selection over a set that includes both original translations as well as edited versions of them.
bration feature. That yielded BLEU scores ranging
from 34.95 to 37.82. With all features combined, we
achieve a higher score of 39.06, which is within the
range of scores for the professional translators.
5.2 Fitness for a Task: Correlation With
Professionals When Ranking MT Systems
We evaluated the selection methods by measuring
correlation with the references, in terms of BLEU
scores assigned to outputs of MT systems. The re-
sults, in Table 1, tell a fairly similar story as eval-
uating with BLEU: references and oracles naturally
perform very well, and the loss in quality when se-
lecting arbitrary Turker translations is largely elimi-
nated using our selection strategy.
Interestingly, when using the Joshua output as
a reference set, the performance is quite abysmal.
Even though its BLEU score is comparable to the
Turker translations, it cannot be used to distinguish
closely matched MT systems from each other.3
6 Analysis
The oracles indicate that there is usually an accept-
able translation from the Turkers for any given sen-
tence. Since the oracles select from a small group of
only 4 translations per source segment, they are not
overly optimistic, and rather reflect the true potential
of the collected translations.
The results indicate that, although some features
are more useful than others, much of the benefit
from combining all the features can be obtained
from any one set of features, with the benefit of
3It should be noted that the Joshua system was not one of
the six MT systems we scored in the correlation experiments.
34.71 35.45 37.14 37.22 37.96
20
25
30
35
40
45
Sentence
features
Turker
features
Rank
features
Calibration
feature
All
features
B
L
E
U
Figure 4: BLEU scores for the five right-most setups from
Figure 3, constrained over the original translations.
adding more features being somewhat orthogonal.
Finally, we performed a series of experiments ex-
ploring the calibration feature, varying the amount
of gold-standard references from 10% all the way up
to 80%. As expected, the performance improved as
more references were used to calibrate the transla-
tors (Figure 5). What?s particularly important about
this experiment is that it shows the added benefit
of the other features: We would have to use 30%?
40% of the references to get the same benefit ob-
tained from combining the non-calibration features
and only 10% for the calibration feature (dashed line
in the Figure; BLEU = 39.06).
6.1 Cost Reduction
While the combined cost of our data collection ef-
fort ($2,500; see 3.4) is quite low considering the
amount of collected data, it would be more attractive
if the cost could be reduced further without losing
much in translation quality. To that end, we inves-
tigated lowering cost along two dimensions: elimi-
nating the need for professional translations, and de-
creasing the amount of edited translations.
1226
Selection Method Pearson?s r2
Reference (ave.) 0.81 ? 0.07
Joshua (syntax) 0.08 ? 0.09
Turker (ave.) 0.60 ? 0.17
Oracle (segment) 0.81 ? 0.09
Oracle (Turker) 0.79 ? 0.10
Lowest TER 0.50 ? 0.26
Best rank 0.74 ? 0.17
Sentence features 0.56 ? 0.21
Turker features 0.59 ? 0.19
Rank features 0.75 ? 0.14
Calibration feature 0.76 ? 0.13
All features 0.77 ? 0.11
Table 1: Correlation (? std. dev.) for different selection
methods, compared against the reference sets.
The professional translations are used in our ap-
proach for computing the worker calibration feature
(subsection 4.3) and for tuning the weights of the
other features. We use a relatively small amount
for this purpose, but we investigate a different setup
whereby no professional translations are used at all.
This eliminates the worker calibration feature, but,
perhaps more critically, the feature weights must be
set in a different fashion, since we cannot optimize
BLEU on reference data anymore. Instead, we use
the rank labels (from 3.3) as a proxy for BLEU, and
set the weights so that better ranked translations re-
ceive higher scores.
Note that the rank features will also be excluded
in this setup, since they are perfect predictors of rank
labels. On the one hand, this means no rank labels
need to be collected, other than for a small set used
for weight tuning, further reducing the cost of data
collection. However, this leads to a significant drop
in performance, yielding a BLEU score of 34.86.
Another alternative for cost reduction would be to
reduce the number of collected edited translations.
To that end, we first investigate completely eliminat-
ing the editing phase, and considering only unedited
translations. In other words, the selection will be
over a group of four English sentences rather than
14 sentences. Completely eliminating the edited
translations has an adverse effect, as expected (Fig-
ure 4). Another option, rather than eliminating the
editing phase altogether, would be to consider the
edited translations of only the translation receiving
37.0
37.5
38.0
38.5
39.0
39.5
40.0
40.5
0 20 40 60 80 100
% References Used for Calibration
B
L
E
U
 10%+other features
(i.e. "All features"
from Figure 3)
Figure 5: The effect of varying the amount of calibra-
tion data (and using only the calibration feature). The
10% point (BLEU = 37.82) and the dashed line (BLEU =
39.06) correspond to the two right-most bars of Figure 3.
the best rank labels. This would reflect a data col-
lection process whereby the editing task is delayed
until after the rank labels are collected, with the rank
labels used to determine which translations are most
promising to post-edit (in addition to using the rank
labels for the ranking features). Using this approach
enables us to greatly reduce the number of edited
translations collected, while maintaining good per-
formance, obtaining a BLEU score of 38.67.
It is therefore our recommendation that crowd-
sourced translation efforts adhere to the follow-
ing pipeline: collect multiple translations for each
source sentence, collect rank labels for the transla-
tions, and finally collect edited versions of the top
ranked translations.
7 Related Work
Dawid and Skene (1979) investigated filtering
annotations using the EM algorithm, estimating
annotator-specific error rates in the context of patient
medical records. Snow et al (2008) were among the
first to use MTurk to obtain data for several NLP
tasks, such as textual entailment and word sense dis-
ambiguation. Their approach, based on majority
voting, had a component for annotator bias correc-
tion. They showed that for such tasks, a few non-
expert labels usually suffice.
Whitehill et al (2009) proposed a probabilistic
model to filter labels from non-experts, in the con-
text of an image labeling task. Their system genera-
tively models image difficulty, as well as noisy, even
1227
adversarial, annotators. They apply their method to
simulated labels rather than real-life labels.
Callison-Burch (2009) proposed several ways to
evaluate MT output on MTurk. One such method
was to collect reference translations to score MT
output. It was only a pilot study (50 sentences in
each of several languages), but it showed the pos-
sibility of obtaining high-quality translations from
non-professionals. As a followup, Bloodgood and
Callison-Burch (2010) solicited a single translation
of the NIST Urdu-to-English dataset we used. Their
evaluation was similar to our correlation experi-
ments, examining how well the collected transla-
tions agreed with the professional translations when
evaluating three MT systems.
That paper appeared in a NAACL 2010 workshop
organized by Callison-Burch and Dredze (2010), fo-
cusing on MTurk as a source of data for speech and
language tasks. Two relevant papers from that work-
shop were by Ambati and Vogel (2010), focusing on
the design of the translation HIT, and by Irvine and
Klementiev (2010), who created translation lexicons
between English and 42 rare languages.
Resnik et al (2010) explore a very interesting
way of creating translations on MTurk, relying only
on monolingual speakers. Speakers of the target
language iteratively identified problems in machine
translation output, and speakers of the source lan-
guage paraphrased the corresponding source por-
tion. The paraphrased source would then be re-
translated to produce a different translation, hope-
fully more coherent than the original.
8 Conclusion and Future Work
We have demonstrated that it is possible to ob-
tain high-quality translations from non-professional
translators, and that the cost is an order of magni-
tude cheaper than professional translation. We be-
lieve that crowdsourcing can play a pivotal role in
future efforts to create parallel translation datasets.
Beyond the cost and scalability, crowdsourcing pro-
vides access to languages that currently fall outside
the scope of statistical machine translation research.
We have begun an ongoing effort to collect transla-
tions for several low resource languages, including
Tamil, Yoruba, and dialectal Arabic. We plan to:
? Investigate improvements from system combi-
nation techniques to the redundant translations.
? Modify our editing step to collect an annotated
corpus of English as a second language errors.
? Calibrate against good Turkers, instead of pro-
fessionals, once they have been identified.
? Predict whether it is necessary to solicit another
translation instead of collecting a fixed number.
? Analyze how much quality matters if our goal
is to train a statistical translation system.
Acknowledgments
This research was supported by the Human Lan-
guage Technology Center of Excellence, by gifts
from Google and Microsoft, and by the DARPA
GALE program under Contract No. HR0011-06-2-
0001. The views and findings are the authors? alone.
We would like to thank Ben Bederson, Philip
Resnik, and Alain De?silets for organizing work-
shops focused on crowdsourcing translation (Bed-
erson and Resnik, 2010; De?silets, 2010). We are
grateful for the feedback of workshop participants,
which helped shape this research.
References
Yaser Al-Onaizan, Ulrich Germann, Ulf Hermjakob,
Kevin Knight, Philipp Koehn, Daniel Marcu, and
Kenji Yamada. 2002. Translation with scarce bilin-
gual resources. Machine Translation, 17(1), March.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT Workshop on Cre-
ating Speech and Language Data With Amazon?s Me-
chanical Turk, pages 62?65.
Ben Bederson and Philip Resnik. 2010. Workshop on
crowdsourcing and translation. http://www.cs.
umd.edu/hcil/monotrans/workshop/.
Michael Bloodgood and Chris Callison-Burch. 2010.
Using Mechanical Turk to build machine translation
evaluation sets. In Proceedings of the NAACL HLT
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk, pages 208?211.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT Workshop on
Creating Speech and Language Data With Amazon?s
Mechanical Turk, pages 1?12.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
1228
chanical Turk. In Proceedings of EMNLP, pages 286?
295.
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the EM
algorithm. Applied Statistics, 28(1):20?28.
Alain De?silets. 2010. AMTA 2010 workshop on collabo-
rative translation: technology, crowdsourcing, and the
translator perspective. http://bit.ly/gPnqR2.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach for
translating new words from nonparallel, comparable
texts. In Proceedings of ACL/CoLing.
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexi-
cons from monolingual corpora. In Proceedings of
ACL/HLT.
Panos Ipeirotis. 2010. New demographics of Mechanical
Turk. http://behind-the-enemy-lines.
blogspot.com/2010/03/
new-demographics-of-mechanical-turk.
html.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Proceedings of the NAACL HLT
Workshop on Creating Speech and Language Data
With Amazon?s Mechanical Turk, pages 108?113.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, pages
133?137.
Dragos Munteanu and Daniel Marcu. 2005. Improving
machine translation performance by exploiting compa-
rable corpora. Computational Linguistics, 31(4):477?
504, December.
Sonja Niessen and Hermann Ney. 2004. Statisti-
cal machine translation with scarce resources using
morpho-syntatic analysis. Computational Linguistics,
30(2):181?204.
Doug Oard, David Doermann, Bonnie Dorr, Daqing He,
Phillip Resnik, William Byrne, Sanjeeve Khudanpur,
David Yarowsky, Anton Leuski, Philipp Koehn, and
Kevin Knight. 2003. Desperately seeking Cebuano.
In Proceedings of HLT/NAACL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL,
pages 160?167.
Kishore Papineni, Salim Poukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL,
pages 311?318.
Katharina Probst, Lori Levin, Erik Peterson, Alon Lavie,
and Jamie Carbonell. 2002. MT for minority lan-
guages using elicitation-based learning of syntactic
transfer rules. Machine Translation, 17(4).
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of ACL.
Philip Resnik and Noah Smith. 2003. The web as a par-
allel corpus. Computational Linguistics, 29(3):349?
380, September.
Philip Resnik, Olivia Buzek, Chang Hu, Yakov Kronrod,
Alex Quinn, and Benjamin Bederson. 2010. Improv-
ing translation via targeted paraphrasing. In Proceed-
ings of EMNLP, pages 127?137.
Charles Schafer and David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity measures
and bridge languages. In Conference on Natural Lan-
guage Learning-2002, pages 146?152.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from comparable
corpora using document level alignment. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 403?411, Los An-
geles, California, June. Association for Computational
Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Translation
in the Americas (AMTA).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast ? but is it
good? Evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of EMNLP, pages
254?263.
Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, and
Moshe Dubiner. 2010. Large scale parallel document
mining for machine translation. In Proc. of the In-
ternational Conference on Computational Linguistics
(COLING).
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. In Proceedings of
NIPS, pages 2035?2043.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
1229
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 37?41,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
The Arabic Online Commentary Dataset:
an Annotated Dataset of Informal Arabic with High Dialectal Content
Omar F. Zaidan and Chris Callison-Burch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,ccb}@cs.jhu.edu
Abstract
The written form of Arabic, Modern Standard
Arabic (MSA), differs quite a bit from the
spoken dialects of Arabic, which are the true
?native? languages of Arabic speakers used in
daily life. However, due to MSA?s prevalence
in written form, almost all Arabic datasets
have predominantly MSA content. We present
the Arabic Online Commentary Dataset, a
52M-word monolingual dataset rich in dialec-
tal content, and we describe our long-term an-
notation effort to identify the dialect level (and
dialect itself) in each sentence of the dataset.
So far, we have labeled 108K sentences, 41%
of which as having dialectal content. We also
present experimental results on the task of au-
tomatic dialect identification, using the col-
lected labels for training and evaluation.
1 Introduction
The Arabic language is characterized by an interest-
ing linguistic dichotomy, whereby the written form
of the language, Modern Standard Arabic (MSA),
differs in a non-trivial fashion from the various spo-
ken varieties of Arabic. As the variant of choice for
written and official communication, MSA content
significantly dominates dialectal content, and in turn
MSA dominates in datasets available for linguistic
research, especially in textual form.
The abundance of MSA data has greatly aided re-
search on computational methods applied to Arabic,
but only the MSA variant of it. A state-of-the-art
Arabic-to-English machine translation system per-
forms quite well when translating MSA source sen-
tences, but often produces incomprehensible output
when the input is dialectal. For example, most words
Src
 (M
SA
):   
     
     
     
    

 
?

 ? 
?  
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 93?98,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Enriched MT Grammar for Under $100
Omar F. Zaidan and Juri Ganitkevitch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,juri}@cs.jhu.edu
Abstract
We propose a framework for improving out-
put quality of machine translation systems, by
operating on the level of grammar rule fea-
tures. Our framework aims to give a boost to
grammar rules that appear in the derivations
of translation candidates that are deemed to
be of good quality, hence making those rules
more preferable by the system. To that end, we
ask human annotators on Amazon Mechanical
Turk to compare translation candidates, and
then interpret their preferences of one candi-
date over another as an implicit preference for
one derivation over another, and therefore as
an implicit preference for one or more gram-
mar rules. Our framework also allows us to
generalize these preferences to grammar rules
corresponding to a previously unseen test set,
namely rules for which no candidates have
been judged.
1 Introduction
When translating between two languages, state-
of-the-art statistical machine translation sys-
tems (Koehn et al, 2007; Li et al, 2009) generate
candidate translations by relying on a set of relevant
grammar (or phrase table) entries. Each of those
entries, or rules, associates a string in the source
language with a string in the target language, with
these associations typically learned by examining
a large parallel bitext. By the very nature of the
translation process, a target side sentence e can
be a candidate translation for a source sentence f
only if e can be constructed using a small subset
of the grammar, namely the subset of rules with
source side sequences relevant to the word sequence
of f . However, even this limited set of candidates
(call it E(f)) is quite large, with |E(f)| growing
exponentially in the length of f . The system is able
to rank the translations within E(f) by assigning a
score s(e) to each candidate translation. This score
is the dot product:
s(e) = ~?(e) ? ~w (1)
where ~?(e) is a feature vector characterizing e, and
~w is a system-specific weight vector characterizing
the system?s belief of how much the different fea-
tures reflect translation quality. The features of a
candidate e are computed by examining the way e is
constructed (or derived), and so if we let d(e) be the
derivation of e, the feature vector can be denoted:1
~?(d(e)) = ??1(d(e)), . . . , ?m(d(e))? (2)
where ?i(d(e)) is the value of ith feature function
of d(e) (with a corresponding weight wi in ~w).
To compute the score for a candidate, we examine
its derivation d(e), enumerating the grammar rules
used to construct e: d(e) = (r1, . . . , rk). Typically,
each of the rules will itself have a vector of m fea-
tures, and we calculate the value of a derivation fea-
ture ?i(d(e)) as the sum of the ith feature over all
rules in the derivation:
?i(d(e)) =
?
r?d(e)
?i(r) (3)
1There are other features computed directly, without ex-
amining the derivation (e.g. candidate length, language model
score), but we omit these features from the motivation discus-
sion for clarity.
93
These features are usually either relative frequen-
cies estimated from the training corpus, relating the
rule?s source and target sides, or features that char-
acterize the structure of the rule itself, independently
from the corpus.
Either way, the weightwi is chosen so as to reflect
some belief regarding the correlation between the ith
feature and translation quality. This is usually done
by choosing weights that maximize performance on
a tuning set separate from the training bitext. Un-
like system weights, the grammar rule feature val-
ues are fixed once extracted, and are not modified
during this tuning phase. In this paper, we propose a
framework to augment the feature set to incorporate
additional intuition about how likely a rule is to pro-
duce a translation preferred by a human annotator.
This knowledge is acquired by directly asking hu-
man judges to compare candidate translations, there-
fore determining which subset of grammar rules an-
notators seem to prefer over others. We also seek to
generalize this intuition to rules for which no can-
didates were judged, hence allowing us to impact a
much larger set of rules than just those used in trans-
lating the tuning set.
The paper is organized as follows. We first give
a general description of our framework. We then
discuss our data collection efforts on Amazon Me-
chanical Turk for an Urdu-English translation task,
and make explicit the type of judgments we col-
lect and how they can be used to augment grammar
rules. Before concluding, we propose a framework
for generalizing judgments to unseen grammar rules,
and analyze the data collection process.
2 The General Framework
As initially mentioned, when tuning a SMT system
on a development set, we typically only perform
high-level optimization of the system weights. In
this section we outline an approach that could allow
for lower-level optimization, on the level of individ-
ual grammar rules.
We kick off the process by soliciting judgments
from human annotators regarding the quality of a
subset of candidates (the following section outlines
how candidates are chosen). The resulting judg-
ments on sentences are interpreted to be judgments
on individual grammar rules used in the derivations
of these candidates. And so, if an annotator declares
a candidate to be of high quality, this is considered
a vote of confidence on the individual rules giving
rise to this candidate, and if an annotator declares a
candidate to be of lowl quality, this is considered a
vote of no confidence on the individual rules.
To make use of the collected judgments, we ex-
tend the set of features used in the decoder by a new
feature ?:
~?? = ??1, . . . , ?m, ?? (4)
This feature is the cornerstone of our framework,
as it will hold the quantified and cumulated judg-
ments for each rule, and will be used by the system
at decoding time, in addition to the existing m fea-
tures, incorporating the annotators? judgments into
the translation process.2
The range of possible values for this feature, and
how the feature is computed, depends on how one
chooses to ask annotators to score candidates, and
what form those judgments assume (i.e. are those
judgments scores on a scale? Are they ?better?
vs. ?worse? judgments, and if so, compared to how
many other possibilities?). At this point, we will
only emphasize that the value of ? should reflect
the annotators? preference for the rule, and that it
should be computed from the collected judgments.
We will propose one such method of computing ? in
Section 4, after describing the type of judgments we
collected.
3 Data Collection
We apply our approach to an Urdu-to-English trans-
lation task. We used a syntactically rich SAMT
grammar (Venugopal and Zollmann, 2006), where
each rule in the grammar is characterized by 12 fea-
tures. The grammar was provided by Chris Callison-
Burch (personal communication), and was extracted
from a parallel corpus of 88k sentence pairs.3 One
system using this grammar produced significantly
improved output over submissions to the NIST 2009
Urdu-English task (Baker et al, 2009).
We use the Joshua system (Li et al, 2009)
as a decoder, with system weights tuned using
2In fact, the collected judgments can only cover a small por-
tion of the grammar. We address this coverage problem in Sec-
tion 4.
3LDC catalog number LDC2009E12.
94
Z-MERT (Zaidan, 2009) on a tuning set of 981 sen-
tences, a subset of the 2008 NIST Urdu-English test
set.4 We choose candidates to be judged from the
300-best candidate lists.5
Asking a worker to make a quantitative judgment
of the quality of a particular candidate translation
(e.g. on a 1?7 scale) is a highly subjective and
annotator-dependent process. Instead, we present
workers with pairs of candidates, and ask them to
judge which candidate is of better quality.
How are candidate pairs chosen? We would like
a judgment to have the maximum potential for be-
ing informative about specific grammar rules. In es-
sense, we prefer a pair of candidates if they have
highly similar derivations, yet differ noticeably in
terms of how the decoder ranks them. In other
words, if a relatively minimal change in derivation
causes a relatively large difference in the score as-
signed by the decoder, we are likely to attribute the
difference to very few rule comparisons (or perhaps
only one), hence focusing the comparison on indi-
vidual rules, all the while shielding the annotators
from having to compare grammar rules directly.
Specifically, each pair of candidates (e, e?) is as-
signed a potential score pi(e, e?), defined as:
pi(e, e?) =
s(e)s(e?)
lev(d(e),d(e?))
, (5)
where s(e) is the score assigned by the decoder,
and lev(d,d?) is a distance measure between two
derivations which we will now descibe in more de-
tail. In Joshua, the derivation of a candidate is cap-
tured fully and exactly by a derivation tree, and so
we define lev(d,d?) as a tree distance metric as fol-
lows. We first represent the trees as strings, using the
familiar nested string representation, then compute
the word-based Levenshtein edit distance between
the two strings. An edit has a cost of 1 in general, but
we assign a cost of zero to edit operations on termi-
nals, since we want to focus on the structure of the
derivation trees, rather than on terminal-level lexi-
cal choices.6 Furthermore, we ignore differences in
4LDC catalog number LDC2009E11.
5We exclude source sentences shorter than 4 words long or
that have fewer than 4 candidate translations. This eliminates
roughly 6% of the development set.
6This is not to say that lexical choices are not important, but
lexical choice is heavily influenced by context, which is not cap-
?pure? pre-terminal rules, that only have terminals
as their right-hand side. These decisions effectively
allow us to focus our efforts on grammar rules with
at least one nonterminal in their right-hand side.
We perform the above potential computation on
all pairs formed by the cross product of the top 10
candidates and the top 300 candidates, and choose
the top five pairs ranked by potential.
Our HIT template is rather simple. Each HIT
screen corresponds to a single source sentence,
which is shown to the worker along with the five
chosen candidate pairs. To aid workers who are not
fluent in Urdu7 better judge translation quality, the
HIT also displays one of the available references
for that source sentence. To eliminate potential bias
associated with the order in which candidates are
presented (an annotator might be biased to choos-
ing the first presented candidate, for example), we
present the two candidates in random or- der. Fur-
thermore, for quality assurance, we embed a sixth
candidate pair to be judged, where we pair up a ran-
domly chosen candidate with another reference for
that sentence.8 Presumably, a faithful worker would
be unlikely to prefer a random candidate over the
reference, and so this functions as an embedded self-
verification test. The order of this test, relative to the
five original pairs, is chosen randomly.
4 Incorporating the Judgements
4.1 Judgement Quantification
The judgments we obtain from the procedure de-
scribed in the previous section relate pairs of can-
didate translations. However, we have defined the
accumulation feature ? as a feature for each rule.
Thus, in order to compute ?, we need to project the
judgments onto the rules that tell the two candidates
apart. A simple way to do this is the following: for
a judged candidate pair (e, e?) let U(e) be the set of
tured well by grammar rules. Furthermore, lexical choice is a
phenomenon already well captured by the score assigned to the
candidate by the language model, a feature typically included
when designing ~?.
7We exclude workers from India and restrict the task to
workers with an existing approval rating of 90% or higher.
8The tuning set contains at least three different human refer-
ences for each source sentence, and so the reference ?candidate?
shown to the worker is not the same as the sentence already
identified as a reference.
95
rules that appear in d(e) but not in d(e?), and vice
versa.9 We will assume that the jugdment obtained
for (e, e?) applies for every rule pair in the cartesian
product of U(e) and U(e?). This expansion yields
a set of judged grammar rule pairs J = {(a, b)}
with associated vote counts va>b and vb>a, captur-
ing how often the annotators preferred a candidate
that was set apart by a over a candidate containing
b, and vice versa.
So, following our prior definiton as an expression
of the judges? preference, we can calculate the value
of ? for a rule r as the relative frequency of favorable
judgements:
?(r) =
?
(r,b)?J vr>b
?
(r,b)?J vb>r + vr>b
(6)
4.2 Generalization to Unseen Rules
This approach has a substantial problem: ?, com-
puted as given above, is undefined for a rule that
was never judged (i.e. a rule that never set apart
a pair of candidates presented to the annotators).
Furthermore, as described, the coverage of the col-
lected judgments will be limited to a small subset
of the entire grammar, meaning that when the sys-
tem is asked to translate a new source sentence, it
is highly unlikely that the relevant grammar rules
would have already been judged by an annotator.
Therefore, it is necessary to generalize the collected
judgments/votes and propagate them to previously
unexamined rules.
In order to do this, we propose the following gen-
eral approach: when observing a judgment for a pair
of rules (a, b) ? J , we view that judgement not as
a vote on one of them specifically, but rather as a
comparison of rules similar to a versus rules similar
to b. When calculating ?(r) for any rule r we use
a distance measure over rules, ?, to estimate how
each judgment in J projects to r. This leads to the
following modified computatio of ?(r):
?(r) =
?
(a,b)?J
?(a, r)v?b>a + ?(b, r)v
?
a>b
?(a, r) + ?(b, r)
(7)
9The way we select candidate pairs ensures that U(e) and
U(e?) are both small and expressive in terms of impact on the
decoder ranking. On our data U(e) contained an average of 4
rules.
where v?a>b (and analogously v
?
b>a) is defined as the
relative frequency of a being preferred over b:
v?a>b =
va>b
va>b + vb>a
4.3 A Vector Space Realization
Having presented a general framework for judgment
generalization, we will now briefly sketch a concrete
realization of this approach.
In order to be able to use the common distance
metrics on rules, we define a rule vector space. The
basis of this space will be a new set of rule features
designed specifically for the purpose of describing
the structure of a rule, ~? = ??1, . . . , ?k?. Provided
the exact features chosen are expressive and well-
distributed over the grammar, we expect any con-
ventional distance metric to correlate with rule sim-
ilarity.
We deem a particular ?i good if it quantifies a
quality of the rule that describes the rule?s nature
rather than the particular lexical choices it makes,
i.e. a statistic (such as the rule length, arity, number
of lexical items in the target or source side or the av-
erage covered span in the training corpus), informa-
tion relevant to the rule?s effect on a derivation (such
as nonterminals occuring in the rule and wheter they
are re-ordered) or features that capture frequent lex-
ical cues that carry syntactic information (such as
the co-occurrence of function words in source and
target language, possibly in conjunction with certain
nonterminal types).
5 Results and Analysis
The judgments were collected over a period of
about 12 days (Figure 1). A total of 16,374 labels
were provided (2,729 embedded test labels + 13,645
?true? labels) by 658 distinct workers over 83.1 hours
(i.e. each worker completed an average of 4.2 HITs
over 7.6 minutes). The reward for each HIT was
$0.02, with an additional $0.005 incurred for Ama-
zon Fees. Since each HIT provides five labels, we
obtain 200 (true) labels on the dollar. Each HIT
took an average of 1.83 minutes to complete, for a
labeling rate of 164 true labels/hour, and an effec-
tive wage of $0.66/hour. The low reward does not
seem to have deterred Turkers from completing our
HITs faithfully, as the success rate on the embedded
96
 0 10 20 30 40 50 60 70 80 90 100
 1  2  3  4  5  6  7  8  9  10  11% Completed Time (Days)
Figure 1: Progress of HIT submission over time. There
was a hiatus of about a month during which we collected
no data, which we are omitting for clairty.
True Questions Validation Questions
Preferred % Preferred %
High-
Ranked
40.0% Reference 83.7%
Low-
Ranked
24.1% Random
Candidate
11.7%
No
Difference
35.9% No
Difference
4.65%
Table 1: Distributions of the collected judgments over
the true questions and over the embedded test questions.
?High-Ranked? (resp. ?Low-Ranked?) refers to whether
the decoder assigned a high (low) score to the candidate.
And so, annotators agreed with the decoder 40.0% of the
time, and disagreed 24.1% of the time.
questions was quite high (Table 1).10 From our set
of comparatively judged candidate translations we
extracted competing rule pairs. To reduce the in-
fluence of lexical choices and improve comparabil-
ity, we excluded pure preterminal rules and limited
the extraction to rules covering the same span in the
Urdu source. Figure 3 shows an interesting example
of one such rule pair. While the decoder demon-
strates a clear preference for rule (a) (including it
into its higher-ranked translation 100% of the time),
the Turkers tend to prefer translations generated us-
ing rule (b), disagreeing with the SMT system 60%
of the time. This indicates that preferring the second
rule in decoding may yield better results in terms of
human judgment, in this case potentially due to the
10It should be mentioned that the human references them-
selves are of relatively low quality.
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10
Candidate Rank
%
 
T
i
m
e
 
C
h
o
s
e
n
 
f
o
r
 
C
o
m
p
a
r
i
s
o
n
Figure 2: Histogram of the rank of the higher-ranked
candidate chosen in pair comparisons. For instance, in
about 29% of chosen pairs, the higher-ranked candidate
was the top candidate (of 300) by decoder score.
(a)  [NP] ! " [NP] [NN+IN] !" # the [NN+IN] [NP] $
(b)  [NP] ! " [NP] !" [NN] !"  # [NN] of [NP] $
Figure 3: A example pair of rules for which judgements
were obtained. The first rule is preferred by the decoder,
while human annotators favor the second rule.
cleaner separation of noun phrases from the prepo-
sitional phrase.
We also examine the distribution of the chosen
candidates. Recall that each pair consists of a high-
ranked candidate from the top-ten list, and a low-
ranked candidate from the top-300 list. The His-
togram of the higher rank (Figure 2) shows that the
high-ranked candidate is in fact a top-three candi-
date over 50% of the time. We also see (Figure 4)
that the low-ranked candidate tends to be either close
in rank to the top-ten list, or far away. This again
makes sense given our definition of potential for a
pair: potential is high if the derivations are very
close (left mode) or if the decoder scores differ con-
siderably (right mode).
Finally, we examine inter-annotator agreement,
since we collect multiple judgments per query. We
find that there is full agreement among the anno-
tators in 20.6% of queries. That is, in 20.6% of
queries, all three annotators answering that query
gave the same answer (out of the three provided
answers). This complete agreement rate is signif-
icantly higher than a rate caused by pure chance
(11.5%). This is a positive result, especially given
97
05
10
15
20
1
-
2
0
2
1
-
4
0
4
1
-
6
0
6
1
-
8
0
8
1
-
1
0
0
1
0
1
-
1
2
0
1
2
1
-
1
4
0
1
4
1
-
1
6
0
1
6
1
-
1
8
0
1
8
1
-
2
0
0
2
0
1
-
2
2
0
2
2
1
-
2
4
0
2
4
1
-
2
6
0
2
6
1
-
2
8
0
2
8
1
-
3
0
0
Candidate Rank
%
 
T
i
m
e
 
C
h
o
s
e
n
 
f
o
r
 
C
o
m
p
a
r
i
s
o
n
Figure 4: Histogram of the rank of the lower-ranked can-
didate chosen in pair comparisons. For instance, in about
16% of chosen candidate pairs, the lower-ranked candi-
date was ranked in the top 20.
how little diversity usually exists in n-best lists,
a fact (purposely) exacerbated by our strategy of
choosing highly similar pairs of candidates. On the
other hand, we observe complete disagreement in
only 14.9% of queries, which is significantly lower
than a rate caused by pure chance (which is 22.2%).
One thing to note is that these percentages are
calculated after excluding the validation questions,
where the complete agreement rate is an expectedly
even higher 64.9%, and the complete disagreement
rate is an expectedly even lower 3.60%.
6 Conclusions and Outlook
We presented a framework that allows us to ?tune?
MT systems on a finer level than system-level fea-
ture weights, going instead to the grammar rule level
and augmenting the feature set to reflect collected
human judgments. A system relying on this new fea-
ture during decoding is expected to have a slightly
different ranking of translation candidates that takes
human judgment into account. We presented one
particular judgment collection procedure that relies
on comparing candidate pairs (as opposed to eval-
uating a candidate in isolation) and complemented
it with one possible method of propagating human
judgments to cover grammar rules relevant to new
sentences.
While the presented statistics over the collected
data suggest that the proposed candidate selection
procedure yields consistent and potentially informa-
tive data, the quantitative effects on a machine trans-
lation system remain to be seen.
Additionally, introducing ? as a new feature
makes it necessary to find a viable weight for it.
While this can be done trivially in running MERT
on arbitrary development data, it may be of interest
to extend the weight optimization procedure in or-
der to preserve the partial ordering induced by the
judgments as best as possible.
Acknowledgments
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
by the DARPA GALE program under Contract No.
HR0011-06-2-0001, and the NSF under grant IIS-
0713448.
References
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). In SCALE 2009 Summer Workshop Final Re-
port, pages 135?139.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, Demonstration Session,
pages 177?180, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proc. of the Fourth
Workshop on Statistical Machine Translation, pages
135?139.
Ashish Venugopal and Andreas Zollmann. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. of the NAACL 2006 Workshop on Statistical
Machine Translation, pages 138?141. Association for
Computational Linguistics.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
98
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17?53,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Findings of the 2010 Joint Workshop on
Statistical Machine Translation and Metrics for Machine Translation
Chris Callison-Burch
Johns Hopkins University
ccb@cs.jhu.edu
Philipp Koehn
University of Edinburgh
pkoehn@inf.ed.ac.uk
Christof Monz
University of Amsterdam
c.monz@uva.nl
Kay Peterson and Mark Przybocki
National Institute of Standards and Technology
kay.peterson,mark.przybocki@nist.gov
Omar F. Zaidan
Johns Hopkins University
ozaidan@cs.jhu.edu
Abstract
This paper presents the results of the
WMT10 and MetricsMATR10 shared
tasks,1 which included a translation task,
a system combination task, and an eval-
uation task. We conducted a large-scale
manual evaluation of 104 machine trans-
lation systems and 41 system combina-
tion entries. We used the ranking of these
systems to measure how strongly auto-
matic metrics correlate with human judg-
ments of translation quality for 26 metrics.
This year we also investigated increasing
the number of human judgments by hiring
non-expert annotators through Amazon?s
Mechanical Turk.
1 Introduction
This paper presents the results of the shared
tasks of the joint Workshop on statistical Ma-
chine Translation (WMT) and Metrics for MA-
chine TRanslation (MetricsMATR), which was
held at ACL 2010. This builds on four previ-
ous WMT workshops (Koehn and Monz, 2006;
Callison-Burch et al, 2007; Callison-Burch et al,
2008; Callison-Burch et al, 2009), and one pre-
vious MetricsMATR meeting (Przybocki et al,
2008). There were three shared tasks this year:
a translation task between English and four other
European languages, a task to combine the out-
put of multiple machine translation systems, and
a task to predict human judgments of translation
quality using automatic evaluation metrics. The
1The MetricsMATR analysis was not complete in time for
the publication deadline. An updated version of paper will be
made available on http://statmt.org/wmt10/ prior
to July 15, 2010.
performance on each of these shared task was de-
termined after a comprehensive human evaluation.
There were a number of differences between
this year?s workshop and last year?s workshop:
? Non-expert judgments ? In addition to hav-
ing shared task participants judge translation
quality, we also collected judgments from
non-expert annotators hired through Ama-
zon?s Mechanical Turk. By collecting a large
number of judgments we hope to reduce the
burden on shared task participants, and to in-
crease the statistical significance of our find-
ings. We discuss the feasibility of using non-
experts evaluators, by analyzing the cost, vol-
ume and quality of non-expert annotations.
? Clearer results for system combination ?
This year we excluded Google translations
from the systems used in system combina-
tion. In last year?s evaluation, the large mar-
gin between Google and many of the other
systems meant that it was hard to improve on
when combining systems. This year, the sys-
tem combinations perform better than their
component systems more often than last year.
? Fewer rule-based systems ? This year there
were fewer rule-based systems submitted. In
past years, University of Saarland compiled a
large set of outputs from rule-based machine
translation (RBMT) systems. The RBMT
systems were not submitted this year. This
is unfortunate, because they tended to outper-
form the statistical systems for German, and
they were often difficult to rank properly us-
ing automatic evaluation metrics.
The primary objectives of this workshop are to
evaluate the state of the art in machine transla-
17
tion, to disseminate common test sets and pub-
lic training data with published performance num-
bers, and to refine evaluation methodologies for
machine translation. As with past years, all of the
data, translations, and human judgments produced
for our workshop are publicly available.2 We hope
they form a valuable resource for research into sta-
tistical machine translation, system combination,
and automatic evaluation of translation quality.
2 Overview of the shared translation and
system combination tasks
The workshop examined translation between En-
glish and four other languages: German, Span-
ish, French, and Czech. We created a test set for
each language pair by translating newspaper arti-
cles. We additionally provided training data and
two baseline systems.
2.1 Test data
The test data for this year?s task was created
by hiring people to translate news articles that
were drawn from a variety of sources from mid-
December 2009. A total of 119 articles were se-
lected, in roughly equal amounts from a variety
of Czech, English, French, German and Spanish
news sites:3
Czech: iDNES.cz (5), iHNed.cz (1), Lidov-
ky (16)
French: Les Echos (25)
Spanish: El Mundo (20), ABC.es (4), Cinco
Dias (11)
English: BBC (5), Economist (2), Washington
Post (12), Times of London (3)
German: Frankfurter Rundschau (11), Spie-
gel (4)
The translations were created by the profes-
sional translation agency CEET4. All of the trans-
lations were done directly, and not via an interme-
diate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
2http://statmt.org/wmt10/results.html
3For more details see the XML test files. The docid
tag gives the source and the date for each document in the
test set, and the origlang tag indicates the original source
language.
4http://www.ceet.eu/
train language models, and development sets to
tune parameters. Some statistics about the train-
ing materials are given in Figure 1.
2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits
for phrase-based and parsing-based statistical ma-
chine translation (Koehn et al, 2007; Li et al,
2009).
2.4 Submitted systems
We received submissions from 33 groups from 29
institutions, as listed in Table 1, a 50% increase
over last year?s shared task.
We also evaluated 2 commercial off the shelf
MT systems, and two online statistical machine
translation systems. We note that these companies
did not submit entries themselves. The entries for
the online systems were done by translating the
test data via their web interfaces. The data used
to train the online systems is unconstrained. It is
possible that part of the reference translations that
were taken from online news sites could have been
included in the online systems? language models.
2.5 System combination
In total, we received 153 primary system submis-
sions along with 28 secondary submissions. These
were made available to participants in the sys-
tem combination shared task. Based on feedback
that we received on last year?s system combina-
tion task, we provided two additional resources to
participants:
? Development set: We reserved 25 articles
to use as a dev set for system combination.
These were translated by all participating
sites, and distributed to system combination
participants along with reference translations.
? n-best translations: We requested n-best
lists from sites whose systems could produce
them. We received 20 n-best lists accompa-
nying the system submissions.
Table 2 lists the 9 participants in the system
combination task.
3 Human evaluation
As with past workshops, we placed greater em-
phasis on the human evaluation than on the auto-
matic evaluation metric scores. It is our contention
18
Europarl Training Corpus
Spanish? English French? English German? English
Sentences 1,650,152 1,683,156 1,540,549
Words 47,694,560 46,078,122 50,964,362 47,145,288 40,756,801 43,037,967
Distinct words 173,033 95,305 123,639 95,846 316,365 92,464
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 98,598 84,624 100,269 94,742
Words 2,724,141 2,432,064 2,405,082 2,101,921 2,505,583 2,443,183 2,050,545 2,290,066
Distinct words 69,410 46,918 53,763 43,906 101,529 47,034 125,678 45,306
United Nations Training Corpus
Spanish? English French? English
Sentences 6,222,450 7,230,217
Words 213,877,170 190,978,737 243,465,100 216,052,412
Distinct words 441,517 361,734 402,491 412,815
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German
Sentence 1,843,035 1,822,021 1,855,589 1,772,039
Words 50,132,615 51,223,902 54,273,514 43,781,217
Distinct words 99,206 178,934 127,689 328,628
News Language Model Data
English Spanish French German Czech
Sentence 48,653,884 3,857,414 15,670,745 17,474,133 13,042,040
Words 1,148,480,525 106,716,219 382,563,246 321,165,206 205,614,201
Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376
News Test Set
English Spanish French German Czech
Sentences 2489
Words 62,988 65,654 68,107 62,390 53,171
Distinct words 9,457 11,409 10,775 12,718 15,825
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and
the number of distinct words is based on the provided tokenizer.
19
ID Participant
AALTO Aalto University, Finland (Virpioja et al, 2010)
CAMBRIDGE Cambridge University (Pino et al, 2010)
CMU Carnegie Mellon University?s Cunei system (Phillips, 2010)
CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2010)
COLUMBIA Columbia University
CU-BOJAR Charles University Bojar (Bojar and Kos, 2010)
CU-TECTO Charles University Tectogramatical MT (Z?abokrtsky? et al, 2010)
CU-ZEMAN Charles University Zeman (Zeman, 2010)
DCU Dublin City University (Penkale et al, 2010)
DFKI Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (Federmann et al, 2010)
EU European Parliament, Luxembourg (Jellinghaus et al, 2010)
EUROTRANS commercial MT provider from the Czech Republic
FBK Fondazione Bruno Kessler (Hardmeier et al, 2010)
GENEVA University of Geneva
HUICONG Shanghai Jiao Tong University (Cong et al, 2010)
JHU Johns Hopkins University (Schwartz, 2010)
KIT Karlsruhe Institute for Technology (Niehues et al, 2010)
KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al, 2010)
LIMSI LIMSI (Allauzen et al, 2010)
LIU Linko?ping University (Stymne et al, 2010)
LIUM University of Le Mans (Lambert et al, 2010)
NRC National Research Council Canada (Larkin et al, 2010)
ONLINEA an online machine translation system
ONLINEB an online machine translation system
PC-TRANS commercial MT provider from the Czech Republic
POTSDAM Potsdam University
RALI RALI - Universite? de Montre?al (Huet et al, 2010)
RWTH RWTH Aachen (Heger et al, 2010)
SFU Simon Fraser University (Sankaran et al, 2010)
UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010)
UEDIN University of Edinburgh (Koehn et al, 2010)
UMD University of Maryland (Eidelman et al, 2010)
UPC Universitat Polite`cnica de Catalunya (Henr??quez Q. et al, 2010)
UPPSALA Uppsala University (Tiedemann, 2010)
UPV Universidad Polite?cnica de Valencia (Sanchis-Trilles et al, 2010)
UU-MS Uppsala University - Saers (Saers et al, 2010)
Table 1: Participants in the shared translation task. Not all groups participated in all language pairs.
20
ID Participant
BBN-COMBO BBN system combination (Rosti et al, 2010)
CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010)
CMU-COMBO-HYPOSEL CMU system combo with hyp. selection (Hildebrand and Vogel, 2010)
DCU-COMBO Dublin City University system combination (Du et al, 2010)
JHU-COMBO Johns Hopkins University system combination (Narsale, 2010)
KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)
LIUM-COMBO University of Le Mans system combination (Barrault, 2010)
RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010)
UPV-COMBO Universidad Polite?cnica de Valencia (Gonza?lez-Rubio et al, 2010)
Table 2: Participants in the system combination task.
Language Pair Sentence Ranking Edited Translations Yes/No Judgments
German-English 5,212 830 824
English-German 6,847 755 751
Spanish-English 5,653 845 845
English-Spanish 2,587 920 690
French-English 4,147 925 921
English-French 3,981 1,325 1,223
Czech-English 2,688 490 488
English-Czech 6,769 1,165 1,163
Totals 37,884 7,255 6,905
Table 3: The number of items that were collected for each task during the manual evaluation. An item
is defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/no
judgment in the judgment task.
21
that automatic measures are an imperfect substi-
tute for human assessment of translation quality.
Therefore, we define the manual evaluation to be
primary, and use the human judgments to validate
automatic metrics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct it on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partic-
ipants, interested volunteers, and a small number
of paid annotators. More than 120 people partic-
ipated in the manual evaluation5, with 89 people
putting in more than an hour?s worth of effort, and
29 putting in more than four hours. A collective
total of 337 hours of labor was invested.6
We asked people to evaluate the systems? output
in two different ways:
? Ranking translated sentences relative to each
other. This was our official determinant of
translation quality.
? Editing the output of systems without dis-
playing the source or a reference translation,
and then later judging whether edited transla-
tions were correct.
The total number of judgments collected for the
different modes of annotation is given in Table 3.
In all cases, the output of the various translation
systems were judged on equal footing; the output
of system combinations was judged alongside that
of the individual system, and the constrained and
unconstrained systems were judged together.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the in-
structions simple:
Rank translations from Best to Worst rel-
ative to the other choices (ties are al-
lowed).
5We excluded data from three errant annotators, identified
as follows. We considered annotators completing at least 3
screens, whose P (A) with others (see 3.2) is less than 0.33.
Out of seven such annotators, four were affiliated with shared
task teams. The other three had no apparent affiliation, and
so we discarded their data, less than 5% of the total data.
6Whenever an annotator appears to have spent more than
ten minutes on a single screen, we assume they left their sta-
tion and left the window open, rather than actually needing
more than ten minutes. In those cases, we assume the time
spent to be ten minutes.
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions. For each of the lan-
guage pairs, there were more than 5 submissions.
We did not attempt to get a complete ordering over
the systems, and instead relied on random selec-
tion and a reasonably large sample size to make
the comparisons fair.
Relative ranking is our official evaluation met-
ric. Individual systems and system combinations
are ranked based on how frequently they were
judged to be better than or equal to any other sys-
tem. The results of this are reported in Section 4.
Appendix A provides detailed tables that contain
pairwise comparisons between systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
We were interested in determining the inter- and
intra-annotator agreement for the ranking task,
since a reasonable degree of agreement must ex-
ist to support our process as a valid evaluation
setup. To ensure we had enough data to measure
agreement, we purposely designed the sampling of
source segments shown to annotators so that items
were likely to be repeated, both within an annota-
tor?s assigned tasks and across annotators. We did
so by assigning an annotator a batch of 20 screens
(each with three ranking sets; see 3.1) that were to
be completed in full before generating new screens
for that annotator.
Within each batch, the source segments for nine
of the 20 screens (45%) were chosen from a small
pool of 60 source segments, instead of being sam-
pled from the larger pool of 1,000 source segments
designated for the ranking task.7 The larger pool
was used to choose source segments for nine other
screens (also 45%). As for the remaining two
screens (10%), they were chosen randomly from
the set of eighteen screens already chosen. Fur-
thermore, in the two ?local repeat? screens, the
system choices were also preserved.
Heavily sampling from a small pool of source
segments ensured we had enough data to measure
inter-annotator agreement, while purposely mak-
ing 10% of each annotator?s screens repeats of pre-
viously seen sets in the same batch ensured we
7Each language pair had its own 60-sentence pool, dis-
joint from other language pairs? pools, but ach of the 60-
sentence pools was a subset of the 1,000-sentence pool.
22
INTER-ANNOTATOR AGREEMENT
P (A) K
With references 0.658 0.487
Without references 0.626 0.439
WMT ?09 0.549 0.323
INTRA-ANNOTATOR AGREEMENT
P (A) K
With references 0.755 0.633
Without references 0.734 0.601
WMT ?09 0.707 0.561
Table 4: Inter- and intra-annotator agreement for
the sentence ranking task. In this task, P (E) is
0.333.
had enough data to measure intra-annotator agree-
ment.
We measured pairwise agreement among anno-
tators using the kappa coefficient (K), which is de-
fined as
K =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance.
For inter-annotator agreement for the ranking
tasks we calculated P (A) by examining all pairs
of systems which had been judged by two or more
judges, and calculated the proportion of time that
they agreed thatA > B, A = B, orA < B. Intra-
annotator agreement was computed similarly, but
we gathered items that were annotated on multiple
occasions by a single annotator.
Table 4 gives K values for inter-annotator and
intra-annotator agreement. These give an indi-
cation of how often different judges agree, and
how often single judges are consistent for repeated
judgments, respectively. The exact interpretation
of the kappa coefficient is difficult, but according
to Landis and Koch (1977), 0? .2 is slight, .2? .4
is fair, .4 ? .6 is moderate, .6 ? .8 is substantial
and the rest is almost perfect.
Based on these interpretations the agreement
for sentence-level ranking is moderate for inter-
annotator agreement and substantial for intra-
annotator agreement. These levels of agreement
are higher than in previous years, partially due to
the fact that that year we randomly included the
references along the system outputs. In general,
judges tend to rank the reference as the best trans-
lation, so people have stronger levels of agreement
when it is included. That said, even when compar-
isons involving reference are excluded, we still see
an improvement in agreement levels over last year.
3.3 Editing machine translation output
In addition to simply ranking the output of sys-
tems, we also had people edit the output of MT
systems. We did not show them the reference
translation, which makes our edit-based evalu-
ation different from the Human-targeted Trans-
lation Edit Rate (HTER) measure used in the
DARPA GALE program (NIST, 2008). Rather
than asking people to make the minimum number
of changes to the MT output in order capture the
same meaning as the reference, we asked them to
edit the translation to be as fluent as possible with-
out seeing the reference. Our hope was that this
would reflect people?s understanding of the out-
put.
The instructions given to our judges were as fol-
lows:
Correct the translation displayed, mak-
ing it as fluent as possible. If no correc-
tions are needed, select ?No corrections
needed.? If you cannot understand the
sentence well enough to correct it, select
?Unable to correct.?
A screenshot is shown in Figure 2. This year,
judges were shown the translations of 5 consec-
utive source sentences, all produced by the same
machine translation system. In last year?s WMT
evaluation they were shown only one sentence at a
time, which made the task more difficult because
the surrounding context could not be used as an
aid to understanding.
Since we wanted to prevent judges from see-
ing the reference before editing the translations,
we split the test set between the sentences used
in the ranking task and the editing task (because
they were being conducted concurrently). More-
over, annotators edited only a single system?s out-
put for one source sentence to ensure that their un-
derstanding of it would not be influenced by an-
other system?s output.
3.4 Judging the acceptability of edited output
Halfway through the manual evaluation period, we
stopped collecting edited translations, and instead
asked annotators to do the following:
23
Edit Machine Translation Outputs
Instructions:
You are shown several machine translation outputs.
Your task is to edit each translation to make it as fluent as possible.
It is possible that the translation is already fluent. In that case, select No corrections needed.
If you cannot understand the sentence well enough to correct it, select Unable to correct.
The sentences are all from the same article. You can use the earlier and later sentences
to help understand a confusing sentence.
Your edited translations           The machine translations
   
The shortage of snow in mountain worries the hoteliers
Edited     No corrections needed     Unable to
correct         Reset
 
The shortage of snow in mountain
worries the hoteliers
   
The deserted tracks are not putting down problem only at the exploitants 
of skilift.
Edited     No corrections needed     Unable to
correct         Reset
 
The deserted tracks are not
putting down problem only at the
exploitants of skilift.
   
The lack of snow deters the people to reserving their stays at the ski in 
the hotels and pension.
Edited     No corrections needed     Unable to
correct         Reset
 
The lack of snow deters the people
to reserving their stays at the ski
in the hotels and pension.
   
Thereby, is always possible to track free bedrooms for all the dates in 
winter, including Christmas and Nouvel An.
Edited     No corrections needed     Unable to
correct         Reset
 
Thereby, is always possible to
track free bedrooms for all the
dates in winter, including
Christmas and Nouvel An.
   
We have many of visit on our site
Figure 2: This screenshot shows what an annotator sees when beginning to edit the output of a machine
translation system.
24
Indicate whether the edited transla-
tions represent fully fluent and meaning-
equivalent alternatives to the reference
sentence. The reference is shown with
context, the actual sentence is bold.
In addition to edited translations, unedited items
that were either marked as acceptable or as incom-
prehensible were also shown. Judges gave a sim-
ple yes/no indication to each item.
4 Translation task results
We used the results of the manual evaluation to
analyze the translation quality of the different sys-
tems that were submitted to the workshop. In our
analysis, we aimed to address the following ques-
tions:
? Which systems produced the best translation
quality for each language pair?
? Did the system combinations produce better
translations than individual systems?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 5 shows the best individual systems. We
define the best systems as those which had no
other system that was statistically significantly
better than them under the Sign Test at p ? 0.1.
Multiple systems are listed as the winners for
many language pairs because it was not possible to
draw a statistically significant difference between
the systems. There is no individual system clearly
outperforming all other systems across the differ-
ent language pairs. With the exception of French-
English and English-French one can observe that
top-performing constrained systems did as well as
the unconstrained system ONLINEB.
Table 6 shows the best combination systems.
For all language directions, except Spanish-
English, one can see that the system combina-
tion runs outperform the individual systems and
that in most cases the differences are statistically
significant. While this is to be expected, system
combination is not guaranteed to improve perfor-
mance as some of the lower ranked combination
runs show, which are outperformed by individual
systems. Also note that except for Czech-English
translation the online systems ONLINEA and ON-
LINEB where not included for the system combi-
nation runs
Understandability
Our hope is that judging the acceptability of edited
output as discussed in Section 3 gives some indi-
cation of how often a system?s output was under-
standable. Figure 3 gives the percentage of times
that each system?s edited output was judged to
be acceptable (the percentage also factors in in-
stances when judges were unable to improve the
output because it was incomprehensible).
This style of manual evaluation is experimental
and should not be taken to be authoritative. Some
caveats about this measure:
? There are several sources of variance that are
difficult to control for: some people are better
at editing, and some sentences are more dif-
ficult to edit. Therefore, variance in the un-
derstandability of systems is difficult to pin
down.
? The acceptability measure does not strongly
correlate with the more established method of
ranking translations relative to each other for
all the language pairs.
5 Shared evaluation task overview
In addition to allowing the analysis of subjective
translation quality measures for different systems,
the judgments gathered during the manual evalu-
ation may be used to evaluate how well the au-
tomatic evaluation metrics serve as a surrogate to
the manual evaluation processes. NIST began run-
ning a ?Metrics for MAchine TRanslation? chal-
lenge (MetricsMATR), and presented their find-
ings at a workshop at AMTA (Przybocki et al,
2008). This year we conducted a joint Metrics-
MATR and WMT workshop, with NIST running
the shared evaluation task and analyzing the re-
sults.
In this year?s shared evaluation task 14 different
research groups submitted a total of 26 different
automatic metrics for evaluation:
Aalto University of Science and Technology
(Dobrinkat et al, 2010)
? MT-NCD ? A machine translation metric
based on normalized compression distance
(NCD), a general information-theoretic mea-
sure of string similarity. MT-NCD mea-
sures the surface level similarity between two
strings with a general compression algorithm.
More similar strings can be represented with
25
French-English
551?755 judgments per system
System C? ?others
LIUM ?? Y 0.71
ONLINEB ? N 0.71
NRC ?? Y 0.66
CAMBRIDGE ?? Y +GW 0.66
LIMSI ? Y +GW 0.65
UEDIN Y 0.65
RALI ?? Y +GW 0.65
JHU Y 0.59
RWTH ?? Y +GW 0.55
LIG Y 0.53
ONLINEA N 0.52
CMU-STATXFER Y 0.51
HUICONG Y 0.51
DFKI N 0.42
GENEVA Y 0.27
CU-ZEMAN Y 0.21
English-French
664?879 judgments per system
System C? ?others
UEDIN ?? Y 0.70
ONLINEB ? N 0.68
RALI ?? Y +GW 0.66
LIMSI ?? Y +GW 0.66
RWTH ?? Y +GW 0.63
CAMBRIDGE ? Y +GW 0.63
LIUM Y 0.63
NRC Y 0.62
ONLINEA N 0.55
JHU Y 0.53
DFKI N 0.40
GENEVA Y 0.35
EU N 0.32
CU-ZEMAN Y 0.26
KOC Y 0.26
Czech-English
788?868 judgments per system
System C? ?others
ONLINEB ? N 0.7
UEDIN ? Y 0.61
CMU Y 0.55
CU-BOJAR N 0.55
AALTO Y 0.43
ONLINEA N 0.37
CU-ZEMAN Y 0.22
German-English
723?879 judgments per system
System C? ?others
ONLINEB ? N 0.73
KIT ?? Y +GW 0.72
UMD ?? Y 0.68
UEDIN ? Y 0.66
FBK ? Y +GW 0.66
ONLINEA ? N 0.63
RWTH Y +GW 0.62
LIU Y 0.59
UU-MS Y 0.55
JHU Y 0.53
LIMSI Y +GW 0.52
UPPSALA Y 0.51
DFKI N 0.50
HUICONG Y 0.47
CMU Y 0.46
AALTO Y 0.42
CU-ZEMAN Y 0.36
KOC Y 0.23
English-German
1284?1542 judgments per system
System C? ?others
ONLINEB ? N 0.70
DFKI ? N 0.62
UEDIN ?? Y 0.62
KIT ? Y 0.60
ONLINEA N 0.59
FBK ? Y 0.56
LIU Y 0.55
RWTH Y 0.51
LIMSI Y 0.51
UPPSALA Y 0.47
JHU Y 0.46
SFU Y 0.34
KOC Y 0.30
CU-ZEMAN Y 0.28
English-Czech
1375?1627 judgments per system
System C? ?others
ONLINEB ? N 0.70
CU-BOJAR ? N 0.66
PC-TRANS ? N 0.62
UEDIN ?? Y 0.62
CU-TECTO Y 0.60
EUROTRANS N 0.54
CU-ZEMAN Y 0.50
SFU Y 0.45
ONLINEA N 0.44
POTSDAM Y 0.44
DCU N 0.38
KOC Y 0.33
Spanish-English
1448?1577 judgments per system
System C? ?others
ONLINEB ? N 0.70
UEDIN ?? Y 0.69
CAMBRIDGE Y +GW 0.61
JHU Y 0.61
ONLINEA N 0.54
UPC ? Y 0.51
HUICONG Y 0.50
DFKI N 0.45
COLUMBIA Y 0.45
CU-ZEMAN Y 0.27
English-Spanish
540?722 judgments per system
System C? ?others
ONLINEB ? N 0.71
ONLINEA ? N 0.69
UEDIN ? Y 0.61
DCU N 0.61
DFKI ? N 0.55
JHU ? Y 0.55
UPV ? Y 0.55
CAMBRIDGE ? Y +GW 0.54
UHC-UPV ? Y 0.54
SFU Y 0.40
CU-ZEMAN Y 0.23
KOC Y 0.19
Systems are listed in the order of how often their translations were ranked higher than or equal to any other system. Ties are
broken by direct comparison.
C? indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, and
optionally the LDC?s GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).
? indicates a win in the category, meaning that no other system is statistically significantly better at p-level?0.1 in pairwise
comparison.
? indicates a constrained win, no other constrained system is statistically better.
For all pairwise comparisons between systems, please check the appendix.
Table 5: Official results for the WMT10 translation task, based on the human evaluation (ranking trans-
lations relative to each other)
26
French-English
589?716 judgments per combo
System ?others
RWTH-COMBO ? 0.77
CMU-HYP-COMBO ? 0.77
DCU-COMBO ? 0.72
LIUM ? 0.71
CMU-HEA-COMBO ? 0.70
UPV-COMBO ? 0.68
NRC 0.66
CAMBRIDGE 0.66
UEDIN ? 0.65
LIMSI ? 0.65
JHU-COMBO 0.65
RALI 0.65
LIUM-COMBO 0.64
BBN-COMBO 0.64
RWTH 0.55
English-French
740?829 judgments per combo
System ?others
RWTH-COMBO ? 0.75
CMU-HEA-COMBO ? 0.74
UEDIN 0.70
KOC-COMBO ? 0.68
UPV-COMBO 0.66
RALI ? 0.66
LIMSI 0.66
RWTH 0.63
CAMBRIDGE 0.63
Czech-English
766?843 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.71
ONLINEB ? 0.7
BBN-COMBO ? 0.70
RWTH-COMBO ? 0.65
UPV-COMBO ? 0.63
JHU-COMBO 0.62
UEDIN 0.61
German-English
743?835 judgments per combo
System ?others
BBN-COMBO ? 0.77
RWTH-COMBO ? 0.75
CMU-HEA-COMBO 0.73
KIT ? 0.72
UMD ? 0.68
JHU-COMBO 0.67
UEDIN ? 0.66
FBK 0.66
CMU-HYP-COMBO 0.65
UPV-COMBO 0.64
RWTH 0.62
KOC-COMBO 0.59
English-German
1340?1469 judgments per combo
System ?others
RWTH-COMBO ? 0.65
DFKI ? 0.62
UEDIN ? 0.62
KIT ? 0.60
CMU-HEA-COMBO ? 0.59
KOC-COMBO 0.59
FBK ? 0.56
UPV-COMBO 0.55
English-Czech
1405?1496 judgments per combo
System ?others
DCU-COMBO ? 0.75
ONLINEB ? 0.70
RWTH-COMBO 0.70
CMU-HEA-COMBO 0.69
UPV-COMBO 0.68
CU-BOJAR 0.66
KOC-COMBO 0.66
PC-TRANS 0.62
UEDIN 0.62
Spanish-English
1385?1535 judgments per combo
System ?others
UEDIN ? 0.69
CMU-HEA-COMBO ? 0.66
UPV-COMBO ? 0.66
BBN-COMBO 0.62
JHU-COMBO 0.55
UPC 0.51
English-Spanish
516?673 judgments per combo
System ?others
CMU-HEA-COMBO ? 0.68
KOC-COMBO 0.62
UEDIN ? 0.61
UPV-COMBO 0.60
RWTH-COMBO 0.59
DFKI ? 0.55
JHU 0.55
UPV 0.55
CAMBRIDGE ? 0.54
UPV-NNLM ? 0.54
System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.
Ties are broken by direct comparison. We show the best individual systems alongside the system combinations, since the goal
of combination is to produce better quality translation than the component systems.
? indicates a win for the system combination meaning that no other system or system combination is statistically signifi-
cantly better at p-level?0.1 in pairwise comparison.
? indicates an individual system that none of the system combinations beat by a statistically significant margin at p-
level?0.1.
For all pairwise comparisons between systems, please check the appendix.
Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks,
except in the Czech-English and English-Czech conditions, where ONLINEB was included.
Table 6: Official results for the WMT10 system combination task, based on the human evaluation (rank-
ing translations relative to each other)
27
System % Yes Yes count No count N/A count Total count *** en-cz ***
ref 0.97 63 2 0 65 en-cz
dcu-c 0.58 29 21 0 50 en-cz
onlineB 0.55 22 18 0 40 en-cz
rwth-c 0.49 56 59 0 115 en-cz
koc-c 0.45 29 36 0 65 en-cz
pc-trans 0.43 26 34 0 60 en-cz
upv-c 0.42 23 32 0 55 en-cz
cu-bojar 0.4 20 30 0 50 en-cz
eurotrans 0.4 18 27 0 45 en-cz
uedin 0.34 24 46 0 70 en-cz
cu-tecto 0.34 29 55 1 85 en-cz
cmu-hea-c 0.29 13 32 0 45 en-cz
sfu 0.24 14 44 0 58 en-cz
potsdam 0.24 13 42 0 55 en-cz
cu-zeman 0.21 15 55 0 70 en-cz
koc 0.21 21 79 0 100 en-cz
onlineA 0.2 13 52 0 65 en-cz
dcu 0.19 13 57 0 70 en-cz
0.1260077028
*** en-de ***
ref 0.94 47 3 0 50 en-de
onlineA 0.8 20 5 0 25 en-de
koc-c 0.68 17 8 0 25 en-de
uppsala 0.65 26 14 0 40 en-de
uedin 0.62 50 30 0 80 en-de
kit 0.62 37 23 0 60 en-de
upv-c 0.57 30 23 0 53 en-de
onlineB 0.52 21 19 0 40 en-de
dfki 0.52 13 12 0 25 en-de
koc 0.51 18 17 0 35 en-de
limsi 0.51 18 16 1 35 en-de
liu 0.51 28 27 0 55 en-de
rwth 0.5 15 15 0 30 en-de
rwth-c 0.49 22 23 0 45 en-de
jhu 0.48 12 13 0 25 en-de
cmu-hea-c 0.47 14 16 0 30 en-de
fbk 0.4 4 6 0 10 en-de
sfu 0.31 11 24 0 35 en-de
cu-zeman 0.19 10 40 3 53 en-de
0.1364453014
System % Yes Yes count No count N/A count Total count *** en-es ***
ref 0.83 48 10 0 58 en-es
onlineB 0.58 25 18 0 43 en-es
upv 0.5 20 20 0 40 en-es
rwth-c 0.46 13 15 0 28 en-es
dcu 0.42 16 22 0 38 en-es
koc 0.4 17 24 1 42 en-es
upv-nnlm 0.39 15 23 0 38 en-es
onlineA 0.38 11 18 0 29 en-es
jhu 0.38 17 27 1 45 en-es
koc-c 0.38 20 33 0 53 en-es
uedin 0.36 12 21 0 33 en-es
upb-c 0.32 13 27 0 40 en-es
cmu-hea-c 0.32 16 34 0 50 en-es
camb 0.3 12 27 1 40 en-es
dfki 0.29 7 17 0 24 en-es
cu-zeman 0.29 16 39 0 55 en-es
sfu 0.26 9 25 0 34 en-es
0.0845946216
System % Yes Yes count No count N/A count Total count *** en-fr ***
ref 0.91 64 4 2 70 en-fr
rwth-c 0.54 27 23 0 50 en-fr
onlineB 0.52 47 42 1 90 en-fr
upv-c 0.51 34 33 0 67 en-fr
koc-c 0.48 32 34 0 66 en-fr
uedin 0.48 30 32 1 63 en-fr
rali 0.47 21 24 0 45 en-fr
rwth 0.45 25 30 0 55 en-fr
lium 0.43 20 27 0 47 en-fr
camb 0.42 26 36 0 62 en-fr
onlineA 0.41 15 22 0 37 en-fr
limsi 0.37 26 44 0 70 en-fr
jhu 0.37 27 46 0 73 en-fr
nrc 0.36 13 23 0 36 en-fr
cmu-hea-c 0.32 22 47 0 69 en-fr
geneva 0.31 32 70 0 102 en-fr
eu 0.3 13 30 0 43 en-fr
dfki 0.28 16 42 0 58 en-fr
koc 0.21 12 44 1 57 en-fr
cu-zeman 0.17 11 52 0 63 en-fr
0.1045877454
System % Yes Yes count No count N/A count Total count *** cz-en ***
ref 1.00 33 0 0 33 cz-en
cu-bojar 0.6 3 2 0 5 cz-en
upv-c 0.43 15 20 0 35 cz-en
cmu-hea-c 0.35 14 26 0 40 cz-en
rwth-c 0.32 16 34 0 50 cz-en
onlineB 0.3 12 28 0 40 cz-en
bbn-c 0.28 17 43 0 60 cz-en
uedin 0.28 11 28 1 40 cz-en
aalto 0.27 8 22 0 30 cz-en
jhu-c 0.26 13 37 0 50 cz-en
onlineA 0.2 6 24 0 30 cz-en
cmu 0.17 5 25 0 30 cz-en
cu-zeman 0.09 4 40 1 45 cz-en
0.1292958787
System % Yes Yes count No count N/A count Total count *** de-en ***
ref 0.98 44 1 0 45 de-en
umd 0.8 8 2 0 10 de-en
bbn-c 0.67 10 5 0 15 de-en
onlineB 0.65 13 7 0 20 de-en
cmu-hea-c 0.52 12 11 0 23 de-en
jhu-c 0.51 18 17 0 35 de-en
upv-c 0.51 18 16 1 35 de-en
fbk 0.5 20 20 0 40 de-en
uppsala 0.5 20 19 1 40 de-en
limsi 0.46 30 34 1 65 de-en
kit 0.45 18 22 0 40 de-en
liu 0.44 19 24 0 43 de-en
uedin 0.44 11 14 0 25 de-en
dfki 0.4 12 18 0 30 de-en
onlineA 0.4 6 9 0 15 de-en
rwth 0.4 14 21 0 35 de-en
cmu-hyp-c 0.37 11 19 0 30 de-en
huicong 0.36 9 16 0 25 de-en
koc-c 0.36 9 14 2 25 de-en
rwth-c 0.36 10 18 0 28 de-en
koc 0.31 11 23 1 35 de-en
cu-zeman 0.3 12 28 0 40 de-en
uu-ms 0.26 13 37 0 50 de-en
jhu 0.26 9 26 0 35 de-en
cmu 0.24 6 19 0 25 de-en
aalto 0.07 1 14 0 15 de-en
0.1512635669
System % Yes Yes count No count N/A count Total count *** es-en ***
ref 0.98 39 0 1 40 es-en
onlineB 0.71 39 15 1 55 es-en
onlineA 0.64 32 18 0 50 es-en
upv-c 0.6 36 24 0 60 es-en
huicong 0.54 27 23 0 50 es-en
jhu 0.54 35 30 0 65 es-en
cmu-hea-c 0.52 26 23 1 50 es-en
bbn-c 0.51 36 33 1 70 es-en
uedin 0.51 33 30 2 65 es-en
jhu-c 0.47 28 31 1 60 es-en
dfki 0.46 16 18 1 35 es-en
upc 0.43 28 36 1 65 es-en
cu-zeman 0.4 18 26 1 45 es-en
camb 0.36 25 45 0 70 es-en
columbia 0.29 19 46 0 65 es-en
0.1104436607
System % Yes Yes count No count N/A count Total count *** fr-en ***
ref 0.91 32 3 0 35 fr-en
cmu-hyp-c 0.7 21 9 0 30 fr-en
uedin 0.58 23 17 0 40 fr-en
bbn-c 0.56 14 10 1 25 fr-en
rwth-c 0.53 16 14 0 30 fr-en
onlineB 0.51 28 27 0 55 fr-en
camb 0.5 20 19 1 40 fr-en
rali 0.48 31 34 0 65 fr-en
lium 0.46 23 27 0 50 fr-en
dcu-c 0.45 15 16 2 33 fr-en
lig 0.45 9 11 0 20 fr-en
cmu-statxfer 0.44 11 14 0 25 fr-en
nrc 0.43 15 20 0 35 fr-en
dfki 0.4 8 12 0 20 fr-en
jhu 0.4 10 14 1 25 fr-en
jhu-c 0.4 22 30 3 55 fr-en
upv-c 0.4 14 20 1 35 fr-en
lium-c 0.4 27 41 0 68 fr-en
cmu-hea-c 0.35 14 26 0 40 fr-en
limsi 0.35 14 26 0 40 fr-en
onlineA 0.33 20 40 0 60 fr-en
huicong 0.32 13 25 2 40 fr-en
cu-zeman 0.24 6 19 0 25 fr-en
geneva 0.24 6 19 0 25 fr-en
rwth 0.2 1 4 0 5 fr-en
0.1143475506
0
0.25
0.5
0.75
1
r
e
f
d
c
u
-
c
o
n
l
i
n
e
B
r
w
t
h
-
c
k
o
c
-
c
p
c
-
t
r
a
n
s
u
p
v
-
c
c
u
-
b
o
j
a
r
e
u
r
o
t
r
a
n
s
u
e
d
i
n
c
u
-
t
e
c
t
o
c
m
u
-
h
e
a
-
c
s
f
u
p
o
t
s
d
a
m
c
u
-
z
e
m
a
n
k
o
c
o
n
l
i
n
e
A
d
c
u
.19.2.21.21.24.24.29.34.34.4.4.42.43.45.49.55.58.97
English-Czech
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
A
k
o
c
-
c
u
p
p
s
a
l
a
u
e
d
i
n
k
i
t
u
p
v
-
c
o
n
l
i
n
e
B
d
f
k
i
k
o
c
l
i
m
s
i
l
i
u
r
w
t
h
r
w
t
h
-
c
j
h
u
c
m
u
-
h
e
a
-
c
f
b
k
s
f
u
c
u
-
z
e
m
a
n
.19.31.4.47.48.49.5.51.51.51.52.52.57.62.62.65.68.8.94
English-German
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
u
p
v
r
w
t
h
-
c
d
c
u
k
o
c
u
p
v
-
n
n
l
m
o
n
l
i
n
e
A
j
h
u
k
o
c
-
c
u
e
d
i
n
u
p
b
-
c
c
m
u
-
h
e
a
-
c
c
a
m
b
d
f
k
i
c
u
-
z
s
f
u
.26.29.29.3.32.32.36.38.38.38.39.4.42.46.5.58.83
English-Spanish
0
0.25
0.5
0.75
1
r
e
f
r
w
t
h
-
c
o
n
l
i
n
e
B
u
p
v
-
c
k
o
c
-
c
u
e
d
i
n
r
a
l
i
r
w
t
h
l
i
u
m
c
a
m
b
o
n
l
i
n
e
A
l
i
m
s
i
j
h
u
n
r
c
c
m
u
-
h
e
a
-
c
g
e
n
e
v
a
e
u
d
f
k
i
k
o
c
c
u
-
z
e
m
a
n
.17.21.28.3.31.32.36.37.37.41.42.43.45.47.48.48.51.52.54.91
English-French
0
0.25
0.5
0.75
1
r
e
f
u
m
d
b
b
n
-
c
o
n
l
i
n
e
B
c
m
u
-
h
e
a
-
c
j
h
u
-
c
u
p
v
-
c
f
b
k
u
p
p
s
a
l
a
l
i
m
s
i
k
i
t
l
i
u
u
e
d
i
n
d
f
k
i
o
n
l
i
n
e
A
r
w
t
h
c
m
u
-
h
y
p
-
c
h
u
i
c
o
n
g
k
o
c
-
c
r
w
t
h
-
c
k
o
c
c
u
-
z
e
m
a
n
u
u
-
m
s
j
h
u
c
m
u
a
a
l
t
o
.07.24.26.26.3.31.36.36.36.37.4.4.4.44.44.45.46.5.5.51.51.52.65.67.8.98
German-English
0
0.25
0.5
0.75
1
r
e
f
c
u
-
b
o
j
a
r
u
p
v
-
c
c
m
u
-
h
e
a
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
b
b
n
-
c
u
e
d
i
n
a
a
l
t
o
j
h
u
-
c
o
n
l
i
n
e
A
c
m
u
c
u
-
z
e
m
a
n
.09.17.2.26.27.28.28.3.32.35.43.61.0
Czech-English
0
0.25
0.5
0.75
1
r
e
f
o
n
l
i
n
e
B
o
n
l
i
n
e
A
u
p
v
-
c
h
u
i
c
o
n
g
j
h
u
c
m
u
-
h
e
a
-
c
b
b
n
-
c
4
5
j
h
u
-
c
d
f
k
i
u
p
c
c
u
-
z
e
m
a
n
c
a
m
b
c
o
l
u
m
b
i
a
.29.36.4.43.46.47.51.51.52.54.54.6.64.71.98
Spanish-English
0
0.25
0.5
0.75
1
r
e
f
c
m
u
-
h
y
p
-
c
u
e
d
i
n
b
b
n
-
c
r
w
t
h
-
c
o
n
l
i
n
e
B
c
a
m
b
r
a
l
i
l
i
u
m
d
c
u
-
c
l
i
g
c
m
u
-
s
t
a
t
x
f
e
r
n
r
c
d
f
k
i
j
h
u
j
h
u
-
c
u
p
v
-
c
l
i
u
m
-
c
c
m
u
-
h
e
a
-
c
l
i
m
s
i
o
n
l
i
n
e
A
h
u
i
c
o
n
g
c
u
-
z
e
m
a
n
g
e
n
e
v
a
r
w
t
h
.2.24.24.32.33.35.35.4.4.4.4.4.43.44.45.45.46.48.5.51.53.56.58.7.91
French-English
Figure 3: The percent of time that each system?s edited output was judged to be an acceptable translation.
These numbers also include judgments of the system?s output when it was marked either incomprehen-
sible or acceptable and left unedited. Note that the reference translation was edited alongside the system
outputs. Error bars show one positive and one negative standard deviation for the systems in that lan-
guage pair.
28
a shorter description when concatenated be-
fore compression than when concatenated af-
ter compression. MT-NCD does not require
any language specific resources.
? MT-mNCD ? Enhances MT-NCD with flex-
ible word matching provided by stemming
and synonyms. It works analogously to
M-BLEU and M-TER and uses METEOR?s
aligner module to find relaxed word-to-word
alignments. MT-mNCD exploits English
WordNet data and increases correlation to hu-
man judgments for English over MT-NCD.
Due to a processing issue inherent to the metric,
the scores reported were generated excluding the
first segment of each document. Also, a separate
issue was found for the MT-mNCD metric, and ac-
cording to the developer the scores reported here
would like change with a correction of the issue.
BabbleQuest International8
? Badger 2.0 full ? Uses the Smith-Waterman
alignment algorithm with Gotoh improve-
ments to measure segment similarity. The
full version uses a multilingual knowledge
base to assign a substitution cost which sup-
ports normalization of word infection and
similarity.
? Badger 2.0 lite ? The lite version uses default
gap, gap extension and substitution costs.
City University of Hong Kong (Wong and Kit,
2010)
? ATEC 2.1 ? This version of ATEC extends
the measurement of word choice and word or-
der by various means. The former is assessed
by matching word forms at linguistic levels,
including surface form, stem, sense and se-
mantic similarity, and further by weighting
the informativeness of both matched and un-
matched words. The latter is quantified in
term of the discordance of word position and
word sequence between an MT output and its
reference.
Due to a version discrepancy of the metric, final
scores for ATECD-2.1 differ from those reported
here, but only minimally.
8http://www.babblequest.com/badger2
Carnegie Mellon University (Denkowski and
Lavie, 2010)
? METEOR-NEXT-adq ? Evaluates a machine
translation hypothesis against one or more
reference translations by calculating a simi-
larity score based on an alignment between
the hypothesis and reference strings. Align-
ments are based on exact, stem, synonym,
and paraphrase matches between words and
phrases in the strings. Metric parameters are
tuned to maximize correlation with human
judgments of translation quality (adequacy
judgments).
? METEOR-NEXT-hter ? METEOR-NEXT
tuned to HTER.
? METEOR-NEXT-rank ? METEOR-NEXT
tuned to human judgments of rank.
Columbia University9
? SEPIA ? A syntactically-aware machine
translation evaluation metric designed with
the goal of assigning bigger weight to gram-
matical structural bigrams with long surface
spans that cannot be captured with surface n-
gram metrics. SEPIA uses a dependency rep-
resentation produced for both hypothesis and
reference(s). SEPIA is configurable to allow
using different combinations of structural n-
grams, surface n-grams, POS tags, depen-
dency relations and lemmatization. SEPIA is
a precision-based metric and as such employs
clipping and length penalty to minimize met-
ric gaming.
Charles University Prague (Bojar and Kos,
2010)
? SemPOS ? Computes overlapping of autose-
mantic (content-bearing) word lemmas in the
candidate and reference translations given a
fine-grained semantic part of speech (sem-
pos) and outputs average overlapping score
over all sempos types. The overlapping is de-
fined as the number of matched lemmas di-
vided by the total number of lemmas in the
candidate and reference translations having
the same sempos type.
9http://www1.ccls.columbia.edu/?SEPIA/
29
? SemPOS-BLEU ? A linear combination of
SemPOS and BLEU with equal weights.
BLEU is computed on surface forms of au-
tosemantic words that are used by SemPOS,
i.e. auxiliary verbs or prepositions are not
taken into account.
Dublin City University (He et al, 2010)
? DCU-LFG ? A combination of syntactic and
lexical information. It measures the similar-
ity of the hypothesis and reference in terms
of matches of Lexical Functional Grammar
(LFG) dependency triples. The matching
module can also access the WordNet syn-
onym dictionary and Snover?s paraphrase
database10.
University of Edinburgh (Birch and Osborne,
2010)
? LRKB4 ? A novel metric which directly mea-
sures reordering success using Kendall?s tau
permutation distance metrics. The reordering
component is combined with a lexical metric,
capturing the two most important elements
of translation quality. This simple combined
metric only has one parameter, which makes
its scores easy to interpret. It is also fast
to run and language-independent. It uses
Kendall?s tau permutation.
? LRHB4 ? LRKB4, replacing Kendall?s tau
permutation distance metric with the Ham-
ming distance permutation distance metric.
Due to installation issues, the reported submitted
scores for these two metrics have not been verified
to produce identical scores at NIST.
Harbin Institute of Technology, China
? I-letter-BLEU ? Normal BLEU based on let-
ters. Moreover, the maximum length of N-
gram is decided by the average length for
each sentence, respectively.
? I-letter-recall ? A geometric mean of N-gram
recall based on letters. Moreover, the maxi-
mum length of N-gram is decided by the av-
erage length for each sentence, respectively.
10Available at http://www.umiacs.umd.edu/
?snover/terp/.
? SVM-RANK ? Uses support vector ma-
chines rank models to predict an ordering
over a set of system translations with lin-
ear kernel. Features include Meteor-exact,
BLEU-cum-1, BLEU-cum-2, BLEU-cum-5,
BLEU-ind-1, BLEU-ind-2, ROUGE-L re-
call, letter-based TER, letter-based BLEU-
cum-5, letter-based ROUGE-L recall, and
letter-based ROUGE-S recall.
National University of Singapore (Liu et al,
2010)
? TESLA-M ? Based on matching of bags of
unigrams, bigrams, and trigrams, with con-
sideration of WordNet synonyms. The match
is done in the framework of real-valued lin-
ear programming to enable the discounting of
function words.
? TESLA ? Built on TESLA-M, this metric
also considers bilingual phrase tables to dis-
cover phrase-level synonyms. The feature
weights are tuned on the development data
using SVMrank.
Stanford University
? Stanford ? A discriminatively trained
string-edit distance metric with various
similarity-matching, synonym-matching, and
dependency-parse-tree-matching features.
The model resembles a Conditional Random
Field, but performs regression instead of
classification. It is trained on Arabic, Chi-
nese, and Urdu data from the MT-Eval 2008
dataset.
Due to installation issues, the reported scores for
this metric have not been verified to produce iden-
tical scores at NIST.
University of Maryland11
? TER-plus (TERp) ? An extension of the
Translation Edit Rate (TER) metric that mea-
sures the number of edits between a hypoth-
esized translation and a reference translation.
TERp extends TER by using stemming, syn-
onymy, and paraphrases as well as tunable
edit costs to better measure the distance be-
tween the two translations. This version
of TERp improves upon prior versions by
adding brevity and length penalties.
11http://www.umiacs.umd.edu/?snover/
terp
30
Scores were not submitted along with this metric,
and due to installation issues were not produced at
NIST in time to be included in this report.
University Polite`cnica de Catalunya/University
de Barcelona (Comelles et al, 2010)
? DR ? An arithmetic mean over a set of
three metrics based on discourse representa-
tions, respectively computing lexical overlap,
morphosyntactic overlap, and semantic tree
matching.
? DRdoc ? Is analogous to DR but, instead of
operating at the segment level, it analyzes
similarities over whole document discourse
representations.
? ULCh ? An arithmetic mean over a
heuristically-defined set of metrics operat-
ing at different linguistic levels (ROUGE,
METEOR, and measures of overlap between
constituent parses, dependency parses, se-
mantic roles, and discourse representations).
University of Southern California, ISI
? BEwT-E ? Basic Elements with Transfor-
mations for Evaluation, is a recall-oriented
metric that compares basic elements, small
portions of contents, between the two trans-
lations. The basic elements (BEs) consist
of content words and various combinations
of syntactically-related words. A variety of
transformations are performed to allow flexi-
ble matching so that words and syntactic con-
structions conveying similar content in dif-
ferent manners may be matched. The trans-
formations cover synonymy, preposition vs.
noun compounding, differences in tenses,
etc. BEwT-E was originally created for sum-
marization evaluation and is English-specific.
? Bkars ? Measures overlap between character
trigrams in the system and reference trans-
lations. It is heavily weighted toward recall
and contains a fragmentation penalty. Bkars
produces a score both with and without stem-
ming (using the Snowball package of stem-
mers) and averages the results together. It is
not English-specific.
Scores were not submitted for BEwT-E; the run-
time required for this metric to process the WMT-
10 data set prohibited the production of scores in
time for publication.
6 Evaluation task results
The results reported here are preliminary; a final
release of results will be published on the WMT10
website before July 15, 2010. Metric developers
submitted metrics for installation at NIST; they
were also asked to submit metric scores on the
WMT10 test set alng with their metrics. Not
all developers submitted scores, and not all met-
rics were verified to produce the same scores as
submitted at NIST in time for publication. Any
such caveats are reported with the description of
the metrics above.
The results reported here are limited to a com-
parison of metric scores on the full WMT10
test set with human assessments on the human-
assessed subset. An analysis comparing the hu-
man assessments with the automatic metrics run
only on the human-assessed subset will follow at
a later date.
The WMT10 system output used to generate
the reported metric scores was found to have im-
properly escaped characters for a small number of
segments. While we plan to regenerate the met-
ric scores with this issue resolved, we do not ex-
pect this to significantly alter the results, given the
small number of segments affected.
6.1 System Level Metric Scores
The tables in Appendix B list the metric scores
for the language pairs processed by each metric.
These first four tables present scores for transla-
tions out of English into Czech, French, German
and Spanish. In addition to the metric scores of
the submitted metrics identified above, we also
present (1) the ranking of the system as deter-
mined by the human assessments; and (2) the
metrics scores for two popular baseline metrics,
BLEU as calculated by NIST?s mteval software12
and the NIST score. For each method of system
measurement the absolute highest score is identi-
fied by being outlined in a box.
Similarly, the remaining tables in Appendix B
list the metric scores for the submitted metrics and
the two baseline metrics, and the ranking based
on the human assessments for translations into En-
glish from Czech, French, German and Spanish.
As some metrics employ language-specific re-
sources, not all metrics produced scores for all lan-
guage pairs.
12ftp://jaguar.ncsl.nist.gov/mt/
resources/mteval-v13a-20091001.tar.gz
31
cz-
en
fr-
en
de-
en
es-
en
avg
SemPOS .78 .77 .60 .95 .77
IQmt-DRdoc .61 .79 .65 .98 .76
SemPOS-BLEU .75 .70 .61 .96 .75
i-letter-BLEU .71 .70 .60 .98 .75
NIST .85 .72 .55 .86 .74
TESLA .70 .70 .60 .97 .74
MT-NCD .71 .72 .58 .95 .74
Bkars .71 .67 .58 .98 .74
ATEC-2.1 .71 .67 .59 .96 .73
meteor-next-rank .69 .68 .60 .96 .73
IQmt-ULCh .70 .64 .60 .99 .73
IQmt-DR .68 .67 .60 .97 .73
meteor-next-hter .71 .66 .59 .95 .73
meteor-next-adq .69 .67 .60 .96 .73
badger-2.0-lite .70 .70 .56 .94 .73
DCU-LFG .69 .69 .58 .96 .73
badger-2.0-full .69 .70 .57 .94 .73
SEPIA .71 .70 .57 .92 .73
SVM-rank .66 .65 .61 .98 .73
i-letter-recall .65 .64 .61 .98 .72
TESLA-M .67 .67 .57 .95 .72
BLEU-4-v13a .69 .68 .52 .90 .70
LRKB4 .63 .62 .53 .89 .67
LRHB4 .62 .65 .50 .87 .66
MT-mNCD .69 .64 .52 .70 .64
Stanford .58 .19 .60 .46 .46
Table 7: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation into English.
It is noticeable that system combinations are of-
ten among those achieving the highest scores.
6.2 System-Level Correlations
To assess the performance of the automatic met-
rics, we correlated the metrics? scores with the hu-
man rankings at the system level. We assigned a
consolidated human-assessment rank to each sys-
tem based on the number of times that the given
system?s translations were ranked higher than or
equal to the translations of any other system in
the manual evaluation of the given language pair.
We then compared the ranking of systems by the
human assessments to that provided by the au-
tomatic metric system level scores on the com-
plete WMT10 test set for each language pair, us-
ing Spearman?s ? rank correlation coefficient. The
correlations are shown in Table 7 for translations
to English, and Table 8 out of English, with base-
line metrics listed at the bottom. The highest cor-
relation for each language pair and the highest
overall average are bolded.
Overall, correlations are higher for translations
to English than compared to translations from En-
glish. For all language pairs, there are a number
of new metrics that yield noticeably higher corre-
en-
cz
en-
fr
en-
de
en-
es
avg
SVM-rank .29 .54 .68 .67 .55
TESLA-M .27 .49 .74 .66 .54
LRKB4 .39 .58 .47 .71 .54
i-letter-recall .28 .51 .61 .66 .52
LRHB4 .39 .59 .41 .63 .51
i-letter-BLEU .26 .49 .56 .65 .49
ATEC-2.1 .38 .52 .44 .62 .49
badger-2.0-full .37 .58 .41 .59 .49
Bkars .22 .54 .52 .66 .48
BLEU-4-v13a .35 .58 .39 .57 .47
badger-2.0-lite .32 .57 .41 .59 .47
TESLA .09 .62 .66 .50 .47
meteor-next-rank .34 .59 .39 .51 .46
Stanford .34 .48 .70 .32 .46
MT-NCD .17 .54 .51 .61 .46
NIST .30 .52 .41 .50 .43
MT-mNCD .26 .49 .17 .43 .34
SemPOS .31 n/a n/a n/a .31
SemPOS-BLEU .29 n/a n/a n/a .29
Table 8: The system-level correlation of the au-
tomatic evaluation metrics with the human judg-
ments for translation out of English.
lations with human assessments than either of the
two included baseline metrics. In particular, Bleu
performed in the bottom half of the into-English
and out-of-English directions.
6.3 Segment-Level Metric Analysis
The method employed to collect human judgments
of rank preferences at the segment level produces
a sparse matrix of decision points. It is unclear
whether attempts to normalize the segment level
rankings to 0.0?1.0 values, representing the rela-
tive rank of a system per segment given the num-
ber of comparisons it is involved with, is proper.
An intuitive display of how well metrics mirror the
human judgments may be shown via a confusion
matrix. We compare the human ranks to the ranks
as determined by a metric. Below, we show an ex-
ample of the confusion matrix for the SVM-rank
metric which had the highest summed diagonal
(occurrences when a particular rank by the met-
ric?s score exactly matches the human judgments)
for all segments translated into English. The num-
bers provided are percentages of the total count.
The summed diagonal constitutes 39.01% of all
counts in this example matrix. The largest cell is
the 1/1 ranking cell (top left). We included the
reference translation as a system in this analysis,
which is likely to lead to a lot of agreement on the
highest rank between humans and automatic met-
rics.
32
Metric Human Rank
Rank 1 2 3 4 5
1 12.79 4.48 2.75 1.82 0.92
2 2.77 7.94 5.55 3.79 2.2
3 1.57 4.29 6.74 5.4 4.46
4 0.97 2.42 3.76 4.99 6.5
5 0.59 1.54 1.84 3.38 6.55
No allowances for ties were made in this analy-
sis. That is, if a human ranked two system transla-
tions the same, this analysis expects the metrics to
provide the same score in order to get them both
correct. Future analysis could relax this constraint.
As not all human rankings start with the highest
possible rank of ?1? (due to ties and withholding
judgment on a particular system output being al-
lowed), we set the highest automatic metric rank
to the highest human rank and shifted the lower
metric ranks down accordingly.
Table 9 shows the summed diagonal percent-
ages of the total count of all datapoints for all met-
rics that WMT10 scores were available for, both
combined for all languages to English (X-English)
and separately for each language into English.
The results are ordered by the highest percent-
age for the summed diagonal on all languages
to English combined. There are quite noticeable
changes in ranking of the metrics for the separate
language pairs; further analysis into the reasons
for this will be necessary.
We plan to also analyze metric performance for
translation into English.
7 Feasibility of Using Non-Expert
Annotators in Future WMTs
In this section we analyze the data that we col-
lected data by posting the ranking task on Ama-
zon?s Mechanical Turk (MTurk). Although we did
not use this data when creating the official results,
our hope was that it may be useful in future work-
shops in two ways. First, if we find that it is pos-
sible to obtain a sufficient amount of data of good
quality, then we might be able to reduce the time
commitment expected from the system develop-
ers in future evaluations. Second, the additional
collected labels might enable us to detect signifi-
cant differences between systems that would oth-
erwise be insignificantly different using only the
data from the volunteers (which we will now refer
to as the ?expert? data).
7.1 Data collection
To that end, we prepared 600 ranking sets for each
of the eight language pairs, with each set con-
taining five MT outputs to be ranked, using the
same interface used by the volunteers. We posted
the data to MTurk and requested, for each one,
five redundant assignments, from different work-
ers. Had all the 5? 8? 600 = 24,000 assignments
been completed, we would have obtained 24,000
? 5 = 120,000 additional rank labels, compared
to the 37,884 labels we collected from the volun-
teers (Table 3). In actuality, we collected closer to
55,000 rank labels, as we discuss shortly.
To minimize the amount of data that is of poor
quality, we placed two requirements that must be
satisfied by any worker before completing any of
our tasks. First, we required that a worker have an
existing approval rating of at least 85%. Second,
we required a worker to reside in a country where
the target language of the task can be assumed to
be the spoken language. Finally, anticipating a
large pool of workers located in the United States,
we felt it possible for us to add a third restriction
for the *-to-English language pairs, which is that a
worker must have had at least five tasks previously
approved on MTurk.13 We organized the ranking
sets in groups of 3 per screen, with a monetary re-
ward of $0.05 per screen.
When we created our tasks, we had no expecta-
tion that all the assignments would be completed
over the tasks? lifetime of 30 days. This was in-
deed the case (Table 10), especially for language
pairs with a non-English target language, due to
workers being in short supply outside the US.
Overall, we see that the amount of data collected
from non-US workers is relatively small (left half
of Table 10), whereas the pool of US-based work-
ers is much larger, leading to much higher com-
pletion rates for language pairs with English as the
target language (right half of Table 10). This is in
spite of the additional restriction we placed on US
workers.
13We suspect that newly registered workers on MTurk al-
ready start with an ?approval rating? of 100%, and so requir-
ing a high approval rating alone might not guard against new
workers. It is not entirely clear if our suspicion is true, but our
past experiences with MTurk usually involved a noticeably
faster completion rate than what we experienced this time
around, indicating our suspicion might very well be correct.
33
Metric *-English Czech-English French-English German-English Spanish-English
SVM-rank 39.01 41.21 36.07 38.81 40.3
i-letter-recall 38.85 41.71 36.19 38.8 39.5
MT-NCD 38.77 42.55 35.31 38.7 39.48
i-letter-BLEU 38.69 40.54 36.05 38.82 39.64
meteor-next-rank 38.5 40.1 34.41 39.25 40.05
meteor-next-adq 38.27 39.58 34.41 39.5 39.35
meteor-next-hter 38.21 38.61 34.1 39.13 40.18
Bkars 37.98 40.1 35.08 38.6 38.52
Stanford 37.97 39.87 36.19 38.27 38.09
ATEC-2.1 37.95 40.06 34.96 38.6 38.53
TESLA 37.57 38.68 34.38 38.67 38.36
NIST 37.47 39.54 35.54 37.13 38.2
SemPOS 37.21 38.8 37.39 35.73 37.69
SemPOS-BLEU 37.16 38.05 36.57 37.11 37.21
badger-2.0-full 37.12 37.5 36 36.21 38.62
badger-2.0-lite 37.08 37.2 35.88 36.23 38.69
SEPIA 37.06 38.98 34.6 36.46 38.52
BLEU-4-v13a 36.71 37.83 34.84 36.44 37.81
LRHB4 36.14 38.35 34.65 34.24 37.93
TESLA-M 36.13 37.01 34 35.79 37.6
LRKB4 36.12 38.72 33.47 35.25 37.63
IQmt-ULCh 35.86 37.64 33.95 35.81 36.45
IQmt-DR 35.77 36.27 34.43 34.43 37.74
DCU-LFG 34.72 36.38 32.29 33.87 36.49
MT-mNCD 34.51 34.93 31.78 35.73 35.13
IQmt-DRdoc 31.9 33.85 28.99 32.9 32.18
Table 9: The segment-level performance for metrics for the into-English direction.
en-de en-es en-fr en-cz de-en es-en fr-en cz-en
Location DE ES/MX FR CZ US US US US
Completed 1 time 37% 38% 29% 19% 3.5% 1.5% 14% 2.0%
Completed 2 times 18% 14% 12% 1.5% 6.0% 5.5% 19% 4.5%
Completed 3 times 2.5% 4.5% 0.5% 0.0% 8.5% 11% 20% 10%
Completed 4 times 1.5% 0.5% 0.5% 0.0% 22% 19% 23% 17%
Completed 5 times 0.0% 0.5% 0.0% 0.0% 60% 63% 22% 67%
Completed ? once 59% 57% 42% 21% 100% 99% 96% 100%
Label count 2,583 2,488 1,578 627 12,570 12,870 9,197 13,169
(% of expert data) (38%) (96%) (40%) (9%) (241%) (228%) (222%) (490%)
Table 10: Statistics for data collected on MTurk for the ranking task. In total, 55,082 rank labels were
collected across the eight language pairs (145% of expert data). Each language pair had 600 sets, and
we requested each set completed by 5 different workers. Since each set provides 5 labels, we could have
potentially obtained 600 ? 5 ? 5 = 15,000 labels for each language pair. The Label count row indicates
to what extent that potential was met (over the 30-day lifetime of our tasks), and the ?Completed...? rows
give a breakdown of redundancy. For instance, the right-most column indicates that, in the cz-en group,
2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5
workers, with 100% of the sets completed at least once. The total cost of this data collection effort was
roughly $200.
34
INTER-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.466 0.198 0.487
Without references 0.441 0.161 0.439
INTRA-ANNOTATOR AGREEMENT
P (A) K K?
With references 0.539 0.309 0.633
Without references 0.538 0.307 0.601
Table 11: Inter- and intra-annotator agreement for
the MTurk workers on the sentence ranking task.
(As before, P (E) is 0.333.) For comparison, we
repeat here the kappa coefficients of the experts
(K?), taken from Table 4.
7.2 Quality of MTurk data
It is encouraging to see that we can collect a large
amount of rank labels from MTurk. That said, we
still need to guard against data from bad work-
ers, who are either not being faithful and click-
ing randomly, or who might simply not be compe-
tent enough. Case in point, if we examine inter-
and intra-annotator agreement on the MTurk data
(Table 11), we see that the agreement rates are
markedly lower than their expert counterparts.
Another indication of the presence of bad work-
ers is a low reference preference rate (RPR),
which we define as the proportion of time a ref-
erence translation wins (or ties in) a comparison
when it appears in one. Intuitively, the RPR
should be quite high, since it is quite rare that an
MT output ought to be judged better than the refer-
ence. This rate is 96.5% over the expert data, but
only 83.7% over the MTurk data. Compare this
to a randomly-clicking RPR of 66.67% (because
the two acceptable answers are that the reference
is either better than a system?s output or tied with
it).
Also telling would be the rate at which MTurk
workers agree with experts. To ensure that we ob-
tain enough overlapping data to calculate such a
rate, we purposely select one-sixth14 of our rank-
ing sets so that the five-system group is exactly one
that has been judged by an expert. This way, at
least one-sixth of the comparisons obtained from
an MTurk worker?s labels are comparisons for
14This means that on average Turkers ranked a set of sys-
tem outputs that had been ranked by experts on every other
screen, since each screen?s worth of work had three sets.
which we already have an expert judgment. When
we calculate the rate of agreement on this data,
we find that MTurk workers agree with the ex-
pert workers 53.2% of the time, or K = 0.297, and
when references are excluded, the agreement rate
is 50.0%, or K = 0.249. Ideally, we would want
those values to be in the 0.4?0.5 range, since that
is where the inter-annotator kappa coefficient lies
for the expert annotators.
7.3 Filtering MTurk data by agreement with
experts
We can use the agreement rate with experts to
identify MTurk workers who are not performing
the task as required. For each worker w of the
669 workers for whom we have such data, we
compute the worker?s agreement rate with the ex-
perts, and from it a kappa coefficient Kexp(w) for
that worker. (Given that P (E) is 0.333, Kexp(w)
ranges between?0.5 and +1.0.) We sort the work-
ers based on Kexp(w) in ascending order, and ex-
amine properties of the MTurk data as we remove
the lowest-ranked workers one by one (Figure 4).
We first note that the amount of data we ob-
tained from MTurk is so large, that we could af-
ford to eliminate close to 30% of the labels, and
we would still have twice as much data than us-
ing the expert data alone. We also note that two
workers in particular (the 103rd and 130th to be
removed) are likely responsible for the majority
of the bad data, since removing their data leads to
noticeable jumps in the reference preference rate
and the inter-annotator agreement rate (right two
curves of Figure 4). Indeed, examining the data for
those two workers, we find that their RPR values
are 55.7% and 51.9%, which is a clear indication
of random clicking.15
Looking again at those two curves shows de-
grading values as we continue to remove workers
in large droves, indicating a form of ?overfitting?
to agreement with experts (which, naturally, con-
tinues to increase until reaching 1.0; bottom left
curve). It is therefore important, if one were to fil-
ter out the MTurk data by removing workers this
way, to choose a cutoff carefully so that no crite-
rion is degraded dramatically.
In Appendix A, after reporting head-to-head
comparisons using only the expert data, we also
report head-to-head comparisons using the expert
15In retrospect, we should have performed this type of
analysis as the data was being collected, since such workers
could have been identified early on and blocked.
35
-20
40
60
80
100
120
140
160
0 100 200 300 400 500 600 700
# Workers Removed
M
T
u
r
k
 
D
a
t
a
 
R
e
m
a
i
n
i
n
g
(
%
 
o
f
 
E
x
p
e
r
t
 
D
a
t
a
)
-
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0 100 200 300 400 500 600 700
# Workers Removed
A
g
r
e
e
m
e
n
t
 
w
i
t
h
 
E
x
p
e
r
t
 
D
a
t
a
 
(
k
a
p
p
a
)
82%
84%
86%
88%
90%
92%
94%
96%
98%
100%
0 100 200 300 400 500 600 700
# Workers Removed
R
e
f
e
r
e
n
c
e
 
P
r
e
f
e
r
e
n
c
e
 
R
a
t
e
0.10
0.15
0.20
0.25
0.30
0 100 200 300 400 500 600 700
# Workers Removed
I
n
t
e
r
-
A
n
n
o
t
a
t
o
r
 
A
g
r
e
e
m
e
n
t
 
(
k
a
p
p
a
)
Figure 4: The effect of removing an increasing number of MTurk workers. The order in which workers
are removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).
data combined with the MTurk data, in order to
be able to detect more significant differences be-
tween the systems. We choose the 300-worker
point as a reasonable cutoff point before combin-
ing the MTurk data with the expert data, based
on the characteristics of the MTurk data at that
point: a high reference preference rate, high inter-
annotator agreement, and, critically, a kappa co-
efficient vs. expert data of 0.449, which is close
to the expert inter-annotator kappa coefficient of
0.439.
7.4 Feasibility of using only MTurk data
In the previous subsection, we outlined an ap-
proach by which MTurk data can be filtered out
using expert data. Since we were to combine the
filtered MTurk data with the expert data to ob-
tain more significant differences, it was reason-
able to use agreement with experts to quantify the
MTurk workers? competency. However, we also
would like to know whether it is feasible to use the
MTurk data alone. Our aim here is not to boost the
differences we see by examining expert data, but
to eliminate our reliance on obtaining expert data
in the first place.
We briefly examined some simple ways of fil-
tering/combining the MTurk data, and measured
the Spearman rank correlations obtained from the
MTurk data (alone), as compared to the rankings
obtained using the expert data (alone), and report
them in Table 12. (These correlations do not in-
clude the references.)
We first see that even when using the MTurk
data untouched, we already obtain relatively high
correlation with expert ranking (?Unfiltered?).
This is especially true for the *-to-English lan-
guage pairs, where we collected much more data
than English-to-*. In fact, the relationship be-
tween the amount of data and the correlation val-
ues is very strong, and it is reasonable to expect
the correlation numbers for English-to-* to catch
up had more data been collected.
We also measure rank correlations when apply-
ing some simple methods of cleaning/weighting
MTurk data. The first method (?Voting?) is per-
forming a simple vote whenever redundant com-
parisons (i.e. from different workers) are avail-
able. The second method (?Kexp-filtered?) first re-
moves labels from the 300 worst workers accord-
ing to agreement with experts. The third method
36
(?RPR-filtered?) first removes labels from the 62
worst workers according to their RPR. The num-
bers 300 and 62 were chosen since those are the
points at which the MTurk data reaches the level
of expert data in the inter-annotator agreement and
RPR of the experts.
The fourth and fifth methods (?Weighted by
Kexp? and ?Weighted by K(RPR)?) do not re-
move any data, instead assigning weights to work-
ers based on their agreement with experts and their
RPR, respectively. Namely, for each worker, the
weight assigned by the fourth method is Kexp for
that worker, and the weight assigned by the fifth
method is K(RPR) for that worker.
Examining the correlation coefficients obtained
from those methods (Table 12), we see mixed re-
sults, and there is no clear winner among those
methods. It is also difficult to draw any conclusion
as to which method performs best when. However,
it is encouraging to see that the two RPR-based
methods perform well. This is noteworthy, since
there is no need to use expert data to weight work-
ers, which means that it is possible to evaluate a
worker using inherent, ?built-in? properties of that
worker?s own data, without resorting to making
comparisons with other workers or with experts.
8 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic eval-
uation of machine translation performance for
translating from European languages into English,
and vice versa.
The number of participants grew substantially
compared to previous editions of the WMT work-
shop, with 33 groups from 29 institutions partic-
ipating in WMT10. Most groups participated in
the translation task only, while the system combi-
nation task attracted a somewhat smaller number
of participants
Unfortunately, fewer rule-based systems partic-
ipated in this year?s edition of WMT, compared
to previous editions. We hope to attract more
rule-based systems in future editions as they in-
crease the variation of translation output and for
some language pairs, such as German-English,
tend to outperform statistical machine translation
systems.
This was the first time that the WMT workshop
was held as a joint workshop with NIST?s Metric-
sMATR evaluation initiative. This joint effort was
very productive as it allowed us to focus more on
the two evaluation dimensions: manual evaluation
of MT performance and the correlation between
manual metrics and automated metrics.
This year was also the first time we have in-
troduced quality assessments by non-experts. In
previous years all assessments were carried out
through peer evaluation exclusively consisting of
developers of machine translation systems, and
thereby people who are used to machine transla-
tion output. This year we have facilitated Ama-
zon?s Mechanical Turk to investigate two as-
pects of manual evaluation: How stable are man-
ual assessments across different assessor profiles
(experts vs. non-experts) and how reliable are
quality judgments of non-expert users? While
the intra- and inter-annotator agreements between
non-expert assessors are considerably lower than
for their expert counterparts, the overall rankings
of translation systems exhibit a high degree of cor-
relation between experts and non-experts. This
correlation can be further increased by applying
various filtering strategies reducing the impact of
unreliable non-expert annotators.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.16
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-
0022, and the US National Science Foundation un-
der grant IIS-0713448.
References
Alexandre Allauzen, Josep M. Crego, lknur Durgar El-
Kahlout, and Francois Yvon. 2010. Limsi?s statisti-
cal translation systems for wmt?10. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 29?34, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Lo??c Barrault. 2010. Many: Open source mt system
combination at wmt?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
16http://www.statmt.org/wmt09/results.
html
37
Label Unfiltered Voting Kexp-filtered RPR-filtered Weighted by Weighted by
count Kexp K(RPR)
en-de 2,583 0.862 0.779 0.818 0.862 0.868 0.862
en-es 2,488 0.759 0.785 0.797 0.797 0.768 0.806
en-fr 1,578 0.826 0.840 0.791 0.814 0.802 0.814
en-cz 627 0.833 0.818 0.354 0.833 0.851 0.828
de-en 12,570 0.914 0.925 0.920 0.931 0.933 0.926
es-en 12,870 0.934 0.969 0.965 0.987 0.978 0.987
fr-en 9,197 0.880 0.865 0.920 0.919 0.907 0.917
cz-en 13,169 0.951 0.909 0.965 0.944 0.930 0.944
Table 12: Spearman rank coefficients for the MTurk data across the various language pairs, using differ-
ent methods to clean the data or weight workers. (These correlations were computed after excluding the
references.) Kexp is the kappa coefficient of the worker?s agreement rate with experts, with P (A) = 0.33.
K(RPR) is the kappa coefficient of the worker?s RPR (see 7.2), with P (A) = 0.66. In Kexp-filtering,
42% of labels remain, after removing 300 workers. In K(RPR)-filtering, 69% of labels remain, after
removing 62 workers.
and MetricsMATR, pages 252?256, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Ergun Bicici and S. Serdar Kozat. 2010. Adaptive
model weighting and transductive regression for pre-
dicting best system combinations. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 257?262, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Ergun Bicici and Deniz Yuret. 2010. L1 regularized
regression for reranking and system combination in
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 263?270, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Alexandra Birch and Miles Osborne. 2010. Lrscore for
evaluating lexical and reordering quality in mt. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
302?307, Uppsala, Sweden, July. Association for
Computational Linguistics.
Ondrej Bojar and Kamil Kos. 2010. 2010 failures
in english-czech phrase-based mt. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 35?41, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creat-
ing speech and language data with amazons mechan-
ical turk. In Proceedings NAACL-2010 Workshop on
Creating Speech and Language Data With Amazons
Mechanical Turk, Los Angeles.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2007.
(Meta-) evaluation of machine translation. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation (WMT07), Prague, Czech Repub-
lic.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Proceedings of the Third Workshop on Statistical
Machine Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, , Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the findings
of the 2009 workshop on statistical machine trans-
lation. In Proceedings of the Fourth Workshop on
Statistical Machine Translation (WMT09), Athens,
Greece.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: Evaluating translation quality using amazon?s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2009), Singapore.
Elisabet Comelles, Jesus Gimenez, Lluis Marquez,
Irene Castellon, and Victoria Arranz. 2010.
Document-level automatic mt evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 308?313, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Hui Cong, Zhao Hai, Lu Bao-Liang, and Song Yan.
2010. An empirical study on development set se-
lection strategy for machine translation learning.
In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 42?46, Uppsala, Sweden, July. Association
for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2010. Meteor-
next and the meteor paraphrase tables: Improved
38
evaluation support for five target languages. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 314?
317, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Marcus Dobrinkat, Tero Tapiovaara, Jaakko Va?yrynen,
and Kimmo Kettunen. 2010. Normalized compres-
sion distance based measures for metricsmatr 2010.
In Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
318?323, Uppsala, Sweden, July. Association for
Computational Linguistics.
Jinhua Du, Pavel Pecina, and Andy Way. 2010. An
augmented three-pass system combination frame-
work: Dcu combination system for wmt 2010. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
271?276, Uppsala, Sweden, July. Association for
Computational Linguistics.
Vladimir Eidelman, Chris Dyer, and Philip Resnik.
2010. The university of maryland statistical ma-
chine translation system for the fifth workshop on
machine translation. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 47?51, Uppsala, Sweden,
July. Association for Computational Linguistics.
Christian Federmann, Andreas Eisele, Yu Chen, Sabine
Hunsicker, Jia Xu, and Hans Uszkoreit. 2010.
Further experiments with shallow hybrid mt sys-
tems. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 52?56, Uppsala, Sweden, July. Association
for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Germa?n Sanchis-Trilles, Joan-
Andreu Sa?nchez, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Pascual Mart??nez-Go?mez, Martha-Alicia
Rocha, and Francisco Casacuberta. 2010. The upv-
prhlt combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 277?
281, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Greg Hanneman, Jonathan Clark, and Alon Lavie.
2010. Improved features and grammar selection for
syntax-based mt. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 57?62, Uppsala, Sweden, July.
Association for Computational Linguistics.
Christian Hardmeier, Arianna Bisazza, and Marcello
Federico. 2010. Fbk at wmt 2010: Word lattices for
morphological reduction and chunk-based reorder-
ing. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 63?67, Uppsala, Sweden, July. Association
for Computational Linguistics.
Yifan He, Jinhua Du, Andy Way, and Josef van Gen-
abith. 2010. The dcu dependency-based metric in
wmt-metricsmatr 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 324?328, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Kenneth Heafield and Alon Lavie. 2010. Cmu multi-
engine machine translation for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 68?
73, Uppsala, Sweden, July. Association for Compu-
tational Linguistics.
Carmen Heger, Joern Wuebker, Matthias Huck, Gregor
Leusch, Saab Mansour, Daniel Stein, and Hermann
Ney. 2010. The rwth aachen machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 74?78, Uppsala, Sweden,
July. Association for Computational Linguistics.
Carlos A. Henr??quez Q., Marta Ruiz Costa-jussa`, Vi-
das Daudaravicius, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Using collocation segmentation to
augment the phrase table. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 79?83, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Almut Silja Hildebrand and Stephan Vogel. 2010.
Cmu system combination via hypothesis selection
for wmt?10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 282?285, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Ste?phane Huet, Julien Bourdaillet, Alexandre Patry,
and Philippe Langlais. 2010. The rali machine
translation system for wmt 2010. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 84?90, Upp-
sala, Sweden, July. Association for Computational
Linguistics.
Michael Jellinghaus, Alexandros Poulis, and David
Kolovratn??k. 2010. Exodus - exploring smt for eu
institutions. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 91?95, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL
2006 Workshop on Statistical Machine Translation,
New York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions, Prague, Czech Republic.
39
Philipp Koehn, Barry Haddow, Philip Williams, and
Hieu Hoang. 2010. More linguistic annotation for
statistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 96?101, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger
Schwenk. 2010. Lium smt machine translation
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 102?107, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Samuel Larkin, Boxing Chen, George Foster, Ulrich
Germann, Eric Joanis, Howard Johnson, and Roland
Kuhn. 2010. Lessons from nrcs portage system at
wmt 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 108?113, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Gregor Leusch and Hermann Ney. 2010. The rwth
system combination system for wmt 2010. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 290?
295, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, Athens, Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with
syntax, semirings, discriminative training and other
goodies. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 114?118, Uppsala, Sweden, July.
Association for Computational Linguistics.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. Tesla: Translation evaluation of sentences
with linear-programming-based analysis. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 329?
334, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Sushant Narsale. 2010. Jhu system combination
scheme for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 286?289, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Jan Niehues, Teresa Herrmann, Mohammed Mediani,
and Alex Waibel. 2010. The karlsruhe institute
for technology translation system for the acl-wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 119?123, Uppsala, Sweden, July. Association
for Computational Linguistics.
NIST. 2008. Evaluation plan for gale go/no-go phase
3 / phase 3.5 translation evaluations. June 18, 2008.
Sergio Penkale, Rejwanul Haque, Sandipan Dandapat,
Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du,
Pavel Pecina, Sudip Kumar Naskar, Mikel L. For-
cada, and Andy Way. 2010. Matrex: The dcu mt
system for wmt 2010. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 124?129, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Aaron Phillips. 2010. The cunei machine translation
platform for wmt ?10. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation
and MetricsMATR, pages 130?135, Uppsala, Swe-
den, July. Association for Computational Linguis-
tics.
Juan Pino, Gonzalo Iglesias, Adria` de Gispert, Graeme
Blackwood, Jamie Brunning, and William Byrne.
2010. The cued hifst system for the wmt10 trans-
lation shared task. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 136?141, Uppsala, Sweden,
July. Association for Computational Linguistics.
Marion Potet, Laurent Besacier, and Herve? Blanchon.
2010. The lig machine translation system for wmt
2010. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 142?147, Uppsala, Sweden, July. Association
for Computational Linguistics.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. Bbn system descrip-
tion for wmt10 system combination task. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 296?
301, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010.
Linear inversion transduction grammar alignments
as a second translation path. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 148?152, Uppsala,
40
Sweden, July. Association for Computational Lin-
guistics.
Germa?n Sanchis-Trilles, Jesu?s Andre?s-Ferrer, Guillem
Gasco?, Jesu?s Gonza?lez-Rubio, Pascual Mart??nez-
Go?mez, Martha-Alicia Rocha, Joan-Andreu
Sa?nchez, and Francisco Casacuberta. 2010.
Upv-prhlt english?spanish system for wmt10. In
Proceedings of the Joint Fifth Workshop on Statisti-
cal Machine Translation and MetricsMATR, pages
153?157, Uppsala, Sweden, July. Association for
Computational Linguistics.
Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.
2010. Incremental decoding for phrase-based sta-
tistical machine translation. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 197?204, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Lane Schwartz. 2010. Reproducible results in parsing-
based machine translation: The jhu shared task sub-
mission. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 158?163, Uppsala, Sweden, July.
Association for Computational Linguistics.
Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.
2010. Vs and oovs: Two problems for translation
between german and english. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 164?169, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Jo?rg Tiedemann. 2010. To cache or not to cache?
experiments with adaptive models in statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 170?175, Uppsala, Sweden,
July. Association for Computational Linguistics.
Sami Virpioja, Jaakko Va?yrynen, Andre Man-
sikkaniemi, and Mikko Kurimo. 2010. Apply-
ing morphological decompositions to statistical ma-
chine translation. In Proceedings of the Joint Fifth
Workshop on Statistical Machine Translation and
MetricsMATR, pages 176?181, Uppsala, Sweden,
July. Association for Computational Linguistics.
Zdene?k Z?abokrtsky?, Martin Popel, and David Marec?ek.
2010. Maximum entropy translation model in
dependency-based mt framework. In Proceedings
of the Joint Fifth Workshop on Statistical Machine
Translation and MetricsMATR, pages 182?187, Up-
psala, Sweden, July. Association for Computational
Linguistics.
Billy Wong and Chunyu Kit. 2010. The parameter-
optimized atec metric for mt evaluation. In Pro-
ceedings of the Joint Fifth Workshop on Statistical
Machine Translation and MetricsMATR, pages 335?
339, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Francisco Zamora-Martinez and Germa?n Sanchis-
Trilles. 2010. Uch-upv english?spanish system for
wmt10. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Met-
ricsMATR, pages 188?192, Uppsala, Sweden, July.
Association for Computational Linguistics.
Daniel Zeman. 2010. Hierarchical phrase-based mt
at the charles university for the wmt 2010 shared
task. In Proceedings of the Joint Fifth Workshop on
Statistical Machine Translation and MetricsMATR,
pages 193?196, Uppsala, Sweden, July. Association
for Computational Linguistics.
41
A Pairwise system comparisons by human judges
Tables 13?20 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row. Bolding indicates the winner of the two systems. The difference between
100 and the sum of the complimentary cells is the percent of time that the two systems were judged to
be equal.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
B Automatic scores
The tables on pages 33?32 give the automatic scores for each of the systems.
C Pairwise system comparisons for combined expert and non-expert data
Tables 21?20 show pairwise comparisons between systems for the into English direction when non-
expert judgments have been added.
The number of pairwise comparisons at the ? level of significance increases from 48 to 50, and the
number at the ? level of significants increases from 79 to 80 (basically same number). However, the
? level of significance went up considerably, from 280 to 369. That?s a 31% increase. 75 of ? are
comparisons involving the reference, then the non-reference ? count went up from 205 to 294, a 43%
increase.
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
D
C
U
-C
O
M
B
O
JH
U
-C
O
M
B
O
L
IU
M
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .00? .00? .00? .00? .04? .03? .00? .00? .00? .00? .04? .00? .04? .00? .00? .00? .00? .05? .06? .03? .09? .04? .04?
CAMBRIDGE .79? ? .36 .16? .12? .23? .27 .43 .26? .38 .24 .3 .28 .51 .34 .23 .37 .24 .32 .46 .24 .29 .45 .59? .44
CMU-STATXFER .84? .58 ? .16? .48 .14? .19 .39 .33 .54 .54? .50? .36 .50 .70? .55? .50 .46 .58? .67? .50 .56? .48 .58? .52?
CU-ZEMAN 1.00? .77? .72? ? .76? .37 .73? .74? .79? .77? .77? .81? .75? .94? .86? .77? .89? .67 .77? .79? .81? .81? .77? .96? .86?
DFKI 1.00? .72? .45 .12? ? .32 .48 .50 .52 .53 .56 .65 .53 .62 .55 .43 .61? .50 .68? .73? .70? .60 .59? .72? .71?
GENEVA 1.00? .69? .76? .48 .56 ? .47 .71? .79? .72? .79? .71? .68? .76? .83? .57 .86? .72? .71? .69? .76? .65? .88? .96? .70
HUICONG .86? .54 .29 .12? .26 .37 ? .48 .31 .43 .63? .62? .53 .55 .53? .44 .50 .55 .52 .68? .52? .51 .52? .57 .53
JHU .83? .39 .42 .13? .33 .19? .3 ? .3 .36 .56? .56? .47 .52 .46 .29 .36 .42 .42 .59? .50 .31 .43 .29 .37
LIG .97? .63? .36 .15? .37 .18? .40 .60 ? .62? .57? .39 .35 .54? .46 .33 .34 .38 .54? .48? .42 .44 .50 .61? .56
LIMSI .96? .41 .23 .19? .31 .17? .32 .50 .28? ? .35 .42 .21 .62? .25 .21 .33 .22 .42 .35 .43 .32 .26 .35 .41
LIUM .83? .33 .21? .13? .41 .05? .13? .15? .09? .3 ? .39 .19 .36 .43 .26 .23? .28 .29 .45 .28 .26 .28 .33 .28
NRC .96? .3 .10? .10? .32 .24? .15? .22? .22 .33 .43 ? .26 .58 .26 .24 .3 .50 .36 .45 .47? .23 .38 .36? .35
ONLINEA .96? .55 .57 .14? .42 .16? .42 .4 .39 .53 .52 .47 ? .52? .46 .36 .64 .57 .59 .50 .59 .42 .46 .43 .48
ONLINEB .87? .37 .33 .03? .29 .12? .31 .26 .16? .12? .39 .35 .20? ? .33 .38 .17? .36 .29 .21 .33 .3 .3 .32 .21?
RALI .89? .45 .15? .06? .35 .04? .12? .42 .35 .46 .32 .42 .39 .52 ? .32 .31 .26 .43 .41 .27 .43 .40 .63? .26
RWTH .91? .46 .21? .05? .51 .36 .44 .46 .53 .39 .48 .48 .39 .48 .48 ? .39 .38 .39 .52 .46 .53? .52 .50? .25
UEDIN .96? .40 .33 .03? .28? .03? .28 .29 .49 .38 .61? .3 .32 .50? .34 .24 ? .42 .33 .43 .48 .18? .13 .27 .38
BBN-C .90? .48 .46 .29 .39 .22? .27 .27 .46 .43 .28 .35 .33 .39 .29 .34 .26 ? .28 .44? .33 .26 .62? .36 .28
CMU-HEA-C .89? .50 .23? .14? .30? .21? .26 .25 .17? .33 .43 .16 .36 .43 .26 .29 .24 .24 ? .48 .27 .13 .25 .30 .15
CMU-HYP-C .81? .17 .19? .11? .19? .19? .14? .14? .19? .40 .23 .18 .29 .46 .35 .29 .21 .15? .17 ? .26 .18 .07? .32 .21
DCU-C .88? .27 .25 .11? .22? .24? .20? .28 .21 .35 .50 .10? .31 .44 .27 .29 .22 .21 .2 .30 ? .12? .26 .26 .08
JHU-C .86? .48 .16? .16? .33 .21? .35 .41 .32 .44 .39 .35 .39 .37 .26 .19? .50? .23 .32 .43 .40? ? .36 .27 .39
LIUM-C .87? .41 .36 .13? .31? .08? .21? .48 .31 .47 .44 .24 .39 .52 .28 .28 .33 .27? .25 .67? .26 .44 ? .54? .48
RWTH-C .88? .18? .13? .04? .22? .04? .14 .24 .25? .3 .33 .05? .43 .50 .30? .13? .23 .14 .18 .21 .19 .23 .11? ? .24
UPV-C .92? .25 .12? .10? .16? .3 .25 .34 .29 .31 .34 .29 .39 .65? .39 .36 .3 .45 .27 .36 .23 .16 .24 .28 ?
> others .90 .44 .31 .13 .33 .18 .29 .37 .34 .42 .44 .38 .37 .51 .41 .31 .38 .35 .38 .48 .39 .36 .40 .46 .37
>= others .98 .66 .51 .21 .42 .27 .51 .59 .53 .65 .71 .66 .52 .71 .65 .55 .65 .64 .70 .77 .72 .65 .64 .77 .68
Table 13: Sentence-level ranking for the WMT10 French-English News Task
42
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
F
K
I
E
U
G
E
N
E
V
A
JH
U
K
O
C
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .08? .02? .00? .04? .08? .13? .06? .09? .09? .07? .16? .11? .12? .12? .12? .05? .07? .08? .09?
CAMBRIDGE .82? ? .16? .24? .15? .07? .35 .10? .42 .36 .43 .27 .67? .46 .39 .44 .40 .46 .48? .40
CU-ZEMAN .98? .82? ? .47 .54? .62? .71? .41 .79? .82? .70? .67? .85? .90? .75? .72? .92? .82? .88? .82?
DFKI .95? .66? .31 ? .46 .25? .78? .36 .59 .62? .75? .65? .45 .56? .75? .69? .71? .63? .57 .65?
EU .96? .78? .30? .41 ? .55 .68? .16? .76? .72? .82? .67? .63? .86? .78? .78? .76? .76? .75? .71?
GENEVA .86? .81? .23? .55? .34 ? .65? .25? .65? .70? .69? .66? .77? .71? .70? .89? .75? .63? .84? .75?
JHU .77? .42 .15? .22? .22? .22? ? .06? .58? .47 .52? .49 .70? .61? .53 .64? .53? .65? .68? .50
KOC .85? .67? .4 .58 .55? .69? .82? ? .76? .85? .81? .72? .86? .82? .86? .85? .77? .77? .74? .79?
LIMSI .84? .23 .08? .29 .09? .30? .21? .08? ? .33 .37 .17? .51 .40 .29 .45 .49 .40 .61? .28
LIUM .85? .39 .07? .32? .11? .21? .44 .07? .46 ? .44 .4 .32 .44 .37 .64? .35 .40 .35 .42
NRC .91? .43 .15? .20? .11? .25? .21? .09? .31 .45 ? .32 .48 .44 .49 .61? .52? .30 .58? .40
ONLINEA .80? .51 .21? .33? .23? .15? .41 .14? .60? .42 .54 ? .52? .56? .36 .67? .61? .45 .50 .44
ONLINEB .87? .23? .08? .43 .23? .11? .12? .08? .27 .36 .43 .25? ? .38 .31 .33 .52 .33? .46 .29
RALI .83? .38 .05? .27? .11? .15? .22? .10? .36 .44 .49 .31? .50 ? .38 .44 .42 .37 .38 .34
RWTH .76? .33 .11? .12? .15? .17? .34 .05? .34 .44 .29 .42 .49 .40 ? .56 .48 .44 .53? .50
UEDIN .84? .29 .20? .17? .12? .09? .19? .07? .33 .23? .24? .24? .56 .31 .3 ? .36? .27 .51 .18?
CMU-HEAFIELD-COMBO .90? .23 .04? .23? .18? .12? .22? .11? .32 .41 .20? .23? .28 .31 .31 .11? ? .29 .24 .3
KOC-COMBO .91? .26 .08? .31? .17? .28? .20? .07? .23 .26 .19 .36 .57? .37 .32 .32 .42 ? .38 .34
RWTH-COMBO .85? .21? .02? .36 .16? .07? .12? .07? .16? .3 .30? .4 .34 .32 .06? .26 .35 .16 ? .21?
UPV-COMBO .87? .38 .08? .30? .19? .19? .37 .11? .39 .24 .33 .37 .44 .27 .34 .46? .35 .28 .50? ?
> others .87 .43 .15 .30 .22 .25 .38 .13 .44 .45 .46 .41 .53 .49 .44 .52 .53 .45 .53 .45
>= others .92 .63 .26 .40 .32 .35 .53 .26 .66 .63 .62 .55 .68 .66 .63 .70 .74 .68 .75 .66
Table 14: Sentence-level ranking for the WMT10 English-French News Task
43
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
C
M
U
-H
Y
P
O
S
E
L
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .03? .00? .06? .03? .00? .00? .05? .00? .00? .03? .06? .09? .06? .00? .09? .03? .03? .14? .03? .06? .03? .03? .06? .00?
AALTO 1.00? ? .50 .31 .60 .69? .39 .41 .71? .31 .45 .60? .59? .65? .66? .64? .81? .45 .41 .69? .72? .75? .55 .55? .76? .57?
CMU .93? .31 ? .29 .49 .57? .38 .50 .74? .13? .44 .59? .57? .59? .60? .67? .59? .41 .50 .68? .67? .46 .64? .55? .67? .54?
CU-ZEMAN 1.00? .44 .56 ? .58 .64? .17 .44 .75? .38 .50 .54? .76? .79? .73? .72? .72? .50? .73? .78? .80? .68? .72? .62? .68? .73?
DFKI .92? .25 .32 .27 ? .53 .36 .46 .65? .07? .50 .47 .47 .69? .56 .35 .55 .58 .47 .67? .61? .52 .47 .38 .67? .51
FBK .97? .20? .16? .14? .38 ? .11? .31 .45 .10? .22? .36 .50 .57? .37 .43 .40 .12? .17? .48? .43 .35 .38 .22 .38 .39
HUICONG .93? .35 .28 .46 .43 .75? ? .52 .69? .16? .39 .42 .64? .79? .31 .51? .78? .27 .41 .49 .74? .68? .60? .37 .68? .56?
JHU .86? .34 .29 .16 .43 .31 .26 ? .61? .15? .35 .36 .45 .69? .52? .56? .64? .27 .36 .70? .53 .47 .66? .52 .68? .44
KIT .89? .21? .10? .14? .29? .33 .19? .14? ? .03? .27 .21? .36 .46 .17? .29 .24 .25? .25? .48 .23? .31 .38 .2 .36 .12?
KOC .96? .58 .77? .48 .70? .77? .58? .71? .97? ? .77? .90? .72? .82? .76? .84? .81? .84? .66? .83? .87? .79? .77? .75? .93? .71?
LIMSI 1.00? .23 .28 .35 .35 .53? .33 .45 .41 .19? ? .49 .48 .63? .49 .63? .52 .36 .29 .73? .53? .45 .59? .29 .56? .59?
LIU .88? .12? .15? .16? .39 .21 .46 .36 .61? .00? .27 ? .44 .63? .49 .45 .53 .27? .33 .67? .55? .46 .44 .32 .37 .55
ONLINEA .92? .15? .23? .24? .42 .34 .21? .35 .50 .10? .32 .36 ? .41 .4 .44 .37 .32 .34 .36 .4 .47 .3 .26 .48 .41
ONLINEB .68? .18? .29? .17? .26? .24? .18? .23? .33 .18? .23? .27? .34 ? .3 .15? .29 .24? .15? .44 .28 .33? .20? .21? .38 .3
RWTH .88? .17? .20? .20? .37 .49 .41 .23? .61? .16? .4 .3 .43 .56 ? .39 .50 .26 .49 .37 .29 .34 .41 .26 .44 .2
UEDIN .89? .14? .22? .13? .62 .34 .18? .22? .39 .03? .17? .3 .44 .67? .42 ? .39 .15? .14? .52? .40 .36 .43 .26 .41 .38
UMD .91? .07? .14? .08? .36 .34 .11? .25? .48 .16? .24 .34 .52 .56 .41 .45 ? .16? .21? .41 .28 .29 .43 .29 .25 .23
UPPSALA .97? .32 .34 .17? .36 .54? .23 .37 .70? .00? .41 .62? .56 .68? .57 .64? .59? ? .2 .63? .69? .51? .60? .33 .69? .63?
UU-MS .82? .22 .43 .14? .45 .51? .19 .21 .68? .14? .39 .52 .60 .64? .44 .53? .61? .28 ? .36 .58? .52? .53? .30 .64? .44
BBN-C .86? .25? .10? .07? .27? .17? .23 .18? .35 .07? .15? .12? .32 .41 .3 .19? .22 .15? .27 ? .39 .06? .23? .11? .21 .18?
CMU-HEA-C .87? .14? .15? .08? .29? .33 .04? .26 .53? .00? .20? .24? .44 .31 .46 .23 .53 .15? .13? .27 ? .40 .2 .14? .22 .28
CMU-HYP-C .94? .25? .24 .14? .44 .3 .15? .26 .47 .08? .45 .31 .42 .67? .24 .36 .46 .14? .21? .50? .32 ? .43 .28 .51? .42
JHU-C .97? .34 .11? .20? .29 .34 .29? .03? .38 .12? .07? .29 .55 .67? .34 .32 .23 .24? .24? .48? .40 .32 ? .27 .37 .31
KOC-C .88? .00? .23? .21? .53 .44 .29 .22 .43 .08? .36 .50 .53 .63? .39 .37 .39 .28 .19 .64? .61? .38 .55 ? .48? .46
RWTH-C .82? .09? .06? .29? .25? .25 .18? .18? .24 .03? .19? .26 .36 .54 .25 .26 .33 .06? .14? .29 .22 .23? .3 .17? ? .13?
UPV-C .97? .17? .21? .17? .36 .36 .23? .19 .67? .20? .18? .29 .41 .40 .40 .38 .48 .17? .31 .50? .43 .27 .27 .27 .65? ?
> others .91 .23 .25 .20 .39 .42 .24 .30 .53 .11 .31 .38 .47 .59 .42 .43 .48 .27 .30 .53 .49 .42 .44 .31 .51 .41
>= others .96 .42 .46 .36 .50 .66 .47 .53 .72 .23 .52 .59 .63 .73 .62 .66 .68 .51 .55 .77 .73 .65 .67 .59 .75 .64
Table 15: Sentence-level ranking for the WMT10 German-English News Task
R
E
F
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
S
F
U
U
E
D
IN
U
P
P
S
A
L
A
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .03? .06? .01? .02? .05? .00? .00? .01? .04? .03? .01? .01? .01? .02? .01? .01? .05? .06?
CU-ZEMAN .97? ? .85? .67? .62? .78? .58? .70? .64? .80? .85? .64? .52 .80? .61? .79? .69? .76? .73?
DFKI .89? .14? ? .36? .24? .38 .30? .27? .36? .36? .55 .35? .21? .41 .39 .46 .38? .47 .37?
FBK .97? .30? .59? ? .35? .42 .12? .36 .48 .48 .64? .39 .29? .46 .30? .44 .46 .48 .38
JHU .98? .27? .72? .57? ? .59? .30? .51 .53 .56? .65? .43 .39 .66? .45 .56 .61? .52 .47
KIT .92? .18? .55 .42 .29? ? .23? .32 .32? .43 .53? .41 .27? .43 .23? .41 .41 .42 .37
KOC 1.00? .37? .64? .82? .62? .70? ? .74? .74? .74? .82? .63? .48 .62? .65? .73? .67? .81? .71?
LIMSI .95? .27? .68? .39 .45 .49 .17? ? .49 .74? .70? .51 .28? .58? .32 .51 .53? .52? .31
LIU .95? .32? .59? .4 .36 .58? .21? .37 ? .39 .74? .33? .23? .55? .36? .49 .42 .46 .38
ONLINEA .95? .16? .55? .4 .36? .45 .21? .23? .50 ? .56? .38 .23? .41 .23? .48 .4 .50 .33?
ONLINEB .92? .12? .42 .26? .27? .33? .14? .23? .21? .32? ? .24? .14? .39 .19? .29? .27? .36 .32?
RWTH .98? .33? .61? .51 .47 .46 .30? .33 .52? .55 .71? ? .33? .57? .45 .40 .51? .47 .46
SFU .98? .42 .77? .66? .51 .69? .48 .68? .69? .72? .77? .56? ? .82? .53 .65? .69? .73? .62?
UEDIN .94? .17? .51 .4 .31? .49 .34? .25? .30? .52 .52 .36? .10? ? .33? .31 .42 .38 .22?
UPPSALA .97? .36? .55 .51? .47 .70? .25? .46 .57? .67? .71? .41 .38 .54? ? .53? .42 .58? .40
CMU-HEAFIELD-COMBO .96? .17? .49 .36 .36 .37 .21? .35 .49 .42 .64? .38 .28? .48 .28? ? .35 .46 .35
KOC-COMBO .99? .27? .56? .32 .27? .32 .23? .32? .41 .55 .64? .30? .21? .37 .36 .41 ? .34 .36
RWTH-COMBO .92? .17? .50 .34 .35 .41 .09? .25? .38 .4 .54 .38 .20? .42 .19? .28 .35 ? .16?
UPV-COMBO .93? .23? .58? .38 .36 .51 .23? .50 .49 .57? .60? .42 .28? .51? .3 .38 .46 .48? ?
> others .95 .24 .57 .44 .37 .48 .24 .39 .45 .51 .63 .40 .27 .51 .34 .45 .44 .49 .39
>= others .98 .28 .62 .56 .46 .60 .30 .51 .55 .59 .70 .51 .34 .62 .47 .59 .59 .65 .55
Table 16: Sentence-level ranking for the WMT10 English-German News Task
44
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .01? .01? .01? .00? .00? .00? .00? .00? .01? .02? .05? .01? .04?
CAMBRIDGE .95? ? .23? .14? .34? .31? .41 .34 .62? .45? .35 .40? .42 .22? .44
COLUMBIA .97? .58? ? .25? .52 .45 .59? .53? .65? .60? .47 .56? .55? .45 .58?
CU-ZEMAN .96? .71? .59? ? .60? .68? .79? .66? .75? .80? .66? .79? .78? .69? .75?
DFKI .97? .51? .37 .23? ? .43 .59? .52? .66? .62? .48 .53? .55? .55? .64?
HUICONG .95? .50? .34 .21? .41 ? .45 .50 .66? .61? .39 .50? .59? .40 .52?
JHU .98? .39 .22? .12? .30? .33 ? .37 .56? .51? .34 .39 .34? .22? .34
ONLINEA .96? .46 .37? .23? .32? .38 .44 ? .59? .53? .4 .50 .36 .30? .54?
ONLINEB .88? .25? .21? .16? .23? .21? .27? .23? ? .35 .24? .28? .34? .22? .36
UEDIN .96? .31? .28? .10? .25? .19? .25? .31? .48 ? .23? .27? .31 .23? .2
UPC .94? .47 .4 .20? .41 .33 .43 .46 .66? .56? ? .50? .52? .48? .49?
BBN-COMBO .95? .26? .31? .09? .32? .34? .33 .37 .54? .44? .33? ? .35 .24? .34
CMU-HEAFIELD-COMBO .91? .39 .21? .08? .34? .22? .16? .42 .57? .45 .31? .31 ? .14? .27
JHU-COMBO .95? .40? .32 .15? .36? .31 .44? .50? .66? .50? .32? .47? .43? ? .43?
UPV-COMBO .92? .35 .28? .16? .27? .23? .38 .28? .47 .30 .28? .26 .35 .25? ?
> others .95 .41 .30 .15 .33 .32 .39 .39 .56 .48 .34 .41 .43 .32 .43
>= others .99 .61 .45 .27 .45 .50 .61 .54 .70 .69 .51 .62 .66 .55 .66
Table 17: Sentence-level ranking for the WMT10 Spanish-English News Task
R
E
F
C
A
M
B
R
ID
G
E
C
U
-Z
E
M
A
N
D
C
U
D
F
K
I
JH
U
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
S
F
U
U
E
D
IN
U
P
V
U
C
H
-U
P
V
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .00? .02? .07? .15? .07? .02? .11? .14? .07? .07? .03? .06? .09? .06? .03? .07?
CAMBRIDGE .91? ? .28? .45 .38 .45 .11? .52 .61? .21? .52 .47 .35 .54 .51 .39 .49
CU-ZEMAN .95? .70? ? .79? .75? .85? .49 .83? .82? .74? .87? .67? .85? .81? .80? .70? .74?
DCU .93? .32 .21? ? .45 .32 .09? .70? .59 .24? .48 .38 .29 .32 .36 .24 .14?
DFKI .80? .41 .15? .45 ? .38 .12? .64? .57 .4 .57 .31 .41 .59 .50 .48 .47
JHU .90? .37 .10? .52 .56 ? .17? .67? .67? .26? .34 .3 .49 .54 .53? .47 .35
KOC .98? .87? .47 .88? .73? .76? ? .76? .87? .67? .83? .86? .90? .87? .90? .86? .86?
ONLINEA .82? .42 .08? .30? .18? .24? .20? ? .49 .36 .25? .17? .25? .45 .30? .29 .18?
ONLINEB .76? .26? .10? .32 .37 .22? .10? .34 ? .21? .28 .24? .32 .33 .22? .19? .27?
SFU .91? .54? .19? .67? .51 .63? .27? .64 .72? ? .74? .57? .68? .77? .71? .64? .46
UEDIN .91? .3 .08? .4 .38 .34 .14? .71? .49 .09? ? .34 .4 .58 .33 .3 .31
UPV .94? .34 .07? .41 .53 .54 .07? .73? .61? .27? .45 ? .37 .51 .44 .38 .48?
UCH-UPV .90? .55 .07? .58 .51 .41 .08? .69? .52 .24? .51 .46 ? .47 .41 .49 .49
CMU-HEAFIELD-COMBO .83? .29 .13? .37 .38 .35 .07? .48 .54 .08? .29 .26 .28 ? .17? .21? .21
KOC-COMBO .88? .27 .15? .40 .42 .24? .03? .62? .60? .15? .41 .27 .34 .53? ? .3 .40
RWTH-COMBO .92? .36 .21? .52 .33 .31 .10? .55 .65? .14? .37 .22 .41 .52? .48 ? .31
UPV-COMBO .91? .32 .13? .69? .4 .32 .09? .76? .52? .36 .38 .19? .31 .45 .35 .28 ?
> others .89 .39 .15 .48 .44 .41 .14 .61 .58 .29 .46 .36 .42 .51 .44 .39 .40
>= others .93 .54 .23 .61 .55 .55 .19 .69 .71 .40 .61 .55 .54 .68 .62 .59 .60
Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task
45
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .02? .03? .00? .02? .00? .03? .03? .04? .01? .04? .02?
AALTO .88? ? .49 .51 .22? .38 .64? .55? .57? .71? .64? .65? .59?
CMU .97? .35 ? .4 .14? .18? .59? .49? .45? .57? .50? .34 .43
CU-BOJAR .90? .33 .43 ? .12? .20? .64? .45 .45 .54? .42 .42 .41
CU-ZEMAN .99? .60? .77? .75? ? .56? .81? .78? .88? .79? .84? .84? .76?
ONLINEA .92? .46 .68? .59? .28? ? .65? .54? .72? .75? .58? .57? .66?
ONLINEB .97? .27? .28? .21? .10? .17? ? .25? .32 .22 .21? .32 .28
UEDIN .95? .28? .26? .38 .07? .22? .49? ? .60? .52? .33 .31 .32
BBN-COMBO .92? .31? .20? .39 .08? .15? .41 .16? ? .27 .25 .3 .26
CMU-HEAFIELD-COMBO .90? .13? .23? .25? .07? .15? .31 .23? .34 ? .18? .35 .28
JHU-COMBO .93? .20? .19? .33 .08? .25? .48? .39 .38 .52? ? .37 .42
RWTH-COMBO .92? .18? .37 .38 .13? .25? .34 .28 .43 .40 .26 ? .25
UPV-COMBO .96? .25? .36 .41 .11? .27? .45 .35 .37 .44 .31 .34 ?
> others .93 .28 .36 .38 .11 .23 .49 .38 .47 .48 .38 .40 .40
>= others .98 .43 .55 .55 .22 .37 .70 .61 .70 .71 .62 .65 .63
Table 19: Sentence-level ranking for the WMT10 Czech-English News Task
R
E
F
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
C
U
-Z
E
M
A
N
D
C
U
E
U
R
O
T
R
A
N
S
K
O
C
O
N
L
IN
E
A
O
N
L
IN
E
B
P
C
-T
R
A
N
S
P
O
T
S
D
A
M
S
F
U
U
E
D
IN
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
D
C
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-C
O
M
B
O
U
P
V
-C
O
M
B
O
REF ? .04? .04? .03? .01? .05? .03? .08? .04? .04? .03? .02? .02? .04? .08? .04? .07? .04?
CU-BOJAR .87? ? .46 .27? .12? .28? .16? .17? .44 .4 .11? .27? .41 .28 .52? .28 .42 .43
CU-TECTO .88? .36 ? .30? .23? .38 .17? .28? .56? .44 .29? .27? .36 .45 .51? .4 .58? .35
CU-ZEMAN .91? .58? .51? ? .38 .49 .19? .39 .62? .63? .36 .41 .48 .51? .58? .48? .54? .55?
DCU .98? .73? .52? .43 ? .59? .22? .47 .74? .63? .47? .53? .56? .77? .77? .62? .76? .71?
EUROTRANS .88? .61? .47 .33 .30? ? .10? .33 .51 .54? .25? .27? .49 .57? .59? .49 .57? .60?
KOC .93? .69? .67? .54? .49? .77? ? .54? .71? .70? .51? .55? .64? .72? .78? .65? .76? .78?
ONLINEA .91? .62? .57? .51 .39 .44 .24? ? .66? .62? .39 .43 .55? .60? .61? .59? .73? .61?
ONLINEB .91? .31 .29? .27? .13? .33 .14? .19? ? .44 .22? .09? .39 .19 .34 .24? .22? .39
PC-TRANS .88? .45 .43 .24? .26? .29? .21? .24? .49 ? .22? .27? .37 .43 .55? .33? .49 .41
POTSDAM .88? .60? .51? .40 .27? .59? .25? .47 .63? .64? ? .45 .52? .56? .69? .61? .70? .68?
SFU .95? .52? .56? .4 .30? .61? .27? .39 .65? .64? .29 ? .55? .54? .76? .53? .70? .60?
UEDIN .94? .39 .44 .33 .23? .32 .20? .26? .32 .49 .25? .26? ? .43 .57? .18 .46? .42
CMU-HEAFIELD-COMBO .91? .42 .39 .23? .10? .27? .14? .19? .23 .35 .24? .19? .28 ? .48? .28 .34 .29
DCU-COMBO .84? .23? .27? .23? .03? .31? .10? .21? .42 .31? .15? .10? .16? .20? ? .18? .27? .22?
KOC-COMBO .91? .37 .49 .25? .10? .39 .17? .32? .42? .55? .17? .27? .26 .33 .41? ? .32 .22
RWTH-COMBO .88? .29 .34? .28? .05? .26? .10? .17? .48? .43 .16? .15? .24? .33 .46? .36 ? .29
UPV-COMBO .92? .37 .52 .22? .09? .25? .10? .19? .28 .47 .15? .25? .33 .24 .49? .34 .39 ?
> others .91 .45 .44 .32 .20 .39 .16 .29 .49 .49 .25 .28 .40 .43 .54 .39 .50 .45
>= others .96 .66 .60 .50 .38 .54 .33 .44 .70 .62 .44 .45 .62 .69 .75 .66 .70 .68
Table 20: Sentence-level ranking for the WMT10 English-Czech News Task
46
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/. +,-0 +,/+ +,-1 +,-0 +,-. +,-1 +,/+ +,-/ +,2- +,3/ +,24 +,./ +,/3 +,--''567*8'* +,4+ +,/1 !"#$ !"#% !"#% !"#% !"#% !"#! !"## !"#& !"#' !"() !"'& !"*& !"%% !"#$ !"%)78&69:67*8'* !"&' +,/1 +,// +,/- +,/- +,/. +,/. +,/+ +,/- !"#& +,/+ !"() !"'& +,.; !"%% !"#$ +,-178& +,// +,// +,/. +,/2 +,/2 +,/3 +,/2 +,-; +,/2 +,/- +,-1 +,24 !"'& +,.. +,-3 +,/- +,-;7&6'*<"= +,// +,/; +,/. +,/+ +,/3 +,-0 +,/+ +,-/ +,/3 +,/. +,-4 +,2/ +,3/ +,.+ +,.4 +,/. +,-;7&6>?8"5 +,22 +,-0 +,/2 +,-/ +,-; +,-/ +,-/ +,.0 +,-/ +,-; +,-2 +,23 +,3. +,2+ +,.+ +,/+ +,-3<@&67*8'* +,;2 +,/4 !"#$ !"#% !"#% +,/. !"#% +,-1 +,/. +,// +,-0 +,21 !"'& +,./ +,-2 +,/. +,-4*5#A5?B +,.4 +,/- +,/2 +,-0 +,/+ +,-1 +,/+ +,-2 +,-4 +,-0 +,-- +,2- +,3- +,24 +,.- +,-0 +,-2*5#A5?C +,4+ !"#) !"#$ !"#% +,/. +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,.; +,-. +,/. +,-;=D)@67*8'* +,;/ +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/. +,/; +,-0 !"() !"'& +,./ +,-2 +,// +,-1&?EA5 +,;3 +,/4 +,/- +,/2 +,/. +,/3 +,/2 +,-1 +,/. +,// +,-0 +,21 +,3; +,.- +,-2 +,// +,-1&FG67*8'* +,;. +,/1 +,// !"#% !"#% +,/. +,/. +,-0 +,/- +,/; +,/+ !"() !"'& +,./ +,-. +,// +,-1!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-0 +,/2 -,04 !"%' +,+2 +,+4 +,24 +,-2 +,2/ +,/2 +,/2 +,31 ;,2+''567*8'* +,/- !"#& /,-; !"%+ !"!* +,33 !"** !"%+ !"*! !"#+ !"#+ !"(# &"'!78&69:67*8'* +,/- +,/4 /,-. !"%+ !"!* +,+0 !"** +,-1 !"*! +,/4 +,/4 !"(# ;,0178& +,/+ +,/. /,3+ +,-/ +,+2 +,+0 +,.3 +,-; +,21 +,/- +,/- +,22 ;,;;7&6'*<"= +,/2 +,// /,2. +,-/ +,+2 +,+/ +,.+ +,-/ +,24 +,// +,// +,30 ;,2-7&6>?8"5 +,-4 +,-0 -,44 +,.; +,+2 +,+. +,2. +,.4 +,22 +,-0 +,-0 +,3- /,-/<@&67*8'* +,/. +,// /,2/ +,-; +,+2 +,+. +,.2 +,-; +,20 +,/; +,/; +,2. 4,++*5#A5?B +,-0 +,/3 -,14 +,-2 +,+2 +,+4 +,24 +,-. +,2/ +,/2 +,/2 +,31 ;,3.*5#A5?C !"## !"#& /,-3 !"%+ !"!* !"'$ +,.2 +,-4 !"*! !"#+ !"#+ +,2- ;,3.=D)@67*8'* +,/- +,/; /,-+ !"%+ !"!* +,+4 +,.2 +,-4 +,20 +,/4 +,/4 +,2- 4,+;&?EA5 +,/. !"#& /,-+ !"%+ !"!* +,3- +,.2 +,-4 +,20 +,/4 +,/4 +,2. ;,;-&FG67*8'* +,/- !"#& #"%& !"%+ !"!* +,+4 !"** +,-4 +,20 !"#+ !"#+ +,2- ;,00
="5H
IJK)"5L*=E MNJFO(#?P?=(=?7"## KQR(J"5H MNKSB CNDM6N CH"=%
RNMNTJ("EU RNMNTJ(="5HRNMNTJ(@)?= KNVOB K?8VTK,-./012345670888((66((((((R?)=A7%(%&'8A)?E()*(WOKM(R?)=A7%RBMJ(2+3+X(F#&%()D*('"%?#A5?(8?)=A7%(!CSNY("5E(WOKM$,((K7*=?%(L*=(Z"##Z(!?5[=?(\RM3+()?%)%?)$("5E(Z%&'Z(!%&'%?)(*L()@?(@&8"5#]("%%?%%?E(E")"$("=?(%@*D5, SJ^C- SJ9C-
WOKMCSNY(G3."
RM6W_I RM68W_I C"E`?=(L&## C"E`?=(#A)? BMN_(2,3
I=E*7 YS_@O(#?P?=(CSNY
K?8VTK(CSNY I_Y6S:a
MNKSB(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 %&'% %&'* ./2.+"-'56789 ./00 ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* ./1. ./1; %&'* ./2.+-&*<=*+,-', ./>. ./0. %&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 ./1; %&'* ./2.+-&*?@A*+,-', %&(( ./0. %&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* %&-/ %&-) ./2> ./2.+-&*%B"BCD95 ./24 ./2: ./22 ./22 ./21 ./1; ./20 ./2: ./24 ./E; %&/( %&.( %&-. ./20 ./1:+&*F9-") ./E4 ./2E ./22 ./24 ./2. ./10 ./2E ./21 ./1: ./E1 %&/' %&,. %&.( ./23 ./117+&*+,-', ./>E %&+/ %&'( %&') %&'* %&'- %&') ./0E %&'- %&.. %&/* %&-. %&'% %&'* %&'/7G6 ./1E ./20 ./23 ./2E ./24 ./10 ./23 ./21 ./1; ./E2 ./4> ./3E ./1. ./22 ./1089)9H" ./E> ./22 ./2E ./1; ./1; ./11 ./2. ./24 ./10 ./E3 ./41 ./E> ./3> ./23 ./11?&6+,)8 ./24 ./20 ./22 ./22 ./21 ./2. ./20 ./2: ./24 ./E: ./4> ./3E ./11 ./22 ./1>I?&*+,-', ./02 ./0. %&'( ./2: %&'* ./23 ./2: ./04 ./23 ./3E %&/* ./1. ./1: %&'* ./2.I?& ./2; ./0. %&'( ./2> ./20 ./2E ./2: ./0. ./23 ./34 %&/* ./3: ./1> ./2> ./1;#68 ./23 ./2; ./20 ./22 ./22 ./24 ./20 ./2; ./2E ./3. ./4> ./30 ./12 ./20 ./1:#6-%6 ./02 ./2; ./20 ./2> ./20 ./2E ./2> ./0. ./23 ./34 ./4> ./3: ./1> ./2> ./1:#6&-*+,-', ./01 ./0. %&'( ./2: ./2> ./23 ./2: ./04 ./23 %&.. %&/* ./14 ./1; ./2> ./1;#6&- ./>4 ./0. %&'( ./2: ./2> ./23 %&') ./04 %&'- ./3E %&/* ./14 ./1; %&'* ./2.)5+ ./00 ./0. %&'( ./2> ./20 ./2E ./2: ./04 ./23 ./3E %&/* ./3; ./1: ./2> ./1;,)#6)9J ./2E ./2; ./20 ./22 ./22 ./2. ./20 ./2: ./24 ./3. ./4> ./30 ./11 ./21 ./10,)#6)9K ./>4 ./0. %&'( ./2: %&'* ./23 ./2: ./04 %&'- ./3E %&/* ./1E ./1; ./2> ./1;5"#6 ./02 ./0. ./20 ./2> ./20 ./2E ./2: ./04 %&'- ./3E %&/* ./1. ./1: ./2> ./1;5LB?*+,-', %&(( ./0. %&'( ./2; %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* ./2.5LB? ./22 ./2; ./20 ./2> ./20 ./2E ./2: ./0. ./23 ./34 ./4> ./3: ./10 %&'( ./1;&976) ./02 ./0. ./20 ./2> ./2> ./23 ./2: ./04 %&'- ./3E %&/* ./1. ./1; %&'* ./1;&AH*+,-', ./0: ./0. %&'( ./2: %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* %&'/!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', %&'* ./0. 2/:. ./24 %&%. ./.E %&.( ./2. %&.- %&+/ ./3. (&*/+"-'56789 ./2> ./0. 2/:. ./24 %&%. ./.2 %&.( ./2. ./33 %&+/ ./3. >/02+-&*<=*+,-', %&'* ./0. 2/:4 %&', %&%. ./.3 %&.( %&'/ ./31 %&+/ ./3. >/0;+-&*?@A*+,-', %&'* ./0. 2/:1 %&', %&%. ./.3 %&.( %&'/ %&.- %&+/ ./3. >/0;+-&*%B"BCD95 ./22 ./2> 2/20 ./1: %&%. ./.3 ./31 ./1; ./34 ./2: ./E2 >/40+&*F9-") ./24 ./21 2/3. ./1. ./.E ./.E ./E> ./1. ./E> ./22 ./4; 0/4E7+&*+,-', %&'* %&+/ 2/:> ./2E %&%. ./.3 %&.( %&'/ %&.- %&+/ %&./ >/>:7G6 ./24 ./20 2/10 ./10 ./.E ./.1 ./34 ./12 ./E; ./22 ./4; 0/.489)9H" ./1; ./22 2/E; ./12 ./.E ./.2 ./E; ./13 ./E> ./21 ./4> 2/0>?&6+,)8 ./22 ./2: 2/0. ./1: %&%. ./.3 ./33 ./1> ./34 ./2> ./E1 0/;;I?&*+,-', ./2> ./0. 2/>> ./24 %&%. ./.4 ./30 ./2. ./33 ./0. ./E; >/04I?& ./2> ./2; 2/>1 ./2. %&%. ./.: ./32 ./2. ./3E ./0. ./E: >/11#68 ./22 ./2: 2/0E ./1: %&%. ./.2 ./31 ./1; ./34 ./2: ./E0 >/43#6-%6 ./20 ./2; 2/>E ./24 %&%. ./.2 ./30 ./2. ./33 ./2; ./E> >/34#6&-*+,-', ./2> ./0. 2/>> ./24 %&%. ./.3 %&.( %&'/ ./33 ./0. ./E; >/>.#6&- %&'* %&+/ '&** %&', %&%. ./.0 %&.( %&'/ %&.- %&+/ ./E; >/2;)5+ ./2> ./0. 2/>; ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E; >/23,)#6)9J ./22 ./2: 2/21 ./2. %&%. ./.: ./33 ./1: ./34 ./2: ./E2 >/E;,)#6)9K %&'* ./0. 2/:. %&', %&%. %&// %&.( %&'/ %&.- %&+/ ./E; >/>35"#6 ./2> ./0. 2/:. %&', %&%. ./.0 ./30 ./2. ./33 ./0. ./E: >/115LB?*+,-', %&'* %&+/ 2/:> %&', %&%. ./.4 %&.( %&'/ %&.- %&+/ ./3. >/>35LB? ./20 ./2; 2/>3 ./2. %&%. ./.1 ./30 ./2. ./33 ./0. ./E: >/3;&976) ./2> ./0. 2/:E ./24 %&%. ./.1 ./30 ./2. ./33 %&+/ ./E: >/11&AH*+,-', %&'* %&+/ '&** %&', %&%. ./.3 %&.( ./2. %&.- %&+/ ./3. >/01
0123456738#9:5;(((**(((((M9B56+%(%&'-6N97(B,(OPQR(M9B56+%MJRS(E.4.T(A#&%(BL,('"%9#6)9(-9B56+%(!KUVW(")7(OPQR$/((Q+,59%(D,5(X"##X(!9)Y59(ZMR4.(B9%B%9B$(")7(X%&'X(!%&'%9B(,D(B?9(?&-")#@("%%9%%97(7"B"$("59(%?,L)/5")[ MR*O\] MR*-O\] K"7895(D&## K"7895(#6B9 ]\W*U=^ US_K1 US<K1Q9-`aQ Q9-`aQ(KUVW
P(#9N95(KUVW P(#9N95(59+"## QbM(S")[ RVQUJ(M RVQUJ QB")D,57
MVRVaS("7c MVRVaS(?B95 MVRVaS(5")[ QV`PJJRV\(E/4
KUVW(43" OPQRRVSA ]S ]57,+ WU\? KVLR*V K["5%
47
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,-. +,/- +,/0 +,/+ +,-1 +,-0 +,/+ +,/. +,-2 +,./ +,3/ +,./ +,0/ +,-4 +,02''567*8'* !"## !"$% +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. !"'( +,39 +,02 +,-2 !"$& !"&)78&6:;67*8'* +,90 +,/4 +,// !"$$ !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& !"&)78&6<=>67*8'* +,2/ +,/4 +,// +,/- +,/0 +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-378& +,-2 +,/- +,/3 +,/+ +,/+ +,-- +,/3 +,/0 +,-9 +,.2 +,32 +,0+ +,04 +,/3 +,047&6?@8"5 +,02 +,/3 +,/0 +,-9 +,-9 +,-. +,-4 +,/+ +,-/ +,.0 +,3- +,.+ +,0. +,/+ +,02ABC +,/+ +,/- +,/. +,-1 +,-4 +,-- +,/3 +,/. +,-9 +,.- +,32 +,.1 +,0/ +,/+ +,02DE +,22 +,/4 +,/- +,/3 +,/3 +,-1 +,// +,/9 +,/+ +,.9 +,32 +,03 +,-. +,/. +,01<&C7*5F +,-9 +,/0 +,/3 +,-1 +,-4 +,-0 +,/+ +,/. +,-9 +,./ +,3/ +,./ +,02 +,/+ +,02G<&67*8'* +,29 +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3G<& +,/0 +,// +,/. +,/. +,/3 +,-/ +,/3 +,/- +,-9 +,.9 +,32 +,.1 +,09 +,-1 +,09EC) +,9. +,/4 +,/- +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-+E*767*8'* +,/1 +,/2 +,/- +,/0 +,/. +,-4 +,/0 +,/2 +,-1 +,.4 +,32 +,03 +,-3 +,/. +,-+E*7 +,.0 +,-1 +,-9 +,-/ +,-/ +,09 +,-0 +,-/ +,-3 +,.3 +,30 +,.3 +,.4 +,-1 +,0/#C8%C +,/. +,/9 +,/- +,/. +,/3 +,-4 +,/- +,/2 +,/+ +,.4 +,32 +,0. +,-3 +,/. +,01#C& +,/1 +,/9 +,/- +,/3 +,/+ +,-9 +,/0 +,// +,-1 +,.9 +,32 +,0. +,-3 +,/. +,01*5#C5@H +,20 +,/9 +,/- +,/. +,/3 +,-4 +,/0 +,// +,-1 +,.9 +,32 +,03 +,-+ +,-1 +,02*5#C5@I +,90 !"$% !"$* !"$$ !"$& !"$) +,/9 !"*! !"$' !"'( !"(+ !"'# !"&# !"$& +,-3JK)<67*8'* +,9/ +,/4 +,// +,/- !"$& +,/+ +,/2 +,/1 +,/. +,0+ +,39 +,0/ +,-/ !"$& +,-3JK)< +,2. +,/4 +,// +,/0 +,/. +,-1 +,// +,/9 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-3&@AC5 +,22 +,/4 +,/- +,/0 +,/. +,/+ +,// +,/9 +,/3 +,.1 +,32 +,00 +,-0 +,/0 +,01&8A +,24 +,/4 +,// +,/0 +,/. +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,00 +,-- +,/0 +,-+&>>%"#" +,/3 +,// +,/0 +,/+ +,/+ +,-/ +,/3 +,/- +,-4 +,.2 +,3/ +,.4 +,04 +,/3 +,04&>L67*8'* +,2- +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3&&68% +,// +,// +,/. +,/+ +,-1 +,-/ +,/3 +,/0 +,-9 +,.2 +,3/ +,.1 +,09 +,/3 +,04!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$""#)* +,/+ +,/. /,+3 +,-3 +,+. +,+3 +,.1 +,-. +,./ +,/0 +,31 2,04''567*8'* +,// +,/9 /,-2 +,-4 !"!' +,+- +,0/ +,-9 +,0+ +,/4 !")* #")+78&6:;67*8'* +,// +,/9 /,-9 +,-4 !"!' +,+0 +,0- +,-2 +,0+ +,/4 !")* 9,3178&6<=>67*8'* +,/- +,/9 /,-2 +,-9 !"!' +,+- +,00 +,-2 +,.1 +,/4 +,./ 9,+178& +,/+ +,/0 /,+4 +,-. +,+. +,+0 +,0+ +,-- +,.2 +,/. +,.3 2,-97&6?@8"5 +,-4 +,/3 -,14 +,09 +,+. +,+. +,.2 +,01 +,.0 +,/3 +,32 /,93ABC +,-1 +,/0 /,3. +,-/ +,+. +,+2 +,03 +,-/ +,.4 +,/0 +,39 /,1/DE +,/- +,/9 /,-0 +,-9 !"!' +,+. +,00 +,-2 +,.4 +,/4 +,.. 2,29<&C7*5F +,/+ +,/0 /,33 +,-. +,+. +,+. +,0+ +,-0 +,.2 +,/. +,31 2,30G<&67*8'* +,/- +,/9 /,-0 +,-9 !"!' +,+. +,0- +,-2 +,.1 +,/4 +,./ 9,30G<& +,/3 +,/0 /,3. +,-. +,+. +,+/ +,03 +,-- +,.9 +,/- +,.3 2,90EC) +,/- +,/4 /,/3 +,-4 !"!' +,+/ +,0- +,-9 +,0+ +,/4 +,.- 2,13E*767*8'* +,/. +,// /,03 +,-/ +,+. +,+. +,00 +,-/ +,.4 +,/2 +,.- 2,44E*7 +,-0 +,-4 -,9. +,02 +,+. +,+3 +,.9 +,-+ +,.- +,-9 +,32 /,01#C8%C +,/0 +,/2 /,0/ +,-9 +,+. +,+- +,0. +,-2 +,.4 +,/9 +,.0 2,90#C& +,/0 +,/2 /,0- +,-2 +,+. +,+0 +,0. +,-2 +,.4 +,/2 +,.. 2,2+*5#C5@H +,/0 +,// /,.- +,-2 +,+. +,3+ +,00 +,-2 +,.1 +,/2 +,.3 2,44*5#C5@I !"$* !"*! $"#( !"$) !"!' !"(# !"'* !"&% !"') !"*( !")* 9,34JK)<67*8'* +,// +,/4 /,/+ +,-4 !"!' +,+. +,0/ +,-9 +,0+ +,/1 !")* 9,.+JK)< +,/- +,/9 /,-- +,-9 !"!' +,+0 +,00 +,-2 +,.1 +,/4 +,.- 2,12&@AC5 +,/- +,/4 /,/0 +,-1 !"!' +,+2 +,0- +,-4 +,0+ +,/4 +,.0 2,4+&8A +,// +,/4 /,/+ +,-4 !"!' +,+9 +,00 +,-9 +,.1 +,/4 +,.0 2,99&>>%"#" +,/3 +,/- /,.3 +,-0 +,+. +,+. +,03 +,-- +,.9 +,/- +,.3 2,/3&>L67*8'* +,/- +,/9 /,-/ +,-4 !"!' +,+0 +,0- +,-/ +,.1 +,/4 +,./ 9,3/&&68% +,/+ +,/- /,34 +,-- +,+. +,+. +,03 +,-/ +,.9 +,/- +,.+ 2,0.
,-./0123145678(((((66(((((M@)JC7%(%&'8CN@A()*(OPQR(M@)JC7%MHRS(.+3+T(>#&%()K*('"%@#C5@(8@)JC7%(!IUVW("5A(OPQR$,((Q7*J@%(X*J(Y"##Y(!@5ZJ@([MR3+()@%)%@)$("5A(Y%&'Y(!%&'%@)(*X()<@(<&8"5#=("%%@%%@A(A")"$("J@(%<*K5,J"5E MR6O\] MR68O\] I"AF@J(X&## I"AF@J(#C)@ ]\W6U;^ US_I- US:I-Q@8`aQ Q@8`aQ(IUVW
P(#@N@J(IUVW P(#@N@J(J@7"## QbM(S"5E RVQUH(M RVQUH Q)"5X*JA
MVRVaS("Ac MVRVaS(<)@J MVRVaS(J"5E QV`PHHRV\(.,3
IUVW(30" OPQRRVS> ]S ]JA*7 WU\< IVKR6V IE"J%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./01 ./02 ./34 ./35 ./35 ./36 ./0. ./07 ./33 ./76 ./24 ./67 ./37 ./35 !"#$+"-'89:;< ./02 ./0. ./34 ./34 ./3= ./37 ./35 ./01 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+-&*>?*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+,#&-'9" ./63 ./34 ./30 ./33 ./36 ./3. ./30 ./35 ./31 ./7. ./2= ./73 ./6= ./3= ./64+&*@<-") ./1= ./33 ./34 ./31 ./31 ./64 ./36 ./30 ./3. ./1= ./23 ./13 ./62 ./33 ./63:A9 ./63 ./34 ./33 ./36 ./37 ./65 ./30 ./34 ./32 ./14 ./24 ./73 ./60 ./30 ./6=B&9+,); ./3. ./3= ./30 ./30 ./33 ./32 ./34 ./02 ./37 ./72 ./2= ./73 ./6= ./3= ./64CB&*+,-', ./33 ./0. ./34 ./34 ./34 ./37 ./35 ./01 ./36 ./77 ./24 ./6. ./32 ./34 ./3.CB& ./02 ./0. ./3= ./3= ./3= ./37 ./35 ./01 ./36 ./77 ./24 ./75 ./3. ./34 ./65,)#9)"D ./36 ./0. ./3= ./3= ./3= ./37 ./34 ./02 ./36 ./71 ./2= ./75 ./64 ./33 ./60,)#9)<E !"%! !"&' !"#( !"&) !"&$ !"#& !"&) !"&# !"#% !"'& !")! !"*% !"## ./35 ./3.&<:9) ./05 ./02 ./3= ./34 ./34 ./36 ./0. ./07 ./33 ./77 ./24 ./61 ./31 ./35 !"#$&F+ ./32 ./35 ./30 ./33 ./33 ./32 ./34 ./0. ./37 ./72 ./2= ./7= ./64 ./34 ./65&FG*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0. ./07 ./33 ./76 ./24 ./61 ./37 !"&! !"#$!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$'')*+,-', ./35 ./02 3/51 ./31 !"!' ./.6 ./74 ./3. ./76 ./01 ./71 =/56+"-'89:;< ./34 ./0. 3/43 ./32 !"!' ./.= ./7= ./3. ./77 ./02 ./72 =/==+-&*>?*+,-', ./35 ./02 3/5. ./31 !"!' ./.6 ./7= ./3. ./76 ./01 ./72 =/4=+,#&-'9" ./33 ./3= 3/37 ./6= ./.1 ./.6 ./76 ./60 ./7. ./3= ./1= =/25+&*@<-") ./36 ./30 3/60 ./61 ./.1 ./.7 ./14 ./62 ./1= ./30 ./11 0/03:A9 ./36 ./34 3/07 ./3. ./.1 ./.4 ./76 ./6= ./72 ./34 ./17 0/0.B&9+,); ./3= ./35 3/=7 ./65 !"!' ./.3 ./76 ./64 ./72 ./35 ./10 =/7=CB&*+,-', ./34 ./0. 3/47 ./32 !"!' ./.2 ./7= ./3. ./77 ./02 ./7. =/42CB& ./34 ./0. 3/47 ./32 !"!' ./.4 ./70 ./65 ./77 ./02 ./15 =/=1,)#9)"D ./3= ./35 3/=. ./31 !"!' ./22 ./70 ./3. ./71 ./02 ./14 =/==,)#9)<E !"&$ !"&' &"!' !"#& !"!' !")# !"*! !"#' !"'& !"&* !"'' +"'(&<:9) ./35 ./01 3/5= ./36 !"!' ./21 ./74 ./32 ./76 ./01 ./7. =/04&F+ ./30 ./0. 3/=3 ./3. !"!' ./.7 ./73 ./64 ./71 ./0. ./10 =/13&FG*+,-', ./35 ./01 3/5= ./37 !"!' ./.6 ./74 ./3. ./76 ./01 ./72 =/41
,-./01234/560127((((**(((((H<I89+%(%&'-9J<:(I,(KLMN(H<I89+%HDNO(1.2.P(F#&%(IQ,('"%<#9)<(-<I89+%(!ERST("):(KLMN$/((M+,8<%(U,8(V"##V(!<)W8<(XHN2.(I<%I%<I$("):(V%&'V(!%&'%<I(,U(IB<(B&-")#Y("%%<%%<:(:"I"$("8<(%B,Q)/8")Z HN*K[\ HN*-K[\ E":;<8(U&## E":;<8(#9I< \[T*R?] RO^E6 RO>E6M<-_`M M<-_`M(ERST
L(#<J<8(ERST L(#<J<8(8<+"## MaH(O")Z NSMRD(H NSMRD MI")U,8:
HSNS`O(":b HSNS`O(BI<8 HSNS`O(8")Z MS_LDDNS[(1/2
ERST(27" KLMNNSOF \O \8:,+ TR[B ESQN*S EZ"8%
48
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /034 /035 /03/ /056 /076 !"#$ /076 /052 /038 /055)&+'.9": /011 /036 /035 /052 /051 /071 /057 /071 /056 /03/ /057)&+;<);. /01/ /036 /037 /051 /057 /075 /05/ /077 /05= /054 /058)&+><*"? /03/ /033 /03= /051 /057 /078 /072 /075 /05= /051 /072@)&+).*'. !"%$ !"$& !"$$ !"$' !"#( !")& !"#$ !")( !"$' !"$* !"#+@)& /074 /035 /038 /051 /057 /07= /072 /077 /058 /052 /057<&:.;:"?% /035 /035 /038 /057 /058 /0=2 /076 /078 /074 /054 /05=A.)+).*'. /011 /036 /035 /052 /051 /076 /057 /076 /056 /038 /053A.) /077 /03= /052 /055 /058 /0=2 /071 /077 /072 /052 /05=.?#B?<C /055 /033 /038 /053 /058 /078 /074 /077 /058 /054 /05=.?#B?<D /06/ /034 /035 /052 /051 /076 /055 /076 /054 /038 /053E)+;:"?% /01= /033 /038 /055 /058 /078 /076 /0=2 /071 /052 /05=E.;%@"* /055 /033 /038 /053 /05= /078 /074 /075 /05= /052 /05=:F;G+).*'. /06/ /036 /037 /03/ /056 /074 /055 /076 /052 /038 /053%H& /053 /033 /03= /055 /058 /078 /074 /077 /058 /052 /057&<@B? /01= /036 /037 /054 /053 /071 /05= /076 /056 /03/ /055&EI+).*'. /014 /034 /033 /03/ /056 /074 !"#$ /076 /052 /038 /053!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. !"#+ /056 504/ !"$$ +/0/= /0/3 /037 /086 301=)&+'.9": /053 /051 506/ /037 ,!"!' /0/1 /038 /081 3056)&+;<);. /057 /053 501= /037 ,!"!' /0/2 /03/ /087 30=3)&+><*"? /058 /05= 5071 /03/ ,!"!' /0/3 /056 /08= 5021@)&+).*'. /056 !"#( #"&! !"$$ ,!"!' !"'# !"$# !"'& $"&+@)& /05/ /058 50=3 /056 ,!"!' /0/3 /051 /087 5023<&:.;:"?% /05/ /05= 505= /038 ,!"!' /0/5 /051 /08/ 5055A.)+).*'. /053 /051 506/ /037 ,!"!' /0/= /038 /086 3034A.) /074 /05/ 50=5 /057 ,!"!' /0/7 /055 /08= 5037.?#B?<C /058 /057 5054 /038 ,!"!' /0/1 /056 /08= 5013.?#B?<D !"#+ !"#( 5046 /035 ,!"!' /08/ /037 /086 3031E)+;:"?% /058 /057 503= /03/ ,!"!' /0/3 /056 /08/ 5031E.;%@"* /05/ /05= 5074 /052 ,!"!' /0/7 /051 /08= 5065:F;G+).*'. !"#+ /056 5048 !"$$ ,!"!' /0/5 /038 /084 304=%H& /058 /057 5055 /03/ ,!"!' /0/5 /056 /088 5014&<@B? /053 /051 5067 /035 ,!"!' /088 /03= /081 3057&EI+).*'. !"#+ !"#( 5045 /035 ,!"!' /0/1 /037 /084 3044
:"?A
JKL;"?H.:@ MNKEO(#<P<:(:<)"## LQR(K"?A MNLSC DNFM+N DA":%
RNMNTK("@U RNMNTK(:"?ARNMNTK(G;<: LNVOC L<*VTL-./0123,45673888((++((((((R<;:B)%(%&'*B;<@(;.(WOLM(R<;:B)%RCMK(=/8/X(E#&%(;F.('"%<#B?<(*<;:B)%(!DSNY("?@(WOLM$0((L).:<%(H.:(Z"##Z(!<?[:<(\RM8/(;<%;%<;$("?@(Z%&'Z(!%&'%<;(.H(;G<(G&*"?#]("%%<%%<@(@";"$(":<(%G.F?0 SK^D5 SK,D5
WOLMDSNY(I87"
RM+W_J RM+*W_J D"@`<:(H&## D"@`<:(#B;< CMN_(=08
J:@.) YS_GO(#<P<:(DSNY
L<*VTL(DSNY J_Y+S-a
MNLSC(R
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0145 0146 !"#$ 0144 0164 0167 0148 0140)*&9:;9)<*'< 0156 0148 !"## !"#$ !"#% !"&$ !"#' !"%! !"#()&9=/*"> 01?2 014@ 0143 0165 0162 0138 0163 0146 0163-A, 0160 0146 014@ 0140 0167 0137 0166 0144 0162/& 013? 0143 014@ 0168 0165 0138 0166 0142 0162./>/B" 0134 0144 014@ 0167 0168 0135 016? 0146 0166CD& 0143 0145 0143 0146 0143 0166 0167 0148 0168E<)9)<*'< 0128 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(E<) 01?2 0143 014@ 0167 0168 0135 0163 0144 0162#,*%, 0122 0148 0146 0144 0146 0164 0167 0147 0140#,&* 0123 0148 0146 0142 0144 0162 !"#' !"%! 014@>+) 012? 0145 0146 0146 0143 0164 0167 0148 0167<>#,>/F 0144 0145 0143 0146 0143 0166 0168 0148 0167<>#,>/G 0128 0148 0146 !"#$ 0144 0162 !"#' !"%! 014@+"#, 0122 0148 0146 0142 0144 0162 0140 !"%! 014@+HID9)<*'< !"$# !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(+HID 0123 0148 0146 0144 0146 0164 0140 0147 014@&/-,> 0150 0148 0146 0142 0144 0164 0140 !"%! 014@&JB9)<*'< 0122 !"#) !"## !"#$ !"#% !"&$ !"#' !"%! !"#(!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0144 0145 4143 014? 9010? 0102 0148 01?8 5165)*&9:;9)<*'< !"#$ 0147 4156 0146 *!"!' 0103 !"%! !"+! 514?)&9=/*"> 0140 014? 4103 0160 90103 0103 014? 01@2 4163-A, 014@ 0146 41?7 0162 9010? 0102 0146 01?0 21@2/& 0140 0146 41?@ 0162 9010? 0103 0143 01@7 4176./>/B" 0140 014? 41@0 0168 90103 0104 014? 01@8 2106CD& 0144 0145 414@ 0140 9010? 0104 0145 01?6 2124E<)9)<*'< !"#$ 0147 4152 0146 *!"!' 0100 !"%! 01?7 5143E<) 0167 014@ 4102 0164 9010? 010? 014? 01?0 21@6#,*%, 0142 0148 4125 0146 9010? 0102 0148 01?5 51?0#,&* !"#$ 0147 4152 0146 *!"!' 0105 0147 01?7 5138>+) 0142 0148 412@ 014? *!"!' 0102 0148 01?5 51??<>#,>/F 0144 0145 4148 014? 9010? 0105 0148 01?4 2177<>#,>/G !"#$ !"%! #",' 0146 9010? !"'& !"%! 01?8 513?+"#, !"#$ 0147 4153 0146 *!"!' 0102 0147 01?8 513?+HID9)<*'< !"#$ 0147 4157 !"## *!"!' 0106 !"%! !"+! 512?+HID 0142 0148 4127 0143 *!"!' 0102 0147 01?8 51?8&/-,> 0142 0148 4150 0146 *!"!' 0107 0147 01?8 51?6&JB9)<*'< !"#$ 0147 4158 !"## *!"!' 0104 !"%! !"+! $"%#
-./0123*456.738(((99(((((K/I+,)%(%&'*,L/-(I<(MNOP(K/I+,)%KFPQ(?0@0R(J#&%(IH<('"%/#,>/(*/I+,)%(!GSTU(">-(MNOP$1((O)<+/%(V<+(W"##W(!/>X+/(YKP@0(I/%I%/I$(">-(W%&'W(!%&'%/I(<V(ID/(D&*">#Z("%%/%%/-(-"I"$("+/(%D<H>1+">E KP9M[\ KP9*M[\ G"-./+(V&## G"-./+(#,I/ \[U9S;] SQ^G6 SQ:G6O/*_`O O/*_`O(GSTU
N(#/L/+(GSTU N(#/L/+(+/)"## OaK(Q">E PTOSF(K PTOSF OI">V<+-
KTPT`Q("-b KTPT`Q(DI/+ KTPT`Q(+">E OT_NFFPT[(?1@
GSTU(@3" MNOPPTQJ \Q \+-<) US[D GTHP9T GE"+%
49
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+,-+).*'. /012 /011 /034 /034 /035 /064 /074 !"#$ !"%&)&+89*": /07; /035 !"&' /037 /03< /06/ /077 /036 /06<=>? /057 /016 /035 /033 /033 /063 /073 /031 /066@A /015 /011 /034 /031 /031 /065 /075 /031 /063BC& /035 /016 /034 /035 /031 /063 /071 /036 /06<A?D /05/ /011 /034 /034 /035 /065 /075 /031 /066A.)+).*'. /012 /011 /034 /034 /035 /064 /075 /035 !"%&A.) /06/ /01/ /031 /037 /037 /06< /077 /033 /067#?*%? /01< /013 /035 /031 /033 /061 /071 /031 /066#?& /011 /013 /034 /035 /031 /061 /071 /033 /066.:#?:9E /012 /013 /035 /035 /031 /061 /071 /033 /067.:#?:9F !"$! !"&( /03; !"#) !"#$ !"%* !"') !"#$ !"%&GHDC+).*'. /051 /011 /034 /034 /035 /064 /074 /035 !"%&GHDC /01< /013 /034 /035 /031 /061 /075 /031 /063%I& /063 /01< /031 /033 /036 /072 /07< /062 /07;&9=?: /057 /011 /034 /035 /031 /065 /075 /035 /063&JJ%"#" /034 /013 /034 /031 /033 /061 /071 /031 /063&JK+).*'. /011 /011 /034 /034 !"#$ /064 /074 /031 /063!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)*&+C9"L9#=+).*'. /034 /03; 30;5 /031 /0/6 /0/1 /016 /0<; 102;)&+89*": /03< /037 3067 /066 /0/6 /0/6 /034 /0<7 10/6=>? /031 /034 3042 /035 !"!# !"+' /01/ /0<3 107;@A /035 /034 3045 /031 /0/6 /0/6 /017 /0<5 10;1BC& /033 /031 3017 /037 /0/6 /0/4 /01/ /0<1 1042A?D /034 /03; 30;6 /031 /0/6 /0/5 /016 /0<4 102;A.)+).*'. /035 /034 3044 /031 /0/6 /0// /016 /0<4 102;A.) /03< /037 303/ /062 /0/6 /0/7 /035 /0<3 10<2#?*%? /031 /035 3055 /036 /0/6 /0/1 /01< /0<5 104/#?& /035 /03; 3045 /031 /0/6 /0/3 /017 /0<5 10;1.:#?:9E /035 /03; 30;7 /035 !"!# /0/2 /017 /0<1 1051.:#?:9F !"#* !"&! &"!& !"#) !"!# /07/ !"&& !"+* ("+)GHDC+).*'. /035 /03; 3044 /031 /0/6 /0/6 /016 /0<4 50/5GHDC /031 /034 304< /033 /0/6 /0/1 /017 /0<5 10;5%I& /06; /062 6023 /06; /0/6 /0/6 /031 /0<< 30;7&9=?: /034 /03; 30;1 /035 /0/6 /0/; /016 /0<4 1023&JJ%"#" /031 /035 3051 /036 /0/6 /0/6 /01< /0<5 1043&JK+).*'. /035 /034 3041 /033 /0/6 /0/6 /016 /0<4 50</
,-./012345678-(((((++(((((M9DG?)%(%&'*?N9=(D.(OPQR(M9DG?)%MERS(7/</T(J#&%(DH.('"%9#?:9(*9DG?)%(!FUVW(":=(OPQR$0((Q).G9%(I.G(X"##X(!9:YG9(ZMR</(D9%D%9D$(":=(X%&'X(!%&'%9D(.I(DC9(C&*":#[("%%9%%9=(="D"$("G9(%C.H:0G":A MR+O\] MR+*O\] F"=^9G(I&## F"=^9G(#?D9 ]\W+U-_ US`F3 US,F3Q9*abQ Q9*abQ(FUVW
P(#9N9G(FUVW P(#9N9G(G9)"## QcM(S":A RVQUE(M RVQUE QD":I.G=
MVRVbS("=d MVRVbS(CD9G MVRVbS(G":A QVaPEERV\(70<
FUVW(<6" OPQRRVSJ ]S ]G=.) WU\C FVHR+V FA"G%
!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$( !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0123 0124 0125 0126 0122 0132 0172 0126 0120)*&89:8);*'; 015< 0150 !"#$ 0124 012< 013< !"%& 012< 0127)&8=/*"> 017? 0122 !"#$ 012@ 0120 013@ 0177 0127 013?-)& 015@ 0150 !"#$ 012< 0126 0136 0172 012< 012@-A, 0122 0126 0123 0123 0127 0137 017? 0123 0136BC& 0122 0124 0125 0126 0122 0132 0172 0125 013<D;)8);*'; 0157 0150 !"#$ 012< 0126 0136 !"%& 012< 012@D;) 01@4 0123 012? 012@ 0134 01?< 017@ 012? 0135;>#,>/E 0154 0150 !"#$ 012< 0126 013< 0172 012< 012@;>#,>/F !"$' !"&' !"#$ !"&! !"#( !")( !"%& !"&! !"#*+GHC8);*'; 0124 0124 0125 0124 012< 013< !"%& 012< 012@%I& 0130 012< 0122 0122 0123 0137 0173 012? 0132&/-,> 015@ 0150 0125 012< 0126 0136 0172 012< 012@&J'8);*'; 0150 0150 !"#$ 0124 012< 013< !"%& 012< 012@&JK8>>#* 0123 0124 0125 0125 0122 0132 0173 0126 0134&JK 0122 0124 0125 0126 0125 0135 0172 0126 0120!"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$ !"##$ !%&'$)"*'+,-./ 0126 0124 2162 0132 !"!' 0100 0150 0174 6120)*&89:8);*'; 0124 015@ 2140 013< !"!' 0100 0157 !"*% 6145)&8=/*"> 0123 0125 213@ 0130 0100 0100 0125 017@ 5123-)& 0124 015@ 2140 0136 !"!' 0100 015@ 01?0 6167-A, 0123 0126 2122 0137 0100 0100 0126 0173 51<?BC& 0126 0124 2167 0132 !"!' 0100 0150 017< 612<D;)8);*'; 012< 0150 21<3 0135 !"!' 0100 015@ 01?0 6164D;) 012@ 012? 21@4 01?5 0100 0100 012? 017@ 51?6;>#,>/E 0124 0157 2146 013< !"!' 0100 0157 01?0 6152;>#,>/F !"&! !"&* &"'% !")( !"!' 0100 0153 !"*% 61<6+GHC8);*'; 0124 015@ 21<4 0136 !"!' 0100 0150 01?@ $"($%I& 0123 0122 21?2 0137 0100 0100 0125 0173 61@<&/-,> 012< 015@ 21<5 0136 !"!' !"!' 015@ 01?0 6155&J'8);*'; 012< 0150 21<< 0136 !"!' 0100 015@ 01?@ 614?&JK8>>#* 0126 0124 216@ 0133 !"!' 0100 0124 017< 61?4&JK 012< 0150 21<0 0132 !"!' 0100 0150 0174 613<
+,-./012345,/016((((88(((((L/H+,)%(%&'*,M/-(H;(NOPQ(L/H+,)%LEQR(70@0S(J#&%(HG;('"%/#,>/(*/H+,)%(!FTUV(">-(NOPQ$1((P);+/%(I;+(W"##W(!/>X+/(YLQ@0(H/%H%/H$(">-(W%&'W(!%&'%/H(;I(HC/(C&*">#Z("%%/%%/-(-"H"$("+/(%C;G>1+">D LQ8N[\ LQ8*N[\ F"-./+(I&## F"-./+(#,H/ \[V8T:] TR^F3 TR9F3P/*_`P P/*_`P(FTUV
O(#/M/+(FTUV O(#/M/+(+/)"## PaL(R">D QUPTE(L QUPTE PH">I;+-
LUQU`R("-b LUQU`R(CH/+ LUQU`R(+">D PU_OEEQU[(71@
FTUV(@?" NOPQQURJ \R \+-;) VT[C FUGQ8U FD"+%
50
R
E
F
A
A
L
T
O
C
M
U
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
R
W
T
H
-C
U
P
V
-C
REF ? .03? .02? .03? .01? .03? .02? .05? .02? .06? .03? .05? .03?
AALTO .93? ? .54? .54? .23? .36 .58? .56? .65? .69? .64? .67? .62?
CMU .94? .30? ? .47 .14? .22? .52? .41 .50? .57? .45? .44 .38
CU-BOJAR .94? .26? .38 ? .10? .22? .61? .47? .46 .55? .42 .49? .44
CU-ZEMAN .98? .58? .73? .77? ? .55? .79? .71? .84? .80? .77? .79? .75?
ONLINEA .94? .41 .61? .57? .23? ? .68? .63? .71? .71? .63? .54? .61?
ONLINEB .93? .30? .31? .26? .10? .17? ? .32? .35 .31 .22? .29? .38
UEDIN .91? .27? .35 .34? .11? .18? .47? ? .54? .50? .35 .29 .35
BBN-C .95? .21? .22? .36 .06? .17? .38 .26? ? .32 .24? .31? .26?
CMU-HEA-C .90? .17? .19? .23? .09? .18? .32 .27? .34 ? .31? .31? .30?
JHU-C .93? .19? .30? .35 .09? .24? .50? .34 .47? .45? ? .41? .36
RWTH-C .91? .16? .35 .29? .12? .27? .41? .37 .42? .42? .23? ? .24?
UPV-C .94? .24? .40 .36 .09? .28? .39 .32 .46? .47? .33 .36? ?
> others .93 .26 .37 .38 .11 .24 .47 .40 .49 .49 .38 .41 .40
>= others .97 .42 .56 .55 .25 .39 .67 .62 .70 .70 .61 .65 .62
Table 21: Sentence-level ranking for the WMT10 Czech-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
51
R
E
F
A
A
L
T
O
C
M
U
C
U
-Z
E
M
A
N
D
F
K
I
F
B
K
H
U
IC
O
N
G
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
A
O
N
L
IN
E
B
R
W
T
H
U
E
D
IN
U
M
D
U
P
P
S
A
L
A
U
U
-M
S
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
JH
U
-C
K
O
C
-C
R
W
T
H
-C
U
P
V
-C
REF ? .00? .02? .00? .07? .04? .03? .00? .06? .04? .00? .02? .07? .07? .07? .02? .09? .03? .03? .10? .04? .04? .03? .02? .07? .06?
AALTO 1.00? ? .43 .39 .48 .60? .38 .41 .74? .18? .42 .57? .50? .63? .55? .68? .79? .42 .33 .71? .61? .66? .54 .51? .66? .56?
CMU .95? .34 ? .19? .45 .52? .38 .50 .63? .17? .51? .55? .56? .66? .55? .60? .56? .30 .40 .62? .64? .49? .58? .46 .64? .46?
CU-ZEMAN 1.00? .44 .64? ? .43 .72? .31 .45? .69? .36 .55 .62? .75? .75? .78? .75? .75? .48? .56? .79? .82? .72? .68? .63? .67? .84?
DFKI .92? .29 .33 .35 ? .37 .40 .34 .59 .08? .42 .50 .49 .64? .35 .44 .44 .50 .41 .70? .61? .57 .46 .47 .62? .44
FBK .93? .26? .23? .17? .49 ? .12? .30 .52? .08? .20? .45? .41 .62? .44 .44 .48? .18? .25? .53? .47 .38 .38 .22? .41 .51?
HUICONG .92? .34 .39 .37 .38 .71? ? .53? .67? .18? .51? .47 .60? .65? .49? .55? .78? .35 .41 .56? .77? .74? .58? .41 .65? .57?
JHU .92? .35 .30 .17? .52 .45 .25? ? .58? .16? .43 .38 .57? .60? .54? .60? .70? .29 .25 .65? .75? .56? .62? .49? .66? .48?
KIT .90? .14? .16? .14? .35 .28? .19? .16? ? .03? .29? .20? .35 .53? .21? .24? .30 .20? .22? .44 .29 .38 .35 .24 .40 .24?
KOC .95? .66? .71? .51 .75? .80? .58? .68? .93? ? .75? .87? .72? .74? .74? .81? .81? .78? .66? .89? .85? .80? .80? .72? .91? .73?
LIMSI .99? .26 .24? .32 .45 .61? .25? .38 .50? .10? ? .50? .55? .69? .52? .57? .57? .29? .22? .60? .52? .42 .47? .37 .60? .56?
LIU .87? .17? .20? .14? .34 .22? .31 .38 .66? .04? .27? ? .51? .53? .52? .53? .51 .20? .33 .64? .59? .48? .48 .51 .37 .53?
ONLINEA .90? .25? .29? .18? .34 .43 .23? .28? .49 .08? .32? .30? ? .44 .38 .40 .42 .32? .35? .39 .47 .51 .27? .35 .43 .40
ONLINEB .76? .22? .24? .14? .27? .27? .25? .25? .32? .22? .21? .28? .32 ? .27? .21? .30? .23? .15? .41 .31 .40 .23? .16? .42 .29
RWTH .89? .22? .23? .13? .49 .35 .29? .21? .62? .15? .32? .29? .46 .57? ? .39 .49 .25 .38 .41 .27 .34 .36 .27 .48? .22?
UEDIN .91? .15? .20? .12? .49 .35 .24? .22? .49? .04? .22? .30? .46 .62? .43 ? .39 .11? .15? .45 .33 .40 .45 .33 .34 .33
UMD .91? .12? .23? .06? .35 .29? .11? .16? .47 .14? .23? .35 .40 .55? .36 .47 ? .16? .17? .44 .29? .27 .37 .26 .27 .24?
UPPSALA .94? .30 .41 .23? .35 .53? .26 .37 .66? .03? .54? .71? .57? .65? .45 .72? .67? ? .25 .59? .69? .49? .63? .33 .60? .64?
UU-MS .83? .28 .42 .24? .41 .49? .28 .42 .68? .10? .55? .48 .55? .63? .49 .56? .60? .32 ? .52? .58? .61? .64? .46? .64? .50?
BBN-C .90? .15? .16? .10? .22? .17? .22? .18? .41 .06? .16? .21? .35 .45 .30 .26 .34 .13? .20? ? .42? .14? .27 .11? .25 .21?
CMU-HEA-C .83? .20? .18? .07? .29? .32 .06? .10? .49 .05? .26? .21? .41 .33 .37 .43 .58? .10? .14? .18? ? .33 .32 .11? .34 .24?
CMU-HYPO-C .96? .24? .20? .07? .37 .33 .12? .21? .40 .10? .41 .26? .40 .54 .25 .37 .44 .13? .17? .49? .31 ? .34 .23? .51? .45
JHU-C .97? .33 .22? .18? .31 .30 .27? .18? .33 .12? .19? .33 .59? .60? .39 .32 .30 .19? .20? .44 .29 .34 ? .21? .36 .23
KOC-C .93? .11? .31 .17? .41 .50? .25 .27? .44 .11? .42 .36 .47 .68? .43 .41 .40 .33 .18? .59? .57? .46? .47? ? .52? .43
RWTH-C .87? .20? .10? .21? .25? .27 .15? .23? .24 .02? .20? .30 .34 .47 .27? .34 .36 .14? .20? .33 .26 .21? .24 .20? ? .17?
UPV-C .93? .14? .20? .10? .42 .29? .25? .25? .57? .20? .22? .33? .39 .45 .47? .40 .50? .24? .28? .44? .42? .27 .34 .28 .56? ?
> others .92 .25 .28 .18 .39 .41 .25 .30 .52 .12 .34 .39 .47 .57 .42 .46 .51 .27 .28 .52 .49 .45 .44 .34 .50 .42
>= others .96 .46 .49 .35 .53 .62 .45 .51 .71 .24 .54 .58 .63 .72 .63 .66 .70 .50 .51 .75 .73 .68 .67 .59 .74 .64
Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
R
E
F
C
A
M
B
R
ID
G
E
C
O
L
U
M
B
IA
C
U
-Z
E
M
A
N
D
F
K
I
H
U
IC
O
N
G
JH
U
O
N
L
IN
E
A
O
N
L
IN
E
B
U
E
D
IN
U
P
C
B
B
N
-C
C
M
U
-H
E
A
-C
JH
U
-C
U
P
V
-C
REF ? .05? .01? .02? .03? .03? .01? .02? .04? .03? .04? .03? .07? .05? .04?
CAMBRIDGE .90? ? .24? .11? .35? .26? .43 .35 .50? .45? .33? .40 .46 .28? .41
COLUMBIA .97? .61? ? .25? .47 .44 .61? .53? .62? .59? .48? .59? .57? .45? .57?
CU-ZEMAN .92? .73? .59? ? .62? .66? .71? .65? .75? .79? .58? .75? .78? .71? .72?
DFKI .95? .50? .41 .21? ? .46 .56? .52? .65? .62? .47 .52? .56? .52? .60?
HUICONG .93? .57? .34 .21? .36 ? .47? .43 .67? .58? .40 .51? .62? .46? .52?
JHU .94? .39 .22? .16? .30? .32? ? .41 .52? .47? .37 .41 .33? .28 .35
ONLINEA .92? .45 .35? .24? .34? .41 .41 ? .60? .58? .38 .55? .46 .36 .57?
ONLINEB .87? .34? .24? .15? .21? .19? .33? .25? ? .34? .26? .34? .37? .24? .40
UEDIN .94? .33? .26? .12? .24? .22? .25? .25? .50? ? .25? .28? .32? .25? .26
UPC .89? .45? .36? .23? .39 .37 .42 .48 .62? .57? ? .54? .51? .50? .53?
BBN-C .91? .33 .25? .11? .32? .30? .34 .31? .51? .41? .30? ? .36 .26? .31
CMU-HEA-C .89? .37 .20? .10? .29? .23? .23? .35 .50? .44? .31? .34 ? .23? .31
JHU-C .89? .39? .31? .17? .37? .33? .38 .42 .63? .47? .31? .42? .42? ? .37?
UPV-C .91? .35 .30? .16? .29? .26? .32 .28? .44 .35 .27? .27 .30 .24? ?
> others .92 .42 .29 .16 .33 .32 .39 .37 .54 .48 .34 .42 .44 .35 .43
>= others .97 .62 .45 .29 .46 .50 .61 .52 .68 .68 .51 .64 .65 .58 .66
Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
52
R
E
F
C
A
M
B
R
ID
G
E
C
M
U
-S
T
A
T
X
F
E
R
C
U
-Z
E
M
A
N
D
F
K
I
G
E
N
E
V
A
H
U
IC
O
N
G
JH
U
L
IG
L
IM
S
I
L
IU
M
N
R
C
O
N
L
IN
E
A
O
N
L
IN
E
B
R
A
L
I
R
W
T
H
U
E
D
IN
B
B
N
-C
C
M
U
-H
E
A
-C
C
M
U
-H
Y
P
O
-C
D
C
U
-C
JH
U
-C
L
IU
M
-C
R
W
T
H
-C
U
P
V
-C
REF ? .02? .00? .00? .00? .00? .05? .02? .00? .00? .00? .02? .06? .02? .04? .02? .04? .03? .02? .05? .05? .04? .05? .06? .02?
CAMBRIDGE .82? ? .42 .16? .12? .35 .31 .45 .21? .47 .29 .38 .28? .54 .43 .33 .38 .28 .39 .45? .24 .25 .34 .54? .37
CMU-STATXFER .91? .50 ? .17? .41 .17? .28 .44 .36 .48? .56? .57? .47 .56? .70? .49 .50 .47 .61? .68? .55? .50 .42 .52? .51?
CU-ZEMAN 1.00? .74? .71? ? .74? .46 .67? .73? .73? .74? .75? .76? .75? .89? .78? .66? .83? .74? .87? .73? .80? .83? .77? .95? .82?
DFKI 1.00? .77? .48 .17? ? .27? .49 .52 .48 .64? .69? .67? .47 .62? .53 .47 .64? .60? .73? .72? .79? .58? .66? .73? .74?
GENEVA .98? .58 .70? .44 .59? ? .55? .67? .70? .70? .77? .73? .63? .81? .81? .69? .77? .73? .62? .66? .75? .60? .73? .88? .67?
HUICONG .89? .53 .34 .13? .34 .30? ? .41 .36 .43 .70? .56? .57 .59? .56? .43 .55? .45 .51? .64? .48 .49 .49 .53? .57?
JHU .88? .36 .38 .11? .34 .25? .35 ? .33? .46 .49? .48 .40 .50 .40 .34 .36 .39 .33 .59? .54? .41 .42 .40 .41
LIG .98? .65? .34 .18? .44 .26? .39 .56? ? .60? .55? .51? .45 .54? .53 .39 .38 .52? .54? .53? .51? .53? .55 .51 .58?
LIMSI .98? .40 .24? .23? .23? .15? .29 .38 .25? ? .28 .38 .27? .64? .35 .30 .41 .27 .33 .49 .45 .37 .28 .45 .39
LIUM .90? .40 .19? .12? .30? .11? .11? .26? .15? .36 ? .36 .25? .37 .39 .26 .29 .24 .34 .49? .34 .33 .34 .31 .38
NRC .93? .31 .06? .15? .29? .23? .20? .32 .16? .38 .36 ? .23? .53 .36 .24? .31 .44 .37 .47? .45? .29 .39 .38 .42
ONLINEA .92? .60? .47 .15? .44 .22? .32 .46 .34 .57? .52? .60? ? .52? .34 .44 .57? .56 .51 .51 .64? .46 .51 .41 .60
ONLINEB .85? .35 .32? .09? .33? .10? .29? .31 .25? .17? .40 .34 .24? ? .38 .32? .28 .39 .30 .42 .37 .41 .35 .32 .22?
RALI .90? .31 .19? .10? .38 .10? .17? .47 .35 .38 .33 .38 .48 .48 ? .29? .31 .29 .38 .40 .38 .34 .31 .57? .21?
RWTH .93? .43 .33 .12? .47 .26? .39 .40 .47 .35 .45 .49? .44 .53? .54? ? .44? .42 .48 .51? .54? .48? .49 .50? .26
UEDIN .92? .42 .32 .10? .22? .10? .28? .30 .42 .30 .55 .36 .23? .43 .33 .20? ? .41 .24 .52? .46 .25 .22 .27 .37
BBN-C .92? .49 .33 .24? .28? .18? .40 .39 .28? .45 .27 .27 .36 .39 .35 .35 .31 ? .26 .45? .43 .26 .58? .36 .28
CMU-HEA-C .90? .41 .21? .06? .23? .29? .28? .27 .22? .39 .40 .22 .39 .43 .29 .30 .40 .28 ? .43 .28 .15? .25 .26 .16
CMU-HYPO-C .84? .18? .20? .14? .20? .22? .21? .19? .16? .31 .22? .21? .36 .38 .34 .27? .22? .16? .24 ? .36 .23 .10? .33 .24
DCU-C .92? .27 .24? .12? .17? .23? .30 .29? .24? .32 .43 .22? .28? .41 .23 .27? .28 .22 .23 .25 ? .23 .23 .24 .17
JHU-C .88? .47 .26 .10? .33? .24? .36 .34 .24? .41 .39 .40 .42 .39 .34 .25? .42 .28 .37? .38 .39 ? .37 .32 .38?
LIUM-C .90? .48 .42 .13? .25? .20? .33 .50 .30 .44 .37 .34 .37 .52 .43 .34 .33 .22? .34 .56? .33 .43 ? .49? .44
RWTH-C .89? .22? .19? .03? .23? .12? .19? .23 .27 .30 .36 .19 .47 .54 .26? .16? .27 .19 .26 .28 .16 .22 .16? ? .22
UPV-C .89? .27 .15? .10? .16? .29? .30? .31 .25? .36 .42 .24 .32 .64? .46? .34 .27 .44 .33 .44 .23 .17? .31 .24 ?
> others .91 .43 .32 .14 .31 .21 .31 .39 .31 .42 .44 .40 .38 .52 .43 .33 .40 .37 .40 .49 .43 .38 .4 .44 .39
>= others .97 .64 .51 .24 .40 .31 .50 .59 .50 .63 .68 .65 .51 .68 .65 .55 .66 .63 .69 .75 .71 .64 .62 .74 .67
Table 24: Sentence-level ranking for the WMT10 French-English News Task (Combining expert and
non-expert Mechanical Turk judgments)
53
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 1?11,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
A Grain of Salt for the WMT Manual Evaluation?
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel
Charles University in Prague
Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{bojar,popel}@ufal.mff.cuni.cz
ercegovcevic@hotmail.com
Omar F. Zaidan
Department of Computer Science
Johns Hopkins University
ozaidan@cs.jhu.edu
Abstract
The Workshop on Statistical Machine
Translation (WMT) has become one of
ACL?s flagship workshops, held annually
since 2006. In addition to soliciting pa-
pers from the research community, WMT
also features a shared translation task for
evaluating MT systems. This shared task
is notable for having manual evaluation as
its cornerstone. The Workshop?s overview
paper, playing a descriptive and adminis-
trative role, reports the main results of the
evaluation without delving deep into ana-
lyzing those results. The aim of this paper
is to investigate and explain some interest-
ing idiosyncrasies in the reported results,
which only become apparent when per-
forming a more thorough analysis of the
collected annotations. Our analysis sheds
some light on how the reported results
should (and should not) be interpreted, and
also gives rise to some helpful recommen-
dation for the organizers of WMT.
1 Introduction
The Workshop on Statistical Machine Translation
(WMT) has become an annual feast for MT re-
searchers. Of particular interest is WMT?s shared
translation task, featuring a component for man-
ual evaluation of MT systems. The friendly com-
petition is a source of inspiration for participating
teams, and the yearly overview paper (Callison-
Burch et al, 2010) provides a concise report of the
state of the art. However, the amount of interest-
ing data collected every year (the system outputs
? This work has been supported by the grants EuroMa-
trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003 of
the Czech Republic), P406/10/P259, MSM 0021620838, and
DARPA GALE program under Contract No. HR0011-06-2-
0001. We are grateful to our students, colleagues, and the
three reviewers for various observations and suggestions.
and, most importantly, the annotator judgments)
is quite large, exceeding what the WMT overview
paper can afford to analyze with much depth.
In this paper, we take a closer look at the data
collected in last year?s workshop, WMT101, and
delve a bit deeper into analyzing the manual judg-
ments. We focus mainly on the English-to-Czech
task, as it included a diverse portfolio of MT sys-
tems, was a heavily judged language pair, and also
illustrates interesting ?contradictions? in the re-
sults. We try to explain such points of interest,
and analyze what we believe to be the positive and
negative aspects of the currently established eval-
uation procedure of WMT.
Section 2 examines the primary style of man-
ual evaluation: system ranking. We discuss how
the interpretation of collected judgments, the com-
putation of annotator agreement, and document
that annotators? individual preferences may render
two systems effectively incomparable. Section 3
is devoted to the impact of embedding reference
translations, while Section 4 and Section 5 discuss
some idiosyncrasies of other WMT shared tasks
and manual evaluation in general.
2 The System Ranking Task
At the core of the WMT manual evaluation is the
system ranking task. In this task, the annotator
is presented with a source sentence, a reference
translation, and the outputs of five systems over
that source sentence. The instructions are kept
minimal: the annotator is to rank the presented
translations from best to worst. Ties are allowed,
but the scale provides five rank labels, allowing the
annotator to give a total order if desired.
The five assigned rank labels are submitted at
once, making the 5-tuple a unit of annotation. In
the following, we will call this unit a block. The
blocks differ from each other in the choice of the
1http://www.statmt.org/wmt10
1
Language Pair Systems Blocks Labels Comparisons Ref ? others Intra-annot. ? Inter-annot. ?
German-English 26 1,050 5,231 10,424 0.965 0.607 0.492
English-German 19 1,407 6,866 13,694 0.976 0.560 0.512
Spanish-English 15 1,140 5,665 11,307 0.989 0.693 0.508
English-Spanish 17 519 2,591 5,174 0.935 0.696 0.594
French-English 25 837 4,156 8,294 0.981 0.722 0.452
English-French 20 801 3,993 7,962 0.917 0.636 0.449
Czech-English 13 543 2,691 5,375 0.976 0.700 0.504
English-Czech 18 1,395 6,803 13,538 0.959 0.620 0.444
Average 19 962 4,750 9,471 0.962 0.654 0.494
Table 1: Statistics on the collected rankings, quality of references and kappas across language pairs. In
general, a block yields a set of five rank labels, which yields a set of
(5
2
)
= 10 pairwise comparisons.
Due to occasional omitted labels, the Comparisons/Blocks ratio is not exactly 10.
source sentence and the choice of the five systems
being compared. A couple of tricks are introduced
in the sampling of the source sentences, to en-
sure that a large enough number of judgments is
repeated across different screens for meaningful
computation of inter- and intra-annotator agree-
ment. As for the sampling of systems, it is done
uniformly ? no effort is made to oversample or un-
dersample a particular system (or a particular pair
of systems together) at any point in time.
In terms of the interface, the evaluation utilizes
the infrastructure of Amazon?s Mechanical Turk
(MTurk)2, with each MTurk HIT3 containing three
blocks, corresponding to three consecutive source
sentences.
Table 1 provides a brief comparison of the vari-
ous language pairs in terms of number of MT sys-
tems compared (including the reference), number
of blocks ranked, the number of pairwise com-
parisons extracted from the rankings (one block
with 5 systems ranked gives 10 pairwise compar-
isons, but occasional unranked systems are ex-
cluded), the quality of the reference (the percent-
age of comparisons where the reference was better
or equal than another system), and the ? statistic,
which is a measure of agreement (see Section 2.2
for more details).4
We see that English-to-Czech, the language pair
on which we focus, is not far from the average in
all those characteristics except for the number of
collected comparisons (and blocks), making it the
second most evaluated language pair.
2http://www.mturk.com/
3?HIT? is an acronym for human intelligence task, which
is the MTurk term for a single screen presented to the anno-
tator.
4We only use the ?expert? annotations of WMT10, ignor-
ing the data collected from paid annotators on MTurk, since
they were not part of the official evaluation.
2.1 Interpreting the Rank Labels
The description in the WMT overview paper says:
?Relative ranking is our official evaluation met-
ric. [Systems] are ranked based on how frequently
they were judged to be better than or equal to
any other system.? (Emphasis added.) The WMT
overview paper refers to this measure as ?? oth-
ers?, with a variant of it called ?> others? that does
not reward ties.
We first note that this description is somewhat
ambiguous, and an uninformed reader might in-
terpret it in one of two different ways. For some
system A, each block in which A appears includes
four implicit pairwise comparisons (against the
other presented systems). How is A?s score com-
puted from those comparisons?
The correct interpretation is that A is re-
warded once for each of the four comparisons in
which A wins (or ties).5 In other words, A?s score
is the number of pairwise comparisons in which
A wins (or ties), divided by the total number of
pairwise comparisons involving A. We will use
?? others? (resp. ?> others?) to refer to this inter-
pretation, in keeping with the terminology of the
overview paper.
The other interpretation is that A is rewarded
only if A wins (or ties) all four comparisons. In
other words, A?s score is the number of blocks in
whichA wins (or ties) all comparisons, divided by
the number of blocks in which A appears. We will
use ?? all in block? (resp. ?> all in block?) to
refer to this interpretation.6
5Personal communication with WMT organizers.
6There is yet a third interpretation, due to a literal read-
ing of the description, where A is rewarded at most once per
block if it wins (or ties) any one of its four comparisons. This
is probably less useful: it might be good at identifying the
bottom tier of systems, but would fail to distinguish between
all other systems.
2
REF C
U
-B
O
JA
R
C
U
-T
E
C
T
O
E
U
R
O
T
R
A
N
S
O
N
L
IN
E
B
P
C
-T
R
A
N
S
U
E
D
IN
? others 95.9 65.6 60.1 54.0 70.4 62.1 62.2
> others 90.5 45.0 44.1 39.3 49.1 49.4 39.6
? all in block 93.1 32.3 30.7 23.4 37.5 32.5 28.1
> all in block 81.3 13.6 19.0 13.3 15.6 18.7 10.6
Table 2: Sentence-level ranking scores for the
WMT10 English-Czech language pair. The ??
others? and ?> others? scores reproduced here
exactly match numbers published in the WMT10
overview paper. A boldfaced score marks the best
system in a given row (besides the reference).
For quality control purposes, the WMT organiz-
ers embed the reference translations as a ?system?
alongside the actual entries (the idea being that an
annotator clicking randomly would be easy to de-
tect, since they would not consistently rank the
reference ?system? highly). This means that the
reference is as likely as any other system to ap-
pear in a block, and when the score for a system A
is computed, pairwise comparisons with the refer-
ence are included.
We use the publicly released human judgments7
to compute the scores of systems participating in
the English-Czech subtask, under both interpreta-
tions. Table 2 reports the scores, with our ?? oth-
ers? (resp. ?> others?) scores reproduced exactly
matching those reported in Table 21 of the WMT
overview paper. (For clarity, Table 2 is abbreviated
to include only the top six systems of twelve.)
Our first suggestion is that both measures could
be reported in future evaluations, since each tells
us something different. The first interpretation
gives partial credit for an MT system, hence distin-
guishing systems from each other at a finer level.
This is especially important for a language pair
with relatively few annotations, since ?? others?
would produce a larger number of data points (four
per system per block) than ?? all in block? (one
per system per block). Another advantage of the
official ?? others? is greater robustness towards
various factors like the number of systems in the
competition, the number of systems in one block
or the presence of the reference in the block (how-
ever, see Section 3).
As for the second interpretation, it helps iden-
tify whether or not a single system (or a small
group of systems) is strongly dominant over the
other systems. For the systems listed in Table 2,
7http://statmt.org/wmt10/results.html
-
10 0
 
10
 
20
 
30
 
40
 
50
 
60  1
0
 
20
 
30
 
40
 
50
 
60
 
70
 
80
>= All in Block
>= 
Othe
rs
Czec
h-En
glish
Engl
ish-C
zech
Engl
ish-F
renc
h
Engl
ish-G
erma
n
Engl
ish-S
pani
sh
Fren
ch-E
nglis
h
Germ
an-E
nglis
h
Span
ish-E
nglis
h
a*x
+b
Figure 1: ?? all in block? and ?? others? provide
very similar ordering of systems.
?> all in block? suggests its potential in the con-
text of system combination: CU-TECTO and PC-
TRANS win almost one fifth of the blocks in which
they appear, despite the fact that either a refer-
ence translation or a combination system already
appears alongside them. (See also Table 4 below.)
Also, note that if the ranking task were designed
specifically to cater to the ?? all in block? inter-
pretation, it would only have two ?rank? labels (ba-
sically, ?top? and ?non-top?). In that case, an-
notators would spend considerably less time per
block than they do now, since all they need to do
is identify the top system(s) per block, without dis-
tinguishing non-top systems from each other.
Even for those interested in distinguishing non-
state-of-the-art systems from each other, we point
out that the ?? all in block? interpretation ulti-
mately gives a system ordering that is very simi-
lar to that of the official ?? others? interpretation,
even for the lower-tier systems (Figure 1).
2.2 Annotator Agreement
The WMT10 overview paper reports inter- and
intra-annotator agreement over the pairwise com-
parisons, to show the validity of the evaluation
setup and the ?? others? metric. Agreement is
quantified using the following formula:
? =
P (A)? P (E)
1? P (E)
(1)
where P (A) is the proportion of times two anno-
tators are observed to agree, and P (E) is the ex-
pected proportion of times two annotators would
agree by chance. Note that ? has a value of at most
1, with higher ? values indicating higher rates of
agreement. The ? measure is more meaningful
3
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8
 
0.9 1  
0
 
5
 
10
 
15
 
20
 
25
 
30
 
35
Kappa
Sour
ce le
ngth
Intra
. inc
l. ref
.
Intra
. exc
l. ref
.
Inter
. inc
l. ref
.
Inter
. exc
l. ref
.
Mod
erate
 agre
eme
nt
Figure 2: Intra-/inter-annotator agreement
with/without references, across various source
sentence lengths (lengths of n and n + 1 are used
to plot the point at x = n). This figure is based on
all language pairs.
than reporting P (A) as is, since it takes into ac-
count, via P (E), how ?surprising? it is for annota-
tors to agree in the first place.
In the context of pairwise comparisons, an
agreement between two annotators occurs when
they compare the same pair of systems (S1,S2),
and both agree on their relative ranking: either
S1 > S2, S1 = S2, or S1 < S2. P (E) is then:
P (E) = P 2(S1>S2)+P 2(S1=S2)+P 2(S1<S2) (2)
In the WMT overview paper, all three cate-
gories are assumed equally likely, giving P (E) =
1
9 +
1
9 +
1
9 =
1
3 . For consistency with the WMT
overview paper, and unless otherwise noted, we
also use P (E) = 13 whenever a ? value is re-
ported. (Though see Section 2.2.2 for a discussion
about P (E).)
2.2.1 Observed Agreement for Different
Sentence Lengths
In Figure 2 we plot the ? values across different
source sentence lengths. We see that the inter-
annotator agreement (when excluding references)
is reasonably high only for sentences up to 10
words in length ? according to Landis and Koch
(1977), and as cited by the WMT overview paper,
not even ?moderate? agreement can be assumed if
? is less than 0.4. Another popular (and controver-
sial) rule of thumb (Krippendorff, 1980) is more
strict and says that ? < 0.67 is not suitable even
for tentative conclusions.
For this reason, and given that a majority of sen-
tences are indeed more than 10 words in length
(the median is 20 words), we suggest that future
evaluations either include fewer outputs per block,
or divide longer sentences into shorter segments
(e.g. on clause boundaries), so these segments are
more easily and reliably comparable. The latter
suggestions assumes word alignment as a prepro-
cessing and presenting the annotators the context
of the judged segment.
2.2.2 Estimating P (E), the Expected
Agreement by Chance
Several agreement measures (usually called kap-
pas) were designed based on the Equation 1 (see
Artstein and Poesio (2008) and Eugenio and Glass
(2004) for an overview and a discussion). Those
measures differ from each other in how to de-
fine the individual components of Equation 2, and
hence differ in what the expected agreement by
chance (P (E)) would be:8
? The S measure (Bennett et al, 1954) assumes
a uniform distribution over the categories.
? Scott?s pi (Scott, 1955) estimates the distribu-
tion empirically from actual annotation.
? Cohen?s ? (Cohen, 1960) estimates the dis-
tribution empirically as well, and further as-
sumes a separate distribution for each anno-
tator.
Given that the WMT10 overview paper assumes
that the three categories (S1 > S2, S1 = S2, and
S1 < S2) are equally likely, it is using the S mea-
sure version of Equation 1, though it does not ex-
plicitly say so ? it simply calls it ?the kappa coef-
ficient? (K).
Regardless of what the measure should be
called, we believe that the uniform distribution it-
self is not appropriate, even though it seems to
model a ?random clicker? adequately. In partic-
ular, and given the design of the ranking inter-
face, 13 is an overestimate of P (S1 = S2) for
a random clicker, and should in fact be 15 : each
system receives one of five rank labels, and for
two systems to receive the same rank label, there
are only five (out of 25) label pairs that satisfy
S1 = S2. Therefore, with P (S1 = S2) = 15 ,
8These three measures were later generalized to more than
two annotators (Fleiss, 1971; Bartko and Carpenter, 1976),
Thus, without loss of generality, our examples involve two
annotators.
4
?? Others? S pi
Inter incl. ref. 0.487 0.454excl. ref. 0.439 0.403
Intra incl. ref. 0.633 0.609excl. ref. 0.601 0.575
Table 3: Summary of two variants of kappa: S
(or K as it is reported in the WMT10 paper) and
our proposed Scott?s pi. We report inter- vs. intra-
annotator agreement and collected from all com-
parisons (?incl. ref.?) vs. collected only from
comparisons without the reference (?excl. ref.?)
because it is generally easier to agree that the ref-
erence is better than the other systems. This table
is based on all language pairs.
we have P (S1 > S2) = P (S1 < S2) = 25 , and
therefore P (E) = 0.36 rather than 0.333.
Taking the discussion a step further, we actually
advocate following the idea of Scott?s pi, whereby
the distribution of each category is estimated em-
pirically from the actual annotation, rather than
assuming a random annotator ? these frequencies
are easy to compute, and reflect a more meaning-
ful P (E).9
Under this interpretation, P (S1 = S2) is cal-
culated to be 0.168, reflecting the fraction of pair-
wise comparisons that correspond to a tie. (Note
that this further supports the claim that setting
P (S1 = S2) = 13 for a random clicker, as used
in the WMT overview paper, is an overestimate.)
This results in P (E) = 0.374, yielding, for in-
stance, pi = 0.454 for ?? others? inter-annotator
agreement, somewhat lower than ? = 0.487 (re-
ported in Table 3).
We do note that the difference is rather small,
and that our aim is to be mathematically sound
above all. Carefully defining P (E) would be im-
portant when comparing kappas across different
tasks with different P (E), or when attempting
to satisfy certain thresholds (as the cited 0.4 and
0.67). Furthermore, if one is interested in mea-
suring agreement for individual annotators, such
as identifying those who have unacceptably low
intra-annotator agreement, the question of P (E) is
quite important, since annotation behavior varies
noticeably from one annotator to another. A ?con-
servative? annotator who prefers to rank systems
as being tied most of the time would have a high
9We believe that P (E) should not reflect the chance that
two random annotators would agree, but the chance that two
actual annotators would agree randomly. The two sound sub-
tly related but are actually quite different.
P (E), whereas an annotator using ties moderately
would have a low P (E). Hence, two annotators
with equal agreement rates (P (A)) are not neces-
sarily equally proficient, since their P (E) might
differ considerably.10
2.3 The ? variant vs. the > variant
Even within the same interpretation of how sys-
tems could be scored, there is a question of
whether or not to reward ties. The overview paper
reports both variants of its measure, but does not
note that there are non-trivial differences between
the two orderings. Compare for example the ??
others? ordering vs. the ?> others? ordering of
CU-BOJAR and PC-TRANS (Table 2), showing an
unexpected swing of 7.9%:
? others > others
CU-BOJAR 65.6 45.0
PC-TRANS 62.1 49.4
CU-BOJAR seems better under the? variant, but
loses out when only strict wins are rewarded. The-
oretically, this could be purely due to chance, but
the total number of pairwise comparisons in ??
others? is relatively large (about 1,500 pairwise
comparisons for each system), and ought to can-
cel such effects.
A similar pattern could be seen under the ?all in
block? interpretation as well (e.g. for CU-TECTO
and ONLINEB). Table 4 documents this effect by
looking at how often a system is the sole winner
of a block. Comparing PC-TRANS and CU-BOJAR
again, we see that PC-TRANS is up there with CU-
TECTO and DCU-COMBO as the most frequent sole
winners, winning 71 blocks, whereas CU-BOJAR
is the sole winner of only 53 blocks. This is in
spite of the fact that PC-TRANS actually appeared
in slightly fewer blocks than CU-BOJAR (385 vs.
401).
One possible explanation is that the two vari-
ants (??? and ?>?) measure two subtly different
things about MT systems. Digging deeper into Ta-
ble 2?s values, we find that CU-BOJAR is tied with
another system 65.6 ? 45.0 = 20.4% of the time,
while PC-TRANS is tied with another system only
62.1? 49.4 = 12.7% of the time. So it seems that
PC-TRANS?s output is noticeably different from
another system more frequently than CU-BOJAR,
which reduces the number of times that annotators
10Who?s more impressive: a psychic who correctly pre-
dicts the result of a coin toss 50% of the time, or a psychic
who correctly predicts the result of a die roll 50% of the time?
5
Blocks Sole Winner
305 Reference
73 CU-TECTO
71 PC-TRANS
70 DCU-COMBO
57 RWTH-COMBO
54 ONLINEB
53 CU-BOJAR
46 EUROTRANS
41 UEDIN
41 UPV-COMBO
175 One of eight other systems
409 No sole winner
1395 Total English-to-Czech Blocks
Table 4: A breakdown of the 1,395 blocks for the
English-Czech task, according to which system (if
any) is the sole winner. On average, a system ap-
pears in 388 blocks.
mark PC-TRANS as tied with another system.11 In
that sense, the ??? ranking is hurting PC-TRANS,
since it does not benefit from its small number of
ties. On the other hand, the ?>? variant would not
reward CU-BOJAR for its large number of ties.
The ?? others? score may be artificially boosted
if several very similar systems (and therefore
likely to be ?tied?) take part in the evaluation.12
One possible solution is to completely disregard
ties and calculate the final score as winswins+losses . We
recommend to use this score instead of ?? others?
( wins+tieswins+ties+losses ) which is biased toward often tied
systems, and ?> others? ( winswins+ties+losses ) which is
biased toward systems with few ties.
2.4 Surprise? Does the Number of
Evaluations Affect a System?s Score?
When examining the system scores for the
English-Czech task, we noticed a surprising pat-
tern: it seemed that the more times a system is
sampled to be judged, the lower its ?? others?
score (?? all in block? behaving similarly). A
scatter plot of a system?s score vs. the number of
blocks in which it appears (Figure 3) makes the
pattern obvious.
We immediately wondered if the pattern holds
in other language pairs. We measured Pearson?s
correlation coefficient within each language pair,
reported in Table 5. As it turns out, English-
11Indeed, PC-TRANS is a commercial system (manually)
tuned over a long period of time and based on resources very
different from what other participants in WMT use.
12In the preliminary WMT11 results, this seems to hap-
pen to four Moses-like systems (UEDIN, CU-BOJAR, CU-
MARECEK and CU-TAMCHYNA) which have better ?? oth-
ers? score but worse ?> others? score than CU-TECTO.
Correlation of Block Count
Source Target vs. ?? Others?
English Czech -0.558
English Spanish -0.434
Czech English -0.290
Spanish English -0.240
English French -0.227
English German -0.161
French English -0.024
German English 0.146
Overall -0.092
Table 5: Pearson?s correlation between the num-
ber of blocks where a system was ranked and the
system?s ?? others? score. (The reference itself is
not included among the considered systems).
 
30
 
35
 
40
 
45
 
50
 
55
 
60
 
65
 
70
 
75
 
80  35
0
 
360
 
370
 
380
 
390
 
400
 
410
 
420
>= Others
Num
ber o
f judg
ments
cmu
-hea
field
-com
bo
cu-
bojar
cu-
tecto cu
-ze
ma
n
dcu
dcu-
com
bo
eur
otra
ns
koc
koc-
com
bo
onlin
eA
onlin
eB
pc-tr
ans
pots
dam
rwth
-com
bo
sfu
uedi
n
upv-
com
bo
a*x
+b
Figure 3: A plot of ?? others? system score vs.
times judged, for English-Czech.
Czech happened to be the one language pair where
the ?correlation? is strongest, with only English-
Spanish also having a somewhat strong correla-
tion. Overall, though, there is a consistent trend
that can be seen across the language pairs. Could
it really be the case that the more often a system is
judged, the worse its score gets?
Examining plots for the other language pairs
makes things a bit clearer. Consider for example
the plot for English-Spanish (Figure 4). As one
would hope, the data points actually come together
to form a cloud, indicating a lack of correlation.
The reason that a hint of a correlation exists is the
presence of two outliers in the bottom right cor-
ner. In other words, the very worst systems are,
indeed, the ones judged quite often. We observed
this pattern in several other language pairs as well.
The correlation naturally does not imply cau-
sation. We are still not sure how to explain the
artifact. A subtle possibility lies in the MTurk
interface: annotators have the choice to accept a
HIT or skip it before actually providing their la-
6
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80  13
0
 
135
 
140
 
145
 
150
 
155
 
160
 
165
 
170
>= Others
Num
ber o
f judg
mentscamb
ridge
cmu
-hea
field
-com
bo
cu-
zem
an
dcu dfk
ijhu
koc
koc-
com
bo
onlin
eA
onlin
eB
rwth
-com
bo
sfu
uedi
n upb-
com
bo
upv
upv-
nnlm
a*x
+b
Figure 4: A plot of ?? others? system score vs.
times judged, for English-Spanish.
bels. It might be the case that some annotators are
more willing to accept HITs when there is an ob-
viously poor system (since that would make their
task somewhat easier), and who are more prone
to skipping HITs where the systems seem hard to
distinguish from each other. So there might be a
causation effect after all, but in the reverse order:
a system gets judged more often if it is a bad sys-
tem.13 A suggestion from the reviewers is to run a
pilot annotation with deliberate inclusion of a poor
system among the ranked ones.
2.5 Issues of Pairwise Judgments
The WMT overview paper also provides pairwise
system comparisons: each cell in Table 6 indicates
the percentage of pairwise comparisons between
the two systems where the system in the column
was ranked better (>) than the system in the row.
For instance, there are 81 ranking responses where
both CU-TECTO and CU-BOJAR were present and
indeed ranked14 among the 5 systems in the block.
In 37 (45.7%) of the cases, CU-TECTO was ranked
better, in 29 (35.8%), CU-BOJAR was ranked better
and there was a tie in the remaining 15 (18.5%)
cases. The ties are not explicitly shown in Table 6
but they are implied by the total of 100%. The cell
is in bold where there was a win in the pairwise
comparison, so 45.7 is bold in our example.
An interesting ?discrepancy? in Table 6 is that
CU-TECTO wins pairwise comparisons with CU-
BOJAR and UEDIN but it scores worse than them
in the official ?? others?, cf. Table 2. Simi-
larly, UEDIN outperformed ONLINEB in the pair-
13No pun intended!
14The users sometimes did not fill any rank for a system.
Such cases are ignored.
R
E
F
C
U
-B
O
JA
R
C
U
-T
E
C
T
O
E
U
R
O
T
R
A
N
S
O
N
L
IN
E
B
P
C
-T
R
A
N
S
U
E
D
IN
REF - 4.3 4.3 5.1 3.8 3.6 2.3
CU-BOJAR 87.1 - 45.7 28.3 44.4 39.5 41.1
CU-TECTO 88.2 35.8 - 38.0 55.8 44.0 36.0
EUROTRANS 88.5 60.9 46.8 - 50.7 53.8 48.6
ONLINEB 91.2 31.1 29.1 32.8 - 43.8 39.3
PC-TRANS 88.0 45.3 42.9 28.6 49.3 - 36.6
UEDIN 94.3 39.3 44.2 31.9 32.1 49.5 -
Table 6: Pairwise comparisons extracted from
sentence-level rankings of the WMT10 English-
Czech News Task. Re-evaluated to reproduce the
numbers published in WMT10 overview paper.
Bold in column A and row B means that system
A is pairwise better than system B.
wise comparisons but it was ranked worse in both
> and ? official comparison.
In the following, we focus on the CU-BOJAR
(B) and CU-TECTO (T) pair because they are in-
teresting competitors on their own. They both use
the same parallel corpus for lexical mapping but
operate very differently: CU-BOJAR is based on
Moses while CU-TECTO transfers at a deep syn-
tactic layer and generates target text which is more
or less grammatically correct but suffers in lexical
choice.
2.5.1 Different Set of Sentences
The mismatch in the outcomes of ?? others? and
pairwise comparisons could be caused by different
set of sentences. The pairwise ranking is collected
from the set of blocks where both CU-BOJAR and
CU-TECTO appeared (and were indeed ranked).
Each of the systems however competes in other
blocks as well, which contributes to the official ??
others?.
The set of sentences underlying the comparison
is very different and more importantly that the ba-
sis for pairwise comparisons is much smaller than
the basis of the official ?? others? interpretation.
The outcome of the official interpretation however
depends on the random set of systems your system
was compared to. In our case, it is impossible to
distinguish, whether CU-TECTO had just bad luck
on sentences and systems it was compared to when
CU-BOJAR was not in the block and/or whether the
81 blocks do not provide a reliable picture.
2.5.2 Pairwise Judgments Unreliable
To complement WMT10 rankings for the two sys-
tems and avoid the possible lower reliability due
to 5-fold ranking instead of a targeted compari-
7
Author of B says:
both both
B>T T>B fine wrong Total
T
sa
ys
:
B>T 9 - 1 1 11
T>B 2 13 - 3 18
both fine 2 - 2 3 7
both wrong 10 5 1 11 27
Total 23 18 4 18 63
Table 7: Additional annotation of 63 CU-BOJAR
(B) vs. CU-TECTO (T) sentences by two annota-
tors.
Better Both
Annotator B T fine wrong
A 24 23 5 11
C 10 12 5 36
D 32 20 2 9
M 11 18 7 27
O 23 18 4 18
Z 25 27 2 9
Total 125 118 25 110
Table 8: Blurry picture of pairwise rankings of
CU-BOJAR vs. CU-TECTO. Wins in bold.
son, we asked the main authors of both CU-BOJAR
and CU-TECTO to carry out a blind pairwise com-
parison on the exact set of 63 sentences appearing
across the 81 blocks in which both systems were
ranked. As the totals in Table 7 would suggest,
each author unwittingly recognized his system and
slightly preferred it. The details however reveal a
subtler reason for the low agreement: one of the
annotators was less picky about MT quality and
accepted 10+5 sentences completely rejected by
the other annotator. In total, these two annotators
agreed on 9 + 13 + 2 + 11 = 35 (56%) of cases
and their pairwise ? is 0.387.
A further annotation of these 63 sentences by
four more people completes the blurry picture:
the pairwise ? for each pair of our five annota-
tors ranges from 0.242 to 0.615 with the aver-
age 0.407?0.106. The multi-annotator ? (Fleiss,
1971) is 0.394 and all six annotators agree on a
single label only in 24% of cases. The agree-
ment is not better even if we merge the categories
?Both fine? and ?Both wrong? into a single one:
The pairwise ? ranges from 0.212 to 0.620 with
the average 0.405?0.116, the multi-annotator ? is
0.391. Individual annotations are given in Table 8.
Naturally, the set of these 63 sentences is not a
representative sample. Even if one of the systems
SRC It?s not completely ideal.
REF Nen?? to u?plne? idea?ln??. Ranks
PC-TRANS To nen?? u?plne? idea?ln??. 2 5
CU-BOJAR To nen?? u?plne? idea?ln??. 5 4
Table 9: Two rankings by the same annotator.
SRC FCC awarded a tunnel in Slovenia for 64 million
REF FCC byl pr?ide?len tunel ve Slovinsku za 64 milionu?
Gloss FCC was awarded a tunnel in Slovenia for 64 million
HYP1 FCC pr?ide?lil tunel ve Slovinsku za 64 milio?nu?
HYP2 FCC pr?ide?lila tunel ve Slovinsku za 64 milionu?
Gloss FCC awardedmasc/fem a tunnel in Slovenia for 64 million
Figure 5: A poor reference translation confuses
human judges. The SRC and REF differ in the ac-
tive/passive form, attributing completely different
roles to ?FCC?.
actually won, such an observation could not have
been generalized to other test sets. The purpose
of the exercise was to check whether we are at all
able to agree which of the systems translates this
specific set of sentences better. As it turns out,
even a simple pairwise ranking can fail to pro-
vide an answer because different annotators sim-
ply have different preferences.
Finally, Table 9 illustrates how poor the
WMT10 rankings can be. The exact same string
produced by two systems was ranked differently
each time ? by the same annotator. (The hypothe-
sis is a plausible translation, only the information
structure of the sentence is slightly distorted so the
translation may not fit well it the surrounding con-
text.)
3 The Impact of the Reference
Translation
3.1 Bad Reference Translations
Figure 5 illustrates the impact of poor reference
translation on manual ranking as carried out in
Section 2.5.2. Of our six independent annotations,
three annotators marked the hypotheses as ?both
fine? given the match with the source and three
annotators marked them as ?both wrong? due to
the mismatch with the reference. Given the con-
struction of the WMT test set, this particular sen-
tence comes from a Spanish original and it was
most likely translated directly to both English and
Czech.
8
Correlation of
Source Target Reference vs. ?? others?
Spanish English 0.341
English French 0.164
French English 0.098
German English 0.088
Czech English -0.041
English Czech -0.145
English Spanish -0.411
English German -0.433
Overall -0.107
Table 10: Pearson?s correlation of the relative per-
centage of blocks where the reference was in-
cluded in the ranking and the final ?? others?
of the system (the reference itself is not included
among the considered systems).
 
25
 
30
 
35
 
40
 
45
 
50
 
55
 
60
 
65
 
70
 
75  0.1
9
 
0.2
 
0.21
 
0.22
 
0.23
 
0.24
 
0.25
 
0.26
>= Others
Rela
tive 
pres
ence
 of th
e ref
eren
ce
cmu
-hea
field
-com
bo
cu-
zem
an
dfki
fbk
jhu
kit
koc
koc-
com
bo
lims
i
liu
onlin
eA
onlin
eB
rwth
rwth
-com
bo sfu
uedi
n
upps
ala
upv-
com
bo
a*x
+b
Figure 6: Correlation of the presence of the ref-
erence and the official ?? others? for English-
German evaluation.
3.2 Reference Can Skew Pairwise
Comparisons
The exact set of competing systems in each 5-fold
ranking in WMT10 evaluation is random. The ??
others? however is affected by this: a system may
suffer more losses if often compared to the refer-
ence, and similarly it may benefit from being com-
pared to a poor competitor.
To check this, we calculate the correlation be-
tween the relative presence of the reference among
the blocks where a system was judged and the
system?s official ?? others? score. Across lan-
guage, there is almost no correlation (Pearson?s
coefficient: ?0.107). However, for some language
pairs, the correlation is apparent, as listed in Ta-
ble 10. Negative correlation means: the more of-
ten the system was compared along with the refer-
ence, the worse the score of the system.
Figure 6 plots the extreme case of English-
German evaluation.
Source Target Min Avg?StdDev Max
English Czech 40 65?19 115
English French 40 66?17 110
English German 10 40?16 80
English Spanish 30 54?15 85
Czech English 5 38?13 60
French English 5 37?15 70
German English 10 32?12 65
Spanish English 35 56?11 70
Table 11: The number of post-edits per system for
each language pair to complement Figure 3 (page
12) of the WMT10 overview paper.
4 Other WMT10 Tasks
4.1 Blind Post-Editing Unreliable
WMT often carries out one more type of manual
evaluation: ?Editing the output of systems without
displaying the source or a reference translation,
and then later judging whether edited translations
were correct.? (Callison-Burch et al, 2010). We
call the evaluation ?blind post-editing? for short.
We feel that blind post-editing is more infor-
mative than system ranking. First, it constitutes
a unique comprehensibility test, and after all, MT
should aim at comprehensible output in the first
place. Second, blind post-editing can be further
analyzed to search for specific errors in system
output, see Bojar (2011) for a preliminary study.
Unfortunately, the amount of post-edits col-
lected in WMT10 varied a lot across systems and
language pairs. Table 11 provides the minimum,
average and maximum number of post-edits of
outputs of a particular MT system. We see that
e.g. while English-to-Czech has many judgments
of this kind per system, Czech-to-English is one of
the worst supported directions.
It is not surprising that conclusions based on 5
observations can be extremely deceiving. For in-
stance CU-BOJAR seems to produce 60% of out-
puts comprehensible (and thus wins in Figure 3 on
page 12 in the WMT overview paper), far better
than CMU. This is not in line with the ranking re-
sults where both rank equally (Table 5 on page 10
in the WMT overview paper). In fact, CU-BOJAR
was post-edited 5 times and 3 of these post-edits
were acceptable while CMU was post-edited 30
times and 5 of these post-edits were acceptable.
4.2 A Remark on System Combination Task
One results of WMT10 not observed in previous
years was that system combinations indeed per-
formed better than individual systems. Previous
9
Dev Set Test Set
Sententes 455 2034 Diff
GOOGLE 17.32?1.25 16.76?0.60 ?
BOJAR 16.00?1.15 16.90?0.61 ?
TECTOMT 11.48?1.04 13.19?0.58 ?
PC-TRANS 10.24?0.92 10.84?0.46 ?
EUROTRAN 9.64?0.92 11.04?0.48 ?
Table 12: BLEU scores of sample five systems in
English-to-Czech combination task.
years failed to show this clearly, because Google
Translate used to be included among the combined
systems, making it hard to improve. In WMT10,
Google Translate was excluded from system com-
bination task (except for translations involving
Czech, where it was accidentally included).
Our Table 12 provides an additional explanation
why the presence of Google among combined sys-
tems leads to inconclusive results. While the test
set was easier (based on BLEU) than the develop-
ment set for most systems, it was much harder for
Google. All system combinations were thus likely
to overfit and select Google n-grams most often.
Without access to Google powerful language mod-
els, the combination systems were likely to under-
perform Google in final fluency of the output.
5 Further Issues of Manual Evaluation
We have already seen that the comprehensibility
test by blind post-editing provides a different pic-
ture of the systems than the official ranking. Berka
et al (2011) introduced a third ?quiz-based evalu-
ation?. The quiz-like evaluation used the English-
to-Czech WMT10 systems, applied to different
texts: short text snippets were translated and an-
notators were asked to answer three yes/no ques-
tions complementing each snippet. The order of
the systems was rather different from the official
WMT10 results: CU-TECTO won the quiz-based
evaluation despite being the fourth in WMT10.
Because the texts were different in WMT10 and
the quiz-based evaluation, we asked a small group
of annotators to apply the ranking technique on the
text snippets. While not exactly comparable to the
WMT10 ranking, the WMT10 ranking was con-
firmed: CU-TECTO was again among the lowest-
scoring systems and Google won the ranking.
Bojar (2011) applies the error-flagging manual
evaluation by Vilar et al (2006) to four systems
of WMT09 English-to-Czech task. Again, the
overall order of the systems is somewhat differ-
ent when ranked by the number of errors flagged.
Mireia Farru?s and Fonollosa (2010) use a coarser
but linguistically motivated error classification for
Catalan-Spanish and suggest that differences in
ranking are caused by annotators treating some
types of errors as more serious.
In short, different types of manual evaluations
lead to different results even when identical sys-
tems and texts are evaluated.
6 Conclusion
We took a deeper look at the results of the WMT10
manual evaluation, and based on our observations,
we have some recommendations for future evalu-
ations:
? We propose to use a score which ignores
ties instead of the official ?? others? metric
which rewards ties and ?> others? which pe-
nalizes ties. Another score, ?? all in block?,
could help identify which systems are more
dominant.
? Inter-annotator agreement decreases dramat-
ically with sentence length; we recommend
including fewer sentences per block, at least
for longer sentences.
? We suggest agreement be measured based on
an empirical estimate of P (E), or at least us-
ing a more correct random clicking P (E) =
0.36.
? There is evidence of a negative correlation
between the number of times a system is
judged and its score; we recommend a deeper
analysis of this issue.
? We recommend the reference be sampled at
a lower rate than other systems, so as to play
a smaller role in the evaluation. We also rec-
ommend better quality control over the pro-
duction of the references.
And to the readers of the WMT overview paper,
we point out:
? Pairwise comparisons derived from 5-fold
rankings are sometimes unreliable. Even a
targeted pairwise comparison of two systems
can shed little light as to which is superior.
? The acceptability of post-edits is sometimes
very unreliable due to the low number of ob-
servations.
10
References
R. Artstein and M. Poesio. 2008. Inter-coder agree-
ment for computational linguistics. Computational
Linguistics, 34(4):555?596.
John J. Bartko and William T. Carpenter. 1976. On the
methods and theory of reliability. Journal of Ner-
vous and Mental Disease, 163(5):307?317.
E. M. Bennett, R. Alpert, and A. C. Goldstein. 1954.
Communications through limited questioning. Pub-
lic Opinion Quarterly, 18(3):303?308.
Jan Berka, Martin C?erny?, and Ondr?ej Bojar. 2011.
Quiz-Based Evaluation of Machine Translation.
Prague Bulletin of Mathematical Linguistics, 95:77?
86, March.
Ondr?ej Bojar. 2011. Analyzing Error Types in
English-Czech Machine Translation. Prague Bul-
letin of Mathematical Linguistics, 95:63?76, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):37?46.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: A second look. Computational lin-
guistics, 30(1):95?101.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Klaus Krippendorff. 1980. Content Analysis: An In-
troduction to Its Methodology. Sage Publications,
Beverly Hills, CA. Chapter 12.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Jose? B. Marin?o Mireia Farru?s, Marta R. Costa-jussa`
and Jose? A. R. Fonollosa. 2010. Linguistic-based
evaluation criteria to identify statistical machine
translation errors. In Proceedings of the 14th Annual
Conference of the Euoropean Association for Ma-
chine Translation (EAMT?10), pages 167?173, May.
William A. Scott. 1955. Reliability of content analy-
sis: The case of nominal scale coding. Public Opin-
ion Quarterly, 19(3):321?325.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In International Conference on Lan-
guage Resources and Evaluation, pages 697?702,
Genoa, Italy, May.
11
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 22?64,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Findings of the 2011 Workshop on Statistical Machine Translation
Chris Callison-Burch
Center for Language and Speech Processing
Johns Hopkins University
Philipp Koehn
School of Informatics
University of Edinburgh
Christof Monz
Informatics Institute
University of Amsterdam
Omar F. Zaidan
Center for Language and Speech Processing
Johns Hopkins University
Abstract
This paper presents the results of the WMT11
shared tasks, which included a translation
task, a system combination task, and a task for
machine translation evaluation metrics. We
conducted a large-scale manual evaluation of
148 machine translation systems and 41 sys-
tem combination entries. We used the rank-
ing of these systems to measure how strongly
automatic metrics correlate with human judg-
ments of translation quality for 21 evaluation
metrics. This year featured a Haitian Creole
to English task translating SMS messages sent
to an emergency response service in the af-
termath of the Haitian earthquake. We also
conducted a pilot ?tunable metrics? task to test
whether optimizing a fixed system to differ-
ent metrics would result in perceptibly differ-
ent translation quality.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at EMNLP 2011. This
workshop builds on five previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al,
2007; Callison-Burch et al, 2008; Callison-Burch
et al, 2009; Callison-Burch et al, 2010). The work-
shops feature three shared tasks: a translation task
between English and other languages, a task to com-
bine the output of multiple machine translation sys-
tems, and a task to predict human judgments of
translation quality using automatic evaluation met-
rics. The performance for each of these shared tasks
is determined through a comprehensive human eval-
uation. There were a two additions to this year?s
workshop that were not part of previous workshops:
? Haitian Creole featured task ? In addition to
translation between European language pairs,
we featured a new translation task: translating
Haitian Creole SMS messages that were sent
to an emergency response hotline in the im-
mediate aftermath of the 2010 Haitian earth-
quake. The goal of this task is to encourage re-
searchers to focus on challenges that may arise
in future humanitarian crises. We invited Will
Lewis, Rob Munro and Stephan Vogel to pub-
lish a paper about their experience developing
translation technology in response to the crisis
(Lewis et al, 2011). They provided the data
used in the Haitian Creole featured translation
task. We hope that the introduction of this new
dataset will provide a testbed for dealing with
low resource languages and the informal lan-
guage usage found in SMS messages.
? Tunable metric shared task ? We conducted
a pilot of a new shared task to use evaluation
metrics to tune the parameters of a machine
translation system. Although previous work-
shops have shown evaluation metrics other than
BLEU are more strongly correlated with human
judgments when ranking outputs from multiple
systems, BLEU remains widely used by system
developers to optimize their system parameters.
We challenged metric developers to tune the
parameters of a fixed system, to see if their met-
rics would lead to perceptibly better translation
quality for the system?s resulting output.
22
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
of translation quality.
2 Overview of the Shared Translation and
System Combination Tasks
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by
hiring people to translate news articles that were
drawn from a variety of sources from early Decem-
ber 2010. A total of 110 articles were selected, in
roughly equal amounts from a variety of Czech, En-
glish, French, German, and Spanish news sites:2
Czech: aktualne.cz (4), Novinky.cz (7), iH-
Ned.cz (4), iDNES.cz (4)
French: Canoe (5), Le Devoir (5), Le Monde (5),
Les Echos (5), Liberation (5)
Spanish: ABC.es (6), Cinco Dias (6), El Period-
ico (6), Milenio (6), Noroeste (7)
English: Economist (4), Los Angeles Times (6),
New York Times (4), Washington Post (4)
German: FAZ (3), Frankfurter Rundschau (2), Fi-
nancial Times Deutschland (3), Der Spie-
gel (5), Su?ddeutsche Zeitung (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
1http://statmt.org/wmt11/results.html
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, in some cases errors still cropped up. For in-
stance, in parts of the English-French translations,
some of the English source remains in the French
reference as if the translator forgot to delete it.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Baseline systems
To lower the barrier of entry for newcomers to
the field, we provided two open source toolkits for
phrase-based and parsing-based statistical machine
translation (Koehn et al, 2007; Li et al, 2010).
2.4 Submitted systems
We received submissions from 56 groups across 37
institutions, as listed in Tables 1, 2 and 3. We also
included two commercial off-the-shelf MT systems,
two online statistical MT systems, and five online
rule-based MT systems. (Not all systems supported
all language pairs.) We note that these nine compa-
nies did not submit entries themselves, and are there-
fore anonymized in this paper. Rather, their entries
were created by translating the test data via their web
interfaces.4 The data used to construct these systems
is not subject to the same constraints as the shared
task participants. It is possible that part of the refer-
ence translations that were taken from online news
sites could have been included in the online systems?
models, for instance. We therefore categorize all
commercial systems as unconstrained when evalu-
ating the results.
2.5 System combination
In total, we had 148 primary system entries (includ-
ing the 46 entries crawled from online sources), and
60 contrastive entries. These were made available to
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries (2), Christian Federmann for the statistical
MT entries (14), and Herve? Saint-Amand for the rule-based MT
entries (30)!
23
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,786,594 1,825,077 1,739,154 462,351
Words 51,551,370 49,411,045 54,568,499 50,551,047 45,607,269 47,978,832 10,573,983 12,296,772
Distinct words 171,174 113,655 137,034 114,487 362,563 111,934 152,788 56,095
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 132,571 115,562 136,227 122,754
Words 3,739,293 3,285,305 3,290,280 2,866,929 3,401,766 3,309,619 2,658,688 2,951,357
Distinct words 73,906 53,699 59,911 50,323 120,397 53,921 130,685 50,457
United Nations Training Corpus
Spanish? English French? English
Sentences 10,662,993 12,317,600
Words 348,587,865 304,724,768 393,499,429 344,026,111
Distinct words 578,599 564,489 621,721 729,233
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 7,227,409
Words 72,993,427 84,856,749
Distinct words 1,088,642 522,770
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,032,006 1,942,761 2,002,266 1,985,560 479,636
Words 54,720,731 55,105,358 57,860,307 48,648,697 10,770,230
Distinct words 119,315 176,896 141,742 376,128 154,129
News Language Model Data
English Spanish French German Czech
Sentence 30,888,595 3,416,184 11,767,048 17,474,133 12,333,268
Words 777,425,517 107,088,554 302,161,808 289,171,939 216,692,489
Distinct words 2,020,549 595,681 1,250,259 3,091,700 2,068,056
News Test Set
English Spanish French German Czech
Sentences 3003
Words 75,762 79,710 85,999 73,729 65,427
Distinct words 10,088 11,989 11,584 14,345 16,922
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
24
ID Participant
ALACANT University of Alicante (Sa?nchez-Cartagena et al, 2011)
CEU-UPV CEU University Cardenal Herrera
& Polytechnic University of Valencia (Zamora-Martinez and Castro-Bleda, 2011)
CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)
CMU-DYER Carnegie Mellon University - Dyer (Dyer et al, 2011)
CMU-HANNEMAN Carnegie Mellon University - Hanneman (Hanneman and Lavie, 2011)
COPENHAGEN Copenhagen Business School
CST Centre for Language Technology @ Copenhagen University (Rish?j and S?gaard, 2011)
CU-BOJAR Charles University - Bojar (Marec?ek et al, 2011)
CU-MARECEK Charles University - Marec?ek (Marec?ek et al, 2011)
CU-POPEL Charles University - Popel (Popel et al, 2011)
CU-TAMCHYNA Charles University - Tamchyna (Bojar and Tamchyna, 2011)
CU-ZEMAN Charles University - Zeman (Zeman, 2011)
DFKI-FEDERMANN Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Federmann
(Federmann and Hunsicker, 2011)
DFKI-XU Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz - Xu (Xu et al, 2011b)
HYDERABAD IIIT-Hyderabad
ILLC-UVA Institute for Logic, Language and Computation @ University of Amsterdam
(Khalilov and Sima?an, 2011)
JHU Johns Hopkins University (Weese et al, 2011)
KIT Karlsruhe Institute of Technology (Herrmann et al, 2011)
KOC Koc University (Bicici and Yuret, 2011)
LATL-GENEVA Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)
LIA-LIG Laboratoire Informatique d?Avignon @ The University of Avignon
& Laboratoire d?Informatique de Grenoble @ University of Grenoble (Potet et al, 2011)
LIMSI LIMSI (Allauzen et al, 2011)
LINGUATEC Linguatec Language Technologies (Aleksic and Thurmair, 2011)
LIU Linko?ping University (Holmqvist et al, 2011)
LIUM University of Le Mans (Schwenk et al, 2011)
PROMT ProMT
RWTH-FREITAG RWTH Aachen - Freitag (Huck et al, 2011)
RWTH-HUCK RWTH Aachen - Huck (Huck et al, 2011)
RWTH-WUEBKER RWTH Aachen - Wu?bker (Huck et al, 2011)
SYSTRAN SYSTRAN
UEDIN University of Edinburgh (Koehn et al, 2007)
UFAL-UM Charles University and University of Malta (Corb??-Bellot et al, 2005)
UOW University of Wolverhampton (Aziz et al, 2011)
UPM Technical University of Madrid (Lo?pez-Luden?a and San-Segundo, 2011)
UPPSALA Uppsala University (Koehn et al, 2007)
UPPSALA-FBK Uppsala University
& Fondazione Bruno Kessler (Hardmeier et al, 2011)
ONLINE-[A,B] two online statistical machine translation systems
RBMT-[1?5] five online rule-based machine translation systems
COMMERCIAL-[1,2] two commercial machine translation systems
Table 1: Participants in the shared translation task (European language pairs; individual system track). Not all teams
participated in all language pairs. The translations from commercial and online systems were crawled by us, not
submitted by the respective companies, and are therefore anonymized.
25
ID Participant
BBN-COMBO Raytheon BBN Technologies (Rosti et al, 2011)
CMU-HEAFIELD-COMBO Carnegie Mellon University (Heafield and Lavie, 2011)
JHU-COMBO Johns Hopkins University (Xu et al, 2011a)
KOC-COMBO Koc University (Bicici and Yuret, 2011)
LIUM-COMBO University of Le Mans (Barrault, 2011)
QUAERO-COMBO Quaero Project? (Freitag et al, 2011)
RWTH-LEUSCH-COMBO RWTH Aachen (Leusch et al, 2011)
UOW-COMBO University of Wolverhampton (Specia et al, 2010)
UPV-PRHLT-COMBO Polytechnic University of Valencia (Gonza?lez-Rubio and Casacuberta, 2011)
UZH-COMBO University of Zurich (Sennrich, 2011)
Table 2: Participants in the shared system combination task. Not all teams participated in all language pairs.
? The Quaero Project entry combined outputs they received directly from LIMSI, KIT, SYSTRAN, and RWTH.
participants in the system combination shared task.
Continuing our practice from last year?s workshop,
we separated the test set into a tuning set and a final
held-out test set for system combinations. The tun-
ing portion was distributed to system combination
participants along with reference translations, to aid
them set any system parameters.
In the European language pairs, the tuning set
consisted of 1,003 segments taken from 37 docu-
ments, whereas the test set consisted of 2,000 seg-
ments taken from 73 documents. In the Haitian Cre-
ole task, the split was 674 segments for tuning and
600 for testing.
Table 2 lists the 10 participants in the system com-
bination task.
3 Featured Translation Task
The featured translation task of WMT11 was to
translate Haitian Creole SMS messages into En-
glish. These text messages were sent by people in
Haiti in the aftermath of the January 2010 earth-
quake. In the wake of the earthquake, much of the
country?s conventional emergency response services
failed. Since cell phone towers remained stand-
ing after the earthquake, text messages were a vi-
able mode of communication. Munro (2010) de-
scribes how a text-message-based emergency report-
ing system was set up by a consortium of volunteer
organizations named ?Mission 4636? after a free
SMS short code telephone number that they estab-
lished. The SMS messages were routed to a system
for reporting trapped people and other emergencies.
Search and rescue teams within Haiti, including the
US Military, recognized the quantity and reliabil-
ity of actionable information in these messages and
used them to provide aid.
The majority of the SMS messages were writ-
ten in Haitian Creole, which was not spoken by
most of first responders deployed from overseas.
A distributed, online translation effort was estab-
lished, drawing volunteers from Haitian Creole- and
French-speaking communities around the world.
The volunteers not only translated messages, but
also categorized them and pinpointed them on a
map.5 Collaborating online, they employed their lo-
cal knowledge of locations, regional slang, abbre-
viations and spelling variants to process more than
40,000 messages in the first six weeks alone. First
responders indicated that this volunteer effort helped
to save hundreds of lives and helped direct the first
food and aid to tens of thousands. Secretary of State
Clinton described one success of the Mission 4636
program:?The technology community has set up in-
teractive maps to help us identify needs and target
resources. And on Monday, a seven-year-old girl
and two women were pulled from the rubble of a
collapsed supermarket by an American search-and-
rescue team after they sent a text message calling
for help.? Ushahidi@Tufts described another:?The
World Food Program delivered food to an informal
camp of 2500 people, having yet to receive food or
water, in Diquini to a location that 4636 had identi-
5A detailed map of Haiti was created by a crowdsourcing
effort in the aftermath of the earthquake (Lacey-Hall, 2011).
26
ID Participant
BM-I2R Barcelona Media
& Institute for Infocomm Research (Costa-jussa` and Banchs, 2011)
CMU-DENKOWSKI Carnegie Mellon University - Denkowski (Denkowski and Lavie, 2011b)
CMU-HEWAVITHARANA Carnegie Mellon University - Hewavitharana (Hewavitharana et al, 2011)
HYDERABAD IIIT-Hyderabad
JHU Johns Hopkins University (Weese et al, 2011)
KOC Koc University (Bicici and Yuret, 2011)
LIU Linko?ping University (Stymne, 2011)
UMD-EIDELMAN University of Maryland - Eidelman (Eidelman et al, 2011)
UMD-HU University of Maryland - Hu (Hu et al, 2011)
UPPSALA Uppsala University (Hardmeier et al, 2011)
Table 3: Participants in the featured translation task (Haitian Creole SMS into English; individual system track). Not
all teams participated in both the ?Clean? and ?Raw? tracks.
fied for them.?
In parallel with Rob Munro?s crowdsourcing
translation efforts, the Microsoft Translator team de-
veloped a Haitian Creole statistical machine transla-
tion engine from scratch in a compressed timeframe
(Lewis, 2010). Despite the impressive number
of translations completed by volunteers, machine
translation was viewed as a potentially useful tool
for higher volume applications or to provide trans-
lations of English medical documents into Haitian
Creole. The Microsoft Translator team quickly as-
sembled parallel data from a number of sources,
including Mission 4636 and from the archives of
Carnegie Mellon?s DIPLOMAT project (Frederking
et al, 1997). Through a series of rapid prototyp-
ing efforts, the team improved their system to deal
with non-standard orthography, reduced pronouns,
and SMS shorthand. They deployed a functional
translation system to relief workers in the field in
less than 5 days ? impressive even when measured
against previous rapid MT development efforts like
DARPA?s surprise language exercise (Oard, 2003;
Oard and Och, 2003).
We were inspired by the efforts of Rob Munro and
Will Lewis on translating Haitian Creole in the af-
termath of the disaster, so we worked with them to
create a featured task at WMT11. We thank them for
generously sharing the data they assembled in their
own efforts. We invited Rob Munro, Will Lewis,
and Stephan Vogel to speak at the workshop on the
topic of developing translation technology for future
crises, and they recorded their thoughts in an invited
publication (Lewis et al, 2011).
3.1 Haitian Creole Data
For the WMT11 featured translation task, we
anonymized the SMS Haitian Creole messages
along with the translations that the Mission 4636
volunteers created. Examples of these messages are
given in Table 4. The goal of anonymizing the SMS
data was so that it may be shared with researchers
who are developing translation and mapping tech-
nologies to support future emergency relief efforts
and social development. We ask that any researcher
working with these messages to be aware that they
are actual communications sent by people in need in
a time of crisis. Researchers who use this data are
asked to be cognizant of the following:
? Some messages may be distressing in content.
? The people who sent the messages (and who
are discussed in them) were victims of a natural
disaster and a humanitarian crisis. Please treat
the messages with the appropriate respect for
these individuals.
? The primary motivation for using this data
should be to understand how we can better re-
spond to future crises.
Participants who received the Haitian Creole data
for WMT11 were given anonymization guidelines
27
mwen se [FIRSTNAME] mwen gen twaset ki mouri mwen
mande nou ed pou nou edem map tan repons
I am [FIRSTNAME], I have three sisters who have died. I
ask help for us, I await your response.
Ki kote yap bay manje Where are they giving out food?
Eske lekol kolej marie anne kraze?mesi Was the College Marie Anne school destroyed? Thank you.
Nou pa ka anpeche moustik yo mo`de nou paske yo anpil. We can?t prevent the mosquitoes from biting because there
are so many.
tanpri ke`m ap kase mwen pa ka pran nouvel manmanm. Please heart is breaking because I have no news of my
mother.
4636:Opital Medesen san Fwontie` delmas 19 la fe`men.
Opital sen lwi gonzag nan delma 33 pran an chaj gratwit-
man tout moun ki malad ou blese
4636: The Doctors without Borders Hospital in Delmas 19
is closed. The Saint Louis Gonzaga hospital in Delmas 33
is taking in sick and wounded people for free
Mwen re?se?voua mesaj nou yo 5 sou 5 men mwen ta vle di
yon bagay kile` e koman nap kapab fe`m jwin e`d sa yo pou
moune b la kay mwen ki sinistwe? adre`s la se?
I received your message 5/5 but I would like to ask one
thing when and how will you be able to get the aid to me for
the people around my house who are victims of the earth-
quake? The address is
Sil vous plait map chehe [LASTNAME][FIRSTNAME].di
yo relem nan [PHONENUMBER].mwen se [LAST-
NAME] [FIRSTNAME]
I?m looking for [LASTNAME][FIRSTNAME]. Tell him
to call me at [PHONENUMBER] I am [LASTNAME]
[FIRSTNAME]
Bonswa mwen rele [FIRSTNAME] [LASTNAME] kay
mwen krase mwen pagin anyin poum mange ak fanmi-m
tampri di yon mo pou mwen fem jwen yon tante tou ak
mange. .mrete n
Hello my name is [FIRSTNAME] [LASTNAME]my house
fell down, I?ve had nothing to eat and I?m hungry. Please
help me find food. I live
Mwen viktim kay mwen kraze e`skem ka ale sendomeng
mwen gen paspo`
I?m a victim. My home has been destroyed. Am I allowed
to go to the Dominican Republic? I have a Passport.
KISAM DWE FE LEGEN REPLIK,ESKE MOUN SAINT
MARC AP JWENN REPLIK.
What should I do when there is an aftershock? Will the
people of Saint Marc have aftershocks?
MWEN SE YON JEN ETIDYAN AN ASYANS ENFO-
MATIK KI PASE ANPIL MIZE NAN TRANBLEMAN
DE TE 12 JANVYE A TOUT FANMIM FIN MOURI
MWEN SANTIM SEL MWEN TE VLE ALE VIV
I?m a young student in computer science, who has suffered
a lot during and after the earthquake of January 12th. All
my family has died and I feel alone. I wanted to go live.
Mw rele [FIRSTNAME], mw fe` mason epi mw abite
laple`n. Yo dim minustah ap bay djob mason ki kote pou
mw ta pase si mw ta vle jwenn nan djob sa yo.
My name is [FIRSTNAME], I?m a construction worker and
I live in La Plaine. I heard that the MINUSTAH was giving
jobs to construction workers. What do I have to go to find
one of these jobs?
Souple mande lapolis pou fe on ti pase nan magloire am-
broise prolonge zone muler ak cadet jeremie ginyin jen ga-
son ki ap pase nan zone sa yo e ki agresi
please ask the police to go to magloire ambroise going to-
wards the ?muler? area and cadet jeremie because there are
very aggressive young men in these areas
KIBO MOUN KA JWENN MANJE POU YO MANJE
ANDEYO KAPITAL PASKE DEPI 12 JANVYE YO
VOYE MANJE POU PEP LA MEN NOU PA JANM
JWENN ANYEN. NAP MOURI AK GRANGOU
Where can people get food to eat outside of the capital be-
cause since January 12th, they?ve sent food for the people
but we never received anything. We are dying of hunger
Mwen se [FIRSTNAME][LASTNAME] mwen nan aken
mwen se yon je`n ki ansent mwen te genyen yon paran ki tap
ede li mouri po`toprens, mwen pral akouye nan ko`mansman
feviye
I am [FIRSTNAME][LASTNAME] I am in Aquin I am a
pregnant young person I had a parent who was helping me,
she died in Port-au-Prince, I?m going to give birth at the
start of February
Table 4: Examples of some of the Haitian Creole SMS messages that were sent to the 4636 short code along with
their translations into English. Translations were done by volunteers who wanted to help with the relief effort. Prior
to being distributed, the messages were anonymized to remove names, phone numbers, email addresses, etc. The
anonymization guidelines specified that addresses be retained to facilitate work on mapping technologies.
28
Training set Parallel Words
sentences per lang
In-domain SMS data 17,192 35k
Medical domain 1,619 10k
Newswire domain 13,517 30k
Glossary 35,728 85k
Wikipedia parallel sentence 8,476 90k
Wikipedia named entities 10,499 25k
The bible 30,715 850k
Haitisurf dictionary 3,763 4k
Krengle dictionary 1,687 3k
Krengle sentences 658 3k
Table 5: Training data for the Haitian Creole-English fea-
tured translation task. The in-domain SMS data consists
primarily of raw (noisy) SMS data. The in-domain data
was provided by Mission 4636. The other data is out-of-
domain. It comes courtesy of Carnegie Mellon Univer-
sity, Microsoft Research, Haitisurf.com, and Krengle.net.
alongside the SMS data. The WMT organizers re-
quested that if they discovered messages with incor-
rect or incomplete anonymization, that they notify
us and correct the anonymization using the version
control repository.
To define the shared translation task, we divided
the SMS messages into an in-domain training set,
along with designated dev, devtest, and test sets. We
coordinated with Microsoft and CMU to make avail-
able additional out-of-domain parallel corpora. De-
tails of the data are given in Table 5. In addition
to this data, participants in the featured task were
allowed to use any of the data provided in the stan-
dard translation task, as well as linguistic tools such
as taggers, parsers, or morphological analyzers.
3.2 Clean and Raw Test Data
We provided two sets of testing and development
data. Participants used their systems to translate two
test sets consisting of 1,274 unseen Haitian Creole
SMS messages. One of the test sets contains the
?raw? SMS messages as they were sent, and the
other contains messages that were cleaned up by hu-
man post-editors. The English side is the same in
both cases, and the only difference is the Haitian
Creole input sentences.
The post-editors were Haitian Creole language
informants hired by Microsoft Research. They pro-
vided a number of corrections to the SMS messages,
including expanding SMS shorthands, correcting
spelling/grammar/capitalization, restoring diacritics
that were left out of the original message, and
cleaning up accented characters that were lost when
the message was transmitted in the wrong encoding.
Original Haitian Creole messages:
Sil vou ple? e?de mwen avek moun ki vik-
tim yo nan tranbleman de te? a,ki kite? poto-
prins ki vini nan provins- mwen ede ak ti
kob mwen te ginyin kounie? a
4636: Manje vin pi che nan PaP apre tran-
bleman te-a. mamit diri ap van?n 250gd
kounye, sete 200gd avan. Mayi-a 125gd,
avan sete 100gd
Edited Haitian Creole messages:
Silvouple ede mwen ave`k moun ki viktim
yo nan tranblemannte` a, ki kite Po`toprens
ki vini nan pwovens, mwen ede ak ti ko`b
mwen te genyen kounye a
4636: Manje vin pi che` nan PaP apre tran-
blemannte` a. Mamit diri ap vann 250gd
kounye a, sete 200gd avan. Mayi-a 125gd,
avan sete 100gd.
For the test and development sets the informants
also edited the English translations. For instance,
there were cases where the original crowdsourced
translation summarized the content of the message
instead of translating it, instances where parts of
the source were omitted, and where explanatory
notes were added. The editors improved the trans-
lations so that they were more suitable for machine
translation, making them more literal, correcting
disfluencies on the English side, and retranslating
them when they were summaries.
Crowdsourced English translation:
We are in the area of Petit Goave, we
would like .... we need tents and medi-
cation for flu/colds...
Post-edited translation:
We are in the area of Petit Goave, we
would like to receive assistance, however,
29
it should not be the way I see the Minus-
tah guys are handling the people. We need
lots of tents and medication for flu/colds,
and fever
The edited English is provided as the reference for
both the ?clean? and the ?raw? sets, since we intend
that distinction to refer to the form that the source
language comes in, rather than the target language.
Tables 47 and 48 in the Appendix show a signifi-
cant difference in the translation quality between the
clean and the raw test sets. In most cases, systems?
output for the raw condition was 4 BLEU points
lower than for the clean condition. We believe that
the difference in performance on the raw vs. cleaned
test sets highlight the importance of handling noisy
input data.
All of the in-domain training data is in the raw for-
mat. The original SMS messages are unaltered, and
the translations are just as the volunteered provided
them. In some cases, the original SMS messages are
written in French or English instead of Haitian Cre-
ole, or contain a mixture of languages. It may be
possible to further improve the quality of machine
translation systems trained from this data by improv-
ing the quality of the data itself.
3.3 Goals and Challenges
The goals of the Haitian Creole to English transla-
tion task were:
? To focus researchers on the problems presented
by low resource languages
? To provide a real-world data set consisting of
SMS messages, which contain abbreviations,
non-standard spelling, omitted diacritics, and
other noisy character encodings
? To develop techniques for building translation
systems that will be useful in future crises
There are many challenges in translating noisy
data in a low resource language, and there are a vari-
ety of strategies that might be considered to attempt
to tackle them. For instance:
? Automated cleaning of the raw (noisy) SMS
data in the training set.
? Leveraging a larger French-English model to
translate out of vocabulary Haitian words, by
creating a mapping from Haitian words onto
French.
? Incorporation of morphological and/or syntac-
tic models to better cope with the low resource
language pair.
It is our hope that by introducing this data as a
shared challenge at WMT11 that we will establish a
useful community resource so that researchers may
explore these challenges and publish about them in
the future.
4 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of
our workshop. We distributed the workload across
a number of people, including shared-task partici-
pants, interested volunteers, and a small number of
paid annotators (recruited by the participating sites).
More than 130 people participated in the manual
evaluation, with 91 people putting in more than an
hour?s worth of effort, and 29 putting in more than
four hours. There was a collective total of 361 hours
of labor.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for the different ranking tasks is given in Ta-
ble 6.
We performed the manual evaluation of the indi-
vidual systems separately from the manual evalua-
tion of the system combination entries, rather than
comparing them directly against each other. Last
year?s results made it clear that there is a large (ex-
pected) gap in performance between the two groups.
This year, we opted to reduce the number of pairwise
30
comparisons with the hope that we would be more
likely to find statistically significant differences be-
tween the systems in the same groups. To that same
end, we also eliminated the editing/acceptability
task that was featured in last year?s evaluation, in-
stead we had annotators focus solely on the system
ranking task.
4.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
With the exception of a few tasks in the system
combination track, there were many more than 5
systems participating in any given task?up to 23
for the English-German individual systems track.
Rather than attempting to get a complete ordering
over the systems, we instead relied on random se-
lection and a reasonably large sample size to make
the comparisons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than or equal to other systems. Specif-
ically, each block in which A appears includes four
implicit pairwise comparisons (against the other pre-
sented systems). A is rewarded once for each of the
four comparisons in which A wins or ties. A?s score
is the number of such winning (or tying) pairwise
comparisons, divided by the total number of pair-
wise comparisons involving A.
The system scores are reported in Section 5. Ap-
pendix A provides detailed tables that contain pair-
wise head-to-head comparisons between pairs of
systems.
4.2 Inter- and Intra-annotator agreement in
the ranking task
We were interested in determining the inter- and
intra-annotator agreement for the ranking task, since
a reasonable degree of agreement must exist to sup-
port our process as a valid evaluation setup. To en-
sure we had enough data to measure agreement, we
purposely designed the sampling of source segments
and translations shown to annotators in a way that
ensured some items would be repeated, both within
the screens completed by an individual annotator,
and across screens completed by different annota-
tors.
We did so by ensuring that 10% of the generated
screens are exact repetitions of previously gener-
ated screen within the same batch of screens. Fur-
thermore, even within the other 90%, we ensured
that a source segment appearing in one screen ap-
pears again in two more screens (though with differ-
ent system outputs). Those two details, intentional
repetition of source sentences and intentional repeti-
tion of system outputs, ensured we had enough data
to compute meaningful inter- and intra-annotator
agreement rates.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
The above definition of ? is actually used by sev-
eral definitions of agreement measures, which differ
in how P (A) and P (E) are computed.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
31
Inividual System Track System Combination Track
Language Pair # Systems Label Labels # Systems Label Labels
Count per System Count per System
Czech-English 8 2,490 276.7 4 1,305 261.0
English-Czech 10 8,985 816.8 2 2,700 900.0
German-English 20 4,620 220.0 8 1,950 216.7
English-German 22 6,540 284.4 4 2,205 441.0
Spanish-English 15 2,850 178.1 6 2,115 302.1
English-Spanish 15 5,595 349.7 4 3,000 600.0
French-English 18 3,540 186.3 6 1,500 214.3
English-French 17 4,590 255.0 2 900 300.0
Haitian (Clean)-English 9 3,360 336.0 3 1,200 300.0
Haitian (Raw)-English 6 1,875 267.9 2 900 300.0
Urdu-English 8 3,165 351.7 N/A N/A N/A
(tunable metrics task)
Overall 148 47,610 299.4 41 17,775 348.5
Table 6: A summary of the WMT11 ranking task, showing the number of systems and number of labels collected in
each of the individual and system combination tracks. The system count does not include the reference translation,
which was included in the evaluation, and so a value under ?Labels per System? can be obtained only after adding 1
to the system count, before dividing the label count (e.g. in German-English, 4, 620/21 = 220.0).
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.6
6Even if we wanted to assume a ?random clicker? model,
setting P (E) = 13 is still not entirely correct. Given that
Table 7 gives ? values for inter-annotator and
intra-annotator agreement across the various evalu-
ation tasks. These give an indication of how often
different judges agree, and how often single judges
are consistent for repeated judgments, respectively.
There are some general and expected trends that
can be seen in this table. First of all, intra-annotator
agreement is higher than inter-annotator agreement.
Second, reference translations are noticeably better
than other system outputs, which means that anno-
tators have an artificially high level of agreement on
pairwise comparisons that include a reference trans-
lation. For this reason, we also report the agreement
levels when such comparisons are excluded.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moder-
ate, 0.6? 0.8 is substantial, and 0.8? 1.0 is almost
perfect. Based on these interpretations, the agree-
ment for sentence-level ranking is moderate to sub-
stantial for most tasks.
annotators rank five outputs at once, P (A = B) = 15 , not
1
3 , since there are only five (out of 25) label pairs that satisfy
A = B. Working this back into P (E)?s definition, we have
P (A > B) = P (A < B) = 25 , and therefore P (E) = 0.36
rather than 0.333.
32
INTER-ANNOTATOR AGREEMENT (I.E. ACROSS ANNOTATORS)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320
European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389
Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446
Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509
Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437
WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409
INTRA-ANNOTATOR AGREEMENT (I.E. SELF-CONSISTENCY)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512
European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571
Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539
Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675
Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774
WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580
Table 7: Inter- and intra-annotator agreement rates, for the various manual evaluation tracks of WMT11. See Tables 49
and 50 below for a detailed breakdown by language pair.
However, one result that is of concern is that
agreement rates are noticeably lower for European
language pairs, in particular for the individual sys-
tems track. When excluding reference comparisons,
the inter- and intra-annotator agreement levels are
0.320 and 0.512, respectively. Not only are those
numbers lower than for the other tasks, but they
are also lower than last year?s numbers, which were
0.409 and 0.580.
We investigated this result a bit deeper. Tables 49
and 50 in the Appendix break down the results fur-
ther, by reporting agreement levels for each lan-
guage pair. One observation is that the agreement
level for some language pairs deviates in a non-
trivial amount from the overall agreement rate.
Let us focus on inter-annotator agreement rates
in the individual track (excluding reference compar-
isons), in the top right portion of Table 49. The over-
all ? is 0.320, but it ranges from 0.264 for German-
English, to 0.477 for Spanish-English.
What distinguishes those two language pairs from
each other? If we examine the results in Table 8,
we see that Spanish-English had two very weak sys-
tems, which were likely easy for annotators to agree
on comparisons involving them. (This is the con-
verse of annotators agreeing more often on com-
parisons involving the reference.) English-French is
similar in that regard, and it too has a relatively high
agreement rate.
On the other hand, the participants in German-
English formed a large pool of more closely-
matched systems, where the gap separating the bot-
tom system is not as pronounced. So it seems that
the low agreement rates are indicative of a more
competitive evaluation and more closely-matched
systems.
5 Results of the Translation Tasks
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
33
Czech-English
1023?1166 comparisons/system
System C? ?others
UEDIN ?? Y 0.69
ONLINE-B ? N 0.68
CU-BOJAR N 0.60
JHU N 0.57
UPPSALA Y 0.57
SYSTRAN N 0.51
CST Y 0.47
CU-ZEMAN Y 0.44
Spanish-English
583?833 comparisons/system
System C? ?others
ONLINE-B ? N 0.72
ONLINE-A ? N 0.72
KOC ? Y 0.67
SYSTRAN ? N 0.66
ALACANT ? N 0.66
RBMT-1 N 0.63
RBMT-3 N 0.61
RBMT-2 N 0.60
RBMT-4 N 0.60
RBMT-5 N 0.51
UEDIN Y 0.51
UPM Y 0.50
UFAL-UM Y 0.47
HYDERABAD Y 0.17
CU-ZEMAN Y 0.16
French-English
608?883 comparisons/system
System C? ?others
ONLINE-A ? N 0.66
LIMSI ?? Y+G 0.66
ONLINE-B ? N 0.66
LIA-LIG Y 0.64
KIT ?? Y+G 0.64
LIUM Y+G 0.63
CMU-DENKOWSKI ? Y 0.62
JHU Y+G 0.61
RWTH-HUCK Y+G 0.58
RBMT-1 ? N 0.58
CMU-HANNEMAN Y+G 0.58
RBMT-3 N 0.55
SYSTRAN N 0.54
RBMT-4 N 0.53
RBMT-2 N 0.52
UEDIN Y 0.50
RBMT-5 N 0.45
CU-ZEMAN Y 0.37
English-Czech
3126?3397 comparisons/system
System C? ?others
ONLINE-B ? N 0.65
CU-BOJAR N 0.64
CU-MARECEK ? N 0.63
CU-TAMCHYNA N 0.62
UEDIN ? Y 0.59
CU-POPEL ? Y 0.58
COMMERCIAL2 N 0.51
COMMERCIAL1 N 0.51
JHU N 0.49
CU-ZEMAN Y 0.43
English-Spanish
1300?1480 comparisons/system
System C? ?others
ONLINE-B ? N 0.74
ONLINE-A ? N 0.72
RBMT-3 ? N 0.71
PROMT ? N 0.70
CEU-UPV ? Y 0.65
UEDIN ? Y 0.64
UPPSALA ? Y 0.61
RBMT-4 N 0.61
RBMT-1 N 0.60
UOW Y 0.59
RBMT-2 N 0.57
KOC Y 0.56
RBMT-5 N 0.54
CU-ZEMAN Y 0.49
UPM Y 0.34
English-French
868?1121 comparisons/system
System C? ?others
LIMSI ?? Y+G 0.73
ONLINE-B ? N 0.70
KIT ?? Y+G 0.69
RWTH-HUCK Y+G 0.65
LIUM Y+G 0.64
RBMT-1 N 0.61
ONLINE-A N 0.60
UEDIN Y 0.58
RBMT-3 N 0.58
RBMT-5 N 0.55
UPPSALA Y 0.55
JHU Y 0.55
UPPSALA-FBK Y 0.54
RBMT-4 N 0.49
RBMT-2 N 0.46
LATL-GENEVA N 0.39
CU-ZEMAN Y 0.20
German-English
741?998 comparisons/system
System C? ?others
ONLINE-B ? N 0.72
CMU-DYER ?? Y+G 0.66
ONLINE-A ? N 0.66
RBMT-3 N 0.64
LINGUATEC N 0.63
RBMT-4 N 0.61
RBMT-1 N 0.60
DFKI-XU N 0.60
RWTH-WUEBKER ? Y+G 0.59
KIT Y+G 0.57
LIU Y 0.57
LIMSI Y+G 0.56
RBMT-5 N 0.56
UEDIN Y 0.55
RBMT-2 N 0.54
CU-ZEMAN Y 0.47
UPPSALA Y 0.47
KOC Y 0.45
JHU Y+G 0.43
CST Y 0.37
English-German
1051?1230 comparisons/system
System C? ?others
RBMT-3 ? N 0.73
ONLINE-B ? N 0.73
RBMT-1 ? N 0.70
DFKI-FEDERMANN ? N 0.68
DFKI-XU N 0.67
RBMT-4 ? N 0.66
RBMT-2 ? N 0.66
ONLINE-A ? N 0.65
LIMSI ? Y+G 0.65
KIT ? Y 0.64
UEDIN Y 0.60
LIU Y 0.59
RBMT-5 N 0.58
RWTH-FREITAG Y 0.56
COPENHAGEN ? Y 0.56
JHU Y 0.54
KOC Y 0.53
UOW Y 0.53
CU-TAMCHYNA Y 0.50
UPPSALA Y 0.49
ILLC-UVA Y 0.48
CU-ZEMAN Y 0.38
C? indicates whether system is constrained: trained only using supplied training data, standard monolingual linguis-
tic tools, and, optionally, LDC?s English Gigaword. Eentries that used the Gigaword are marked with +G.
? indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 8: Official results for the WMT11 translation task. Systems are ordered by their ?others score, reflecting how
often their translations won or tied pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
34
Czech-English
1036?1042 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.64
BBN-COMBO ? 0.62
JHU-COMBO 0.58
UPV-PRHLT-COMBO 0.47
English-Czech
1788?1792 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.48
UPV-PRHLT-COMBO 0.41
German-English
811?927 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.70
RWTH-LEUSCH-COMBO 0.65
BBN-COMBO 0.61
UZH-COMBO ? 0.60
JHU-COMBO 0.56
UPV-PRHLT-COMBO 0.52
QUAERO-COMBO 0.46
KOC-COMBO 0.45
English-German
1746?1752 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.61
UZH-COMBO ? 0.58
UPV-PRHLT-COMBO 0.56
KOC-COMBO 0.46
Spanish-English
1132?1249 comparisons/combo
System ?others
RWTH-LEUSCH-COMBO ? 0.71
CMU-HEAFIELD-COMBO ? 0.67
BBN-COMBO ? 0.64
UPV-PRHLT-COMBO 0.64
JHU-COMBO 0.62
KOC-COMBO 0.56
English-Spanish
2360?2378 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.69
UOW-COMBO 0.63
UPV-PRHLT-COMBO 0.59
KOC-COMBO 0.58
French-English
820?916 comparisons/combo
System ?others
BBN-COMBO ? 0.67
RWTH-LEUSCH-COMBO ? 0.63
CMU-HEAFIELD-COMBO 0.62
JHU-COMBO ? 0.59
LIUM-COMBO 0.53
UPV-PRHLT-COMBO 0.53
English-French
586?587 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.51
UPV-PRHLT-COMBO 0.43
? indicates a win: no other system combination is statistically significantly better at p-level?0.10 in pairwise
comparison.
Table 9: Official results for the WMT11 system combination task. Systems are ordered by their ?others score,
reflecting how often their translations won or tied pairwise comparisons. For detailed head-to-head comparisons, see
Appendix A.
35
Haitian Creole (Clean)-English
(individual systems)
1256?1435 comparisons/system
System ?others
BM-I2R ? 0.71
CMU-DENKOWSKI 0.66
CMU-HEWAVITHARANA 0.64
UMD-EIDELMAN 0.63
UPPSALA 0.57
LIU 0.55
UMD-HU 0.52
HYDERABAD 0.43
KOC 0.31
Haitian Creole (Raw)-English
(individual systems)
1065?1136 comparisons/system
System ?others
BM-I2R ? 0.65
CMU-HEWAVITHARANA 0.60
CMU-DENKOWSKI 0.59
LIU 0.55
UMD-EIDELMAN 0.52
JHU 0.41
Haitian Creole (Clean)-English
(system combinations)
896?898 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO ? 0.52
UPV-PRHLT-COMBO 0.48
KOC-COMBO 0.38
Haitian Creole (Raw)-English
(system combinations)
600?600 comparisons/combo
System ?others
CMU-HEAFIELD-COMBO 0.47
UPV-PRHLT-COMBO 0.43
? indicates a win: no other system is statistically significantly better at p-level?0.10 in pairwise comparison.
Table 10: Official results for the WMT11 featured translation task (Haitian Creole SMS into English). Systems are
ordered by their ?others score, reflecting how often their translations won or tied pairwise comparisons. For detailed
head-to-head comparisons, see Appendix A.
36
Tables 8?10 show the system ranking for each
of the translation tasks. For each language pair,
we define a system as ?winning? if no other system
was found statistically significantly better (using the
Sign Test, at p ? 0.10). In some cases, multiple sys-
tems are listed as winners, either due to a large num-
ber of participants or a low number of judgments per
system pair, both of which are factors that make it
difficult to achieve statistical significance.
We start by examining the results for the individ-
ual system track for the European languages (Ta-
ble 8). In Spanish?English and German?English,
unconstrained systems are observed to perform bet-
ter than constrained systems. In other language
pairs, particularly French?English, constrained
systems are found to be able to be on the same level
or outperform unconstrained systems. It also seems
that making use of the Gigaword corpora is likely
to yield better systems, even when translating out of
English, as in English-French and English-German.
For English-German the rule-based MT systems per-
formed well.
Of the participating teams, there is no individ-
ual system clearly outperforming all other systems
across the different language pairs. However, one
of the crawled systems, ONLINE-B, performs con-
sistently well, being one of the winners in all eight
language pairs.
As for the system combination track (Table 9),
the CMU-HEAFIELD-COMBO entry performed quite
well, being a winner in seven out of eight language
pairs. This performance is carried over to the Haitian
Creole task, where it again comes out on top (Ta-
ble 10). In the individual track of the Haitian Creole
task, BM-I2R is the sole winner in both the ?clean?
and ?raw? tracks.
6 Evaluation Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
the manual evaluation is useful for validating auto-
matic evaluation metrics. Our evaluation shared task
is similar to the MetricsMATR workshop (Metrics
for MAchine TRanslation) that NIST runs (Przy-
bocki et al, 2008; Callison-Burch et al, 2010). Ta-
ble 11 lists the participants in this task, along with
their metrics.
A total of 21 metrics and their variants were sub-
mitted to the evaluation task by 9 research groups.
We asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 39?48. The main goal of the
evaluation shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
This year the strongest metric was a new metric
developed by Columbia and ETS called MTeRater-
Plus. MTeRater-Plus is a machine-learning-based
metric that use features from ETS?s e-rater, an auto-
mated essay scoring engine designed to assess writ-
ing proficiency (Attali and Burstein, 2006). The fea-
tures include sentence-level and document-level in-
formation. Some examples of the e-rater features
include:
? Preposition features that calculate the proba-
bility of prepositions appearing in the given
context of a sentence (Tetreault and Chodorow,
2008)
? Collocation features that indicate whether the
collocations in the document are typical of na-
tive use (Futagi et al, 2008).
? A sentence fragment feature that counts the
number of ill-formed sentences in a document.
? A feature that counts the number of words with
inflection errors
? A feature that counts the the number of article
errors in the sentence citeHan2006.
MTeRater uses only the e-rater features, and mea-
sures fluency without any need for reference transla-
tions. MTeRater-Plus is a meta-metric that incorpo-
rates adequacy by combining MTeRater with other
MT evaluation metrics and heuristics that take the
reference translations into account.
Please refer to the proceedings for papers provid-
ing detailed descriptions of all of the metrics.
37
Metric IDs Participant
AMBER, AMBER-NL, AMBER-IT National Research Council Canada (Chen and Kuhn, 2011)
F15, F15G3 Koc? University (Bicici and Yuret, 2011)
METEOR-1.3-ADQ, METEOR-1.3-RANK Carnegie Mellon University (Denkowski and Lavie, 2011a)
MTERATER, MTERATER-PLUS Columbia / ETS (Parton et al, 2011)
MP4IBM1, MPF, WMPF DFKI (Popovic?, 2011; Popovic? et al, 2011)
PARSECONF DFKI (Avramidis et al, 2011)
ROSE, ROSE-POS The University of Sheffield (Song and Cohn, 2011)
TESLA-B, TESLA-F, TESLA-M National University of Singapore (Dahlmeier et al, 2011)
TINE University of Wolverhampton (Rios et al, 2011)
BLEU provided baseline (Papineni et al, 2002)
TER provided baseline (Snover et al, 2006)
Table 11: Participants in the evaluation shared task. For comparison purposes, we include the BLEU and TER metrics
as baselines.
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
A
V
E
R
A
G
E
W
/O
C
Z
System-level correlation for translation out of English
TESLA-M .90 .95 .96 .94
TESLA-B .81 .90 .91 .87
MPF .72 .63 .87 .89 .78 .80
WMPF .72 .61 .87 .89 .77 .79
MP4IBM1 -.76 -.91 -.71 -.61 .75 .74
ROSE .65 .41 .90 .86 .71 .73
BLEU .65 .44 .87 .86 .70 .72
AMBER-TI .56 .54 .88 .84 .70 .75
AMBER .56 .53 .87 .84 .70 .74
AMBER-NL .56 .45 .88 .83 .68 .72
F15G3 .50 .30 .89 .84 .63 .68
METEORrank .65 .30 .74 .85 .63 .63
F15 .52 .19 .86 .85 .60 .63
TER -.50 -.12 -.81 -.84 .57 .59
TESLA-F .86 .80 -.83 .28
Table 12: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value. We did not calculate correlations with the hu-
man judgments for the system combinations for the out of
English direction, because none of them had more than 4
items.
6.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation quality
at the system-level using Spearman?s rank correla-
tion coefficient ?. We converted the raw scores as-
signed to each system into ranks. We assigned a hu-
man ranking to the systems based on the percent of
time that their translations were judged to be better
than or equal to the translations of any other system
in the manual evaluation. The reference was not in-
cluded as an extra translation.
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 13 for translations into English, and Table 12
out of English, sorted by average correlation across
the language pairs. The highest correlation for
each language pair and the highest overall average
are bolded. This year, nearly all of the metrics
38
C
Z
-E
N
-
8
S
Y
S
T
E
M
S
D
E
-E
N
-
20
S
Y
S
T
E
M
S
D
E
-E
N
-
8
C
O
M
B
O
S
E
S
-E
N
-
15
S
Y
S
T
E
M
S
E
S
-E
N
-
6
C
O
M
B
O
S
F
R
-E
N
-
18
S
Y
S
T
E
M
S
F
R
-E
N
-
6
C
O
M
B
O
S
A
V
E
R
A
G
E
(E
U
R
O
P
E
A
N
L
A
N
G
S
)
H
T
-E
N
(C
L
E
A
N
)
-
9
S
Y
S
T
E
M
S
H
T
-E
N
(R
A
W
)
-
6
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
(A
L
L
L
A
N
G
S
)
System-level correlation for metrics scoring translations into English
MTERATER-PLUS -.95 -.90 -.93 -.91 -.94 -.93 -.77 .90 -.82 -.54 .85
TINE-SRL-MATCH .95 .69 .95 .95 1.00 .87 .66 .87
TESLA-F .95 .70 .98 .96 .94 .90 .60 .86 .93 .83 .87
TESLA-B .98 .88 .98 .91 .94 .91 .31 .84 .93 .83 .85
MTERATER -.91 -.88 -.91 -.88 -.89 -.79 -.60 .83 .13 .77 .55
METEOR-1.3-ADQ .93 .68 .91 .91 .83 .93 .66 .83 .95 .77 .84
TESLA-M .95 .94 .95 .82 .94 .87 .31 .83 .95 .83 .84
METEOR-1.3-RANK .91 .71 .91 .88 .77 .93 .66 .82 .95 .83 .84
AMBER-NL .88 .58 .91 .88 .94 .94 .60 .82
AMBER-TI .88 .63 .93 .85 .83 .94 .60 .81
AMBER .88 .59 .91 .86 .83 .95 .60 .80
MPF .95 .69 .91 .83 .60 .87 .54 .77 .95 .77 .79
WMPF .95 .66 .86 .83 .60 .87 .54 .76 .93 .77 .78
F15 .93 .45 .88 .96 .49 .87 .60 .74
F15G3 .93 .48 .83 .94 .49 .88 .60 .74
ROSE .88 .59 .83 .92 .60 .86 .26 .70 .93 .77 .74
BLEU .88 .48 .83 .90 .49 .85 .43 .69 .90 .83 .73
TER -.83 -.33 -.64 -.89 -.37 -.77 -.89 .67 -.93 -.83 .72
MP4IBM1 -.91 -.56 -.50 -.12 -.43 -.08 .14 .35
DFKI-PARSECONF .31 .52
Table 13: System-level Spearman?s rho correlation of the automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute value for the European languages. We did not calculate
correlations with the human judgments for the system combinations for Czech to English and for Haitian Creole to
English, because they had too few items (? 4) for reliable statistics.
39
F
R
-E
N
(6
33
7
PA
IR
S
)
D
E
-E
N
(8
95
0
PA
IR
S
)
E
S
-E
N
(5
97
4
PA
IR
S
)
C
Z
-E
N
(3
69
5
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
MTERATER-PLUS .30 .36 .45 .36 .37
TESLA-F .28 .24 .39 .32 .31
TESLA-B .28 .26 .36 .29 .30
METEOR-1.3-RANK .23 .25 .38 .28 .29
METEOR-1.3-ADQ .24 .25 .37 .27 .28
MPF .25 .23 .34 .28 .28
AMBER-TI .24 .26 .33 .27 .28
AMBER .24 .25 .33 .27 .27
WMPF .24 .23 .34 .26 .27
AMBER-NL .24 .24 .30 .27 .26
MTERATER .19 .26 .33 .24 .26
TESLA-M .21 .23 .29 .23 .24
TINE-SRL-MATCH .20 .19 .30 .24 .23
F15G3 .17 .15 .29 .21 .21
F15 .16 .14 .27 .22 .20
MP4IBM1 .15 .16 .18 .12 .15
DFKI-PARSECONF n/a .24 n/a n/a
Table 14: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year included two metrics, MTeRater and TINE,
as well as metrics that have demonstrated strong cor-
relation in previous years like TESLA and Meteor.
6.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
lation coefficient. The reference was not included as
an extra translation.
We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
E
N
-F
R
(6
93
4
PA
IR
S
)
E
N
-D
E
(1
07
32
PA
IR
S
)
E
N
-E
S
(8
83
7
PA
IR
S
)
E
N
-C
Z
(1
16
51
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
AMBER-TI .32 .22 .31 .21 .27
AMBER .31 .21 .31 .22 .26
MPF .31 .22 .30 .20 .26
WMPF .31 .22 .29 .19 .25
AMBER-NL .30 .19 .29 .20 .25
METEOR-1.3-RANK .31 .14 .26 .19 .23
F15G3 .26 .08 .22 .13 .17
F15 .26 .07 .22 .12 .17
MP4IBM1 .21 .13 .13 .06 .13
TESLA-B .29 .20 .28 n/a
TESLA-M .25 .18 .27 n/a
TESLA-F .30 .19 .26 n/a
Table 15: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 14 for trans-
lations into English, and Table 15 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
40
ID Participant Metric Name
CMU-METEOR Carnegie Mellon University METEOR (Denkowski and Lavie, 2011a)
CU-SEMPOS-BLEU Charles University SemPOS/BLEU (Macha?c?ek and Bojar, 2011)
NUS-TESLA-F National University of Singapore TESLA-F (Dahlmeier et al, 2011)
RWTH-CDER RWTH Aachen CDER (Leusch and Ney, 2009)
SHEFFIELD-ROSE The University of Sheffield ROSE (single reference) (Song and Cohn, 2011)
STANFORD-DCP Stanford DCP (based on Liu and Gildea (2005))
BLEU provided baseline BLEU
BLEU-SINGLE provided baseline BLEU (single reference)
Table 16: Participants in the tunable-metric shared task. For comparison purposes, we included two BLEU-optimized
systems in the evaluation as baselines.
bolded. There is a clear winner for the metrics that
score translations into English: the MTeRater-Plus
metric (Parton et al, 2011) has the highest segment
level correlation across the board. For metrics that
score translation into other languages, there is not
such a clear-cut winner. The AMBER metric variants
do well, as do MPF and WMPF.
7 Tunable Metrics Task
This year we introduced a new shared task that fo-
cuses on using evaluation metrics to tune the param-
eters of a statistical machine translation system. The
intent of this task was to get researchers who de-
velop automatic evaluation metrics for MT to work
on the problem of using their metric to optimize
the parameters of MT systems. Previous workshops
have demonstrated that a number of metrics perform
better than BLEU in terms of having stronger cor-
relation with human judgments about the rankings
of multiple machine translation systems. However,
most MT system developers still optimize the pa-
rameters of their systems to BLEU. Here we aim
to investigate the question of whether better metrics
will result in better quality output when a system is
optimized to them.
Because this was the first year that we ran the
tunable metrics task, participation was limited to a
few groups on an invitation-only basis. Table 16
lists the participants in this task. Metrics developers
were invited to integrate their evaluation metric into
a MERT optimization routine, which was then used
to tune the parameters of a fixed statistical machine
translation system. We evaluated whether the sys-
tem tuned on their metrics produced higher-quality
output than the baseline system that was tuned to
BLEU, as is typically done. In order to evaluate
whether the quality was better, we conducted a man-
ual evaluation, in the same fashion that we evalu-
ate the different MT systems submitted to the shared
translation task.
We provide the participants with a fixed MT sys-
tem for Urdu-English, along with a small parallel
set to be used for tuning. Specifically, we provide
developers with the following components:
? Decoder - the Joshua decoder was used in this
pilot.
? Decoder configuration file - a Joshua configu-
ration file that ensures all systems use the same
search parameters.
? Translation model - an Urdu-to-English trans-
lation model, with syntax-based SCFG rules
(Baker et al, 2010).
? Language model - a large 5-gram language
model trained on the English Gigaword corpus
? Development set - a development set, with 4
English reference sets, to be used to optimize
the system parameters.
? Test set - a test set consisting of 883 Urdu sen-
tences, to be translated by the tuned system (no
references provided).
? Optimization routine - we provide an imple-
mentation of minimum error rate training that
allows new metrics to be easily integrated as
the objective function.
41
Tunable Metrics Task
1324?1484 comparisons/system
System ?others >others
BLEU ? 0.79 0.28
BLEU-SINGLE ? 0.77 0.27
CMU-METEOR ? 0.76 0.27
RWTH-CDER 0.76 0.26
CU-SEMPOS-BLEU ? 0.74 0.29
STANFORD-DCP ? 0.73 0.27
NUS-TESLA-F 0.68 0.28
SHEFFIELD-ROSE 0.05 0.00
? indicates a win: no other system combination is sta-
tistically significantly better at p-level?0.10 in pair-
wise comparison.
Table 17: Official results for the WMT11 tunable-metric
task. Systems are ordered by their ?others score, re-
flecting how often their translations won or tied pairwise
comparisons. The > column reflects how often a system
strictly won a pairwise comparison.
We provided the metrics developers with Omar
Zaidan?s Z-MERT software (Zaidan, 2009), which
implements Och (2003)?s minimum error rate train-
ing procedure. Z-MERT is designed to be modular
with respect to the objective function, and allows
BLEU to be easily replaced with other automatic
evaluation metrics. Metric developers incorporated
their metrics into Z-MERT by subclassing the Eval-
uationMetric.java abstract class. They ran Z-MERT
on the dev set with the provided decoder/models,
and created a weight vector for the system param-
eters.
Each team produced a distinct final weight vec-
tor, which was used to produce English translations
of sentences in the test set. The different transla-
tions produced by tuning the system to different met-
rics were then evaluated using the manual evaluation
pipeline.7
7.1 Results of the Tunable Metrics Task
The results of the evaluation are in Table 18. The
scores show that the entries were quite close to each
other, with the notable exception of the SHEFFIELD-
ROSE-tuned system, which produced overly-long
7We also recased and detokenized each system?s output, to
ensure the outputs are more readable and easier to evaluate.
R
E
F
B
L
E
U
B
L
E
U
-S
IN
G
L
E
C
M
U
-M
E
T
E
O
R
C
U
-S
E
M
P
O
S
-B
L
E
U
N
U
S
-T
E
S
L
A
-F
R
W
T
H
-C
D
E
R
S
H
E
F
F
IE
L
D
-R
O
S
E
S
T
A
N
F
O
R
D
-D
C
P
REF ? .15? .11? .13? .09? .09? .10? .00? .11?
BLEU .78? ? .15 .11 .20 .19? .13? .01? .14
BLEU-SINGLE .82? .20 ? .11 .16 .21 .11 .00? .20
CMU-METEOR .84? .09 .15 ? .21 .20 .19 .00? .19
CU-SEMPOS-BLEU .82? .23 .21 .21 ? .12? .18 .00? .21
NUS-TESLA-F .80? .32? .31 .28 .28? ? .31 .00? .28
RWTH-CDER .79? .22? .16 .16 .22 .23 ? .00? .15
SHEFFIELD-ROSE .98? .93? .93? .96? .95? .95? .93? ? .94?
STANFORD-DCP .82? .17 .18 .26 .27 .28 .15 .00? ?
> others .83 .28 .27 .27 .29 .28 .26 .00 .27
>= others .90 .79 .77 .76 .74 .68 .76 .05 .73
Table 18: Head to head comparisons for the tunable met-
rics task. The numbers indicate how often the system in
the column was judged to be better than the system in
the row. The difference between 100 and the sum of the
corresponding cells is the percent of time that the two
systems were judged to be equal.
and erroneous output (possibly due to an implemen-
tation issue). This is also evident from the fact that
38% of pairwise comparisons indicated a tie be-
tween the two systems, with the tie rate increasing
to a full 47% when excluding comparisons involving
the reference. This is a very high tie rate ? the cor-
responding figure in, say, European language pairs
(individual systems) is only 21%.
What makes the different entries appear even
more closely-matched is that the ranking changes
significantly when ordering systems by their
>others score rather than the ?others score (i.e.
when rewarding only wins, and not rewarding ties).
NUS-TESLA-F goes from being a bottom entry to be-
ing a top entry, with CU-SEMPOS-BLEU also bene-
fiting, changing from the middle to the top rank.
Either way, we see that a BLEU -tuned system
is performing just as well as systems tuned to the
other metrics. This might be an indication that some
work remains to be done before a move away from
BLEU-tuning is fully justified. On the other hand,
the close results might be an artifact of the language
pair choice. Urdu-English translation is still a rel-
atively difficult problem, and MT outputs are still
of a relatively low quality. It might be the case that
human annotators are simply not very good at distin-
42
guishing one bad translation from another bad trans-
lation, especially at such a fine-grained level.
It is worth noting that the designers of the TESLA
family replicated the setup of this tunable metric task
for three European language pairs, and found that
human judges did perceive a difference in quality
between a TESLA-tuned system and a BLEU -tuned
system (Liu et al, 2011).
7.2 Anticipated Changes Next Year
This year?s effort was a pilot of the task, so we in-
tentionally limited the task to some degree, to make
it easier to iron out the details. Possible changes for
next year include:
? More language pairs / translations into lan-
guages other than English. This year we fo-
cus on Urdu-English because the language pair
requires a lot of reordering, and our syntactic
model has more parameters to optimize than
the standard Hiero and phrase-based models.
? Provide some human judgments about the
model?s output, so that people can experiment
with regression models.
? Include a single reference track along with the
multiple reference track. Some metrics may be
better at dealing with the (more common) case
of there being only a single reference transla-
tion available for every source sentence.
? Allow for experimentation with the MIRA op-
timization routine instead of MERT. MIRA can
scale to a greater number of features, but re-
quires that metrics be decomposable.
8 Summary
As in previous editions of this workshop we car-
ried out an extensive manual and automatic evalua-
tion of machine translation performance for translat-
ing from European languages into English, and vice
versa.
The number of participants grew slightly com-
pared to previous editions of the WMT workshop,
with 36 groups from 27 institutions participating in
the translation task of WMT11, 10 groups from 10
institutions participating in the system combination
task, and 10 groups from 8 institutions participating
in the featured translation task (Haitian Creole SMS
into English).
This year was also the first time that we included a
language pair (Haitian-English) with non-European
source language and with very limited resources for
the source language side. Also the genre of the
Haitian-English task differed from previous WMT
tasks as the Haitian-English translations are SMS
messages.
WMT11 also introduced a new shared task focus-
ing on evaluation metrics to tune the parameters of
a statistical machine translation system in which 6
groups have participated.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.8
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. A big
thank you to Ondr?ej Bojar, Simon Carter, Chris-
tian Federmann, Will Lewis, Rob Munro and Herve?
Saint-Amand, and to the shared task participants.
References
Vera Aleksic and Gregor Thurmair. 2011. Personal
Translator at WMT2011. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Alexandre Allauzen, He?le`ne Bonneau-Maynard, Hai-Son
Le, Aure?lien Max, Guillaume Wisniewski, Franc?ois
Yvon, Gilles Adda, Josep Maria Crego, Adrien
Lardilleux, Thomas Lavergne, and Artem Sokolov.
2011. LIMSI @ WMT11. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3):159?174.
Eleftherios Avramidis, Maja Popovic?, David Vilar, and
Aljoscha Burchardt. 2011. Evaluate with confidence
8http://statmt.org/wmt11/results.html
43
estimation: Machine ranking of translation outputs us-
ing grammatical features. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Wilker Aziz, Miguel Rios, and Lucia Specia. 2011. Shal-
low semantic trees for SMT. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Kathryn Baker, Michael Bloodgood, Chris Callison-
Burch, Bonnie Dorr, Scott Miller, Christine Pi-
atko, Nathaniel W. Filardo, and Lori Levin. 2010.
Semantically-informed syntactic machine translation:
A tree-grafting approach. In Proceedings of AMTA.
Lo??c Barrault. 2011. MANY improvements for
WMT?11. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Ergun Bicici and Deniz Yuret. 2011. RegMT system for
machine translation, system combination, and evalua-
tion. In Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
translation model by monolingual data. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion (WMT07), Prague, Czech Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation (WMT09), Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT10), Uppsala, Swe-
den.
Boxing Chen and Roland Kuhn. 2011. Amber: A mod-
ified bleu, enhanced ranking metric. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measur-
ment, 20(1):37?46.
Antonio M. Corb??-Bellot, Mikel L. Forcada, Sergio Ortiz-
Rojas, Juan Antonio Pe?rez-Ortiz, Gema Ram??rez-
Sa?nchez, Felipe Sa?nchez-Mart??nez, In?aki Alegria,
Aingeru Mayor, and Kepa Sarasola. 2005. An open-
source shallow-transfer machine translation engine for
the romance languages of Spain. In Proceedings of the
European Association for Machine Translation, pages
79?86.
Marta R. Costa-jussa` and Rafael E. Banchs. 2011. The
BM-I2R Haitian-Cre?ole-to-English translation system
description for the WMT 2011 evaluation campaign.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
Daniel Dahlmeier, Chang Liu, and Hwee Tou Ng. 2011.
TESLA at WMT 2011: Translation evaluation and tun-
able metric. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Michael Denkowski and Alon Lavie. 2011a. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Michael Denkowski and Alon Lavie. 2011b. METEOR-
Tuned Phrase-Based SMT: CMU French-English and
Haitian-English Systems for WMT 2011. Technical
Report CMU-LTI-11-011, Language Technologies In-
stitute, Carnegie Mellon University.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Vladimir Eidelman, Kristy Hollingshead, and Philip
Resnik. 2011. Noisy SMS machine translation in low-
density languages. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Christian Federmann and Sabine Hunsicker. 2011.
Stochastic parse tree selection for an existing RBMT
system. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Robert Frederking, Alexander Rudnicky, and Christopher
Hogan. 1997. Interactive speech translation in the
DIPLOMAT project. In Proceedings of the ACL-1997
Workshop on Spoken Language Translation.
Markus Freitag, Gregor Leusch, Joern Wuebker, Stephan
Peitz, Hermann Ney, Teresa Herrmann, Jan Niehues,
Alex Waibel, Alexandre Allauzen, Gilles Adda,
Josep Maria Crego, Bianka Buschbeck, Tonio Wand-
macher, and Jean Senellart. 2011. Joint WMT sub-
mission of the QUAERO project. In Proceedings of
the Sixth Workshop on Statistical Machine Translation.
Yoko Futagi, Paul Deane, Martin Chodorow, and Joel
Tetreault. 2008. A computational approach to de-
tecting collocation errors in the writing of non-native
speakers of English. Computer Assisted Language
Learning Journal.
Jesu?s Gonza?lez-Rubio and Francisco Casacuberta. 2011.
The UPV-PRHLT combination system for WMT 2011.
44
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
Greg Hanneman and Alon Lavie. 2011. CMU syntax-
based machine translation at WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Christian Hardmeier, Jo?rg Tiedemann, Markus Saers,
Marcello Federico, and Mathur Prashant. 2011. The
Uppsala-FBK systems at WMT 2011. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Kenneth Heafield and Alon Lavie. 2011. CMU system
combination in WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Teresa Herrmann, Mohammed Mediani, Jan Niehues,
and Alex Waibel. 2011. The Karlsruhe Institute of
Technology translation systems for the WMT 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Sanjika Hewavitharana, Nguyen Bach, Qin Gao, Vamshi
Ambati, and Stephan Vogel. 2011. CMU Haitian
Creole-English translation system for WMT 2011. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Maria Holmqvist, Sara Stymne, and Lars Ahrenberg.
2011. Experiments with word alignment, normaliza-
tion and clause reordering for SMT between English
and German. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
Haitian Creole emergency SMS messages. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Matthias Huck, Joern Wuebker, Christoph Schmidt,
Markus Freitag, Stephan Peitz, Daniel Stein, Arnaud
Dagnelies, Saab Mansour, Gregor Leusch, and Her-
mann Ney. 2011. The RWTH Aachen machine trans-
lation system for WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Maxim Khalilov and Khalil Sima?an. 2011. ILLC-UvA
translation system for EMNLP-WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Pro-
ceedings of the ACL-2007 Demo and Poster Sessions,
Prague, Czech Republic.
Oliver Lacey-Hall. 2011. The guardian?s poverty matters
blog: How remote teams can help the rapid response
to disasters, March.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
Gregor Leusch and Hermann Ney. 2009. Edit distances
with block movements and error rate confidence esti-
mates. Machine Translation, 23:129?140.
Gregor Leusch, Markus Freitag, and Hermann Ney.
2011. The RWTH system combination system for
WMT 2011. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
William Lewis, Robert Munro, and Stephan Vogel. 2011.
Crisis MT: Developing a cookbook for MT in crisis
situations. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
William D. Lewis. 2010. Haitian Creole: How to
build and ship an MT engine from scratch in 4 days,
17hours, & 30 minutes. In Proceedings of EAMT
2010.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR, Upp-
sala, Sweden, July.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic Evalu-
ation Measures for Machine Translation and/or Sum-
marization, pages 25?32.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng. 2011.
Better evaluation metrics lead to better machine trans-
lation. In Proceedings of EMNLP.
Vero?nica Lo?pez-Luden?a and Rube?n San-Segundo. 2011.
UPM system for the translation task. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Matous? Macha?c?ek and Ondr?ej Bojar. 2011. Approxi-
mating a deep-syntactic metric for MT evaluation and
tuning. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
45
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Robert Munro. 2010. Crowdsourced translation for
emergency response in Haiti: the global collabora-
tion of local knowledge. In Proceedings of the AMTA
Workshop on Collaborative Crowdsourcing for Trans-
lation.
Douglas W. Oard and Franz Josef Och. 2003. Rapid-
response machine translation for unexpected lan-
guages. In Proceedings of MT Summit IX.
Douglas W. Oard. 2003. The surprise language exer-
cises. ACM Transactions on Asian Language Infor-
mation Processing, 2(2):79?84.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics (ACL-2003), Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL-2002), Philadelphia, Pennsylvania.
Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-
tin Chodorow. 2011. E-rating machine translation. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation.
Martin Popel, David Marec?ek, Nathan Green, and
Zdene?k Z?abokrtsky?. 2011. Influence of parser choice
on dependency-based MT. In Proceedings of the Sixth
Workshop on Statistical Machine Translation.
Maja Popovic?, David Vilar, Eleftherios Avramidis, and
Aljoscha Burchardt. 2011. Evaluation without ref-
erences: IBM1 scores as evaluation metrics. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Maja Popovic?. 2011. Morphemes and POS tags for n-
gram based evaluation metrics. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Marion Potet, Raphae?l Rubino, Benjamin Lecouteux,
Ste?phane Huet, Laurent Besacier, Herve? Blanchon,
and Fabrice Lefe`vre. 2011. The LIGA (LIG/LIA)
machine translation system for WMT 2011. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation, Honolulu, Hawaii.
Miguel Rios, Wilker Aziz, and Lucia Specia. 2011.
TINE: A metric to assess MT adequacy. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Christian Rish?j and Anders S?gaard. 2011. Factored
translation with unsupervised word clusters. In Pro-
ceedings of the Sixth Workshop on Statistical Machine
Translation.
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, and
Richard Schwartz. 2011. Expected BLEU training for
graphs: BBN system description for WMT11 system
combination task. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
V??ctor M. Sa?nchez-Cartagena, Felipe Sa?nchez-Mart??nez,
and Juan Antonio Pe?rez-Ortiz. 2011. The Univer-
sitat d?Alacant hybrid machine translation system for
WMT 2011. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. LIUM?s SMT machine trans-
lation systems for WMT 2011. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Rico Sennrich. 2011. The UZH system combination sys-
tem for WMT 2011. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Americas
(AMTA-2006), Cambridge, Massachusetts.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence level MT eval-
uation. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Lucia Specia, Dhwaj Raj, and Marco Turchi. 2010. Ma-
chine translation evaluation versus quality estimation.
Machine Translation, 24(1):39?50.
Sara Stymne. 2011. Spell checking techniques for re-
placement of unknown words and data cleaning for
Haitian Creole SMS translation. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection. In Proceedings
of COLING, Manchester, UK.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 90?94.
46
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011a.
Description of the JHU system combination scheme
for WMT 2011. In Proceedings of the Sixth Workshop
on Statistical Machine Translation.
Jia Xu, Hans Uszkoreit, Casey Kennington, David Vilar,
and Xiaojun Zhang. 2011b. DFKI hybrid machine
translation system for WMT 2011 - on the integration
of SMT and RBMT. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Francisco Zamora-Martinez and Maria Jose Castro-
Bleda. 2011. CEU-UPV English-Spanish system for
WMT11. In Proceedings of the Sixth Workshop on
Statistical Machine Translation.
Daniel Zeman. 2011. Hierarchical phrase-based MT at
the Charles University for the WMT 2011 shared task.
In Proceedings of the Sixth Workshop on Statistical
Machine Translation.
47
A Pairwise System Comparisons by Human Judges
Tables 19?38 show pairwise comparisons between systems for each language pair. The numbers in each of
the tables? cells indicate the percentage of times that the system in that column was judged to be better than
the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and
the sum of the complementary cells is the percent of time that the two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences
(rather than differences that are attributable to chance). In the following tables ? indicates statistical signif-
icance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at
p ? 0.01, according to the Sign Test.
B Automatic Scores
Tables 39?48 give the automatic scores for each of the systems.
C Meta-evaluation
Tables 49 and 50 give a detailed breakdown of intra- and inter-annotator agreement rates for all of manual
evaluation tracks of WMT11, broken down by language pair.
48
R
E
F
C
S
T
C
U
-B
O
JA
R
C
U
-Z
E
M
A
N
JH
U
O
N
L
IN
E
-B
S
Y
S
T
R
A
N
U
E
D
IN
U
P
P
S
A
L
A
REF ? .02? .04? .01? .04? .04? .04? .05? .04?
CST .88? ? .49? .36 .49? .59? .41 .58? .44?
CU-BOJAR .91? .27? ? .27? .30 .48? .28? .41? .41
CU-ZEMAN .94? .31 .49? ? .47? .67? .47? .64? .49?
JHU .89? .29? .39 .28? ? .47? .36 .41? .36
ONLINE-B .84? .20? .27? .19? .28? ? .24? .30 .27?
SYSTRAN .91? .31 .49? .30? .39 .59? ? .56? .37
UEDIN .89? .16? .25? .16? .27? .36 .23? ? .25?
UPPSALA .84? .28? .40 .24? .37 .49? .38 .45? ?
> others .89 .23 .36 .23 .33 .46 .31 .43 .33
>= others .96 .47 .60 .44 .57 .68 .51 .69 .57
Table 19: Ranking scores for entries in the Czech-English task (individual system track).
R
E
F
C
O
M
M
E
R
C
IA
L
-1
C
O
M
M
E
R
C
IA
L
-2
C
U
-B
O
JA
R
C
U
-M
A
R
E
C
E
K
C
U
-P
O
P
E
L
C
U
-T
A
M
C
H
Y
N
A
C
U
-Z
E
M
A
N
JH
U
O
N
L
IN
E
-B
U
E
D
IN
REF ? .05? .04? .04? .04? .05? .05? .04? .03? .04? .04?
COMMERCIAL-1 .91? ? .36 .53? .50? .47? .44? .33? .33? .55? .45?
COMMERCIAL-2 .87? .42 ? .52? .47? .47? .50? .30? .40 .50? .43
CU-BOJAR .89? .31? .31? ? .29 .41 .21? .19? .27? .42? .31?
CU-MARECEK .88? .31? .37? .27 ? .35? .28 .21? .30? .39 .28?
CU-POPEL .85? .33? .29? .43 .45? ? .41 .27? .31? .50? .39
CU-TAMCHYNA .87? .34? .35? .30? .32 .40 ? .22? .25? .45? .32
CU-ZEMAN .91? .47? .52? .56? .56? .55? .55? ? .44? .64? .54?
JHU .91? .43? .41 .50? .47? .51? .51? .31? ? .52? .48?
ONLINE-B .86? .27? .32? .33? .39 .33? .29? .18? .23? ? .31?
UEDIN .85? .34? .40 .40? .37? .42 .36 .24? .25? .44? ?
> others .88 .33 .34 .39 .39 .40 .36 .23 .28 .44 .35
>= others .96 .51 .51 .64 .63 .58 .62 .43 .49 .65 .59
Table 20: Ranking scores for entries in the English-Czech task (individual system track).
49
R
E
F
C
M
U
-D
Y
E
R
C
S
T
C
U
-Z
E
M
A
N
D
F
K
I-
X
U
JH
U
K
IT
K
O
C
L
IM
S
I
L
IN
G
U
A
T
E
C
L
IU
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-W
U
E
B
K
E
R
U
E
D
IN
U
P
P
S
A
L
A
REF ? .05? .02? .03? .04? .00? .08? .04? .00? .07? .05? .07? .14? .02? .08? .00? .06? .08? .02? .10? .08?
CMU-DYER .95? ? .18? .17? .33 .26? .22? .12? .29? .43 .23? .43 .54 .32 .20? .40 .43 .48 .31 .19? .18?
CST .96? .74? ? .42 .62? .35 .68? .44? .47? .78? .62? .77? .73? .81? .70? .74? .67? .53? .65? .47 .51
CU-ZEMAN .97? .67? .22 ? .56? .26? .41 .22? .48 .66? .46 .60? .62? .73? .57? .60? .62? .53? .40 .44 .48
DFKI-XU .94? .44 .06? .24? ? .10? .26 .17? .49? .47 .21? .42 .45 .52 .42 .45 .51 .39 .40 .48 .29
JHU 1.00?.61? .33 .55? .64? ? .59? .45 .51? .59 .52? .68? .63? .62? .64? .65? .58? .46 .61? .44 .38
KIT .87? .65? .12? .21 .44 .23? ? .34 .40 .54 .30 .43 .57? .44 .43 .47 .50 .53 .40 .28 .17?
KOC .96? .64? .09? .49? .66? .36 .43 ? .43 .69? .57? .69? .63? .62? .41 .63? .59 .52? .51 .59? .40
LIMSI .96? .54? .24? .30 .22? .25? .38 .27 ? .63? .52 .43 .55? .43 .43 .59? .47 .40 .41 .32 .44
LINGUATEC .91? .45 .13? .24? .38 .32 .34 .18? .27? ? .26? .45 .62? .46 .20? .49 .53 .36 .41 .32? .29?
LIU .89? .49? .14? .29 .54? .25? .48 .24? .31 .64? ? .47 .61? .52 .46 .48 .50 .23? .48 .37 .36
ONLINE-A .88? .47 .12? .25? .42 .18? .41 .19? .39 .39 .30 ? .32 .26? .28 .46 .36 .35 .42 .19? .27?
ONLINE-B .78? .38 .16? .23? .33 .28? .26? .16? .26? .29? .22? .38 ? .23? .23? .29? .29? .22? .27 .22? .18?
RBMT-1 .96? .42 .09? .18? .35 .21? .51 .23? .43 .41 .38 .56? .62? ? .31 .46 .39 .13 .48 .50 .30?
RBMT-2 .86? .54? .15? .28? .48 .29? .43 .41 .39 .55? .44 .51 .64? .43 ? .55? .47 .54? .44 .41 .29?
RBMT-3 .92? .42 .11? .27? .32 .23? .47 .18? .19? .34 .38 .49 .55? .38 .26? ? .36 .29? .34 .33 .28?
RBMT-4 .88? .36 .19? .24? .38 .29? .43 .38 .45 .32 .37 .44 .56? .33 .34 .45 ? .35 .29? .51 .24?
RBMT-5 .92? .45 .27? .27? .45 .32 .37 .27? .47 .47 .61? .55 .67? .26 .24? .53? .46 ? .45 .47 .39
RWTH-WUEBKER .93? .50 .23? .26 .33 .20? .24 .36 .41 .44 .39 .47 .55 .44 .38 .53 .56? .45 ? .21 .39
UEDIN .88? .59? .24 .28 .28 .33 .50 .24? .45 .65? .40 .67? .62? .34 .39 .52 .41 .36 .43 ? .48
UPPSALA .92? .64? .27 .29 .39 .44 .58? .32 .41 .66? .53 .68? .69? .59? .59? .58? .61? .54 .36 .31 ?
> others .92 .50 .17 .28 .40 .26 .40 .26 .38 .51 .40 .51 .57 .43 .38 .49 .47 .39 .41 .36 .32
>= others .95 .66 .37 .47 .60 .43 .57 .45 .56 .63 .57 .66 .72 .60 .54 .64 .61 .56 .59 .55 .47
Table 21: Ranking scores for entries in the German-English task (individual system track).
50
R
E
F
C
O
P
E
N
H
A
G
E
N
C
U
-T
A
M
C
H
Y
N
A
C
U
-Z
E
M
A
N
D
F
K
I-
F
E
D
E
R
M
A
N
N
D
F
K
I-
X
U
IL
L
C
-U
V
A
JH
U
K
IT
K
O
C
L
IM
S
I
L
IU
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-F
R
E
IT
A
G
U
E
D
IN
U
O
W
U
P
P
S
A
L
A
REF ? .08? .06? .00? .13? .02? .05? .05? .02? .02? .16? .06? .11? .07? .14? .14? .19? .11? .11? .16? .07? .07? .08?
COPENHAGEN .85? ? .31 .09? .60? .39 .25 .32 .41 .27 .36 .34 .49? .61? .56? .61? .64? .64? .60 .26 .49 .30 .16
CU-TAMCHYNA .92? .37 ? .13? .61? .48? .30 .38 .58? .33 .39 .41? .55? .57? .72? .69? .81? .49 .59? .47 .39 .40 .43
CU-ZEMAN 1.00?.60? .41? ? .76? .78? .51? .47? .64? .53? .66? .49? .77? .68? .69? .64? .70? .64? .72? .55? .47 .44 .50
DFKI-FEDERMANN .72? .19? .17? .16? ? .39 .25? .38 .38 .24? .32 .29 .35 .40 .43 .33 .39 .19 .33? .22? .31 .11? .30
DFKI-XU .84? .31 .21? .08? .37 ? .25? .32 .34 .12? .37 .30 .35 .47 .54? .30 .51? .43 .37 .20? .22? .25? .14?
ILLC-UVA .90? .39 .37 .25? .63? .50? ? .41? .58? .35 .56? .38 .55? .63? .61? .63? .71? .75? .62? .33 .56? .38 .41
JHU .91? .45 .27 .27? .41 .40 .20? ? .37 .27 .43 .50? .58? .59? .43 .55? .72? .50 .50 .50? .47 .46 .22?
KIT .87? .24 .23? .17? .41 .43 .26? .37 ? .16? .51 .27? .37 .45? .47 .39 .58? .53 .47 .23? .24 .21? .17?
KOC .95? .35 .35 .13? .61? .65? .38 .42 .57? ? .47? .33 .47? .62? .61? .53? .64? .63? .45 .20 .38 .37 .18?
LIMSI .77? .31 .26 .11? .48 .35 .18? .30 .33 .23? ? .36 .39 .50? .52 .47 .48 .39 .42 .18? .22? .28 .14?
LIU .84? .32 .20? .25? .51 .38 .26 .21? .51? .35 .39 ? .51 .49? .63? .52? .56 .48? .56 .29 .38 .25 .25
ONLINE-A .75? .21? .24? .09? .48 .41 .22? .30? .37 .25? .37 .37 ? .46 .37 .41 .47 .33 .44 .27? .28 .22? .16?
ONLINE-B .91? .17? .15? .13? .44 .22 .17? .16? .20? .15? .24? .25? .27 ? .43 .35 .48 .33 .17? .17? .26 .12? .20?
RBMT-1 .80? .23? .11? .20? .37 .28? .18? .29 .38 .25? .36 .30? .41 .38 ? .34 .45 .36 .02? .17? .17? .28? .24?
RBMT-2 .80? .20? .10? .16? .43 .38 .20? .27? .45 .22? .36 .30? .38 .51 .43 ? .48 .40 .42 .31? .28? .16? .25?
RBMT-3 .65? .18? .14? .15? .37 .29? .17? .22? .25? .20? .27 .33 .33 .29 .30 .31 ? .34 .16? .24? .35 .20? .11?
RBMT-4 .80? .21? .28 .22? .19 .26 .09? .32 .29 .27? .39 .27? .43 .44 .38 .38 .45 ? .42 .29? .36 .27? .31?
RBMT-5 .88? .35 .31? .15? .54? .51 .26? .34 .36 .36 .44 .35 .44 .59? .37? .33 .62? .38 ? .29 .45 .38 .30
RWTH-FREITAG .80? .31 .27 .17? .62? .55? .19 .25? .56? .30 .49? .41 .53? .59? .56? .53? .62? .57? .45 ? .36 .38 .24
UEDIN .82? .27 .27 .27 .46 .47? .17? .28 .36 .33 .48? .27 .47 .43 .75? .55? .52 .50 .43 .21 ? .35 .27
UOW .86? .39 .21 .23 .74? .53? .36 .38 .64? .20 .38 .41 .74? .61? .56? .64? .57? .65? .38 .26 .41 ? .31
UPPSALA .79? .32 .35 .29 .54 .57? .34 .51? .51? .45? .53? .43 .73? .70? .55? .64? .77? .57? .55 .43 .33 .41 ?
> others .84 .29 .24 .17 .48 .42 .24 .31 .42 .27 .40 .34 .46 .51 .51 .47 .56 .46 .41 .29 .34 .29 .25
>= others .91 .56 .50 .38 .68 .67 .48 .54 .64 .53 .65 .59 .65 .730 .70 .66 .732 .66 .58 .56 .60 .53 .49
Table 22: Ranking scores for entries in the English-German task (individual system track).
R
E
F
A
L
A
C
A
N
T
C
U
-Z
E
M
A
N
H
Y
D
E
R
A
B
A
D
K
O
C
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
S
Y
S
T
R
A
N
U
E
D
IN
U
FA
L
-U
M
U
P
M
REF ? .03? .02? .00? .02? .03? .12? .15? .04? .07? .05? .02? .03? .03? .03? .07?
ALACANT .86? ? .07? .08? .30 .52 .31 .27? .29? .54 .49 .32? .51 .27? .26? .26?
CU-ZEMAN .98? .89? ? .48 .84? .85? .94? .90? .83? .87? .85? .78? .97? .79? .79? .91?
HYDERABAD .98? .86? .27 ? .88? .95? .92? .85? .96? .74? .82? .80? .88? .91? .80? .86?
KOC .93? .48 .06? .06? ? .28 .39 .40 .34 .44 .38 .26? .59? .22? .20? .18?
ONLINE-A .90? .28 .02? .02? .48 ? .32 .34 .34 .26? .34 .19? .35 .20? .11? .20?
ONLINE-B .79? .33 .04? .00? .47 .30 ? .24? .31? .31? .27? .25? .33 .27? .21? .07?
RBMT-1 .81? .52? .05? .11? .50 .57 .62? ? .50 .36 .34 .17 .40 .39 .34 .30?
RBMT-2 .96? .61? .09? .04? .52 .47 .59? .37 ? .39 .46 .27 .58? .29? .24? .45
RBMT-3 .88? .31 .09? .13? .44 .56? .60? .53 .37 ? .47 .14? .52 .40 .23? .31
RBMT-4 .90? .38 .08? .16? .50 .53 .60? .41 .43 .38 ? .43 .52 .33? .18? .22?
RBMT-5 .94? .61? .06? .10? .54? .70? .63? .37 .45 .59? .41 ? .66? .42 .50 .43
SYSTRAN .92? .33 .02? .10? .25? .53 .53 .42 .30? .36 .38 .27? ? .21? .41 .24?
UEDIN .95? .63? .13? .02? .63? .67? .59? .47 .61? .53 .59? .42 .53? ? .32? .45
UFAL-UM .94? .63? .10? .11? .56? .70? .74? .51 .61? .59? .74? .36 .47 .61? ? .44
UPM .85? .54? .02? .03? .62? .61? .81? .59? .45 .55 .68? .40 .60? .42 .38 ?
> others .91 .51 .07 .10 .52 .56 .59 .48 .48 .47 .48 .35 .54 .39 .34 .36
>= others .96 .66 .16 .17 .67 .723 .723 .63 .60 .61 .60 .51 .66 .51 .47 .50
Table 23: Ranking scores for entries in the Spanish-English task (individual system track).
51
R
E
F
C
E
U
-U
P
V
C
U
-Z
E
M
A
N
K
O
C
O
N
L
IN
E
-A
O
N
L
IN
E
-B
P
R
O
M
T
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
U
E
D
IN
U
O
W
U
P
M
U
P
P
S
A
L
A
REF ? .06? .03? .09? .09? .09? .05? .03? .06? .04? .08? .02? .08? .02? .03? .04?
CEU-UPV .84? ? .21? .20? .43 .36 .42 .37 .34? .50? .31 .34 .32 .21? .13? .22
CU-ZEMAN .87? .56? ? .38? .56? .56? .58? .46? .40 .70? .46? .49? .51? .45? .19? .49?
KOC .84? .41? .22? ? .56? .51? .48? .54? .39 .55? .42 .35 .51? .44 .11? .34
ONLINE-A .72? .31 .24? .15? ? .36 .37 .28? .23? .35 .25? .20? .29? .25? .08? .09?
ONLINE-B .72? .30 .17? .18? .26 ? .29 .23? .20? .37 .20? .19? .19? .22? .02? .23?
PROMT .76? .29 .21? .25? .42 .43 ? .24? .24 .19 .27? .26? .32 .25? .18? .21?
RBMT-1 .85? .37 .29? .23? .51? .54? .48? ? .35 .45? .40? .05? .47 .39 .25? .39
RBMT-2 .86? .50? .35 .38 .51? .48? .35 .39 ? .41? .34 .36 .45 .36 .23? .41
RBMT-3 .86? .26? .18? .22? .40 .35 .19 .20? .22? ? .25? .23? .24? .33 .10? .22?
RBMT-4 .80? .45 .29? .34 .53? .51? .43? .21? .38 .43? ? .24? .34 .30 .20? .45?
RBMT-5 .96? .43 .29? .42 .57? .61? .46? .22? .38 .49? .47? ? .50 .46 .27? .47
UEDIN .74? .28 .20? .21? .46? .48? .43 .37 .31 .49? .45 .35 ? .20? .14? .23
UOW .90? .44? .18? .32 .46? .52? .56? .39 .39 .44 .45 .36 .38? ? .10? .32
UPM .93? .65? .53? .67? .74? .71? .69? .59? .51? .74? .60? .51? .64? .68? ? .62?
UPPSALA .84? .36 .21? .32 .49? .42? .45? .39 .35 .45? .29? .41 .35 .30 .15? ?
> others .83 .38 .24 .30 .47 .46 .41 .33 .32 .43 .35 .29 .38 .33 .14 .31
>= others .94 .65 .49 .56 .72 .74 .70 .60 .57 .71 .61 .54 .64 .59 .34 .61
Table 24: Ranking scores for entries in the English-Spanish task (individual system track).
R
E
F
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
A
N
N
E
M
A
N
C
U
-Z
E
M
A
N
JH
U
K
IT
L
IA
-L
IG
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-H
U
C
K
S
Y
S
T
R
A
N
U
E
D
IN
REF ? .10? .18? .06? .03? .14? .15? .14? .14? .12? .05? .12? .09? .05? .06? .05? .05? .07? .02?
CMU-DENKOWSKI .79? ? .35 .12? .34 .32 .41 .35 .21? .47? .46 .49 .32 .33 .36 .35 .25 .45 .29
CMU-HANNEMAN .79? .35 ? .17? .29 .44? .43 .52? .45 .45 .49 .51 .39 .44 .38 .35 .35 .43 .37
CU-ZEMAN .94? .61? .67? ? .54? .66? .66? .58? .60? .59? .88? .62? .59? .63? .60? .56 .68? .64? .40
JHU .82? .34 .29 .22? ? .26 .54? .40 .36 .43 .40 .49 .42 .40 .34 .35 .36 .47 .20?
KIT .79? .39 .20? .16? .40 ? .26? .46 .34 .38 .52 .38 .35 .39 .28 .38 .15? .32 .30
LIA-LIG .75? .24 .31 .28? .24? .59? ? .49 .27 .40 .46 .35 .26 .31? .29 .32 .32 .33? .35
LIMSI .86? .30 .25? .21? .31 .26 .26 ? .38 .40 .42 .35 .18? .43 .34 .16? .34 .34 .33
LIUM .78? .45? .33 .16? .38 .34 .44 .40 ? .38 .30 .44 .26? .33? .38 .28 .29 .33 .28
ONLINE-A .80? .23? .21 .22? .37 .35 .36 .33 .46 ? .43 .35 .16? .33 .24? .20? .26 .34 .27?
ONLINE-B .86? .37 .31 .04? .46 .22 .36 .33 .43 .26 ? .40 .20? .16? .44 .20? .41 .38 .22?
RBMT-1 .87? .44 .35 .23? .46 .44 .54 .48 .44 .53 .54 ? .39 .37 .33 .11? .39 .17? .35
RBMT-2 .84? .47 .37 .26? .40 .50 .45 .52? .54? .58? .67? .45 ? .51 .35 .22? .51 .57 .41
RBMT-3 .89? .44 .42 .19? .40 .43 .54? .46 .61? .50 .71? .37 .32 ? .42 .35 .42 .47 .40
RBMT-4 .85? .53 .36 .26? .51 .47 .55 .52 .46 .59? .40 .43 .50 .42 ? .34 .46 .44 .41
RBMT-5 .93? .58 .55 .33 .54 .54 .59 .70? .56 .66? .65? .36? .54? .46 .37 ? .50 .54? .54
RWTH-HUCK .92? .43 .38 .14? .36 .59? .41 .44 .29 .53 .48 .46 .30 .46 .32 .38 ? .37 .17?
SYSTRAN .93? .39 .38 .24? .44 .48 .60? .50 .40 .55 .57 .45? .36 .29 .44 .21? .49 ? .36
UEDIN .93? .48 .41 .40 .51? .48 .54 .49 .46 .60? .57? .52 .37 .47 .39 .39 .51? .52 ?
> others .85 .39 .36 .21 .39 .41 .46 .46 .41 .46 .50 .41 .33 .39 .35 .28 .37 .39 .32
>= others .91 .62 .58 .37 .61 .64 .64 .661 .63 .661 .66 .58 .52 .55 .53 .45 .58 .54 .50
Table 25: Ranking scores for entries in the French-English task (individual system track).
52
R
E
F
C
U
-Z
E
M
A
N
JH
U
K
IT
L
A
T
L
-G
E
N
E
V
A
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-1
R
B
M
T
-2
R
B
M
T
-3
R
B
M
T
-4
R
B
M
T
-5
R
W
T
H
-H
U
C
K
U
E
D
IN
U
P
P
S
A
L
A
U
P
P
S
A
L
A
-F
B
K
REF ? .07? .06? .25? .07? .13? .20? .15? .20? .10? .09? .18? .11? .12? .14? .18? .16? .16?
CU-ZEMAN .92? ? .83? .86? .63? .85? .90? .86? .81? .89? .70? .75? .75? .61? .78? .79? .81? .81?
JHU .91? .07? ? .55? .30? .60? .50? .55? .59? .45 .41 .34? .30? .50 .40 .42 .42 .44
KIT .63? .04? .29? ? .18? .47 .37 .30? .37 .38 .30? .37 .24? .34 .28 .34 .24? .13?
LATL-GENEVA .86? .29? .54? .73? ? .77? .67? .71? .79? .55? .39 .66? .52 .58? .58? .51 .52 .58?
LIMSI .75? .04? .21? .29 .13? ? .23? .28? .37 .27? .27? .24? .24? .21? .27? .28? .25? .31
LIUM .76? .04? .26? .44 .24? .46? ? .33 .52 .48 .25? .36 .25? .28? .43 .40 .35 .32
ONLINE-A .78? .10? .31? .51? .22? .51? .46 ? .44 .39 .36 .41 .30? .41 .41 .32? .46 .33
ONLINE-B .70? .06? .27? .41 .13? .39 .32 .30 ? .47 .22? .26? .13? .28? .32 .26? .33 .27?
RBMT-1 .83? .07? .38 .46 .23? .56? .39 .41 .42 ? .17? .34 .36 .13 .52 .33? .40 .40
RBMT-2 .88? .25? .47 .59? .37 .65? .63? .51 .57? .54? ? .58? .39 .54? .63? .61? .47 .42
RBMT-3 .80? .19? .54? .42 .20? .60? .47 .44 .52? .42 .18? ? .21? .43 .51 .55 .41 .39
RBMT-4 .82? .22? .54? .63? .33 .63? .64? .54? .59? .41 .44 .46? ? .47 .68? .53 .42 .39
RBMT-5 .86? .18? .46 .53 .20? .62? .56? .46 .61? .22 .33? .40 .34 ? .43 .52 .40 .53?
RWTH-HUCK .76? .08? .33 .38 .21? .60? .40 .38 .43 .36 .18? .37 .21? .38 ? .39 .22? .29
UEDIN .78? .15? .37 .46 .34 .49? .38 .53? .58? .56? .33? .35 .36 .37 .47 ? .38 .31
UPPSALA .77? .07? .36 .53? .36 .49? .46 .46 .56 .46 .38 .42 .39 .55 .57? .39 ? .47
UPPSALA-FBK .80? .10? .40 .71? .27? .50 .47 .51 .53? .42 .48 .41 .52 .29? .50 .47 .40 ?
> others .80 .12 .39 .51 .25 .55 .48 .45 .52 .43 .32 .41 .33 .39 .46 .43 .39 .38
>= others .86 .20 .55 .69 .39 .73 .64 .60 .70 .61 .46 .58 .49 .55 .65 .58 .55 .54
Table 26: Ranking scores for entries in the English-French task (individual system track).
R
E
F
B
M
-I
2R
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
E
W
A
V
IT
H
A
R
A
N
A
H
Y
D
E
R
A
B
A
D
K
O
C
L
IU
U
M
D
-E
ID
E
L
M
A
N
U
M
D
-H
U
U
P
P
S
A
L
A
REF ? .03? .01? .03? .02? .01? .00? .01? .01? .02?
BM-I2R .91? ? .28? .27? .13? .08? .19? .30? .30? .24?
CMU-DENKOWSKI .93? .44? ? .25 .22? .15? .28? .33 .29? .31?
CMU-HEWAVITHARANA .91? .40? .31 ? .21? .16? .29? .35 .39 .30
HYDERABAD .96? .71? .59? .58? ? .27? .56? .57? .42 .52?
KOC .94? .78? .75? .64? .55? ? .65? .69? .62? .64?
LIU .92? .56? .42? .44? .27? .24? ? .43 .41 .39
UMD-EIDELMAN .94? .44? .35 .35 .17? .17? .34 ? .37 .31?
UMD-HU .90? .50? .57? .45 .35 .21? .46 .45 ? .42
UPPSALA .93? .48? .47? .39 .31? .20? .40 .43? .37 ?
> others .93 .49 .42 .39 .25 .17 .35 .40 .36 .35
>= others .98 .71 .66 .64 .43 .31 .55 .63 .52 .57
Table 27: Ranking scores for entries in the Haitian Creole (Clean)-English task (individual system track).
53
R
E
F
B
M
-I
2R
C
M
U
-D
E
N
K
O
W
S
K
I
C
M
U
-H
E
W
A
V
IT
H
A
R
A
N
A
JH
U
L
IU
U
M
D
-E
ID
E
L
M
A
N
REF ? .05? .03? .04? .02? .02? .03?
BM-I2R .83? ? .29? .25? .22? .30? .30?
CMU-DENKOWSKI .89? .44? ? .37? .23? .37 .30?
CMU-HEWAVITHARANA .86? .43? .26? ? .27? .37 .32
JHU .96? .62? .53? .49? ? .52? .47?
LIU .92? .48? .38 .34 .31? ? .36
UMD-EIDELMAN .92? .48? .44? .42 .29? .41 ?
> others .90 .43 .34 .33 .23 .34 .30
>= others .97 .65 .59 .60 .41 .55 .52
Table 28: Ranking scores for entries in the Haitian Creole (Raw)-English task (individual system track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .01? .02? .01? .01?
BBN-COMBO .91? ? .25 .18? .16?
CMU-HEAFIELD-COMBO .90? .24 ? .17? .12?
JHU-COMBO .92? .27? .29? ? .20?
UPV-PRHLT-COMBO .94? .41? .42? .36? ?
> others .92 .23 .24 .18 .12
>= others .99 .62 .64 .58 .47
Table 29: Ranking scores for entries in the Czech-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .04? .04?
CMU-HEAFIELD-COMBO .86? ? .17?
UPV-PRHLT-COMBO .88? .30? ?
> others .87 .17 .11
>= others .96 .48 .41
Table 30: Ranking scores for entries in the English-Czech task (system combination track).
54
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
Q
U
A
E
R
O
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
U
Z
H
-C
O
M
B
O
REF ? .11? .09? .04? .09? .10? .14? .05? .09?
BBN-COMBO .79? ? .45? .32 .21? .28? .39 .31? .36
CMU-HEAFIELD-COMBO .84? .23? ? .21? .17? .19? .25? .19? .31
JHU-COMBO .85? .42 .55? ? .25? .28? .40? .28? .47?
KOC-COMBO .83? .56? .62? .45? ? .41 .54? .40? .51?
QUAERO-COMBO .86? .52? .64? .45? .36 ? .54? .49? .48
RWTH-LEUSCH-COMBO .83? .28 .41? .22? .20? .22? ? .22? .38
UPV-PRHLT-COMBO .85? .47? .57? .42? .25? .26? .48? ? .49?
UZH-COMBO .86? .34 .38 .31? .29? .32 .41 .30? ?
> others .84 .36 .46 .30 .22 .26 .39 .27 .39
>= others .91 .61 .70 .56 .45 .46 .65 .52 .60
Table 31: Ranking scores for entries in the German-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
U
Z
H
-C
O
M
B
O
REF ? .11? .09? .10? .11?
CMU-HEAFIELD-COMBO .81? ? .19? .23? .32
KOC-COMBO .84? .48? ? .38? .47?
UPV-PRHLT-COMBO .81? .36? .23? ? .37?
UZH-COMBO .80? .34 .24? .31? ?
> others .81 .320 .19 .25 .318
>= others .90 .61 .46 .56 .58
Table 32: Ranking scores for entries in the English-German task (system combination track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
K
O
C
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .05? .09? .05? .07? .06? .08?
BBN-COMBO .81? ? .34 .27 .21? .27 .26
CMU-HEAFIELD-COMBO .84? .31 ? .18? .15? .29 .20
JHU-COMBO .83? .25 .32? ? .27 .35? .25
KOC-COMBO .84? .39? .39? .32 ? .39? .31?
RWTH-LEUSCH-COMBO .81? .24 .23 .16? .17? ? .14?
UPV-PRHLT-COMBO .77? .30 .26 .27 .22? .35? ?
> others .82 .25 .27 .21 .18 .28 .21
>= others .93 .64 .67 .62 .56 .71 .64
Table 33: Ranking scores for entries in the Spanish-English task (system combination track).
55
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
O
W
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .10? .07? .09? .08?
CMU-HEAFIELD-COMBO .70? ? .15? .21? .17?
KOC-COMBO .76? .35? ? .36? .19
UOW-COMBO .72? .29? .22? ? .25?
UPV-PRHLT-COMBO .76? .35? .16 .35? ?
> others .73 .27 .15 .25 .17
>= others .91 .69 .58 .63 .59
Table 34: Ranking scores for entries in the English-Spanish task (system combination track).
R
E
F
B
B
N
-C
O
M
B
O
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
JH
U
-C
O
M
B
O
L
IU
M
-C
O
M
B
O
R
W
T
H
-L
E
U
S
C
H
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .04? .04? .06? .06? .06? .02?
BBN-COMBO .82? ? .35 .25 .18? .21? .21?
CMU-HEAFIELD-COMBO .90? .29 ? .30 .20? .29 .25?
JHU-COMBO .83? .35 .40 ? .31? .36 .21?
LIUM-COMBO .83? .42? .40? .44? ? .38? .35
RWTH-LEUSCH-COMBO .83? .34? .29 .30 .22? ? .21?
UPV-PRHLT-COMBO .91? .49? .40? .34? .30 .40? ?
> others .85 .32 .31 .28 .21 .28 .21
>= others .95 .67 .62 .59 .53 .63 .53
Table 35: Ranking scores for entries in the French-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .11? .11?
CMU-HEAFIELD-COMBO .74? ? .23?
UPV-PRHLT-COMBO .77? .38? ?
> others .76 .24 .17
>= others .89 .51 .43
Table 36: Ranking scores for entries in the English-French task (system combination track).
56
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
K
O
C
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .01? .01? .01?
CMU-HEAFIELD-COMBO .94? ? .29? .21?
KOC-COMBO .96? .48? ? .41?
UPV-PRHLT-COMBO .94? .34? .29? ?
> others .95 .28 .20 .21
>= others .99 .52 .38 .48
Table 37: Ranking scores for entries in the Haitian Creole (Clean)-English task (system combination track).
R
E
F
C
M
U
-H
E
A
F
IE
L
D
-C
O
M
B
O
U
P
V
-P
R
H
L
T
-C
O
M
B
O
REF ? .02? .02?
CMU-HEAFIELD-COMBO .83? ? .24
UPV-PRHLT-COMBO .86? .29 ?
> others .84 .16 .13
>= others .98 .47 .43
Table 38: Ranking scores for entries in the Haitian Creole (Raw)-English task (system combination track).
57
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Czech-English News Task
BBN-COMBO 0.24 0.24 0.25 0.29 0.31 0.19 ?9627 ?10667 1.97 0.53 0.49 0.61 0.34 ?65 44 0.48 0.03 0.51 43
CMU-HEAFIELD-COMBO 0.24 0.24 0.24 0.28 0.3 0.18 ?9604 ?10933 1.97 0.54 0.5 0.60 0.33 ?65 43 0.48 0.03 0.52 42
CST 0.19 0.19 0.2 0.16 0.21 0.10 ?27410 ?27880 1.94 0.64 0.40 0.5 0.28 ?65 34 0.38 0.02 0.42 33
CU-BOJAR 0.21 0.21 0.22 0.19 0.24 0.13 ?23441 ?22289 1.95 0.64 0.44 0.55 0.30 ?65 37 0.42 0.02 0.46 36
CU-ZEMAN 0.20 0.2 0.21 0.14 0.21 0.11 ?33520 ?30938 1.93 0.66 0.38 0.52 0.29 ?66 31 0.37 0.02 0.40 30
JHU 0.22 0.21 0.22 0.2 0.25 0.13 ?21278 ?20480 1.95 0.60 0.43 0.55 0.30 ?65 37 0.42 0.02 0.46 36
JHU-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?12563 ?12688 1.97 0.53 0.5 0.60 0.33 ?65 44 0.48 0.03 0.52 43
ONLINE-B 0.24 0.23 0.24 0.29 0.31 0.19 ?10673 ?11506 1.97 0.52 0.50 0.60 0.33 ?65 44 0.49 0.03 0.52 43
SYSTRAN 0.20 0.2 0.21 0.18 0.22 0.11 ?23996 ?24570 1.94 0.63 0.42 0.52 0.29 ?65 36 0.4 0.02 0.45 34
UEDIN 0.22 0.22 0.23 0.22 0.26 0.14 ?14958 ?15342 1.96 0.59 0.45 0.57 0.31 ?65 40 0.44 0.03 0.48 39
UPPSALA 0.21 0.20 0.21 0.20 0.23 0.12 ?22233 ?22509 1.95 0.62 0.43 0.53 0.29 ?65 37 0.41 0.02 0.46 36
UPV-PRHLT-COMBO 0.24 0.23 0.24 0.29 0.31 0.19 ?13904 ?15260 1.97 0.54 0.49 0.60 0.33 ?65 44 0.48 0.03 0.52 43
Table 39: Automatic evaluation metric scores for systems in the WMT11 Czech-English News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
D
F
K
I-
PA
R
S
E
C
O
N
F
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
German-English News Task
BBN-COMBO 0.23 0.22 0.23 0.25 0.28 0.16 ?17103 ?17837 1.97 0.56 0.46 0.06 0.59 0.32 ?43 42 0.46 0.03 0.49 41
CMU-DYER 0.21 0.21 0.22 0.22 0.25 0.13 ?26089 ?29214 1.95 0.59 0.44 0.04 0.56 0.31 ?45 39 0.43 0.03 0.47 38
CMU-HEAFIELD-COMBO 0.23 0.22 0.23 0.24 0.27 0.15 ?12868 ?16156 1.96 0.57 0.47 0.07 0.58 0.32 ?44 41 0.46 0.03 0.51 40
CST 0.19 0.18 0.19 0.17 0.22 0.11 ?61131 ?60157 1.94 0.63 0.39 0.03 0.5 0.27 ?46 34 0.37 0.02 0.41 33
CU-ZEMAN 0.2 0.19 0.20 0.14 0.22 0.11 ?64860 ?61329 1.93 0.65 0.37 0.06 0.51 0.28 ?47 31 0.37 0.02 0.4 30
DFKI-XU 0.21 0.20 0.21 0.21 0.25 0.14 ?40171 ?39455 1.95 0.58 0.44 0.03 0.54 0.3 ?45 38 0.42 0.02 0.46 37
JHU 0.19 0.19 0.2 0.17 0.22 0.11 ?62997 ?58673 1.94 0.64 0.39 0.03 0.51 0.28 ?45 34 0.38 0.02 0.41 33
JHU-COMBO 0.22 0.22 0.23 0.24 0.27 0.15 ?30492 ?27016 1.96 0.57 0.46 0.04 0.57 0.31 ?44 41 0.45 0.03 0.48 39
KIT 0.21 0.21 0.22 0.22 0.25 0.13 ?31064 ?31930 1.95 0.6 0.44 0.05 0.55 0.31 ?44 39 0.43 0.02 0.47 37
KOC 0.2 0.2 0.20 0.18 0.23 0.12 ?52337 ?50231 1.94 0.63 0.41 0.05 0.52 0.29 ?45 35 0.39 0.02 0.43 34
KOC-COMBO 0.21 0.21 0.21 0.22 0.26 0.14 ?40002 ?38374 1.96 0.59 0.44 0.03 0.54 0.3 ?44 38 0.42 0.02 0.46 37
LIMSI 0.21 0.20 0.21 0.20 0.24 0.13 ?39419 ?38297 1.95 0.61 0.43 0.04 0.54 0.3 ?44 38 0.42 0.02 0.46 36
LINGUATEC 0.19 0.19 0.2 0.16 0.22 0.11 ?26064 ?31116 1.94 0.68 0.42 0.15 0.53 0.29 ?46 35 0.42 0.02 0.47 34
LIU 0.21 0.20 0.21 0.2 0.24 0.13 ?40281 ?40496 1.95 0.62 0.43 0.04 0.53 0.29 ?44 37 0.41 0.02 0.45 36
ONLINE-A 0.22 0.21 0.22 0.21 0.26 0.14 ?25411 ?25675 1.95 0.6 0.45 0.06 0.57 0.31 ?44 39 0.45 0.03 0.48 38
ONLINE-B 0.22 0.22 0.23 0.23 0.27 0.15 ?15149 ?19578 1.96 0.58 0.46 0.06 0.57 0.32 ?44 41 0.46 0.03 0.5 39
QUAERO-COMBO 0.21 0.21 0.22 0.22 0.26 0.14 ?34486 ?33449 1.96 0.58 0.45 0.03 0.55 0.30 ?44 39 0.43 0.03 0.47 38
RBMT-1 0.20 0.2 0.21 0.16 0.21 0.11 ?32960 ?34972 1.94 0.67 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.46 34
RBMT-2 0.19 0.19 0.2 0.15 0.2 0.1 ?40842 ?43413 1.94 0.69 0.4 0.11 0.50 0.28 ?45 34 0.4 0.02 0.44 33
RBMT-3 0.20 0.2 0.21 0.17 0.22 0.11 ?32476 ?33417 1.94 0.65 0.42 0.09 0.53 0.29 ?44 36 0.42 0.02 0.47 35
RBMT-4 0.20 0.2 0.21 0.17 0.22 0.11 ?34287 ?34604 1.94 0.66 0.42 0.08 0.52 0.29 ?45 36 0.42 0.02 0.47 35
RBMT-5 0.19 0.19 0.20 0.15 0.20 0.10 ?49097 ?46635 1.94 0.68 0.40 0.07 0.50 0.28 ?46 34 0.4 0.02 0.44 33
RWTH-LEUSCH-COMBO 0.22 0.22 0.23 0.24 0.28 0.16 ?22878 ?22089 1.96 0.56 0.46 0.03 0.58 0.32 ?44 41 0.45 0.03 0.49 40
RWTH-WUEBKER 0.21 0.20 0.21 0.21 0.24 0.13 ?35973 ?37140 1.95 0.60 0.44 0.04 0.54 0.3 ?45 38 0.42 0.02 0.45 37
UEDIN 0.21 0.20 0.21 0.19 0.23 0.12 ?32791 ?34633 1.95 0.63 0.43 0.07 0.54 0.3 ?45 37 0.42 0.02 0.46 36
UPPSALA 0.20 0.2 0.21 0.2 0.23 0.12 ?40448 ?41548 1.95 0.63 0.42 0.06 0.53 0.29 ?45 37 0.41 0.02 0.44 36
UPV-PRHLT-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?33413 ?31778 1.96 0.58 0.45 0.03 0.57 0.31 ?44 40 0.44 0.03 0.48 39
UZH-COMBO 0.22 0.21 0.22 0.23 0.27 0.15 ?16326 ?20831 1.96 0.58 0.45 0.07 0.57 0.31 ?44 40 0.45 0.03 0.48 39
Table 40: Automatic evaluation metric scores for systems in the WMT11 German-English News Task
(newssyscombtest2011)
58
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
French-English News Task
BBN-COMBO 0.25 0.25 0.26 0.31 0.32 0.21 ?19552 ?22107 1.98 0.48 0.51 0.64 0.36 ?43 47 0.49 0.03 0.54 46
CMU-DENKOWSKI 0.24 0.24 0.24 0.26 0.29 0.17 ?34357 ?37807 1.97 0.53 0.48 0.61 0.34 ?45 43 0.46 0.03 0.50 42
CMU-HANNEMAN 0.24 0.23 0.24 0.27 0.29 0.17 ?33662 ?37698 1.97 0.52 0.49 0.60 0.33 ?45 44 0.46 0.03 0.51 42
CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.30 0.31 0.2 ?18365 ?22937 1.98 0.5 0.51 0.63 0.35 ?44 46 0.49 0.03 0.54 45
CU-ZEMAN 0.22 0.22 0.23 0.17 0.24 0.13 ?67586 ?64688 1.94 0.6 0.41 0.56 0.31 ?47 34 0.39 0.02 0.42 33
JHU 0.24 0.24 0.24 0.25 0.29 0.17 ?41567 ?39578 1.96 0.53 0.47 0.61 0.34 ?45 42 0.46 0.03 0.5 41
JHU-COMBO 0.25 0.25 0.25 0.31 0.32 0.20 ?32785 ?31712 1.98 0.49 0.50 0.63 0.35 ?43 47 0.48 0.03 0.53 45
KIT 0.25 0.24 0.25 0.29 0.31 0.19 ?22678 ?28283 1.98 0.51 0.50 0.63 0.35 ?44 46 0.49 0.03 0.53 44
LIA-LIG 0.25 0.24 0.25 0.29 0.3 0.18 ?34063 ?34716 1.97 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.52 44
LIMSI 0.25 0.24 0.25 0.28 0.29 0.18 ?26269 ?29363 1.97 0.52 0.5 0.62 0.34 ?44 45 0.48 0.03 0.52 44
LIUM 0.25 0.24 0.25 0.29 0.30 0.19 ?29288 ?36137 1.98 0.52 0.49 0.62 0.34 ?44 45 0.48 0.03 0.53 44
LIUM-COMBO 0.25 0.24 0.25 0.31 0.31 0.2 ?30678 ?35365 1.98 0.50 0.5 0.62 0.34 ?44 46 0.48 0.03 0.53 45
ONLINE-A 0.25 0.24 0.25 0.27 0.3 0.18 ?38761 ?34096 1.97 0.52 0.49 0.62 0.34 ?44 44 0.48 0.03 0.52 43
ONLINE-B 0.25 0.24 0.25 0.29 0.31 0.19 ?19157 ?25284 1.98 0.50 0.51 0.62 0.35 ?45 46 0.49 0.03 0.54 44
RBMT-1 0.24 0.23 0.24 0.23 0.26 0.15 ?49115 ?39153 1.96 0.59 0.46 0.60 0.33 ?43 42 0.46 0.03 0.51 41
RBMT-2 0.23 0.22 0.23 0.21 0.24 0.13 ?59549 ?50466 1.95 0.63 0.44 0.57 0.32 ?43 40 0.43 0.02 0.48 39
RBMT-3 0.23 0.23 0.23 0.22 0.25 0.14 ?52047 ?45073 1.96 0.59 0.46 0.58 0.32 ?44 41 0.45 0.02 0.50 40
RBMT-4 0.23 0.22 0.24 0.22 0.25 0.14 ?54507 ?42933 1.96 0.63 0.45 0.59 0.33 ?43 40 0.44 0.02 0.49 39
RBMT-5 0.23 0.22 0.23 0.21 0.24 0.13 ?55545 ?48332 1.95 0.62 0.45 0.57 0.32 ?44 40 0.44 0.02 0.49 38
RWTH-HUCK 0.24 0.24 0.25 0.28 0.3 0.18 ?44018 ?42549 1.97 0.52 0.49 0.61 0.34 ?44 44 0.47 0.03 0.51 43
RWTH-LEUSCH-COMBO 0.26 0.25 0.26 0.31 0.32 0.20 ?21914 ?21746 1.98 0.49 0.51 0.64 0.35 ?43 47 0.50 0.03 0.54 46
SYSTRAN 0.24 0.23 0.24 0.25 0.27 0.16 ?34321 ?40119 1.96 0.54 0.48 0.59 0.33 ?44 43 0.46 0.03 0.51 41
UEDIN 0.23 0.23 0.24 0.25 0.27 0.16 ?47202 ?47955 1.96 0.56 0.47 0.59 0.33 ?45 42 0.45 0.03 0.49 40
UPV-PRHLT-COMBO 0.25 0.25 0.26 0.31 0.32 0.20 ?26947 ?28689 1.98 0.5 0.51 0.63 0.35 ?43 47 0.49 0.03 0.54 46
Table 41: Automatic evaluation metric scores for systems in the WMT11 French-English News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
T
IN
E
-S
R
L
-M
A
T
C
H
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Spanish-English News Task
ALACANT 0.24 0.23 0.24 0.27 0.28 0.17 ?30135 ?29622 1.97 0.53 0.46 0.61 0.34 ?45 43 0.46 0.03 0.50 42
BBN-COMBO 0.25 0.25 0.25 0.32 0.33 0.21 ?15284 ?16192 1.98 0.48 0.5 0.64 0.35 ?44 47 0.49 0.03 0.53 46
CMU-HEAFIELD-COMBO 0.25 0.25 0.25 0.32 0.31 0.20 ?13456 ?16113 1.98 0.5 0.5 0.64 0.35 ?44 47 0.5 0.03 0.54 46
CU-ZEMAN 0.20 0.20 0.21 0.16 0.22 0.12 ?49428 ?48440 1.93 0.61 0.36 0.51 0.28 ?49 32 0.35 0.02 0.38 31
HYDERABAD 0.20 0.20 0.21 0.17 0.21 0.11 ?47754 ?47059 1.94 0.61 0.39 0.50 0.28 ?47 34 0.36 0.02 0.41 33
JHU-COMBO 0.25 0.25 0.25 0.32 0.32 0.20 ?23939 ?22685 1.98 0.49 0.49 0.63 0.35 ?44 47 0.48 0.03 0.52 46
KOC 0.24 0.24 0.24 0.26 0.29 0.17 ?22724 ?25857 1.96 0.53 0.46 0.61 0.34 ?45 42 0.46 0.03 0.49 41
KOC-COMBO 0.25 0.24 0.25 0.28 0.30 0.19 ?22678 ?22267 1.97 0.52 0.48 0.62 0.34 ?44 44 0.48 0.03 0.52 43
ONLINE-A 0.25 0.24 0.25 0.28 0.3 0.18 ?19017 ?20120 1.97 0.52 0.48 0.63 0.35 ?44 45 0.48 0.03 0.52 43
ONLINE-B 0.24 0.24 0.24 0.29 0.30 0.19 ?11980 ?18589 1.97 0.50 0.49 0.62 0.34 ?45 45 0.49 0.03 0.53 44
RBMT-1 0.24 0.24 0.25 0.28 0.28 0.17 ?31202 ?26151 1.97 0.57 0.46 0.61 0.34 ?44 45 0.47 0.03 0.51 43
RBMT-2 0.23 0.23 0.24 0.24 0.25 0.15 ?35157 ?31405 1.96 0.6 0.44 0.59 0.33 ?44 42 0.44 0.02 0.49 41
RBMT-3 0.23 0.23 0.24 0.25 0.26 0.15 ?28289 ?26082 1.97 0.59 0.45 0.6 0.33 ?43 43 0.46 0.03 0.51 42
RBMT-4 0.24 0.23 0.24 0.25 0.26 0.16 ?27892 ?25546 1.97 0.59 0.46 0.60 0.33 ?43 43 0.46 0.03 0.52 42
RBMT-5 0.24 0.23 0.24 0.27 0.26 0.16 ?36770 ?31613 1.96 0.58 0.45 0.6 0.33 ?45 43 0.45 0.03 0.50 42
RWTH-LEUSCH-COMBO 0.25 0.25 0.26 0.32 0.32 0.21 ?15172 ?15261 1.98 0.49 0.5 0.64 0.35 ?43 48 0.50 0.03 0.54 47
SYSTRAN 0.24 0.23 0.24 0.27 0.28 0.17 ?20129 ?26051 1.97 0.53 0.47 0.60 0.33 ?46 44 0.46 0.03 0.51 42
UEDIN 0.22 0.22 0.23 0.22 0.25 0.14 ?25462 ?31678 1.96 0.58 0.45 0.57 0.32 ?47 40 0.44 0.03 0.48 39
UFAL-UM 0.23 0.22 0.23 0.23 0.24 0.14 ?42123 ?37765 1.96 0.60 0.43 0.58 0.32 ?43 41 0.43 0.02 0.48 40
UPM 0.22 0.22 0.23 0.22 0.24 0.14 ?39748 ?38433 1.95 0.58 0.43 0.57 0.32 ?45 40 0.42 0.02 0.46 38
UPV-PRHLT-COMBO 0.25 0.25 0.26 0.32 0.32 0.20 ?16094 ?17723 1.98 0.50 0.49 0.64 0.35 ?43 47 0.5 0.03 0.54 46
Table 42: Automatic evaluation metric scores for systems in the WMT11 Spanish-English News Task
(newssyscombtest2011)
59
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
W
M
P
F
English-Czech News Task
CMU-HEAFIELD-COMBO 0.2 0.19 0.20 0.19 0.22 0.12 2.03 0.62 0.24 ?62 29 27
COMMERCIAL1 0.16 0.15 0.16 0.11 0.16 0.08 2.01 0.70 0.19 ?65 22 21
COMMERCIAL2 0.12 0.10 0.13 0.09 0.15 0.06 2.00 0.73 0.18 ?65 21 19
CU-BOJAR 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.23 ?63 26 24
CU-MARECEK 0.18 0.17 0.18 0.16 0.2 0.1 2.02 0.65 0.22 ?63 26 24
CU-POPEL 0.17 0.16 0.18 0.14 0.19 0.1 2.02 0.66 0.21 ?64 25 23
CU-TAMCHYNA 0.18 0.17 0.18 0.15 0.2 0.1 2.02 0.65 0.22 ?63 26 24
CU-ZEMAN 0.17 0.16 0.17 0.13 0.18 0.09 2.02 0.66 0.21 ?63 23 22
JHU 0.18 0.18 0.18 0.16 0.21 0.11 2.02 0.63 0.22 ?63 26 24
ONLINE-B 0.2 0.19 0.20 0.2 0.22 0.12 2.03 0.62 0.24 ?63 29 27
UEDIN 0.19 0.18 0.19 0.17 0.21 0.11 2.03 0.63 0.23 ?63 27 26
UPV-PRHLT-COMBO 0.2 0.19 0.20 0.20 0.23 0.13 2.03 0.61 0.24 ?63 29 28
Table 43: Automatic evaluation metric scores for systems in the WMT11 English-Czech News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-German News Task
CMU-HEAFIELD-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.39 ?46 36 0.41 0.03 0.45 35
COPENHAGEN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 33 0.38 0.02 0.42 32
CU-TAMCHYNA 0.17 0.17 0.18 0.11 0.18 0.09 1.94 0.70 0.36 ?48 31 0.36 0.02 0.4 30
CU-ZEMAN 0.16 0.15 0.16 0.05 0.17 0.08 1.92 0.71 0.34 ?51 25 0.31 0.02 0.34 25
DFKI-FEDERMANN 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32
DFKI-XU 0.18 0.17 0.18 0.15 0.19 0.1 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34
ILLC-UVA 0.15 0.14 0.15 0.12 0.18 0.08 1.95 0.68 0.33 ?49 32 0.36 0.02 0.4 31
JHU 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32
KIT 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.68 0.37 ?47 35 0.39 0.03 0.43 34
KOC 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.69 0.35 ?47 32 0.36 0.02 0.40 31
KOC-COMBO 0.18 0.17 0.18 0.15 0.2 0.1 1.95 0.67 0.37 ?47 34 0.38 0.02 0.42 33
LIMSI 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.36 ?47 35 0.39 0.03 0.44 33
LIU 0.17 0.17 0.18 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.38 0.02 0.43 33
ONLINE-A 0.18 0.17 0.18 0.15 0.19 0.09 1.96 0.67 0.37 ?47 35 0.40 0.03 0.45 33
ONLINE-B 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.65 0.38 ?46 36 0.42 0.03 0.46 35
RBMT-1 0.17 0.17 0.18 0.13 0.18 0.08 1.95 0.7 0.35 ?46 34 0.39 0.03 0.45 33
RBMT-2 0.16 0.16 0.17 0.12 0.16 0.08 1.94 0.73 0.33 ?47 32 0.37 0.03 0.43 31
RBMT-3 0.18 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?46 35 0.39 0.03 0.46 34
RBMT-4 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.70 0.34 ?47 33 0.38 0.03 0.45 32
RBMT-5 0.17 0.16 0.17 0.12 0.17 0.08 1.95 0.71 0.34 ?47 33 0.38 0.03 0.44 32
RWTH-FREITAG 0.17 0.17 0.17 0.15 0.19 0.09 1.95 0.68 0.36 ?47 34 0.37 0.02 0.41 33
UEDIN 0.17 0.17 0.18 0.14 0.18 0.09 1.95 0.69 0.36 ?47 34 0.38 0.02 0.42 33
UOW 0.17 0.16 0.17 0.13 0.17 0.08 1.95 0.7 0.35 ?47 33 0.37 0.02 0.42 32
UPPSALA 0.17 0.16 0.17 0.14 0.18 0.09 1.95 0.68 0.35 ?47 33 0.37 0.02 0.42 32
UPV-PRHLT-COMBO 0.18 0.18 0.19 0.17 0.20 0.10 1.96 0.66 0.38 ?46 36 0.4 0.03 0.44 35
UZH-COMBO 0.19 0.18 0.19 0.17 0.21 0.11 1.96 0.66 0.38 ?46 36 0.40 0.03 0.44 35
Table 44: Automatic evaluation metric scores for systems in the WMT11 English-German News Task
(newssyscombtest2011)
60
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-French News Task
CMU-HEAFIELD-COMBO 0.25 0.25 0.26 0.34 0.35 0.23 2.02 0.5 0.57 ?41 52 0.54 ?0.01 0.60 50
CU-ZEMAN 0.18 0.17 0.18 0.13 0.19 0.09 1.96 0.68 0.39 ?46 35 0.34 ?0.03 0.40 33
JHU 0.23 0.23 0.24 0.27 0.31 0.19 2.01 0.53 0.52 ?43 47 0.49 ?0.01 0.55 45
KIT 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.52 0.53 ?42 49 0.51 ?0.01 0.57 47
LATL-GENEVA 0.20 0.2 0.21 0.19 0.23 0.12 1.99 0.62 0.44 ?43 41 0.44 ?0.02 0.51 39
LIMSI 0.24 0.24 0.24 0.3 0.31 0.19 2.01 0.53 0.53 ?41 49 0.51 ?0.01 0.58 48
LIUM 0.24 0.23 0.24 0.29 0.31 0.19 2.01 0.53 0.53 ?42 49 0.51 ?0.01 0.57 47
ONLINE-A 0.24 0.23 0.24 0.27 0.3 0.18 2.01 0.53 0.52 ?42 47 0.5 ?0.01 0.56 46
ONLINE-B 0.25 0.25 0.25 0.33 0.35 0.23 2.02 0.5 0.56 ?42 51 0.53 ?0.01 0.59 50
RBMT-1 0.23 0.22 0.23 0.24 0.27 0.16 2.00 0.56 0.5 ?41 45 0.48 ?0.02 0.56 44
RBMT-2 0.22 0.21 0.22 0.22 0.25 0.14 1.99 0.58 0.47 ?42 44 0.46 ?0.02 0.53 42
RBMT-3 0.23 0.22 0.23 0.25 0.28 0.16 2.00 0.56 0.5 ?41 46 0.48 ?0.02 0.56 44
RBMT-4 0.22 0.21 0.22 0.23 0.26 0.15 1.99 0.58 0.47 ?42 43 0.45 ?0.02 0.51 42
RBMT-5 0.22 0.22 0.23 0.23 0.27 0.15 2 0.57 0.49 ?41 45 0.47 ?0.02 0.55 43
RWTH-HUCK 0.23 0.23 0.24 0.29 0.30 0.18 2.01 0.54 0.52 ?42 48 0.5 ?0.01 0.56 47
UEDIN 0.23 0.22 0.23 0.27 0.3 0.18 2.01 0.54 0.51 ?42 47 0.49 ?0.01 0.55 46
UPPSALA 0.23 0.22 0.23 0.27 0.29 0.17 2.00 0.55 0.51 ?42 46 0.48 ?0.01 0.55 45
UPPSALA-FBK 0.23 0.23 0.23 0.28 0.29 0.18 2.01 0.55 0.51 ?42 47 0.49 ?0.01 0.55 46
UPV-PRHLT-COMBO 0.25 0.24 0.25 0.32 0.34 0.22 2.02 0.50 0.55 ?41 51 0.53 ?0.01 0.59 49
Table 45: Automatic evaluation metric scores for systems in the WMT11 English-French News Task
(newssyscombtest2011)
A
M
B
E
R
A
M
B
E
R
-N
L
A
M
B
E
R
-T
I
B
L
E
U
F
15
F
15
G
3
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
4I
B
M
1
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
English-Spanish News Task
CEU-UPV 0.24 0.24 0.24 0.29 0.3 0.18 2.01 0.51 0.55 ?45 46 0.45 0.01 0.45 45
CMU-HEAFIELD-COMBO 0.26 0.25 0.26 0.35 0.34 0.22 2.02 0.47 0.58 ?44 50 0.49 0.01 0.49 49
CU-ZEMAN 0.23 0.22 0.23 0.22 0.27 0.15 1.99 0.55 0.52 ?48 39 0.41 0.00 0.41 38
KOC 0.23 0.23 0.23 0.25 0.27 0.16 2 0.54 0.52 ?46 43 0.42 0.00 0.43 42
KOC-COMBO 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.5 0.56 ?44 47 0.46 0.01 0.47 46
ONLINE-A 0.25 0.24 0.25 0.31 0.32 0.2 2.01 0.49 0.56 ?44 48 0.46 0.01 0.47 46
ONLINE-B 0.25 0.25 0.25 0.33 0.32 0.2 2.02 0.50 0.57 ?44 49 0.47 0.01 0.47 48
PROMT 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?45 45 0.44 0.01 0.46 43
RBMT-1 0.23 0.23 0.23 0.25 0.27 0.16 2 0.55 0.51 ?45 43 0.42 0.00 0.44 42
RBMT-2 0.23 0.22 0.23 0.25 0.26 0.15 1.99 0.55 0.5 ?44 43 0.41 0.00 0.42 41
RBMT-3 0.24 0.23 0.24 0.28 0.28 0.17 2.00 0.53 0.52 ?44 45 0.43 0.00 0.45 43
RBMT-4 0.23 0.22 0.23 0.26 0.26 0.16 1.99 0.54 0.51 ?44 44 0.42 0.00 0.43 42
RBMT-5 0.23 0.22 0.23 0.24 0.26 0.15 1.99 0.57 0.49 ?45 42 0.41 0.00 0.43 41
UEDIN 0.24 0.24 0.24 0.31 0.3 0.18 2.01 0.51 0.55 ?45 47 0.45 0.01 0.45 46
UOW 0.23 0.23 0.24 0.28 0.28 0.16 2.00 0.53 0.53 ?45 45 0.42 0.01 0.43 44
UOW-COMBO 0.25 0.25 0.25 0.33 0.32 0.2 2.01 0.50 0.56 ?44 49 0.47 0.01 0.47 47
UPM 0.21 0.21 0.21 0.21 0.22 0.12 1.98 0.61 0.47 ?47 39 0.37 0.00 0.37 38
UPPSALA 0.24 0.24 0.24 0.3 0.29 0.18 2.01 0.51 0.54 ?45 46 0.44 0.01 0.44 45
UPV-PRHLT-COMBO 0.25 0.25 0.25 0.33 0.32 0.21 2.02 0.49 0.57 ?44 49 0.47 0.01 0.48 48
Table 46: Automatic evaluation metric scores for systems in the WMT11 English-Spanish News Task
(newssyscombtest2011)
61
B
L
E
U
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Haitian Creole (clean)-English Haitian Creole SMS Emergency Response Featured Translation Task
BM-I2R 0.33 ?6798 ?4575 1.96 0.51 0.62 0.34 43 0.44 0.03 0.46 43
CMU-DENKOWSKI 0.29 ?6849 ?6172 1.95 0.53 0.58 0.32 40 0.39 0.02 0.40 39
CMU-HEAFIELD-COMBO 0.32 ?6188 ?4347 1.96 0.51 0.61 0.34 42 0.43 0.03 0.45 42
CMU-HEWAVITHARANA 0.28 ?6523 ?6341 1.95 0.57 0.57 0.32 39 0.38 0.02 0.40 38
HYDERABAD 0.14 ?7548 ?8502 1.92 0.66 0.50 0.28 26 0.3 0.02 0.30 26
KOC 0.23 ?6490 ?9020 1.94 0.67 0.49 0.27 36 0.32 0.02 0.34 35
KOC-COMBO 0.29 ?4901 ?5349 1.95 0.57 0.56 0.31 39 0.38 0.02 0.4 39
LIU 0.27 ?6526 ?6078 1.95 0.59 0.56 0.31 38 0.38 0.02 0.39 37
UMD-EIDELMAN 0.26 ?4407 ?6215 1.95 0.57 0.55 0.31 38 0.37 0.02 0.4 37
UMD-HU 0.22 ?6379 ?7460 1.94 0.59 0.51 0.28 35 0.36 0.02 0.39 34
UPPSALA 0.27 ?5497 ?6754 1.95 0.59 0.54 0.3 38 0.36 0.02 0.39 37
UPV-PRHLT-COMBO 0.32 ?6896 ?5968 1.96 0.53 0.6 0.33 42 0.41 0.02 0.43 41
Table 47: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (clean)-English Haitian
Creole SMS Emergency Response Featured Translation Task (newssyscombtest2011)
B
L
E
U
M
T
E
R
A
T
E
R
M
T
E
R
A
T
E
R
-P
L
U
S
R
O
S
E
T
E
R
M
E
T
E
O
R
-1
.3
-A
D
Q
M
E
T
E
O
R
-1
.3
-R
A
N
K
M
P
F
T
E
S
L
A
-B
T
E
S
L
A
-F
T
E
S
L
A
-M
W
M
P
F
Haitian Creole (raw)-English Haitian Creole SMS Emergency Response Featured Translation Task
BM-I2R 0.29 ?3885 ?3017 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38
CMU-DENKOWSKI 0.25 ?3965 ?3905 1.95 0.60 0.53 0.3 35 0.38 0.02 0.4 35
CMU-HEAFIELD-COMBO 0.28 ?3057 ?2588 1.96 0.57 0.57 0.32 39 0.42 0.02 0.44 38
CMU-HEWAVITHARANA 0.25 ?3701 ?3824 1.95 0.61 0.53 0.3 35 0.37 0.02 0.39 35
JHU 0.14 ?3207 ?4279 1.92 0.74 0.43 0.24 26 0.30 0.02 0.32 26
LIU 0.25 ?3447 ?3445 1.95 0.60 0.54 0.30 36 0.38 0.02 0.4 35
UMD-EIDELMAN 0.24 ?2826 ?3754 1.94 0.64 0.52 0.29 34 0.36 0.02 0.39 34
UPV-PRHLT-COMBO 0.28 ?3591 ?3370 1.95 0.58 0.56 0.32 38 0.4 0.02 0.42 38
Table 48: Automatic evaluation metric scores for systems in the WMT11 Haitian Creole (raw)-English Haitian Creole
SMS Emergency Response Featured Translation Task (newssyscombtest2011)
62
INTER-ANNOTATOR AGREEMENT (I.E. ACROSS ANNOTATORS)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
Czech-English, individual systems 0.591 0.354 0.367 0.535 0.343 0.293
English-Czech, individual systems 0.608 0.359 0.388 0.552 0.350 0.312
German-English, individual systems 0.562 0.377 0.298 0.536 0.370 0.264
English-German, individual systems 0.564 0.352 0.327 0.528 0.348 0.276
Spanish-English, individual systems 0.695 0.398 0.493 0.683 0.393 0.477
English-Spanish, individual systems 0.574 0.343 0.352 0.548 0.339 0.317
French-English, individual systems 0.616 0.367 0.393 0.584 0.361 0.349
English-French, individual systems 0.631 0.382 0.403 0.603 0.376 0.363
European languages, individual systems 0.601 0.362 0.375 0.561 0.355 0.320
Czech-English, system combinations 0.700 0.334 0.549 0.577 0.369 0.329
English-Czech, system combinations 0.812 0.348 0.711 0.696 0.392 0.500
German-English, system combinations 0.675 0.353 0.498 0.629 0.341 0.437
English-German, system combinations 0.608 0.346 0.401 0.547 0.334 0.320
Spanish-English, system combinations 0.638 0.335 0.456 0.604 0.359 0.382
English-Spanish, system combinations 0.657 0.335 0.485 0.603 0.371 0.369
French-English, system combinations 0.654 0.336 0.479 0.608 0.336 0.410
English-French, system combinations 0.678 0.352 0.503 0.595 0.339 0.388
European languages, system combinations 0.671 0.335 0.505 0.598 0.342 0.389
Haitian (Clean)-English, individual systems 0.693 0.364 0.517 0.640 0.353 0.443
Haitian (Raw)-English, individual systems 0.689 0.357 0.517 0.639 0.344 0.450
Haitian-English, individual systems 0.691 0.362 0.516 0.639 0.350 0.446
Haitian (Clean)-English, system combinations 0.770 0.367 0.636 0.645 0.333 0.468
Haitian (Raw)-English, system combinations 0.745 0.345 0.611 0.753 0.361 0.613
Haitian-English, system combinations 0.761 0.358 0.628 0.674 0.335 0.509
Tunable metrics task (Urdu-English) 0.692 0.337 0.535 0.641 0.363 0.437
WMT10 (European languages, individual vs. individual) 0.663 0.394 0.445 0.620 0.385 0.382
WMT10 (European languages, combo vs. combo) 0.728 0.344 0.586 0.629 0.334 0.443
WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.634 0.360 0.428
WMT10 (European languages, all systems) 0.658 0.374 0.454 0.626 0.367 0.409
Table 49: Inter-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down by
language pair. The highlighted rows correspond to rows in the top half of Table 7. See Table 50 below for detailed
intra-annotator agreement rates.
63
INTRA-ANNOTATOR AGREEMENT (I.E. SELF-CONSISTENCY)
ALL COMPARISONS NO REF COMPARISONS
P (A) P (E) ? P (A) P (E) ?
Czech-English, individual systems 0.762 0.354 0.632 0.713 0.343 0.564
English-Czech, individual systems 0.743 0.359 0.598 0.700 0.350 0.539
German-English, individual systems 0.675 0.377 0.478 0.670 0.370 0.475
English-German, individual systems 0.704 0.352 0.543 0.700 0.348 0.541
Spanish-English, individual systems 0.750 0.398 0.585 0.719 0.393 0.537
English-Spanish, individual systems 0.644 0.343 0.458 0.601 0.339 0.396
French-English, individual systems 0.829 0.367 0.730 0.816 0.361 0.712
English-French, individual systems 0.716 0.382 0.541 0.681 0.376 0.488
European languages, individual systems 0.722 0.362 0.564 0.685 0.355 0.512
Czech-English, system combinations 0.756 0.334 0.633 0.657 0.369 0.457
English-Czech, system combinations 0.923 0.348 0.882 0.842 0.392 0.740
German-English, system combinations 0.732 0.353 0.586 0.716 0.341 0.569
English-German, system combinations 0.722 0.346 0.575 0.676 0.334 0.513
Spanish-English, system combinations 0.783 0.335 0.673 0.720 0.359 0.562
English-Spanish, system combinations 0.741 0.335 0.610 0.711 0.371 0.540
French-English, system combinations 0.772 0.336 0.657 0.659 0.336 0.487
English-French, system combinations 0.841 0.352 0.755 0.714 0.339 0.568
European languages, system combinations 0.787 0.335 0.680 0.717 0.342 0.571
Haitian (Clean)-English, individual systems 0.758 0.364 0.619 0.686 0.353 0.515
Haitian (Raw)-English, individual systems 0.783 0.357 0.663 0.756 0.344 0.628
Haitian-English, individual systems 0.763 0.362 0.628 0.700 0.350 0.539
Haitian (Clean)-English, system combinations 0.882 0.367 0.813 0.778 0.333 0.667
Haitian (Raw)-English, system combinations 0.882 0.345 0.820 0.802 0.361 0.690
Haitian-English, system combinations 0.882 0.358 0.816 0.784 0.335 0.675
Tunable metrics task (Urdu-English) 0.857 0.337 0.784 0.856 0.363 0.774
WMT10 (European languages, individual vs. individual) 0.757 0.394 0.599 0.728 0.385 0.557
WMT10 (European languages, combo vs. combo) 0.783 0.344 0.670 0.719 0.334 0.578
WMT10 (European languages, individual vs. combo) N/A N/A N/A 0.746 0.360 0.603
WMT10 (European languages, all systems) 0.755 0.374 0.609 0.734 0.367 0.580
Table 50: Intra-annotator agreement rates, for the various manual evaluation tracks of WMT11, broken down by
language pair. The highlighted rows correspond to rows in the bottom half of Table 7. See Table 49 above for detailed
inter-annotator agreement rates.
64
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 130?134,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
MAISE: A Flexible, Configurable, Extensible Open Source Package for
Mass AI System Evaluation
Omar F. Zaidan
Dept. of Computer Science
and
The Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218, USA
ozaidan@cs.jhu.edu
Abstract
The past few years have seen an increasing
interest in using Amazon?s Mechanical Turk
for purposes of collecting data and perform-
ing annotation tasks. One such task is the
mass evaluation of system output in a variety
of tasks. In this paper, we present MAISE,
a package that allows researchers to evalu-
ate the output of their AI system(s) using hu-
man judgments collected via Amazon?s Me-
chanical Turk, greatly streamlining the pro-
cess. MAISE is open source, easy to run, and
platform-independent. The core of MAISE?s
codebase was used for the manual evaluation
of WMT10, and the completed package is be-
ing used again in the current evaluation for
WMT11. In this paper, we describe the main
features, functionality, and usage of MAISE,
which is now available for download and use.
1 Introduction
The ability to evaluate system output is one of the
most important aspects of system development. A
properly designed evaluation paradigm could help
researchers test and illustrate the effectiveness, or
lack thereof, of any changes made to their system.
The use of an automatic metric, whether it is a sim-
ple one such as classification accuracy, or a more
task-specific metric such as BLEU and TER for ma-
chine translation, has become a standard part of any
evaluation of empricial methods. There is also ex-
tensive interest in exploring manual evaluation of
system outputs, and in making such a process fea-
sible and efficient, time- and cost-wise. Such human
feedback would also be valuable because it would
help identify systematic errors and guide future sys-
tem development.
Amazon?s Mechanical Turk (MTurk) is a virtual
marketplace that allows anyone to create and post
tasks to be completed by human workers around the
globe. Each instance of those tasks, called a Human
Intelligence Task (HIT) in MTurk lingo, typically
requires human understanding and perception that
machines are yet to achieve, hence making MTurk
an example of ?artificial artificial intelligence,? as
the developers of MTurk aptly put it. Arguably, the
most attractive feature of MTurk is the low cost asso-
ciated with completing HITs and the speed at which
they are completed.
Having discovered this venue, many researchers
in the fields of artificial intelligence and machine
learning see MTurk as a valuable and effective
source of annotations, labels, and data, namely the
kind requiring human knowledge.
One such kind of data is indeed human evalua-
tion of system outputs. For instance, if you construct
several speech recognition systems, and would like
to know how well each of the systems performs,
you could create HITs on MTurk that ?showcase? the
transcriptions obtained by the different systems, and
ask annotators to indicate which systems are supe-
rior and which ones are inferior. The same can be
applied to a variety of tasks, such as machine trans-
lation, object recognition, emotion detection, etc.
The aim of the MAISE package is to stream-
line the process of creating those evaluation tasks
and uploading the relevant content to MTurk to be
judged, without having to familiarize and involve
oneself with the mechanics, if you will, of Mechan-
ical Turk. This would allow you to spend more
time worrying about improving your system rather
than dealing with file input and output and MTurk?s
sometimes finicky interface.
130
2 Overview
MAISE is a collection of tools for Mass AI System
Evaluation. MAISE allows you to evaluate the out-
put of different systems (and/or different variations
of a system) using the workforce of Amazon?s Me-
chanical Turk (MTurk). MAISE can be used to com-
pare two simple variants of the same system, work-
ing with a couple of variations of your task, or it can
be used to perform complete evaluation campaigns
involving tens of systems and many variations.
The core of MAISE?s codebase was written to
run the manual component of WMT10?s evaluation
campaign. In the manual evaluation, various MT
systems are directly compared to each other, by an-
notators who indicate which systems produce better
outputs (i.e. better translations). Starting in 2010,
the evaluation moved from using a locally hosted
web server, and onto MTurk, taking advantage of
MTurk?s existing infrastructure, and making avail-
able the option to collect data from a large pool of
annotators, if desired, rather than relying solely on
recruited volunteers. That evaluation campaign in-
volved around 170 submissions over eight different
language pairs. In 2011, the number increased to
190 submissions over ten language pairs.
We note here that although MAISE was written
with MT in mind, it can be used for other ML/AI
tasks as well. Some of the supported features are
meant to make MT evaluation easier (e.g. MAISE is
aware of which language is being translated to and
from), but those could simply be ignored for other
tasks. As long as the task has some concept of ?in-
put? and some concept of ?output? (e.g. a foreign
sentence and a machine translation), then MAISE is
appropriate.
Given this paper?s venue of publication, the re-
mainder of the paper assumes the task at hand is ma-
chine translation.
3 The Mechanics of MAISE
The components of MAISE have been designed to
completely eliminate the need to write any data
processing code, and to minimize the need for the
user to perform any manual tasks on MTurk?s inter-
face, since MAISE facilitates communication with
MTurk. Whenever MAISE needs to communicate
with MTurk, it will rely on MTurk?s Java SDK,
which is already included in the MAISE release
(allowed under the SDK?s license, Apache License
V2.0).
Once you create your evaluation tasks and upload
the necessary content to MTurk, workers will begin
to complete the corresponding HITs. On a regular
(e.g. daily) basis, you will tell MAISE to retrieve the
new judgments that workers provided since the last
time MAISE checked. The process continues until
either all your tasks are completed, or you decide
you have enough judgments.
You can use MAISE with any evaluation setup
you like, as long as you design the user interface
for it. Currently, MAISE comes with existing sup-
port for a particular evaluation setup that asks anno-
tators to rank the outputs of different systems rela-
tive to each other. When we say ?existing support?
we mean the user interface is included, and so is an
analysis tool that can make sense of the judgments.
This way, you don?t need to do anything extra to ob-
tain rankings of the systems. You can read more
about this evaluation setup in the overview papers
of the Workshop on Statistical Machine Translation
(WMT) for the past two years.
3.1 Requirements and Setup
MAISE is quite easy to use. Beyond compiling
a few Java programs, there is no need to install
anything, modify environment variables, etc. Fur-
thermore, since it is Java-based, it is completely
platform-independent.
To use MAISE, you will need:
? Java 6
? Apache Ant
? A hosting location (where you place certain
HTML files)
? An MTurk Requester account
You will also need an active Internet connection
whenever new tasks need to be uploaded to MTurk,
and whenever judgments need to be collected from
MTurk. The setup details are beyond the scope
of this paper, but are straightforward, and can be
found in MAISE?s documentation, including guid-
ance with all the MTurk-related administrative is-
sues (e.g. the last point in the above list).
131
3.2 Essential Files
MAISE will assume that the user has a certain set of
?essential files? that contain all the needed informa-
tion to perform an evaluation. These files are:
1) The system outputs should be in plain
text format, one file per system. The
filenames should follow the pattern
PROJECT.xx-yy.sysname, where
PROJECT is any identifying string cho-
sen by the user, xx is a short name for the
source language, and yy is a short name for
the the target language.
2) The source files should be in plain text
as well, one file per language pair. The
source filenames should follow the pattern
PROJECT.xx-yy.src, where PROJECT
matches the identifying string used in the sub-
mission filenames. (The contents of such a file
are in the xx language.)
3) The reference files, also one per language pair,
with filenames PROJECT.xx-yy.ref. (The
contents of such a file are in the yy language.)
4) A specification file that contains values for var-
ious parameters about the project (e.g. the lo-
cation of the above files).
5) A batch details file that contains information
about the desired number of MTurk tasks and
their particular properties.
As one could see, the user need only provide the
bare minimum to get their evaluation started. More
details about items (4) and (5) are provided in the
documentation. Essentially, they are easily readable
and editable files, and all the user needs to do to cre-
ate them is to fill out the provided templates.
3.3 The Components of MAISE
There are three main steps necessary to perform an
evaluation on MTurk: create the evaluation tasks,
upload them to MTurk, and retrieve answers for
them. Each of those three steps corresponds to a
single component in MAISE.
3.3.1 The BatchCreator
The first step is to create some input files for
MTurk: the files that contain actual instantiations of
our tasks, with actual sentences. This will be the first
step that requires you to make some real executive
decisions regarding your tasks. Among other things,
you will decide how many judgments to collect and
who to allow to give you those judgments.
Each batch corresponds to a single task on
MTurk. Typically, each batch corresponds to a sin-
gle language pair. So, if you are performing a
full evaluation campaign, you would be creating as
many batches as there are language pairs. If you are
merely comparing several variants of the same sys-
tem, say, for Arabic-English, you would probably
have just one batch.
That said, you may have more than one batch for
the same language pair, that nonetheless differ in
other properties. In fact, each batch has a number
of settings that need to be specified, including:
1) what language pair does this batch involve?
2) how many HITs does this batch include?
3) how many times should each HIT be com-
pleted?
4) what is the reward per assignment?
5) what are the qualifications necessary for an an-
notator to be allowed to perform the task (e.g.
location, approval rating)?
Those settings are all specified in a single file,
the abovementioned batch details file. The user
them simply runs the BatchCreator component,
which processes all this information and creates the
necessary files for each batch.
3.3.2 The Uploader
After the BatchCreator creates the different
files for the different batches, those files must be
uploaded to MTurk in order to create the various
batches. There will be a single file, called the up-
load info file, that contains the locations of the files
to be uploaded. The upload info file is created au-
tomatically, and all the user needs to do is pass it
as a parameter to the next MAISE component, the
Uploader.
132
The Uploader communicates with MTurk via
a web connection. Once it has completed execution,
HITs for your tasks will start to appear on the MTurk
website, available for MTurk?s workers to view and
complete them.
3.3.3 The Retriever
At this point, you would be waiting for Turkers to
find your task and start accepting HITs and complet-
ing them. You can retrieve those answers by using
another MAISE component that communicates with
MTurk called the Retriever. It can be instructed
to retrieve all answers for your HITs or only a subset
of them. It retrieves all the answers for those HITs,
and appends those answers to an answer log file.
Note that the Retriever does not necessarily
approve any of the newly submitted assignments. It
can be instructed to explicitly retrieve those answers
without approving them, giving you the chance to
first review them for quality. Alternatively, it can be
instructed to approve the assignments as it retrieves
them, and also to reject certain assignments or cer-
tain annotators that you have identified as being of
sub-par quality. All this information is placed in
plain text files, easy to create and maintain.
When you use MAISE to perform an actual eval-
uation on MTurk, you should run the Retriever
fairly regularly, perhaps once every day or two.
Each time, review the retrieved results, and rerun
the Retriever in ?decision mode? enabled, to
aprove/reject the pending submissions.
4 Analyzing the Results: An Example
Once the tasks have been completed, all the an-
swers will have been written into an answers log file.
The log file is in plain format, and contains exten-
sive information about each HIT completed, includ-
ing a worker ID, time required to complete, and, of
course, the answers themselves. Naturally, analyz-
ing the results of the evaluation depends on what the
task was, and what the interface you designed looks
like. You can write your own code to read the log
file and make sense out of them.
MAISE already comes equipped with an analy-
sis tool for one particular task: the ranking task. In
this setup, the annotator evaluates system outputs by
ranking them from best to worst. The rank labels
are interpreted as pairwise comparisons (e.g. 5 rank
labels correspond to
(5
2
)
= 10 pairwise compar-
isons), and each system is assigned a score reflect-
ing how often it wins those pairwise comparisons.
This is the setup used in the evaluation campaigns
of WMT10 and WMT11.
The analysis tool takes as input the answers log
file as is, and extracts from it all the rank labels.
Each system?s score is computed, and the tool pro-
duces a table for each language pair displaying the
participating systems, in descending order of their
scores. It also creates an additional head-to-head ta-
ble, that summarizes for a specific pair of systems
how often each system outranked the other. The out-
put is created in HTML format, for easy viewing in
a browser.
Furthermore, the tool produces a detailed worker
profile table. Each row in this table corresponds to
one worker, identified by their Amazon worker ID,
and includes certain measures that can help guide
you identify bad workers, who are either clicking
randomly, or perhaps simply not doing the task prop-
erly. Those measures include:
? Average time required per HIT: a suspi-
ciously fast annotator might not be performing
the task diligently.
? The reference preference rate (RPR): how of-
ten did the annotator correctly prefer an em-
bedded reference translation; a low RPR almost
certainly indicates random clicking, with typi-
cal good values at 0.97 and up.
? Prevalence of tied rank labels: an overly high
percentage of tied comparisons indicates an
overly ?conservative? worker, hesitant to distin-
guish between outputs.
? The annotator?s intra-annotator agreement:
i.e. the annotator?s consistency with them-
selves, based on how often they repeated the
same judgment when comparing the same sys-
tem pair.
To appreciate the tool?s output, the reader is en-
couraged to view the results of a real-life evaluation
campaign at http://bit.ly/jJYzkO. These
are results of analyzing 85,000+ rank labels in an
evaluation campaign of 40+ MT systems over six
language pairs.
133
5 Download and Licensing
MAISE can be obtained from the author?s webpage:
http://cs.jhu.edu/?ozaidan/maise/.
The release includes MAISE?s source code, in-
structions, documentation, and a tutorial. MAISE
is an open-source tool, licensed under the terms of
the GNU Lesser General Public License (LGPL).
Therefore, it is free for personal and scientific use
by individuals and/or research groups. It may not
be modified or redistributed, publicly or privately,
unless the licensing terms are observed. If in doubt,
contact the author for clarification and/or an explicit
permission. The distribution also includes the
MTurk Java SDK v1.2.2, which is licensed under
the terms of the Apache License V2.0.
Acknowledgments
I developed MAISE while I was funded by DARPA?s
GALE Program, and in part by the EuroMatrixPlus
Project. I would like to thank Chris Callison-Burch,
Ondr?ej Bojar, and everybody who gave feedback
during the WMT10 evaluation campaign. More
importantly, much thanks goes to Josh Schroeder,
the author of the previous implementation used pre-
2010, who helped me navigate his code when I first
started reimplementing the data processing compo-
nents.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
134
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 484?493,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Evaluating (and Improving) Sentence Alignment under Noisy Conditions
Omar Zaidan
Microsoft Research, USA
ozaidan@cs.jhu.edu
Vishal Chowdhary
Microsoft Research, USA
vishalc@microsoft.com
Abstract
Sentence alignment is an important step
in the preparation of parallel data. Most
aligners do not perform very well when
the input is a noisy, rather than a highly-
parallel, document pair. Evaluating align-
ers under noisy conditions would seem to
require creating an evaluation dataset by
manually annotating a noisy document for
gold-standard alignments. Such a costly
process hinders our ability to evaluate
an aligner under various types and lev-
els of noise. In this paper, we propose
a new evaluation framework for sentence
aligners, which is particularly suitable for
noisy-data evaluation. Our approach is
unique as it requires no manual labeling,
instead relying on small parallel datasets
(already at the disposal of MT researchers)
to generate many evaluation datasets that
mimic a variety of noisy conditions. We
use our framework to perform a compre-
hensive comparison of three aligners un-
der noisy conditions. Furthermore, our
framework facilitates the fine-tuning of a
state-of-the-art sentence aligner, allowing
us to substantially increase its recall rates
by anywhere from 5% to 14% (absolute)
across several language pairs.
1 Introduction
Virtually all training pipelines of statistical ma-
chine translation systems expect training data to
be in the form of a sequence of parallel sentence
pairs. This means that a pair of parallel documents
must first be segmented into a sequence of aligned
sentence pairs, discarding or combining sentences
when needed, and aligning sentences as appropri-
ate. The performance and output of an SMT sys-
tem is directly dependent on the amount and qual-
ity of available training data. Therefore, it is crit-
ical to perform this sentence alignment step prop-
erly, ensuring both high recall (to have as much
training data as possible) and high precision (to
avoid noisy training data).
While sentence aligners achieve excellent per-
formance on highly-parallel, clean data, the task is
much more difficult under noisy conditions. Some
prior work has investigated evaluation under noisy
conditions (see section 6), but the major focus of
prior work has been the clean-data scenario, where
accuracy rates exceed 98% (e.g. Simard et al
(1993), Moore (2002)). For one thing, this meant
that the various sentence alignment algorithms dif-
fer only slightly in absolute terms. Similarly, fine-
tuning any one of those algorithms might not seem
to have an impact on performance. More impor-
tantly, this also meant that we do not have a clear
understanding of how well these algorithms would
perform under noisy conditions.
Arguably, there was little need to examine sen-
tence alignment of noisy datasets in early MT re-
search, since almost all training data came from
high-quality, highly-parallel sources, such as UN
documents or parliamentary proceedings.1 How-
ever, recent efforts have attempted to utilize web
resources and non-perfectly-parallel texts, such as
Wikipedia articles and news stories (e.g. Resnik
and Smith (2003), Utiyama and Isahara (2003),
Munteanu and Marcu (2005), and Smith et al
(2010)). Such resources naturally contain signifi-
cantly more noise, at a level that would render sen-
tence alignment a much less straightforward task.
Because sentence alignment algorithms had
usually been evaluated under a clean-data sce-
nario, there are fewer empirical results to guide
those who wish to extract parallel data from noisy
1Also, parallel datasets created explicitly for MT research
(by having a source corpus translated into the target language)
would be already sentence-aligned by mere construction if
the source side is split into sentences beforehand.
484
sources. Furthermore, there is also no easy way
to fine-tune an aligner of interest. For building
the Microsoft Translation service, we are con-
tinuously mining inherently-noisy web resources,
from which we extract MT training data for dozens
of the world?s languages. Therefore, having a
principled method to evaluate and fine-tune our
aligner was critical.
In this paper, we describe our framework for
evaluating sentence alignment under noisy con-
ditions. We use this framework to examine and
evaluate the Moore alignment algorithm (Moore,
2002), which was empirically shown to be state-
of-the-art under clean conditions, and which we
regularly use to extract parallel data from web re-
sources to create training data. We perform a com-
prehensive comparison of this aligner against two
other algorithms, and furthermore use our frame-
work to fine-tune the algorithm along dimensions
of interest (such as the aligner?s search parame-
ters) by quantitatively evaluating how the aligner?s
performance is affected by such changes.
The paper is organized as follows. We briefly
define sentence alignment and existing approaches
in section 2. We then discuss the evaluation of
alignment algorithms in section 3, and present our
evaluation framework. In section 4, we perform a
comparative assessment of three alignment algo-
rithms using our framework, illustrating the dif-
ferences between them under noisy conditions. In
section 5, we present two additional applications
of our framework, namely fine-tuning an aligner
and performing training data cleanup. Finally, we
give an overview in section 6 of prior work that
has tackled the specific issue of evaluating sen-
tence aligners.
2 Sentence Alignment
Sentence alignment is the process by which a pair
of parallel documents lacking explicit sentence
links are used to extract a parallel dataset consist-
ing of sentence pairs that are translations of each
other. Specifically, let S and T be the document
pair to be aligned, with S composed of the sen-
tence sequence s1, s2, ..., sm, and T composed of
the sentence sequence t1, t2, ..., tn. A sentence
alignment of S and T is a segmentation of each
of S and T into p sequences s?1, s?2, ..., s?p and
t?1, t?2, ..., t?p such that the following holds about the
segmentation of S: (a similar set of conditions ex-
ist that correspond to T )
? s?i = CS [a, b) for some 1 ? a ? b ? m ?i
? s?1 = CS [1, b) for some b >= 1
? s?p = CS [a,m) for some a <= m
? If s?i = CS [a, b), then s?i+1 = CS [b, c)
? If s?i = CS [x, x), then t?i = CT [y, z) such
that y 6= z
Above, CS [a, b) is the concatenation of
sa, sa+1, ..., sb?1, which indicates the possibility
of aligning multiple source sentences to a single
sentence (or combined sequence of sentences) on
the target side. Note that CS [a, a) is the empty
string, which indicates deletion on the target side
(i.e. a target sentence is aligned to the empty
string). The last condition disallows aligning an
empty string to another empty string, thus elimi-
nating the possibility for an infinite segmentation
sequence.
Note that the result of this segmentation is q
(non-empty) sentence pairs, where q <= p (and
naturally q <= m and q <= n). The deleted sen-
tences, each aligned with an empty string, are left
out of the resulting parallel corpus.
2.1 Approaches to Sentence Alignment
Tiedemann (2007) and Santos (2011) each pro-
vide a broad overview of sentence alignment, giv-
ing a timeline of relevant research and discussing
algorithms and performance metrics for sentence
alignment. In general, there are two main ap-
proaches to sentence alignment: length-based and
lexical-based.
In length-based alignment approaches (e.g.
Brown et al (1991), Gale and Church (1991), and
Kay and Ro?scheisen (1993)), the aligner relies on
a probabilistic model that describes the source-
to-target sentence length ratio for a pair of corre-
sponding sentences. Such a model would account
both for the average or typical length ratio as well
as its variance. The aligner proceeds to align sen-
tence pairs such that the output would be highly
likely under the length ratio model.
In lexical-based alignment approaches (e.g.
Chen (1993), Melamed (1997), Simard and Pla-
mondon (1998), Menezes and Richardson (2001),
and the LDC alignment tool, Champollion (Ma,
2006)), the aligner relies on a probabilistic model
that describes the lexical similarity between a pair
of sentences. The model could either be a fully-
trained translation model, or a simpler bilingual
485
lexicon that finds corresponding word pairs. In
contrast to length-based algorithms, lexical-based
approaches typically require external bilingual re-
sources, and usually perform better.
Previous work on sentence alignment varies
across a few other dimensions as well. Some
lexical-based algorithms build the needed bilin-
gual resources from the very dataset that is to
be aligned, whereas other approaches assume that
such resources are externally provided. Another
dimension is the need to provide anchor points
within the text to be aligned, such as in the form
of paragraph-level alignment. Such anchor points
are typically needed to restrict the search space to
a manageable size.
Another group of aligners take a hybrid ap-
proach, relying both on sentence length and lexical
similarity (e.g. Zhao and Vogel (2002)). One no-
table example is the algorithm by Moore (2002),
which has the benefit of relying only on the in-
put data when training the lexical similarity model,
rather than needing external resources (bilingual
lexicon or parallel training data) for that purpose.
The Moore algorithm is a state-of-the-art algo-
rithm, and has been used, for example, to align
the data for the Europarl corpus (Koehn, 2005),
and is often a strong baseline in papers propos-
ing new alignment algorithms (e.g. Braune and
Fraser (2010)). In section 4, we use our pro-
posed framework to evaluate Moore?s algorithm,
and compare it against two other aligners, illustrat-
ing our framework?s utility as a comparative tool.
3 Evaluating Sentence Alignment
Algorithms under Noisy Conditions
In much of the prior work mentioned above in 2.1,
and in other comparative evaluation work (e.g.
Simard et al (1993), Langlais et al (1998), and
Ve?ronis and Langlais (2000)), sentence align-
ment algorithms were evaluated using a manually-
created gold-standard dataset. This is done by
taking a parallel dataset, and manually annotating
sentence pairs that are translations of each other
(and should therefore be aligned). This evaluation
dataset is provided as input to the aligner, which
is evaluated based on the precision and recall of
its output, as measured against the set of hand-
annotated sentence pairs.
While this is a reasonable approach that mir-
rors the evaluation model in many other tasks
within machine learning (i.e. to manually create
an evaluation set with gold-standard labels, based
on which the learner?s output is judged), it suffers
from some drawbacks.
For one thing, all the difficulties of creating an
evaluation dataset apply here as well. Most signif-
icantly, manually labeling sentence pairs is costly
and time-consuming. This problem is magnified
in the context of machine translation, since one
should ideally evaluate a sentence alignment algo-
rithm under several language pairs, rather than a
single one, requiring the creation of several evalu-
ation sets, rather than a single one.
Furthermore, prior work usually used a fairly
clean dataset to annotate, on which it is relatively
easy for an aligner to achieve very high precision
and recall rates. This means that differences be-
tween algorithms are sometimes fairly small in ab-
solute terms, making it difficult to attribute such
differences to the algorithms themselves or to sta-
tistical noise.
The noisy-data scenario is extremely important
in the web domain. The web is a huge repository
of parallel documents that machine translation
systems leverage for training data, and we continu-
ally extract content from noisy online sources. Un-
like the above evaluation setup, we are concerned
with scenarios where the data has a relatively high
degree of noise, where by ?noise? we mean both
non-perfect translations but also additional content
on one side that is not translated at all. Both kinds
of noise should be dealt with appropriately: the
first introduces imperfect training data, while the
second could eliminate good translations, or might
send word alignment into a frenzy.
Because prior work mostly focused on the
clean-data scenario, it is unknown whether previ-
ous evaluations would hold for noisy input. This
makes it difficult to judge how these algorithms
would compare to each other under more noisy
conditions, or when any other experimental di-
mension is varied, such as domain and the lan-
guage pair in question.
3.1 Creating Noisy Datasets for Evaluation
Purposes
How can we create a noisy-data scenario under
which to evaluate a sentence alignment algorithm?
One approach is to mimic prior work: in a dataset
that is known to be noisy, have an annotator select
the sentence pairs that should be aligned to each
other. However, this approach would be expensive
486
and time-consuming.
We propose a completely different approach.
Rather than attempting to annotate corresponding
sentences in a dataset that is known to be noisy,
we deliberately introduce noise into a dataset that
is already perfectly-aligned (and for which, as a
consequence, we already know the sentence cor-
respondence).
Specifically, we start with a parallel dataset
D that we know to be perfectly-aligned. Such
datasets are abundant and readily available for MT
researchers in the form of a myriad of tuning and
test datasets across many language pairs and do-
mains. We introduce noise into D (using any of
the methods described below and detailed in sub-
section 4.2) to obtain a modified dataset D?. The
source side of D? is a subset of the source side of
D (possibly reordered), and the same holds for the
target side. Since we know what the correct sen-
tence alignments are in D, we also know, by mere
construction, what the correct alignments in D? are
as well. This allows us to easily compute precision
and recall of a sentence alignment algorithm when
it is given D? as input, without the need to collect
a single annotation.
We employ several methods to create a noisy
dataset D? from a perfectly-aligned dataset D:2
? Clean dataset. The source and target sides of
D? are exactly the unaltered source and target
sides of D. This represents the easiest test set
for a sentence aligner, as the test set consists
entirely of 1-to-1 mappings, all of which fall
exactly along the search matrix diagonal.
? Random deletions. The source side of D? is
a subset of the source side of D, where the
number of discarded sentences is determined
by a source deletion rate dels. For example,
for a dataset D with 1000 sentences on the
source side and dels = 0.10, the source side
of D? consists of 900 randomly-chosen sen-
tences from the source side of D (with no re-
ordering). The target side of D? is created
similarly, using a target deletion rate delt.
Note that the deletion on the target side is
done independently from the deletion on the
2In a few of our experiments, we make use of two datasets
(that are non-overlapping and non-related), say D1 and D2,
to createD?. The way we frame the creation ofD?, as a map-
ping from a single dataset D, still applies here: D is simply
the concatenation of D1 and D2.
source side. That is, the probability of delet-
ing the ith sentence on the target side is delt,
regardless of whether the ith sentence on the
source side was deleted or not.
? Random combinations. The source and tar-
get sides of D? are the same as those from
D, but with random consecutive pairs of sen-
tences combined into a single sentence. The
degree to which sentences are combined is
determined by source and target combination
rates combs and combt. For example, for a
dataset D with 1000 sentences on the source
side and combs = 0.10, 100 sentence pairs
(each consisting of consecutive sentences)
are chosen randomly, and each pair is com-
bined into a single sentence, yielding a set
of 900 source sentences in D?. The goal of
this scenario is to test the aligner?s ability to
recover 1-to-many and many-to-1 mappings,
rather than focusing solely on 1-to-1 map-
pings.3 As with random deletions, the combi-
nation processes on the source side and on the
target side are independent from each other.
? Randomized order. The source side of D?
consists of the source side of D, but in ran-
dom order. The target side of D is also ran-
domized.
? Length-aligned from same dataset. The
source side of D? is exactly the same as the
source side of D. The noise is introduced
into the target side, where all the target sen-
tences from D are preserved, but they are re-
ordered. The reordering is not completely
stochastic. Rather, an attempt is made to have
the sentences length-aligned as much as pos-
sible. This is somewhat of an adversarial sce-
nario, since a length-based alignment method
would align too many sentences that are com-
pletely unrelated to each other.
? Different datasets. The dataset D? is formed
by taking two datasets D1 and D2, and align-
ing the source side of D1 with the target side
of D2, and vice versa. A good sentence
aligner would deem that the source and tar-
get sides are unrelated, yielding a very low
alignment rate.
3With high enough combination rates, many-to-many
mappings arise as well.
487
4 Experimental Results
Even though this paper is not mainly concerned
with comparing aligners to each other, we utilize
our proposed framework and apply it to three dif-
ferent aligners as a demonstration. In this section,
we describe the aligners to be compared, and pro-
vide specific details about how our test sets were
generated. We then describe the metrics we use,
and present results based on these metrics.
4.1 Sentence Aligners
The first aligner (LEN) is a length-based aligner
based on the algorithm described in Brown et al
(1991). It segments the source and target sides
by finding the highest-likelihood segmentation ac-
cording to a model describing the relationship be-
tween source sentence length and target sentence
length. In particular, this relationship is modeled
using a Poisson distribution that has as its mean
the length ratio observed in the dataset to align.4
The second aligner (MRE) is based on Moore?s
algorithm (Moore, 2002), which makes use of the
length-based aligner?s output to build a tentative
model 1. Moore?s algorithm takes the output from
this ?first phase? and builds a bilingual lexicon that
allows it to compute translation model scores. For
a given pair of sentences, the likelihood that they
are translations of each other is now computed
based not only on their lengths, but also on their
lexical similarity.
The third aligner (MRE+) is similar to the sec-
ond aligner, but uses a much stronger translation
model. The stronger translation model is simply
the translation system that has already been built
for that particular language pair and now helps
aligning new data. While this requires the avail-
ability of external resources, this setup closely re-
sembles the resources we have, given our parallel
training datasets. We note here that our evaluation
datasets have no overlap with the data used to train
the translation models used by MRE+.
4.2 Noisy Dataset Generation
For random deletions, we use six different dele-
tion rates (from 0.00 to 0.25, with 0.05 incre-
ments), both on the source side and the target side,
for a total of 35 test sets. For random combi-
nations, we use four different combination rates
(from 0.00 to 0.15, with 0.05 increments), both
4Note that we follow Moore (2002) in using a Poisson
distribution instead of the Gaussian of Brown et al
on the source side and the target side, for a to-
tal of 15 test sets. Note that we do not consider
the case when both deletion/combination rates are
0.00, since that mimics the clean-dataset scenario.
For the length-aligned scenario, we align each
source sentence with a randomly-selected sen-
tence from the target side that is closest in length
to that source sentence. (We take the target-to-
source length ratio into consideration, and multi-
ply the source length by that ratio before trying to
find the closest-length target sentence.) If several
target sentences have lengths that are equally close
to the desired length, we pick one at random.
We note here that if the source sentences are
processed sequentially, there will be a clustering
of overly long target sentences at the bottom of
the dataset, since such sentences are never chosen
based on length ? they are simply too long. There-
fore, we process the source sentences in random
order rather than sequentially, to avoid this clus-
tering of long sentences.
4.3 Performance Metrics
We report the following metrics for quantitatively
evaluating and describing the output of the sen-
tence aligner:
? Precision: of the sentence pairs produced
by the aligner, what percentage are sentence
pairs in the gold-standard dataset D?
? Recall: of the sentence pairs in the gold-
standard dataset D, what percentage are pro-
duced by the aligner?
? Alignment rate: what proportion of the sen-
tences in the input dataset D? were aligned
by the aligner? Due to the possibility that the
source and target sides of D? have different
sizes, there are two alignment rates, and we
report their average.5
Higher precision and higher recall are, by defi-
nition, indicators of better performance. This can-
not be said of the alignment rate. For instance,
consider the noisy deletion scenario of 3.1 above.
By mere construction of D?, there will be source
(resp. target) sentences that should not be aligned
to anything on the target (resp. source) side, since
we deliberately deleted the corresponding sen-
tence. In such cases, an alignment rate of 100%
5Of course, the dataset returned by the aligner always has
source and target sides of equal sizes.
488
Language Test Scenario LEN MRE MRE+
Pair
Clean (no noise) 100%, 82%, 82% 100%, 85%, 85% 100%, 99%, 99%
dels = delt = 0.05 100%, 46%, 44% 99%, 71%, 68% 100%, 96%, 91%
EN-ES combs = combt = 0.05 100%, 39%, 38% 99%, 66%, 64% 100%, 92%, 89%
Randomized 0%, 0%, 1% 0%, 0%, 4% 34%, 1%, 4%
Length-aligned 0%, 0%, 82% 0%, 0%, 15% 0%, 0%, 7%
Clean (no noise) 100%, 55%, 55% 100%, 60%, 60% 100%, 89%, 89%
dels = delt = 0.05 99%, 27%, 26% 99%, 44%, 42% 100%, 82%, 78%
EN-AR combs = combt = 0.05 99%, 22%, 21% 99%, 41%, 39% 99%, 77%, 74%
Randomized N/A, 0%, 0% 17%, <1%, <1% 26%, <1%, 1%
Length-aligned 0%, 0%, 59% 0%, 0%, 9% 5%, <1%, 2%
Clean (no noise) 100%, 66%, 66% 100%, 72%, 72% 100%, 97%, 97%
dels = delt = 0.05 100%, 40%, 39% 99%, 56%, 55% 100%, 92%, 88%
EN-CH combs = combt = 0.05 99%, 35%, 34% 99%, 52%, 50% 99%, 87%, 82%
Randomized 0%, 0%, <1% 0%, 0%, <1% 29%, <1%, 2%
Length-aligned 0%, 0%, 62% 0%, 0%, 13% 2%, <1%, 5%
Clean (no noise) 100%, 68%, 68% 100%, 72%, 72% 100%, 95%, 95%
Average dels = delt = 0.05 100%, 38%, 36% 99%, 57%, 55% 100%, 90%, 86%
(over the combs = combt = 0.05 99%, 32%, 31% 99%, 53%, 51% 99%, 85%, 82%
3 LP?s) Randomized 0%, 0%, <1% 6%, <1%, 2% 30%, 1%, 2%
Length-aligned 0%, 0%, 68% 0%, 0%, 12% 2%, <1%, 5%
Table 1: Results of the comparative experiment of the three aligners. For brevity, we report the results
for only five scenarios (per language pair and aligner) out of the more than fifty scenarios we propose.
Each cell contains three percentages: precision, recall, and alignment rate. The N/A precision value for
LEN in the EN-AR randomized scenario indicates the aligner produced no output.
for example (i.e. all input sentences were aligned
to some other sentence) is indicative of pervasive
alignment rather than good performance.6
Hence, alignment rate is not a performance
measure in the conventional sense, as it is not an
objective to be maximized or minimized. Still, it is
a useful descriptor that sheds light on the aligner?s
behavior, as we see in the next subsection.
4.4 Results
We carried out experiments covering three lan-
guage pairs: English-Spanish, English-Arabic,
and English-Chinese. The comparative experi-
ment is quite telling, and the results (Table 1) point
to consistent and noticeable differences between
the three examined aligners. While all aligners
have very high alignment precision rates in non-
adversary scenarios, always exceeding 99%, the
difference is in how well they recover sentence
pairs that should be aligned to each other, illus-
6Even an oracle aligner with perfect precision and recall
will almost surely have an alignment rate less than 100% (or
even 90%) when D? is constructed using high deletion rates.
trated by significant differences in recall rates.
The clearest trend is that the length-based al-
gorithm (LEN) performs worse than Moore?s al-
gorithm (MRE), which in turn benefits quite a bit
when it?s aided by an external strong translation
model (MRE+). It is worth pointing out that the gap
between MRE and MRE+ is typically larger than the
gap between LEN and MRE, suggesting the impor-
tant of external bilingual resources to aid the sen-
tence aligner.
The results of the adversary scenarios (random-
ized and length-aligned) are particularly interest-
ing. Looking at precision and recall alone, it might
seem that there is not much to separate the three
algorithms. For example, they all have 0% pre-
cision and 0% recall in the length-aligned EN-ES
scenario (fifth row of Table 1). However, looking
at the alignment rate, we find that LEN was prone
to over-aligning the data, having an (unnecessarily
very high) alignment rate of 82%. On the other
hand, MRE and MRE+, have much lower alignment
rates of 15% and 7%, respectively. This means
that they would introduce only a fraction of the
489
bad data that LEN would, which is a great advan-
tage for MRE and especially MRE+.
5 Applications of the Evaluation
Framework
In the previous section, we utilized our framework
to perform a comparison between three different
aligners, by evaluating them under various noisy-
data circumstances. In this section, we use our
framework in two more applications relevant to
sentence alignment and machine translation.
5.1 Fine-tuning Aligner Parameters
We explore using the evaluation setup to fine-tune
the parameters of the MRE+ algorithm. Lacking
a principled way to evaluate the aligner?s output,
it was not possible to fine-tune the aligner?s var-
ious parameters. Now, equipped with our eval-
uation framework, it is possible to quantitatively
determine the effect of changing the value of any
parameter, and pick the best value. This is prefer-
able to accepting whatever default parameters are
in already place, which are more than likely suit-
able for a specific domain, dataset, or low-to-
nonexistent noise.
5.1.1 Experimental Design
We fine-tune the parameters of the MRE+ algo-
rithm by optimizing its performance on a tuning
dataset generated using the noisy deletion setup,
and then measure its performance on a different
evaluation set that was also generated using the
noisy deletion setup. We investigate two cases,
one with dels = delt = 0.05, and one with
dels = delt = 0.20, to examine the benefit of
fine-tuning both under a relatively low noise level
and under a relatively high noise level.
We optimize the performance of the MRE+ algo-
rithm along three dimensions:
? Prior probabilities (PRIOR). As explained
in section 2, sentence alignment is essentially
a segmentation of the source and target sides
of the parallel dataset. In addition to relying
on length similarity and lexical correspon-
dence, the MRE+ aligner also relies on a set of
prior probabilities for each insert/delete/align
action it could take. By default, the probabil-
ity assigned to deletion and insertion was set
at 0.02. It is reasonable to assume that this
might be too low, especially for highly-noisy
input data, and so this is the first dimension
that we optimize.
? Search beam size (SIZE). The algorithm also
pays attention to the location of a candidate
sentence pair. While positional similarity
does not play a direct role in computing the
alignment probability, the aligner does prune
the search space based on location. For ex-
ample, when considering a sentence half-way
through the source side, only sentences that
are close to the half-way point in the target
side will be considered. How far the aligner
is willing to deviate from the diagonal7 is a
tunable parameter, making it our second di-
mension.
? Alignment threshold (THRESHOLD). The
aligner assigns a probability to each sentence
pair it considers for alignment, reflecting its
confidence that the sentence pair should be
aligned. By default, the aligner eliminates
any sentence pair that fails to meet a thresh-
old of 0.99. This alignment threshold is the
third dimension we optimize, as it should be
lowered or increased to reflect our confidence
in the translation model and/or the variability
of the length-correspondence model.
5.1.2 Experimental Results
The results in Tables 2 and 3 show the benefit of
optimizing the aligner?s parameters. It is bene-
ficial to optimize the prior probabilities and the
alignment threshold, as indicated by higher recall
rates compared to the default values. On the other
hand, the tuning of the search beam size had mini-
mal impact. This indicates that the mistakes made
by the sentence aligner are usually model errors
rather than search error.
The effect of optimizing the prior probabilities
is more pronounced in the high-noise scenario (Ta-
ble 3), where it proves to provide the most gain
over the baseline. Contrast this with the low-noise
scenario (Table 2), where optimizing the align-
ment threshold is at least equally important, if not
more so. This is to be expected, since the de-
fault prior of 0.02 in the high-noise scenario sig-
nificantly underestimates the amount of deletion
that has actually taken place, making the prior the
most important parameter to optimize.
7If we were to create a grid of alignment probabilities, this
pruning of the search space means that grid cells far off the
diagonal of this grid are never considered.
490
Tuned EN-ES EN-AR EN-CH
parameter(s)
None 95.7% 82.4% 92.0%
PRIOR 96.2% 85.6% 93.5%
SIZE 95.8% 82.8% 92.0%
THRESHOLD 96.8% 86.7% 92.9%
All 97.1% 87.5% 93.7%
Table 2: Results of the MRE+ fine-tuning experi-
ment for the 0.05 deletion rate scenario. For clar-
ity, we show only recall rates ? all precision rates
are 99% or higher.
Tuned EN-ES EN-AR EN-CH
parameter(s)
None 87.8% 68.1% 81.9%
PRIOR 92.7% 81.5% 88.4%
SIZE 88.0% 68.8% 82.3%
THRESHOLD 89.3% 70.4% 84.3%
All 93.0% 82.8% 90.6%
Table 3: Results of the MRE+ fine-tuning experi-
ment for the 0.20 deletion rate scenario. For clar-
ity, we show only recall rates ? all precision rates
are 98% or higher.
It is worth pointing out the work of Yu et al
(2012), who perform a comparative study of sen-
tence aligners, and show that Moore?s algorithm
does not perform as well as other aligners on a
noisy dataset. As they provide no details regarding
the values of the various parameters of Moore?s
algorithm, one can assume that they used default
values and performed no tuning. Of course, such
tuning would not have been easy to perform, given
the lack of a tuning dataset. This is exactly why
we propose our evaluation framework, so that fu-
ture researchers would not have to guess parame-
ter values or accept default values if they believe
that would lead to suboptimal performance. Given
the results of our experiments, it is conceivable
that the performance of Moore?s algorithm in Yu
et al?s work (and other algorithms they examined
as well) might have been improved had their pa-
rameters been optimized.
5.2 Using Sentence Alignment to Filter
Training Data
Much of our training data comes from noisy
sources, both online and otherwise. Due to the vast
amount of data, it is not possible to go through it to
discard noisy sentence pairs. Now, equipped with
a better understanding of our sentence aligner and
its performance, we use it to trim down our train-
ing data by eliminating sentence pairs to which the
aligner does not assign a high weight.
5.2.1 Experimental Design
We provide our current training data as input to the
sentence aligner, and treat the output of the aligner
as a filtered version of our data, since sentences
that are discarded (not aligned) by the aligner tend
to be noisy data. To evaluate the effectiveness
of this process, we compare models trained with
pre-filtered data vs. ones trained with the filtered
data. We examine how the filtering affects the
data and model size, since trimming those down
would speed up training and translation. This is
especially relevant for us given the large number
of language pairs for which we train models. To
ensure the translation quality doesn?t degrade, we
measure the effect on translation quality for two
in-house evaluation datasets.
We consider three scenarios:
? No filtering. As a baseline, we use our train-
ing data as-is to train the MT system, without
any filtering.
? Uniform filtering. We provide our training
data as input to the sentence aligner, and use
the aligner?s output as the training data to
train the MT system. (We refer to this as
?uniform? filtering in contrast to the next sce-
nario.)
? Filtering ?web? datasets. Here, we apply
sentence alignment filtering only to certain
hand-picked datasets that we believe to con-
tain a relatively high level of noise. The
datasets are not picked by inspecting their
content, but simply by deciding that any
dataset that came from online sources (aka
?web? data) should undergo filtering.
5.2.2 Experimental Results
We performed our filtering experiments on two
systems, Arabic-English and Urdu-English, with
the results displayed in Tables 4 and 5, respec-
tively. In all cases but one, the BLEU score went
up or down by less than a quarter of a point, indi-
cating general stability in performance quality.
This line of experiments is still in progress. We
plan to carry out another set of experiments where
491
Scenario Data Model Test1 Test2
Size Size BLEU BLEU
No filtering 100% 100% 31.44 30.57
All filtered 94.8% 96.7% 31.29 30.34
Web only 96.6% 96.0% 31.54 30.52
Table 4: Results of the data filtering experiments
for the Arabic-English system.
Scenario Data Model Test1 Test2
Size Size BLEU BLEU
No filtering 100% 100% 38.03 13.32
All filtered 81.6% 85.9% 38.19 13.13
Web only 99.1% 99.1% 37.80 12.78
Table 5: Results of the data filtering experiments
for the Urdu-English system.
the prior deletion probability is customized for
each portion of our training data, based on our
belief of how noisy that portion of the dataset is.
We are also expanding the experiments to include
more language pairs.
6 Related Work
Singh and Husain (2005) evaluate several sentence
alignment algorithms. Their work does have a hint
of proposing a fuller evaluation framework, in that
they have one test scenario where noise is added to
their test set (in the form of adding sentences from
another, unrelated dataset). Another major differ-
ence from our work is that they rely on manual
evaluation of the output, as is the case for much of
prior work.
Moore does point out that the error rates ob-
tained by his algorithm are very low partly because
the data being aligned is highly parallel, there-
fore making it ?fairly easy data to align? (Moore
(2002), p. 142). He therefore presents one ad-
ditional experiment where a single block of sen-
tences is deleted from one side of the input to
mimic a noisy condition. While this is similar in
spirit to our noisy deletions scenario, it introduces
only a very small amount of noise in practice. This
is because the deleted sentences are all sequential
rather than being at different positions in the cor-
pus, are all on one side of the corpus, and since
the deletion rate was very low (varied up to only
3.0%). Case in point, the resulting dataset was still
very easy to align, with error rates that remained
below 2.0% even for the baseline aligner.
Yu et al (2012) use the BAF dataset (Simard,
2006) as an evaluation dataset, since it is known
to contain a relatively high degree of 0-1 and 1-0
beads (what they call ?null links?), and use that
dataset specifically to evaluate an alignment al-
gorithm customized to handle noisy data. Simi-
larly, Rosen (2005) evaluates several aligners us-
ing three datasets, one of which is characterized
as being more noisy than the others.
Abdul-Rauf et al (2012) compare several algo-
rithms to each other, across several datasets, in-
cluding the noisy BAF dataset. However, they do
not propose a full framework for evaluating sen-
tence alignment itself, and instead emphasize the
differences in performance of MT systems trained
on the aligned data.
There is a good amount of prior work deal-
ing with filtering noisy data from parallel datasets.
Taghipour et al (2010) propose a discriminative
framework to filter noisy sentence pairs from par-
allel data, and apply it to a Farsi-English dataset.
Denkowski et al (2012) briefly describe a filter-
ing method to clean up training data for a French-
English system submitted to WMT 2010, relying
on deviations from typical values for certain sta-
tistical measures to identify noisy sentence pairs.
7 Conclusion
In this paper, we proposed a new evaluation frame-
work for sentence aligners, which is specifically
designed with noisy-data conditions in mind. Our
approach is unique in that it requires absolutely
no manual labeling, and relies on parallel datasets
that are already in existence. We provide sev-
eral methods to deliberately introduce noise into a
dataset that is already perfectly-aligned, thus cre-
ating a whole host of evaluation test sets quickly
and at no cost.
Our framework allows us and other researchers
to easily compare and contrast several aligners to
each other. Furthermore, our framework can be
used to improve the performance of an aligner by
facilitating the fine-tuning of any or all of its hy-
perparameters.
References
Sadaf Abdul-Rauf, Mark Fishel, Patrik Lambert, San-
dra Noubours, and Rico Sennrich. 2012. Extrin-
sic evaluation of sentence alignment systems. In
Proceedings of LREC Workshop on Creating Cross-
492
language Resources for Disconnected Languages
and Styles, CREDISLAS, pages 6?10.
Fabienne Braune and Alexander Fraser. 2010. Im-
proved unsupervised sentence alignment for sym-
metrical and asymmetrical parallel corpora. In Pro-
ceedings of COLING: Poster Volume, pages 81?89.
Peter F. Brown, Jennifer C. Lai, and Robert L. Mercer.
1991. Aligning sentences in parallel corpora. In
Proceedings of ACL, pages 169?176.
Stanley F. Chen. 1993. Aligning sentences in bilingual
corpora using lexical information. In Proceedings of
ACL, pages 9?16.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The CMU-Avenue French-English transla-
tion system. In Proceedings of the NAACL Work-
shop on Statistical Machine Translation, pages 261?
266.
William A. Gale and Kenneth W. Church. 1991. A
program for aligning sentences in bilingual corpora.
In Proceedings of ACL, pages 177?184.
Martin Kay and Martin Ro?scheisen. 1993. Text-
translation alignment. Computational Linguistics,
19(1):121?142.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit, pages 79?86.
Philippe Langlais, Michel Simard, and Jean Ve?ronis.
1998. Methods and practical issues in evaluating
alignment techniques. In ACL/COLING, pages 711?
717.
Xiaoyi Ma. 2006. Champollion: A robust parallel text
sentence aligner. In Proceedings of LREC, pages
489?492.
I. Dan Melamed. 1997. A portable algorithm for map-
ping bitext correspondence. In Proceedings of ACL,
pages 305?312.
Arul Menezes and Stephen D. Richardson. 2001. A
best-first alignment algorithm for automatic extrac-
tion of transfer mappings from bilingual corpora. In
Proceedings of the ACL Workshop on Data-Driven
Methods in Machine Translation, pages 39?46.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Stephen D.
Richardson, editor, AMTA 2002: From Research to
Real Users, pages 135?144. Springer Berlin Heidel-
berg.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Philip Resnik and Noah A. Smith. 2003. The web
as a parallel corpus. Computational Linguistics,
29(3):349?380.
Alexandr Rosen. 2005. In search of the best method
for sentence alignment in parallel texts. In Proceed-
ings of SLOVKO.
Andre? Santos. 2011. A survey on parallel corpora
alignment. In Proceedings of MI-Star, pages 117?
128.
Michel Simard and Pierre Plamondon. 1998. Bilin-
gual sentence alignment: Balancing robustness and
accuracy. Machine Translation, 13:59?80.
Michel Simard, George F. Foster, and Pierre Isabelle.
1993. Using cognates to align sentences in bilin-
gual corpora. In Proceedings of the Conference of
the Centre for Advanced Studies on Collaborative
Research: Distributed Computing - Volume 2, pages
1071?1082.
Michel Simard. 2006. The BAF: A corpus of English-
French bitext. In Proceedings of LREC, pages 489?
494.
Anil Kumar Singh and Samar Husain. 2005. Com-
parison, selection and use of sentence alignment al-
gorithms for new language pairs. In Proceedings of
the ACL Workshop on Building and Using Parallel
Texts, pages 99?106.
Jason R. Smith, Chris Quirk, and Kristina Toutanova.
2010. Extracting parallel sentences from compara-
ble corpora using document level alignment. In Pro-
ceedings of NAACL, pages 403?411.
Kaveh Taghipour, Nasim Afhami, Shahram Khadivi,
and Saeed Shiry. 2010. A discriminative approach
to filter out noisy sentence pairs from bilingual cor-
pora. In Proceedings of International Symposium on
Telecommunications, pages 537?541.
Jo?rg Tiedemann. 2007. Improved sentence alignment
for movie subtitles. In Proceedings of Recent Ad-
vances in Natural Language Processing.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning japanese-english news articles
and sentences. In Proceedings of ACL, pages 72?79.
Jean Ve?ronis and Philippe Langlais. 2000. Evaluation
of parallel text alignment systems: The ARCADE
project. In Jean Ve?ronis, editor, Parallel Text Pro-
cessing: Alignment and Use of Translation Corpora,
pages 369?388. Kluwer Academic Publishers.
Qian Yu, Aure?lien Max, and Franc?ois Yvon. 2012.
Revisiting sentence alignment algorithms for align-
ment visualization and evaluation. In Proceedings
of the LREC Workshop on Building and Using Com-
parable Corpora, pages 10?16.
Bing Zhao and Stephan Vogel. 2002. Adaptive paral-
lel sentences mining from web bilingual news col-
lection. In IEEE International Conference on Data
Mining, pages 745?748.
493
