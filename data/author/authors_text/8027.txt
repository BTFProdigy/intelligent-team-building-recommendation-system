Tagging Gene and Protein Names in Full Text Articles 
Lorraine Tanabe and W. John Wilbur 
National Center for Biotechnology Information 
NLM, NIH 
Bethesda, Maryland 20894 
 
 
Abstract 
Current information extraction efforts 
in the biomedical domain tend to 
focus on finding entities and facts in 
structured databases or MEDLINE? 
abstracts.  We apply a gene and 
protein name tagger trained on 
Medline abstracts (ABGene) to a 
randomly selected set of full text 
journal articles in the biomedical 
domain.  We show the effect of 
adaptations made in response to the 
greater heterogeneity of full text. 
1 Introduction 
The application of large-scale genomics and 
proteomics technologies towards a wide variety 
of biological questions has resulted in a 
continuous stream of information regarding 
thousands of genes and gene products into the 
Medline database of biomedical abstracts.  This 
repository has been recognized as a rich 
knowledge source for biological information 
retrieval, information extraction and text mining.  
However, abbreviated scientific abstracts cannot 
contain the same volume of information as the 
full text articles that they represent.  It was 
recently shown that only 30% of protein 
interactions contained in the Dictionary of 
Interacting Proteins (DIP) (Xenarios et al, 
2000) could be found in Medline sentences 
containing DIP protein pairs (Blaschke et al, 
2000).  This finding suggests that current 
information extraction efforts being applied to 
biomedical abstracts should be extended to full 
text databases.   
The basic task of identifying gene and 
protein names is a necessary first step towards 
making full use of the information encoded in 
biomedical text.  This remains a challenging 
task due to the irregularities and ambiguities in 
gene and protein nomenclature. The 
irregularities are mainly the result of a lack of 
naming conventions, as well as the widespread 
practice of using many synonyms for one gene 
or protein.  A glance at the Nomenclature 
section of the Nature Genetics website 
(http://www.nature.com/ng/web_specials/nomen
/) shows the scope of the problem, as well as 
ideas for addressing it.  The nomenclature 
guidelines implore authors to consult relevant 
nomenclature committees before announcing 
new genes, and to provide synonyms for genes 
in abstracts.  Additional rules specify that: 
4. Gene symbols are always italicised 
and never contain hyphens, greek 
letters, roman numerals, subscripts or 
superscripts. 
5. All letters in human genes are upper-
case?all letters in mouse genes are 
lower-case? 
 Unfortunately, we are currently at a stage 
where these types of rules are not consistently 
applied to most biomedical abstracts, let alne to 
full text documents.  Until the biomedical 
community adheres uniformly to nomenclature 
guidelines, ambiguities regarding gene/protein 
names will continue to be an obstacle for natural 
language processing of biomedical text.  These 
ambiguities become apparent at the 
morphological, syntactic and semantic levels.  
For example,  caco-2 refers to a cell line, but 
pai-1 is a gene name.  Gene and protein names 
can contain verbs and other parts of speech that 
are hard to distinguish from the surrounding 
text, as in deleted in azoospermia-like, son of 
sevenless, ran, man, young arrest and never in 
mitosis.  Genes can be transfected into cells, or 
combined with chemicals, resulting in 
ambiguous terms like CHO-A(3) and 
ca2+/calmodulin.  The semantic notion of a gene 
or protein is quite arbitrary ? is 
ACTTGGAATGACC a gene name?  In addition 
to sequences, there are mutations, motifs, 
receptors, antibodies, hormones, channels, 
                                            Association for Computational Linguistics.
                             the Biomedical Domain, Philadelphia, July 2002, pp. 9-13.
                         Proceedings of the Workshop on Natural Language Processing in
chromosomal locations and disease loci to 
consider.  The domain-specific irregularities and 
ambiguities just described are superimposed 
upon the ambiguities in the natural language 
itself, so it is not surprising that the 
identification of gene and protein names in 
biomedical text remains a difficult and 
challenging task.  The methodologies applied to 
this fundamental problem include rule-based 
and/or pattern matching methods (Fukuda et al, 
1998) (Thomas et al, 2000) (Yoshida et al, 
2000) (Jenssen et al, 2001) (Ono et al, 2001) 
(Yu at al, 2002) (Bunescu et al, 2002), a 
modified BLAST algorithm (Krauthammer et 
al., 2000), Hidden Markov Models (HMMs) 
(Collier et al, 2000) (Proux et al, 1998), Naive 
Bayes and decision trees (Nobata et al, 1999), 
under specified parsing with knowledge sources 
(Rindflesch et al 2000), and context-free 
grammars (Gaizauskas, 2000). 
In this paper, we evaluate the application of a 
gene and protein name tagger trained on 
Medline abstracts (ABGene) (Tanabe and 
Wilbur, 2002) to a randomly selected set of 
1,000 PUBMEDCENTRAL? (PMC) articles.  
PMC is a digital archive of full text peer-
reviewed biomedical articles launched in 
February 2000 by the National Center for 
Biotechnology Information (NCBI) and the U.S. 
National Library of Medicine (NLM?) (Roberts 
et al, 2001). We present two adaptations made 
in response to the greater heterogeneity of full 
text, and evaluate how they affect the 
performance of ABGene on a test set of 2600 
full text sentences.  
2 Methods 
We first give an overview of ABGene?s 
method for extracting gene and protein names 
from biomedical citations, and then present 
some modifications to ABGene designed to 
improve its performance on full text articles.  
 
2.1 ABGene Overview 
We previously trained the Brill POS tagger 
(Brill, 1994) to recognize protein and gene 
names in biomedical text using a training set of 
7,000 Medline sentences. We updated the 
lexicon included in the Brill package (Brown 
Corpus plus Wall Street Journal corpus) with 
entries from the UMLS? SPECIALIST lexicon 
(McCray et al 1994, Humphreys et al 1998), 
and generated a list of bigrams and a word list 
from all of MEDLINE to customize the training 
for our purposes.  ABGene processing begins by 
using these automatically generated rules from 
the Brill tagger to extract single word gene and 
protein names from biomedical abstracts (see 
Table 1). 
 
    
 
 
 
 
   This is followed by extensive filtering for false 
positives and false negatives. A key step during 
the filtering stage is the extraction of multi-word 
gene and protein names that are prevalent in the 
literature but inaccessible to the Brill tagger. 
 During the false positive filtering step, the 
GENE tag is removed from a word if it matches 
a term from a list of 1,505 precompiled general 
biological terms (acids, antagonist, assembly, 
antigen, etc.), 39 amino acid names, 233 
restriction enzymes, 593 cell lines, 63,698 
organism names from the NCBI Taxonomy 
Lexical Rule Description 
NNP gene fgoodleft 
GENE 
Change the tag of a word 
from NNP to GENE if the 
word gene can appear to the 
right 
-A hassuf 2 GENE 
Change the tag of a word 
from anything to GENE if it 
contains the suffix -A 
c- haspref 2 GENE 
Change the tag of a word 
from anything to GENE if it 
contains the prefix c- 
GENE cell 
fgoodright NNP 
Change the tag of a word 
from GENE to NNP if the 
word cell can appear to the 
left 
Contextual Rule Description 
NNP GENE 
PREV1OR2WD 
genes 
Change the tag of a word 
from NNP to GENE if one 
of the two preceding words 
is genes 
NNP GENE 
NEXTBIGRAM ( 
GENE 
Change the tag of a word 
from NNP to GENE if the 
two following words are 
tagged ( and GENE 
CD GENE 
SURROUNDTAG 
CC ) 
Change the tag of a word 
from CD to GENE if the 
preceding word is tagged 
CC and the following word 
is tagged ) 
VBG JJ NEXTTAG 
GENE 
Change the tag of a word 
from VBG to JJ if the next 
word is tagged GENE 
Table 1.  Examples of lexical and contextual rules learned by 
the Brill tagger.  NNP = proper noun, CD = cardinal number, 
CC = coordinating conjunction, JJ = adjective, VBG = verb, 
gerund/present participle 
Database (Wheeler et al 2000) or 4,357 non-
biological terms. Non-biological terms were 
obtained by comparing word frequencies in 
MEDLINE versus the Wall Street Journal (WSJ) 
using the following expression, where p is the 
probability of occurrence: 
log(p(word occurs in MEDLINE)/ p(word occurs in WSJ) )< 1 
 Additional false positives are found by regular 
expressions including numbers followed by 
measurements (25 mg/ml) and common drug 
suffixes (-ole, -ane, -ate, -ide, -ine, -ite, -ol, -ose, 
cooh). 
 The false negative filter recovers a single 
word name if it: 1) matches a list of 34,555 
single word names and 7611 compound word 
names compiled from LocusLink (Pruitt & 
Maglott 2001) and the Gene Ontology 
Consortium (2000) (Wain et al, 2002) and 
contains a good context word before or after the 
name, or 2) contains a low frequency trigram 
and a good context word before or after the 
name. The context words were automatically 
generated by a probabilistic algorithm, using the 
LocusLink/Gene Ontology set and a large 
collection of texts in which these gene names 
occur. We computed a log odds score or 
Bayesian weight for all non-gene name words 
indicating their propensity to predict an adjacent 
gene name in the texts.  
Compound word names are recovered using 
terms that occur frequently in known gene 
names.  Recombination of these terms produce 
compound words that also tend to be 
gene/protein names.  These terms include the 
digits 1-9, the letters a-z, the roman numerals, 
the Greek letters, functional descriptors 
(adhesion), organism identifiers (hamster), 
activity descriptors (promoting), placement 
indicators (early), and generic descriptors 
(light). In addition to the 415 exact terms, we 
added regular expressions that allow for partial 
matches or special patterns such as words 
without vowels, words with numbers and letters, 
words in capital letters, and common prefixes 
and suffixes (-gene, -like, -ase).   
Finally, Bayesian learning (Langley 1996, 
Mitchell 1997, Wilbur 2000) is applied to rank 
documents by similarity to documents with 
known gene/protein names. Documents below a 
certain threshold are considered to have no 
gene/protein names in them. 
 
2.2 Modifications for Full Text Articles 
The full text PMC articles are longer than 
abstracts, and contain extraneous information 
like grant numbers and laboratory reagents, 
along with figures and tables.  An attempt to 
take windows of varying sizes of the full text in 
order to rank the windows by similarity to 
abstracts with known gene names was 
unsuccessful.  High scoring windows often hid 
false positives, and low scoring windows could 
contain gene and protein name contexts 
infrequently encountered in Medline abstracts.  
However, we determined that the classifier 
could be used on the sentence level for full text 
articles, and show the effect of an assumption 
that sentences below a zero threshold do not 
contain gene/protein names. 
We tried to increase the performance of 
ABGene on the PMC articles by adding a final  
processing step.  We ran ABGene on 2.16 
million Medline abstracts similar to documents 
with known gene names, and extracted 2.42 
million unique gene/protein names.  We counted 
the number of times each unique name was 
given the GENE tag by ABGene in the 2.16 
million abstracts, and  then extracted three 
groups of putative gene/protein names from this 
large set, with count thresholds at 10 (134,809 
names), 100 (13,865 names) and 1000 (1136 
names).   
During the final stage of processing, terms in 
sentences with scores greater than 2 are checked 
against these lists of supposed gene/protein 
names.  We show the effect of tagging terms 
with counts of at least 10, 100 and 1000 in the 
putative gene/protein list.   
3 Experiment and Results 
We evaluated the performance of ABGene 
on 2600 PMC sentences from 13 score levels 
ranging from ?8 to 60+.  No attempt was made 
to narrow the set using query terms.  The 
sentences were selected as follows:  half of the 
test set consists of the first 100 sentences from 
each score level, and the other half consists of 
100 sentences selected at random from each 
score level.  Precision and recall results are 
shown for each individual score range in Table 
2, and cumulative results are shown in Table 3.  
The number of words tested varies for each 
score level because longer sentences tend to 
have higher scores.  Also, sentences with scores 
near zero tend to be table or figure entries, with 
only a few words each.   
 
 
Table 2.  Precision and recall for each score range.  TP+FN = number of gene names; P = precision without final step, R = recall without 
final step, P 1000 = precision with 1000 count threshold at final step, R 1000 = recall with 1000 count threshold at final step, P 100 = 
precision with 100 count threshold at final step, R 100 = recall with 100 count threshold at final step, P 10 = precision with 10 count 
threshold at final step, R 10 = recall with 10 count threshold at final step. 
 
Table 3.  Cumulative precision and recall using the score as a lower threshold.
 
3.1 Problematic Areas in Full Text 
The false positive gene/protein names found in 
the PMC articles reveal new difficulties for the 
basic task of identifying gene and protein names 
in biomedical text.  For example, in abstracts, 
entities like restriction enzyme sites, laboratory 
protocol kits, primers, vectors, molecular 
biology supply companies and chemical 
reagents are usually scarce.  However, in the 
methods section of a full document, they appear 
regularly, adding to the morphological, syntactic 
and semantic ambiguities previously mentioned.  
Illustrative examples include bio-rad, centricon-
30 spin, xbai sites, mg2, geneamp and pgem3z.  
A significant source of false negatives consists 
of tables and figures from full text, which 
completely lack contextual cues and/or indicator 
words.  These problems can be addressed by 
eliminating processing of materials and methods 
sections, tables and figures.  Another significant 
source of false negatives is an artifact of the 
PMC format, for example, beta is translated to 
[beta], thus a name like beta1 integrin becomes 
[beta]1 integrin in PMC. This is easily 
addressed by removing the PMC formatting 
prior to processing, and has already been 
completed for future work on PMC articles. 
4 Conclusion 
We conclude that an information extraction 
system to tag gene and protein names in 
Medline abstracts (ABGene) can be applied to 
full text articles in the biomedical domain. We 
Score 
Range  
#words 
tested TP + FN P R P 1000 R 1000
P 
100 
R 
100 
P 
10 
R 
10 
60+ 13,442 1347 0.742 0.640 0.726 0.667 0.686 0.692 0.603 0.716 
30 to 60 7,953 530 0.672 0.638 0.673 0.667 0.649 0.699 0.590 0.765 
20 to 30 6,392 401 0.757 0.646 0.751 0.671 0.708 0.748 0.624 0.801 
15 to 20 5,508 302 0.722 0.593 0.719 0.619 0.672 0.659 0.561 0.735 
10 to 15 5,100 269 0.755 0.688 0.743 0.710 0.681 0.747 0.579 0.792 
8 to 10 4,618 226 0.707 0.588 0.689 0.637 0.615 0.686 0.512 0.770 
6 to 8 4,327 170 0.703 0.571 0.692 0.594 0.641 0.641 0.479 0.724 
4 to 6 4,054 122 0.571 0.590 0.562 0.631 0.500 0.648 0.392 0.713 
2 to 4 3,667 59 0.541 0.559 0.508 0.559 0.404 0.610 0.270 0.644 
0 to 2 1,551 9 0.200 0.444 0.200 0.444 0.200 0.444 0.200 0.444 
-2 to 0 4,595 0 no tp no tp no tp no tp no tp no tp no tp no tp 
-4 to -2 5,299 1 0.040 1.000 0.040 1.000 0.040 1.000 0.040 1.000 
-8 to -4 5,495 0 no tp no tp no tp no tp no tp no tp no tp no tp 
SCORE P R P  1000 
R  
1000 
P  
100 
R  
100 
P  
10 
R  
10 
60 0.742 0.251 0.726 0.261 0.686 0.273 0.603 0.283 
30 0.721 0.349 0.710 0.364 0.675 0.381 0.599 0.402 
20 0.727 0.424 0.717 0.443 0.681 0.468 0.604 0.496 
15 0.727 0.476 0.717 0.497 0.680 0.526 0.598 0.560 
10 0.729 0.530 0.720 0.553 0.680 0.584 0.596 0.622 
8 0.728 0.569 0.718 0.595 0.675 0.629 0.589 0.673 
6 0.727 0.597 0.716 0.624 0.673 0.661 0.582 0.709 
4 0.720 0.618 0.710 0.646 0.665 0.684 0.573 0.734 
2 0.716 0.628 0.706 0.656 0.659 0.695 0.563 0.745 
0 0.713 0.629 0.702 0.657 0.656 0.696 0.562 0.746 
have shown how modifications to the processing 
(applying a sentence score threshold, and using 
a large pool of putative gene/protein names) can 
affect the system?s performance.  We are 
currently exploring methods to filter the 2.16 
million putative gene/protein names extracted 
from Medline using our system.  The resulting 
set of gene/protein names, a significant addition 
to the 42K names available from the Gene 
Ontology Consortium and LocusLink, will be 
used to improve the performance of text 
processing on full text articles in the biomedical 
domain. 
 
References 
Blaschke, C. and Valencia, A.  (2001)  Can bibliographic pointers 
for known biological data be found automatically?  Protein 
interactions as a case study.  Comparative and Functional 
Genomics, 2, 196-206. 
 
Brill, Eric. (1994) Some advances in transformation-based   part of 
speech tagging. In Proceedings of the National Conference on 
Artificial Intelligence.  AAAI Press, pp.  722-727. 
 
Bunescu, R., Ge, R., Mooney, R.J., Marcotte, E., and Ramani, 
A.K.  (2002) Extracting gene and protein names from biomedical 
abstracts.  http://www.cs.utexas.edu/users/ml/publication/ie.html. 
 
Collier, N., Nobata, C., and Tsujii, J. (2000) Extracting the names 
of genes and gene products with a hidden markov model. In 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING?2000), pp. 201-207. 
 
Fukuda, K., Tsunoda. T., Tamura, A. and Takagi. T. (1998) 
Toward information extraction: identifying protein names from 
biological papers. In Proceedings of the Pacific Symposium on 
Biocomputing (PSB98), pp. 705-716. 
 
The Gene Ontology Consortium. (2000) Gene ontology:  tool for 
the unification of biology. Nat. Genet., 25, 25-29. 
 
Humphreys K., Demetriou G., and Gaizauskas, R. (2000) Two 
applications of information extraction to biological science journal 
articles: enzyme interactions and protein structures. In 
Proceedings of the Pacific Symposium on Biocomputing 
(PSB2000) ,  pp. 502-513. 
  
Jenssen, T., Laegreid, A., Kormorowski, J., and Hovig, E.  (2001) 
A literature network of human genes for high-throughput analysis 
of gene expression.  Nat Genet., 28, 21-28. 
 
Krauthammer, M., Rzhetsky, A., Morozov, P., and Friedman, C.  
(2000) Using BLAST for identifying gene and protein names in 
journal articles.  Gene, 259, 245-252. 
 
Langley, P. (1996) Elements of Machine Learning. Morgan 
Kaufmann Publishers, Inc., San Francisco. 
 
McCray, A.T., Srinivasan, S. and Browne, A. C. Lexical methods 
for managing variation in biomedical terminologies.  In SCAMC 
?94, pp. 235-239. 
 
Mitchell, T. M. (1997) Machine Learning. WCB/McGraw-Hill, 
Boston. 
 
Nobata, C., Collier, N., and Tsujii, J. (1999) Automatic term 
identification and classification in biology texts.  In  Proceedings 
of the Natural Language Pacific Rim Symposium, pp. 369-374. 
 
Ono, T., Hishigaki, H., Tanigami, A., and Takagi, T.  (2001) 
Automated extraction of information on protein-protein 
interactions from the biological literature.  Bioinformatics, 17, 
155-161. 
 
Proux, D., Rechenmann, F., Julliard, L., Pillet, V., and Jacq, B. 
(1998) Detecting gene symbols and names in biological texts: a 
first step toward pertinent information extraction. In Proceedings 
of the Ninth Workshop on Genome Informatics, pp. 72-80. 
 
Pruitt, K.D. and Maglott, D.R.  (2001)  RefSeq and LocusLink:  
NCBI gene-centered resources.  Nucleic Acids Res., 29, 137-140. 
 
Rindflesch, T. C., Tanabe, L., Weinstein, J. W., and Hunter, L. 
(2000)  EDGAR:  extraction of drugs, genes and relations from the 
biomedical literature.  In Proceedings of the Pacific Symposium on 
Biocomputing (PSB2000), pp. 514-525. 
 
Roberts, R.J., Varmus, H.E., and Ashburner, M.  (2001)  
Information access:  building a Genbank of the published 
literature.  Science, 291, 2318-2319. 
 
Tanabe, L., and Wilbur, W.J.  (2002)  Tagging gene and protein 
names in biomedical text.  Bioinformatics, in press. 
 
Thomas, J., Milward, D., Ouzounis, C., Pulman, S., and Carroh, 
M. (2000) Automatic extraction of protein interactions from 
scientific abstracts.  In Proceedings of the Pacific Symposium on 
Biocomputing (PSB2000), pp. 541-552. 
 
Wain, H. M., Lush, M., Ducluzeau, F. , and Povey, S.  (2002) 
Genew:  the human gene nomenclature database.  Nucleic Acids 
Res., 30, 169-171. 
 
Wheeler, D.L., Chappey, C., Lash, A.E., Leipe, D.D., Madden, 
T.L., Schuler, G.D., Tatusova, T.A., and Rapp, B.A.  (2000)  
Database resources of the National Center for Biotechnology 
Information.  Nucleic Acids Res., 28, 10-14. 
 
Wilbur, W. J. (2000) Boosting naive bayesian learning on a large 
subset of MEDLINE. In American Medical Informatics 2000 
Annual Symposium, Los Angeles, CA, pp. 918-922. 
 
Xenarios, I., Rice, D.W., Salwinski, L., Baron, M.K., Marcotte, 
E.M., and Eisenberg, D.  (2000)  DIP:  the database of interacting 
proteins.  Nucleic Acids Res., 28, 289-291. 
 
Yoshida, M., Fukuda, K., and Takagi, T.  (2000)  PNAD-CSS:  a 
workbench for constructing a protein name abbreviation 
dictionary.  Bioinformatics, 16, 169-175. 
 
Yu, H., Hripcsak, G., and Friedman, C.  (2002)  Mapping 
abbreviations to full forms in biomedical articles.  J Am Med 
Inform Assoc., 9, 262-272. 
Proceedings of the ACL-ISMB Workshop on Linking Biological Literature, Ontologies and Databases: Mining
Biological Semantics, pages 32?37, Detroit, June 2005. c?2005 Association for Computational Linguistics
MedTag: A Collection of Biomedical Annotations
L.H. Smith
 
, L. Tanabe
 
, T. Rindflesch

, W.J. Wilbur
 
 
National Center for Biotechnology Information

Lister Hill National Center for Biomedical Communications
NLM, NIH, 8600 Rockville Pike, Bethesda, MD 20894

lsmith,tanabe,wilbur  @ncbi.nlm.nih.gov
rindesch@nlm.nih.gov
Abstract
We present a database of annotated
biomedical text corpora merged into a
portable data structure with uniform con-
ventions. MedTag combines three cor-
pora, MedPost, ABGene and GENETAG,
within a common relational database data
model. The GENETAG corpus has been
modified to reflect new definitions of
genes and proteins. The MedPost cor-
pus has been updated to include 1,000
additional sentences from the clinical
medicine domain. All data have been up-
dated with original MEDLINE text ex-
cerpts, PubMed identifiers, and tokeniza-
tion independence to facilitate data accu-
racy, consistency and usability.
The data are available in flat files along
with software to facilitate loading the
data into a relational SQL database
from ftp://ftp.ncbi.nlm.nih.gov/pub/lsmith
/MedTag/medtag.tar.gz.
1 Introduction
Annotated text corpora are used in modern computa-
tional linguistics research and development to fine-
tune computer algorithms for analyzing and classi-
fying texts and textual components. Two important
factors for useful text corpora are 1) accuracy and
consistency of the annotations, and 2) usability of
the data. We have recently updated the text corpora
we use in our research with respect to these criteria.
Three different corpora were combined. The AB-
Gene corpus consists of over 4 000 sentences anno-
tated with gene and protein named entities. It was
originally used to train the ABGene tagger to recog-
nize gene/protein names in MEDLINE records, and
recall and precision rates in the lower 70 percentile
range were achieved (Tanabe and Wilbur, 2002).
The MedPost corpus consists of 6 700 sentences,
and is annotated with parts of speech, and gerund
arguments. The MedPost tagger was trained on
3 700 of these sentences and achieved an accuracy
of 97.4% on the remaining sentences (Smith et. al.,
2004). The GENETAG corpus for gene/protein
named entity identification, consists of 20 000 sen-
tences and was used in the BioCreative 2004 Work-
shop (Yeh et. al., 2005; Tanabe et. al., 2005) (only
15 000 sentences are currently released, the remain-
ing 5 000 are being retained for possible use in a fu-
ture workshop). Training on a portion of the data,
the top performing systems achieved recall and pre-
cision rates in the lower 80 percentile range. Be-
cause of the scarcity of good annotated data in the
realm of biomedicine, and because good perfor-
mance has been obtained using this data, we feel
there is utility in presenting it to a wider audience.
All of the MedTag corpora are based on MED-
LINE abstracts. However, they were queried at dif-
ferent times, and used different (but similar) algo-
rithms to perform tokenization and sentence seg-
mentation. The original annotations were assigned
to tokens, or sequences of tokens, and extensively
reviewed by the authors at different times for the dif-
ferent research projects.
The main goals in combining and updating these
32
MedTag? Collection?
MedPost? ABGene? GENETAG?
6,700 sentences?
5,700 molecular biology?
1,000 clinical medicine?
60 part-of-speech tags?
97.4% accuracy?
1 annotator?
4,265 sentences?
Molecular Biology?
Single Genes/Proteins tagged?
1 annotator?
15,000 sentences?
50% Molecular Biology?
50% Other Biomedical?
All Genes/Proteins tagged?
3 annotators?
EXCERPT?
ID?
PubMed?  ID?
Corpus code?
Original? text?
Citation?
ANNOTATION?
ID?
Corpus code?
Character offsets?
Annotated text?
Data Model?
SQL Relational?
Database with?
Web Interface?
MEDLINE?
MEDLINE?
Figure 1: Component corpora, common data model
and main record types of the MedTag collection.
corpora into a single corpus were to
1. update the text for all corpora to that currently
found in MEDLINE, storing a correct citation
and the original, untokenized text for each ex-
cerpt
2. eliminate tokenization dependence
3. put all text and annotations into a common
database format
4. provide programs to convert from the new cor-
pus format to the data formats used in previous
research
2 Merging the Corpora
We describe what was done to merge the original
corpora, locating original sources and modifying the
text where needed. An overview is given in Figure
1. Some basic statistics are given in Table 1.
2.1 Identifying Source Data
The original data of the three corpora were assem-
bled and the text was used to search MEDLINE to
Corpus sentences tokens most frequent tag
GENETAG-05 15,000 418,246 insulin GENE(112)
MedPost 6,700 181,626 the DD(8,507)
AbGene 4,265 123,208 cyclin GENE(165)
MedPost
Adj Adv Aux Noun Punct Verb
14,648 4,553 56,262 60,732 21,806 23,625
GENETAG-05
GENE ALTGENE
24,562 19,216
ABGene
GENE ALTGENE
8,185 0
Table 1: MedTag Corpora. GENE = gene and pro-
tein names, ALTGENE = acceptable alternatives for
gene and protein names. MedPost tagset contains
60 parts of speech which have been binned here for
brevity.
find the closest match. An exact or near exact match
was found for all but a few excerpts. For only a
few excerpts, the MEDLINE record from which the
excerpt was originally taken had been removed or
modified and an alternative sentence was selected.
Thus, each excerpt in the database is taken from a
MEDLINE record as it existed at one time in 2004.
In order to preserve the reference for future work,
the PubMed ID and citation data were also retrieved
and stored with each excerpt. Each excerpt in the
current database roughly corresponds to a sentence,
although the procedure that extracted the sentence is
not specified.
2.2 Eliminating Tokenization Dependence
In the original ABGene and GENETAG corpora, the
gene and protein phrases were specified by the to-
kens contained in the phrase, and this introduced
a dependence on the tokenization algorithm. This
created problems for researchers who wished to use
a different tokenization. To overcome this depen-
dence, we developed an alternative way of specify-
33
ing phrases. Given the original text of an excerpt,
the number of non-whitespace characters to the start
of the phrase does not depend on the tokenization.
Therefore, all annotations now refer to the first and
last character of the phrase that is annotated. For
example the protein serum LH in the excerpt
There was no correlation between serum
LH and chronological or bone age in this
age group, which suggests that the corre-
lation found is not due to age-related par-
allel phenomena.
is specified as characters 28 to 34 (the first character
is 0).
2.3 Data Model
There are two main record types in the database,
EXCERPT and ANNOTATION. Each EXCERPT
record stores an identifier and the original corpus
code (abgene, medpost, and genetag) as well as sub-
corpus codes that were defined in the original cor-
pora. The original text, as it was obtained from
MEDLINE, is also stored, and a human readable ci-
tation to the article containing the reference.
Each ANNOTATION record contains a reference
to the excerpt (by identifier and corpus), the char-
acter offset of the first and last characters of the
phrase being annotated (only non-whitespace char-
acters are counted, starting with 0), and the corre-
sponding annotation. The annotated text is stored
for convenience, though it can be obtained from
the corresponding excerpt record by counting non-
whitespace characters.
The data is provided as an ASCII file in a standard
format that can be read and loaded into a relational
database. Each record in the file begins with a line
of the form  table name where table name is the
name of the table for that record. Following the table
name is a series of lines with the form eld: value
where eld is the name of the field and value is the
value stored in that field.
Scripts are provided for loading the data into a re-
lational database, such as mysql or ORACLE. SQL
queries can then be applied to retrieve excerpts and
annotations satisfying any desired condition. For
example, here is an SQL query to retrieve excerpts
from the MedPost corpus containing the token p53
and signaling or signalling
Figure 2: A screen capture of the annotator?s inter-
face and the GENETAG-05 annotations for a sen-
tence.
select text from excerpt
where text like ?%p53%?
and text rlike ?signa[l]*ing?;
2.4 Web Interface
A web-based corpus editor was used to enter and
review annotations. The code is being made avail-
able, as is, and requires that the data are loaded into a
mysql database that can be accessed by a web server.
The interface supports two annotation types: Med-
Post tags and arbitrary phrase annotations. MedPost
tags are selectable from a pull-down menu of pre-
programmed likely tags. For entering phrase anno-
tations, the user highlights the desired phrase, and
pressing the enter key computes and saves the first
and last character offsets. The user can then enter
the annotation code and an optional comment be-
fore saving it in the database. A screen dump of the
phrase annotations for a sentence in the genetag cor-
pus is shown in figure 2.
The data from the database was dumped to the flat
file format for this release. We have also included
some files to accommodate previous users of the
corpora. A perl program, alt eval.perl is in-
34
cluded that replaces the GENETAG evaluation pro-
gram using non-whitespace character numbers in-
stead of token numbers. Copies of the ABGene and
MedPost corpora, in the original formats, are also
included.
3 Updates of Component Corpora
3.1 MedPost Update
The MedPost corpus (Smith et. al., 2004) originally
contained 5 700 tokenized sentences. An additional
1 000 annotated sentences have been added for this
release. Each sentence in the MedPost corpus is
fully tokenized, that is, divided into non-overlapping
annotated portions, and each token is annotated with
one of 60 part of speech tags (see Table 1). Minor
corrections to the annotations have been made since
the original release.
Since most of the original corpus, and all of the
sentences used for training the MedPost tagger, were
in the area of molecular biology, we added an addi-
tional 1 000 sentences selected from random MED-
LINE abstracts on the subject of clinical medicine.
As a preliminary result, the trained MedPost tag-
ger achieves approximately 96.9% accuracy, which
is comparable to the 97.4% accuracy achieved on the
subset of 1 000 sentences selected randomly from all
of MEDLINE. An example of a sentence from the
clinical medicine collection is
EvidenceNN isVBZ nowRR availableJJ
toTO showVVI aDD beneficialJJ effectNN
ofII bezafibrateNN onII retardingVVGN
atheroscleroticJJ processesNNS andCC inII
reducingVVGN riskNN ofII coronaryJJ heartNN
diseaseNN .
In addition to the token-level annotations, all
of the gerunds in the MedPost corpus (these are
tagged VVGN) were also examined and it was noted
whether the gerund had an explicit subject, direct
object, or adjective complement. This annotation
is stored with an annotation of type gerund. To il-
lustrate, the two gerunds in the previous example,
retarding and reducing both have direct objects (re-
tarding processes and reducing risk), and the gerund
tag is entered as ?o?. The gerund annotations have
been used to improve a noun phrase bracketer able
to recognize gerundive phrases.
3.2 GENETAG Update
GENETAG is a corpus of MEDLINE sentences that
have been annotated with gene and protein names.
The closest related work is the GENIA corpus (Kim
et. al., 2003). GENIA provides detailed coverage of
a large number of semantic entities related to a spe-
cific subset of human molecular biology, whereas
GENETAG provides gene and protein name anno-
tations only, for a wide range of organisms and
biomedical contexts (molecular biology, genetics,
biochemistry, clinical medicine, etc.)
We are including a new version of GENE-
TAG, GENETAG-05, as part of the MedTag sys-
tem. GENETAG-05 differs from GENETAG in
four ways: 1) the definition of a gene/protein en-
tity has been modified, 2) significant annotation er-
rors in GENETAG have been corrected, 3) the con-
cept of a non-specific entity has been refined, and 4)
character-based indices have been introduced to re-
duce tokenization problems. We believe that these
changes result in a more accurate and robust corpus.
GENETAG-05 maintains a wide definition of a
gene/protein entity including genes, proteins, do-
mains, sites, sequences, and elements, but exclud-
ing plasmids and vectors. The specificity con-
straint requires that a gene/protein name must be
included in the tagged entity. This constraint has
been applied more consistently in GENETAG-05.
Additionally, plain sequences like ATTGGCCTT-
TAAC are no longer tagged, embedded names are
tagged (ras-mediated), and significantly more terms
have been judged to violate the specificity constraint
(growth factor, proteases, protein kinase, ribonu-
clease, snoRNA, rRNA, tissue factor, tumor anti-
gen, complement, hormone receptors, nuclear fac-
tors, etc.).
The original GENETAG corpus contains some en-
tities that were erroneously tagged as gene/proteins.
Many of these errors have been corrected in the up-
dated corpus. Examples include camp-responsive
elements, mu element, VDRE, melanin, dentin,
myelin, auxin, BARBIE box, carotenoids, and cel-
lulose. Error analysis resulted in the updated anno-
tation conventions given in Table 1.
Enzymes are a special class of proteins that cat-
alyze biochemical reactions. Enzyme names have
varying degrees of specificity, so the line drawn for
35
tagging purposes is based on online resources1 as
well as background knowledge. In general, tagged
enzymes refer to more specific entities than un-
tagged enzymes (tyrosine kinase vs. protein kinase,
ATPase vs. protease). Enzymes that can refer to
either DNA or RNA are tagged if the reference is
specified (DNA endonuclease vs. endonuclease).
Enzymes that do not require DNA/RNA distinction
are tagged (lipase vs. ligase, cyclooxygenase vs.
methylase). Non-specific enzymes are tagged if they
clearly refer to a gene or protein, as in (1).
1) The structural gene for hydrogenase en-
codes a protein product of molecular mass
45820 Da.
Semantic constraints in GENETAG-05 are the
same as those for GENETAG. To illustrate, the name
in (2) requires rabies because RIG implies that the
gene mentioned in this sentence refers to the rabies
immunoglobulin, and not just any immunoglobulin.
In (3), the word receptor is necessary to differen-
tiate IGG receptor from IGG, a crucial biological
distinction. In (4), the number 1 is needed to ac-
curately describe a specific type of tumor necrosis
factor, although tumor necrosis factor alone might
be adequate in a different context.
2) rabies immunoglobulin (RIG)
3) IGG receptor
4) Tumor necrosis factor 1
Application of the semantic constraint can result in
apparent inconsistencies in the corpus (immunoglob-
ulin is sufficient on its own in some sentences in the
corpus, but is insufficient in (2)). However, we be-
lieve it is important that the tagged entity retain its
true meaning in the sentence context.
4 Recommended Uses
We have found the component corpora of MedTag
to be useful for the following functions:
1) Training and evaluating part-of-speech
taggers
2) Training and evaluating gene/protein
named entity taggers
1http://cancerweb.ncl.ac.uk/omd/copyleft.html
http://www.onelook.com/
3) Developing and evaluating a noun
phrase bracketer for PubMed phrase
indexing
4) Statistical analysis of grammatical
usage in medical text
5) Feature generation for machine learn-
ing
The MedPost tagger was recently ported to Java
and is currently being employed in MetaMap, a pro-
gram that maps natural language text into the UMLS
(Aronson,A.R., 2001).
5 Conclusion
We have merged three biomedical corpora into a col-
lection of annotations called MedTag. MedTag uses
a common relational database format along with a
web interface to facilitate annotation consistency.
We have identified the MEDLINE excerpts for each
sentence and eliminated tokenization dependence,
increasing the usability of the data. In GENETAG-
05, we have clarified many grey areas for annotation,
providing better guidelines for tagging these cases.
For users of previous versions of the component cor-
pora, we have included programs to convert from the
new standardized format to the formats used in the
older versions.
References
Aronson, A. R. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proc. AMIA Symp., 1721.
Kim, J.-D., Ohta, T., Tateisi, Y. and Tsujii, J. 2003. GE-
NIA corpus: a semantically annotated corpus for bio-
textmining. Bioinformatics, 19: 180 - 182.
Tanabe, L and Wilbur, WJ. 2002. Tagging gene and
protein names in biomedical text. Bioinformatics, 18,
1124-1132.
Tanabe L, Xie N, Thom, LH, Matten W, Wilbur, WJ:
GENETAG: a tagged gene corpus for gene/protein
named entity recognition. BMC Bioinformatics 2005.
Smith, L, Rindflesch, T, and Wilbur, WJ. 2004. MedPost:
a part of speech tagger for biomedical text. Bioinfor-
matics, 20(13) 2320-2321.
Yeh A, Hirschman L, Morgan A, Colosimo M: BioCre-
AtIvE task 1A: gene mention finding evaluation. BMC
Bioinformatics 2005.
36
Entity Type Problem GENETAG-05
Convention
Positive Examples Negative
Examples
Protein
Families
Some are named after
structural motifs.
Do not tag
structures alone,
but tag structurally
related gene and
protein families.
Zinc finger protein,
bZIP transcription
factor, homeobox
gene, TATA binding
protein
Zinc finger,
helix-turn-helix
motif, leucine
zipper, homeobox,
TATA box
Domains Name can refer to 1) the
amino acid content of a
sequence (PEST), 2) the
protein that binds the
sequence (TFIIIA DNA
binding domain), 3) a
homologous gene (SH2 - Src
homology domain 2), 4) the
first proteins in which the
domain was discovered (LIM,
PDZ), or 5) structural entities
(POZ, zinc finger domain).
Tag only if the
domain refers to a
gene or protein.
Immuno-globulin
regions are tagged.
(VH refers to the
Immuno-globulin
heavy chain V
region).
BTB domain, LIM
domain, HECT
domain, VH
domain, SH2
domain, TFIIIA
DNA binding
domain,
Kru?ppel-associated
box (KRAB)
domains, NF-IL6
beta leucine zipper
domain
PEST domain, SR
domain, zinc finger
domain, b-Zip
domain, POZ
domain, GATA
domain, RS
domain, GAR
domain
Boxes,
Response
Elements and
Sites
Name can refer to 1) the
sequence or site itself
(TAAG), 2) a non-protein that
binds to it (Glucocorticoid
Response Element), 3) a
protein that binds to it (Sp1),
or 4) to homologous genes
(VL30).
Tag only if the
sequence or site
refers to a gene or
protein.
VL30 element, Zta
response elements,
activating protein 1
(AP-1) site, Ets
binding site, SP1
site, AP-2 box
GRE, TRE, cyclic
AMP response
element ( CRE),
TAAG sites, TGn
motif, TAR element,
UP element
Hormones Some are peptide hormones. Tag only peptide
hormones.
Insulin, Glucagon,
growth hormone
Estrogen,
Progesterone,
thyroid hormone
?and?
constructs
Some conjuncts require the
entire construct.
Unless both
conjuncts can stand
alone, tag them
together.
TCR alpha and
beta, D-lactate and
D-glycerate
dehydrogenase
TCR alpha, beta,
D-lactate,
D-glycerate
dehydrogenase
Viral
Sequences
Promoters, enhancers, repeats
are distinguished by
organism.
Tag only if the
organism is present.
Viral LTR, HIV
long terminal
repeat, SV40
promoter
LTR, long terminal
repeat
Sequences Some sequences lack gene or
protein names.
Tag only if a gene
name is included.
NF kappa B
enhancer
(TGGAAATTCC)
TCTTAT, TTGGGG
repeats
Embedded
Names
Some names are embedded in
non-gene text.
Tag only the gene
part.
P-47-deficient,
ras-transformed
P-47-deficient,
ras-transformed
Transposons,
Satellites
Often repetitive sequences. Tag if specific. L1 element, TN44,
copia
retrotransposon
non-LTR
retrotransposon
Antibodies Often use organism or disease
name.
Tag if specific. anti-SF group
rickettsiae (SFGR)
antinuclear
antibody
Alternative
Transcripts
Names differ from primary
transcript.
Tag if primary
transcript named.
I kappa B
gamma,VEGF20
Exon 2, IIA
Table 2: Some problematic gene/protein annotations and conventions followed in GENETAG-05.
37
A resource for constructing customized test suites 
for molecular biology entity identification systems 
 
K. Bretonnel Cohen 
Center for Computational Pharmacology, 
University of Colorado School of Medicine 
kevin.cohen@uchsc.edu 
Lorraine Tanabe 
National Center for Biotechnology 
Information, NLM, NIH 
tanabe@ncbi.nlm.nih.gov 
Shuhei Kinoshita 
Fujitsu Ltd. Bio-IT Lab, and Center for 
Computational Pharmacology, University of 
Colorado School of Medicine 
shuhei.kinoshita@uchsc.edu 
Lawrence Hunter 
Center for Computational Pharmacology, 
University of Colorado School of Medicine 
larry.hunter@uchsc.edu 
 
Abstract 
This paper describes a data source and 
methodology for producing customized test 
suites for molecular biology entity 
identification systems.  The data consists of: 
(a) a set of gene names and symbols classified 
by a taxonomy of features that are relevant to 
the performance of entity identification 
systems, and (b) a set of sentential 
environments into which names and symbols 
are inserted to create test data and the 
associated gold standard.  We illustrate the 
utility of test sets producible by this 
methodology by applying it to five entity 
identification systems and describing the error 
patterns uncovered by it, and investigate 
relationships between performance on a 
customized test suite generated from this data 
and the performance of a system on two 
corpora. 
 
1 Introduction 
This paper describes a methodology and data for the 
testing of molecular biology entity identification (EI) 
systems by developers and end users.  Molecular 
biology EI systems find names of genes and gene 
products in free text.  Several years? publication history 
has established precision, recall, and F-score as the de 
facto standards for evaluating EI systems for molecular 
biology texts at the publication stage and in 
competitions like BioCreative 
(www.mitre.org/public/biocreative).  These measures 
provide important indices of a system?s overall output 
quality.  What they do not provide is the detailed sort of 
information about system performance that is useful for 
the system developer who is attempting to assess the 
strengths and weaknesses of a work in progress, nor do 
they provide detailed information to the potential 
consumer who would like to compare two systems 
against each other. Hirschman and Mani (2003) point 
out that different evaluation methods are useful at 
different points in the software life-cycle.  In particular, 
what they refer to as feature-based evaluation via test 
suites is useful at two points: in the development phase, 
and for acceptance testing.  We describe here a 
methodology and a set of data for constructing 
customized feature-based test suites for EI in the 
molecular biology domain.  The data consists of two 
sets.  One is a set of names and symbols of entities as 
that term is most commonly understood in the 
molecular biology domain?genes and gene products.  
(Sophisticated ontologies such as GENIA (Ohta et al 
2002) include other kinds of entities relevant to 
molecular biology as well, such as cell lines.)  The 
names and symbols exemplify a wide range of the 
features that characterize entities in this domain?case 
variation, presence or absence of numbers, presence or 
absence of hyphenation, etc. The other is a set of 
sentences that exemplify a range of sentential contexts 
in which the entities can appear, varying with respect to 
position of the entity in the sentence (initial, medial, or 
final), presence of keywords like gene and protein, 
tokenization issues, etc.  Both the entities and the 
sentential contexts are classified in terms of a taxonomy 
of features that are relevant to this domain in particular 
and to natural language processing and EI in general.  
The methodology consists of generating customized test 
suites that address specific performance issues by 
combining sets of entities that have particular 
characteristics with sets of contexts that have particular 
characteristics.  Logical combination of subsets of 
characteristics of entities and contexts allows the 
developer to assess the effect of specific characteristics 
on performance, and allows the user to assess 
performance of the system on types of inputs that are of 
                                            Association for Computational Linguistics.
                     Linking Biological Literature, Ontologies and Databases, pp. 1-8.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
particular interest to them.  For example, if the 
developer or end-user wants to assess the ability of a 
system to recognize gene symbols with a particular 
combination of letter case, hyphenation, and presence 
or absence of numerals, the data and associated code 
that we provide can be used to generate a test suite 
consisting of symbols with and without that 
combination of features in a variety of sentential 
contexts. 
Inspiration for this work comes on the one hand 
from standard principles of software engineering and 
software testing, and on the other hand from descriptive 
linguistics (Harris 1951, Samarin 1967).  In Hirschman 
and Mani?s taxonomy of evaluation techniques, our 
methodology is referred to as feature-based, in that it is 
based on the principle of classifying the inputs to the 
system in terms of some set of features that are relevant 
to the application of interest.  It is designed to provide 
the developer or user with detailed information about 
the performance of her EI system.  We apply it to five 
molecular biology EI and information extraction 
systems: ABGene (Tanabe and Wilbur 2002a, Tanabe 
and Wilbur 2002b); KeX/PROPER (Fukuda et al 
1997); Yapex (Franz?n et al 2002); the stochastic POS 
tagging-based system described in Cohen et al (in 
submission); and the entity identification component of 
Ono et al?s information extraction system (Ono et al 
2001), and show how it gives detailed useful 
information about each that is not apparent from the 
standard metrics and that is not documented in the cited 
publications.  (Since we are not interested in punishing 
system developers for graciously making their work 
available by pointing out their flaws, we do not refer to 
the various systems by name in the remainder of this 
paper.)   
Software testing techniques can be grouped into 
structured (Beizer 1990), heuristic (Kaner et al 2002), 
and random categories.  Testing an EI system by 
running it on a corpus of texts and calculating precision, 
recall, and F-score for the results falls into the category 
of random testing.  Random testing is a powerful 
technique, in that it is successful in finding bugs.  When 
done for the purpose of evaluation, as distinct from 
testing (see Hirschman and Thompson 1997 for the 
distinction between the two, referred to there as 
performance evaluation and diagnostic evaluation), it 
also is widely accepted as the relevant index of 
performance for publication.  However, its output lacks 
important information that is useful to a system 
developer (or consumer): it tells you how often the 
system failed, but not what it failed at; it tells you how 
often the system succeeds, but not where its strengths 
are.   
For the developer or the user, a structured test suite 
offers a number of advantages in answering these sorts 
of questions.  The utility of such test suites in general 
software testing is well-accepted.  Oepen et al (1998) 
lists a number of advantages of test suites vs. 
naturalistic corpora for testing natural language 
processing software in particular: 
? Control over test data: test suites allow for 
?focussed and fine-grained diagnosis of system 
performance? (15).  This is important to the 
developer who wants to know exactly what 
problems need to be fixed to improve performance, 
and to the end user who wants to know that 
performance is adequate on exactly the data that 
they are interested in. 
? Systematic coverage: test suites can allow for 
systematic evaluation of variations in a particular 
feature of interest.  For example, the developer 
might want to evaluate how performance varies as 
a function of name length, or case, or the presence 
or absence of hyphenation within gene symbols.  
The alternative to using a structured test suite is to 
use a corpus, and then search through it for the 
relevant inputs and hope that they are actually 
attested. 
? Control of redundancy: while redundancy in a 
corpus is representative of actual redundancy in 
inputs, test suites allow for reduction of 
redundancy when it obscures the situation, or for 
increasing it when it is important to test handling of 
a feature whose importance is greater than its 
frequency in naturally occurring data.  For 
example, names of genes that are similar to names 
of inherited diseases might make up only a small 
proportion of the gene names that occur in PubMed 
abstracts, but the user whose interests lie in 
curating OMIM might want to be able to assure 
herself that coverage of such names is adequate, 
beyond the level to which corpus data will allow.   
? Inclusion of negative data: in the molecular 
biology domain, a test suite can allow for 
systematic evaluation of potential false positives.   
? Coherent annotation: even the richest metadata is 
rarely adequate or exactly appropriate for exactly 
the questions that one wants to ask of a corpus.  
Generation of structured, feature-based test suites 
obviates the necessity for searching through 
corpora for the entities and contexts of interest, and 
allows instead the structuring of contexts and 
labeling of examples that is most useful to the 
developer. 
The goal of this paper is to describe a methodology and 
publicly available data set for constructing customized 
and refinable test suites in the molecular biology 
domain quickly and easily.  A crucial difference 
between similar work that simply documents a 
distributable test suite (e.g. Oepen (1998) and Volk 
(1998)) and the work reported in this paper is that we 
are distributing not a static test suite, but rather data for 
generating test suites?data that is structured and 
classified in such a way as to allow software developers 
and end users to easily generate test suites that are 
customized to their own assessment needs and 
development questions.  We build this methodology 
and data on basic principles of software engineering 
and of linguistic analysis.  The first such principle 
involves making use of the software testing notion of 
the catalogue.   
A catalogue is a list of test conditions, or qualities 
of particular test inputs (Marick 1997).  It corresponds 
to the features of feature-based testing, discussed in 
Hirschman and Mani (2003) and to the schedule 
(Samarin 1967:108-112) of descriptive linguistic 
technique.  For instance, a catalogue of test conditions 
for numbers might include: 
? zero, non-zero, real, integer 
? positive, negative, unsigned 
? the smallest number representable in some data 
type, language, or operating system; smaller than 
the smallest number representable 
? the largest number representable; larger than the 
largest number representable  
Note that the catalogue includes both ?clean? 
conditions and ?dirty? ones.  This approach to software 
testing has been highly successful, and indeed the best-
selling book on software testing (Kaner et al 1999) can 
fairly be described as a collection of catalogues of 
various types. 
The contributions of descriptive linguistics include 
guiding our thinking about what the relevant features, 
conditions, or categories are for our domain of interest.  
In this domain, that will include the questions of what 
features may occur in names and what features may 
occur in sentences?particularly features in the one that 
might interact with features in the other.  Descriptive 
linguistic methodology is described in detail in e.g. 
Harris (1951) and Samarin (1967); in the interests of 
brevity, we focus on the software engineering 
perspective here, but the thought process is very 
similar.  The software engineering equivalent of the 
descriptive linguist?s hypothesis is the fault model 
(Binder 1999)?an explicit hypothesis about a potential 
source of error based on ?relationships and components 
of the system under test? (p. 1088).  For instance, 
knowing that some EI systems make use of POS tag 
information, we might hypothesize that the presence of 
some parts of speech within a gene name might be 
mistaken for term boundaries (e.g. the of in bag of 
marbles, LocusID 43038).  Catalogues are used to 
develop a set of test cases that satisfies the various 
qualities.  (They can also be used post-hoc to group the 
inputs in a random test bed into equivalence classes, 
although a strong motivation for using them in the first 
place is to obviate this sort of search-based post-hoc 
analysis.)  The size of the space of all possible test 
cases can be estimated from the Cartesian product of all 
catalogues; the art of software testing (and linguistic 
fieldwork) consisting, then, of selecting the highest-
yielding subset of this often enormous space that can be 
run and evaluated in the time available for testing.   
At least three kinds of catalogues are relevant to 
testing an EI system.  They fall into one of two very 
broad categories: syntagmatic, having to do with 
combinatory properties, and paradigmatic, having to do 
with varieties of content.  The three kinds of catalogues 
are: 
1. A catalogue of environments in which gene names 
can appear.  This is syntagmatic. 
2. A catalogue of types of gene names.  This is 
paradigmatic. 
3. A catalogue of false positives.  This is both 
syntagmatic and paradigmatic. 
The catalogue of environments would include, for 
example, elements related to sentence position, such as 
sentence-initial, sentence-medial, and sentence-final; 
elements related to list position, such as a single gene 
name, a name in a comma-separated list, or a name in a 
conjoined noun phrase; and elements related to 
typographic context, such as location within 
parentheses (or not), having attached punctuation (e.g. a 
sentence-final period) (or not), etc.  The catalogue of 
types of names would include, for example, names that 
are common English words (or not); names that are 
words versus ?names? that are symbols; single-word 
versus multi-word names; and so on.  The second 
category also includes typographic features of gene 
names, e.g. containing numbers (or not), consisting of 
all caps (or not), etc.  We determined candidate features 
for inclusion in the catalogues through standard 
structuralist techniques such as examining public-
domain databases containing information about genes, 
including FlyBase, LocusLink, and HUGO, and by 
examining corpora of scientific writing about genes, 
and also by the software engineering techniques of 
?common sense, experience, suspicion, analysis, [and] 
experiment? (Binder 1999).  The catalogues then 
suggested the features by which we classified and 
varied the entities and sentences in the data.   
General format of the data 
The entities and sentences are distributed in XML 
format and are available at a supplemental web site 
(compbio.uchsc.edu/Hunter_lab/testing_ei).  A plain-
text version is also available.  A representative entity is 
illustrated in Figure 1 below, and a representative 
sentence is illustrated in Figure 2.  All data in the 
current version is restricted to the ASCII character set.   
Test suite generation   
Data sets are produced by selecting sets of entity 
features and sets of sentential context features and 
inserting the entities into slots in the sentences.    This 
can be accomplished with the user?s own tools, or using 
applications available at the supplemental web site.  
The provided applications produce two files: a file 
containing raw data for use as test inputs, and a file 
containing the corresponding gold standard data marked 
up in an SGML-like format.  For example, if the raw 
data file contains the sentence ACOX2 polymorphisms 
may be correlated with an increased risk of larynx 
cancer, then the gold standard file will contain the 
corresponding sentence <gp>ACOX2</gp> 
polymorphisms may be correlated with an increased 
risk of larynx cancer.  Not all users will necessarily 
agree on what counts as the ?right? gold standard?see 
Olsson et al (2002) and the BioCreative site for some 
of the issues.  Users can enforce their own notions of 
correctness by using our data as input to their own 
generation code, or by post-processing the output of our 
applications.   
 
ID: 136 
name_vs_symbol: n 
length: 3 
case: a 
contains_a_numeral: y 
contains_Arabic_numeral: y 
Arabic_numeral_position: f 
contains_Roman_numeral: 
<several typographic features omitted> 
contains_punctuation: 1 
contains_hyphen: 1 
contains_forward_slash: 
<several punctuation-related features omitted> 
contains_function_word: 
function_word_position: 
contains_past_participle: 1 
past_participle_position: i 
contains_present_participle: 
present_participle_position: 
source_authority: HGNC ID: 2681 "Approved Gene 
Name" field 
original_form_in_source: death-associated 
protein 6 
data: death-associated protein 6 
Figure 1  A representative entry from the entity data 
file.  A number of null-valued features are omitted for 
brevity?see the full entry at the supplemental web site.  
The data field (last line of the figure) is what is output 
by the generation software. 
ID: 25 
type: tp 
total_number_of_names: 1 
list_context:  
position: I 
typographic_context:  
appositive:  
source_id: PMID: 14702106 
source_type: title 
original_form_in_source: Stat-3 is required 
for pulmonary homeostasis during hyperoxia. 
slots: <> is required for pulmonary 
homeostasis during hyperoxia.    
 
Figure 2  A representative entry from the sentences 
file.  Features and values are explained in section 2.2 
Feature set for sentential contexts below.   The slots 
field (last line of the figure) shows where an entity 
would be inserted when generating test data.   
2   The taxonomy of features for entities 
and sentential contexts 
In this section we describe the feature sets for entities 
and sentences, and motivate the inclusion of each, 
where not obvious.   
2.1   Feature set for entities 
Conceptually, the features for describing name-inputs 
are separated into four categories: 
orthographic/typographic, morphosyntactic, source, and 
lexical.   
? Orthographic/typographic features describe the 
presence or absence of features on the level of 
individual characters, for example the case of letters, 
the presence or absence of punctuation marks, and the 
presence or absence of numerals.   
? Morphosyntactic features describe the presence or 
absence of features on the level of the morpheme or 
word, such as the presence or absence of participles, 
the presence or absence of genitives, and the presence 
or absence of function words.   
? Source features are defined with reference to the 
source of an input.  (It should be noted that in software 
engineering, as in Chomskyan theoretical linguistics, 
data need not be naturally-occurring to be useful; 
however, with the wealth of data available for gene 
names, there is no reason not to include naturalistic 
data, and knowing its source may be useful, e.g. in 
evaluating performance on FlyBase names, etc.)  
Source features include source type, e.g. literature, 
database, or invention; identifiers in a database; 
canonical form of the entity in the database; etc. 
? Lexical features are defined with respect to the 
relationship between an input and some outside source 
of lexical information, for instance whether or not an 
input is or contains a common English word.  This is 
also the place to indicate whether or not an input is 
present in a resource such as LocusLink, whether or not 
it is on a particular stoplist, whether it is in-vocabulary 
or out-of-vocabulary for a particular language model, 
etc. 
The distinction between these three broad 
categories of features is not always clear-cut.  For 
example, presence of numerals is an 
orthographic/typographic feature, and is also 
morphosyntactic when the numeral postmodifies a 
noun, e.g. in heat shock protein 60.  Likewise, features 
may be redundant?for example, the presence of a 
Greek letter in the square-bracket- or curly-bracket-
enclosed formats, or the presence of an apostrophized 
genitive, are not independent of the presence of the 
associated punctuation marks.  However, Boolean 
queries over the separate feature sets let them be 
manipulated and queried independently. So, entities 
with names like A' can be selected independently of 
names like Parkinson?s disease.   
2.1.1   Orthographic/typographic features 
Length:  Length is defined in characters for 
symbols and in whitespace-tokenized words for names.   
Case: This feature is defined in terms of five 
possible values: all-upper-case, all-lower-case, upper-
case-initial-only, each-word-upper-case-initial (e.g. 
Pray For Elves), and mixed.  The fault model 
motivating this feature hypothesizes that taggers may 
rely on case to recognize entities and may fail on some 
combinations of cases with particular sentential 
positions.  For example, one system performed well on 
gene symbols in general, except when the symbols are 
lower-case-initial and in sentence-initial position (e.g. 
p100 is abundantly expressed in liver? (PMID 
1722209) and bif displays strong genetic interaction 
with msn (PMID 12467587).    
Numeral-related features: A set of features 
encodes whether or not an entity contains a numeral, 
whether the numeral is Arabic or Roman, and the 
positions of numerals within the entity (initial, medial, 
or final).  The motivation for this feature is the 
hypothesis that a system might be sensitive to the 
presence or absence of numerals in entities.  One 
system failed when the entity was a name (vs. a 
symbol), it contained a number, and the number was in 
the right-most (vs. a medial) position in a word.  It 
correctly tagged entities like glucose 6 phosphate 
dehydrogenase but missed the boundary on 
<gp>alcohol dehydrogenase</gp> 6. This pattern was 
specific to numbers?letters in the same position are 
handled correctly.  
Punctuation-related features: A set of features 
includes whether an entity contains any punctuation, the 
count of punctuation marks, and which marks they are 
(hyphen, apostrophe, etc.).  One system failed to 
recognize names (but typically not symbols) when they 
included hyphens.  Another system had a very reliable 
pattern of failure involving apostrophes just in case they 
were in genitives. 
Greek-letter-related features:  These features 
encode whether or not an entity contains a Greek letter, 
the position of the letter, and the format of the letter.  
(This feature is an example of an orthographic feature 
which may be defined on a substring longer than a 
character, e.g. beta.)  Two systems had problems 
recognizing gene names when they contained Greek 
letters in the PubMed Central format, i.e. [beta]1 
integrin.   
2.1.2   Morphosyntactic features 
The most salient morphosyntactic feature is whether an 
entity is a name or a symbol.  The fault model 
motivating this feature suggests that a system might 
perform differently depending on whether an input is a 
name or a symbol.  The most extreme case of a system 
being sensitive to this feature was one system that 
performed very well on symbols but recognized no 
names whatsoever.   
Features related to function words: a set of 
features encodes whether or not an entity contains a 
function word, the number of function words in the 
entity, and their positions?for instance, the facts: that 
scott of the antarctic (FlyBase ID FBgn0015538) 
contains two function words; that they are of and the; 
and that they are medial to the string.  This feature is 
motivated by two fault models.  One posits that a 
system might apply a stoplist to its input and that 
processing of function words might therefore halt at an 
early stage.  The other posits that a system might 
employ shallow parsing to find boundaries of entities 
and that the shallow parser might insert boundaries at 
the locations of function words, causing some words to 
be omitted from the entity.  One system always had 
partial hits on names that were multi-word unless each 
word in it was upper-case-initial, or there was an 
alphanumeric postmodifier (i.e. a numeral, upper-cased 
singleton letter, or Greek letter) at the right edge.   
Features related to inflectional morphology: a 
set of features encodes whether or not an entity contains 
nominal number or genitive morphology or verbal 
participial morphology, and the positions of the words 
in the entity that contain those morphemes, for instance 
the facts that apoptosis antagonizing transcription 
factor (HUGO ID 19235) contains a present participle 
and that the word that contains it is medial to the string.   
Features related to parts of speech: Future 
development of the data will include features encoding 
the parts of speech present in names. 
2.1.3   Source features 
Source or authority:  This feature encodes the 
source of or authority cited for an entity.  For many of 
the entries in the current data, it is an identifier from 
some database.  For others, it is a website (e.g. 
www.flynome.org).  Other possible values include the 
PMID of a document in which it was observed.   
Original form in source:  Where there is a source 
for the entity or for some canonical form of the entity, 
the original form is given.  This is not equivalent to the 
?official? form, but rather is the exact form in which the 
entity occurs; it may even contain typographic errors 
(e.g. the extraneous space in nima ?related kinase, 
LocusID 189769 (reported to the NCBI service desk).   
2.1.4   Lexical features 
These might be better called lexicographic features.  
They can be encoded impressionistically, or can be 
defined with respect to an external source, such as 
WordNet, the UMLS, or other lexical resources.  They 
may also be useful for encoding strictly local 
information, such as whether or not a gene was attested 
in training data or whether it is present in a particular 
language model or other local resource.  These features 
are allowed in the taxonomy but are not implemented in 
the current data.  Our own use of the entity data 
suggests that it should be, especially encoding of 
whether or not names include common English words.  
(The presence of function words is already encoded.)   
2.2   Feature set for sentential contexts 
In many ways, this data is much harder to build and 
classify than the names data, for at least two reasons. 
Many more features interact with each other, and as 
soon as a sentence contains more than one gene name, 
it contains more than one environment, and the number 
of features for the sentence as a whole is multiplied, as 
are the interactions between them.  For this reason, we 
have focussed our attention so far on sentences 
containing only a single gene name, although the 
current version of the data does include a number of 
multi-name sentences.  
2.2.1   Positivity 
The fundamental distinction in the feature set for 
sentences has to do with whether the sentence is 
intended to provide an environment in which gene 
names actually appear, or whether it is intended to 
provide a non-trivial opportunity for false positives. 
True positive sentences contain some slot in which 
entities from the names data can be inserted, e.g. <> 
polymorphisms may be correlated with an increased 
risk of larynx cancer or <> interacts with <> and <> 
in the two-hybrid system.   
False positive sentences contain one or more 
tokens that are deliberately intended to pose 
challenging opportunities for false positives.  Certainly 
any sentence which does not consist all and only of a 
single gene name contains opportunities for false 
positives, but not all potential false positives are created 
equal.  We include in the data set sentences that contain 
tokens with orthographic and typographic 
characteristics that mimic the patterns commonly seen 
in gene names and symbols, e.g.  The aim of the present 
study is to evaluate the impact on QoL? where QoL is 
an abbreviation for quality of life.  We also include 
sentences that contain ?keywords? that may often be 
associated with genes, such as gene, protein, mutant, 
expression, etc., e.g. Demonstration of antifreeze 
protein activity in Antarctic lake bacteria.  
2.2.2   Features for TP sentences 
Number and positional features encode the total number 
of slots in the sentence, and their positions.  The value 
for the position feature is a list whose values range over 
initial, medial, and final.  For example, the sentence  
<> interacts with <> and <> in the two-hybrid system 
has the value I,M (initial and medial) for the position 
feature.   
Typographic context features encode issues 
related to tokenization, specifically related to 
punctuation, for example if a slot has punctuation on 
the left or right edge, and the identity of the punctuation 
marks. 
List context features encode data about position in 
lists.  These include the type of list (coordination, 
asyndetic coordination, or complex coordination).  
The appositive feature is for the special case of 
appositioned symbols or abbreviations and their full 
names or definitions, e.g. The Arabidopsis INNER NO 
OUTER (INO) gene is essential for formation and?  
For the systems that we have tested with it, it has not 
revealed problems that are independent of the 
typographic context.  However, we expect it to be of 
future use in testing systems for abbreviation expansion 
in this domain.   
Source features encode the identification and type 
of the source for the sentence and its original form in 
the source.  The source identifier is often a PubMed ID.  
It bears pointing out again that there is no a priori 
reason to use sentences with any naturally-occurring 
?source? at all, as opposed to the products of the 
software engineer?s imagination.  Our primary rationale 
for using naturalistic sources at all for the sentence data 
has more to do with convincing the user that some of 
the combinations of entity features and sentential 
features that we claim to be worth generating actually 
do occur.  For instance, it might seem counterintuitive 
that gene symbols or names would ever occur lower-
case-initial in sentence initial position, but in fact we 
found many instances of this phenomenon; or that a 
multi-word gene name would occur in text in all upper-
case letters, but see the INNER NO OUTER example 
above.   
Syntactic features encode the characteristics of 
the local environment.  Some are very lexical, such as: 
whether the following word is a keyword; whether the 
preceding word is a species name.  Others are more 
abstract, such as whether the preceding word is an 
article; whether the preceding word is an adjective; 
whether the preceding word is a conjunction; whether 
the preceding word is a preposition.  Interactions with 
the list context features are complex.  The fault model 
motivating these features hypothesizes that POS context 
and the presence of keywords might affect a system?s 
judgments about the presence and boundaries of names.   
2.2.3   Features for FP sentences 
Most features for FP sentences encode the 
characteristics that give the contents of the sentence 
their FP potential.  The keyword feature is a list of 
keywords present in the sentence, e.g. gene, protein, 
expression, etc.  The typographic features feature 
encodes whether or not the FP potential comes from 
orthographic or typographic features of some token in 
the sentence, such as mixed case, containing hyphens 
and a number, etc.  The morphological features feature 
encodes whether or not the FP potential comes from 
apparent morphology, such as words that end with ase 
or in.   
3   Testing the relationship between 
predictions from performance on a test 
suite and performance on a corpus 
Precision and recall on data in a structured test suite 
should not be expected to predict precision and recall 
on a corpus, since there is no relation between the 
prevalence of features in the test suite and prevalence of 
features in the corpus.  However, we hypothesized that 
performance on an equivalence class of inputs in a test 
suite might predict performance on the same 
equivalence class in a corpus.  To test this hypothesis, 
we ran a number of test suites through one of the 
systems and analyzed the results, looking for patterns of 
errors.  The test suites were very simple, varying only 
entity length, case, hyphenation, and sentence position.  
Then we ran two corpora through the same system and 
examined the output for the actual corpora to see if the 
predictions based on the system?s behavior on the test 
suite actually described performance on similar entities 
in the corpora.     
One corpus, which we refer to as PMC (since it 
was sampled from PubMed Central), consists of 2417 
sentences sampled randomly from a set of 1000 full-
text articles.  This corpus contains 3491 entities.  It is 
described in Tanabe and Wilbur (2002b).  The second 
corpus was distributed as training data for the 
BioCreative competition.  It consists of 10,000 
sentences containing 11,851 entities and is described in 
detail at www.mitre.org/public/biocreative.  Each 
corpus is annotated for entities.   
The predictions based on the system?s performance 
on the test suite data were: 
1. The system will have low recall on entities that 
have numerals in initial position, followed by a 
dash, e.g. 825-Oak, 12-LOX, and 18-wheeler 
(/^\d+-/ in Perl). 
2. The system will have low recall on names that 
contain stopwords, such as Pray For Elves and ken 
and barbie.   
3. The system will have low recall on sentence-
medial terms that begin with a capital letter, such 
as Always Early.   
4. The system will have low recall on three-character-
long symbols.   
5. The system will have good recall on (long) names 
that end with numerals. 
We then examined the system?s true positive, false 
positive, and false negative outputs from the two 
corpora for outputs that belonged to the equivalence 
classes in 1-5.  Table 1 shows the results. 
 
 BioCreative 
 TP FP FN P R 
1 12 57 17 .17 .41 
2 0 1 38 0.0 0.0 
4 556 278 512 .67 .52 
5 284 251 72 .53 .80 
 PubMed Central 
 TP FP FN P R 
1 8 10 0 .44 1.0 
2 1 0 2 1.0 .33 
4 163 64 188 .72 .46 
5 108 54 46 .67 .70 
 
Table 1  Performance on two corpora for the 
predictable categories  Numbers in the far left column 
refer to the predictions listed above.  Overall 
performance on the corpora was: BioCreative P = .65, 
R = .68, and PMC P = .71, R = .62.   
For equivalence classes 1, 2, and 4, the predictions 
mostly held.  Low recall was predicted, and actual 
recall was .41, 0.0, .52, 1.0 (the one anomaly), .33, and 
.46 for these classes of names, versus overall recall of 
.68 on the BioCreative corpus and .62 on the PMC 
corpus.  The prediction held for equivalence class 5, as 
well; good recall was predicted, and actual recall was 
.80 and .70?higher than the overall recalls for the two 
corpora.  The third prediction could not be evaluated 
due to the normalization of case in the gold standards.  
These results suggest that a test suite can be a good 
predictor of performance on entities with particular 
typographic characteristics.   
4   Conclusion 
We do not advocate using this approach to replace the 
quantitative evaluation of EI systems by precision, 
recall, and F-measure.  Arguably, overall performance 
on real corpora is the best evaluation metric for entity 
identification, in which case the standard metrics are 
well-suited to the task.  However, at specific points in 
the software lifecycle, viz. during development and at 
the time of acceptance testing, the standard metrics do 
not provide the right kind of information.  We can, 
however, get at this information if we bear in mind two 
things: 
1. Entity identification systems are software, and as 
such can be assessed by standard software testing 
techniques.  
2. Entity identification systems are in some sense 
instantiations of hypotheses about linguistic 
structure, and as such can be assessed by standard 
linguistic ?field methods.?  
This paper describes a methodology and a data set for 
utilizing the principles of software engineering and 
linguistic analysis to generate test suites that answer the 
right kinds of questions for developers and for end 
users.  Readers are invited to contribute their own data.   
Acknowledgments 
The authors gratefully acknowledge support for this 
work from NIH/NIAAA grant U01-AA13524-02; 
comments from Andrew E. Dolbey on an earlier 
version of this work; Philip V. Ogren for help with 
stochastic-POS-tagging-based system; the Center for 
Computational Pharmacology NLP reading group and 
the anonymous reviewers for insightful comments on 
the current version; and Fukuda et al, Ono et al, and 
Franz?n et al for generously making their systems 
publicly available. 
 
References 
Beizer, Boris (1990).  Software testing techniques, 2nd 
ed.  Van Nostrand Reinhold.   
Binder, Robert V. (1999).  Testing object-oriented 
systems: models, patterns, and tools.  Addison-
Wesley.   
Cohen, K. Bretonnel; Philip V. Ogren; Shuhei 
Kinoshita; and Lawrence Hunter (in submission).  
Entity identification in the molecular biology domain 
with a stochastic POS tagger.  ISMB 2004.   
Cole, Ronald; Joseph Mariani; Hans Uszkoreit; Annie 
Zaenen; and Victor Zue (1997).  Survey of the state of 
the art in human language technology.  Cambridge 
University Press.   
Franz?n, Kristofer; Gunnar Eriksson; Fredrik Olsson; 
Lars Asker; Per Lid?n; and Joakim C?ster (2002).  
Protein names and how to find them.  International 
Journal of Medical Informatics 67(1-3):49-61.   
Fukuda, K.; T. Tsunoda; A. Tamura; and T. Takagi 
(1997).  Toward information extraction: identifying 
protein names from biological papers.  Pacific 
Symposium on Biocomputing 1998, pp. 705-716.   
Harris, Zellig S. (1951).  Methods in structural 
linguistics.  University of Chicago Press.   
Hirschman, Lynette; and Inderjeet Mani (2003).  
Evaluation.  In Mitkov (2003), pp. 415-429.   
Hirschman, Lynette; and Henry S. Thompson (1997).  
Overview of evaluation in speech and natural 
language processing.  In Cole et al (1997), pp. 409-
414.   
Kaner, Cem; Hung Quoc Nguyen; and Jack Falk 
(1999).  Testing computer software, 2nd ed.  John 
Wiley & Sons.   
Kaner, Cem; James Bach; and Bret Pettichord (2002).  
Lessons learned in software testing: a context-driven 
approach.  John Wiley & Sons.   
Marick, Brian (1997).  The craft of software testing: 
subsystem testing including object-based and object-
oriented testing.  Prentice Hall.   
Mitkov, Ruslan (2003).  The Oxford Handbook of 
Computational Linguistics.  Oxford University Press.   
Nerbonne, John (1998).  Linguistic Databases.  CSLI 
Publications.   
Ohta, Tomoko; Yuka Tateisi; Jin-Dong Kim; Hideki 
Mima; and Jun-ichi Tsujii (2002).  The GENIA 
corpus: an annotated corpus in molecular biology.  
Proceedings of the Human Language Technology 
Conference.   
Oepen, Stephan; Klaus Netter; and Judith Klein (1998).  
TSNLP ? Test Suites for Natural Language 
Processing.  In Nerbonne (1998), pp. 13-36.   
Olsson, Fredrik; Gunnar Eriksson; Kristofer Franz?n; 
Lars Asker; and Per Lid?n (2002).  Notions of 
correctness when evaluating protein name taggers.  
Proceedings of the 19th International Conference on 
Computational Linguistics (COLING 2002), Taipei, 
Taiwan.   
Ono, Toshihide; Haretsugu Hishigaki; Akira Tanigami; 
and Toshihisa Takagi (2001).  Automated extraction 
of information on protein-protein interactions from 
the biological literature.  Bioinformatics 17(2):155-
161.   
Samarin, William J. (1967).  Field linguistics: a guide 
to linguistic field work.  Irvington.   
Tanabe, Lorraine; and W. John Wilbur (2002a).  
Tagging gene and protein names in biomedical text.  
Bioinformatics 18(8):1124-1132.   
Tanabe, Lorraine; and W. John Wilbur (2002b).  
Tagging gene and protein names in full text articles.  
Proceedings of the workshop on natural language 
processing in the biomedical domain, pp. 9-13.  
Association for Computational Linguistics.   
Volk, Martin (1998).  Markup of a test suite with 
SGML.  In Nerbonne (1998), pp. 59-76.   
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 33?40,
New York City, June 2006. c?2006 Association for Computational Linguistics
A Priority Model for Named Entities 
 
 
Lorraine Tanabe W. John Wilbur 
National Center for Biotechnology 
Information 
National Center for Biotechnology 
Information 
Bethesda, MD 20894 Bethesda, MD 20894 
tanabe@ncbi.nlm.nih.gov wilbur@ncbi.nlm.nih.gov 
  
 
 
 
Abstract 
We introduce a new approach to named 
entity classification which we term a Pri-
ority Model. We also describe the con-
struction of a semantic database called 
SemCat consisting of a large number of 
semantically categorized names relevant 
to biomedicine. We used SemCat as train-
ing data to investigate name classification 
techniques. We generated a statistical lan-
guage model and probabilistic context-
free grammars for gene and protein name 
classification, and compared the results 
with the new model.  For all three meth-
ods, we used a variable order Markov 
model to predict the nature of strings not 
represented in the training data.  The Pri-
ority Model achieves an F-measure of 
0.958-0.960, consistently higher than the 
statistical language model and probabilis-
tic context-free grammar.   
1 Introduction 
Automatic recognition of gene and protein names 
is a challenging first step towards text mining the 
biomedical literature. Advances in the area of gene 
and protein named entity recognition (NER) have 
been accelerated by freely available tagged corpora 
(Kim et al, 2003, Cohen et al, 2005, Smith et al, 
2005, Tanabe et al, 2005).  Such corpora have 
made it possible for standardized evaluations such 
as Task 1A of the first BioCreative Workshop 
(Yeh et al, 2005). 
Although state-of-the-art systems now perform 
at the level of 80-83% F-measure, this is still well 
below the range of 90-97% for non-biomedical 
NER.  The main reasons for this performance dis-
parity are 1) the complexity of the genetic nomen-
clature and 2) the confusion of gene and protein 
names with other biomedical entities, as well as 
with common English words. In an effort to allevi-
ate the confusion with other biomedical entities we 
have assembled a database consisting of named 
entities appearing in the literature of biomedicine 
together with information on their ontological 
categories. We use this information in an effort to 
better understand how to classify names as repre-
senting genes/proteins or not.  
2 Background  
A successful gene and protein NER system must 
address the complexity and ambiguity inherent in 
this domain.  Hand-crafted rules alone are unable 
to capture these phenomena in large biomedical 
text collections.  Most biomedical NER systems 
use some form of language modeling, consisting of 
an observed sequence of words and a hidden se-
quence of tags.  The goal is to find the tag se-
quence with maximal probability given the 
observed word sequence.  McDonald and Pereira 
(2005) use conditional random fields (CRF) to 
identify the beginning, inside and outside of gene 
and protein names.   GuoDong et al (2005) use an 
ensemble of one support vector machine and two 
Hidden Markov Models (HMMs).  Kinoshita et al 
(2005) use a second-order Markov model.  Dingare 
et al (2005) use a maximum entropy Markov 
model (MEMM) with large feature sets.   
33
NER is a difficult task because it requires both 
the identification of the boundaries of an entity in 
text, and the classification of that entity.  In this 
paper, we focus on the classification step.  Spasic 
et al (2005) use the MaSTerClass case-based rea-
soning system for biomedical term classification.  
MaSTerClass uses term contexts from an annotated 
corpus of 2072 MEDLINE abstracts related to nu-
clear receptors as a basis for classifying new 
terms.  Its set of classes is a subset of the UMLS 
Semantic Network (McCray, 1989), that does not 
include genes and proteins.  Liu et al (2002) clas-
sified terms that represent multiple UMLS con-
cepts by examining the conceptual relatives of the 
concepts.  Hatzivassiloglou et al (2001) classified 
terms known to belong to the classes Protein, Gene 
and/or RNA using unsupervised learning, achieving 
accuracy rates up to 85%.  The AZuRE system 
(Podowski et al, 2004) uses a separate modified 
Naive Bayes model for each of 20K genes.  A term 
is disambiguated based on its contextual similarity 
to each model. Nenadic et al (2003) recognized 
the importance of terminological knowledge for 
biomedical text mining. They used the C/NC-
methods, calculating both the intrinsic characteris-
tics of terms (such as their frequency of occurrence 
as substrings of other terms), and the context of 
terms as linear combinations.  These biomedical 
classification systems all rely on the context sur-
rounding named entities. While we recognize the 
importance of context, we believe one must strive 
for the appropriate blend of information coming 
from the context and information that is inherent in 
the name itself.  This explains our focus on names 
without context in this work.  
We believe one can improve gene and protein 
entity classification by using more training data 
and/or using a more appropriate model for names.  
Current sources of training data are deficient in 
important biomedical terminologies like cell line 
names.  To address this deficiency, we constructed 
the SemCat database, based on a subset of the 
UMLS Semantic Network enriched with categories 
from the GENIA Ontology (Kim et al 2003), and a 
few new semantic types. We have populated Sem-
Cat with over 5 million entities of interest from 
Figure 1. SemCat Physical Object Hierarchy.  White = UMLS SN, Light Grey = GENIA semantic 
types, Dark Grey = New semantic types. 
34
standard knowledge sources like the UMLS 
(Lindberg et al, 1993), the Gene Ontology (GO) 
(The Gene Ontology Consortium, 2000), Entrez 
Gene (Maglott et al, 2005), and GENIA, as well as 
from the World Wide Web.  In this paper, we use 
SemCat data to compare three probabilistic frame-
works for named entity classification.   
3 Methods 
We constructed the SemCat database of biomedical 
entities, and used these entities to train and test 
three probabilistic approaches to gene and protein 
name classification: 1) a statistical language model 
with Witten-Bell smoothing, 2) probabilistic con-
text-free grammars (PCFGs) and 3) a new ap-
proach we call a Priority Model for named entities. 
As one component in all of our classification algo-
rithms we use a variable order Markov Model for 
strings.   
3.1 SemCat Database Construction 
The UMLS Semantic Network (SN) is an ongoing 
project at the National Library of Medicine.  Many 
users have modified the SN for their own research 
domains.  For example, Yu et al (1999) found that 
the SN was missing critical components in the ge-
nomics domain, and added six new semantic types 
including Protein Structure and Chemical Com-
plex. We found that a subset of the SN would be 
sufficient for gene and protein name classification, 
and added some new semantic types for better cov-
erage.  We shifted some semantic types from 
suboptimal nodes to ones that made more sense 
from a genomics standpoint. For example, there 
were two problems with Gene or Genome. Firstly, 
genes and genomes are not synonymous, and sec-
ondly, placement under the semantic type Fully 
Formed Anatomical Structure is suboptimal from a 
genomics perspective. Since a gene in this context 
is better understood as an organic chemical, we 
deleted Gene or Genome, and added the GENIA 
semantic types for genomics entities under Or-
ganic Chemical. The SemCat Physical Object hier-
archy is shown in Figure 1.  Similar hierarchies 
exist for the SN Conceptual Entity and Event trees.  
A number of the categories have been supple-
mented with automatically extracted entities from 
MEDLINE, derived from regular expression pat-
tern matching.  Currently, SemCat has 77 semantic 
types, and 5.11M non-unique entries. Additional 
entities from MEDLINE are being manually classi-
fied via an annotation website.  Unlike the Ter-
mino database (Harkema et al (2004), which 
contains terminology annotated with morpho-
syntactic and conceptual information, SemCat cur-
rently consists of gazetteer lists only.  
For our experiments, we generated two sets of 
training data from SemCat, Gene-Protein (GP) and 
Not-Gene-Protein (NGP).  GP consists of specific 
terms from the semantic types DNA MOLECULE, 
PROTEIN MOLECULE, DNA FAMILY, 
PROTEIN FAMILY, PROTEIN COMPLEX and 
PROTEIN SUBUNIT.  NGP consists of entities 
from all other SemCat types, along with generic 
entities from the GP semantic types.  Generic enti-
ties were automatically eliminated from GP using 
pattern matching to manually tagged generic 
phrases like abnormal protein, acid domain, and 
RNA.  
Many SemCat entries contain commas and pa-
rentheses, for example, ?receptors, tgf beta.?  A 
better form for natural language processing would 
be ?tgf beta receptors.?  To address this problem, 
we automatically generated variants of phrases in 
GP with commas and parentheses, and found their 
counts in MEDLINE.  We empirically determined 
the heuristic rule of replacing the phrase with its 
second most frequent variant, based on the obser-
vation that the most frequent variant is often too 
generic.  For example, the following are the phrase 
variant counts for ?heat shock protein (dnaj)?: 
 
? heat shock protein (dnaj) 0 
? dnaj heat shock protein  84 
? heat shock protein  122954 
? heat shock protein dnaj  41 
 
Thus, the phrase kept for GP is dnaj heat shock 
protein.  
After purifying the sets and removing ambigu-
ous full phrases (ambiguous words were retained), 
GP contained 1,001,188 phrases, and NGP con-
tained 2,964,271 phrases. From these, we ran-
domly generated three train/test divisions of 90% 
train/10% test (gp1, gp2, gp3), for the evaluation.   
3.2    Variable Order Markov Model for Strings 
As one component in our classification algorithms 
we use a variable order Markov Model for strings.  
Suppose C represents a class and 1 2 3... nx x x x  repre-
35
sents a string of characters. In order to estimate the 
probability that 1 2 3... nx x x x  belongs to  we apply 
Bayes? Theorem to write 
C
 
( ) ( ) ( )( )1 2 31 2 3 1 2 3
... |
| ...
...
n
n
n
p x x x x C p C
p C x x x x
p x x x x
=      (1) 
 
Because ( )1 2 3... np x x x x does not depend on the 
class and because we are generally comparing 
probability estimates between classes, we ignore 
this factor in our calculations and concentrate our 
efforts on evaluating ( ) ( )1 2 3... |np x x x x C p C . 
First we write 
 
( ) (1 2 3 1 2 3 11... | | ... ,nn kkp x x x x C p x x x x x C?==? )k
)
     (2) 
 
which is an exact equality. The final step is to give 
our best approximation to each of the num-
bers ( 1 2 3 1| ... ,k kp x x x x x C? . To make these ap-
proximations we assume that we are given a set of 
strings and associated probabilities ( ){ } 1, Mi i is p =  
where for each i ,  and 0ip > ip  is assumed to 
represent the probability that  belongs to the 
class C .  Then for the given string 
is
1 2 3... nx x x x  and 
a given  we let  be the smallest integer for 
which 
k 1r ?
1 2...r r r kx x x x+ +  is a contiguous substring in 
at least one of the strings .  Now let is N?  be the 
set of all i  for which 1 2...r r r kx x x x+ +  is a substring 
of  and let  be the set of all  for which is N i
1 2... 1r r r kx x x x+ + ?  is a substring of . We set is
( )1 2 3 1| ... , ii Nk k
ii N
p
p x x x x x C
p
??
?
?
= ?? . (3) 
In some cases it is appropriate to assume that 
( )p C  is proportional to 1M ii p=?  or there may be 
other ways to make this estimate. This basic 
scheme works well, but we have found that we can 
obtain a modest improvement by adding a unique 
start character to the beginning of each string. This 
character is assumed to occur nowhere else but as 
the first character in all strings dealt with including 
any string whose probability we are estimating.  
This forces the estimates of probabilities near the 
beginnings of strings to come from estimates based 
on the beginnings of strings.  We use this approach 
in all of our classification algorithms. 
 
Table 1. Each fragment in the left column appears in the 
training data and the probability in the right column 
represents the probability of seeing the underlined por-
tion of the string given the occurrence of the initial un-
underlined portion of the string in a training string.  
GP 
!apoe 79.55 10??  
oe-e 32.09 10??  
e-epsilon 24.00 10??  
( )|p apoe epsilon GP?  117.98 10??  
( )|p GP apoe epsilon?  0.98448 
NGP 
!apoe 88.88 10??  
poe- 21.21 10??  
oe-e 26.10 10??  
e-epsilon 36.49 10??  
( )|p apoe epsilon NGP?  134.25 10??  
( )|p NGP apoe epsilon?  0.01552 
In Table 1, we give an illustrative example of 
the string apoe-epsilon which does not appear in 
the training data.  A PubMed search for apoe-
epsilon gene returns 269 hits showing the name is 
known. But it does not appear in this exact form in 
SemCat. 
3.3   Language Model with Witten-Bell Smooth-
ing 
A statistical n-gram model is challenged when a 
bigram in the test set is absent from the training 
set, an unavoidable situation in natural language 
due to Zipf?s law.  Therefore, some method for 
assigning nonzero probability to novel n-grams is 
required.  For our language model (LM), we used 
Witten-Bell smoothing, which reserves probability 
mass for out of vocabulary values (Witten and 
Bell, 1991, Chen and Goodman, 1998).  The dis-
counted probability is calculated as 
 
   
)...()...(#
)...(#
)...(?
1111
1
11
?+??+?
+?
?+? += iniini
ini
ini wwDww
ww
wwP    (4)   
 
36
where  is the number of distinct 
words that can appear after in the 
training data. Actual values assigned to tokens out-
side the training data are not assigned uniformly 
but are filled in using a variable order Markov 
Model based on the strings seen in the training 
data.  
)...( 11 ?+? ini wwD
11... ?+? ini ww
3.4   Probabilistic Context-Free Grammar 
The Probabilistic Context-Free Grammar 
(PCFG) or Stochastic Context-Free Grammar 
(SCFG) was originally formulated by Booth 
(1969).  For technical details we refer the reader to 
Charniak (1993). For gene and protein name classi-
fication, we tried two different approaches.  In the 
first PCFG method (PCFG-3), we used the follow-
ing simple productions: 
 
1) CATP ? CATP CATP 
2) CATP ? CATP postCATP 
3) CATP ? preCATP CATP 
 
CATP refers to the category of the phrase, GP 
or NGP.  The prefixes pre and post refer to begin-
nings and endings of the respective strings.  We 
trained two separate grammars, one for the positive 
examples, GP, and one for the negative examples, 
NGP.  Test cases were tagged based on their score 
from each of the two grammars. 
In the second PCFG method (PCFG-8), we 
combined the positive and negative training exam-
ples into one grammar.  The minimum number of 
non-terminals necessary to cover the training sets 
gp1-3 was six {CATP, preCATP, postCATP, Not-
CATP, preNotCATP, postNotCATP}. CATP 
represents a string from GP, and NotCATP repre-
sents a string from NGP.  We used the following 
production rules: 
 
1) CATP ? CATP CATP 
2) CATP ? CATP postCATP 
3) CATP ? preCATP CATP 
4) CATP ? NotCATP CATP 
5) NotCATP ? NotCATP NotCATP 
6) NotCATP ? NotCATP postNotCATP 
7) NotCATP? preNotCATP NotCATP 
8) NotCATP ? CATP NotCATP 
 
It can be seen that (4) is necessary for strings like 
?human p53,? and (8) covers strings like ?p53 
pathway.? 
     In order to deal with tokens that do not ap-
pear in the training data we use variable order 
Markov Models for strings. First the grammar is 
trained on the training set of names. Then any to-
ken appearing in the training data will have as-
signed to it the tags appearing on the right side of 
any rule of the grammar (essentially part-of-speech 
tags) with probabilities that are a product of the 
training.  We then construct a variable order 
Markov Model for each tag type based on the to-
kens in the training data and the assigned prob-
abilities for that tag type. These Models (three for 
PCFG-3 and six for PCFG-8) are then used to as-
sign the basic tags of the grammar to any token not 
seen in training.  In this way the grammars can be 
used to classify any name even if its tokens are not 
in the training data.  
3.5   Priority Model 
There are problems with the previous ap-
proaches when applied to names. For example, 
suppose one is dealing with the name ?human liver 
alkaline phosphatase? and class  represents pro-
tein names and class  anatomical names. In that 
case a language model is no more likely to favor 
 than . We have experimented with PCFGs 
and have found the biggest challenge to be how to 
choose the grammar. After a number of attempts 
we have still found problems of the ?human liver 
alkaline phosphatase? type to persist. 
1C
2C
1C 2C
The difficulties we have experienced with lan-
guage models and PCFGs have led us to try a dif-
ferent approach to model named entities. As a 
general rule in a phrase representing a named en-
tity a word to the right is more likely to be the head 
word or the word determining the nature of the 
entity than a word to the left. We follow this rule 
and construct a model which we will call a Priority 
Model. Let  be the set of training data (names) 
for class  and likewise  for . Let 
1T
1C 2T 2C { } At? ??  
denote the set of all tokens used in names con-
tained in . Then for each token 1T T? 2 ,  t A? ? ? , 
we assume there are associated two probabilities 
p?  and q?  with the interpretation that p?  is the 
37
probability that the appearance of the token t?  in a 
name indicates that name belongs to class  and 1C
q?  is the probability that t?  is a reliable indicator 
of the class of a name. Let  be 
composed of the tokens on the right in the given 
order. Then we compute the probability 
( ) ( ) ( )1 2 kn t t t? ? ?= ?
 
( ) ( ) ( )( ) ( ) ( ) ( )( )1 1 22 1 .| 1 1k kj i iijp C n p q q p q? ? ?? == == ? + ??? ?k jj i ?+
      (5)                                         
 
This formula comes from a straightforward in-
terpretation of priority in which we start on the 
right side of a name and compute the probability 
the name belongs to class  stepwise. If  is 
the rightmost token we multiple the reliability 
 times the significance 
1C ( )kt?
( )kq? ( )kp?  to obtain 
, which represents the contribution of 
. The remaining or unused probability is 
 and this is passed to the next token to the 
left, . The probability  is scaled by 
the reliability and then the significance of 
( ) ( )k kq p? ?
( )kt?
( )1 kq??
( )1kt? ? ( )1 kq??
( )1kt? ?  to 
obtain , which is the contri-
bution of  toward the probability that the 
name is of class . The remaining probability is 
now  and this is again 
passed to the next token to the left, etc. At the last 
token on the left the reliability is not used to scale 
because there are no further tokens to the left and 
only significance 
( ) ( ) ( )1(1 )k k kq q p? ? ??? 1?
)k?
( )1kt? ?
1C
( )( ) ( )(11 1kq q? ?? ?
( )1p?  is used.  
We want to choose all the parameters p?  and 
q?  to maximize the probability of the data. Thus 
we seek to maximize 
 
 ( )( ) ( )( )
1 2
1log | log 2 |n T n TF p C n p C n? ?= +? ? .
 (6) 
                 
Because probabilities are restricted to be in the 
interval [ ]0,1 , it is convenient to make a change of 
variables through the definitions 
,  
1 1
x y
x
ep q
e e
? ?
y
e
? ?? ?= =+ + . (7) 
Then it is a simple exercise to show that 
( ) (1 ,  1dp dqp p q q
dx dy
? ? )? ? ?
? ?
= ? = ? ? . (8) 
From (5), (6), and (8) it is straightforward to com-
pute the gradient of  as a function of F x?  and y?  
and because of (8) it is most naturally expressed in 
terms of p?  and q? . Before we carry out the op-
timization one further step is important. Let B  
denote the subset of A? ?  for which all the oc-
currences of t?  either occur in names in  or all 
occurrences occur in names in . For any such 
1T
2T ?  
we set 1q? =  and if all occurrences of t?  are in 
names in   we set 1T 1p? = , while if all occur-
rences are in names in  we set .  These 
choices are optimal and because of the form of (8) 
it is easily seen that 
2T 0p? =
0F F
x y? ?
? ?= =? ?  (9) 
for such an ? . Thus we may ignore all the B? ?  
in our optimization process because the values of 
p?  and q?  are already set optimally. We therefore 
carry out optimization of  using the F
,  ,  x y A? ? B? ? ? . For the optimization we have 
had good success using a Limited Memory BFGS 
method (Nash et al, 1991). 
 
When the optimization of  is complete we 
will have estimates for all the 
F
p?  and q? , A? ? . 
We still must deal with tokens t?  that are not in-
cluded among the t? .  For this purpose we train 
variable order Markov Models 1MP  based on the 
weighted set of strings ( ){ }, At p? ? ??  and 2MP  
based on ( ){ },1 At p? ? ??? . Likewise we train 
1MQ  based on ( ){ }, At q? ? ??  and 2MQ  based on 
( ){ },1 At q? ? ??? . Then if we allow ( )imp t?  to 
represent the prediction from model iMP  and ( )imq t?  that from model iMQ , we set 
38
 ( )
( ) ( )
( )
( ) ( )11 2 1 2,  
mp t mq t
p q
mp t mp t mq t mq t
?
? ?
? ? ?
= =+
1 ?
?+
 (10) 
This allows us to apply the priority model to 
any name to predict its classification based on 
equation 5.  
4 Results 
We ran all three methods on the SemCat sets gp1, 
gp2 and gp3.  Results are shown in Table 2.  For 
evaluation we applied the standard information 
retrieval measures precision, recall and F-measure.   
_
( _ _ )
rel retprecision
rel ret non rel ret
= + ?  
_
( _ _ _ )
rel retrecall
rel ret rel not ret
= +  
 
2* *
( )
precision recallF measure
precision recall
? = +  
 
For name classification, rel_ret refers to true posi-
tive entities, non-rel_ret to false positive entities 
and rel_ not_ret to false negative entities. 
 
Table 2. Three-fold cross validation results. P = Preci-
sion, R = Recall, F = F-measure. PCFG = Probabilistic 
Context-Free Grammar, LM = Bigram Model with Wit-
ten-Bell smoothing,  PM = Priority Model. 
Method Run P R F 
PCFG-3 gp1 0.883 0.934 0.908 
 gp2 0.882 0.937 0.909 
 gp3 0.877 0.936 0.906 
PCFG-8 gp1 0.939 0.966 0.952 
 gp2 0.938 0.967 0.952 
 gp3 0.939 0.966 0.952 
LM gp1 0.920 0.968 0.944 
 gp2 0.923 0.968 0.945 
 gp3 0.917 0.971 0.943 
PM gp1 0.949 0.968 0.958 
 gp2 0.950 0.968 0.960 
 gp3 0.950 0.967 0.958 
5 Discussion 
Using a variable order Markov model for strings 
improved the results for all methods (results not 
shown).  The gp1-3 results are similar within each 
method, yet it is clear that the overall performance 
of these methods is PM > PCFG-8 > LM > PCFG-
3. The very large size of the database and the very 
uniform results obtained over the three independ-
ent random splits of the data support this conclu-
sion.  
The improvement of PCFG-8 over PCFG-3 can 
be attributed to the considerable ambiguity in this 
domain. Since there are many cases of term over-
lap in the training data, a grammar incorporating 
some of this ambiguity should outperform one that 
does not. In PCFG-8, additional production rules 
allow phrases beginning as CATPs to be overall 
NotCATPs, and vice versa.   
The Priority Model outperformed all other meth-
ods using F-measure.  This supports our impres-
sion that the right-most words in a name should be 
given higher priority when classifying names.  A 
decrease in performance for the model is expected 
when applying this model to the named entity ex-
traction (NER) task, since the model is based on 
terminology alone and not on the surrounding 
natural language text.  In our classification experi-
ments, there is no context, so disambiguation is not 
an issue. However, the application of our model to 
NER will require addressing this problem. 
 SemCat has not been tested for accuracy, but 
we retain a set of manually-assigned scores that 
attest to the reliability of each contributing list of 
terms.  Table 2 indicates that good results can be 
obtained even with noisy training data.   
6 Conclusion 
In this paper, we have concentrated on the infor-
mation inherent in gene and protein names versus 
other biomedical entities.  We have demonstrated 
the utility of the SemCat database in training prob-
abilistic methods for gene and protein entity classi-
fication.  We have also introduced a new model for 
named entity prediction that prioritizes the contri-
bution of words towards the right end of terms. 
The Priority Model shows promise in the domain 
of gene and protein name classification.  We plan 
to apply the Priority Model, along with appropriate 
contextual and meta-level information, to gene and 
protein named entity recognition in future work.  
We intend to make SemCat freely available. 
 
39
Acknowledgements 
This research was supported in part by the Intra-
mural Research Program of the NIH, National Li-
brary of Medicine. 
References  
T. L. Booth.  1969.  Probabilistic representation of for-
mal languages.  In:  IEEE Conference Record of the 
1969 Tenth Annual Symposium on Switching and 
Automata Theory, 74-81. 
 
Stanley F. Chen and Joshua T. Goodman. 1998.  An 
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University. 
 
Eugene Charniak.  1993.  Statistical Language Learn-
ing.  The MIT Press,  Cambridge, Massachusetts. 
 
K. Bretonnel Cohen, Lynne Fox, Philip V. Ogren and 
Lawrence Hunter. 2005. Corpus design for biomedi-
cal natural language processing. Proceedings of the 
ACL-ISMB Workshop on Linking Biological Litera-
ture, Ontologies and Databases, 38-45. 
 
The Gene Ontology Consortium.  2000. Gene Ontology: 
tool for the unification of biology, Nat Genet. 25: 25-
29. 
Henk Harkema, Robert Gaizauskas, Mark Hepple, An-
gus Roberts, Ian Roberts, Neil Davis and Yikun Guo.  
2004.  A large scale terminology resource for bio-
medical text processing.  Proc BioLINK 2004, 53-60. 
Vasileios Hatzivassiloglou, Pablo A. Dubou? and An-
drey Rzhetsky.  2001.  Disambiguating proteins, 
genes, and RNA in text: a machine learning ap-
proach.  Bioinformatics 17 Suppl 1:S97-106. 
J.-D. Kim, Tomoko Ohta, Yuka Tateisi and Jun-ichi 
Tsujii. 2003. GENIA corpus--semantically annotated 
corpus for bio-textmining. Bioinformatics 19 Suppl 
1:i180-2.  
 
Donald A. Lindberg, Betsy L. Humphreys and Alexa T. 
McCray. 1993. The Unified Medical Language Sys-
tem. Methods Inf Med 32(4):281-91. 
 
Hongfang Liu, Stephen B. Johnson, and Carol Fried-
man.  2002.  Automatic resolution of ambiguous terms 
based on machine learning and conceptual relations in 
the UMLS.  J Am Med Inform Assoc 9(6): 621?636. 
 
Donna Maglott, Jim Ostell, Kim D. Pruitt and Tatiana 
Tatusova. 2005. Entrez Gene: gene-centered informa-
tion at NCBI. Nucleic Acids Res. 33:D54-8.   
 
Alexa T. McCray. 1989. The UMLS semantic network. 
In: Kingsland LC (ed). Proc 13rd Annu Symp Com-
put Appl Med Care. Washington, DC: IEEE Com-
puter Society Press, 503-7. 
 
Ryan McDonald and Fernando Pereira.  2005.  Identify-
ing gene and protein mentions in text using condi-
tional random fields.  BMC Bioinformatics 6 Supp 
1:S6. 
 
S. Nash and J. Nocedal. 1991. A numerical study of the 
limited memory BFGS method and the truncated-
Newton method for large scale optimization, SIAM J. 
Optimization1(3): 358-372. 
 
Goran Nenadic, Irena Spasic and Sophia Ananiadou.  
2003.  Terminology-driven mining of biomedical lit-
erature.  Bioinformatics 19:8, 938-943. 
 
Raf M. Podowski, John G. Cleary, Nicholas T. Gon-
charoff, Gregory Amoutzias and William S. Hayes.  
2004.  AZuRE, a scalable system for automated term 
disambiguation of gene and protein Names IEEE 
Computer Society Bioinformatics Conference, 415-
424. 
 
Lawrence H. Smith, Lorraine Tanabe, Thomas C. Rind-
flesch and W. John Wilbur. 2005. MedTag: A collec-
tion of biomedical annotations. Proceedings of the 
ACL-ISMB Workshop on Linking Biological Litera-
ture, Ontologies and Databases, 32-37.  
 
Lorraine Tanabe, Natalie Xie, Lynne H. Thom, Wayne 
Matten and W. John Wilbur. 2005. GENETAG: a 
tagged corpus for gene/protein named entity recogni-
tion. BMC Bioinformatics 6 Suppl 1:S3.  
 
I. Witten and T. Bell, 1991. The zero-frequency prob-
lem:  Estimating the probabilities of novel events in 
adaptive text compression. IEEE Transactions on In-
formation Theory 37(4).  
 
Alexander Yeh, Alexander Morgan, Mark Colosimo and 
Lynette Hirschman. 2005. BioCreAtIvE Task 1A: 
gene mention finding evaluation. BMC Bioinformat-
ics 6 Suppl 1:S2.  
 
Hong Yu, Carol Friedman, Andrey Rhzetsky and 
Pauline Kra. 1999. Representing genomic knowledge 
in the UMLS semantic network. Proc AMIA Symp. 
181-5. 
40
