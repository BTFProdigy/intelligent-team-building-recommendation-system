Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 1?9,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Third PASCAL Recognizing Textual Entailment Challenge 
 
Danilo Giampiccolo 
CELCT 
Via alla Cascata 56/c 
38100 POVO TN 
giampiccolo@celct.it 
Bernardo Magnini 
FBK-ITC 
Via Sommarive 18, 
38100 Povo TN 
magnini@itc.it  
Ido Dagan 
Computer Science Department 
Bar-Ilan University 
Ramat Gan 52900, Israel 
dagan@macs.biu.ac.il 
Bill Dolan 
Microsoft Research 
Redmond, WA, 98052, USA 
billdol@microsoft.com 
Abstract 
This paper presents the Third PASCAL 
Recognising Textual Entailment Chal-
lenge (RTE-3), providing an overview of 
the dataset creating methodology and the 
submitted systems. In creating this 
year?s dataset, a number of longer texts 
were introduced to make the challenge 
more oriented to realistic scenarios. Ad-
ditionally, a pool of resources was of-
fered so that the participants could share 
common tools. A pilot task was also set 
up, aimed at differentiating unknown en-
tailments from identified contradictions 
and providing justifications for overall 
system decisions. 26 participants submit-
ted 44 runs, using different approaches 
and generally presenting new entailment 
models and achieving higher scores than 
in the previous challenges. 
1.1 The RTE challenges 
 
The goal of the RTE challenges has been to cre-
ate a benchmark task dedicated to textual en-
tailment ? recognizing that the meaning of one 
text is entailed, i.e. can be inferred, by another1. 
In the recent years, this task has raised great in-
terest since applied semantic inference concerns 
many practical Natural Language Processing 
(NLP) applications, such as Question Answering 
(QA), Information Extraction (IE), Summariza-
tion, Machine Translation and Paraphrasing, and 
certain types of queries in Information Retrieval 
(IR). More specifically, the RTE challenges 
have aimed to focus research and evaluation on 
this common underlying semantic inference task 
and separate it from other problems that differ-
ent NLP applications need to handle. For exam-
ple, in addition to textual entailment, QA sys-
tems need to handle issues such as answer re-
trieval and question type recognition.  
By separating out the general problem of tex-
tual entailment from these task-specific prob-
lems, progress on semantic inference for many 
application areas can be promoted. Hopefully, 
research on textual entailment will finally lead to 
the development of entailment ?engines?, which 
can be used as a standard module in many appli-
cations (similar to the role of part-of-speech tag-
gers and syntactic parsers in current NLP appli-
cations). 
In the following sections, a detailed descrip-
tion of RTE-3 is presented. After a quick review 
                                                 
1
 The task was first defined by Dagan and Glickman 
(2004). 
1
of the previous challenges (1.2), section 2 de-
scribes the preparation of the dataset. In section 
3 the evaluation process and the results are pre-
sented, together with an analysis of the perform-
ance of the participating systems. 
1.2  The First and Second RTE Challenges 
 
The first RTE challenge2 aimed to provide the 
NLP community with a new benchmark to test 
progress in recognizing textual entailment, and 
to compare the achievements of different groups. 
This goal proved to be of great interest, and the 
community's response encouraged the gradual 
expansion of the scope of the original task. 
The Second RTE challenge3 built on the suc-
cess of the first, with 23 groups from around the 
world (as compared to 17 for the first challenge) 
submitting the results of their systems. Repre-
sentatives of participating groups presented their 
work at the PASCAL Challenges Workshop in 
April 2006 in Venice, Italy. The event was suc-
cessful and the number of participants and their 
contributions to the discussion demonstrated that 
Textual Entailment is a quickly growing field of 
NLP research. In addition, the workshops 
spawned an impressive number of publications 
in major conferences, with more work in pro-
gress. Another encouraging sign of the growing 
interest in the RTE challenge was represented by 
the increase in the number of downloads of the 
challenge datasets, with about 150 registered 
downloads for the RTE-2 development set. 
1.3 The Third Challenge 
 
RTE-3 followed the same basic structure of the 
previous campaigns, in order to facilitate the 
participation of newcomers and to allow "veter-
ans" to assess the improvements of their systems 
in a comparable test exercise. Nevertheless, 
some innovations were introduced, on the one 
hand to make the challenge more stimulating 
and, on the other, to encourage collaboration 
between system developers. In particular, a lim-
ited number of longer texts, i.e. up to a para-
graph in length, were incorporated in order to 
move toward more comprehensive scenarios, 
                                                 
2
 http://www.pascal-network.org/Challenges/RTE/. 
3
 http://www.pascal-network.org/Challenges/RTE2./ 
which incorporate the need for discourse analy-
sis. However, the majority of examples re-
mained similar to those in the previous chal-
lenges, providing pairs with relatively short 
texts.  
Another innovation was represented by a re-
source pool4, where contributors had the possi-
bility to share the resources they used. In fact, 
one of the key conclusions at the second RTE 
Challenge Workshop was that entailment model-
ing requires vast knowledge resources that cor-
respond to different types of entailment reason-
ing. Moreover, entailment systems also utilize 
general NLP tools such as POS taggers, parsers 
and named-entity recognizers, sometimes posing 
specialized requirements to such tools. In re-
sponse to these demands, the RTE Resource 
Pool was built, which may serve as a portal and 
forum for publicizing and tracking resources, 
and reporting on their use.  
In addition, an optional pilot task, called "Ex-
tending the Evaluation of Inferences from Texts" 
was set up by the US National Institute of Stan-
dards and Technology (NIST), in order to ex-
plore two other sub-tasks closely related to tex-
tual entailment: differentiating unknown entail-
ments from identified contradictions and provid-
ing justifications for system decisions. In the 
first sub-task, the idea was to drive systems to 
make more precise informational distinctions, 
taking a three-way decision between "YES", 
"NO" and "UNKNOWN?, so that a hypothesis 
being unknown on the basis of a text would be 
distinguished from a hypothesis being shown 
false/contradicted by a text. As for the other sub-
task, the goal for providing justifications for de-
cisions was to explore how eventual users of 
tools incorporating entailment can be made to 
understand how decisions were reached by a 
system, as users are unlikely to trust a system 
that gives no explanation for its decisions. The 
pilot task exploited the existing RTE-3 Chal-
lenge infrastructure and evaluation process by 
using the same test set, while utilizing human 
assessments for the new sub-tasks. 
                                                 
4 http://aclweb.org/aclwiki/index.php?title=Textual_Entail 
ment_Resource_Pool. 
2
 Table 1: Some examples taken from the Development Set. 
 
2 The RTE-3 Dataset 
2.1 Overview 
 
The textual entailment recognition task required the 
participating systems to decide, given two text 
snippets t and h, whether t entails h. Textual en-
tailment is defined as a directional relation between 
two text fragments, called text (t, the entailing 
text), and hypothesis (h, the entailed text), so that a 
human being, with common understanding of lan-
guage and common background knowledge, can 
infer that h is most likely true on the basis of the 
content of t. 
As in the previous challenges, the RTE-3 dataset 
consisted of 1600 text-hypothesis pairs, equally 
divided into a development set and a test set. While 
the length of the hypotheses (h) was  the same as in 
the past datasets, a certain number of texts (t) were 
longer than in previous datasets, up to a paragraph. 
The longer texts were marked as L, after being se-
lected automatically when exceeding 270 bytes. In 
the test set they were about 17% of the total.  
As in RTE-2, four applications ? namely IE, IR, 
QA and SUM ? were considered as settings or con-
texts for the pairs generation (see 2.2 for a detailed 
description). 200 pairs were selected for each ap-
plication in each dataset. Although the datasets 
were supposed to be perfectly balanced, the num-
ber of negative examples were slightly higher in 
both development and test sets (51.50% and 
51.25% respectively; this was unintentional). Posi-
tive entailment examples, where t entailed h, were 
annotated YES; the negative ones, where entailment 
did not hold, NO. Each pair was annotated with its 
TASK TEXT HYPOTHESIS ENTAILMENT 
IE At the same time the Italian digital rights group, Elec-
tronic Frontiers Italy, has asked the nation's government 
to investigate Sony over its use of anti-piracy software. 
Italy's govern-
ment investigates 
Sony. 
NO 
IE Parviz Davudi was representing Iran at a meeting of the 
Shanghai Co-operation Organisation (SCO), the fledg-
ling association that binds Russia, China and four for-
mer Soviet republics of central Asia together to fight 
terrorism 
China is a mem-
ber of SCO. 
YES 
IR Between March and June, scientific observers say, up to 
300,000 seals are killed. In Canada, seal-hunting means 
jobs, but opponents say it is vicious and endangers the 
species, also threatened by global warming 
Hunting endan-
gers seal species. 
YES 
IR The Italian parliament may approve a draft law allow-
ing descendants of the exiled royal family to return 
home. The family was banished after the Second World 
War because of the King's collusion with the fascist 
regime, but moves were introduced this year to allow 
their return. 
Italian royal fam-
ily returns home. 
NO 
QA Aeschylus is often called the father of Greek tragedy; 
he wrote the earliest complete plays which survive from 
ancient Greece. He is known to have written more than 
90 plays, though only seven survive. The most famous 
of these are the trilogy known as Orestia. Also well-
known are The Persians and Prometheus Bound. 
"The Persians" 
was written by 
Aeschylus. 
YES 
SUM A Pentagon committee and the congressionally char-
tered Iraq Study Group have been preparing reports for 
Bush, and Iran has asked the presidents of Iraq and 
Syria to meet in Tehran. 
Bush will meet 
the presidents of 
Iraq and Syria in 
Tehran. 
NO 
3
related task (IE/IR/QA/SUM) and entailment 
judgment (YES/NO, obviously released only in the 
development set). Table 1 shows some examples 
taken from the development set. 
The examples in the dataset were based mostly 
on outputs (both correct and incorrect) of Web-
based systems. In order to avoid copyright prob-
lems, input data was limited to either what had al-
ready been publicly released by official competi-
tions or else was drawn from freely available 
sources such as WikiNews and Wikipedia. 
In choosing the pairs, the following judgment 
criteria and guidelines were considered: 
 
? As entailment is a directional relation, the 
hypothesis must be entailed by the given 
text, but the text need not be entailed by 
the hypothesis. 
? The hypothesis must be fully entailed by 
the text. Judgment must be NO if the hy-
pothesis includes parts that cannot be in-
ferred from the text. 
? Cases in which inference is very probable 
(but not completely certain) were judged as 
YES.  
? Common world knowledge was assumed, 
e.g. the capital of a country is situated in 
that country, the prime minister of a state is 
also a citizen of that state, and so on. 
2.2 Pair Collection 
 
As in RTE-2, human annotators generated t-h pairs 
within 4 application settings.  
 
The IE task was inspired by the Information Ex-
traction (and Relation Extraction) application, 
where texts and structured templates were replaced 
by t-h pairs. As in the 2006 campaign, the pairs 
were generated using four different approaches: 
1) Hypotheses were taken from the relations 
tested in the ACE-2004 RDR task, while 
texts were extracted from the outputs of ac-
tual IE systems, which were provided with 
relevant news articles. Correctly extracted  
instances were used to generate positive 
examples and incorrect instances to gener-
ate negative examples. 
2) The same procedure was followed using 
output of IE systems on the dataset of the 
MUC-4 TST3 task, in which the events are 
acts of terrorism. 
3) The annotated MUC-4 dataset and the 
news articles were also used to manually 
generate entailment pairs based on ACE re-
lations.  
4) Hypotheses corresponding to relations not 
found in the ACE and MUC datasets  were 
used both to be given to IE systems and to 
manually generate t-h pairs from collected 
news articles. Examples of these relations, 
taken from various semantic fields, were 
?X beat Y?, ?X invented Y?, ?X steal Y? 
etc. 
 
The common aim of all these processes was to 
simulate the need of IE systems to recognize that 
the given text indeed entails the semantic relation 
that is expected to hold between the candidate tem-
plate slot fillers.  
 
In the IR (Information Retrieval) application set-
ting, the hypotheses were propositional IR queries, 
which specify some statement, e.g. ?robots are 
used to find avalanche victims?. The hypotheses 
were adapted and simplified from standard IR 
evaluation datasets (TREC and CLEF). Texts (t) 
that did or did not entail the hypotheses were se-
lected from documents retrieved by different search 
engines (e.g. Google, Yahoo and MSN) for each 
hypothesis. In this application setting it was as-
sumed that relevant documents (from an IR per-
spective) should entail the given propositional hy-
pothesis. 
 
For the QA (Question Answering) task, annotators 
used questions taken from the datasets of official 
QA competitions, such as TREC QA and 
QA@CLEF datasets, and the corresponding an-
swers extracted from the Web by actual QA sys-
tems. Then they transformed the question-answer 
pairs into t-h pairs as follows: 
 
? An answer term of the expected answer 
type was picked from the answer passage -
either a correct or an incorrect one.  
? The question was turned into an affirma-
tive sentence plugging in the answer term. 
? t-h pairs were generate, using the affirma-
tive sentences as hypotheses (h?s) and the 
original answer passages as texts (t?s).  
4
For example, given the question ?How high is 
Mount Everest?? and a text (t) ?The above men-
tioned expedition team comprising of 10 members 
was permitted to climb 8848m. high Mt. Everest 
from Normal Route for the period of 75 days from 
15 April, 2007 under the leadership of Mr. Wolf 
Herbert of Austria?, the annotator, extracting the 
piece of information ?8848m.? from the text, 
would turn the question into an the affirmative sen-
tence ?Mount Everest is 8848m high?, generating a 
positive entailment pair. This process simulated the 
need of a QA system to verify that the retrieved 
passage text actually entailed the provided answer. 
 
In the SUM (Summarization) setting, the 
entailment pairs were generated using two proce-
dures. 
In the first one, t?s and h?s were sentences taken 
from a news document cluster, a collection of news 
articles that describe the same news item. Annota-
tors were given the output of multi-document 
summarization systems -including the document 
clusters and the summary generated for each clus-
ter. Then they picked sentence pairs with high lexi-
cal overlap, preferably where at least one of the 
sentences was taken from the summary (this sen-
tence usually played the role of t). For positive ex-
amples, the hypothesis was simplified by removing 
sentence parts, until it was fully entailed by t. 
Negative examples were simplified in a similar 
manner. In alternative, ?pyramids? produced for 
the experimental evaluation mehod in DUC 2005 
(Passonneau et al 2005) were exploited. In this 
new evaluation method, humans select sub-
sentential content units (SCUs) in several manually 
produced summaries on a subject, and collocate 
them in a ?pyramid?, which has at the top the 
SCUs with the higher frequency, i.e. those which 
are present in most summaries. Each SCU is identi-
fied by a label, a sentence in natural language 
which expresses the content. Afterwards, the anno-
tators individuate the SCUs present in summaries 
generated automatically (called peers), and link 
them to the ones present in the pyramid, in order to 
assign each peer a weight. In this way, the SCUs in 
the automatic summaries linked to the SCUs in the 
higher tiers of the pyramid are assigned a heavier 
weight than those at the bottom. For the SUM set-
ting, the RTE-3 annotators selected relevant pas-
sages from the peers and used them as T?s, mean-
while the labels of the corresponding SCUs were 
used as H?s. Small adjustments were allowed, 
whenever the texts were not grammatically accept-
able. This process simulated the need of a summa-
rization system to identify information redundancy, 
which should be avoided in the summary. 
2.3 Final dataset  
 
Each pair of the dataset was judged by three anno-
tators. As in previous challenges, pairs on which 
the annotators disagreed were filtered-out.  
On the test set, the average agreement between 
each pair of annotators who shared at least 100 ex-
amples was 87.8%, with an average Kappa level of 
0.75, regarded as substantial agreement according 
to Landis and Koch (1997).  
19.2 % of the pairs in the dataset were removed 
from the test set due to disagreement. The dis-
agreement was generally due to the fact that the h 
was more specific than the t, for example because 
it contained more information, or made an absolute 
assertion where t proposed only a personal opinion. 
In addition, 9.4 % of the remaining pairs were dis-
carded, as they seemed controversial, too difficult, 
or too similar when compared to other pairs.  
As far as the texts extracted from the web are 
concerned, spelling and punctuation errors were 
sometimes fixed by the annotators, but no major 
change was allowed, so that the language could be 
grammatically and stylistically imperfect. The hy-
potheses were finally double-checked by a native 
English speaker. 
3 The RTE-3 Challenge 
3.1 Evaluation measures 
 
The evaluation of all runs submitted in RTE-3 was 
automatic. The judgments (classifications) returned 
by the system were compared to the Gold Standard 
compiled by the human assessors. The main 
evaluation measure was accuracy, i.e. the percent-
age of matching judgments. 
For systems that provided a confidence-ranked 
list of the pairs, in addition to the YES/NO judg-
ment, an Average Precision measure was also 
computed. This measure evaluates the ability of 
systems to rank all the T-H pairs in the test set ac-
cording to their entailment confidence (in decreas-
ing order from the most certain entailment to the 
least certain). Average precision is computed as the 
5
average of the system's precision values at all 
points in the ranked list in which recall increases, 
that is at all points in the ranked list for which the 
gold standard annotation is YES, or, more for-
mally:  
 
?
=
?n
i i
iUpToPairEntailmentiE
R 1
)(#)(1
          (1) 
 
where n is the number of the pairs in the test set, R 
is the total number of positive pairs in the test set, 
E(i) is 1 if the i-th pair is positive and 0 otherwise, 
and i ranges over the pairs, ordered by their rank-
ing.  
In other words, the more the system was confi-
dent that t entails h, the higher was the ranking of 
the pair. A perfect ranking would have placed all 
the positive pairs (for which the entailment holds) 
before all the negative ones, yielding an average 
precision value of 1. 
3.2 Submitted systems 
 
Twenty-six teams participated in the third chal-
lenge, three more than in previous year. Table 2 
presents the list of the results of each submitted 
runs and the components used by the systems. 
Overall, we noticed a move toward deep ap-
proaches, with a general consolidation of ap-
proaches based on the syntactic structure of Text 
and Hypothesis. There is an evident increase of 
systems using some form of logical inferences (at 
least seven systems). However, these approaches, 
with few notably exceptions, do not seem to be 
consolidated enough, as several systems show re-
sults  not still at the state of art (e.g. Natural Logic 
introduced by Chambers et al). For many systems 
an open issue is the availability and integration of 
different and complex semantic resources-  
A more extensive and fine grained use of spe-
cific semantic phenomena is also emerging. As an 
example, Tatu and Moldovan carry on a sophisti-
cated analysis of named entities, in particular Per-
son names, distinguishing first names from last 
names. Some form of relation extraction, either 
through manually built patterns (Chambers et al) 
or through the use of an information extraction sys-
tem (Hickl and Bensley) have been introduced this 
year, even if still on a small scale (i.e. few rela-
tions).  
On the other hand, RTE-3 confirmed that both 
machine learning using lexical-syntactic features 
and transformation-based approaches on depend-
ency representations are well consolidated tech-
niques to address textual entailment. The extension 
of transformation-based approaches toward prob-
abilistic settings is an interesting direction investi-
gated by some systems (e.g. Harmeling). On the 
side of ?light? approaches to textual entailment, 
Malakasiotis and Androutpoulos provide a useful 
baseline for the task (0.61%) using only POS tag-
ging and then applying string-based measures to 
estimate the similarity between Text and Hypothe-
sis. 
As far as resources are concerned, lexical data-
bases (mostly WordNet and DIRT) are still widely 
used. Extended WordNet is also a common re-
source (for instance in Iftene and Balahur-
Dobrescu) and the Extended Wordnet Knowledge 
Base has been successfully used in (Tatu and 
Moldovan). Verb-oriented resources are also 
largely present in several systems, including Fra-
menet (e.g. Burchardt et al), Verbnet (Bobrow et 
al.) and Propbank (e.g. Adams et al). It seems that 
the use of the Web as a resource is more limited 
when compared to the previous RTE workshop. 
However, as in RTE-2, the use of large semantic 
resources is still a crucial factor affecting the per-
formance of systems (see, for instance, the use of a 
large corpus of entailment examples in Hickl and 
Bensley).  
Finally, an interesting aspect is that, stimulated 
by the percentage of longer texts included this year, 
a number of participating systems addressed anaph-
ora resolution (e.g. Delmonte, Bar-Haim et al, 
Iftene and Balahur-Dobrescu). 
3.3 Results 
 
The accuracy achieved by the participating sys-
tems ranges from 49% to 80% (considering the best 
run of each group), while most of the systems ob-
tained a score in between 59% and 66%. One sub-
mission, Hickl and Bensley achieved 80% accu-
racy, scoring 8% higher than the second system 
(Tatu and Moldovan, 72%), and obtaining the best 
absolute result achieved in the three RTE chal-
lenges. 
6
 Table 2: Submission results and components of the systems.
 . 
System Components 
First Author Accuracy 
Average 
precision L
ex
ic
al
 
R
el
at
io
n
,
 
W
o
rd
N
et
 
 
n
-
gr
am
\w
o
rd
 
sim
ila
rit
y 
Sy
n
ta
ct
ic
 
M
at
ch
-
in
g\
A
lig
n
in
g 
Se
m
an
tic
 
R
o
le
 
La
be
lin
g\
 
Fr
am
en
et
\P
ro
ba
n
k,
 
V
er
bn
et
 
Lo
gi
ca
l I
n
fe
re
n
ce
 
Co
rp
u
s/ 
W
eb
-
ba
se
d 
St
at
ist
ic
s,
 
LS
A
 
M
L 
Cl
as
sif
ic
at
io
n
 
A
n
ap
ho
ra
 
re
so
lu
tio
n
 
 
En
ta
ilm
en
t 
Co
rp
o
ra
 
?
 
D
IR
T 
Ba
ck
gr
o
u
n
d 
K
n
o
w
le
dg
e 
Adams 0.6700  X X    X X   
0.6112 0.6118 X  X   X  X X Bar-Haim 
0.5837 0.6093  X  X   X  X  
Baral 0.4963 0.5364 X    X    X 
0.6050 0.5897 X  X    X   Blake 
  0.6587 0.6096 X  X    X   
0.5112 0.5720  X   X X     Bobrow 
  0.5150 0.5807 X   X X     
0.6250  X  X X      Burchardt 
0.6262           
0.5500   X    X    Burek 
0.5500 0.5514          
0.6050 0.6341 X  X  X  X X  Chambers 
  0.6362 0.6527 X  X  X  X X  
0.5088 0.4961  X   
 
 X    X Clark  
0.4725 0.4961  X    X    X 
Delmonte 0.5875 0.5830 X  X X X   X  
0.6563  X X X       Ferrandez 
0.6375           
0.6062  X X     X   Ferr?s 
0.6150  X X     X   
0.5600 0.5813 X  X    X   Harmling 
0.5775 0.5952 X  X    X   
Hickl 0.8000 0.8815 X X   X  X X X 
0.6913  X  X      X Iftene 
0.6913  X  X      X 
0.6400  X X     X   Li 
0.6488           
Litkowski   0.6125           
Malakasiotis  0.6175 0.6808  X     X   
Marsi 0.5913    X      X 
0.5888  X X X    X   Montejo-R?ez 
0.6038  X X X    X   
0.6238  X X X    X   Rodrigo 
0.6312  X X X    X   
0.6262  X X       X Roth 
0.5975    X     X  
0.6100 0.6195 X X     X   Settembre 
  0.6262 0.6274 X X     X   
0.7225 0.6942 X    X   X X Tatu 
  0.7175 0.6797 X    X   X  
0.6650    X    X   Wang  
0.6687           
0.6675 0.6674 X  X    X   Zanzotto 
  0.6575 0.6732 X  X    X   
7
As far as the per-task results are concerned, the 
trend registered in RTE-2 was confirmed, in that 
there was a marked difference in the performances 
obtained in different task settings. 
In fact, the average accuracy achieved in the QA 
setting (0.71) was 20 points higher than that 
achieved in the IE setting (0.52); the average accu-
racy in the IR and Sum settings was 0.66 and 0.58 
respectively. In RTE-2 the best results were 
achieved in SUM, while the lower score was al-
ways recorded in IE. As already pointed out by 
Bar-Haim (2006), these differences should be fur-
ther investigated, as they could lead to a sensible 
improvement of the performance. 
As for the LONG pairs, which represented a 
new element of this year?s challenge, no substan-
tial difference was noted in the systems? perform-
ances: the average accuracy over the long pairs 
was 58.72%, compared to 61.93% over the short 
ones.  
4 Conclusions and future work 
 
At its third round, the Recognizing Textual En-
tailment task has reached a noticeable level of ma-
turity, as the very high interest in the NLP commu-
nity and the continuously increasing number of 
participants in the challenges demonstrate. The 
relevance of Textual Entailment Recognition to 
different applications, such as the AVE5 track at 
QA at CLEF6, has also been acknowledged. Fur-
thermore, the debates and the numerous publica-
tions about the Textual Entailment have contrib-
uted to the better understanding the task and its 
nature.  
To keep a good balance between the consoli-
dated main task and the need for moving forward, 
longer texts were introduced in the dataset, in order 
to make the task more challenging, and a pilot task 
was proposed. The Third RTE Challenge have also 
confirmed that the methodology for the creation of 
the datasets, developed in the first two campaigns, 
is robust. Overall, the transition of the challenge 
coordination from Bar-Ilan ?which organized the 
first two challenges- to CELCT was successful, 
though some problems were encountered, espe-
cially in the preparation of the data set. The sys-
                                                 
5
 http://nlp.uned.es/QA/ave/. 
6
 http://clef-qa.itc.it/. 
tems which took part in RTE-3 showed that the 
technology applied to Entailment Recognition has 
made significant progress, confirmed by the results, 
which were generally better than last year. In par-
ticular, visible progress in defining several new 
principled scenarios for RTE was represented, such 
as Hickl?s commitment-based approach, Bar 
Haim?s proof system, Harmeling?s probabilistic 
model, and Standford?s use of Natural Logic. 
If, on the one hand, the success that RTE has 
had so far is very encouraging, on the other, it in-
cites to overcome certain current limitations, and to 
set realistic and, at the same time, stimulating goals 
for the future. First at all, theoretical refinements 
both of the task and the models applied to it need 
to be developed. In particular, more efforts are re-
quired to improve knowledge acquisition, as little 
progress has been made on this front so far. Also 
the data set generation and the evaluation method-
ology  need to be refined and extended. A major 
problem in the current setting of the data collection 
is that the distribution of the examples is arbitrary 
to a large extent, being determined by manual se-
lection. Therefore new evaluation methodologies, 
which can reflect realistic distributions should be 
investigated, as well as the possibility of evaluating 
Textual Entailment Recognition within additional 
concrete application scenarios, following the spirit 
of the QA Answer Validation Exercise.  
 
 
Acknowledgments 
 
The following sources were used in the preparation 
of the data: 
 
? PowerAnswer question answering system, from 
Language Computer Corporation, provided by Dan 
Moldovan and Marta Tatu. 
http://www.languagecomputer.com/solutions/question answer-
ing/power answer/ 
 
? Cicero Custom and Cicero Relation information 
extraction systems, from Language Computer Cor-
poration, provided by Sanda M. Harabagiu, An-
drew Hickl, John Lehmann and  and Paul Aarseth. 
http://www.languagecomputer.com/solutions/information_ext
action/cicero/index.html 
 
? Columbia NewsBlaster multi-document summa-
rization system, from the Natural Language Proc-
8
essing group at Columbia University?s Departmen-
tof Computer Science. 
http://newsblaster.cs.columbia.edu/ 
 
? NewsInEssence multi-document summarization 
system provided by Dragomir R. Radev and Jahna 
Otterbacher from the Computational Linguistics 
and Information Retrieval research group, Univer-
sity of Michigan. 
http://www.newsinessence.com 
 
? New York University?s information extraction 
system, provided by Ralph Grishman, Department 
of Computer Science, Courant Institute of Mathe-
matical Sciences, New York University. 
 
? MUC-4 information extraction dataset, from the 
National Institute of Standards and Technology 
(NIST).  
http://www.itl.nist.gov/iaui/894.02/related projects/muc/ 
 
? ACE 2004 information extraction templates, 
from the National Institute of Standards and Tech-
nology (NIST). 
http://www.nist.gov/speech/tests/ace/ 
 
? TREC IR queries and TREC-QA question collec-
tions, from the National Institute of Standards and 
Technology (NIST). 
http://trec.nist.gov/ 
 
? CLEF IR queries and CLEF-QA question collec-
tions, from DELOS Network of Excellence  
for Digital Libraries. 
 http://www.clef-campaign.org/, http://clef-qa.itc.it/ 
 
? DUC 2005 annotated peers, from Columbia Uni-
versity, NY, provided by Ani Nenkova. 
http://www1.cs.columbia.edu/~ani/DUC2005/ 
 
We would like to thank the people and organiza-
tions that made these sources available for the 
challenge. In addition, we thank Idan Szpektor and 
Roy Bar Haim from Bar-Ilan University  for their 
assistance and advice, and Valentina Bruseghini 
from CELCT for managing the RTE-3 website. 
 
We would also like to acknowledge the people 
and organizations involved in creating and annotat-
ing the data: Pamela Forner, Errol Hayman, Cam-
eron Fordyce from CELCT and Courtenay 
Hendricks, Adam Savel and Annika Hamalainen 
from the Butler Hill Group, which was funded by 
Microsoft Research. 
 
This work was supported in part by the IST Pro-
gramme of the European Community, under the 
PASCAL Network of Excellence, IST-2002-
506778. We wish to thank the managers of the 
PASCAL challenges program, Michele Sebag and 
Florence d?Alche-Buc, for their efforts and sup-
port, which made this challenge possible. We also 
thank David Askey, who helped manage the RTE 3 
website.  
 
References 
 
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, 
Danilo Giampiccolo, Bernardo Magnini and Idan 
Szpektor. 2006. The Second PASCAL Recognizing 
Textual Entailment Challenge. In Proceedings of the 
Second PASCAL Challenges Workshop on Recog-
nizing Textual Entailment, Venice, Italy. 
Ido Dagan, Oren Glickman, and Bernardo Magnini. 
2006. The PASCAL Recognizing Textual Entailment 
Challenge. In Qui?onero-Candela et al, editors, 
MLCW 2005, LNAI Volume 3944, pages 177-190. 
Springer-Verlag. 
J. R. Landis and G. G. Koch. 1997. The measurements 
of observer agreement for categorical data. Biomet-
rics, 33:159?174. 
Rebecca Passonneau, Ani Nenkova., Kathleen McKe-
own, and Sergey Sigleman. 2005. Applying the 
pyramid method in DUC 2005. In Proceedings of the 
Document Understanding Conference (DUC 05), 
Vancouver, B.C., Canada. 
Ellen M. Voorhees and Donna Harman. 1999. Overview 
of the seventh text retrieval conference. In Proceed-
ings of the Seventh Text Retrieval Conference 
(TREC-7). NIST Special Publication. 
 
 
9
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 195?198,
Prague, June 2007. c?2007 Association for Computational Linguistics
IRST-BP: Web People Search Using Name Entities 
 
Octavian Popescu 
FBK-irst, Trento (Italy) 
popescu@itc.it 
Bernardo Magnini 
FBK-irst, Trento (Italy) 
magnini@itc.it 
 
 
Abstract 
In this paper we describe a person clus-
tering system for web pages and report 
the results we have obtained on the test 
set of the Semeval 2007 Web Person 
Search task. Deciding which particular 
person a name refers to within a text 
document depends mainly on the capac-
ity to extract the relevant information 
out of texts when it is present. We con-
sider ?relevant? here to stand primarily 
for two properties: (1) uniqueness and 
(2) appropriateness. In order to address 
both (1) and (2) our method gives pri-
mary importance to Name Entities 
(NEs), defined according to the ACE 
specifications. The common nouns not 
referring to entities are considered fur-
ther as coreference clues only if they are 
found within already coreferred docu-
ments. 
1 Introduction 
Names are ambiguous items (Artiles, Gonzalo 
and  Sekine 2007). As reported on an experiment 
carried out on an Italian news corpus (Magnini 
et al 2006) within a 4 consecutive days from a 
local newspaper the perplexity is 56% and 14% 
for first and last name respectively. Deciding 
which particular person a name refers to within a 
text document depends mainly on the capacity to 
extract the relevant information out of texts 
when it is present1. We consider ?relevant? here 
to stand primarily for two properties: (1) 
uniqueness and (2) appropriateness. A feature is 
unique as long as it appears only with one per-
son. Consider a cluster of web pages that charac-
terizes only one person. Many of the N-grams in 
this cluster are unique compared to other cluster. 
Yet the uniqueness may come simply from the 
sparseness. Appropriateness is the property of an 
N-gram to characterize that person. 
Uniqueness may be assured by ontological 
properties (for example, ?There is a unique 
president of a republic at a definite moment of 
time?, ?Alberta University is in Canada). How-
ever, the range of ontological information we are 
able to handle is quite restricted and we are not 
able to realize the coreference solely relying on 
them. Uniqueness may be assured by estimating 
a very unlike probability of the occurrence of 
certain N-grams for different persons (as, for 
example, ?Dekang Lin professor Alberta Canada 
Google?).  
Appropriateness is a difficult issue because of 
two reasons: (a) it is a dynamic feature (b) it is 
hard to be localized and extracted from text. The 
greatest help comes from the name of the page, 
when it happens to be a suggestive name such as 
?homepage?, ?CV?, ?resume? or ?about?. Gene-
                                                 
1
 It is very difficult to evaluate whether the informa-
tion allowing the coreference of two instances of a 
(same) name is present in a web page or news. A 
crude estimation on our news corpus for the names 
occurring between 6-20 times, which represent 8% of 
the names inventory for the whole collection, is that 
in much more than 50% of the news, the relevant 
information is not present. 
195
alogy pages are very useful, to the extent that the 
information could be accurately extracted and 
that the same information occurs in some other 
pages as well. However, in general, for plain 
web pages, we rely on paragraphs in which a 
single person is mentioned and consequently, the 
search space for similarity is also within this 
type of paragraphs. 
Our proposal is to rely on special N-grams for 
coreference and it is a variant of agglomerative 
clustering based on social net-
works(Bagga&Baldwin 1998, Malin 2005) . The 
terms the N-grams contain are crucial. Suppose 
we have the same name shared by two different 
persons who happen to also have the same pro-
fession, let?s say, ?lawyer?, and who also prac-
tice in the same state. While all three words ? 
(name, profession, state) - might be rare words 
for the whole corpus, their probability computed 
as chance to be seen in the same document is 
low, their three-gram fails to cluster correctly the 
documents referring to the two persons2. Know-
ing that the ?lawyer? is a profession that has dif-
ferent specializations, which are likely to be 
found as determiners, we may address this prob-
lem more accurately considering the same three-
gram by changing ?lawyer? with a word more 
specific denoting her specialization. 
The present method for clustering people web 
pages containing names according addresses 
both uniqueness and appropiateness. We rely on 
a procedure that firstly identifies the surest cases 
of coreference and then recursively discover new 
cases. It is not necessarily the case that the latest 
found coreferences are more doubtful, but rather 
that the evidence required for their coreference 
is harder to achieve. 
The cluster metrics gives a primary impor-
tance to words denoting entities which are de-
fined according to ACE definitions: PER, LOC, 
ORG, GPE.  
In Section 2 we present in detail the architec-
ture of our system and in Section 3 we present 
its behavior and the results we obtained on the 
test set of Semeval 2007 Web Person Search 
task. In section 4 we present our conclusions and 
future directions for improvement. 
                                                 
2
 The traditional idf methods used in document clus-
tering must be further refined in order to be effective 
in person coreference. 
2 System Architecture 
First, the text is split into paragraphs, based 
mainly on the html structure of the page. We 
have a Perl script which decides weather the 
name of interest is present within a paragraph. If 
the test is positive the paragraph is marked as a 
person-paragraph, and our initial assumption is 
that each person-paragraph refers to a different 
person.  
The second step is considered the first proce-
dure of the feature extraction module. To each 
paragraph person we associate a set of NEs, rare 
words and temporal expressions, each of them 
counting as independent items. For all of these 
items which are inside of the same dependency 
path we also consider the N-grams made out of 
the respective items preserving the order. For 
each person-paragraph we compute the list of 
above items and consider them as features for 
clustering. This set is called the association set. 
The first step in making the coreference is the 
most important one and consists in two opera-
tions: (1) the most similar pages are clustered 
together and (2) for each cluster, we make a list 
of the pages which most likely do not refer to the 
same person. Starting with this initial estimation, 
the next steps are repeated till no new corefer-
ence is made.  
For each cluster of pages, a new set of items 
is computed starting from the association sets. 
Only the ones which are specific to the respec-
tive cluster - comparing against all other clusters 
and against the list of pages not related (see (2) 
above) ? are kept in the new association set. 
These are the features we use further for cluster-
ing. The clustering score of two person-
paragraphs is given by summing up the individ-
ual score of common features in their association 
sets. The score of a feature is determined based 
on its type - (NE, distinctive words, temporal 
expressions) - , its length in terms of words 
compounding it, and the number of its occur-
rences inside the cluster and inside the whole 
corpus, considering only the web pages relative 
to that name and the absolute frequency of the 
words. The feature score is finally weighed with 
a factor which expresses the distance between 
the name and the respective feature. An empiri-
cal threshold has been chosen. 
196
Each of the above paragraphs representing a 
module in our system is explained in one of the 
next subsections respectively. 
2.1 Preprocessing 
Web pages contain a lot of information outside 
the raw text. We wrote Perl scripts for identify-
ing the e-mail addresses, phone and fax numbers 
and extract them if they were in the same para-
graph with the name of interest. It seems that a 
lot can be gained considering the web addresses, 
the type of page, the links outside the pages and 
so on. However, we have not exploited up to 
now these extra clues for coreference. The whole 
corpus associated with a name is searched only 
once. If the respective items are found in two 
different pages, these two pages are clustered.  
In web pages, the visual structure plays an 
important role, and many times the graphics de-
sign substitutes for linguistics features. Using a 
normal html parser, such as lynx, the text may 
lack its usual grammatical structure which may 
drastically decrease the performances of sen-
tence splitters, Name Entity Recognizers and 
parsers. To alleviate this problem, the text is first 
tagged with PoS. If a paragraph, ?\n?, does not 
have a main verb, then it is treated separately. If 
the text contains only nouns and determiners and 
if the paragraph is within a paragraph containing 
the name of interest, the phrase ?You are talking 
about? is added in front of it to make it a normal 
sentence. 
The text is split into person-paragraphs, and 
each person-paragraph is split into sentences, 
lemmatized, the NEs are recognized 3  and the 
text is parsed using MiniPar (Dekang Lin 1998). 
We are interested only in dependency paths that 
are rooted in NEs ? the NP which are included in 
bigger XP, or sister of NPs, or contain time ex-
pressions. 
The person-paragraphs are checked for the in-
terest names. We write rules for recognizing the 
valid names. If a page does not have a valid 
name of interest, it is discarded. A page is also 
discarded when a valid name of interest has its 
entity type ?ORG?. 
                                                 
3
 We thank to the Textec group at IRST for making it 
possible for everyone to pre process the text very 
easily with state of the art performances. 
2.2 Feature Extraction 
The association set contains a set of features. 
The features are NEs or part of NEs, because the 
closed class words, the very frequent words ? 
computed on the set of all web pages for all per-
sons ? are deleted from the NEs4. When we refer 
to the length of a feature we mean the number of 
words it is made of, after deletion. 
We consider words (phrases) which are not 
NEs as features but only if they are frequent in 
already coreferred person-paragraphs. That is, 
initially the coreference is determined solely on 
NEs. If there is enough evidence, i.e. when a 
word is frequent within the cluster and not pre-
sent within other clusters, then the respective 
word (phrase) is taken into account for corefer-
ence. 
Time expressions are relevant indicators for 
coreference if they are appropriately linked to a 
person. We consider them always, just like a 
NE, but when they appear in particular depend-
ency trees they have a special value. If they are 
dominated by a name of interest and/or by the 
lemma ?birth?, ?born? we consider them as a 
sure factor for coreference.  
For all composed features we also consider 
the order preserved combinations of their parts 
obtaining new features. 
The association sets increase their cardinality 
by coreference. At each step, the new added fea-
tures are checked against the ones from the other 
clusters. The common features are kept in sepa-
rate sets. The coreference is not decided on their 
basis, but these features are used to identify the 
paragraph persons that do not refer to a particu-
lar person, and therefore should not be included 
in the same cluster. We do not explicitly weigh 
differently the features (apart of the cases men-
tioned above) but they are actually weighed dif-
ferently implicitly. The words within a com-
posed feature are repeated, a feature of length n 
produces n(n-1) new features, n> 2. Besides, as 
we will see in the next section, the similarity 
score uses the length of a feature. 
                                                 
4
 Sometimes, correctly or not,  the SVM base NER 
we use includes, especially inside of LOC and GPE 
name entities, common words. In order to remain as 
precise as possible, we choose not to consider these 
words when we compute the similarity score. 
197
2.3 Similarity Measure 
Our similarity score for two person-paragraphs 
is the sum of the individual scores of the com-
mon features which are weighed according to 
the maximum of distances between the name of 
interest and the feature. 
There are three parameters on which we rely 
for computing similarity: the length, the number 
of occurrences, and the absolute frequency of a 
feature. The score considers the cube of the fea-
ture length (which means that the one word fea-
tures do not score). We compute the ratio be-
tween the number of occurrences within the 
cluster and the number of occurrences in the web 
pages relative to that name. The third parameter 
is the absolute frequency of the words. As usu-
ally, if the word is a rare word it counts as more 
evidence for coreference. We regard these pa-
rameters as independent, in spite of their relative 
dependency, and we simply multiply them. 
We define the distance between a feature and 
a name as a discrete measure. If the name and 
the feature are sisters of the same head then their 
distance is minimum, therefore their importance 
for similarity is the highest. The second lower 
distance value is given within the same sentence 
and the distance increases with the number of 
sentences. If there are no other names mentioned 
in the paragraph, the distance is divided by half. 
We have established an empirical threshold 
which initially is very high, as the features are 
not checked among the clusters in the first run. 
After the first run, it is relaxed and the common 
and individual sets are computed as we have 
described in the previous section. 
3 Evaluation 
The system performance on the test set of Seme-
val 2007 Web Person Search task is F?=0.5 = 
0.75, harmonic means of purity, and F=0.2 = 0.80 
- the inverse purity mean. The data set has been 
divided in three sets: SET1 ACL people, SET2 
Wikipedia people, and SET3 census people. The 
results are presented in table 1. The fact that the 
system is less accurate on SET2 may be due to 
the fact that larger person paragraph are consid-
ered and therefore more inappropriate similarity 
are declared. 
 
 
Test 
Set 
Purity Inverse 
Purity 
F?=0.5 
SET1 0,75 0,80 0,77 
SET2 0,83 0,71 0,77 
SET3 0,81 0,75 0,78 
4 Conclusion and Further Research 
Our method is greedy and it depends a lot on the 
accuracy of coreference as the system propa-
gates the errors from step to step. 
One of the big problems of our system is the 
preprocessing step and further improvement is 
required. That is because we rely on the per-
formances of NER and parsers. We also hope 
that by the inclusion of extra textual information 
the html carries, we will have better results. 
A second direction for us is to exactly under-
stand the role of ontological information. For the 
moment, we recognized some of the words de-
noting professions and we tried to guess their 
determinators. We think that having hierarchical 
relationships among LOC, GPE and also for 
ORG may make a difference in results especially 
for massive corpora. 
References 
Artiles, J., Gonzalo, J. and Sekine, S. (2007). 
Establishing a benchmark for the Web People 
Search Task: The Semeval 2007 WePS Track. 
In Proceedings of Semeval 2007, Association 
for Computational Linguistics. 
Bagga A., Baldwin B.,(1998) Entity-Based 
cross-document-referencing using vector 
space model, In proceedings of 17th  Interna-
tional Conference on Computational Linguis-
tics 
Magnini B., Pianta E., Popescu O. and Speranza 
M. (2006). Ontology Population from Textual 
Mentions: Task Definition and Benchmark. 
Proceedings of the OLP2 workshop on Ontol-
ogy Population and Learning, Sidney, Austra-
lia,. Joint with ACL/Coling  
Malin. B., (2005): Unsupervised Name Disam-
biguation via Network Similarity, In proceed-
ings SIAM Conference on Data Mining 2005 
Zanolli R., Pianta E. (2006) Technical report, 
ITC IRST 
198
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 36?43,
Suntec, Singapore, 6 August 2009.
c?2009 ACL and AFNLP
Optimizing Textual Entailment Recognition Using Particle Swarm
Optimization
Yashar Mehdad
University of Trento and FBK - Irst
Trento, Italy
mehdad@fbk.eu
Bernardo Magnini
FBK - Irst
Trento, Italy
magnini@fbk.eu
Abstract
This paper introduces a new method to im-
prove tree edit distance approach to tex-
tual entailment recognition, using particle
swarm optimization. Currently, one of the
main constraints of recognizing textual en-
tailment using tree edit distance is to tune
the cost of edit operations, which is a dif-
ficult and challenging task in dealing with
the entailment problem and datasets. We
tried to estimate the cost of edit operations
in tree edit distance algorithm automati-
cally, in order to improve the results for
textual entailment. Automatically estimat-
ing the optimal values of the cost opera-
tions over all RTE development datasets,
we proved a significant enhancement in
accuracy obtained on the test sets.
1 Introduction
One of the main aspects of natural languages is to
express the same meaning in many possible ways,
which directly increase the language variability
and emerges the complex structure in dealing with
human languages. Almost all computational lin-
guistics tasks such as Information Retrieval (IR),
Question Answering (QA), Information Extrac-
tion (IE), text summarization and Machine Trans-
lation (MT) have to cope with this notion. Textual
Entailment Recognition was proposed by (Dagan
and Glickman, 2004), as a generic task in order to
conquer the problem of lexical, syntactic and se-
mantic variabilities in languages.
Textual Entailment can be explained as an as-
sociation between a coherent text (T) and a lan-
guage expression, called hypothesis (H) such that
entailment function for the pair T-H returns the
true value when the meaning of H can be inferred
from the meaning of T and false, otherwise.
Amongst the approaches to the problem of tex-
tual entailment, some methods utilize the no-
tion of distance between the pair of T and H as
the main feature which separates the entailment
classes (positive and negative). One of the suc-
cessful algorithms implemented Tree Edit Dis-
tance (TED), based on the syntactic features that
are represented in the structured parse tree of each
string (Kouylekov and Magnini, 2005). In this
method the distance is computed as the cost of
the edit operations (insertion, deletion and substi-
tution) that transform the text T into the hypothesis
H. Each edit operation has an associated cost and
the entailment score is calculated such that the set
of operations would lead to the minimum cost.
Generally, the initial cost is assigned to each
edit operation empirically, or based on the ex-
pert knowledge and experience. These methods
emerge a critical problem when the domain, field
or application is new and the level of expertise and
empirical knowledge is very limited. In dealing
with textual entailment, (Kouylekov and Magnini,
2006) tried to experiment different cost values
based on various linguistics knowledge and prob-
abilistics estimations. For instance, they defined
the substitution cost as a function of similarity
between two nodes, or, for insertion cost, they
employed Inverse Document Frequency (IDF) of
the inserted node. However, the results could not
proven to be optimal.
Other approaches towards estimating the cost
of operations in TED tried to learn a generic or
discriminative probabilistic model (Bernard et al,
2008; Neuhaus and Bunke, 2004) from the data,
without concerning the optimal value of each op-
eration. One of the drawbacks of those approaches
is that the cost values of edit operations are hidden
behind the probabilistic model. Additionally, the
cost can not be weighted or varied according to
the tree context and node location (Bernard et al,
2008).
In order to overcome these drawbacks, we are
proposing a stochastic method based on Particle
36
Swarm Optimization (PSO), to estimate the cost
of each edit operation for textual entailment prob-
lem. Implementing PSO, we try to learn the op-
timal cost for each operation in order to improve
the prior textual entailment model. In this paper,
the goal is to automatically estimate the best possi-
ble operation costs on the development set. A fur-
ther advantage of such method, besides automatic
learning of the operation costs, is being able to in-
vestigate the cost values to better understand how
TED approaches the data in textual entailment.
The rest of the paper is organized as follows:
After describing the TED approach to textual en-
tailment in the next section, PSO optimization al-
gorithm and our method in applying it to the prob-
lem are explained in sections 4 and 5. Then we
present our experimental setup as well as the re-
sults, in detail. Finally, in the conclusion, the main
advantages of our approach are reviewed and fur-
ther developments are proposed accordingly.
2 Tree Edit Distance and Textual
Entailment
One of the approaches to textual entailment is
based on the Tree Edit Distance (TED) between
T and H. The tree edit distance measure is a simi-
larity metric for rooted ordered trees. This metric
was initiated by (Tai, 1979) as a generalization of
the string edit distance problem and was improved
by (Zhang and Shasha, 1989) and (Klein, 1998).
The distance is computed as the cost of editing
operations (i.e. insertion, deletion and substitu-
tion), which are required to transform the text T
into the hypothesis H, while each edit operation on
two text fragments A and B (denoted as A ? B)
has an associated cost (denoted as ? (A ? B)). In
textual entailment context, the edit operations are
defined in the following way based on the depen-
dency parse tree of T and H:
? Insertion (? ? A): insert a node A from
the dependency tree of H into the depen-
dency tree of T. When a node is inserted it
is attached to the dependency relation of the
source label.
? Deletion (A ? ?): delete a node A from
the dependency tree of T. When A is deleted
all its children are attached to the parent of
A. It is not required to explicitly delete the
children of A, as they are going to be either
deleted or substituted in a following step.
? Substitution (A ? B): change the label of
a node A in the source tree into a label of a
node B of the target tree. In the case of substi-
tution, the relation attached to the substituted
node is changed with the relation of the new
node.
According to (Zhang and Shasha, 1989), the min-
imum cost mappings of all the descendants of
each node has to be computed before the node
is encountered, so the least-cost mapping can be
selected right away. To accomplish this the al-
gorithm keeps track of the keyroots of the tree,
which are defined as a set that contains the root
of the tree plus all nodes which have a left sibling.
This problem can be easily solved using recursive
methods (Selkow, 1977), or as it was suggested in
(Zhang and Shasha, 1989) by dynamic program-
ming. (Zhang and Shasha, 1989) defined the rel-
evant subproblems of tree T as the prefixes of all
special subforests rooted in the keyroots. This ap-
proach computes the TED (?) by the following
equations:
?(F
T
, ?) =
?(F
T
? r
F
T
, ?) + ?(r
F
T
? ?) (1)
?(?, F
H
) =
?(?, F
H
? r
F
H
) + ?(? ? r
F
H
) (2)
?(F
T
, F
H
) =
min
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?(F
T
? r
F
T
, F
H
) + ?(r
F
T
? ?)
?(F
T
, F
H
? r
F
H
) + ?(? ? r
F
H
)
?(F
T
(r
F
T
), F
H
(r
F
H
))+
?(F
T
? T (r
F
T
), F
H
?H(r
F
H
))+
?(r
F
T
? r
F
H
)
(3)
where F
T
and F
H
are forests of T and H , while
r
F
T
and r
F
H
are the rightmost roots of the trees
in F
T
and F
H
respectively. ? is an empty forest.
Moreover, F
T
(r
F
T
) and F
H
(r
F
H
) are the forests
rooted in r
F
T
and r
F
H
respectively.
Estimating ? as the bottom line of the compu-
tation is directly related to the cost of each oper-
ation. Moreover, the cost of edit operations can
simply change the way that a tree is transformed
to another. As Figure 1
1
shows (Demaine et al,
2007), there could exist more than one edit script
for transforming each tree to another. Based on the
1
The example adapted from (Demaine et al, 2007)
37
Figure 1: Two possible edit scripts to transform one tree to another.
main definition of this approach, TED is the cost
of minimum cost edit script between two trees.
The entailment score for a pair is calculated on
the minimal set of edit operations that transform
the dependency parse tree of T into H. An entail-
ment relation is assigned to a T-H pair where the
overall cost of the transformations is below a cer-
tain threshold. The threshold, which corresponds
to tree edit distace, is empirically estimated over
the dataset. This method was implemented by
(Kouylekov and Magnini, 2005), based on the al-
gorithm by (Zhang and Shasha, 1989).
In this method, a cost value is assigned to each
operation initially, and the distance is computed
based on the initial cost values. Considering that
the distance can vary in different datasets, con-
verging to an optimal set of values for operations
is almost empirically impossible. In the follow-
ing sections, we propose a method for estimat-
ing the optimum set of values for operation costs
in TED algorithm dealing with textual entailment
problem. Our method is built on adapting PSO
optimization approach as a search process to auto-
mate the procedure of the cost estimation.
3 Particle Swarm Optimization
PSO is a stochastic optimization technique which
was introduced based on the social behaviour of
bird flocking and fish schooling (Eberhart et al,
2001). It is one of the population-based search
methods which takes advantage of the concept of
social sharing of information. The main struc-
ture of this algorithm is not very different from
other evolutionary techniques such as Genetic Al-
gorithms (GA); however, the easy implementation
and less complexity of PSO, as two main charac-
teristics, are good motivations to apply this opti-
mization approach in many areas.
In this algorithm each particle can learn from
the experience of other particles in the same pop-
ulation (called swarm). In other words, each parti-
cle in the iterative search process, would adjust its
flying velocity as well as position not only based
on its own acquaintance, but also other particles?
flying experience in the swarm. This algorithm has
found efficient in solving a number of engineering
problems. In the following, we briefly explain the
main concepts of PSO.
To be concise, for each particle at each itera-
tion, the position X
i
(Equation 4) and velocity V
i
(Equation 5) is updated. X
bi
is the best position
of the particle during its past routes and X
gi
is
the best global position over all routes travelled by
the particles of the swarm. r
1
and r
2
are random
variables drawn from a uniform distribution in the
range [0,1], while c
1
and c
2
are two acceleration
constants regulating the relative velocities with re-
spect to the best local and global positions. The
weight ? is used as a tradeoff between the global
and local best positions and its value is usually
selected slightly less than 1 for better global ex-
ploration (Melgani and Bazi, 2008). The optimal
position is computed based on the fitness func-
tion defined in association with the related prob-
lem. Both position and velocity are updated dur-
ing the iterations until convergence is reached or
iterations attain the maximum number defined by
the user. This search process returns the best fit-
ness function over the particles, which is defined
as the optimized solution.
X
i
= X
i
+ V
i
(4)
V
i
= ?V
i
+ c
1
r
1
(X
bi
?X
i
)
+ c
2
r
2
(X
gi
?X
i
) (5)
Algorithm 1 shows a simple pseudo code of
how this optimization algorithm works. In the rest
of the paper, we describe our method to integrate
this algorithm with TED.
38
Algorithm 1 PSO algorithm
for all particles do
Initialize particle
end for
while Convergence or maximum iteration
do
for all particles do
Calculate fitness function
if fitness function value > X
bi
then
X
bi
? fitness function value
end if
end for
choose the best particle amongst all in X
gi
for all particles do
calculate V
i
update X
i
end for
end while
return best particle
4 Automatic Cost Estimation
One of the challenges in applying TED for rec-
ognizing textual entailment is estimating the cost
of each edit operation which transforms the text T
into the hypothesis H in an entailment pair. Since
the cost of edit operations can directly affect the
distance, which is the main criteria to measure the
entailment, it is not trivial to estimate the cost of
each operation. Moreover, considering that imply-
ing different costs for edit operations can affect the
results in different data sets and approaches, it mo-
tivates the idea of optimizing the cost values.
4.1 PSO Setup
One of the most important steps in applying PSO
is to define a fitness function which could lead the
swarm to the optimized particles based on the ap-
plication and data. The choice of this function
is very crucial, since PSO evaluates the quality
of each candidate particle for driving the solution
space to optimization, on the basis of the fitness
function. Moreover, this function should possibly
improve the textual entailment recognition model.
In order to attain these goals, we tried to define
two main fitness functions as follows.
1. Bhattacharyya Distance: This measure was
proposed by (Bhattacharyya, 1943) as a sta-
tistical measure to determine the similarity
or distance between two discrete probabil-
ity distributions. In binary classification, this
method is widely used to measure the dis-
tance between two different classes. In the
studies by (Fukunaga, 1990), Bhattacharyya
distance was occluded to be one of the most
effective measure specifically for estimating
the separability of two classes. Figure 2
shows the intuition behind this measure.
Figure 2: Bhattacharyya distance between two
classes with similar variances.
Bhattacharyya distance is calculated based on
the covariance (?) and mean (?) of each dis-
tribution based on its simplest formulation in
Equation 6 (Reyes-Aldasoro and Bhalerao,
2006). Maximizing the distance between
the classes would result a better separability
which aims to a better classification results.
Furthermore, estimating the costs using this
function would indirectly improve the perfor-
mance specially in classification problems. It
could be stated that, maximizing the Bhat-
tacharyya distance would increase the separa-
bility of two entailment classes which result
in a better performance.
BD(c
1
, c
2
) =
1
4
ln{
1
4
(
?
2
c
1
?
2
c
2
+
?
2
c
2
?
2
c
1
+ 2)}
+
1
4
{
(?
c
1
? ?
c
2
)
2
?
2
c
1
+ ?
2
c
2
} (6)
2. Accuracy: Accuracy or any performance
measure obtained from a TED based system,
can define a good fitness function in optimiz-
ing the cost values. Since maximizing the
accuracy would directly increase the perfor-
mance of the system or enhance the model
to solve the problem, this measure is a pos-
sible choice to adapt in order to achieve our
aim. In this method, trying to maximize the
fitness function will compute the best model
based on the optimal cost values in the parti-
cle space of PSO algorithm.
In other words, by defining the accuracy ob-
tained from 10 fold cross-validation over the
39
development set, as the fitness function, we
could estimate the optimized cost of the edit
operations. Maximizing the accuracy gained
in this way, would lead to find the set of edit
operation costs which directly increases our
accuracy, and consequently guides us to the
main goal of optimization.
In the following section, the procedure of esti-
mating the optimal costs are described in detail.
4.2 Integrating TED with PSO for Textual
Entailment Problem
The procedure describing the proposed system to
optimize and estimate the cost of edit operations
in TED applying PSO algorithm is as follows.
a) Initialization
Step 1) Generate a random swarm of particles
(in a simple case each particle is de-
fined by the cost of three operations).
Step 2) For each position of the particle from
the swarm, obtain the fitness function
value (Bhattacharyya distance or accu-
racy) over the training data.
Step 3) Set the best position of each particle
with its initial position (X
bi
).
b) Search
Step 4) Detect the best global position (X
gi
)
in the swarm based on maximum value
of the fitness function over all explored
routes.
Step 5) Update the velocity of each particle
(V
i
).
Step 6) Update the position of each particle
(X
i
). In this step, by defining the
boundaries, we could stop the particle
to exit the allowed search space.
Step 7) For each candidate particle calculate
the fitness function (Bhattacharyya
distance or accuracy).
Step 8) Update the best position of each parti-
cle if the current position has a larger
value.
c) Convergence
Step 9) Run till the maximum number of iter-
ation (in our case set to 10) is reached
or start the search process.
d) Results
Step 10) Return the best fitness function value
and the best particle. In this step the
optimum costs are returned.
Following the steps above, in contrary to de-
termine the entailment relation applying tree edit
distance, the operation costs can be automatically
estimated and optimized. In this process, both fit-
ness functions could be easily compared and the
cost values leading to the better model would be
selected. In the following section, the experimen-
tal procedure for obtaining the optimal costs by
exploiting the PSO approach to TE is described.
5 Experimental Design
In our experiments we show an increase in the per-
formance of TED based approach to textual en-
tailment, by optimizing the cost of edit operations.
In the following subsections, the framework and
dataset of our experiments are elaborated.
5.1 Dataset Description
Our experiments were conducted on the basis
of the Recognizing Textual Entailment (RTE)
datasets
2
, which were developed under PASCAL
RTE challenge. Each RTE dataset includes its own
development and test set, however, RTE-4 was re-
leased only as a test set and the data from RTE-1
to RTE-3 were used as development set. More de-
tails about the RTE datasets are illustrated in Table
5.1.
Number of pairs
Development Test
Datasets YES NO YES NO
RTE-1 283 284 400 400
RTE-2 400 400 400 400
RTE-3 412 388 410 390
RTE-4 ? ? 500 500
Table 1: RTE-1 to RTE-4 datasets.
5.2 Experimental Framework
In our experiments, in order to deal with TED
approach to textual entailment, we used EDITS
3
package (Edit Distance Textual Entailment Suite)
2
http://www.pascal-network.org/Challenges/RTE1-4
3
The EDITS system has been supported by the EU-
funded project QALL-ME (FP6 IST-033860). Available at
http://edits.fbk.eu/
40
(Magnini et al, 2009). This system is an open
source software based on edit distance algorithms,
and computes the T-H distance as the cost of the
edit operations (i.e. insertion, deletion and substi-
tution) that are necessary to transform T into H.
By defining the edit distance algorithm and a cost
scheme (assigning a cost to the edit operations),
this package is able to learn a TED threshold, over
a set of string pairs, to decide if the entailment ex-
ists in a pair.
In addition, we partially exploit the JSwarm-
PSO
4
(Cingolani, 2005) package, with some adap-
tations, as an implementation of PSO algorithm.
Each pair in the datasets is converted to two syn-
tactic dependency parse trees using the Stanford
statistical parser
5
, developed in the Stanford uni-
versity NLP group by (Klein and Manning, 2003).
Figure 3: Five main steps of the experimental
framework.
In order to take advantage of PSO optimization
approach, we integrated EDITS and JSwarm-PSO
to provide a flexible framework for the experi-
ments (Figure 5.3). In this way, we applied the
defined fitness functions in the integrated system.
The Bhattacharyya distance between two classes
(YES and NO), in each experiment, could be com-
puted based on the TED score of each pair in the
dataset. Moreover, the accuracy, by default, is
computed by EDITS over the training set based
on 10-fold cross-validation.
5.3 Experimental Scheme
We conducted six different experiments in two sets
on each RTE dataset. The costs were estimated on
the training set and the results obtained based on
the estimated costs over the test set. In the first
4
http://jswarm-pso.sourceforge.net/
5
http://nlp.stanford.edu/software/lex-parser.shtml
set of experiments, we set a simple cost scheme
based on three operations. Implementing this cost
scheme, we expect to optimize the cost of each
edit operation without considering that the opera-
tion costs may vary based on different character-
istics of a node, such as size, location or content.
The results were obtained considering three dif-
ferent settings: 1) the random cost assignment; 2)
assigning the cost based on the human expertise
knowledge and intuition (called Intuitive), and 3)
automatic estimated and optimized cost for each
operation. In the second case, we used the same
scheme which was used in EDITS by its develop-
ers (Magnini et al, 2009).
In the second set of experiments, we tried to
compose an advanced cost scheme with more
fine-grained operations to assign a weight to the
edit operations based on the characteristics of the
nodes. For example if a node is in the list of stop-
words, the deletion cost is set to zero. Otherwise,
the cost of deletion would be equal to the number
of words in H multiplied by word?s length (num-
ber of characters). Similarly, the cost of inserting
a word w in H is set to 0 if w is a stop word,
and to the number of words in T multiplied by
words length otherwise. The cost of substituting
two words is the Levenshtein distance (i.e. the edit
distance calculated at the level of characters) be-
tween their lemmas, multiplied by the number of
words in T, plus number of words in H. By this in-
tuition, we tried to optimize nine specialized costs
for edit operations (i.e. each particle is defined by
9 parameters to be optimized). We conducted the
experiments using all three cases mentioned in the
simple cost scheme.
In each experiment, we applied both fitness
functions in the optimization; however, at the final
phase, the costs which led to the maximum results
were chosen as the estimated operation costs. In
order to save breath and time, we set the number
of iterations to 10, in addition, the weight ? was
set to 0.95 for better global exploration (Melgani
and Bazi, 2008).
6 Results
Our results are summarized in Table 2. We show
the accuracy gained by a distance-based (word-
overlap) baseline for textual entailment (Mehdad
and Magnini, 2009) to be compared with the re-
sults achieved by the random, intuitive and op-
timized cost schemes using EDITS system. For
41
Data set
Model RTE-4 RTE-3 RTE-2 RTE-1
Simple
Random 49.6 53.62 50.37 50.5
Intuitive 51.3 59.6 56.5 49.8
Optimized 56.5 61.62 58 58.12
Advanced
Random 53.60 52.0 54.62 53.5
Intuitive 57.6 59.37 57.75 55.5
Optimized 59.5 62.4 59.87 58.62
Baseline 55.2 60.9 54.8 51.4
RTE-4 Challenge 57.0
Table 2: Comparison of accuracy on all RTE datasets based on optimized and unoptimized cost schemes.
the better comparison, we also present the results
of the EDITS system in RTE-4 challenge using a
combination of different distances as features for
classification (Cabrio et al, 2008).
In the first experiment, we estimated the cost of
each operation using the simple cost scheme. Ta-
ble 2 shows that in all datasets, accuracy improved
up to 9% by optimizing the cost of each edit opera-
tion. Results prove that the optimized cost scheme
enhances the quality of the system performance,
even more than the cost scheme used by experts
(Intuitive cost scheme) (Magnini et al, 2009).
Furthermore, in the second set of experiments,
using the fine-grained and weighted cost scheme
for edit operations we could achieve the highest re-
sults in accuracy. The chart in Figure 4, illustares
that all optimized results outperform the word-
overlap baseline for textual entailment as well as
the accuracy obtained in RTE-4 challenge using
combination of different distances as features for
classification (Cabrio et al, 2008).
By exploring the estimated optimal cost of each
operation, another interesting point was discov-
ered. The estimated cost of deletion in the first
set of experiments was 0, which means that delet-
ing a node from the dependency tree of T does not
effect the quality of results. This proves that by
setting different cost schemes, we could explore
even some linguistics phenomena which exists in
the entailment dataset. Studying the dataset from
this point of view might be interesting to find some
hidden information which can not be explored eas-
ily.
In addition, the optimized model can reflect
more consistency and stability (from 58 to 62 in
accuracy) than other models, while in unoptimized
models the result varies more, on different datasets
(from 50 in RTE-1 to 59 in RTE-3). Moreover, we
believe that by changing some parameters such as
maximum number of iterations, or by defining a
better cost scheme, there could be still a room for
improvement.
Figure 4: Accuracy obtained by different experi-
mental setups.
7 Conclusion
In this paper, we proposed a novel approach for es-
timating the cost of edit operations for the tree edit
distance approach to textual entailment. With this
work we illustrated another step forward in im-
proving the foundation of working with distance-
based algorithms for textual entailment. The ex-
perimental results confirm our working hypothe-
sis that by improving the results in applying tree
edit distance for textual entailment, besides out-
performing the distance-based baseline for recog-
42
nizing textual entailment.
We believe that for further development, ex-
tending the cost scheme to find weighted and
specialized cost operations to deal with different
cases, can lead to more interesting results. Besides
that, exploring and studying the estimated cost of
operations, could be interesting from a linguistics
point of view.
Acknowledgments
Besides my special thanks to Farid Melgani
for his helpful ideas, I acknowledge Milen
Kouylekov for his academic and technical sup-
ports. This work has been partially sup-
ported by the three-year project LiveMemories
(http://www.livememories.org/), funded by the
Provincia Autonoma di Trento.
References
Marc Bernard, Laurent Boyer, Amaury Habrard, and
Marc Sebban. 2008. Learning probabilistic models
of tree edit distance. Pattern Recogn., 41(8):2611?
2629.
A. Bhattacharyya. 1943. On a measure of diver-
gence between two statistical populations defined by
probability distributions. Bull. Calcutta Math. Soc.,
35:99109.
Elena Cabrio, Milen Kouylekovand, and Bernardo
Magnini. 2008. Combining specialized entailment
engines for rte-4. In Proceedings of TAC08, 4th
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment.
Pablo Cingolani. 2005. Jswarm-pso: Particle swarm
optimization package. Available at http://jswarm-
pso.sourceforge.net/.
Ido Dagan and Oren Glickman. 2004. Probabilis-
tic textual entailment: Generic applied modeling of
language variability. In Proceedings of the PAS-
CAL Workshop of Learning Methods for Text Under-
standing and Mining.
E. Demaine, S. Mozes, B. Rossman, and O. Weimann.
2007. An optimal decomposition algorithm for tree
edit distance. In Proceedings of the 34th Inter-
national Colloquium on Automata, Languages and
Programming (ICALP), pages 146?157.
Russell C. Eberhart, Yuhui Shi, and James Kennedy.
2001. Swarm Intelligence. The Morgan Kaufmann
Series in Artificial Intelligence. Morgan Kaufmann.
Keinosuke Fukunaga. 1990. Introduction to statisti-
cal pattern recognition (2nd ed.). Academic Press
Professional, Inc., San Diego, CA, USA.
Dan Klein and Christopher D. Manning. 2003. Fast
exact inference with a factored model for natural
language parsing. In Advances in Neural Informa-
tion Processing Systems 15, pages 3?10, Cambridge,
MA. MIT Press.
Philip N. Klein. 1998. Computing the edit-distance
between unrooted ordered trees. In ESA ?98: Pro-
ceedings of the 6th Annual European Symposium on
Algorithms, pages 91?102, London, UK. Springer-
Verlag.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance
algorithms. In PASCAL Challenges on RTE, pages
17?20.
Milen Kouylekov and Bernardo Magnini. 2006. Tree
edit distance for recognizing textual entailment: Es-
timating the cost of insertion. In PASCAL RTE-2
Challenge.
Bernardo Magnini, Milen Kouylekov, and Elena
Cabrio. 2009. Edits - edit distance tex-
tual entailment suite user manual. Available at
http://edits.fbk.eu/.
Yashar Mehdad and Bernardo Magnini. 2009. A word
overlap baseline for the recognizing textual entail-
ment task. Available at http://edits.fbk.eu/.
Farid Melgani and Yakoub Bazi. 2008. Classi-
fication of electrocardiogram signals with support
vector machines and particle swarm optimization.
IEEE Transactions on Information Technology in
Biomedicine, 12(5):667?677.
Michel Neuhaus and Horst Bunke. 2004. A proba-
bilistic approach to learning costs for graph edit dis-
tance. In ICPR ?04, pages 389?393, Washington,
DC, USA. IEEE Computer Society.
C. C. Reyes-Aldasoro and A. Bhalerao. 2006. The
bhattacharyya space for feature selection and its ap-
plication to texture segmentation. Pattern Recogn.,
39(5):812?826.
Stanley M. Selkow. 1977. The tree-to-tree editing
problem. Inf. Process. Lett., 6(6):184?186.
Kuo-Chung Tai. 1979. The tree-to-tree correction
problem. J. ACM, 26(3):422?433.
K. Zhang and D. Shasha. 1989. Simple fast algorithms
for the editing distance between trees and related
problems. SIAM J. Comput., 18(6):1245?1262.
43
  	
 ffWeakly Supervised Approaches for Ontology Population
Hristo Tanev Tanev
ITC-irst
38050, Povo
Trento, Italy
htanev@yahoo.co.uk
Bernardo Magnini
ITC-irst
38050, Povo
Trento, Italy
magnini@itc.it
Abstract
We present a weakly supervised approach
to automatic Ontology Population from
text and compare it with other two unsu-
pervised approaches. In our experiments
we populate a part of our ontology of
Named Entities. We considered two high
level categories - geographical locations
and person names and ten sub-classes for
each category. For each sub-class, from
a list of training examples and a syntac-
tically parsed corpus, we automatically
learn a syntactic model - a set of weighted
syntactic features, i.e. words which typ-
ically co-occur in certain syntactic posi-
tions with the members of that class. The
model is then used to classify the unknown
Named Entities in the test set. The method
is weakly supervised, since no annotated
corpus is used in the learning process. We
achieved promising results, i.e. 65% accu-
racy, outperforming significantly previous
unsupervised approaches.
1 Introduction
Automatic Ontology Population (OP) from texts
has recently emerged as a new field of application
for knowledge acquisition techniques (see, among
others, (Buitelaar et al, 2005)). Although there
is no a univocally accepted definition for the OP
task, a useful approximation has been suggested
(Bontcheva and Cunningham, 2003) as Ontology
Driven Information Extraction, where, in place of
a template to be filled, the goal of the task is the ex-
traction and classification of instances of concepts
and relations defined in a Ontology. The task has
been approached in a variety of similar perspec-
tives, including term clustering (e.g. (Lin, 1998a)
and (Almuhareb and Poesio, 2004)) and term cat-
egorization (e.g. (Avancini et al, 2003)).
A rather different task is Ontology Learning
(OL), where new concepts and relations are sup-
posed to be acquired, with the consequence of
changing the definition of the Ontology itself (see,
for instance, (Velardi et al, 2005)).
In this paper OP is defined in the following sce-
nario. Given a set of terms T = t1, t2, ..., tn, a
document collection D, where terms in T are sup-
posed to appear, and a set of predefined classes
C = c1, c2, ..., cm denoting concepts in an Ontol-
ogy, each term ti has to be assigned to the proper
class in C. For the purposes of the experiments
presented in this paper we assume that (i) classes
in C are mutually disjoint and (ii) each term is as-
signed to just one class.
As we have defined it, OP shows a strong sim-
ilarity with Named Entity Recognition and Clas-
sification (NERC). However, a major difference is
that in NERC each occurrences of a recognized
term has to be classified separately, while in OP it
is the term, independently of the context in which
it appears, that has to be classified.
While Information Extraction, and NERC in
particular, have been addressed prevalently by
means of supervised approaches, Ontology Popu-
lation is typically attacked in an unsupervised way.
As many authors have pointed out (e.g. (Cimiano
and Vo?lker, 2005)), the main motivation is the fact
that in OP the set of classes is usually larger and
more fine grained than in NERC (where the typ-
ical set includes Person, Location, Organization,
GPE, and a Miscellanea class for all other kind
of entities). In addition, by definition, the set of
classes in C changes as a new ontology is consid-
ered, making the creation of annotated data almost
impossible practically.
17
According with the demand for weakly super-
vised approaches to OP, we propose a method,
called Class ? Example, which learns a classi-
fication model from a set of classified terms, ex-
ploiting lexico-syntactic features. Unlike most of
the approaches which consider pair wise similarity
between terms ((Cimiano and Vo?lker, 2005); (Lin,
1998a)), the Class-Example method considers the
similarity between a term ti and a set of training
examples which represent a certain class. This re-
sults in a great number of class features and opens
the possibility to exploit more statistical data, such
as the frequency of appearance of a class feature in
different training terms.
In order to show the effectiveness of the Class-
Example approach, it has been compared against
two different approaches: (i) aClass-Pattern unsu-
pervised approach, in the style of (Hearst, 1998);
(ii) an unsupervised approach that considers the
word of the class as a pivot word for acquiring
relevant contexts for the class (we refer to this
method as Class?Word). Results of the compar-
ison show that the Class-Example method outper-
forms significantly the other two methods, making
it appealing even considering the need of supervi-
sion.
Although the Class-Example method we pro-
pose is applicable in general, in this paper we
show its usefulness when applied to terms denot-
ing Named Entities. The motivation behind this
choice is the practical value of Named Entity clas-
sifications, as, for instance, in applications such as
Questions Answering and Information Extraction.
Moreover, some Named Entity classes, including
names of writers, athletes and organizations, dy-
namically change over the time, which makes it
impossible to capture them in a static Ontology.
The rest of the paper is structured as follows.
Section 2 describes the state-of-the-art methods in
Ontology Population. Section 3 presents the three
approaches to the task we have compared. Section
4 introduces Syntactic Network, a formalism used
for the representation of syntactic information and
exploited in both the Class-Word and the Class-
Example approaches. Section 5 reports on the
experimental settings, results obtained, and dis-
cusses the three approaches. Section 6 concludes
the paper and suggests directions for future work.
2 Related Work
There are two main paradigms distinguishing On-
tology Population approaches. In the first one
Ontology Population is performed using patterns
(Hearst, 1998) or relying on the structure of terms
(Velardi et al, 2005). In the second paradigm the
task is addressed using contextual features (Cimi-
ano and Vo?lker, 2005).
Pattern-based approaches search for phrases
which explicitly show that there is an ?is-a? re-
lation between two words, e.g. ?the ant is an in-
sect? or ?ants and other insects?. However, such
phrases do not appear frequently in a text cor-
pus. For this reason, some approaches use theWeb
(Schlobach et al, 2004). (Velardi et al, 2005) ex-
perimented several head-matching heuristics ac-
cording to which if a term1 is in the head of
term2, then there is an ?is-a? relation between
them: For example ?Christmas tree? is a kind of
?tree?.
Context feature approaches use a corpus to ex-
tract features from the context in which a se-
mantic class tends to appear. Contextual features
may be superficial (Fleischman and Hovy, 2002)
or syntactic (Lin, 1998a), (Almuhareb and Poe-
sio, 2004). Comparative evaluation in (Cimiano
and Vo?lker, 2005) shows that syntactic features
lead to better performance. Feature weights can
be calculated either by Machine Learning algo-
rithms (Fleischman and Hovy, 2002) or by statisti-
cal measures, like Point Wise Mutual Information
or the Jaccard coefficient (Lin, 1998a).
A hybrid approach using both pattern-based,
term structure, and contextual feature methods is
presented in (Cimiano et al, 2005).
State-of-the-art approaches may be divided in
two classes, according to different use of train-
ing data: Unsupervised approaches (see (Cimi-
ano et al, 2005) for details) and supervised ap-
proaches which use manually tagged training data,
e.g. (Fleischman and Hovy, 2002). While state-
of-the-art unsupervised methods have low perfor-
mance, supervised approaches reach higher ac-
curacy, but require the manual construction of a
training set, which impedes them from large scale
applications.
3 Weakly supervised approaches for
Ontology Population
In this Section we present three Ontology Popula-
tion approaches. Two of them are unsupervised:
18
(i) a pattern-based approach described in (Hearst,
1998), which we refer to as Class-Pattern and (ii)
a feature similarity method reported in (Cimiano
and Vo?lker, 2005) to which we will refer as Class-
Word. Finally, we describe a new weakly super-
vised approach for ontology population which ac-
cepts as a training data lists of instances for each
class under consideration. This method we call
Class-Example.
3.1 Class-Pattern approach
This approach was described first in (Hearst,
1998). The main idea is that if a term t belongs
to a class c, then in a text corpus we may expect
the occurrence of phrases like such c as t,.... In our
experiments for ontology population we used the
patterns described in the Hearst?s paper plus the
pattern t is (a | the) c:
1. t is (a | the) c
2. such c as t
3. such c as (NP,)*, (and | or) t
4. t (,NP)* (and | or) other c
5. c, (especially | including) (NP, )* t
For each instance from the test set t and for each
concept c we instantiated the patterns and searched
with them in the corpus. If a pattern which is in-
stantiated with a concept c and a term t appears
in the corpus, then we assume the t belongs to c.
For example, if the term to be classified is ?Etna?
and the concept is ?mountain?, one of the instan-
tiated patterns will be ?mountains such as Etna?;
if this pattern is found in the text, then ?Etna? is
considered to be a ?mountain?. If the algorithm
assigns a term to several categories, we choose the
one which co-occurs most often with the term.
3.2 Class-Word approach
(Cimiano and Vo?lker, 2005) describes an unsu-
pervised approach for ontology population based
on vector-feature similarity between each concept
c and a term to be classified t. For example,
in order to conclude how much ?Etna? is an ap-
propriate instance of the class ?mountain?, this
method finds the feature-vector similarity between
the word ?Etna? and the word ?mountain?. Each
instance from the test set T is assigned to one of
the classes in the set C. Features are collected
from Corpus and the classification algorithm on
classify(T , C, Corpus)
foreach(t in T ) do{
vt = getContextV ector(t, Corpus);}
foreach(c in C) do{
vc = getContextV ector(c, Corpus);}
foreach(t in T ) do{
classes[t] = argmaxc?Csim(vt, vc);}
return classes[];
end classify
Figure 1: Unsupervised algorithm for Ontology
Population.
figure 1 is applied. The problem with this ap-
proach is that the context distribution of a name
(e.g. ?Etna?) is sometimes different than the con-
text distribution of the class word (e.g. ?moun-
tain?). Moreover, a single word provides a limited
quantity of contextual data.
In this algorithm the context vectors vt and
vc are feature vectors whose elements represent
weighted context features from Corpus of the
term t (e.g. ?Etna?) or the concept word c (e.g.
?mountain?). Cimiano and Vo?lker evaluate differ-
ent context features and prove that syntactic fea-
tures work best. Therefore, in our experimen-
tal settings we considered only such features ex-
tracted from a corpus parsed with a dependency
parser. Unlike the original approach which relies
on pseudo-syntactic features, we used features ex-
tracted from dependency parse trees. Moreover,
we used virtually all the words connected syntacti-
cally to a term, not only the modifiers. A syntactic
feature is a pair: (word, syntactic relation) (Lin,
1998a). We use two feature types: First order fea-
tures, which are directly connected to the training
or test examples in the dependency parse trees of
Corpus; second order features, which are con-
nected to the training or test instances indirectly
by skipping one word (the verb) in the dependency
tree. As an example, let?s consider two sentences:
?Edison invented the phonograph? and ?Edison
created the phonograph?. If ?Edison? is a name
to be classified, then two first order features of this
name exist - (?invent?, subject-of) and (?create?,
subject-of). One second order feature can be ex-
tracted - (?phonograph?, object-of+subject); it co-
occurs two times with the word ?Edison?. In our
experiments second order features are considered
only those words which are governed by the same
verb whose subject is the name which is a training
19
or test instance (in this example ?Edison?).
3.3 Weakly Supervised Class-Example
Approach
The approach we put forward here uses the same
processing stages as the one presented in Fig-
ure 1 and relies on syntactic features extracted
from a corpus. However, the Class-Example al-
gorithm receives as an additional input parame-
ter the sets of training examples for each class
c ? C. These training sets are simple lists of
instances (i.e. terms denoting Named Entities),
without context, and can be acquired automati-
cally or semi-automatically from an existing on-
tology or gazetteer. To facilitate their acquisition,
the Class-Example approach imposes no restric-
tions to the training examples - they can be am-
biguous and have different frequencies. However,
they have to appear in Corpus (in our experimen-
tal settings - at least twice). For example, for the
class ?mountain? training examples are: ?Ever-
est?, ?Mauna Loa?, etc.
The algorithm learns from each training set
Train(c) a single feature vector vc, called the syn-
tactic model of the class. Therefore, in our algo-
rithm, the statement
vc = getContextV ector(c, Corpus)
in Figure 1 is substituted with
vc = getSyntacticModel(Train(c), Corpus).
For each class c, a set of syntactic features F (c)
are collected by finding the union of the features
extracted from each occurrence in the corpus of
each training instance in Train(c). Next, the fea-
ture vector vc is constructed: If a feature is not
present in F (c), then its corresponding coordinate
in vc has value 0; otherwise, it has a value equal to
the feature weight.
The weight of a feature fc ? F (c) is calculated
in three steps:
1. First, the co-occurrence of fc with the train-
ing set is calculated:
weight1(fc) =
?
t?Train(c)
?.log( P (fc, t)P (fc).P (t)
)
where P (fc, t) is the probability that feature
fc co-occurs with t, P (fc) and P (t) are the
probabilities that fc and t appear in the cor-
pus, ? = 14 for syntactic features with lexi-
cal element noun and ? = 1 for all the other
syntactic features. The ? parameter reflects
the linguistic intuition that nouns are more in-
formative than verbs and adjectives which in
most cases represent generic predicates. The
values of ? were automatically learned from
the training data.
2. We normalize the feature weights, since we
observed that they vary a lot between dif-
ferent classes: for each class c we find the
feature with maximal weight and denote its
weight with mxW (c),
mxW (c) = maxfc?F (c)weight1(fc)
Next, the weight of each feature fc ? F (c) is
normalized by dividing it with mxW (c):
weightN (fc) =
weight1(fc)
mxW (c)
3. To obtain the final weight of fc, we divide
weightN (fc) by the number of classes in
which this feature appears. This is motivated
by the intuition that a feature which appears
in the syntactic models of many classes is not
a good class predictor.
weight(fc) =
weightN (fc)
|Classes(fc)|
where |Classes(fc)| is the number of classes
for which fc is present in the syntactic model.
As shown in Figure 1, the classification uses a
similarity function sim(vt, vc) whose arguments
are the feature vector of a term vt and the feature
vector vc for a class c. We defined the similarity
function as the dot product of the two feature vec-
tors: sim(vt, vc) = vc.vt. Vectors vt are binary
(i.e. the feature value is 1 if the feature is present
and, 0-otherwise), while the features in the syntac-
tic model vectors vc receive weights according to
the approach described in this Section.
4 Representing Syntactic Information
Since both the Class-Word and the Class-Example
methods work with syntactic features, the main
source of information is a syntactically parsed cor-
pus. We parsed about half a gigabyte of a news
corpus with MiniPar (Lin, 1998b). It is a statis-
tically based dependency parser which is reported
to reach 89% precision and 82% recall on press re-
portage texts. MiniPar generates syntactic depen-
dency structures - directed labeled graphs whose
20
g1 g2 SyntNet(g1, g2)
loves|1
s
??
o
%%JJ
J
J
J
J
J
J
J
J
J
loves|4 o //
s
??
Jane|6 loves|1,4
(1,2)(4,5)
??
(4,6)
o
//
(1,3)
o
**TTT
T
T
T
T
T
T
T
T
T
T
T
T
T
T
Jane|6
John|2 Mary|3 John|5 John|2,5 Mary|3
Figure 2: Two syntactic graphs and their Syntactic Network.
vertices represent words and the edges between
them represent syntactic relations like subject, ob-
ject, modifier, etc. Examples for two dependency
structures - g1 and g2, are shown in Figure 2:
They represent the sentences ?John loves Mary?
and ?John loves Jane?; labels s and o on their
edges stand for subject and object respectively.
The syntactic structures generated by MiniPar are
dendroid (tree-like), but still cycles appear in some
cases.
In order to extract information from the parsed
corpus, we had to choose a model for represent-
ing dependency trees which allows to search ef-
ficiently for syntactic structures and to calculate
their frequencies. Building a classic index at word
level was not an option, since we have to search for
syntactic structures, not words. On the other hand,
indexing syntactic relations (i.e. word pair and the
relation between the words) would be useful, but
still does not resolve the problem, since in many
cases we search for more complex structures than
a relation between two words: for example, when
we have to find which words are syntactically re-
lated to a Named Entity composed by two words,
we have to search for syntactic structures which
consists of three vertices and two edges.
In order to trace efficiently more complex struc-
tures in the corpus, we put forward a model for
representation of a set of labeled graphs, called
Syntactic Network (SyntNet for short). The model
is inspired by a model presented earlier in (Szpek-
tor et al, 2004), however our model allows more
efficient construction of the representation. The
scope of SyntNet is to represent a set of labeled
graphs through one aggregate structure in which
the isomorphic sub-structures overlap. When
SyntNet represents a syntactically parsed text cor-
pus, its vertices are labeled with words from the
text while edges represent syntactic relations from
the corpus and are labeled accordingly.
An example is shown in Figure 2, where two
syntactic graphs, g1 and g2, are merged into
one aggregate representation SyntNet(g1, g2).
Vertices labeled with equal words in g1 and
g2 are merged into one generalizing vertex in
SyntNet(g1, g2). For example, the vertices with
label John in g1 and g2 are merged into one vertex
John in SyntNet(g1, g2).
Edges are merged in a similar way:
(loves, John) ? g1 and (loves, John) ? g2
are represented through one edge (loves, John)
in SyntNet(g1, g2).
Each vertex in g1 and g2 is labeled addition-
ally with a numerical index which is unique
for the graph set. Numbers on vertices in
SyntNet(g1, g2) show which vertices from g1
and g2 are merged in the corresponding Synt-
Net vertices. For example, vertex loves ?
SyntNet(g1, g2) has a set {1, 4} which means
that vertices 1 and 4 are merged in it. In a similar
way the edge (loves, John) ? SyntNet(g1, g2)
is labeled with two pairs of indices (4, 5) and
(1, 2), which shows that it represents two edges:
the edge between vertices 4 and 5 and the edge
between 1 and 2.
Two properties of SyntNet are important: first
isomorphic sub-structures from all the graphs rep-
resented via a SyntNet are mapped into one struc-
ture. This allows us to easily find all the oc-
currences of multiword terms and named enti-
ties. Second, using the numerical indices on ver-
tices and edges, we can efficiently calculate which
structures are connected syntactically to the train-
ing and test terms. As an example, let?s try to cal-
culate in which constructions the word ?Mary? ap-
pears considering SyntNet in Figure 2. First, in
SyntNet we can directly observe that there is the
relation loves?Mary labeled with the pair 1 ? 3
- therefore this relation appears once in the corpus.
Next, tracing the numerical indices on the ver-
tices and edges we can find a path from ?Mary? to
?John? through ?loves?. The path passes through
the following numerical indices: 3 ? 1 ? 2: this
means that there is one appearance of the structure
21
?John lovesMary? in the corpus, spanning through
vertices 1, 2, and 3. Such a path through the nu-
merical indices cannot be found between ?Mary?
and ?Jane? which means that they do not appear in
the same syntactic construction in the corpus.
SyntNet is built incrementally in a straightfor-
ward manner: Each new vertex or edge added to
the network is merged with the identical vertex or
edge, if such already exists in SyntNet. Otherwise,
a new vertex or edge is added to the network. The
time necessary for building a SyntNet is propor-
tional to the number of the vertices and the edges
in the represented graphs (and does not otherwise
depend on their complexity).
The efficiency of the SyntNet model when
representing and searching for labeled structures
makes it very appropriate for the representation of
a syntactically parsed corpus. We used the prop-
erties of SyntNet in order to trace efficiently the
occurrences of Named Entities in the parsed cor-
pus, to calculate their frequencies, to find the syn-
tactic features which co-occur with these Named
Entities, as well as the frequencies of these co-
occurrences. Moreover, the SyntNet model al-
lowed us to extract more complex, second order
syntactic features which are connected indirectly
to the terms in the training and the test set.
5 Experimental settings and results
We have evaluated all the three approaches de-
scribed in Section 3. The same evaluation settings
were used for the three experiments. The source
of features was a news corpus of about half a gi-
gabyte. The corpus was parsed with MiniPar and
a Syntactic Network representation was built from
the dependency parse trees produced by the parser.
Syntactic features were extracted from this Synt-
Net.
We considered two high-level Named Entity
categories: Locations and Persons. For each of
them five fine-grained sub-classes were taken into
consideration. For locations: mountain, lake,
river, city, and country; for persons: statesman,
writer, athlete, actor, and inventor.
For each class under consideration we created
a test set of Named Entities using WordNet 2.0
and Internet sites like Wikipedia. For the Class-
Example approach we also provided training data
using the same resources. WordNet was the pri-
mary data source for training and test data. The ex-
amples from it were extracted automatically. We
P (%) R (%) F (%)
mountain 58 78 67
lake 75 50 60
river 69 55 61
city 56 76 65
country 86 93 89
locations macro 69 70 68
locations micro 78 78 78
statesman 42 72 53
writer 93 55 69
athlete 90 47 62
actor 90 73 80
inventor 12 33 18
persons macro 65 56 57
persons micro 57 57 57
total macro 67 63 62
total micro 65 65 65
category location 83 91 87
category person 95 89 92
Table 1: Performance of the Class-Example ap-
proach.
used Internet to get additional examples for some
classes. To do this, we created automatic text ex-
traction scripts for Web pages and manually fil-
tered their output when it was necessary.
Totally, the test data comprised 280 Named En-
tities which were not ambiguous and appeared at
least twice in the corpus.
For the Class-Example approach we provided
a training set of 1194 names. The only require-
ment to the names in the training set was that
they appear at least twice in the parsed corpus.
They were allowed to be ambiguous and no man-
ual post-processing or filtering was carried out on
this data.
For both context feature approaches (i.e. Class-
Word and Class-Example), we used the same type
of syntactic features and the same classification
function, namely the one described in Section 3.3.
This was done in order to compare better the ap-
proaches.
Results from the comparative evaluation are
shown in Table 2. For each approach we mea-
sured macro average precision, macro average re-
call, macro average F-measure and micro average
F; for Class-Word and Class-Example micro F is
equal to the overall accuracy, i.e. the percent of the
instances classified correctly. The first row shows
22
macro P (%) macro R (%) macro F (%) micro F(%)
Class-Patterns 18 6 9 10
Class-Word 32 41 33 42
Class-Example 67 63 62 65
Class-Example (sec. ord.) 65 61 62 68
Table 2: Comparison of different approaches.
the results obtained with superficial patterns. The
second row presents the results from the Class-
Word approach. The third row shows the results
of our Class-Example method. The fourth line
presents the results for the same approach but us-
ing second-order features for the person category.
The Class-Pattern approach showed low perfor-
mance, similar to the random classification, for
which macro and micro F=10%. Patterns suc-
ceeded to classify correctly only instances of the
classes ?river? and ?city?. For the class ?city?
the patterns reached precision of 100% and recall
65%; for the class ?river? precision was high (i.e.
75%), but recall was 15%.
The Class-Word approach showed significantly
better performance (macro F=33%, micro F=42%)
than the Class-Pattern approach.
The performance of the Class-Example (62%
macro F and 65%-68% micro F) is much higher
than the performance of Class-Word (29% in-
crease in macro F and 23% in micro F). The last
row of the table shows that second-order syntactic
features augment further the performance of the
Class-Example method in terms of micro average
F (68% vs. 65%).
A more detailed evaluation of the Class-
Example approach is shown in Table 1. In this
table we show the performance of the approach
without the second-order features. Results vary
between different classes: The highest F is mea-
sured for the class ?country? - 89% and the low-
est is for the class ?inventor? - 18%. However,
the class ?inventor? is an exception - for all the
other classes the F measure is over 50%. Another
difference may be observed between the Location
and Person classes: Our approach performs sig-
nificantly better for the locations (68% vs. 57%
macro F and 78% vs. 57% micro F). Although
different classes had different number of training
examples, we observed that the performance for
a class does not depend on the size of its training
set. We think, that the variation in performance be-
tween categories is due to the different specificity
of their textual contexts. As a consequence, some
classes tend to co-occur with more specific syn-
tactic features, while for other classes this is not
true.
Additionally, we measured the performance
of our approach considering only the macro-
categories ?Location? and ?Person?. For this pur-
pose we did not run another experiment, we rather
used the results from the fine-grained classifica-
tion and grouped the already obtained classes. Re-
sults are shown in the last two rows of table 1: It
turns out that the Class-Example method makes
very well the difference between ?location? and
?person? - 90% of the test instances were classi-
fied correctly between these categories.
6 Conclusions and future work
In this paper we presented a new weakly super-
vised approach for Ontology Population, called
Class-Example, and confronted it with two other
methods. Experimental results show that the
Class-Example approach has best performance. In
particular, it reached 65% of accuracy, outper-
forming in our experimental framework the state-
of-the-art Class-Word method by 42%. Moreover,
for location names the method reached accuracy
of 78%. Although the experiments are not com-
parable, we would like to state that some super-
vised approaches for fine-grained Named Entity
classification, e.g. (Fleischman, 2001), have sim-
ilar accuracy. On the other hand, the presented
weakly supervised Class-Example approach re-
quires as a training data only a list of terms for
each class under consideration. Training exam-
ples can be automatically acquired from existing
ontologies or other sources, since the approach
imposes virtually no restrictions on them. This
makes our weakly supervised methodology appli-
cable on larger scale than supervised approaches,
still having significantly better performance than
the unsupervised ones.
In our experimental framework we used syntac-
tic features extracted from dependency parse trees
23
and we put forward a novel model for the repre-
sentation of a syntactically parsed corpus. This
model allows for performing a comprehensive ex-
traction of syntactic features from a corpus includ-
ing more complex second-order ones, which re-
sulted in an improvement of performance. This
and other empirical observations not described in
this paper lead us to the conclusion that the per-
formance of an Ontology Population system im-
proves with the increase of the types of syntactic
features under consideration.
In our future work we consider applying our
Ontology Population methodology to more se-
mantic categories and to experiment with other
types of syntactic features, as well as other types
of feature-weighting formulae and learning algo-
rithms. We consider also the integration of the
approach in a Question Answering or Information
Extraction system, where it can be used to perform
fine-grained type checking.
References
A. Almuhareb and M. Poesio. 2004. Attribute-
based and value-based clustering: An evaluation.
In Proceedings of EMNLP 2004, pages 158?165,
Barcelona, Spain.
H. Avancini, A. Lavelli, B. Magnini, F. Sebastiani, and
R. Zanoli. 2003. Expanding Domain-Specific Lex-
icons by Term Categorization. In Proceedings of
SAC 2003, pages 793?79.
K. Bontcheva and H. Cunningham. 2003. The Se-
mantic Web: A New Opportunity and Challenge for
HLT. In Proceedings of the Workshop HLT for the
Semantic Web and Web Services at ISWC 2003.
P. Buitelaar, P. Cimiano, and B. Magnini, editors.
2005. Ontology Learning from Text: Methods, Eval-
uation and Applications. IOS Press, Amsterdam,
The Netherlands.
P. Cimiano and J. Vo?lker. 2005. Towards large-scale,
open-domain and ontology-based named entity clas-
sification. In Proceedings of RANLP?05, pages 166?
172, Borovets, Bulgaria.
P. Cimiano, A. Pivk, L.S. Thieme, and S. Staab. 2005.
Learning Taxonomic Relations from Heterogeneous
Sources of Evidence. In Ontology Learning from
Text: Methods, Evaluation and Applications. IOS
Press.
M. Fleischman and E. Hovy. 2002. Fine Grained
Classification of Named Entities. In Proceedings of
COLING 2002, Taipei, Taiwan, August.
M. Fleischman. 2001. Automated Subcategorization
of Named Entities. In 39th Annual Meeting of the
ACL, Student Research Workshop, Toulouse, France,
July.
M. Hearst. 1998. Automated Discovery of Word-
Net Relations. In WordNet: An Electronic Lexical
Database. MIT Press.
D. Lin. 1998a. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of COLING-ACL98,
Montreal, Canada, August.
D. Lin. 1998b. Dependency-based Evaluation of Mini-
Par. In Proceedings of Workshop on the Evaluation
of Parsing Systems, Granada, Spain.
S. Schlobach, M. Olsthoorn, and M. de Rijke. 2004.
Type Checking in Open-Domain Question Answer-
ing. In Proceedings of ECAI 2004.
I. Szpektor, H. Tanev, I. Dagan, and B. Coppola. 2004.
Scaling Web-based Acquisition of Entailment Rela-
tions. In Proceedings of EMNLP 2004, Barcelona,
Spain.
P. Velardi, R.Navigli, A. Cuchiarelli, and F.Neri. 2005.
Evaluation of Ontolearn, a Methodology for Auto-
matic Population of Domain Ontologies. In P. Buite-
laar, P. Cimiano, and B. Magnini, editors, Ontology
Learning from Text: Methods, Evaluation and Ap-
plications. IOS Press.
24
Is It the Right Answer?
Exploiting Web Redundancy for Answer Validation
Bernardo Magnini, Matteo Negri, Roberto Prevete and Hristo Tanev
ITC-Irst, Centro per la Ricerca Scientifica e Tecnologica
[magnini,negri,prevete,tanev]@itc.it
Abstract
Answer Validation is an emerging topic
in Question Answering, where open do-
main systems are often required to rank
huge amounts of candidate answers. We
present a novel approach to answer valida-
tion based on the intuition that the amount
of implicit knowledge which connects an
answer to a question can be quantitatively
estimated by exploiting the redundancy of
Web information. Experiments carried out
on the TREC-2001 judged-answer collec-
tion show that the approach achieves a
high level of performance (i.e. 81% suc-
cess rate). The simplicity and the effi-
ciency of this approach make it suitable to
be used as a module in Question Answer-
ing systems.
1 Introduction
Open domain question-answering (QA) systems
search for answers to a natural language question
either on the Web or in a local document collec-
tion. Different techniques, varying from surface pat-
terns (Subbotin and Subbotin, 2001) to deep seman-
tic analysis (Zajac, 2001), are used to extract the text
fragments containing candidate answers. Several
systems apply answer validation techniques with the
goal of filtering out improper candidates by check-
ing how adequate a candidate answer is with re-
spect to a given question. These approaches rely
on discovering semantic relations between the ques-
tion and the answer. As an example, (Harabagiu
and Maiorano, 1999) describes answer validation as
an abductive inference process, where an answer is
valid with respect to a question if an explanation for
it, based on background knowledge, can be found.
Although theoretically well motivated, the use of se-
mantic techniques on open domain tasks is quite ex-
pensive both in terms of the involved linguistic re-
sources and in terms of computational complexity,
thus motivating a research on alternative solutions
to the problem.
This paper presents a novel approach to answer
validation based on the intuition that the amount of
implicit knowledge which connects an answer to a
question can be quantitatively estimated by exploit-
ing the redundancy of Web information. The hy-
pothesis is that the number of documents that can
be retrieved from the Web in which the question and
the answer co-occur can be considered a significant
clue of the validity of the answer. Documents are
searched in the Web by means of validation pat-
terns, which are derived from a linguistic process-
ing of the question and the answer. In order to test
this idea a system for automatic answer validation
has been implemented and a number of experiments
have been carried out on questions and answers pro-
vided by the TREC-2001 participants. The advan-
tages of this approach are its simplicity on the one
hand and its efficiency on the other.
Automatic techniques for answer validation are
of great interest for the development of open do-
main QA systems. The availability of a completely
automatic evaluation procedure makes it feasible
QA systems based on generate and test approaches.
In this way, until a given answer is automatically
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 425-432.
                         Proceedings of the 40th Annual Meeting of the Association for
proved to be correct for a question, the system will
carry out different refinements of its searching crite-
ria checking the relevance of new candidate answers.
In addition, given that most of the QA systems rely
on complex architectures and the evaluation of their
performances requires a huge amount of work, the
automatic assessment of the relevance of an answer
with respect to a given question will speed up both
algorithm refinement and testing.
The paper is organized as follows. Section 2
presents the main features of the approach. Section 3
describes how validation patterns are extracted from
a question-answer pair by means of specific question
answering techniques. Section 4 explains the basic
algorithm for estimating the answer validity score.
Section 5 gives the results of a number of experi-
ments and discusses them. Finally, Section 6 puts
our approach in the context of related works.
2 Overall Methodology
Given a question   and a candidate answer  the an-
swer validation task is defined as the capability to as-
sess the relevance of  with respect to   . We assume
open domain questions and that both answers and
questions are texts composed of few tokens (usually
less than 100). This is compatible with the TREC-
2001 data, that will be used as examples throughout
this paper. We also assume the availability of the
Web, considered to be the largest open domain text
corpus containing information about almost all the
different areas of the human knowledge.
The intuition underlying our approach to an-
swer validation is that, given a question-answer pair
([   ,  ]), it is possible to formulate a set of valida-
tion statements whose truthfulness is equivalent to
the degree of relevance of  with respect to   . For
instance, given the question ?What is the capital of
the USA??, the problem of validating the answer
?Washington? is equivalent to estimating the truth-
fulness of the validation statement ?The capital of
the USA is Washington?. Therefore, the answer
validation task could be reformulated as a problem
of statement reliability. There are two issues to be
addressed in order to make this intuition effective.
First, the idea of a validation statement is still insuf-
ficient to catch the richness of implicit knowledge
that may connect an answer to a question: we will
attack this problem defining the more flexible idea
of a validation pattern. Second, we have to design
an effective and efficient way to check the reliability
of a validation pattern: our solution relies on a pro-
cedure based on a statistical count of Web searches.
Answers may occur in text passages with low
similarity with respect to the question. Passages
telling facts may use different syntactic construc-
tions, sometimes are spread in more than one sen-
tence, may reflect opinions and personal attitudes,
and often use ellipsis and anaphora. For instance, if
the validation statement is ?The capital of USA is
Washington?, we have Web documents containing
passages like those reported in Table 1, which can
not be found with a simple search of the statement,
but that nevertheless contain a significant amount of
knowledge about the relations between the question
and the answer. We will refer to these text fragments
as validation fragments.
1. Capital Region USA: Fly-Drive Holidays in
and Around Washington D.C.
2. the Insider?s Guide to the Capital Area Music
Scene (Washington D.C., USA).
3. The Capital Tangueros (Washington, DC
Area, USA)
4. I live in the Nation?s Capital, Washington
Metropolitan Area (USA).
5. in 1790 Capital (also USA?s capital): Wash-
ington D.C. Area: 179 square km
Table 1: Web search for validation fragments
A common feature in the above examples is the
co-occurrence of a certain subset of words (i.e.
?capital?,?USA? and ?Washington?). We will make
use of validation patterns that cover a larger portion
of text fragments, including those lexically similar
to the question and the answer (e.g. fragments 4 and
5 in Table 1) and also those that are not similar (e.g.
fragment 2 in Table 1). In the case of our example
a set of validation statements can be generalized by
the validation pattern:
[capital  text  USA  text  Washington]
where  text  is a place holder for any portion of
text with a fixed maximal length.
To check the correctness of  with respect to  
we propose a procedure that measures the number
of occurrences on the Web of a validation pattern
derived from  and   . A useful feature of such pat-
terns is that when we search for them on the Web
they usually produce many hits, thus making statis-
tical approaches applicable. In contrast, searching
for strict validation statements generally results in a
small number of documents (if any) and makes sta-
tistical methods irrelevant. A number of techniques
used for finding collocations and co-occurrences of
words, such as mutual information, may well be
used to search co-occurrence tendency between the
question and the candidate answer in the Web. If we
verify that such tendency is statistically significant
we may consider the validation pattern as consistent
and therefore we may assume a high level of correla-
tion between the question and the candidate answer.
Starting from the above considerations and given
a question-answer pair     , we propose an answer
validation procedure based on the following steps:
1. Compute the set of representative keywords
	
  and 	  both from   and from  ; this step is
carried out using linguistic techniques, such as
answer type identification (from the question)
and named entities recognition (from the an-
swer);
2. From the extracted keywords compute the vali-
dation pattern for the pair [    ];
3. Submit the patterns to the Web and estimate an
answer validity score considering the number
of retrieved documents.
3 Extracting Validation Patterns
In our approach a validation pattern consists of two
components: a question sub-pattern (Qsp) and an
answer sub-pattern (Asp).
Building the Qsp. A Qsp is derived from the input
question cutting off non-content words with a stop-
words filter. The remaining words are expanded
with both synonyms and morphological forms in
order to maximize the recall of retrieved docu-
ments. Synonyms are automatically extracted from
the most frequent sense of the word in WordNet
(Fellbaum, 1998), which considerably reduces the
risk of adding disturbing elements. As for morphol-
ogy, verbs are expanded with all their tense forms
(i.e. present, present continuous, past tense and past
participle). Synonyms and morphological forms are
added to the Qsp and composed in an OR clause.
The following example illustrates how the Qsp
is constructed. Given the TREC-2001 question
?When did Elvis Presley die??, the stop-words filter
removes ?When? and ?did? from the input. Then
synonyms of the first sense of ?die? (i.e. ?decease?,
?perish?, etc.) are extracted from WordNet. Finally,
morphological forms for all the corresponding verb
tenses are added to the Qsp. The resultant Qsp will
be the following:
[Elvis  text  Presley  text  (die OR died OR
dying OR perish OR ...)]
Building the Asp. An Asp is constructed in two
steps. First, the answer type of the question is iden-
tified considering both morpho-syntactic (a part of
speech tagger is used to process the question) and
semantic features (by means of semantic predicates
defined on the WordNet taxonomy; see (Magnini et
al., 2001) for details). Possible answer types are:
DATE, MEASURE, PERSON, LOCATION, ORGANI-
ZATION, DEFINITION and GENERIC. DEFINITION
is the answer type peculiar to questions like ?What
is an atom?? which represent a considerable part
(around 25%) of the TREC-2001 corpus. The an-
swer type GENERIC is used for non definition ques-
tions asking for entities that can not be classified as
named entities (e.g. the questions: ?Material called
linen is made from what plant?? or ?What mineral
helps prevent osteoporosis??)
In the second step, a rule-based named entities
recognition module identifies in the answer string
all the named entities matching the answer type cat-
egory. If the category corresponds to a named en-
tity, an Asp for each selected named entity is cre-
ated. If the answer type category is either DEFINI-
TION or GENERIC, the entire answer string except
the stop-words is considered. In addition, in order
to maximize the recall of retrieved documents, the
Asp is expanded with verb tenses. The following
example shows how the Asp is created. Given the
TREC question ?When did Elvis Presley die?? and
the candidate answer ?though died in 1977 of course
some fans maintain?, since the answer type category
is DATE the named entities recognition module will
select [1977] as an answer sub-pattern.
4 Estimating Answer Validity
The answer validation algorithm queries the Web
with the patterns created from the question and an-
swer and after that estimates the consistency of the
patterns.
4.1 Querying the Web
We use a Web-mining algorithm that considers the
number of pages retrieved by the search engine. In
contrast, qualitative approaches to Web mining (e.g.
(Brill et al, 2001)) analyze the document content,
as a result considering only a relatively small num-
ber of pages. For information retrieval we used the
AltaVista search engine. Its advanced syntax allows
the use of operators that implement the idea of vali-
dation patterns introduced in Section 2. Queries are
composed using NEAR, OR and AND boolean opera-
tors. The NEAR operator searches pages where two
words appear in a distance of no more than 10 to-
kens: it is used to put together the question and the
answer sub-patterns in a single validation pattern.
The OR operator introduces variations in the word
order and verb forms. Finally, the AND operator is
used as an alternative to NEAR, allowing more dis-
tance among pattern elements.
If the question sub-pattern 
 does not return
any document or returns less than a certain thresh-
old (experimentally set to 7) the question pattern
is relaxed by cutting one word; in this way a new
query is formulated and submitted to the search en-
gine. This is repeated until no more words can be
cut or the returned number of documents becomes
higher than the threshold. Pattern relaxation is per-
formed using word-ignoring rules in a specified or-
der. Such rules, for instance, ignore the focus of the
question, because it is unlikely that it occurs in a
validation fragment; ignore adverbs and adjectives,
because are less significant; ignore nouns belonging
to the WordNet classes ?abstraction?, ?psychologi-
cal feature? or ?group?, because usually they specify
finer details and human attitudes. Names, numbers
and measures are preferred over all the lower-case
words and are cut last.
4.2 Estimating pattern consistency
The Web-mining module submits three searches to
the search engine: the sub-patterns [Qsp] and [Asp]
and the validation pattern [QAp], this last built as
the composition [Qsp NEAR Asp]. The search en-
gine returns respectively: 
 , Experiments in Word Domain Disambiguation for Parallel 
Texts 
Bernardo  Magnin i  and Car lo  S t rapparava  
ITC- i rs t ,  I s t i tuto  per  la R icerca  Scienti f ica e Tecnologica,  1-38050 Trento ,  ITALY  
email :  {magnin i , s t rappa)@irs t . i t  c. it 
Abst rac t  
This paper describes ome preliminary 
results about Word Domain Disambigua- 
tion, a variant of Word Sense Disam- 
bignation where words in a text are 
tagged with a domain label in place of a 
sense label. The English WoRDNET and 
its aligned Italian version, MULTIWORD- 
NET, both augmented with domain la- 
bels, are used as the main information 
repositories. A baseline algorithm for 
Word Domain Disambiguation is pre- 
sented and then compared with a mu- 
tual help disambignation strategy, which 
takes advantages of the shared senses of 
parallel bilingual texts. 
1 In t roduct ion  
This work describes some preliminary results 
about Word Domain Disambignation (WDD), a 
variant of Word Sense Disambiguation (WSD) 
where for each word in a text a domain label 
(among those allowed by the word) has to be cho- 
sen instead of a sense label. Domain labels, such 
as MEDICINE and ARCHITECTURE, provide a nat- 
ural way to establish semantic relations among 
word senses, grouping them into homogeneous 
clusters. A relevant consequence of the appli- 
cation of domain clustering over the WORDNET 
senses is the reduction of the word polysemy (i.e. 
the number of domains for a word is generally 
lower than the number of senses for that word). 
We wanted to investigate the hypothesis that 
the polysemy reduction caused by domain clus- 
tering can profitably help the word domain disam- 
bignation process. A preliminary experiment has 
been set up with two goals: first, providing exper- 
imental evidences that a frequency based WDD 
algorithm can outperform a WSD baseline algo- 
rithm; second, exploring WDD in the context of 
parallel, not aligned, text disambignation. 
The English WOR~DNET and the Italian aligned 
version MULTIWORDNET, both augmented with 
domain labels, are used as the main information 
repositories. A baseline algorithm for Word Do- 
main Disambignation is presented and then com- 
pared with a mutual help disambignation strategy, 
which makes use of the shared senses of parallel 
bilingual texts. 
Several works in the literature have remarked 
that for many practical purposes the fine-grained 
sense distinctions provided by WoRDNET are not 
necessary (see for example \[Wilks and Stevenson, 
98\], \[Gonzalo et al, 1998\], \[Kilgarriff and Yallop, 
2000\] and the SENSEVAL initiative) and make 
it hard word sense disambignation. Two related 
works are also \[Buitelaar, 1998\] and \[Buitelaar, 
2000\], where the reduction of the WORDNET pol- 
? ysemy is obtained on the basis of regnlar polysemy 
relations. Our approach is based on sense clusters 
derived by domain proximity, which in some case 
may overlap with regular polysemy derived clus- 
ters (e.g. both "book" as composition and "book" 
as physical object belong to PUBLISHIN6), but in 
many cases may not (e.g. "lamb" as animal be- 
longs to ZOOLOQY, while "lamb" as meat belongs 
to FooD). Following this line we propose Word 
Domain Disambiguation asa practical alternative 
for applications that do not require fine grained 
sense distinctions. 
The paper is organized as follows. Section 2 
introduces domain labels, their organization and 
the extensions to WORDNET. Section 3 discusses 
Word Domain Disambiguation and presents the 
algorithms used in the experiment. Section 4 gives 
the experimental setting. Results are discussed in 
Section 5. 
2 WordNet  and  Sub ject  F ie ld  
Codes  
In this work we will make use of an augmented 
WoRDNET, whose synsets have been annotated 
with one or more subject field codes. This re- 
source, discussed in \[Magnini and Cavagli~, 2000\], 
27 
currently covers all the noun synsets of WORD- 
NET 1.6 \[Miller, 1990\], and it is under develop- 
ment for the remaining lexical categories. 
Subject Field Codes (SFC) group together 
words relevant for a specific domain. The best 
approximation of SFCs are the field labels used 
in dictionaries (e.g. MEDICINE, A~;CHITECTURE), 
even if their use is restricted to word usages be- 
longing to specific terminological domains. In 
WORDNET, too, SFCs seem to be treed occasion- 
ally and without a consistent desi~;n. 
Information brought by SFCs is complemen- 
tary to what is already in WoRDNET. First of 
all a SFC may include synsets of different syntac- 
tic categories: for instance MEDICINE 1 groups to- 
gether senses from Nouns, such as doctora l  and 
hospital#I, and from Verbs such as operate#7. 
Second, a SFC may also contain .,~nses from dif- 
ferent WORDNET sub-hierarchies (i.e. deriving 
from different "unique beginners, or from dif- 
ferent "lexicographer files"). For example, the 
SPORT SFC contains senses such as athlete#I, 
deriving from li:~e~orm#1, game_equipment#1 
from physical_object#1, sport#1 from act#2, 
and playingJield#1 from location#1. 
We have organized about 250 SFCs in a hier- 
archy, where each level is made up of codes of 
the same degree of specificity: for example, the 
second level includes SFCs such as BOTANY, LIN- 
GUISTICS, HISTORY, SPORT and RELIGION, while 
at the third level we can find specializations such 
as AMERICAN.HISTORY, GRAMMAR,  PHONETICS 
and TENNIS. 
A problem arises for synsets that do not belong 
to a specific SFC, but rather can appear in almost 
all of them. For this reason, a FACTOTUM SFC has 
been created which basically includes two types of 
synsets: 
Gener/c synsets, which are hard to classify in 
a particular SFC, are generally placed high 
in the WoRDNET hierarchy and are related 
senses of highly polysemous words. For ex- 
ample: 
man#1 an adult male person (as opposed to a 
woman) 
man#3 the generic use of the word to refer to any 
human being 
date#1 day of the month 
aThroughout the paper subject field codes are ino 
cUcated with this TYPEFACE while word senses are re- 
ported with this typeface#l, with their corresponding 
numbering in WORDNET 1.6. Moreover, we use sub. 
ject field code, domain label and semantic field with 
the same meaning. 
dal;e#3 appointment, engagement 
? Stop Senses ynsets which appear frequently 
in different contexts, such as numbers, week 
days, colors, etc. These synsets usually be- 
long to non polysemous words and they be- 
have much as stop words, because they do not 
significantly contribute to the overall mean- 
ing of a text. 
A single domain label may group together more 
than one word sense, resulting in a reduction of 
the polysemy. Figure 1 shows an example. The 
word "book" has seven different senses in WORD- 
NET 1.6: three of them are grouped under the 
PUBLISHING domain, causing the reduction of the 
polysemy from 7 to 5 senses. 
3 Word  Domain  D isambiguat ion  
In this section we present wo baseline algorithms 
for word domain disambiguation and we propose 
some variants of them to deal with WDD in the 
context of parallel texts. 
3.1 Basel ine a lgor i thms 
To decide a proper baseline for Word Domain Dis- 
ambiguation we wanted to be sure that it was ap- 
plicable to both the languages (i.e. English and 
Italian) used in the experiment. This caused the 
exclusion of a selection based on the domain fre- 
quency computed as a function of the frequency 
of the WORDNET senses, because we did not 
have a frequency estimation for Italian senses. 
We adopted two alternative frequency measures, 
based respectively on the intra text frequency and 
the intra word frequency of a domain label. Both 
of them are computed with a two-stage disam- 
bignation process, structurally similar to the al- 
gorithm used in \[Voorhees, 1998\]. 
Baseline 1: Intra text domain  frequency. 
The baseline algorithm follows two steps. First, 
all the words in the text are considered and for 
each domain label allowed by the word the label 
score is incremented by one. At the second step 
each word is reconsidered, and the domain label 
(or labels, depending on how many best solutions 
are requested) with the highest score is selected 
as the result of the disambiguation. 
Basel ine 2: In t ra  word  domain  f requency.  
In this version of the baseline algorithm, step 1 is 
modified in that each domain label allowed by the 
word is incremented by the frequency of the la- 
bel among the senses of that word. For instance, 
28 
{book#1 - p~blishtd co, npositio. }
PUBLISHING 
"book" 
{book#2 volume#3 - book a.? a physical objea} 
{daybook#2 book#7 ledger#2 - an accounting book as aphisical object} 
{book#6 - book of the B/b/e} PtrBHSmNG RELIGION 
THEA~ 
{script#1 book#4 playscript#1-mriuenvemionojraplay} 
~ ok#1 
COM~nZCE 
book#5 ledger#l - recorda of commercial acc.ount} 
FACTOTt~ 
record#5 recordbook#1 book#3-compilationoflmowfact~ 
regaMing $ome~ing or someone} 
Figure I: An  example of polysemy reduction 
if "book" is the word (see Figure 1), PUBLISH- 
ING will receive .42 (i.e. three senses out of seven 
belong to PUBLISHING), while the others domain 
labels will receive .14 each. 
3.1.1 The  " fac to tum"  effect 
As we mentioned in Section 2, a FACTOTUM la- 
bel is used to mark WORDNET senses that do not 
belong to a specific domain, but rather are highly 
widespread across texts of different domains. A 
consequence is that very often, at the end of step 1 
of the disambignation algorithm, FACTOTUM out- 
performs the other domains, this way affecting the 
selection carried out at step 2 (i.e. in case of am- 
biguity FACTOTUM is often preferred). 
For the purposes of the experiment described 
in the next sections the FACTOTUM problem has 
been resolved with a slight modification at step 2 
of the baseline algorithm: when FACTOTUM is the 
best selection for a word, also the second available 
choice is considered as a result of the disambigua- 
tion process. 
3.2 Extens ions  for paral le l  texts  
We started with the following working hypothe- 
sis. Using aligned wordnets to disambiguate par- 
allel texts allows us to calculate the intersection 
among the synsets accessible from an English text 
through the English WoRDNET and the synsets 
accessible from the parallel Italian text through 
the Italian WORDNET. It would seem reasonable 
that the synset intersection maximizes the num- 
ber of significant synsets for the two texts, and 
at the same time tends to exclude synsets whose 
meaning is not pertinent to the content of the text. 
Let us try to make the point clearer with an 
example. Suppose we find in an English text the 
word "bank" and in the Italian parallel text the 
word "banca',  which we do not know being the 
translation of "bank", because we do not have 
word alignments. For "bank" we get ten senses 
from WORDNET 1.6 (reported in Figure 2), while 
for "banca" we get two senses from MULTIWORD- 
NET (reported in Figure 2). As the two wordnets 
are aligned (i.e. they share synset offsets), the 
intersection can be straightforwardly determined. 
In this case it includes 06227059, corresponding to
bank#1 and banca#1, and 02247680, correspond- 
ing to bank#4 and banca#2, which both pertain 
to the BANKING domain, and excludes, among the 
others, bank#2, which happens to be an homonym 
sense in English but not in Italian. 
Incidentally, if "istituto di credito" were not in 
the synset 06227059 (e.g. because of the incom- 
pleteness of the Italian WORDNET) and it were 
the only word present in the Italian news to de- 
notate the bank#1 sense, the synset intersection 
would have been empty. 
As far as disambiguation is concerned it seems 
a reasonable hypothesis that the synset intersec- 
tion could bring constraints on the sense selection 
for a word (i.e. it is highly probable that the cor- 
rect choice belongs to the intersection). Following 
this line we have elaborated a mutua l  he lp  d i sam-  
biguation strategy where the synset intersection 
can be accessed to help the disambiguation pro- 
cess of both English and Italian texts. 
In addition to the synset intersection, we 
wanted to consider the intersection of domain la- 
bels, that is domains that are shared among the 
29 
Bank (from WordNet 1.6) 
1. J{06227059}\[ depos i tory  f inanc ia l  ins t i tu t ion ,  bank, banking concern, banking company 
I I 
-- (a financial institution that accepts deposits and channels the money into lending 
act iv i t ies ;  ) 
2. {06800223} bank -- (sloping land (especially the slope beside a body of water)) 
3. {09626760} bank -- (a supply or stock held in reserve especially for future use 
(especially in emergencies)) 
4. {02247680}\[ bank, bank building -- (a building in which commercial banking is 
transacted; ) 
B 
5. {06250735} bank -- (an ~rrangement of similar objects in a row or in tiers; ) 
6. {03277560} savings bank, coin bank, money box, bank -- (a container (usually with a 
slot in the top) for keeping money at home;) 
7. {06739355} bank -- (a long ridge or pile; "a huge bank of earth") 
8. {09616845} bank -- (the :funds held by a gambling house or the dealer in some gambling 
games; )
9. {06800468} bank, cant, camber -- (a slope in the turn of a road or track;) 
I0. {00109955} bank -- (a flight maneuver; aircraft tips laterally about its longitudinal 
axis  (espec ia l l y  in turn ing) )  
Banca (from MultiWordnet) 
1. 1{06227059}1 i s t i tu to .d i _c red i to  cassa banco banca 
q B 
I{0  4,680}\[ ban. 
Figure 2: An example of sysnet intersection i  MULTIWORDNET 
senses of the parallel texts. In the example above 
the domain intersection would include just one la- 
bel (i.e. BANKING), in place of the two synsets 
of the synset intersection. The hypothesis i that 
domain intersection could reduce problems due to 
possible misalignments among the synsets of the 
two wordnets. 
Two mutual help algorithms have been imple- 
mented, weak mutual help and strong mutual help, 
which are described in the following. 
Weak Mutua l  help. In this version of the mu- 
tual help algorithm, step 1 of the baseline is mod- 
itied in that, if the domain label is found in the 
synset or domain intersection, a bonus is assigned 
to that label, doubling its score. In case of empty 
intersection (i.e. either no synset or no domain is 
shared by the two texts) this algorithm guarantees 
the same performances of the baseline. 
Strong Mutua l  help. In the strong version of 
the mutual help strategy, step 1 of the baseline 
is modified in that the domain label is scored if 
and only if it is found in the synset or domain 
intersection. While this algorithm does not guar- 
antee the baseline performance (because the inter- 
section may not contain all the correct synsets or 
domains), the precision score will give us indica- 
tions about the quality of the synset intersection. 
4 Exper imenta l  Set t ing  
The goal of the experiment is to establish some 
reference figures for Word Domain Disambigua- 
tion. Only nouns have been considered, mostly 
because the coverage of both MULTIWORDNET 
and of the domain mapping for verbs is far from 
being complete. 
Lemmas I Senses \[Mean Polysemy 
WN 1.6 94474 116317 1.23 
Ital WN 19104 25226 1.32 
DISC 56134 118029 2.10 
Table 1: Overview of the used resources (Noun 
part of speech) 
4.1 Lexlcal resources 
Besides the English WORDNET 1.6 we used MUL- 
TIWoRDNET \[Artale et al, 1997; Magnini and 
Strapparava, 1997\], an Italian version of the En- 
glish WoRDNET. It is based on the assump- 
tion that a large part of the conceptual relations 
defined for the English language can be shared 
with Italian. From an architectural point of view, 
MULTIWORDNET implements an extension of the 
WoRDNET lexical matrix to a "multilingual lexi- 
30 
Mean Values for Nouns Italian News English News 
Lexical Coverage WN 1.6 
ItalWN 
Disc 
# Synsets English 
Italian 
Intersection 
93% 
100% 
111.38 
35.48 
98% 
155.21 
Table 2: Mean lexical coverage and synset amount for AdnKronos news 
Mean Values for Nouns \] Italian News English News 
Sense Polysemy WN 1.6 
ItalWN 
Disc 
Domain Polysemy English 
Italian 
3.22 
6.82 
2.68 
4.37 
3.58 
Table 3: Mean sense and domain polysemy for AdnKronos news 
cal matrix" through the addition of a third dimen- 
sion relative to the language. MULTIWORDNET 
currently includes about 30,000 lemmas. 
As a matter of comparison, in particular to es- 
timate the lack of coverage of MULTIWORDNET, 
we consider some data from the Italian dictionary 
"DISC" \[Sabatini and Coletti, 1997\], a large size 
monolingual dictionary, available both as printed 
version and as CD-ROM. 
Table 1 shows some general figures (only for 
nouns) about the number of lemmas, the number 
of senses and the average polysemy for the three 
lexical resources considered. 
4.2 Para l le l  Texts 
Experiments have been carried out on a news cor- 
pus kindly placed at our disposal by AdnKronos, 
an important Italian news provider. The corpus 
consists of 168 parallel news (i.e. each news has 
both an Italian and an English version) concerning 
various topics (e.g. politics, economy, medicine, 
food, motors, fashion, culture, holidays). The av- 
erage length of the news is about 265 words. 
Table 2 reports the average lexical coverage (i.e. 
percent of lemmas found in the news corpus) for 
WORDNET 1.6, MULT IWORDNET and the Disc 
dictionary. A practically zero variance among the 
various news is exhibited. We observe a full cov- 
erage for the Disc dictionary; in addition, the in- 
completeness of MULT IWORDNET is limited to 
5% with respect to WoRDNET 1.6. The table 
also reports the average amount of unique synsets 
for each news. In this case the incompleteness of 
Italian WoRDNET with respect to WORDNET 1.6 
raises to 30%, showing that a significant amount 
of word senses is missing. 
Table 3 shows the average polysemy of the news 
corpus considering both word senses and word do- 
main labels. The figures reveal a polysemy reduc- 
tion of 17-18% when we deal with domain poly- 
semy. 
Manua l  Annotation. A subset of forty news 
pairs (about half of the initial corpus) have been 
manually annotated with the correct domain la- 
bel. Annotators were instructed about the domain 
hierarchy and then asked to select one domain la- 
bel for each lemma among those allowed by that 
lemma. 
Uncertain cases have been reviewed by a sec- 
ond annotator and, in case of persisting conflict, a 
third annotator was consulted to take a decision. 
Lemmatization errors as well as cases of incom- 
plete coverage of domain labels have been detected 
and excluded. The whole manual set consists of 
about 2500 annotated nouns. 
Although we do not have empirical evidences, 
our practical experience confirms the intuition 
that annotating texts with domain labels is an 
easier task than sense annotation. 
Forty-two domain labels, representing the more 
informative level of the domain hierarchy men- 
tioned in Section 1, have been used for the ex- 
periment. Table 4 reports the complete list. 
5 Resu l ts  and  D iscuss ion  
WSD and WDD on the  Semcor  Brown Cor-  
pus. In the first experiment we wanted to verify 
that, because of the polysemy reduction induced 
by domain clustering, WDD is a simpler task than 
31 
administration 
art 
commerce 
fashion 
mathematics 
play 
sociology 
agriculture 
artisanship 
computer.science 
history 
medicine 
politics 
sport 
alimentation 
astrology 
earth 
industry 
military 
psychology 
telecommunication 
anthropology 
astronomy 
economy 
law 
pedagogy 
publishing 
tourism 
archaeology 
biology 
engineering 
linguistics 
philosophy 
religion 
transport 
architecture 
chemistry 
factotum 
literature 
physics 
sexuality 
veterinary 
Table 4: Domain labels used in the experiment. 
Baseline 1 Baseline ~,) Weak Mutual (baseline 2) 
Synset Inter. I Domain Inter. 
Italian .83 .86 .87 .88 
English .85 .86 .87 .87 
Stron 0 Mutual (baseline ~) 
Synset Inter. I Domain Inter. 
.74 1.68 I .77 / .91 
.7o/.57 I .8o/.9  
Table 5: Precision and B,ecall (English and Italian) for different WDD algorithms 
WSD. For the experiment we used a subset of the 
Semcor corpus. As for WSD we obtained .66 of 
correct disambiguation with a sense frequency al- 
gorithm on polysemous noun words and .80 on all 
nouns (this last is also reported in the literature, 
for example in \[Mihalcea nd Moldovan, 1999\]). 
As for WDD, precision has been computed consid- 
ering the intersection between the word senses be- 
longing to the domain label with the higher score 
and the sense tag for that word reported in Sem- 
cor. Baseline I and baseline 2, described in section 
3.1, respectively gave .81 and .82 in precision, with 
a significant improvement over the WSD baseline, 
which confirms the initial hypothesis. 
WDD in paral le l  texts .  In this experiment 
we wanted to test WDD in the context of par- 
allel texts. Table 5 reports the precision and re- 
call (just in case it is not I) scores for six dif- 
ferent WDD algorithms applied to parallel En- 
glish/Italian texts. Numbers refer to polysemous 
words only. 
Both the baseline algorithms perfbrm quite well: 
83% for Italian and 85% for English in case of 
baseline 1, and 86% for both languages in case of 
baseline 2 are similar to the results obtained on 
the SemCor corpus. 
The algorithm which includes word domain fre- 
quency (i.e. baseline 2) reaches the highest score 
in both languages, indicating that the combina- 
tion of domain word frequency (considered at step 
1 of the algorithm) and domain text frequency 
(considered at step 2) is a good one. In addition, 
the fact that results are the same for both lan- 
guages indicates that the method can smooth the 
coverage differences among the wordnets. 
We expected a better esult for the bilingual ex- 
tensions. The weak mutual strategy, either con- 
sidering the synset intersection or the domain la- 
bels intersection, brings just minor improvements 
with respect o the baselines; the strong mutual 
strategy lowers both the precision and the recall. 
There are several explanations for these results. 
The difference in sense coverage between the two 
wordnets, about 30%, may affect the quality of 
the synset intersection: this would also explain 
the low degree of recall (68% for Italian and 57% 
for English). This is particularly evident for the 
strong mutual strategy, where the relative lexi- 
cal poorness of the Italian synsets can strongly 
reduce the number of synsets in the intersection. 
Note also that the length of the synset intersec- 
tion is about 30-40% of the mean synset number 
for Italian and English news respectively. This 
means less material which the disambiguation al- 
gorithms can take advantage of: relevant sysnsets 
can be left out of the intersection. For these rea- 
sons it is crucial having wordnet resources at the 
same level of completion to exploit he mutual help 
hypothesis. 
Furthermore, there may be a significant amount 
of senses which are "quasi" aligned. This may 
happen when two parallel senses map into close 
synsets, but not in the same one (e.g. one is the di- 
rect hypernym of the other). This problem could 
be overcome considering the IS-A relations during 
the computation of the intersection. In this situ- 
ation it is also probable that the senses maintain 
the same domain label. This would explain why 
the domain intersection behaves better than the 
synset intersection (from 74%-68% to 77%-91% 
for the Italian and from 70%-57% to 80%-91% for 
the English). 
32 
6 Conc lus ions  
We have introduced Word Domain Disambigua- 
tion, a variant of Word Sensse Disambiguation 
where words in a text are tagged with a domain 
label in place of a sense label. Two baseline algo- 
rithms has been presented as well as some exten- 
sions to deal with domain disambiguation i  the 
context of parallel translation texts. 
Two aligned wordnets, the English WORDNET 
1.6 and the Italian MULTIWORDNET, both aug- 
mented with domain labels, have been used as the 
main information repositories. 
The experimental results encourage to further 
investigate the potentiality of word domain dis- 
ambiguation. There are two interesting perspec- 
tives for the future work: first, we want to ex- 
ploit the relations among different lexical cate- 
gories (mainly nouns and verbs) when they share 
the same domain label; second, it seems reason- 
able that the disambiguation process may take ad- 
vantage of both WDD and WSD, where the initial 
word ambiguity is first reduced with WDD and 
then resolved with more fine grained information. 
Finally, an in-depth investigation is necessary for 
what we called factotum effect, which is peculiar 
of WDD. 
As for the applicative scenarios, we want to ap- 
ply WDD to the problem of content based user 
modelling. In particular we are developing a per- 
sonal agent for a news web site that learns user's 
interests from the requested pages that are an- 
alyzed to generate or to update a model of the 
user \[Strapparava et al, 2000\]. Exploiting this 
model, the system anticipates which documents 
in the web site could be interesting for the user. 
Using MULTIWORDNET and domain disambigua- 
tion algorithms, a content-based user model can 
be built as a semantic network whose nodes, in- 
dependent from the language, represent the word 
sense frequency rather then word frequency. Fur- 
therrnore, the resulting user model is indepen- 
dent from the language of the documents browsed. 
This is particular valuable with muitilingual web 
sites, that are becoming very common especially 
in news sites or in electronic ommerce domains. 
Re ferences  
A. Artale, B. Magnini, and C. Strapparava. 
WoRDNET for italian and its use for lexical 
discrimination. In AI*IA97: Advances in Ar- 
tificial Intelligence. Springer Verlag, 1997. 
P. Buitelaar. CoPJ~LEX: An ontology of sys- 
tematic polysemous classes. In Proceedings of 
FOIS98, International Conference on Formal 
Ontology in Information Systems, Trento, Italy, 
June 6-8 1998. IOS Press, 1998. 
P. Buitelaar. Reducing lexical semantic omplex- 
ity with systematic polysemous classes and un- 
derspecification. In Proceedings of ANLP2000 
Workshop on Syntactic and Semantic Complex- 
ity in Natural Language Processing Systems, 
Seattle, USA, April 30 2000, 2000. 
J. Gonzalo, F. Verdejio, C. Peters, and N. Calzo- 
lari. Applying eurowordnet to cross-language 
text retrieval. Computers and Humanities, 
32(2-3):185--207, 1998. 
A. Kilgarriff and C. Yallop. What's in a the- 
sanrus? In Proceedings of LREC-BO00, Sec- 
ond International Conference on Language Re- 
sources and Evaluation, Athens, Greece, June 
2000. 
B. Maguini and G. Cavagli~. Integrating subject 
field codes into WordNet. In Proceedings of 
LREC-2000, Second International Conference 
on Language Resources and Evaluation, Athens, 
Greece, June 2000. 
B. Maguini and C. Strapparava. Costruzione di 
una base di conoscenza lessicale per l'italiano 
basata su WordNet. In M. Carapezza, D. Gam- 
barara, and F. Lo Piparo, editors, Linguaggio e 
Cognizione. Bulzoni, Palermo, Italy, 1997. 
K. Mihalcea nd D. Moldovan. A method for word 
sense disambiguation of unrestricted text. In 
Proc. of A CL-99, College Park Maryland, June 
1999. held in conjunction with UM'96. 
G. Miller. An on-line lexical database. Interna- 
tional Journal of Lexicography, 13(4):235-312, 
1990. 
F. Sabatini and V. Coletti. Dizionario Italiano 
Sabatini Coletti. Giunti, 1997. 
C. Strapparava, B. Magnini, and A. Stefani. 
Seuse-based user modelling for web sites. In 
Adaptive Hyperraedia nd Adaptive Web-Based 
Systems - Lecture Notes in Computer Science 
1892. Springer Verlag, 2000. 
E. Voorhees. Using wordnet for text retrieval. In 
C. Fellbaum, editor, WordNet - an Electronic 
Lexical Database. MIT Press, 1998. 
Y. Wilks and M. Stevenson. Word sense dis- 
ambiguation using optimised combination of 
knowledge sources. In Proc. of COLING- 
A CL '98, 98. 
33 
Exploiting Lexical Expansions and Boolean Compositions for 
Web Querying 
Bernardo Magnini and Roberto Prevete 
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica 
Via Sommarive 
38050 Povo (TN), Italy, 
{magnini I prevete} @irst.itc.it 
http://ecate.itc.it: 1024/projects/question-answering.html 
Abstract 
This paper describes an experiment aiming at 
evaluating the role of NLP based optimizations 
(i.e. morphological derivation and synonymy 
expansion) in web search strategies. Keywords 
and their expansions are composed in two 
different Boolean expressions (i.e. expansion 
insertion and Cartesian combination) and then 
compared with a keyword conjunctive 
composition, considered as the baseline. 
Results confirm the hypothesis that linguistic 
optirnizations significantly improve the search 
engine performances. 
Introduction 
The purpose of this work was to verify if, and in 
which measure, some linguistic optimizations 
on the input query can improve the performance 
of an existing search engine on the web 1. 
First of all we tried to determine a proper 
baseline to compare the optimized search 
strategies. Such a baseline should reflect as 
much as possible the average use of the search 
engine by typical users when querying the web. 
A query is usually composed of a limited 
number of keywords (i.e. two or three), in a 
lemmatized form, that the search engine 
composes by default in a conjunctive 
1 The results reported in this paper are part of a more 
extended project under development at ITC-irst, 
which involves a collaboration with Kataweb, an 
Italian web portal. We thank both Kataweb and 
Inktomi Corporation for kindly having placed the 
search engine for the experiments atour disposal. 
expression. Starting from this level (we call it 
"basic level") we have designed two more 
sophisticated search strategies that introduce a 
number of linguistic optirnizations over the 
keywords and adopt wo composition modalities 
allowed by the "advanced search" capabilities of 
the search engine. One modality (i.e. Keyword 
expansion Insertion Search - KIS) first expands 
each keyword of the base level with 
morphological derivations and synonyms, then 
it builds a Boolean expression where each 
expansion is added to the base keyword list. The 
second modality (i.e. Keyword Cartesian 
expansion Search KCS) adopts the same 
expansions of the previous one, but composes a
Boolean expression where all the possible tuples 
among the base keywords and expansions are 
considered. 
The working hypothesis i  that the introduction 
of lexical expansions should bring an 
improvement in the retrieval of relevant 
documents. To verify the hypothesis, a 
comparative valuation has been carried out 
using the three search modalities described 
above over a set of factual questions. The results 
of the queries have been manually scored along 
a five value scale, with the aim of taking into 
account not only the presence in the document 
of the answer to the question, but also the 
degree of contextual information provided by 
the document i self with respect o the question. 
Both the presence of the answer and the 
contextual information have been estimated by 
two relevance functions, one that considers the 
document position, the other that does not. 
The experiment results confirm that the 
introduction of a limited number of lexical 
expansions (i.e. 2-3) improves the engine 
performance. In addition, the Cartesian 
13 
composition of the expansions behaves 
significantly better than the; search modality 
based on keyword insertion. 
Some of the problems that we faced with in this 
work have been already discussed in previous 
works in the literature. The use of query 
expansions for text retrieval is a debated topic. 
Voorhees (1998) argues that WordNet derived 
query expansions are effective for very short 
queries, while they do not bring any 
improvements for long queries. From a number 
of experiments (Mandala et al, 1998) conclude 
that WordNet query expansions can increase 
recall but degrade precision performances. Three 
reasons are suggested to explain this behavior: 
(i) the lack of relations among terms of different 
parts of speech in WordNet; (ii) many semantic 
relations are not present in WordNet; (iii) proper 
names are not included in WordNet. (Gonzalo et 
al., 1998) pointed out some more weaknesses of
WordNet for Information Retrieval purposes, in 
particular the lack of domain information and 
the fact that sense distinctions are excessively 
fine-grained for the task. A related topic of 
query expansion is query I~anslation, which is 
performed in Cross-Language Information 
Retrieval (Verdejo et al 2000). 
This work brings additional elements in favor of 
the thesis that using linguistic expansions can 
improve IR in a web search scenario. In addition 
we argue that, to be effective, query expansion 
has to be combined with proper search 
modalities. The evaluation experiment we 
carried out, even within the limitations due to 
time and budget constraints, was designed to 
take into account the indications that came out 
at the recent TREC workshop on Question 
Answering (Voorhees, 2000). 
The paper is structured as follows. Section 1 and 
2 respectively present he modalities for the 
linguistic expansion and for the query 
composition. Section 3 reports the experimental 
setting for the comparative valuation of the 
three search modalities. Section 4 describes and 
discusses the results obtained, while in the 
conclusions we propose some directions for 
future work. 
1 Lexical expansion 
Two kinds of lexical expansion have been used 
in the experiment: morphological derivations 
and synonym expansions. Both of them try to 
expand a "basic-keyword", that is a keyword 
direcdy derived from a natural language 
question. The language used in the experiments 
is Italian. 
1.1 Basic keywords 
The idea is that this level of keywords hould 
reflect as much as possible the words used by an 
average user to query a web search engine. 
Given a question expressed with a natural 
language sentence, its basic keywords are 
derived selecting the lernmas for each content 
word of the question. Verbs are transformed in
their corresponding ominalization. Furthermore 
we decided to consider collocations and 
multiwords as single keywords, as most of the 
currently available search engines allow the user 
to specify "phrases" in a very simple way. In the 
experiments presented in the paper multiword 
expressions are manually recognized and then 
added to the basic keyword list. 
Figure 1 shows a couple of questions with their 
respective basic keywords. 
NL-QUEST ION:  Chi ha inventato la luce 
e lettr ica? (Who invented the electric light?) 
BASIC-KEYWORDS : inventore (inventor) 
luce_e let t r ica  (electric_light) 
NL-QUESTION: Quale ~ il fiume pi~ 
lungo del mondo? (Which is the longest world 
river?) 
BASIC-KEYWORDS: fiume (river) pi~_lungo 
(longest) mondo (world) 
Figure 1: Basic keywords extraction from questions. 
1.2 Morphological derivation 
Morphological derivations are considered 
because they introduce new lemmas that we 
might find in possible correct answers to the 
question, improving in this way the engine 
recall. For instance, for a question like "Chi ha 
inventato la luce elettrica?" ("Who invented the 
electric light?") we can imagine different 
contexts for the correct answer, such as "la luce 
elettrica fu inventata da Edison" ("Electric light 
14 
was invented by Edison"), "L'inventore della 
luce elettrica fu Edison" ("The inventor of 
electric light was Edison"), "L'invenzione della 
luce elettrica % dovuta a Edison" ("The invention 
of electric light is due to Edison"), where 
different morphological derivations of the same 
basic keyword "inventore" ("inventor") appear. 
Derivations have been automatically extracted 
from an Italian monolingual dictionary (Disc, 
1997), and collected without considering the 
derivation order (i.e. "inventare" belongs to the 
derivation set of "inventore" even if in the actual 
derivation it is the noun that derives from the 
verb). 
1.3 Synonyms 
Keyword expansion based on synonyms can 
potentially improve the system recall, as the 
answer to the question might contain synonyms 
of the basic keyword. For instance, the answer 
to the question "Chi ha inventato la luce 
elettrica?" ("Who invented the electric light?") 
might be one among "Lo scopntore della lute 
elettrica fu Edison" ("The discoverer of electric 
light was Edison"), "'L'inventore della 
illuminazione elettrica fu Edison" ("The 
inventor of electric illumination was Edison"), 
"La scopritore della illuminazione lettrica fu 
Edison" ("The discoverer of electric 
illumination was Edison"), where different 
synonyms of "inventore" ( inventor") and "luce 
elettrica'" ("electric light") appear. In the 
experiment reported in section 3 Italian 
synonyms have been manually extracted from 
the ItalianWordnet database (Roventini et al, 
2000), a further extension of the Italian Wordnet 
produced by the EuroWordNet project (Vossen, 
1998). Once the correct synset for a basic 
keyword is selected, its synonyms are added to 
the expansion list. In the near future we plan to 
automate the process of synset selection using 
word domain disambiguation, a variant of word 
sense disambiguation based on subject field 
code information added to WordNet (Magnini 
and Cavaglih, 2000). 
1.4 Expans ion chains 
The expansions described in the previous 
sections could be recursively applied to every 
lemma derived by a morphological or a 
synonym expansion. For example, at the first 
expansion level we can pass from "inventore" 
"inventor" to its synonym "scopritore" 
"discoverer", from which in turn we can 
morphologically derive the noun "discovery", 
and so on (cfr. Figure 2). This would allow the 
retrieval of answers uch as "La scoperta della 
lampada d incandescenza ~ dovuta a Edison" 
("The discovery of the incandescent lamp is due 
to Edison"). 
Although in the experiment reported in this 
paper we do not use recursive xpansions (i.e. 
we stop at the first level of the expansion chain), 
a long term goal of this work is to verify their 
effects on the document relevance. 
inventore 
) 
derivation 
daivaaca ) 
(inventor) 
scopritore (discoverer), 
ideatore (artificer) 
invenzione (invention) 
I sy~ ) scoperta  (discoverer) 
inventare  (invenO 
I synyn~ ) scopr i re  (discover) 
Figure 2: Lexical chain for "inventore" ( inventor") 
2 Query compositions 
We wanted to take advantage of the "advanced" 
capabilities of the search engine. In particular 
we experimented the "Boolean phraase" 
modality, which allows the user to submit 
queries with keywords composed by means of 
logical operators. However we quickly realised 
that realistic choices were restricted to disjoint 
compositions of short AND clauses (i.e. with a 
limited number of elements, typically not more 
than four). This constrained us to two 
hypothesis, described in sections 2.2 and 2.3, 
which have been compared with a baseline 
composition strategy, described in 2.1. 
2.1 Keyword  "aa~lY' composi t ion search 
(KAS) 
This search strategy corresponds to the default 
method that most search engines implement. 
Given a list of basic keywords, no expansion is 
performed and keywords are composed in an 
AND clause. An example is reported in Figure 
3. 
15 
NL-QUEST ION:  Chi ha inventato la luce 
e lettr ica? (Who invented the electric lightD 
BASIC-KEYWORDS : inventore  (inventor) 
l uee_e le t t r i ca  (electric_light) 
EXPANS IONS : 
COMPOSIT ION:  ( inventore AND 
luce_elettr ica)  
Figure 3: Example of AND composition search 
2.2 Keyword expansion insertion search 
(Icls) 
In this composition modality a disjunctive 
expression is constructed where each disjoint 
element is an AND clause formed by the base 
keywords plus the insertion of a single 
expansion. In addition, to guarantee that at least 
the same documents of the KAS modality are 
retrieved, both an AND clause with the basic 
keywords and all the single basic keywords are 
added as disjoint elements. Figure 4 reports an 
example. If the AND combination of the basic 
keywords produces a non empty set of 
documents, then the KIS modality should return 
the same set of documents remTanged by the 
presence of the keyword expansions. What we 
expect is an improvement in the position of a 
significant document, which is relevant when 
huge amounts of documents are retrieved. 
NL-QUEST ION:  Chi ha inventato la luce 
e lettr ica? (Who inven~d the e~ctric l~ht~ 
BASIC-KEYWORDS : inventore (mvenW~ 
luce_e let t r ica  (e~ctric lighO 
EXPANSIONS:  
inventore 
~mmflm > scopritore, ideatore 
dnivai~ > invenzione 
s~myn~ ) scoperta 
dniv~ > inventare 
I sy~rm > scoprire 
luce_e let t r ica  
I ~ ) lampada_a_ incandescenza 
COMPOSIT ION:  
(OR (inventoreAND luce_elettricaAND 
scopritore) 
OR (inventoreAND luce_elettricaAND 
ideatore) 
OR (inventoreAND luce_elettricaAND 
invenzione) 
OR (inventoreAND luce_elettricaAND 
scoperta) 
OR (inventore AND luce_elettricaAND 
inventare) 
OR (inventoreAND luce_elettricaAND 
scoprire) 
OR (inventore AND luce_elettricaAND 
lampada_a_incandescenza) 
OR (inventoreAND luce_elettrica) 
OR inventore OR luce_elettrica) 
. Figure 4: Example of expansion insertion 
composition 
2.3 Keyword Cartesian composition 
search (KCS) 
In this composition modality a disjunctive 
expression is constructed where each disjoint 
element is an AND clause formed by one of the 
possible tuple derived by the expansion set of 
each base keyword. In addition, to guarantee 
that at least the same documents of the KAS 
modality are retrieved, the single basic 
keywords are added as disjoint elements. Figure 
5 reports an example. 
As in the previous case we expect hat at least 
the same results of the KAS search are returned, 
because the AND composition of the basic 
keywords is guaranteed. We also expect a 
possible improvement of the recall, because new 
AND clauses are inserted. 
NL-QUEST ION:  Chi ha inventato  la luce 
elettr ica? 
BAS IC-KEYWORDS:  inventore  
luce_e le t t r i ca  
EXPANSIONS:  
inventore 
synonyms > scopritore,  ideatore 
dn~v~m > invenz ione 
I ~ ) scoperta  
) inventare  
I s~ny.~ ) scopr i re  
luce_e let t r ica  
\[ s~mnyn~ ) lampadaa_ incandescenza  
COMPOSIT ION:  
(OR (inventore AND luce_elettrica) 
OR (inventore AND lampada_a_incandescenza) 
OR (scopritore AND luce_elettrica) 
OR (scopritore AND lampada_a_incandescenza) 
OR (ideatore AND luce_elettrica) 
OR (ideatore AND lampada_a_incandescenza) 
OR (invenzione AND luce_elettrica) 
OR (invenzione AND lampada_a_incandescenza) 
OR (scoperta AND luce_elettrica) 
OR (scoperta AND lampada_a_incandescenza) 
16 
OR ( inventareAND luce_elettrica) 
OR (inventare AND lampada_a_incandescenza) 
OR (scoprire AND luce_elettrica) 
OR (scopr i reAND lampadaa_incandescenza) 
OR inventore OR luce_elettrica)) 
Figure 5: Example of Cartesian composition search 
3 Comparison experiment 
This section reports about the problems we 
faced with comparing the three search strategies 
presented in section 2. The question set, the 
document assessment and the scoring used in 
the experiment are described. 
3.1 Creating the Question Set 
Initially, a question set of 40 fact-based, short- 
answer questions uch as "Chi ~ l'autore della 
Divina Commedia?" ("Who is the author of The 
Divine Comedy?") was created. Language was 
Italian and each question was guaranteed to have 
at least one web document hat answered the 
question. Ambiguous questions (about 15%) 
were not eliminated (see Voorhees, 2000 for a 
discussion). A total of 20 questions from the 
initial question set have been randomly 
selected, this way preventing possible bias in 
favour of queries that would perform better with 
lexical expansions. Figure 6 reports the final 
question set of the experiment. 
Chi ha inventato la luce elettrica? 
(Who invented the electric light?) 
Come si chiama l'autore del libro "I 
Malavoglia"? 
(Who is the author of the book '7 
Malavoglia"?) 
Chi ha scoperto la legge di gravit~t? 
(who discovered the gravitational law) 
Chi ha inventato la stampa? 
(Who is the inventor of printing) 
Chi ha vinto il campionato di calcio nel 
1985 ? 
(Who won the soccer championship in 
1985?) 
Chi ~ il regista di "I Mostri" 
(Who is the director of "I Mostri') 
Quale attore ha recitato con Benigni nel film 
"I1 piccolo Diavolo"? 
(Who played with Benigni in the film "'ll 
piccolo Diavolo "?) 
Chi ha ucciso John Kennedy? 
(Who assassinated John Kennedy?) 
Chi detiene il recod italiano dei 200 metri? 
(Who holds the Italian record for the 200- 
meters dash ?) 
10 Chi ~ stato il primo uomo sulla Luna? 
(Who was the first man on the moon?) 
11 Chi ha inventato il Lisp? 
(Who is the inventor of the Lisp) .. 
12 Premio nobel per la letteratura nel 1998 
(1998 Nobel Prize in literature) 
13 Quale ~ il flume pih lungo del mondo? 
(Which is the longest river of the worM?) 
14 In quale squadra di calcio Italiana ha gioeato 
Van Basten? 
(Which Italian soccer team did Van Basten 
play in ?) 
15 Chi ha vinto i mondiali di Calcio nel 1986? 
(Who won the Worm Cup Soccer in 1986?) 
16 Chi ha progettato laReggia di Caserta? 
(Who was the architect of the Caserta royal 
palace?) 
17 Dove ~ nato Alessandro Manzoni? 
(Where was Alessandro Manzoni born?) 
18 Quale ~ il lago pifi grande d'Italia? 
(Which is the largest Italian lake?) 
19 Chi ha fondato la Microsoft? 
(who is the founder of Microsoft?) 
20 Chi ~ il padre della relativitY? 
(who is the father of the relativity theory?) 
Figure 6: Question set used in the experiments. 
Each question was then associated with a 
corresponding human-generated set of basic 
keywords, resulting in an ordered list of \[nl- 
question, basic-keywords \] pairs. We supposed a 
maximum of 3 basic keywords for each 
question, obtaining an average of 2.25. This is 
in line with (Jansen et al, 1998) where it is 
reported that, over a sample of 51.473 queries 
submitted to a major search service (Excite), the 
average query length was 2.35. Basic keywords 
are then expanded with their morphological 
derivations and synonyms (see Section 2), with 
an average of two expansions for question 
(rnin=0, max=6). 
3.2 Document assessment  
An automatic query generator has been realised 
that, given a question with its basic keywords 
and lexical expansions, builds up three queries, 
corresponding to KAS, KIS and KCS, and 
submits them to the search engine. Results are 
collected considering up to ten documents for 
search; then the union set is used for the 
evaluation experiment. There was no way for 
the assessor to relate a document o the search 
modality the document was retrieved by. Query 
17 
generation, web querying and result displaying 
were all been made mntime, during the 
evaluation session. 
Fifteen researchers at ITC-irst were selected as 
assessors in the experiment. They were asked to 
judge the web documents returned by the query 
generator with respect to a given question, 
choosing avalue among the fo\]tlowing five: 
1) answer in context: The answer 
corresponding to the question is recovered and 
the document context is appropriate. For 
example, if the question is "Who is the inventor 
of the electric light?" then "Edison" is reported 
in the document, in some way, as the inventor of 
the electric light and the whole document deals 
with inventors and/or Edison's life. 
2) answer_nocontext: The answer to the 
question is recovered but the document context 
is not appropriate. (e.g. the document does not 
deal neither with inventors or Edison's life). 
3) noanswerin_context: The answer 
corresponding to the question is not recovered 
but the document context is appropriate. 
4) noanswerno_context: The answer 
corresponding to the question is not recovered 
and the document context is not appropriate. 
5) no_document: he requested ocument is not 
retrieved. 
The following instructions were provided to 
assessors :  
? The judgement has to be based on the 
document text only, that is no further links 
exploration is allowed. 
? If a question is considered ambiguous then 
give it just one interpretation and use that 
interpretation to judge aH question-related 
documents consistently. For example, if the 
question "Chi ~ il vincitore del Tour de 
France? " ("Who is the winner of the Tour 
de France?") is considered ambiguous 
because the answer may change over time, 
then the assessor could decide that the 
correct interpretation is "Who is the winner 
of the 1999 Tour de France?" and judge all 
the documents consistently. 
? A document contains the answer only if it is 
explicitly reported in the text. That is, if the 
question is "Who is the author of Options?" 
it is not sufficient hat the string "Robert 
Sheckley" or "Sheckley" is in the text, but 
the document has to say that Robert 
Sheckley is the author of Options. 
Each question was judged independently by 
three assessors. The number of texts to be 
judged for a question ranged from 10 to 18, with 
an average of 12. For each question k we 
obtained three sets VKm.k, VKXS,k and VKCS,k of 
(pos, assessment) pairs corresponding to the 
three search methods, where pos is the position 
? of the document in the ordered list returned by 
the search method, and assessment is the 
assessment of one participant. 
3.3 Assessment scor ing 
We eliminated all the (pos, assessment) pairs 
whose assessment was equal to no_document. 
Said i a (pos, assessment) pair belonging to 
VKAS, k, Vras, k or VKcs. k we define: 
0 i f  assessment is no_answer_no_context 
~1 if assessment is no_ answer_ in_ context 
r( i) = 12 if assessment is answer no_ context 
\[3 if assessment is answer_ in_ context 
Given a question k and a set V~ of (pos, 
assessment) pairs corresponding to an ordered 
list Lk of documents, toevaluate the relevance of 
L~ with respect to k we have defined two 
relevance functions, defined in \[1\]: f? that 
considers the document position, andf  that does 
not. 
X v(i) X v(i) / p(i) 
f - (k) = i~v~ f .  (k) = i~v, 
ra 
m ~l / j  
j=l  
where 
- p(i) is the position of the web document in the 
ordered list. 
- v(O=~(r(i)).r(O+13(r(O) 
a(x), 13(x) : 10,1,2,3} ~ (0,1) are 
tuning functions that allow to weight he 
assessments. 
- m is the maximum length of an ordered list of 
web documents. 
For each search method we obtained a set of 20 
~,  f?) pairs by the assessing process, i.e., we 
obtained 20 (f, f?)~s, kpairs, 20 (f, f?)ms, kpairs 
and 20 (f, f?)KCS, kpairs. 
18 
4 Results and discussion 
During the assessing process, some requested 
URLs were not retrieved. We have a total of 546 
URLs and 516 retrieved web documents, 
meaning that about 6% of URLs were not 
retrieved (see Table 1). 
KAS KIS KCS Total 
Total URLs 146 200 200 546 
Retrieved 137 191 188 516 
URLs 
% Retrieved 94% \[ 95% 94% 94% 
URLs L 
Table 1: URLs returned by KAS, KIS and KCS 
methods and URLs retrieved uring the assessing 
process. 
Table 2 shows the assessments on the KAS 
search method, which we consider the baseline 
of the experiment, being search by keywords a 
standard search method on the Web. 
Results are presented for three partitions of the 
question set. QS1 is the subset of questions 
whose number of morphological derivations and 
synonyms is higher than three; QS2 is the subset 
whose number of lexical expansions i  equal to 
two or three; QS3 is the subset whose number of 
lexical expansions is lower than two. The table 
reports the average values of f. (i.e. document 
order not considered) and f? (i.e. order 
considered) with respect o each partition. The 
obtained values, f 0.23 and f? 0.25, indicate 
that, on average, about 2 web documents have 
an answer  in context  assessment and 7 web 
documents have noanswer  no context  
assessment out of 10 documents returned by this 
method. 
QS1 
Qs2 
qs3 
all 
KAS 
Mean 
Y- A 
C- pos.) C+pos.) 
0.14 0.20 
y. 
(- pos.) 
0.20 
0.37 0.31 0.43 
0.22 0.23 0.20 
0.21 0.23 0.25 
Sdev 
f? 
(+ pos.) 
0.23 
0.34 
0.21 
0.23 
Table 2: Mean and standard deviation of the 
relevance values f. (without position) and f? (with 
position) of retrieved web documents returned by 
KAS method. 
Table 3 reports the relevance values for the 
documents retrieved respectively by KIS and 
KCS. For KIS we have a growth of the 19% 
and 13% compared with the KAS method. For 
KCS the average growth is 33 % and 22% 
compared with KAS. On QS2 there is a 
remarkable improvement in the KCS 
performances compared with KAS (+59% and 
+77%). In this case the average value off+ is 
greater than f .  meaning that KCS recovers good 
web documents in a better position than KAS. 
On QS3 there is also a good performance of 
both KIS and KCS compared with K.AS (+18% 
and +17% for KIS, +23% and +17% for KCS). 
On the contrary, on the subset QS1 both KIS 
and KCS performances are comparable to KAS. 
QS1 
QS2 
QS3 
all 
KIS KCS 
% KAS % KAS 
Y f? 
(- pos.) (+ pos.) 
+7 % -15% 
-3 % +19 % 
+18 % +17 % 
+19 % +13 % 
f. f+ 
(- pos.) (+ pos.) 
+7% - 15 % 
+59 % +77 % 
+23 % +17 % 
+33 % +22 % 
Table 3: KIS and KCS increasing of the average 
relevance with respect to K/kS. 
From the data presented here it does not emerge 
a clear correlation between the performance of a 
search method and the number of lexical 
expansions. It can be noted that both KIS and 
KCS perform quite well, compared with KAS, 
on the set of questions having no expansions. 
This can be explained because KIS and KCS 
create queries less restrictive than KAS and are 
able to recover the same documents of KAS as 
well as other documents that can be meaningful. 
In case lexical expansions are present, the best 
performance compared with KAS is carried out 
by KCS method on question 1 (Figure 6), which 
have a total of four derivations and four 
synonyms. In this case K.AS recovered two 
documents and KCS more than ten documents, 
improving also the answer  in context  
assessments hanks to both the morphological 
derivation "invenzione" (" invention") and the 
synonym "lampadina elettrica" 
("electr ic  lamp").  
19 
It is not clear if synonyms affect search 
performance more than morphological 
derivation or vice versa. It seems that synonyms 
and morphological derivations are significant 
expansions in the same way. If we consider the 
set of the questions characterised by an 
improvement i  the KCS and KIS performance 
compared with K.AS performance, then there are 
four questions having the number of synonyms 
greater than the number of morphological 
derivations, three questions having the number 
of synonyms lower than the number of 
morphological derivations and three questions 
having the number of synonyms equal to the 
number of morphological derivations (zero 
included). 
If we consider the set of questions having the 
number of synonyms higher than the number of 
morphological derivations, then there are four 
cases out of eight where KIS and KCS enhance 
the performance of KAS. If instead we consider 
the set of questions having the number of 
synonyms lower than the number of 
morphological derivations there are three cases 
out of six where KIS and KCS enhance the 
performance of KAS. 
Finally, Table 4 synthetically shows how KIS 
and KCS perform with respect o document 
"context retrieval", that is the degree of 
contextual information provided by the 
document with respect o the question, no 
matter if the answer to the question was present 
or not in the document itself. To focus on 
context we set the tuning functions tx(x) and 
~(x) to tx(O )=0, or(l)= 1, tx(2)=O, ot(3)=1/3 and 
~(x)=O. The reason for considering a context 
retrieval score is that, in case the answer is not 
present, context increases the probability that 
other relevant documents can be found 
following hypertextual links, possibly including 
the correct answer to the question. 
Results obtained with KIS and KCS confirm 
that they provide a significant increase (from 
31% to 41%) of context retrieval score. 
KIS 
KCS 
% context retrieval increasing with 
respect o KAS 
f. (- l, os.) f? (+ pos.) 
37% +31% 
41% + 38 % 
Table 4: KIS and KCS context retrieval increasing 
with respect to KAS. 
Conclusion 
A comparative experiment among three search 
strategies has been carried out with the aim of 
estimating the benefits of lexical expansions and 
? of composition strategies over the basic 
keywords of a query. Results lead us believe 
that search strategies that combine a number of 
linguistic optirnizations with a proper Boolean 
composition can improve the performance of an 
existing search engine on the web. In particular 
given KAS (no expansions, with AND 
composition search) as baseline, KIS (expansion 
insertion search) performs better but one case 
(i.e. with expansions greater than 3) and KCS 
(Cartesian composition search) performs better 
than KIS. Furthermore, KCS has a maximum 
performance, with expansions equal to 2 or 3, 
significantly higher than KIS, probably because 
KCS retrieves web documents that are not 
retrieved by K/S, which basically reearranges the 
order of KAS documents. 
At present we still have no clear data to 
determine which number and which kind (i.e. 
morphological derivations and synonyms) of 
lexical expansions performs better for a single 
question, even if all the three search strategies 
definitely perform better with questions with a 
limited number of expansions (i.e. two or three). 
An evaluation that will take into considerations 
such variations i  planned for the near future. A 
crucial related problem for the future is that of 
the automatic evaluation of the search strategies 
(see Breck et al, 2000), which will enormously 
speed up the design and evaluation cycle. 
The experiments reported in this paper are part 
of a feasibility study for the realisation of a 
Natural Language Based search engine on the 
Web. At the present state of development, some 
steps in the query expansion (i.e. multiword 
recognition and synset selection) have been 
done manually, while both the keyword 
composition and the actual search are automatic 
and very efficient. In order to completely 
automate the process, the main source of 
inefficiency is likely to be keywords 
disambiguation in WordNet. The idea is to use a 
20 
two stage disarnbiguation algorithm (Voorhees, 
1998), based on topic information, which 
performs linearly with respect to the number of 
words to be disambiguated. 
References 
Breck, E.J., Burger J.D., Ferro L., Hirschman 
L., House D., Light M., Mani I (2000) How 
to Evaluate Your Question Answering System 
Every Day ...and Still Get Real Work Done. 
Proceedings of LREC-2000, Second 
International Conference on Language 
Resources and Evaluation, pp. 1495-1500. 
Disc (1997) Dizionario Italiano Sabatini 
Coletti, Firenze, Giunti. 
Gonzalo J., Verdejo F., Peters C. and Calzolari 
N. (1998) Applying EuroWordnet to Cross- 
language Text Retrieval. Computers and the 
Humanities, 32, 2-3, pp. 185-207. 
Gonzalo J., Verdejo F., Chugur I., Cigar J. 
(1998) Indexing with WordNet synsets can 
improve text retrieval. Proceedings of the 
Workshop "Usage of Wordnet in NLP 
systems" Coling-ACL. 
Jansen B. J., Spink A., Bateman J., Saracevic 
T. (1998) Real life information retrieval: A 
study of user queries on the Web. SIGIR 
Forum, 32(1), 5-17. 
Magnini B. and Cavaglih G. (2000) Integrating 
Subject Field Codes into Wordnet. 
Proceedings of LREC-2000, Second 
International Conference on Language 
Resources and Evaluation, pp. 1413-1418. 
Mandala R., Takenobu T. and Hozumi T. (1998) 
The Use of WordNet in Information 
Retrieval. Proceedings ofColing-ACL. 
Roventini A., Alonge A., Bertagna F., Calzolari 
N., Magnini B., Martinelli R. (2000) 
ItalWordNet, a large semantic database for 
Italian. Proceedings of LREC-2000, Second 
International Conference on Language 
Resources and Evaluation, pp. 783-790. 
Verdejo F., Gonzalo J., Penas A., Lopez F., 
Fernandez D. (2000) Evaluating wordnets in 
Cross-language Information Retrieval: the 
ITEM search engine. Proceedings of LREC- 
2000, Second International Conference on 
Language Resources and Evaluation, pp. 
1769-1774. 
Voorhees, Ellen M., Using WordNet for Text 
Retrieval, in Fellbaum C. (1998) WordNet, 
an Electronic Lexical Database. MIT Press. 
Voorhees, Ellen M. and Tice Dawn M. (2000) 
Implementing a Question Answering 
Evaluation. Proceedings o f  the Workshop 
"Using Evaluation within HLT Programs: 
Results and Trends", Athens, Greece, May 
30, 2000. 
Vossen P. (1998) EuroWordnet: a multilingual 
database with lexical semantic networks. 
Kluver Academic Publishers. 
21 
   
		 MEANING: a Roadmap to Knowledge Technologies 
 
German Rigau. TALP Research Center. UPC. Barcelona. rigau@lsi.upc.es 
Bernardo Magnini. ITC-IRST. Povo-Trento. magnini@itc.it 
Eneko Agirre. IXA group. EHU. Donostia. eneko@si.ehu.es  
Piek Vossen. Irion Technologies. Delft. Piek.Vossen@irion.nl  
John Carroll. COGS. U. Sussex. Brighton. johnca@cogs.susx.ac.uk 
 
Abstract  
Knowledge Technologies need to extract 
knowledge from existing texts, which 
calls for advanced Human Language 
Technologies (HLT). Progress is being 
made in Natural Language Processing but 
there is still a long way towards Natural 
Language Understanding. An important 
step towards this goal is the development 
of technologies and resources that deal 
with concepts rather than words. The 
MEANING project argues that we need to 
solve two complementary and 
intermediate tasks to enable the next 
generation of intelligent open domain 
HLT application systems: Word Sense 
Disambiguation and large-scale 
enrichment of Lexical Knowledge Bases. 
Innovations in this area will lead to HLT 
with deeper understanding of texts, and 
immediate progress in real applications of 
Knowledge Technologies. 
Introduction 
The field of Information Society Technologies 
(IST) is one of the main thematic priorities of 
the European Commission for the 6th Framework 
programme. In this field, Knowledge 
Technologies (KT) aim to provide meaning to 
the petabytes of information content our 
societies will generate in the near future. 
Information and knowledge management 
systems need to evolve accordingly, to enable 
the next generation of intelligent open domain 
Human Language Technologies (HLT) that will 
deal with the growing potential of the 
knowledge-rich and multilingual society. 
In order to develop a trustable semantic web 
infrastructure and a multilingual ontology 
framework to support knowledge management a 
wide range of techniques are required to 
progressively automate the knowledge lifecycle. 
In particular, this involves extracting high-level 
meaning from the large collections of content 
data and its representation and management in a 
common knowledge base. 
Even now, building large and rich knowledge 
bases takes a great deal of expensive manual 
effort; this has severely hampered Knowledge-
Technologies and HLT application development. 
For example, dozens of person-years have been 
invest into the development of wordnets1 for 
various languages, but the data in these 
resources is still not sufficiently rich to support 
advanced concept-based HLT applications 
directly. Furthermore, resources produced by 
introspection usually fail to register what really 
occurs in texts. Applications will not scale up to 
working in the open domain without more 
detailed and rich general-purpose, which should 
perhaps include domain-specific linguistic 
knowledge.  
The MEANING project identifies two 
complementary intermediate tasks which we 
think are crucial in order to enable the next 
generation of intelligent open domain HLT 
application systems: Word Sense 
Disambiguation (WSD) and large-scale 
enrichment of Lexical Knowledge Bases.  
                                                     
1 A wordnet is a conceptually structured knowledge 
base of word senses. The English WordNet (Miller 
90, Fellbaum 98) has been developed at Princeton 
University over the past 14 years. EuroWordNet 
(Vossen 1998) is a multilingual database with 
wordnets for several European languages (Dutch, 
Italian, Spanish, German, French, Czech and 
Estonian). Balkanet is building wordnets for the 
Balkan languages following the EuroWordNet 
design. 
The advance in these two areas will allow for 
large-scale extractions of shallow meaning from 
texts, in the form of relations among concepts. 
WSD provides the technology to convert 
relations between words into relations between 
concepts. Rich and large-scale Lexical 
Knowledge Bases will have be the repositories 
of extracted relations and other linguistic 
knowledge.  
However, progress is difficult due to the 
following interdependence: 
? In order to achieve accurate WSD, we need 
far more linguistic and semantic knowledge 
than is available in current lexical 
knowledge bases (e.g. current wordnets).  
? In order to enrich Lexical Knowledge Bases 
we need to acquire information from 
corpora, which have been accurately tagged 
with word senses.  
Providing innovative technology to solve this 
problem will be one of the main challenges to 
access KTs.  
Following this introduction section 1 presents 
the major research goals in HLT. Section 2 
presents the MEANING roadmap. Finally, 
section 4 draws the conclusions. 
1 Major research goals in HLT 
In order to extend the state-of-the-art in human 
language technologies (HLT) future research 
must devise: (1) innovative processes and tools 
for automatic acquisition of lexical knowledge 
from large-scale document collections; (2) novel 
techniques for accurately selecting the sense of 
open-class words in a large number of 
languages; (3) ways to enrich existing 
multilingual linguistic knowledge resources with 
new kinds of lexical information by 
automatically mapping information across 
languages. We present each one in turn. 
1.1 Dealing with knowledge acquisition 
The acquisition of linguistic knowledge from 
corpora has been a very successful line of 
research. Research in the acquisition of 
subcategorization information, selectional 
preferences, in thematic role assignments and 
diathesis alternations (Agirre and Mart?nez 
2001, 2002, McCarthy and Korhonen, 1998; 
Korhonen et al, 2000; McCarthy 2001), domain 
information (Magnini and Cavagli? 2000), topic 
signatures (Agirre et al 2001b), lexico-semantic 
relations between words (Agirre et al 2002) etc. 
has obtained encouraging results. The 
acquisition process usually involves large bodies 
of text, which have been previously processed 
with shallow language processors.  
Much of the use of the acquired knowledge 
has been hampered by the fact that the texts are 
not sense-disambiguated, and therefore, only 
knowledge for words can be acquired, that is, 
subcategorization for words, selectional 
preferences for words, etc. It is a well 
established fact that much of the linguistic 
behavior of words can be better explained if it is 
keyed to word senses.  
For instance, the subcategorization frames of 
verbs are highly dependent of the sense of the 
verb. Some senses of a given verb allow for a 
particular combination of complements, while 
others do not (McCarthy, 2001). The same is 
applicable to selectional preferences; traditional 
approaches that learn selectional preferences for 
a verb, tend to mix e.g. all subjects for differents 
senses, even if verbs can have different 
selectional preferences for each word sense 
(Agirre & Martinez, 2002). 
Having texts automatically sense-tagged with 
high accuracy will produce significantly better 
acquired knowledge at a sense level, including 
subcategorization frequencies, domain 
information, topic signatures, selectional 
preferences, specific lexico-semantic relations, 
thematic role assignments and diathesis 
alternations. It will also facilitate the 
investigation on automatic methods for dealing 
with new senses not present in current wordnets 
and clustering of word senses. Furthermore, 
linguistic information keyed to word senses that 
are linked to interlingual concepts (as proposed 
in the EuroWordNet model), can be easily 
integrated in a multilingual Lexical Knowledge 
Base (cf. section 2.3) 
2.2 Dealing with WSD 
Word Sense Disambiguation (WSD) is the task 
of assigning the appropriate meaning (sense) to a 
given word in a text or discourse. Ide and 
Veronis (1998) argue that word sense ambiguity 
is a central problem for many established HLT 
applications (for example Machine Translation, 
Information Extraction and Information 
Retrieval). This is also the case for associated 
sub-tasks (i.e. reference resolution and parsing). 
For this reason many international research 
groups are working on WSD, using a wide range 
of approaches. However, no large-scale broad-
coverage accurate WSD system has been built 
up to date2. With current state-of-the-art 
accuracy in the range 60-70%, WSD is one of 
the most important open problems in Natural 
Language Processing. 
A promising current line of research uses 
semantically annotated corpora to train Machine 
Learning (ML) algorithms to decide which word 
sense to choose in which contexts. The words in 
these annotated corpora are tagged manually 
with semantic classes taken from a particular 
lexical semantic resource (most commonly 
WordNet). Many standard ML techniques have 
been tried, such as Bayesian learning, Exemplar 
based learning, Decision Lists, and recently 
margin-based classifiers like Boosting and 
Support Vector Machines (Escudero et al, 
2000a, 2000b, 2000c, 2000d, 2001; Mart?nez 
and Agirre, 2000). These approaches are termed 
"supervised" because they learn from previously 
sense annotated data and therefore they require a 
large amount of human intervention to annotate 
the training data. 
Supervised WSD systems are data hungry. 
They suffer from the "knowledge acquisition 
bottleneck", it takes them mere seconds to digest 
all of the processed corpus contained in training 
materials that take months to annotate manually. 
So, although Machine Learning classifiers are 
undeniably effective, they are not feasible until 
we can obtain reliable unsupervised training 
data. Ng (1997) estimates that the manual 
annotation effort necessary to build a broad 
coverage word-sense annotated English corpus 
is about 16 person-years; and this effort would 
have to be replicated for each different language. 
Unfortunately, many people think that Ng?s 
estimate might fell short, as the annotated corpus 
thus produced is not guaranteed to enable high 
accuracy WSD.  
Some recent work is focusing on reducing 
the acquisition cost and the need for supervision 
                                                     
2 See the conclusions of the SENSEVAL-2 
competition: http://www.sle.sharp.co.uk/senseval2/ 
in corpus-based methods for WSD. Leacock et 
al. (1998) and Mihalcea and Moldovan (1999) 
automatically generate arbitrarily large corpora 
for unsupervised WSD training, using the 
synonyms or definitions of word senses 
provided in WordNet to formulate search engine 
queries over the Web. In another line of 
research, (Yarowsky, 1995) and (Blum and 
Mitchell, 1998) have shown that it is possible to 
reduce the need for supervision with the help of 
large amounts of unannotated data. Applying 
these ideas, (Agirre and Mart?nez, 2000) has 
developed knowledge-based prototypes for 
obtaining accurate examples from the web for 
specific WordNet synsets, as well as, large 
quantities of unannotated examples. 
But in order to make significant advances in 
WSD system accuracy, systems need to be able 
to use types of lexical knowledge that are not 
currently available in wide-coverage lexical 
knowledge bases: for example subcategorisation 
frequencies for predicates (particularly verbs) 
rely on word senses, selectional preferences of 
predicates for classes of arguments, amongst 
others (Carroll and McCarthy, 2000; McCarthy 
et al, 2001; Agirre and Mart?nez, 2002;).  
2.3 Dealing with multilingualism  
Language diversity is at the same time a 
valuable cultural heritage worth preserving, and 
an obstacle to achieving a more cohesive social 
and economic development. This situation has 
been further stressed as a major challenge in IST 
research lines. Improving language 
communication capabilities is a prerequisite for 
increasing industrial competitiveness, this way 
leading to a sound growth in key economic 
sectors.  
However, this obstacle can be helpful 
because all languages realize the meaning in 
different ways. We can benefit from this fact 
using a novel multilingual mapping process that 
exploits the EuroWordNet architecture. In 
EuroWordNet local wordnets are linked via an 
Inter-Lingual-Index (ILI) allowing the 
connection from words in one language to 
translation equivalent words in any of the other 
languages. In that way, technological advances 
in one language can help the other.  
For instance, for Basque, being an 
agglutinative language with very rich 
morphological-syntactic information, it is 
possible to extract semantic relations that would 
be more difficult to capture in other languages. 
Below we can see an example of the relation 
betwewen silversmith and silver, extracted from 
the Basque words zilargile ? zilar respectively. 
This relation has been disambiguated into the 
?maker_of? lexico-semantic relation (Agirre & 
Lersundi, 2000).  
On the contrary, Basque is not largely present 
in the web as the others. Using this approach it is 
possible to balance both gaps.  
Although the technology to provide 
compatibility across wordnets exits (Daud? et al 
1999, 2000, 2001), new research is needed for 
porting and uploading the various types of 
knowledge across languages, and new ways to 
test the validity of the ported knowledge in the 
target languages.  
3. The MEANING Roadmap 
The improvements mentioned above have been 
explored separately with relative success. In 
fact, no research group in isolation has tried to 
combine all this aforementioned factors. We 
designed the MEANING project3 convinced that 
only a combination of all relevant knowledge 
and resources will be able to produce significant 
advances in this crucial research area.  
MEANING will treat the web as a (huge) 
corpus to learn information from, since even the 
largest conventional corpora available (e.g. the 
Reuters corpus, the British National Corpus) are 
not large enough to be able to acquire reliable 
information in sufficient detail about language 
behaviour. Moreover, most languages do not 
have large or diverse enough corpora available. 
MEANING proposes an innovative 
bootstrapping process to deal with the inter-
dependency between WSD and knowledge 
acquisition: 
1. Train accurate WSD systems and apply 
them to very large corpora by coupling 
knowledge-based techniques on the existing 
EuroWordNet (e.g. to populate it with 
domain labels, to induce automatically 
                                                     
                                                     3 Started in March 2002, MEANING IST-2001-
34460 "Developing Multilingual Web-scale 
Language Technologies" is a three years research 
project funded by the EC. 
training examples) with ML techniques that 
combine very large amounts of labeled and 
unlabeled data. When ready, use also the 
knowledge acquired in 2. 
2. Use the obtained accurate WSD data in 
conjunction with shallow parsing techniques 
and domain tagging to extract new linguistic 
knowledge to incorporate into 
EuroWordNet. 
This method will be able to break this 
interdependency in a series of cycles thanks to 
the fact that the WSD system will be based on 
all domain information, sophisticated linguistic 
knowledge, large numbers of automatically 
tagged examples from the web, and a 
combination of annotated and unannotated data. 
The first WSD system will have weaker 
linguistic knowledge, but the sole combination 
of the rest of the factors will produce significant 
performance gains. Besides, some of the 
required linguistic knowledge can be acquired 
from unnanotated data, and can therefore be 
acquired without using any WSD system. Once 
acceptable WSD is available, the acquired 
knowledge will be of a higher quality, and will 
allow for better WSD performance. 
Multilingualism will be also helpful for 
MEANING. The idiosyncratic way the meaning 
is realised in a particular language will be 
captured and ported to the rest of languages 
involved in the project4 using EuroWordNet as a 
Multilingual Central Repository in three 
consecutive phases (see figure 1). 
For instance, selectional preferences acquired 
for verb senses based on the English corpora, 
can be uploaded into the Multilingual Central 
Repository. As the selectional prefenrece 
relation is keyed to concepts in the repository, 
this knowledge can be ported to the other 
languages. Of course, the ported knowledge 
needs to be checked in order to evaluate the 
validity of this approach.  
Below, we can see the selectional preference 
for the first sense of know from (Agirre & 
martinez, 2002). The first sense of know is 
univocally linked to <know, cognize,
cognise>, which in EuroWordNet is linked to 
4 MEANING will work with three major European 
languages (English, Spanish and Italian) and two 
minority languages (Catalan and Basque).  
w
S
a
B
s
0
0
0
0
0
4
W
s
m
p
m
c
e
Multilingual Central Repository 
EANING is going to constitute 
wledge resource for a number of 
sses that need large amounts of 
to be effective tools (e.g. web 
P tools and software of the next 
l benefit from the MEANING 
Multilingual
Central Repository
Italian
EWN
Basque
EWN
Spanish
EWN
English
EWN
Basque
Web Corpus
Italian
Web Corpus
English
Web Corpus
Catalan
EWN
Spanish
Web Corpus
Catalan
Web Corpus
ACQ
ACQACQ
ACQ
UPLOADUPLOAD
UPLOADUPLOAD
PORT
PORT
PORT
PORT
WSD
WSD
WSD
WSD
 access applications are based on 
NG will open the way for access 
gual web based on concepts, 
lications with capabilities that 
ceed those currently available. 
ill facilitate development of 
pen domain Internet applications 
tion/Answering, Cross Lingual 
etrieval, Summarisation, Text 
Event Tracking, Information 
achine Translation, etc.). 
EANING will supply a common 
cture to Internet documents, thus 
owledge management of web 
ommon conceptual structure is a ord senses conocer_1 and saber_1 in 
panish, con?ixer_1 and saber_1 in Catalan 
nd antzeman_1, jakin_2 and ezagutu_1 in 
asque.  
ense 1: know, cognize -- (be
cognizant or aware of a fact or a
specific piece of information;
possess knowledge or information
about;
,1128 <communication> 
,0615 <measure quantity amount quantum> 
,0535 <attribute> 
,0389 <object physical_object> 
,0307 <cognition knowledge> 
 Conclusions 
here the acquisition of knowledge  from large-
cale document collections will be  one of the 
ajor challenge for the next generation of text 
rocessing applications, MEANING emphasises 
ultilingual  content-based access to web 
ontent. Moreover, it can provide a keystone 
nabling technologies for the semantic web. In 
particular, the 
produced by M
the natural kno
semantic proce
linguistic data 
ontologies). NL
generation wil
outcomes.  
Figure 1: MEANING data flow. 
Current web
words; MEANI
to the multilin
providing app
significantly ex
MEANING w
concept-based o
(such as Ques
Information R
Categorisation, 
Extraction, M
Furthermore, M
conceptual stru
facilitating kn
content. This c
decisive enabling technology for allowing the 
semantic web. 
Acknowledgements 
The MEANING project is funded by the 
European Commission (IST-2001-34460). 
References 
Agirre E. and Lersundi M. Extracci?n de relaciones 
l?xico-sem?nticas a partir de palabras derivadas 
usando patrones de definici?n. Proceedings of the 
Annual SEPLN meeting. Spain, 2000. 
Agirre E., Lersundi M. and Mart?nez D. A 
Multilingual Approach to Disambiguate 
Prepositions and Case Suffixes. Proceeding of the 
Workshop ?Word Sense Disambiguation: Recent 
Successes and Future Directions? organized by 
ACL 2002.  
Agirre E. and Mart?nez D. Exploring automatic word 
sense disambiguation with decision lists and the 
Web.  Proceedings of the Workshop ?Semantic 
Annotation And Intelligent Annotation? organized 
by COLING 2000. Luxembourg. 2000.  
Agirre E. and Martinez D. Learning class-to-class 
selectional preferences. Proceedings of the 
Workshop "Computational Natural Language 
Learning" (CoNLL-2001). In conjunction with 
ACL'2001/EACL'2001. Toulouse. 2001. 
Agirre E., Ansa O., Mart?nez D. and Hovy E. 
Enriching WordNet concepts with topic signatures. 
Proceedings of the NAACL workshop on WordNet 
and Other lexical Resources: Applications, 
Extensions and Customizations. Pittsburg. 2001. 
Agirre E. and Martinez D. Integrating selectional 
preferences in WordNet. Proceedings of the first 
International WordNet Conference. Mysore, India, 
2002. 
Blum A. and Mitchel T. Combining labelled and 
unlabeled data with co-training. In Proceedings of 
the 11th Annual Conference on Computational 
Learning Theory. 1998. 
Carroll, J. and McCarthy, D. Word sense 
disambiguation using automatically acquired 
verbal preferences. Computers and the Humanities. 
Senseval Special Issue, Vol. 34, No 1-2. 2000. 
Daud? J., Padr? L. and Rigau G., Mapping 
Multilingual Hierarchies using Relaxation 
Labelling, Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora (EMNLP/VLC'99). Maryland, 
1999.  
Daud? J., Padr? L. and Rigau G., Mapping WordNets 
Using Structural Information , 38th Anual Meeting 
of the ACL. Hong Kong, 2000.  
Daud? J., Padr? L. and Rigau G., A Complete WN1.5 
to WN1.6 Mapping, Proceedings of NAACL 
Workshop "WordNet and Other Lexical Resources: 
Applications, Extensions and Customizations". 
Pittsburg, PA, 2001. 
Escudero G., M?rquez L. and Rigau G., Boosting 
Applied to Word Sense Disambiguation. 
Proceedings of the 11th European Conference on 
Machine Learning. Barcelona. 2000.  
Escudero G., M?rquez L. and Rigau G., Naive Bayes 
and Exemplar-Based approaches to Word Sense 
Disambiguation Revisited.  Proceedings of the 14th 
European Conference on Artificial Intelligence, 
Berlin. 2000.  
Escudero G., M?rquez L. and Rigau G., A 
Comparison between Supervised Learning 
Algorithms for Word Sense Disambiguation. 
Proceedings of Fourth Computational Natural 
Language Learning Workshop. Lisbon. 2000.  
Escudero G., M?rquez L. and Rigau G., An Empirical 
Study of the Domain Dependence of Supervised 
Word Sense Disambiguation Systems. Proceedings 
of Joint SIGDAT Conference on Empirical 
Methods in Natural Language Processing and Very 
Large Corpora. Hong Kong. 2000. 
Escudero G., M?rquez L. and Rigau G., Using 
LazyBoosting for Word Sense Disambiguation. 
Proceedings of 2nd International Workshop 
?Evaluating Word Sense Disambiguation 
Systems?, SENSEVAL-2. Toulouse. 2001. 
Fellbaum C. editor. WordNet An Electronic Lexical 
Database. The MIT Press. 1998. 
Ide, N. and V?ronis, J. Introduction to the special 
issue on word sense disambiguation: The state of 
the art. Computational Linguistics, 24 (1), 1998. 
Korhonen A., Gorrell, G. and McCarthy D. Statistical 
Filtering and Subcategorization Frame 
Acquisition. In Proceedings of the Joint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora. 
Hong Kong. 2000. 
Leacock, C. Chodorow, M. and Miller, G.A. Using 
Corpus Statistics and WordNet Relations for Sense 
Identication, Computational Linguistics, 24(1), 
1998. 
Magnini B. and Cavagli? G., Integrating subject field 
codes into WordNet. In Proceedings of the 2nd 
International Conference on Language Resources 
and Evaluation, Athens. 2000. 
Mart?nez D. and Agirre E. One Sense per Collocation 
and Genre/Topic Variations. Proceedings of the 
Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora. Hong Kong, 2000. 
McCarthy, D. and Korhonen, A. Detecting verbal 
participation in diathesis alternations. Proceedings 
of the 17th International Conference on 
Computational Linguistics and 36th Annual 
Meeting of the Association for Computational 
Linguistics COLING-ACL'98. Montreal. 1998.  
McCarthy D., Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Aternations, 
Subcategorization Frames and Selectional 
Preferences. Ph.D. thesis, University of Sussex. 
2001. 
McCarthy D., Carroll J. and Preiss J. Disambiguating 
noun and verb senses using automatically acquired 
selectional preferences. Proceedings of the 
SENSEVAL-2 Workshop at ACL/EACL'01, 
Toulouse. 2001. 
Mihalcea R. and Moldovan D. An automatic method 
for generating sense tagged corpora. In 
Proceedings of American Association for Artificial 
Intelligence. 1999. 
Miller G. Five papers on WordNet, Special Issue of 
International Journal of Lexicogrphy 3(4). 1990. 
Ng. H. T. Getting Serious about Word Sense 
Disambiguation. In Proceedings of Workshop 
?Tagging Text with Lexical Semantics: Why, what 
and how??, Washington, 1997. 
Vossen P. EuroWordNet: A Multilingual Database 
with Lexical Semantic Networks, Kluwer Academic 
Publishers, Dordrecht. 1998. 
Yarowsky D., Unsupervised word sense 
disambiguation rivaling supervised methods. In 
Proceedings of the 33rd Annual Meeting of the 
Association for Computational Linguistics. 1995. 
The Italian Lexical Sample Task at SENSEVAL-3
Bernardo Magnini, Danilo Giampiccolo and Alessandro Vallin
ITC-Irst, Istituto per la Ricerca Scientifica e Tecnologica
Via Sommarive, 18 ? 38050 Trento, Italy
{magnini, giampiccolo, vallin}@itc.it
Abstract
The Italian lexical sample task at
SENSEVAL-3 provided a framework to
evaluate supervised and semi-supervised
WSD systems. This paper reports on the
task preparation ? which offered the op-
portunity to review and refine the Italian
MultiWordNet ? and on the results of the
six participants, focussing on both the
manual and automatic tagging procedures.
1 Introduction
The task consisted in automatically determining
the correct meaning of a word within a given con-
text (i.e. a short text snippet). Systems? results
were compared on the one hand to those achieved
by human annotators (upper bound), and on the
other hand to those returned by a basic algorithm
(baseline).
In the second section of this paper an overview
of the task preparation is given and in the follow-
ing one the main features of the participating sys-
tems are briefly outlined and the results of the
evaluation exercise are presented.
In the conclusions we give an overall judgement
of the outcome of the task, suggesting possible im-
provements for the next campaign.
2 Manual Annotation
A collection of manually labeled instances was
built for three main reasons:
1. automatic evaluation (using the Scorer2 pro-
gram) required a Gold Standard list of senses
provided by human annotators;
2. supervised WSD systems need a labeled set of
training data, that in our case was twice larger
than the test set;
3. manual semantic annotation is a time-
consuming activity, but SENSEVAL repre-
sents the framework to build reusable bench-
mark resources. Besides, manual sense tagging
entails the revision of the sense inventory,
whose granularity does not always satisfy an-
notators.
2.1 Corpus and Words Choice
The document collection from which the anno-
tators selected the text snippets containing the
lemmata to disambiguate was the macro-balanced
section of the Meaning Italian Corpus (Bentivogli
et al, 2003). This corpus is an open domain col-
lection of newspaper articles that contains about 90
million tokens covering a time-spam of 4 years
(1998-2001). The corpus was indexed in order to
browse it with the Toolbox for Lexicographers
(Giuliano, 2002), a concordancer that enables tag-
gers to highlight the occurrences of a token within
a context.
Two taggers chose 45 lexical entries (25 nouns,
10 adjectives and 10 verbs) according to their
polysemy in the sense inventory, their polysemy in
the corpus and their frequency (Edmonds, 2000).
The words that had already been used at
SENSEVAL-2 were avoided. Ten words were
shared with the Spanish, Catalan and Basque lexi-
cal sample tasks.
Annotators were provided with a formula that
indicated the number of labeled instances for each
lemma1, so they checked that the words were con-
1 No. of labeled instances for each lemma = 75 + (15*no. of attested senses) +
(7* no. of attested multiwords), where 75 is a fixed number of examples distrib-
uted over all the attested senses.
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
siderably frequent and polysemous before starting
to tag and save the instances.
As a result, average polysemy attested in the la-
beled data turned out to be quite high: six senses
for the nouns, six for the adjectives and seven for
the verbs.
2.2 Sense Inventory and Manual Tagging
Differently from the Italian lexical sample task
at SENSEVAL-2, where the instances were tagged
according to ItalWordNet (Calzolari et al, 2002),
this year annotators used the Italian MultiWord-
Net, (hereafter MWN) developed at ITC-Irst (Pi-
anta, 2002). This lexical-semantic database
includes about 42,000 lemmata and 60,000 word
senses, corresponding to 34,000 synsets. Instead of
distributing to participants the senses of each
lemma and a limited hierarchical data structure of
the semantic relations of the senses (as happened at
SENSEVAL-2), the entire resource was made
available. Nevertheless, none of the six participat-
ing systems, being supervised, actually needed
MWN.
The annotators? task was to tag one occurrence
of each selected word in all the saved instances,
assigning only one sense drawn from the Italian
MWN. The Toolbox for Lexicographers enabled
annotators to browse the document collection and
to save the relevant text snippets, while a graphical
interface2 was used to annotate the occurrences,
storing them in a database. Generally, instances
consisted of the sentence containing the ambiguous
lemma, with a preceding and a following sentence.
Nevertheless, annotators tended to save the mini-
mal piece of information that a human would need
to disambiguate the lemma, which was often
shorter than three sentences.
The two annotators were involved simultane-
ously: firstly, each of them saved a part of the in-
stances and tagged the occurrences, secondly they
tagged the examples that had been chosen by the
other one.
More importantly, they interacted with a lexi-
cographer, who reviewed the sense inventory
whenever they encountered difficulties. Sometimes
there was an overlap between two or more word
senses, while in other cases MWN needed to be
enriched, adding new synsets, relations or defini-
2 This tool was designed and developed by Christian Girardi at ITC-Irst, Trento,
Italy.
tions. All the 45 lexical entries we considered were
thoroughly reviewed, so that word senses were as
clear as possible to the annotators. On the one
hand, the revision of MWN made manual tagging
easier, while on the other hand it led to a high Inter
Tagger Agreement (that ranged between 73 and 99
per cent), consequently reflected in the K statistics
(that ranged between 0.68 and 0.99).
Table 1 below summarizes the results of the
manual tagging.
Table 1. Manual Annotation Results
Once the instances had been collected and
tagged by both the annotators, we asked them to
discuss the examples about which they disagreed
and to find a definitive meaning for them.
Since the annotators built the corpus while tag-
ging, they tended to choose occurrences whose
meaning was immediately straightforward, avoid-
ing problematic cases. As a consequence, the ITA
turned out to be so high and the distribution of the
senses in the labeled data set did not reflect the
actual frequency in the Italian language, which
may have affected the systems? performance.
Annotators assigned different senses to 674 in-
stances over a total of 7584 labeled examples.
Generally, disagreement depended on trivial mis-
takes, and in most cases one of the two assigned
meanings was chosen as the final one. Neverthe-
less, in 46 cases the third and last annotation was
different from the previous two, which could dem-
onstrate that a few word senses were not com-
pletely straightforward even after the revision of
the sense inventory.
For example, the following instance for the
lemma ?vertice? (vertex, acme, peak) was anno-
tated in three different ways:
La struttura lavorativa ? spiega Grandi ? ha un carattere paramilita-
re. Al vertice della piramide c?? il direttore, poi i manager, quelli con
la cravatta e la camicia a mezze maniche.
Annotator 1 tagged with sense 2 (Factotum,
?the highest point of something?), while annotator
2 decided for sense 4 (Geometry, ?the point of in-
Average
polysemy
in MWN
Average
polysemy in
the labeled set
I.T.A.
Average
K
# training
examples
# test
examples
25 nouns 10 6 0.9 2835 1343
10 adjectives 8 6 0.89 1111 524
10 verbs 9 7 0.89 1199 572
tersection of lines or the point opposite the base of
a figure?) because the text refers to the vertex of a
pyramid. Actually, the snippet reported this ab-
stract image to describe the structure of an enter-
prise, so in the end the two taggers opted for sense
5 (Administration, ?the group of the executives of
a corporation?). Therefore, subjectivity in manual
tagging was considerably reduced by adjusting the
sense repository and selecting manually each sin-
gle instance, but it could not be eliminated.
3 Automatic Annotation
We provided participants with three data sets:
labeled training data (twice larger than the test set),
unlabeled training data (about 10 times the labeled
instances) and test data. In order to facilitate par-
ticipation, we PoS-tagged the labeled data sets us-
ing an Italian version of the TnT PoS-tagger
(Brants, 2000), trained on the Elsnet corpus.
3.1 Participants? results
Three groups participated in the Italian lexical
sample task, testing six systems: two developed by
ITC-Irst - Italy - (IRST-Kernels and IRST-Ties),
three by Swarthmore College - U.S.A. - (swat-hk-
italian, Italian-swat_hk-bo and swat-italian) and
one by UNED - Spain.
Table 2 below reports the participants? results,
sorted by F-measure.
system precision recall attempted F-measure
IRST-Kernels 0.531 0.531 100% 0.531
swat-hk-italian 0.515 0.515 100% 0.515
UNED 0.498 0.498 100% 0.498
italian-swat_hk-bo 0.483 0.483 100% 0.483
swat-italian 0.465 0.465 100% 0.465
IRST-Ties 0.552 0.309 55.92% 0.396
baseline 0.183 0.183 100% 0.183
Table 2. Automatic Annotation Results (fine-grained score)
The baseline results were obtained running a sim-
ple algorithm that assigned to the instances of the
test set the most frequent sense of each lemma in
the training set. All the systems outperformed the
baseline and obtained similar results. Compared to
the baseline of the other Lexical Sample tasks, ours
is much lower because we interpreted the formula
described above (see footnote 1), and tagged the
same number of instances for all the senses of each
lemma disregarding their frequency in the docu-
ment collection. As a result, the distribution of the
examples over the attested senses did not reflect
the one in natural language, which may have af-
fected the systems? performance.
While at SENSEVAL-2 test set senses were
clustered in order to compute mixed- and coarse-
grained scores, this year we decided to return just
the fine-grained measure, where an automatically
tagged instance is correct only if the sense corre-
sponds to the one assigned by humans, and wrong
otherwise (i.e. one-to-one mapping).
There are different sense clustering methods,
but grouping meanings according to some sort of
similarity is always an arbitrary decision. We in-
tended to calculate a domain-based coarse-grained
score, where word senses were clustered according
to the domain information provided in WordNet
Domains (Magnini and Cavagli?, 2000). Unfortu-
nately, this approach would have been significant
with nouns, but not with adjectives and verbs, that
belong mostly to the generic Factotum domain, so
we discarded the idea.
All the six participating systems were super-
vised, which means they all used the training data
set and no one utilized either unlabelled instances
or the lexical database. UNED used also SemCor
as an additional source of training examples.
IRST-Kernels system exploited Kernel methods
for pattern abstraction and combination of different
knowledge sources, in particular paradigmatic and
syntagmatic information, and achieved the best F-
measure score.
IRST-Ties, a generalized pattern abstraction
system originally developed for Information Ex-
traction tasks and mainly based on the boosted
wrapper induction algorithm, used only lemma and
POS as features. Proposed as a ?baseline? system
to discover syntagmatic patterns, it obtained a quite
low recall (about 55 per cent), which affected the
F-measure, but proved to be the most precise sys-
tem.
Swarthmore College wrote three supervised
classifiers: a clustering system based on cosine
similarity, a decision list system and a naive bayes
classifier. Besides, Swarthmore group took advan-
tage of two systems developed at the Hong Kong
Polytechnic University: a maximum entropy classi-
fier and system which used boosting (Italian-
swat_hk-bo). The run swat-hk-italian joined all the
five classifiers according to a simple majority-vote
scheme, while swat-hk-italian did the same using
only the three classifiers developed in Swarthmore.
The system presented by the UNED group em-
ployed similarity as a learning paradigm, consid-
ering the co-occurrence of different nouns and
adjectives.
3.2 General Remarks on Task Complexity
As we mentioned above, the 45 words for the
Italian lexical sample task were chosen according
to their polysemy and frequency. We addressed
difficult words, that had at least 5 senses in MWN.
Actually, polysemy does not seem to be directly
related to systems? results (Calzolari, 2002), in fact
the average F-measure of our six runs for the
nouns (0.512) was higher than for adjectives
(0.472) and verbs (0.448), although the former had
more attested senses in the labeled data.
Complexity in returning the correct sense seems
to depend on the blurred distinction between simi-
lar meanings rather than on the number of senses
themselves. If we consider the nouns ?attacco?
(attack) and ?esecuzione? (performance, execu-
tion), for which the systems obtained the worst and
one of the best average results respectively, we
notice that the 4 attested senses of ?esecuzione?
were clearly distinguished and referred to different
domains (Factotum, Art, Law and Politics), while
the 6 attested senses of ?attacco? were more subtly
defined. Senses 2, 7 and 11 were very difficult to
discriminate and often appeared in metaphorical
contexts. Senses 5 and 6, for their part, belong to
the Sport domain and are not always easy to dis-
tinguish.
4 Conclusions
The results of the six systems participating in
the evaluation exercise showed some improve-
ments compared to the average performance at
SENSEVAL-2, though data sets and sense reposi-
tories were considerably different.
We are pleased with the successful outcome of
the experiments in terms of participation, although
regrettably no system exploited the unlabeled
training set, which was intended to offer a less
time-consuming resource. On the other hand, the
labeled instances that have been collected represent
a useful and reusable benchmark.
As a final remark we think it could be interest-
ing to consider the actual distribution of word
senses in Italian corpora in collecting the examples
for the next campaign.
Acknowledgements
We would like to thank Christian Girardi and
Oleksandr Vagin for their technical support;
Claudio Giuliano and the Ladin Cultural Centre
for the use of their Toolbox for Lexicographers;
Pamela Forner, Daniela Andreatta and Elisabetta
Fauri for the revision of the Italian MWN and on
the semantic annotation of the examples; and Luisa
Bentivogli and Emanuele Pianta for their precious
suggestions during the manual annotation.
References
Luisa Bentivogli, Christian Girardi and Emanuele
Pianta. 2003. The MEANING Italian Corpus. In Pro-
ceedings of the Corpus Linguistics 2003 conference,
Lancaster, UK: 103-112.
Francesca Bertagna, Claudia Soria and Nicoletta Cal-
zolari. 2001. The Italian Lexical Sample Task. In
Proceedings of SENSEVAL-2: Second International
Workshop on Evaluating Word Sense Disambigua-
tion Systems, Toulouse, France: 29-32.
Thorsten Brants. 2000. TnT - a Statistical Part-of-
Speech Tagger. In Proceedings of the Sixth Applied
Natural Language Processing Conference ANLP-
2000, Seattle, WA: 224-231.
Nicoletta Calzolari, Claudia Soria, Francesca Bertagna
and Francesco Barsotti. 2002. Evaluating lexical re-
sources using SENSEVAL. Natural Language Engi-
neering, 8(4): 375-390.
Philip Edmonds. 2000. Designing a task for
SENSEVAL-2.
(http://www.sle.sharp.co.uk/SENSEVAL2/archive/in
dex.htm)
Claudio Giuliano. 2002. A Toolbox for Lexicographers
In Proceedings of the tenth EURALEX International
Congress, Copenhagen, Denmark: 113-118.
Bernardo Magnini and Gabriela Cavagli?. 2000. Inte-
grating Subject Field Codes into WordNet. In Pro-
ceedings of LREC-2000, Athens, Greece: 1413-1418.
Emanuele Pianta, Luisa Bentivogli and Christian
Girardi. 2002. MultiWordNet: developing an aligned
multilingual database. In Proceedings of the First
International Conference on Global WordNet, My-
sore, India: 293-302.
The ?Meaning? System on the English Allwords Task
L. Villarejo   , L. Ma`rquez   , E. Agirre  , D. Mart??nez  , , B. Magnini  ,
C. Strapparava  , D. McCarthy  , A. Montoyo  , and A. Sua?rez 
 
TALP Research Center, Universitat Polite`cnica de Catalunya,  luisv,lluism  @lsi.upc.es
 IXA Group, University of the Basque Country,  eneko,davidm  @si.ehu.es
 ITC-irst (Istituto per la Ricerca Scientifica e Tecnologica),  magnini,strappa  @itc.it
 University of Sussex, dianam@sussex.ac.uk
 LSI, University of Alicante, montoyo@dlsi.ua.es,armando.suarez@ua.es
1 Introduction
The ?Meaning? system has been developed within
the framework of the Meaning European research
project1 . It is a combined system, which integrates
several supervised machine learning word sense
disambiguation modules, and several knowledge?
based (unsupervised) modules. See section 2 for de-
tails. The supervised modules have been trained ex-
clusively on the SemCor corpus, while the unsuper-
vised modules use WordNet-based lexico?semantic
resources integrated in the Multilingual Central
Repository (MCR) of the Meaning project (Atserias
et al, 2004).
The architecture of the system is quite simple.
Raw text is passed through a pipeline of linguis-
tic processors (tokenizers, POS tagging, named en-
tity extraction, and parsing) and then a Feature Ex-
traction module codifies examples with features ex-
tracted from the linguistic annotation and MCR.
The supervised modules have priority over the un-
supervised and they are combined using a weighted
voting scheme. For the words lacking training ex-
amples, the unsupervised modules are applied in a
cascade sorted by decreasing precision. The tuning
of the combination setting has been performed on
the Senseval-2 allwords corpus.
Several research groups have been providers of
resources and tools, namely: IXA group from the
University of the Basque Country, ITC-irst (?Is-
tituto per la Ricerca Scientifica e Tecnologica?),
University of Sussex (UoS), University of Alicante
(UoA), and TALP research center at the Technical
University of Catalonia. The integration was carried
out by the TALP group.
2 The WSD Modules
We have used up to seven supervised learning sys-
tems and five unsupervised WSD modules. Some
of them have also been applied individually to the
1Meaning, Developing Multilingual Web-scale Lan-
guage Technologies (European Project IST-2001-34460):
http://www.lsi.upc.es/  nlp/meaning/meaning.html.
Senseval-3 lexical sample and allwords tasks.
 Naive Bayes (NB) is the well?known Bayesian
algorithm that classifies an example by choos-
ing the class that maximizes the product, over
all features, of the conditional probability of
the class given the feature. The provider of this
module is IXA. Conditional probabilities were
smoothed by Laplace correction.
 Decision List (DL) are lists of weighted clas-
sification rules involving the evaluation of one
single feature. At classification time, the algo-
rithm applies the rule with the highest weight
that matches the test example (Yarowsky,
1994). The provider is IXA and they also ap-
plied smoothing to generate more robust deci-
sion lists.
 In the Vector Space Model method (cosVSM),
each example is treated as a binary-valued fea-
ture vector. For each sense, one centroid vec-
tor is obtained from training. Centroids are
compared with the vectors representing test ex-
amples, using the cosine similarity function,
and the closest centroid is used to classify the
example. No smoothing is required for this
method provided by IXA.
 Support Vector Machines (SVM) find the hy-
perplane (in a high dimensional feature space)
that separates with maximal distance the pos-
itive examples from the negatives, i.e., the
maximal margin hyperplane. Providers are
TALP (SVM  ) and IXA (SVM 	 ) groups. Both
used the freely available implementation by
(Joachims, 1999), linear kernels, and one?vs?
all binarization, but with different parameter
tuning and feature filtering.
 Maximum Entropy (ME) are exponential
conditional models parametrized by a flexible
set of features. When training, an iterative opti-
mization procedure finds the probability distri-
bution over feature coefficients that maximizes
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
the entropy on the training data. This system is
provided by UoA.
 AdaBoost (AB) is a method for learning an en-
semble of weak classifiers and combine them
into a strong global classification rule. We
have used the implementation described in
(Schapire and Singer, 1999) with decision trees
of depth fixed to 3. The provider of this system
is TALP.
 Domain Driven Disambiguation (DDD) is an
unsupervised method that makes use of do-
main information in order to solve lexical am-
biguity. The disambiguation of a word in
its context is mainly a process of compari-
son between the domain of the context and
the domains of the word?s senses (Magnini
et al, 2002). ITC-irst provided two variants
of the system DDD   and DDD  , aiming at
maximizing precision and F  score, respec-
tively. The UoA group also provided another
domain?based unsupervised classifier (DOM).
Their approach exploits information contained
in glosses of WordNet Domains and introduces
a new lexical resource ?Relevant Domains? ob-
tained from Association Ratio over glosses of
WordNet Domains.
 Automatic Predominant Sense (autoPS) pro-
vide an unsupervised first sense heuristic for
the polysemous words in WordNet. This
is produced by UoS automatically from the
BNC (McCarthy et al, 2004). The method
uses automatically acquired thesauruses for the
main PoS categories. The nearest neighbors
for each word are related to its WordNet senses
using a WordNet similarity measure.
 We also used a Most Frequent Sense tagger,
according to the WordNet ranking of senses
(MFS).
3 Evaluation of Individual Modules
For simplicity, and also due to time constraints, the
supervised modules were trained exclusively on the
SemCor-1.6 corpus, intentionally avoiding the use
of other sources of potential training examples, e.g,
other corpora, WordNet examples and glosses, sim-
ilar/substitutable examples extracted from the same
Semcor-1.6, etc. An independent training set was
generated for each polysemous word (of a certain
part?of?speech) with 10 or more examples in the
SemCor-1.6 corpus. This makes a total of 2,440 in-
dependent learning problems, on which all super-
vised WSD systems were trained.
The feature representation of the training exam-
ples was shared between all learning modules. It
consists of a rich feature representation obtained
using the Feature Extraction module of the TALP
team in the Senseval-3 English lexical sample task.
The feature set includes the classic window?based
pattern features extracted from a local context and
the ?bag?of?words? type of features taken from a
broader context. It also contains a set of features
representing the syntactic relations involving the
target word, and semantic features of the surround-
ing words extracted from the MCR of the Meaning
project. See (Escudero et al, 2004) for more details
on the set of features used.
The validation corpus for these classifiers was the
Senseval-2 allwords dataset, which contains 2,473
target word occurrences. From those, 2,239 occur-
rences correspond to polysemous words. We will
refer to this subcorpus as S2-pol. Only 1,254 words
from S2-pol were actually covered by the classifiers
trained on the SemCor-1.6 corpus. We will refer to
this subset of words as the S2-pol-sup corpus. The
conversion between WordNet-1.6 synsets (SemCor-
1.6) and WordNet-1.7 (Senseval-2) was performed
on the output of the classifiers by applying an auto-
matically derived mapping provided by TALP2.
Table 1 shows the results (precision and cover-
age) obtained by the individual supervised modules
on the S2-pol-sup subcorpus, and by the unsuper-
vised modules on the S2-pol subcorpus (i.e., we
exclude from evaluation the monosemous words).
Support Vector Machines and AdaBoost are the best
performing methods, though all of them perform in
a small accuracy range from 53.4% to 59.5%.
Regarding the unsupervised methods, DDD is
clearly the best performing method, achieving a re-
markable precision of 61.9% with the DDD   vari-
ant, at a cost of a lower coverage. The DDD  ap-
pears to be the best system for augmenting the cov-
erage of the former. Note that the autoPS heuristic
for ranking senses is a more precise estimator than
the WordNet most?frequent?sense (MFS).
4 Integration of WSD modules
All the individual modules have to be integrated in
order to construct a complete allwords WSD sys-
tem. Following the architecture described in section
1, we decided to apply the unsupervised modules
only to the subset of the corpus not covered by the
training examples. Some efforts on applying the
unsupervised modules jointly with the supervised
failed at improving accuracy. See an example in ta-
ble 3.
2http://www.lsi.upc.es/  nlp/tools/mapping.html
supervised, S2-pol-sup corpus unsupervised, S2-pol corpus
SVM   AB cosVSM SVM  ME NB DL DDD  DDD  autoPS MFS DOM
prec. 59.5 59.1 57.8 57.1 56.3 54.6 53.4 61.9 50.2 45.2 32.5 23.8
cov. 100.0 100.0 100.0 100.0 100.0 100.0 100.0 48.8 99.6 89.6 98.0 49.1
Table 1: Results of individual supervised and unsupervised WSD modules
As a first approach, we devised three baseline
systems (Base-1, Base-2, and Base-3), which use
the best modules available in both subsets. Base-1
applies the SVM  supervised method and the MFS
for the non supervised part. Base-2 applies also the
SVM  supervised method and the cascade DDD   ?
MFS for the non supervised part (MFS is used in the
cases in which DDD   abstains). Base-3 shares the
same approach but uses a third unsupervised mod-
ule: DDD   ?DDD  ?MFS.
The precision results of the baselines systems can
be found in the right hand side of table 3. As it can
be observed, the positive contribution of the DDD  
module is very significant since Base-2 performs
2.2 points higher than Base-1. The addition of the
third unsupervised module (DDD   ) makes Base-3
to gain 0.4 extra precision points.
As simple combination schemes we considered
majority voting and weighted voting. More sophis-
ticated combination schemes are difficult to tune
due to the extreme data sparseness on the valida-
tion set. In the case of unsupervised systems, these
combination schemes degraded accuracy because
the least accurate systems perform much worse that
the best ones. Thus, we simply decided to apply a
cascade of unsupervised modules sorted by preci-
sion on the Senseval-2 corpus.
In the case of the supervised classifiers there is a
chance of improving the global performance, since
there are several modules performing almost as well
as the best. Previous to the experiments, we cal-
culated the agreement rates on the outputs of each
pair of systems (low agreements increase the prob-
ability of uncorrelatedness between errors of differ-
ent systems). We obtained an average agreement of
83.17%, with values between 64.7% (AB vs SVM 	 )
and 88.4% (SVM 	 vs cosVSM).
The ensembles were obtained by incrementally
aggregating, to the best performing classifier, the
classifiers from a list sorted by decreasing accu-
racy. The ranking of classifiers can be performed
by evaluating them at different levels of granular-
ity: from particular words to the overall accuracy
on the whole validation set. The level of granularity
defines a tradeoff between classifier specialization
and risk of overfitting to the tuning corpus. We de-
cided to take an intermediate level of granularity,
and sorted the classifiers according to their perfor-
mance on word sets based on the number of training
examples available3 .
Table 2 contains the results of the ranking exper-
iment, by considering five word-sets of increasing
number of training examples: between 10 and 20,
between 21 and 40, between 41 and 80, etc. At each
cell, the accuracy value is accompanied by the rel-
ative position the system achieves in that particu-
lar subset. Note that the resulting orderings, though
highly correlated, are quite different from the one
derived from the overall results.
(10,20) (21,40) (41,80) (81,160)  160
SVM  60.9-1 59.1-1 64.2-2 61.1-2 56.4-1
AB 60.9-1 56.6-2 60.0-7 64.7-1 56.1-2
c-VSM 59.9-2 56.6-2 62.6-3 57.0-4 55.8-3
SVM  50.8-5 55.1-4 61.6-4 57.4-3 53.1-5
ME 56.7-3 55.3-3 65.3-1 53.3-5 53.8-4
NB 59.9-2 54.6-5 61.1-5 49.2-6 51.5-7
DL 56.4-4 49.9-6 60.5-6 47.2-7 52.5-6
Table 2: Results on frequency?based word sets
Table 3 shows the precision results4 of the Mean-
ing system obtained on the whole Senseval-2 corpus
by combining from 1 to 7 supervised classifiers ac-
cording to the classifier orderings of table 2 for each
subset of words. The unsupervised classifiers are
all applied in a cascade sorted by precision. M-Vot
stands for a majority voting scheme, while W-Vot
refers to the weighted voting scheme. The weights
for the classifiers are simply the accuracy values on
the validation corpus. As an additional example,
the column M-Vot+ shows the results of the vot-
ing scheme when the unsupervised DDD   module
is also included in the ensemble. The table also in-
cludes the baseline results.
Unfortunately, the ensembles of classifiers did
not provide significant improvements on the final
precision. Only in the case of weighted voting a
slight improvement is observed when adding up to
3 classifiers. From the fourth classifier performance
also degrades. The addition of unsupervised sys-
tems to the supervised ensemble systematically de-
graded performance.
As a reference, the best result (67.5% precision
3One of the factors that differentiates between learning al-
gorithms is the amount of training examples needed to learn.
4Coverage of the combined systems is 98% in all cases.
M-Vot W-Vot M-Vot+ Base-1 Base-2 Base-3
1 67.3 67.3 66.4 ? ? ?
2 ? 67.4 66.3 ? ? ?
3 67.2 67.5 67.1 ? ? ?
4 ? 67.1 66.9 ? ? ?
5 66.5 66.5 66.7 ? ? ?
6 ? 66.3 66.3 ? ? ?
7 65.7 65.9 66.0 ? ? ?
best 67.3 67.5 67.1 64.8 67.0 67.4
Table 3: Results of the combination of systems
System prec. recall F 
Meaning-c 61.1% 61.0% 61.05
Meaning-wv 62.5% 62.3% 62.40
Table 4: Results on the Senseval-3 test corpus
and 98.0% coverage) would have put our combined
system in second place in the Senseval-2 allwords
task.
5 Evaluation on the Senseval-3 Corpus
The Senseval-3 test set contains 2,081 target words,
1,851 of them polysemous. The subset covered by
the SemCor-1.6 training contains 1,211 target words
(65.42%, compared to the 56.0% of the Senseval-2
corpus). We submitted the outputs of two different
configurations of the Meaning system: Meaning-
c and Meaning-wv. These systems correspond to
Base-3 and W-Vot (in the best configuration) from
table 3, respectively. The results from the official
evaluation are given in table 4. Again, we applied an
automatic mapping from WordNet-1.6 to WordNet-
1.7.1 synset labels. However, there are senses in
1.7.1 that do not exist in 1.6, thus our system sim-
ply cannot assign them.
It can be observed that, even though on the tun-
ing corpus both variants obtained very similar pre-
cision (67.4 and 67.5), on the test set the weighted
voting scheme is clearly better than the baseline sys-
tem, probably due to the robustness achieved by the
ensemble. The performance decrease observed on
the test set with respect to the Senseval-2 corpus is
very significant (   5 points). Given that the baseline
system performs worse than the voted approach, it
seems unlikely that there is overfitting during the
ensemble tuning. However, we plan to repeat the
tuning experiments directly on the Senseval-3 cor-
pus to see if the same behavior and conclusions
are observed. Probably, the decrease in perfor-
mance is due to the differences between the train-
ing and test corpora. We intend to investigate the
differences between SemCor-1.6, Senseval-2, and
Senseval-3 corpora at different levels of linguistic
information in order to check the appropriateness of
using SemCor-1.6 as the main information source.
6 Acknowledgements
This research has been possible thanks to the sup-
port of European and Spanish research projects:
IST-2001-34460 (Meaning), TIC2000-0335-C03-
02 (Hermes). The authors would like to thank also
Gerard Escudero for letting us use the Feature Ex-
traction module and German Rigau for helpful sug-
gestions and comments.
References
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Car-
roll, B. Magnini, and P. Vossen. 2004. The
Meaning multilingual central repository. In Pro-
ceedings of the Second International WordNet
Conference.
G. Escudero, L. Ma`rquez, and G. Rigau. 2004.
TALP system for the english lexical sample task.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
T. Joachims. 1999. Making large?scale SVM learn-
ing practical. In B. Scho?lkopf, C. J. C. Burges,
and A. J. Smola, editors, Advances in Kernel
Methods ? Support Vector Learning, pages 169?
184. MIT Press, Cambridge, MA.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
D. McCarthy, R. Koeling, J. Weeds, and J. Car-
roll. 2004. Using automatically acquired pre-
dominant senses for word sense disambiguation.
In Proceedings of the Senseval-3 ACL Workshop,
Barcelona, Spain.
R. Schapire and Y. Singer. 1999. Improved boost-
ing algorithms using confidence?rated predic-
tions. Machine Learning, 37(3):297?336.
David Yarowsky. 1994. Decision lists for lexi-
cal ambiguity resolution: Application to accent
restoration in Spanish and French. In Proceed-
ings of the 32nd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 88?95,
Las Cruces, NM.
Revising the WORDNET DOMAINS Hierarchy: semantics, coverage and 
balancing 
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, Emanuele Pianta 
ITC-irst ? Istituto per la Ricerca Scientifica e Tecnologica 
Via Sommarive 18, Povo ? Trento, Italy, 38050 
email:{bentivo, forner, magnini, pianta}@itc.it 
 
Abstract 
The continuous expansion of the multilingual 
information society has led in recent years to a pressing 
demand for multilingual linguistic resources suitable to 
be used for different applications.  
In this paper we present the WordNet Domains 
Hierarchy (WDH), a language-independent resource 
composed of 164, hierarchically organized, domain 
labels (e.g. Architecture, Sport, Medicine). Although 
WDH has been successfully applied to various Natural 
Language Processing tasks, the first available version 
presented some problems, mostly related to the lack of a 
clear semantics of the domain labels. Other correlated 
issues were the coverage and the balancing of the 
domains. We illustrate a new version of WDH 
addressing these problems by an explicit and systematic 
reference to the Dewey Decimal Classification. The new 
version of WDH has a better defined semantics and is 
applicable to a wider range of tasks. 
1 Introduction 
The continuous expansion of the multilingual 
information society with a growing number of new 
languages present on the Web has led in recent 
years to a pressing demand for multilingual 
applications. To support such applications, 
multilingual language resources are needed, which 
however require a lot of human effort to be built. 
For this reason, the development of language-
independent resources which factorize what is 
common to many languages, and are possibly 
linked to the language-specific resources, could 
bring great advantages to the development of the 
multilingual information society. 
A language-independent resource, usable in 
many automatic and human applications, is 
represented by domain hierarchies. The notion of 
domain is related to similar notions such as 
semantic field, subject matter, broad topic, subject 
code, subject domain, category. These notions are 
used, sometimes interchangeably, sometimes with 
significant distinctions, in various fields such as 
linguistics, lexicography, cataloguing, text 
categorization. As far as this work is concerned, 
we define a domain as an area of knowledge which 
is somehow recognized as unitary. A domain can 
be characterized by the name of a discipline where 
a certain knowledge area is developed (e.g. 
chemistry) or by the specific object of the 
knowledge area (e.g. food). Although objects of 
knowledge and disciplines that study them are 
clearly related, the relation between these two 
points of view on domains is sometimes blurred 
and may be a source of uncertainty on their exact 
definition. 
Another interesting duality when speaking about 
domains is related to the fact that knowledge 
manifests itself in both words and texts. So the 
notion of domain can be applied both to the study 
of words, where a domain is the area of knowledge 
to which a certain lexical concept belongs, or to the 
study of texts, where the domain of a text is its 
broad topic. In this work we will assume that also 
these two points of view on domains are strictly 
intertwined.  
By their nature, domains can be organized in 
hierarchies based on a relation of specificity. For 
instance we can say that TENNIS is a more specific 
domain than SPORT, or that ARCHITECTURE is more 
general than TOWN PLANNING. 
Domain hierarchies can be usefully integrated 
into other linguistic resources and are also 
profitably used in many Natural Language 
Processing (NLP) tasks such as Word Sense 
Disambiguation (Magnini et al 2002), Text 
Categorization (Schutze, 1998), Information 
Retrieval (Walker and Amsler, 1986).  
As regards the usage of Domain hierarchies in 
the field of multilingual lexicography, an example 
is given by the EuroWordNet Domain-ontology, a 
language independent domain hierarchy to which 
interlingual concepts (ILI-records) can be assigned 
(Vossen, 1998). In the same line, see also the 
SIMPLE domain hierarchy (SIMPLE, 2000).  
Large domain hierarchies are also available on 
the Internet, mainly meant for classifying web 
documents. See for instance the Google and Yahoo 
directories. 
A large-scale application of a domain hierarchy 
to a lexicon is represented by WORDNET DOMAINS 
(Magnini and Cavagli?, 2000). WORDNET 
DOMAINS is a lexical resource developed at ITC-
irst where each WordNet synset (Fellbaum, 1998) 
is annotated with one or more domain labels 
selected from a domain hierarchy which was 
specifically created to this purpose. As the 
WORDNET DOMAINS Hierarchy (WDH) is 
language-independent, it has been possible to 
exploit it in the framework of MultiWordNet 
(Pianta et al, 2002), a multilingual lexical database 
developed at ITC-irst in which the Italian 
component is strictly aligned with the English 
WordNet. In MultiWordNet, the domain 
information has been automatically transferred 
from English to Italian, resulting in a Italian 
version of WORDNET DOMAINS. For instance, as 
the English synset {court, tribunal, judicature} was 
annotated with the domain LAW, also the Italian 
synset {corte, tribunale}, which is aligned with the 
corresponding English synset, results automatically 
annotated with the LAW domain. This procedure 
can be applied to any other WordNet (or part of it) 
aligned with Princeton WordNet (see for instance 
the Spanish WordNet). 
It is worth noticing that two of the main on-
going projects addressing the construction of 
multilingual resources, that is MEANING (Rigau 
et al 2002) and BALKANET (see web site), make 
use of WORDNET DOMAINS. Finally, WORDNET 
DOMAINS is being profitably used by the NLP 
community mainly for Word Sense 
Disambiguation tasks in various languages. 
Another application of domain hierarchies can 
be found in the field of corpus creation. In many 
existing corpora (see for instance the BNC, the 
ANC, the Brown and LOB Corpora) domain is one 
of the most used criteria for text selection and/or 
classification. Given that a domain hierarchy is 
language independent, if the same domain 
hierarchy is used to build reference corpora for 
different languages, then it would be easy to create 
(a first approximation of) comparable corpora by 
putting in correspondence corpora sections 
belonging to the same domain. 
An example of a corpus in which the complete 
representation of domains is pursued in a 
systematic way is represented by the MEANING 
Italian corpus, a large size corpus of written 
contemporary Italian in which a subset of the 
WDH labels has been chosen as the fundamental 
criterion for the selection of the texts to be 
included in the corpus (Bentivogli et al, 2003). 
Given the relevance of language-independent 
domain hierarchies for multilingual applications, it 
is of primary importance that these resources have 
a well-defined semantics and structure in order to 
be useful in various application fields. This paper 
reports the work done to improve the WDH so that 
it complies with such requirements. In particular, 
the WDH revision has been carried out with 
reference to the Dewey Decimal Classification. 
The paper is organized as follows. Section 2 
briefly introduces the WORDNET DOMAINS 
Hierarchy and its main characteristics, with a short 
overview of the Dewey Decimal Classification 
system. Section 3 describes features and properties 
of the revision. Finally, in section 4, conclusions 
are reported. 
2 The WordNet Domains Hierarchy 
The first version of the WDH was composed of 
164 domain labels selected starting from the 
subject field codes used in current dictionaries, and 
the subject codes contained in the Dewey Decimal 
Classification (DDC), a general knowledge 
organization tool which is the most widely used 
taxonomy for library organization purposes. 
Domain labels were organized in five main trees, 
reaching a maximum depth of four. Figure 1 shows 
a fragment of one of the five main trees in the 
WORDNET DOMAINS original hierarchy. 
Doctrines
Psychology
Art
Religion
Psychoanalysis
Dance
Drawing
Music
Photography
Plastic Arts
Sculpture
Numismatics
Jewellery
Painting
Philately
Philosophy
Theatre
Mythology
Occultism
Roman Catholic
Theology
Figure 1: Fragment of the original WDH 
Domain labels were initially conceived to be 
application-oriented, that is, they have been 
integrated in WordNet with the main purpose of 
allowing the categorization of word senses and to 
provide useful information during the 
disambiguation process. 
The second level of WDH, where the so-called 
Basic Domains are represented, includes labels 
such as ART, SPORT, RELIGION and HISTORY, 
while in the third level a degree of major 
specialization is reproduced, and domains, like for 
example, DRAWING, PAINTING, TENNIS, 
VOLLEYBALL, and ARCHAEOLOGY can be found. For 
NLP tasks, the set of Basic Domains has proved to 
possess a suitable level of abstraction and 
granularity. 
Although the first version of WDH found many 
applications in different scenarios, it presented 
some problems. First, the domain labels did not 
have a defined semantics. The content of the labels 
could be suggested by the lexical meaning of their 
name, but there was no explicit indication about 
their intended interpretation. 
Second, it was not clear whether the Basic 
Domains met certain requirements such as 
knowledge coverage and balancing. In fact, the 
Basic Domains are supposed to possess a 
comparable degree of granularity and, at the same 
time, to cover all human knowledge. However, 
they did not always posses such characteristics. For 
instance VETERINARY was put at the same level as 
ECONOMY, although these two domains obviously 
do not posses the same level of granularity. 
Moreover not all branches of human knowledge 
were represented (see for instance the HOME 
domain). 
The purpose of the work presented here was, 
therefore, to find a solution for such problems, in 
order to improve the applicability of WDH in a 
wider range of fields. The solution we propose is 
crucially based on the Dewey Decimal 
Classification (edition 21), which has been used as 
a reference point for defining a clear semantics, 
preventing overlapping among domains, and 
assessing the Basic Domains coverage and 
granularity issues.  
2.1 The Dewey Decimal Classification (DDC)  
The Dewey Decimal Classification (DDC) system 
(Mitchell et al 1996) is the most widely used 
taxonomy for library classification purposes 
providing a logical system for the organization of 
every item of knowledge through well-defined 
subject codes hierarchically organized. The 
semantics of each subject code is determined by a 
numeric code, a short lexical description associated 
to it, and by the hierarchical relations with the 
other subject codes. Another characteristic of the 
DDC is that a handbook is available explaining 
how texts should be classified under subject codes. 
The DDC is not just for organizing book 
collections; it has also been licensed for 
cataloguing internet resources (see for example 
BUBL http://bubl.ac.uk/link/) and it was conceived 
to accommodate the expansion and evolution of 
the body of human knowledge.  
The DDC hierarchy is arranged by disciplines 
(or fields of study), and this entails that a subject 
may appear in more than one discipline, depending 
on the aspect of the topic discussed.  
The DDC hierarchical structure allows a topic to 
be defined as part of the broader topic above it, and 
that determines the meaning of the class and its 
relation to other classes. At the broadest level, 
called Main Classes (or First summary), the DDC 
is composed of ten mutually exclusive main 
classes, which together cover the entire world of 
knowledge. Each main class is sub-divided into ten 
divisions, (the Hundred Divisions, or Second 
Summary) and each division is split into ten 
sections (the Thousand Section, also called Third 
Summary). 
Each category in the DDC is represented by a 
numeric code as the example below shows.  
 
700  Art 
 730  Plastic Arts 
  736 Carving 
   736.2 Precious Stones 
    736.23 Diamonds 
    736.25 Sapphires 
   736.4 Wood 
  738 Ceramic Arts 
  739 Art Metalwork 
 740  Drawing 
 750  Painting 
 
The first digit of the numbers indicates the main 
class, (700 is used for all Arts) the second digit 
indicates the hundred division, (730 corresponds to 
Plastic arts, 740 to Drawing, 750 to Painting) and 
the third digit indicates the section (736 represents 
Carving, 738 Ceramic arts, 739 Art metalwork). 
Moreover, almost all sub-classes are further 
subdivided. A decimal point follows the third digit 
until the degree of specification needed (736.23 
Diamonds, 736.25 Sapphires).  
3 The Revision of the WDH 
The revision of the first version of the WDH aimed 
at satisfying the following properties and 
characteristics:  
 
o semantics: each WDH label should have an 
explicit semantics and should be 
unambiguously identified; 
o disjunction: the interpretation of all WDH 
labels should not overlap; 
o basic coverage: all human knowledge should 
be covered  by the Basic Domains; 
o basic balancing: most Basic Domains should 
have a comparable degree of granularity. 
 
In the following sections we are going to show 
how a systematic mapping between WDH and 
DDC can be used to enforce each of the above 
characteristics.  
3.1 Semantics 
To give the domain labels a clear semantics so that 
they can be unambiguously identified and 
interpreted, we decided to associate each domain 
label to one or more DDC codes as shown below in 
Table 1.  
WDH Domains 
 
DDC Codes 
 
 Art 
 
[700-(790-(791.43,792,793.3), 
          710,720,745.5)] 
       Plastic arts 730 
                   Sculpture [731:735] 
                   Numismatics 737 
       Jewellery 739.27 
       Drawing [740-745.5] 
       Painting 750 
       Graphic arts 760 
                   Philately 769.56 
       Photography 770 
       Music 780 
       Cinema 791.43 
       Theatre [792-792.8] 
       Dance [792.8,793.3] 
Table 1: Fragment of the new WDH with the 
respective DDC codes 
In many cases we found a one-to-one mapping 
between a WDH label and a DDC code (e.g. 
PAINTING mapped onto 750 or CINEMA onto 
791.43). When one-to-one mappings were not 
found, artificial DDC codes were created. An 
artificial code, represented within square brackets, 
is created with reference to various DDC codes or 
parts of them. To describe artificial nodes, certain 
conventions have been adopted.  
(i) A series of non-consecutive codes is listed 
separated by a comma (see DANCE). 
(ii) A series of consecutive codes is indicated by a 
range. For instance, the series [731, 732, 733, 734, 
735] is abbreviated as [731:735] (see SCULPTURE). 
(iii) A part of a tree is represented as the difference 
between a tree and one or more of its subtrees, 
where the tree and the subtrees are identified by 
their roots (see DRAWING). 
(iv) The square brackets should be interpreted as 
meaning ?the generalities? of the composition of 
codes contained in the brackets. So, for instance, 
[731:735] should be interpreted as the generalities 
of the codes going from 731 to 735. In the original 
DDC, generalities are identified by the 0 decimal. 
For instance, the code 700 refers to the generalities 
of the codes from 710 to 790. 
To establish a mapping between labels and codes 
we exploited the names of the DDC categories and 
their description in the DDC manual. This worked 
pretty well in most cases, but there are some 
exceptions. Take for instance the TOURISM domain. 
Apparently tourism does not occur as a category in 
the DDC. On a closer inspection it came out that 
the categories which are most clearly related to 
tourism are 910.202:World travel guides and 
910.4:Accounts of travel. 
Note that a WDH domain can be mapped onto 
codes included in different DDC main classes, i.e. 
disciplines. For example ARTISANSHIP 
(745.5:Handicrafts, 338.642:Small business) maps 
onto categories located partly under 700:Art and 
partly under 300:Social Sciences. The same 
happens with SEXUALITY, a domain that following 
the DDC is studied by many different disciplines, 
e.g. philosophy, medicine, psychology, body care. 
As a consequence of the systematic specification 
of the semantics of the WDH domains, some of 
them have been re-labeled with regard to the 
previous version of the hierarchy. For instance, the 
domain BOTANY has been changed to PLANTS, 
ZOOLOGY to ANIMALS, and ALIMENTATION to FOOD. 
This change of focus from the name of the 
discipline to the name of the object of the 
discipline is not only in compliance with the new 
edition of the DDC, but it also reflects current and 
international usage (see, for example, Google 
categories). In some cases the change of the 
domain name comes along with a change of its 
intended interpretation. For instance, we have 
decided to enlarge the semantics of the domain 
ZOOTECHNICS and to call it ANIMAL HUSBANDRY, a 
more generic domain which was missing in the 
previous hierarchy.  
In most cases the hierarchical relations between 
the WDH domains are the same as the relations 
holding between the corresponding DDC codes: 
MUSIC is more specific than ART in the same way 
as 780:Music is more specific than 700:The Arts. 
To reinforce the hierarchical parallelism between 
the WDH and the DCC, we re-located some 
domains with regard to the previous WDH 
hierarchy. For example, OCCULTISM, which was 
placed under RELIGION in the old hierarchy, has 
been moved under the newly created domain 
PARANORMAL. Also, TOPOGRAPHY, previously placed 
under ASTRONOMY, has now been moved under 
GEOGRAPHY.  
In a few cases however we did not respect the 
hierarchical relations specified by the DDC, as in 
the case of the ARCHITECTURE domain shown in 
Table 2. ARCHITECTURE has been mapped onto 
720:Architecture and TOWN PLANNING onto 
710:Civic & landscape art.  
WDH Domains DDC Codes 
 Architecture  [645,690,710,720] 
 
Town Planning 710 
 
Buildings 690 
 
Furniture 645 
Table 2: A fragment of WDH for ARCHITECTURE 
However, whereas the 710 code is sibling of 720 
in the DDC, TOWN PLANNING is child of 
ARCHITECTURE in WDH. Also, ARCHITECTURE and 
TOWN PLANNING should be under ART according to 
the DDC, but they have been placed under 
APPLIED SCIENCE in WDH. 
3.2 Disjunction 
This property requires that no DDC code is 
associated to more than one WDH label. In only 
one case this requirement has not been met. 
Apparently, the DDC does not distinguish between 
the disciplines of Sociology and Anthropology, 
and reserves the codes that go from 301 to 307 to 
both of them. Although these two disciplines are 
strictly connected, it seems to us that in the current 
practice they are considered as distinct. So the 
WDH contains two distinct domains for 
SOCIOLOGY and ANTHROPOLOGY, which partially 
overlap because they both map onto the same DDC 
codes 301:307. 
3.3 Basic Coverage 
The term basic coverage refers to the ideal 
requirement that all human knowledge be covered 
by the totality of the Basic Domains (i.e. the 
domains composing the second level of WDH). 
Also in this case, we used the DDC as a gold 
standard to measure the coverage of WDH. Given 
the fact that the DDC has been used for more than 
a century to classify books and written documents 
all over the world, we can assume that the DDC 
guarantees a complete representation of all 
branches of knowledge. So the basic coverage has 
been manually checked by verifying that all (or 
almost all) the DDC categories can be assigned to 
at least one Basic Domain.  
From a practical point of view, it would be very 
complicated to check all the thousands of codes 
contained in the DDC. Thus, our check relied on 
two assumptions. First, when the Basic Domains 
are taken as a stand alone set, the semantics of a 
Basic Domain is given by its specific code together 
with the codes of its subdomains. Second, once a 
DDC code is covered by a Basic Domain, 
inductively, all the more specific categories are 
covered as well. These assumptions allowed us to 
actually check only the topmost DDC codes. For 
example, let?s take the 300 main class of the DDC. 
Table 3 below shows that all the sub-codes of the 
300 class are covered by one or more domains.  
In order to improve the overall WDH coverage, 
5 completely new domains have been introduced 
(the first three are Basic): PARANORMAL, HOME, 
HEALTH, FINANCE and GRAPHIC ARTS. 
Codes DDC Categories WDH Domains 
300 ? Social sciences 
? SOCIAL SCIENCE 
? SOCIOLOGY 
? ANTHROPOLOGY 
310 ? General statistics ? SOCIOLOGY 
320 ? Political science ? POLITICS 
330 ? Economics ? ECONOMY 
340 ? Law ? LAW 
350 ? Public administration & military service 
? ADMINISTRATION  
? MILITARY 
360 ? Social problems & 
services 
? SOCIOLOGY 
? ECONOMY 
? SEXUALITY 
370 ? Education ? PEDAGOGY 
380 
? Commerce, 
communication, 
transport 
? COMMERCE  
? TELECOMMUNICATION  
? TRANSPORT 
390 ? Customs, etiquette, folklore 
? FASHION  
? ANTHROPOLOGY 
? SEXUALITY  
Table 3: Coverage of the 300 DDC class 
We can now assume that the domain-coverage of 
the new version of WDH is almost equivalent to 
that of the DDC, thus ensuring the complete 
representation of all branches of knowledge. 
The new WDH allowed us to fix a number of 
synset classifications that were unsatisfactory in 
the previous version of WORDNET DOMAINS. For 
instance, in the first version of WORDNET 
DOMAINS the English/Italian synset {microwave 
oven, microwave}/{forno a microonde, 
microonde} was annotated with the FURNITURE 
domain, while the synset {detergent}/{detersivo} 
was annotated with FACTOTUM (i.e. no specific 
domain) as no better solution was available. The 
new WDH hierarchy allows for a more appropriate 
classification of both synsets within the new HOME 
domain. 
A few DDC codes are not covered by the new 
list of domains either. These are the codes under 
the 000:Generalities class which includes 
disciplines such as 010:Bibliography, 020:Library 
& information sciences, 030:Encyclopedic works, 
080:General collections. This section has been 
specifically created for cataloguing general and 
encyclopedic works and collections. So it is a 
idiosyncratic category which is not based on 
subject but on the genre of texts. 
Another set of codes which remains not covered 
by WDH are those going from 420 to 490 and from 
810 to 890. These DDC codes are devoted to 
specific languages and literatures of different 
countries, for example, 430:Germanic Languages, 
440:Romance Languages, 810:American Literature 
in English, etc. These codes are undoubtedly 
relevant for the classification of books, but are not 
compatible with the rationale of WDH, which is 
meant to be a language-independent resource. 
3.4 Basic Balancing 
The requirement about basic balancing is meant to 
assure that all Basic Domains have a comparable 
degree of granularity. 
Defining a granularity metrics for domains is a 
complex issue, for which only a tentative solution 
is provided here. At a first glance, three aspects 
could be taken into consideration: the number of 
publications about a domain, the number of sub-
codes in the DDC, and the relevance of a domain 
in the social life.  
As a first attempt, balancing could be evaluated 
referring to the number of publications classified 
under each Basic Domain. In fact, data are 
available about the number of texts classified 
under each of the DDC codes. Unfortunately, the 
number of books published under a certain 
category may not be indicative of its social 
relevance: very specialized domains may include a 
high number of publications, which however 
circulate in a restricted circle, with low social 
impact. For example, the number of texts classified 
in the History domain turns out to be more then ten 
times the number of texts catalogued under the 
Computer Science domain. However, if one looks 
at the number of HTML pages available on the 
Internet, or the number of magazines sold in a 
newspaper stand, or the number of terms used in 
everyday life, one cannot maintain that History is 
ten times more relevant than Computer Science. 
Another approach for evaluating the granularity 
of domains could be to take into account the 
number of DDC sub-codes corresponding to each 
Basic Domain. Unfortunately, also this approach 
gives results which are far from being satisfactory. 
The fact that a discipline has many subdivisions 
seems not to be clearly correlated with its 
relevance. For instance in the DDC manual 
(version 21) 105 pages can be put in 
correspondence with the ENGINEERING domain, 
whereas only 26 correspond to SPORT. It should 
also be said that there is no correlation between the 
number of publications and the number of sub-
categories in the DDC. For instance, 
ARCHITECTURE has a great number of publications 
classified under it, but on the contrary, the number 
of sub-categories in the DDC is very limited. 
The third criterion to evaluate the granularity of 
domains is their social relevance, which seems not 
to be captured adequately by the previous two 
criteria. Of course, social relevance is very difficult 
to evaluate. We tentatively took into consideration 
the organization of Internet hierarchies such as the 
Google and Yahoo directories, which seem to be 
closer than the DDC to represent the current social 
relevance of certain domains. See for instance the 
huge number of HTML pages classified in Google 
under the topic Television Programs. Of course 
Internet is only a partial view of the organization 
of human knowledge, so we cannot simply rely on 
the Internet to evaluate the granularity of the 
domains. 
None of the approaches analyzed so far seems to 
fit our needs. Thus we took into consideration a 
fourth criterion, which is based on the DDC as 
well. Instead of counting the number of 
subdivisions under a certain DDC code, we 
measured the depth of the code from the top of the 
hierarchy. For instance we can say that 700:Art has 
depth 1, 780:Music has depth 2, 782:Vocal Music 
has depth 3, and so on. We make the assumption 
that two DDC codes with the same depth have the 
same granularity. For instance we assume that 
782:Vocal Music and 382:Foreign Trade have the 
same granularity (both have depth 3).  
In order to evaluate the granularity of the Basic 
Domains against the DDC, we can compare WDH 
labels and DDC codes with the same depth. Given 
that the Basic Domains have depth 2, we should 
compare them to the so called Hundred Divisions 
(000, 010, 020, 030, ?, 100, 110, 120, etc.). 
Summing up, we will say that the Basic Domains 
are balanced if they can all be mapped onto the 
Hundred Divisions. Also, in the comparison we 
should take into account that the Basic Domains 
are 45, whereas the Hundred Divisions are 100. So, 
we expect that in the average, one Basic Domain 
maps onto two Hundred Divisions with a small 
degree of variance with respect to the average.  
What we have obtained from the analysis of the 
new WDH is the following: out of 45 Basic 
Domains 
 
o 4 domains map onto a Main Class (depth 1) 
o 18 domains are mapped at the Hundred 
Divisions level (depth 2) 
o 6 domains are mapped at different DDC levels, 
with the majority of DDC codes at depth 2 
o 17 domains map onto subdivisions of depth 3 
and 4. 
 
As for the average number of DDC codes 
covered by each Basic Domain, the variance is 
quite high. Certain Basic Domains cover a big 
number of codes from the Hundred Divisions. For 
instance HISTORY, and ART cover 6 codes each. 
Instead, in  most cases, one Basic Domain covers 
only one DDC code (e.g. LAW and 340:Law). 
The evaluation of the granularity of the Basic 
Domains according to the proposed criterion can 
be considered satisfactory even if the results 
diverge somewhat from what expected in principle.  
To explain this partial divergence in the 
granularity of domains, one should take into 
consideration that the DDC has been created 
relying heavily on the academic organization of 
knowledge disciplines. On the other side, in the 
practical WDH reorganization process we tried to 
balance somehow this discipline-oriented 
approach, by taking into account also the social 
relevance of domains. This has been done by 
relying on the organization of Internet directories 
and on our personal intuitions. 
Such an approach led us to put at the Basic level 
WDH labels corresponding to DDC codes with 
depth higher than 2 (more specific than the 
Hundreds Divisions). See for instance the 
positioning of RADIO+TV, FOOD, HEALTH, and 
ENVIRONMENT at the Basic level, even if they 
correspond to DDC codes of level 3 and 4.  
Instead, ANIMALS and PLANTS were not Basic in 
the previous version of WDH, but have been 
promoted to the Basic level in accordance with the 
granularity level they have in the DDC.  
Other domain labels have been placed at a lower 
level then expected with reference to the DDC. For 
instance PHILOSOPHY, ART, RELIGION, and 
LITERATURE have been put at the Basic Level, 
even if they correspond to DDC codes belonging to 
the Main Classes (depth 1). On the other side 
ASTROLOGY, ARCHAEOLOGY,  BODY CARE, and  
VETERINARY which were Basic in the previous 
version of the WDH, have been demoted at a lower 
level in accordance with the granularity they have 
in the DDC. Only in one case this process of 
demotion has led to the elimination of a sub-
domain, that is TEXTILE.  
4 Conclusions 
In this paper we described the revision of the 
WORDNET DOMAINS Hierarchy (WDH), with the 
aim of providing it with a clear semantics, and 
evaluating the coverage and balancing of a subset 
of the WDH, called Basic Domains. This has been 
done mostly by relying on the information 
available in the Dewy Decimal Classification 
(DDC). A semantics has been provided to the 
WDH labels by defining one or more pointers to 
DDC codes. The coverage of the Basic Domains 
has been evaluated by checking that each DDC 
code is covered by at least one Basic Domain. 
Finally, balancing has been evaluated mostly by 
comparing the granularity of the Basic Domains 
with the granularity of a subset of the DDC called 
the Hundred Divisions. Balancing is the aspect of 
the Basic Domains which diverges more clearly 
from the DDC. This is explained by the fact that 
we took in higher consideration the social 
relevance of domains. 
We think that the new version of the WDH is 
better suited to act as a useful language-
independent resource in the fields of computational 
lexicography, corpus building, and various NLP 
applications.  
5 Acknowledgements 
Thanks to Alfio Gliozzo for his useful comments 
and suggestions about how to improve the 
WORDNET DOMAINS Hierarchy. 
References  
BALKANET http://www.ceid.upatras.gr/Balkanet/ 
L. Bentivogli, C. Girardi and E. Pianta. 2003. The 
MEANING Italian Corpus. In Proceedings of the 
Corpus Linguistics 2003 Conference. Lancaster, 
United Kingdom. 
C. Fellbaum. 1998. WordNet. An Electronic 
Lexical Database. The MIT Press, Boston. 
B. Magnini and G. Cavagli?. 2000. Integrating 
Subject Field Codes into WordNet. In 
Proceedings of LREC-2000. Athens, Greece. 
B. Magnini, C. Strapparava, G. Pezzulo and A. 
Gliozzo. 2002. The Role of Domain Information 
in Word Sense Disambiguation. Journal of 
Natural Language Engineering (Special Issue on 
evaluating Word Sense Disambiguation 
Systems), 9(1):359:373. 
J.S. Mitchell, J. Beall, W.E. Matthews and G.R. 
New (eds). 1996. Dewey Decimal Classification  
Edition 21 (DDC 21). Forest Press, Albany, New 
York. 
E. Pianta, L. Bentivogli and C. Girardi. 2002. 
MultiWordNet: developing an aligned 
multilingual database. In Proceedings of the 
First Global WordNet Conference. Mysore, 
India. 
G. Rigau, B. Magnini, E. Agirre, P. Vossen and J. 
Carrol. 2002. MEANING: a Roadmap to 
Knowledge Technologies. In Proceedings of the 
COLING-2002 workshop "A Roadmap for 
Computational Linguistics". Taipei, Taiwan. 
H. Schutze. 1998. Automatic Word Sense 
Discrimination. Computational Linguistics, 
24(1):97-123. 
SIMPLE. 2000. Linguistic Specifications. 
Deliverable D2.1, March 2000.  
P. Vossen (ed). 1998. Computers and the 
Humanities (Special Issue on EuroWordNet), 
32(2-3). 
D.E. Walker and R.A. Amsler. 1986. Analyzing 
Language in Restricted Domain. Sublanguage 
description and Processing. Lawrence Earlbaum, 
Hillsdale NJ.  
Appendix : The first two levels of the WDH new version with the corresponding DDC codes 
 
TOP-LEVEL BASIC DOMAINS DDC 
Humanities   
 History [920:990] 
 Linguistics 410 
 Literature [800, 400] 
 Philosophy [100-(130, 150, 176)] 
 Psychology 150 
 Art [700-(710, 720, 745.5, 790-(791.43, 792, 793.3))] 
 Paranormal 130 
 Religion 200 
   
Free_Time  [790-(791.43, 792, 793.3)] 
 Radio-Tv [791.44, 791.45] 
 Play [793.4:795-794.6] 
 Sport [794.6, 796:799] 
   
Applied_Science  600 
 Agriculture [338.1, 630] 
 Food [613.2, 613.3, 641, 642] 
 Home [640-(641, 642, 645)] 
 Architecture [645, 690, 710, 720] 
 Computer_Science [004:006] 
 Engineering 620 
 Telecommunication [383, 384] 
 Medicine [610-(611, 612, 613)] 
   
Pure_Science  500 
 Astronomy  520 
 Biology [570-577, 611, 612-612.6] 
 Animals  590 
 Plants 580 
 Environment  577 
 Chemistry  540 
 Earth  [550, 560, 910-(910.4, 910.202)] 
 Mathematics 510 
 Physics  530 
   
Social_Science  [300.1:300.9] 
 Anthropology [301:307, 395, 398] 
 Health [613-(613.2, 613.3, 613.8, 613.9)] 
 Military [355:359] 
 Pedagogy 370 
 Publishing 070 
 Sociology [301:319-(305.8, 306.7), 360-(363.4, 368)] 
 Artisanship [338.642, 745.5] 
 Commerce [381, 382] 
 Industry [338-(338.1, 338.642), 660, 670, 680] 
 Transport [385:389] 
 Economy [330-(334, 338), 368, 650] 
 Administration [351:354] 
 Law 340 
 Politics 320 
 Tourism [910.202, 910.4] 
 Fashion [390-(392.6, 395, 398), 687] 
 Sexuality [155.3, 176, 306.7, 363.4, 392.6, 612.6, 613.96] 
   
 Factotum  
 
 
Unsupervised Domain Relevance Estimation
for Word Sense Disambiguation
Alfio Gliozzo and Bernardo Magnini and Carlo Strapparava
ITC-irst, Istituto per la Ricerca Scientica e Tecnologica, I-38050 Trento, ITALY
{gliozzo, magnini, strappa}@itc.it
Abstract
This paper presents Domain Relevance Estima-
tion (DRE), a fully unsupervised text categorization
technique based on the statistical estimation of the
relevance of a text with respect to a certain cate-
gory. We use a pre-defined set of categories (we
call them domains) which have been previously as-
sociated to WORDNET word senses. Given a cer-
tain domain, DRE distinguishes between relevant
and non-relevant texts by means of a Gaussian Mix-
ture model that describes the frequency distribution
of domain words inside a large-scale corpus. Then,
an Expectation Maximization algorithm computes
the parameters that maximize the likelihood of the
model on the empirical data.
The correct identification of the domain of the
text is a crucial point for Domain Driven Dis-
ambiguation, an unsupervised Word Sense Disam-
biguation (WSD) methodology that makes use of
only domain information. Therefore, DRE has been
exploited and evaluated in the context of a WSD
task. Results are comparable to those of state-of-
the-art unsupervised WSD systems and show that
DRE provides an important contribution.
1 Introduction
A fundamental issue in text processing and under-
standing is the ability to detect the topic (i.e. the do-
main) of a text or of a portion of it. Indeed, domain
detection allows a number of useful simplifications
in text processing applications, such as, for instance,
in Word Sense Disambiguation (WSD).
In this paper we introduce Domain Relevance Es-
timation (DRE) a fully unsupervised technique for
domain detection. Roughly speaking, DRE can be
viewed as a text categorization (TC) problem (Se-
bastiani, 2002), even if we do not approach the
problem in the standard supervised setting requir-
ing category labeled training data. In fact, recently,
unsupervised approaches to TC have received more
and more attention in the literature (see for example
(Ko and Seo, 2000).
We assume a pre-defined set of categories, each
defined by means of a list of related terms. We
call such categories domains and we consider them
as a set of general topics (e.g. SPORT, MEDICINE,
POLITICS) that cover the main disciplines and ar-
eas of human activity. For each domain, the list
of related words is extracted from WORDNET DO-
MAINS (Magnini and Cavaglia`, 2000), an extension
of WORDNET in which synsets are annotated with
domain labels. We have identified about 40 domains
(out of 200 present in WORDNET DOMAINS) and
we will use them for experiments throughout the pa-
per (see Table 1).
DRE focuses on the problem of estimating a de-
gree of relatedness of a certain text with respect to
the domains in WORDNET DOMAINS.
The basic idea underlying DRE is to combine the
knowledge in WORDNET DOMAINS and a proba-
bilistic framework which makes use of a large-scale
corpus to induce domain frequency distributions.
Specifically, given a certain domain, DRE considers
frequency scores for both relevant and non-relevant
texts (i.e. texts which introduce noise) and represent
them by means of a Gaussian Mixture model. Then,
an Expectation Maximization algorithm computes
the parameters that maximize the likelihood of the
empirical data.
DRE methodology originated from the effort to
improve the performance of Domain Driven Dis-
ambiguation (DDD) system (Magnini et al, 2002).
DDD is an unsupervised WSD methodology that
makes use of only domain information. DDD as-
signes the right sense of a word in its context com-
paring the domain of the context to the domain of
each sense of the word. This methodology exploits
WORDNET DOMAINS information to estimate both
Domain #Syn Domain #Syn Domain #Syn
Factotum 36820 Biology 21281 Earth 4637
Psychology 3405 Architecture 3394 Medicine 3271
Economy 3039 Alimentation 2998 Administration 2975
Chemistry 2472 Transport 2443 Art 2365
Physics 2225 Sport 2105 Religion 2055
Linguistics 1771 Military 1491 Law 1340
History 1264 Industry 1103 Politics 1033
Play 1009 Anthropology 963 Fashion 937
Mathematics 861 Literature 822 Engineering 746
Sociology 679 Commerce 637 Pedagogy 612
Publishing 532 Tourism 511 Computer Science 509
Telecommunication 493 Astronomy 477 Philosophy 381
Agriculture 334 Sexuality 272 Body Care 185
Artisanship 149 Archaeology 141 Veterinary 92
Astrology 90
Table 1: Domain distribution over WORDNET synsets.
the domain of the textual context and the domain of
the senses of the word to disambiguate. The former
operation is intrinsically an unsupervised TC task,
and the category set used has to be the same used
for representing the domain of word senses.
Since DRE makes use of a fixed set of target cat-
egories (i.e. domains) and since a document col-
lection annotated with such categories is not avail-
able, evaluating the performance of the approach is
a problem in itself. We have decided to perform an
indirect evaluation using the DDD system, where
unsupervised TC plays a crucial role.
The paper is structured as follows. Section 2
introduces WORDNET DOMAINS, the lexical re-
source that provides the underlying knowledge to
the DRE technique. In Section 3 the problem of es-
timating domain relevance for a text is introduced.
In particular, Section 4 briefly sketchs the WSD sys-
tem used for evaluation. Finally, Section 5 describes
a number of evaluation experiments we have carried
out.
2 Domains, WORDNET and Texts
DRE heavily relies on domain information as its
main knowledge source. Domains show interesting
properties both from a lexical and a textual point of
view. Among these properties there are: (i) lexi-
cal coherence, since part of the lexicon of a text is
composed of words belonging to the same domain;
(ii) polysemy reduction, because the potential am-
biguity of terms is sensibly lower if the domain of
the text is specified; and (iii) lexical identifiability
of text?s domain, because it is always possible to as-
sign one or more domains to a given text by consid-
ering term distributions in a bag-of-words approach.
Experimental evidences of these properties are re-
ported in (Magnini et al, 2002).
In this section we describe WORDNET DO-
MAINS1 (Magnini and Cavaglia`, 2000), a lexical re-
source that attempts a systematization of relevant
aspects in domain organization and representation.
WORDNET DOMAINS is an extension of WORD-
NET (version 1.6) (Fellbaum, 1998), in which each
synset is annotated with one or more domain la-
bels, selected from a hierarchically organized set of
about two hundred labels. In particular, issues con-
cerning the ?completeness? of the domain set, the
?balancing? among domains and the ?granularity?
of domain distinctions, have been addressed. The
domain set used in WORDNET DOMAINS has been
extracted from the Dewey Decimal Classification
(Comaroni et al, 1989), and a mapping between the
two taxonomies has been computed in order to en-
sure completeness. Table 2 shows how the senses
for a word (i.e. the noun bank) have been associated
to domain label; the last column reports the number
of occurrences of each sense in Semcor2.
Domain labeling is complementary to informa-
tion already present in WORDNET. First of all,
a domain may include synsets of different syn-
tactic categories: for instance MEDICINE groups
together senses from nouns, such as doctor#1
and hospital#1, and from verbs, such as
operate#7. Second, a domain may include
senses from different WORDNET sub-hierarchies
(i.e. deriving from different ?unique beginners? or
from different ?lexicographer files?). For example,
SPORT contains senses such as athlete#1, deriv-
ing from life form#1, game equipment#1
from physical object#1, sport#1
1WORDNET DOMAINS is freely available at
http://wndomains.itc.it
2SemCor is a portion of the Brown corpus in which words
are annotated with WORDNET senses.
Sense Synset and Gloss Domains Semcor frequencies
#1 depository financial institution, bank, banking con-
cern, banking company (a financial institution. . . )
ECONOMY 20
#2 bank (sloping land. . . ) GEOGRAPHY, GEOLOGY 14
#3 bank (a supply or stock held in reserve. . . ) ECONOMY -
#4 bank, bank building (a building. . . ) ARCHITECTURE, ECONOMY -
#5 bank (an arrangement of similar objects...) FACTOTUM 1
#6 savings bank, coin bank, money box, bank (a con-
tainer. . . )
ECONOMY -
#7 bank (a long ridge or pile. . . ) GEOGRAPHY, GEOLOGY 2
#8 bank (the funds held by a gambling house. . . ) ECONOMY, PLAY
#9 bank, cant, camber (a slope in the turn of a road. . . ) ARCHITECTURE -
#10 bank (a flight maneuver. . . ) TRANSPORT -
Table 2: WORDNET senses and domains for the word ?bank?.
from act#2, and playing field#1 from
location#1.
Domains may group senses of the same word
into thematic clusters, which has the important side-
effect of reducing the level of ambiguity when we
are disambiguating to a domain. Table 2 shows
an example. The word ?bank? has ten differ-
ent senses in WORDNET 1.6: three of them (i.e.
bank#1, bank#3 and bank#6) can be grouped
under the ECONOMY domain, while bank#2 and
bank#7 both belong to GEOGRAPHY and GEOL-
OGY. Grouping related senses is an emerging topic
in WSD (see, for instance (Palmer et al, 2001)).
Finally, there are WORDNET synsets that do not
belong to a specific domain, but rather appear in
texts associated with any domain. For this reason,
a FACTOTUM label has been created that basically
includes generic synsets, which appear frequently
in different contexts. Thus the FACTOTUM domain
can be thought of as a ?placeholder? for all other
domains.
3 Domain Relevance Estimation for Texts
The basic idea of domain relevance estimation for
texts is to exploit lexical coherence inside texts.
From the domain point of view lexical coherence
is equivalent to domain coherence, i.e. the fact that
a great part of the lexicon inside a text belongs to
the same domain.
From this observation follows that a simple
heuristic to approach this problem is counting the
occurrences of domain words for every domain in-
side the text: the higher the percentage of domain
words for a certain domain, the more relevant the
domain will be for the text. In order to perform this
operation the WORDNET DOMAINS information is
exploited, and each word is assigned a weighted list
of domains considering the domain annotation of
its synsets. In addition, we would like to estimate
the domain of the text locally. Local estimation
of domain relevance is very important in order to
take into account domain shifts inside the text. The
methodology used to estimate domain frequency is
described in subsection 3.1.
Unfortunately the simple local frequency count
is not a good domain relevance measure for sev-
eral reasons. The most significant one is that very
frequent words have, in general, many senses be-
longing to different domains. When words are used
in texts, ambiguity tends to disappear, but it is not
possible to assume knowing their actual sense (i.e.
the sense in which they are used in the context) in
advance, especially in a WSD framework. The sim-
ple frequency count is then inadequate for relevance
estimation: irrelevant senses of ambiguous words
contribute to augment the final score of irrelevant
domains, introducing noise. The level of noise is
different for different domains because of their dif-
ferent sizes and possible differences in the ambigu-
ity level of their vocabularies.
In subsection 3.2 we propose a solution for that
problem, namely the Gaussian Mixture (GM) ap-
proach. This constitutes an unsupervised way to es-
timate how to differentiate relevant domain infor-
mation in texts from noise, because it requires only
a large-scale corpus to estimate parameters in an
Expectation Maximization (EM) framework. Using
the estimated parameters it is possible to describe
the distributions of both relevant and non-relevant
texts, converting the DRE problem into the problem
of estimating the probability of each domain given
its frequency score in the text, in analogy to the
bayesian classification framework. Details about
the EM algorithm for GM model are provided in
subsection 3.3.
3.1 Domain Frequency Score
Let t ? T , be a text in a corpus T composed by a list
of words wt1, . . . , wtq . Let D = {D1, D2, ..., Dd} be
the set of domains used. For each domain Dk the
domain ?frequency? score is computed in a window
of c words around wtj . The domain frequency scoreis defined by formula (1).
F (Dk, t, j) =
j+c
X
i=j?c
Rword(Dk, wti)G(i, j, (
c
2)
2
) (1)
where the weight factor G(x, ?, ?2) is the density
of the normal distribution with mean ? and standard
deviation ? at point x and Rword(D,w) is a function
that return the relevance of a domain D for a word
w (see formula 3). In the rest of the paper we use the
notation F (Dk, t) to refer to F (Dk, t,m), where m
is the integer part of q/2 (i.e. the ?central? point of
the text - q is the text length).
Here below we see that the information contained
in WORDNET DOMAINS can be used to estimate
Rword(Dk, w), i.e. domain relevance for the word
w, which is derived from the domain relevance of
the synsets in which w appears.
As far as synsets are concerned, domain informa-
tion is represented by the function Dom : S ?
P (D)3 that returns, for each synset s ? S, where
S is the set of synsets in WORDNET DOMAINS, the
set of the domains associated to it. Formula (2) de-
fines the domain relevance estimation function (re-
member that d is the cardinality of D):
Rsyn(D, s) =
8
<
:
1/|Dom(s)| : if D ? Dom(s)
1/d : if Dom(s) = {FACTOTUM}
0 : otherwise
(2)
Intuitively, Rsyn(D, s) can be perceived as an es-
timated prior for the probability of the domain given
the concept, as expressed by the WORDNET DO-
MAINS annotation. Under these settings FACTO-
TUM (generic) concepts have uniform and low rel-
evance values for each domain while domain con-
cepts have high relevance values for a particular do-
main.
The definition of domain relevance for a word is
derived directly from the one given for concepts. In-
tuitively a domain D is relevant for a word w if D
is relevant for one or more senses c of w. More
formally let V = {w1, w2, ...w|V |} be the vocab-
ulary, let senses(w) = {s|s ? S, s is a sense of
w} (e.g. any synset in WORDNET containing the
word w). The domain relevance function for a word
R : D ? V ? [0, 1] is defined as follows:
Rword(Di, w) =
1
|senses(w)|
X
s?senses(w)
Rsyn(Di, s) (3)
3P (D) denotes the power set of D
3.2 The Gaussian Mixture Algorithm
As explained at the beginning of this section, the
simple local frequency count expressed by formula
(1) is not a good domain relevance measure.
In order to discriminate between noise and rel-
evant information, a supervised framework is typ-
ically used and significance levels for frequency
counts are estimated from labeled training data. Un-
fortunately this is not our case, since no domain
labeled text corpora are available. In this section
we propose a solution for that problem, namely the
Gaussian Mixture approach, that constitutes an un-
supervised way to estimate how to differentiate rel-
evant domain information in texts from noise. The
Gaussian Mixture approach consists of a parameter
estimation technique based on statistics of word dis-
tribution in a large-scale corpus.
The underlying assumption of the Gaussian Mix-
ture approach is that frequency scores for a cer-
tain domain are obtained from an underlying mix-
ture of relevant and non-relevant texts, and that the
scores for relevant texts are significantly higher than
scores obtained for the non-relevant ones. In the
corpus these scores are distributed according to two
distinct components. The domain frequency distri-
bution which corresponds to relevant texts has the
higher value expectation, while the one pertaining to
non relevant texts has the lower expectation. Figure
1 describes the probability density function (PDF )
for domain frequency scores of the SPORT domain
estimated on the BNC corpus4 (BNC-Consortium,
2000) using formula (1). The ?empirical? PDF ,
describing the distribution of frequency scores eval-
uated on the corpus, is represented by the continu-
ous line.
From the graph it is possible to see that the empir-
ical PDF can be decomposed into the sum of two
distributions, D = SPORT and D = ?non-SPORT?.
Most of the probability is concentrated on the left,
describing the distribution for the majority of non
relevant texts; the smaller distribution on the right
is assumed to be the distribution of frequency scores
for the minority of relevant texts.
Thus, the distribution on the left describes the
noise present in frequency estimation counts, which
is produced by the impact of polysemous words
and of occasional occurrences of terms belonging
to SPORT in non-relevant texts. The goal of the
technique is to estimate parameters describing the
distribution of the noise along texts, in order to as-
4The British National Corpus is a very large (over 100 mil-
lion words) corpus of modern English, both spoken and written.
050
100
150
200
0 0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04
Density
Non-relevant
Relevant
F(D, t)
de
ns
ity
 fu
nc
tio
n
Figure 1: Gaussian mixture for D = SPORT
sociate high relevance values only to relevant fre-
quency scores (i.e. frequency scores that are not re-
lated to noise). It is reasonable to assume that such
noise is normally distributed because it can be de-
scribed by a binomial distribution in which the prob-
ability of the positive event is very low and the num-
ber of events is very high. On the other hand, the
distribution on the right is the one describing typical
frequency values for relevant texts. This distribution
is also assumed to be normal.
A probabilistic interpretation permits the evalu-
ation of the relevance value R(D, t, j) of a certain
domain D for a new text t in a position j only by
considering the domain frequency F (D, t, j). The
relevance value is defined as the conditional prob-
ability P (D|F (D, t, j)). Using Bayes theorem we
estimate this probability by equation (4).
R(D, t, j) = P (D|F (D, t, j)) = (4)
= P (F (D, t, j)|D)P (D)
P (F (D, t, j)|D)P (D) + P (F (D, t, j)|D)P (D)
where P (F (D, t, j)|D) is the value of the PDF
describing D calculated in the point F (D, t, j),
P (F (D, t, j)|D) is the value of the PDF describ-
ing D, P (D) is the area of the distribution describ-
ing D and P (D) is the area of the distribution for
D.
In order to estimate the parameters describing the
PDF of D and D the Expectation Maximization
(EM) algorithm for the Gaussian Mixture Model
(Redner and Walker, 1984) is exploited. Assuming
to model the empirical distribution of domain fre-
quencies using a Gaussian mixture of two compo-
nents, the estimated parameters can be used to eval-
uate domain relevance by equation (4).
3.3 The EM Algorithm for the GM model
In this section some details about the algorithm for
parameter estimation are reported.
It is well known that a Gaussian mixture (GM)
allows to represent every smooth PDF as a linear
combination of normal distributions of the type in
formula 5
p(x|?) =
m
?
j=1
ajG(x, ?j , ?j) (5)
with
aj ? 0 and
m
?
j=1
aj = 1 (6)
and
G(x, ?, ?) = 1?
2pi?
e?
(x??)2
2?2 (7)
and ? = ?a1, ?1, ?1, . . . , am, ?m, ?m? is a pa-
rameter list describing the gaussian mixture. The
number of components required by the Gaussian
Mixture algorithm for domain relevance estimation
is m = 2.
Each component j is univocally determined by its
weight aj , its mean ?j and its variance ?j . Weights
represent also the areas of each component, i.e. its
total probability.
The Gaussian Mixture algorithm for domain rele-
vance estimation exploits a Gaussian Mixture to ap-
proximate the empirical PDF of domain frequency
scores. The goal of the Gaussian Mixture algorithm
is to find the GM that maximize the likelihood on
the empirical data, where the likelihood function is
evaluated by formula (8).
L(T , D, ?) =
?
t?T
p(F (D, t)|?) (8)
More formally, the EM algorithm for GM models
explores the space of parameters in order to find the
set of parameters ? such that the maximum likeli-
hood criterion (see formula 9) is satisfied.
?D = argmax
??
L(T , D, ??) (9)
This condition ensures that the obtained model
fits the original data as much as possible. Estima-
tion of parameters is the only information required
in order to evaluate domain relevance for texts us-
ing the Gaussian Mixture algorithm. The Expecta-
tion Maximization Algorithm for Gaussian Mixture
Models (Redner and Walker, 1984) allows to effi-
ciently perform this operation.
The strategy followed by the EM algorithm is
to start from a random set of parameters ?0, that
has a certain initial likelihood value L0, and then
iteratively change them in order to augment like-
lihood at each step. To this aim the EM algo-
rithm exploits a growth transformation of the like-
lihood function ?(?) = ?? such that L(T , D, ?) 6
L(T , D, ??). Applying iteratively this transforma-
tion starting from ?0 a sequence of parameters is
produced, until the likelihood function achieve a
stable value (i.e. Li+1 ? Li 6 ). In our settings
the transformation function ? is defined by the fol-
lowing set of equations, in which all the parameters
have to be solved together.
?(?) = ?(?a1, ?1, ?1, a2, ?2, ?2?) (10)
= ?a?1, ??1, ??1, a?2, ??2, ??2?
a?j =
1
|T |
|T |
?
k=1
ajG(F (D, tk), ?j , ?j)
p(F (D, tk), ?)
(11)
??j =
?|T |
k=1 F (D, tk) ?
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?)
?|T |
k=1
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?)
(12)
??j =
?|T |
k=1 (F (D, tk) ? ??j)2 ?
aiG(F (D,tk),?i,?i)
p(F (D,tk),?)
?|T |
k=1
ajG(F (D,tk),?j ,?j)
p(F (D,tk),?) (13)
As said before, in order to estimate distribu-
tion parameters the British National Corpus (BNC-
Consortium, 2000) was used. Domain frequency
scores have been evaluated on the central position
of each text (using equation 1, with c = 50).
In conclusion, the EM algorithm was used to es-
timate parameters to describe distributions for rele-
vant and non-relevant texts. This learning method
is totally unsupervised. Estimated parameters has
been used to estimate relevance values by formula
(4).
4 Domain Driven Disambiguation
DRE originates to improve the performance of Do-
main Driven Disambiguation (DDD). In this sec-
tion, a brief overview of DDD is given. DDD is a
WSD methodology that only makes use of domain
information. Originally developed to test the role of
domain information for WSD, the system is capable
to achieve a good precision disambiguation. Its re-
sults are affected by a low recall, motivated by the
fact that domain information is sufficient to disam-
biguate only ?domain words?. The disambiguation
process is done comparing the domain of the con-
text and the domains of each sense of the lemma to
disambiguate. The selected sense is the one whose
domain is relevant for the context5 .
In order to represent domain information we in-
troduced the notion of Domain Vectors (DV), that
are data structures that collect domain information.
These vectors are defined in a multidimensional
space, in which each domain represents a dimen-
sion of the space. We distinguish between two kinds
of DVs: (i) synset vectors, which represent the rel-
evance of a synset with respect to each considered
domain and (ii) text vectors, which represent the rel-
evance of a portion of text with respect to each do-
main in the considered set.
More formally let D = {D1, D2, ..., Dd} be the
set of domains, the domain vector ~s for a synset s
is defined as ?R(D1, s), R(D2, s), . . . , R(Dd, s)?
where R(Di, s) is evaluated using equation
(2). In analogy the domain vector ~tj for
a text t in a given position j is defined as
?R(D1, t, j), R(D2, t, j), . . . , R(Dd, t, j)? where
R(Di, t, j) is evaluated using equation (4).
The DDD methodology is performed basically in
three steps:
1. Compute ~t for the context t of the word w to be disam-
biguated
2. Compute s? = argmaxs?Senses(w)score(s, w, t) where
score(s,w, t) = P (s|w) ? sim(~s,
~t)
P
s?Senses(w) P (s|w) ? sim(~s,~t)
3. if score(s?, w, t) > k (where k ? [0, 1] is a confidence
threshold) select sense s?, else do not provide any answer
The similarity metric used is the cosine vector
similarity, which takes into account only the direc-
tion of the vector (i.e. the information regarding the
domain).
P (s|w) describes the prior probability of sense
s for word w, and depends on the distribution of
the sense annotations in the corpus. It is esti-
mated by statistics from a sense tagged corpus (we
used SemCor)6 or considering the sense order in
5Recent works in WSD demonstrate that an automatic es-
timation of domain relevance for texts can be profitable used
to disambiguate words in their contexts. For example, (Escud-
ero et al, 2001) used domain relevance extraction techniques
to extract features for a supervised WSD algorithm presented
at the Senseval-2 competion, improving the system accuracy of
about 4 points for nouns, 1 point for verbs and 2 points for ad-
jectives, confirming the original intuition that domain informa-
tion is very useful to disambiguate ?domain words?, i.e. words
which are strongly related to the domain of the text.
6Admittedly, this may be regarded as a supervised compo-
nent of the generally unsupervised system. Yet, we considered
this component as legitimate within an unsupervised frame-
WORDNET, which roughly corresponds to sense
frequency order, when no example of the word
to disambiguate are contained in SemCor. In the
former case the estimation of P (s|w) is based on
smoothed statistics from the corpus (P (s|w) =
occ(s,w)+?
occ(w)+|senses(w)|?? , where ? is a smoothing fac-
tor empirically determined). In the latter case
P (s|w) can be estimated in an unsupervised way
considering the order of senses in WORDNET
(P (s|w) = 2(|senses(w)|?sensenumber(s,w)+1)|senses(w)|(|senses(w)|+1) where
sensenumber(s, w) returns the position of sense
s of word w in the sense list for w provided by
WORDNET.
5 Evaluation in a WSD task
We used the WSD framework to perform an evalu-
ation of the DRE technique by itself.
As explained in Section 1 Domain Relevance Es-
timation is not a common Text Categorization task.
In the standard framework of TC, categories are
learned form examples, that are used also for test.
In our case information in WORDNET DOMAINS is
used to discriminate, and a test set, i.e. a corpus of
texts categorized using the domain of WORDNET
DOMAINS, is not available. To evaluate the accu-
racy of the domain relevance estimation technique
described above is thus necessary to perform an in-
direct evaluation.
We evaluated the DDD algorithm described in
Section 4 using the dataset of the Senseval-2 all-
words task (Senseval-2, 2001; Preiss and Yarowsky,
2002). In order to estimate domain vectors for the
contexts of the words to disambiguate we used the
DRE methodology described in Section 3. Varying
the confidence threshold k, as described in Section
4, it is possible to change the tradeoff between preci-
sion and recall. The obtained precision-recall curve
of the system is reported in Figure 2.
In addition we evaluated separately the perfor-
mance on nouns and verbs, suspecting that nouns
are more ?domain oriented? than verbs. The effec-
tiveness of DDD to disambiguate domain words is
confirmed by results reported in Figure 3, in which
the precision recall curve is reported separately for
both nouns and verbs. The performances obtained
for nouns are sensibly higher than the one obtained
for verbs, confirming the claim that domain infor-
mation is crucial to disambiguate domain words.
In Figure 2 we also compare the results ob-
tained by the DDD system that make use of the
DRE technique described in Section 3 with the re-
work since it relies on a general resource (SemCor) that does
not correspond to the test data (Senseval all-words task).
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6
Pr
ec
isi
on
Recall
DDD new
DDD old
Figure 2: Performances of the system for all POS
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Pr
ec
isi
on
Recall
Nouns
Verbs
Figure 3: Performances of the system for Nouns and
Verbs
sults obtained by the DDD system presented at the
Senseval-2 competition described in (Magnini et al,
2002), that is based on the same DDD methodol-
ogy and exploit a DRE technique that consists ba-
sically on the simply domain frequency scores de-
scribed in subsection 3.1 (we refer to this system
using the expression old-DDD, in contrast to the ex-
pression new-DDD that refers to the implementation
described in this paper).
Old-DDD obtained 75% precision and 35% re-
call on the official evaluation at the Senseval-2 En-
glish all words task. At 35% of recall the new-DDD
achieves a precision of 79%, improving precision
by 4 points with respect to old-DDD. At 75% pre-
cision the recall of new-DDD is 40%. In both cases
the new domain relevance estimation technique im-
proves the performance of the DDD methodology,
demonstrating the usefulness of the DRE technique
proposed in this paper.
6 Conclusions and Future Works
Domain Relevance Estimation, an unsupervised TC
technique, has been proposed and evaluated in-
side the Domain Driven Disambiguation frame-
work, showing a significant improvement on the
overall system performances. This technique also
allows a clear probabilistic interpretation providing
an operative definition of the concept of domain rel-
evance. During the learning phase annotated re-
sources are not required, allowing a low cost imple-
mentation. The portability of the technique to other
languages is allowed by the usage of synset-aligned
wordnets, being domain annotation language inde-
pendent.
As far as the evaluation of DRE is concerned, for
the moment we have tested its usefulness in the con-
text of a WSD task, but we are going deeper, con-
sidering a pure TC framework.
Acknowledgements
We would like to thank Ido Dagan and Marcello
Federico for many useful discussions and sugges-
tions.
References
BNC-Consortium. 2000. British national corpus,
http://www.hcu.ox.ac.uk/BNC/.
J. P. Comaroni, J. Beall, W. E. Matthews, and G. R.
New, editors. 1989. Dewey Decimal Classica-
tion and Relative Index. Forest Press, Albany,
New York, 20th edition.
G. Escudero, L. Ma`rquez, and G. Rigau. 2001.
Using lazy boosting for word sense disambigua-
tion. In Proc. of SENSEVAL-2 Second Inter-
national Workshop on Evaluating Word Sense
Disambiguation System, pages 71?74, Toulose,
France, July.
C. Fellbaum. 1998. WordNet. An Electronic Lexical
Database. The MIT Press.
Y. Ko and J. Seo. 2000. Automatic text categoriza-
tion by unsupervised learning. In Proceedings of
COLING-00, the 18th International Conference
on Computational Linguistics, Saarbru?cken, Ger-
many.
B. Magnini and G. Cavaglia`. 2000. Integrating sub-
ject field codes into WordNet. In Proceedings
of LREC-2000, Second International Conference
on Language Resources and Evaluation, Athens,
Greece, June.
B. Magnini, C. Strapparava, G. Pezzulo, and
A. Gliozzo. 2002. The role of domain informa-
tion in word sense disambiguation. Natural Lan-
guage Engineering, 8(4):359?373.
M. Palmer, C. Fellbaum, S. Cotton, L. Delfs, and
H.T. Dang. 2001. English tasks: All-words
and verb lexical sample. In Proceedings of
SENSEVAL-2, Second International Workshop on
Evaluating Word Sense Disambiguation Systems,
Toulouse, France, July.
J. Preiss and D. Yarowsky, editors. 2002. Pro-
ceedings of SENSEVAL-2: Second International
Workshop on Evaluating Word Sense Disam-
biguation Systems, Toulouse, France.
R. Redner and H. Walker. 1984. Mixture densi-
ties, maximum likelihood and the EM algorithm.
SIAM Review, 26(2):195?239, April.
F. Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Computing Sur-
veys, 34(1):1?47.
Senseval-2. 2001. http://www.senseval.org.
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 26?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ontology Population from Textual Mentions:
Task Definition and Benchmark
Bernardo Magnini, Emanuele Pianta, Octavian Popescu and
Manuela Speranza
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
Via Sommarive 18, 38050 Povo (TN), Italy
{magnini, pianta, popescu, manspera}@itc.it
Abstract
In this paper we propose and investigate
Ontology Population from Textual Mentions
(OPTM), a sub-task of Ontology Population
from text where we assume that mentions for
several kinds of entities (e.g. PERSON,
O R G A N I Z A T I O N , LO C A T I O N , GEO-
POLITICAL_ ENTITY) are already extracted
from a document collection. On the one
hand, OPTM simplifies the general Ontology
Population task, limiting the input textual
material; on the other hand, it introduces
challenging extensions to Ontology Popula-
tion restricted to named entities, being open
to a wider spectrum of linguistic phenomena.
We describe a manually created benchmark
for OPTM and discuss several factors which
determine the difficulty of the task.
1 Introduction
Mentions are portions of text which refer to enti-
ties
1
. As an example, given a particular textual
context, both the mentions ?George W. Bush?
and ?the U.S. President.? refer to the same entity,
i.e. a particular instance of Person whose first
name is ?George?, whose middle initial is ?W.?,
whose family name is ?Bush? and whose role is
?U.S. President?.
In this paper we propose and investigate Ontol-
ogy Population from Textual Mentions (OPTM),
a sub-task of Ontology Learning and Population
                                                 
1
 The terms ?mention? and ?entity? have been intro-
duced within the ACE Program (Linguistic Data Con-
sortium, 2004). ?Mentions? are equivalent to ?refer-
ring expressions? and ?entities? are equivalent to
?referents?, as widely used in computational linguis-
tics. In this paper, we use italics for ?mentions? and
small caps for ENTITY and ENTITY_ATTRIBUTE.
(OLP) from text where we assume that mentions
for several kinds of entities (e.g. PERSON,
ORGANIZATION, LO C A T I O N , GEO-POLITICAL
_ENTITY) are already extracted from a document
collection.
We assume an ontology with a set of classes
C={c
1
, ?, c
n
} with each class c
1
 being described
by a set of attribute value pairs [a
1
, v
1
]. Given a
set of mentions M={m
1,c1
, ?,  m
n,cn
}, where each
mention m
j
 is classified into a class c
i
 in C, the
OPTM task is defined in three steps: Recognition
and Classification of Entity Attributes, Normali-
zation, and Resolution of inter-text Entity Co-
reference.
(i) Recognition and Classification of Entity
Attributes (RCEA). The textual material
expressed in a mention is extracted and dis-
tributed along the attribute-value pairs al-
ready defined for the class c
i
 of the mention;
as an example, given the PERSON mention
?U.S. President Bush?, we expect that the
attribute LAST_NAME is filled with the value
?Bush? and the attribute ROLE is filled with
the value ?U.S. President?. Note that fillers,
at this step, are still portions of text.
(ii) Normalization. The textual material ex-
tracted at step (i) is assigned to concepts and
relations already defined in the ontology; for
example, the entity BUSH is created as an in-
stance of COUNTRY_PRESIDENT, and an in-
stance of the relation PRESIDENT_OF is cre-
ated between BUSH and U.S.A. At this step
different instances are created for co-
referring mentions.
(iii) Resolution of inter-text Entity Co-
reference (REC). Each mention m
j
 has to be
assigned to a single individual entity be-
longing to a class in C . For example, we
recognize that the instances created at step
(i) for ?U.S. President Bush? and ?George
W. Bush? actually refer to the same entity.
26
In this paper we address steps (i) and (iii),
while step (ii) is work in progress. The input of
the OPTM task consists of classified mentions
and the output consists of individual entities
filled with textual material (i.e. there is no nor-
malization) with their co-reference relations. The
focus is on the definition of the task and on an
empirical analysis of the aspects that determine
its complexity, rather than on approaches and
methods for the automatic solution of OPTM.
There are several advantages of OPTM which
make it appealing for OLP. First, mentions pro-
vide an obvious simplification with respect to the
more general Ontology Population from text (cf.
Buitelaar et al 2005); in particular, mentions are
well defined and there are systems for automatic
mention recognition. Although there is no univo-
cally accepted definition for the OP task, a useful
approximation has been suggested by
(Bontcheva and Cunningham, 2005) as Ontology
Driven Information Extraction with the goal of
extracting and classifying instances of concepts
and relations defined in a Ontology, in place of
filling a template. A similar task has been ap-
proached in a variety of perspectives, including
term clustering (Lin, 1998 and Almuhareb and
Poesio, 2004) and term categorization (Avancini
et al 2003). A rather different task is Ontology
Learning, where new concepts and relations are
supposed to be acquired, with the consequence of
changing the definition of the Ontology itself
(Velardi et al 2005). However, since mentions
have been introduced as an evolution of the tra-
ditional Named Entity Recognition task (see
Tanev and Magnini, 2006), they guarantee a rea-
sonable level of difficulty, which makes OPTM
challenging both for the Computational Linguis-
tic side and the Knowledge Representation
community. Second, there already exist anno-
tated data with mentions, delivered under the
ACE (Automatic Content Extraction) initiative
(Ferro et al 2005, Linguistic Data Consortium
2004), which makes the exploitation of machine
learning based approaches possible. Finally,
having a limited scope with respect to OLP, the
OPTM task allows for a better estimation of per-
formance; in particular, it is possible to evaluate
more easily the recall of the task, i.e. the propor-
tion of information correctly assigned to an en-
tity out of the total amount of information pro-
vided by a certain mention.
In the paper we both define the OPTM task
and describe an OPTM benchmark, i.e. a docu-
ment collection annotated with mentions as well
as an ontology where information from mentions
has been manually extracted. The general archi-
tecture of the OPTM task has been sketched
above, considering three sub tasks. The docu-
ment collection we use consists of about 500
Italian news items. Currently, mentions referring
to PE R S O N , ORGANIZATION and GEO-
POLITICAL_ ENTITY have been annotated and co-
references among such mentions have been es-
tablished. As for the RCEA sub task, we have
considered mentions referring to PERSON and
have built a knowledge base of instances, each
described with a number of attribute-value pairs.
The paper is structured as follows. Section 2
provides the useful background as far as men-
tions and entities are concerned. Section 3 de-
fines the OPTM task and introduces the dataset
we have used, as well as the annotation proce-
dures and guidelines we have defined for the re-
alization of the OPTM benchmark corpus. Sec-
tion 4 reports on a number of quantitative and
qualitative analyses of the OPTM benchmark
aimed at determining the difficulty of the task.
Finally, Section 5 proposes future extensions and
developments of our work.
2 Mentions and Entities
As indicated in the ACE Entity Detection
task, the annotation of entities (e.g. PERSON,
ORGANIZAT I O N , LOCAT I O N  a n d  GEO-
POLITICAL_ENTITY) requires that the entities
mentioned in a text be detected, their syntactic
head marked, their sense disambiguated, and that
selected attributes of these entities be extracted
and merged into a unified representation for each
entity.
As it often happens that the same entity is
mentioned more than once in the same text, two
inter-connected levels of annotation have been
defined: the level of the entity, which provides a
representation of an object in the world, and the
level of the entity mention, which provides in-
formation about the textual references to that
object.  For instance, if  the entity
GEORGE_W._BUSH (e.g. the individual in the
world who is the current president of the U.S.) is
mentioned in two different sentences of a text as
?the U.S. president? and as ?the president?, these
two expressions are considered as two co-
referring entity mentions.
The kinds of reference made by entities to
something in the world are described by the fol-
lowing four classes:
? specific referential entities are those where
the entity being referred to is a unique object
27
or set of objects (e.g. ?The president of
thecompany is here?)
 2
;
? generic referential entities refer to a kind or
type of entity and not to a particular object (or
set of objects) in the world (e.g. ?The presi-
dent is elected every 5 years?);
? under-specified referential entities are non-
generic non-specific references, including im-
precise quantifications (e.g. ?everyone?) and
estimates (e.g. ?more than 10.000 people?);
? negatively quantified entities refer to the
empty set of the mentioned type of object (e.g.
?No lawyer?).
The textual extent of mentions is defined as
the entire nominal phrase used to refer to an en-
tity, thus including modifiers (e.g. ?a big fam-
ily?), prepositional phrases (e.g. ?the President of
the Republic?) and dependent clauses (e.g. ?the
girl who is working in the garden?).
The classification of entity mentions is based
on syntactic features; among the most significant
categories defined by LDD (Linguistic Data
Consortium 2004) there are:
- NAM: proper names (e.g. ?Ciampi?, ?the
UN?);
- NOM: nominal constructions (e.g. ?good chil-
dren?, ?the company?);
- PRO: pronouns, e.g. personal (?you?) and in-
definite (?someone?);
- WHQ: wh-words, such as relatives and inter-
rogatives (e.g. ?Who?s there??);
- PTV: partitive constructions (e.g. ?some of
them?, ?one of the schools?);
- APP: appositive constructions (e.g. ?Dante,
famous poet? , ?Juventus, Italian football
club?).
Since the dataset presented in this paper has
been developed for Italian, some new types of
mentions have been added to those listed in the
LDC guidelines; for instance, we have created a
specific tag, ENCLIT, to annotate the clitics
whose extension can not be identified at word-
level (e.g. ?veder[lo]?/?to see him?). Some types
of mentions, on the other hand, have been elimi-
nated; this is the case for pre-modifiers, due to
syntactic differences between English, where
both adjectives and nouns can be used as pre-
modifiers, and Italian, which only admits adjec-
tives in that position.
In extending the annotation guidelines, we
have decided to annotate all conjunctions of en-
tities, not only those which share the same modi-
fiers as indicated in the ACE guidelines, and to
mark them using a specific new tag, CONJ (e.g.
                                                 
2
 Notice that the corpus is in Italian, but we present English
examples for the sake of readability.
?mother and child?)
3
.
According to the ACE standards, each dis-
tinct person or set of people mentioned in a
document refers to an entity of type PERSON. For
example, people may be specified by name
(?John Smith?), occupation (?the butcher?),
family relation (?dad?), pronoun (?he?), etc., or
by some combination of these.
PERSON (PE), the class we have considered
for the Ontology Population from Textual Men-
tion task, is further classified with the following
subtypes:
? INDIVIDUAL_PERSON: PES which refer to a
single person (e.g. ?George W. Bush?);
? GROUP_PERSON: PES which refer to more than
one person (e.g. ?my parents?, ?your family?,
etc.);
? INDEFINITE_PERSON: a PE is classified as in-
definite when it is not possible to judge from
the context whether it refers to one or more
persons (e.g. ?I wonder who came to see me?).
3 Task definition
In Section 3.1 we first describe the document
collection we have used for the creation of the
OPTM benchmark. Then, Section 3.2 provides
details about RCEA, the first step in OPTM.
3.1 Document collection
The OPTM benchmark is built on top of a
document collection (I-CAB, Italian Content
Annotated Bank)
4
 annotated with entity men-
tions. I-CAB (Magnini et al 2006) consists of
525 news documents taken from the local news-
paper ?L?Adige?
5
. The selected news stories be-
long to four different days (September, 7th and
8th 2004 and October, 7th and 8th 2004) and are
grouped into five categories: News Stories, Cul-
tural News, Economic News, Sports News and
Local News (see Table 1).
09/07 09/08 10/07 10/08 Total
News 23 25 18 21 87
Culture 20 18 16 18 72
Economy 13 15 12 14 54
Sport 29 41 27 26 123
Local 46 43 49 51 189
TOTAL 131 142 122 130 525
Table 1: Number of news stories per category.
                                                 
3
 Appositive and conjoined mentions are complex construc-
tions. Although LDC does not identify heads for complex
constructions, we have decided to annotate all the extent as
head.
4
 A demo is available at http://ontotext.itc.it/webicab
5
 http://www.ladige.it/
28
I-CAB is further divided into training and
test sections, which contain 335 and 190 docu-
ments respectively. In total, I-CAB consists of
around 182,500 words: 113,500 and 69,000
words in the training and the test sections re-
spectively (the average length of a news story is
around 339 words in the training section and 363
words in the test section).
The annotation of I-CAB is being carried out
manually, as we intend I-CAB to become a
benchmark for various automatic Information
Extraction tasks, including recognition and nor-
malization of temporal expressions, entities, and
relations between entities (e.g. the relation af-
filiation connecting a person to the organization
to which he or she is affiliated).
3.2 Recognition and Classification
As stated in Section 1, we assume that for
each type of entity there is a set of attribute-value
pairs, which typically are used for mentioning
that entity type. The same entity may have dif-
ferent values for the same attribute and, at this
point no normalization of the data is made, so
there is no way to differentiate between different
values of the same attribute, e.g. there is no
stipulation regarding the relationship between
?politician? and ?political leader?. Finally, we
currently assume a totally flat structure among
the possible values for the attributes.
The work we describe in this Section and in
the next one concerns a pilot study on entities of
type PERSON. After an empirical investigation on
the dataset described in Section 3.1 we have as-
sumed that the attributes listed in the first column
of Table 2 constitute a proper set for this type of
entity. The second column lists some possible
values for each attribute.
The textual extent of a value is defined as the
maximal extent containing pertinent information.
For instance, if we have a person mentioned as
?the thirty-year-old sport journalist?, we will
select ?sport journalist? as value for the attribute
ACTIVITY. In fact, the age of the journalist in not
pertinent to the activity attribute and is left out,
whereas ?sport? contributes to specifying the
activity performed.
As there are always less paradigmatic values
for a given attribute, we shortly present further
the guidelines in making a decision in those
cases. Generally, articles and prepositions are not
admitted at the beginning of the textual extent of
a value, an exception being made in the case of
articles in nicknames.
Attributes Possible values
FIRST_NAME Ralph, Greg
MIDDLE_NAME J., W.
LAST_NAME McCarthy, Newton
NICKNAME Spider, Enigmista
TITLE prof., Mr.
SEX actress
ACTIVITY
AFFILIATION
ROLE
journalist, doctor
The New York Times
director, president
PROVENIENCE South American
FAMILY_RELATION father, cousin
AGE_CATEGORY boy, girl
MISCELLANEA The men with red shoes
Table 2. Attributes for PERSON.
Typical examples for the TITLE attribute are
?Mister?, ?Miss?, ?Professor?, etc. We consider
as TITLE the words which are used to address
people with special status, but which do not refer
specifically to their activity. In Italian, profes-
sions are often used to address people (e.g. ?av-
vocato/lawyer?, ?ingegnere/engineer?). In order
to avoid a possible overlapping between the
TITLE attribute and the ACTIVITY attribute, pro-
fessions are considered values for title only if
they appear in abbreviated forms (?avv.?, ?ing.?
etc.) before a proper name.
With respect to the SEX attribute, we con-
sider as values all the portions of text carrying
this information. In most cases, first and middle
names are relevant. In addition, the values of the
SEX attribute can be gendered words (e.g. ?Mis-
ter? vs. ?Mrs.?, ?husband? vs. ?wife?) and words
from grammatical categories carrying informa-
tion about gender (e.g. adjectives).
The attributes A CTIVITY, RO L E , AF -
FILIATION are three strictly connected attributes.
ACTIVITY refers to the actual activity performed
by a person, while ROLE refers to the position
they occupy. So, for instance, ?politician? is a
possible value for ACTIVITY, while ?leader of the
Labour Party? refers to a ROLE. Each group of
these three attributes is associated with a mention
and all the information within a group has to be
derived from the same mention. If different
pieces of information derive from distinct men-
tions, we will have two separate groups. Con-
sider the following three mentions of the same
entity:
29
(1) ?the journalist of Radio Liberty?
(2) ?the redactor of breaking news?
(3) ?a spare time astronomer?
These three mentions lead to three different
groups of ACTIVITY, ROLE and AFFILIATION.
The obvious inference that the first two mentions
conceptually belong to the same group is not
drawn. This step is to be taken at a further stage.
The PROVENIENCE attribute can have as
values all phrases denoting geographical/racial
origin or provenience and religious affiliation.
The attribute AGE_CATEGORY can have either
numerical values, such as ?three years old?, or
words indicating age, such as ?middle-aged?, etc.
In the next section we will analyze the occur-
rences of the values of these attributes in a news
corpus.
4 Data analysis
The difficulty of the OPTM task is directly cor-
related to four factors: (i) the extent to which the
linguistic form of mentions varies; (ii) the per-
plexity of the values of the attributes; (iii) the
size of the set of the potential co-references and
(iv) the number of different mentions per entity.
In this section we present the work we have un-
dertaken so far and the results we have obtained
regarding the above four factors.
We started with a set of 175 documents be-
longing to the I-CAB corpus (see Section 3.1).
Each document has been manually annotated
observing the specifications described in Section
3.2. We focused on mentions referring to
INDIVIDUAL PERSON (Mentions in Table 3), ex-
cluding from the dataset both mentions referring
to different entity types (e.g. ORGANIZATION)
and PERSON GROUP. In addition, for the pur-
poses of this work we decided to filter out the
following mentions: (i) mentions consisting of a
single pronoun; (ii) nested mentions, (in particu-
lar in the case where a larger mention, e.g.
?President Ciampi?, contained a smaller one, e.g.
?Ciampi?, only the larger mention was consid-
ered). The total number of remaining mentions
(Meaningful mentions in Table 3) is 2343. Fi-
nally, we filtered out repetitions of mentions (i.e.
string equal) that co-refer inside the same docu-
ment, obtaining a set of 1139 distinct mentions.
The average number of mentions for an entity
in a document is 2.09, while the mentions/entity
proportion within the whole collection is 2.68.
The detailed distribution of mentions with re-
spect to document entities is presented in Table
4. Columns 1 and 3 list the number of mentions
and columns 2 and 4 list the number of entities
which are mentioned for the respective number
of times (from 1 to 9 and more than 10). For in-
stance, in the dataset there are 741 entities which,
within a single document, have just one mention,
while there are 27 entities which are mentioned
more than 10 times in the same document. As an
indication of variability, only 14% of document
entities have been mentioned in two different
ways.
Documents 175
Words 57 033
Words in mentions 8116
Mentions 3157
Meaningful mentions 2343
Distinct mentions 1139
Document entities 1117
Collection entities 873
Table 3. Documents, mentions and entities in the
OPTM dataset.
#M/E #occ #M/E #occ
1 741 6 15
2 164 7 11
3 64 8 12
4 47 9 5
5 31 ?10 27
Table 4. Distribution of mentions per entity.
4.1 Co-reference density
We can estimate the a priori probability that two
entities selected from different documents co-
refer. Actually, this is the estimate of the prob-
ability that two entities co-refer conditioned by
the fact that they have been correctly identified
inside the documents. We can compute such
probability as the complement of the ratio be-
tween the number of different entities and the
number of the document entities in the collec-
tion.
entitiesdocument
entitiescollection
corefcrossP
?
?
?=?
#
#
1)(
From Table 3 we read these values as 873
and 1117 respectively, therefore, for this corpus,
the probability of intra-document co-reference is
approximately 0.22.
30
A cumulative factor in estimating the diffi-
culty of the co-reference task is the ratio between
the number of different entities and the number
of mentions. We call this ratio the co-reference
density and it shows the a priori expectation that
a correct identified mention refers to a new en-
tity.
mentions
entitiescollection
densitycoref
#
# ?
=?
The co-reference density takes values in the
interval with limits [0-1]. The case where the co-
reference density tends to 0 means that all the
mentions refer to the same entity, while where
the value tends to 1 it means that each mention in
the collection refers to a different entity. Both
limits render the co-reference task superfluous.
The figure for co-reference density we found in
our corpus is 873/2343 ? 0.37, and it is far from
being close to one of the extremes.
A last measure we introduce is the ratio
between the number of different entities and the
number of distinct mentions. Let?s call it pseudo
co-reference density. In fact it shows the value of
co-reference density conditioned by the fact that
one knows in advance whether two mentions that
are identical also co-refer.
mentionsdistinct
entitiescollection
densitypcoref
?
?
=?
#
#
The pseudo co-reference for our corpus is
873/1139 ? 0.76. This information is not directly
expressed in the collection, so it should be ap-
proximated. The difference between co-reference
density and pseudo co-reference density (see Ta-
ble 5) shows the increase in recall, if one consid-
ers that two identical mentions refer to the same
entity with probability 1. On the other hand, the
loss in accuracy might be too large (consider for
example the case when two different people hap-
pen to have the same first name).
co-reference density 0.37
pseudo co-reference density 0.76
cross co-reference 0.22
Table 5. A priori estimation of difficulty of co-
reference
4.2 Attribute variability
The estimation of the variability of the values for
a certain attribute is given in Table 6. The first
column indicates the attribute under considera-
tion; the second column lists the total number of
mentions of the attribute found in the corpus; the
third column lists the number of different values
that the attribute actually takes and, between pa-
rentheses, its proportion over the total number of
values; the fourth column indicates the propor-
tion of the occurrences of the attribute with re-
spect to the total number of mentions (distinct
mentions are considered).
Table 6. Variability of values for attributes.
In Table 7 we show the distribution of the at-
tributes inside one mention. That is, we calculate
how many times one entity contains more than
one attribute. Columns 1 and 3 list the number of
attributes found in a mention, and columns 2 and
4 list the number of mentions that actually con-
tain that number of values for attributes.
#attributes #mentions #attributes #mentions
1 398 5 55
2 220 6 25
3 312 7 8
4 117 8 4
Table 7. Number of attributes inside a mention.
An example of a mention from our dataset that
includes values for eight attributes is the follow-
ing:
The correspondent of Al Jazira, Amr Abdel
Hamid, an Egyptian of Russian nationality?
We conclude this section with a statistic re-
garding the coverage of attributes (miscellanea
excluded). There are 7275 words used in 1139
Attributes total
occ.
distinct
occ. (%)
occ.
prob.
FIRST_NAME 535 303 (44%) 27,0%
MIDDLE_NAME 25 25 (100%) 2,1%
LAST_NAME 772 690 (11%) 61,0%
NICKNAME 14 14 (100%) 1,2%
TITLE 12 10 (17%) 0,8%
SEX 795 573 (23%) 51,0%
ACTIVITY 145 88 (40%) 7,0%
AFFILIATION 134 121 (10%) 11,0%
ROLE 155 92 (42%) 8,0%
PROVENIENCE 120 80 (34%) 7,3%
FAMILY_REL. 17 17(100%) 1,4%
AGE_CATEGORY 31 31(100%) 2,7%
MISCELLANEA 106 106 (100%) 9,3%
31
distinct mentions, out of which 3606, approxi-
mately 49%, are included in the values of the
attributes.
5 Conclusion and future work
We have presented work in progress aiming at
a better definition of the general OLP task. In
particular we have introduced Ontology Popula-
tion from Textual Mentions (OPTM) as a simpli-
fication of OLP, where the source textual mate-
rial are already classified mentions of entities.
An analysis of the data has been conducted over
a OPTM benchmark manually built from a cor-
pus of Italian news. As a result a number of indi-
cators have been extracted that suggest the com-
plexity of the task for systems aiming at auto-
matic resolution of OPTM.
Our future work is related to the definition and
extension of the OPTM benchmark for the nor-
malization step (see Introduction). For this step it
is crucial the construction and use of a large-
scale ontology, including the concepts and rela-
tions referred by mentions. A number of inter-
esting relations between mentions and ontology
are likely to emerge.
The work presented in this paper is part of the
ONTOTEXT project, a larger initiative aimed at
developing text mining technologies to be ex-
ploited in the perspective of the Semantic Web.
The project focuses on the study and develop-
ment of innovative knowledge extraction tech-
niques for producing new or less noisy informa-
tion to be made available to the Semantic Web.
ONTOTEXT addresses three key research as-
pects: annotating documents with semantic and
relational information, providing an adequate
degree of interoperability of such relational in-
formation, and updating and extending the on-
tologies used for Semantic Web annotation. The
concrete evaluation scenario in which algorithms
will be tested with a number of large-scale ex-
periments is the automatic acquisition of infor-
mation about people from newspaper articles.
6 Acknowledgements
This work was partially funded the three-
year project ONTOTEXT
6
 funded by the Provin-
cia Autonoma di Trento. We would like to thank
Nicola Tovazzi for his contribution to the anno-
tation of the dataset.
                                                 
6
 http://tcc.itc.it/projects/ontotext
References
Almuhareb, A. and Poesio, M.. 2004. Attribute-
based and value-based clustering: An evalua-
tion. In Proceedings of EMNLP 2004, pages
158--165, Barcelona, Spain.
Avancini, H., Lavelli, A., Magnini, B.,
Sebastiani, F., Zanoli, R. (2003). Expanding
Domain-Specific Lexicons by Term Categori-
zation. In: Proceedings of SAC 2003, 793-79.
Cunningham, H. and Bontcheva, K. Knowledge
Management and Human Language: Crossing
the Chasm. Journal of Knowledge Manage-
ment, 9(5), 2005.
Buitelaar, P., Cimiano, P. and Magnini, B. (Eds.)
Ontology Learning from Text: Methods,
Evaluation and Applications. IOS Press, 2005.
Ferro, L., Gerber, L., Mani, I., Sundheim, B. and
Wilson, G. (2005). TIDES 2005 Standard for
the Annotation of Temporal Expressions.
Technical report, MITRE.
Lavelli, A., Magnini, B., Negri, M., Pianta, E.,
Speranza, M. and Sprugnoli, R. (2005). Italian
Content Annotation Bank (I-CAB): Temporal
Expressions (V. 1.0.). Technical Report T-
0505-12. ITC-irst, Trento.
Lin, D. (1998). Automatic Retrieval and Clus-
tering of Similar Words. In: Proceedings of
COLING-ACL98, Montreal, Canada, 1998.
Linguistic Data Consortium (2004). ACE
(Automatic Content Extraction) English An-
notation Guidelines for Entities, version 5.6.1
2005.05.23.
http://projects.ldc.upenn.edu/ace/docs/English-
Entities-Guidelines_v5.6.1.pdf
Magnini, B., Pianta, E., Girardi, C., Negri, M.,
Romano, L., Speranza, M., Bartalesi Lenzi, V.
and Sprugnoli, R. (2006). I-CAB: the Italian
Content Annotation Bank. Proceedings of
LREC-2006, Genova, Italy, 22-28 May, 2006.
Tanev, H. and Magnini, B. Weakly Supervised
Approaches for Ontology Population. Pro-
ceedings of EACL-2006, Trento, Italy, 3-7
April, 2006.
Velardi, P., Navigli, R., Cuchiarelli, A., Neri, F.
(2004). Evaluation of Ontolearn, a Methodol-
ogy for Automatic Population of Domain On-
tologies. In: Buitelaar, P., Cimiano, P.,
Magnini, B. (eds.): Ontology Learning from
Text: Methods, Evaluation and Applications,
IOS Press, Amsterdam, 2005.
32
Representing and Accessing Multilevel Linguistic Annotation using 
the MEANING Format  
 
 
Emanuele Pianta 
ITC-irst 
38050, Povo 
Trento, Italy 
pianta@itc.it 
Luisa Bentivogli 
ITC-irst 
38050, Povo 
Trento, Italy 
bentivo@itc.it 
Christian Girardi 
ITC-irst 
38050, Povo 
Trento, Italy 
cgirardi@itc.it 
Bernardo Magnini 
ITC-irst 
38050, Povo 
Trento, Italy 
magnini@itc.it 
 
  
 
Abstract 
We present an XML annotation format 
(MEANING Annotation Format, MAF) 
specifically designed to represent and in-
tegrate different levels of linguistic anno-
tations and a tool that provides flexible 
access to them (MEANING Browser). 
We describe our experience in integrating 
linguistic annotations coming from dif-
ferent sources, and the solutions we 
adopted to implement efficient access to 
corpora annotated with the Meaning 
Format. 
1 Introduction 
It is well known that when using XML-based 
annotation schemes to represent multi layer an-
notations, it can be difficult to handle partially 
overlapping annotations. Annotating discontinu-
ous elements may be considered as a variant of 
the same problem (Pianta and Bentivogli, 2004). 
Other difficulties can arise from the necessity of 
integrating manual and automatic annotations, as 
we will show in this paper. 
One of the most effective solutions to the 
above mentioned problems is the so called stand-
off annotation, based on the separation between 
textual data and annotations, and between vari-
ous types of annotation, possibly pointing to 
same text. This approach has been systematically 
adopted in the design of MAF, a multilayer XML 
format developed for the EU-funded MEANING 
project, in the context of the creation of the Ital-
ian MEANING Corpus (Bentivogli et al, 2003).  
In this paper we will describe our experience 
in the use of MAF, with special emphasis on how 
we solved issues related to representing annota-
tion levels which come from different sources, 
and can possibly overlap. We will also give de-
tails about the solutions we adopted to allow for 
efficient access and human browsing of MAF 
standoff annotations. 
The rest of the paper is organized as follows. 
Section 2 describes MAF and the types of anno-
tations which have been represented with it. Sec-
tion 3 reports on the integration into MAF of lin-
guistic annotations coming from different 
sources. Section 4 illustrates the strategies 
adopted to make the information encoded in 
MAF quickly accessible. Finally, Section 5 pre-
sents the MEANING Browser, a tool for access-
ing and navigating corpora linguistically anno-
tated with MAF. 
2 The MEANING Format 
Following the proposals for the ISO/TC 37/SC 4 
standard for linguistic resources (Ide and 
Romary, 2002), the MAF scheme is based on 
annotation structures and data categories. Each 
type of annotation structure (nestable <struct> 
elements) corresponds to a specific kind of lin-
guistic object (e.g. tokens, lexical units, multi-
words), and each instance of a linguistic object is 
identified by a unique identifier. Data categories 
(<feat> tags) represent attributes of the linguistic 
objects. Different representation levels are con-
tained in separate documents, or document sec-
tions. The XLink and XPointer syntax is used to 
represent relations between elements in different 
XML documents, and IDREFs attributes for rela-
tions within the same document. 
2.1 First  version 
The first version of the MEANING Format has 
been used to represent seven kinds of informa-
tion: orthographic features, the structure of the 
77
text, morphosyntactic information, multiwords, 
syntactic information, named entities, and word 
senses. 
Annotation levels are related to each other fol-
lowing a hierarchy of annotation levels, which 
reflects a theoretically grounded hierarchy of 
linguistic objects. The basic (orthographic) anno-
tation level, representing tokens, is implemented 
with pointers to the character positions in the hub 
corpus. Then the morphosyntactic level, repre-
senting word-related morphological information, 
contains pointers to the tokens, whereas the mul-
tiword level points to the words described at 
morphosyntactic level. 
The following example shows how the mor-
phosyntactic features of the Italian word ?an-
dare? (to go) are represented. 
 
<struc   type="w-level" id="w_12" 
             xlink:href="#xpointer(id('t_10'))"> 
     <feat type="lemma">andare</feat> 
     <feat type="stem">and</feat> 
     <feat type="pos">v</feat> 
     <feat type="elra-tag">VF</feat> 
     <feat type="mood">inf</feat> 
     <feat type="tense">pres</feat> 
 </struc> 
 
MAF also specifically addresses the problem  
of discontinuous units, such as for instance non-
contiguous multiwords; see ?andarci veramente 
piano? (take it really easy). A detailed study of 
how standoff annotation allows for an elegant 
treatment of this phenomenon can be found in 
(Pianta and Bentivogli 2004). 
2.2 Second version 
The first version of the MEANING Format has 
recently been extended within the FU-PAT ON-
TOTEXT project (Magnini et al 2005). 
Within this project, we are creating the Italian 
Content Annotation Bank (I-CAB), a corpus of 
Italian news stories annotated with different 
kinds of semantic information. Annotation is be-
ing carried out manually, as we intend I-CAB to 
become a benchmark for automatic Information 
Extraction and Ontology Population tasks, in-
cluding recognition and normalization of various 
types of entities, temporal expressions, relations 
between entities, and relations between entities 
and temporal expressions (e.g. the relation date-
of-birth connecting a person to a date). 
To fulfill I-CAB annotation needs, we ex-
tended MAF, by adding a number of new lin-
guistic annotation levels, i.e.: 
 
? temporal expressions  
? entities of type person and organization 
? mentions (i.e. the textual expressions re-
ferring to the entities)  
 
According to the hierarchical approach to rep-
resenting relations between annotation levels in 
the first version of the MEANING Format, tem-
poral expressions and entity mentions are repre-
sented with pointers to morphosyntactic level 
entities. Entities, instead, are represented with 
pointers to entity mentions.  
To manually annotate temporal expressions 
we followed the TIMEX2 markup standard, 
while to mark entities and mentions we relied on 
the ACE entity detection task guidelines. To per-
form the annotation task we used Callisto 
(http://callisto.mitre.org). 
3 Converting linguistic annotations into 
MAF  
The manual annotations produced through Cal-
listo, which is related to novel annotation levels 
such  as temporal expressions and entity men-
tions, had to be integrated with more traditional 
annotations which are performed automatically 
with the TextPro tool, an automatic linguistic 
analysis Tool Suite developed at ITC-irst. 
News
MEANING
Annotation 
Format
Callisto
Lucene
AIF format
Manual 
annotation
Automatic 
annotation
TextPro
TextPro format
Indexing
Data Base
Conversion
MEANING
Browser
I-CAB 
Corpus
 
As one can see in the above figure, two different 
annotation processes (automatic and manual) 
produce two different formats which must be 
converted and integrated into MAF in order to be 
accessed by the MEANING Browser (or any 
other NLP tool). 
3.1 From TextPro format to MEANING 
Format 
TextPro takes a raw text as input and carries out 
basic processing tasks such as tokenization, mor-
78
phological analysis, PoS tagging, lemmatization, 
and multiword recognition. The results of 
TextPro analyses are represented in a table, 
where each token is on a row, and columns con-
tain multiple annotation levels. Converting from 
the TextPro to the MEANING Format requires 
retrieving the character positions of tokens in the 
hub corpus, which are not directly available in 
the TextPro output. 
3.2 From AIF format to MEANING format 
The Callisto manual annotation tool produces a 
coding format called AIF (Atlas Interchange For-
mat), which implements a stand-off XML anno-
tation scheme. 
When using the Callisto graphical interface, 
all annotations of temporal expressions and en-
tity mentions are carried out by selecting a se-
quence of contiguous characters. As a conse-
quence, all AIF annotations make reference to 
character positions. 
However, from Section 2.2 we know that in 
MAF temporal expressions and entity mentions 
make reference to morphosyntactic linguistic 
objects, not characters. This implies that, to go 
from AIF to the MEANING Format, we need to 
translate annotations making reference to the po-
sition of characters into annotations that point to 
morphological entities. More precisely, we need 
to substitute pointers to character positions with 
pointers to morphosyntactic objects which have 
been marked automatically by TextPro. Carrying 
out this step will also achieve the integration of 
manual and automatic annotations.  
The integration step is possible because the 
MAF hierarchy of annotation levels points, at the 
lowest level, to character positions. By following 
the hierarchy of links relating the various annota-
tion levels it is always possible to trace back a 
linguistic object to some sequence of characters 
in the raw text, and in the opposite direction, 
given a string, we know what linguistic objects 
correspond to it. Summing up, the integration of 
AIF annotations into MAF requires that, given 
the character positions contained in the AIF an-
notation of some string, we substitute the point-
ers to characters with the pointers to the linguis-
tic objects that cover the same string. 
 
4 Data Access 
MAF turned out to be a flexible and expressive 
means to represent and integrate multiple levels 
of linguistic annotation. This was achieved 
mainly thanks to the adoption of the standoff 
annotation approach.  However accessing and 
retrieving information spread in possibly very 
large repositories (hundreds of thousands) of 
XML files may be a challenging task even for 
Database Management Systems specifically de-
signed to handle XML. To solve this problem we 
first analyzed existing native XML databases 
such as eXist, and Apache Xindice, but found 
that what was available at the time did not suited 
our needs. For this reason we approached the 
access problem through a two-fold strategy: 
? converting  XML data into a relational 
database 
? indexing XML data and accessing them 
through a  search engine (LUCENE) 
The conversion of MAF data into a relational 
database is based on the following strategy. Each 
annotation level is mapped into a table, where 
rows represent instances of the relevant linguistic 
object (e.g. words), and columns represent its 
attributes (e.g. lemma, PoS, etc). Specific col-
umns contain the object identifiers and the point-
ers to objects of other types/tables. 
Once MAF data are stored in a relational data-
base, they can be accessed quite efficiently. 
However, when the access to data requires joins 
of many tables, access times become incompati-
ble with various kinds of applications, such as 
on-line corpus browsing. For this reason we tried 
to complement the use of a relational database 
with the exploitation of the indexing capability 
of the LUCENE search engine 
(http://lucene.apache.org/). To this extent we 
modified the LUCENE analyzer so as to be able 
to parse XML structures. In this way LUCENE 
can be configured in order to index any XML 
structure. 
The fast access capabilities of a relational da-
tabase combined with the extended indexing ca-
pabilities of LUCENE enabled us to implement a 
browser of MAF annotated corpora.  
5 The MEANING Browser 
The MEANING Browser can be used by humans 
to navigate any corpus encoded with MAF. The 
browser is built upon an API which can be used 
by any automatic system.  
In the following, we are going to demonstrate 
how I-CAB texts and their annotations can be 
accessed through the MEANING Browser. 
The first kind of access to the corpus is word-
oriented, and amounts to a concordancer, i.e. a 
79
tool able to provide all the occurrences of a cer-
tain word in the corpus. The user can alterna-
tively search for all occurrences of a word form, 
or a lemma, possibly constraining the search to a 
certain PoS. Free combinations between these 
constraints are allowed. The system will return a 
KWIC-like concordance of all the tokens in the 
corpus that match the request, within a chosen 
word window. By clicking on the magnifying 
glass, one can see the sentence in which the 
searched word occurs (see Appendix 1). 
By clicking on a specific icon a new window 
is opened where the whole text is displayed and 
its linguistic annotations are made accessible. A 
number of graphical widgets allow the user to 
highlight the desired annotations: e.g. nouns, 
verbs, multiwords, temporal expressions, men-
tions of a specific entity. 
In Appendix 2 the browser is used to show 
both nouns (automatically annotated) and entity 
mentions (from manual annotation). Appendix 3 
shows time expressions and discontinuous mul-
tiwords; see how the multiword ?ha rassegnato 
? le dimissioni? (he resigned) is made discon-
tinuous by the occurrence of a time expression 
ieri (yesterday). The browser will also give mor-
phosyntactic information about single words 
composing multiwords (governo, government). 
From the same window one can access the XML 
files encoding multiple annotation levels for the 
same document. 
References  
Bentivogli, L., Girardi, C., Pianta, E. 2003. The ME-
ANING Italian Corpus. In Proceedings of the Cor-
pus Linguistics 2003 conference, Lancaster, UK. 
Ide, N. & Romary, L. 2002. Standards for Language 
Resources. In Proceedings of LREC 2002, Las 
Palmas, Canary Islands, Spain.  
Magnini, B., Negri, M., Pianta, E., Romano, L., Sper-
anza, M., Serafini, L., Girardi, C., Bartalesi, V., 
Sprugnoli, R. 2005. From Text to Knowledge for 
the Semantic Web: the ONTOTEXT Project. In 
Proceedings of  SWAP 2005 Workshop, Trento, I-
taly. 
Pianta, E. and Bentivogli, L. 2004. Annotating Dis-
continuous Structures in XML: the Multiword 
Case. In Proceedings of the LREC 2004 Satellite 
Workshop on "XML-based richly annotated cor-
pora", Lisbon, Portugal. 
 
 
 
Appendix 1   
Kwic Concordancer 
 
 
Appendix 2 
Browsing nouns (in grey, automatic annotation)  and 
entity mentions (Tony Blair, manual annotation) 
 
 
Appendix 3  
Browsing discontinuous multiwords (ha rassegnato ? 
le dimissioni, he resigned), time expressions (ieri, 
yesterday) and word information (governo) 
 
80
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 1?6,
Prague, June 2007. c?2007 Association for Computational Linguistics
SemEval-2007 Task 01: Evaluating WSD
on Cross-Language Information Retrieval
Eneko Agirre
IXA NLP group
University of the Basque Country
Donostia, Basque Counntry
e.agirre@ehu.es
Oier Lopez de Lacalle
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibloleo@ehu.es
German Rigau
IXA NLP group
University of the Basque Country
Donostia, Basque Country
german.rigau@ehu.es
Bernardo Magnini
ITC-IRST
Trento, Italy
magnini@itc.it
Arantxa Otegi
IXA NLP group
University of the Basque Country
Donostia, Basque Country
jibotusa@ehu.es
Piek Vossen
Irion Technologies
Delftechpark 26
2628XH Delft, Netherlands
Piek.Vossen@irion.nl
Abstract
This paper presents a first attempt of an
application-driven evaluation exercise of
WSD. We used a CLIR testbed from the
Cross Lingual Evaluation Forum. The ex-
pansion, indexing and retrieval strategies
where fixed by the organizers. The par-
ticipants had to return both the topics and
documents tagged with WordNet 1.6 word
senses. The organization provided training
data in the form of a pre-processed Semcor
which could be readily used by participants.
The task had two participants, and the orga-
nizer also provide an in-house WSD system
for comparison.
1 Introduction
Since the start of Senseval, the evaluation of Word
Sense Disambiguation (WSD) as a separate task is a
mature field, with both lexical-sample and all-words
tasks. In the first case the participants need to tag the
occurrences of a few words, for which hand-tagged
data has already been provided. In the all-words task
all the occurrences of open-class words occurring in
two or three documents (a few thousand words) need
to be disambiguated.
The community has long mentioned the neces-
sity of evaluating WSD in an application, in order
to check which WSD strategy is best, and more im-
portant, to try to show that WSD can make a differ-
ence in applications. The use of WSD in Machine
Translation has been the subject of some recent pa-
pers, but less attention has been paid to Information
Retrieval (IR).
With this proposal we want to make a first try to
define a task where WSD is evaluated with respect
to an Information Retrieval and Cross-Lingual Infor-
mation Retrieval (CLIR) exercise. From the WSD
perspective, this task will evaluate all-words WSD
systems indirectly on a real task. From the CLIR
perspective, this task will evaluate which WSD sys-
tems and strategies work best.
We are conscious that the number of possible con-
figurations for such an exercise is very large (in-
cluding sense inventory choice, using word sense in-
duction instead of disambiguation, query expansion,
WSD strategies, IR strategies, etc.), so this first edi-
tion focuses on the following:
? The IR/CLIR system is fixed.
? The expansion / translation strategy is fixed.
? The participants can choose the best WSD
strategy.
1
? The IR system is used as the upperbound for
the CLIR systems.
We think that it is important to start doing this
kind of application-driven evaluations, which might
shed light to the intricacies in the interaction be-
tween WSD and IR strategies. We see this as the
first of a series of exercises, and one outcome of this
task should be that both WSD and CLIR communi-
ties discuss together future evaluation possibilities.
This task has been organized in collabora-
tion with the Cross-Language Evaluation Forum
(CLEF1). The results will be analyzed in the CLEF-
2007 workshop, and a special track will be pro-
posed for CLEF-2008, where CLIR systems will
have the opportunity to use the annotated data
produced as a result of the Semeval-2007 task.
The task has a webpage with all the details at
http://ixa2.si.ehu.es/semeval-clir.
This paper is organized as follows. Section 2
describes the task with all the details regarding
datasets, expansion/translation, the IR/CLIR system
used, and steps for participation. Section 3 presents
the evaluation performed and the results obtained by
the participants. Finally, Section 4 draws the con-
clusions and mention the future work.
2 Description of the task
This is an application-driven task, where the appli-
cation is a fixed CLIR system. Participants disam-
biguate text by assigning WordNet 1.6 synsets and
the system will do the expansion to other languages,
index the expanded documents and run the retrieval
for all the languages in batch. The retrieval results
are taken as the measure for fitness of the disam-
biguation. The modules and rules for the expansion
and the retrieval will be exactly the same for all par-
ticipants.
We proposed two specific subtasks:
1. Participants disambiguate the corpus, the cor-
pus is expanded to synonyms/translations and
we measure the effects on IR/CLIR. Topics2 are
not processed.
1http://www.clef-campaign.org
2In IR topics are the short texts which are used by the sys-
tems to produce the queries. They usually provide extensive
information about the text to be searched, which can be used
both by the search engine and the human evaluators.
2. Participants disambiguate the topics per lan-
guage, we expand the queries to syn-
onyms/translations and we measure the effects
on IR/CLIR. Documents are not processed
The corpora and topics were obtained from the
ad-hoc CLEF tasks. The supported languages in the
topics are English and Spanish, but in order to limit
the scope of the exercise we decided to only use En-
glish documents. The participants only had to dis-
ambiguate the English topics and documents. Note
that most WSD systems only run on English text.
Due to these limitations, we had the following
evaluation settings:
IR with WSD of topics , where the participants
disambiguate the documents, the disam-
biguated documents are expanded to syn-
onyms, and the original topics are used for
querying. All documents and topics are in En-
glish.
IR with WSD of documents , where the partic-
ipants disambiguate the topics, the disam-
biguated topics are expanded and used for
querying the original documents. All docu-
ments and topics are in English.
CLIR with WSD of documents , where the partic-
ipants disambiguate the documents, the dis-
ambiguated documents are translated, and the
original topics in Spanish are used for query-
ing. The documents are in English and the top-
ics are in Spanish.
We decided to focus on CLIR for evaluation,
given the difficulty of improving IR. The IR results
are given as illustration, and as an upperbound of
the CLIR task. This use of IR results as a reference
for CLIR systems is customary in the CLIR commu-
nity (Harman, 2005).
2.1 Datasets
The English CLEF data from years 2000-2005 com-
prises corpora from ?Los Angeles Times? (year
1994) and ?Glasgow Herald? (year 1995) amounting
to 169,477 documents (579 MB of raw text, 4.8GB
in the XML format provided to participants, see Sec-
tion 2.3) and 300 topics in English and Spanish (the
topics are human translations of each other). The
relevance judgments were taken from CLEF. This
2
might have the disadvantage of having been pro-
duced by pooling the results of CLEF participants,
and might bias the results towards systems not using
WSD, specially for monolingual English retrieval.
We are considering the realization of a post-hoc
analysis of the participants results in order to ana-
lyze the effect on the lack of pooling.
Due to the size of the document collection, we de-
cided that the limited time available in the competi-
tion was too short to disambiguate the whole collec-
tion. We thus chose to take a sixth part of the corpus
at random, comprising 29,375 documents (874MB
in the XML format distributed to participants). Not
all topics had relevant documents in this 17% sam-
ple, and therefore only 201 topics were effectively
used for evaluation. All in all, we reused 21,797
relevance judgements that contained one of the doc-
uments in the 17% sample, from which 923 are pos-
itive3. For the future we would like to use the whole
collection.
2.2 Expansion and translation
For expansion and translation we used the publicly
available Multilingual Central Repository (MCR)
from the MEANING project (Atserias et al, 2004).
The MCR follows the EuroWordNet design, and
currently includes English, Spanish, Italian, Basque
and Catalan wordnets tightly connected through the
Interlingual Index (based on WordNet 1.6, but linked
to all other WordNet versions).
We only expanded (translated) the senses returned
by the WSD systems. That is, given a word like
?car?, it will be expanded to ?automobile? or ?railcar?
(and translated to ?auto? or ?vago?n? respectively) de-
pending on the sense in WN 1.6. If the systems re-
turns more than one sense, we choose the sense with
maximum weight. In case of ties, we expand (trans-
late) all. The participants could thus implicitly affect
the expansion results, for instance, when no sense
could be selected for a target noun, the participants
could either return nothing (or NOSENSE, which
would be equivalent), or all senses with 0 score. In
the first case no expansion would be performed, in
the second all senses would be expanded, which is
equivalent to full expansion. This fact will be men-
tioned again in Section 3.5.
3The overall figures are 125,556 relevance judgements for
the 300 topics, from which 5700 are positive
Note that in all cases we never delete any of the
words in the original text.
In addition to the expansion strategy used with the
participants, we tested other expansion strategies as
baselines:
noexp no expansion, original text
fullexp expansion (translation in the case of English
to Spanish expansion) to all synonyms of all
senses
wsd50 expansion to the best 50% senses as returned
by the WSD system. This expansion was tried
over the in-house WSD system of the organizer
only.
2.3 IR/CLIR system
The retrieval engine is an adaptation of the Twenty-
One search system (Hiemstra and Kraaij, 1998) that
was developed during the 90?s by the TNO research
institute at Delft (The Netherlands) getting good re-
sults on IR and CLIR exercises in TREC (Harman,
2005). It is now further developed by Irion technolo-
gies as a cross-lingual retrieval system (Vossen et al,
). For indexing, the TwentyOne system takes Noun
Phrases as an input. Noun Phases (NPs) are detected
using a chunker and a word form with POS lexicon.
Phrases outside the NPs are not indexed, as well as
non-content words (determiners, prepositions, etc.)
within the phrase.
The Irion TwentyOne system uses a two-stage re-
trieval process where relevant documents are first
extracted using a vector space matching and sec-
ondly phrases are matched with specific queries.
Likewise, the system is optimized for high-precision
phrase retrieval with short queries (1 up 5 words
with a phrasal structure as well). The system can be
stripped down to a basic vector space retrieval sys-
tem with an tf.idf metrics that returns documents for
topics up to a length of 30 words. The stripped-down
version was used for this task to make the retrieval
results compatible with the TREC/CLEF system.
The Irion system was also used for pre-
processing. The CLEF corpus and topics were con-
verted to the TwentyOne XML format, normalized,
and named-entities and phrasal structured detected.
Each of the target tokens was identified by an unique
identifier.
2.4 Participation
The participants were provided with the following:
3
1. the document collection in Irion XML format
2. the topics in Irion XML format
In addition, the organizers also provided some of
the widely used WSD features in a word-to-word
fashion4 (Agirre et al, 2006) in order to make partic-
ipation easier. These features were available for both
topics and documents as well as for all the words
with frequency above 10 in SemCor 1.6 (which can
be taken as the training data for supervised WSD
systems). The Semcor data is publicly available 5.
For the rest of the data, participants had to sign and
end user agreement.
The participants had to return the input files en-
riched with WordNet 1.6 sense tags in the required
XML format:
1. for all the documents in the collection
2. for all the topics
Scripts to produce the desired output from word-
to-word files and the input files were provided by
organizers, as well as DTD?s and software to check
that the results were conformant to the respective
DTD?s.
3 Evaluation and results
For each of the settings presented in Section 2 we
present the results of the participants, as well as
those of an in-house system presented by the orga-
nizers. Please refer to the system description papers
for a more complete description. We also provide
some baselines and alternative expansion (transla-
tion) strategies. All systems are evaluated accord-
ing to their Mean Average Precision 6 (MAP) as
computed by the trec eval software on the pre-
existing CLEF relevance-assessments.
3.1 Participants
The two systems that registered sent the results on
time.
PUTOP They extend on McCarthy?s predominant
sense method to create an unsupervised method
of word sense disambiguation that uses auto-
matically derived topics using Latent Dirichlet
4Each target word gets a file with all the occurrences, and
each occurrence gets the occurrence identifier, the sense tag (if
in training), and the list of features that apply to the occurrence.
5http://ixa2.si.ehu.es/semeval-clir/
6http://en.wikipedia.org/wiki/
Information retrieval
Allocation. Using topic-specific synset similar-
ity measures, they create predictions for each
word in each document using only word fre-
quency information. The disambiguation pro-
cess took aprox. 12 hours on a cluster of 48 ma-
chines (dual Xeons with 4GB of RAM). Note
that contrary to the specifications, this team
returned WordNet 2.1 senses, so we had to
map automatically to 1.6 senses (Daude et al,
2000).
UNIBA This team uses a a knowledge-based WSD
system that attempts to disambiguate all words
in a text by exploiting WordNet relations. The
main assumption is that a specific strategy for
each Part-Of-Speech (POS) is better than a sin-
gle strategy. Nouns are disambiguated basi-
cally using hypernymy links. Verbs are dis-
ambiguated according to the nouns surrounding
them, and adjectives and adverbs use glosses.
ORGANIZERS In addition to the regular partic-
ipants, and out of the competition, the orga-
nizers run a regular supervised WSD system
trained on Semcor. The system is based on
a single k-NN classifier using the features de-
scribed in (Agirre et al, 2006) and made avail-
able at the task website (cf. Section 2.4).
In addition to those we also present some com-
mon IR/CLIR baselines, baseline WSD systems, and
an alternative expansion:
noexp a non-expansion IR/CLIR baseline of the
documents or topics.
fullexp a full-expansion IR/CLIR baseline of the
documents or topics.
wsdrand a WSD baseline system which chooses a
sense at random. The usual expansion is ap-
plied.
1st a WSD baseline system which returns the sense
numbered as 1 in WordNet. The usual expan-
sion is applied.
wsd50 the organizer?s WSD system, where the 50%
senses of the word ranking according to the
WSD system are expanded. That is, instead of
expanding the single best sense, it expands the
best 50% senses.
3.2 IR Results
This section present the results obtained by the par-
ticipants and baselines in the two IR settings. The
4
IRtops IRdocs CLIR
no expansion 0.3599 0.3599 0.1446
full expansion 0.1610 0.1410 0.2676
UNIBA 0.3030 0.1521 0.1373
PUTOP 0.3036 0.1482 0.1734
wsdrand 0.2673 0.1482 0.2617
1st sense 0.2862 0.1172 0.2637
ORGANIZERS 0.2886 0.1587 0.2664
wsd50 0.2651 0.1479 0.2640
Table 1: Retrieval results given as MAP. IRtops
stands for English IR with topic expansion. IR-
docs stands for English IR with document expan-
sion. CLIR stands for CLIR results for translated
documents.
second and third columns of Table 1 present the re-
sults when disambiguating the topics and the docu-
ments respectively. Non of the expansion techniques
improves over the baseline (no expansion).
Note that due to the limitation of the search en-
gine, long queries were truncated at 50 words, which
might explain the very low results of the full expan-
sion.
3.3 CLIR results
The last column of Table 1 shows the CLIR results
when expanding (translating) the disambiguated
documents. None of the WSD systems attains the
performance of full expansion, which would be the
baseline CLIR system, but the WSD of the organizer
gets close.
3.4 WSD results
In addition to the IR and CLIR results we also pro-
vide the WSD performance of the participants on
the Senseval 2 and 3 all-words task. The documents
from those tasks were included alongside the CLEF
documents, in the same formats, so they are treated
as any other document. In order to evaluate, we had
to map automatically all WSD results to the respec-
tive WordNet version (using the mappings in (Daude
et al, 2000) which are publicly available).
The results are presented in Table 2, where we can
see that the best results are attained by the organizers
WSD system.
3.5 Discussion
First of all, we would like to mention that the WSD
and expansion strategy, which is very simplistic, de-
grades the IR performance. This was rather ex-
Senseval-2 all words
precision recall coverage
ORGANIZERS 0.584 0.577 93.61%
UNIBA 0.498 0.375 75.39%
PUTOP 0.388 0.240 61.92%
Senseval-3 all words
precision recall coverage
ORGANIZERS 0.591 0.566 95.76%
UNIBA 0.484 0.338 69.98%
PUTOP 0.334 0.186 55.68%
Table 2: English WSD results in the Senseval-2 and
Senseval-3 all-words datasets.
pected, as the IR experiments had an illustration
goal, and are used for comparison with the CLIR
experiments. In monolingual IR, expanding the top-
ics is much less harmful than expanding the docu-
ments. Unfortunately the limitation to 50 words in
the queries might have limited the expansion of the
topics, which make the results rather unreliable. We
plan to fix this for future evaluations.
Regarding CLIR results, even if none of the WSD
systems were able to beat the full-expansion base-
line, the organizers system was very close, which is
quite encouraging due to the very simplistic expan-
sion, indexing and retrieval strategies used.
In order to better interpret the results, Table 3
shows the amount of words after the expansion in
each case. This data is very important in order to un-
derstand the behavior of each of the systems. Note
that UNIBA returns 3 synsets at most, and therefore
the wsd50 strategy (select the 50% senses with best
score) leaves a single synset, which is the same as
taking the single best system (wsdbest). Regarding
PUTOP, this system returned a single synset, and
therefore the wsd50 figures are the same as the ws-
dbest figures.
Comparing the amount of words for the two par-
ticipant systems, we see that UNIBA has the least
words, closely followed by PUTOP. The organizers
WSD system gets far more expanded words. The
explanation is that when the synsets returned by a
WSD system all have 0 weights, the wsdbest expan-
sion strategy expands them all. This was not explicit
in the rules for participation, and might have affected
the results.
A cross analysis of the result tables and the num-
ber of words is interesting. For instance, in the IR
exercise, when we expand documents, the results in
5
English Spanish
No WSD noexp 9,900,818 9,900,818fullexp 93,551,450 58,491,767
UNIBA
wsdbest 19,436,374 17,226,104
wsd50 19,436,374 17,226,104
PUTOP wsdbest 20,101,627 16,591,485wsd50 20,101,627 16,591,485
Baseline 1st 24,842,800 20,261,081
WSD wsdrand 24,904,717 19,137,981
ORG. wsdbest 26,403,913 21,086,649wsd50 36,128,121 27,528,723
Table 3: Number of words in the document col-
lection after expansion for the WSD system and all
baselines. wsdbest stands for the expansion strategy
used with participants.
the third column of Table 1 show that the ranking for
the non-informed baselines is the following: best for
no expansion, second for random WSD, and third
for full expansion. These results can be explained
because of the amount of expansion: the more ex-
pansion the worst results. When more informed
WSD is performed, documents with more expansion
can get better results, and in fact the WSD system of
the organizers is the second best result from all sys-
tem and baselines, and has more words than the rest
(with exception of wsd50 and full expansion). Still,
the no expansion baseline is far from the WSD re-
sults.
Regarding the CLIR result, the situation is in-
verted, with the best results for the most productive
expansions (full expansion, random WSD and no ex-
pansion, in this order). For the more informed WSD
methods, the best results are again for the organizers
WSD system, which is very close to the full expan-
sion baseline. Even if wsd50 has more expanded
words wsdbest is more effective. Note the very high
results attained by random. These high results can
be explained by the fact that many senses get the
same translation, and thus for many words with few
translation, the random translation might be valid.
Still the wsdbest, 1st sense and wsd50 results get
better results.
4 Conclusions and future work
This paper presents the results of a preliminary at-
tempt of an application-driven evaluation exercise
of WSD in CLIR. The expansion, indexing and re-
trieval strategies proved too simplistic, and none of
the two participant systems and the organizers sys-
tem were able to beat the full-expansion baseline.
Due to efficiency reasons, the IRION system had
some of its features turned off. Still the results are
encouraging, as the organizers system was able to
get very close to the full expansion strategy with
much less expansion (translation).
For the future, a special track of CLEF-2008 will
leave the avenue open for more sophisticated CLIR
techniques. We plan to extend the WSD annotation
to all words in the CLEF English document collec-
tion, and we also plan to contact the best performing
systems of the SemEval all-words tasks to have bet-
ter quality annotations.
Acknowledgements
We wish to thank CLEF for allowing us to use their data, and the
CLEF coordinator, Carol Peters, for her help and collaboration.
This work has been partially funded by the Spanish education
ministry (project KNOW)
References
E. Agirre, O. Lopez de Lacalle, and D. Martinez. 2006.
Exploring feature set combinations for WSD. In Proc.
of the SEPLN.
J. Atserias, L. Villarejo, G. Rigau, E. Agirre, J. Carroll,
B. Magnini, and P. Vossen. 2004. The MEANING
Multilingual Central Repository. In Proceedings of the
2.nd Global WordNet Conference, GWC 2004, pages
23?30. Masaryk University, Brno, Czech Republic.
J. Daude, L. Padro, and G. Rigau. 2000. Mapping Word-
Nets Using Structural Information. In Proc. of ACL,
Hong Kong.
D. Harman. 2005. Beyond English. In E. M. Voorhees
and D. Harman, editors, TREC: Experiment and Eval-
uation in Information Retrieval, pages 153?181. MIT
press.
D. Hiemstra and W. Kraaij. 1998. Twenty-One in ad-hoc
and CLIR. In E.M. Voorhees and D. K. Harman, ed-
itors, Proc. of TREC-7, pages 500?540. NIST Special
Publication.
P. Vossen, G. Rigau, I. Alegria, E. Agirre, D. Farwell,
and M. Fuentes. Meaningful results for Information
Retrieval in the MEANING project. In Proc. of the
3rd Global Wordnet Conference.
6
Coling 2010: Poster Volume, pages 99?107,
Beijing, August 2010
Toward Qualitative Evaluation of Textual Entailment Systems
Elena Cabrio
FBK-Irst, University of Trento
cabrio@fbk.eu
Bernardo Magnini
FBK-Irst
magnini@fbk.eu
Abstract
This paper presents a methodology for a
quantitative and qualitative evaluation of
Textual Entailment systems. We take ad-
vantage of the decomposition of Text Hy-
pothesis pairs into monothematic pairs,
i.e. pairs where only one linguistic phe-
nomenon at a time is responsible for en-
tailment judgment, and propose to run TE
systems over such datasets. We show
that several behaviours of a system can
be explained in terms of the correlation
between the accuracy on monothematic
pairs and the accuracy on the correspond-
ing original pairs.
1 Introduction
Since 2005, Recognizing Textual Entailment
(RTE) has been proposed as a task whose aim is
to capture major semantic inference needs across
applications in Computational Linguistics (Dagan
et al, 2009). Systems are asked to automatically
judge whether the meaning of a portion of text, re-
ferred as Text (T), entails the meaning of another
text, referred as Hypothesis (H). This evaluation
provides useful cues for researchers and develop-
ers aiming at the integration of TE components in
larger applications (see, for instance, the use of a
TE engine in the QALL-ME project system1, the
use in relation extraction (Romano et al, 2006),
and in reading comprehension systems (Nielsen
et al, 2009)).
Although the RTE evaluations showed pro-
gresses in TE technologies, we think that there is
1http://qallme.fbk.eu/
still large room for improving qualitative analysis
of both the RTE datasets and the system results.
In particular, we intend to focus this paper on the
following aspects:
1. There is relatively poor analysis of the lin-
guistic phenomena that are relevant for the
RTE datasets, and very little is known about
the distribution of such phenomena, and
about the ability of participating systems to
correctly detect and judge them in T,H pairs.
Experiments like the ablation tests attempted
in the last RTE-5 campaign on lexical and
lexical-syntactic resources go in this direc-
tion, although the degree of comprehension
is still far from being optimal.
2. We are interested in the correlations among
the capability of a system to address single
linguistic phenomena in a pair and the ability
to correctly judge the pair itself. Despite the
strong intuition about such correlation (i.e.
the more the phenomena for which a system
is trained, the better the final judgment), no
empirical evidences support it.
3. Although the ability to detect and manage
single phenomena seems to be a crucial fea-
ture of high performing systems, very little is
known about how systems manage to com-
bine such results in a global score for a pair.
The mechanism underlying such composi-
tion may shed light on meaning composition
related to TE tasks.
4. Finally, we are interested in the relation be-
tween the above mentioned items over the
different kinds of pairs represented in RTE
99
datasets, specifically entailment, contradic-
tion and unknown pairs. In this case the in-
tuition is that some phenomena are more rel-
evant for a certain judgment rather than for
another.
To address the issues above, we propose an
evaluation methodology aiming at providing a
number of quantitative and qualitative indicators
about a TE system. The method is based on
the decomposition of T,H pairs into monothematic
pairs, each representing one single linguistic phe-
nomenon relevant for entailment judgment. Eval-
uation is carried out both on the original T,H pair
and on the monothematic pairs originated from it.
We define a correlation index between the accu-
racy of the system on the original T,H pairs and
the accuracy on the corresponding monothematic
pairs. We investigate the use of such correlations
on different subsets of the evaluation dataset (i.e.
positive vs negative pairs) and we try to induce
regular patterns of evaluation.
The method we propose has been tested on a
sample of 60 pairs, each decomposed in the corre-
sponding monothematic pairs, and using two sys-
tems that obtained similar performances in RTE-
5. We show that the main features and differences
of these systems come to light when evaluated us-
ing qualitative criteria. Futhermore, we compare
such systems with two different baseline systems,
the first one performing Word Overlap, while the
second one is an ideal system that knows a priori
the probability of a linguistic phenomenon to be
associated with a certain entailment judgement.
The paper is structured as follows. Sec-
tion 2 explains the procedure for the creation of
monothematic pairs starting from RTE pairs. Sec-
tion 3 presents the evaluation methodology we
propose, while Section 4 describes our pilot study.
Section 5 concludes the paper and proposes future
developments.
2 Decomposing RTE pairs
Our proposal on qualitative evaluation takes ad-
vantage of previous work on specialized entail-
ment engines and monothematic datasets. A
monothematic pair is defined (Magnini and
Cabrio, 2009) as a T,H pair in which a certain
phenomenon relevant to the entailment relation is
highlighted and isolated. The main idea is to cre-
ate the monothematic pairs basing on the phenom-
ena that are actually present in the original RTE
pairs, so that the actual distribution of the linguis-
tic phenomena involved in the entailment relation
emerges.
For the decomposition procedure, we refer to
the methodology described in (Bentivogli et al,
2010), consisting of a number of steps carried out
manually. The starting point is a [T,H] pair taken
from one of the RTE data sets, that should be
decomposed in a number of monothematic pairs
[T,Hi], where T is the original Text and Hi are
the Hypotheses created for each linguistic phe-
nomenon relevant for judging the entailment re-
lation in [T,H]. In details, the procedure for the
creation of monothematic pairs is composed of the
following steps:
1. Individuate the phenomena contributing to
the entailment decision in [T,H].
2. For each linguistic phenomenon i:
(a) Detect a general entailment rule ri for
i, and instantiate it using the part of T
expressing i as the left hand side (LHS)
of the rule, and information from H on i
as the right side (RHS).
(b) substitute the portion of T that matches
the LHS of ri with the RHS of ri.
(c) consider the result of the previous step
as Hi, and compose the monothematic
pair [T,Hi]. Mark the pair with phe-
nomenon i.
3. Assign an entailment judgment to each
monothematic pair.
Relevant linguistic phenomena are grouped us-
ing both fine-grained categories and broader cate-
gories, defined referring to widely accepted clas-
sifications in the literature (e.g. (Garoufi, 2007))
and to the inference types typically addressed in
RTE systems: lexical, syntactic, lexical-syntactic,
discourse and reasoning. Each macro category in-
cludes fine-grained phenomena (Table 2 lists the
phenomena detected in RTE-5 datasets).
100
Text snippet (pair 125) Phenomena Judg.
T Mexico?s new president, Felipe Calderon, seems to be doing
all the right things in cracking down on Mexico?s drug traffickers. [...]
H Felipe Calderon is the outgoing President of Mexico. lexical:semantic-opposition C
syntactic:argument-realization, syntactic:apposition
H1 Mexico?s outgoing president, Felipe Calderon, seems to be doing all lexical:semantic-opposition C
all the right things in cracking down on Mexico?s drug traffickers. [...]
H2 The new president of Mexico, Felipe Calderon, seems to be doing syntactic:argument-realization E
all the right things in cracking down on Mexico?s drug traffickers. . [...]
H3 Felipe Calderon is Mexico?s new president. syntactic:apposition E
Table 1: Application of the decomposition methodology to an original RTE pair
Table 1 shows an example of the decompo-
sition of a RTE pair (marked as contradiction)
into monothematic pairs. At step 1 of the
methodology both the phenomena that preserve
the entailment and those that break the entailment
rules causing a contradiction in the pair are
detected, i.e. argument realization, apposition
and semantic opposition (column phenomena in
the table). While the monothematic pairs created
basing on the first two phenomena preserve the
entailment, the semantic opposition generates a
contradiction (column judgment). As an example,
let?s apply step by step the procedure to the
phenomenon of semantic opposition. At step 2a
of the methodology the general rule:
Pattern: x? / ? y
Constraint: semantic opposition(y,x)
is instantiated (new? / ?outgoing), and at step
2b the substitution in T is carried out (Mexico?s
outgoing president, Felipe Calderon [...]). At
step 2c a negative monothematic pair T,H1 is
composed (column text snippet in the table) and
marked as semantic opposition (macro-category
lexical), and the pair is judged as contradiction.
3 Evaluation methodology
Aim of the evaluation methodology we propose is
to provide quantitative and qualitative indicators
about the behaviours of actual TE systems.
3.1 General Method
The basic assumption of the evaluation methodol-
ogy is that the more a system is able to correctly
solve the linguistic phenomena underlying the en-
tailment relation separately, the more the system
should be able to correctly judge more complex
pairs, in which different phenomena are present
and interact in a complex way. Such assumption is
motivated by the notion of meaning composition-
ality, according to which the meaning of a com-
plex expression e in a language L is determined
by the structure of e in L and the meaning of the
constituents of e in L (Frege, 1892). In a parallel
way, we assume that it is possible to understand
the entailment relation of a T,H pair (i.e. to cor-
rectly judge the entailment/contradiction relation)
only if all the phenomena contributing to such re-
lation are solved.
According to such assumption, we expect that
the higher the accuracy of a system on the
monothematic pairs and the compositional strat-
egy, the better its performances on the original
RTE pairs. Furthermore, the precision a system
gains on single phenomena should be maintained
over the general dataset, thanks to suitable mech-
anisms of meaning combination.
Given a dataset composed of original RTE pairs
[T,H], a dataset composed of all the monothe-
matic pairs derived from it [T,H]mono, and a TE
system S, the evaluation methodology we propose
consists of the following steps:
1. Run S both on [T,H] and on [T,H]mono, to
obtain the accuracies of S both on the RTE
original and on monothematic pairs;
2. Extract data concerning the behaviour of S on
each phenomenon or on categories of phe-
nomena, and calculate separate accuracies.
This way it is possible to evaluate how much
a system is able to correctly deal with single
or with categories of phenomena;
3. Calculate the correlation between the ability
of the system to correctly judge the monothe-
matic pairs of [T,H]mono with respect to the
101
ability to correctly judge the original ones
in [T,H]. Such correlation is expressed
through a Correlation Index (CI), as defined
in Section 3.2;
4. In order to check if the same CI is main-
tained over both entailment and contradiction
pairs (i.e. to verify if the system has peculiar
strategies to correctly assign both judgments,
and if the high similarity of monothematic
pairs does not bias its behaviour), we calcu-
late a Deviation Index (DI) as the difference
between the CIs on entailment and on con-
tradiction pairs, as explained in more details
in Section 3.3.
3.2 Correlation Index (CI)
As introduced before, we assume that the ac-
curacy obtained on [T,H]mono should positively
correlate with the accuracy obtained on [T,H].
We define aCorrelation Index as the ratio between
the accuracy of the system on the original RTE
dataset and the accuracy obtained on the monothe-
matic dataset, as follows:
CI = acc[T,H]acc[T,H]mono
(1)
We expect the correlation index of an optimal
ideal system (or the human goldstandard) to be
equal to 1, i.e. 100% accuracy on the monothe-
matic dataset should correspond to 100% accu-
racy on the original RTE dataset. For this reason,
we consider CI = 1 as the ideal correlation, and
we calculate the difference between such ideal CI
and the correlation obtained for a system S.
Given such expectations, CIS can assume three
different configurations with respect to the upper-
bound (i.e. the ideal correlation):
? CIS ?= 1 (ideal correlation): When CIS ap-
proaches to 1, the system shows high corre-
lation with the ideal behaviour assumed by
the compositionality principle. As a conse-
quence, we can predict that improving sin-
gle modules will correspondingly affect the
global performance.
? CIS < 1 (missing correlation): The system
is not able to exploit the ability in solving sin-
gle phenomena to correctly judge the origi-
nal RTE pairs. This may be due to the fact
that the system does not adopt suitable com-
bination mechanisms and loses the potential-
ity shown by its performances on monothe-
matic pairs.
? CIS > 1 (over correlation): The system does
not exploit the ability to solve single linguis-
tic components to solve the whole pairs, and
has different mechanisms to evaluate the en-
tailment. Probably, such a system is not in-
tended to be modularized.
Beside this ?global? correlation index calcu-
lated on the complete RTE data and on all the
monothematic pairs created from it, the CI can
also be calculated i) on categories of phenomena,
to verify which phenomena a system is more able
to solve both when isolated and when interacting
with other phenomena, e.g. :
CIlex =
acc[T,H]lex
acc[T,H]mono?lex
(2)
including in [T,H]lex all the pairs in which at
least one lexical phenomenon is present and con-
tribute to the entailment/contradiction judgments,
and in [T,H]mono?lex all the monothematic pairs
in which a lexical phenomenon is isolated; or ii)
on kind of judgment (entailment, contradiction,
unknown), allowing deeper qualitative analysis of
the performances of a system.
3.3 Deviation Index (DI)
We explained that a low CI (i.e. < 1) of a system
reflects the inability to correctly exploit the poten-
tially promising results obtained on monothematic
pairs to correctly judge RTE pairs. Actually, it
could also be the case that the system does not
perform a correct combination because even the
results got on the monothematic pairs were due to
chance (e.g. a word overlap system performs well
on monothematic pairs because of the high sim-
ilarity between T and H, and not because it has
linguistic strategies).
We detect such cases by decomposing the eval-
uation datasets, separating positive (i.e. entail-
ment) from negative (i.e. contradiction, unknown)
examples both in [T,H] and in [T,H]mono, and
102
independently run S on the new datasets. Then,
we have more fine grained evaluation patterns
through which we can analyze the system be-
haviour.
In the ideal case, we expect to have good cor-
relation between the accuracy obtained on the
monothematic pairs and the accuracy obtained on
the original ones (0 < CIpos ? 1 and 0 <
CIneg ? 1). On the contrary, we expect that sys-
tems either without a clear composition strategy or
without strong components on specific linguistic
phenomena (e.g. a word overlap system), would
show a significant difference of correlation on the
different datasets. More specifically, situations of
inverse correlation on the entailment and contra-
diction pairs (e.g. over correlation on contradic-
tion pairs and missing correlation on entailment
pairs) may reveal that the system itself is affected
by the nature of the dataset (i.e. its behaviour
is biased by the high similarity of [T,H]mono),
and weaknesses in the ability of solving phenom-
ena that more frequently contribute to the assign-
ment of a contradiction (or an entailment) judg-
ment come to light.
We formalize such intuition defining a Devia-
tion Index (DI) as the difference between the cor-
relation indexes, respectively, on entailment and
contradiction/unknown pairs, as follows:
|DI| = CIpos ? CIneg (3)
For instance, an high Deviation Index due to
a missing correlation on positive entailment pairs
and an over correlation for negative pairs, is in-
terpreted as an evidence that the system has low
accuracy on [T,H]mono - T and H are very sim-
ilar and the system has no strategies to under-
stand that the phenomenon that is present has to
be judged as contradictory -, and a higher accu-
racy on [T,H], probably due to chance. In the
ideal case DIS ?= 0, since we assumed the ideal
CIs on both positive and negative examples to be
as close as possible to 1 (see Section 3.2).
4 Experiments and discussion
This Section describes the experimental setup of
our pilot study, carried out using two systems that
took part in RTE-5 i.e EDITS and VENSES. We
show the results obtained and the qualitative anal-
ysis performed basing on the proposed evaluation
methodology. Their respective CIs and DIs are
compared with two baselines: a word overlap sys-
tem, and a system biased by the knowledge of
the probability that a linguistic phenomenon con-
tributes to the assignment of a certain entailment
judgment.
4.1 Dataset
The evaluation method has been tested on a
dataset composed of 60 pairs from RTE-5 test set
([T,H]RTE5?sample, composed of 30 entailment,
and 30 contradiction randomly extracted exam-
ples), and a dataset composed of all the monothe-
matic pairs derived by the first one following
the procedure described in Section 2. The sec-
ond dataset [T,H]RTE5?mono is composed of 167
pairs (135 entailment, 32 contradiction examples,
considering 35 different linguistic phenomena)2.
On average, 2.78 monothematic pairs have been
created from the original pairs. In this pilot study
we decided to limit our analysis to entailment and
contradiction pairs since, as observed in (Ben-
tivogli et al, 2010), in most of the unknown pairs
no linguistic phenomena relating T to H could be
detected.
4.2 TE systems
EDITS The EDITS system (Edit Distance Tex-
tual Entailment Suite)3 (Negri et al, 2009) as-
sumes that the distance between T and H is a
characteristic that separates the positive pairs, for
which entailment holds, from the negative pairs,
for which entailment does not hold (two way
task). It is based on edit distance algorithms, and
computes the [T,H] distance as the overall cost of
the edit operations (i.e. insertion, deletion and
substitution) required to transform T into H. For
our experiments we applied the model that pro-
duced EDITS best run at RTE-5 (acc. on test set:
60.2%). The main features are: Tree Edit Dis-
tance algorithm on the parsed trees of T and H,
Wikipedia lexical entailment rules, and PSO opti-
mized operation costs (Mehdad et al, 2009).
2http://hlt.fbk.eu/en/Technology/TE- Specialized Data
3Available as open source at http://edits.fbk.eu/
103
VENSES The other system used in our ex-
periments is VENSES4 (Delmonte et al, 2009),
that obtained performances similar to EDITS at
RTE-5 (acc. on test set: 61.5%). It applies a
linguistically-based approach for semantic infer-
ence, and is composed of two main components:
i) a grammatically-driven subsystem validates the
well-formedness of the predicate-argument struc-
ture and works on the output of a deep parser pro-
ducing augmented head-dependency structures;
and ii) a subsystem detects allowed logical and
lexical inferences basing on different kind of
structural transformations intended to produce a
semantically valid meaning correspondence. Also
in this case, we applied the best configuration of
the system used in RTE-5.
Baseline system 1: Word Overlap algorithm
The first baseline applies a Word Overlap (WO)
algorithm on tokenized text. The threshold to sep-
arate positive from negative pairs has been learnt
on the whole RTE-5 training dataset.
Baseline system 2: Linguistic biased system
The second baseline is produced by a more so-
phisticated but biased system. It exploits the
probability of linguistic phenomena to contribute
more to the assignment of a certain judgment than
to another. Such probabilities are learnt on the
[T,H]RTE5?mono goldstandard: given the list of
the phenomena with their frequency in monothe-
matic positive and negative pairs (columns 1,2,3
of Table 2), we calculate the probability P of phe-
nomenon i to appear in a positive (or in a negative)
pair as follows:
P (i|[T,H]positive) = #(i|[T,H]RTE5?positive?mono)#(i|[T,H]RTE5?mono)
(4)
For instance, if the phenomenon apposition ap-
pears in 11 monothematic positive pairs and in 6
negative pairs, it has a probability of 64.7% to ap-
pear in positive examples and 35.3% to appear in
negative ones. Such knowledge is then stored in
the system, and is used in the classification phase,
assigning the most probable judgment associated
to a certain phenomenon.
4http://project.cgm.unive.it/venses en.html
When applied to [T,H]RTE5?sample, this sys-
tem uses a simple combination strategy: if phe-
nomena associated with different judgments are
present in a pair, and one phenomenon is associ-
ated with a contradiction judgment with a proba-
bility > 50%, the pair is marked as contradiction,
otherwise it is marked as entailment.
4.3 Results
Following the methodology described in Sec-
tion 3, at step 1 we run EDITS and VENSES
on [T,H]RTE5?sample, and on [T,H]RTE5?mono
(Table 3 reports the accuracies obtained).
At step 2, we calculate the accuracy of ED-
ITS and VENSES on each single linguistic phe-
nomenon, and on categories of phenomena. Ta-
ble 2 shows the distribution of the phenomena on
the dataset, reflected in the number of positive and
negative monothematic pairs created for each phe-
nomenon. As can be seen, some phenomena ap-
pear more frequently than others (e.g. corefer-
ence, general inference). Furthermore, some lin-
guistic phenomena allow only the creation of pos-
itive or negative examples, while others can con-
tribute to the assignment of both judgments. Due
to the small datasets we used, some phenomena
appear rarely; the accuracy on them cannot be
considered completely reliable.
Nevertheless, from these data the main features
of the systems can be identified. For instance,
EDITS obtains the highest accuracy on positive
monothematic pairs, while it seems it has no pe-
culiar strategies to deal with phenomena caus-
ing contradiction (e.g. semantic opposition, and
quantity mismatching). On the contrary, VENSES
shows an opposite behaviour, obtaining the best
results on the negative cases.
At step 3 of the proposed evaluation methodol-
ogy, we calculate the correlation index between
the ability of the system to correctly judge the
monothematic pairs of [T,H]RTE5?mono with re-
spect to the ability to correctly judge the original
ones in [T,H]RTE5?sample.
Table 3 compares EDITS and VENSES CI with
the two baseline systems described before. As can
be noticed, even if EDITS CI outperforms theWO
system, they show a similar behaviour (high ac-
curacy on monothematic pairs, and much lower
104
phenomena # [T,H] EDITS VENSES
RTE5?mono % acc. % acc.
pos. neg. pos. neg. pos. neg.
lex:identity 1 3 100 0 100 33.3
lex:format 2 - 100 - 100 -
lex:acronymy 3 - 100 - 33.3 -
lex:demonymy 1 - 100 - 100 -
lex:synonymy 11 - 90.9 - 90.9 -
lex:semantic-opp. - 3 - 0 - 100
lex:hypernymy 3 - 100 - 66.6 -
lex:geo-knowledge 1 - 100 - 100 -
TOT lexical 22 6 95.4 0 77.2 66.6
lexsynt:transp-head 2 - 100 - 50 -
lexsynt:verb-nom. 8 - 87.5 - 25 -
lexsynt:causative 1 - 100 - 100 -
lexsynt:paraphrase 3 - 100 - 66.6 -
TOT lex-syntactic 14 - 92.8 - 42.8 -
synt:negation - 1 - 0 - 0
synt:modifier 3 1 100 0 33.3 100
synt:arg-realization 5 - 100 - 40 -
synt:apposition 11 6 100 33.3 54.5 83.3
synt:list 1 - 100 - 100 -
synt:coordination 3 - 100 - 33.3 -
synt:actpass-altern. 4 2 100 0 25 50
TOT syntactic 28 9 96.4 22.2 42.8 77.7
disc:coreference 20 - 95 - 50 -
disc:apposition 3 - 100 - 0 -
disc:anaphora-zero 5 - 80 - 20 -
disc:ellipsis 4 - 100 - 25 -
disc:statements 1 - 100 - 0 -
TOT discourse 33 - 93.9 - 36.3 -
reas:apposition 2 1 100 0 50 100
reas:modifier 3 - 66.6 - 100 -
reas:genitive 1 - 100 - 100 -
reas:relative-clause 1 - 100 - 0 -
reas:elliptic-expr. 1 - 100 - 0 -
reas:meronymy 1 1 100 0 100 0
reas:metonymy 3 - 100 - 33.3 -
reas:representat. 1 - 100 - 0 -
reas:quantity - 5 - 0 - 80
reas:spatial 1 - 100 - 0 -
reas:gen-inference 24 10 87.5 50 37.5 90
TOT reasoning 38 17 89.4 35.2 42.1 82.3
TOT (all phenom) 135 32 93.3 25 45.9 81.2
Table 2: Systems? accuracy on phenomena
on the RTE sample). According to our defini-
tion, their CIs (0 < CI < 1) show a good ability
of the systems to deal with linguistic phenomena
when isolated, but a scarce ability in combining
them to assign the final judgment. EDITS CI is
not far from the CI of the linguistic biased base-
line system, even if we were expecting a higher
CI for the latter system. The reason is that beside
the linguistic phenomena that allow only the cre-
ation of negative monothematic pairs, all the phe-
nomena that allow both judgments have a higher
probability to contribute to the creation of positive
monothematic pairs.
Comparing the CI of the four analyzed systems
with the ideal correlation (CIS ?= 1, see Section
3.2), VENSES is the closest one (? = 0.15), even
if it shows a light over correlation (probably due
to the nature of the dataset). The second closest
acc. % acc. % CI
RTE5?sample RTE5?mono
EDITS 58.3 80.8 0.72
VENSES 60 52.6 1.15
Word Overlap 38.3 77.24 0.49
ling baseline 68.3 86.8 0.79
Table 3: Evaluation on RTE pairs and on
monothematic pairs
categories of linguistic phenomena
RTE5 data lex. lex-synt. synt. disc. reas.
EDITS sample 47.8 64.3 51.7 75 62.5
mono 75 92.8 78.3 93.9 72.7
CI 0.63 0.69 0.66 0.79 0. 85
VENSES sample 47.2 42.8 62 46.4 67.5
mono 75 42.8 51.3 33 54.5
CI 0.62 1 1.2 1.4 1.23
WO sample 36.3 57.1 34.4 50 35
baseline mono 78.5 71.4 72.9 96.9 69
CI 0.46 0.79 0.47 0.51 0.5
ling- sample 82.6 92.8 58.6 82.1 70
biased mono 96.4 100 75.6 96.9 80
baseline CI 0.85 0.92 0.77 0.84 0.87
Table 4: Evaluation on categories of phenomena
one is the linguistic biased system (? = 0.21),
showing that the knowledge of the most probable
judgment assigned to a certain phenomenon can
be a useful information.
Table 4 reports an evaluation of the four sys-
tems on categories of linguistic phenomena.
To check if the same CI is maintained over
both entailment and contradiction pairs, we cal-
culate a Deviation Index as the difference be-
tween the CIs on entailment and on contradiction
pairs (step 4 of our methodology). As described
in Section 3, we created four datasets dividing
both [T,H]RTE5?sample and [T,H]RTE5?mono
into positive (i.e. entailment) and negative (i.e.
contradiction) examples. We run EDITS and
VENSES on the datasets and we calculate the
CI on positive and on negative examples sepa-
rately. If we obtained missing correlation be-
tween the accuracy on the monothematic pairs
and the accuracy on RTE original ones, it would
mean that the potentiality that the systems show
on monothematic pairs is not exploited to cor-
rectly judge more complex pairs, therefore com-
positional mechanisms should be improved.
Table 5 shows that the DIs of the linguistic bi-
ased system and of VENSES are close to the ideal
case (DIS ?= 0), indicating a good capacity to
correctly differentiate entailment from contradic-
tion cases. EDITS results demonstrate that the
105
Figure 1: Correlation Index on entailment and contradiction pairs for EDITS and VENSES
% acc. RTE5 % acc. RTE5 CI DI
sample mono
EDITS E 83.3 94.7 0.88 0.5
C 33.3 24 1.38
VENSES E 50 47.01 1.08 0.16
C 70 75.7 0.92
WO E 50 88 0.56 0.24
baseline C 26.6 33 0.80
ling-biased E 96.6 98.5 0.98 0.03
baseline C 40 39.4 1.01
Table 5: Evaluation on entail. and contr. pairs
shallow approach implemented by the system has
no strategies to correctly judge negative examples
(similarly to the WO system), therefore should be
mainly improved with this respect.
We also calculated the CI for every pair of the
dataset, putting into relation each original pair
with all the monothematic pairs derived from it.
Figure 1 shows EDITS and VENSES?s CI on each
pair of our sample.5 Even if the systems obtained
similar performances in the challenge, the second
system seems to behave in an opposite way with
respect to EDITS, showing higher CI for negative
cases than for the positive ones.
5The ideal case CI=1 corresponds to 0 on the logarithmic
scale.
5 Conclusion and Future work
We have proposed a methodology for the evalu-
ation of TE systems based on the analysis of the
system behaviour on monothematic pairs with re-
spect to the behaviour on corresponding original
pairs. Through the definition of two indicators,
a Correlation Index and a Deviation Index, we
infer evaluation patterns which indicate strength
and weaknesses of the system. As a pilot study,
we have compared two systems that took part in
RTE-5. We discovered that, although the two sys-
tems have similar accuracy on RTE-5 datasets,
they show significant differences in their respec-
tive abilities to manage different linguistic phe-
nomena and to properly combine them. We hope
that the analysis provided by our methodology
may bring interesting elements both to TE system
developers and for deep discussion on the nature
of TE itself.
As future work, we plan to refine the evaluation
methodology introducing the possibility to assign
different relevance to the phenomena.
6 Acknowledgements
Thanks to Professor Rodolfo Delmonte and to
Sara Tonelli for running the VENSES system on
our data sets.
106
References
Bentivogli, Luisa, Bernardo Magnini, Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The Fifth PASCAL Recognizing Textual Entailment
Challenge. Proceedings of the TAC 2009 Workshop
on Textual Entailment. Gaithersburg, Maryland. 17
November.
Bentivogli, Luisa, Elena Cabrio, Ido Dagan,
Danilo Giampiccolo, Medea Lo Leggio, and
Bernardo Magnini. 2010. Building Textual En-
tailment Specialized Data Sets: a Methodology
for Isolating Linguistic Phenomena Relevant to
Inference. Proceedings of the 7th International
Conference on Language Resources and Evaluation
(LREC) . Valletta, Malta. 19-21 May.
Dagan, Ido, Bill Dolan, Bernardo Magnini, and
Dan Roth. 2009. Recognizing textual entailment:
Rational, evaluation and approaches. Natural Lan-
guage Engineering (JNLE), Volume 15, Special Is-
sue 04, October 2009, pp i-xvii. Cambridge Univer-
sity Press.
Delmonte, Rodolfo, Sara Tonelli, Rocco Tripodi.
2009. Semantic Processing for Text Entailment
with VENSES. Proceedings of the TAC 2009 Work-
shop on Textual Entailment. To appear. Gaithers-
burg, Maryland. 17 November.
Garoufi, Konstantina. 2007. Towards a Better Un-
derstanding of Applied Textual Entailment. Mas-
ter Thesis. Saarland University. Saarbru?cken, Ger-
many.
Gottlob, Frege. 1892. U?ber Sinn und Bedeutung.
Zeitschrift fu?r Philosophie und philosophische Kri-
tik. 100.25-50.
Magnini, Bernardo, and Elena Cabrio. 2009. Combin-
ing Specialized Entailment Engines. Proceedings of
the 4th Language & Technology Conference (LTC
?09). Poznan, Poland. 6-8 November.
Mehdad, Yashar, Matteo Negri, Elena Cabrio,
Milen Kouylekov, and Bernardo Magnini. 2009.
Using Lexical Resources in a Distance-Based Ap-
proach to RTE. Proceedings of the TAC 2009 Work-
shop on Textual Entailment. Gaithersburg, Mary-
land. 17 November.
Negri, Matteo, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards
Extensible Textual Entailment Engines: The EDITS
Package. AI*IA 2009: Emergent Perspectives in
Artificial Intelligence, Lecture Notes in Computer
Science. Volume 5883. ISBN 978-3-642-10290-5.
Springer-Verlag Berlin Heidelberg, p. 314.
Nielsen, Rodney D., Wayne Ward, and James H. Mar-
tin. 2009. Recognizing entailment in intelligent tu-
toring systems. In Ido Dagan, Bill Dolan, Bernardo
Magnini and Dan Roth (Eds.) The Journal of Nat-
ural Language Engineering, (JNLE). , 15, pp 479-
501. Copyright Cambridge University Press, Cam-
bridge, United Kingdom.
Romano, Lorenza, Milen Ognianov Kouylekov,
Idan Szpektor, Ido Kalman Dagan, and Al-
berto Lavelli, 2006. Investigating a Generic
Paraphrase-Based Approach for Relation Extrac-
tion. Proceedings of the 11th Conference of the
European Chapter of the Association for Computa-
tional Linguistics (EACL 2006). Trento, Italy. 3-7
April.
107
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 43?48,
Baltimore, Maryland USA, June 23-24, 2014.
c?2014 Association for Computational Linguistics
The Excitement Open Platform for Textual Inferences
Bernardo Magnini
?
, Roberto Zanoli
?
, Ido Dagan
?
, Kathrin Eichler
?
, G?unter Neumann
?
,
Tae-Gil Noh
?
, Sebastian Pado
?
, Asher Stern
?
, Omer Levy
?
?
FBK (magnini|zanoli@fbk.eu)
?
Heidelberg, Stuttgart Univ. (pado|noh@cl.uni-heidelberg.de)
?
DFKI (neumann|eichler@dfki.de)
?
Bar Ilan University (dagan|sterna3|omerlevy@cs.biu.ac.il)
Abstract
This paper presents the Excitement Open
Platform (EOP), a generic architecture and
a comprehensive implementation for tex-
tual inference in multiple languages. The
platform includes state-of-art algorithms,
a large number of knowledge resources,
and facilities for experimenting and test-
ing innovative approaches. The EOP is
distributed as an open source software.
1 Introduction
In the last decade textual entailment (Dagan et al.,
2009) has been a very active topic in Computa-
tional Linguistics, providing a unifying framework
for textual inference. Several evaluation exercises
have been organized around Recognizing Textual
Entailment (RTE) challenges and many method-
ologies, algorithms and knowledge resources have
been proposed to address the task. However, re-
search in textual entailment is still fragmented and
there is no unifying algorithmic framework nor
software architecture.
In this paper, we present the Excitement Open
Platform (EOP), a generic architecture and a com-
prehensive implementation for multilingual textual
inference which we make available to the scien-
tific and technological communities. To a large
extent, the idea is to follow the successful experi-
ence of the Moses open source platform (Koehn et
al., 2007) in Machine Translation, which has made
a substantial impact on research in that field. The
EOP is the result of a two-year coordinated work
under the international project EXCITEMENT.
1
A
consortium of four academic partners has defined
the EOP architectural specifications, implemented
the functional interfaces of the EOP components,
imported existing entailment engines into the EOP
1
http://www.excitement-project.eu
and finally designed and implemented a rich envi-
ronment to support open source distribution.
The goal of the platform is to provide function-
ality for the automatic identification of entailment
relations among texts. The EOP is based on a modu-
lar architecture with a particular focus on language-
independent algorithms. It allows developers and
users to combine linguistic pipelines, entailment al-
gorithms and linguistic resources within and across
languages with as little effort as possible. For ex-
ample, different entailment decision approaches
can share the same resources and the same sub-
components in the platform. A classification-based
algorithm can use the distance component of an
edit-distance based entailment decision approach,
and two different approaches can use the same set
of knowledge resources. Moreover, the platform
has various multilingual components for languages
like English, German and Italian. The result is an
ideal software environment for experimenting and
testing innovative approaches for textual inferences.
The EOP is distributed as an open source software
2
and its use is open both to users interested in using
inference in applications and to developers willing
to extend the current functionalities.
The paper is structured as follows. Section 2
presents the platform architecture, highlighting
how the EOP component-based approach favors
interoperability. Section 3 provides a picture of
the current population of the EOP in terms of both
entailment algorithms and knowledge resources.
Section 4 introduces expected use cases of the plat-
form. Finally, Section 5 presents the main features
of the open source package.
2 Architecture
The EOP platform takes as input two text portions,
the first called the Text (abbreviated with T), the
second called the Hypothesis (abbreviated with H).
2
http://hltfbk.github.io/
Excitement-Open-Platform/
43
Linguis'c)Analysis)Pipeline)(LAP))
Entailment)Core)(EC))
Entailment)Decision))Algorithm)(EDA))
Dynamic)and)Sta'c)Components)(Algorithms)and)Knowledge))
Linguis'c)Analysis)Components)
Decision)
1)
Raw)Data)
Figure 1: EOP architecture
The output is an entailment judgement, either ?En-
tailment? if T entails H, or ?NonEntailment? if the
relation does not hold. A confidence score for the
decision is also returned in both cases.
The EOP architecture (Pad?o et al., 2014) is based
on the concept of modularization with pluggable
and replaceable components to enable extension
and customization. The overall structure is shown
in Figure 1 and consists of two main parts. The
Linguistic Analysis Pipeline (LAP) is a series of
linguistic annotation components. The Entailment
Core (EC) performs the actual entailment recog-
nition. This separation ensures that (a) the com-
ponents in the EC only rely on linguistic analysis
in well-defined ways and (b) the LAP and EC can
be run independently of each other. Configuration
files are the principal means of configuring the EOP.
In the rest of this section we first provide an intro-
duction to the LAP, then we move to the EC and
finally describe the configuration files.
2.1 Linguistic Analysis Pipeline (LAP)
The Linguistic Analysis Pipeline is a collection of
annotation components for Natural Language Pro-
cessing (NLP) based on the Apache UIMA frame-
work.
3
Annotations range from tokenization to
part of speech tagging, chunking, Named Entity
Recognition and parsing. The adoption of UIMA
enables interoperability among components (e.g.,
substitution of one parser by another one) while
ensuring language independence. Input and output
of the components are represented in an extended
version of the DKPro type system based on UIMA
3
http://uima.apache.org/
Common Analysis Structure (CAS) (Gurevych et
al., 2007; Noh and Pad?o, 2013).
2.2 Entailment Core (EC)
The Entailment Core performs the actual entail-
ment recognition based on the preprocessed text
made by the Linguistic Analysis Pipeline. It con-
sists of one or more Entailment Decision Algo-
rithms (EDAs) and zero or more subordinate com-
ponents. An EDA takes an entailment decision
(i.e., ?entailment? or ?no entailment?) while com-
ponents provide static and dynamic information for
the EDA.
Entailment Decision Algorithms are at the top
level in the EC. They compute an entailment deci-
sion for a given Text/Hypothesis (T/H) pair, and
can use components that provide standardized al-
gorithms or knowledge resources. The EOP ships
with several EDAs (cf. Section 3).
Scoring Components accept a Text/Hypothesis
pair as an input, and return a vector of scores.
Their output can be used directly to build minimal
classifier-based EDAs forming complete RTE sys-
tems. An extended version of these components are
the Distance Components that can produce normal-
ized and unnormalized distance/similarity values
in addition to the score vector.
Annotation Components can be used to add dif-
ferent annotations to the Text/Hypothesis pairs. An
example of such a type of component is one that
produces word or phrase alignments between the
Text and the Hypothesis.
Lexical Knowledge Components describe se-
mantic relationships between words. In the
EOP, this knowledge is represented as directed
rules made up of two word?POS pairs, where
the LHS (left-hand side) entails the RHS (right-
hand side), e.g., (shooting star,Noun) =?
(meteorite,Noun). Lexical Knowledge Compo-
nents provide an interface that allows for (a) listing
all RHS for a given LHS; (b) listing all LHS for
a given RHS; and (c) checking for an entailment
relation for a given LHS?RHS pair. The interface
also wraps all major lexical knowledge sources cur-
rently used in RTE research, including manually
constructed ontologies like WordNet, and encyclo-
pedic resources like Wikipedia.
Syntactic Knowledge Components capture en-
tailment relationships between syntactic and
44
lexical-syntactic expressions. We represent such
relationships by entailment rules that link (option-
ally lexicalized) dependency tree fragments that
can contain variables as nodes. For example, the
rule fall of X =? X falls, or X sells Y to Z =?
Z buys Y from X express general paraphrasing pat-
terns at the predicate-argument level that cannot be
captured by purely lexical rules. Formally, each
syntactic rule consists of two dependency tree frag-
ments plus a mapping from the variables of the
LHS tree to the variables of the RHS tree.
4
2.3 Configuration Files
The EC components can be combined into actual
inference engines through configuration files which
contain information to build a complete inference
engine. A configuration file completely describes
an experiment. For example, it specifies the re-
sources that the selected EDA has to use and the
data set to be analysed. The LAP needed for data
set preprocessing is another parameter that can be
configured too. The platform ships with a set of
predefined configuration files accompanied by sup-
porting documentation.
3 Entailment Algorithms and Resources
This section provides a description of the Entail-
ment Algorithms and Knowledge Resources that
are distributed with the EOP.
3.1 Entailment Algorithms
The current version of the EOP platform ships with
three EDAs corresponding to three different ap-
proaches to RTE: an EDA based on transformations
between T and H, an EDA based on edit distance
algorithms, and a classification based EDA using
features extracted from T and H.
Transformation-based EDA applies a sequence
of transformations on T with the goal of making
it identical to H. If each transformation preserves
(fully or partially) the meaning of the original text,
then it can be concluded that the modified text
(which is actually the Hypothesis) can be inferred
from the original one. Consider the following sim-
ple example where the text is ?The boy was located
by the police? and the Hypothesis is ?The child
was found by the police?. Two transformations for
?boy? ? ?child? and ?located? ? ?found? do the
job.
4
Variables of the LHS may also map to null, when material
of the LHS must be present but is deleted in the inference step.
In the EOP we include a transformation based
inference system that adopts the knowledge based
transformations of Bar-Haim et al. (2007), while in-
corporating a probabilistic model to estimate trans-
formation confidences. In addition, it includes a
search algorithm which finds an optimal sequence
of transformations for any given T/H pair (Stern et
al., 2012).
Edit distance EDA involves using algorithms
casting textual entailment as the problem of map-
ping the whole content of T into the content of H.
Mappings are performed as sequences of editing
operations (i.e., insertion, deletion and substitu-
tion) on text portions needed to transform T into H,
where each edit operation has a cost associated with
it. The underlying intuition is that the probability
of an entailment relation between T and H is related
to the distance between them; see Kouylekov and
Magnini (2005) for a comprehensive experimental
study.
Classification based EDA uses a Maximum En-
tropy classifier to combine the outcomes of sev-
eral scoring functions and to learn a classification
model for recognizing entailment. The scoring
functions extract a number of features at various
linguistic levels (bag-of-words, syntactic dependen-
cies, semantic dependencies, named entities). The
approach was thoroughly described in Wang and
Neumann (2007).
3.2 Knowledge Resources
As described in Section 2.2, knowledge resources
are crucial to recognize cases where T and H use
different textual expressions (words, phrases) while
preserving entailment. The EOP platform includes
a wide range of knowledge resources, including lex-
ical and syntactic resources, where some of them
are grabbed from manual resources, like dictionar-
ies, while others are learned automatically. Many
EOP resources are inherited from pre-existing RTE
systems migrated into the EOP platform, but now
use the same interfaces, which makes them acces-
sible in a uniform fashion.
There are about two dozen lexical (e.g. word-
nets) and syntactic resources for three languages
(i.e. English, Italian and German). However,
since there is still a clear predominance of En-
glish resources, the platform includes lexical and
syntactic knowledge mining tools to bootstrap re-
sources from corpora, both for other languages and
45
EDA Accuracy / F1
Transformation-based English RTE-3 67.13%
Transformation-based English RTE-6 49.55%
Edit-Distance English RTE-3 64.38%
Edit-Distance German RTE-3 59.88%
Edit-Distance Italian RTE-3 63.50%
Classification-based English RTE-3 65.25%
Classification-based German RTE-3 63.75%
Median of RTE-3 (English) submissions 61.75%
Median of RTE-6 (English) submissions 33.72%
Table 1: EDAs results
for specific domains. Particularly, the EOP plat-
form includes a language independent tool to build
Wikipedia resources (Shnarch et al., 2009), as well
as a language-independent framework for building
distributional similarity resources like DIRT (Lin
and Pantel, 2002) and Lin similarity(Lin, 1998).
3.3 EOP Evaluation
Results for the three EDAs included in the EOP
platform are reported in Table 1. Each line rep-
resents an EDA, the language and the dataset
on which the EDA was evaluated. For brevity,
we omit here the knowledge resources used for
each EDA, even though knowledge configuration
clearly affects performance. The evaluations were
performed on RTE-3 dataset (Giampiccolo et al.,
2007), where the goal is to maximize accuracy. We
(manually) translated it to German and Italian for
evaluations: in both cases the results fix a refer-
ence for the two languages. The two new datasets
for German and English are available both as part
of the EOP distribution and independently
5
. The
transformation-based EDA was also evaluated on
RTE-6 dataset (Bentivogli et al., 2010), in which
the goal is to maximize the F1 measure.
The results of the included EDAs are higher than
median values of participated systems in RTE-3,
and they are competing with state-of-the-arts in
RTE-6 results. To the best of our knowledge, the
results of the EDAs as provided in the platform are
the highest among those available as open source
systems for the community.
4 Use Cases
We see four primary use cases for the EOP. Their
requirements were reflected in our design choices.
Use Case 1: Applied Textual Entailment. This
category covers users who are not interested in the
5
http://www.excitement-project.eu/
index.php/results
details of RTE but who are interested in an NLP
task in which textual entailment can take over part
of or all of the semantic processing, such as Ques-
tion Answering or Intelligent Tutoring. Such users
require a system that is as easy to deploy as possi-
ble, which motivates our offer of the EOP platform
as a library. They also require a system that pro-
vides good quality at a reasonable efficiency as
well as guidance as to the best choice of parame-
ters. The latter point is realized through our results
archive in the official EOP Wiki on the EOP site.
Use Case 2: Textual Entailment Development.
This category covers researchers who are interested
in Recognizing Textual Entailment itself, for exam-
ple with the goal of developing novel algorithms
for detecting entailment. In contrast to the first
category, this group need to look ?under the hood?
of the EOP platform and access the source code of
the EOP. For this reason, we have spent substantial
effort to provide the code in a well-structured and
well-documented form.
A subclass of this group is formed by researchers
who want to set up a RTE infrastructure for lan-
guages in which it does not yet exist (that is, al-
most all languages). The requirements of this class
of users comprises clearly specified procedures to
replace the Linguistic Analysis Pipeline, which are
covered in our documentation, and simple methods
to acquire knowledge resources for these languages
(assuming that the EDAs themselves are largely
language-independent). These are provided by the
language-independent knowledge acquisition tools
which we offer alongside the platform (cf. Section
3.2).
Use Case 3: Lexical Semantics Evaluation. A
third category consists of researchers whose pri-
mary interest is in (lexical) semantics.
As long as their scientific results can be phrased
in terms of semantic similarities or inference rules,
the EOP platform can be used as a simple and stan-
dardized workbench for these results that indicates
the impact that the semantic knowledge under con-
sideration has on deciding textual entailment. The
main requirement for this user group is the simple
integration of new knowledge resources into the
EOP platform. This is catered for through the defi-
nition of the generic knowledge component inter-
faces (cf. Section 2.2) and detailed documentation
on how to implement these interfaces.
46
Use Case 4: Educational Use. The fourth and
final use case is as an educational tool to support
academic courses and projects on Recognizing Tex-
tual Entailment and inference more generally. This
use case calls, in common with the others, for easy
usability and flexibility. Specifically for this use
case, we have also developed a series of tutorials
aimed at acquainting new users with the EOP plat-
form through a series of increasingly complexity
exercises that cover all areas of the EOP. We are
also posting proposals for projects to extend the
EOP on the EOP Wiki.
5 EOP Distribution
The EOP infrastructure follows state-of-the-art soft-
ware engineering standards to support both users
and developers with a flexible, scalable and easy to
use software environment. In addition to communi-
cation channels, like the mailing list and the issue
tracking system, the EOP infrastructure comprises
the following set of facilities.
Version Control System: We use GitHub,
6
a
web-based hosting service for code and documen-
tation storage, development, and issue tracking.
Web Site: The GitHub Automatic Page Genera-
tor was used to build the EOP web site and Wiki,
containing a general introduction to the software
platform, the terms of its license, mailing lists to
contact the EOP members and links to the code
releases.
Documentation: Both user and developer docu-
mentation is available from Wiki pages; the pages
are written with the GitHub Wiki Editor and hosted
on the GitHub repository. The documentation in-
cludes a Quick Start guide to start using the EOP
platform right away, and a detailed step by step
tutorial.
Results Archive: As a new feature for commu-
nity building, EOP users can, and are encouraged
to, share their results: the platform configuration
files used to produce results as well as contact infor-
mation can be saved and archived into a dedicated
page on the EOP GitHub repository. That allows
other EOP users to replicate experiments under
the same condition and/or avoid doing experiments
that have already been done.
6
https://github.com/
Build Automation Tool: The EOP has been de-
veloped as a Maven
7
multi-modules project, with
all modules sharing the same Maven standard struc-
ture, making it easier to find files in the project once
one is used to Maven.
Maven Artifacts Repository: Using a Maven
repository has a twofold goal: (i) to serve as an
internal private repository of all software libraries
used within the project (libraries are binary files
and should not be stored under version control sys-
tems, which are intended to be used with text files);
(ii) to make the produced EOP Maven artifacts
available (i.e., for users who want to use the EOP
as a library in their own code). We use Artifactory
8
repository manager to store produced artifacts.
Continuous Integration: The EOP uses Jenk-
ins
9
for Continuous Integration, a software develop-
ment practice where developers of a team integrate
their work frequently (e.g., daily).
Code Quality Tool: Ensuring the quality of the
produced software is one of the most important
aspects of software engineering. The EOP uses
tools like PMD
10
that can automatically be run
during development to help the developers check
the quality of their software.
5.1 Project Repository
The EOP Java source code is hosted on the EOP
Github repository and managed using Git. The
repository consists of three main branches: the
release branch contains the code that is supposed to
be in a production-ready state, whereas the master
branch contains the code to be incorporated into the
next release. When the source code in the master
branch reaches a stable point and is ready to be
released, all of the changes are merged back into
release. Finally, the gh-pages branch contains the
web site pages.
5.2 Licensing
The software of the platform is released under the
terms of General Public License (GPL) version
3.
11
The platform contains both components and
resources designed by the EOP developers, as well
as others that are well known and freely available
7
http://maven.apache.org/
8
http://www.jfrog.com/
9
http://jenkins-ci.org/
10
http://pmd.sourceforge.net
11
http://www.gnu.org/licenses/gpl.html
47
in the NLP research community. Additional com-
ponents and resources whose license is not compat-
ible with the EOP license have to be downloaded
and installed separately by the user.
6 Conclusion
This paper has presented the main characteristics
of Excitement Open Platform platform, a rich envi-
ronment for experimenting and evaluating textual
entailment systems. On the software side, the EOP
is a complex endeavor to integrate tools and re-
sources in Computational Linguistics, including
pipelines for three languages, three pre-existing
entailment engines, and about two dozens of lex-
ical and syntactic resources. The EOP assumes a
clear and modular separation between linguistic
annotations, entailment algorithms and knowledge
resources which are used by the algorithms. A
relevant benefit of the architectural design is that
a high level of interoperability is reached, provid-
ing a stimulating environment for new research in
textual inferences.
The EOP platform has been already tested in sev-
eral pilot research projects and educational courses,
and it is currently distributed as open source soft-
ware under the GPL-3 license. To the best of our
knowledge, the entailment systems and their con-
figurations provided in the platform are the best
systems available as open source for the commu-
nity. As for the future, we are planning several
initiatives for the promotion of the platform in the
research community, as well as its active experi-
mentation in real application scenarios.
Acknowledgments
This work was partially supported by the EC-
funded project EXCITEMENT (FP7ICT-287923).
References
Roy Bar-Haim, Ido Dagan, Iddo Greental, and Eyal
Shnarch. 2007. Semantic inference at the lexical-
syntactic level. In Proceedings of AAAI, pages 871?
876, Vancouver, BC.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang
Dang, and Danilo Giampiccolo. 2010. The Sixth
PASCAL Recognizing Textual Entailment Chal-
lenge. In Proceedings of TAC, Gaithersburg, MD.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: Ratio-
nal, evaluation and approaches. Journal of Natural
Language Engineering, 15(4):i?xvii.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The Third PASCAL Recog-
nising Textual Entailment Challenge. In Proceed-
ings of the ACL-PASCAL Workshop on Textual En-
tailment and Paraphrasing, Prague, Czech Repub-
lic.
Iryna Gurevych, Max M?uhlh?auser, Christof M?uller,
J?urgen Steimle, Markus Weimer, and Torsten Zesch.
2007. Darmstadt knowledge processing repository
based on UIMA. In Proceedings of the First Work-
shop on Unstructured Information Management Ar-
chitecture (UIMA@GSCL 2007), T?ubingen, Ger-
many.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the ACL demo session, pages 177?
180, Prague, Czech Republic.
Milen Kouylekov and Bernardo Magnini. 2005. Rec-
ognizing textual entailment with tree edit distance al-
gorithms. In Proceedings of the First PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 17?20, Southampton, UK.
Dekang Lin and Patrick Pantel. 2002. Discovery of
Inference Rules for Question Answering. Journal of
Natural Language Engineering, 7(4):343?360.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of ACL/COLING,
pages 768?774, Montr?eal, Canada.
Tae-Gil Noh and Sebastian Pad?o. 2013. Using
UIMA to structure an open platform for textual en-
tailment. In Proceedings of the 3rd Workshop on
Unstructured Information Management Architecture
(UIMA@GSCL 2013).
Sebastian Pad?o, Tae-Gil Noh, Asher Stern, Rui Wang,
and Roberto Zanoli. 2014. Design and realiza-
tion of a modular architecture for textual entailment.
Journal of Natural Language Engineering. doi:
10.1017/S1351324913000351.
Eyal Shnarch, Libby Barak, and Ido Dagan. 2009. Ex-
tracting lexical reference rules from Wikipedia. In
Proceedings of ACL-IJCNLP, pages 450?458, Sin-
gapore.
Asher Stern, Roni Stern, Ido Dagan, and Ariel Felner.
2012. Efficient search for transformation-based in-
ference. In Proceedings of ACL, pages 283?291,
Jeju Island, South Korea.
Rui Wang and G?unter Neumann. 2007. Recogniz-
ing textual entailment using a subsequence kernel
method. In Proceedings of AAAI, pages 937?945,
Vancouver, BC.
48
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, pages 86?94,
Uppsala, July 2010.
Contradiction-Focused Qualitative Evaluation of Textual Entailment
Bernardo Magnini
FBK-Irst
Trento, Italy
magnini@fbk.eu
Elena Cabrio
FBK-Irst, University of Trento
Trento, Italy
cabrio@fbk.eu
Abstract
In this paper we investigate the rela-
tion between positive and negative pairs
in Textual Entailment (TE), in order to
highlight the role of contradiction in TE
datasets. We base our analysis on the de-
composition of Text-Hypothesis pairs into
monothematic pairs, i.e. pairs where only
one linguistic phenomenon at a time is re-
sponsible for entailment judgment and we
argue that such a deeper inspection of the
linguistic phenomena behind textual en-
tailment is necessary in order to highlight
the role of contradiction. We support our
analysis with a number of empirical ex-
periments, which use current available TE
systems.
1 Introduction
Textual Entailment (TE) (Dagan et al, 2009) pro-
vides a powerful and general framework for ap-
plied semantics. TE has been exploited in a series
of evaluation campaigns (RTE - Recognizing Tex-
tual Entailment) (Bentivogli et al, 2009), where
systems are asked to automatically judge whether
the meaning of a portion of text, referred as Text
(T), entails the meaning of another text, referred
as Hypothesis (H).
RTE datasets have been mainly built with the
purpose of showing the applicability of the TE
framework to different semantic applications in
Computational Linguistics. Starting from 2005,
[T,H] pairs were created including samples from
summarization, question answering, information
extraction, and other applications. This evaluation
provides useful cues for researchers and develop-
ers aiming at the integration of TE components in
larger applications (see, for instance, the use of a
TE engine for question answering in the QALL-
ME project system
1
, the use in relation extraction
(Romano et al, 2006), and in reading comprehen-
sion systems (Nielsen et al, 2009)).
Although the RTE evaluations showed pro-
gresses in TE technologies, we think that there is
still large room for improving qualitative analysis
of both the RTE datasets and the system results. In
particular, we intend to focus this paper on contra-
diction judgments and on a deep inspection of the
linguistic phenomena that determine such judg-
ments. More specifically, we address two distin-
guishing aspects of TE: (i) the variety of linguis-
tic phenomena that are relevant for contradiction
and how their distribution is represented in RTE
datasets; (ii) the fact that in TE it is not enough to
detect the polarity of a sentence, as in traditional
semantic analysis, but rather it is necessary to ana-
lyze the dependencies between two sentences (i.e.
the [T,H] pair) in order to establish whether a con-
tradiction holds between the pair. Under this re-
spect we are interested to investigate both how
polarity among Text and Hypothesis affects the
entailment/contradiction judgments and how dif-
ferent linguistic phenomena interact with polarity
(e.g. whether specific combinations of phenomena
are more frequent than others).
As an example, let us consider the pair:
T: Mexico?s new president, Felipe Calderon, seems to be
doing all the right things in cracking down on Mexico?s drug
traffickers.[...]
H: Felipe Calderon is the outgoing President of Mexico.
In order to detect the correct contradiction judg-
ment between T and H it is necessary to solve the
semantic inference that being the new President of
a country is not compatible with being the outgo-
ing President of the same country. This kind of
inference requires that (i) the semantic opposition
is detected, and that (ii) such opposition is consid-
1
http://qallme.fbk.eu/
86
Text snippet (pair 125) Phenomena Judg.
T Mexico?s new president, Felipe Calderon, seems to be doing
all the right things in cracking down on Mexico?s drug traffickers. [...]
lexical:semantic-opposition C
H Felipe Calderon is the outgoing President of Mexico. syntactic:argument-realization
syntactic:apposition
H1 Mexico?s outgoing president, Felipe Calderon, seems to be lexical:semantic-opposition C
doing all the right things in cracking down on Mexico?s drug
traffickers. [...]
H2 The new president of Mexico, Felipe Calderon, seems to be syntactic:argument-realization E
doing all the right things in cracking down on Mexico?s drug
traffickers. [...]
H3 Felipe Calderon is Mexico?s new president. syntactic:apposition E
Table 1: Application of the decomposition methodology to an original RTE pair
ered relevant for the contradiction judgment in the
specific context of the pair.
In order to address the issues above, we pro-
pose a methodology based on the decomposition
of [T,H] pairs into monothematic pairs, each rep-
resenting one single linguistic phenomenon rele-
vant for entailment judgment. Then, the analy-
sis is carried out both on the original [T,H] pair
and on the monothematic pairs originated from
it. In particular, we investigate the correlations on
positive and on negative pairs separately, and we
show that the strategies adopted by the TE sys-
tems to deal with phenomena contributing to the
entailment or to the contradiction judgment come
to light when analyzed using qualitative criteria.
We have experimented the decomposition method-
ology over a dataset of pairs, which either are
marked with a contradiction judgment, or show a
polarity phenomenon (either in T or H) which, al-
though present, is not relevant for cotradiction.
The final goal underlying our analysis of con-
tradiction in current RTE datasets is to discover
good strategies for systems to manage contradic-
tion and, more generally, entailment judgments.
To this aim, in Section 5 we propose a comparison
between two systems participating at the last RTE-
5 campaign and try to analyze their behaviour ac-
cording to the decomposition into monothematic
pairs.
The paper is structured as follows. Section 2
presents the main aspects related to contradiction
within the RTE context. Section 3 explains the
procedure for the creation of monothematic pairs
starting from RTE pairs. Section 4 describes the
experimental setup of our pilot study, as well as
the results of the qualitative analysis. Section 5
outlines the preliminary achievements in terms of
comparison of systems? strategies in order to man-
age contradiction. Finally, Section 6 reports on
previous work on contradiction and textual entail-
ment.
2 Contradiction and Textual Entailment
In RTE, two kinds of judgment are allowed: two
ways (yes or no entailment) or three way judg-
ment. In the latter, systems are required to decide
whether the hypothesis is entailed by the text (en-
tailment), contradicts the text (contradiction), or
is neither entailed by nor contradicts the text (un-
known). The RTE-4 and RTE-5 datasets are anno-
tated for a 3-way decision: entailment (50% of the
pairs), unknown (35%), contradiction (15%). This
distribution among the three entailment judgments
aims at reflecting the natural distribution of en-
tailment in a corpus, where the percentage of text
snippets neither entailing nor contradicting each
other is higher than the contradicting ones. Even if
this balance seems artificial since in a natural set-
ting the presence of unknown pairs is much higher
than the other two judgments (as demonstrated in
the Pilot Task proposed in RTE-5 (Bentivogli et
al., 2009)), the reason behind the choice of RTE
organizers is to maintain a trade-off between the
natural distribution of the data in real documents,
and the creation of a dataset balanced beween pos-
itive and negative examples (as in two way task).
As already pointed out in (Wang, 2009), the
similarity between T?s and H?s in pairs marked as
entailment and contradiction is much higher with
respect to the similarity between T?s and H?s in
pairs marked as unknown. To support this in-
tuition, (Bentivogli et al, 2009) provides some
data on the lexical overlap between T?s and H?s
in the last RTE Challenges. For instance, in RTE-
4 the lexical overlap is 68.95% in entailment pairs,
67.97% in contradiction pairs and only 57.36% in
87
the unknown pairs. Similarly, in RTE-5 the lexical
overlap between T?s and H?s is 77.14% in entail-
ment pairs, 78.93% in contradiction pairs and only
62.28% in the unknown pairs.
For this reason, for contradiction detection it is
not sufficient to highlight mismatching informa-
tion between sentences, but deeper comprehension
is required. For applications in information anal-
ysis, it can be very important to detect incompat-
ibility and discrepancies in the description of the
same event, and the contradiction judgment in the
TE task aims at covering this aspect. More specif-
ically, in the RTE task the contradiction judgment
is assigned to a T,H pair when the two text frag-
ments are extremely unlikely to be true simultane-
ously.
According to Marneffe et al (2008), contra-
dictions may arise from a number of different
constructions, defined in two primary categories:
i) those occurring via antonymy, negation, and
numeric mismatch, and ii) contradictions arising
from the use of factive or modal words, structural
and subtle lexical contrasts, and world knowledge.
Comparing the distribution of contradiction types
for RTE-3 and the real contradiction corpus they
created collecting contradiction ?in the wild? (e.g.
from newswire, Wikipedia), they noticed that in
the latter there is a much higher rate of negations,
numeric and lexical contradictions with respect
to RTE dataset, where contradictions of category
(ii) occur more frequently. Analyzing RTE data
of the previous challenges, we noticed that the
tendency towards longer and more complex
sentences in the datasets in order to reproduce
more realistic scenarios, is also reflected in more
complex structures determining contradictions.
For instance, contradictions arising from overt
negation as in (pair 1663, RTE-1 test set):
T: All residential areas in South Africa are segregated by
race and no black neighborhoods have been established in
Port Nolloth.
H: Black neighborhoods are located in Port Nolloth.
are infrequent in the datasets of more recent RTE
challenges. For instance, in RTE-5 test set, only in
4 out of 90 contradiction pairs an overt negation
is responsible for the contradiction judgment.
In agreement with (Marneffe et al, 2008), we
also remarked that most of the contradiction
involve numeric mismatch, wrong appositions,
entity mismatch and, above all, deeper inferences
depending on background and world knowledge,
as in (pair 567, RTE-5 test set):
T: ?[...] we?ve done a series of tests on Senator Kennedy
to determine the cause of his seizure. He has had no further
seizures, remains in good overall condition, and is up and
walking around the hospital?.
H: Ted Kennedy is dead.
These considerations do not mean that overt
negations do not appear in the RTE pairs. On the
contrary, they are often present in T,H pairs, but
most of the times their presence is irrelevant in the
assignment of the correct entailment judgment to
the pair. For instance, the scope of the negation
can be a phrase or a sentence with additional infor-
mation with respect to the relevant parts of T and
H that allow to correctly judge the pair. This fact
could be misleading for systems that do not cor-
recly exploit syntactic information, as the experi-
ments using Linear Distance described in (Cabrio
et al, 2008).
3 Decomposing RTE pairs
The qualitative evaluation we propose takes
advantage of previous work on monothematic
datasets. A monothematic pair (Magnini and
Cabrio, 2009) is defined as a [T,H] pair in which a
certain phenomenon relevant to the entailment re-
lation is highlighted and isolated. The main idea is
to create such monothematic pairs on the basis of
the phenomena which are actually present in the
original RTE pairs, so that the actual distribution
of the linguistic phenomena involved in the entail-
ment relation emerges.
For the decomposition procedure, we refer to
the methodology described in (Bentivogli et al,
2010), consisting of a number of steps carried
out manually. The starting point is a [T,H] pair
taken from one of the RTE datasets, that should be
decomposed in a number of monothematic pairs
[T,H
i
]
mono
, where T is the original Text and H
i
are the Hypotheses created for each linguistic phe-
nomenon relevant for judging the entailment rela-
tion in [T,H].
In detail, the procedure for the creation of
monothematic pairs is composed of the following
steps:
1. Individuate the linguistic phenomena which
contribute to the entailment in [T,H].
2. For each phenomenon i:
88
(a) Individuate a general entailment rule r
i
for the phenomenon i, and instantiate
the rule using the portion of T which ex-
presses i as the left hand side (LHS) of
the rule, and information from H on i as
the right hand side (RHS) of the rule.
(b) Substitute the portion of T that matches
the LHS of r
i
with the RHS of r
i
.
(c) Consider the result of the previous step
as H
i
, and compose the monothematic
pair [T,H
i
]
mono
. Mark the pair with
phenomenon i.
3. Assign an entailment judgment to each
monothematic pair.
Relevant linguistic phenomena are grouped us-
ing both fine-grained categories and broader cate-
gories. Macro categories are defined referring to
widely accepted linguistic categories in the liter-
ature (e.g. (Garoufi, 2007)) and to the inference
types typically addressed in RTE systems: lexical,
syntactic, lexical-syntactic, discourse and reason-
ing. Each macro category includes fine-grained
phenomena (Table 2 reports a list of some of the
phenomena detected in RTE-5 dataset).
Table 1 shows an example of the decomposi-
tion of a RTE pair (marked as contradiction) into
monothematic pairs. At step 1 of the methodology
both the phenomena that preserve the entailment
and the phenomena that break the entailment rules
causing a contradiction in the pair are detected,
i.e. argument realization, apposition and seman-
tic opposition (column phenomena in the table).
While the monothematic pairs created basing on
the first two phenomena preserve the entailment,
the semantic opposition generates a contradiction
(column judgment).
As an example, let?s apply step by step the
procedure to the phenomenon of semantic oppo-
sition. At step 2a of the methodology the general
rule:
Pattern: x ? / ? y
Constraint: semantic opposition(y,x)
is instantiated (new? / ?outgoing), and at step
2b the substitution in T is carried out (Mexico?s
outgoing president, Felipe Calderon [...]). At
step 2c a negative monothematic pair T,H
1
is
composed (column text snippet in the table) and
marked as semantic opposition (macro-category
lexical), and the pair is judged as contradiction.
In (Bentivogli et al, 2010), critical issues con-
cerning the application of such procedure are dis-
cussed in detail, and more examples are provided.
Furthermore, a pilot resource is created, composed
of a first dataset with 60 pairs from RTE-5 test
set (30 positive, and 30 negative randomly ex-
tracted examples), and a dataset composed of all
the monothematic pairs derived by the first one
following the procedure described before. The
second dataset is composed of 167 pairs (134 en-
tailment, 33 contradiction examples, considering
35 different linguistic phenomena).
2
4 Analysis and discussion
Our analysis has been carried out taking advan-
tage of the pilot resource created by Bentivogli
et al (2010). From their first dataset we ex-
tracted a sample of 48 pairs ([T,H]
sample?contr
)
composed of 30 contradiction pairs and 18 entail-
ment pairs, the latter containing either in T or in
H a directly or an indirectly licensed negation.
3
Furthermore, a dataset of 129 monothematic pairs
(96 entailment and 33 contradiction examples),
i.e. [T,H]
mono?contr
, was derived by the pairs
in [T,H]
sample?contr
applying the procedure de-
scribed in Section 3. The linguistic phenomena
isolated in the monothematic pairs (i.e. considered
relevant to correctly assign the entailment judg-
ment to our sample) are listed in Table 2.
In RTE datasets only a subpart of the potentially
problematic phenomena concerning negation and
negative polarity items is represented. At the same
time, the specificity of the task lies in the fact that
it is not enough to find the correct representation
of the linguistic phenomena underlying a sentence
meaning, but correct inferences should be derived
from the relations that these phenomena contribute
to establish between two text fragments. The
mere presence of a negation in T is not relevant
for the TE task, unless the scope of the negation (a
token or a phrase) is present as non-negated in H
2
Both datasets are freely available at
http://hlt.fbk.eu/en/Technology/TE Specialized Data
3
Following (Harabagiu et al, 2006) overt (directly li-
censed) negations include i) overt negative markers such as
not, n?t; ii) negative quantifiers as no, and expressions such
as no one and nothing; iii) strong negative adverbs like never.
Indirectly licensed negations include: i) verbs or phrasal
verbs (e.g. deny, fail, refuse, keep from); ii) prepositions (e.g.
without, except); weak quantifiers (e.g. few, any, some), and
iv) traditional negative polarity items (e.g. a red cent or any-
more).
89
phenomena # pairs [T,H]
RTE5?mono?contr
entailment contradiction
# mono probab. # mono probab.
lex:identity 1 0.25 3 0.75
lex:format 2 1 - -
lex:acronymy 1 1 - -
lex:demonymy 1 1 - -
lex:synonymy 6 1 - -
lex:semantic-opp. - - 3 1
lex:hypernymy 2 1 - -
TOT lexical 13 0.68 6 0.32
lexsynt:transp-head 2 1 - -
lexsynt:verb-nom. 6 1 - -
lexsynt:causative 1 1 - -
lexsynt:paraphrase 2 1 - -
TOT lexical-syntactic 11 1 - -
synt:negation - - 1 1
synt:modifier 3 0.75 1 0.25
synt:arg-realization 4 1 - -
synt:apposition 9 0.6 6 0.4
synt:list 1 1 - -
synt:coordination 2 1 - -
synt:actpass-altern. 4 0.67 2 0.33
TOT syntactic 23 0.7 10 0.3
disc:coreference 16 1 - -
disc:apposition 2 1 - -
disc:anaphora-zero 3 1 - -
disc:ellipsis 3 1 - -
disc:statements 1 1 - -
TOT discourse 25 1 - -
reas:apposition 1 0.5 1 0.5
reas:modifier 2 1 - -
reas:genitive 1 1 - -
reas:meronymy 1 0.5 1 0.5
reas:quantity - - 5 1
reas:spatial 1 1 - -
reas:gen-inference 18 0.64 10 0.36
TOT reasoning 24 0.59 17 0.41
TOT (all phenomena) 96 0.74 33 0.26
Table 2: Occurrences of linguistic phenomena in
TE contradiction pairs
(or viceversa), hence a contradiction is generated.
For this reason, 18 pairs of [T,H]
sample?contr
are judged as entailment even if a negation is
present, but it is not relevant to correctly assign
the entailment judgment to the pair as in (pair
205, RTE-5 test set):
T: A team of European and American astronomers say
that a recently discovered extrasolar planet, located not far
from Earth, contains oceans and rivers of hot solid water. The
team discovered the planet, Gliese 436 b [...].
H: Gliese 436 b was found by scientists from America and
Europe.
As showed in Table 2, only in one pair of
our sample the presence of a negation is relevant
to assign the contradiction judgment to the pair.
In the pairs we analyzed, contradiction mainly
arise from quantity mismatching, semantic oppo-
sition (antonymy), mismatching appositions (e.g.
the Swiss Foreign Minister x contradicts y is the
Swiss Foreign Minister), and from general infer-
ence (e.g. x became a naturalized citizen of the
U.S. contradicts x is born in the U.S.). Due to the
small sample we analyzed, some phenomena ap-
pear rarely, and their distribution can not be con-
sidered as representative of the same phenomenon
in a natural setting. In 27 out of 30 contradiction
pairs, only one monothematic pair among the ones
derived from each example was marked as con-
tradiction, meaning that on average only one lin-
guistic phenomenon is responsible for the contra-
diction judgment in a TE original pair. Hence the
importance of detecting it.
Given the list of the phenomena isolated in
[T,H]
mono?contr
with their frequency both in
monothematic positive pairs and monothematic
negative pairs, we derived the probability of lin-
guistic phenomena to contribute more to the as-
signment of a certain judgment than to another
(column probab. in Table 2). Such probability P
of a phenomenon i to appear in a positive (or in a
negative) pair is calculated as follows:
P (i|[T,H]
positive
) =
#(i|[T,H]
RTE5?positive?mono
)
#(i|[T,H]
RTE5?mono
)
(1)
For instance, if the phenomenon semantic op-
position appears in 3 pairs of our sample and all
these pairs are marked as contradiction, we as-
sign a probability of 1 to a pair containing a se-
mantic opposition to be marked as contradiction.
If the phenomenon apposition (syntax) appears in
9 monothematic positive pairs and in 6 negative
pairs, that phenomenon has a probability of 0.6 to
appear in positive examples and 0.4 to appear in
negative examples. Due to their nature, some phe-
nomena are strongly related to a certain judgment
(e.g. semantic opposition), while other can appear
both in positive and in negative pairs. Learning
such correlations on larger datasets could be an in-
teresting feature to be exploited by TE systems in
the assignment of a certain judgment if the phe-
nomenon i is detected in the pair.
Table 3 reports the cooccurrences of the linguis-
tic phenomena relevant to inference in the pairs
marked as contradiction. On the first horizontal
row all the phenomena that at least in one pair
determine contradiction are listed, while in the
first column there are all the phenomena cooc-
curring with them in the pairs. The idea un-
delying this table is to understand if it is possi-
ble to identify recurrent patterns of cooccurrences
between phenomena in contradiction pairs. As
can be noticed, almost all phenomena occur to-
gether with expressions requiring deeper inference
90
l
e
x
:
i
d
e
n
t
i
t
y
l
e
x
:
s
e
m
o
p
p
o
s
i
t
i
o
n
s
y
n
t
:
n
e
g
a
t
i
o
n
s
y
n
t
:
m
o
d
i
fi
e
r
s
y
n
t
:
a
p
p
o
s
i
t
i
o
n
s
y
n
t
:
a
c
t
p
a
s
s
a
l
t
e
r
n
r
e
a
s
:
m
e
r
o
n
y
m
y
r
e
a
s
:
q
u
a
n
t
i
t
y
r
e
a
s
:
g
e
n
i
n
f
e
r
e
n
c
e
lex:identity 1 1
lex:format 1
lex:acronymy 1
lex:synonymy 1 1 1 1
lex:hypernymy 1
lexsynt:vrb-nom 1 1 1
lexsynt:caus. 1
synt:modifier 1
synt:arg-realiz. 1 1
synt:apposition 2 3
synt:coord. 1
synt:actpass 1 1
disc:coref. 3 1 4
disc:apposition
disc:anaph-0 1 1
disc:ellipsis 1 1 2
disc:statements 1
reas:genitive 1
reas:meronymy 1
reas:gen-infer. 1 1 3 1 2 1
Table 3: Cooccurrencies of phenomena in contra-
diction pairs
(reas:general inference), but this is due to the fact
that this category is the most frequent one. Beside
this, it seems that no specific patterns can be high-
lighted, but it could be worth to extend this analy-
sis increasing the number of pairs of the sample.
5 Comparing RTE systems? behaviour
on contradiction pairs
As introduced before, from a contradiction pair it
is possible to extract on average 3 monothematic
pairs (Bentivogli et al, 2009), and only one of
these monothematic pairs is marked as contradic-
tion. This means that on average only one lin-
guistic phenomenon is responsible for the contra-
diction judgment in a RTE pair, while the others
maintain the entailment relation (i.e. it is possible
to correcly apply an entailment rule as exemplified
in Section 3). On the contrary, in a pair judged
as entailment, all the monothematic pairs derived
from it are marked as entailment.
These observations point out the fact that if a
TE system is able to correctly isolate and judge
the phenomenon that generates the contradiction,
the system should be able to assign the correct
judgment to the original contradiction pair, despite
possible mistakes in handling the other phenom-
ena present in that pair.
In order to understand how it is possible to
take advantage of the data analyzed so far to
improve a TE system, we run two systems that
took part into the last RTE challenge (RTE-5) on
[T,H]
mono?contr
.
The first system we used is the EDITS system
(Edit Distance Textual Entailment Suite) (Negri et
al., 2009)
4
, that assumes that the distance between
T and H is a characteristics that separates the pos-
itive pairs, for which entailment holds, from the
negative pairs, for which entailment does not hold
(it is developed according to the two way task). It
is based on edit distance algorithms, and computes
the [T,H] distance as the overall cost of the edit op-
erations (i.e. insertion, deletion and substitution)
that are required to transform T into H. In partic-
ular, we applied the model that produced EDITS
best run at RTE-5 (acc. on RTE-5 test set: 60.2%).
The main features of this run are: Tree Edit Dis-
tance algorithm on the parsed trees of T and H,
Wikipedia lexical entailment rules, and PSO opti-
mized operation costs, as described in (Mehdad et
al., 2009).
The other system used in our experiments
is VENSES
5
(Delmonte et al, 2009), that ob-
tained performances similar to EDITS at RTE-5
(acc. on test set: 61.5%). VENSES applies a
linguistically-based approach for semantic infer-
ence, composed of two main components: i) a
grammatically-driven subsystem that validates the
well-formedness of the predicate-argument struc-
ture and works on the output of a deep parser
producing augmented (i.e. fully indexed) head-
dependency structures; and ii) a subsystem that
detects allowed logical and lexical inferences bas-
ing on different kind of structural transformations
intended to produce a semantically valid mean-
ing correspondence. The system has a pronomi-
nal binding module that works at text/hypothesis
level separately for lexical personal, possessive
and reflexive pronouns, which are substituted by
the heads of their antecedents. Also in this case,
we applied the same configuration of the system
used in RTE evaluation.
Table 4 reports EDITS and VENSES accuracies
on the monothematic pairs of [T,H]
mono?contr
.
As said before, the accuracy reported for some
very rare phenomena cannot be considered com-
pletely reliable. Nevertheless, from these data the
main features of the systems can be identified. For
instance, EDITS obtains the highest accuracies on
the positive monothematic pairs, while it seems it
has no peculiar strategies to deal with phenomena
4
http://edits.fbk.eu/
5
http://project.cgm.unive.it/venses en.html
91
phenomena EDITS VENSES
% acc. % acc.
pos. neg. pos. neg.
lex:identity 100 0 100 33.3
lex:format 100 - 100 -
lex:acronymy 100 - 0 -
lex:demonymy 100 - 100 -
lex:synonymy 80.3 - 80.3 -
lex:semantic-opp. - 0 - 100
lex:hypernymy 100 - 100 -
TOT lexical 96.7 0 80 66.6
lexsynt:transp-head 100 - 50 -
lexsynt:verb-nom. 83.3 - 16 -
lexsynt:causative 100 - 100 -
lexsynt:paraphrase 100 - 100 -
TOT lexical-syntactic 95.8 - 66.5 -
synt:negation - 0 - 0
synt:modifier 100 0 33.3 100
synt:arg-realization 100 - 50 -
synt:apposition 100 33.3 55.5 83.3
synt:list 100 - 100 -
synt:coordination 100 - 50 -
synt:actpass-altern. 100 0 25 50
TOT syntactic 100 22.2 52.3 77.7
disc:coreference 95 - 50 -
disc:apposition 100 - 0 -
disc:anaphora-zero 100 - 33.3 -
disc:ellipsis 100 - 33.3 -
disc:statements 100 - 0 -
TOT discourse 99 - 23.3 -
reas:apposition 100 0 100 100
reas:modifier 50 - 100 -
reas:genitive 100 - 100 -
reas:meronymy 100 0 100 0
reas:quantity - 0 - 80
reas:spatial 100 - 0 -
reas:gen-inference 87.5 50 37.5 90
TOT reasoning 89.5 35.2 72.9 82.3
TOT (all phenomena) 96.2 25 59 81.2
Table 4: RTE systems? accuracy on phenomena
that generally cause contradiction (e.g. seman-
tic opposition, negation, and quantity mismatch-
ing). On the contrary, VENSES shows an oppo-
site behaviour, obtaining the best results on the
negative cases. Analysing such data it is possible
to hypothesize systems? behaviours: for example,
on the monothematic dataset EDITS produces a
pretty high number of false positives, meaning that
for this system if there are no evidences of con-
tradiction, a pair should be marked as entailment
(in order to improve such system, strategies to de-
tect contradiction pairs should be thought). On the
contrary, VENSES produces a pretty high number
of false negatives, meaning that if the system is not
able to find evidences of entailment, it assigns the
contradiction value to the pairs (for this system,
being able to correctly detect all the phenomena
contributing to entailment in a pair is fundamen-
tal, otherwise it will be marked as contradiction).
6 Related Work
Condoravdi et al (2003) first proposed contra-
diction detection as an important NLP task, then
(Harabagiu et al, 2006) provided the first em-
pirical results for it, focusing on contradiction
caused by negation, antonymy, and paraphrases.
Voorhees (2008) carries out an analysis of RTE-
3 extended task, examining systems? abilities to
detect contradiction and providing explanations
of their reasoning when making entailment deci-
sions.
Beside defining the categories of construction
from which contradiction may arise, Marneffe et
al. (2008) provide the annotation of the RTE
datasets (RTE-1 and RTE-2) for contradiction.
Furthermore, they also collect contradiction ?in
the wild? (e.g. from newswire, Wikipedia) to sam-
ple naturally occurring ones.
6
Ritter et al (2008) extend (Marneffe et al,
2008)?s analysis to a class of contradiction that can
only be detected using backgroud knowledge, and
describe a case study of contradiction detection
based on functional relations. They also automat-
ically generate a corpus of seeming contradiction
from the Web text.
7
Furthermore, some of the systems presented in
the previous editions of the RTE challenges at-
tempted specic strategies to focus on the phe-
nomenon of negation. For instance, (Snow et al,
2006) presents a framework for recognizing tex-
tual entailment that focuses on the use of syntactic
heuristics to recognize false entailment. Among
the others, heuristics concerning negation mis-
match and antonym match are defined. In (Tatu
et al, 2007) the logic representation of sentences
with negated concepts was altered to mark as
negated the entire scope of the negation. (Ferran-
dez et al, 2009) propose a system facing the en-
tailment recognition by computing shallow lexical
deductions and richer inferences based on seman-
tics, and features relating to negation are extracted.
In (Iftene et al, 2009) several rules are extracted
and applied to detect contradiction cases.
7 Conclusion
We have proposed a methodology for the qualita-
tive analysis of TE systems focusing on contradic-
tion judgments and on the linguistic phenomena
that determine such judgments. The methodology
is based on the decomposition of [T,H] pairs into
monothematic pairs, each representing one sin-
gle linguistic phenomenon relevant for entailment
6
Their corpora are available at http://www-
nlp.stanford.edu/projects/contradiction.
7
Available at http://www.cs.washington.edu/research/ au-
contraire/
92
judgment.
In particular, the phenomena from which con-
tradiction may arise and their distribution in RTE
datasets have been highlighted, and a pilot study
comparing the performancies of two RTE systems
both on monothematic pairs and on the corre-
sponding original ones has been carried out. We
discovered that, although the two systems have
similar performances in terms of accuracy on the
RTE-5 datasets, they show significant differences
in their respective abilities to correctly manage dif-
ferent linguistic phenomena that generally cause
contradiction. We hope that the analysis of con-
tradiction in current RTE datasets may bring inter-
esting elements to TE system developers to define
good strategies to manage contradiction and, more
generally, entailment judgments.
8 Acknowledgements
This work has been partially supported by the
LiveMemories project (Active Digital Memo-
ries of Collective Life) funded by the Au-
tonomous Province of Trento under the call ?Ma-
jor Projects?. We would like to thank Professor
Rodolfo Delmonte and Sara Tonelli for running
the VENSES system on our datasets.
References
Bentivogli, Luisa, Bernardo Magnini, Ido Dagan,
Hoa Trang Dang, and Danilo Giampiccolo. 2009.
The Fifth PASCAL RTE Challenge. Proceedings of
the TAC 2009 Workshop on Textual Entailment. To
appear. Gaithersburg, Maryland. 17 November.
Bentivogli, Luisa, Elena Cabrio, Ido Dagan,
Danilo Giampiccolo, Medea Lo Leggio, and
Bernardo Magnini. 2010. Building Textual En-
tailment Specialized Data Sets: a Methodology
for Isolating Linguistic Phenomena Relevant to
Inference. Proceedings of the 7th LREC conference.
Valletta, Malta. 19-21 May.
Cabrio, Elena, Milen Ognianov Kouylekov and
Bernardo Magnini, 2008. Combining Specialized
Entailment Engines for RTE-4, Proceedings of the
Text Analysis Conference (TAC 2008). Gaithersburg,
Maryland, USA, 17-18 November.
Condoravdi, Cleo, Dick Crouch, Valeria de Paiva,
Reinhard Stolle, and Daniel Bobrow. 2003. Entail-
ment, Intentionality and Text Understanding Pro-
ceedings of the HLT-NAACL 2003 Workshop on Text
Meaning. Edmonton, Alberta, Canada. 31 May.
Dagan, Ido, Bill Dolan, Bernardo Magnini, and
Dan Roth. 2009. Recognizing textual entailment:
Rational, evaluation and approaches. Natural Lan-
guage Engineering (JNLE), Volume 15, Special Is-
sue 04, October 2009, pp i-xvii. Cambridge Univer-
sity Press.
De Marneffe, Marie-Catherine, Anna N. Rafferty and
Christopher D. Manning. 2008. Finding Contradic-
tions in Text. Proceedings of ACL-08: HLT, pages
10391047. Columbus, Ohio, USA, June.
Delmonte, Rodolfo, Sara Tonelli, Rocco Tripodi.
2009. Semantic Processing for Text Entailment with
VENSES. Proceedings of the TAC 2009 Workshop
on TE. Gaithersburg, Maryland. 17 November.
Garoufi, Konstantina. 2007. Towards a Better Un-
derstanding of Applied Textual Entailment. Master
Thesis. Saarland University. Saarbr?ucken, Germany.
Ferr?andez,
?
Oscar, Rafael Mu?noz, and Manuel Palomar.
2009. Alicante University at TAC 2009: Experi-
ments in RTE. Proceedings of the TAC 2009 Work-
shop on Textual Entailment. Gaithersburg, Mary-
land. 17 November.
Harabagiu, Sanda, Andrew Hickl, and Finley Lacatusu.
2006. Negation, Contrast and Contradiction in Text
Processing. In Proceedings of AAAI-06. Boston,
Massachusetts. July 16-20.
Iftene, Adrian, Mihai-Alex Moruz 2009. UAIC
Participation at RTE-5. Proceedings of the TAC
2009 Workshop on Textual Entailment. To appear.
Gaithersburg, Maryland. 17 November.
Magnini, Bernardo, and Elena Cabrio. 2009. Com-
bining Specialized Entailment Engines. Proceed-
ings of the LTC ?09 conference. Poznan, Poland.
6-8 November.
Mehdad, Yashar, Matteo Negri, Elena Cabrio,
Milen Kouylekov, and Bernardo Magnini. 2009.
Using Lexical Resources in a Distance-Based Ap-
proach to RTE. Proceedings of the TAC 2009 Work-
shop on TE. Gaithersburg, Maryland. 17 November
2009.
Negri, Matteo, Milen Kouylekov, Bernardo Magnini,
Yashar Mehdad, and Elena Cabrio. 2009. Towards
Extensible Textual Entailment Engines: the EDITS
Package. AI*IA 2009: Emergent Perspectives in Ar-
tificial Intelligence. Lecture Notes in Computer Sci-
ence, Springer-Verlag, pp. 314-323. 2009.
Nielsen, Rodney D., Wayne Ward, and James H. Mar-
tin. 2009. Recognizing entailment in intelligent tu-
toring systems. In Ido Dagan, Bill Dolan, Bernardo
Magnini and Dan Roth (Eds.) The Journal of Natu-
ral Language Engineering, (JNLE). 15, pp 479-501.
Copyright Cambridge University Press, Cambridge,
United Kingdom.
Ritter, Alan, Doug Downey, Stephen Soderland, and
Oren Etzioni. 2008. It?s a Contradiction - No, it?s
not: A Case Study using Functional Relations. Pro-
ceedings of 2008 Conference on Empirical Methods
93
in Natural Language Processing. Honolulu, Hawaii.
25-27 October.
Romano, Lorenza, Milen Ognianov Kouylekov,
Idan Szpektor, Ido Kalman Dagan, and Al-
berto Lavelli. 2006. Investigating a Generic
Paraphrase-Based Approach for Relation Extrac-
tion. Proceedings of EACL 2006. Trento, Italy. 3-7
April.
Snow, Rion, Lucy Vanderwende, and Arul Menezes.
2006. Effectively using syntax for recognizing false
entailment. Proceedings of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics. New York, 4-9 June.
Tatu, Marta, Dan I. Moldovan. 2007. COGEX at
RTE 3. Proceedings of the ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing. Prague,
Czech Republic, 28-29 June.
Voorhees, Ellen M. 2008. Contradictions and Justifi-
cations: Extentions to the Textual Entailment Task.
Proceedings of ACL-08: HLT. Columbus, Ohio,
USA. 15-20 June.
Wang, Rui, and Yi Zhang. 2009. Recognizing Tex-
tual Relatedness with Predicate-Argument Struc-
tures. Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing.
Singapore, 6-7 August.
94
Towards Component-Based Textual Entailment
Elena Cabrio1,2 and Bernardo Magnini1
1FBK-irst, Trento, Italy
2University of Trento, Italy
{cabrio,magnini}@fbk.eu
Abstract
In the Textual Entailment community, a shared effort towards a deeper understanding of the core
phenomena involved in textual inference is recently arose. To analyse how the common intuition
that decomposing TE would allow a better comprehension of the problem from both a linguistic
and a computational viewpoint, we propose a definition for strong component-based TE, where each
component is in itself a complete TE system, able to address a TE task on a specific phenomenon
in isolation. We review the literature according to our definition, trying to position relevant work as
more or less close to our idea of strong component-based TE. Several dimensions of the problem are
discussed: i) the implementation of system components to address specific inference types, ii) the
analysis of the phenomena relevant to component-based TE, and iii) the development of evaluation
methodologies to assess TE systems capabilities to address single phenomena in a pair.
1 Introduction
The Recognizing Textual Entailment (RTE) task (Dagan et al (2009)) aims at capturing a broad range
of inferences that are relevant for several Natural Language Processing applications, and consists of
deciding, given two text fragments, whether the meaning of one text (the hypothesis H) is entailed, i.e.
can be inferred, from another text (the text T).
Although several approaches to face this task have been experimented, and progresses in TE tech-
nologies have been shown in RTE evaluation campaigns, a renewed interest is rising in the TE community
towards a deeper and better understanding of the core phenomena involved in textual inference. In line
with this direction, we are convinced that crucial progress may derive from a focus on decomposing the
complexity of the TE task into basic phenomena and on their combination. This belief demonstrated
to be shared by the RTE community, and a number of recently published works (e.g. Sammons et al
(2010), Bentivogli et al (2010)) agree that incremental advances in local entailment phenomena are
needed to make significant progress in the main task, which is perceived as omnicomprehensive and not
fully understood yet. According to this premise, the aim of this work is to systematize and delve into the
work done so far in component-based TE, focusing on the aspects that contribute to highlight a common
framework and to define a clear research direction that deserves further investigation.
Basing on the original definition of TE, that allows to fomulate textual inferences in an application
independent way and to take advantage of available datasets for training provided in the RTE evaluation
campaigns, we intend to analyse how the common intuition of decomposing TE would allow a better
comprehension of the problem from both a linguistic and a computational viewpoint. Aspects related to
meaning compositionality, which are absent in the original proposal, could potentially be introduced into
TE and may bring new light into textual inference.
In this direction, we propose a definition for ?strong? component-based TE, where each component
is in itself a complete TE system, able to address a TE task on a specific phenomenon in isolation.
Then, we review the literature in the TE field according to our definition, trying to position relevant
work as more or less close to our idea of strong component-based TE. We have analysed and carried
out research on several dimensions of the problem, including: i) the definition and implementation of
320
system components able to address specific inference types (Section 2); ii) the analysis of the phenomena
relevant to component-based TE (Section 3); iii) the development of methodologies for the analysis of
component-based TE systems, providing a number of qualitative indicators to assess the capabilities that
systems have to address single phenomena in a pair and to combine them (Section 4).
2 Component-based TE framework
We define a component-based TE architecture as a set of clearly identifiable TE modules that can be
singly used on specific entailment sub-problems and can be then combined to produce a global entailment
judgement. Each component receives a certain example pair as input, and outputs an entailment judgment
concerning the inference type it is built to address. In other words, each component is in turn a TE
system, that performs the same task focusing only on a certain sub-aspect of entailment. According
to our proposal the following requirements need to be fulfilled in component-based TE architecture: i)
each compenent must provide a 3-way judgment (i.e. entailment, contradiction, unknown) on a specific
aspect underlying entailment, where the unknown judgement might be interpreted as the absence of the
phenomenon in the TE pair; ii) in a component-based architecure, the same inference type (e.g. temporal,
spatial inferences) can not be covered by more than one component; this is because in the combination
phase we do not want that the same phenomen is counted more than one time.
No specific constraints are defined with respect to how such components should be implemented,
i.e. they can be either a set of classifiers or rule-based modules. In addition, linguistic processing and
annotation of the input data (e.g. parsing, NER, semantic role labeling) can be required by a component
according to the phenomenon it considers. An algorithm is then applied to judge the entailment relation
between T and H with respect to that specific aspect. Unlike similarity algorithms, with whom algorithms
performing entailment are often associated in the literature, the latter are characterized by the fact that
the relation on which they are asked to judge is directional. According to such definition, the nature
of the TE task is not modified, since each sub-task independently performed by the system components
keeps on being an entailment task. Suitable composition mechanisms should then be applied to combine
the output of each single module to obtain a global judgment for a pair.
The definition presented above provides a strong interpretation of the compositional framework for
TE, that can be described as a continuum that tends towards systems developed combining identifiable
and separable components addressing specific inference types. A number of works in the literature can
be placed along this continuum, according to how much they get closer to this interpretation.
Systems addressing TE exploiting machine learning techniques with a variety of features, including
lexical-syntactic and semantic features (e.g. Kozareva and Montoyo (2006), Zanzotto et al (2007)) tend
towards the opposite extreme of this framework, since even if linguistic features are used, they bring
information about a specific aspect relevant to the inference task but they do not provide an independent
judgment on it. These systems are not modular, and it is difficult to assess the contribution of a cer-
tain feature in providing the correct overall judgment for a pair. A step closer towards the direction of
component-based TE is done by Bar-Haim et al (2008), that model semantic inference as application
of entailment rules specifying the generation of entailed sentences from a source sentence. Such rules
capture semantic knowledge about linguistic phenomena (e.g. paraphrases, synonyms), and are applied
in a transformation-based framework. Even if these rules are clearly identifiable, their application per se
does not provide any judgment about an existing entailment relation between T and H.
A component-based system has been developed by Wang and Neumann (2008), based on three spe-
cialized RTE-modules: (i) to tackle temporal expressions; (ii) to deal with other types of NEs; (iii) to deal
with cases with two arguments for each event. Besides these precision-oriented modules, two robust but
less accurate backup strategies are considered, to deal with not yet covered cases. In the final stage, the
results of all specialized and backup modules are joint together, applying a weighted voting mechanism.
Getting closer to the definition of component-based TE presented at the beginning of this Section, in
Magnini and Cabrio (2009) we propose a framework for the definition and combination of specialized
entailment engines, each of which able to deal with a certain aspect of language variability. A distance-
321
based framework is assumed, where the distance d between T and H is inversely proportional to the
entailment relation in the pair. We assume an edit distance approach (Kouylekov and Magnini (2005)),
where d is estimated as the sum of the costs of the edit operations (i.e. insertion, deletion, substitution),
which are necessary to transform T into H. Issues underlying the combination of the specialized entail-
ment engines are discussed, i.e. the order of application and the combination of individual results in
order to produce a global result.
3 Linguistic analysis and resources for component-based TE
The idea underlying component-based TE is that each component should independently solve the en-
tailment relation on a specific phenomenon relevant to inference, and then the judgments provided by
all the modules are combined to obtain an overall judgment for a pair. Our definition abstracts from the
different theories underlying the categorization of linguistic phenomena, so a straightforward relation
between TE component and linguistic phenomena cannot be defined a priori. Some work has already
been done in investigating in depth sub-aspects of entailment, and in developing ad hoc resources to
assess the impact of systems components created to address specific inference types. Earlier works in the
field (e.g. Vanderwende et al (2005), Clark et al (2007)) carried out partial analysis of the data sets in
order to evaluate how many entailment examples could be accurately predicted relying only on lexical,
syntactic or world knowledge. Bar-Haim et al (2005) defined two intermediate models of textual entail-
ment, corresponding to lexical and lexical-syntactic levels of representation, and a sample from RTE-1
data set was annotated according to each model.
A step further, other RTE groups have developed focused data sets with the aim of investigating
and experimenting on specific phenomena underlying language variability. For instance, to evaluate a
contradiction detection module Marneffe et al (2008) created a corpus where contradictions arise from
negation, by adding negative markers to the RTE-2 test data. Kirk (2009) describes his work of building
an inference corpus for spatial inference about motion, while Akhmatova and Dras (2009) experiment
current approaches on hypernymy acquisition to improve entailment classification.
The first systematic work of annotation of TE data sets is done by Garoufi (2007), that propose a
scheme for manual annotation of textual entailment data sets (ARTE). The aim is to highlight a wide
variety of entailment phenomena in the data, in relation to three levels, i.e. Alignment, Context and
Coreference. 23 different features are extracted for positive entailment annotation, while for the negative
pairs a more basic scheme is conceived. The ARTE scheme has been applied to the complete positive
entailment RTE-2 Test Set (400 pairs), and to a random 25% portion of the negative entailment Test Set.
More recently, in Bentivogli et al (2010) we present a methodology for the creation of specialized
TE data sets, made of monothematic T-H pairs, i.e. pairs in which a certain phenomenon relevant to the
entailment relation is highlighted and isolated (Magnini and Cabrio (2009)). Such monothematic pairs
are created basing on the phenomena that are actually present in the RTE pairs, so that the distribution of
the linguistic phenomena involved in the entailment relation emerges. A number of steps are carried out
manually, starting from a T-H pair taken from one of the RTE data sets, and decomposing it in a number of
monothematic pairs T-Hi, where T is the original text and Hi are the hypotheses created for each linguistic
phenomenon relevant for judging the entailment relation in T-H. Phenomena are grouped using both fine-
grained and broader categories (e.g. lexical, syntactic, lexical-syntactic, discourse and reasoning). After
applying the proposed methodology, all the monothematic pairs T-Hi relative to the same phenomenon i
are grouped together, resulting in several data sets specialized for phenomenon i. Unlike previous work
of analysis of RTE data, the result of this study is a resource that allows evaluation of TE systems on
specific phenomena relevant to inference, both when isolated and when interacting with the others (the
annotation of RTE data with the linguistic phenomena underlying the entailment/contradiction relations
in the pairs is also provided). A pilot study has been carried out on 90 pairs from RTE-5 data set.1
Highlighting the need of resources for solving textual inference problems in the context of RTE,
Sammons et al (2010) challenge the NLP community to contribute to a joint, long term effort in this
1The resulting data sets are freely available at http://hlt.fbk.eu/en/Technology/TE_Specialized_Data
322
direction, making progress both in the analysis of relevant linguistic phenomena and their interaction, and
developing resources and approaches that allow more detailed assessment of RTE systems. The authors
propose a linguistically-motivated analysis of entailment data based on a step-wise procedure to resolve
entailment decision, by first identifying parts of T that match parts of H, and then identifying connecting
structure. Their inherent assumption is that the meanings of T and H could be represented as sets of
n-ary relations, where relations could be connected to other relations (i.e. could take other relations as
arguments). The authors carried out a feasibility study applying the procedure to 210 examples from
RTE-5, marking for each example the entailment phenomena that are required for the inference.
4 Evaluation in component-based TE
The evaluation measure adopted in the RTE challenges is accuracy, i.e. the percentage of pairs correctly
judged by a TE system. In the last RTE-5 and RTE-6 campaigns, participating groups were asked to
run ablation tests, to evaluate the contribution of publicly available knowledge resources to the systems?
performances. Such ablation tests consist of removing one module at a time from a system, and rerunning
the system on the test set with the other modules, except the one tested. The results obtained were not
satisfactory, since the impact of a certain resource on system performances is really dependent on how it
is used by the system. In some cases, resources like WordNet demonstrated to be very useful, while for
other systems their contribution is limited or even damaging, as observed also in Sammons et al (2010).
To provide a more detailed evaluation of the capabilities of a TE system to address specific infer-
ence types, in Cabrio and Magnini (2010) we propose a methodology for a qualitative evaluation of TE
systems, that takes advantage of the decomposition of T-H pairs into monothematic pairs (described in
Section 3). The assumption is that the more a system is able to correctly solve the linguistic phenomena
underlying the entailment relation separately, the more the system should be able to correctly judge more
complex pairs, in which different phenomena are present and interact in a complex way. According to
such assumption, the higher the accuracy of a system on the monothematic pairs and the compositional
strategy, the better its performances on the original RTE pairs. The precision a system gains on single
phenomena should be maintained over the general data set, thanks to suitable mechanisms of meaning
combination. A number of quantitative and qualitative indicators about strength and weaknesses of TE
systems result from the application of this methodology. Comparing the qualitative analysis obtained
for two TE systems, the authors show that several systems? behaviors can be explained in terms of the
correlation between the accuracy on monothematic pairs and the accuracy on the corresponding original
pairs. In a component based framework, such analysis would allow a separate evaluation of TE modules,
focusing on their ability to correctly address the inference types they are built to deal with.
5 Conclusions
This paper provides a definition for strong component-based TE framework, exploiting the common
intuition that decomposing the complexity of TE would allow a better comprehension of the problem
from both a linguistic and a computational viewpoint. We have reviewed the literature according to
our definition, trying to position relevant works as more or less close to our idea of strong component-
based TE. We hope that the analysis of the different dimensions of the problem we provided may bring
interesting elements for future research works. In this direction, we propose a research program in
which for different applications (e.g. domain, genre) specific TE component-based architectures could
be optimized, i.e. composed by modules that meet the requirements of that specific genre/domain.
References
Akhmatova, E. and M. Dras (2009). Using hypernymy acquisition to tackle (part of) textual entailment.
In Proceedings of TextInfer 2009, Singapore. 6 August.
323
Bar-Haim, R., J. Berant, I. Dagan, I. Greental, S. Mirkin, E. Shnarch, and I. Szpektor (2008). Efficient
semantic deduction and approximate matching over compact parse forests. In Proceedings of the TAC
2008 Workshop on TE, Gaithersburg, Maryland, USA. 17 November.
Bar-Haim, R., I. Szpektor, and O. Glickman (2005). Definition and analysis of intermediate entailment
levels. In Proceedings of the ACL 2005 Workshop on Empirical Modeling of Semantic Equivalence
and Entailment, Ann Arbor, Michigan. 30 June.
Bentivogli, L., E. Cabrio, I. Dagan, D. Giampiccolo, M. L. Leggio, and B. Magnini (2010). Building
textual entailment specialized data sets: a methodology for isolating linguistic phenomena relevant to
inference. In Proceedings of LREC 2010, Valletta, Malta. 19-21 May.
Bentivogli, L., B. Magnini, I. Dagan, H. Dang, and D. Giampiccolo (2009). The fifth pascal recogniz-
ing textual entailment challenge. In Proceedings of the TAC 2009 Workshop on TE, Gaithersburg,
Maryland. 17 November.
Cabrio, E. and B. Magnini (2010). Toward qualitative evaluation of textual entailment systems. In
Proceedings of COLING 2010: Posters, Beijing, China. 23-27 August.
Clark, P., P. Harrison, J. Thompson, W. Murray, J. Hobbs, and C. Fellbaum (2007). On the role of lexical
and world knowledge in rte3. In Proceedings of the ACL-07 Workshop on TE and Paraphrasing,
Prague, Czech Republic. 28-29 June.
Dagan, I., B. Dolan, B. Magnini, and D. Roth (2009). Recognizing textual entailment: Rational, evalua-
tion and approaches. Natural Language Engineering (JNLE) 15(Special Issue 04), i?xvii.
Garoufi, K. (2007). Towards a better understanding of applied textual entailment. In Master Thesis,
Saarland University. Saarbru?cken, Germany.
Kirk, R. (2009). Building an annotated textual inference corpus for motion and space. In Proceedings of
TextInfer 2009, Singapore. 6 August.
Kouylekov, M. and B. Magnini (2005). Tree edit distance for textual entailment. In Proceedings of
RALNP-2005, Borovets, Bulgaria. 21-23 September.
Kozareva, Z. and A. Montoyo (2006). Mlent: The machine learning entailment system of the university
of alicante. In Proc. of the second PASCAL Challenge Workshop on RTE, Venice, Italy. 10 April.
Magnini, B. and E. Cabrio (2009). Combining specialized entailment engines. In Proceedings of LTC?09,
Poznan, Poland. 6-8 November.
Marneffe, M. D., A. Rafferty, and C. Manning (2008). Finding contradictions in text. In Proceedings of
ACL-08, Columbus, OH, 15-20 June.
Sammons, M., V. Vydiswaran, and D. Roth (2010). Ask not what textual entailment can do for you... In
Proceedings of ACL-10, Uppsala, Sweden. 11-16 July.
Vanderwende, L., D. Coughlin, and B. Dolan (2005). What syntax can contribute in entailment task. In
Proceedings of the First PASCAL Challenges Workshop on RTE, Southampton, U.K., 11-13 April.
Wang, R. and G. Neumann (2008). An accuracy-oriented divide-and-conquer strategy. In Proceedings
of the TAC 2008 Workshop on TE, Gaithersburg, Maryland. 17 November.
Zanzotto, F., M. Pennacchiotti, and A. Moschitti (2007). Shallow semantics in fast textual entailment
rule learners. In Proceedings of the ACL-PASCAL Workshop on TE and Paraphrasing, Prague, Czech
Republic. 23-30 June.
324
Proceedings of the 3rd Workshop on the People?s Web Meets NLP, ACL 2012, pages 34?43,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Extracting Context-Rich Entailment Rules from Wikipedia Revision History
Elena Cabrio
INRIA
2004, route de Lucioles BP93
06902 Sophia Antipolis, France.
elena.cabrio@inria.fr
Bernardo Magnini
FBK
Via Sommarive 18
38100 Povo-Trento, Italy.
magnini@fbk.eu
Angelina Ivanova
University of Oslo
Gaustadalle?en 23B
Ole-Johan Dahls hus
N-0373 Oslo, Norway.
angelii@ifi.uio.no
Abstract
Recent work on Textual Entailment has shown
a crucial role of knowledge to support entail-
ment inferences. However, it has also been
demonstrated that currently available entail-
ment rules are still far from being optimal. We
propose a methodology for the automatic ac-
quisition of large scale context-rich entailment
rules from Wikipedia revisions, taking advan-
tage of the syntactic structure of entailment
pairs to define the more appropriate linguis-
tic constraints for the rule to be successfully
applicable. We report on rule acquisition ex-
periments on Wikipedia, showing that it en-
ables the creation of an innovative (i.e. ac-
quired rules are not present in other available
resources) and good quality rule repository.
1 Introduction
Entailment rules have been introduced to provide
pieces of knowledge that may support entailment
judgments (Dagan et al, 2009) with some degree of
confidence. More specifically, an entailment rule is
defined (Szpektor et al, 2007) as a directional rela-
tion between two sides of a pattern, corresponding
to text fragments with variables (typically phrases
or parse sub-trees). The left-hand side (LHS) of
the pattern entails the right-hand side (RHS) of the
same pattern under the same variable instantiation.
Given the Text-Hypothesis pair (T-H) in Example 1:
Example 1.
T: Dr. Thomas Bond established a hospital in Philadel-
phia for the reception and cure of poor sick persons.
H: Dr. Bond created a medical institution for sick people.
a (directional) lexical rule like:
1) LHS: hospital? RHS: medical institution
probability: 0.8
brings to a TE system (aimed at recognizing that
a particular target meaning can be inferred from
different text variants in several NLP application,
e.g. Question Answering or Information Extraction)
the knowledge that the word hospital in Text can
be aligned, or transformed, into the word medical
institution in the Hypothesis, with a probability 0.8
that this operation preserves the entailment relation
among T and H. Similar considerations apply for
more complex rules involving verbs, as:
2) LHS: X establish Y ? RHS: X create Y
probability: 0.8
where the variables may be instantiated by any tex-
tual element with a specified syntactic relation with
the verb. Both kinds of rules are typically ac-
quired either from structured sources (e.g. WordNet
(Fellbaum, 1998)), or from unstructured sources ac-
cording for instance to distributional properties (e.g.
DIRT (Lin and Pantel, 2001)). Entailment rules
should typically be applied only in specific contexts,
defined in (Szpektor et al, 2007) as relevant con-
texts. Some existing paraphrase and entailment ac-
quisition algorithms add constraints to the learned
rules (e.g. (Sekine, 2005), (Callison-Burch, 2008)),
but most do not. Because of a lack of an adequate
representation of the linguistic context in which the
34
rules can be successfully applied, their concrete use
reflects this limitation. For instance, rule 2 (ex-
tracted from DIRT) fails if applied to ?The mathe-
matician established the validity of the conjecture?,
where the sense of establish is not a synonym of
create (but of prove, demonstrate), decreasing sys-
tem?s precision. Moreover, these rules often suffer
from lack of directionality, and from low accuracy
(i.e. the strength of association of the two sides of
the rule is often weak, and not well defined). Such
observations are also in line with the discussion on
ablation tests carried out at the last RTE evaluation
campaigns (Bentivogli et al, 2010).
Additional constraints specifying the variable
types are therefore required to correctly instantiate
them. In this work, we propose to take advantage
of Collaboratively Constructed Semantic Resources
(CSRs) (namely, Wikipedia) to mine information
useful to context-rich entailment rule acquisition.
More specifically, we take advantage of material ob-
tained through Wikipedia revisions, which provides
at the same time real textual variations from which
we may extrapolate the relevant syntactic context,
and several simplifications with respect to alterna-
tive resources. We consider T-H pairs where T is a
revision of a Wikipedia sentence and H is the origi-
nal sentence, as the revision is considered more in-
formative then the revised sentence.
We demonstrate the feasibility of the proposed
approach for the acquisition of context-rich rules
from Wikipedia revision pairs, focusing on two case
studies, i.e. the acquisition of entailment rules for
causality and for temporal expressions. Both phe-
nomena are highly frequent in TE pairs, and for both
there are no available resources yet. The result of
our experiments consists in a repository that can be
used by TE systems, and that can be easily extended
to entailment rules for other phenomena.
The paper is organized as follows. Section 2
reports on previous work, highlighting the speci-
ficity of our work. Section 3 motivates and de-
scribes the general principles underlying our ac-
quisition methodology. Section 4 describes in de-
tails the steps for context-rich rules acquisition from
Wikipedia pairs. Section 5 reports about the experi-
ments on causality and temporal expressions and the
obtained results. Finally, Section 6 concludes the pa-
per and suggests directions for future improvements.
2 Related work
The use of Wikipedia revision history in NLP tasks
has been previously investigated by a few works.
In (Zanzotto and Pennacchiotti, 2010), two versions
of Wikipedia and semi-supervised machine learning
methods are used to extract large TE data sets sim-
ilar to the ones provided for the RTE challenges.
(Yatskar et al, 2010) focus on using edit histories
in Simple English Wikipedia to extract lexical sim-
plifications. Nelken and Yamangil (2008) compare
different versions of the same document to collect
users? editorial choices, for automated text correc-
tion, sentence compression and text summarization
systems. (Max and Wisniewski, 2010) use the revi-
sion history of French Wikipedia to create a corpus
of natural rewritings, including spelling corrections,
reformulations, and other local text transformations.
In (Dutrey et al, 2011), a subpart of this corpus is
analyzed to define a typology of local modifications.
Because of its high coverage, Wikipedia is used
by the TE community for lexical-semantic rules ac-
quisition, named entity recognition, geographical in-
formation1 (e.g. (Mehdad et al, 2009), (Mirkin et
al., 2009), (Iftene and Moruz, 2010)), i.e. to provide
TE systems with world and background knowledge.
However, so far it has only been used as source of
factual knowledge, while in our work the focus is on
the acquisition of more complex rules, concerning
for instance spatial or temporal expressions.
The interest of the research community in produc-
ing specific methods to collect inference and para-
phrase pairs is proven by a number of works in the
field, which are relevant to the proposed approach.
As for paraphrase, Sekine?s Paraphrase Database
(Sekine, 2005) is collected using an unsupervised
method, and focuses on phrases connecting two
Named Entities. In the Microsoft Research Para-
phrase Corpus2, pairs of sentences are extracted
from news sources on the web, and manually an-
notated. As for rule repositories collected using dis-
tributional properties, DIRT (Discovery of Inference
Rules from Text)3 is a collection of inference rules
1http://www.aclweb.org/aclwiki/index.
php?title=RTE_Knowledge_Resources
2http://research.microsoft.com/en-us/
downloads
3http://www.aclweb.org/aclwiki/index.
php?title=DIRT_Paraphrase_Collection
35
(Lin and Pantel, 2001), obtained extracting binary
relations between a verb and an object-noun (or a
small clause) from dependency trees. Barzilay and
Lee (2003) present an approach for generating sen-
tence level paraphrases, learning structurally simi-
lar patterns of expression from data and identifying
paraphrasing pairs among them using a comparable
corpus. Since the data sets cited so far are para-
phrase collections, rules are bidirectional, while one
of the peculiarities of the entailment relation is the
directionality, addressed in our work.
Aharon et al (2010) presented FRED, an algo-
rithm for generating entailment rules between pred-
icates from FrameNet. Moreover, the TEASE col-
lection of entailment rules (Szpektor et al, 2004)
consists of 136 templates provided as input, plus
all the learned templates. Their web-based extrac-
tion algorithm is applied to acquire verb-based ex-
pressions. No directionality of the pairs is specified,
but additional guessing mechanisms it are proposed.
In (Szpektor and Dagan, 2008), two approaches for
unsupervised learning of unary rules (i.e. between
templates with a single variable) are investigated.
In (Zhao et al, 2009), a pivot approach for ex-
tracting paraphrase patterns from bilingual paral-
lel corpora is presented, while in (Callison-Burch,
2008) the quality of paraphrase extraction from par-
allel corpora is improved by requiring that phrases
and their paraphrases have the same syntactic type.
Our approach is different from theirs in many re-
spects: their goal is paraphrase extraction, while we
are extracting directional entailment rules; as textual
resources for pattern extraction they use parallel cor-
pora (using patterns in another language as pivots),
while we rely on monolingual Wikipedia revisions
(taking benefit from its increasing size); the para-
phrases they extract are more similar to DIRT, while
our approach allows to focus on the acquisition of
rules for specific phenomena frequent in entailment
pairs, and not covered by other resources.
3 General methodology
The general approach we have implemented is based
on the idea that, given a seed word, we extract all
the entailment rules from Wikipedia revision pairs
where the seed word appears as the head of the rule
either in T or H. The head is the non-variable part
of the rule on which the other parts depend (i.e. the
word establish is the head of rule 2).
Entailment judgment. A Wikipedia revision may
be consistent with the original sentence, bringing to
an entailment relation, or it may introduce inconsis-
tency, expressing a contradiction w.r.t. the original
sentence. We manually checked a sample of revision
pairs (?200), and we found out that in about 95%
of the revisions entailment is preserved, in line with
(Zanzotto and Pennacchiotti, 2010). We assume this
one as the default case in our experiments.
Monothematic pairs. The capability of automatic
extraction of entailment rules is affected by the com-
plexity of the pairs from which we extract the rules.
In our experiments we take advantage of revision
pairs with minimal difference between T and H, and
we assume that for such pairs we have only one rule
to extract. Under this perspective, T-H pairs derived
from Wikipedia revisions have strong similarity with
monothematic pairs (i.e. pairs where the entailment
judgment is due to only one linguistic phenomenon,
as suggested in (Bentivogli et al, 2010)). Section
4.2 describes the algorithm for filtering out revision
pairs with more than one phenomenon.
Directionality. A Wikipedia revision, in principle,
may be interpreted as either T entailing H, or as H
entailing T. However, through a manual inspection
of a revision sample (?200 pairs), it came out that
in most of the cases the meaning of the revised sen-
tence (T) entails the meaning of the original one (H).
Given such observation, for our experiments (Sec-
tions 4 and 5) we assume that for all revision pairs,
the revised sentence (T) entails the original one (H).
Context of a rule. We have defined the notion of
context of a rule R as a set of morpho-syntactic con-
straints C over the application of R in a specific T-H
pair. Ideally, the set of such constraints should be
the minimal set of constraints over R such that the
proportion of successful applications of R is max-
imized (e.g. the precision-recall mean is highest).
Intuitively, given an entailment rule, in absence of
constraints we have the highest recall (the rule is al-
ways applied when the LHS is activated in T and
the RHS is activated in H), although we may find
cases of wrong application of the rule (i.e. low preci-
sion). On the other side, as syntactic constraints are
36
required (e.g. the subject of a verb has to be a noun)
the number of successful applications increases, al-
though we may find cases where the constraints pre-
vent the correct application (e.g. low recall).
In the absence of a data set where we can em-
pirically estimate precision and recall of rule appli-
cation, we have approximated the ideal context on
the basis of linguistic intuitions. More specifically,
for different syntactic heads of the rules, we define
the most appropriate syntactic constraints through a
search algorithm over the syntactic tree produced on
T and H (see Section 4.4 for a detailed explanation).
4 Entailment rules acquisition
In the next sections, the steps for the acquisition of
rules from Wikipedia pairs are described in detail.
4.1 Step 1: preprocessing Wikipedia dumps
We downloaded two dumps of the English
Wikipedia (one dated 6.03.2009, Wiki 09, and
one dated 12.03.2010, Wiki 10).4 We used the
script WikiExtractor.py5 to extract plain text from
Wikipedia pages, discarding any other information
or annotation, but keeping the reference to the orig-
inal document. For our goal, we consider only non-
identical documents present in both Wiki 09 and Wiki
10 (i.e. 1,540,870 documents).
4.2 Step 2: extraction of entailment pairs
For both Wiki 09 and Wiki 10 each document has
been sentence-splitted, and the sentences of the two
versions have been aligned to create pairs. To mea-
sure the similarity between the sentences in each
pair, we adopted the Position Independent Word Er-
ror Rate (PER) (Tillmann et al, 1997), a metric
based on the calculation of the number of words
which differ between a pair of sentences (diff func-
tion in (1)). Such measure is based on Levenshtein
distance, but works at word level, and allows for re-
ordering of words and sequences of words between
the two texts (e.g. a translated text s and a reference
translation r). It is expressed by the formula:
PER(s, r) = diff(s,r)+diff(r,s)?r? (1)
4http://en.wikipedia.org/wiki/Wikipedia:
Database_download
5http://medialab.di.unipi.it/wiki/
Wikipedia_Extractor
Pairs are clustered according to different thresholds:
? Pairs composed by identical sentences were
discarded; if only one word was different in the
two sentences, we checked if it was a typo cor-
rection using (Damerau, 1964) distance. If that
was the case, we discarded such pairs as well.
? Pairs in which one of the sentences contains the
other one, meaning that the users added some
information to the new version, without modi-
fying the old one (set a: 1,547,415 pairs).
? Pairs composed by very similar sentences,
where users carried out minor editing (PER <
0.2) (set b: 1,053,114 pairs). We filtered out
pairs where differences were correction of mis-
spelling and typos, and two-word sentences.
? Pairs composed by sentences where major edit-
ing was carried out (0.2 < PER < 0.6), but still
describe the same event (set c: 2,566,364).
? Pairs in which the similarity between sentences
is low (PER > 0.6) were discarded.
To extract entailment rules, we consider only the
pairs contained in set b. For each pair, we intuitively
set the sentence extracted from Wiki 10 as the Text,
since we assume that it contains more (and more
precise) information w.r.t. the sentence extracted
from Wiki 09. We set the sentence extracted from
Wiki 09 as the Hypothesis (see Examples 2 and 3).
Example 2.
T: The Oxford Companion to Philosophy says ?there is
no single defining position that all anarchists hold [...]?
H: According to the Oxford Companion to Philosophy
?there is no single defining position that all anarchists
hold [...] ?
Example 3.
T: Bicycles are used by all socio-economic groups be-
cause of their convenience [...].
H: Bicycles are used by all socio-economic groups due to
their convenience [...].
4.3 Step 3: extraction of entailment rules
Pairs in set b are collected in a data set, and pro-
cessed with the Stanford parser (Klein and Manning,
37
2003); chunks are extracted from each pair using
the script chunklink.pl.6 The assumption underlying
our approach is that the difference between T and
H (i.e. the editing made by the user on a specific
structure) can be extracted from such pairs and
identified as an entailment rule. The rule extraction
algorithm was implemented to this purpose. In
details, for each sentence pair the algorithm itera-
tively compares the chunks of T and H to extract
the ones that differ. It can be the case that several
chunks of H are identical to a given chunk of T, as in:
T:<NP>[The DT][Oxford NNP][Companion NNP]
</NP><PP>[to TO]</PP> <NP>[Philosophy NNP]
</NP><VP>[says VBZ]</VP>...
H:<PP>[According VBG]</PP><PP>[to TO]</PP>
<NP>[the DT][Oxford NNP][Companion NNP]</NP>
<PP>[to TO]</PP><NP>[Philosophy NNP]</NP>...
Therefore, to decide for instance which chunk
<PP>[to TO]</PP> from H corresponds to the
identical chunk in T, the algorithm checks if the
previous chunks are equal as well. If this is the
case, such chunks are matched. In the example
above, the second chunk <PP>to</PP> from H
is considered as a good match because previous
chunks in T and H are equal as well (<NP>the
Oxford Companion</NP>). If the previous
chunks in T and H are not equal, the algorithm
keeps on searching. If such match is not found, the
algorithm goes back to the first matching chunk
and couples the chunk from T with it. Rules are
created setting the unmatched chunks from T as
the left-hand side of the rule, and the unmatched
chunks from H as the right-hand side of the rule.
Two consecutive chunks (different in T and H) are
considered part of the same rule. For instance, from
Examples 2 and 3:
2) <LHS> says </LHS>
<RHS> according to </RHS>
3) <LHS> because of </LHS>
<RHS> due to </RHS>
On the contrary, two non consecutive chunks gener-
ate two different entailment rules.
6http://ilk.uvt.nl/team/sabine/
chunklink/README.html
4.4 Step 4: rule expansion with minimal
context
As introduced before, our work aims at providing
precise and context-rich entailment rules, to maxi-
mize their correct application to RTE pairs. So far,
rules extracted by the rule extraction algorithm (Sec-
tion 4.3) are too general with respect to our goal.
To add the minimum context to each rule (as dis-
cussed in Section 3), we implemented a rule expan-
sion algorithm: both the file with the syntactic rep-
resentation of the pairs (obtained with the Stanford
parser), and the file with the rules extracted at Step 3
are provided as input. For every pair, and separately
for T and H, the words isolated in the corresponding
rule are matched in the syntactic tree of that sen-
tence, and the common subsumer node is detected.
Different strategies are applied to expand the rule,
according to linguistic criteria. In details, if the
common subsumer node is i) a Noun Phrase (NP)
node, the rule is left as it is; ii) a Prepositional
Phrase node (PP), all the terminal nodes of the
subtree below PP are extracted; iii) a clause intro-
duced by a subordinating conjunction (SBAR), all
the terminal nodes of the subtree below SBAR are
extracted; iv) an adjectival node (ADJP), all the
terminal nodes of the tree below the ADJP node
are extracted; v) a Verbal Phrase node (VP), the
dependency tree under the VP node is extracted.
For Example 3 (see Figure 1), the LHS of the rule
because of is matched in the syntactic tree of T and
the prepositional phrase (PP) is identified as com-
mon subsumer node. All the terminal nodes and the
PoS of the tree below PP are then extracted. The
same is done for the RHS of the rule, where the com-
mon subsumer node is an adjectival phrase (ADJP).
5 Experiments and results
In the previous section, we described the steps
carried out to acquire context-rich entailment rules
from Wikipedia revisions. To show the applicability
of the adopted methodology, we have performed
two experiments focusing, respectively, on entail-
ment rules for causality and temporal expressions.
In particular, as case studies we chose two seeds:
the conjunction because to derive rules for causality,
and the preposition before for temporal expressions.
38
(a) LHS rule (b) RHS rule
Figure 1: Rule expansion with minimal context (Example 3)
causality (because) temporal exp. (before)
(PP(RB because)(IN of)(NP(JJ)(NNS))? (SBAR(IN before)(S))?
(ADJP(JJ due)(PP(TO to)(NP(JJ)(NNS)))) (ADVP(RB prior)(PP(TO to)(S)
e.g.: because of contractual conflicts ? due to contractual conflicts e.g.: before recording them ? prior to recording them
(SBAR(IN because)(S))? (VP(PP(IN on)(NP(DT the) (ADVP(RB prior)(PP(TO to)(NP(DT)(NN))))?
(NNS grounds)))(SBAR (IN that)(S) (SBAR(IN before)(NP(DT)(NN)))
e.g.: because it penalized people ? on the grounds that it penalized people e.g.: prior to the crash ? before the crash
(PP(RB because)(IN of)(NP(DT)(NN)))? (PP(IN as)(NP (SBAR(IN until)(NP(CD)))?
(NP(DT a)(NN result))(PP(IN of)(NP(DT)(NN))))) (SBAR(IN before)(NP(CD)))
e.g.: because of an investigation ? as a result of an investigation e.g.: until 1819 ? before 1819
Table 1: Sample of extracted entailment rules.
Accordingly, we extracted from set b only the pairs
containing one of these two seeds (either in T or
in H) and we built two separate data sets for our
experiments. We run the rule extraction algorithm,
and then we filtered again the rules acquired, to
collect only those containing one of the two seeds
(either in the LHS or in the RHS). This second
filtering has been done because there could be pairs
in which either because or before are present, but
the differences in T and H do not concern those
seeds. The algorithm for rule expansion has then
been applied to the selected rules to add the minimal
context. The resulting rule for Example 3 is:
<rule ruleid="23" docid="844" pairid="15">
<LHS> (PP
(RB 8 because) (IN 9 of)(NP
(PRP 10 their)
(NN 11 convenience))) </LHS>
<RHS> (ADJP
(JJ 8 due)(PP
(TO 9 to) (NP
(PRP 10 their)
(NN 11 convenience)))) </RHS>
</rule>
To create entailment rules balancing high-
precision with their recall (Section 3), when the
words of the context added to the rule in Step 4
are identical we substitute them with their PoS. For
Example 3, the rule is generalized as follows:
<rule ruleid="23" docid="844" pairid="15">
<LHS> (PP
(RB because) (IN of)(NP
(PRP)
(NN))) </LHS>
<RHS> (ADJP
(JJ due)(PP
(TO to) (NP
(PRP)
(NN)))) </RHS>
</rule>
The intuition underlying the generalization phase is
to allow a more frequent application of the rule,
while keeping some constraints on the allowed con-
text. The application of the rule from Example 3 is
39
allowed if the subtrees below the seed words are the
same (the rule can be applied in another T-H pair as,
e.g. because of his status? due to his status).
Contradictions (e.g. antonyms and semantic op-
positions) are generally very infrequent, but in cer-
tain cases they can have high impact (one of the most
frequent rule collected for temporal expression is be-
fore S? after S). For this reason, we used WordNet
(Fellbaum, 1998) to identify and filter antonyms out
during the generalization phase. We also checked
for awkward inconsistencies due to mistakes of the
algorithm on noisy Wikipedia data (e.g. rules with
the same seed word in both the LHS and the RHS),
and we automatically filtered them out. Table 1 re-
ports a sample of rules extracted for each seed word.
Statistics about the resulting data sets, i.e. the num-
ber of acquired rules both before and after the gener-
alization phase are shown in Table 2. Identical rules
are collapsed into a unique one, but the value of their
frequency is kept in the header of that rule. Such in-
dex can then be used to estimate the correctness of
the rule and, according to our intuition, the probabil-
ity that the rule preserves the entailment relation.7
causality temporal exp.
# rules before gen. 1671 813
# rules after gen. 977 457
rules frequency ? 2 66 27
Table 2: Resulting sets of entailment rules
5.1 Evaluation
Due to the sparseness of the phenomena under con-
sideration (i.e. causality and temporal expressions)
in RTE data sets, evaluating the acquired rules on
such data does not provide interesting results.
For this reason, (following (Zhao et al, 2009),
(Callison-Burch, 2008), (Szpektor et al, 2004)), we
opted for a manual analysis of a sample of 100
rules per set, including all the rules whose fre-
quency is ?2 (Table 2), plus a random set of rules
with frequency equal to 1. Two annotators with
skills in linguistics annotated such rules according
7It is difficult to compare our results with related work, since
such phenomena are not covered by other resources. The cor-
rect comparison would be with the subset of e.g. DIRT para-
phrases dealing with causality and temporal relations, if any.
to five possible values (rules have been presented
with the sentence pairs from which they have been
acquired): entailment=yes (YES), i.e. correctness of
the rule; entailment=more-phenomena (+PHEN), i.e.
the rule is correct, but more than one phenomenon
is involved, see Section 5.2; entailment=unknown
(UNK), i.e. there is no entailment between the LHS
and the RHS of the rule, often because the editing
changed the semantics of the proposition; entail-
ment=unknown:reverse entailment (REV), wrong
directionality, i.e. the RHS of the rule entails the
LHS; entailment=error (ERR), i.e. the rule is wrong,
either because the editing in Wiki10 was done to cor-
rect mistakes, or because the rule is not well-formed
due to mistakes produced by our algorithm.
The inter-annotator agreement has been calcu-
lated, counting when judges agree on the assigned
value. It amounts to 80% on the sample of rules
for causality, and to 77% on the sample of rules for
temporal expressions. The highest inter-annotator
agreement is for correct entailment rules, whereas
the lowest agreement rates are for unknown and er-
ror judgments. This is due to the fact that detecting
correct rules is straightforward, while it is less clear
whether to consider a wrong rule as well-formed but
with an unknown judgment, or to consider it as not
appropriate (i.e. error). Table 3 shows the outcomes
of the analysis of the two sets of rules, as resulting
after a reconciliation phase carried out by the an-
notators. Such results, provided both for the whole
samples8 and for the rules whose frequency is ?2
only, are discussed in the next section.
YES +PHEN UNK REV ERR
caus.
all 67 2 13 8 10
fr?2 80.3 0 16.7 1.5 1.5
temp.
all 36 6 23 7 28
fr?2 52 3.7 37 7.3 0
Table 3: Accuracy (%) of the extracted sets of rules.
5.2 Discussion and error analysis
Due to the amount of noisy data present in
Wikipedia, on average 19% of the collected rules
8We are aware of the fact that including all the most frequent
rules in the sample biases the results upwards, but our choice is
motivated by the fact that we aim at verifying that with redun-
dancy the accuracy is actually improved.
40
include editing done by the users for spelling and
typos corrections, or are just spam (Table 3). To dis-
card such cases, spell-checkers or dictionary-based
filters should be used to improve our filtering tech-
niques. Moreover, to select only reliable rules we
consider making use of their frequency in the data to
estimate the confidence that a certain rule maintains
the entailment. The accuracy of the rules occurring
more than once is indeed much higher than the ac-
curacy estimated on the whole sample. Also the per-
centage of incorrect rules is strongly reduced when
considering redundant rules. Our assumption about
the directionality of entailment rules extracted from
Wikipedia versions is also verified (less than 10% of
the rules per set are tagged as reverse-entailment).
However, since the acquisition procedure privi-
leges precision, only a few rules appear very fre-
quently (Table 2), and this can be due to the con-
straints defined for the context extraction. This fact
motivates also the lower precision of the rules for
temporal expressions, where 73% of the sample we
analyzed involved rules with frequency equal to 1.
Moreover, in most of the rules annotated as un-
known, the editing of Wiki10 changed the semantics
of the pair, e.g. before 1990 ? 1893, or when x
produced? because x produced. Further strategies
to empirically estimate precision and recall of rule
application should be experimented as future work.
Indeed, several rules appearing only once represent
correct rules, and should not be discarded a priori.
Finally, the idea of using only very similar pairs to
extract entailment rules is based on the assumption
that such rules should concern one phenomenon at a
time (Bentivogli et al, 2010). Despite the strategies
adopted to avoid multiple phenomena per rule, in
about 10% of the cases two phenomena (e.g lexical
and syntactic) are collapsed on consecutive tokens,
making it complex to separate them automatically:
e.g. in because of his divorce settlement cost? due
to the cost of his divorces settlement, the causative
(because of x? due to x) and the argument realiza-
tion (x cost? cost of x) rules should be separated.
6 Conclusion and future work
We have presented a methodology for the automatic
acquisition of entailment rules from Wikipedia re-
vision pairs. The main benefits are the follow-
ing: i) potential large-scale acquisition, given the in-
creasing size of Wikipedia revisions; ii) new cover-
age, because Wikipedia revisions contain linguistic
phenomena (e.g. causality, temporal expressions),
which are not covered by existing resources: as a
consequence, the coverage of current TE systems
can be significantly extended; iii) quality: we intro-
duce the notion of context of a rule as the minimal
set of syntactic features maximizing its successful
application, and we have implemented it as a search
over the syntactic representation of revision pairs.
Results obtained on two experimental acquisi-
tions on causality and temporal expressions (seeds
because and before) show both good quality and
coverage of the extracted rules. The obtained re-
sources9: i) cover entailment and paraphrasing as-
pects not represented in other similar sets of rules,
ii) can be easily extended by applying the algorithms
to automatically collect rules for other phenomena
relevant to inference; and iii) are periodically up-
dated, as Wikipedia revisions change continuously.
We consider such aspects as part of our future work.
These results encourage us to further improve the
approach, considering a number of directions. First,
we plan to improve our filtering techniques to ex-
clude revision pairs containing more than one phe-
nomenon considering the syntactic structure of the
sentence. Moreover, we are planning to carry out
more extended evaluations, according to two pos-
sible strategies: i) applying the instance-based ap-
proach (Szpektor et al, 2007) on the Penn Treebank
data (i.e. for each PTB sentence that contains the
LHS of an entailment rule from our set, a pair sen-
tence will be generated by replacing the LHS of the
rule with its RHS. Human judges will then judge
each pair); ii) integrating the extracted rules into
existing TE systems. However, this evaluation has
to be carefully designed, as the ablation tests car-
ried on at the RTE challenges show. In particular,
as RTE tasks are moving towards real applications
(e.g. summarization) we think that knowledge re-
flecting real textual variations produced by humans
(as opposed to knowledge derived from linguistic re-
sources) may introduce interesting and novel hints.
9Available at http://www.aclweb.org/aclwiki/
index.php?title=Textual_Entailment_
Resource_Pool. We encourage its integration into TE
systems, to obtain feedback on its utility in TE tasks.
41
Acknowledgments
This work has been partially supported by the EC-
funded project EXCITEMENT (FP7 ICT-287923).
References
Roni Ben Aharon, Idan Szpektor, Ido Dagan. 2010. Gen-
erating Entailment Rules from FrameNet. Proceedings
of the ACL 2010 Conference Short Papers. July 11-16.
Uppsala, Sweden.
Regina Barzilay, Lillian Lee. 2003. Learning to Para-
phrase: An Unsupervised Approach Using Multiple-
Sequence Alignment. Proceedings of the HLT-
NAACL. May 27-June 1. Edmonton, Canada.
Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa T. Dang,
Danilo Giampiccolo. 2010. The Sixth PASCAL Rec-
ognizing Textual Entailment Challenge. Proceedings
of the TAC 2010 Workshop on TE. November 15-16.
Gaithersburg, Maryland.
Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-
ampiccolo, Medea Lo Leggio, Bernardo Magnini.
2010. Building Textual Entailment Specialized Data
Sets: a Methodology for Isolating Linguistic Phenom-
ena Relevant to Inference. Proceedings of the Seventh
conference on International Language Resources and
Evaluation. May 19-21. Malta.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP2008) October
25-27. Honolulu, Hawaii.
Ido Dagan, Bill Dolan, Bernardo Magnini, Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing (JNLE). Special Issue 04, volume 15, i-xvii. Cam-
bridge University Press.
Fred J. Damerau. 1964. A technique for computer de-
tection and correction of spelling errors. Commun.
ACM, 7 (3), pages 171?176. ACM, New York, NY,
USA.
Camille Dutrey, Houda Bouamor, Delphine Bernhard and
Aurelien Max 2011. Local modifications and para-
phrases in Wikipedia?s revision history. SEPLN jour-
nal (Revista de Procesamiento del Lenguaje Natural),
46:51-58.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Language, Speech and Communi-
cation. MIT Press.
Adrian Iftene, Mihai-Alex Moruz. 2010. UAIC Partici-
pation at RTE-6. Proceedings of the TAC 2010 Work-
shop on TE. November 15-16. Gaithersburg, Mary-
land.
Dan Klein, Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings of the 41st Meet-
ing of the Association for Computational Linguistics.
July 7-12. Sapporo, Japan.
Dekang Lin, Patrick Pantel. 2001. Discovery of Infer-
ence Rules for Question Answering. Natural Language
Engineering 7(4):343-360.
Rowan Nairn, Cleo Condoravdi, Lauri Karttunen. 2006.
Computing relative polarity for textual inference. In-
ference in Computational Semantics (ICoS-5). April
20-21. Buxton, UK.
Aurelien Max, Guillaume Wisniewski. 2010. Mining
naturally-occurring corrections and paraphrases from
wikipedia?s revision history. Proceedings of the Sev-
enth conference on International Language Resources
and Evaluation. May 19-21. Valletta, Malta.
Yashar Mehdad, Matteo Negri, Elena Cabrio,
Milen Kouylekov, Bernardo Magnini. 2009. Using
Lexical Resources in a Distance-Based Approach to
RTE. Proceedings of the TAC 2009 Workshop on TE.
November 17. Gaithersburg, Maryland.
Shachar Mirkin, Roy Bar-Haim, Jonathan Beran, Ido Da-
gan, Eyal Shnarch, Asher Stern, Idan Szpektor. 2009.
Addressing Discourse and Document Structure in the
RTE Search Task. Proceedings of the TAC 2009 Work-
shop on TE. November 17. Gaithersburg, Maryland.
Rani Nelken, Elif Yamangil. 2008. Mining Wikipedia?s
Article Revision History for Training Computational
Linguistics Algorithms. Proceedings of the AAAI
Workshop on Wikipedia and Artificial Intelligence.
July 13-14, Chicago, Illinois.
Satoshi Sekine. 2005. Automatic Paraphrase Discovery
based on Context and Kwywords between NE Pairs.
Proceedings of the International Workshop on Para-
phrasing (IWP-05). October 14. Jeju Island, South
Korea.
Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaven-
tura Coppola. 2004. Scaling Web-based Acquisition of
Entailment Relations. Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Language
Processing. July 25-26. Barcelona, Spain.
Idan Szpektor, Ido Dagan. 2008. Learning Entailment
Rules for Unary Templates. Proceedings of the 22nd
International Conference on Computational Linguis-
tics (Coling 2008). August 18-22. Manchester, UK.
Idan Szpektor, Eyal Shnarch, Ido Dagan. 2007.
Instance-based Evaluation of Entailment Rule Acqui-
sition. Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics. June 23-
30. Prague, Czech Republic.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, Hassan Sawaf. 1997. Accelerated DP
based search for statistical translation. Proceedings
42
of the European Conf. on Speech Communication and
Technology, pages 26672670. September. Rhodes,
Greece.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, Lillian Lee. 2010. For the sake of simplicity:
Unsupervised extraction of lexical simplifications from
Wikipedia. Proceedings of the NAACL, pp. 365-368,
2010. Short paper. June 1-6. Los Angeles, USA.
Fabio Massimo Zanzotto, Marco Pennacchiotti. 2010.
Expanding textual entailment corpora from Wikipedia
using co-training. Proceedings of the COLING-
Workshop on The Peoples Web Meets NLP: Collabo-
ratively Constructed Semantic Resources. August 28.
Beijing, China.
Shiqi Zhao, Haifeng Wang, Ting Liu, Sheng Li. 2009.
Extracting Paraphrase Patterns from Bilingual Paral-
lel Corpora. Journal of Natural Language Engineer-
ing, 15 (4): 503:526.
43
