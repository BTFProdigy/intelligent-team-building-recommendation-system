Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 467?473,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Ranking Human and Machine Summarization Systems
Peter Rankel
University of Maryland
College Park, Maryland
rankel@math.umd.edu
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland
conroyjohnm@gmail.com
Eric V. Slud
University of Maryland
College Park, Maryland
evs@math.umd.edu
Dianne P. O?Leary
University of Maryland
College Park, Maryland
oleary@cs.umd.edu
Abstract
The Text Analysis Conference (TAC) ranks
summarization systems by their average score
over a collection of document sets. We in-
vestigate the statistical appropriateness of this
score and propose an alternative that better
distinguishes between human and machine
evaluation systems.
1 Introduction
For the past several years, the National Institute of
Standards and Technology (NIST) has hosted the
Text Analysis Conference (TAC) (previously called
the Document Understanding Conference (DUC))
(Nat, 2010). A major theme of this conference is
multi-document summarization: machine summa-
rization of sets of related documents, sometimes
query-focused and sometimes generic. The sum-
marizers are judged by how well the summaries
match human-generated summaries in either auto-
matic metrics such as ROUGE (Lin and Hovy, 2003)
or manual metrics such as responsiveness or pyra-
mid evaluation (Nenkova et al, 2007). Typically the
systems are ranked by their average score over all
document sets.
Ranking by average score is quite appropriate un-
der certain statistical hypotheses, for example, when
each sample is drawn from a distribution which
differs from the distribution of other samples only
through a location shift (Randles and Wolfe, 1979).
However, a non-parametric (rank-based) analysis of
variance on the summarizers? scores on each docu-
ment set revealed an impossibly small p-value (less
Figure 1: Confidence Intervals from a non-parametric
Tukey?s honestly significant difference test for 46 TAC
2010 update document sets. The blue confidence interval
(for document set d1032) does not overlap any of the 30
red intervals. Hence, the test concludes that 30 document
sets have mean significantly different from the mean of
d1032.
467
Figure 2: Overall Responsiveness scores.
Figure 3: Linguistic scores.
Figure 4: Pyramid scores.
Figure 5: ROUGE-2 scores for the TAC 2010 update
summary task, organized by document set (y-axis) and
summarizer (x-axis). The 51 summarizers fall into two
distinct groups: machine systems (first 43 columns) and
humans (last 8 columns). Note that each human only
summarized half of the document sets, thus creating 23
missing values in each of the last 8 columns. Black is
used to indicate missing values in the last 8 columns and
low scores in the first 43 columns.
than 10?12 using Matlab?s kruskalwallis 1),
providing evidence that a summary?s score is not
independent of the document set. This effect can
be seen in Figure 1, showing the confidence bands,
as computed by a Tukey honestly significant differ-
ence test for each document set?s difficulty as mea-
sured by the mean rank responsiveness score for
TAC 2010. The test clearly shows that the summa-
rizer performances on different document sets have
different averages.
We further illustrate this in Figures 2 ? 5, which
show the scores of various summarizers on vari-
ous document sets using standard human and au-
tomatic evaluation methods (Dang and Owczarzak,
2008) of overall responsiveness, linguistic quality,
pyramid scores, and ROUGE-2 using color to indi-
cate the value of the score. Some rows are clearly
darker, indicating overall lower scores for the sum-
1The Kruskal-Wallis test performs a one-way analysis of
variance of document-set differences after first converting the
summary scores for each sample to their ranks within the pooled
sample. Computed from the converted scores, the Kruskal-
Wallis test statistic is essentially the ratio of the between-group
sum of squares to the combined within-group sum of squares.
468
maries of these documents, and the variances of the
scores differ row-by-row. These plots show qualita-
tively what the non-parametric analysis of variance
demonstrates statistically. While the data presented
was for the TAC 2010 update document sets, similar
results hold for all the TAC 2008, 2009, and 2010
data. Hence, it may be advantageous to measure
summarizer quality by accounting for heterogeneity
of documents within each test set. A non-parametric
paired test like the Wilcoxon signed-rank is one way
to do this. Another way would be paired t-tests.
In the paper (Conroy and Dang, 2008) the authors
noted that while there is a significant gap in perfor-
mance between machine systems and human sum-
marizers when measured by average manual met-
rics, this gap is not present when measured by the
averages of the best automatic metric (ROUGE). In
particular, in the DUC 2005-2007 data some systems
have ROUGE performance within the 95% confi-
dence intervals of several human summarizers, but
their pyramid, linguistic, and responsiveness scores
do not achieve this level of performance. Thus,
the inexpensive automatic metrics, as currently em-
ployed, do not predict well how machine summaries
compare to human summaries.
In this work we explore the use of document-
paired testing for summarizer comparison. Our main
approach is to consider each pair of two summa-
rizers? sets of scores (over all documents) as a bal-
anced two-sample dataset, and to assess that pair?s
mean difference in scores through a two-sample T
or Wilcoxon test, paired or unpaired. Our goal has
been to confirm that human summarizer scores are
uniformly different and better on average than ma-
chine summarizer scores, and to rate the quality of
the statistical method (T or W, paired or unpaired)
by the consistency with which the human versus
machine scores show superior human performance.
Our hope is that paired testing, using either the stan-
dard paired two-sample t-test or the distribution-
free Wilcoxon signed-rank test, can provide greater
power in the statistical analysis of automatic metrics
such as ROUGE.
2 Size and Power of Tests
Statistical tests are generally compared by choosing
rejection thresholds to achieve a certain small prob-
ability of Type I error (usually as ? = .05). Given
multiple tests with the same Type I error, one prefers
the test with the smallest probability of Type II error.
Since power is defined to be one minus the Type II
error probability, we prefer the test with the most
power. Recall that a test-statistic S depending on
available data-samples gives rise to a rejection re-
gion by defining rejection of the null hypothesis H0
as the event {S ? c} for a cutoff or rejection thresh-
old c chosen so that
P (S ? c) ? ?
for all probability laws compatible with the null hy-
pothesis where the (nominal) significance level ?
is chosen in advance by the statistician, usually as
? = .05. However, in many settings, the null hy-
pothesis comprises many possible probability laws,
as here where the null hypothesis is that the under-
lying probability laws for the score-samples of two
separate summarizers are equal, without specifying
exactly what that probability distribution is. In this
case, the significance level is an upper bound for the
attained size of the test, defined as supP?H0 P (S ?
c), the largest rejection probability P (S ? c)
achieved by any probability law compatible with the
null hypothesis. The power of the test then depends
on the specific probability law Q from the consid-
ered alternatives in HA. For each such Q, and given
a threshold c, the power for the test at Q is the re-
jection probability Q(S ? c). These definitions re-
flect the fact that the null and alternative hypothe-
ses are composite, that is, each consists of multiple
probability laws for the data. One of the advan-
tages of considering a distribution-free two-sample
test statistic such as the Wilcoxon is that the proba-
bility distribution for the statistic S is then the same
for all (continuous, or non-discrete) probability laws
P ? H0, so that one cutoff c serves for all of H0
with all rejection probabilities equal to ?. 2
Two test statistics, say S and S?, are generally
compared in terms of their powers at fixed alterna-
tives Q in the alternative hypothesis HA, when their
respective thresholds c, c? have been defined so that
the sizes of the respective tests, supP?H0 P (S ?
2The Wilcoxon test is not distribution-free for discrete data.
However, the discrete TAC data can be thought of as rounded
continuous data, rather than as truly discrete data.
469
c) and supP?H0 P (S? ? c?), are approximatelyequal. In this paper, the test statistics under consid-
eration are ? in one-sided testing ? the (unpaired)
two-sample t test with pooled sample variance (T ),
the paired two-sample t test (T p), and the (paired)
signed-rank Wilcoxon test (W ); and for two-sided
testing, S is defined by the absolute value of one
of these statistics. The thresholds c for the tests
can be defined either by theoretical distributions, by
large-sample approximations, or by data-resampling
(bootstrap) techniques, and (only) in the last case
are these thresholds data-dependent, or random. We
explain these notions with respect to the two-sample
data-structure in which the scores from the first sum-
marizer are denoted X1, . . . , Xn, where n is the
number of documents with non-missing scores for
both summarizers, and the scores from the second
summarizer are Y1, . . . , Yn. Let Zk = Xk ? Yk
denote the document-wise differences between the
summarizers? scores, and Z? = n?1?nk=1 Zk betheir average. Then the paired statistics are defined
as
T p =
?
n(n? 1) Z?/(
n?
k=1
(Zk ? Z?)2)1/2
and
W =
n?
k=1
sgn(Zk)R+k
where R+k is the rank of |Zk| among
|Z1|, . . . , |Zn|. Note that under both null and alter-
native hypotheses, the variates Zk are assumed in-
dependent identically distributed (iid), while under
H0, the random variables Zk are symmetric about 0.
The t-statistic T p is ?parametric? in the sense that
exact theoretical calculations of probabilities P (a <
T p < b) depend on the assumption of normality of
the differences Zk, and when that holds, the two-
sided cutoff c = c(T p) is defined as the 1 ? ?/2
quantile of the tn?1 distribution with n ? 1 degrees
of freedom. However, when n is moderately or
very large, the cutoff is well approximated by the
standard-normal 1 ? ?/2 quantile z?/2, and T p be-
comes approximately nonparametrically valid with
this cutoff, by the Central Limit Theorem. The
Wilcoxon signed-rank statistic W has theoretical
cutoff c = c(W ) which depends only on n, when-
ever the data Zk are continuously distributed; but for
large n, the cutoff is given simply as ?n3/12 ? z?/2.
When there are ties (as might be common in discrete
data), the calculation of cutoffs and p-values for
Wilcoxon becomes slightly more complicated and
is no longer fully nonparametric except in a large-
sample approximate sense.
The situation for the two-sample unpaired t-
statistic T currently used in TAC evaluation is not
so neat. Even when the two samplesX = {Xk}nk=1and Y = {Yk}nk=1 are independent, exact theoret-ical distribution of cutoffs is known only under the
parametric assumption that the scores are normally
distributed (and in the case of the pooled-sample-
variance statistic, that Var(Xk) = Var(Yk).) How-
ever, an essential element of the summarization data
is the heterogeneity of documents. This means that
while {Xk}nk=1 can be viewed as iid scores whendocuments are selected randomly ? and not neces-
sarily equiprobably ? from the ensemble of all pos-
sible documents, the Yk and Xk samples are de-
pendent. Still, the pairs {(Xk, Yk)}nk=1, and there-fore the differences {Zk}nk=1, are iid which is whatmakes paired testing valid. However, there is no the-
oretical distribution for T from which to calculate
valid quantiles c for cutoffs, and therefore the use of
the unpaired t-statistic cannot be recommended for
TAC evaluation.
What can be done in a particular dataset, like the
TAC summarization score datsets we consider, to
ascertain the approximate validity of theoretically
derived large-sample cutoffs for test statistics? In
the age of plentiful and fast computers, quite a lot,
through the powerful computational machinery of
the bootstrap (Efron and Tibshirani, 1993).
The idea of bootstrap hypothesis testing (Efron
and Tibshirani, 1993), (Bickel and Ren, 2001) is to
randomly sample with replacement (the rows with
non-missing data in) the dataset {(Xk, Yk)}nk=1 insuch a way as to generate representative data that
plausibly would have been seen if two-sample score
data had been generated from two equally effec-
tive summarizers with score distributional charac-
teristics like the pooled scores from the two ob-
served summarizers. We have done this in two dis-
tinct ways, each creating 2000 datasets with n paired
scores:
MC Monte Carlo Method. For each of many it-
470
erations (in our case 2000), define a new
dataset {(X ?k, Y ?k)}nk=1 by independently swap-ping Xk and Yk with probability 1/2. Hence,
(X ?k, Y ?k) = (Xk, Yk) with probability 1/2 and
(Yk, Xk) with probability 1/2.
HB Hybrid MC/Bootstrap. For each of 2000
iterations, create a re-sampled dataset
{(X ??k , Y ??k )}nk=1 in the following way. First,sample n pairs (Xk, Yk) with replacement
from the original dataset. Then, as above,
randomly swap the components of each pair,
each with 1/2 probability.
Both of these two methods can be seen to gener-
ate two-sample data satisfying H0, with each score-
sample?s distribution obtained as a mixture of the
distributions actually generating the X and Y sam-
ples. The empirical qth quantiles for a statistic
S = S(X,Y) such as |W | or |T p| are estimated
from the resampled data as F??1S (q), where F?S(t) issimply the fraction of times (out of 2000) that the
statistic S applied to the constructed dataset had a
value less than or equal to t. The upshot is that the
1 ? ? empirical quantile for S based on either of
these simulation methods serves as a data-dependent
cutoff c attaining approximate size ? for all H0-
generated data. The MC and HB methods will be
employed in Section 4 to check the theoretical p-
values.
3 Relative Efficiency ofW versus T p
Statistical theory does have something to say about
the comparative powers of paired W versus T p
statistics. These statistics have been studied (Ran-
dles and Wolfe, 1979), in terms of their asymp-
totic relative efficiency for location-shift alternatives
based on symmetric densities (f(z??) is a location-
shift of f(z)). For many pairs of parametric and
rank-based statistics S, S?, including W and T p, the
following assertion has been proved for testing H0
at significance level ?.
First assume the Zk are distributed according to
some density f(z ? ?), where f(z) is a symmet-
ric function (f(?z) = f(z)). Next assume ? = 0
under H0. When n gets large the powers at any al-
ternatives with very small ? = ?/?n, ? 6= 0, can
be made asymptotically equal by using samples of
size n with statistic S and of size ? ? n with statistic
S?. Here ? = ARE(S, S?) is a constant not depend-
ing on n or ? but definitely depending on f , called
asymptotic relative efficiency of S with respect to S?.
(The smaller ? < 1 is, the more statistic S? is pre-
ferred among the two.)
Using this definition, it is known (Randles and
Wolfe 1979, Sec. 5.4 leading up to Table 5.4.7 on
p. 167) that the Wilcoxon signed-rank statistic W
provides greater robustness and often much greater
efficiency than the paired T, with ARE which is 0.95
with f a standard normal density, and which is never
less than 0.864 for any symmmetric density f . How-
ever, in our context, continuous scores such as pyra-
mid exhibit document-specific score differences be-
tween summarizers which often have approximately
normal-looking histograms, and although the alter-
natives perhaps cannot be viewed as pure location
shifts, it is unsurprising in view of the ARE theory
cited above that the W and T paired tests have very
similar performance. Nevertheless, as we found by
statistical analysis of the TAC data, both are far su-
perior to the unpaired T-statistic, with either theoret-
ical or empirical bootstrapped p-values.
4 Testing Setup and Results
To evaluate our ideas, we used the TAC data from
2008-2010 and focused on three manual metrics
(overall responsiveness, pyramid score, and lin-
guistic quality score) and two automatic metrics
(ROUGE-2 and ROUGE-SU4). We make the as-
sumption, backed by both the scores given and com-
ments made by NIST summary assessors 3, that au-
tomatic summarization systems do not perform at
the human level of performance. As such, if a statis-
tic based on an automatic metric, such as ROUGE-
2, were to show fewer systems performing at human
level of performance than the statistic of averaging
scores, such a statistic would be preferable because
3Assessors have commented privately at the Text Analysis
Conference 2008, that while the origin of the summary is hid-
den from them, ?we know which ones are machine generated.?
Thus, automatic summarization fails the Turing test of machine
intelligence (Turing, 1950). This belief is also supported by
(Conroy and Dang, 2008) and (Dang and Owczarzak, 2008). Fi-
nally, our own results show no matter how you compare human
and machine scores all machines systems score significantly
worse than humans.
471
2008: 2145 = (662
) pairs 2009: 1830 = (612
) pairs 2010: 1275 = (512
) pairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 1234 1416 1410 1000 1182 1173 841 939 934
Overall 1202 1353 1342 982 1149 1146 845 894 889
Pyramid 1263 1417 1418 1075 1238 1216 875 933 926
ROUGE-2 1243 1453 1459 1016 1182 1193 812 938 939
ROUGE-SU4 1333 1493 1507 1059 1241 1254 894 983 976
Table 1: Number of significant differences found when testing for the difference of all pairs of summarization systems
(including humans).
2008: 464 = 58? 8 pairs 2009: 424 = 53? 8 pairs 2010: 344 = 43? 8 pairs
Metric Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc. Unpair-T Pair-T Wilc.
Linguistic 464 464 464 424 424 424 344 344 344
Overall 464 464 464 424 424 424 344 344 344
Pyramid 464 464 464 424 424 424 344 344 344
ROUGE-2 375 409 402 323 350 341 275 309 305
ROUGE-SU4 391 418 414 354 378 373 324 331 328
Table 2: Number of significant differences resulting from 8 ? (N ? 8) tests for human-machine system means or
signed-rank comparisons.
of its greater power in the machine vs. human sum-
marization domain.
For each of these metrics, we first created a score
matrix whose (i, j)-entry represents the score for
summarizer j on document set i (these matrices gen-
erated the colorplots in Figures 2 ? 5). We then per-
formed a Wilcoxon signed-rank test on certain pairs
of columns of this matrix (any pair consisting of one
machine system and one human summarizer). As a
baseline, we did the same testing with a paired and
an unpaired t-test. Each of these tests resulted in a
p-value, and we counted how many were less than
.05 and called these the significant differences.
The results of these tests (shown in Table 2),
were somewhat surprising. Although we expected
the nonparametric signed-rank test to perform better
than an unpaired t-test, we were surprised to see that
a paired t-test performed even better. All three tests
always reject the null hypotheses when human met-
rics are used. This is what we?d like to happen with
automatic metrics as well. As seen from the table,
the paired t-test and Wilcoxon signed-rank test offer
a good improvement over the unpaired t-test.
The results in Table 1 are less clear, but still posi-
tive. In this case, we are comparing pairs of machine
summarization systems. In contrast to the human vs.
machine case, we do not know the truth here. How-
ever, since the number of significant differences in-
creases with paired testing here as well, we believe
this also reflects the greater discriminatory power of
paired testing.
We now apply the Monte Carlo and Hybrid Monte
Carlo to check the theoretical p-values reported in
Tables 1 and 2. The empirical quantiles found
by these methods generally confirm the theoreti-
cal p-value test results reported there, especially
in Table 2. In the overall tallies of all compar-
isons (Table 1), it seems that the bootstrap results
(comparing only W and the un-paired T ) make
W look still stronger for linguistic and overall re-
sponsiveness versus the T ; but for the pyramid
and ROUGE scores, the bootstrap p-values bring T
slightly closer to W although it still remains clearly
inferior, achieving roughly 10% fewer rejections.
5 Conclusions and Future Work
In this paper we observed that summarization sys-
tems? performance varied significantly across doc-
ument sets on the Text Analysis Conference (TAC)
data. This variance in performance suggested that
paired testing may be more appropriate than the
t-test currently employed at TAC to compare the
472
performance of summarization systems. We pro-
posed a non-parametric test, the Wilcoxon signed-
rank test, as a robust more powerful alternative to
the t-test. We estimated the statistical power of the
t-test and the Wilcoxon signed-rank test by calcu-
lating the number of machine systems whose per-
formance was significantly different than that of hu-
man summarizers. As human assessors score ma-
chine systems as not achieving human performance
in either content or responsiveness, automatic met-
rics such as ROUGE should ideally indicate this dis-
tinction. We found that the paired Wilcoxon test
significantly increases the number of machine sys-
tems that score significantly different than humans
when the pairwise test is performed on ROUGE-2
and ROUGE-SU4 scores. Thus, we demonstrated
that the Wilcoxon paired test shows more statistical
power than the t-test for comparing summarization
systems.
Consequently, the use of paired testing should not
only be used in formal evaluations such as TAC, but
also should be employed by summarization devel-
opers to more accurately assess whether changes to
an automatic system give rise to improved perfor-
mance.
Further study needs to analyze more summariza-
tion metrics such as those proposed at the recent
NIST evaluation of automatic metrics, Automati-
cally Evaluating Summaries of Peers (AESOP) (Nat,
2010). As metrics become more sophisticated and
aim to more accurately predict human judgements
such as overall responsiveness and linguistic qual-
ity, paired testing seems likely to be a more power-
ful statistical procedure than the unpaired t-test for
head-to-head summarizer comparisons.
Throughout our research in this paper, we treated
each separate kind of scores on a document set as
data for one summarizer to be compared with the
same kind of scores for other summarizers. How-
ever, it might be more fruitful to treat all the scores
as multivariate data and compare the summarizers
that way. Multivariate statistical techniques such as
Principal Component Analysis may play a construc-
tive role in suggesting highly discriminating new
composite scores, perhaps leading to statistics with
even more power to measure a summary?s quality.
ROUGE was inspired by the success of the
BLEU (BiLingual Evaluation Understudy), an n-
gram based evaluation for machine translation (Pa-
pineni et al, 2002). It is likely that paired testing
may also be appropriate for BLEU as well and will
give additional discriminating power between ma-
chine translations and human translations.
References
Peter J. Bickel and Jian-Jian Ren. 2001. The Bootstrap
in Hypothesis Testing. In State of the Art in Statistics
and Probability Theory, Festschrift for Willem R. van
Zwet, volume 36 of Lecture Notes? Monograph Series,
pages 91?112. Institute of Mathematical Statistics.
John M. Conroy and Hoa Trang Dang. 2008. Mind the
Gap: Dangers of Divorcing Evaluations of Summary
Content from Linguistic Quality. In Proceedings of
the 22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 145?152,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hoa T. Dang and Karolina Owczarzak. 2008. Overview
of the tac 2008 update summarization task. In Pro-
ceedings of the 1st Text Analysis Conference (TAC),
Gaithersburg, Maryland, USA.
B. Efron and R. J. Tibshirani. 1993. An Introduction to
the Bootstrap. Chapman & Hall, New York.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic Eval-
uation of Summaries Using N-gram Co-Occurrences
Statistics. In Proceedings of the Conference of the
North American Chapter of the Association for Com-
putational Linguistics, Edmonton, Alberta.
National Institute of Standards and Technology. 2010.
Text Analysis Conference, http://www.nist.gov/tac.
Ani Nenkova, Rebecca Passonneau, and Kathleen McK-
eown. 2007. The Pyramid Method: Incorporating
Human Content Selection Variation in Summarization
Evaluation. ACM Transactions on Speech and Lan-
guage Processing, 4(2).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
R.H. Randles and D.A. Wolfe. 1979. Introduction to
the Theory of Nonparametric Statistics. Wiley series
in probability and mathematical statistics. Probability
and mathematical statistics. Wiley.
Alan Turing. 1950. Computing Machinery and Intelli-
gence. Mind, 59(236):433?460.
473
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?362,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Assessing the Effect of Inconsistent Assessors on Summarization Evaluation
Karolina Owczarzak
National Institute of Standards and Technology
Gaithersburg, MD 20899
karolina.owczarzak@gmail.com
Peter A. Rankel
University of Maryland
College Park, Maryland
rankel@math.umd.edu
Hoa Trang Dang
National Institute of Standards and Technology
Gaithersburg, MD 20899
hoa.dang@nist.gov
John M. Conroy
IDA/Center for Computing Sciences
Bowie, Maryland
conroy@super.org
Abstract
We investigate the consistency of human as-
sessors involved in summarization evaluation
to understand its effect on system ranking and
automatic evaluation techniques. Using Text
Analysis Conference data, we measure anno-
tator consistency based on human scoring of
summaries for Responsiveness, Readability,
and Pyramid scoring. We identify inconsis-
tencies in the data and measure to what ex-
tent these inconsistencies affect the ranking
of automatic summarization systems. Finally,
we examine the stability of automatic metrics
(ROUGE and CLASSY) with respect to the
inconsistent assessments.
1 Introduction
Automatic summarization of documents is a re-
search area that unfortunately depends on human
feedback. Although attempts have been made at au-
tomating the evaluation of summaries, none is so
good as to remove the need for human assessors.
Human judgment of summaries, however, is not per-
fect either. We investigate two ways of measuring
evaluation consistency in order to see what effect it
has on summarization evaluation and training of au-
tomatic evaluation metrics.
2 Assessor consistency
In the Text Analysis Conference (TAC) Summariza-
tion track, participants are allowed to submit more
than one run (usually two), and this option is of-
ten used to test different settings or versions of the
same summarization system. In cases when the sys-
tem versions are not too divergent, they sometimes
produce identical summaries for a given topic. Sum-
maries are randomized within each topic before they
are evaluated, so the identical copies are usually in-
terspersed with 40-50 other summaries for the same
topic and are not evaluated in a row. Given that each
topic is evaluated by a single assessor, it then be-
comes possible to check assessor consistency, i.e.,
whether the assessor judged the two identical sum-
maries in the same way.
For each summary, assessors conduct content
evaluation according to the Pyramid framework
(Nenkova and Passonneau, 2004) and assign it Re-
sponsiveness and Readability scores1, so assessor
consistency can be checked in these three areas sep-
arately. We found between 230 (in 2009) and 430
(in 2011) pairs of identical summaries for the 2008-
2011 data (given on average 45 topics, 50 runs, and
two summarization conditions: main and update),
giving in effect anywhere from around 30 to 60 in-
stances per assessor per year. Using Krippendorff?s
alpha (Freelon, 2004), we calculated assessor con-
sistency within each year, as well as total consis-
tency over all years? data (for those assessors who
worked multiple years). Table 1 shows rankings of
assessors in 2011, based on their Readability, Re-
sponsiveness, and Pyramid judgments for identical
summary pairs (around 60 pairs per assessor).
Interestingly, consistency values for Readability
are lower overall than those for Responsiveness and
Pyramid, even for the most consistent assessors.
Given that Readability and Responsiveness are eval-
uated in the same way, i.e. by assigning a numeri-
cal score according to detailed guidelines, this sug-
1http://www.nist.gov/tac/2011/Summarization/Guided-
Summ.2011.guidelines.html
359
ID Read ID Resp ID Pyr
G 0.867 G 0.931 G 0.975
D 0.866 D 0.875 D 0.970
A 0.801 H 0.808 H 0.935
H 0.783 A 0.750 A 0.931
F 0.647 F 0.720 E 0.909
C 0.641 E 0.711 C 0.886
E 0.519 C 0.490 F 0.872
Table 1: Annotator consistency in assigning Readability
and Responsiveness scores and in Pyramid evaluation, as
represented by Krippendorff?s alpha for interval values,
on 2011 data.
gests that Readability as a quality of text is inher-
ently more vague and difficult to pinpoint.
On the other hand, Pyramid consistency values
are generally the highest, which can be explained
by how the Pyramid evaluation is designed. Even
if the assessor is inconsistent in selecting Sum-
mary Content Units (SCUs) across different sum-
maries, as long as the total summary weight is sim-
ilar, the summary?s final score will be similar, too.2
Therefore, it would be better to look at whether as-
sessors tend to find the same SCUs (information
?nuggets?) in different summaries on the same topic,
and whether they annotate them consistently. This
can be done using the ?autoannotate? function of
the Pyramid process, where all SCU contributors
(selected text strings) from already annotated sum-
maries are matched against the text of a candidate
(un-annotated) summary. The autoannotate func-
tion works fairly well for matching between extrac-
tive summaries, which tend to repeat verbatim whole
sentences from source documents.
For each summary in 2008-2011 data, we autoan-
notated it using all remaining manually-annotated
summaries from the same topic, and then we com-
pared the resulting ?autoPyramid? score with the
score from the original manual annotation for that
summary. Ideally, the autoPyramid score should
be lower or equal to the manual Pyramid score: it
would mean that in this summary, the assessor se-
lected as relevant all the same strings as s/he found
in the other summaries on the same topic, plus possi-
bly some more information that did not appear any-
2The final score is based on total weight of all SCUs found
in the summary, so the same weight can be obtained by select-
ing a larger number of lower-weight SCUs or a smaller number
of higher-weight SCUs (or the same number of similar-weight
SCUs which nevertheless denote different content).
Figure 1: Annotator consistency in selecting SCUs in
Pyramid evaluation, as represented by the difference be-
tween manual Pyramid and automatic Pyramid scores
(mP-aP), on 2011 data.
where else. If the autoPyramid score is higher than
the manual Pyramid score, it means that either (1)
the assessor missed relevant strings in this summary,
but found them in other summaries; or (2) the strings
selected as relevant elsewhere in the topic were acci-
dental, and as such not repeated in this summary. Ei-
ther way, if we then average out score differences for
all summaries for a given topic, it will give us a good
picture of the annotation consistency in this partic-
ular topic. Higher average autoPyramid scores sug-
gest that the assessor was missing content, or other-
wise making frequent random mistakes in assigning
content. Figure 1 shows the macro-average differ-
ence between manual Pyramid scores and autoPyra-
mid scores for each assessor in 2011.3 For the most
part, it mirrors the consistency ranking from Table
1, confirming that some assessors are less consistent
than others; however, certain differences appear: for
instance, Assessor A is one of the most consistent in
assigning Readability scores, but is not very good at
selecting SCUs consistently. This can be explained
by the fact that the Pyramid evaluation and assigning
Readability scores are different processes and might
require different skills and types of focus.
3 Impact on evaluation
Since human assessment is used to rank participat-
ing summarizers in the TAC Summarization track,
3Due to space constraints, we report figures for only 2011,
but the results for other years are similar.
360
Pearson?s r Spearman?s rho
-1 worst -2 worst -1 worst -2 worst
Readability 0.995 0.993 0.988 0.986
Responsiveness 0.996 0.989 0.986 0.946
Pyramid 0.996 0.992 0.978 0.960
mP-aP 0.996 0.987 0.975 0.943
Table 2: Correlation between the original summarizer
ranking and the ranking after excluding topics by one or
two worst assessors in each category.
we should examine the potential impact of incon-
sistent assessors on the overall evaluation. Because
the final summarizer score is the average over many
topics, and the topics are fairly evenly distributed
among assessors for annotation, excluding noisy
topics/assessors has very little impact on summa-
rizer ranking. As an example, consider the 2011 as-
sessor consistency data in Table 1 and Figure 1. If
we exclude topics by the worst performing assessor
from each of these categories, recalculate the sum-
marizer rankings, and then check the correlation be-
tween the original and newly created rankings, we
obtain results in Table 2.
Although the impact on evaluating automatic
summarizers is small, it could be argued that exclud-
ing topics with inconsistent human scoring will have
an impact on the performance of automatic evalua-
tion metrics, which might be unfairly penalized by
their inability to emulate random human mistakes.
Table 3 shows ROUGE-2 (Lin, 2004), one of the
state-of-the-art automatic metrics used in TAC, and
its correlations with human metrics, before and af-
ter exclusion of noisy topics from 2011 data. The
results are fairly inconclusive: it seems that in most
cases, removing topics does more harm than good,
suggesting that the signal-to-noise ratio is still tipped
in favor of signal. The only exception is Readability,
where ROUGE records a slight increase in correla-
tion; this is unsurprising, given that consistency val-
ues for Readability are the lowest of all categories,
and perhaps here removing noise has more impact.
In the case of Pyramid, there is a small gain when
we exclude the single worst assessor, but excluding
two assessors results in a decreased correlation, per-
haps because we remove too much valid information
at the same time.
A different picture emerges when we examine
how well ROUGE-2 can predict human scores on
the summary level. We pooled together all sum-
Readability Responsiveness Pyramid mP-aP
before 0.705 0.930 0.954 0.954
-1 worst 0.718 0.921 0.961 0.942
-2 worst 0.718 0.904 0.952 0.923
Table 3: Correlation between the summarizer rankings
according to ROUGE-2 and human metrics, before and
after excluding topics by one or two worst assessors in
that category.
Readability Responsiveness Pyramid mP-aP
before 0.579 0.694 0.771 0.771
-1 worst 0.626 0.695 0.828 0.752
-2 worst 0.628 0.721 0.817 0.741
Table 4: Correlation between ROUGE-2 and human met-
rics on a summary level before and after excluding topics
by one or two worst assessors in that category.
maries annotated by each particular assessor and cal-
culated the correlation between ROUGE-2 and this
assessor?s manual scores for individual summaries.
Then we calculated the mean correlation over all
assessors. Unsurprisingly, inconsistent assessors
tend to correlate poorly with automatic (and there-
fore always consistent) metrics, so excluding one
or two worst assessors from each category increases
ROUGE?s average per-assessor summary-level cor-
relation, as can be seen in Table 4. The only ex-
ception here is when we exclude assessors based on
their autoPyramid performance: again, because in-
consistent SCU selection doesn?t necessarily trans-
late into inconsistent final Pyramid scores, exclud-
ing those assessors doesn?t do much for ROUGE-2.
4 Impact on training
Another area where excluding noisy topics might be
useful is in training new automatic evaluation met-
rics. To examine this issue we turned to CLASSY
(Rankel et al, 2011), an automatic evaluation met-
ric submitted to TAC each year from 2009-2011.
CLASSY consists of four different versions, each
aimed at predicting a particular human evaluation
score. Each version of CLASSY is based on one
of three regression methods: robust regression, non-
negative least squares, or canonical correlation. The
regressions are calculated based on a collection of
linguistic and content features, derived from the
summary to be scored.
CLASSY requires two years of marked data to
score summaries in a new year. In order to predict
361
the human metrics in 2011, for example, CLASSY
uses the human ratings from 2009 and 2010. It first
considers each subset of the features in turn, and us-
ing each of the regression methods, fits a model to
the 2009 data. The subset/method combination that
best predicts the 2010 scores is then used to pre-
dict scores for 2011. However, the model is first re-
trained on the 2010 data to calculate the coefficients
to be used in predicting 2011.
First, we trained all four CLASSY versions on
all available 2009-2010 topics, and then trained
again excluding topics by the most inconsistent as-
sessor(s). A different subset of topics was ex-
cluded depending on whether this particular version
of CLASSY was aiming to predict Responsiveness,
Readability, or the Pyramid score. Then we tested
CLASSY?s performance on 2011 data, ranking ei-
ther automatic summarizers (NoModels case) or hu-
man and automatic summarizers together (AllPeers
case), separately for main and update summaries,
and calculated its correlation with the metrics it was
aiming to predict. Table 5 shows the result of this
comparison. For Pyramid, (a) indicates that ex-
cluded topics were selected based on Krippendorff?s
alpha, and (b) indicates that topics were excluded
based on their mean difference between manual and
automatic Pyramid scores.
The results are encouraging; it seems that remov-
ing noisy topics from training data does improve the
correlations with manual metrics in most cases. The
greatest increase takes place in CLASSY?s correla-
tions with Responsiveness for main summaries in
AllPeers case, and for correlations with Readabil-
ity. While none of the changes are large enough
to achieve statistical significance, the pattern of im-
provement is fairly consistent.
5 Conclusions
We investigated the consistency of human assessors
in the area of summarization evaluation. We con-
sidered two ways of measuring assessor consistency,
depending on the metric, and studied the impact of
consistent scoring on ranking summarization sys-
tems and on the performance of automatic evalu-
ation systems. We found that summarization sys-
tem ranking, based on scores for multiple topics,
was surprisingly stable and didn?t change signifi-
NoModels AllPeers
main update main update
Pyramid
CLASSY1 Pyr 0.956 0.898 0.945 0.936
CLASSY1 Pyr new (a) 0.950 0.895 0.932 0.955
CLASSY1 Pyr new (b) 0.960 0.900 0.940 0.955
Responsiveness
CLASSY2 Resp 0.951 0.903 0.948 0.963
CLASSY2 Resp new 0.954 0.907 0.973 0.950
CLASSY4 Resp 0.951 0.927 0.830 0.949
CLASSY4 Resp new 0.943 0.928 0.887 0.946
Readability
CLASSY3 Read 0.768 0.705 0.844 0.907
CLASSY3 Read new 0.793 0.721 0.858 0.906
Table 5: Correlations between CLASSY and human met-
rics on 2011 data (main and update summaries), before
and after excluding most inconsistent topic from 2009-
2010 training data for CLASSY.
cantly when several topics were removed from con-
sideration. However, on a summary level, remov-
ing topics scored by the most inconsistent assessors
helped ROUGE-2 increase its correlation with hu-
man metrics. In the area of training automatic met-
rics, we found some encouraging results; removing
noise from the training data allowed most CLASSY
versions to improve their correlations with the man-
ual metrics that they were aiming to model.
References
Deen G. Freelon. 2010. ReCal: Intercoder Reliability
Calculation as a Web Service. International Journal
of Internet Science, Vol 5(1).
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop,
78?81. Barcelona, Spain.
Ani Nenkova and Rebecca J. Passonneau. 2004. Evaluat-
ing content selection in summarization: The Pyramid
method. Proceedings of the Human Language Tech-
nology Conference of the North American Chapter of
the Association for Computational Linguistics, 145?
152. Boston, MA.
Rebecca J. Passonneau, Ani Nenkova, Kathleen McKe-
own, and Sergey Sigelman. 2005. Applying the Pyra-
mid method in DUC 2005. Proceedings of the 5th
Document Understanding Conference (DUC). Van-
couver, Canada.
Peter A. Rankel, John M. Conroy, and Judith D.
Schlesinger. 2012. Better Metrics to Automatically
Predict the Quality of a Text Summary. Proceedings
of the SIAM Data Mining Text Mining Workshop 2012.
362
