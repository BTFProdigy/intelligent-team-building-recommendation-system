Proceedings of NAACL HLT 2009: Short Papers, pages 49?52,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Modeling Dialogue Structure with  Adjacency Pair Analysis and Hidden Markov Models  Kristy Elizabeth Boyer*1 Robert Phillips1,2 Eun Young Ha1 Michael D.  Wallis1,2 Mladen A.  Vouk1 James C. Lester1  1Department of Computer Science North Carolina State University Raleigh, NC, USA  2Applied Research Associates Raleigh, NC, USA  *keboyer@ncsu.edu  Abstract 
Automatically detecting dialogue structure within corpora of human-human dialogue is the subject of increasing attention.  In the do-main of tutorial dialogue, automatic discovery of dialogue structure is of particular interest because these structures inherently represent tutorial strategies or modes, the study of which is key to the design of intelligent tutor-ing systems that communicate with learners through natural language.  We propose a methodology in which a corpus of human-human tutorial dialogue is first manually an-notated with dialogue acts.  Dependent adja-cency pairs of these acts are then identified through ?2 analysis, and hidden Markov mod-eling is applied to the observed sequences to induce a descriptive model of the dialogue structure.       
1 Introduction Automatically learning dialogue structure from corpora is an active area of research driven by a recognition of the value offered by data-driven ap-proaches (e.g., Bangalore et al, 2006).  Dialogue structure information is of particular importance when the interaction is centered around a learning task, such as in natural language tutoring, because techniques that support empirical identification of dialogue strategies can inform not only the design of intelligent tutoring systems (Forbes-Riley et al, 2007), but also contribute to our understanding of 
the cognitive and affective processes involved in learning through tutoring (VanLehn et al, 2007).        Although traditional top-down approaches (e.g., Cade et al, 2008) and some empirical work on analyzing the structure of tutorial dialogue (Forbes-Riley et al, 2007) have yielded significant results, the field is limited by the lack of an auto-matic, data-driven approach to identifying dialogue structure.  An empirical approach to identifying tutorial dialogue strategies, or modes, could ad-dress this limitation by providing a mechanism for describing in succinct probabilistic terms the tuto-rial strategies that actually occur in a corpus.      Just as early work on dialogue act interpretation utilized hidden Markov models (HMMs) to capture linguistic structure (Stolcke et al, 2000), we pro-pose a system that uses HMMs to capture the structure of tutorial dialogue implicit within se-quences of already-tagged dialogue acts.  This ap-proach operates on the premise that at any given point in the tutorial dialogue, the collaborative in-teraction is in a dialogue mode that characterizes the nature of the exchanges between tutor and stu-dent.  In our model, a dialogue mode is defined by a probability distribution over the observed sym-bols (e.g., dialogue acts and adjacency pairs).      Our previous work has noted some limitations of first-order HMMs as applied to sequences of individual dialogue acts (Boyer et al, in press).  Chief among these is that HMMs allow arbitrarily frequent transitions between hidden states, which does not conform well to human intuition about how tutoring strategies are applied.  Training an HMM on a sequence of adjacency pairs rather than individual dialogue acts is one way to generate a 
49
more descriptive model without increasing model complexity more than is required to accommodate the expanded set of observation symbols.  To this end, we apply the approach of Midgley et al (2006) for empirically identifying significant adja-cency pairs within dialogue, and proceed by treat-ing adjacency pairs as atomic units for the purposes of training the HMM.   2 Corpus Analysis This analysis uses a corpus of human-human tuto-rial dialogue collected in the domain of introduc-tory computer science.  Forty-three learners interacted remotely with a tutor through a key-board-to-keyboard remote learning environment yielding 4,864 dialogue moves.    The tutoring corpus was manually tagged with dialogue acts designed to capture the salient char-acteristics of the tutoring process (Table 1).  Tag Act Example Q Question Where should I  declare i? EQ Evaluation Question How does that look? S Statement You need a  closing brace. G Grounding Ok.  EX Extra-Domain You may use  your book. PF Positive Feedback Yes, that?s right. LF Lukewarm Feedback Sort of. NF Negative Feedback No, that?s not right. Table 1. Dialogue Act Tags  The correspondence between utterances and dia-logue act tags is one-to-one.  Compound utterances (i.e., a single utterance comprising more than one dialogue act) were split by the primary annotator prior to the inter-rater reliability study.1      The importance of adjacency pairs is well-established in natural language dialogue (e.g., Schlegoff & Sacks, 1973), and adjacency pair analysis has illuminated important phenomena in tutoring as well (Forbes-Riley et al, 2007).  For the current corpus, bigram analysis of dialogue acts yielded a set of commonly-occurring pairs.  How-ever, as noted in (Midgley et al, 2006), in order to                                                            1 Details of the study procedure used to collect the corpus, as well as Kappa statistics for inter-rater reliability, are reported in (Boyer et al, 2008). 
establish that two dialogue acts are truly related as an adjacency pair, it is important to determine whether the presence of the first member of the pair is associated with a significantly higher prob-ability of the second member occurring.  For this analysis we utilize a ?2 test for independence of the categorical variables acti and acti+1 for all two-way combinations of dialogue act tags.  Only pairs in which speaker(acti)?speaker(acti+1) were consid-ered.  Other dialogue acts were treated as atomic elements in subsequent analysis, as discussed in Section 3.  Table 2 displays a list of the dependent pairs sorted by descending (unadjusted) statistical significance; the subscript indicates tutor (t) or stu-dent (s).  acti acti+1 P(acti+1|       acti) P(acti+1|    ?acti) ?2 val p-val EQs PFt 0.48 0.07 654 <0.0001 Gs Gt 0.27 0.03 380 <0.0001 EXs EXt 0.34 0.03 378 <0.0001 EQt PFs 0.18 0.01 322 <0.0001 EQt Ss 0.24 0.03 289 <0.0001 EQs LFt 0.13 0.01 265 <0.0001 Qt Ss 0.65 0.04 235 <0.0001 EQt LFs 0.07 0.00 219 <0.0001 Qs St 0.82 0.38 210 <0.0001 EQs NFt 0.08 0.01 207 <0.0001 EXt EXs 0.19 0.02 177 <0.0001 NFs Gt 0.29 0.03 172 <0.0001 EQt NFs 0.11 0.01 133 <0.0001 Ss Gt 0.16 0.03 95 <0.0001 Ss PFt 0.30 0.10 90 <0.0001 St Gs 0.07 0.04 36 <0.0001 PFs Gt 0.14 0.04 34 <0.0001 LFs Gt 0.22 0.04 30 <0.0001 St EQs 0.11 0.07 29 <0.0001 Gt EXs 0.07 0.03 14 0.002 St Qs 0.07 0.05 14 0.0002 Gt Gs 0.10 0.05 9 0.0027 EQt EQs 0.13 0.08 8 0.0042 Table 2. Dependent Adjacency Pairs 3 HMM on Adjacency Pair Sequences The keyboard-to-keyboard tutorial interaction re-sulted in a sequence of utterances that were anno-tated with dialogue acts.  We have hypothesized that a higher-level dialogue structure, namely the tutorial dialogue mode, overlays the observed dia-logue acts.  To build an HMM model of this struc-
50
ture we treat dialogue mode as a hidden variable and train a hidden Markov model to induce the dialogue modes and their associated dialogue act emission probability distributions.    An adjacency pair joining algorithm (Figure 1) was applied to each sequence of dialogue acts.  This algorithm joins pairs of dialogue acts into atomic units according to a priority determined by the strength of the adjacency pair dependency.  Sort adjacency pair list L by descending statistical significance For each adjacency pair (act1, act2) in L         For each dialogue act sequence (a1, a2, ?, an)          in the corpus                 Replace all pairs (ai=act1, ai+1=act2) with a                 new single act (act1act2) Figure 1.  Adjacency Pair Joining Algorithm     Figure 2 illustrates the application of the adja-cency pair joining algorithm on a sequence of dia-logue acts.  Any dialogue acts that were not grouped into adjacency pairs at the completion of the algorithm are treated as atomic units in the HMMianalysis.   Original Dialogue Act Sequence: Qs - St - LFt - St - St - Gs - EQs - LFt - St - St - Qs - St After Adjacency Pair Joining Algorithm: QsSt - LFt - St - StGs - EQsLFt - St - St - QsSt Figure 2.  DA Sequence Before/After Joining     The final set of observed symbols consists of 39 tags: 23 adjacency pairs (Table 2) plus all individ-ual dialogue acts augmented with a tag for the speaker (Table 1).      It was desirable to learn n, the best number of hidden states, during modeling rather than specify-ing this value a priori.  To this end, we trained and ten-fold cross-validated seven models (each featur-ing randomly-initialized parameters) for each number of hidden states n from 2 to 15, inclusive.2  The average log-likelihood was computed across all seven models for each n, and this average log-                                                           2 n=15 was chosen as an initial maximum number of states because it comfortably exceeded our hypothesized range of 3 to 7 (informed by the tutoring literature).  The Akaike Infor-mation Criterion measure steadily worsened above n = 5, con-firming no need to train models with n > 15. 
likelihood ln was used to compute the Akaike In-formation Criterion, a maximum-penalized likeli-hood estimator that penalizes more complex models (Scott, 2002).  The best fit was obtained with n=4 (Figure 3).  The transition probability distribution among hidden states is depicted in Figure 4, with the size of the nodes indicating rela-tive frequency of each hidden state; specifically, State 0 accounts for 63% of the corpus, States 1 and 3 account for approximately 15% each, and State 2 accounts for 7%.  
  Figure 3.  Dialogue Act Emission Probability  Distribution by Dialogue Mode3 4 Discussion and Future Work This exploratory application of hidden Markov models involves training an HMM on a mixed in-put sequence consisting of both individual dialogue acts and adjacency pairs.  The best-fit HMM con-sists of four hidden states whose emission symbol probability distributions lend themselves to inter-pretation as tutorial dialogue modes.  For example, State 0 consists primarily of tutor statements and positive feedback, two of the most common dia-logue  acts  in our corpus.  The transition probabili- 
51
 Figure 4.  Transition Probability Distribution4  ties also reveal that State 0 is highly stable; a self-transition is most likely with probability 0.835.  State 3 is an interactive state featuring student re-flection in the form of questions, statements, and requests for feedback.  The transition probabilities show that nearly 60% of the time the dialogue transitions from State 3 to State 0; this may indi-cate that after establishing what the student does or does not know in State 3, the tutoring switches to a less collaborative ?teaching? mode represented by State 0.        Future evaluation of the HMM presented here will include comparison with other types of graphical models.  Another important step is to correlate the dialogue profile of each tutoring ses-sion, as revealed by the HMM, to learning and af-fective outcomes of the tutoring session.  This type of inquiry can lead directly to design recommenda-tions for tutorial dialogue systems that aim to maximize particular learner outcomes.  In addition, leveraging knowledge of the task state as well as surface-level utterance content below the dialogue act level are promising directions for refining the descriptive and predictive power of these models.      Acknowledgements  This research was supported by the National Science Foundation under Grants REC-0632450, IIS-0812291, CNS-0540523, and GRFP.  Any opinions, findings, and conclusions or recommendations ex-pressed in this material are those of the 
authors and do not necessarily reflect the views of the National Science Foundation.  References Boyer, K.E., Phillips, R., Wallis, M., Vouk, M., & Lester, J. (2008).  Balancing cognitive and moti-vational scaffolding in tutorial dialogue.  Pro-ceedings of the 9th International Conference on Intelligent Tutoring Systems, Montreal, Canada, 239-249. Boyer, K.E., Ha, E.Y., Wallis, M., Phillips, R., Vouk, M. & Lester, J. (in press).  Discovering tutorial dialogue strategies with hidden Markov models.  To appear in Proceedings of the 14th International Conference on Artificial Intelligence in Educa-tion, Brighton, U.K. Bangalore, S., DiFabbrizio, G., Stent, A. (2006).  Learning the structure of task-driven human-human dialogs.  Proceedings of ACL, Sydney, Australia, 201-208. Cade, W., Copeland, J., Person, N., & D'Mello, S. (2008). Dialog modes in expert tutoring. Proceed-ings of the 9th International Conference on Intel-ligent Tutoring Systems, Montreal, Canada, 470-479.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context de-pendencies for adaptive system development. Proceedings of NAACL HLT, Companion Volume, 41-44.  Midgley, T. D., Harrison, S., & MacNish, C. (2007). Empirical verification of adjacency pairs using dialogue segmentation. Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, 104-108.  Schlegoff, E.A., Sacks, H. (1973).  Opening up clos-ings.  Semiotica, 8(4), 289-327. Scott, S. L. (2002). Bayesian methods for hidden Markov models: Recursive computing in the 21st century. Journal of the American Statistical Asso-ciation, 97(457), 337-351. Stolcke, A., Coccaro, N., Bates, R., Taylor, P., Van Ess-Dykema, C., Ries, K., Shirberg, E., Jurafsky, D., Martin, R., Meteer, M. (2000).  Dialog act modeling for automatic tagging and recognition of conversational speech.  Computational Linguistics 26(3), 339-373. VanLehn, K., Graesser, A., Jackson, G.T., Jordan, P., Olney, A., Rose, C.P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  
52
Pronominalization in Generated Discourse and Dialogue
Charles B. Callaway
Istituto per la Ricerca Scientifica e
Tecnologica (ITC-irst), Italy
callaway@irst.itc.it
James C. Lester
Department of Computer Science
North Carolina State University, USA
lester@adm.csc.ncsu.edu
Abstract
Previous approaches to pronominalization
have largely been theoretical rather than
applied in nature. Frequently, such meth-
ods are based on Centering Theory, which
deals with the resolution of anaphoric pro-
nouns. But it is not clear that complex the-
oretical mechanisms, while having satis-
fying explanatory power, are necessary for
the actual generation of pronouns. We first
illustrate examples of pronouns from vari-
ous domains, describe a simple method for
generating pronouns in an implemented
multi-page generation system, and present
an evaluation of its performance.
1 Introduction
Pronominalization is an important element in the au-
tomatic creation of multi-paragraph and multi-page
texts using natural language generation (NLG). Au-
thors routinely use pronouns in texts spanning all
types of genres, such as newspaper copy, science
fiction and even academic papers. Indeed, without
pronouns, texts quickly become confusing as readers
begin to pay more attention to the writing style than
to the content that makes the text informative or en-
joyable (Callaway and Lester, 2001a). Even worse,
incorrect pronouns can lead readers to misinterpret
the text or draw unsound inferences.
Furthermore, current pronominalization strategies
are ill-equipped to deal with the wide variety of
reasons that pronouns are used in naturally occur-
ring texts. Almost without exception, they focus on
anaphoric pronouns as described in Focus/Centering
Theory (Webber, 1979; Sidner, 1983; Grosz and Sid-
ner, 1986; Walker, 1998), ignoring the multitude of
other possible types. However, it is certainly true
that authors make use of pronouns which are not mo-
tivated by anaphoric reference.
In addition, because such approaches are oriented
towards anaphora resolution during parsing, they ig-
nore structures such as the discourse plan which are
present during generation but not parsing. A typi-
cal discourse plan can include vital information for
pronominalization such as time and clause bound-
aries, ordering of propositions, and semantic de-
tails verbal arguments. Current approaches based
on Centering algorithms thus attempt to recreate a
text coherence structure that duplicates work already
done by the discourse planner.
Finally, there are significant obstacles to verifying
the correctness of existing pronominalization algo-
rithms for any pronominalization theory (Not, 1996;
Yeh and Mellish, 1997; McCoy and Strube, 1999;
Henschel et al, 2000; Kibble and Power, 2000): the
lack of natural language generation systems that can
produce large enough texts to bring discourse-level
processes into play. Because of this, researchers are
forced to simulate by hand how their algorithms will
work on a given text. It is also not sufficient to use
template generation systems to perform this task be-
cause they lack the low-level discourse representa-
tion needed to provide the information upon which
most algorithms base their decisions.
In this paper we first summarize related work
in both anaphora resolution and anaphora genera-
tion. We next describe the range of pronoun types
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 88-95.
                         Proceedings of the 40th Annual Meeting of the Association for
that we found in a wide variety of texts. We pro-
ceed to describe an algorithm for determining ap-
propriate pronominalizations that uses existing NLG
structures and simple numeric techniques. We also
briefly describe an implemented generation system
that contains enough low-level discourse informa-
tion to motivate pronominalization decisions using
this method. Finally, we quantitatively demonstrate
the performance of this simple numerical approach
in both a newspaper and fictional narrative domain.
2 Background and Related Work
Because most NLG systems have focused on lin-
guistic phenomena at the paragraph level and be-
low, there has been intensive investigation into the
core areas of generation that are required to pro-
duce them: discourse planning, sentence planning
and surface realization. Since pronouns are more
likely to be a multiparagraph, discourse-level phe-
nomenon, it has been possible to ignore their inclu-
sion into working NLG systems which are not called
upon to generate lengthy passages.
Indeed, most work on pronouns in computational
linguistics has come under the heading of anaphora
resolution as an element of parsing rather than the
heading of pronominalization as an element of gen-
eration. Since discourse anaphora resolution was
first studied theoretically (Grosz, 1977; Webber,
1979; Sidner, 1983; Grosz and Sidner, 1986), it has
come to be dominated by Centering Theory (Grosz
et al, 1995; Di Eugenio, 1998; Walker, 1998) which
proposes rules for the determination of focus and
salience within a given segment of discourse. Rel-
atively little work has been done on alternate ap-
proaches to pronoun resolution (Hobbs, 1976; Bald-
win, 1995).
While many NLG researchers have attempted to
transfer the ideas of Centering Theory to genera-
tion (Not, 1996; Yeh and Mellish, 1997; McCoy
and Strube, 1999; Henschel et al, 2000; Kibble
and Power, 2000), there has yet been no substan-
tial return contribution to the field of anaphora res-
olution. There are two principal reasons for this.
First, it is extremely difficult to create an NLG sys-
tem that generates the large quantity of texts needed
to exhibit discourse-level phenomena while consis-
tently employing the deep linguistic representations
needed to determine appropriate pronominal forms.
Second, Centering Theory is still vague on the ex-
act definition of terms such as ?segment? (Poesio et
al., 1999a), making it difficult to create a mutually
agreeable implementation.
An additional area of NLG research that deals
with pronouns is that of referring expression gen-
eration (Appelt, 1985; Heeman and Hirst, 1986;
Claassen, 1992; Dale, 1992), which attempts to find
the optimal noun phrase (whether full description,
definite description, deixis, pronoun, or reduced
noun phrase) to enable a reader to mentally select the
intended referent from the set of possible referents
(Reiter and Dale, 1997). Comparatively, referring
expression generation is a process for local disam-
biguation and is not generally concerned with single
phenomena spanning multiple paragraphs. Because
of this, and because the domains and genres we have
studied typically do not involve sets of very simi-
lar referents, we concentrate on discourse-motivated
sources of pronominalization.
3 Examples of Pronominalization
Pronominalization is the appropriate determination,
marking and grammatical agreement of pronouns
(he, she, their, herself, it, mine, those, each other,
one, etc.) as a short-hand reference to an entity or
event mentioned in the discourse. As with anaphora
resolution, the task of a pronominalization algorithm
is to correctly predict which pronoun a person would
prefer in the same situation. The range of possibili-
ties includes leaving the noun phrase as it is, reduc-
ing it by removing some of its modifiers, or replac-
ing it with a pronoun construction.
Our corpora analyses have identified a number of
motivations for converting nouns into pronouns:
1. Anaphoric pronouns: These are the most-
studied cases of pronoun occurrences, which
sequentially follow a specific entity known as
the referent. Anaphors are divided into two
classes, short-distance (within the same sen-
tence) and long-distance (previous sentences).
But John
i
had never been to New Orleans,
and he
i
couldn?t remember if anyone in his
i
family had either.
2. Cataphoric pronouns: According to Quirk et
al. (1985), cataphors are those pronouns which
occur before their referents in the linear flow of
text within the same sentence, where the pro-
noun is either at a lower structural level or is
part of a fronted circumstantial clause or prepo-
sitional phrase which could have appeared after
the reference. Additionally, this category could
include clefting pronouns.
Before he
i
joined the navy, Gerald
i
made
peace with his family.
3. Pronouns Lacking Textual Antecedents: This
category includes document deixis (via a
demonstrative pronoun), authorial or reader
reference, and situational pronouns.
This is the first document to show . . .
We discuss these strategies in the next section.
The group had never seen anything like it.
4. Reflexive and Reciprocal Pronouns: Most
verbs use special pronouns when the subject
and object corefer. A discourse history algo-
rithm can employ that knowledge to mark re-
flexive and reciprocal pronouns appropriately.
Kittens
i
often watch themselves
i
in mirrors.
Baby lions
j
tackle each other
j
when playing.
5. Partitive pronouns: It is important to know con-
ceptually what it is that the pronoun is trying
to replace. Otherwise, it becomes impossible
to achieve the types of pronominalizations that
authors are routinely capable of creating. This
requires accurate information in the knowledge
base or linguistic structure from which the sen-
tences are derived.
As the horses ran by, she roped one.
* As the horses ran by, she roped it.
* As the horses ran by, she roped them.
In addition to these motivations, we identified
several factors that prevent pronouns from occurring
where they otherwise might:
6. Pronouns across boundaries: After a chapter,
section or other obvious boundary, such as a
change in time, place, or both, as in (McCoy
and Strube, 1999), authors will typically ?re-
set? pronominalization just as if it were the
beginning of the entire text. Antecedent ref-
erences that break these boundaries are some-
times marked by the authors in academic texts:
As we saw in the previous section, . . .
7. Restrictions from modifiers: Because pronouns
cannot have modifiers like nouns, adding an ad-
jective, relative clause, or some other modifier
prevents a noun from being replaced by a pro-
noun. For instance:
The mayor had already read the full proposal.
* The mayor had already read the full it.
8. Focused nouns: Especially after a vocally
stressed discourse marker (Wolters and Byron,
2000) or some other marked shift in topic, a
word that normally would be pronominalized
is often not, as in this example:
. . . and you frequently find that mice occupy
an important part of the modern medical labo-
ratory. In other words, mice are especially nec-
essary for diagnosing human cancers . . .
9. Semantic and syntactic considerations: A
small number of semantic relations and syntac-
tic constructions prohibit pronominalization:
* The stranger was just called him. (Bob)
* Roberta was no longer a her. (child)
* The father, a tyrant of a him, . . . (man)
10. Optional pronominalization: Often there are
borderline cases where some authors will use
pronouns while others won?t. A single algo-
rithm may be tuned to match a particular au-
thor?s style, but parameterization will be nec-
essary to match a variety of styles. Thus it is
extremely difficult to exactly match any partic-
ular text without having the ability to adjust the
pronominalization algorithm.
Pronominalization occurs equally as often in ex-
position as in dialogue, but dialogue can have
slightly different pronominalizations depending on
the relationship between the utterer and the hearer:
11. Speaker self-reference:
?John thinks John will go find John?s shoes,?
John said.
changes to first person singular pronouns:
?I think I will go find my shoes,? John said.
12. Speaker references hearer(s):
?Mary should go find Mary?s shoes,? John
said.
changes to second person pronouns:
?You should go find your shoes,? John said.
13. Reference to speaker and hearer (or to speaker
and a third party):
?John and Mary should go find John and
Mary?s shoes,? John said.
changes to first person plural pronouns:
?We should go find our shoes,? John said.
14. Reference to a third party:
?Bob and Mary went to eat Bob and Mary?s
breakfast,? John said.
changes to third person plural pronouns:
?They went to eat their breakfast,? John said.
15. Finally, the treatment of pronouns differs de-
pending if they are inside or outside of the di-
rect quotation. For example:
?Oh man, I forgot to eat my breakfast!? John
muttered to himself while grabbing his shoes.
Although this enumeration is surely incomplete,
it provides a basic description of the types of phe-
nomena that must be handled by a generation system
in order to produce text with the types of pronouns
found in routine human-produced prose.
4 Architectural Concerns
In order to correctly account for these phenomena
during generation, it is necessary to have detailed
information about the underlying discourse struc-
ture. Although a template generation system could
be augmented to record this information, in practice
only deep structure, full-scale NLG systems have the
requisite flexibility. Because a pronominalization al-
gorithm typically follows the discourse planner, it
frequently has access to the full discourse plan.
A typical discourse plan is a tree structure, where
internal nodes represent structuring relations while
leaf nodes represent individual sentential elements
that are organized semantically. In addition, the ele-
ments of the discourse tree are typically rooted in the
semantic knowledge base which the discourse plan-
ner drew from when constructing the discourse plan.
The discourse plan supplies the following informa-
tion that is useful for pronominalization:
 Linearization: The sequencing information
stored in the discourse tree can be used to mo-
tivate anaphoric and cataphoric pronouns as
shown in items 1 & 2 of Section 3.
 Semantic Structure: The original subgraphs
(or semantic subnetworks) derived from the
knowledge base can motivate content vs. sit-
uational knowledge (item 3) reflexive and re-
ciprocal pronouns via argument lists (item 4),
partitive pronouns (item 5), and the existence
of NP modifiers (item 7), and can identify se-
mantic types in relations (item 9).
 Discourse Structure: The rhetorical relations
that hold between different sentences typically
imply where section boundaries are located
(item 6), indicate what types of discourse mark-
ers are employed (item 8), and in the case of
dialogue, know which actors are speaking, lis-
tening, or not present (items 11-15).
This detailed knowledge of the discourse is avail-
able to an implemented pronominalization compo-
nent utilizing any theory, including Centering the-
ory. We thus now turn our attention to what role this
information plays in a pronominalization algorithm.
5 A Simple Pronominalization Algorithm
At an abstract level, the pronominalization algo-
rithms derived from Centering theory are easily ex-
pressed: if Centering theory predicts a pronoun
would be used in anaphora resolution in a given seg-
ment of text, then generate the appropriate pronoun.
While this works for many cases of anaphoric pro-
nouns [84.7% in (McCoy and Strube, 1999), 87-
90% in (Henschel et al, 2000)], we have seen that
these form only a subset of the potential reasons for
pronominalization. Furthermore, this approach as-
sumes that the discourse tree was constructed with
Centering theory in mind.
Given:
LNE, the linearized list of nominal elements
NE, the current nominal element
SEEN , the list of encountered nominal elements
D, the dialogue state of the current leaf node
RS, the rhetorical structure near the leaf node
SC, the sentence counter
Do:
SEEN ( ; SC ( 0
while LNE 6=  do
NE ( first(LNE)
if NE 62 SEEN
then reset-counters(NE),
SEEN ( SEEN NE
else update-counters(NE)
D ( updateDialogueState()
RS ( updateLocalRhetoricalStructure()
if (topic-shift _ time-shift) 2 RS
then SC ( SC + 10
else if modifiers(NE;RS) =  ^
(special-relation _ appositive) 62 RS
if D == QuotedDialogue
then mark(quoted-pronoun(NE;RS))
else if subject-matches-object(NE;RS)
then mark(ReflexivePronoun)
else if sent-distance(NE;SC) = 0
then mark(MultipleInSentencePronoun)
else if 3 <= sent-distance(NE;SC) < 1
and nominal-distance(NE) < 3
then mark(LongDistancePronoun),
else if recency(NE) > 3
then mark(ShortDistancePronoun),
LNE ( remove-first(LNE); SC ( SC + 1
Figure 1: The Pronominalization Algorithm
However, it is not clear that Centering theory itself
is necessary in generation, let alne its accompany-
ing algorithms and data structures. Because Cen-
tering theory is typically applied to parsing (which
starts with no discourse tree), it may not be the most
efficient technique to use in generation (which has a
complete discourse tree available for inference).
Instead, we attempted to determine if the informa-
tion already present in the discourse tree was enough
to motivate a simpler algorithm based on the follow-
ing available data:
 Ordered sequence of nominal elements: Be-
cause the discourse tree is linearized and in-
dividual leaves of the tree annotate which ele-
ments have certain semantic roles, a very good
guess can be made as to which nominal ele-
ments precede others at the clause level.
 Known paragraph and sentence boundaries:
Analysis of the rhetorical structure of the dis-
course tree allows for the determination of
boundaries and thus the concept of metric dis-
tance between elements.
 Rhetorical relations: The rhetorical relations
can tell us which nominal elements follow dis-
course markers and which are used reflexively
or reciprocally.
 Dialogue: By recording the participants in dia-
logue, the discourse tree allows for the appro-
priate assignment of pronouns both inside and
outside of the direct quote itself.
The algorithm we developed considers the cur-
rent discourse leaf node and the rhetorical structure
above it, and also makes use of the following data:
 Nominal element distance: How many total
(non-distinct) nominal elements ago a particu-
lar element was last used.
 Recency: How many distinct nominal elements
have been seen since its last use.
 Sentential distance: How many sentences (pro-
totypical clauses) have appeared since the last
usage of this nominal element.
The algorithm itself (Figure 1) is best character-
ized as a counting method, that is, it loops once
through the linearized list of nominal elements and
makes pronominalization decisions based on the lo-
cal information described above, and then updates
those numerical counters. Numerical parameters
(e.g., recency(NE) > 3) are derived from empir-
ical experimentation in generating multi-page prose
in a narrative domain.
While it lacks the explanatory power of a rela-
tively mature linguistic theory, it also lacks the ac-
companying complexity and is immediately appli-
cable to real-world deep generation systems. The al-
gorithm is traced in Figure 2, although due to space
limitations some phenomena such as dialogue, long
distance and reflexive pronouns are not shown.
6 Implementation and Evaluation
STORYBOOK (Callaway and Lester, 2001b; Call-
away and Lester, in press) is an implemented nar-
rative generation system that converts a pre-existing
Sentences as seen by the reader (antecedents underlined, pronouns in bold):
Now, it happened that a wolf
1
, a very cruel, greedy creature
2
also heard Little Red Riding Hood
3
as
she
4
passed, and he
5
longed to eat her
6
for his
7
breakfast
8
. But he
9
knew Hugh
10
, the woodman
11
,
was at work
12
very near with his
13
great dog
14
.
Sentences as produced by the discourse planner before revision:
S1: Now, it happened that a wolf
1
, a very cruel, greedy creature
2
also heard Little Red Riding Hood
3
as Little Red Riding Hood
4
passed.
S2: The wolf
5
longed to eat Little Red Riding Hood
6
for the wolf?s
7
breakfast
8
.
S3: But the wolf
9
knew Hugh
10
, the woodman
11
, was at work
12
very near with Hugh?s
13
great dog
14
.
Each noun element is processed in the order linearized from the discourse plan:
1. The first mention of wolf
1
in the narrative resets its discourse history entry.
2. Creature
2
is the second mention of wolf, but it is in an appositive structure (see pronoun category #9).
3. LRRH
3
was mentioned just before in the prior paragraph, but ?Now,? is a prosodic discourse marker
(see pronoun category #8), thus modifiers(NE, RS) 6= .
4. For LRRH
3
and LRRH
4
, sentence-distance(NE, SC) = 0 resulting in a multiple-in-sentence-pronoun.
5. Sentence-distance(NE, SC) = 1, but recency(NE) = 2, resulting in a short-distance-pronoun.
6. Similarly, LRRH
6
is converted into a short-distance-pronoun.
7. As with element #4, this is a case resulting in a multiple-in-sentence-pronoun.
9. As with element #5, this is a case resulting in a short-distance-pronoun.
10. The first mention of Hugh
10
in the narrative resets its discourse history entry.
11. As with element #2, the discourse plan reports that this is an appositive.
13. Finally, Hugh
13
is repeated in the same sentence.
Figure 2: A Brief Trace of the Pronominalization Algorithm for Anaphoric Pronouns from STORYBOOK
narrative (discourse) plan into a multi-page fic-
tional narrative in the fairy tale domain. Using a
pipelined generation architecture, STORYBOOK per-
forms pronominalization before sentence planning,
and includes a revision component that is sensitive
to pronominalization choices during clause aggre-
gation. A previous large-scale evaluation of STORY-
BOOK (Callaway and Lester, 2001a) which included
both a full version and a version with the pronomi-
nalization component ablated showed that including
such a component significantly increases the quality
of the resulting prose.
However, there are significant practical obstacles
to comparing the performance of different pronomi-
nalization algorithms using corpus matching criteria
instead of ?quality? as evaluated by human judges.
Because systems that can handle a large quantity of
text are very recent and because it can require years
to create and organize the necessary knowledge to
produce even one multi-paragraph text, much re-
search on anaphora generation has instead relied on
one of two techniques:
 Checking algorithms by hand: One verification
method is to manually examine a text, identify-
ing candidates for pronominalization and simu-
lating the rules of a particular theory. However,
this method is prone to human error.
 Checking algorithms semiautomatically: Other
researchers opt instead to annotate a corpus
for pronominalization and their antecedents as
well as the pronoun forms that should occur,
and then simulate a pronominalization algo-
rithm on the marked-up text (Henschel et al,
2000). Similarly, this approach can suffer from
interannotator agreement errors (Poesio et al,
1999b).
To verify our pronominalization algorithm more
rigorously, we instead used the STORYBOOK deep
generation system to recreate pre-existing multi-
page texts with automatically selected pronouns.
McCoy & Strube Henschel et al STORYBOOK STORYBOOK
NYT News NYT News NYT News LRRH Narrative
Animate Anaphora 370/437 (84.7%) N/A 415/449 (92.4%) 170/174 (97.7%)
All Anaphora N/A 469/527 (89.0%) 441/475 (92.8%) 177/181 (97.8%)
Cataphora N/A N/A 1/2 (50.0%) 1/2 (50.0%)
Dialogue N/A N/A 46/46 (100.0%) 65/65 (100.0%)
Deixis N/A N/A 9/9 (100.0%) None present
Reflex./Recip. N/A N/A 5/6 (83.3%) 2/2 (100.0%)
Partitive N/A N/A 1/2 (50.0%) 1/1 (100.0%)
Table 1: Pronouns Correct by Algorithm/Text vs. Pronoun Type
Without a full-scale implementation, it is impossible
to determine whether an algorithm performs imper-
fectly due to human error, a lack of available corpus
data for making decisions, or if it is a fault with the
algorithm itself.
Using the algorithm described in Figure 1, we
modified STORYBOOK to substitute the types of
pronouns described in Section 3. We then created
the discourse plan and lexicon necessary to generate
the same three articles from the New York Times as
(McCoy and Strube, 1999). The results for both the
newspaper texts and the Little Red Riding Hood nar-
rative described in (Callaway and Lester, in press)
are shown in Table 1.
With the same three texts from the New York
Times, STORYBOOK performed better than the pre-
vious reported results of 85-90% described in (Mc-
Coy and Strube, 1999; Henschel et al, 2000) on both
animate and all anaphora using a corpus matching
technique. Furthermore, this was obtained solely by
adjusting the recency parameter to 4 (it was 3 in our
narrative domain), and without considering other en-
hancements such as gender/number constraints or
domain-specific alterations.1
7 Conclusions
Pronominalization is an important element in the au-
tomatic creation of multi-paragraph and multi-page
1It is important to note, however, that our counts of pronouns
and antecedents do not match theirs. This may stem from a vari-
ety of factors, such as including single instances of nominal de-
scriptions, whether dialogue pronouns were considered, and if
borderline quantifiers and words like ?everyone? were counted.
The generation community to-date has not settled on standard,
marked corpora for comparison purposes as has the rest of the
computational linguistics community.
texts. Previous approaches, based largely on theo-
retical approaches such as Centering Theory, deal
exclusively with anaphoric pronouns and have com-
plex processing and definitional requirements.
Given the full rhetorical structure available to an
implemented generation system, we devised a sim-
pler method of determining appropriate pronom-
inalizations which was more accurate than exist-
ing methods simulated by hand or performed semi-
automatically. This shows that approaches designed
for use with anaphora resolution, which must build
up discourse knowledge from scratch, may not be
the most desirable method for use in NLG, where
discourse knowledge already exists. The positive re-
sults from our simple counting algorithm, after only
minor changes in parameters from a narrative do-
main to that of newspaper text, indicates that future
high-quality prose generation systems are very near.
8 Acknowledgements
We would like to thank Michael Young and Renate
Henschel for their helpful comments; Kathy McCoy
very quickly provided the original 3 NYT articles
upon request; the anonymous reviewers whose com-
ments greatly improved this paper. Support for this
work was provided by ITC-irst and the IntelliMedia
Initiative of North Carolina State University.
References
Douglas E. Appelt. 1985. Planning English referring
expressions. Artificial Intelligence, 26:1?33.
Frederick Baldwin. 1995. CogNIAC: A Discourse Pro-
cessing Engine. Ph.D. thesis, The University of Penn-
sylvania, Philadelphia, PA.
Charles B. Callaway and James C. Lester. 2001a. Eval-
uating the effects of natural language generation on
reader satisfaction. In Proceedings of the Twenty-
Third Annual Conference of the Cognitive Science So-
ciety, pages 164?169, Edinburgh, UK.
Charles B. Callaway and James C. Lester. 2001b. Nar-
rative prose generation. In Proceedings of the Seven-
teenth International Joint Conference on Artificial In-
telligence, pages 1241?1248, Seattle, WA.
Charles B. Callaway and James C. Lester. 2003. Narra-
tive prose generation. Artificial Intelligence. In press.
Wim Claassen. 1992. Generating referring expressions
in a multimodal environment. In R. Dale, E. Hovy,
D. Rosner, and O. Stock, editors, Aspects of Auto-
mated Natural Language Generation, pages 247?62.
Springer-Verlag, Berlin.
Robert Dale. 1992. Generating Referring Expressions.
MIT Press.
Barbara Di Eugenio. 1998. Centering in Italian. In
Marilyn A. Walker, Aravind K. Joshi, and Ellen F.
Prince, editors, Centering in Discourse. Oxford Uni-
versity Press, Cambridge, MA.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modelling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2).
Barbara J. Grosz. 1977. The representation and use of
focus in a system for understanding dialogs. In Pro-
ceedings of the Fifth International Joint Conference on
Artificial Intelligence, pages 67?76, Cambridge, MA.
Peter Heeman and Graeme Hirst. 1986. Collaborating
on referring expressions. Computational Linguistics,
12(3):351?382.
Renate Henschel, Hua Cheng, and Massimo Poesio.
2000. Pronominalization revisited. In COLING?
2000: Proceedings of the 18th International Con-
ference on Computational Linguistics, Saarbruecken,
Germany.
Jerry R. Hobbs. 1976. Pronoun resolution. Technical
Report 76-1, Department of Computer Science, City
College, CUNY, New York, NY.
Roger Kibble and Richard Power. 2000. An inte-
grated framework for text planning and pronominali-
sation. In Proceedings of the First International Con-
ference on Natural Language Generation, pages 194?
200, Mitzpe Ramon, Israel.
Kathleen F. McCoy and Michael Strube. 1999. Taking
time to structure discourse: Pronoun generation be-
yond accessibility. In Proceedings of the Twenty-First
Conference of the Cognitive Science Society, pages
378?383, Vancouver, CA, August.
Elena Not. 1996. A computational model for generating
referring expressions in a multilingual application do-
main. In COLING?1996: Proceedings of the 16th In-
ternational Conference on Computational Linguistics,
Copenhagen, Denmark, August.
M. Poesio, H. Cheng, R. Henschel, J. Hitzeman, R. Kib-
ble, and R. Stevenson. 1999a. Specifying the parame-
ters of centering theory: A corpus-based evaluation us-
ing text from application-oriented domains. In Book-
title, page Pages, Address, Month.
M. Poesio, R. Henschel, J. Hitzeman, R. Kibble, S. Mon-
tague, and K. van Deemter. 1999b. Towards an anno-
tation scheme for noun phrase generation. In Bookti-
tle, page Pages, Address, Month.
R. Quirk, S. Greenbaum, G. Leech, and J. Svartvik. 1985.
A Comprehensive Grammar of the English Language.
Longman Publishers.
Ehud Reiter and Robert Dale. 1997. Building ap-
plied natural-language generation systems. Journal of
Natural-Language Engineering, 3:57?87.
Candace L. Sidner. 1983. Focusing in the com-
prehension of definite anaphora. In M. Brady and
R. Berwick, editors, Computational Models of Dis-
course, pages 267?330. MIT Press, Cambridge, MA.
Marilyn A. Walker. 1998. Centering, anaphora resolu-
tion, and discourse structure. In Marilyn A. Walker,
Aravind K. Joshi, and Ellen F. Prince, editors, Center-
ing in Discourse. Oxford University Press, Cambridge,
MA.
Bonnie Webber. 1979. A Formal Approach to Discourse
Anaphora. Garland, NY.
Maria Wolters and Donna K. Byron. 2000. Prosody and
the resolution of pronominal anaphora. In COLING?
2000: Proceedings of the 18th International Con-
ference on Computational Linguistics, Saarbruecken,
Germany.
C. Yeh and C. Mellish. 1997. An empirical study on
the generation of anaphora in Chinese. Computational
Linguistics, 23(1):169?190.
Proceedings of the Third ACL Workshop on Innovative Use of NLP for Building Educational Applications, pages 53?61,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Learner Characteristics and Feedback in Tutorial Dialogue 
 
 
Kristy Elizabeth 
Boyera 
Robert  
Phillipsab 
Michael D. 
Wallisab 
Mladen A. 
Vouka 
James C. 
Lestera 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates, Inc. 
Raleigh, North Carolina, USA 
{keboyer, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
 
 
Abstract 
Tutorial dialogue has been the subject of in-
creasing attention in recent years, and it has 
become evident that empirical studies of hu-
man-human tutorial dialogue can contribute 
important insights to the design of computa-
tional models of dialogue.  This paper reports 
on a corpus study of human-human tutorial 
dialogue transpiring in the course of problem-
solving in a learning environment for intro-
ductory computer science.  Analyses suggest 
that the choice of corrective tutorial strategy 
makes a significant difference in the outcomes 
of both student learning gains and self-
efficacy gains.  The findings reveal that tuto-
rial strategies intended to maximize student 
motivational outcomes (e.g., self-efficacy 
gain) may not be the same strategies that 
maximize cognitive outcomes (i.e., learning 
gain).  In light of recent findings that learner 
characteristics influence the structure of tuto-
rial dialogue, we explore the importance of 
understanding the interaction between learner 
characteristics and tutorial dialogue strategy 
choice when designing tutorial dialogue sys-
tems.  
1 Introduction 
Providing intelligent tutoring systems (ITSs) with 
the ability to engage learners in rich natural lan-
guage dialogue has been a goal of the ITS commu-
nity since the inception of the field.  Tutorial 
dialogue has been studied in the context of a num-
ber of systems devised to support a broad range of 
conversational phenomena.  Systems such as 
CIRCSIM (Evens and Michael 2006), BEETLE (Zinn 
et al 2002), the Geometry Explanation Tutor 
(Aleven et al 2003), Why2/Atlas (VanLehn et al 
2002), ITSpoke (Litman et al 2006), SCOT (Pon-
Barry et al 2006), ProPL (Lane and VanLehn 
2005) and AutoTutor (Graesser et al 2003) support 
research that has begun to the see the emergence of 
a core set of foundational requirements for mixed-
initiative natural language interaction that occurs in 
the kind of tutorial dialogue investigated here.  
Moreover, recent years have witnessed the appear-
ance of corpus studies empirically investigating 
speech acts in tutorial dialogue (Marineau et al 
2000), dialogues? correlation with learning 
(Forbes-Riley et al 2005, Core et al 2003, Ros? et 
al. 2003, Katz et al 2003), student uncertainty in 
dialogue (Liscombe et al 2005, Forbes-Riley and 
Litman 2005), and comparing text-based and spo-
ken dialogue (Litman et al 2006). 
     Recent years have also seen the emergence of a 
broader view of learning as a complex process in-
volving both cognitive and affective states.  To 
empirically explore these issues, a number of ITSs 
such as AutoTutor (Jackson et al 2007), Betty?s 
Brain (Tan and Biswas 2006), ITSpoke (Forbes-
Riley et al 2005), M-Ecolab (Rebolledo-Mendez 
et al 2006), and MORE (del Soldato and Boulay 
1995) are being used as platforms to investigate the 
impact of tutorial interactions on affective and mo-
tivational outcomes (e.g., self-efficacy) along with 
purely cognitive measures (i.e., learning gains).  A 
central problem in this line of investigation is iden-
53
tifying tutorial strategies (e.g., Graesser et al 
1995) that can appropriately balance the tradeoffs 
between cognitive and affective student outcomes 
(Lepper et al 1993).  While a rich set of cognitive 
and affective tutorial strategies is emerging (e.g., 
Porayska-Pomsta et al 2004), the precise nature of 
the interdependence between these types of strate-
gies is not well understood.  In addition, it may be 
the case that different populations of learners en-
gage in qualitatively different forms of dialogue.  
Students with particular characteristics may have 
specific dialogue profiles, and knowledge of such 
profiles could inform the design of tutorial systems 
whose strategies leverage the characteristics of the 
target population.  The extent to which different 
tutorial strategies, and specific instances of them in 
certain contexts, may be used to enhance tutorial 
effectiveness is an important question to designers 
of ITSs.    
     Given that human-human tutorial dialogue of-
fers a promising model for effective communica-
tion (Chi et al 2001), our methodology is to study 
naturally occurring tutorial dialogues in a task-
oriented learning environment to investigate the 
relationship between the structure of tutorial dia-
logue, the characteristics of learners, and the im-
pact of cognitive and motivational corrective 
tutorial strategies on learning and self-efficacy 
(Boyer et al in press).  A text-based dialogue inter-
face was incorporated into a learning environment 
for introductory computer science.  In the envi-
ronment, students undertook a programming task 
and conversed with human tutors while designing, 
implementing, and testing Java programs.    
     The results of the study suggest that the choice 
of corrective tutorial strategy has a significant im-
pact on the learning gains and self-efficacy of stu-
dents.  These findings reinforce those of other 
studies (e.g., Lepper et al 1993, Person et al 1995, 
Keller et al 1983) that indicate that some cognitive 
and motivational goals may be at odds with one 
other because a tutorial strategy designed to maxi-
mize one set of goals (e.g., cognitive goals) can 
negatively impact the other.  We contextualize our 
findings in light of recent results that learner char-
acteristics such as self-efficacy influence the struc-
ture of task-oriented tutorial dialogue (Boyer et al 
2007), and may therefore produce important inter-
action effects when considered alongside tutorial 
strategy.    
     This paper is organized as follows.  Section 2 
describes the corpus study, including experimental 
design and tagging of dialogue and student prob-
lem-solving actions.  Section 3 presents analysis 
and results.  Discussion and design implications 
are considered in Section 4, and concluding re-
marks follow in Section 5.  
 
2 Corpus Study 
The corpus was gathered by logging text-based 
dialogues between tutors and novice computer sci-
ence students.  The learning task was to complete a 
Java programming problem that required students 
to apply fundamental concepts such as iteration, 
modularization, and sequential-access data struc-
tures.  This study was conducted to compare the 
impact of certain corrective cognitive and motiva-
tional tutorial strategies on student learning and 
self-efficacy in human-human tutoring.  Specifi-
cally, the study considered the motivational strate-
gies of praise and reassurance (Lepper et al 1993) 
and the category of informational tutorial utter-
ances termed cognitive feedback (Porayska-Pomsta 
et al 2004, Tan and Biswas 2006) that followed 
questionable student problem-solving action.  Fol-
lowing the approach of Forbes-Riley (2005) and 
others (Marineau et al 2000), utterances from a 
corpus of human-human tutorial dialogues were 
annotated with dialogue acts.  Then, adopting the 
approach proposed by Ohlsson et al (2007), statis-
tical modeling techniques were employed to quan-
tify the relative impact of these different tutorial 
strategies on the outcomes of interest (in this case, 
learning and self-efficacy gains).     
 
2.1 Experimental Design 
Subjects were students enrolled in an introductory 
computer science course and were primarily 
freshman or sophomore engineering majors in dis-
ciplines such as mechanical, electrical, and com-
puter engineering. 
     The corpus was gathered from tutor-student 
interactions between 43 students and 14 tutors dur-
ing a two-week study.  Tutors and students were 
completely blind to each other?s characteristics as 
they worked together remotely from separate labs.  
Tutors observed student problem-solving actions 
54
(e.g., programming, scrolling, executing programs) 
in real time.  Tutors had varying levels of tutoring 
experience, and were not instructed about specific 
tutorial strategies. 
     Subjects first completed a pre-survey including 
items about self-efficacy, attitude toward computer 
science, and attitude toward collaboration.  Sub-
jects then completed a ten item pre-test over spe-
cific topic content.  The tutorial session was 
controlled at 55 minutes for all subjects, after 
which subjects completed a post-survey and post-
test containing variants of the items on the pre- 
versions.   
 
2.2 Problem-Solving Tagging 
The raw corpus contains 4,864 dialogue moves:  
1,528 student utterances and 3,336 tutor utterances.  
As a chronology of tutorial dialogue interleaved 
with student problem-solving (programming) ac-
tions that took place during the tutoring sessions, 
the corpus contains 29,996 programming key-
strokes and 1,277 periods of scrolling ? all per-
formed by students.  Other problem-solving 
actions, such as opening and closing files or run-
ning the program, were sparse and were therefore 
eliminated from the analyses.  Of the 3,336 tutor 
utterances, 1,243 occur directly after ?question-
able? student problem-solving action.  (The notion 
of ?questionable? is defined below.)  This subset of 
tutorial utterances serves as the basis for the tuto-
rial strategy comparison. 
     Student problem-solving actions were logged 
throughout tutoring sessions.  Two actions were 
under consideration for the analysis:  typing in the 
programming interface and scrolling in the pro-
gram editor window.  To interpret the raw logged 
student problem-solving actions, these events were 
automatically tagged using a heuristic measure for 
correctness: if a problem-solving action was a pro-
gramming keystroke (character) that survived until 
the end of the session, this event was tagged prom-
ising, to indicate it was probably correct.  If a prob-
lem-solving act was a programming keystroke 
(character) that did not survive until the end of the 
session, the problem-solving act was tagged ques-
tionable.  Both these heuristics are based on the 
observation that in this tutoring context, students 
solved the problem in a linear fashion and tutors 
did not allow students to proceed past a step that 
had incorrect code in place.  Finally, periods of 
consecutive scrolling were also marked question-
able because in a problem whose entire solution 
fits on one printed page, scrolling was almost uni-
formly undertaken by a student who was confused 
and looking for answers in irrelevant skeleton code 
provided to support the programming task.   
 
2.3 Dialogue Act Tagging 
Because utterances communicate through two 
channels, a cognitive channel and a motiva-
tional/affective channel, each utterance was 
annotated with both a required cognitive dialogue 
tag (Table 1) and an optional motiva-
tional/affective dialogue tag (Table 2).  While no 
single standardized dialogue act tag set has been 
identified for tutorial dialogue, the tags applied 
here were drawn from several schemes in the tuto-
rial dialogue and broader dialogue literature.  A 
coding scheme for tutorial dialogue in the domain 
of qualitative physics influenced the creation of the 
tag set (Forbes-Riley et al 2005), as did the four-
category scheme (Marineau et al 2000).  A more 
expansive general dialogue act tag set alo contrib-
uted commonly occurring acts (Stolcke et al 
2000).  The motivational tags were drawn from 
work by Lepper (1993) on motivational strategies 
of human tutors.   
     Table 1 displays the cognitive subset of this 
dialogue act tag set, while Table 2 displays the mo-
tivational/affective tags.  It should be noted that a 
cognitive tag was required for each utterance, 
while a motivational/affective tag was applied only 
to the subset of utterances that communicated in 
that channel.  If an utterance constituted a strictly 
motivational/affective act, its cognitive channel 
was tagged with EX (EXtra-domain) indicating 
there was no relevant cognitive content.  On the 
other hand, some utterances had both a cognitive 
component and a motivational/affective compo-
nent.  For example, a tutorial utterance of, ?That 
looks great!? would have been tagged as positive 
feedback (PF) in the cognitive channel, and as 
praise (P) in the motivational/affective channel.  In 
contrast, the tutorial move ?That?s right,? would be 
tagged as positive feedback (PF) in the cognitive 
channel and would not be annotated with a motiva-
tional/affective tag.  Table 3 shows an excerpt 
from the corpus with dialogue act tags applied. 
55
     The entire corpus was tagged by a single human 
annotator, with a second tagger marking 1,418 of 
the original 4,864 utterances.  The resulting kappa 
statistics were 0.76 in the cognitive channel and 
0.64 in the motivation channel.   
3 Analysis and Results 
Overall, these tutoring sessions were effective: 
they yielded learning gains (difference between 
posttest and pretest) with mean 5.9% and median 
7.9%, which were statistically significant 
(p=0.038), and they produced self-efficacy gains
Table 1:  Cognitive Channel Dialogue Acts 
56
(difference between pre-survey and post-survey 
scores) with mean 12.1% and median 12.5%, 
which were also statistically significant 
(p<0.0001).  Analyses revealed that statistically 
significant relationships hold between tutorial 
strategy and learning, as well as between tutorial 
strategy and self-efficacy gains.   
 
3.1 Analysis 
First, the values of learning gain and self-efficacy 
gain were grouped into binary categories (?Low?, 
?High?) based on the median value.  We then ap-
plied multiple logistic regression with the gain 
category as the predicted value.  Tutorial strategy, 
incoming self-efficacy rating, and pre-test score 
were predictors in the model.  The binarization 
approach followed by multiple logistic regression 
was chosen over multiple linear regression on a 
continuous response variable because the learning 
instruments (10 items each) and self-efficacy ques-
tionnaires (5 items each) yielded few distinct val-
ues of learning gain, meaning the response variable 
(learning gain and self-efficacy gain, respectively) 
would not have been truly continuous in nature.  
Logistic regression is used for binary response 
variables; it computes the odds of a particular out-
come over another (e.g., ?Having high learning 
gain versus low learning gain?) given one value of 
the predictor variable over another (e.g., ?The cor-
rective tutorial strategy chosen was positive cogni-
tive feedback instead of praise?). 
 
Table 2:  Motivational/Affective Channel Dialogue Acts 
57
3.2 Results 
After accounting for the effects of pre-test score 
and incoming self-efficacy rating (both of which 
were significant in the model with p<0.001), ob-
servations containing tutorial encouragement were 
56% less likely to result in high learning gain than 
observations without explicit tutorial encourage-
ment (p=0.001).  On the other hand, an analogous 
model of self-efficacy gain revealed that tutorial 
encouragement was 57% more likely to result in 
high self-efficacy gain compared to tutorial re-
sponses that had no explicit praise or reassurance 
(p=0.054).  These models suggested that the pres-
ence of tutorial encouragement in response to 
questionable student problem-solving action may 
enhance self-efficacy gain but detract from learn-
ing gain. 
    Another significant finding was that observa-
tions in which the tutor used cognitive feedback 
plus praise were associated with 40% lower likeli-
hood of high learning gain than observations in 
which the tutor used purely cognitive feedback.  
No impact was observed on self-efficacy gain.  
These results suggest that in response to question-
able student problem-solving action, to achieve 
learning gains, purely cognitive feedback is pre-
ferred over cognitive feedback plus praise, while 
self-efficacy gain does not appear to be impacted 
either way. 
     Among students with low incoming self-
efficacy, observations in which the tutor employed 
a standalone motivational act were 300% as likely 
to be in the high self-efficacy gain group as obser-
vations in which the tutor employed a purely cog-
nitive statement or a cognitive statement combined 
with encouragement (p=0.039).  In contrast, among 
students with high initial self-efficacy, a purely 
motivational tactic resulted in 90% lower odds of 
being in the high self-efficacy gain group.  These 
results suggest that standalone praise or reassur-
ance may be useful for increasing self-efficacy 
gain among low initial self-efficacy students, but 
may decrease self-efficacy gain in high initial self-
efficacy students.   
     Considering strictly cognitive feedback, posi-
tive feedback resulted in 190% increased odds of 
high student self-efficacy gain compared to the 
other cognitive strategies (p=0.0057).  Positive 
cognitive feedback did not differ significantly from 
other types of cognitive strategies in a Chi-square 
comparison with respect to learning gains 
(p=0.390).  The models thus suggest when dealing 
with questionable student problem-solving action, 
positive cognitive feedback is preferable to other 
types of cognitive feedback for eliciting self-
efficacy gains, but this type of feedback is not 
Table 3:  Dialogue Excerpts 
58
found to be better or worse than other cognitive 
feedback for effecting learning gains. 
 
4 Discussion 
The study found that the presence of direct tutorial 
praise or encouragement in response to question-
able student problem-solving action increased the 
odds that the student reported high self-efficacy 
gain while lowering the odds of high learning gain.  
The study also found that, with regard to learning 
gains, purely cognitive feedback was preferable to 
cognitive feedback with an explicitly motivational 
component.  These empirical findings are consis-
tent with theories of Lepper et al (1993) who 
found that some cognitive and affective goals in 
tutoring are ?at odds.?  As would be predicted, the 
results also echo recent quantitative results from 
other tutoring domains such as qualitative physics 
(Jackson et al 2007) and river ecosystems (Tan 
and Biswas 2006) that, in general, overt motiva-
tional feedback contributes to motivation but cog-
nitive feedback matters more for learning.   
      Of the corrective tutorial strategies that were 
exhibited in the corpus, positive cognitive feed-
back emerged as an attractive approach for re-
sponding to plausibly incorrect student problem-
solving actions.  Responding positively (e.g., 
?Right?) to questionable student actions is an ex-
ample of indirect correction, which is recognized 
as a polite strategy (e.g., Porayska-Pomsta et al 
2004).  A qualitative investigation of this phe-
nomenon revealed that in the corpus, tutors gener-
ally followed positive feedback in this context with 
more substantive cognitive feedback to address the 
nature of the student?s error.  As such, the positive 
feedback approach seems to have an implicit, yet 
perceptible, motivational component while retain-
ing its usefulness as cognitive feedback. 
    This study found that explicit motivational acts, 
when applied as corrective tutorial approaches, had 
different impacts on different student subgroups.  
Students with low initial self-efficacy appeared to 
benefit more from praise and reassurance than stu-
dents with high initial self-efficacy.  In a prior cor-
pus study to investigate the impact of learner 
characteristics on tutorial dialogue (Boyer et al 
2007), we also found that learners from different 
populations exhibited significantly different dia-
logue profiles.  For instance, high self-efficacy 
students made more declarative statements, or as-
sertions, than low self-efficacy students.  In addi-
tion, tutors paired with high self-efficacy students 
gave more conversational acknowledgments than 
tutors paired with low self-efficacy students, de-
spite the fact that tutors were not made aware of 
any learner characteristics before the tutoring ses-
sion.  Additional dialogue profile differences 
emerged between high and low-performing stu-
dents, as well as between males and females.  To-
gether these two studies suggest that learner 
characteristics influence the structure of tutorial 
dialogue, and that the choice of tutorial strategy 
may impact student subgroups in different ways.             
 
5 Conclusion 
The work reported here represents a first step to-
ward understanding the effects of learner charac-
teristics on task-oriented tutorial dialogue and the 
use of feedback.  Results suggest that positive cog-
nitive feedback may prove to be an appropriate 
strategy for responding to questionable student 
problem-solving actions in task-oriented tutorial 
situations because of its potential for addressing 
the sometimes competing cognitive and affective 
needs of students.  For low self-efficacy students, it 
was found that direct standalone encouragement 
can be used to bolster self-efficacy, but care must 
be used in correctly diagnosing student self-
efficacy because the same standalone encourage-
ment does not appear helpful for high self-efficacy 
students.  These preliminary findings highlight the 
importance of understanding the interaction be-
tween learner characteristics and tutorial strategy 
as it relates to the design of tutorial dialogue sys-
tems. 
     Several directions for future work appear prom-
ising.  First, it will be important to explore the in-
fluence of learner characteristics on tutorial 
dialogue in the presence of surface level informa-
tion about students? utterances.  This line of inves-
tigation is of particular interest given recent results 
indicating that lexical cohesion in tutorial dialogue 
with low-performing students is found to be highly 
correlated with learning (Ward and Litman 2006).   
Second, while the work reported here has consid-
ered a limited set of motivational dialogue acts, 
namely praise and reassurance, future work should 
target an expanded set of affective dialogue acts to 
59
facilitate continued exploration of motivational and 
affective phenomena in this context.  Finally, the 
current results reflect human-human tutoring 
strategies that proved to be effective; however, it 
remains to be seen whether these same strategies 
can be successfully employed in tutorial dialogue 
systems.  Continuing to identify and empirically 
compare the effectiveness of alternative tutorial 
strategies will build a solid foundation for choos-
ing and implementing strategies that consider 
learner characteristics and successfully balance the 
cognitive and affective concerns surrounding the 
complex processes of teaching and learning 
through tutoring. 
Acknowledgments 
The authors wish to thank Scott McQuiggan and 
the members of the Intellimedia Center for Intelli-
gent Systems for their ongoing intellectual contri-
butions, and the Realsearch Group at NC State 
University for extensive project development sup-
port.  This work was supported in part by the Na-
tional Science Foundation through Grant REC-
0632450, an NSF Graduate Research Fellowship, 
and the STARS Alliance Grant CNS-0540523.  
Any opinions, findings, conclusions or recommen-
dations expressed in this material are those of the 
author(s) and do not necessarily reflect the views 
of the National Science Foundation.  Support was 
also provided by North Carolina State University 
through the Department of Computer Science and 
the Office of the Dean of the College of Engineer-
ing.   
 
References  
Kristy Elizabeth Boyer, Robert Phillips, Michael Wallis, 
Mladen Vouk, and James Lester.  In press.  Balanc-
ing cognitive and motivational scaffolding in tutorial 
dialogue.  To appear in Proceedings of the 9th Inter-
national Conference on Intelligent Tutoring Systems. 
Kristy Elizabeth Boyer, Mladen Vouk, and James Les-
ter.  2007.  The influence of learner characteristics on 
task-oriented tutorial dialogue.  Proceedings of 
AIED, pp. 127-134.  IOS Press. 
Vincent Aleven, Kenneth R. Koedinger, and Octav 
Popescu.  2003.  A tutorial dialog system to support 
self-explanation: Evaluation and open questions.  
Proceedings of the 11th International Conference on 
Artificial Intelligence in Education, pp. 39-46.  Am-
sterdam.  IOS Press. 
Vincent Aleven, Bruce McLaren, Ido Roll, and Kenneth 
Koedinger.  2004.  Toward tutoring help seeking: 
Applying cognitive modeling to meta-cognitive 
skills.  J. C. Lester, R. M. Vicari, and F. Paragua?u 
(Eds.), Proceedings of the 7th International Confer-
ence on Intelligent Tutoring Systems, pp. 227-239.  
Berlin: Springer Verlag. 
Albert Bandura.  2006.  Guide for constructing self-
efficacy scales.  T. Urdan and F. Pajares (Eds.): Self-
Efficacy Beliefs of Adolescents, pp. 307-337.  Infor-
mation Age Publishing, Greenwich, Connecticut. 
Michelene T. H. Chi, Nicholas De Leeuw, Mei-Hung 
Chiu, and Christian LaVancher.  1994.  Eliciting self-
explanations improves understanding.  Cognitive Sci-
ence, 18:439-477. 
Michelene T. H. Chi, Stephanie A. Siler, Heisawn 
Jeong, Takashi Yamauchi, and Robert G. Hausmann.  
2001.  Learning from human tutoring.  Cognitive Sci-
ence, 25(4):471-533. 
Mark G. Core, Johanna D. Moore, and Claus Zinn.  
2003.  The role of initiative in tutorial dialogue.  Pro-
ceedings of the Tenth Conference on European 
Chapter of the Association for Computational Lin-
guistics, pp. 67-74. 
Teresa del Soldato and Benedict du Boulay.  1995.  Im-
plementation of motivational tactics in tutoring sys-
tems.  Journal of Artificial Intelligence in Education, 
6(4):337-378.  Association for the Advancement of 
Computing in Education, USA. 
Martha Evens and Joel Michael.  2006.  One-on-One 
Tutoring by Humans and Computers.  Mahwah, New 
Jersey: Lawrence Erlbaum Associates. 
Kate Forbes-Riley and Diane Litman.  2005.  Using 
bigrams to identify relationships between student cer-
tainness states and tutor responses in a spoken dia-
logue corpus.  Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue.  Lisbon, Por-
tugal. 
Kate Forbes-Riley, Diane Litman, Alison Huettner, and 
Arthur Ward.  2005.  Dialogue-learning correlations 
in spoken dialogue tutoring.  Looi, C-k., Mccalla, G., 
Bredeweg, B., Breuker, J. (Eds.): Proceedings of 
AIED, pp. 225-232.  IOS Press. 
Arthur C. Graesser, George T. Jackson, Eric Mathews, 
Heather H. Mitchell, Andrew Olney, Mathew Ven-
tura, Patrick Chipman, Donald R. Franceschetti, 
Xiangen Hu, Max M. Louwerse, Natalie K. Person, 
and the Tutoring Research Group.  2003.  
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog.  Proceed-
ings of the Twenty-Fifth Annual Conference of the 
Cognitive Science Society, pp. 474-479. 
Arthur C. Graesser, Natalie K. Person, and Joseph P. 
Magliano.  1995.  Collaborative dialogue patterns in 
naturalistic one-to-One tutoring.  Applied Cognitive 
Psychology, 9(6):495-522.  John Wiley & Sons, Ltd. 
60
G. Tanner Jackson and Art Graesser.  2007.  Content 
matters: An investigation of feedback categories 
within an ITS.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:127-134.  
IOS Press. 
Sandra Katz, David Allbritton, and John Connelly.  
2003.  Going beyond the problem given: How human 
tutors use post-solution discussions to support trans-
fer.  International Journal of Artificial Intelligence in 
Education, 13:79-116. 
John M. Keller.  1983.  Motivational design of instruc-
tion.  Reigeluth, C.M. (Ed.): Instructional-Design 
Theories and Models: An Overview of Their Current 
Status, pp. 383-429.  Lawrence Erlbaum Associates, 
Inc., Hillsdale, NJ. 
H. Chad Lane and Kurt VanLehn.  2005.  Teaching the 
tacit knowledge of programming to novices with 
natural language tutoring.  Computer Science Educa-
tion, 15:183-201. 
Mark R. Lepper, Maria Woolverton, Donna L. Mumme, 
and Jean-Luc Gurtner.  1993.  Motivational tech-
niques of expert human tutors: Lessons for the design 
of computer-based tutors.  Lajoie, S.P., Derry, S. J. 
(Eds.): Computers as Cognitive Tools, pp. 75-105. 
Lawrence Erlbaum Associates, Inc., Hillsdale NJ. 
Jackson Liscombe, Julia Hirschberg, and Jennifer J. 
Venditti.  2005.  Detecting certainness in spoken tu-
torial dialogues.  Proceedings of Interspeech, 2005. 
Diane J. Litman, Carolyn P. Ros?, Kate Forbes-Riley, 
Kurt VanLehn, Dumisizwe Bhembe, and Scott Silli-
man.  2006.  Spoken versus typed human and com-
puter dialogue tutoring.  International Journal of 
Artificial Intelligence in Education, 16:145-170. 
Johanna Marineau, Peter Wiemer-Hastings, Derek 
Harter, Brent Olde, Patrick Chipman, Ashish Kar-
navat, Victoria Pomeroy, Sonya Rajan, Art Graesser, 
and the Tutoring Research Group.  2000.  Classifica-
tion of speech acts in tutorial dialog.  Proceedings of 
the Workshop on Modeling Human Teaching Tactics 
and Strategies of ITS 2000, pp. 65-71.  Montreal, 
Canada. 
Stellan Ohlsson, Barbara Di Eugenio, Bettina Chow, 
Davide Fossati, Xin Lu, and Trina C. Kershaw.  
2007.  Beyond the code-and-count analysis of tutor-
ing dialogues.  Luckin, R., Koedinger, K. R., Greer, 
J. (Eds.): Proceedings of AIED 2007, 158:349-356.  
IOS Press. 
Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, and 
Arthur C. Graesser.  1995.  Pragmatics and peda-
gogy: Conversational rules and politeness strategies 
may inhibit effective tutoring.  Cognition and In-
struction, 13(2):161-188.  Lawrence Erlbaum Asso-
ciates, Inc., Hillsdale, NJ. 
Heather Pon-Barry, Karl Schultz, Elizabeth Owen Bratt, 
Brady Clark, and Stanley Peters.  2006.  Responding 
to student uncertainty in spoken tutorial dialogue sys-
tems.  International Journal of Artificial Intelligence 
in Education, 16:171-194. 
Ka?ka Porayska-Pomsta and Helen Pain.  2004.  Provid-
ing cognitive and affective scaffolding through teach-
ing strategies:  Applying linguistic politeness to the 
educational context.  J.C. Lester, Vicari, R. M., Para-
gua?u, F. (Eds.): Proceedings of ITS 2004, LNCS 
3220:77-86.  Springer-Verlag Berlin / Heidelberg. 
Genaro Rebolledo-Mendez, Benedict du Boulay, and 
Rosemary Luckin.  2006.  Motivating the learner: an 
empirical evaluation. Ikeda, M., Ashlay, K. D., Chan, 
T.-W. (Eds.): Proceedings of ITS 2006,  LNCS 
4053:545-554.  Springer Verlag Berlin / Heidelberg. 
Carolyn P. Ros?, Dumisizwe Bhembe, Stephanie Siler, 
Ramesh Srivastava, and Kurt VanLehn.  2003.  The 
role of why questions in effective human tutoring.  
Hoppe, U., Verdejo, F., Kay, J. (Eds.): Proceedings 
of AIED 2003, pp. 55-62.  IOS Press. 
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth 
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Carol Van Ess-Dykema, and 
Marie Meteer.  Dialogue act modeling for automatic 
tagging and recognition of conversational speech.  
2000.  Computational Linguistics, 26:339-373. 
Jason Tan and Gautam Biswas.  2006.  The role of feed-
back in preparation for future learning:  A case study 
in learning by teaching environments.  Ikeda, M., 
Ashley, K., Chan, T.-W. (Eds.): Proceedings of ITS 
2006, LNCS 4053:370-381. Springer-Verlag Berlin / 
Heidelberg. 
Kurt VanLehn, Pamela W. Jordan, Carolyn P. Ros?, 
Dumisizwe Bhembe, Michael Bottner, Andy Gaydos, 
Maxim Makatchev, Umarani Pappuswamy, Michael 
Ringenberg, Antonio Roque, Stephanie Siler, and 
Ramesh Srivastava.  2002.  The architecture of 
Why2-Atlas: A coach for qualitative physics essay 
writing.  Proceedings of the 6th International Con-
ference on Intelligent Tutoring Systems, LNCS 
2363:158-167. 
Arthur Ward and Diane Litman.  2006.  Cohesion and 
learning in a tutorial spoken dialog system.  Proceed-
ings of the 19th International FLAIRS (Florida Artifi-
cial Intelligence Research Society) Conference.  
Melbourne Beach, FL. 
Claus Zinn, Johanna D. Moore, and Mark G. Core.  
2002.  A 3-tier planning architecture for managing 
tutorial dialogue.  Intelligent Tutoring Systems, Sixth 
International Conference.  LNCS 2363:574-584.  
Springer-Verlag, London, UK. 
 
 
61
Proceedings of the NAACL HLT Workshop on Innovative Use of NLP for Building Educational Applications, pages 19?26,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Inferring Tutorial Dialogue Structure with Hidden Markov Modeling 
 
 
Kristy 
Elizabeth  
  Boyera 
Eun Young  
 Haa 
     Robert  
Phillipsab 
 Michael     
     D.  
  Wallisab 
Mladen A.  
 Vouka 
James C.  
 Lestera 
 
 
aDepartment of Computer Science, North Carolina State University 
bApplied Research Associates 
Raleigh, NC, USA 
 
{keboyer, eha, rphilli, mdwallis, vouk, lester}@ncsu.edu 
 
 
 
Abstract 
The field of intelligent tutoring systems has 
seen many successes in recent years.  A 
significant remaining challenge is the 
automatic creation of corpus-based tutorial 
dialogue management models.  This paper 
reports on early work toward this goal.  We 
identify tutorial dialogue modes in an 
unsupervised fashion using hidden Markov 
models (HMMs) trained on input 
sequences of manually-labeled dialogue 
acts and adjacency pairs.  The two best-fit 
HMMs are presented and compared with 
respect to the dialogue structure they 
suggest; we also discuss potential uses of 
the methodology for future work. 
1 Introduction 
 
The field of intelligent tutoring systems has made 
great strides toward bringing the benefits of one-
on-one tutoring to a wider population of learners.  
Some intelligent tutoring systems, called tutorial 
dialogue systems, support learners by engaging in 
rich natural language dialogue, e.g., (Graesser et 
al. 2003; Zinn, Moore & Core 2002; Evens & 
Michael 2006; Aleven, Koedinger & Popescu 
2003; Litman et al 2006; Arnott, Hastings & 
Allbritton 2008; VanLehn et al 2002).  However, 
creating these systems comes at a high cost: it 
entails handcrafting each pedagogical strategy the 
tutor might use and then realizing these strategies 
in a dialogue management framework that is also 
custom-engineered for the application.  It is hoped 
that the next generation of these systems can 
leverage corpora of tutorial dialogue in order to 
provide more robust dialogue management models 
that capture the discourse phenomena present in 
effective natural language tutoring.   
The structure of tutorial dialogue has 
traditionally been studied by manually examining 
corpora and focusing on cognitive and 
motivational aspects of tutorial strategies (e.g., 
Lepper et al 1993; Graesser, Person & Magliano 
1995).  While these approaches yielded 
foundational results for the field, such analyses 
suffer from two serious limitations:  manual 
approaches are not easily scalable to different or 
larger corpora, and the rigidity of handcrafted 
dialogue structure tagging schemes may not 
capture all the phenomena that occur in practice.   
In contrast, the stochastic nature of dialogue 
lends itself to description through probabilistic 
models.  In tutorial dialogue, some early work has 
adapted language processing techniques, namely n-
gram analyses, to examine human tutors? responses 
to student uncertainty (Forbes-Riley & Litman 
2005), as well as to find correlations between local 
tutoring strategies and student outcomes (Boyer et 
al. 2008).  However, this work is limited by its 
consideration of small dialogue windows. 
Looking at a broader window of turns is often 
accomplished by modeling the dialogue as a 
Markov decision process.  With this approach, 
19
techniques such as reinforcement learning can be 
used to compare potential policies in terms of 
effectiveness for student learning.  Determining 
relevant feature sets (Tetreault & Litman 2008) 
and conducting focussed experiments for localized 
strategy effectiveness (Chi et al 2008) are active 
areas of research in this line of investigation.  
These approches often fix the dialogue structures 
under consideration in order to compare the 
outcomes associated with those structures or the 
features that influence policy choice.    
    In contrast to treating dialogue structure as a 
fixed entity, one approach for modeling the 
progression of complete dialogues involves 
learning the higher-level structure in order to infer 
succinct probabilistic models of the interaction.  
For example, data-driven approaches for 
discovering dialogue structure have been applied to 
corpora of human-human task-oriented dialogue 
using general models of task structure (Bangalore, 
Di Fabbrizio & Stent 2006).  Encouraging results 
have emerged from using a general model of the 
task structure to inform automatic dialogue act 
tagging as well as subtask segmentation.  
    Our current work examines a modeling 
technique that does not require a priori knowledge 
of the task structure:  specifically, we propose to 
use hidden Markov models (HMMs) (Rabiner 
1989) to capture the structure of tutorial dialogue 
implicit within sequences of tagged dialogue acts.  
Such probablistic inference of discourse structure 
has been used in recent work with HMMs for topic 
identification (Barzilay & Lee 2004) and related 
graphical models for segmenting multi-party 
spoken discourse (Purver et al 2006).  
Analogously, our current work focuses on 
identifying dialogic structures that emerge during 
tutorial dialogue.  Our approach is based on the 
premise that at any given point in the tutorial 
dialogue, the collaborative interaction is ?in? a 
dialogue mode (Cade et al 2008) that characterizes 
the nature of the exchanges between tutor and 
student; these modes correspond to the hidden 
states in the HMM.  Results to date suggest that 
meaningful descriptive models of tutorial dialogue 
can be generated by this simple stochastic 
modeling technique.  This paper focuses on the 
comparison of two first-order HMMs:  one trained 
on sequences of dialogue acts, and the second 
trained on sequences of adjacency pairs.   
 
2 Corpus Analysis 
The HMMs were trained on a corpus of human-
human tutorial dialogue collected in the domain of 
introductory computer science.  Forty-three 
learners interacted remotely with one of fourteen 
tutors through a keyboard-to-keyboard remote 
learning environment yielding 4,864 dialogue 
moves. 
2.1 Dialogue Act Tagging 
The tutoring corpus was manually tagged with 
dialogue acts designed to capture the salient 
characteristics of the tutoring process (Table 1). 
 
Tag Act Example 
Q Question Where should I  
Declare i? 
EQ Evaluation Question How does that look? 
S Statement You need a  
closing brace. 
G Grounding Ok.  
EX Extra-Domain You may use  
your book. 
PF Positive Feedback Yes, that?s right. 
LF Lukewarm Feedback Sort of. 
NF Negative Feedback No, that?s not right. 
Table 1. Dialogue Act Tags 
 
    The correspondence between utterances and 
dialogue act tags is one-to-one; compound 
utterances were split by the primary annotator prior 
to the inter-rater reliability study.1  This dialogue 
act tagging effort produced sequences of dialogue 
acts that have been used in their un-altered forms 
to train one of the two HMMs presented here 
(Section 3).      
2.2 Adjacency Pair Identification 
In addition to the HMM trained on sequences of 
individual dialogue acts, another HMM was 
trained on sequences of dialogue act adjacency 
pairs.  The importance of adjacency pairs is well-
established in natural language dialogue (e.g., 
Schlegoff & Sacks 1973), and adjacency pair 
analysis has illuminated important phenomena in 
tutoring as well (Forbes-Riley et al 2007).  The 
                                                           
1 Details of the study procedure used to collect the corpus, as 
well as Kappa statistics for inter-rater reliability, are reported 
in (Boyer et al 2008). 
20
intuition behind adjacency pairs is that certain 
dialogue acts naturally occur together, and by 
grouping these acts we capture an exchange 
between two conversants in a single structure.  
This formulation is of interest for our purposes 
because when treating sequences of dialogue acts 
as a Markov process, with or without hidden states, 
the addition of adjacency pairs may offer a 
semantically richer observation alphabet.   
    To find adjacency pairs we utilize a ?2 test for 
independence of the categorical variables acti and 
acti+1 for all sequential pairs of dialogue acts that 
occur in the corpus.  Only pairs in which 
speaker(acti) ? speaker(acti+1) were considered.  
Table 2 displays a list of all dependent adjacency 
pairs sorted by descending (unadjusted) statistical 
significance; the subscript on each dialogue act tag 
indicates tutor (t) or student (s). 
    An adjacency pair joining algorithm was applied 
to join statistically significant pairs of dialogue 
acts (p<0.01) into atomic units according to a 
priority determined by the strength of the statistical 
significance.  Dialogue acts that were ?left out? of 
adjacency pair groupings were treated as atomic 
elements in subsequent analysis.  Figure 1 
illustrates the application of the adjacency pair 
joining algorithm on a sequence of dialogue acts 
from the corpus. 
 
 
Figure 1.  DA Sequence Before/After Joining 
3 HMM of Dialogue Structure 
A hidden Markov model is defined by three 
constituents:  1) the set of hidden states (dialogue 
modes), each characterized by its emission 
probability distribution over the possible 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
observations (dialogue acts and/or adjacency 
pairs), 2) the transition probability matrix among 
 
acti acti+1 
P(acti+1|   
    acti) 
P(acti+1| 
   ?acti) 
?2 
val p-val 
EQs PFt 0.48 0.07 654 <0.0001 
Gs Gt 0.27 0.03 380 <0.0001 
EXs EXt 0.34 0.03 378 <0.0001 
EQt PFs 0.18 0.01 322 <0.0001 
EQt Ss 0.24 0.03 289 <0.0001 
EQs LFt 0.13 0.01 265 <0.0001 
Qt Ss 0.65 0.04 235 <0.0001 
EQt LFs 0.07 0.00 219 <0.0001 
Qs St 0.82 0.38 210 <0.0001 
EQs NFt 0.08 0.01 207 <0.0001 
EXt EXs 0.19 0.02 177 <0.0001 
NFs Gt 0.29 0.03 172 <0.0001 
EQt NFs 0.11 0.01 133 <0.0001 
Ss Gt 0.16 0.03 95 <0.0001 
Ss PFt 0.30 0.10 90 <0.0001 
St Gs 0.07 0.04 36 <0.0001 
PFs Gt 0.14 0.04 34 <0.0001 
LFs Gt 0.22 0.04 30 <0.0001 
St EQs 0.11 0.07 29 <0.0001 
Gt EXs 0.07 0.03 14 0.002 
St Qs 0.07 0.05 14 0.0002 
Gt Gs 0.10 0.05 9 0.0027 
EQt EQs 0.13 0.08 8 0.0042 
Table 2. All Dependent Adjacency Pairs 
 
hidden states, and 3) the initial hidden state 
(dialogue mode) probability distribution.   
3.1  Discovering Number of Dialogue Modes 
In keeping with the goal of automatically 
discovering dialogue structure, it was desirable to 
learn n, the best number of hidden states for the 
HMM, during modeling.  To this end, we trained 
and ten-fold cross-validated seven models, each 
featuring randomly-initialized parameters, for each 
number of hidden states n from 2 to 15, inclusive.2  
The average log-likelihood fit from ten-fold cross-
                                                           
2 n=15 was chosen as an initial maximum number of states 
because it comfortably exceeded our hypothesized range of 3 
to 7 (informed by the tutoring literature).  The Akaike 
Information Criterion measure steadily worsened above n = 5, 
confirming no need to train models with n > 15. 
21
validation was computed across all seven models 
for each n, and this average log-likelihood ln was 
used to compute the Akaike Information Criterion, 
a maximum-penalized likelihood estimator that 
prefers simpler models (Scott 2002).  This 
modeling approach was used to train HMMs on 
both the dialogue act and the adjacency pair input 
sequences. 
3.2  Best-Fit Models 
The input sequences of individual dialogue acts 
contain 16 unique symbols because each of the 8 
dialogue act tags (Table 1) was augmented with a 
label of the speaker, either tutor or student.  The 
best-fit HMM for this input sequence contains 
nDA=5 hidden states.  The adjacency pair input 
sequences contain 39 unique symbols, including all 
dependent adjacency pairs (Table 2) along with all 
individual dialogue acts because each dialogue act 
occurs at some point outside an adjacency pair.  
The best-fit HMM for this input sequence contains 
nAP=4 hidden states.  In both cases, the best-fit 
number of dialogue modes implied by the hidden 
states is within the range of what is often 
considered in traditional tutorial dialogue analysis 
(Cade et al 2008; Graesser, Person & Magliano 
1995).   
4 Analysis 
Evaluating the impact of grouping the dialogue 
acts into adjacency pairs requires a fine-grained 
examination of the generated HMMs to gain 
insight into how each model interprets the student 
sessions.     
4.1 Dialogue Act HMM 
Figure 2 displays the emission probability 
distributions for the dialogue act HMM.  State 0DA, 
Tutor Lecture,3 is strongly dominated by tutor 
statements with some student questions and 
positive tutor feedback.  State 1DA constitutes 
Grounding/Extra-Domain, a conversational state 
consisting of acknowledgments, backchannels, and 
discussions that do not relate to the computer 
science task.  State 2DA, Student Reflection, 
                                                           
3 For simplicity, the states of each HMM have been named 
according to an intuitive interpretation of the emission 
probability distribution.   
generates student evaluation questions, statements, 
and positive and negative feedback.  State 3DA is 
comprised of tutor utterances, with positive 
feedback occurring most commonly followed by 
statements, grounding, lukewarm feedback, and 
negative feedback.  This state is interpreted as a 
Tutor Feedback mode.  Finally, State 4DA, Tutor 
Lecture/Probing, is characterized by tutor 
statements and evaluative questions with some 
student grounding statements.   
 
 
Figure 2.  Emission Probability Distributions for 
Dialogue Act HMM 
 
    The state transition diagram (Figure 3) illustrates 
that Tutor Lecture (0DA) and Grounding/Extra-
Domain (1DA) are stable states whose probability of 
self-transition is high:  0.75 and 0.79, respectively.  
Perhaps not surprisingly, Student Reflection (2DA) 
is most likely to transition to Tutor Feedback (3DA) 
with probability 0.77.  Tutor Feedback (3DA) 
transitions to Tutor Lecture (0DA) with probability 
0.60, Tutor Lecture/Probing (4DA) with probability 
0.26, and Student Reflection (2DA) with probability 
0.09.  Finally, Tutor Lecture/Probing (4DA) very 
often transitions to Student Reflection (2DA) with 
probability 0.82. 
 
22
 
Figure 3. Transition diagram for dialogue act HMM 
4.2 Adjacency Pair HMM 
Figure 4 displays the emission probability 
distributions for the HMM that was trained on the 
input sequences of adjacency pairs.  State 0AP, 
Tutor Lecture, consists of tutorial statements, 
positive feedback, and dialogue turns initiated by 
student questions.  In this state, student evaluation 
questions occur in adjacency pairs with positive 
tutor feedback, and other student questions are 
answered by tutorial statements.  State 1AP, Tutor 
Evaluation, generates primarily tutor evaluation 
questions, along with the adjacency pair of tutorial 
statements followed by student acknowledgements.  
State 2AP generates conversational grounding and 
extra-domain talk; this Grounding/Extra-Domain 
state is dominated by the adjacency pair of student 
grounding followed by tutor grounding.  State 3AP 
is comprised of several adjacency pairs:  student 
questions followed by tutor answers, student 
statements with positive tutor feedback, and 
student evaluation questions followed by positive 
feedback.  This Question/Answer state also 
generates some tutor grounding and student 
evaluation questions outside of adjacency pairs.   
 
 
Figure 4.  Emission Probability Distributions for 
Adjacency Pair HMM 
 
 
Figure 5. Transition diagram for adjacency pair HMM 
0DA 
3DA 
2DA 
1DA 
4DA 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
0AP 
3AP 
2AP 
1AP 
p > 0.5 
0.1 ? p ? 0.50 
0.05 ? p < 0.1 
23
 4.3 Dialogue Mode Sequences 
In order to illustrate how the above models fit the 
data, Figure 6 depicts the progression of dialogue 
modes that generate an excerpt from the corpus. 
 
 
Figure 6.  Best-fit sequences of hidden states 
In both models, the most commonly-occurring 
dialogue mode is Tutor Lecture, which generates 
45% of observations in the dialogue act model and 
around 60% in the adjacency pair model.  
Approximately 15% of the dialogue act HMM 
observations are fit to each of states Student 
Reflection, Tutor Feedback, and Tutor 
Lecture/Probing.  This model spends the least 
time, around 8%, in Grounding/Extra Domain.  
The adjacency pair model fits approximately 15% 
of its observations to each of Tutor Evaluation and 
Question/Answer, with around 8% in 
Grounding/Extra-Domain.   
4.4 Model Comparison 
While the two models presented here describe the 
same corpus, it is important to exercise caution 
when making direct structural comparisons.  The 
models contain neither the same number of hidden 
states nor the same emission symbol alphabet; 
therefore, our comparison will be primarily 
qualitative.  It is meaningful to note, however, that 
the adjacency pair model with nAP=4 achieved an 
average log-likelihood fit on the training data that 
was 5.8% better than the same measure achieved 
by the dialogue act model with nDA=5, despite the 
adjacency pair input sequences containing greater 
than twice the number of unique symbols.4   
                                                           
4 This comparison is meaningful because the models depicted 
here provided the best fit among all sizes of models trained for 
the same input scenario. 
    Our qualitative comparison begins by examining 
the modes that are highly similar in the two 
models.  State 2AP generates grounding and extra-
domain statements, as does State 1DA.  These two 
states both constitute a Grounding/Extra-Domain 
dialogue mode.  One artifact of the tutoring study 
design is that all sessions begin in this state due to 
a compulsory greeting that signaled the start of 
each session.  More precisely, the initial state 
probability distribution for each HMM assigns 
probability 1 to this state and probability 0 to all 
other states.     
    Another dialogue mode that is structurally 
similar in the two models is Tutor Lecture, in 
which the majority of utterances are tutor 
statements.  This mode is captured in State 0 in 
both models, with State 0AP implying more detail 
than State 0DA because it is certain in the former 
that some of the tutor statements and positive 
feedback occurred in response to student questions.  
While student questions are present in State 0DA, no 
such precise ordering of the acts can be inferred, as 
discussed in Section 1.    
    Other states do not have one-to-one 
correspondence between the two models.  State 
2DA, Student Reflection, generates only student 
utterances and the self-transition probability for the 
state is very low; the dialogue usually visits State 
2DA for one turn and then transitions immediately 
to another state.  Although this aspect of the model 
reflects the fact that students rarely keep the floor 
for more than one utterance at a time in the corpus, 
such quick dialogue mode transitions are 
inconsistent with an intuitive understanding of 
tutorial dialogue modes as meta-structures that 
usually encompass more than one dialogue turn.  
This phenomenon is perhaps more accurately 
captured in the adjacency pair model.  For 
example, the dominant dialogue act of State 2DA is 
a student evaluation question (EQs).  In contrast, 
these dialogue acts are generated as part of an 
adjacency pair by State 3AP; this model joins the 
student questions with subsequent positive 
feedback from the tutor rather than generating the 
question and then transitioning to a new dialogue 
mode.  Further addressing the issue of frequent 
state transitions is discussed as future work in 
Section 6. 
     
24
5 Discussion and Limitations 
Overall, the adjacency pair model is preferable for 
our purposes because its structure lends itself more 
readily to interpretation as a set of dialogue modes 
each of which encompasses more than one 
dialogue move.  This structural property is 
guaranteed by the inclusion of adjacency pairs as 
atomic elements.  In addition, although the set of 
emission symbols increased to include significant 
adjacency pairs along with all dialogue acts, the 
log-likelihood fit of this model was slightly higher 
than the same measure for the HMM trained on the 
sequences of dialogue acts alone.  The remainder 
of this section focuses on properties of the 
adjacency pair model. 
    One promising result of this early work emerges 
from the fact that by applying hidden Markov 
modeling to sequences of adjacency pairs, 
meaningful dialogue modes have emerged that are 
empirically justified.  The number of these 
dialogue modes is consistent with what researchers 
have traditionally used as a set of hypothesized 
tutorial dialogue modes.  Moreover, the 
composition of the dialogue modes reflects some 
recognizable aspects of tutoring sessions:  tutors 
teach through the Tutor Lecture mode and give 
feedback on student knowledge in a Tutor 
Evaluation mode.  Students ask questions and state 
their own perception of their knowledge in a 
Question/Answer mode.  Both parties engage in 
?housekeeping? talk containing such things as 
greetings and acknowledgements, and sometimes, 
even in a controlled environment, extra-domain 
conversation occurs between the conversants in the 
Grounding/Extra-Domain mode.   
    Although the tutorial modes discovered may not 
map perfectly to sets of handcrafted tutorial 
dialogue modes from the literature (e.g., Cade et 
al. 2008), it is rare for such a perfect mapping to 
exist even between those sets of handcrafted 
modes.  In addition, the HMM framework allows 
for succinct probabilistic description of the 
phenomena at work during the tutoring session:  
through the state transition matrix, we can see the 
back-and-forth flow of the dialogue among its 
modes. 
6 Conclusions and Future Work 
Automatically learning dialogue structure is an 
important step toward creating more robust tutorial 
dialogue management systems.  We have presented 
two hidden Markov models in which the hidden 
states are interpreted as dialogue modes for task-
oriented tutorial dialogue.  These models were 
learned in an unsupervised fashion from manually-
labeled dialogue acts.  HMMs offer concise 
stochastic models of the complex interaction 
patterns occurring in natural language tutorial 
dialogue.  The evidence suggests this 
methodology, which as presented requires only a 
sequence of dialogue acts as input, holds promise 
for automatically discovering the structure of 
tutorial dialogue.   
    Future work will involve conducting evaluations 
to determine the benefits gained by using HMMs 
compared to simpler statistical models.  In 
addition, it is possible that more general types of 
graphical models will prove useful in overcoming 
some limitations of HMMs, such as their arbitrarily 
frequent state transitions, to more readily capture 
the phenomena of interest.  The descriptive insight 
offered by these exploratory models may also be 
increased by future work in which the input 
sequences are enhanced with information about the 
surface-level content of the utterance.  In addition, 
knowledge of the task state within the tutoring 
session can be used to segment the dialogue in 
meaningful ways to further refine model structure.   
    It is also hoped that these models can identify 
empirically-derived tutorial dialogue structures that 
can be associated with measures of effectiveness 
such as student learning (Soller & Stevens 2007).  
These lines of investigation could inform the 
development of next-generation natural language 
tutorial dialogue systems.   
Acknowledgments 
Thanks to Marilyn Walker and Dennis Bahler for 
insightful early discussions on the dialogue and machine 
learning aspects of this work, respectively.  This 
research was supported by the National Science 
Foundation under Grants REC-0632450, IIS-0812291, 
CNS-0540523, and GRFP.  Any opinions, findings, and 
conclusions or recommendations expressed in this 
material are those of the authors and do not necessarily 
reflect the views of the National Science Foundation. 
25
 
References 
Aleven, V., K. Koedinger, and O. Popescu. 2003. A 
tutorial dialog system to support self-explanation: 
Evaluation and open questions. Proceedings of the 
11th International Conference on Artificial 
Intelligence in Education: 39-46. 
Arnott, E., P. Hastings, and D. Allbritton. 2008. 
Research methods tutor: Evaluation of a dialogue-
based tutoring system in the classroom. Behavioral 
Research Methods 40(3): 694-698. 
Bangalore, S., Di Fabbrizio, G., and Stent, A. 2006. 
Learning the structure of task-driven human-human 
dialogs.  Proceedings of the 21st International 
Conference on Computational Linguistics and 44th 
Annual Meeting of the ACL: 201-208. 
Barzilay, R., and Lee, L. 2004. Catching the drift: 
Probabilistic content models, with applications to 
generation and summarization.  Proceedings of 
NAACL HLT: 113?120. 
Boyer, K. E., Phillips, R., Wallis, M., Vouk, M., and 
Lester, J. 2008. Balancing cognitive and 
motivational scaffolding in tutorial dialogue.  
Proceedings of the 9th International Conference on 
Intelligent Tutoring Systems: 239-249. 
Cade, W., Copeland, J., Person, N., and D'Mello, S. 
2008. Dialog modes in expert tutoring.  Proceedings 
of the 9th International Conference on Intelligent 
Tutoring Systems: 470-479. 
Chi, M., Jordan, P., VanLehn, K., and Hall, M. 2008. 
Reinforcement learning-based feature selection for 
developing pedagogically effective tutorial dialogue 
tactics.  Proceedings of the 1st International 
Conference  on Educational Data Mining: 258-265. 
Evens, M., and J. Michael. 2006. One-on-one tutoring 
by humans and computers. Lawrence Erlbaum 
Associates, Mahwah, New Jersey. 
Forbes-Riley, K., and Litman, D. J. 2005. Using 
bigrams to identify relationships between student 
certainness states and tutor responses in a spoken 
dialogue corpus. Proceedings of the 6th SIGdial 
Workshop on Discourse and Dialogue: 87-96. 
Forbes-Riley, K., Rotaru, M., Litman, D. J., and 
Tetreault, J. 2007. Exploring affect-context 
dependencies for adaptive system development. 
Proceedings of NAACL HLT: 41-44. 
Graesser, A., G. Jackson, E. Mathews, H. Mitchell, A. 
Olney, M. Ventura, and P. Chipman. 2003. 
Why/AutoTutor: A test of learning gains from a 
physics tutor with natural language dialog. 
Proceedings of the Twenty-Fifth Annual Conference 
of the Cognitive Science Society: 1-6. 
Graesser, A. C., N. K. Person, and J. P. Magliano. 1995. 
Collaborative dialogue patterns in naturalistic one-
to-one tutoring. Applied Cognitive Psychology 9(6): 
495?522. 
Lepper, M. R., M. Woolverton, D. L. Mumme, and J. L. 
Gurtner. 1993. Motivational techniques of expert 
human tutors: Lessons for the design of computer-
based tutors. Pages 75-105 in S. P. Lajoie, and S. J. 
Derry, editors. Computers as cognitive tools. 
Lawrence Erlbaum Associates, Hillsdale, New 
Jersey. 
Litman, D. J., C. P. Ros?, K. Forbes-Riley, K. VanLehn, 
D. Bhembe, and S. Silliman. 2006. Spoken versus 
typed human and computer dialogue tutoring. 
International Journal of Artificial Intelligence in 
Education 16(2): 145-170. 
Purver, M., Kording, K. P., Griffiths, T. L., and 
Tenenbaum, J. B. 2006. Unsupervised topic 
modelling for multi-party spoken discourse.  
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the ACL: 17-24. 
Rabiner, L. R. 1989. A tutorial on hidden Markov 
models and selected applications in speech 
recognition. Proceedings of the IEEE 77(2): 257-
286. 
Schlegoff, E., and H. Sacks. 1973. Opening up closings. 
Semiotica 7(4): 289-327. 
Scott, S. L. 2002. Bayesian methods for hidden Markov 
models: Recursive computing in the 21st century. 
Journal of the American Statistical Association 
97(457): 337-352. 
Soller, A., and R. Stevens. 2007. Applications of 
stochastic  analyses for collaborative learning and 
cognitive assessment. Pages 217-253 in G. R. 
Hancock, and K. M. Samuelsen, editors. Advances 
in latent variable mixture models. Information Age 
Publishing. 
Tetreault, J. R., and D. J. Litman. 2008. A 
reinforcement learning approach to evaluating state 
representations in spoken dialogue systems. Speech 
Communication 50(8-9): 683-696. 
VanLehn, K., P. W. Jordan, C. P. Rose, D. Bhembe, M. 
Bottner, A. Gaydos, M. Makatchev, U. 
Pappuswamy, M. Ringenberg, and A. Roque. 2002. 
The architecture of Why2-atlas: A coach for 
qualitative physics essay writing. Proceedings of 
Intelligent Tutoring Systems Conference: 158?167. 
Zinn, C., Moore, J. D., and Core, M. G. 2002. A 3-tier 
planning architecture for managing tutorial dialogue.  
Proceedings of the 6th International Conference on 
Intelligent Tutoring Systems: 574-584. 
26
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190?1199,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Affect-Enriched Dialogue Act Classification Model  for Task-Oriented Dialogue 
Kristy  Elizabeth  Boyer Joseph F. Grafsgaard Eun Young  Ha Robert  Phillips* James C.  Lester  Department of Computer Science North Carolina State University Raleigh, NC, USA  * Dual Affiliation with Applied Research Associates, Inc. Raleigh, NC, USA  {keboyer, jfgrafsg, eha, rphilli, lester}@ncsu.edu 
 
 
Abstract 
Dialogue act classification is a central chal-lenge for dialogue systems. Although the im-portance of emotion in human dialogue is widely recognized, most dialogue act classifi-cation models make limited or no use of affec-tive channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dia-logue that models facial expressions of users, in particular, facial expressions related to con-fusion. The findings indicate that the affect-enriched classifiers perform significantly bet-ter for distinguishing user requests for feed-back and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively lever-age affective channels to improve dialogue act classification.  1 Introduction Dialogue systems aim to engage users in rich, adaptive natural language conversation. For these systems, understanding the role of a user?s utter-ance in the broader context of the dialogue is a key challenge (Sridhar, Bangalore, & Narayanan, 2009). Central to this endeavor is dialogue act classification, which categorizes the intention be-hind the user?s move (e.g., asking a question, providing declarative information). Automatic dia-logue act classification has been the focus of a 
large body of research, and a variety of approach-es, including sequential models (Stolcke et al, 2000), vector-based models (Sridhar, Bangalore, & Narayanan, 2009), and most recently, feature-enhanced latent semantic analysis (Di Eugenio, Xie, & Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users? expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp & Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, & Fernandez-Dols, 2003; Schmidt & Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dia-logue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spo-ken utterances to aid in online dialogue act classi-fication (Sridhar, Bangalore, & Narayanan, 2009). Other research on emotion in dialogue has in-volved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, & Tetreault, 2009; L?pez-C?zar, Silovsky, & Griol, 2010), but this work has not explored leveraging affect information for automatic user dialogue act classification. Outside of dialogue, sentiment anal-ysis within discourse is an active area of research (L?pez-C?zar et al, 2010), but it is generally lim-
1190
ited to modeling textual features and not multi-modal expressions of emotion such as facial ac-tions. Such multimodal expressions have only just begun to be explored within corpus-based dialogue research (Calvo & D'Mello, 2010; Cavicchio, 2009).   This paper presents a novel affect-enriched dia-logue act classification approach that leverages knowledge of users? facial expressions during computer-mediated textual human-human dia-logue. Intuitively, the user?s affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a ques-tion). We focus specifically on occurrences of stu-dents? confusion-related facial actions during task-oriented tutorial dialogue.  Confusion was selected as the focus of this work for several reasons. First, confusion is known to be prevalent within tutoring, and its implications for student learning are thought to run deep (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Second, while identifying the ?ground truth? of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular faci-al action units and confusion during learning (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). Finally, automatic facial action recognition technologies are developing rap-idly, and confusion-related facial action events are among those that can be reliably recognized auto-matically (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems.  2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classifi-cation within dialogue systems, it has been an ac-tive area of research for some time. Early work on automatic dialogue act classification modeled dis-course structure with hidden Markov models, ex-perimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to 
aid in automatic speech recognition (Stolcke et al, 2000). In contrast to this sequential modeling ap-proach, which is best suited to offline processing, recent work has explored how lexical, syntactic, and prosodic features perform for online dialogue act tagging (when only partial dialogue sequences are available) within a maximum entropy frame-work (Sridhar, Bangalore, & Narayanan, 2009). A recently proposed alternative approach involves treating dialogue utterances as documents within a latent semantic analysis framework, and applying feature enhancements that incorporate such infor-mation as speaker and utterance duration (Di Eugenio et al, 2010). Of the approaches noted above, the modeling framework presented in this paper is most similar to the vector-based maximum entropy approach of Sridhar et al (2009). Howev-er, it takes a step beyond the previous work by in-cluding multimodal affective displays, specifically facial expressions, as features available to an af-fect-enriched dialogue act classification model. 2.2 Detecting Emotions in Dialogue Detecting emotional states during spoken dialogue is an active area of research, much of which focus-es on detecting frustration so that a user can be automatically transferred to a human dialogue agent (L?pez-C?zar et al, 2010). Research on spo-ken dialogue has leveraged lexical features along with discourse cues and acoustic information to classify user emotion, sometimes at a coarse grain along a positive/negative axis (Lee & Narayanan, 2005). Recent work on an affective companion agent has examined user emotion classification within conversational speech (Cavazza et al, 2010). In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communica-tion for which a deeper understanding of user af-fect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial ac-tions, electrocardiograms, skin conductance, and posture sensors (Calvo & D'Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 
1191
which promising automatic technologies exist (Bartlett et al, 2006; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification.  2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dia-logue are often application-specific, for example, focusing on the frustration of users who are inter-acting with a spoken dialogue system (L?pez-C?zar et al, 2010), or on uncertainty expressed by students while interacting with a tutor (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007). In con-trast, the most widely utilized emotion frameworks are not application-specific; for example, Ekman?s Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding fa-cial movements based on human facial anatomy (Ekman & Friesen, 1978).  Within this framework, facial movements are categorized into facial action units, which represent discrete movements of mus-cle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman?s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion ex-pressed within blog text (Das & Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell?s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not fre-quently experience Ekman?s basic emotions of happiness, sadness, anger, fear, surprise, and dis-gust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo & D'Mello, 2010). Our work leverages Ekman?s facial tagging scheme to identi-fy a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007).   2.4 Importance of Confusion in Tutorial Dia-logue Among the affective states that students experience during tutorial dialogue, confusion is prevalent, and its implications for student learning are signif-
icant. Confusion is associated with cognitive dise-quilibrium, a state in which students? existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Students may express such confu-sion within dialogue as uncertainty, to which hu-man tutors often adapt in a context-dependent fashion (Forbes-Riley et al, 2007). Moreover, im-plementing adaptations to student uncertainty with-in a dialogue system can improve the effectiveness of the system (Forbes-Riley et al, 2009).  For tutorial dialogue, the importance of under-standing student utterances is paramount for a sys-tem to positively impact student learning (Dzikovska, Moore, Steinhauser, & Campbell, 2010). The importance of frustration as a cogni-tive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classi-fication of student utterances. This paper explores the use of facial expression features in this way.  3 Task-Oriented Dialogue Corpus The corpus was collected during a textual human-human tutorial dialogue study in the domain of introductory computer science (Boyer, Phillips, et al, 2010). Students solved an introductory com-puter programming problem and carried on textual dialogue with tutors, who viewed a synchronized version of the students? problem-solving work-space. The original corpus consists of 48 dia-logues, one per student. Each student interacted with one of two tutors. Facial videos of students were collected using built-in webcams, but were not shown to the tutors. Video quality was ranked based on factors such as obscured foreheads due to hats or hair, and improper camera position result-ing in students? faces not being fully captured on the video. The highest-quality set contained 14 videos, and these videos were used in this analysis. They have a total running time of 11 hours and 55 minutes, and include dialogues with three female subjects and eleven male subjects.  3.1 Dialogue act annotation The dialogue act annotation scheme (Table 1) was applied manually. The kappa statistic for inter-annotator agreement on a 10% subset of the corpus was ?=0.80, indicating good reliability.   
1192
Table 1. Dialogue act tags and relative frequencies across fourteen dialogues in video corpus Student Dialogue Act Example Rel. Freq. EXTRA-DOMAIN (EX) Little sleep deprived today .08 GROUNDING (G) Ok or Thanks .21 NEGATIVE FEEDBACK WITH ELABORATION (NE) I?m still confused on what this next for loop is doing. .02 NEGATIVE FEEDBACK (N) I don?t see the diff. .04 POSITIVE FEEDBACK WITH ELABORATION (PE) 
It makes sense now that you explained it, but I never used an else if in any of my other programs .04 POSITIVE FEEDBACK (P) Second part complete. .11 QUESTION (Q) Why couldn?t I have said if (i<5) .11 STATEMENT (S) i is my only index .07 
REQUEST FOR FEEDBACK (RF) So I need to create a new method that sees how many elements are in my array? .16 RESPONSE (RSP) You mean not the length but the contents .14 UNCERTAIN FEEDBACK WITH ELABORATION (UE) I?m trying to remember how to copy arrays .008 UNCERTAIN FEEDBACK (U) Not quite yet .008  3.2 Task action annotation The tutoring sessions were task-oriented, focusing on a computer programming exercise. The task had several subtasks consisting of programming mod-ules to be implemented by the student. Each of those subtasks also had numerous fine-grained goals, and student task actions either contributed or did not contribute to the goals. Therefore, to obtain a rich representation of the task, a manual annota-tion along two dimensions was conducted (Boyer, Phillips, et al, 2010). First, the subtask structure was annotated hierarchically, and then each task action was labeled for correctness according to the requirements of the assignment. Inter-annotator agreement was computed on 20% of the corpus at the leaves of the subtask tagging scheme, and re-
sulted in a simple kappa of ?=.56. However, the leaves of the annotation scheme feature an implicit ordering (subtasks were completed in order, and adjacent subtasks are semantically more similar than subtasks at a greater distance); therefore, a weighted kappa is also meaningful to consider for this annotation. The weighted kappa is ?weighted=.80. An annotated excerpt of the corpus is displayed in Table 2.   Table 2. Excerpt from corpus illustrating annota-tions and interplay between dialogue and task 13:38:09 Student: How do I know where to end? [RF] 13:38:26 Tutor: Well you told me how to get how many elements in an array by using .length right? 13:38:26 Student: [Task action:  Subtask 1-a-iv, Buggy] 13:38:56 Tutor: Great 13:38:56 Student: [Task action: Subtask 1-a-v, Correct] 13:39:35 Student: Well is it "array.length"? [RF]  **Facial Expression: AU4 13:39:46 Tutor: You just need to use the correct array name 13:39:46 Student: [Task action:  Subtask 1-a-iv, Buggy] 3.3 Lexical and Syntactic Features In addition to the manually annotated dialogue and task features described above, syntactic features of each utterance were automatically extracted using the Stanford Parser (De Marneffe et al, 2006). From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts dur-ing task-oriented tutorial dialogue (Boyer, Ha et al 2010).  
1193
4 Facial Action Tagging An annotator who was certified in the Facial Ac-tion Coding System (FACS) (Ekman, Friesen, & Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test designed to ana-lyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman & Rosenberg, 2005). This annotator viewed the vide-os continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Brow Lowerer) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). To establish reliability of the annotation, a se-cond FACS-certified annotator independently an-notated 36% of the video corpus (5 of 14 dialogues), chosen randomly after stratification by gender and tutor. This annotator followed the same method as the first annotator, pausing the video at any point to tag facial action events. At any given time in the video, the coder was first identifying whether an action unit event existed, and then de-scribing the facial movements that were present. The annotators also specified the beginning and ending time of each event. In this way, the action unit event tags spanned discrete durations of vary-ing length, as specified by the coders. Because the two coders were not required to tag at the same point in time, but rather were permitted the free-dom to stop the video at any point where they felt a notable facial action event occurred, calculating agreement between annotators required discretiz-ing the continuous facial action time windows across the tutoring sessions. This discretization was performed at granularities of 1/4, 1/2, 3/4, and 1 second, and inter-rater reliability was calculated at each level of granularity (Table 3). Windows in which both annotators agreed that no facial action event was present were tagged by default as neu-tral. Figure 1 illustrates facial expressions that dis-play facial Action Unit 4. 
  Table 3. Kappa values for inter-annotator agree-ment on facial action events  Granularity  ? sec ? sec ? sec 1 sec Presence of AU4 (Brow Lowerer)  .84 .87 .86 .86   
  
  Figure 1. Facial expressions displaying AU4 (Brow Lowerer)  Despite the fact that promising automatic ap-proaches exist to identifying many facial action units (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emo-tions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other sponta-neous student facial expressions that differ from those associated with confusion.    
1194
   
  Figure 2. Other facial expressions from the corpus 5 Models The goal of the modeling experiment was to de-termine whether the addition of confusion-related facial expression features significantly boosts dia-logue act classification accuracy for student utter-ances.  5.1 Features We take a vector-based approach, in which the fea-tures consist of the following:  Utterance Features ? Dialogue act features: Manually annotated dialogue act for the past three utterances. These features include tutor dialogue acts, annotated with a scheme analogous to that used to annotate student utterances (Boyer et al, 2009). ? Speaker: Speaker for past three utterances ? Lexical features: Word unigrams ? Syntactic features: Top-most syntactic node and its first two children  Task-based Features ? Subtask: Hierarchical subtask structure for past three task actions (semantic pro-gramming actions taken by student) ? Correctness: Correctness of past three task actions taken by student ? Preceded by task: Indicator for whether the most recent task action immediately pre-ceded the target utterance, or whether it 
was immediately preceded by the last dia-logue move  Facial Expression Features ? AU4_1sec: Indicator for the display of the brow lowerer within 1 second prior to this utterance being sent, for the most recent three utterances ?  AU4_5sec: Indicator for the display of the brow lowerer within 5 seconds prior to this utterance being sent, for the most recent three utterances ? AU4_10sec: Indicator for the display of the brow lowerer within 10 seconds prior to this utterance being sent, for the most recent three utterances  5.2 Modeling Approach A logistic regression approach was used to classify the dialogue acts based on the above feature vec-tors. The Weka machine learning toolkit (Hall et al, 2009) was used to learn the models and to first perform feature selection in a best-first search. Lo-gistic regression is a generalized maximum likeli-hood model that discriminates between pairs of output values by calculating a feature weight vec-tor over the predictors.  The goal of this work is to explore the utility of confusion-related facial features in the context of particular dialogue act types. For this reason, a specialized classifier was learned by dialogue act. 5.3 Classification Results The classification accuracy and kappa for each specialized classifier is displayed in Table 4. Note that kappa statistics adjust for the accuracy that would be expected by majority-baseline chance; a kappa statistic of zero indicates that the classifier performed equal to chance, and a positive kappa statistic indicates that the classifier performed bet-ter than chance. A kappa of 1 constitutes perfect agreement. As the table illustrates, the feature se-lection chose to utilize the AU4 feature for every dialogue act except STATEMENT (S). When consid-ering the accuracy of the model across the ten folds, two of the affect-enriched classifiers exhibit-ed statistically significantly better performance. For GROUNDING (G) and REQUEST FOR FEEDBACK (RF), the facial expression features significantly 
1195
improved the classification accuracy compared to a model that was learned without affective features.  6 Discussion Dialogue act classification is an essential task for dialogue systems, and it has been addressed with a variety of modeling approaches and feature sets. We have presented a novel approach that treats facial expressions of students as constraining fea-tures for an affect-enriched dialogue act classifica-tion model in task-oriented tutorial dialogue. The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and REQUEST FOR FEEDBACK.   Table 4. Classification accuracy and kappa for spe-cialized DA classifiers. Statistically significant differences (across ten folds, one-tailed t-test) are shown in bold.    Classifier with AU4 Classifier without AU4  Dialogue Act % acc ? % acc ? p-value EX 90.7 .62 89.0 .28 >.05 G 92.6 .76 91 .71 .018 P 93 .49 92.2 .40 >.05 Q 94.6 .72 94.2 .72 >.05 S Not chosen in feat. sel. 93 .22 n/a RF 90.7 .62 88.3 .53 .003 
RSP 93 .68 95 .75 >.05 NE * *  N * * PE * * U * * UE * * *Too few instances for ten-fold cross-validation. 
6.1 Features Selected for Classification Out of more than 1500 features available during feature selection, each of the specialized dialogue act classifiers selected between 30 and 50 features in each condition (with and without affect fea-tures). To gain insight into the specific features that were useful for classifying these dialogue acts, it is useful to examine which of the AU4 history features were chosen during feature selection.  For GROUNDING, features that indicated the presence of absence of AU4 in the immediately preceding utterance, either at the 1 second or 5 se-cond granularity, were selected. Absence of this confusion-related facial action unit was associated with a higher probability of a grounding act, such as an acknowledgement. This finding is consistent with our understanding of how students and tutors interacted in this corpus; when a student experi-enced confusion, she would be unlikely to then make a simple grounding dialogue move, but in-stead would tend to inspect her computer program, ask a question, or wait for the tutor to explain more. For REQUEST FOR FEEDBACK, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified). This finding suggests that there may be some lag between the student experiencing confusion and then choosing to make a request for feedback, and that the confusion-related facial expressions may re-emerge as the student is making a request for feedback, since the five-second window prior to the student sending the textual dialogue message would overlap with the student?s construction of the message itself.    Although the improvements seen with AU4 fea-tures for QUESTION, POSITIVE FEEDBACK, and EXTRA-DOMAIN acts were not statistically reliable, examining the AU4 features that were selected for classifying these moves points toward ways in which facial expressions may influence classifica-tion of these acts (Table 5).      
1196
Table 5. Number of features, and AU4 features selected, for specialized DA classifiers  Dialogue Act # fea-tures selected AU4 features selected G 43 One utterance ago: AU4_1sec, AU4_5sec 
RF 37 Three utterances ago: AU4_10sec Target utterance: AU4_5sec EX 50 Three utterances ago: AU4_1sec P 36 Current utterance: AU4_10sec Q 30 One utterance ago: AU4_5sec  6.2 Implications The results presented here demonstrate that lever-aging knowledge of user affect, in particular of spontaneous facial expressions, may improve the performance of dialogue act classification models. Perhaps most interestingly, displays of confusion-related facial actions prior to a student dialogue move enabled an affect-enriched classifier to rec-ognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008). Requesting feedback also seems to be an important behavior of students, characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, & Lester, 2007). 6.3 Limitations The experiments reported here have several nota-ble limitations. First, the time-consuming nature of manual facial action tagging restricted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been suf-ficiently clear to permit tagging, which would have increased the sample size and likely revealed sta-tistically significant trends. For example, the per-
formance of the affect-enriched classifier was bet-ter for dialogue acts of interest such as positive feedback and questions, but this difference was not statistically reliable.  An additional limitation stems from the more fundamental question of which affective states are indicated by particular external displays. The field is only just beginning to understand facial expres-sions during learning and to correlate these facial actions with emotions. Additional research into the ?ground truth? of emotion expression will shed additional light on this area. Finally, the results of manual facial action annotation may constitute up-per-bound findings for applying automatic facial expression analysis to dialogue act classification. 7 Conclusions and Future Work Emotion plays a vital role in human interactions. In particular, the role of facial expressions in human-human dialogue is widely recognized. Facial ex-pressions offer a promising channel for under-standing the emotions experienced by users of dialogue systems, particularly given the ubiquity of webcam technologies and the increasing number of dialogue systems that are deployed on webcam-enabled devices. This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act REQUEST FOR FEEDBACK, and the conversational moves of GROUNDING. These early findings highlight the importance of future work in this area. Dialogue act classifica-tion models have not fully leveraged some of the techniques emerging from work on sentiment anal-ysis. These approaches may prove particularly use-ful for identifying emotions in dialogue utterances. Another important direction for future work in-volves more fully exploring the ways in which af-fect expression differs between textual and spoken dialogue. Finally, as automatic facial tagging tech-nologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future.    
1197
Acknowledgments This work is supported in part by the North Caroli-na State University Department of Computer Sci-ence and by the National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the Na-tional Science Foundation.  References  A. Andreevskaia and S. Bergler. 2008. When specialists and generalists work together: Overcoming do-main dependence in sentiment tagging. Proceed-ings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT), 290-298.  M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2006. Fully Automatic Facial Action Recognition in Spontaneous Behav-ior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 223-230.  K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influ-ence of learner characteristics on task-oriented tu-torial dialogue. Proceedings of the International Conference on Artificial Intelligence in Educa-tion, 365?372.  K.E. Boyer, E.Y. Ha, R. Phillips, M.D. Wallis, M. Vouk, and J.C. Lester. 2010. Dialogue act model-ing in a complex task-oriented domain. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 297-305.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of the Annual Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: Short Papers, 49-52.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2010. Leveraging hidden dialogue state to select tutorial moves. Proceed-ings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, 66-73. R.A. Calvo and S. D?Mello. 2010. Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affec-tive Computing, 1(1): 18-37. 
M. Cavazza, R.S.D.L. C?mara, M. Turunen, J. Gil, J. Hakulinen, N. Crook, et al 2010. How was your day? An affective companion ECA prototype. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 277-280.  F. Cavicchio. 2009. The modulation of cooperation and emotion in dialogue: the REC Corpus. Proceed-ings of the ACL-IJCNLP 2009 Student Research Workshop, 43-48.  J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Mori-yama. 2004. Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior. IEEE International Conference on Systems, Man and Cybernetics, 610-616. S.D. Craig, S. D?Mello, A. Witherspoon, J. Sullins, and A.C. Graesser. 2004. Emotions during learning: The first steps toward an affect sensitive intelli-gent tutoring system. In J. Nall and R. Robson (Eds.), E-learn 2004: World conference on E-learning in Corporate, Government, Healthcare, & Higher Education, 241-250.  D. Das and S. Bandyopadhyay. 2009. Word to sentence level emotion tagging for Bengali blogs. Proceed-ings of the ACL-IJCNLP Conference, Short Pa-pers, 149-152.  S. Dasgupta and V. Ng. 2009. Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification. Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP, 701-709.  B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue Act Classification, Higher Order Dialogue Struc-ture, and Instance-Based Learning. Dialogue & Discourse, 1(2): 1-24.  M. Dzikovska, J.D. Moore, N. Steinhauser, and G. Campbell. 2010. The impact of interpretation problems on tutorial dialogue. Proceedings of the 48th Annual Meeting of the Association for Com-putational Linguistics, Short Papers, 43-48.  S. D?Mello, S.D. Craig, J. Sullins, and A.C. Graesser. 2006. Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTu-tor?s Mixed- Initiative Dialogue. International Journal of Artificial Intelligence in Education, 16(1): 3-28. P. Ekman. 1999. Basic Emotions. In T. Dalgleish and M. J. Power (Eds.), Handbook of Cognition and Emotion. New York: Wiley. P. Ekman, W.V. Friesen. 1978. Facial Action Coding System. Palo Alto, CA: Consulting Psychologists Press. P. Ekman, W.V. Friesen, and J.C. Hager. 2002. Facial Action Coding System: Investigator?s Guide. Salt Lake City, USA: A Human Face. 
1198
P. Ekman and E.L. Rosenberg (Eds.). 2005. What the Face Reveals: Basic and Applied Studies of Spon-taneous Expression Using the Facial Action Cod-ing System (FACS) (2nd ed.). New York: Oxford University Press. K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2007. Exploring affect-context depend-encies for adaptive system development. The Con-ference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL HLT), Short Papers, 41-44.  K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2009. Adapting to student uncertainty improves tutoring dialogues. Proceedings of the 14th International Conference on Artificial Intelli-gence in Education (AIED), 33-40.  A.C. Graesser, S. Lu, B. Olde, E. Cooper-Pye, and S. Whitten. 2005. Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down. Memory & Cognition, 33(7): 1235-1247.  S. Greene and P. Resnik. 2009. More than words: Syn-tactic packaging and implicit sentiment. Proceed-ings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Lan-guage Technologies (NAACL HLT), 503-511.  M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explora-tions, 11(1): 10?18.  R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorpo-rating extra-linguistic information into reference resolution in collaborative task dialogue. Proceed-ings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, 1259-1267.  M.L. Knapp and J.A. Hall. 2006. Nonverbal Communi-cation in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. C.M. Lee, S.S. Narayanan. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2): 293-303.  R. L?pez-C?zar, J. Silovsky, and D. Griol. 2010. F2?New Technique for Recognition of User Emotion-al States in Spoken Dialogue Systems. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 281-288.  B.T. McDaniel, S. D?Mello, B.G. King, P. Chipman, K. Tapp, and A.C. Graesser. 2007. Facial Features for Affective State Detection in Learning Envi-ronments. Proceedings of the 29th Annual Cogni-tive Science Society, 467-472. D. McNeill. 1992. Hand and mind: What gestures reveal about thought. Chicago: University of Chicago Press. 
A. Mehrabian. 2007. Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. T. Nguyen. 2010. Mood patterns and affective lexicon access in weblogs. Proceedings of the ACL 2010 Student Research Workshop, 43-48.  M. Pantic and M.S. Bartlett. 2007. Machine Analysis of Facial Expressions. In K. Delac and M. Grgic (Eds.), Face Recognition, 377-416. Vienna, Aus-tria: I-Tech Education and Publishing. J.A. Russell. 2003. Core affect and the psychological construction of emotion. Psychological Review, 110(1): 145-172. J.A. Russell, J.A. Bachorowski, and J.M. Fernandez-Dols. 2003. Facial and vocal expressions of emo-tion. Annual Review of Psychology, 54, 329-49. K.L. Schmidt and J.F. Cohn. 2001. Human Facial Ex-pressions as Adaptations: Evolutionary Questions in Facial Expression Research. Am J Phys An-thropol, 33: 3-24. V.J. Shute. 2008. Focus on Formative Feedback. Re-view of Educational Research, 78(1): 153-189.  V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4): 407-422. Elsevier Ltd.  A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, et al 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Con-versational Speech. Computational Linguistics, 26(3): 339-373.  C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence and expression level annotation of opinions in us-er-generated discourse. Proceedings of the 48th Annual Meeting of the Association for Computa-tional Linguistics, 575-584.  T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-ing Contextual Polarity: An Exploration of Fea-tures for Phrase-Level Sentiment Analysis. Computational Linguistics, 35(3): 399-433.  Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 31(1): 39-58. 
1199
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 341?344,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
NCSU: Modeling Temporal Relations  with Markov Logic and Lexical Ontology 
 Eun Young Ha Alok Baikadi Carlyle Licata  James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha,abaikad,cjlicata,lester}@ncsu.edu     Abstract As a participant in TempEval-2, we ad-dress the temporal relations task consist-ing of four related subtasks. We take a su-pervised machine-learning technique us-ing Markov Logic in combination with rich lexical relations beyond basic and syntactic features. One of our two submit-ted systems achieved the highest score for the Task F (66% precision), untied, and the second highest score (63% precision) for the Task C, which tied with three other systems.  1 Introduction Time plays a key role in narrative. However, cor-rectly recognizing temporal order among events is a challenging task. As a follow-up to the first TempEval competition, TempEval-2 addresses this challenge. Among the three proposed tasks of TempEval-2, we address the temporal rela-tions task consisting of four subtasks: predicting temporal relations that hold between events and time expressions in the same sentence (Task C), events and the document creation time (Task D), main events in adjacent sentences (Task E), and main events and syntactically dominated events, such as those in subordinated clauses (Task F). We are primarily concerned with Task C, E, and F, because D is not relevant to our application domain.1 However, rather than eliminating Task D altogether, we build a very simple model for this task by using only those features that are shared with other task models (i.e., the document                                                 1 Our application domain concerns analysis of narrative stories written by middle school students, with the analysis being conducted a single story at a time. 
creation time data are not used because none of the other task models need them as features). It was expected that this approach would support more interesting comparisons with other systems that take a more sophisticated approach to the task. Further, we experiment with a joint model-ing technique to examine if the communication with other task models brings a boost to a per-formance of the simple model. Taking a supervised machine-learning ap-proach with Markov Logic (ML) (Richardson and Domingos, 2006), we constructed two systems, NCSU-INDI and NCSU-JOINT. NCSU-INDI con-sists of four independently trained classifiers, one for each task, whereas NCSU-JOINT models all four tasks jointly. The choice of ML as learn-ing technique for temporal relations is motivated both theoretically and practically. Theoretically, it is a statistical relational learning framework that does not make the i.i.d. assumption for the data. This is a desirable characteristic for com-plex problems such as temporal relation classifi-cation, as well as many other natural language problems, in which the features representing a given problem are often correlated with one an-other. Practically, ML allows us to build both individual and joint models in a uniform frame-work; individual models can be easily combined together into a joint model with a set of global formulae governing over them.  In previous work (Yoshikawa et al, 2009), ML was successfully applied to temporal relation classification task. Our approach is different from this work in two primary respects. First, we introduce new lexical relation features derived from English lexical ontologies. Second, our model addresses a new task introduced in Tem-pEval-2, which is to identify temporal relations between main and syntactically dominated events in the same sentence. We also employ phrase-based syntactic features (Bethard and 
341
Martin 2007) rather than dependency-based syn-tactic features. 2 Features We consider three types of features: basic, syn-tactic, and lexical relation features. Basic fea-tures represent the information directly available from the original data provided by the task orga-nizer; syntactic features are extracted from syn-tactic parses generated by Charniak parser (Charniak, 2000); and lexical semantic relations that are derived from two external lexical data-bases, VERBOCEAN (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). 2.1 Basic Features Basic features include the word tokens, stems of the words, and the manually annotated attributes of events and time expressions. In the TempEval-2 data, an event always consists of a single word token, but time expressions often consist of mul-tiple tokens. We treat each word in time expres-sions as a different feature. For example, two word features, ?this? and ?afternoon?, are ex-tracted from a given time expression ?this after-noon?. Stemming is done with the Porter Stemmer in NLTK (Loper and Bird, 2002). The value attributes of time expressions are treated as symbolic features, rather than being decomposed into actual integer values representing dates and times.  2.2 Syntactic Features Our syntactic features draw upon the features previously shown to be effective for temporal relation classification (Bethard and Martin, 2007), including the following: ? pos: the part-of-speech (pos) tags of the event and the time expression word to-kens, assigned by Charniak parser.  ? gov-prep: any prepositions governing the event or time expression (e.g., ?for? in ?for ten years?). ? gov-verb: the verb governing the event or time expression, similar to gov-prep. ? gov-verb-pos: the pos tag of the governing verb. We also investigate both full and partial syn-tactic paths between a pair of event and time ex-pressions, but including these features does not improve the classification results on our devel-opment data set. 
2.3 Lexical Relation Features VERBOCEAN is a graph of semantic relations between verbs. There are 22,306 relations be-tween 3,477 verbs that have been mined using Google searches for lexico-syntactic patterns. VERBOCEAN contains five different types of re-lations (Table 1). Verbs are stored in the lemma-tized forms and senses are not disambiguated. A connection between two verbs indicates that the relation holds between some senses of the verbs. VERBOCEAN?S database is presented as a list of verb pair relations, along with a confidence score. Both the transitive and symmetric closure over the relations were taken before storage in a SQLite database for queries. The transitive clo-sure was calculated using the Warshall algorithm (Agrawal and Jagadish, 1990). The confidence score for the new arc was calculated as the aver-age of the two constituents. The symmetric clo-sure was calculated using a simple pass. The confidence score is the same as the reflected edge for symmetric relations. A set of VER-BOCEAN features were calculated for each target event pair within each of the temporal relations tasks. Each verb was lemmatized using the WordNet lemmatizer in NLTK before being compared against the database. Rather than fo-cusing only on HAPPENS-BEFORE relation as in Mani et al (2006), we consider all five verb rela-tions in two different versions, unweighted and weighted. The unweighted version is a binary feature indicating the existence of an arc between the two target verbs in VERBOCEAN. In the weighted version, the existence of an arc is weighted by the associated confidence score.  In addition to VerbOcean, WordNet was used for its conceptual relations. WordNet is a large lexical database, which contains information on verbs, nouns, adjectives and adverbs, grouped into hierarchically organized cognitive synonym 
                                                2 Examples are taken from http://demo.patrickpantel.com/Content/Verbocean/. 
Relation Example  SIMILARITY ?? produce :: create STRENGTH ? wound :: kill ANTONYMY ? open :: close  ENABLEMENT  fight :: win HAPPENS-BEFORE ? buy :: own  Table 1: Semantic relations between verbs in VERBOCEAN (? and ? denotes symmetric and transitive closure, respectively, holds for the given relation)2 
342
sets (synsets). WordNet was accessed through the WordNetCorpusReader module of NLTK. For each target event pair within each of the temporal relations tasks, a semantic distance be-tween the associated tokens was computed using the path-similarity metric present within the API. The synset chosen was simply the first synset returned by the reader. Similar to the VER-BOCEAN features, we consider both unweighted and weighted versions of the feature.   3 The Systems ML is a probabilistic extension of first-order logic that allows formulae to be violated. It as-signs a weight to each formula, reflecting the strength of the constraint represented by the for-mula. A Markov logic network (MLN) is a set of weighted first-order clauses, which, together with constants, defines a Markov network.  We constructed two systems, NCSU-INDI and NCSU-JOINT using an off-the-shelf tool for ML (Riedel, 2008). 3.1 NCSU-INDI NCSU-INDI consists of four independently trained MLNs, one for each task. Each MLN is defined by a set of local formulae that are con-junctions of predicates representing the features. An example local formula used for Task C is  eventTimex(e, t)  eventWord(e, w)            relEventTimex(e, t, r)        (1)  If a pair of event e and time expression t exists and the event consists of a word token w, for-mula (1) assigns a temporal relation t to the given pair of e and t with some weights. For each task, the features described in Sec-tion 2 were examined on a held-out development data set (about 10% of the training data) for their effectiveness in predicting temporal relations and removed if they do not improve the results. Ta-ble 2 lists the features actually used for the tasks. Interestingly, none of the time expression fea-tures were effective on the development data. 3.2 NCSU-JOINT As well as the local formulae from the four local MLNs, a set of global formulae are added to NCSU-JOINT as hard constraints to ensure the consistency between the classification decisions of local MLNs. For example, formula (2) ensures that if an event e1 happens before the document creation time (dct) and another event e2 happens 
after dct, then e1 happens before e2 and vice ver-sa.  relDctEvent(e1,t,BEFORE) relDctEvent(e2,t, AFTER)                         relEvents(e1, e2, BEFORE)       (2)  A set of global constraints is defined between Tasks C and F, D and F, as well as D and E, re-spectively. 4 Results and Discussion The predicted outputs from our systems exhibit mixed results. NCSU-INDI achieves the highest precision score on the test data for Task F by a relatively large margin (6%) from the second-place system, as well as the second highest preci-sion score on Task C, tied with three other sys-tems. Given the encouraging result for Task F, we would preliminarily conclude that the VE-BOCEAN relations are effective predictors of temporal relations between main and syntacti-cally dominated events. However, the same sys-tem does not achieve the same level of accuracy 
Task Feature C  D E F event-word ? ? ?e2 ?e1,e2 Event event-stem ? ? ?e1.e2 ?e1,e2 event-polarity ? ? ?e1.e2 ?e1,e2 event-modal ? ? ?e1.e2 ?e1,e2 event-pos ? ? ?e1.e2 ?e2 event-tense ?  ?e1.e2 ?e1,e2 event-aspect ? ? ?e1.e2 ?e1,e2 
Event  Attribute 
event-class ? ? ?e1.e2 ?e1,e2 timex-word     Timex timex-stem     timex-type     Timex  Attribute timex-value     pos  ?e ?e1.e2  gov-prep ?e,t ?e ?e1.e2 ?e1,e2 gov-verb ?e,t ?e ?e1.e2 ?e1,e2 
Syntactic Parse 
gov-verb-pos ?e,t  ?e1.e2 ?e1,e2 verb-rel    ? Verb-Ocean verb-rel-w   ?  word-dist   ?  WordNet word-dist-w      Table 2: Features used for each task (subscripts e and t mean event and time expression, re-spectively. Subscripts e1 and e2 mean the first and the second main events for the Task E and the main and the syntactically dominated events for the Task F, respectively)   
343
for Task E, even though it is closely related to Task F. The major difference between the mod-els of Task E and F is that the Task E model uses weighted VERBOCEAN relations along with a WordNet feature, while the Task F model uses unweighted VERBOCEAN relations without the WordNet feature. We suspect these two features might negatively impact the classification deci-sions on the test data, even though they prelimi-narily appeared to be effective predictors on the development data.  NCSU-JOINT also yields mixed results. The performance on both Task D and F dramatically drops with the joint modeling approach, while there is a modest improvement on Task E. Man-ual examination of the results on the test data revealed that the majority of the relations in Task D and F were classified as OVERLAP, which may be due to overly strict global constraints; rather than violating global constraints, the system re-sorted to rather neutral predictions.  5 Conclusions Temporal event order recognition is a challeng-ing task. Using basic, syntactic, and lexical rela-tion features, we built two systems with ML: NCSU-INDI models each subtask independently, and NCSU-JOINT models all four tasks jointly. NCSU-INDI was most effective in predicting temporal relations between main events and syn-tactically dominated events (66% precision), as well as temporal relations between time expres-sions and events (63% precision). Future direc-tions include conducting a more rigorous exami-nation of the predictive power of the features, as well as the impact of global formulae for the joint model.  Acknowledgments This research was supported by the National Sci-ence Foundation under Grant IIS-0757535.  Any opinions, findings, and conclusions or recom-mendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.  
References  R. Agrawal, S. Dar, and H. V. Jagadish. 1990. Direct transitive closure algorithms: design and perform-ance evaluation. ACM Transactions on Database Systems, 15(3): 427-458. S. Bethard and J. H. Martin. 2007. CU-TMP: tempo-ral relation classification using syntactic and se-mantic features. In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, pages 129-132, Prague, Czech Republic. E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American chapter of the Association for Computational Lin-guistics conference, pages 132-139, Seattle, WA. Y. Cheng, M. Asahara, and Y. Matsumoto. 2007. NAIST.Japan: Temporal relation identification us-ing dependency parsed tree. In Proceedings of the 4th International Workshop on Semantic Evalua-tions, pages 245-248, Prague, Czech Republic. T. Chklovski and P. Pantel. 2004.VerbOcean: Mining the Web for Fine-Grained Semantic Verb Rela-tions. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 33-40, Barcelona, Spain. C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press. E. Loper and S. Bird. 2002. NLTK: The Natural Lan-guage Toolkit. In Proceedings of ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62?69, Philadelphia, PA. I. Mani, M. Verhagen, B. Wellner, C. M. Lee, and J. Pustejovsky. 2006. Machine learning of temporal relations. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Compu-tational Linguistics, pages 753-760, Sydney, Aus-tralia. M. Richardson and P. Domingos. 2006. Markov Log-ic Networks. Machine Learning, 62(1): 107-136. 
S. Riedel. 2008. Improving the accuracy and effi-ciency of MAP inference for Markov Logic. In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, pages 468-475, Helsinki, Finland. K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsu-moto. 2009. Jointly Identifying Temporal Relations with Markov Logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Nat-ural Language Processing of the AFNLP, pages 405-413, Suntec, Singapore. 
Precision / Recall (%) System Task C Task D Task E Task F NCSU-INDI 63/63 68/68 48/48 66/66 NCSU-JOINT 62/62 21/21 51/51 25/25  Table 3: Accuracy of the systems on each task 
344
Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing, pages 56?64,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Exploring Individual Differences in Student Writing with a  
Narrative Composition Support Environment 

Julius Goth and Alok Baikadi and Eun Ha and Jonathan Rowe and Bradford Mott 
and James Lester 
Department of Computer Science 
North Carolina State University 
Raleigh, NC, USA 
{jgoth, abaikad, eha, jprowe, bwmott, lester}@ncsu.edu 
 
 
 
Abstract 
Novice writers face significant challenges as 
they learn to master the broad range of skills 
that contribute to composition. Novice and 
expert writers differ considerably, and devis-
ing effective composition support tools for 
novice writers requires a clear understanding 
of the process and products of writing. This 
paper reports on a study conducted with more 
than one hundred middle grade students inter-
acting with a narrative composition support 
environment. The texts are found to pose im-
portant challenges for state-of-the-art natural 
language processing techniques.  Furthermore, 
the study investigates the language usage of 
middle grade students, the cohesion and co-
herence of the resulting texts, and the relation-
ship between students? language arts skills 
and their writing processes.  The findings sug-
gest that composition support environments 
require robust NLP tools that can account for 
the variations in students? writing in order to 
effectively support each phase of the writing 
process. 
1 Introduction 
Writing is fundamentally complex.  Writers must 
simultaneously consider a constellation of factors 
during composition, including writing task re-
quirements, knowledge of audience, domain 
knowledge, language usage, and tone (Hayes and 
Flower, 1981).  Furthermore, effective writing 
involves sophisticated higher-order cognitive 
skills, such as synthesis of ideas, critical thinking, 
and self-regulation.  Text genres, such as narrative 
or expository texts, also introduce distinct re-
quirements and conventions (Hayes and Flower, 
1981). 
Because writing itself is complex, learning to 
write poses significant challenges for students.  
The central role of writing in communication, 
knowledge organization, and sensemaking points 
to the need to devise methods and tools with which 
writing skills can be effectively taught and learned 
(Graham, 2006).  Intelligent tutoring systems 
(VanLehn, 2006) offer a promising means for de-
livering tailored writing support to students.  How-
ever, developing intelligent tutors to scaffold 
student writing poses a number of technical and 
pedagogical hurdles.  Texts written by novice writ-
ers are likely to exhibit significant variation in 
grammar, cohesion, coherence, and content qual-
ity; these characteristics are likely to be problem-
atic for analysis by current natural language 
processing tools.  Furthermore, students? individ-
ual differences in language arts skills, writing self-
efficacy, domain knowledge, and motivation can 
have pedagogical implications.  An effective intel-
ligent writing tutor must do more than just parse 
and understand student texts; it must also provide 
tailored feedback that fosters effective writing 
processes and enhances student motivation for 
writing.  
This paper explores several key questions for 
the design of intelligent composition support tools 
for novice writers.  First, it investigates the per-
formance of current syntactic parsing tools on a 
corpus of narrative texts written by middle grade 
students during interactions in a narrative composi-
56
tion support environment.  A narrative composi-
tion support environment aims to support the prin-
cipal processes of writing, such as planning, 
revision, and text production.  The second question 
the paper explores is how middle school students? 
language art skills affect the cohesion and coher-
ence of texts produced during interactions with a 
narrative composition support environment.  Third, 
the paper investigates how middle school students? 
language art skills affect their writing processes 
during interactions in a narrative composition sup-
port environment.  Studying the interactions be-
tween the environment?s support mechanisms and 
students? individual differences provides insights 
into the affordances and limitations of novices? 
writing abilities, as well as implications for the 
design of intelligent tutors for narrative writing.  
The study presented here investigates novice 
writers? composition processes during interactions 
with a narrative composition support environment.  
In the study, 127 middle grade students interacted 
with the NARRATIVE THEATRE fable composition 
support environment.  The NARRATIVE THEATRE 
uses a multimedia interface to guide students as 
they select key elements of their fable (e.g., moral, 
setting, characters), prompts students through an 
explicit, timed story planning process, and allows 
students to review their earlier planning decisions 
at any point during writing of the main text.  Stu-
dents? literacy ratings and log data from interac-
tions with the NARRATIVE THEATRE environment 
are analyzed to investigate the differences between 
high- and low-skill students and their practice of 
key composition processes in the NARRATIVE 
THEATRE environment, including planning, text 
production, and revision.  Coh-Metrix (Graesser et 
al., 2004) was also used to analyze the cohesion 
and coherence characteristics of the students? fa-
bles.  The observations from this study offer im-
portant implications for the design of intelligent 
composition support tools for novice writers. 
2 Related Work 
Since Hayes and Flower first proposed their semi-
nal model of writing nearly thirty years ago (1981), 
a rich body of work has investigated the cognitive 
functions supporting written composition.  Founda-
tional results are now in place on the core proc-
esses of writing, including idea generation 
(Galbraith et al, 2009), text production (Berninger 
et al, 2002), and revision (McCutchen et al, 
1997).  Furthermore, a detailed account of the 
composition process has begun to emerge across a 
range of writing experience levels (Graham et al, 
2002) and text genres (Langer, 1985).  
Particularly important for the design of compo-
sition support tools for novices is the emergence of 
a consensus account of the characteristics of nov-
ice writers? narrative composition processes.  Em-
pirical studies have suggested that notable 
differences exist between novice and expert writ-
ers, such as novices? use of knowledge-telling 
practices versus experts? use of knowledge-
transformation practices during text production 
(Bereiter and Scardamalia, 1987).  However, it has 
been argued that even novice writers can employ 
high-level knowledge-transformation processes 
when situated within an appropriate task environ-
ment with effective writing scaffolds (Cameron 
and Moshenko, 1996).  Other work has found that 
students? domain and linguistic knowledge influ-
ences the coherence and quality of their expository 
writings (DeGroff, 1987).  These findings under-
score the importance of investigating methods for 
effective and engaging writing instruction targeted 
at novice writers, as well as automated tools to 
tailor feedback and scaffolding to individual stu-
dents. 
In addition to grounding their work in the writ-
ing research literature, designers of composition 
support tools will likely need to avail themselves 
of the full gamut of natural language processing 
techniques to analyze students? texts with regard to 
syntax, semantics, and discourse.  However, in 
texts produced by novice writers, grammatical 
errors and incoherent discourse abound, which 
may present serious challenges for natural lan-
guage processing since the majority of current 
NLP tools have been developed for well-formed 
texts.  While existing NLP tools have been suc-
cessfully used in writing support systems designed 
for expert writers (Mahlow and Piotrowski, 2009), 
common structural issues in novice compositions 
are likely to prove problematic for current tools.  
However, recent work has begun to explore tech-
niques for handling ill-formed texts that are similar 
to those produced by novice writers.  For example, 
Gamon et al conducted a word-level analysis of 
texts written by non-native English speakers 
(2008).  Focusing on two types of errors (deter-
miners and prepositions), they use decision-tree 
57
classifiers in combination with a language model 
trained on a large English corpus to detect and 
correct erroneous selection of words.  Wagner et 
al. investigated the detection of grammatical mal-
formedness of individual sentences (2007).  They 
found it effective to combine a shallow approach 
that uses n-grams and a deep approach that uses 
syntactic parse results.  Higgins et al explored the 
overall coherence of texts written by students 
(2004).  Using support vector machines, their sys-
tem identified the portions of text that resulted in 
coherence breakdowns with regard to relatedness 
to the essay question and relatedness between dis-
course elements. 
To date, a relatively small number of intelligent 
tutoring systems have been developed to support 
student learning in the language arts, and even 
fewer have sought to specifically address writing.  
Sourcer?s Apprentice is a web-based learning envi-
ronment to help high school students gather, evalu-
ate, and integrate information for writing essays 
about history topics (Britt et al, 2004), although 
Sourcer?s Apprentice did not seek to apply NLP 
tools to understand or scaffold students? composi-
tions directly.  Other work on intelligent tutoring 
for language arts, such as Project LISTEN (Mo-
stow and Aist, 2001) and REAP (Heilman et al, 
2007), has addressed vocabulary learning and read-
ing comprehension. 
3 Narrative Corpus Acquisition 
To investigate narrative composition in novice 
writers, a study was conducted with more than one 
hundred middle grade students using a narrative 
composition support environment.  The 
NARRATIVE THEATRE (Figure 1) is an interactive 
environment designed to capture both the process 
and products of writing.1 Targeting a user popula-
tion of sixth grade students (age typically 12 years) 
and the genre of fables, the NARRATIVE THEATRE 
enables students to create stories in an environment 
that was specifically designed to scaffold novices? 
composition activities during a timed story plan-
                                                 
1 The version of the NARRATIVE THEATRE used in the study 
reported in this paper is the forerunner of a more general 
creativity support environment.  It is under development in our 
laboratory that will employ NLP techniques and intelligent 
graphics generation. The study reported here was conducted to 
inform the design of the creativity enhancement environment 
and intelligent tutoring systems to support composition. 
Figure 1.  Narrative Theatre fable composition support environment. 
58
ning and writing process.  The NARRATIVE 
THEATRE employs a multimedia interface created 
with Adobe's Flash? development platform and 
AIR runtime environment.  Its design was inspired 
by a worksheet that is widely used as part of the 
Grade 6 writing curriculum. 
During the planning phase, students select a 
moral, a setting, a cast of characters, and a set of 
objects for the story they will create.  The system 
provides nine different morals, four settings, ten 
characters, and twenty objects from which students 
may choose.  Each setting is accompanied by a 
visual representation, which can be enlarged by 
clicking on the image to highlight salient features 
of the setting.  Characters and objects are also visu-
ally represented by static graphics, which were 
designed to be neutral in gender and expression in 
order to allow students creative choice when filling 
narrative roles with the characters.  
Once the choices have been made, students are 
presented with a screen that allows them to view 
their planning decisions and begin structuring their 
fable.  The planning area allows students to make 
notes about what they would like to have happen 
during the beginning, middle, and ending.  The top 
of the page contains windows that display the set-
ting, characters, and objects that were chosen ear-
lier, and that can provide more information via a 
mouseover.  Students craft a plan for the beginning 
(setting and characters are introduced), middle 
(conflict and problem), and end (conflict resolu-
tion) of their stories.  For each of the three major 
segments of the story, they formulate a textual 
plan.  After the planning information is entered, the 
students may begin writing (Figure 1).  They then 
create the actual prose, which is entered as raw 
text.  The writing and revision phase are supported 
with a spell-correction facility.  All student activi-
ties including interface selections and the text 
streams from planning and writing are logged and 
time-stamped. 
During the study, a total of 127 sixth-grade 
middle school students (67 males, 60 females) 
participated in the study.  The students ranged in 
age from 10 to 13.  Approximately 38% of the 
students were Caucasian, 27% African-American, 
17% Hispanic or Latino, 6% Asian, 2% American 
Indian, and the remaining 10% were of mixed or 
other descent.  Students participated as part of their 
Language Arts class.  The study spanned two days 
for each student involved.  On the first day, the 
students were seated at a computer and asked to fill 
out a pre-experiment questionnaire, which required 
approximately twenty minutes.  On the second day, 
the students were again assigned to a computer.  
They were presented with the NARRATIVE 
THEATRE interface, which asked them to enter a 
unique identification number.  Once correctly en-
tered, the students were presented with a short 
instructional video that described the features and 
operation of the interface.  They were given fifteen 
minutes to complete the planning activity, which 
included choosing a setting, main characters, 
props, and deciding the beginning, middle, and end 
of their story.  Once planning was completed, or 
time ran out, the students were given another 
thirty-five minutes to write their fable.  After their 
fable was completed, the students were asked to 
complete a post-experiment questionnaire.  This 
survey was also allotted twenty minutes for com-
pletion.  In total, the study lasted ninety minutes. 
4 Findings 
Three categories of analyses were performed on 
the NARRATIVE THEATRE corpus: an analysis of 
natural language processing tool performance 
(specifically, an analysis of syntactic parsers), an 
analysis of coherence and cohesion in the written 
texts using the automated cohesion metric tool 
Coh-Metrix (Graesser et al, 2004), and an analysis 
of students? writing processes.  
As part of an investigation of students? individ-
ual differences in writing, students? language arts 
skills were measured by their scores from the prior 
year?s End-of-Grade reading test.  Subjective rat-
ings of writing ability were also obtained for each 
student from their teachers.  The reading scores 
were used in the presented analyses because they 
were obtained through systematic testing, but it is 
interesting to note that the objective reading scores 
and subjective writing scores were found to be 
strongly correlated by calculating the Spearman?s 
correlation coefficient2, rho = .798, p < .0001.  The 
high correlation suggests that reading scores can 
serve as a reasonable indicator of language arts 
skills.   
                                                 
2 Spearman?s correlation coefficient was used because of the 
ordinal nature of the reading and writing measures (Myers and 
Well, 2003).  
59
4.1 Natural Language Processing 
Two syntactic parsing tools were used to analyze 
students? fables and develop an initial account of 
the performance of current natural language proc-
essing tools on a corpus of novice-generated narra-
tive texts. The Link Grammar Parser (Temperley, 
1995) and Stanford Parser (Klein and Manning, 
2003) were run on the entire corpus, and their per-
formance recorded. 
Link parsing provides insight into the number of 
grammatically malformed sentences observed in 
each fable.  Link grammars center on the notion of 
linkable entities directly combined with one an-
other, as opposed to tree-like structures.  Link pars-
ers attempt to identify one or more syntactically 
valid representations, where each entity is paired 
with another.  Passages were split into sentences 
using OpenNLP, and then run against the Link 
Grammar Parser (Temperley, 1995).  If a sentence 
had no suitable link based on the parser (e.g., ?Last 
dog I saw a great movie?), it was considered ?bro-
ken? because it lacked an appropriate linkage.  A 
ratio of sentences without appropriate linkage to 
total sentence count was used to characterize the 
link parser?s performance on each student?s fable. 
On average, the Link Grammar Parser found 
linkages for 41% of sentences (SD=.22).  Interest-
ingly, reading level was shown to have a marginal 
effect on the link parser?s success rate,  
F(2,110) = 5.78, p = .06.  Post hoc Tukey?s tests 
revealed that above-grade level readers were mar-
ginally more likely to write linkable sentences than 
at-grade level readers, p = .07.  The effect was 
strongly significant between above-grade level 
readers and below-grade level readers, p = .003. 
The Stanford parser was used to investigate the 
frequency with which sentences could be success-
fully parsed.  A parsing failure was noted any time 
the tool was forced to fall back to a PCFG parse.  
On average, the Stanford parser produced a parse 
for 91% of students? sentences.  A significant ef-
fect of reading grade-level on Stanford parser suc-
cess rate was observed, F(2,110) = 4.41, p = .015.  
Post hoc tests showed that above-grade level read-
ers wrote significantly more sentences that could 
be parsed than below-grade level readers, p = .02.  
There was also a marginal difference observed 
between below-grade level readers and at-grade 
level readers, p = .08. 
Gender was not found to have an effect on the 
percentage of linkable sentences, nor the number 
of Stanford parser failures. 
4.2 Individual Differences and Written Texts 
Several analyses were conducted to investigate 
individual differences in students? written texts.  
Analyses focused on writing length, cohesion 
characteristics, coherence characteristics, and 
spelling errors.  Fable lengths were measured in 
characters (M = 1346, SD = 601). 
A marginal effect of reading grade-level on fa-
ble length was observed, F(2,110) = 2.89, p = .06.  
Post hoc tests showed that at-grade level readers 
tended to write longer fables than below-grade 
level readers, p = .10.  Gender was also found to 
have a significant effect on writing length.  Spe-
cifically, females tended to write longer fables than 
males, F(1,110) = 4.41, p = .04. 
Table 1. The effects of reading grade-level on select Coh-Metrix features. 
* denotes p < .1 and ** denotes p < .05 
Coh-Metrix feature Below-Grade  At-Grade  Above-Grade  F(2, 110) = 
Hypernym, nouns 5.9 (0.93)  6.17 (0.81)  6.53 (0.43)  1.24 Below-Above** 
Hypernym, verbs 1.44 (0.18)  1.49 (0.18)  1.48 (0.17)  3.72  
Causal cohesion 0.83 (0.09)  0.87 (0.1)  0.39 (0.16)  3.70 Below-Above** 
At-Above** 
LSA, paragraph to paragraph 0.34 (0.19)  0.45 (0.22)  0.49 (0.18)  3.89 Below-Above** 
Below-At* 
LSA, sentence to sentence 0.21 (0.14)  0.24 (0.11)  0.22 (0.09)  0.48  
Personal pronoun usage 107 (35.88)  101 (29.98)  89 (19.37)  2.25 Below-Above* 
Pronoun to noun phrase ratio 0.36 (0.12)  0.35 (0.10)  0.30 (0.06)  2.21  
    
60
To investigate cohesion and coherence in stu-
dents? fables, the corpus was analyzed with Coh-
Metrix, a tool for analyzing the cohesion, 
language, and readability of texts (Graesser et al, 
2004).  At the core of Coh-Metrix is a lexical ana-
lyzer, syntactic parser, part-of-speech tagger, and 
reference corpora (for LSA) that processes text and 
returns linguistic and discourse related features.  
Coh-Metrix measures several types of cohesion, as 
well as concreteness, connectives, diversity in lan-
guage, and syntactic complexity.  Concreteness is 
measured using the hypernym depth values re-
trieved from the WordNet lexical taxonomy, and 
averaged across noun and verb categories. 
Results from an analysis of reading grade-levels 
and Coh-Metrix features are presented in Table 1.  
Interestingly, above-grade level students were ob-
served to have lower causal cohesion scores than 
at-grade level or below-grade level students.  The 
converse is found in an examination of paragraph-
to-paragraph LSA scores, which are often used to 
measure semantic cohesion.  Below-grade level 
readers tended to have lower semantic cohesion 
scores than at-grade level readers.  LSA scores on 
adjacent sentences and all combinations of sen-
tences were not significant across any of the 
groups.  Sentence-to-sentence LSA scores were 
also not significant across groups.  
Gender did not have a significant effect on 
causal cohesion, hypernym depth of verbs, or 
paragraph-to-paragraph LSA values.  However, 
gender was found to have a significant effect on 
hypernym depth of nouns, F(1,110) = 15.96,  
p = .0001.  Males tended to use more concrete 
nouns in their writing passages, with an average 
difference of .6 in hypernym depth. The ratio of 
pronouns to noun phrases was also significant be-
tween genders, F(1,110) = 10.19, p = .002. Fe-
males had a 38% pronoun to NP ratio whereas 
males were at 32%.  Gender had a significant ef-
fect on sentence-to-sentence LSA scores, F(1,110) 
= 19.9, p = .0001.  Males tended to have a higher 
LSA score across adjacent sentences (M = .27, SD 
= .11) than females (M = .18, SD = .1).  Finally, 
gender had a significant effect on personal pronoun 
incidence score, F(1,110) = 9.12, p = .003.  Fe-
males used personal pronouns as 11.1% of their 
content whereas males used them as 9.3% of their 
content. 
An examination of the number of spelling errors 
remaining in student fables, as well as students? 
usage of the built-in spelling corrector, was con-
ducted.  However, no significant effects were ob-
served across reading level or gender.  
4.3 Individual Differences and Writing  
Processes 
Several features in the student interaction logs 
were chosen to investigate key aspects of students? 
writing processes.  Specifically, these features 
include planning length, planning and writing time, 
revision behavior, pauses in text production, and 
reviews of prior planning decisions.  
On average, students spent 665 seconds plan-
ning their fables and 2199 seconds writing their 
fables (SD = 535).  Students also typed 537 charac-
ters on average while planning their fables (SD = 
254).  No significant effect of reading level was 
observed on planning length, but reading level did 
have a significant effect on time spent in the plan-
ning phase, F(2,110) = 12.76, p < .0001.  Below-
grade level readers spent significantly more time 
Table 2. The effects of reading grade-level on writing process characteristics. 
* denotes p < .1, ** denotes p < .05, and *** denotes p < .01. 
 
Writing process feature Below-Grade  At-Grade  Above-Grade  F(2, 110) = 
Avg length of deletion, planning 21.30 (8.31)  28.18 (11.14)  30.99 (12.37)  8.34 Below-Above*** 
Below-At*** 
Avg length of deletion, writing 23.42 (9.26)  27.74 (10.28)  33.06 (16.80)  5.18 Below-Above*** 
Mouseovers/min 0.19 (0.15)  0.09 (0.07)  0.07 (0.05)  11.81 Below-Above*** 
Below-At*** 
5+ second revision count, planning 9.14 (6.62)  5.43 (4.43)  2.37 (2.36)  12.70 Below-Above*** 
Below-At*** 
At-Above* 
5+ second revision count, writing 18.04 (7.84)  14.21 (7.81)  13.37 (7.52)  3.83 Below-Above* 
Below-At* 
 
61
on planning than at-grade level readers, p = .001, 
as well as above-grade level readers, p < .0001.  
There were also significant differences in writing 
time across reading level groups, F(2, 110) = 6.47, 
p = .002.  Below-grade level readers took signifi-
cantly more time composing their fables than at-
grade level readers, p = .05.  Also, below-grade 
level readers took significantly more time to write 
their fables than above-grade level readers, 
p = .003. 
Females tended to write longer passages in the 
planning section than males F(1,110) = 4.68,  
p = .03.  Time spent on the planning section was 
lower among females than males, F(1,110) = 3.92, 
p = .05.  Females also spent less time on the writ-
ing section than males, F(1,110) = 3.87, p = .05. 
Students? revision behaviors were gauged using 
a heuristic that measures edit distances between 
successive snapshots of fables collected at one-
minute intervals during composition.  Each minute, 
a static snapshot of student?s fable progress was 
taken and logged.  Edit distances between succes-
sive snapshots of students? fables were measured 
using the Google Diff, Match and Patch tools to 
make ?before? and ?after? comparisons (Google, 
2009).  Comparing two successive snapshots of a 
single fable, a revision was defined as any inser-
tion of text that occurred before the tail end of the 
fable.  
The effects of reading level on revision in both 
the planning and writing stages is presented in 
Table 2.  During the writing stage, a significant 
effect of grade-level was observed on average revi-
sion length between below-grade level readers and 
at-grade level readers, as well as between below-
grade level readers and above-grade level readers.  
Within the planning section, at-grade level readers 
revised more text than below-grade level readers, 
and above-grade level readers revised more text 
than at-grade level readers.  Self-efficacy for writ-
ing was found to be significantly correlated with 
average revision length in both the planning, r = 
.21, p = .03, and writing, r = .31, p = .001, stages. 
Pauses between successive keystrokes were 
investigated during both the planning and writing 
stages of NARRATIVE THEATRE interactions.  For 
the purpose of this work, a pause is defined as a 
keystroke made five or more seconds after the 
preceding keystroke.  Keystroke pauses were cate-
gorized as either an appendage or a revision, de-
pending on whether they occurred before the tail 
end of the passage (revision) or after the tail end 
(appendage).  For the planning section, below-
grade readers paused significantly more often than 
at-grade readers.  Also, at-grade readers paused 
before revising significantly more often than 
above-grade readers.  The effects of reading level 
on a number of writing process subscores are 
shown in Table 2. 
Gender had a significant effect on pauses prior 
to revision in the writing phase, F(1,110) = 3.26, 
p = .07.  Females paused on more occasions than 
males.  However, no gender effect was found for 
pause behavior during the planning phase. 
During the planning and writing stages of 
NARRATIVE THEATRE interactions, students could 
review their prior planning selections?including 
characters, objects, and settings?by hovering the 
mouse over the respective region near the top of 
the screen (mouseover).  Upon hovering the mouse 
over the appropriate region, a graphical illustration 
of the student?s planning selection was presented.  
Mouseover instances were recorded to obtain in-
sight into idea generation, or instances where the 
student was contemplating what to write next.  
Mouseovers were calculated in terms of average 
mouseovers over time (in minutes).  The effects of 
reading ability on mouseover behaviors are shown 
in Table 2.  
For the mouseover metric, reading level had a 
significant effect on the mouseover rate.  Below-
grade level learners tended to use the mouseover 
feature on a more frequent basis than both at-grade 
level and above-grade level readers.  There was not 
a significant difference between at-grade level and 
above-grade level groups.  
The effect of gender on mouseover rate was sig-
nificant, F(1,110) = 9.93, p = .002.  Males used the 
mouseover feature on fewer occasions than fe-
males. 
5 Discussion 
The performance of the two parsers differed 
widely.  The Stanford Parser was able to parse over 
90% of fables, but the Link Grammar Parser was 
only successful for about 40% of the fables.  While 
parser failure is not always indicative of poor 
grammaticality, every sentence that failed on the 
Stanford Parser contained either misspelled words 
or run-on sentences.  Many of these were indicated 
by errors in the sentence segmentation as well.  
62
There were also indications that students? language 
arts skills may influence the grammaticality of 
their written sentences; significant effects of read-
ing level were found on both the Stanford Parser?s 
success rate and the Link parser?s success rate.  
The fact that below-grade level students consti-
tuted a considerable proportion of the study?s stu-
dent population suggests that pedagogical writing 
support tools should be capable of handling the 
variations inherent in students? writings, and lever-
age natural language processing results to inform 
tutorial feedback. 
Paragraph-to-paragraph LSA scores tended to 
increase with reading level.  This has implications 
for semantic cohesion (Graessar, 2004) and indi-
cates that students with a higher reading assess-
ment score produce stories that satisfy this 
particular dimension of cohesion.  However, the 
converse was true for the Coh-Metrix measure of 
causal cohesion, where above-grade level students 
actually produced the lowest cohesion scores.  One 
possible explanation could stem from differences 
in vocabulary skills between above- and below-
grade level students; students who exercise a larger 
vocabulary may be penalized by Coh-Metrix?s 
cohesion metric.  Alternatively, the result may be 
related to the fact that below-grade level students 
tend to produce less text (Graessar, 2004).  Clearly, 
students? individual differences in language arts 
ability affect the cohesiveness of the texts they 
write, but additional investigation is necessary to 
develop a clear understanding of the relationship 
between cohesion and language arts ability, as well 
as the implications for tailoring tutoring. 
With regard to the writing process, the average 
length per revision was significantly greater for 
students of higher reading skill-levels. There is a 
possibility that this may be associated with more 
elaborate revision processes, which requires further 
investigation. It should be noted that the revision 
finding was more salient for the planning stage of 
NARRATIVE THEATRE interactions.  This result 
may also indicate that below-grade level readers 
were somewhat less thorough when planning their 
fables.  Further, differences in mouseover behavior 
were found across reading levels, apparently indi-
cating a decline in the rate of mouseovers as read-
ing level increased. This finding may be the result 
of below-grade level students experiencing diffi-
culties in idea generation, or a lack of motivation. 
Finally, the number of pauses prior to revision was 
found to decrease as reading level increased.  This 
result may point to difficulties with text production 
for lower language arts skill students.  Difficulty 
translating ideas into text may point to a need for 
intelligent writing tutors to help reduce lower read-
ing level students? cognitive load during writing. 
6 Conclusions and Future Work 
We have presented a study conducted with middle 
grade students to investigate the process and prod-
ucts of writing in a narrative composition support 
environment.  The study found significant varia-
tions in syntactic parser performance associated 
with students? language arts abilities, as well as 
relationships between students? reading level and 
the grammaticality of their writing.  For example, 
the stories of below-grade readers had a lower 
level of semantic cohesion than at-grade level 
readers, but surprisingly, above-grade level stu-
dents? writings exhibited lower causal cohesion 
than both at-grade and below-grade level students.  
Reading level had a significant effect on time spent 
in the planning phase, and below-grade level read-
ers spent more time composing fables than at-
grade level readers.  There were also gender differ-
ences, with females spending less time in both the 
planning and writing phases.  There were also dif-
ferences with respect to revision, with above-grade 
readers revising more than below-grade readers. 
The study highlights important issues about how 
to design composition support tools.  Composition 
support tools that are sensitive to students? indi-
vidual writing abilities seem likely to be most ef-
fective.  Natural language processing is critical for 
analyzing students? texts and informing the content 
of adaptive tutorial feedback.  Intelligent writing 
tutors should utilize natural language processing 
techniques that can robustly handle the variations 
in students? writings, and deliver tailored scaffold-
ing informed by analyses of students? texts and 
writing processes.  
The findings suggest that several directions exist 
for future work.  Additional analysis is necessary 
to investigate the correctness of syntactic parses.  
Further investigation of students? individual differ-
ences in writing at the discourse and narrative lev-
els is also necessary.  Results from these analyses 
should then be used to inform the design of tech-
niques for adaptive tutorial feedback in narrative 
composition support environments. 
63
Acknowledgements 
The authors wish to thank members of the North 
Carolina State University IntelliMedia Group for 
their assistance with the NARRATIVE THEATRE.  
This research was supported by the National Sci-
ence Foundation under Grant IIS-0757535.  Any 
opinions, findings, and conclusions or recommen-
dations expressed in this material are those of the 
authors and do not necessarily reflect the views of 
the National Science Foundation. 
References  
C. Bereiter and M. Scardamalia. 1987. The Psychology 
of Written Composition. Lawrence Erlbaum Associ-
ates, Hillsdale, NJ.  
V. W. Berninger, K. Vaughan, R. D. Abbott, K. Begay, 
K. B. Coleman, G. Curtain, J. M. Hawkins, and S. 
Graham. 2002. Teaching spelling and composition a-
lone and together: Implications for the simple view 
of writing. Journal of Educational Psychology, 
94(2):291?304.  
M. A. Britt, P. Wiemer-Hasting, A. Larson, and C. Per-
fetti. 2004. Automated feedback on source citation in 
essay writing. International Journal of Artificial In-
telligence in Education, 14(3?4):359?374. 
C. A. Cameron and B. Moshenko. 1996. Elicitation of 
knowledge transformational reports while children 
write narratives. Canadian Journal of Behavioural 
Science, 28(4):271?280. 
L. J. C. DeGroff. 1987. The influence of prior 
knowledge on writing, conferencing, and revising. 
Elementary School Journal, 88(2):105?118.  
L. Flower and J. Hayes. 1981. A cognitive process 
theory of writing. College Composition and Commu-
nication, 32(4):365?387.  
D. Galbraith, J. Hallam, T. Olive, and N. Le Bigot. 
2009. The role of different components of working 
memory in writing. In Proceedings of Annual Con-
ference of the Cognitive Science Society, Amsterdam, 
The Netherlands.  
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. B. 
Dolan, D. Belenko, and L. Vanderwende. 2008. Us-
ing contextual speller techniques and language 
modeling for ESL error correction. In Proceedings of 
the International Joint Conference on Natural Lan-
guage Processing, pages 449?456, Hyderabad, India.  
Google Diff Match and Patch [Software]. Available 
from http://code.google.com/p/google-diff-match-
patch/ 
A. C. Graesser, D. S. McNamara, M. M. Louwerse, and 
Z. Cai. 2004. Coh-Metrix: Analysis of text on cohe-
sion and language. Behavior Research Methods In-
struments and Computers, 36(2):193?202.  
S. Graham. 2006. Strategy instruction and the teaching 
of writing: A meta-analysis. In C. A. MacArthur, S. 
Graham, and J. Fitzgerald, editors, Handbook of writ-
ing research. Guilford Press, New York, NY, pages 
187?207.   
S. Graham, K. R. Harris, and B. F. Chorzempa. 2002. 
Contribution of spelling instruction to the spelling, 
writing, and reading of poor spellers. Journal of Edu-
cational Psychology, 94(4):669?686.  
J. Hayes. 1996. A new framework for understanding 
cognition and affect in writing. In C. M. Levy and S. 
Ransdell, editors, The Science of Writing: Theories, 
Methods, Individual Differences, and Applications. 
Lawrence Erbaum Associates, Mahwah, NJ, pages 1?
28.  
M. Heilman, K. Collins-Thompson, J. Callan, and M. 
Eskenazi. 2007. Combining lexical and grammtical 
features to improve readability measures for first and 
second language texts. In Proceedings of Human 
Language Technology Conference, pages 460?467, 
Rochester, NY.  
D. Higgins, J. Bustein, D. Marcu, and C. Gentile. 2004. 
Evaluating multiple aspects of coherence in student 
essays. In Proceedings of Human Language Tech-
nology conference/North American chapter of the 
Association for Computational Linguistics, pages 
185?192, Boston, MA.  
J. A. Langer. 1985. Children's sense of genre: A study 
of performance on parallel reading and writing 
Tasks. Written Communication, 2(2):157?187.  
C. Mahlow and M. Piotrowski. 2009. LingURed: Lan-
guage-aware editing functions based on NLP re-
sources. In Proceedings of International 
Multiconference on Computer Science and Informa-
tion Technology, pages 243?250, Mragowo, Poland.  
D. McCutchen, M. Francis, and S. Kerr. 1997. Revising 
for meaning: Effects of knowledge and strategy. 
Journal of Educational Psychology, 89(4):667?676.  
J. Mostow and G. Aist. 2001. Evaluating tutors that 
listen: an overview of project LISTEN. In K. Forbus 
and P. Feltovich, editors, Smart Machines in Educa-
tion. MIT Press, Cambridge, MA, USA, pages 169?
234.  
J. Myers and A. Well. 2003. Research Design and Sta-
tistical Analysis. Erlbaum, Mahwah, NJ. 
K. VanLehn. 2006. The behavior of tutoring systems. 
International Journal of Artificial Intelligence in 
Education, 16(3):227?265.  
J. Wagner, J. Foster, and J. Van Genabith. 2007. A 
comparative evaluation of deep and shallow 
approaches to the automatic detection of common 
grammatical errors. In Proceedings of 2007 Joint 
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 112?121, Prague, Czech 
Republic. 
64
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 66?73,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Leveraging Hidden Dialogue State to Select Tutorial Moves  Kristy Elizabeth    Boyera Robert    Phillipsab Eun Young Haa Michael D.    Wallisab Mladen A.   Vouka James C. Lestera   aDepartment of Computer Science, North Carolina State University bApplied Research Associates Raleigh, NC, USA  {keboyer, rphilli, eha, mdwallis, vouk, lester}@ncsu.edu    Abstract 
A central challenge for tutorial dialogue systems is selecting an appropriate move given the dialogue context. Corpus-based approaches to creating tutorial dialogue management models may facilitate more flexible and rapid development of tutorial dialogue systems and may increase the effectiveness of these systems by allowing data-driven adaptation to learning contexts and to individual learners. This paper presents a family of models, including first-order Markov, hidden Markov, and hierarchical hidden Markov models, for predicting tutor dialogue acts within a corpus. This work takes a step toward fully data-driven tutorial dialogue management models, and the results highlight important directions for future work in unsupervised dialogue modeling. 1 Introduction A central challenge for dialogue systems is selecting appropriate system dialogue moves (Bangalore, Di Fabbrizio, & Stent, 2008; Frampton & Lemon, 2009; Young et al, 2009). For tutorial dialogue systems, which aim to support learners during conceptual or applied learning tasks, selecting an appropriate dialogue move is particularly important because the tutorial approach could significantly influence cognitive and affective outcomes for the learner (Chi, Jordan, VanLehn, & Litman, 2009). The strategies implemented in tutorial dialogue systems have historically been based on handcrafted rules 
derived from observing human tutors (e.g., Aleven, McLaren, Roll, & Koedinger, 2004; Evens & Michael, 2006; Graesser, Chipman, Haynes, & Olney, 2005; Jordan, Makatchev, Pappuswamy, VanLehn, & Albacete, 2006). While these systems can achieve results on par with unskilled human tutors, tutorial dialogue systems have not yet matched the effectiveness of expert human tutors (VanLehn et al, 2007). A more flexible model of strategy selection may enable tutorial dialogue systems to increase their effectiveness by responding adaptively to a broader range of contexts. A promising method for deriving such a model is to learn it directly from corpora of effective human tutoring. Data-driven approaches have shown promise in task-oriented domains outside of tutoring (Bangalore et al, 2008; Hardy et al, 2006; Young et al, 2009), and automatic dialogue policy creation for tutoring has been explored recently (Chi, Jordan, VanLehn, & Hall, 2008; Tetreault & Litman, 2008). Ultimately, devising data-driven approaches for developing tutorial dialogue systems may constitute a key step towards achieving the high learning gains that have been observed with expert human tutors.  The work presented in this paper focuses on learning a model of tutorial moves within a corpus of human-human dialogue in the task-oriented domain of introductory computer science. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact by the user (the student). The modification of this artifact, in our case a computer program, is the focus of the dialogues. Our corpus consists of textual dialogue utterances and a separate synchronous stream of 
66
task actions. Our goal is to extract a data-driven dialogue management model from the corpus, as evidenced by predicting system (tutor) dialogue acts.  In this paper, we present an annotation approach that addresses dialogue utterances and task actions, and we propose a unified sequential representation for these separate synchronous streams of events. We explore the predictive power of three stochastic models ? first-order Markov models, hidden Markov models, and hierarchical hidden Markov models ? for predicting tutor dialogue acts in the unified sequences. By leveraging these models to capture effective tutorial dialogue strategies, this work takes a step toward creating data-driven tutorial dialogue management models. 2 Related Work Much of the research on selecting system dialogue acts relies on a Markov assumption (Levin, Pieraccini, & Eckert, 2000). This formulation is often used in conjunction with reinforcement learning (RL) to derive optimal dialogue policies (Frampton & Lemon, 2009). Sparse data and large state spaces can pose serious obstacles to RL, and recent work aims to address these issues (Ai, Tetreault, & Litman, 2007; Henderson, Lemon, & Georgila, 2008; Heeman, 2007; Young et al, 2009). For tutorial dialogue, RL has been applied to selecting a state space representation that best facilitates learning an optimal dialogue policy (Tetreault & Litman, 2008). RL has also been used to compare specific tutorial dialogue tactic choices (Chi et al, 2008).  While RL learns a dialogue policy through exploration, our work assumes that a flexible, good (though possibly not optimal) dialogue policy is realized in successful human-human dialogues. We extract this dialogue policy by predicting tutor (system) actions within a corpus. Using human dialogues directly in this way has been the focus of work in other task-oriented domains such as finance (Hardy et al, 2006) and catalogue ordering (Bangalore et al, 2008). Like the parse-based models of Bangalore et al, our hierarchical hidden Markov models (HHMM) explicitly capture the hierarchical nesting of tasks and subtasks in our domain. In other work, this level of structure has been studied from a slightly different perspective as conversational game (Poesio & Mikheev, 1998).  
For tutorial dialogue, there is compelling evidence that human tutoring is a valuable model for extracting dialogue system behaviors. The CIRCSIM-TUTOR (Evens & Michael, 2006), ITSPOKE (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007; Forbes-Riley & Litman, 2009), and KSC-PAL (Kersey, Di Eugenio, Jordan, & Katz, 2009) projects have made extensive use of data-driven techniques based on human corpora. Perhaps most directly comparable to the current work are the bigram models of Forbes-Riley et al; we explore first-order Markov models, which are equivalent to bigram models, for predicting tutor dialogue acts.  In addition, we present HMMs and HHMMs trained on our corpus. We found that both of these models outperformed the bigram model for predicting tutor moves. 3 Corpus and Annotation The corpus was collected during a human-human tutoring study in which tutors and students worked to solve an introductory computer programming problem (Boyer et al, in press). The dialogues were effective: on average, students exhibited a 7% absolute gain from pretest to posttest (N=48, paired t-test p<0.0001).  The corpus contains 48 textual dialogues with a separate, synchronous task event stream. Tutors and students collaborated to solve an introductory computer programming problem using an online tutorial environment with shared workspace viewing and textual dialogue. Each student participated in exactly one tutoring session. The corpus contains 1,468 student utterances, 3,338 tutor utterances, and 3,793 student task actions. In order to build the dialogue model, we annotated the corpus with dialogue act tags and task annotation labels. 3.1 Dialogue Act Annotation  We have developed a dialogue act tagset inspired by schemes for conversational speech (Stolcke et al, 2000), task-oriented dialogue (Core & Allen, 1997), and tutoring (Litman & Forbes-Riley, 2006). The dialogue act tags are displayed in Table 1. Overall reliability on 10% of the corpus for two annotators was ?=0.80.   
67
 Table 1. Dialogue act tags 
DA? Description?
Stu.?Rel.?Freq.?
Tut.?Rel.?Freq.? ??ASSESSING?QUESTION?(AQ)? Request?for?feedback?on?task?or?conceptual?utterance.? .20? .11? .91?EXTRA?DOMAIN?(EX)? Asides?not?relevant?to?the?tutoring?task.? .08? .04? .79?GROUNDING?(G)? Acknowledgement/thanks? .26? .06? .92?LUKEWARM?CONTENT?FEEDBACK?(LCF)? Negative?assessment?with?explanation.? .01? .03? .53?LUKEWARM?FEEDBACK?(LF)? Lukewarm?assessment?of?task?action?or?conceptual?utterance.? .02? .03? .49?NEGATIVE?CONTENT?FEEDBACK?(NCF)?
Negative?assessment?with?explanation.? .01? .10? .61?
NEGATIVE?FEEDBACK?(NF)? Negative?assessment?of?task?action?or?conceptual?utterance.? .05? .02? .76?POSITIVE?CONTENT?FEEDBACK?(PCF)? Positive?assessment?with?explanation.? .02? .03? .43?POSITIVE?FEEDBACK?(PF)? Positive?assessment?of?task?action?or?conceptual?utterance.? .09? .16? .81?QUESTION?(Q)? Task?or?conceptual?question.? .09? .03? .85?STATEMENT?(S)? Task?or?conceptual?assertion.? .16? .41? .82?
3.2 Task Annotation The dialogues focused on the task of solving an introductory computer programming problem. The task actions were recorded as a separate but synchronous event stream. This stream included 97,509 keystroke-level user task events. These events were manually aggregated and annotated for subtask structure and then for correctness. The task annotation scheme was hierarchical, reflecting the nested nature of the subtasks. An excerpt from the task annotation scheme is depicted in Figure 1; the full scheme contains 66 leaves. The task annotation scheme was designed to reflect the different depth of possible subtasks nested within the overall task. Each labeled task action was also judged for correctness according to the requirements of the task, with categories CORRECT, BUGGY, INCOMPLETE, and DISPREFERRED (technically 
correct but not accomplishing the pedagogical goals of the task). Each group of task keystrokes that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced (giving judges maximum flexibility to apply the tags). To ensure a conservative reliability statistic, all unmatched subtask tags were treated as disagreements. The resulting unweighted kappa statistic was ?simple= 0.58, but the weighted Kappa ?weighted=0.86 is more meaningful because it takes into account the ordinal nature of the labels that result from sequential subtasks. On task actions for which the two judges agreed on subtask tag, the agreement statistic for correctness was ?simple=0.80. 
 Figure 1. Portion of task annotation scheme 3.3 Adjacency Pair Joining Some dialogue acts establish an expectation for another dialogue act to occur next (Schegloff & Sacks, 1973). Our previous work has found that identifying the statistically significant adjacency pairs in a corpus and joining them as atomic observations prior to model building produces more interpretable descriptive models. The models reported here were trained on hybrid sequences of dialogue acts and adjacency pairs. A full description of the adjacency pair identification methodology and joining algorithm is reported in (Boyer et al, 2009). A partial list of the most highly statistically significant adjacency pairs, 
68
which for this work include task actions, is displayed in Table 2.   Table 2. Subset of significant adjacency pairs CORRECTTASKACTION?CORRECTTASKACTION;??EXTRADOMAINS?EXTRADOMAINT;?GROUNDINGS?GROUNDINGT;?ASSESSINGQUESTIONT?POSITIVEFEEDBACKS;??ASSESSINGQUESTIONS?POSITIVEFEEDBACKT;?QUESTIONT?STATEMENTS;?ASSESSINGQUESTIONT?STATEMENTS;?EXTRADOMAINT?EXTRADOMAINS;?QUESTIONS?STATEMENTT;?NEGATIVEFEEDBACKS?GROUNDINGT;?INCOMPLETETASKACTION?INCOMPLETETASKACTION;?POSITIVEFEEDBACKS?GROUNDINGT;??BUGGYTASKACTION?BUGGYTASKACTION 4 Models We learned three types of models using cross-validation with systematic sampling of training and testing sets. 
4.1 First-Order Markov Model The simplest model we discuss is the first-order Markov model (MM), or bigram model (Figure 2). A MM that generates observation (state) sequence o1o2?ot is defined in the following way. The observation symbols are drawn from the alphabet ?={?1, ?2, ?, ?M}, and the initial probability distribution is ?=[?i] where ?i is the probability of a sequence beginning with observation symbol ?i. The transition probability distribution is A=[aij], where aij is the probability of observation j occurring immediately after observation i. 
 Figure 2. Time-slice topology of MM  We trained MMs on our corpus of dialogue acts and task events using ten-fold cross-validation to produce a model that could be queried for the next predicted tutorial dialogue act given the history.  
4.2 Hidden Markov Model A hidden Markov model (HMM) augments the MM framework, resulting in a doubly stochastic structure (Rabiner, 1989). For a first-order HMM, the observation symbol alphabet is defined as above, along with a set of hidden states S={s1,s2,?,sN}. The transition and initial probability distributions are defined analogously to MMs, except that they operate on hidden states 
rather than on observation symbols (Figure 3). That is, ?=[?i] where ?i is the probability of a sequence beginning in hidden state si. The transition matrix is A=[aij], where aij is the probability of the model transitioning from hidden state i to hidden state j. This framework constitutes the first stochastic layer of the model, which can be thought of as modeling hidden, or unobservable, structure. The second stochastic layer of the model governs the production of observation symbols: the emission probability distribution is B=[bik] where bik is the probability of state i emitting observation symbol k. 
 Figure 3. Time-slice topology of HMM  The notion that dialogue has an overarching unobservable structure that influences the observations is widely accepted. In tutoring, this overarching structure may correspond to tutorial strategies. We have explored HMMs? descriptive power for extracting these strategies (Boyer et al, 2009), and this paper explores the hypothesis that HMMs provide better predictive power than MMs on our dialogue sequences. We trained HMMs on the corpus using the standard Baum-Welch expectation maximization algorithm and applied state labels that reflect post-hoc interpretation (Figure 4).  
 Figure 4. Portion of learned HMM 
69
4.3 Hierarchical Hidden Markov Model Hierarchical hidden Markov models (HHMMs) allow for explicit representation of multilevel stochastic structure. A complete formal definition of HHMMs can be found in (Fine, Singer, & Tishby, 1998), but here we present an informal description.  HHMMs include two types of hidden states: internal nodes, which do not produce observation symbols, and production nodes, which do produce observations. An internal node includes a set of substates that correspond to its potential children, S={s1, s2, ?, sN}, each of which is itself the root of an HHMM. The initial probability distribution ?=[?i] for each internal node governs the probability that the model will make a vertical transition to substate si from this internal node; that is, that this internal node will produce substate si as its leftmost child. Horizontal transitions are governed by a transition probability distribution similar to that described above for flat HMMs. Production nodes are defined by their observation symbol alphabet and an emission probability distribution over the symbols; HHMMs do not require a global observation symbol alphabet. The generative topology of our HHMMs is illustrated in Figure 5. 
 Figure 5. Generative topology of HHMM  HHMMs of arbitrary topology can be trained using a generalized version of the Baum-Welch algorithm (Fine et al, 1998). Our HHMMs featured a pre-specified model topology based on known task/subtask structure. A Bayesian view of a portion of the best-fit HHMM is depicted in Figure 6.  This model was trained using five-fold cross-validation to address the absence of symbols from the training set that were present in the testing set, a sparsity problem that arose from splitting the data hierarchically. 
Figure 6. Portion of learned HHMM 
70
5 Results We trained and tested MMs, HMMs, and HHMMs on the corpus and compared prediction accuracy for tutorial dialogue acts by providing the model with partial sequences from the test set and querying for the next tutorial move. The baseline prediction accuracy for this task is 41.1%, corresponding to the most frequent tutorial dialogue act (STATEMENT). As depicted in Figure 7, a first-order MM performed worse than baseline (p<0.001)1 at 27% average prediction accuracy (
? 
? ? MM=6%). HMMs performed better than baseline (p<0.0001), with an average accuracy of 48% (
? 
? ? HMM=3%). HHMMs averaged 57% accuracy, significantly higher than baseline (p=0.002) but weakly significantly higher than HMMs (p=0.04), and with high variation (
? 
? ? HHMM=23%). 
 Figure 7. Average prediction accuracies of three model types on tutor dialogue acts  To further explore the performance of the HHMMs, Figure 8 displays their prediction accuracy on each of six labeled subtasks. These subtasks correspond to the top level of the hierarchical task/subtask annotation scheme. The UNDERSTAND THE PROBLEM subtask corresponds to the initial phase of most tutoring sessions, in which the student and tutor agree to some extent on a problem-solving plan. Subtasks 1, 2, and 3 account for the implementation and debugging of three distinct modules within the learning task, and Subtask 4 involves testing and assessing the student?s finalized program. The EXTRA-DOMAIN subtask involves side conversations whose topics are outside of the domain.  The HHMM performed as well as or better (p<0.01) than baseline on the first three in-domain subtasks. The performance on SUBTASK 4 was not distinguishable from baseline (p=0.06); relatively few students reached this subtask. The model did                                                 1 All p-values in this section were produced by two-sample one-tailed t-tests with unequal sample variances. 
not outperform baseline (p=0.40) for the UNDERSTAND THE PROBLEM subtask, and qualitative inspection of the corpus reveals that the dialogue during this phase of tutoring exhibits limited regularities between students.  
 Figure 8. Average prediction accuracies of HHMMs by subtask 6 Discussion The results support our hypothesis that HMMs, because of their capacity for explicitly representing dialogue structure at an abstract level, perform better than MMs for predicting tutor moves. The results also suggest that explicitly modeling hierarchical task structure can further improve prediction accuracy of the model. The below-baseline performance of the bigram model illustrates that in our complex task-oriented domain, an immediately preceding event is not highly predictive of the next move. While this finding may not hold for conversational dialogue or some task-oriented dialogue with a more balanced distribution of utterances between speakers, the unbalanced nature of our tutoring sessions may not be as easily captured.  In our corpus, tutor utterances outnumber student utterances by more than two to one. This large difference is due to the fact that tutors frequently guided students and provided multi-turn explanations, the impetus for which are not captured in the corpus, but rather, involve external pedagogical goals. The MM, or bigram model, has no mechanism for capturing this layer of stochastic behavior. On the other hand, the HMM can account for unobserved influential variables, and the HHMM can do so to an even greater extent by explicitly modeling task/subtask structure. Considering the performance of the HHMM on individual subtasks reveals interesting properties of our dialogues. First, the HHMM is unable to outperform baseline on the UNDERSTAND THE PROBLEM subtask. To address this issue, our ongoing work investigates taking into account 
71
student characteristics such as incoming knowledge level and self-confidence. On all four in-domain subtasks, the HHMM achieved a 30% to 50% increase over baseline. For extra-domain dialogues, which involve side conversations that are not task-related, the HHMM achieved 86% prediction accuracy on tutor moves, which constitutes a 115% improvement over baseline. This high accuracy may be due in part to the fact that out-of-domain asides were almost exclusively initiated by the student, and tutors rarely engaged in such exchanges beyond providing a single response. This regularity likely facilitated prediction of the tutor?s dialogue moves during out-of-domain talk. We are aware of only one recent project that reports extensively on predicting system actions from a corpus of human-human dialogue. Bangalore et al?s (2008) flat task/dialogue model in a catalogue-ordering domain achieved a prediction accuracy of 55% for system dialogue acts, a 175% improvement over baseline. When explicitly modeling the hierarchical task/subtask dialogue structure, they report a prediction accuracy of 35.6% for system moves, approximately 75% above baseline (Bangalore & Stent, 2009). These findings were obtained by utilizing a variety of lexical and syntactic features along with manually annotated dialogue acts and task/subtask labels. In comparison, our HHMM achieved an average 42% improvement over baseline using only annotated dialogue acts and task/subtask labels. In ongoing work we are exploring the utility of additional features for this prediction task. Our best model performed better than baseline by a significant margin. The absolute prediction accuracy achieved by the HHMM was 57% across the corpus, which at first blush may appear too low to be of practical use. However, the choice of tutorial move involves some measure of subjectivity, and in many contexts there may be no uniquely appropriate dialogue act. Work in other domains has dealt with this uncertainty by maintaining multiple hypotheses (Wright Hastie, Poesio, & Isard, 2002) and by mapping to clustered sets of moves rather than maintaining policies for each possible system selection (Young et al, 2009). Such approaches may prove useful in our domain as well, and may help to more fully realize 
the potential of a learned dialogue management model.  7 Conclusion and Future Work Learning models that predict system moves within a corpus is a first step toward building fully data-driven dialogue management models. We have presented Markov models, hidden Markov models, and hierarchical hidden Markov models trained on sequences of manually annotated dialogue acts and task events. Of the three models, the hierarchical models appear to perform best in our domain, which involves an intrinsically hierarchical task/subtask structure.  The models? performance points to promising future work that includes utilizing additional lexical and syntactic features along with fixed user (student) characteristics within a hierarchical hidden Markov modeling framework. More broadly, the results point to the importance of considering task structure when modeling a complex domain such as those that often accompany task-oriented tutoring. Finally, a key direction for data-driven dialogue management models involves learning unsupervised dialogue act and task classification models.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Ai, H., Tetreault, J. R., & Litman, D. J. (2007). Comparing user simulation models for dialog strategy learning. Proceedings of NAACL HLT, Companion Volume, Rochester, New York. 1-4.  Aleven, V., McLaren, B., Roll, I., & Koedinger, K. (2004). Toward tutoring help seeking: Applying cognitive modeling to meta-cognitive skills. Proceedings of ITS, 227-239.  Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  
72
Bangalore, S., & Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the EACL, 94-102.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of NAACL HLT (Short Papers), 19-26. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (In press). Characterizing the effectiveness of tutorial dialogue with hidden Markov models. Proceedings of ITS, Pittsburgh, Pennsylvania.  Chi, M., Jordan, P., VanLehn, K., & Hall, M. (2008). Reinforcement learning-based feature selection for developing pedagogically effective tutorial dialogue tactics. Proceedings of EDM, Montreal, Canada. 258-265.  Chi, M., Jordan, P., VanLehn, K., & Litman, D. (2009). To elicit or to tell: Does it matter? Proceedings of AIED, 197-204.  Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.   Evens, M., & Michael, J. (2006). One-on-one tutoring by humans and computers. Mahwah, New Jersey: Lawrence Erlbaum Associates. Fine, S., Singer, Y., & Tishby, N. (1998). The hierarchical hidden Markov model: Analysis and applications. Machine Learning, 32(1), 41-62.  Forbes-Riley, K., Rotaru, M., Litman, D. J., & Tetreault, J. (2007). Exploring affect-context dependencies for adaptive system development. Proceedings of NAACL HLT (Short Papers), 41-44.  Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Graesser, A. C., Chipman, P., Haynes, B. C., & Olney, A. (2005). AutoTutor: An intelligent tutoring system with mixed-initiative dialogue. IEEE Transactions on Education, 48(4), 612-618.  Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354-373.  Heeman, P. A. (2007). Combining reinforcement learning with information-state update rules. Proceedings of NAACL HLT, 268-275.  
Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487-511.  Jordan, P., Makatchev, M., Pappuswamy, U., VanLehn, K., & Albacete, P. (2006). A natural language tutorial dialogue system for physics. Proceedings of FLAIRS, 521-526.  Kersey, C., Di Eugenio, B., Jordan, P., & Katz, S. (2009). KSC-PaL: A peer learning agent that encourages students to take the initiative. Proceedings of the NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Boulder, Colorado. 55-63.  Levin, E., Pieraccini, R., & Eckert, W. (2000). A stochastic model of human-machine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11-23.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Poesio, M., & Mikheev, A. (1998). The predictive power of game structure in dialogue act recognition: Experimental results using maximum entropy estimation. Proceedings of ICSLP, 90-97.  Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Schegloff, E., & Sacks, H. (1973). Opening up closings. Semiotica, 7(4), 289-327.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373.  Tetreault, J. R., & Litman, D. J. (2008). A reinforcement learning approach to evaluating state representations in spoken dialogue systems. Speech Communication, 50(8-9), 683-696.  VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When are tutorial dialogues more effective than reading? Cognitive Science, 31(1), 3-62.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.  
73
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 75?78,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Exploring the Effectiveness of Lexical Ontologies                                                    for Modeling Temporal Relations with Markov Logic 
  Eun Y. Ha, Alok Baikadi, Carlyle J. Licata, Bradford W. Mott, James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha,abaikad,cjlicata,bwmott,lester}@ncsu.edu     Abstract Temporal analysis of events is a central problem in computational models of dis-course. However, correctly recognizing temporal aspects of events poses serious challenges. This paper introduces a joint modeling framework and feature set for temporal analysis of events that utilizes Markov Logic. The feature set includes novel features derived from lexical on-tologies. An evaluation suggests that in-troducing lexical relation features im-proves the overall accuracy of temporal relation models. 1 Introduction Reasoning about the temporal aspects of events is a critical task in discourse understanding. Temporal analysis techniques contribute to a broad range of applications including question answering and document summarization, but temporal reasoning is complex. A recent series of shared task evaluation challenges proposed a framework with standardized sets of temporal analysis tasks, including identifying the temporal entities mentioned in text, such as events and time expressions, as well as identifying the tem-poral relations that hold between those temporal entities (Pustejovsky and Verhagen, 2009).   Our previous work (Ha et al, 2010) addressed modeling temporal relations between temporal entities and proposed a supervised machine-learning approach with Markov Logic (ML) (Richardson and Domingos, 2006). As novel fea-tures, we introduced two types of lexical rela-tions derived from VerbOcean (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). A 
preliminary evaluation showed the effectiveness of our approach. In this paper, we extend our previous work and conduct a more rigorous evaluation, focusing on the impact of joint opti-mization of the features and the effectiveness of the lexical relation features for modeling tempo-ral relations. 2 Related Work Recently, data-driven approaches to modeling temporal relations for written text have been gaining momentum. Boguraev and Ando (2005) apply a semi-supervised learning technique to recognize events and to infer temporal relations between time expressions and their anchored events. Mani et al (2006) model temporal rela-tions between events as well as between events and time expressions using maximum entropy classifiers. The participants of TempEval-1 in-vestigate a variety of techniques for temporal analysis of text (Verhagen et al, 2007).  While most data-driven techniques model temporal relations as local pairwise classifiers, this approach has the limitation that there is no systematic mechanism to ensure global consis-tencies among predicted temporal relations (e.g., if event A happens before event B and event B happens before event C, then A should happen before C). To avoid this drawback, a line of re-search has explored techniques for the global optimization of local classifier decisions. Cham-bers and Jurafsky (2008) add global constraints over local classifiers using Integer Linear Pro-gramming. Yoshikawa et al (2009) jointly model related temporal classification tasks using ML. These approaches are shown to improve the ac-curacy of temporal relation models. Our work is most closely related to Yoshikawa et al (2009) in that ML is used for joint model-
75
ing of temporal relations. We extend their work in three primary respects. First, we introduce new lexical relation features. Second, our model addresses a new task introduced in TempEval-2. Third, we employ phrase-based syntactic features (Bethard and Martin 2007) rather than depend-ency-based syntactic features. 3 Data and Tasks We use the TempEval-2 data for English for both training and testing of our temporal relation models. The data includes 162 news articles (to-taling about 53,000 tokens) as the training set and another 11 news articles as the test set. The corpus is labeled with events, time expressions, and temporal relations. Each labeled event and time expression is further annotated with seman-tic and syntactic attributes. Six types of temporal relations are considered: before, after, overlap, before-or-overlap, overlap-or-after, and vague. Consider the following example from the TempEval-2 data, marked up with a time expres-sion t1 and three events e1, e2, and e3, where e1 and e2 are the main events of the first and the second sentences, respectively, and e3 is syntac-tically dominated by e2. But a [minute and a half]t1 later, a pilot from a nearby flight [calls]e1 in. Ah, we just [saw]e2 an [explosion]e3 up ahead of us here about sixteen thousand feet or something like that. In the first sentence, t1 and e1 are linked by a temporal relation overlap. Temporal relation af-ter holds between the two consecutive main events: e1 occurs after e2. The main event e2 of the second sentence overlaps with e3, which is syntactically dominated by e2. In this paper, we focus on three subproblems of the temporal relation identification task as de-fined by TempEval-2: identifying temporal rela-tions between (1) events and time expressions in the same sentence (ET); (2) two main events in consecutive sentences (MM); and (3) two events in the same sentence when one syntactically dominates another (MS), which is a new task in-troduced in TempEval-2. 4 Features Surface features include the word tokens and stems of the words. In the TempEval-2 data, an event always consists of a single word token, but 
time expressions often consist of multiple tokens. We treat the entire string of words in a given time expression as a single feature. Semantic features are the semantic attributes of individual events and time expressions de-scribed in Section 3. In this work, we use the gold-standard values for these features that were manually assigned by human annotators in the training and the test data.  Syntactic features include three features adopted from Bethard and Martin (2007): gov-prep, any prepositions governing the event or time expression (e.g., ?for? in ?for ten years?); gov-verb, the verb governing the event or time expression; gov-verb-pos, the part-of-speech (pos) tag of the governing verb. We also consider the pos tag of the word in the event and the time expression. Lexical relations are the semantic relations be-tween two events derived from VerbOcean (Chklovski and Pantel, 2004) and WordNet (Fellbaum, 1998). VerbOcean contains five types of relations (similarity, strength, antonymy, en-ablement, and happens-before) that commonly occur between pairs of verbs. To overcome data sparseness, we expanded the original VerbOcean database by calculating symmetric and transitive closures of key relations. With WordNet, a se-mantic distance between the associated tokens of each target event pair was computed. 5 Modeling Temporal Relations with Markov Logic ML is a statistical relational learning framework that provides a template language for defining Markov Logic Networks (MLNs). A MLN is a set of weighted first-order clauses constituting a Markov network in which each ground formula represents a feature (Richardson and Domingos, 2006). Our MLN consists of a set of formulae com-bining two types of predicates: hidden and ob-served. Hidden predicates are those that are not directly observable during test time. A hidden predicate is defined for each task: relEventTimex (temporal relation between an event and a time expression), relMainEvents (temporal relation between two main events), and relMainSub (temporal relation between a main and a domi-nated event). Observed predicates are those that can be fully observed during test time and repre-sent each of the features described in Section 4.  The following is an example formula used in our MLN: 
76
eventTimex(d, e, t)  eventWord(d, e, w)            relEventTimex(d, e, t, r)       (1) The predicate eventTimex(d, e, t) represents the existence of a candidate pair of event e and time expression t in a document d. Given this candi-date pair, formula (1) assigns weights to a tem-poral relation r whenever it observes a word to-ken w in the given event from the training data. This formula is local because it considers only one hidden predicate (relEventTimex). In addition to local formulae, we also define a set of global formulae to ensure consistency be-tween local decisions: relEventTimex(d, e1, t, r1)  relEventTimex(d, e2, t, r2)  relMainSub(d, e1, e2, r3)           (2) Formula (2) is global because it jointly concerns more than one hidden predicate (relEventTimex and relMainSub) at the same time. This formula ensures consistency between the predicted tem-poral relations r1, r2, and r3 given a main event e1, a syntactically dominated event e2, and a time expression t shared by both of these events. Two additional global formulae (3) and (4) are simi-larly defined to ensure consistency as below.  relMainSub(d, e1, e2, r3)  relEventTimex(d, e2, t, r2)    relEventTimex(d, e1, t, r1)       (3) relMainSub(d, e1, e2, r3)  relEventTimex(d, e1, t, r1)    relEventTimex(d, e2, t, r2)       (4) 6 Evaluation To evaluate the proposed approach, we built and compared two models: one model (NoLex) used all of the features described in Section 4 except for the lexical relation features, and the other model (Full) included the full set of features. The features were generated using the Porter Stemmer and WordNet Lemmatizer in NLTK (Loper and Bird, 2002) and the Charniak Parser (Charniak, 2000). The semantic distance between two word tokens was computed using the path-similarity metric provided by NLTK. All of the models were constructed using Markov TheBeast (Riedel, 2008) The feature set was optimized for each task on a held-out development data set consisting of approximately 10% of the entire training set (Ta-ble 1). Our previous work (Ha et al, 2010) ob-served that a local optimization approach that selects for each individual task (i.e., each hidden predicate in the given MLN) in isolation from the other tasks could harm the overall accuracy of a joint model because of resulting inconsistencies 
among individual tasks. In the new experiment described in this section, features were selected for each task to improve overall accuracy of the joint model combining all three tasks, similar to Yoshikawa et al (2009).  Table 2 reports the resulting performance (F1 scores) of the models. To isolate the potential effects of global constraints, we first compare the accuracies of the Full and the NoLex model, av-eraged from a ten-fold cross validation on the training data before global constraints are added. Full achieves relative 12% and 3% improve-ments over NoLex for temporal relation between events and time expressions (ET) and between two main events (MM), respectively. The im-provement for MM was statistically significant (p<0.05) from a two-tailed paired t-test. Note that the ET task itself does not use lexical rela-tion features but still achieves an improved result in Full over NoLex. This is an effect of joint modeling. There is a slight degradation (relative 2%) in the accuracy for temporal relations be-tween main and syntactically dominated events (MS). Overall, Full achieves relative 5% im-provement over NoLex. A similar trend of per-formance improvement in Full over NoLex was observed when the global formulae were added to each model. The second column (Global Con-straints) of Table 2 compares the two models trained on the entire training set and tested on the test set after the global formulae were added. However, no statistical significance was found on these improvements. Compared to the state-
Task Feature ET MM MS event-word ? ? ? event-stem ? ? ? timex-word ?   
Surface Features 
timex-stem ?   event-polarity ? ? ? event-modal ? ? ? event-pos ? ?   ?* event-tense ? ? ? event-aspect ? ? ? event-class ? ? ? timex-type ?   
Semantic Attributes 
timex-value ?   pos ? ? ? gov-prep ? ? ? gov-verb ? ? ? 
Syntactic Features 
gov-verb-pos ? ? ? verb-rel  ? ? Lexical  Relations word-dist  ?   Table 1: Features used to model each task. *The feature is extracted only from the second event in the pair being compared. 
77
of-the-art results achieved by the TempEval-2 participants, Full achieves the same or better re-sults on all three addressed tasks. 7 Conclusions Temporal relations can be modeled with Markov Logic using a variety of features including lexi-cal ontologies. Three tasks relating to the Tem-pEval-2 data were addressed: predicting tempo-ral relations between (1) events and time expres-sions in the same sentence, (2) two main events in consecutive sentences, and (3) two events in the same sentence when one syntactically domi-nates the other. An evaluation suggests that util-izing lexical relation features within a joint mod-eling framework using Markov Logic achieves state-of-the-art performance. The results suggest a promising direction for future work. The proposed approach assumes events and time expressions are already marked in the data. To construct a fully automatic tempo-ral relation identification system, the approach needs to be extended to include models that rec-ognize events and time expressions in text as well as their semantic attributes. A data-driven approach similar to the one described in this pa-per may be feasible for this new modeling task. It will entail exploring a variety of features to fur-ther understand the complexity underlying the problem of temporal analysis of events. Acknowledgments This research was supported by the National Sci-ence Foundation under Grant IIS-0757535. References  S. Bethard and J. H. Martin. 2007. CU-TMP: Temporal relation classification using syntactic and semantic features. In Proceedings of the 4th Inter-national Workshop on Semantic Evaluations, pages 129-132, Prague, Czech Republic. B. Boguraev and R. K. Ando. 2005. TimeML-compliant text analysis for temporal reasoning. In Proceedings of the 19th International Joint Conference on Artificial intelligence, pages 997-1003, Edinburgh, Scotland.  N. Chambers and D. Jurafsky. 2008. Jointly combin-ing implicit constraints improves temporal order-ing. In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages 698-706, Honolulu, HI. E. Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the 1st North American Chapter of the Association for Computational Lin-
Chapter of the Association for Computational Lin-guistics Conference, pages 132-139, Seattle, WA. T. Chklovski and P. Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 33-40, Barcelona, Spain. E. Ha, A. Baikadi, C. Licata, and J. Lester. 2010. NCSU: Modeling temporal relations with Markov Logic and lexical ontology. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 341-344, Uppsala, Sweden. E. Loper and S. Bird. 2002. NLTK: The natural lan-guage toolkit. In Proceedings of ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics, pages 62-69, Philadelphia, PA. J. Pustejovsky and M. Verhagen. 2009. SemEval-2010 task 13: Evaluating events, time expressions, and temporal relations. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 112-116, Boulder, CO. S. Riedel. 2008. Improving the accuracy and effi-ciency of MAP inference for Markov Logic. In Proceedings of the 24th Conference in Uncertainty in Artificial Intelligence, pages 468-475, Helsinki, Finland. M. Richardson and P. Domingos. 2006. Markov Logic networks. Machine Learning, 62(1):107-136. M. Verhagen, R. Gaizauskas, F. Schilder, M. Hepple, G. Katz, and J. Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75-80, Prague, Czech Republic. K. Yoshikawa, S. Riedel, M. Asahara, and Y. Matsu-moto. 2009. Jointly identifying temporal relations with Markov Logic. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 405-413, Suntec, Singapore. 
No Global Constraints Global Constraints Task NoLex Full NoLex Full State-of-the-art Overall 0.60 0.63 (+5%) 0.59 0.61 (+3%) NA ET 0.52 0.58 (+12%) 0.62 0.65 (+5%) 0.63 MM 0.65 0.67 (+3%)* 0.52 0.56 (+8%) 0.55 MS 0.66 0.65 (- 2%) 0.66 0.66 (+0%) 0.66 Table 2. Performance comparison between mod-els in F1 score. *Statistical significance (p<0.05) 
78
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297?305,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Dialogue Act Modeling in a Complex Task-Oriented Domain 
  Kristy Elizabeth Boyer Eun Young Ha Robert Phillips* Michael D. Wallis* Mladen A. Vouk James C. Lester  Department of Computer Science, North Carolina State University Raleigh, North Carolina, USA  *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA  {keboyer,?eha,?rphilli,?mdwallis,?vouk,?lester}@ncsu.edu? Abstract 
Classifying the dialogue act of a user utterance is a key functionality of a dialogue management system. This paper presents a data-driven dialogue act classifier that is learned from a corpus of human textual dialogue. The task-oriented domain involves tutoring in computer programming exercises. While engaging in the task, students generate a task event stream that is separate from and in parallel with the dialogue. To deal with this complex task-oriented dialogue, we propose a vector-based representation that encodes features from both the dialogue and the hierarchically structured task for training a maximum likelihood classifier. This classifier also leverages knowledge of the hidden dialogue state as learned separately by an HMM, which in previous work has increased the accuracy of models for predicting tutorial moves and is hypothesized to improve the accuracy for classifying student utterances. This work constitutes a step toward learning a fully data-driven dialogue management model that leverages knowledge of the user-generated task event stream. 1 Introduction Two central challenges for dialogue systems are interpreting user utterances and selecting system dialogue moves. Recent years have seen an increased focus on data-driven techniques for addressing these challenging tasks (Bangalore et al, 2008; Frampton & Lemon, 2009; Hardy et al, 2006; Sridar et al, 2009; Young et al, 2009). Much of this work utilizes dialogue acts, built on the notion of speech acts (Austin, 1962), which 
provide a valuable intermediate representation that can be used for dialogue management. Data-driven approaches to dialogue act interpretation have included models that take into account a variety of lexical, syntactic, acoustic, and prosodic features for dialogue act tagging (Sridhar et al, 2009; Stolcke et al, 2000). In task-oriented domains, recent work has approached dialogue act classification by learning dialogue management models entirely from human-human corpora (Bangalore et al, 2008; Chotimongkol, 2008; Hardy et al, 2006). Our work adopts this approach for a corpus of human-human dialogue in a task-oriented tutoring domain. Unlike the majority of task-oriented domains that have been studied to date, our domain involves the separate creation of a persistent artifact, in our case a computer program, by the user during the course of the dialogue. Our corpus consists of human-human textual dialogue utterances and a separate, parallel stream of user-generated task actions. We utilize structural features including task/subtask, speaker, and hidden dialogue state along with lexical and syntactic features to interpret user (student) utterances.  This paper makes three contributions. First, it addresses representational issues in creating a dialogue model that integrates task actions with hierarchical task/subtask structure. The task is captured within a separate synchronous event stream that exists in parallel with the dialogue. Second, this paper explores the performance of dialogue act classifiers using different lexical/syntactic and structural feature sets. This comparison includes one model trained entirely on lexical/syntactic features, an important step toward robust unsupervised dialogue act tagging 
297
(Sridhar et al, 2009). Finally, it investigates whether the addition of HMM and task/subtask features improves the performance of the dialogue act classifiers. The findings support this hypothesis for three student dialogue moves, each with important implications for tutorial dialogue.  2 Related Work A variety of modeling approaches have been investigated for statistical dialogue act classification, including sequential approaches and vector-based classifiers. Sequential approaches typically formulate dialogue as a Markov chain in which an observation depends on a finite number of preceding observations. HMM-based approaches make use of the Markov assumption in a doubly stochastic framework that allows fitting optimal dialogue act sequences using the Viterbi algorithm (Rabiner, 1989; Stolcke et al, 2000). Like this work, the approach reported here adopts a first-order Markov formulation to train an HMM on sequences of dialogue acts, but the prediction of this HMM is subsequently encoded in a feature vector for training a vector-based classifier. Vector-based approaches, such as maximum entropy modeling, also frequently take into account both lexical/syntactic and structural features. Lexical and syntactic cues are extracted from local utterance context, while structural features involve longer dialogue act sequences and, in task-oriented domains, task/subtask history. Work by Bangalore et al (2008) on learning the structure of human-human dialogue in a catalogue-ordering domain (also extended to the Maptask and Switchboard corpora) utilizes features including words, part of speech tags, supertags, and named entities, and structural features including dialogue acts and task/subtask labels. In order to perform incremental decoding of dialogue acts and task/subtask structure, they take a greedy approach that does not require the search of complete dialogue sequences. Our work also accomplishes left-to-right incremental interpretation with a greedy approach. Our feature vectors differ from the aforementioned work slightly with respect to lexical/syntactic features and notably in the addition of a set of structural features generated by a separately trained HMM, as described in Section 4.2.  Recent work has explored the use of lexical, syntactic, and prosodic features for online dialogue act tagging (Sridhar et al, 2009); that 
work explores the notion that structural (history) features could be omitted altogether from incremental left-to-right decoding, resulting in computationally inexpensive and robust dialogue act classification. Although our textual dialogue does not feature prosodic cues, we report on the use of lexical/syntactic features alone to perform dialogue act classification, a step toward a fully unsupervised approach.   Like Bangalore et al (2008), we treat task structure as an integral part of the dialogue model. Other work that has taken this approach includes the Amiti?s project, in which a dialogue manager for a financial domain was derived entirely from a human-human corpus (Hardy et al, 2006). The TRIPS dialogue system also closely integrated task and dialogue models, for example, by utilizing the task model to facilitate indirect speech act interpretation (Allen et al, 2001). Work on the Maptask corpus has modeled task structure in the form of conversational games (Wright Hastie et al, 2002). Recent work in task-oriented domains has focused on learning task structure with unsupervised approaches (Chotimongkol, 2008). Emerging unsupervised methods, such as for detecting actions in multi-party discourse, also implicitly capture a task structure (Purver et al, 2006).  Our domain differs from the task-oriented domains described above in that our dialogues center on the user creating a persistent artifact of intrinsic value through a separate, synchronous stream of task actions. To illustrate, consider a catalogue-ordering task in which one subtask is to obtain the customer?s name. The fulfillment of this subtask occurs entirely through the dialogue, and the resulting artifact (a completed order) is produced by the system. In contrast, our task involves the user constructing a solution to a computer programming problem. The fulfillment of this task occurs partially in the dialogue through tutoring, and partially in a separate synchronous stream of user-driven task actions about which the tutor must reason. The stream of user-driven task actions produces an artifact of value in itself (a functioning computer program), and that artifact is the subject of much of the dialogue. We propose a representation that integrates task actions and dialogue acts from these streams into a shared vector-based representation, and we investigate the use of the resulting structural, lexical, and syntactic features for dialogue act classification.  
298
3 Corpus and Annotation The corpus was collected during a controlled human-human tutoring study in which tutors and students worked through textual dialogue to solve an introductory computer programming problem. The dialogues were effective: on average, students exhibited significant learning and self-confidence gains (Boyer et al, 2009).   The corpus contains 48 dialogues each with a separate, synchronous task event stream as depicted in Excerpt 1 of the appendix. There is exactly one dialogue (tutoring session) per student. The corpus captures approximately 48 hours of dialogue and contains 1,468 student utterances and 3,338 tutor utterances. Because the dialogue was textual, utterance segmentation consisted of splitting at existing sentence boundaries when more than one dialogue act was present in the utterance. This segmentation was conducted manually by the principal dialogue act annotator.1  The corpus was manually annotated with dialogue act labels and task/subtask features. Lexical and syntactic features were extracted automatically. The remainder of this section describes the manual annotation. 3.1 Dialogue Act Annotation The dialogue act annotation scheme was inspired by schemes for conversational speech (Stolcke et al, 2000) and task-oriented dialogue (Core & Allen, 1997). It was also influenced by tutoring-specific tagsets (Litman & Forbes-Riley, 2006). Inter-rater reliability for the dialogue act tagging on 10% of the corpus selected via stratified (by tutor) random sampling was ?=0.80. The dialogue act tags, their relative frequencies, and their individual kappa scores from manual annotation are displayed in Table 1.  3.2 Task Annotation All task actions were generated by the student while implementing the solution to an introductory computer programming problem in Java. These task actions were recorded as a separate event stream in parallel with the dialogue corpus. This stream included 97,509 keystroke-level user task events, which were manually aggregated into task/subtask event clusters and annotated for subtask structure and then for correctness. A total of 3,793 aggregated                                                 1 Automatic segmentation is a challenging problem in itself and is left to future work. 
student subtask actions were identified through manual annotation. The task annotation scheme is hierarchical, reflecting the nested nature of the subtasks. A subset of this task annotation scheme is depicted in Figure 1. In the models reported in this paper, the 66 leaves of the task/subtask hierarchy were encoded in the input feature vectors.   Table 1. Student dialogue acts Student?Dialogue?Act? Rel.?Freq.? Human???ACKNOWLEDGMENT?(ACK)? .17? .90?REQUEST?FOR?FEEDBACK?(RF)? .20? .91?EXTRA?DOMAIN?(EX)? .08? .79?GREETING?(GR)? .04? .92?UNCERTAIN?FEEDBACK?WITH?ELABORATION?(UE)? .01? .53?UNCERTAIN?FEEDBACK?(U)? .02? .49?NEGATIVE?FEEDBACK?WITH?ELABORATION?(NE)? .01? .61?NEGATIVE?FEEDBACK?(N)? .05? .76?POSITIVE?FEEDBACK?WITH?ELABORATION?(PE)? .02? .43?POSITIVE?FEEDBACK?(P)? .09? .81?QUESTION?(Q)? .09? .85?STATEMENT?(S)? .16? .82?THANKS?(T)? .05? 1?
 Each group of task events that occurred between dialogue utterances was tagged, possibly with many subtask labels, by a human judge. The judge aggregated the raw task keystrokes and tagged the task/subtask hierarchy for each cluster. (Please see Excerpt 1 in the appendix.) A second judge tagged 20% of the corpus in a reliability study for which one-to-one subtask identification was not enforced, an approach that was intended to give judges maximum flexibility to cluster task actions and subsequently apply the tags. All unmatched subtask tags were treated as disagreements. The resulting kappa statistic at the leaves was ?= 0.58. However, we also observe that the sequential nature of the subtasks within the larger task produces an ordinal relationship between subtasks. For example, in Figure 1, the ?distance? between subtasks 1-a and 1-b can be thought of as ?less than? the distance between subtasks 1-a vs. 3-d because those subtasks are farther from each other within the larger task. The weighted Kappa statistic (Artstein & Poesio, 2008) takes into account such an ordinal relationship and its implicit distance function. The weighted Kappa is 
299
?weighted=0.86, which indicates acceptable inter-rater reliability on the task/subtask annotation. 
 Figure 1. Portion of task annotation scheme  Along with its tag for hierarchical subtask structure, each task event was also judged for correctness according to the requirements of the task as depicted in Table 2. The agreement statistic for correctness was calculated for task events on which the two judges agreed on subtask tag. The resulting unweighted agreement statistic for correctness was ?=0.80.  Table 2. Task correctness labels  Label? Description?CORRECT? Fully? satisfying? the? requirements? of?the? learning? task.? Does? not? require?tutorial?remediation.?BUGGY? Violating? the? requirements? of? the?learning?task.?Often?requires?tutorial?remediation.?INCOMPLETE? Not? violating,? but? not? yet? fully?satisfying,? the? requirements? of? the?learning? task.? May? require? tutorial?remediation.?DISPREFERRED? Technically? satisfying? the?requirements? of? the? learning? task,?but? not? adhering? to? its? pedagogical?intentions.? Usually? requires? tutorial?remediation.?4 Features The vector-based representation for training the dialogue act classifiers integrates several sources of features: lexical and syntactic features, and structural features that include dialogue act labels, task/subtask labels, and set of hidden dialogue state prediction features.   
4.1 Lexical and Syntactic Features Lexical and syntactic features were automatically extracted from the utterances using the Stanford Parser default tokenizer and part of speech (pos) tagger (De Marneffe et al, 2006). The parser created both phrase structure trees and typed dependencies for individual sentences. From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Typed dependencies between pairs of words were extracted from each sentence. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). The pos features were extracted in a similar way. Unigram and bigram word and pos tags were included for feature selection in the classifiers.   4.2 Structural Features Structural features include the annotated dialogue acts, the annotated task/subtask labels, and attributes that represent the hidden dialogue state. Our previous work has found that a set of hidden dialogue states, which correspond to widely accepted notions of dialogue modes in tutoring, can be identified in an unsupervised fashion (without hand labeling of the modes) by HMMs trained on manually labeled dialogue acts and task/subtask features (Boyer et al, 2009). These HMMs performed significantly better than bigram models for predicting human tutor moves (Boyer et al, 2010), which indicates that the hidden dialogue state leveraged by the HMMs has predictive value even in the presence of ?true? (manually annotated) dialogue act labels. Therefore, we hypothesized that an HMM could also improve the performance of models to classify student dialogue acts. To explore this hypothesis, we trained an HMM utilizing the methodology described in (Boyer et al, 2009) and used it to generate hidden dialogue state predictions in the form of a probability distribution over possible user utterances at each step in the dialogue. This set of stochastic features was subsequently passed to the classifier as part of the input vector (Figure 2).  4.3 Input Vectors The features were combined into a shared vector-based representation for training the classifier. As depicted in Table 3, the components of the 
300
feature vector include binary existence vectors for lexical and syntactic features for the current (target) utterance as well as for three utterances of left context (this left context may include both tutor and student utterances, which are distinguished by a separate indicator for the speaker). The task/subtask and correctness history features encode the separate stream of task events. There is no one-to-one correspondence between these history features and the left-hand dialogue context, because several task events could have occurred between a pair of dialogue events (or vice versa). This distinction is indicated in the table by the representation of dialogue time steps as [t, t-1,?] and task history steps as [task(t), task(t-1),?]. In total, the feature vectors included 11,432 attributes that were made available for feature selection.  
 Figure 2. Generation of hidden dialogue state prediction features 5 Experiments This section describes the learning of maximum likelihood vector-based models for classification of user dialogue acts. In addition to investigating the accuracy of the overall model, we also performed experiments regarding the utility of feature types for discriminating between particular dialogue acts of interest.    The classifiers are based on logistic regression, which learns a discriminant for each pair of dialogue acts by assigning weights in a maximum likelihood fashion. 2  The logistic regression models were learned using the Weka machine learning toolkit (Hall et al, 2009). For                                                 2 In general, the model that maximizes likelihood also maximizes entropy under the same constraints (Berger et al, 1996).  
feature selection, we performed attribute subset evaluation with a best-first approach that greedily searched the space of possible features using a hill climbing approach with backtracking. The prediction accuracy of the classifiers was determined through ten-fold cross-validation on the corpus, and the results below are presented in terms of prediction accuracy (number of correct classifications divided by total number of classifications) as well as by the kappa statistic, which adjusts for expected agreement by chance.    Table 3. Feature vectors 
Feature?vector?f? Description?[wt,1,?wt,|w|, pt,1,?,pt,|p|, dt,1,?,dt,|d|, st,1,?,st,|s|] 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?current?target?utterance?t??[wt-k,1,?wt-k,|w|, pt-k,1,?,pt-k,|p|, dt-k,1,?,dt-k,|d|, st-k,1,?,st-k,|s|]  where k=1,?,3 
Binary?existence?vector?for?word?unigrams?&?bigrams,?pos?unigrams?&?bigrams,?dependency?types,?and?syntactic?nodes?for?three?utterances?of?left?context?
[p(o1),?,p(o|S|)] Probability?distribution?for?emission?symbols?in?predicted?next?hidden?state?as?generated?by?HMM??[dat-1, dat-2, dat-3] Dialogue?act?left?context??[spt-1,spt-2, spt-3]? Speaker?label?left?context?[tktask(t-1), tktask(t-2), tktask(t-3)] Three?steps?of?subtask?history?(each?level?of?hierarchy?represented?as?a?separate?feature)??[ctask(t-1), ctask(t-2), ctask(t-3)]  
Three?steps?of?task?correctness?history?
pt Indicator?for?whether?the?target?utterance?was?immediately?preceded?by?a?task?event? 5.1 Overall Classification Task The overall dialogue act classification model was trained to classify each utterance with respect to the thirteen dialogue acts (Table 1). For this task, the feature selection algorithm selected 63 attributes including some syntax, dependency, pos, and word attributes as well as dialogue act, speaker, and task/subtask features. No hidden dialogue state features or task correctness attributes were selected. The overall classification accuracy was 62.8%. This accuracy constitutes a 369% improvement over baseline chance of 17% (the relative frequency of the most frequently occurring dialogue act, ACK). An alternate nontrivial baseline is a bigram model on true dialogue acts (including speaker tags); this model?s accuracy was 36.8%. The 
301
overall kappa for the full classifier was ?=.57. The confusion matrix for this model is depicted in Figure 3.        In addition to the classifier described above, we experimented with classifiers that used only the lexical and syntactic features of each utterance. This approach is of interest in part because it avoids the error propagation that can happen when a model relies on a series of its own previous classifications as features. The classifier that used only the set of lexical and syntactic features achieved a prediction accuracy of 60.2% and ?=.53 using 85 attributes.   
 5.2 Binary Dialogue Act Classification In tutoring, some student dialogue acts are particularly important to identify because of their implications for the tutor?s response or for the student model. For example, a student?s REQUEST FOR FEEDBACK requires the tutor to assess the condition of the task, rather than to query the in-domain factual knowledge base. UNCERTAIN FEEDBACK is another dialogue act of high importance because identifying it allows the tutor to respond in an affectively advantageous way (Forbes-Riley & Litman, 2009).  To explore which features are useful for classifying particular dialogue acts, we constructed binary dialogue act classifiers, one for each dialogue act, by preprocessing the dialogue act labels from the set of thirteen down to TRUE or FALSE depending on whether the label of the utterance matched the target dialogue act for that specialized classifier. Table 4 displays the features that were selected for each binary classifier, along with the percent accuracy and kappa for each model. Note that for some dialogue acts the chance baseline is very high, and therefore even a model with high prediction accuracy achieves a low kappa.         As depicted in Table 4, for several dialogue act models, the feature selection algorithm retained subtask and HMM features.   
Table 4. Binary DA classifiers  
DA? #?Features?Selected? %?Correct? Model???
ACK? 51? Lexical/syntax,?HMM,?DA?history?(preceding=S),?speaker?history?(preceding=Tutor)?? .933? .75?RF? 42? Lexical/syntax,?DA?history,?preceded?by?subtask? .905? .72?
EX? 57? Dependency,?pos,?word,?HMM,?DA?history?(preceding=EX),?subtask? .939? .45?
GR? 11? Syntax,?pos,?word,?DA?(previous=EMPTY),?speaker,?subtask?? .998? .97?UE? 21? Dependency,?pos,?word,?subtask? .991? .33?U? 63? Syntax,?dependency,?pos,?word,?HMM,?subtask? .979? .21?
NE? 44? Dependency,?pos,?word,?HMM,?DA?history?(2?ago=UNCERTAIN),?subtask? .987? 0?N? 83? Lexical/syntax,?DA?history,?subtask? .966? .76?PE? 90? Dependency,?pos,?word,?HMM,?subtask? .976? .10?
P? 110? Dependency,?pos,?word,?HMM,?DA?history?(previous=REQUEST?FEEDBACK)? .945? .58?Q? 43? Syntax,?dep,?pos,?word,?HMM,?subtask? .940? .60?S? 92? Syntax,?pos,?word,?HMM,?DA?history?(previous=EMPTY?or?Q)? .901? .57?
T? 29? Syntax,?pos,?word,?DA?history?(previous=POSITIVE)?(3?ago=POSITIVE)? .992? .92?    In an experiment to quantify the utility of these features, it was found that for many dialogue acts, a binary dialogue act classifier that was trained using only lexical and syntactic features achieved the same or better classification accuracy than the model that was given all features (Figure 4). For comparison, the modified baseline model used the last three true dialogue acts (with speaker tags); this model achieved better than chance for four dialogue acts and achieved nearly as well as the full model for GREETING (GR). The models that were given all possible features for selection outperformed the lexical/syntax-only model for seven of the thirteen dialogue acts (GREETING (GR), REQUEST FOR FEEDBACK (RF), POSITIVE FEEDBACK (P), POSITIVE ELABORATED FEEDBACK (PE), UNCERTAIN ELABORATED FEEDBACK (UE), NEGATIVE FEEDBACK (N), and EXTRA-DOMAIN (EX)); however, it should be noted that none of these differences in performance is statistically reliable at the p=0.05 level.   
Figure 3. Confusion matrix 
302
 Figure 4. Kappa for binary DA classifiers by features available for selection 6 Discussion We have presented a maximum likelihood classifier that assigns dialogue act labels to user utterances from a corpus of human-human tutorial dialogue given a set of lexical, syntactic, and structural features. Overall, this classifier achieved 62.8% accuracy in ten-fold cross-validation on the corpus. This performance is on par with other automatic dialogue act tagging models, both sequential and vector-based, in task-oriented domains that do not feature complex, user-driven parallel tasks. In a catalogue ordering domain with an integrated task and dialogue model, Bangalore et al (2009) report 75% classification accuracy for user utterances using a maximum entropy classifier, a 275% improvement over baseline. Poesio & Mikheev (1998) report 54% classification accuracy by utilizing conversational game structure and speaker changes in the Maptask corpus, an improvement of 170% over baseline. Recent work on Maptask reports a classification accuracy of 65.7% using local utterance (such as lexical/syntactic) features alone, with prosodic cues yielding further slight improvement (Sridhar et al, 2009). This classifier is analogous to our lexical/syntactic feature model, which achieved 60.2% accuracy. The results of these models demonstrate that, consistent with the findings in other task-oriented domains, lexical/syntactic features are highly useful for classifying student dialogue moves in this complex task-oriented domain. Models trained using those lexical/syntactic features 
performed almost universally better (with the exception of the binary classifier for GREETING) than models that were given the same left context of true dialogue act tags.  It was hypothesized that leveraging both the hidden dialogue state and hierarchical subtask features would improve the performance of the classifiers. There is some evidence that the subtask structure was helpful for the overall classifier; however, no HMM features were kept during feature selection for the overall model. Of the binary models, approximately half performed better than the overall model in terms of true positive rate; of those, three did so by including HMM or task/subtask features among the selected attributes to differentiate different tones of student feedback. However, this difference in performance was not statistically reliable. This finding suggests that, given lexical and syntactic features which are strong predictors of dialogue acts, the hidden dialogue state as captured by an an HMM may not contribute significantly to the dialogue act classification task. 7 Conclusion and Future Work Dialogue modeling for complex task-oriented domains poses significant challenges. An effective dialogue model allows systems to detect user dialogue acts so that they can respond in a manner that maximizes the chance of success. Experiments with the data-driven classifiers presented in this paper demonstrate that lexical/syntactic features can effectively classify student dialogue acts in the task-oriented tutoring domain. For POSITIVE, NEGATIVE, and UNCERTAIN ELABORATED student feedback acts, which play a key role in tutorial dialogue system, the addition of hidden dialogue state features (as learned by an HMM) and task/subtask features (annotated manually) improve classification accuracy, but not statistically reliably.    The overarching goal of this work is to create a data-driven tutorial dialogue system that learns its behavior from corpora of effective human tutoring. The dialogue act classification models reported here constitute an important step toward that goal, by integrating the dialogue stream with a parallel user-driven task event stream. The next generation of data-driven systems should leverage models that capture the rich interplay between dialogue and task. Future work will focus on data-driven approaches to task recognition and tutorial planning. Additionally, as dialogue system research addresses 
303
increasingly complex task-oriented domains, it becomes increasingly important to investigate unsupervised approaches for dialogue act classification and task recognition.   Acknowledgements.  This work is supported in part by the North Carolina State University Department of Computer Science and the National Science Foundation through a Graduate Research Fellowship and Grants CNS-0540523, REC-0632450 and IIS-0812291. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of the IUI, 1-8.  Artstein, R., & Poesio, M. (2008). Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4), 555-596.  Austin, J. L. (1962). How to do things with words. Oxford: Oxford University Press. Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259.  Berger, A. L., Pietra, V. J. D., & Pietra, S. A. D. (1996). A maximum entropy approach to natural language processing. Comp. Ling., 22(1), 71.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dialogue structure with adjacency pair analysis and hidden markov models. Proceedings of NAACL-HLT, Short Papers, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging hidden dialogue state to select tutorial moves. Proceedings of the 5th NAACL HLT Workshop on Innovative use of NLP for Building Educational Applications, Los Angeles, California.  Chotimongkol, A. (2008). Learning the structure of task-oriented conversations from the corpus of in-domain dialogs. (Unpublished Ph.D. Dissertation). Carnegie Mellon University School of Computer Science. Core, M., & Allen, J. (1997). Coding dialogs with the DAMSL annotation scheme. AAAI Fall Symposium on Communicative Action in Humans and Machines, 28?35.  De Marneffe, M. C., MacCartney, B., & Manning, C. D. (2006). Generating typed dependency parses 
from phrase structure parses. Proceedings of LREC, Genoa, Italy.   Forbes-Riley, K., & Litman, D. (2009). Adapting to student uncertainty improves tutoring dialogues. Proceedings of AIED, 33-40.  Frampton, M., & Lemon, O. (2009). Recent research advances in reinforcement learning in spoken dialogue systems. The Knowledge Engineering Review, 24(4), 375-408.  Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. (2009). The WEKA data mining software: An update. SIGKDD Explorations, 11(1) Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., & Wu, M. (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Comm., 48(3-4), 354-373.  Litman, D., & Forbes-Riley, K. (2006). Correlations between dialogue acts and learning in spoken tutoring dialogues. Natural Language Engineering, 12(2), 161-176.  Loper, E., & Bird, S. (2004). NLTK: The natural language toolkit. Proceedings of the ACL Demonstration Session, Barcelona, Spain. 214-217.  Porter, M. F. (1980). An algorithm for suffix stripping. Program, 14(3), 130-137.  Purver, M., Kording, K. P., Griffiths, T. L., & Tenenbaum, J. B. (2006). Unsupervised topic modelling for multi-party spoken discourse. Proceedings of the ACL, Sydney, Australia. , 44(1) 17.  Rabiner, L. R. (1989). A tutorial on hidden markov models and selected applications in speech recognition. Proceedings of the IEEE, 77(2), 257-286.  Sridhar, V. K. R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4), 407-422.  Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., Martin, R., Van Ess-Dykema, C., & Meteer, M. (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Comp. Ling., 26(3), 339-373.  Wright Hastie, H., Poesio, M., & Isard, S. (2002). Automatically predicting dialogue structure using prosodic features. Speech Communication, 36(1-2), 63-79.  Young, S., Gasic, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., & Yu, K. (2009). The hidden information state model: A practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2), 150-174.   
304
    
Time Stamp Dialogue Stream Task Stream  2008-04-11 18:23:45 Student:  so do i have to manipulate the array this time? [Q]   2008-04-11 18:23:53 Tutor:  this time, we need to do two things [S]    2008-04-11 18:24:02 Tutor:  first, we need to create a new array to hold the changed values [S]    2008-04-11 18:24:28     i 2008-04-11 18:24:28     n 2008-04-11 18:24:28     t 2008-04-11 18:24:28     \sp 1-a-i BUGGY 2008-04-11 18:24:35     \del  2008-04-11 18:24:36     \sp  2008-04-11 18:24:36     d 2008-04-11 18:24:36     o 2008-04-11 18:24:36     u 2008-04-11 18:24:36     b 2008-04-11 18:24:37     l 2008-04-11 18:24:37     e 2008-04-11 18:24:37     \sp 2008-04-11 18:24:39     [] 
1-a-i CORRECT 
2008-04-11 18:24:40     \sp  2008-04-11 18:24:42     n 2008-04-11 18:24:42     e 2008-04-11 18:24:42     w 2008-04-11 18:24:43     \sp 2008-04-11 18:24:44     \del 2008-04-11 18:24:45     T 2008-04-11 18:24:46     \del 2008-04-11 18:24:54     T 2008-04-11 18:24:54     i 2008-04-11 18:24:54     m 2008-04-11 18:24:54     e 2008-04-11 18:24:54     s 2008-04-11 18:24:55     3 2008-04-11 18:24:57     ; 
1-a-ii CORRECT 
2008-04-11 18:25:11 Student:  good? [RF]    2008-04-11 18:25:14 Tutor:  good so far, yes [PF]    2008-04-11 18:25:29 Student:  so now i have to change parts of the times array right? [Q]    2008-04-11 18:25:34 Tutor:  not quite [LF]    2008-04-11 18:25:57 Tutor:  So, when you create a new object, like a String for example, you'd say something like  String s = new String() [S]    2008-04-11 18:25:59 Tutor:  right? [AQ]    2008-04-11 18:26:06 Student:  right [P]    2008-04-11 18:26:14 Tutor:  arrays are similar [S]         
    
Appendix 
Excerpt 1. Parallel synchronous dialogue and task event streams with annotations. (Note tutor dialogue acts: AQ=ASSESSING QUESTION, LF=LUKEWARM FEEDBACK, PF=POSITIVE FEEDBACK) 
305
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 49?58,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
The Impact of Task-Oriented Feature Sets on  HMMs for Dialogue Modeling   Kristy Elizabeth Boyer  Eun Young Ha   Robert Phillips*   James Lester   Department of Computer Science North Carolina State University *Dual affiliation with Applied Research Associates, Inc. Raleigh, North Carolina, USA {keboyer, eha, rphilli, lester}@ncsu.edu 
  
Abstract 
Human dialogue serves as a valuable model for learning the behavior of dialogue systems. Hidden Markov models? sequential structure is well suited to modeling human dialogue, and their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process with a layer of implicit, highly influen-tial structure. HMMs have been shown to be effective for a variety of descriptive and pre-dictive dialogue tasks. For task-oriented dia-logue, understanding the learning behavior of HMMs is an important step toward building unsupervised models of human dialogue. This paper examines the behavior of HMMs under six experimental conditions including different task-oriented feature sets and preprocessing approaches. The findings highlight the im-portance of providing HMM learning algo-rithms with rich task-based information. Additionally, the results suggest how specific metrics should be used depending on whether the models will be employed primarily in a de-scriptive or predictive manner.  1 Introduction Human dialogue serves as a valuable model for learning the behavior of dialogue systems. For this reason, corpus-based approaches to dialogue man-agement tasks have been an increasingly active area of research (Bangalore, Di Fabbrizio, & Stent, 2006; Di Eugenio, Xie, & Serafin, 2010; Georgila, Lemon, Henderson, & Moore, 2009; Rotaru & Litman, 2009). Modeling the dialogue policies that 
humans employ permits us to directly extract con-versational and task-based expertise. These tech-niques hold great promise for scaling gracefully to large corpora, and for transferring well across do-mains.    The richness and flexibility of human dialogue introduce nondeterministic and complex patterns that present challenges for machine learning ap-proaches. One approach that has been successfully employed in dialogue modeling is the hidden Mar-kov model (HMM) (Rabiner, 1989). These models are well suited to the sequential nature of dialogue (Stolcke et al, 2000). Moreover, their theoretical underpinnings are consistent with the conception of dialogue as a stochastic process whose observations are influenced by a layer of implicit, yet highly rel-evant, structure (Boyer et al, 2009; Woszczyna & Waibel, 1994).  HMMs have been shown to perform well on important dialogue management tasks such as au-tomatic dialogue act classification (Stolcke et al, 2000). Our work has employed HMMs for a differ-ent goal: learning dialogue policies, or strategies, from corpora (Boyer, Phillips, et al, 2010; Boyer, Phillips, Ingram, et al, in press). This work can be viewed from two perspectives. First, a descriptive goal of the work is to learn models that describe the nature of human dialogues in succinct probabilistic terms, in a way that facilitates important qualitative investigations. The second and complementary goal is predictive: learning models that accurately pre-dict the dialogue moves of humans, in order to cap-ture a dialogue policy that can be used within a system.   
49
Both of these goals are of paramount im-portance in tutorial dialogue, in which tutors and students engage in dialogue in support of a learning task (Boyer, Ha, et al, 2010; VanLehn et al, 2007). Descriptive modeling represents a critical step to-ward more fully understanding the phenomena that contribute to the high effectiveness of human tutor-ing, which has to date been unmatched by tutorial dialogue systems. Predictive models, on the other hand, may be used directly as dialogue policies within systems.  The HMMs considered here were learned from an annotated corpus of textual human-human tuto-rial dialogue. In this domain, HMMs have been shown to correspond qualitatively to widely held conceptions of tutorial dialogue strategies, and ad-jacency pair analysis before model learning has been shown to enhance this qualitative correspond-ence (Boyer et al, 2009). Moreover, HMMs can identify in an unsupervised fashion structural com-ponents that correlate with student knowledge gain (Boyer, Phillips, Ingram, et al, in press).  However, to date, several important questions have not been explored. The answers to these ques-tions have implications for learning HMMs for task-oriented dialogues. The questions include the following: 1) How reliably does the HMM learning framework converge to the hyperparameter N, the best-fit number of hidden states? 2) What are the effects of preprocessing approaches, specifically, adjacency pair analysis, on the resulting HMMs? 3) How do different feature sets for task-oriented dialogue impact the descriptive fit and predictive power of learned HMMs? This paper addresses these questions. The findings suggest that model stability and predictive power benefit from the richest possible input sequences, which include not only dialogue acts but also information about the task state and the absence of particular tutor dia-logue moves. Additionally, we find that traditional measures of HMM goodness-of-fit may not identify the most highly predictive models under some con-ditions. 2 Background HMMs have been used for dialogue modeling tasks for many years. Early work utilized HMMs to model underlying linguistic structure for the pur-poses of identifying speech acts and reducing per-plexity for speech recognition (Stolcke et al, 2000; 
Woszczyna & Waibel, 1994). These projects treat-ed underlying dialogue structure as the hidden lay-er, and dialogue utterances as observations. This treatment is analogous to the work presented in this paper, except that our observations are dialogue act tags only, rather than being constituent words in each utterance. Our goals are also different: to cre-ate a qualitatively interpretable model of dialogue structure that corresponds to widely accepted no-tions of task-oriented dialogue, and to learn a high-ly predictive dialogue policy from a human-human dialogue corpus.  HMMs rely on treating dialogue as a sequential Markov process in which each observation depends only on a finite set of preceding observations. Some other approaches that rely on this assumption treat dialogue as a Markov decision process or partially observable Markov decision process, in which state changes are associated with actions and rewards (e.g., Young et al, 2010). Such work focuses on learning an optimal policy, typically utilizing a combination of human and simulated dialogue cor-pora. Reinforcement learning techniques can then be applied to learn the optimal policy based on the observed rewards. In contrast, we start with a rich corpus of human-human dialogue, which may have poor coverage in some areas (though the dialogue act tags were empirically derived and therefore mit-igate this problem to some extent), and subsequent-ly learn a model that explains the variance in that human corpus as well as possible. Capturing the dialogue policy implicit within a corpus of human-human dialogue has been ex-plored in other work in a catalogue-ordering do-main (Bangalore, Di Fabbrizio, & Stent, 2006). That work utilized maximum entropy modeling to predict human agents? dialogue moves within a vector-based framework. Although a vector-based approach differs in many regards from the sequen-tial HMM approach described here, both approach-es assume a dependence only on a finite history. HMMs accomplish this through graphical depend-encies, while vector-based approaches accomplish it by including features for a restricted window of left-hand context. The results of this catalogue-ordering project highlight how challenging it is to predict human agents? dialogue moves in a task-oriented domain. 
50
3 Corpus  The corpus was collected during a human-human tutoring study. Students solved an introductory computer programming problem in the Java pro-gramming language. Tutors were located in a sepa-rate room and communicated with students through textual dialogue while viewing a synchronized view of the student?s problem-solving workspace. Forty-eight students interacted for approximately one hour each with a tutor. Students exhibited sta-tistically significant learning gains from pretest to posttest, indicating that the tutoring was effective (Boyer, Phillips, Ingram, et al, in press). The cor-pus contains 1,468 student moves and 3,338 tutor moves. Overlapping utterances, which are common in dialogue platforms such as instant messaging, were prevented by permitting only one user to con-struct a dialogue message at a time. Because the corpus is textual, utterances were segmented at tex-tual message boundaries except when the lead dia-logue annotator noted the presence of two separate dialogue acts within non-overlapping chunks of text. In these events the utterance was segmented by the primary annotator prior to being tagged by the second dialogue act annotator.  In addition to dialogue act annotation, the cor-pus was manually annotated for task structure and correctness (Section 3.2), and for delayed tutor feedback (Seciton 3.3). The appendix displays an excerpt from the annotated corpus.  3.1 Dialogue Act Annotation As part of prior work, the corpus was annotated with dialogue acts for both tutor (Boyer, Phillips, Ingram, et al, in press) and student (Boyer, Ha, et al, 2010) utterances (Table 1). One annotator tagged the entire corpus, while a second annotator independently tagged a randomly selected 10% of tutoring sessions. The inter-annotator agreement Kappa score was 0.80.  3.2 Task Annotation The corpus includes 97,509 keystroke-level task events (computer programming actions), all taken by the student. Tutors viewed synchronously, but could not edit, the computer program. The task ac-tions were manually clustered and labeled for sub-task structure (Boyer, Phillips, et al, 2010). The task structure annotation was hierarchical, with 
leaves corresponding to specific subtasks such as creating a temporary variable in order to swap two variables? values (subtask 3-c-iii-2). Each problem-solving cluster, or subtask, was then labeled for correctness (Table 2). These correctness labels are utilized in the models presented in this paper. The Kappa agreement statistic for the correctness anno-tation on 20% of the corpus was 0.80. Table 1. Dialogue act tags Dialogue Act Tutor Example ASSESSING Q. Which type should that be? EXTRA-DOMAIN A coordinator will be there soon. GROUNDING Ok. LUKEWARM FDBK That?s close. LUKEWARM CONTENT FDBK Almost there, but the second parameter isn?t quite right. NEGATIVE FDBK That?s not right. NEGATIVE CONTENT FDBK No, the counter has to be an int. POSITIVE  FDBK Perfect. POSITIVE CONTENT FDBK Right, the array is a local varia-ble. QUESTION Which approach do you prefer? RESPONSE It will be an int. STATEMENT They start at 0. Table 2. Task correctness tags Correctness Tag Description CORRECT Fully conforming to the require-ments of the task. BUGGY Violating the requirements of the task. These task events typically require tutorial remediation. INCOMPLETE Not violating, but not yet fulfilling, the requirements of the task. 
DISPREFERRED Technically fulfilling requirements but not utilizing the target con-cepts being tutored. These events typically require tutorial remediation. 3.3 Annotation for Delayed Tutor Feedback The dialogue act and task annotations reflect posi-tive evidence regarding what did occur in the dia-logues. An additional annotation was introduced for what did not occur?specifically, instances in which tutors did not to make a dialogue move in response to students? relevant task actions. The task in our corpus is computer programming, so bugs in the task correspond to errors either in syntax or se-
51
mantics of the computer program compared to the desired outcome. The human tutors were working with only one student at a time and were carefully monitoring student task actions during the dialogue, so we take the absence of a dialogue move at a rel-evant point to be an intentional choice by the tutor to delay feedback as part of the tutorial strategy. The automatic annotation for delayed feedback in-troduced two new event tags: NO-MENTION of cor-rectly completed subtasks, and NO-REMEDIATION of existing bugs within the task.  The intuition behind these tags is that within a learned dialogue policy, specifically modeling when not to intervene is crucial. Typically human tutors mention correctly completed subtasks, but at times other tutorial goals eclipse the importance of doing so. The NO-MENTION tag captures these in-stances. On the other hand, typically when working with novices, human tutors remediate an existing bug quickly. However, tutors may choose to delay this remediation for a variety of reasons such as remediating a different bug instead or asking a con-ceptual question to encourage the student to reflect on the issue. The NO-REMEDIATION tag captures these instances of the absence of remediation given that a bug was present. These two annotations for delayed feedback were performed automatically (Boyer, Phillips, Ha, et al, in press).  3.4 Adjacency Pair Modeling Prior work has demonstrated that adjacency pairs can be identified in an unsupervised fashion from a corpus (Midgley, Harrison, & MacNish, 2006). This technique relies on statistical analysis to de-termine the significant dependencies that exist be-tween pairs of dialogue acts, or in our task-oriented corpus, pairs of dialogue acts or task actions. After the pairs of dependent events are identified, they are joined within the corpus algorithmically (Boyer et al, 2009). Joining a pair of dependent moves in this way is equivalent to introducing a deterministic (probability=1) succession between observation symbols. This type of dependency cannot be learned in the traditional first-order HMM frame-work, but is desirable when two observations are strongly linked.1                                                             1 Enhanced HMM structures, such as autoregressive HMMs, which allow for direct graphical links between observation symbols, can learn such a dependency but only in stochastic terms. 
The experiment that is described in Section 4 utilizes different feature sets to learn and compare HMMs. Table 3 shows these feature sets and their most highly statistically significant adjacency pairs. Table 3. Experimental conditions and top three ad-jacency pairs (subscripts denote speaker, Student or Tutor) Condition Description Significant Adjacency Pairs DAONLY Dialogue acts only QS~RspT  GroundS~GroundT AssessQT~PosFdbkS 
DATASK Dialogue acts & task cor-rectness events 
QS~RspT CorrectTaskS~CorrectTaskS GroundS~GroundT 
DATASK-DELAY 
Dialogue acts, task correctness, & delayed feedback  
QS~RspT NoRemediateT~BuggyTaskS CorrectTaskS~CorrectTaskS 
4 Models HMMs were selected as the modeling framework for this work because their sequential nature is well suited to the structure of human dialogue, and their ?hidden? variable corresponds to widely held con-ceptions of dialogue as having an unobservable, but influential, layer of stochastic structure. For exam-ple, in tutoring, an ?explanation? mode is common, in which the tutor presents new information and the student provides acknowledgments or takes task actions accordingly. Although the presence of the ?explanation? goal is not directly observable in most dialogues, it may be inferred from the obser-vations. These sequences correspond to the input observations for learning an HMM.  4.1 Hidden Markov Models HMMs explicitly model hidden states within a doubly stochastic structure (Rabiner, 1989). A first-order HMM, in which each hidden state depends only on the immediately preceding hidden state, is defined by the following components: ? ? = {?1, ?2, ?, ?M}, the observation sym-bol alphabet ? S = {s1,s2,?,sN}, the set of hidden states 
52
? ?=[?i], i=1,?,N, the initial probability dis-tribution, where ?i is the probability of the model beginning in hidden state si in S  ? A=[aij], a transition probability distribution, where aij is the probability of the model transitioning from hidden state i to hidden state j for i,j=1,?,N ? B=[bik], an emission probability distribu-tion where bik is the probability of state i (i=1,?,N) emitting (or generating) obser-vation symbol k (k=1,?,M). 4.2 Dialogue Modeling with HMMs In this work, the observation symbol alphabet ? is given. For each experimental condition, ? is either 1) all dialogue act tags, 2) all dialogue acts plus task correctness tags, or 3) dialogue act, task cor-rectness, and delayed feedback tags. The transition probability distribution A, emission probability dis-tribution B, and initial probability distribution ? are learned by the standard Baum-Welch algorithm for optimizing HMM parameters (Rabiner, 1989). This algorithm is susceptible to becoming trapped in local optima, so our approach uses ten-time random restart with new initial parameters for each model to reduce the probability of selecting a model that represents only a local optimum.  The hyperparameter N, which is the best number of hidden states, is also learned rather than fixed. This process involves running the full HMM train-ing algorithm, including random restarts in ten-fold cross-validation, across the data and selecting the N that corresponds to the best mean goodness-of-fit measure. For HMMs, a typical goodness-of-fit measure is log-likelihood, which captures how like-ly the observations would be under the current model. The log is taken for practical reasons, to avoid numerical underflow. Higher log-likelihood corresponds to improved model fit. However, typi-cally it is desirable to penalize a higher number of hidden states, since increasing the model complexi-ty results in tradeoffs that may not be fully warrant-ed by the improvement in model fit. In this work, we utilize the Akaike Information Criterion (AIC), a standard penalized log-likelihood metric (Akaike, 1976).     
AIC = 2*N ? 2*ln(likelihood) Lower values of AIC indicate better model fit. 4.3 Experimental Conditions HMMs were learned using three separate feature sets, each providing a progressively more complete picture of the task-oriented dialogues: dialogue acts only (DAONLY), dialogue acts and task events (DATASK), and dialogue acts with both task cor-rectness events and tags for delayed tutor feedback (DATASKDELAY).  In addition to the three different feature sets, each condition included one of two types of pre-processing. Each type of model was trained on un-altered sequences of the annotated tags, which we refer to as the UNIGRAM condition. Additionally, each type of model was trained on sequences with statistically dependent adjacency pairs joined in a preprocessing step as described in Section 3.4. The UNIGRAM and ADJPAIR conditions were explored for each of the three feature sets, resulting in six experimental conditions. These conditions were chosen in order to explore the convergence behav-ior of HMMs under the different feature sets and preprocessing, and to compare measures of descrip-tive fit with measures of predictive power.  4.4 Learned HMMs Figures 1 and 2 show a subset of the DAONLY UNIGRAM model and the DATASKDELAY ADJPAIR model. These figures depict the structure of our HMMs: each hidden state is associated with an emission probability distribution over the possible observation symbols.  5 Goodness-of-Fit Curves The learning algorithm described in Section 4.2 was applied to input sequences under the six exper-imental conditions to learn the best-fit HMM pa-rameters. Figure 3 displays these AIC results, which are discussed in detail in the remainder of this section.   
53
 Figure 1. Subset of learned HMM (N=13) for DAONLY UNIGRAM condition  
 Figure 2. Subset of learned HMM (N=9) for DATASKDELAY ADJPAIR condition  5.1 Impact of Experimental Conditions  For the DAONLY condition, both the UNIGRAM and ADJPAIR models generally improve until N=12 or 13, after which the fit generally worsens. A differ-
ent pattern emerges for the DATASK condition, in which the UNIGRAM sequences are optimally fit to a model with 16 states, while the ADJPAIR se-quences are fit to a model with 8 states. Finally, for the DATASKDELAY condition, the UNIGRAM se-quences are best fit by a model with 10 hidden states, while the ADJPAIR sequences are fit best by 9. Typically, we see that ADJPAIR sequences are fit to slightly simpler models in terms of the hy-perparameter N, number of hidden states.   
Figure 3. Number of hidden states and cor-responding adjusted AIC, shifted to a mini-mum score of zero indicating the best-fit N 
Adjust
ed AIC
 
a) Dialogue ActsOnly (DAONLY) 
  N (number of hidden states) 
Adjust
ed AIC
 
b) Dialogue Act and Task Events (DATASK) 
  N (number of hidden states) 
Adjust
ed AIC
 
c) Dialogue Act, Task, & Delayed Feedback (DATASKDELAY)  
  N (number of hidden states) 
54
Stability in the hyperparameter N is an im-portant consideration because an underlying as-sumption of our work is that the hidden states correspond to unobserved stochastic structures of the real world process?that is, we hypothesize that a ?true? value for N exists. We would like models to exhibit decreasing variation in goodness of fit measures around this true N. To examine this stability we consider the three best AIC values for each condition and their corresponding Ns: the set {Nk-best | k=1,2,3}. The range of this set indicates how ?far apart? the best three Ns are: for example, in the DAONLY UNIGRAM condition, the top three models have Ns of {13,10,15}, yielding a range of 5. Intuitively, a small value for this metric indicates that the model has converged tightly on N.  Figure 4 shows the stability results for the six different experimental conditions. As shown in the figure, for the DATASK and DATASKDELAY condi-tions, the ADJPAIR models achieve the smallest range among the top three values of N; these mod-els converge most tightly to the ?best? value.   
 Figure 4. Stability of N (range of {N1best, N2best, N3best}) ? smaller implies tighter convergence to ?best? N 6 Predictive Analysis Section 5 presented an analysis of the goodness-of-fit curves of HMMs learned from the corpus. The measures of stability and discrimination for N cap-ture important aspects of the behavior of HMMs toward this parameter, which is conceived of as representing ?true? real-world stochastic behavior. In this way, Section 5 has presented a descriptive view of HMM dialogue models.  This section presents a predictive view of the models. Specifically, we consider prediction accu-racy, defined as the percent of tutor dialogue moves 
that the model is able to correctly predict given the dialogue history sequence up to that point.  6.1 Impact of Dependent Adjacency Pairs We first explore whether the preprocessing step of joining dependent adjacency pairs impacted predic-tion accuracy. The prediction accuracy of the best-fit model in each condition is displayed in Figure 5. This figure includes prediction accuracy on training data, which were used to learn model parameters, as well as prediction accuracy on testing data, which were withheld from model training.  
 Figure 5. Prediction accuracy for tutor moves  As shown in Figure 5, joining the adjacency pairs improved model performance on the training sets of all three conditions, indicating that the varia-tion within the training data was better explained by ADJPAIR models. (This measure of predictive power is different from a goodness-of-fit criterion as described in the previous section, a relationship that will be discussed further in Section 7.) In con-trast to the training set performance, the ADJPAIR models performed better than UNIGRAM models for the testing set only in the DATASKDELAY condi-tion.   6.2 Impact of Task-Oriented Feature Sets As illustrated in Figure 5, the three feature sets per-form similarly under the UNIGRAM condition. This performance is slightly above baseline (DAONLY and DATASK baselines = 0.38; DATASKDELAY baseline = 0.30), and diminishes little between the training and testing sets. In contrast, under the ADJPAIR condition, performance between condi-tions and across training and testing sets varies. The DATask model performs far better on predicting observations in the training than the testing set, 
55
suggesting possible overfitting to the training set. This relationship is discussed further in Section 7. The DATASKDELAY model performs well during both training and testing, though with a slight de-crease in accuracy on the testing set.   6.3 Relationship Between Predictive and De-scriptive Metrics Measures of fit such as log-likelihood and AIC cap-ture the likelihood of observing the data given a model. Predictive accuracy, on the other hand, measures the probability that the model can predict the next observation given a partial sequence. In general, we would expect these measures to corre-late well; however, there is not perfect correlation between these metrics because the mechanism by which log-likelihood (and thereby AIC) is derived involves maximizing likelihood over complete se-quences, while prediction is performed over partial sequences.  To examine how well AIC and prediction accu-racy correlate, Figure 6 displays these values for a subset of the models in the DAONLY UNIGRAM condition and the DATASKDELAY ADJPAIR condi-tion. These two conditions represent the extremes of the experimental conditions, with DAONLY con-taining the least information about the task-oriented dialogue while DATASKDELAY contains the most information.  As shown in Figure 6, the correlation for DAONLY UNIGRAM roughly conforms to what would be expected: lower AIC, indicating better model fit, is associated with the highest prediction accuracies. The relationship is less clear for the DATASKDELAY ADJPAIR condition. While its worst AIC is associated with the lowest prediction accuracy as expected, the best AIC is not associated with the highest prediction accuracy. This phenom-enon may be due to the lack of spread among AIC values overall for this condition; as seen in Figure 3, the DATASKDELAY ADJPAIR condition has the flattest AIC curve of all conditions, indicating that for this condition the difference between best-fit and worst-fit models is smaller than for any other condition. The inconsistent relationship between AIC and prediction accuracy, therefore, may be the product of noise surrounding a large set of ?good? models, whereas for the DAONLY UNIGRAM condi-tion, the set of good models is smaller.   
 7 Discussion The results suggest several important findings re-garding feature sets and preprocessing for learning HMMs of task-oriented dialogue. First, the models? convergence patterns to a best-fit N, number of hidden states, indicate that more information em-bedded within the sequences may correspond with a flatter goodness-of-fit curve. Adding more infor-mation to the input sequences may introduce some regularities that partly mitigate the limitations of even a poorly fit HMM. This additional infor-mation may come in the form of adjacency pairs discovered in an unsupervised fashion, which im-proved the stability of convergence on the best-fit N under the DATASK and DATASKDELAY condi-tions. This increased stability is likely due to the fact that under these conditions, leveraging adja-cency pair information augments the HMM?s struc-ture with contextual dependencies that could otherwise not be learned under the traditional HMM framework.  For predictive accuracy, the benefits of richer input sequences are also highlighted. The most highly predictive models included all three sources 
Predic
tion Ac
curacy
 a) DAOnly UNIGRAM Condition 
  AIC 
Predic
tion Ac
curacy
 b) DATASKDELAY ADJPAIR Condition  
  AIC Figure 6. Prediction accuracy vs. AIC 
56
of information: dialogue acts, task events, and de-layed feedback tags. However, with the addition of this rich information to the input sequences and the accompanying flatter goodness-of-fit curve as dis-cussed above, we noted an irregular pattern of cor-relation between goodness-of-fit and predictive accuracy that is worthy of future exploration. Spe-cifically, it appears that the most highly predictive DATASKDELAY ADJPAIR model, which is the most highly predictive of all models in all conditions, does not correspond to the best (lowest) AIC for that condition (Figure 3). This finding suggests that when a predictive task is the primary goal, a predic-tive metric should be used to select the best-fit model. Additional support for such an approach is provided by the close correspondence between training and testing set prediction accuracy. 8 Conclusion Understanding how HMMs behave under different feature sets is an important step toward learning effective models of task-oriented dialogue. This paper has examined how HMMs converge to a best number of hidden states under different experi-mental conditions. We have also considered how well HMMs under these conditions predict tutor dialogue acts within a corpus of task-oriented tutor-ing, a crucial step toward learning dialogue policies from human corpora. The findings highlight the importance of adding rich task-based features to the input sequences in order to learn HMMs that con-verge tightly on the best-fit number of hidden states. The results also indicate that caution should be used when utilizing traditional goodness-of-fit metrics, which are appropriate for descriptive ap-plications, if the goal is to learn a highly predictive model.  This line of research is part of a larger research program of learning unsupervised models of human task-oriented dialogue that can be used to define the behavior of dialogue systems. Developing a framework for learning a dialogue policy from hu-man corpora, as discussed here, is a critical step toward that goal. Future work should focus on un-supervised dialogue act classification, and address the challenges of user plan recognition.  
Acknowledgments. This work is supported in part by National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS 
Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this re-port are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. References Akaike, H. (1976). An information criterion (AIC). Math. Sci., 14(153), 5-9. Bangalore, S., Di Fabbrizio, G., & Stents, A. (2006). Learning the structure of task-driven human-human dialogs. Proceedings of ACL ?06, 201-208.  Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of SIGDIAL (pp. 297-305).  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (in press). Learning a Tutorial Dialogue Policy for Delayed Feedback. Proceedings of the 24th International FLAIRS Con-ference. Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2009). Modeling dia-logue structure with adjacency pair analysis and hid-den Markov models. Proceedings of NAACL HLT, Companion Volume, 49-52.  Boyer, K. E., Phillips, R., Ha, E. Y., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Leveraging Hidden Dialogue State to Select Tutorial Moves. Proceedings of the NAACL HLT 2010 Fifth Work-shop on Innovative Use of NLP for Building Educa-tional Applications (pp. 66-73). Boyer, K. E., Phillips, R., Ingram, A., Young, E., Wallis, M., Vouk, M., et al (in press). Investigating the Re-lationship Between Dialogue Structure and Tutoring Effectiveness: A Hidden Markov Modeling Ap-proach. International Journal of Artificial Intelli-gence in Education. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue Act Classification, Higher Order Dialogue Structure, and Instance-Based Learning. Dialogue & Dis-course, 1(2), 1-24.  Georgila, K., Lemon, O., Henderson, J., & Moore, J. D. (2009). Automatic annotation of context and speech acts for dialogue corpora. Natural Language Engi-neering, 15(3), 315-353.  Midgley, T. D., Harrison, S., & MacNish, C. (2006). Empirical verification of adjacency pairs using dia-logue segmentation. Proceedings of SIGDIAL, 104-108.  Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech recogni-tion. Proceedings of the IEEE, 77(2), 257-286.  
57
Rotaru, M., & Litman, D. J. (2009). Discourse Structure and Performance Analysis : Beyond the Correlation. Proceedings of SIGDIAL (pp. 178-187). Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., et al (2000). Dialogue Act Model-ing for Automatic Tagging and Recognition of Con-versational Speech. Computational Linguistics, 26(3), 339-373.  VanLehn, K., Graesser, A. C., Jackson, G. T., Jordan, P., Olney, A., & Rose, C. P. (2007). When Are Tutorial 
Dialogues More Effective Than Reading? Cognitive Science: A Multidisciplinary Journal, 30(1), 3-62.  Woszczyna, M., & Waibel, A. (1994). Inferring linguis-tic structure in spoken language. Proceedings of the International Conference on Spoken Language Pro-cessing (pp. 847-850). Young, S., Ga?i?, M., Keizer, S., Mairesse, F., Schatzmann, J., Thomson, B., et al (2010). The Hid-den Information State model: A practical framework for POMDP-based spoken dialogue management. Computer Speech & Language, 24(2), 150-174.   Appendix. Excerpt from task-oriented textual human-human tutoring corpus. Speaker Utterance or Event Tag Student: [Task action on subtask 3-c-i-4] BUGGY Student: [Task action on subtask 3-c-ii-5] CORRECT Tutor: [Does not provide remediation for existing bug] NOREMEDIATION Student: [Task action on subtask 3-c-iii-1] BUGGY Student: i don't remember off the top of my head how the swap function worked. most of the time i just copied and pasted it from some of my older code NEGATIVECONTENTFDBK Tutor: The easiest way to swap x and y is to make a tempo-rary variable  Student: Ok ACK Student: do i need to pass the entire array and the indecies of the items to swap? ASSESSQ Tutor:  if you want to use a seperate method to swap, then yes, you'll have to pass those things  POSCONTENTFDBK Tutor:  [Does not mention a correctly completed subtask]	 ? NOMENTIONCOMP Student: oh. i guess i could just swap it in the same method. it is problably easier that way, and less code. we were showed in class how to do it separately, but i had never thought of doing it the other way.  
STMT 
Student: [Task action on subtask 3-c-iii-2] DISPREFERRED Tutor:  Both ways work, but it?s definitely less code to just do it inside this method.  STMT Student: Ok ACK  
58
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, page 2,
Utica, May 2012. c?2012 Association for Computational Linguistics
Invited Speaker
Dr. James Lester
North Carolina State University
Expressive NLG for Next-Generation Learning Environments: Language, Affect, and
 Narrative
Abstract
Recent years have seen the appearance of adaptive learning technologies that offer significant potential  
for bringing about fundamental improvements in education. A promising development in this arena is the 
emergence of  narrative-centered learning environments,  which integrate  the inferential  capabilities of  
intelligent  tutoring  systems  with  the  rich  gameplay  supported  by  commercial  game  engines.  While  
narrative-centered learning environments have demonstrated effectiveness in both student learning and  
engagement,  their  capabilities  will  increase  dramatically  with  expressive  NLG.  In  this  talk  we  will 
introduce the principles motivating the design of narrative-centered learning environments, discuss the  
role of NLG in narrative-centered learning, consider the interaction of NLG, affect, and learning, and 
explore how next-generation learning environments will push the envelope in expressive NLG.
Biography
Dr. James Lester is a professor Department of Computer Science North Carolina State University.  He  
received   the  B.A.  (Highest  Honors),  M.S.C.S.,  and  Ph.D.  Degrees  in  Computer  Science  from the 
University of Texas at Austin and the B.A in History from Baylor University.  A member of Phi Beta  
Kappa, he has served as Program Chair for the ACM conference on Intelligent User Interfaces (2001),  
Program Chair for the International Conference on Intelligent Tutoring Systems (2004),  Conference Co-
Chair for the International Conference on Intelligent Virtual Agents (2008), and on the editorial board of  
Autonomous Agents and Multi-Agent Systems  (1997-2007). His research focuses on intelligent tutoring 
systems, computational linguistics, and intelligent user interfaces.  It has been recognized by several Best  
Paper awards. His research interests include intelligent game-based learning environments, computational  
models of narrative, affective computing, creativity-enhancing technologies, and tutorial dialogue. He is 
Editor-In-Chief of the International Journal of Artificial Intelligence in Education.
2
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 94?98,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
 
 
From Strangers to Partners: Examining Convergence within a  Longitudinal Study of Task-Oriented Dialogue 
 Christopher M. Mitchell Kristy Elizabeth Boyer James C. Lester  Department of Computer Science North Carolina State University Raleigh, NC, USA {cmmitch2, keboyer, lester}@ncsu.edu      Abstract Convergence is thought to be an important phenomenon in dialogue through which interlocutors adapt to each other. Yet, its mechanisms and relationship to dialogue outcomes are not fully understood. This paper explores convergence in textual task-oriented dialogue during a longitudinal study. The results suggest that over time, convergence between interlocutors increases with successive dialogues. Additionally, for the tutorial dialogue domain at hand, convergence metrics were found to be significant predictors of dialogue outcomes such as learning, mental effort, and emotional states including frustration, boredom, and confusion. The results suggest ways in which dialogue systems may leverage convergence to enhance their interactions with users.  1 Introduction Convergence is a widely observed phenomenon in dialogue, in which interlocutors adapt to the patterns in each other?s utterances (Brennan 1996; Pickering and Garrod 2004). These patterns can include lexical choice (Hirschberg 2008; Ward and Litman 2007), syntactic choice (Reitter et al 2006; Stoyanchev and Stent 2009) and loudness (Coulston et al 2002). It is believed that convergence is indicative of shared understanding (Pickering and Garrod 2004), which makes it an important consideration for task-oriented dialogue systems.     In addition to facilitating shared understanding, convergence has also been associated with the success of dialogues in several domains (Steinhauser et al 2011; Ward and Litman 2007), 
and can also be leveraged for lexical and syntactic priming that may improve performance of spoken dialogue systems via more accurate speech recognition (Stoyanchev and Stent 2009). While such results have established that convergence is an important dialogue phenomenon, the field does not yet fully understand how convergence is associated with dialogue success.  This paper examines surface-level and lexical convergence within textual task-oriented dialogues. The analysis considers three levels of convergence: utterance-level short-term priming effects, conversation-level convergence effects, and longitudinal convergence effects, as interlocutors participate in six conversations together over the course of several weeks. Using these measures, we build multiple regression models that indicate ways in which convergence can predict both desirable and undesirable outcomes of task-oriented dialogues.      This paper makes several contributions. First, by examining convergence at several granularity levels and across multiple dialogues with the same partners, we gain insight into how convergence phenomena unfold over time. Second, the findings provide confirmatory evidence that in some domains, such as the tutorial dialogue considered here, lexical priming be associated with unintended consequences. Finally, we demonstrate that dialogue convergence is also associated with affective components such as frustration, engagement, and confusion. These results contribute to an understanding of convergence that may enable us to harness this phenomenon more effectively within dialogue systems.  
94
  
2 Related Work Convergence and the related concepts of alignment and priming have been extensively studied. Alignment, or the development of shared understanding, has been studied by Pickering and Garrod (2004) who propose that alignment on lower-level observable features is indicative of alignment at the level of conceptual models. The influence of shared representation in dialogue has also been explored in the context of learning; for example, Ward and Litman (2007) studied lexical convergence in human-human tutoring and found that the rate of priming, which measures student re-use of tutor words at various distances, was positively associated with learning for students with low initial test scores. Conversely, Steinhauser et al (2011) analyzed lexical convergence in an automated dialogue-based physics tutor, and found that the level of the student mimicking the tutor was negatively correlated with learning. Thus, the relationship between dialogue convergence and learning is not fully understood, and may be highly dependent on context.   In addition to a theoretical link to shared representations, convergence has practical implications, in particular for speech recognition (Stoyanchev and Stent 2009). Brennan (1996) found that users adapt their lexical choices to match those of an automated system in both text-based and speech-based interactions, even when it is apparent that the system understood the user?s original lexical choice. Convergence has even been found to occur in non-lexical aspects of a dialogue, such as users adapting their loudness levels to match that of a software agent (Coulston et al 2002). Together, these results suggest that convergence has implications beyond lexical and syntactic choice. 3 Corpus  The corpus consists of text-based tutorial dialogues between two interlocutors, a tutor and a student, working together to complete tasks in the domain of introductory computer science (excerpt in Appendix A). The corpus was collected over two semesters, in which 67 first-year university students were selected from an introductory engineering course and assigned to one of seven 
tutors of varying levels of tutoring experience. Each student engaged in six task-based dialogues with a single tutor over four weeks with the goal of producing a working software artifact during each session. Each session included several subtasks, and time was strictly limited to forty minutes duration. The remote collaboration interface, shown in Figure 1, facilitated a real-time synchronized view of the workspace and dialogue. This paper considers dialogue utterances only, leaving to future work the analysis of task-related artifacts.  
 Figure 1. Task-oriented dialogue interface  The effectiveness of the dialogue was measured in several ways. First, student learning was measured as difference in score on pre-test and post-tests. Student engagement, or level of involvement during the dialogue, was measured with a brief survey after each dialogue (O?Brien and Toms 2010), as were student?s satisfaction with the exchange, and a rating of how mentally challenging the task was perceived to be (Hart and Staveland 1988). Finally, the tutors were asked to rate their satisfaction with the effectiveness of each session and to report on their perceptions of the affective states of both interlocutors during the session. The students were not asked about their own affective states, as this may have introduced bias in subsequent dialogues.  4 Analysis The goal of the analysis is to identify the characteristics of the dialogues that are predictive of the outcomes of interest, including learning, engagement, affect, and overall success of the 
95
  
dialogue as rated by the interlocutors. Summary statistics for the dialogues were computed, including time duration of the session, number of utterances, number of words, number of characters, mean word length, and lexicon size (Table 1). Stop words were not excluded from the analysis, in part due to specialized usage of common vocabulary in the computer science domain (e.g., for, if ). Although not traditionally considered a form of convergence, we were interested in the relationship between the levels of activity of the two interlocutors.  To this end, we analyzed the number of utterances, words, and characters used by tutor and student, and found a significant positive correlation on these metrics (p<0.0001 for each). The first convergence phenomenon considered centers on lexical priming, the tendency for one interlocutor to re-use words previously introduced by the other. We have utilized a priming metric computed as follows: Interlocutor A?s Priming Ratio (PR) is the percent of Interlocutor A?s words reused by Interlocutor B at a given distance d, where distance is measured in terms of number of Interlocutor B?s utterances. Negative slope of PR over distance indicates a priming effect because an interlocutor was more likely to reuse a word shortly after its use by the other interlocutor. This metric has been used to investigate tutor priming (Steinhauser et al 2011; Ward and Litman 2007), and we generalize it to measure priming for both interlocutors. Note that student PR, which reflects the extent to which the tutor adopted the student?s lexical choice, is of particular interest from the perspective of dialogue system design, in which tutor utterances are system-generated.    Tutor  mean (SD) Student mean (SD) Surface Features Number of utterances 83.7 (28.8) 35.6 (13.1) Number of words 580.9 (202.3) 170.1 (92.6) Number of characters 2383.4 (886.6) 667.3 (386.0) Mean word length 4.1 (0.2) 3.9 (0.3) Lexicon size 329.7 (87.3) 106.3 (47.3)    Convergence Metrics Priming Ratio (1-10) .030 (.02) .047 (.02) ?Priming Ratio (1-10) -.011 (.02) -.017 (.04) Max Priming Ratio .052 (.02) .091 (.04) Matched Word Ratio .233 (.09) .386 (.08) Table 1. Statistics for each metric  In addition to the Priming Ratio, we also computed a metric to reflect convergence: Interlocutor A?s 
Matched Word Ratio (MWR) is the percent of Interlocutor A?s words that had been previously used by Interlocutor B at any point in the dialogue history. Because it is backward-looking, this metric is applicable not only in a corpus study, but could also be used within a runtime system to track convergence as the dialogue unfolds. 5 Models and Results  Mean Matched Word Ratio for both interlocutors increased as sessions progressed, reflecting that the two dialogue partners used more of each other?s words as they spent more time together. The Priming Ratio also revealed several phenomena in the corpus. Similarly to prior observations from tutorial dialogue (Ward and Litman 2007), we found that student reuse of tutor primes decreased with distance, indicating that a lexical priming effect occurred (Figure 2). This trend also occurred for tutor reuse of student primes (Figure 3).  The effect was more pronounced in the tutor?s PR than the student?s PR; that is, there was more evidence that tutors converged to students in the short term. This finding may be associated in part with the higher number of tutor utterances: a distance in terms of number of tutor utterances represents fewer combined student and tutor utterances than the same distance in terms of student utterances. Additionally, tutor convergence may reflect a dimension of intentional pedagogical choice.  The Priming Ratio is designed to reflect short-term priming. However, there is evidence of a longer-term effect as the two interlocutors engaged in dialogue across multiple sessions. Figures 2 and 3 display Tutor?s PR and Student?s PR, respectively, by task set, of which there were six in the corpus study. The last task set displays an overall higher level of lexical convergence than the earlier sessions, and there is a general trend of increasing convergence as the number of sessions together increases. In order to identify the features that were most predictive of dialogue outcomes, all of the convergence metrics and surface summary features were provided as input to a stepwise linear regression model. Standard greedy variable addition and removal was performed, with additional post-processing and re-training to eliminate instances of multicollinearity. The learned models (Appendix B) include a mixture of 
96
  
convergence metrics and surface features, as well as structural features such as the task set number and the time duration of the dialogue. At least one convergence metric was found to be associated with each outcome in the generated models, with the exception of Engagement.  
 Figure 2. Tutor's Priming Ratio aggregated by task set (TS = Task Set)   
 Figure 3. Student's Priming Ratio aggregated by task set (TS = Task Set)  Several significant relationships emerged within the models. We discuss a subset of these here. First, tutor Priming Ratio was a significant predictor for outcomes as rated by both tutor and student. Higher tutor Priming Ratio was associated with higher tutor perception of dialogue success, perhaps because students reflected tutor lexical choice more frequently. The same metric was associated with lower student score for how mentally demanding the tasks were perceived to be, which suggests that a shared lexicon may be associated with decreased cognitive load.        Another significant finding is the relationship between student Priming Ratio and student boredom, confusion, and frustration. In all the models, increased reports of these student 
emotions by the tutor corresponded to lower student Priming Ratio.  This result suggests that tutor reuse of student lexical choice may be associated with positive affective outcomes.       Finally, the tutor?s Matched Word Ratio is a significant negative predictor of learning gains, and also a significant negative predictor for student confusion. This finding may be related to the fact that by reusing more student language, the tutor may be effectively introducing fewer novel contributions that might lead to confusion.  6 Conclusion and Future Work Understanding how convergence unfolds holds significant promise for designing more effective dialogue systems. Toward that end, this paper has explored convergence in task-oriented dialogue at three levels: at the level of pairs of utterances, across a single conversation, and over multiple conversations with the same interlocutors. The results demonstrate that within the corpus, the two interlocutors display increasing levels of convergence longitudinally. Additionally, the results suggest ways in which short-term and long-term convergence are associated with particular positive and negative aspects of dialogue success and user affect.       The findings have significant implications for dialogue systems. First, they suggest that not only may successful lexical priming aid in understanding (Stoyanchev and Stent 2009), it may also be associated with lower cognitive load for users. Additionally, it may be possible to leverage convergence to positively impact users? affective states with respect to emotions such as boredom, confusion, and frustration. These potential relationships suggest that work to further elucidate convergence phenomena is particularly promising because dialogue systems stand to benefit from strategically leveraging convergence and adaptation. Acknowledgments This work is supported in part by the National Science Foundation through Grants DRL-1007962 and CNS-1042468. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation. 
0.015 
0.02 
0.025 
0.03 
0.035 
0.04 
0.045 
0 2 4 6 8 10 
Tu
to
r'
s 
P
ri
m
in
g 
R
at
io
 
Distance (number of utterances) 
TS1 
TS2 
TS3 
TS4 
TS5 
TS6 
0.03 
0.035 
0.04 
0.045 
0.05 
0.055 
0.06 
0.065 
0.07 
0 2 4 6 8 10 
S
tu
de
nt
's
 P
ri
m
in
g 
R
at
io
 
Distance (number of utterances) 
TS1 
TS2 
TS3 
TS4 
TS5 
TS6 
97
  
References  Brennan, S. (1996). Lexical Entrainment in Spontaneous Dialog. In Proceedings of the 1996 International Symposium on Spoken Dialogue, 41-44. Coulston, R., Oviatt, S., and Darves, C. (2002). Amplitude Convergence in Children?s Conversational Speech with Animated Personas. In Proceedings of the 7th International Conference on Spoken Language Processing, 2689?2692. Hart, S. and Staveland, L. (1988). Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research. In P.A. Hancock and N. Meshkati, eds., Human Mental Workload. 1988, 139-183. Hirschberg, J. (2008). High Frequency Word Entrainment in Spoken Dialogue. In Proceedings of ACL HLT, 169-172. O?Brien, H. and Toms, E. (2010). The Development and Evaluation of a Survey to Measure User Engagement. Journal of the American Society for Information Science and Technology, 6(1), 50-69. Pickering, M. and Garrod, S. (2004). Toward a Mechanistic Psychology of Dialogue. Behavioral and Brain Sciences, 27(2), 169-226. Reitter, D., Moore, J., and Keller, F. (2006). Priming of Syntactic Rules in Task-Oriented Dialogue and Spontaneous Conversation. In Proceedings of the 28th Annual Conference of the Cognitive Science Society, 685-690. Steinhauser, N., Campbell, G., Taylor, L., Scott, C., Dzikovska, M., and Moore, J. (2011). Talk Like an Electrician: Student Dialogue Mimicking Behavior in an Intelligent Tutoring System. In Proceedings of the 15th International Conference on Artificial Intelligence in Education, 361-368. Stoyanchev, S. and Stent, A. (2009). Lexical and Syntactic Priming and Their Impact in Deployed Spoken Dialog Systems. In Proceedings of NAACL HLT, 189-192. Ward, A. and Litman, D. (2007). Dialog Convergence and Learning. In Proceedings of the 13th International Conference on Artificial Intelligence in Education, 262-269.  Appendix A. Corpus Excerpt T: yes so what happens with the other paths? S: is it because the last statement is fullfilled so it has no need to print the error? S: i understand what is happening but i do not know how to explain it T: ok so you noticed that when the if statement directly before it is true then it does not go to the else T: but if the if statement directly before the else statement is false then it goes to the else statement S: yes. S: so i need to make all of them else if statements? T: yes 
 
Appendix B. Regression Models  ? p Norm. Learning Gain, R2 = .0687 Tutor?s MWR -.169 .0160 Task set number -.144 .0392  Engagement (Student-reported) R2 = .0892 Tutor?s number of characters -.527 .0007 Student?s mean word length .159 .0033 Tutor?s mean word length -.169 .0053 Tutor?s lexicon size .369 .0189  Mentally demanding (Student-reported) R2 = .217 Tutor?s PR (distances 1-5) -.128 .0118 Session length (ms) .174 .0065 Combined number of utterances .579 .0005 Tutor?s number of utterances -.475 .0040 Tutor?s number of characters -.439 .0031 Tutor?s mean word length -.118 .0496 Tutor?s lexicon size .627 <.0001  Student confusion*, R2 = .319 Student?s PR (distances 1-10) -.233 <.0001 Tutor?s number of matched words 1.04 <.0001 Tutor?s MWR -.523 <.0001 Task set number -.122 .0105 Session length (ms) .292 <.0001 Student?s number of characters .247 .0048 Combined lexicon size -.594 <.0001  Student frustration*, R2 = .300 Max value of Student?s PR .156 .0035 Session length (ms) .239 <.0001 Tutor?s number of utterances .460 <.0001 Tutor?s number of words .342 .0135 Tutor?s lexicon size -.748 <.0001  Student boredom*, R2 = .202 Student?s PR (distances 1-5) -.234 <.0001 Tutor?s number of utterances .261 .0001 Tutor?s lexicon size -.412 <.0001  Session successful overall*, R2 = .246 Tutor?s PR (distances 1-3) .186 .0002 ? Student?s PR (distances 1-10) .122 .0079 Session length (ms) -.420 <.0001 Tutor?s number of utterances .518 <.0001 Tutor?s number of words -.473 .0006 Tutor?s lexicon size .275 .0340 * = from tutor perception survey;  ? = standardized regression coefficient  
98
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 247?256,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Combining Verbal and Nonverbal Features to Overcome the ?Information Gap? in Task-Oriented Dialogue 
 Eun Young Ha, Joseph F. Grafsgaard, Christopher M. Mitchell,  Kristy Elizabeth Boyer, and James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha, jfgrafsg, cmmitch2, keboyer, lester}@ncsu.edu    Abstract Dialogue act modeling in task-oriented dialogue poses significant challenges. It is particularly challenging for corpora consisting of two interleaved communication streams: a dialogue stream and a task stream. In such corpora, information can be conveyed implicitly by the task stream, yielding a dialogue stream with seemingly missing information. A promising approach leverages rich resources from both the dialog and the task streams, combining verbal and non-verbal features. This paper presents work on dialogue act modeling that leverages body posture, which may be indicative of particular dialogue acts. Combining three information sources (dialogue exchanges, task context, and users? posture), three types of machine learning frameworks were compared. The results indicate that some models better preserve the structure of task-oriented dialogue than others, and that automatically recognized postural features may help to disambiguate user dialogue moves.  1 Introduction Dialogue act classification is concerned with understanding users? communicative intentions as reflected in their utterances. It is an important first step toward building automated dialogue systems. To date, the majority of work on dialogue act 
modeling has addressed spoken dialogue (Samuel et al, 1998; Stolcke et al, 2000; Surendran and Levow, 2006; Bangalore et al, 2008; Sridhar et al, 2009; Di Eugenio et al, 2010). However, with the increasing popularity of computer-mediated means of conversation, such as instant messaging and social networking services, automated analysis of textual dialogue holds much appeal. Dialogue act modeling for textual conversations has many practical application areas, which include web-based intelligent tutoring systems (Boyer et al, 2010a), chat-based online customer service (Kim et al, 2010), and social media analysis (Joty et al, 2011). Human interaction involves not only verbal communication but also nonverbal communication. Research on nonverbal communication (Knapp and Hall, 2006; Mehrabian, 2007; Russell et al, 2003) has identified a range of nonverbal cues, such as posture, gestures, eye gaze, and facial and vocal expressions. However, the utility of these nonverbal cues has not been fully explored within the context of dialogue act classification research. Previous research has leveraged prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored. As a first step toward a dialogue system that learns its behavior from a human corpus, this paper proposes a novel approach to dialogue act classification that leverages information about users? posture. Posture has been found to be a significant indicator of a broad range of emotions (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Based on the premise that emotion plays an 
247
important role in dialogue, this work hypothesizes that adding posture features will improve the performance of automatic dialogue act models.   The domain considered in this paper is task-oriented textual dialogue collected in a human tutoring study. In contrast to conventional task-oriented dialogue corpora (e.g., Carletta et al, 1997; Jurafsky et al, 1998; Ivanovic, 2008) in which conversational exchanges are carried out within a single channel of dialogue between the dialogue participants, the corpus used in this work utilizes two separate and interleaved streams of communication. One stream is the textual conversation between a student and a tutor (dialogue stream). The other is the student?s problem-solving activity (task stream). As will be described in Section 3, the interface used in the corpus collection was designed to allow the tutor to monitor the student?s problem-solving activities. Thus, the student?s problem-solving activities and the tutor?s monitoring of those activities functioned as an implicit communication channel. This characteristic of the corpus poses significant challenges for dialogue act modeling. First, because the dialogue stream and the task stream are interleaved, the dialogue stream alone may not be coherent. Second, since information can be exchanged implicitly via the task stream, the dialogue likely contains substantial information gaps1. Addressing these challenges, the dialogue act models described in this paper combine three sources of information: the verbal information from the dialogue stream, the task-related context from the task stream, and information about users? posture. This paper makes several contributions to the dialogue research community. First, it is the first effort to explore posture as a nonverbal cue for dialogue act classification. Second, the proposed approach is fully automatic and ready for real-world application. Third, this paper explicitly defines the notion of information gap in task-oriented dialogue consisting of multiple communication channels, which has only begun to be explored in the context of dialogue act classification (Boyer et al, 2010a). Finally, this                                                 1 In this paper, information gap is defined as the information that is missing from the explicit verbal exchanges between the dialogue participants but conveyed by the implicit task stream. 
paper examines adaptability of previous dialogue act classification approaches in conventional task-oriented domains by comparing three classifiers previously applied to dialogue act modeling for task-oriented dialogue. 2 Related Work A rich body of research has addressed data-driven approaches for dialogue act modeling. Russell et al (2003) applied a transformation-based learning approach for dialogue act tagging for spoken dialogue, using speaker direction, punctuation, marks, and cue phrases. Stolcke et al (2000) modeled the structure of dialogue as an HMM, treating the dialogue acts as the observations emitted from the hidden states of the learned HMM. More recently, Bangalore et al (2008) proposed a unified approach to task-oriented dialogue, in which both the user dialogue act classification and the system dialogue act selection were informed by a shared maximum entropy dialogue act classifier. Sridhar et al (2009) also used a maximum entropy model, exploring the utility of different representations of prosodic features. Di Eugenio et al (2010) used a memory-based classifier, in combination with a modified latent semantic analysis (LSA) technique by augmenting the original word-document matrix in LSA with rich linguistic features. While most work on dialogue act modeling has focused on spoken dialogue, a recent line of investigation has explored the analysis of textual conversation, such as asynchronous online chat conversation (Wu et al, 2005; Forsyth, 2007; Reitter et al, 2010; Joty et al, 2011) and synchronous online chat conversation   (Ivanovic, 2008; Kim et al, 2010; Boyer et al, 2010a). Wu et al (2005) proposed a transformation-based learning approach for an asynchronous chat posting domain, utilizing regular expression-based selection rules. For a similar domain, Forsyth (2007) applied neural networks and Na?ve Bayes classification technique using lexical cues. Ritter et al (2010) and Joty et al (2011) applied unsupervised learning approaches to dialogue act modeling for Twitter conversations, in which dialogue acts were automatically discovered by clustering raw utterances. Work by Ivanovic (2008) and Kim et al (2010) analyzed one-to-one synchronous online chat dialogue in a task-oriented 
248
customer service domain. Ivanovic (2008) applied maximum entropy, na?ve Bayes, and support vector machines using word n-gram features. Kim et al (2010) compared the CRF, HMM-SVM, and Na?ve Bayes classifiers using word n-grams and features extracted from the dialogue structure, in which CRF achieved the highest performance. Boyer et al (2010a) investigated dialogue act modeling for task-oriented tutorial dialogue, applying a logistic regression approach using lexical, syntactic, dialogue structure, and task structure features. Some previous dialogue act modeling work (Boyer et al, 2011; Sridhar et al, 2009; Stolcke et al, 2000) leveraged nonverbal information such as prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011). Stolcke et al (2000) combined various prosodic features such as pitch, duration, and energy. Sridhar et al (2009) represented the sequence of prosodic features as n-grams. Boyer et al (2011) leveraged confusion-related facial expressions for tutorial dialogue. Like Boyer et al (2010a), this work addresses dialogue act classification for task-oriented textual conversation in a web-based tutoring domain. In contrast to Boyer et al (2010a), whose approach directly leveraged manually annotated features, making it challenging to apply the proposed model to a real-world system, the present work is fully automatic and ready for real-world application.  A novel feature of this work is its utilization of nonverbal cues carried by users? posture. This is the first dialogue act classification work that leverages posture information. 3 Data The corpus used in this paper consists of textual exchanges between a student and a tutor in a web-based remote-tutoring interface for introductory programming in Java. The corpus was collected from a series of six tutoring lessons, covering progressive topics in computer science over the course of four weeks. The tutoring interface consisted of four windows: a task window displaying the current programming task; a code window in which the student writes Java code; an output window for displaying the result of compiling and running the code; and a chat window for instant exchange of textual dialogue 
between the student and tutor. With this tutoring interface, the student and the tutor were able to exchange textual dialogue and share a synchronized view of the task. Apart from sending dialogue messages, the only action the tutor could perform to affect the student?s interface was advancing to the next programming task.  3.1 Data Collection The data collection conducted in Fall 2011 paired 42 students with one of four tutors for six forty-minute tutoring sessions on introductory computer science topics.  The students were chosen from a first-year engineering course and were pre-screened to filter out those with significant programming experience. The tutors were graduate students with previous tutoring or teaching experience in Java programming. Students were compensated for their participation with partial course credit. The students worked with the same tutor for the entire study. Each lesson consisted of between four and thirteen distinct subtasks. During each tutoring session, the dialogue text exchanged between the student and the tutor was logged to a database. Additional runtime data including content of the student?s Java code, the result (e.g., success or failure) of compiling and running the student?s code, and the IDs of the subtask were logged. All logged data were time-stamped at a millisecond precision. Students? body posture was recorded at a rate of 8 frames per second with a Kinect depth camera, which emits infrared rays to measure distance for each pixel in a depth image frame. The camera was positioned above the student?s computer monitor, ensuring the student?s upper body is centered in the recorded image. Tutors were not recorded. 3.2 Dialogue Act Annotation For the work described in this paper, a subset of the collected data was manually annotated, which include the first of the six tutoring lessons from 21 students. This corpus contains 2564 utterances (1777 tutor, 787 student). The average number of utterances per tutoring session was 122 (min = 74; max = 201). The average number of tutor utterances per session was 84.6 (min = 51; max = 137) and the average number of student utterances per session was 37.4 (min = 22; max = 64). 
249
Extending a previous annotation scheme used for similar task-oriented tutorial dialogue (Boyer et al, 2010b), the scheme used in this work consists of 13 dialogue act tags (Appendix). The dialogue turns that contained more than one dialogue function were segmented into multiple utterances before being assigned a dialogue act tag. The annotation scheme did not constrain any of the dialogue act tags as applying either to students? or tutors? utterances only; however, the resulting distribution of the tags in the annotated corpus show certain dialogue act tags were more relevant to either students? or tutors? utterances. Figure 1 depicts an excerpt from the corpus with the manually applied dialogue act annotations.  
 Three human annotators were trained to apply the scheme. The training consisted of an iterative process involving collaborative and independent tagging, followed by refinements of the tagging protocol. At the initial phase of training, the annotators tagged the corpus collaboratively. In later phases annotators tagged independently. To compute agreement between different annotators, 24% (5 of the 21 sessions) of the corpus were doubly annotated by two annotators. All possible 
pairs of the annotators participated in double annotation. The aggregate agreement was .80 in Cohen?s Kappa (Cohen, 1960). 3.3 Posture Estimation Posture has been found to be a significant indicator of a broad range of emotions such as anxiety, boredom, confusion, engaged concentration (or flow), frustration, and joy (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Early investigations into posture utilized pressure-sensitive chairs which provided indirect measures of upper-body posture (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Newer, computer vision-based techniques provide more detailed postural data (Sanghvi et al, 2011). The present work uses a posture estimation algorithm developed to automatically detect the head, mid torso, and lower torso through depth image recordings of seated individuals (Grafsgaard et al, 2012). With this estimation algorithm, posture is represented as a triple of head depth (distance between camera and head), mid torso depth, and lower torso depth. A dataset of depth camera recordings from the first of the six tutoring lessons consists of 512,977 depth image frames collected across 18.5 hours of computer-mediated human-human tutoring among 33 participants.2 For each depth image frame, the posture algorithm scanned through the three middle regions that corresponded to head, mid-torso, and lower-torso of the recorded person, and selected a single representative depth pixel from each region. The boundaries for each region were heuristically determined relying on the placement of the students? chairs in the middle of the depth recording view at a common distance. Given these constraints, the model was manually verified by two independent human judges to have 95.1% accuracy across 1,109 depth image snapshots corresponding to one-minute intervals across the dataset. The algorithm output for each depth image was labeled as erroneous if either judge found that any of the posture tracking points did not coincide with its target region. Example output of the algorithm is shown in Figure 2.  
                                                2 The other 9 sessions were not successfully recorded because of technical errors. 
Tutor: hang on :) [S] Tutor: When we show you example code, it is not the code you need to write. [S] Tutor: Look at the task again. [H] Student writes programming code Tutor: YUP [PF] Tutor: Perfect [PF] Tutor: OK. Go ahead and test. [DIR] Student: And I don't need anything in the parentheses? [Q] Tutor: Line 9 is correct. You do NOT need anything inside the parentheses. [A] Student: Ok [ACK] Student compiles and runs code successfully Tutor: Good. [PF]  Tutor: Moving on. [S] Tutor advances to the next task. Student writes programming code Tutor: Syntactically correct. But there is a logic error [LF] Tutor: When will the output statement display your request to the player? [Q] Student: AFTER they put in their name [A] Tutor: Exactly [PF] Figure 1. Corpus Excerpt with Dialogue Act Annotation 
250
4 Features For web-based one-to-one dialogue systems, it is important to achieve efficient runtime performance. To maximize real-world feasibility of the learned dialogue act classifiers, this work only considers the features that can be automatically extracted at runtime. In addition, the use of linguistic analysis software, such as a part-of-speech tagger and a syntactic parser, is intentionally restrained. One might argue that rich linguistic analysis may provide additional information to dialogue act classifiers, potentially improving the performance of learned models. However, there is a trade-off between additional information obtained by rich linguistic analysis and processing time. In addition, previous work (Boyer et al, 2010a) found part-of-speech and syntax features did not provide obvious benefit for dialogue act classification in a domain similar to the one considered in this work. The dialogue act classifiers described in this paper integrate four classes of features automatically extracted from three sources of information: the textual dialogue utterances, task-related runtime information logged into the database, and the images of the students recorded by depth cameras. Each feature class is explained in the following subsections. 4.1 Lexical Features Based on previous dialogue act classification research (Bangalore et al, 2008; Boyer et al, 2010a; Kim et al, 2010), this work utilizes word n-grams as features for dialogue act classification. In the experiment reported in Section 5, unigrams and 
bigrams were used. Adding higher order n-grams did not improve model accuracies. In our corpus (Section 3), the nature of the student dialogues is informal and utterances contain many typos. To remove undesirable noise in the data such as typos and rare words, n-grams were filtered out according to their frequency in the training data (i.e., n-grams that appear less than a predefined cutoff threshold in the training data are not included as features). The value of the cutoff threshold was empirically determined by testing the values between 0 and 10 on a development data set that consisted of 20% of randomly selected dialogue sessions. The value of 3 was selected as it yielded the highest classification accuracy. 4.2 Dialogue Context Features While lexical features characterize the intrinsic nature of individual utterances, the context of the utterance within a larger dialogue structure provides additional information about a given utterance in relation with other utterances. This work considers the following dialogue context features: ? Utterance Position: Specifies the relative position of an utterance at a given turn. The value of this feature indicates whether the utterance is the first one in a given turn, the second or later one in a given turn, or the given turn consists of a single utterance. ? Length: Specifies the number of a given utterance in terms of individual word tokens. ? Previous Author: Indicates whether the author of the previous utterance was student or tutor. ? Previous Tutor Dialogue Act: Specifies dialogue act of the most recent tutor utterance. The value of this feature is directly extracted from the manual annotation in the corpus, because in the broader context of our work, tutor dialogue moves will be determined by an external dialogue management module.   4.3 Task Context Features In our data, students? problem-solving activities (e.g., reading the problem description, writing computer programming code, and compiling and running the code) functioned as an implicit communication channel between students and tutors (Section 1). Because of the existence of this 
Figure 2. Automatically detected posture points (H = headDepth, M = midTorsoDepth, L = lowerTorsoDepth)  
 H 
 M  L 
251
implicit communication channel, the dialogue exchanges between students and tutors likely contain substantial information gaps. To overcome such information gaps, it is important to identify effective task context features. The present work leverages the following task context features, which can be automatically extracted during runtime: ? Previous Task Action: Specifies the type of the most recent problem-solving action performed by the student. The value could be message (writing a textual message to the tutor) code (writing code in the code window), or compile_run (compiling or running the code). ? Task Begin Flag: A binary feature that indicates whether a given utterance is the first one since the current problem task was posted.  ? Task Activity Flag: Another binary feature indicating that a given utterance was preceded by a student?s task activity. ? Last Compile/Run Status: Specifies the status (e.g., begin, stop, success, error, input sent) of the most recent compile/run action performed by the students.  In addition to the listed task context features, the utility of time information was also explored, such as the amount of time taken for previous coding activity and the elapsed time since the beginning of the current task. However, these features did not positively impact the performance of the learned models and were thus excluded. 4.4 Posture Features After preprocessing recorded image frames with the estimation algorithm (Section 3.3), students? postures were represented as tuples of three different integer values, each respectively representing head depth, mid torso depth, and lower torso depth. To extract posture features, the time window of n seconds directly preceding a given utterance was compared with the previous time window of the same size in terms of min, max, median, average, and variance of each depth value. The indicators of whether each of these values has increased, decreased or remained the same were considered as potential posture features. To avoid introducing errors to the model by insignificant changes in posture, an error tolerance ?  was allowed (i.e., the two compared postures 
were considered the same unless the amount of the change in the posture was greater than ?). Optimal values for n and ?  were empirically determined, selecting the values that maximized classification accuracy on the development data set. For n, the values between 0 and 60 were compared at an interval of 10. The value of 50 was selected for head depth and 60 for both mid torso depth and lower torso depth.  Similarly, the value of ?  was determined by comparing the values between 0 and 200 with an increment of 10. The selected value was 100.  All the potential posture features were examined in an informal experiment, in which each of the potential posture features were added to the combination of the lexical, the dialogue context, and the task context features. The posture features that improved the classification accuracy after adding them were included in the present dialogue act models. The selected posture features are min of head depth and max, median, and average of lower torso depth. None of the mid torso depth features were selected. 5 Experiment The goal of this experiment is twofold: (1) to evaluate the effectiveness of the feature classes and (2) to compare the performance of three classifiers: maximum entropy (ME), na?ve Bayes (NB), and conditional random field (CRF). These classifiers are chosen because they have been shown effective for dialogue act modeling in traditional task-oriented textual dialogue, in which conversational exchanges were carried out by a single channel of dialogue (Ivanovic, 2008; Kim et al, 2010). Previous result by Kim et al (2010) suggests a structured model such as CRF yields more accurate dialogue act model compared to unstructured models (e.g., Na?ve Bayes), because of its ability to model the sequential patterns in target classification labels. This experiment examines whether a similar finding is observed for our domain, which exhibits substantial information gaps due to the existence of an implicit communication channel, the task stream. 5.1 Dialogue Act Modeling All classifiers were built using the MALLET package (McCallum, 2002). This experiment used the manually annotated portion of the data 
252
described in Section 3. The original dialogue scheme (Section 3.2) was slightly modified by introducing an additional dialogue act, GR, in order to distinguish conventional expressions, such as greetings and thanks, from other information-delivering utterances. For this modified scheme, annotator agreement was 0.81 in Cohen?s Kappa on the doubly annotated portion of the corpus. 6 among the 21 dialogue sessions in the annotated data do not have accompanying images due to technical problems with the depth camera, thus these sessions were excluded from this experiment. Table 1 shows the distribution of the student dialogue act tags in the resulting corpus of 15 dialogues used in this experiment. The most frequent tag was A (answer), followed by ACK (acknowledgement) and Q (question). The features were extracted by aligning three sources of information (the textual dialogue corpus, the task-related runtime log data, and the recorded images) by timestamp. Word boundaries in the dialogue corpus were recognized by the surrounding white spaces and punctuations. The dialogue context features (D) leveraged in this paper includes previous tutor dialogue act. This feature takes the manually annotated value in the corpus, because this work assumes the existence of an external dialogue manager. However, since the external dialogue manager is not likely to achieve 100% accuracy in predicting human tutor dialogue acts, it would be informative to estimate a reasonable range of the accuracies of the student dialogue act model, taking into account the errors introduced by the dialogue manager. For this reason, two versions of the dialogue context features were considered in this experiment: one that leverages the full set of dialogue context features (D) and the other that excludes previous 
tutor dialogue act (D-). These respectively provide the maximum and the minimum expected accuracy of the student dialogue act model, when used with a dialogue manager. The models were trained and tested using five-fold cross validation, in which the 15 dialogue sessions were partitioned into 5 non-overlapping sets of the same size (i.e., 3 sessions per partition). Each set was used for testing exactly once. 5.2 Results Table 2 reports the average classification accuracies from the five-fold cross validation. The majority baseline accuracy for our data is .347, when the classifier always chooses the most frequent dialog act (A). The first group of rows in Table 3 report the accuracies of individual feature classes. All of the individual features performed better than the baseline. The improvement from the baseline was significant except for D- with CRF. The most powerful feature class was dialogue context class when the full set was used. The second group in Table 3 shows the effects of incrementally combining the feature classes. Adding dialogue act features to the lexical features (L + D) brought significant improvement in the classification accuracy for ME and CRF. Adding posture features (L + D + T + P) also improved the accuracy of ME by a statistically significant margin. The last group shows similar results for ME when the previous tutor dialogue act was excluded from the dialogue context, except that the improvement achieved by adding the posture features (L + D- + T + P) was not significant.  
Student Dialogue Act Distribution A (answer) 192 (34.7%) ACK (acknowledgement) 124 (22.4%) Q (question)  92 (16.6%)  S (statement) 76 (13.7%) GR (greeting and thanks) 52 (9.4%) C (clarification) 6 (1.0%) RF (request for feedback) 5 (.9%) RC (request confirmation) 2 (.4%) O (other) 5 (.9%) Total 554 Table 1. Student dialogue acts in the experiment data 
Features ME NB CRF 
 Indiv
idual  Lexical (L)     .696
**     .703**     .599**  Dialogue (D)     .711**     .715**     .696**  Dialogue- (D-)     .477**     .473**     .405  Task (T)     .405**     .396*      .386*  Posture (P)     .382*     .385*     .399* 
 Max  L + D     .772
??     .724     .692??  L + D + T     .777     .729     .694  L + D + T + P     .789?     .714     .682 
 Min  L + D-     .724
??     .681     .606  L + D- + T     .733     .671     .627  L + D- + T + P     .750     .676     .644 Table 2. Classification accuracies (*p < .05, **p < .01 compared to baseline; ??p < .01 compared to L; and ?p < .05 compared to L + D + T, with paired-samples t-test)  
253
The highest accuracy was achieved by ME when using all four classes of the features, with maximum (L + D + T + P) .789 and minimum (L + D- + T + P) .750. For both the maximum and the minimum conditions, the differences among the classifiers were significant (p < .01, one-way repeated measure ANOVA), with post-hoc Tukey HSD tests revealing ME was significantly better than both NB (p < .05) and CRF (p < .01). There was no significant difference between NB and CRF. 6 Discussion The experiment described in Section 5 compared the utility of lexical, dialogue context, task context, and posture features for dialogue act classification. The results indicate the effectiveness of these features. Particularly, adding the dialogue context and the posture features improved the accuracy of the maximum entropy model. Although the margin of improvement achieved by adding posture features was relatively small, the improvement was statistically significant (p < .05) for the maximum condition (L + D + T + P), which suggests that the users? posture during computer-mediated textual dialogue conveys important communicative messages. The experiment also compared three classifiers: maximum entropy, na?ve Bayes, and CRF. Interestingly, CRF was the worst-performing model for our data, contradicting the previous finding by Kim et al (2010), in which CRF (a structured classifier) performed significantly better than Na?ve Bayes (a non-structured classifier). This contradictive result suggests that, in our domain, the presence of an implicit communication channel resulted in substantial information gaps in the dialogue and it poses new challenges that were not encountered by conventional task-oriented domains consisting of a single communication channel.  The maximum entropy classifier achieved the best overall performance, reaching accuracy of .789. This is an encouraging result compared to previous work in a similar domain. Boyer et al (2010a) reported an accuracy of .628 for dialogue act classification in a similar domain. However, a direct comparison is not applicable since different data were used in their work. 
7 Conclusions and Future Work Dialogue act modeling for a task-oriented domain in which the dialogue stream is interleaved with the task stream poses significant challenges. With the goal of effective dialogue act modeling, this work leverages information about users? posture as non-verbal features. An experiment found that posture is a significant indicator of dialogue acts, in addition to lexical features, dialogue context, and task context. The experiment also compared three statistical classifiers: maximum entropy, naive Bayes, and CRF. The best performing model was maximum entropy. Using all features, the maximum entropy achieved .789 in accuracy. Several directions for future work are promising. First, given the encouraging finding that nonverbal information plays a significant role as a communicative means for task-oriented dialogue, various types of non-verbal information can be investigated, such as gesture and facial expressions. Second, incorporating richer task features, such as in our case, deep analysis of student code, may contribute to more accurate dialogue act modeling. Third, it is important to generalize the findings to a larger data set, including across other task-oriented domains.  Finally, the community is embracing a move toward annotation-lean approaches such as unsupervised or semi-supervised learning, which hold great promise for dialogue modeling. Acknowledgments This research was supported by the National Science Foundation under Grant DRL-1007962. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259. Boyer, K. E., Grafsgaard, J. F., Ha, E. Y., Phillips, R., & Lester, J. C. (2011). An affect-enriched dialogue act classification model for task-oriented dialogue. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human 
254
Language Technologies (pp. 1190-1199). Portland, OR. Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010a). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of the 11th Annual SIGDIAL Meeting on Discourse and Dialogue (pp. 297-305). Tokyo, Japan. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M., Vouk, M., & Lester, J. (2010b). Characterizing the effectiveness of tutorial dialogue with hidden markov models. Proceedings of the 10th international conference on Intelligent Tutoring Systems (pp. 55-64). Pittsburgh, PA. Carletta, J., Isard, A., Isard, S., Kowtko, J., Doherty-Sneddon, G., & Anderson, A. (1997). The reliability of a dialogue structure coding scheme. Computational Linguistics, 23, 13?31. Cavicchio, F. (2009). The modulation of cooperation and emotion in dialogue: The REC corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 81 - 87). Suntec, Singapore. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37 - 46. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue act classification, instance-based learning, and higher order dialogue structure. Dialogue and Discourse, 1(2), 81 - 104. D?Mello, S., & Graesser, A. (2010). Mining Bodily Patterns of Affective Experience during Learning. Proceedings of the 3rd International Conference on Educational Data Mining (pp. 31-40). Pittsburgh, PA. Forsyth, E. N. (2007). Improving Automated Lexical and Discourse Analysis of Online Chat Dialog. Master's thesis. Naval Postgraduate School. Grafsgaard, J. F., Boyer, K. E., Wiebe, E. N., & Lester, J. C. (2012). Analyzing Posture and Affect in Task-Oriented Tutoring. Proceedings of the 25th Florida Artificial Intelligence Research Society Conference (pp. 438-443). Marco Island, FL. Ivanovic, E. (2008). Automatic instant messaging dialogue using statistical models and dialogue acts. Master's thesis. The University of Melbourne. Joty, S. R., Carenini, G., & Lin, C.-Y. (2011). Unsupervised Modeling of Dialog Acts in Asynchronous Conversations. Proceedings of the 22nd International Joint Conference on Artificial 
Intelligence (pp. 1807-1813). Barcelona, Catalonia, Spain. Jurafsky, D., Bates, R., Coccaro, N., Martin, R., Meteer, M., Ries, K., Shriberg, E., et al (1998). Switchboard discourse language modeling project report. Baltimore, MD. Kapoor, A., Burleson, W., & Picard, R. W. (2007). Automatic prediction of frustration. International Journal of Human-Computer Studies, 65(8), 724-736. Kim, S. N., Cavedon, L., & Baldwin, T. (2010). Classifying dialogue acts in one-on-one live chats. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 862-871). Cambridge, MA. Knapp, M. L., & Hall, J. A. (2006). Nonverbal Communication in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. Available from  http://mallet.cs.umass.edu Mehrabian, A. (2007). Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. Ritter, A., Cherry, C., & Dolan, B. (2010). Unsupervised modeling of twitter conversations. Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter (pp. 172 - 180). Los Angeles, CA. Russell, J. A., Bachorowski, J. A., & Fernandez-dols, J. M. (2003). Facial and vocal expressions of emotion. Annual Review of Psychology, 54, 329-349. Sanghvi, J., Castellano, G., Leite, I., Pereira, A., McOwan, P. W., & Paiva, A. (2011). Automatic analysis of affective postures and body motion to detect engagement with a game companion. Proceedings of the 6th international conference on Human-robot interaction (pp. 305-312). Lausanne, Switzerland. Sridhar, R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4), 407 - 422. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373. Surendran, D., & Levow, G.-A. (2006). Dialog act tagging with support vector machines and hidden 
255
Markov models. Proceedings of Interspeech (pp. 1950 - 1953). Pittsburgh, PA. Woolf, B., Burleson, W., Arroyo, I., Dragon, T., Cooper, D., & Picard, R. W. (2009). Affect-aware tutors recognising and responding to student affect. International Journal of Learning Technology, 4(3/4), 129-164. Wu, T., Khan, F. M., Fisher, T. A., Shuler, L. A., & Pottenger, W. M. (2005). Posting Act Tagging Using Transformation-Based Learning. In T. Y. Lin, S. Ohsuga, C.-J. Liau, X. Hu, & S. Tsumoto (Eds.), Foundations of Data Mining and knowledge Discovery (pp. 319 - 331). Springer. 
  
Appendix. Dialogue Act Annotation Scheme and Inter-rater Agreement Tag Description Frequency Agreement (k) H  Hint:  The tutor gives advice to help the student proceed with the task Tutor:     Student:     133 0 .50 DIR   Directive:  The tutor explicitly tells the student the next step to take Tutor:     Student:     121 0 .63 ACK   Acknowledgement:  Either the tutor or the student acknowledges previous utterance; conversational grounding Tutor:       Student:  41 175 .73 RC   Request for Confirmation:  Either the tutor or the student requests confirmation from the other participant (e.g., ?Make sense??) Tutor:       Student:  11 2 Insufficient data RF   Request for Feedback:  The student requests an assessment of performance or work from the tutor Tutor:     Student:    0 5 1.0 PF  Positive Feedback:  The tutor provides a positive assessment of the student?s performance Tutor:     Student:     327 0 .90 LF Lukewarm Feedback:  The tutor provides an assessment that has both positive and negative elements Tutor:      Student:    13 0 .80 NF Negative Feedback:  The tutor provides a negative assessment of the student?s performance Tutor:        Student:     1 0 .40 Q Question:  A question regarding the task that is not a direct request for confirmation or feedback Tutor:     Student:  327 120   .95 A Answer:  An answer to an utterance marked Q Tutor:       Student:  96 295 .94 C Correction:  Correction of a typo in a previous utterance Tutor:       Student:  10 6 .54 S  Statement:  A statement regarding the task that does not fit into any of the above categories Tutor:     Student:  681 174 .71 O Other: Other utterances, usually containing only affective content Tutor:     Student:  6 10 .69 
256
Proceedings of the SIGDIAL 2013 Conference, pages 204?213,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Learning Dialogue Management Models for Task-Oriented Dialogue with Parallel Dialogue and Task Streams 
 Eun Young Ha, Christopher M. Mitchell, Kristy Elizabeth Boyer,  and James C. Lester Department of Computer Science North Carolina State University Raleigh, NC 27695, USA {eha,cmmitch2,keboyer,lester}@ncsu.edu     Abstract 
Learning dialogue management models poses significant challenges. In a complex task-oriented domain in which information is ex-changed via parallel, interleaved dialogue and task streams, effective dialogue management models should be able to make dialogue moves based on both the dialogue and the task context. This paper presents a data-driven ap-proach to learning dialogue management mod-els that determine when to make dialogue moves to assist users? task completion activi-ties, as well as the type of dialogue move that should be selected for a given user interaction context. Combining features automatically ex-tracted from the dialogue and the task, we compare two alternate modeling approaches. The results of an evaluation indicate the learned models are effective in predicting both the timing and the type of system dialogue moves. 1 Introduction Automated dialogue systems allow users to in-teract with information systems in a natural and intuitive manner. With the growth of speech-enabled applications for mobile devices, the de-mands for practical dialogue systems have been increasing at an accelerating pace. The core tasks of automated dialogue systems include dialogue management, which is concerned with selecting system actions in response to a given user input. Traditionally, dialogue managers have been manually constructed. However, manually craft-ing dialogue managers is labor-intensive and yields systems that are brittle with respect to un-expected user behaviors. For rapid creation of robust and adaptive dialogue systems, data-driven approaches to dialogue management hold 
much appeal. Recent work on dialogue systems has explored machine learning techniques to au-tomatically learn dialogue managers from corpo-ra (Scheffler and Young, 2002; Hardy et al, 2006; Williams and Young, 2007; Bangalore et al, 2008; Sridhar et al, 2009). To support more natural human-computer dia-logue, earlier work on dialogue systems envi-sioned rich interaction environments that take into account observed user actions for selecting optimal dialogue strategies (Carberry, 1990; Rich and Sidner, 1998; Allen et al, 2001). However, recent data-driven approaches have primarily focused on application domains in which infor-mation between the user and the system are communicated solely by dialogue, such as tele-phone-based systems (Hardy et al, 2006; Bangalore et al, 2008) and online chat dialogues (Ivanovic, 2008; Kim et al, 2010). With increas-ing demands for natural human-computer inter-action beyond these restricted application do-mains, dialogue systems are required to support more complex types of interaction, in which us-ers perform tasks in parallel to exchanging dia-logue. For instance, dialogue interfaces for task-assistance systems, such as intelligent tutoring systems, should be able to monitor users? task completion activities and incorporate the ob-served activities into dialogue management deci-sions such that the systems can provide users with spontaneous assistance (e.g., providing hints) even without an explicit request from the user.  We have been exploring data-driven ap-proaches for a complex task-oriented application domain in which information is delivered both by exchanging dialogue with users and by ob-serving users? task completion activities. Our previous work has focused on the automatic in-terpretation of user dialogue input (Boyer et al, 
204
2010; Ha et al, 2012). Findings suggest that identifying an effective representation to com-bine information from dialogue and users? task completion activities is key to effective dialogue processing in a domain consisting of parallel dia-logue and task streams. As the next step in this line of investigation on complex task-oriented domains with parallel dia-logue and task streams, this work proposes an approach to automatically learning dialogue management models from a human dialogue cor-pus. The proposed approach combines infor-mation from a dialogue stream and a task stream in order to create spontaneous dialogue interven-tions for users based on monitoring users? activi-ties. Two subtasks of dialogue management are addressed: the first is to determine when to pro-vide dialogue feedback (timing), and the second is to determine what kind of dialogue feedback to provide (type). Dialogue managers in conven-tional domains have primarily focused on the selection of feedback type. However, determin-ing the appropriate timing of system moves is critical for dialogue systems that support parallel dialogue and task streams.  The work presented here makes three contri-butions. First, it endeavors to expand data-driven dialogue management by addressing more com-plex task-oriented domains consisting of parallel dialogue and task streams. Second, it proposes a timing intervention model that determines the correct time to make spontaneous system inter-ventions. Third, it presents a maximum entropy dialogue management model and compares al-ternate approaches. It also compares the predic-tive power of the dialogue and task streams on the targeted dialogue management tasks. 2 Related Work Data-driven approaches to dialogue management continue to be the subject of increasing attention within the dialogue community. Prominent among these are reinforcement learning ap-proaches for learning dialogue policies from cor-pora (Henderson et al, 2008; Levin et al, 2000; Lewis and Di Fabbrizio, 2006; Roy et al, 2000; Scheffler and Young, 2002; Singh et al, 2002; Williams and Young, 2007; Young, 2002). These approaches model dialogue as Markov decision processes, either fully observable (MDPs) or par-tially observable (POMDPs), in which the transi-tions of dialogue states are associated with sys-tem actions and rewards. The goal of reinforce-ment learning is to learn optimal policies that 
maximize aggregate expected rewards, such as user satisfaction (Walker et al, 1997). Learned policies that result from RL exploration do not, by design, necessarily reflect the patterns in the bootstrap dialogue corpus. Additionally, to cover all possible state spaces, reinforcement learning typically requires a very large set of training da-ta, which limits the complexity of the dialogue system in its representation of the dialogue states and the system actions (Young et al, 2013).  A second body of related work focuses on dia-logue act classification. Classification-based ap-proaches aim at learning the patterns of dialogue that are present in the corpus. A variety of ma-chine learning frameworks have been exploited, including hidden Markov models (Stolcke et al, 2000; Boyer et al,  2010), maximum entropy models (Bangalore et al, 2008; Sridhar et al, 2009; Ha et al, 2012), support vector machines (Ivanovic, 2008), conditional random fields (Kim et al, 2010),  and memory-based classifiers in combination with latent semantic analysis (Di Eugenio et al, 2010). Classification-based ap-proaches incorporate rich sets of features, includ-ing not only lexical information, syntactic fea-tures, and dialogue structure, but also prosodic features in the case of spoken dialogue (Stolcke et al, 2000; Sridhar et al, 2009) and non-verbal features such as facial expressions (Boyer et al, 2011) and shifts in posture (Ha et al, 2012). While most work on dialogue act classifica-tion has focused on either offline analysis of dia-logue (Stolcke et al, 2000; Ivanovic, 2008; Kim et al, 2010; Di Eugenio et al, 2010) or interpre-tation of user dialogue (Boyer et al, 2010; Ha et al, 2012), Bangalore et al (2008) utilized dia-logue act classification as a mechanism for de-termining system dialogue moves. They pro-posed a unified dialogue act classification ap-proach for both the interpretation of user utter-ances and selection of system dialogue moves. Our work is similar to Bangalore et al (2008) in that it takes a dialogue act classification ap-proach to the task of selecting system dialogue moves. However, it addresses the problems posed by complex task-oriented application do-mains in which information is communicated not only by dialogue exchanges but also by monitor-ing users? task performance. In such domains, a user?s task activities constitute a full communica-tive stream in its own right, separate from the dialogue stream. The challenges of parallel dia-logue and task streams are addressed by exploit-ing automatically obtained task features com-bined with dialogue features. In contrast to pre-
205
vious work (Bangalore et al 2008, Boyer et al, 2010), in which task information was derived from manual annotation, our work utilizes auto-matically computed task features. Our work also focuses on a growing applica-tion area of dialogue systems: intelligent tutor-ing. In support of student learning, recent work in this area utilized human tutorial dialogue cor-pora to learn effective tutorial strategies using MDPs (Chi et al, 2010; Mitchell et al, 2013), to develop tutorial dialogue models that adapt to students? affective states (Forbes-Riley and Litman, 2011), and to improve robustness of a symbolic tutorial dialogue system (Dzikovska et al, 2013).  3 Task-Oriented Dialogue Corpus To learn dialogue management models from nat-urally occurring human-to-human dialogue we utilize a human tutorial dialogue corpus we col-lected in the domain of introductory program-ming in Java. The corpus consists of textual dia-logue exchanges between students and tutors in a web-based remote-tutoring interface, aligned with task context logs (Appendix A). A subset of the corpus was annotated with dialogue acts, which was used to train and test the dialogue management models described in this paper. 3.1 Human tutoring study The data collection study involved forty-two un-dergraduate students who were paired with one of four tutors. The students were enrolled in a first-year engineering course and were pre-screened to filter out those with significant pro-gramming experience. The students were com-pensated for their participation with partial course credit. The tutors were graduate students with previous tutoring or teaching experience in Java programming, and the students worked with the same tutor for the entire study. Each lesson consisted of between four and thirteen distinct subtasks.  The students completed six forty-minute tutor-ing lessons, covering progressive topics in intro-ductory computer science over four weeks. Each lesson consisted of four to thirteen subtasks, in which later subtasks built upon earlier ones. Dur-ing each tutoring session, the paired student and tutor interacted remotely using a web-based tu-toring interface. With this tutoring interface, the student and the tutor were able to exchange tex-tual dialogue and share a synchronized view of the task.  
For each lesson, students completed a pre-test and a post-test before and after the main tutoring session. The pre- and post-test consisted of the same set of questions to assess students? knowledge related to the lesson?s objectives. Compared to students? pre-test results, signifi-cant learning gains were observed on the post-test, which indicates that the tutorial dialogue was effective for student learning (Mitchell et al, 2012).  3.2 Dialogue annotation A subset of the collected data was manually an-notated with dialogue acts using an annotation scheme consisting of 13 dialogue act tags for task-oriented tutorial dialogue (Table 1). The annotated corpus consists of the first of the six tutoring lessons from 21 students, which contains 2564 utterances (1777 tutor, 787 student). The average number of utterances per tutoring ses-sion was 122 (min = 74; max = 201). The aver-age number of tutor utterances per session was 84.6 (min = 51; max = 137), and the average number of student utterances per session was 37.4 (min = 22; max = 64). Three human annotators were trained to apply the scheme. The training consisted of an iterative process involving collaborative and independent tagging, followed by refinements of the tagging protocol. At the initial phase of training, the an-notators tagged the corpus collaboratively. In later phases annotators tagged independently. To compute agreement between different annotators, 24% (5 of the 21 sessions) of the corpus were doubly annotated by two annotators. All possible pairs of the annotators participated in double an-notation. The aggregate agreement was 0.80 in Cohen?s Kappa (Cohen, 1960). 4 Dialogue Management Models To support a task-oriented dialogue system capa-ble of not only responding to users? dialogue in-put but also providing spontaneous system inter-vention during users? task activities, a dialogue manager should provide two functionalities. The first is to determine the timing of a system dia-logue move (i.e., whether or not to provide a tu-torial dialogue move at a given context). The second is to determine the type of dialogue move (i.e., selecting from available system dialogue acts). In this work, the problem of determining the system?s next dialogue move is cast as a clas-sification task. In previous work we found a maximum entropy approach was effective for 
206
classifying user dialogue acts for task-oriented dialogue with parallel dialogue and task streams (Ha, 2012). Maximum entropy outperformed both Naive Bayes and conditional random fields. Building on these results, we employ a maximum entropy classifier to learn dialogue management models that predict both the timing and the type of the system dialogue move. The following sec-tions describe two alternate approaches to dia-logue management that can both determine the timing and determine the type of system dialogue interventions.  4.1 One-step dialogue management model In the first model, the two dialogue management tasks are framed as a single classification prob-lem by treating the decision of not to make a tu-torial dialogue move as a special dialogue act. Thus, a finite set of dialogue moves allowed for the system is defined as ? = ??,??,? ,?? , in which ? = ?? ? ? ?????? and ?? ={???, ???,? , ???}  is the set of dialogue acts available for the system. Given ?  and the ??? step in a given user interaction history ????? =  ????? , ??????,? , ??, the goal of the dia-logue management model is to predict system?s dialogue move ???? for the next step, which is determined by the following equation. ???? = ?????????? ? ?????              (1) 
The task-oriented dialogue considered in this work includes two parallel and interleaved data streams: an explicit dialogue stream, consisting of textual exchanges between a student and a tutor, and an implicit task stream, consisting of the student?s problem-solving activities. Thus, a given interaction history can be decomposed into a dialogue history and a task history, rewriting equation (1) as follows, ???? = ?????????? ? ????? , ?????     (2) in which ????? = ???? , ??????,? , ??  and ????? = ???? , ??????,? , ??  represent the history of dialogue utterances and the history of student task activities, respectively. In this work, the conditional probability distri-bution in Equation (2) is estimated using the maximum entropy framework (Berger et al, 1996). The maximum entropy framework selects a probability distribution that results in the high-est entropy among all possible solutions. Given a vector ? of feature set, the conditional probabil-ity distribution is estimated by the following equation, ? ? = ?? ? =  ? ??(?) ??????                     (3) in which ? represents weights and ? is a normal-izing factor. This work used MALLET 
Tag Description Agreement H Hint: The tutor gives advice to help the student proceed with the task .50 DIR  Directive: The tutor explicitly tells the student the next step to take .63 ACK  Acknowledgement: Either the tutor or the student acknowledges previous utterance; conversational grounding .73 RC  Request for Confirmation: Either the tutor or the student requests confirmation from the other participant (e.g., ?Make sense??) Insufficient data RF  Request for Feedback: The student requests an assessment of his performance or his work from the tutor 1.0 PF  Positive Feedback: The tutor provides a positive assessment of the student?s perfor-mance .90 LF Lukewarm Feedback: The tutor provides an assessment that has both positive and nega-tive elements .80 NF Negative Feedback: The tutor provides a negative assessment of the student?s perfor-mance .40 Q Question: A question which does not fit into any of the above categories .95 A Answer: An answer to an utterance marked Q .94 C Correction: Correction of a typo in a previous utterance .54 S  Statement: A statement which does not fit into any of the above categories .71 O Other: Other utterances, usually containing only affective content .69 Table 1.  Dialogue act annotation scheme and inter-rater agreement 
207
(McCallum, 2002) to estimate this conditional distribution.  4.2 Two-step dialogue management model A potential shortcoming of the one-step model is that the probability distribution over dialogue acts is prone to distortion depending on the por-tion of NoMove in the training data. To avoid this, the second model takes a two-step approach, treating each dialogue management task as an independent classification task. The two-step model first determines whether or not to make a dialogue move. If a decision is made to provide a dialogue move, the second classifier is called for a selection of the type of dialogue move.  In this model, system?s dialogue move ???? for the next interaction step is determined by a function ? ????? , such that ? ????? = ??????, ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? (4) when  ?? ?????? ????? > ? ???? ?????                               ? ????? = ??????? ? ? ? ?? ?????       (5) otherwise.  Similar to the one-step model, Equation (5) can be written as ? ????? = ??????? ? ? ? ?? ????? , ?????  (6) This conditional probability distribution is also estimated by the maximum entropy framework. 5 Features To learn high-performing dialogue management models for task-oriented dialogue with parallel dialogue and task streams, it is crucial to have an effective representation of user interaction state that captures information from all available data streams. The dialogue management models de-scribed in the previous section determine the sys-tem?s next dialogue move based on user interac-tion state specified by the features extracted from the dialogue and the task streams. In contrast to previous work on task-oriented dialogue, in which task information is incorporated into dia-logue utterances by manual tagging (Bangalore et al, 2008; Boyer et al, 2010), our work does not require manual effort to obtain the relevant task information. Instead, we rely on task context logs generated during students? interactions with the tutoring interface, as well as a notion of stu-dents? task progress automatically estimated by a task analysis algorithm. The same set of features 
is used for the prediction of both the timing and the type of system move. 5.1 Automatic task analysis In order to provide a measure of students? task progress through each of the tutoring sessions, an edit distance metric was implemented. This met-ric computes the minimum edit distance between a student?s program at a particular time t and a representative solution for a given programming task, in order to estimate how far away the stu-dent is from completing the task. Because our tutors were experienced in the subject matter and were familiar with the lesson structures, we can safely assume that they knew what this final state of the code would be and thus had an intuitive metric of student progress, which is analogous to our edit distance metric. As this value changes over a session, one can observe how the stu-dent?s progress is affected by tutor dialogue acts. Because a character-based edit distance would not capture the relative functional importance of each part of the student?s program, our edit dis-tance metric is based on tokenized versions of the program, as generated by the Java compiler, and the output is the number of tokens that differ from the solution for that task. In this way, comments, variable names, or string literals with many characters are all treated as single tokens and do not artificially inflate the edit distance. This tokenization also allows for abstraction of these comments, variable names, and string liter-als into generalized tokens so that they can be more easily compared between students.  5.2 Dialogue features Previous work on dialogue act classification has shown that lexical features extracted from dia-logue utterances are good predictors of dialogue acts (Bangalore et al, 2008; Boyer et al, 2010a; Kim et al, 2010). However, this finding does not apply when the goal of dialogue act classification is to learn dialogue management models because determining system moves precedes system ut-terance generation. Instead, this work exploits features that represent local interaction structure within dialogue streams, which includes current student dialogue act, current tutor dialogue act, previous tutor dialogue act, and tutor utterance count. ? Current student dialogue act represents the interpreted dialogue act for the previ-ous user dialogue input. Student dialogue act interpretation is not addressed in this 
208
paper, assuming the existence of an exter-nal module that carries out user dialogue interpretation (e.g., Ha et al, 2012). ? Current tutor dialogue act represents the type of system dialogue act at the current interaction step. In our tutorial dialogue corpus, we observed tutors often made several dialogue utterances in succession, such as a feedback (?Great Job.?) fol-lowed by a question (?Do you have any questions??). Thus, the value of the cur-rent tutor dialogue act impacts the proba-bility distribution over the tutor?s next dia-logue move. This feature captures such temporal patterns present in tutor dialogue moves as observed in the corpus. ? Previous tutor dialogue act represents the type of system dialogue act generated for the previous interaction step. This is simi-lar to the current tutor dialogue act fea-ture, but models longer temporal patterns by extending the size of interaction history. ?  Tutor utterance count represents the number of system dialogue acts generated in succession without interruption until the current interaction step. In our corpus, it was observed that the tutor dialogue turns often consist of multiple utterances. This feature is included to model system dia-logue turns consisting of multiple utteranc-es. 5.3 Task features To create a rich representation of task context, a number of features were automatically extracted from task streams. Three groups of task infor-mation were considered, including types of task activity taken by user, the amount of time taken between certain activities, and the user?s task progress estimated by the task analysis algorithm (Section 5.1). Alternate representations of these features were empirically compared, resulting in the following task features incorporated in cur-rent dialogue management models. ? Current log type represents the type of activity taken at the current interaction step either by the user or the system. A complete list of log types is shown in Appendix B.  ? Previous log type represents the type of activity taken at the previous interaction step. Analogous to previous tutor dia-logue act in dialogue stream, this feature 
models temporal patterns among task ac-tivities. ? Same log type is a binary feature indi-cating the type of activities at the current and previous interaction step is identical.  ? Previous and current log type is a fea-ture that combines the current and previ-ous log types (i.e., a bigram of log types). ? Elapsed time is the amount of time since the last logged activity, which rep-resents the duration of the user?s inac-tivity. This feature is included to enable the learned dialogue management model to make spontaneous dialogue interven-tions when a user has been detected to be inactive for an excessive period of time.  ? Elapsed coding time specifies the amount of time the user has taken since the beginning of current coding task.  6 Evaluation The dialogue act models were trained and tested using the manually annotated portion of the task-oriented tutorial dialogue corpus described in Section 3. The textual dialogue exchanges in the corpus were aligned with the logged task-completion activities based on the timestamp, resulting in 6919 total interaction logs. Table 2 shows the distribution of different types of ac-tivities in the resulting interaction logs. It was observed that tutors made a dialogue move in 26.5% of the total logged interactions (Table 3). 
Among the thirteen dialogue acts in the origi-nal annotation scheme (Section 3.2), four rarely occurring dialogue acts were combined into other categories, which include LF (lukewarm feed-back) merged with NF (negative feedback) and RC (request for confirmation), RF (request for feedback), and C (correction) merged to O (oth-er).  A new category, GREET (greetings) was 
Interaction Type Frequency (%) Programming 38.2 Compiling the Program 10.8 Running the Program 12.2 Progressing to Next Task 4.2 Exchanging Dialogue 34.6 Table 2. Distribution of interaction types 
Tutor Dialogue Move Frequency (%) Move 26.5 NoMove 73.5 Table 3. Distribution of system dialogue move 
209
added to distinguish conventional expressions for greetings and thanks from more general state-ments and questions. Table 4 shows the resulting distribution of tutor dialogue acts in the corpus. 
The performance of the dialogue act models were evaluated in a ten-fold cross validation. In the cross validation, the corpus was partitioned to ten non-overlapping sets and each set was used as testing data exactly once, while models were trained using the remaining nine sets. 6.1 Results The first study compared the accuracies of the dialogue management models on predicting the timing and the type of tutor dialogue moves. The accuracy of the timing prediction was calculated for all user interaction logs in the data, including both dialogue exchanges and task-completion activities. The accuracy of the type prediction was calculated for dialogue activities by tutors only. The results are shown in Table 5. 
Both the one-step (t(9) = 4.14, p = 0.0013) and the two-step (t(9) = 6.26, p < .0001) models per-formed significantly better than the majority baseline in predicting the timing of tutorial dia-logue moves. The two-step model achieved higher accuracy than the one-step model. The difference between the two models was statisti-cally significant (t(9) = 2.17, p = 0.0291).  The one-step (t(9) = 2.68, p = 0.0126) and the two-step (t(9) = 10.93, p < 0.0001) models 
achieved significantly higher accuracies over the baseline for the task of predicting the type of tu-torial dialogue moves, as well. Again, the two-step model performed significantly better than the one-step model (t(9) = 4.22, p = .0011).  6.2 Comparing dialogue and task streams The second study compared the predictive power of the dialogue stream and the task stream on the given two dialogue management tasks. In this study, the accuracy of the two-step model was compared in three conditions: using the dialogue features only (Dialogue), using the task features only (Task), and using all features (All). Table 6 reports the results. 
For determining when to intervene, the dia-logue and the task features exhibited similar pre-dictive power. No statistical significance was found for the difference between the dialogue and the task conditions. The highest accuracy was achieved by the All condition. Compared to the All condition, the Dialogue condition showed statistically significant decrease in accuracy (t(9) = 2.21, p = 0.0272), which implies the task stream provided important features for the dia-logue management model in determining the tim-ing of tutorial dialogue moves. A similar trend was observed for determining what type of dialogue move to make. The Dia-logue and the Task conditions achieved similar accuracies, with the highest accuracy achieved by the All condition. The drops in accuracy com-pared to the All condition were statistically sig-nificant for both the Dialogue (t(9) = 3.38, p = 0.0040) and the Task conditions. (t(9) = 4.36, p = 0.0009). The results imply that the prediction of the type of tutorial dialogue moves required in-formation from both the dialogue and the task streams.  7 Discussion The experiments presented in Section 6 com-pared two alternate approaches to learning dia-logue management models for two given sub-tasks: determining when to provide the user with a dialogue move, and determining which type of 
Dialogue Act Frequency (%) S (Statement) 35.4 PF (Positive Feedback) 19.8 Q (Question) 16.0 H (Hint)  8.0 DIR (Directive)  6.6 A (Answer)  5.7 GREET (Greetings)  3.1 ACK (Acknowledgement)  2.3 NF (Negative Feedback)  1.5 O (Other)  1.6 Table 4. Distribution of tutor dialogue acts 
Model Timing Type Baseline 73.5 35.4 One-step 79.2* 40.5* Two-step  80.3*?  49.7*? Table 5. Model accuracy (%) on dialogue man-agement tasks (*statistical significance over baseline, ?statistical significance over one-step model) 
Features Timing Type Dialogue 79.6 45.0 Task 80.1 44.9 All  80.3*  49.7*? Table 6. Comparison of features on dialogue management tasks (*statistical significance over Dialogue, ?statistical significance over Task) 
210
dialogue move to choose. The results suggest that the two-step approach, which models the two subtasks as separate classifiers, was more effective than the alternate one-step approach, which combined the two subtasks into a single classification problem. The two-step model achieved higher performance than the one-step model in both the timing and the type prediction. However, the difference in the performance of the two models was more apparent in the type prediction, with the two-step model achieving over 22% higher accuracy than the one-step model. One possible explanation for the superi-ority of the two step-model over the one-step model is that the corpus used to train the models was highly skewed. For more than 73% of the total interaction logs in the corpus, the tutors did not provide any dialogue feedback. Since the one-step model treated NoMove as a special dia-logue act, the skewed distribution over NoMove and Move impacted the learned distribution over dialogue acts.  Two previous investigations reported the accu-racies of dialogue act classification on system utterances. Bangalore et al (2008) reported a prediction accuracy of 55% for system dialogue acts when a flat task model was used in a cata-logue-ordering domain. When a hierarchical task structure was used in the same domain, the achieved prediction accuracy for system dialogue acts was 35.6% (Bangalore and Stent, 2009). Boyer (2010) achieved accuracy of 57% for sys-tem dialogue acts in a task-oriented tutorial dia-logue. While both of these lines of investigation employed task structure features that were manu-ally annotated, our best-performing two-step dia-logue management model resulted in comparable performance utilizing only automatic features, achieving an accuracy of 49.7%. A crucial distinction between user and system dialogue act classification is that lexical features for a given dialogue turn are not available for system dialogue act classification because a sys-tem utterance is generated after a system dia-logue act is selected. The absence of lexical fea-tures poses a significant challenge to system dia-logue act classification, given that lexical fea-tures have been among the most predictive fea-tures for this task. To address this challenge, fu-ture research should continue exploring larger spaces of features to improve prediction accura-cies of learned models. 
8 Conclusions and Future Work Automatically learning dialogue management models for complex task-oriented domains with separate dialogue and task streams poses signifi-cant challenges. Effective dialogue management models in such domains should be able to proac-tively intervene by making spontaneous dialogue moves based on the observed history of both the dialogue and the user?s task activities. With the overarching goal of creating a data-driven auto-mated dialogue system that incorporates parallel dialogue and task streams, this paper has pre-sented classification-based dialogue management models that integrate a rich set of features auto-matically extracted from parallel dialogue and task streams. Two subtasks of dialogue manage-ment were considered: when the system should provide user with a dialogue move and what type of system dialogue act the system should select for a given user interaction context. An evalua-tion found that a two-step approach that modeled the two subtasks as separate classifiers were ef-fective, achieving significantly higher perfor-mance than an alternate approach that modeled the two subtasks with a single classifier. The results suggest several promising direc-tions for future work. First, incorporating richer features may improve the accuracies of learned models, such as more global interaction histories and deeper dialogue structures. Second, develop-ing more sophisticated task analyses will inform the learned models with a representation of the user task context, guiding the models to make more context-appropriate decisions. Finally, it will be important to evaluate the learned models by incorporating them into a dialogue manage-ment system and validating their effectiveness in interactions with users in rich task-oriented dia-logue.  Acknowledgments This research was supported by the National Sci-ence Foundation under Grant DRL-1007962. Any opinions, findings, and conclusions ex-pressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References  Allen, J., Ferguson, G., & Stent, A. (2001). An architecture for more realistic conversational systems. Proceedings of Intelligent User Interfaces (pp. 1?8). Santa Fe, NM. 
211
Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249?1259. Bangalore, S., & Stent, A. J. (2009). Incremental parsing models for dialog task structure. Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (pp. 94?102). Athens, Greece. Berger, A. L., Della Pietra, V. J., & Della Pietra, S. A. (1996). A maximum entropy approach to natural language processing. Computational Linguistics, 22(1), 39?71. Boyer, K. E. (2010). Structural and Dialogue Act Modeling in Task-Oriented Tutorial Dialogue. Ph.D. Dissertation. Department of Computer Science, North Carolina State University. Boyer, K. E., Grafsgaard, J. F., Ha, E. Y., Phillips, R., & Lester, J. C. (2011). An affect-enriched dialogue act classification model for task-oriented dialogue. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (pp. 1190?1199). Portland, OR. Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of the 11th Annual SIGDIAL Meeting on Discourse and Dialogue (pp. 297?305). Tokyo, Japan. Carberry, S. (1991). Plan Recognition in Natural Language Dialogue. MIT Press. Cavicchio, F. (2009). The modulation of cooperation and emotion in dialogue: The REC corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 81?87). Suntec, Singapore. Chi, M., VanLehn, K., Litman, D., & Jordan, P. (2010). Inducing Effective Pedagogical Strategies Using Learning Context Features. Proceedings of the Eighteenth International Conference on User Modeling, Adaptation, and Personalization (pp 147-158). Big Island, HI. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37 ? 46. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue act classification, instance-based learning, and higher order dialogue structure. Dialogue and Discourse, 1(2), 81 ? 104. Dzikovska, M.O., Farrow, E, & Moore, J.D. (2013). Combining deep parsing and classification for improved explanation processing in a tutorial dialogue system. Proceedings of the 16th International Conference on Artificial Intelligence in Education (pp. 279 - 288). Memphis, TN. Forbes-Riley, K. & Litman, D. (2011). Designing and evaluating a wizarded uncertainty-adaptive spoken 
dialogue tutoring system. Computer Speech and Language, 25(1), 105-126. Ha, E. Y., Grafsgaard, J. F., & Mitchell, C. M. (2012). Combining Verbal and Nonverbal Features to Overcome the ?Information Gap? in Task-Oriented Dialogue. Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (pp. 247?256). Seoul, South Korea. Hardy, H., Biermann, A., Inouye, R. B., McKenzie, A., Strzalkowski, T., Ursu, C., Webb, N., et al (2006). The Amiti?s system: Data-driven techniques for automated dialogue. Speech Communication, 48(3-4), 354?373. Henderson, J., Lemon, O., & Georgila, K. (2008). Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4), 487?511. Ivanovic, E. (2008). Automatic instant messaging dialogue using statistical models and dialogue acts. The University of Melbourne. Kim, S. N., Cavedon, L., & Baldwin, T. (2010). Classifying dialogue acts in one-on-one live chats. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 862?871). Cambridge, MA, USA: Association for Computational Linguistics. Levin, E., Pieraccini, R., & Eckert, W. (2000). A Stochastic Model of Human-Machine Interaction for Learning Dialog Strategies. IEEE Transactions on Speech and Audio Processing, 8(1), 11?23. Lewis, C., & Di Fabbrizio, G. (2006). Prompt selection with reinforcement learning in an AT&T call routing application. Proceedings of the Ninth International Conference on Spoken Language Processing (pp. 96?103). Mitchell, C.M., Boyer, K.E., & Lester, J.C. (2013). A Markov Decision Process Model of Tutorial Intervention in Task-Oriented Dialogue.  Proceedings of the International Conference on Artificial Intelligence in Education (pp. 828-831), Memphis, TN. Mitchell, C. M., Ha, E. Y., Boyer, K. E., & Lester, J. C. (2012). Recognizing effective and student-adaptive tutor moves in task-oriented tutorial dialogue. Proceedings of the Intelligent Tutoring Systems Track of the 25th International Conference of the Florida Artificial Intelligence Research Society (pp. 450?455). Rich, C., & Sidner, C. (1998). COLLAGEN: A col-laboration manager for software interface agents. User Modeling and User-Adapted Inter-action, 8(3-4), 315?350. Roy, N., Pineau, J., & Thrun, S. (2000). Spoken dialogue management using probabilistic reasoning. Proceedings of the 38th Annual Meeting on Association for Computational Linguistics (pp. 93?100). Wanchai, Hong Kong. Scheffler, K., & Young, S. (2002). Automatic learning of dialogue strategy using dialogue simulation and reinforcement learning. 
212
Proceedings of the second international conference on Human Language Technology Research (pp. 12?19). San Diego, CA. Singh, S., Litman, D. J., Kearns, M., & Walker, M. (2002). Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, 16, 105?133. Sridhar, R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4), 407 ? 422. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339?373. Walker, M., Litman, D., Kamm, C., & Abella, A. (1997). Paradise: A framework for evaluating 
spoken dialogue agents. Proceedings of ACL (pp. 271?280). Madrid, Spain. Williams, J., & Young, S. (2007). Partially Observable Markov Decision Processes for Spoken Dialog Systems. Computer Speech and Language, 21(2), 393?422. Young, S. (2002). Talking to machines (statistically speaking). Proceedings of ICSLP (pp. 32?41). Denver, CO. Young, S., Gasic, M., Thomson, B., & Williams, J. (2013). POMDP-Based Statistical Spoken Dialog Systems: A Review. Proceedings of the IEEE, 101(5), 1160?1179.    
Appendix A. An Excerpt from the Task-Oriented Dialogue Corpus Lesson ID Task ID Role Type Text Timestamp 1 4 STUDENT CODING System.out.printIn("Hello World" 2011-09-21 08:17:17.737 1 4 STUDENT CODING System.out.printIn("Hello World") 2011-09-21 08:17:19.407 1 4 STUDENT CODING System.out.printIn("Hello World"); 2011-09-21 08:17:19.812 1 4 TUTOR MESSAGE good. 2011-09-21 08:17:24.913 1 4 TUTOR MESSAGE also you can try to compile at anytime. 2011-09-21 08:17:33.805 1 4 STUDENT COMPILE_ BEGIN studentCode\jt101\JavaTutor3.java 2011-09-21 08:17:38.080 1 4 STUDENT COMPILE_ ERROR line 1  : cannot find symbol symbol  : method printIn(java.lang.String) location: class java.io.PrintStream System.out.printIn("Hello World");           ^ 1 error 
2011-09-21 08:17:38.220 
1 4 TUTOR MESSAGE carefully compare your line with the example 2011-09-21 08:17:57.330 Appendix B.  Types of Activity Logs in Corpus Log Type Description Action Initiator MESSAGE Either student or tutor has sent a chat message. Student, Tutor SESSION_PROGRESS Tutor has allowed student to progress to next task. Tutor CODING Student has written programming code. Student COMPILE_BEGIN Student has begun compiling code. Student COMPILE_SUCCESS Recent code compilation has ended successfully. N/A COMPILE_ERROR Recent code compilation has failed with errors. N/A RUN_BEGIN Student has begun running code. Student INPUT_SENT Student has sent an input to a running code. Student RUN_SUCCESS Recent code running has ended successfully. N/A RUN_STOP Tutor has stopped running student?s code because of errors in the code. Tutor 
213
Proceedings of the SIGDIAL 2013 Conference, pages 339?343,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Evaluating State Representations for Reinforcement  Learning of Turn-Taking Policies in Tutorial Dialogue 
 Christopher M. Mitchell Kristy Elizabeth Boyer James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {cmmitch2, keboyer, lester}@ncsu.edu     Abstract 
Learning and improving natural turn-taking behaviors for dialogue systems is a topic of growing importance. In task-oriented dia-logue where the user can engage in task ac-tions in parallel with dialogue, unrestricted turn taking may be particularly important for dialogue success. This paper presents a novel Markov Decision Process (MDP) representa-tion of dialogue with unrestricted turn taking and a parallel task stream in order to automat-ically learn effective turn-taking policies for a tutorial dialogue system from a corpus. It also presents and evaluates an approach to auto-matically selecting features for an MDP state representation of this dialogue. The results suggest that the MDP formulation and the feature selection framework hold promise for learning effective turn-taking policies in task-oriented dialogue systems. 1 Introduction Determining when to make a dialogue move is a topic of growing importance in dialogue systems. While systems historically relied on explicit turn-taking cues, more recent work has focused on learning and improving on natural turn-taking behaviors (Raux and Eskenazi 2012; Selfridge et al 2012). For tutorial dialogue in particular, ef-fectively timing system moves can substantially impact the success of the dialogue. For example, failing to provide helpful feedback to a student who is confused may lead to decreased learning (Shute 2008) or to disengagement (Forbes-Riley and Litman 2012), while providing tutorial feed-back or interventions at inappropriate times could also have a negative impact on the out-come of the dialogue (D?Mello et al 2010).  Reinforcement Learning (RL) is a widely used approach to constructing effective dialogue poli-
cies using either MDPs or POMDPs (Williams and Young 2007). To date, RL has been applied to learn the most effective dialogue move to make, but has not been applied to learning the timings of these moves, although the related con-cept of when to release a turn has been explored (English and Heeman 2005). The domain of tuto-rial dialogue poses an additional modeling chal-lenge: the dialogue is task-oriented, but unlike many task-oriented dialogues in which all infor-mation is communicated via dialogue, students solve problems within a separate task stream which conveys essential information for dialogue management decisions.  This paper addresses dialogue with both unre-stricted turn taking and a parallel task stream with a novel Markov Decision Process represen-tation. Because turn boundaries are not clearly defined or enforced, we apply RL to the problem of when to make a dialogue move, rather than what type of dialogue move to make. In order to determine which criteria are most relevant to making this decision, the approach utilizes a fea-ture selection approach based on a new Separa-tion Ratio metric and compares the selected fea-tures against an existing approach based on ex-pected cumulative reward (Chi et al 2011). Fi-nally, the resulting feature spaces are evaluated with simulated users acquired in a supervised fashion from held-out portions of the corpus. The results inform the development of turn-taking policies in task-oriented dialogue systems. 2 Corpus The corpus used for this work was collected dur-ing 2011 and 2012 as part of the JavaTutor tuto-rial dialogue project. It consists of 66 textual dia-logues between human tutors and students, with an average of 90 tutor dialogue moves and 36 student dialogue moves. Each pair interacted for through a computer-mediated interface to com-
339
plete introductory computer programming tasks. Students edited their computer programs within a parallel task stream also collected as part of the corpus (see Appendix A). Tutors viewed the task actions synchronously through the interface. The success of each dialogue was measured by learn-ing gain between pretest and posttest. Overall the dialogues were effective; the average learning gain was 42.3% (statistically > 0; p < .0001). The substantial variation in learning gains (min=-28.6%; max= 100%) will be leveraged within the MDP reward structure. 3 MDP Representation A Markov Decision Process (MDP) models a system in which a policy can be learned to max-imize reward (Sutton and Barto 1998). It consists of a set of states S, a set of actions A representing possible actions by an agent, a set of transition probabilities indicating how likely it is for the model to transition to each state s? ? S from each state s ? S when the agent performs each action a ? A in state s, and a reward function R that maps real values onto transitions and/or states, thus signifying their utility.  Previous applications of RL to dialogue sys-tems, using both MDPs and POMDPs, have dealt with the decision of what type of dialogue move to make (Chi et al 2011; Williams and Young 2007). These systems make this decision either at predetermined decision points (Tetreault and Litman 2008), following the trigger of a silence threshold (Raux and Eskenazi 2012), or when the system determines it has enough information to advance the dialogue (Selfridge et al 2012). For the JavaTutor corpus, however, the tutor could choose to make a move at any time. Rather than applying handcrafted rules to determine decision points, we apply RL to learn when to make a dia-logue move in order to maximize the success of the dialogue. For this MDP, the action set is de-fined as A = {TutorMove, NoMove}.  The states for the MDP consist of combina-tions of features representing the current state of the session. The possible features available for selection are described in Table 1, and are all automatically extracted from system logs. The Task Trajectory and Edit Distance features are based on computing a token-level edit distance from a student?s program with respect to that student?s final correct solution. This distance measures a student?s progress over the course of a dialogue while avoiding the need to manually annotate the task stream. In a deployed system, 
this edit distance can be estimated by comparing to previously acquired solutions from other stu-dents.   Feature Description Values Current Action The current action being taken by the student  ? TASK ? STUDENTDIAL ? NOACTION 
Task  Trajectory 
The effect of the last task action on the edit distance to the final task solution 
? CLOSER ? FARTHER ? NOCHANGE 
Last  Action Last turn taken by either interlocutor ? TUTORDIAL ? STUDENTDIAL ? TASK Number of Tutor Moves Number of tutor turns taken thus far in the dialogue ? LOW  (< 30) ? MID   (30-59) ? HIGH (> 60) Edit  Distance The edit distance to the final solution ? LOW  (< 20) ? MID   (20-49) ? HIGH (> 50) Elapsed Idle Time The number of se-conds since the last student action ? LOW  (< 7) ? MID   (7-15) ? HIGH (> 15)  Table 1. Features available to be selected Tutor moves are encoded as MDP actions, while student actions are encoded as transitions to a new state with a NoMove tutor action. To account for the possibility that both interlocutors could construct messages simultaneously or that dialogue and task actions could happen at the same time, the following protocol was applied: if a tutor was making a dialogue move (i.e., typing a message), the state transition accompanying a student action was made after the tutor move was complete, and the student move was associated with that TutorMove action.  Another important consideration for this rep-resentation was how to segment the task stream into discrete actions. Through empirical investi-gation the timeout threshold of 1.5 seconds was selected as a balance between large numbers of successive task events or very few, most of which overlapped with tutor turns.  There were three additional states in the MDP: the Initial state and two final states, FinalHigh and FinalLow, occurring only at the end of a dia-logue and providing rewards of +100 and ?100, respectively. A median split on student learning gains was used to assign each dialogue to either the FinalHigh state or FinalLow state. 4 Feature Selection While retaining all six features would allow for a rich state representation, it would also lead to 
340
issues with sparsity (Singh et al 2002). In fact, nearly 90% of states averaged less than one visit per dialogue when using all six features, leading to inadequate coverage of the state space on which to build reliable MDP policies. This sec-tion compares two methods used to select fea-tures from among the six available. The first approach is based on the Expected Cumulative Reward (ECR) in the initial state, a metric previously used to evaluate state represen-tations for a tutorial dialogue system using RL (Chi et al 2011; Tetreault and  Litman 2008). A higher initial-state ECR indicates a higher proba-bility of achieving a favorable outcome when following a reward-maximizing policy. Maxim-izing ECR has also been the focus of other fea-ture selection approaches for RL (Misu and Kashioka 2012,  Li et al 2009). While initial-state ECR provides a measure of the likelihood of a favorable outcome, it does not address how well a particular state representation captures key decision points. That is, it does not directly represent the extent to which each deci-sion along the path to a successful outcome con-tributed to that outcome, or whether the second-best decision in a particular state would have been equally useful. In order to measure this dif-ference, we introduce the Separation Ratio (SR), which represents how much better a particular policy is compared to its alternatives. SR for a state is calculated by taking the absolute differ-ence between the estimated values of two actions in that state and dividing by the mean of the two values. SR for a policy is the mean of the SRs across all states.  An SR near zero for a state indicates that the decision to take one action over another in that state is likely to have little effect on the final out-come of the dialogue. On the other hand, a high SR indicates a crucial decision point, where tak-ing an off-policy action leads to a much lower probability of a successful outcome. The intui-tion behind this metric is that a state representa-tion that supports policies with high SR high-lights features that are useful in executing an ef-fective turn-taking policy, while a state represen-tation that produces policies with low SR fails to capture this information. Using these two metrics, we evaluated the util-ity of each of the six features. Starting with two empty state representations, one for each metric, a greedy algorithm added one feature at a time to each. That is, at each step for each metric, the feature was added that led to the highest value on the metric when combined with the features al-
ready chosen. For each of the two metrics, we built a state representation and used it as the ba-sis for an MDP. This MDP was then trained with policy iteration (Sutton and Barto 1998), and the two state representations that led to the highest value on each metric were carried over to the next iteration. The goal here is to evaluate the relative utility of each feature, so we continued adding features until they were exhausted, lead-ing to a full ordering of features for each condi-tion (Table 2).   Iteration Initial-State ECR Feature Ordering Mean SR  Feature Ordering 1 Last Action Number of Tutor Moves 2 Task Trajectory Edit Distance 3 Current Action Last Action 4 Elapsed Idle Time Current Action 5 Number of Tutor Moves Elapsed Idle Time 6 Edit Distance Task Trajectory  Table 2. Feature selection using Expected Cumu-lative Reward (ECR) and Separation Ratio (SR) Given the orderings in Table 2, the next step in building a RL system is to decide which iteration of the feature spaces to use. That is, how does a system designer determine when to stop adding features? Previous work (Chi et al 2011; Tetreault and Litman 2008) viewed an absolute increase in the value of initial-state ECR as a signal for the quality of a newly added feature. So, one could say that feature addition should stop if initial-state ECR does not increase be-tween iterations. In the current analysis, howev-er, this would result in termination at the second iteration for the mean SR ordering and termina-tion at the first iteration for the initial-state ECR ordering. These undesirably early terminations most likely occur because the first features se-lected in both orderings represent tutor actions: a tutor can always choose to make a move, thus setting the Last Action feature to TUTORDIAL, and a tutor has direct control over the value of Number of Tutor Moves. This control of features leads to deterministic control of state if the con-text provided by student-driven features is ab-sent. This can allow a policy to remain in the state that maximizes the transition probability to the end state, thus increasing ECR for all states due to deterministic transitions. Therefore, a dif-ferent type of stopping criterion is required. 
341
A stopping criterion must balance two com-peting goals. On the one hand, the size of the state space must be limited to avoid issues with sparsity, as state-action pairs that are not well explored during training might not be assigned values proportional to their expected rewards in a deployed system. On the other hand, a feature space that is too small may not sufficiently repre-sent the possible states of the world, and might fail to capture the criteria most relevant to mak-ing decisions. These competing goals of com-pactness and descriptive power must both be considered when choosing an appropriate feature space for a RL model.  In an attempt to balance these goals, we pro-pose a stopping criterion based on the ratio of states that are sparse states. A sparse state is de-fined as any state that occurs less than once per dialogue on average. A sharp increase in sparse states was observed between the third and fourth iterations for both metrics (15% to 56% for ECR and 26% to 47% for SR), so feature addition stopped at the third iteration. This resulted in only one of the three selected features being shared among the two conditions: the Last Action made by either person (Table 2). In addition, both feature sets include a feature related to the task progress of the student: Task Trajectory for ECR and Edit Distance for SR. The next section reports on an experiment to evaluate these two feature spaces. 5 Evaluation A series of simulated dialogues was used to evaluate the two resulting feature spaces via the policies derived using them. These simulations were based on five-fold cross-validation, as in prior work (Henderson et al 2008), with policies trained on four of the five folds and simulated users learned from the remaining fold. As noted above, the rewards in the MDP were based on student learning gain, but learning gain (like user satisfaction in other dialogue domains) is not directly observable during the dialogues. However, we found that students in the high learning gain group had fewer non-zero task ac-tions (actions that changed the edit distance to the final task solution) than students in the low learning gain group (p < 0.05). Therefore, num-ber of non-zero task actions is used as a measure of dialogue success, with lower numbers being better. We derived the average change in edit distance on each state transition from the testing folds, and defined that a simulated dialogue 
would end when the edit distance reached zero (i.e., the student arrived at the correct solution).  Table 3 shows the results of running 5,000 simulations in each fold for both the learned pol-icy and for an anti-policy where each decision was reversed. The anti-policy is included to pro-vide a point of comparison for the policies learned in each feature space, and offers insight into the quality of the learned policies, similar to the inverse policies learned in prior work (Chi et al 2011). The table shows that the learned poli-cies in the ECR feature space had slightly better results overall (lower number of non-zero task actions), while the SR feature space had larger separation between the learned policies and anti-policies. These results suggest that feature selec-tion based on SR was able to identify important decision criteria with only a minor decrease in reward compared to ECR.    Feature Space Policy Average non-zero task action count ECR Learned policy 43.2 Anti-policy 49.6 SR Learned policy 47.3 Anti-policy 97.4  Table 3. Results of simulated dialogues (lower non-zero task action count is better) 6 Conclusion Modeling unrestricted turn taking within an RL framework, particularly for task-oriented dia-logue with both a dialogue and a parallel task stream, presents numerous challenges. This pa-per has presented a novel representation of such dialogue with a tutoring domain, and has pre-sented and evaluated a feature selection method based on a new Separation Ratio metric, which can inform the development of turn-taking poli-cies in dialogue systems. Future work includes a more fine-grained analysis of the timing of dia-logue moves as well as an evaluation of these results in a deployed system.  Acknowledgements This work is supported in part by the National Science Foundation through Grants DRL-1007962 and CNS-1042468. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation.   
342
References Chi, M., VanLehn, K., Litman, D., and Jordan, P. (2011). An Evaluation of Pedagogical Tutorial Tactics for a Natural Language Tutoring System: a Reinforcement Learning Approach. International Journal of Artificial Intelligence in Educa-tion, 21(1), 83?113. D?Mello, S.K., Olney, A., and Person, N. (2010). Mining Collaborative Patterns in Tutorial Dia-logues. Journal of Educational Data Mining, 2(1), 1?37. English, M.S. and Heeman, P.A. (2005). Learning Mixed Initiative Dialog Strategies By Using Rein-forcement Learning On Both Conversants. In Pro-ceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, 1011?1018. Forbes-Riley, K. and Litman, D.J. (2012). Adapting to Multiple Affective States in Spoken Dialogue. In Proceedings of the 13th Annual SIGDIAL Meeting on Discourse and Dialogue, 217?226. Henderson, J., Lemon, O., and Georgila, K. (2008). Hybrid Reinforcement/Supervised Learning of Dialogue Policies from Fixed Data Sets. Computa-tional Linguistics, 34(4), 487?511. Li, L., Williams, J. D., and Balakrishnan, S. (2009). Reinforcement Learning for Dialog Management Using Least-Squares Policy Iteration and Fast Fea-ture Selection. In Proceedings of the Conference of the International Speech Communication Associa-tion. 2475?2478. Misu, T., and Kashioka, H. (2012). Simultaneous Fea-ture Selection and Parameter Optimization for Training of Dialog Policy by Reinforcement Learn-ing. In Proceedings of the IEEE Workshop on Spo-ken Language Technology, 1?6. Raux, A. and Eskenazi, M. (2012). Optimizing the Turn-Taking Behavior of Task-Oriented Spoken Dialog Systems. Transactions on Speech and Lan-guage Processing, 9(1), 1?23. 
Selfridge, E.O., Arizmendi, I., Heeman, P.A., and Williams, J.D. (2012). Integrating Incremental Speech Recognition and POMDP-based Dialogue Systems. In Proceedings of the 13th Annual SIG-DIAL Meeting on Discourse and Dialogue, 275?279. Shute, V.J. (2008). Focus on Formative Feedback. Review of Educational Research, 78(1), 153?189. Singh, S., Litman, D., Kearns, M., and Walker, M. (2002). Optimizing Dialogue Management with Reinforcement Learning: Experiments with the NJFun System. Journal of Artificial Intelligence Research, 16, 105?133. Sutton, R. and Barto, A. (1998). Reinforcement Learning. MIT Press, Cambridge, MA, 1998. Tetreault, J.R. and Litman, D.J. (2008). A Reinforce-ment Learning Approach to Evaluating State Rep-resentations in Spoken Dialogue Systems. Speech Communication, 50(8), 683?696. Williams, J.D. and Young, S. (2007). Partially Ob-servable Markov Decision Processes for Spoken Dialog Systems. Computer Speech & Language, 21(2), 393?422.   Appendix A. Corpus excerpt 1. Student begins declaring a String variable. 2. Student starts typing a message. 3. Student message: Could I type in String The Adventure Quest; ? or would I need to put in quotes or something? 4. Student resumes working on task. 5. Tutor starts typing a message. 6. Tutor message: TheAdventureQuest is fine 7. Student declares variable called The Adven-ture Quest (Incorrect Java syntax) 8. Tutor starts typing a message. 9. Student catches mistake and renames variable to TheAdventureQuest 10. Tutor message: Can't have spaces :) 11. Tutor starts typing a message 12. Tutor message: Good job   
Appendix B. Dialogue interface 343
Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 61?67,
Baltimore, Maryland USA, June 26, 2014. c?2014 Association for Computational Linguistics
Towards Domain-Independent Assessment of Elementary 
 Students? Science Competency using Soft Cardinality 
Samuel P. Leeman-Munk, Angela Shelton, Eric N. Wiebe, James C. Lester 
North Carolina State University 
Raleigh, North Carolina 27695 
{ spleeman, anshelto, wiebe, lester } @ ncsu.edu 
 
Abstract 
Automated assessment of student learning 
has become the subject of increasing atten-
tion. Students? textual responses to short 
answer questions offer a rich source of data 
for assessment. However, automatically 
analyzing textual constructed responses 
poses significant computational challenges, 
exacerbated by the disfluencies that occur 
prominently in elementary students? writ-
ing. With robust text analytics, there is the 
potential to analyze a student?s text re-
sponses and accurately predict his or her 
future success.  In this paper, we propose 
applying soft cardinality, a technique that 
has shown success grading less disfluent 
student answers, on a corpus of fourth-
grade responses to constructed response 
questions. Based on decomposition of 
words into their constituent character sub-
strings, soft cardinality?s evaluations of re-
sponses written by fourth graders correlates 
with summative analyses of their content 
knowledge.  
1 Introduction 
As a tool for automated assessment, short answer 
questions reveal cognitive processes and states in 
students that are difficult to uncover in multiple-
choice equivalents (Nicol, 2007). Even when it 
seems that items could be designed to address the 
same cognitive construct, success in devising 
multiple-choice and short answer items that be-
have with psychometric equivalence has proven 
to be limited (Kuechler & Simkin, 2010). Be-
cause standards-based STEM education in the 
United States explicitly promotes the develop-
ment of writing skills for which constructed re-
sponse items are ideally suited (NGSS Lead 
States, 2013; Porter, McMaken, Hwang, & Yang, 
2011; Southavilay, Yacef, Reimann, & Calvo, 
2013), the prospect of designing text analytics 
techniques for automatically assessing students? 
textual responses has become even more appeal-
ing (Graesser, 2000; Jordan & Butcher, 2013; 
Labeke, Whitelock, & Field, 2013). 
An important family of short answer questions 
is the constructed response question. A con-
structed response question is designed to elicit a 
response of no more than a few sentences and 
features a relatively clear distinction between 
incorrect, partially correct, and correct answers. 
Ideally, a system designed for constructed re-
sponse analysis (CRA) would be machine-
learned from examples that include both graded 
student answers and expert-constructed ?refer-
ence? answers (Dzikovska, Nielsen, & Brew, 
2012). 
The challenges of creating an accurate ma-
chine-learning-based CRA system stem from the 
variety of ways in which a student can express a 
given concept. In addition to lexical and syntac-
tic variety, students often compose ill-formed 
text replete with ungrammatical phrasings and 
misspellings, which significantly complicate 
analysis. The task of automated grading also be-
comes increasingly difficult as the material grad-
ed comes from questions and domains more and 
more distant from that of human graded respons-
es on which the system is trained, leading to in-
terest in domain-independent CRA systems de-
signed to deal with this challenge (Dzikovska et 
al., 2013). 
In this paper we explore the applications of soft 
cardinality (Jimenez, Becerra, & Gelbukh, 2013), 
an approach to constructed response analysis that 
has shown prior success in domain-independent 
CRA. We investigate whether soft cardinality is 
robust to the disfluency common among elemen-
tary students and whether its analyses of a stu-
dent?s work as she progresses through a prob-
lem-solving session can be used to roughly pre-
dict the content knowledge she will have at the 
end. 
Because like other bag of words techniques, 
soft cardinality is independent of word order, it is 
robust to grammatical disfluencies. What distin-
guishes soft cardinality, however, is its character-
overlap technique, which allows it to evaluate 
word similarity across misspellings. We evaluate 
soft cardinality on a dataset of textual responses 
to short-text science questions collected in a 
61
study conducted at elementary schools in two 
states. Responders were in fourth grade and gen-
erally aged between nine and ten. We train our 
system on student responses to circuits questions 
and test it on two domains in the physical scienc-
es?circuits and magnetism. The results indicate 
that, soft cardinality shows promise as a first step 
for predicting a student?s future success with 
similar content even grading unseen domains in 
the presence of high disfluency. 
This paper is structured as follows. Section 2 
provides related work as a context for our re-
search. Section 3 introduces the corpus, collected 
on tablet-based digital science notebook software 
from elementary students. Section 4 describes 
soft cardinality and an evaluation thereof. Sec-
tion 6 discusses the findings and explores how 
soft cardinality may serve as the basis for future 
approaches to real-time formative assessment. 
2 Related Work 
Short answer assessment is a much-studied area 
that has received increased attention in recent 
years. Disfluency and domain-independence 
have been the beneficiaries of some of this atten-
tion, but cutting edge systems seem to be de-
signed first for correctly spelled in-domain text, 
and then have domain-independence and disflu-
ency management added afterwards.  
For example, one system from Educational 
Testing Services (ETS) uses an approach to do-
main independence called ?domain adaptation? 
(Heilman & Madnani, 2013). Domain adaptation 
generates a copy of a given feature for grading 
answers to seen questions, answers to unseen 
questions in seen domain, and answers to ques-
tions in unseen domains, and each of these has a 
separate weight. An item represented in the train-
ing data uses all three of these feature copies, and 
an item from another domain will only use the 
latter, ?generic? feature copy. 
Spell correction is also often treated as a sepa-
rate issue, handled in the data-cleaning step of a 
CRA system. The common approach at this step 
is to mark words as misspelled if they do not ap-
pear in a dictionary and replace them with their 
most likely alternative. This technique only cor-
rects non-word spelling errors (Leacock & 
Chodorow, 2003). Another approach is to use 
Soundex hashes that translate every word into a 
normalized form based on its pronunciation (Ott, 
Ziai, Hahn, & Meurers, 2013). This second ap-
proach is generally featured alongside a more 
traditional direct comparison. 
The primary limitation of CRA for elementary 
school education is that evaluations of state-of-
the-art systems on raw elementary student re-
sponse data are limited. C-rater provides a small 
evaluation on fourth-grade student math respons-
es, but most evaluation is on seventh, eighth and 
eleventh grade students (Leacock & Chodorow, 
2003; Sukkarieh & Blackmore, 2009). Further-
more, the two datasets presented in SemEval?s 
shared task (Dzikovska et al., 2013) for testing 
and training featured relatively few spelling er-
rors. The BEETLE corpus was drawn from under-
graduate volunteers with a relatively strong 
command of the English language, and the Sci-
EntsBank corpus, which was drawn from 3-6th 
graders, was originally intended for speech and 
as such was manually spell-corrected. The 
Hewlett Foundation?s automated student assess-
ment prize (ASAP) shared task for short answer 
scoring was drawn entirely from tenth grade stu-
dents (Hewlett, 2012).  
3 Corpus 
We have been exploring constructed response 
assessment in the context of science education 
for upper elementary students with the LEONAR-
DO CYBERPAD (Leeman-Munk, Wiebe, & Lester, 
2014). Under development in our laboratory for 
three years, the CYBERPAD is a digital science 
notebook that runs on tablet and web based com-
puting platforms. The CYBERPAD integrates in-
telligent tutoring systems technologies into a dig-
ital science notebook that enables students to 
model science phenomena graphically. With a 
focus on the physical and earth sciences, the LE-
ONARDO PADMATE, a pedagogical agent, sup-
ports students? learning with real-time problem-
solving advice. The CYBERPAD?s curriculum is 
based on that of the Full Option Science System 
(Foss Project, 2013). As students progress 
through the curriculum, they utilize LEONARDO?s 
virtual notebook, complete virtual labs, and write 
responses to constructed response questions. To 
date, the LEONARDO CYBERPAD has been im-
plemented in over 60 classrooms around the 
United States. 
The short answer and pre/post-test data used in 
this investigation were gathered from fourth 
grade students during implementations of The 
CYBERPAD in public schools in California and 
North Carolina. The data collection for each 
class took place over a minimum of five class 
periods with students completing one or more 
new investigations each day. Students completed 
62
investigations in one or both of two modules, 
?Energy and Circuits,? and ?Magnetism.? Most 
questions included ?starter text? that students 
were expected to complete. Students were able to 
modify the starter text in any way including de-
leting or replacing it entirely, although most stu-
dents simply added to the starter text. Example 
answers can be found in a previous work on the 
same dataset (Leeman-Munk et al., 2014). 
Two human graders scored students? responses 
from the circuits module on a science score ru-
bric with three categories: incorrect, partially 
correct, and correct. The graders graded one 
class of data and then conferred on disagreeing 
results. They then graded other classes. On a 
sample of 10% of the responses of the classes 
graded after conferring, graders achieved a Co-
hen?s Kappa of 0.72. 
The graders dealt with considerable disfluency 
in the student responses in the LEONARDO cor-
pus. An analysis of constructed responses in the 
Energy and Circuits module reveals that 4.7% of 
tokens in all of student answers combined are not 
found in a dictionary. This number is higher in 
the Magnetism module, 7.8%. This is in contrast 
to other similar datasets, such as the BEETLE 
corpus of undergraduate text answers to science 
questions, which features a 0.8% rate of out-of-
dictionary words (Dzikovska, Nielsen, & Brew, 
2012). In each case, the numbers underestimate 
overall spelling errors. Misspellings such as ?bat-
ter? for ?battery?, are not counted as missing in a 
dictionary test. These real-word spelling errors 
nevertheless misrepresent a student?s meaning 
and complicate analysis. We describe how soft 
cardinality addresses these issues in Section 4. 
4 Methodology and Evaluation 
Soft cardinality (Jimenez, Becerra, & Gelbukh, 
2013) uses decompositions of words into charac-
ter sequences, known as q-grams, to gauge simi-
larity between two words. We use it here to 
bridge the gap between misspellings of the same 
word. Considering ?dcells? in an example an-
swer, ?mor dcells,? and ?D-cells? in the refer-
ence answer, we can find overlaps in ?ce,? ?el,? 
?ll,? ?ls,? ?ell,? ?lls,? and so on up to and includ-
ing ?cells.? This technique functions equally well 
for real-word spelling errors such as if the stu-
dent had forgotten the ?d? and typed only 
?cells.? Such overlaps signify a close match for 
both of these words. We evaluated the soft cardi-
nality implementation of a generic short answer 
grading framework that we developed, 
WRITEEVAL, based on an answer grading system 
described in an earlier work (Leeman-Munk et 
al., 2014). We used 100-fold cross-validation on 
the ?Energy and Circuits? module. We compare 
WRITEEVAL using soft cardinality to the majority 
class baseline and to WRITEEVAL using Prece-
dent Feature Collection (PFC), a latent semantic 
analysis technique that performs competitively 
with the second highest-scoring system in 
Semeval Task 7 on unseen answers on the Sci-
EntsBank corpus (Dzikovska et al., 2013). Using 
a Kruskal-Wallis test over one hundred folds, 
both systems significantly outperform the base-
line (p<.001), which achieved an accuracy score 
of .61.  We could not evaluate the scores directly 
on the Magnetism dataset as we did not have any 
human-graded gold standard for comparison. 
To evaluate soft cardinality?s robustness to dis-
fluency, we created a duplicate of the Energy and 
Circuits dataset and manually spell-corrected it. 
Table 1 and Figures 1 and 2 show our results. 
Using the Kruskal-Wallis Test, on the uncorrect-
ed data PFC?s accuracy suffered with marginal 
significance (p = .054) while macro-averaged 
precision and recall both suffered significantly (p 
< .01). Soft cardinality suffered much less, with a 
marginally significant decrease in performance 
(p=.075) only in recall. The decreases in accura-
cy and precision had p=.88 and p=.25 respective-
ly. 
To determine the usefulness of automatic grad-
ing of science content in predicting the overall 
trajectory of a student?s performance, we com-
puted a running average of the grades given by 
soft cardinality (converted to ?1?, ?2?, and ?3? for 
incorrect, partially correct, correct) on students? 
answers as they progressed through the Energy 
and Circuits module and the Magnetism module. 
Because we would intend to be able to use this 
technique in a classroom on entirely new ques-
tions and student answers, we use running aver-
age instead of a regression, which would require 
prior data on the questions to determine the 
weights.  
Students completed a multiple-choice test be-
fore and after their interaction with the CYBER-
PAD. The Energy and Circuits module and the 
Magnetism module each had different tests ? 
there were ten questions on the Energy and Cir-
cuits test and twenty on the Magnetism test. We 
calculated the correlation of our running average 
of formative assessments against the student?s 
score on the final test.  
A critical assumption underlying the running 
average is that students answered each question 
63
in order. Although WRITEEVAL does not prevent 
students from answering questions out of order, 
it is organized to strongly encourage linear pro-
gression. 
We excluded empty responses from the running 
average because we did not want an artificial 
boost from simply noting what questions stu-
dents did and did not answer. Data from students 
who did not take the pre or post-test was exclud-
ed, and students missing responses to more than 
twenty out of twenty-nine questions in Mag-
netism or fifteen out of twenty questions in En-
ergy and Circuits were excluded from considera-
tion. After cleaning, our results include 85 stu-
dents in Energy and Circuits and 61 in Mag-
netism.  
Table 1. Accuracy and Macro-Averaged Preci-
sion and Recall for Soft-Cardinality and PFC on 
spell-corrected and uncorrected versions of the 
LEONARDO Energy and Circuits module. 
*marginally significant decrease from spell-
checked 
**significant decrease from spell-checked 
Figure 1 depicts the correlation between the 
running average of automatic scoring by 
WRITEEVAL soft cardinality, PFC, and human 
scores with post-test score on the responses in 
the Energy and Circuits module. When spell-
corrected, the correlation, as shown in Figure 2, 
surprisingly becomes worse.  We discuss a pos-
sible reason for this in the discussion section. 
Figure 3 shows correlation of the running aver-
age of Magnetism?s automatic scores with post-
test. For soft cardinality, significant correlation 
starts five questions in and stays for the rest of 
the 29. As it relies heavily on relevant training 
data, PFC is less stable and does not achieve 
nearly as high a correlation.   
5 Discussion 
The evaluation suggests that a relatively simple 
technique such as soft cardinality, despite per-
forming less well than a domain specific tech-
nique in the presence of relevant training data, is 
more robust to spelling errors and can be far 
more effective at grading questions and domains 
not present in the training data.  
 
 
 
Figure 1. Correlation of grading systems on 
Energy and Circuits with post-test score. Dark-
colored points indicate significant correlation 
(p<.05) 
 
Figure 2. Correlation of grading systems on 
spell-corrected Energy and Circuits with post-
test score. Dark-colored points indicate 
significant correlation (p<.05)  
 
Figure 3. Correlation of the Running Average of 
WRITEEVAL with soft cardinality with post-test 
Scores on the Magnetism module of the LEO-
NARDO corpus. Dark-colored points indicate 
significant correlation (p<.05)  
 
Soft cardinality is representative of the poten-
tial of domain independent, disfluency-robust 
CRA systems.  
The improvement against the gold standard on 
spell-corrected data but loss of correlation 
against the post-test scores suggests that poor 
spelling is a predictor of poor post-test 
0
0.2
0.4
0.6
1 3 5 7 9 11 13 15 17 19
C
o
rr
e
la
ti
o
n
 
Questions Graded 
Human Soft Cardinality
WriteEval PFC
-0.2
0
0.2
0.4
0.6
1 3 5 7 9 11 13 15 17 19
C
o
rr
e
la
ti
o
n
 
Questions Graded 
Human Soft Cardinality
WriteEval PFC
0.1
0.3
0.5
5 7 9 11 13 15 17 19 21 23 25 27 29
C
o
rr
e
la
ti
o
n
 
Questions Graded 
Soft Cardinality WriteEval PFC
Sp.Cr. System Accuracy Precision Recall 
Yes SoftCr .68 .55 .54 
No SoftCr .68 .52 .50* 
Yes PFC .78 .61 .58 
No PFC .74* .54** .52** 
64
knowledge at the end of a task. This could be 
because the students were less able to learn the 
material due to their poor language skills, they 
were less able to complete the test effectively 
despite knowing the material again due to poor 
language skills, or it could be a latent factor that 
affects both the students use of language and 
their eventual circuits knowledge such as en-
gagement. This result shows the challenge of 
separating different skills in evaluating students. 
The significance of soft cardinality?s correla-
tion over the running average for all but the 
eighth question as well as the generally high sig-
nificant correlation achieved in the magnetism 
evaluation indicates the predictive potential of 
soft cardinality. Soft cardinality?s performance in 
Magnetism suggests that with only a relatively 
limited breadth of training examples it can effec-
tively evaluate answers to questions in some un-
seen domains. It is important to note that Energy 
and Circuits and Magnetism are both subjects in 
the physical sciences, and the questions and ref-
erence answers themselves were authored by the 
same individuals. As such this result should not 
be overstated, but is still a promising first step 
towards the goal of domain-independence in 
constructed response analysis.  
6 Conclusion 
This paper presents a novel application of the 
soft cardinality text analytics method to support 
assessment of highly disfluent elementary school 
text. Using q-gram overlap to evaluate word sim-
ilarity across nonstandard spellings, soft cardi-
nality was evaluated on highly disfluent con-
structed response texts composed by fourth grade 
students interacting with a tablet-based digital 
science notebook. The evaluation included an in-
domain training corpus and another out-of-
domain corpus. The results of the evaluation 
suggest that soft cardinality generates assess-
ments that are predictive of students? post-test 
performance even in highly disfluent out-of-
domain corpora. It offers the potential to produce 
assessments in real-time that may serve as early 
warning indicators to help teachers support stu-
dent learning.  
Soft cardinality?s current performance levels 
suggest several promising directions for future 
work. First, it will be important to develop tech-
niques to deal with widely varying student re-
sponses without relying directly on training data. 
These techniques will take inspiration in part 
from bag-of-words techniques such as soft cardi-
nality and Precedent Feature Collection, but will 
themselves likely take word order into account as 
there is a sizeable subset of answers whose 
meaning is dependent on word order. The use of 
distributional semantics will also be of help in 
resolving similarities between different words. 
Secondly, work should be done to consider an-
swers in more detail than simple assessment of 
correctness. More detailed rubrics such as Task 
7?s 5-way rubric (Dzikovska et al., 2013) would 
allow for more detailed feedback from tutors. 
Further, detailed analysis of individual under-
standings and misconceptions within answers 
would be even more helpful, and will be the fo-
cus of future work. Third, it will be instructive to 
incorporate the WRITEEVAL framework into the 
LEONARDO CYBERPAD digital science notebook 
to investigate techniques for classroom-based 
formative assessment that artfully utilize both 
intelligent support by the PADMATE onboard 
intelligent tutor and personalized support by the 
teacher. Finally, it will be important to to inves-
tigate additional techniques to evaluate student 
answers more accurately using less training data 
from more distant domains.  
Reliable analysis of constructed response items 
not only provides additional summative analysis 
of writing ability in science, but also gives the 
teacher a powerful formative assessment tool that 
can be used to guide instructional strategies at 
either the individual student or whole class level. 
Given that time for science instruction is limited 
at the elementary level, the use of real-time as-
sessment to address student misconceptions or 
missing knowledge immediately can be an inval-
uable classroom tool. 
7 Acknowledgements 
The authors wish to thank our colleagues on the 
LEONARDO team for their contributions to the 
design, development, and classroom implementa-
tions of LEONARDO: Courtney Behrle, Mike 
Carter, Bradford Mott, Peter Andrew Smith, and 
Robert Taylor. This material is based upon work 
supported by the National Science Foundation 
under Grant No. DRL1020229. Any opinions, 
findings, and conclusions or recommendations 
expressed in this material are those of the authors 
and do not necessarily reflect the views of the 
National Science Foundation. 
  
65
References 
Dzikovska, M., Brew, C., Clark, P., Nielsen, R. D., 
Leacock, C., Mcgraw-hill, C. T. B., & 
Bentivogli, L. (2013). SemEval-2013 Task 7?: 
The joint student response analysis and 8th 
recognizing textual entailment challenge. In 
Second Joint Conference on Lexical and 
Computational Semantics (*SEM), Volume 2: 
Proceedings of the Seventh International 
Workshop on Semantic Evaluation (SemEval 
2013) (Vol. 2, pp. 263?274). 
Dzikovska, M., Nielsen, R., & Brew, C. (2012). 
Towards effective tutorial feedback for 
explanation questions: A dataset and baselines. 
In Proceedings of the 2012 Conference of the 
North American Chapter of the Association for 
Computational Linguistics: Human Language 
Technologies (pp. 200?210). Montreal, Canada. 
Retrieved from 
http://dl.acm.org/citation.cfm?id=2382057 
Foss Project. (2013). Welcome to FossWeb. Retrieved 
October 20, 2013, from 
http://www.fossweb.com/ 
Graesser, A. (2000). Using latent semantic analysis to 
evaluate the contributions of students in 
AutoTutor. Interactive Learning Environments, 
8(2), 1?33. Retrieved from 
http://www.tandfonline.com/doi/full/10.1076/10
49-4820(200008)8%3A2%3B1-B%3BFT129 
Heilman, M., & Madnani, N. (2013). ETS?: Domain 
adaptation and stacking for short answer 
scoring. In Second Joint Conference on Lexical 
and Computational Semantics (*SEM), Volume 
2: Proceedings of the Seventh International 
Workshop on Semantic Evaluation (SemEval 
2013) (Vol. 1, pp. 96?102). 
Hewlett, W. (2012). The Hewlett Foundation: Short 
answer scoring. Retrieved March 16, 2014, 
from https://www.kaggle.com/c/asap-
sas/data?Data_Set_Descriptions.zip 
Jimenez, S., Becerra, C., & Gelbukh, A. (2013). 
SOFTCARDINALITY: hierarchical text 
overlap for student response analysis. In Second 
Joint Conference on Lexical and Computational 
Semantics (*SEM), Volume 2: Proceedings of 
the Seventh International Workshop on 
Semantic Evaluation (SemEval 2013) (Vol. 2, 
pp. 280?284). Retrieved from 
http://www.gelbukh.com/CV/Publications/2013
/SOFTCARDINALITY Hierarchical Text 
Overlap for Student Response Analysis.pdf 
Jordan, S., & Butcher, P. (2013). Does the Sun orbit 
the Earth?? Challenges in using short free-text 
computer-marked questions . In Proceedings of 
HEA STEM Annual Learning and Teaching 
Conference 2013: Where Practice and 
Pedagogy Meet. Birmingham, UK. 
Kuechler, W., & Simkin, M. (2010). Why is 
performance on multiple-choice tests and 
constructed-response tests not more closely 
related? Theory and an empirical test. Decision 
Sciences Journal of Innovative Education, 8(1), 
55?73. Retrieved from 
http://onlinelibrary.wiley.com/doi/10.1111/j.154
0-4609.2009.00243.x/full 
Labeke, N. Van, Whitelock, D., & Field, D. (2013). 
OpenEssayist: extractive summarisation and 
formative assessment of free-text essays. In 
First International Workshop on Discourse-
Centric Learning Analytics. Leuven, Belgium. 
Retrieved from http://oro.open.ac.uk/37548/ 
Leacock, C., & Chodorow, M. (2003). C-rater: 
Automated scoring of short-answer questions. 
Computers and the Humanities, 37(4), 389?405. 
Retrieved from 
http://link.springer.com/article/10.1023/A%3A1
025779619903 
Leeman-Munk, S. P., Wiebe, E. N., & Lester, J. C. 
(2014). Assessing Elementary Students? Science 
Competency with Text Analytics. In 
Proceedings of the Fourth International 
Conference on Learning Analytics & 
Knowledge. Indianapolis, Indiana. 
NGSS Lead States. (2013). Next Generation Science 
Standards: For States, By States. Washington 
DC: National Academic Press. 
Nicol, D. (2007). E-assessment by design: Using 
multiple-choice tests to good effect. Journal of 
Further and Higher Education, 31(1), 53?64. 
Retrieved from 
http://www.tandfonline.com/doi/abs/10.1080/03
098770601167922 
Ott, N., Ziai, R., Hahn, M., & Meurers, D. (2013). 
CoMeT?: Integrating different levels of 
linguistic modeling for meaning assessment. In 
Second Joint Conference on Lexical and 
Computational Semantics (*SEM), Volume 2: 
Proceedings of the Seventh International 
Workshop on Semantic Evaluation (SemEval 
2013) (Vol. 2, pp. 608?616). 
Porter, A., McMaken, J., Hwang, J., & Yang, R. 
(2011). Common core standards the new US 
66
intended curriculum. Educational Researcher, 
40(3), 103?116. Retrieved from 
http://edr.sagepub.com/content/40/3/103.short 
Southavilay, V., Yacef, K., Reimann, P., & Calvo, R. 
A. (2013). Analysis of collaborative writing 
processes using revision maps and probabilistic 
topic models. In Proceedings of the Third 
International Conference on Learning Analytics 
and Knowledge - LAK ?13 (pp. 38?47). New 
York, New York, USA: ACM Press. 
doi:10.1145/2460296.2460307 
Sukkarieh, J., & Blackmore, J. (2009). C-rater: 
Automatic content scoring for short constructed 
responses. Proceedings of the 22nd 
International FLAIRS Conference, 290?295. 
Retrieved from 
http://www.aaai.org/ocs/index.php/FLAIRS/200
9/paper/download/122/302 
 
67
