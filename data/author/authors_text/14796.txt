Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 389?398,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Event Discovery in Social Media Feeds
Edward Benson, Aria Haghighi, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{eob, aria42, regina}@csail.mit.edu
Abstract
We present a novel method for record extrac-
tion from social streams such as Twitter. Un-
like typical extraction setups, these environ-
ments are characterized by short, one sentence
messages with heavily colloquial speech. To
further complicate matters, individual mes-
sages may not express the full relation to be
uncovered, as is often assumed in extraction
tasks. We develop a graphical model that ad-
dresses these problems by learning a latent set
of records and a record-message alignment si-
multaneously; the output of our model is a
set of canonical records, the values of which
are consistent with aligned messages. We
demonstrate that our approach is able to accu-
rately induce event records from Twitter mes-
sages, evaluated against events from a local
city guide. Our method achieves significant
error reduction over baseline methods.1
1 Introduction
We propose a method for discovering event records
from social media feeds such as Twitter. The task
of extracting event properties has been well studied
in the context of formal media (e.g., newswire), but
data sources such as Twitter pose new challenges.
Social media messages are often short, make heavy
use of colloquial language, and require situational
context for interpretation (see examples in Figure 1).
Not all properties of an event may be expressed in
a single message, and the mapping between mes-
sages and canonical event records is not obvious.
1Data and code available at http://groups.csail.
mit.edu/rbg/code/twitter
Carnegie Hall
Artist Venue
Craig Ferguson
DJ Pauly D Terminal 5
Seated at @carnegiehall waiting for @CraigyFerg?s show to begin
RT @leerader : getting REALLY stoked for #CraigyAtCarnegie 
sat night. Craig, , want to join us for dinner at the pub across the 
street? 5pm, be there!
@DJPaulyD absolutely killed it at Terminal 5 last night. 
@DJPaulyD : DJ Pauly D Terminal 5 NYC Insanity ! #ohyeah 
@keadour @kellaferr24
Craig, nice seeing you at #noelnight this weekend @becksdavis!
Twitter Messages
Records
Figure 1: Examples of Twitter messages, along with
automatically extracted records.
These properties of social media streams make exist-
ing extraction techniques significantly less effective.
Despite these challenges, this data exhibits an im-
portant property that makes learning amenable: the
multitude of messages referencing the same event.
Our goal is to induce a comprehensive set of event
records given a seed set of example records, such as
a city event calendar table. While such resources
are widely available online, they are typically high
precision, but low recall. Social media is a natural
place to discover new events missed by curation, but
mentioned online by someone planning to attend.
We formulate our approach as a structured graphi-
cal model which simultaneously analyzes individual
messages, clusters them according to event, and in-
duces a canonical value for each event property. At
the message level, the model relies on a conditional
random field component to extract field values such
389
as location of the event and artist name. We bias lo-
cal decisions made by the CRF to be consistent with
canonical record values, thereby facilitating consis-
tency within an event cluster. We employ a factor-
graph model to capture the interaction between each
of these decisions. Variational inference techniques
allow us to effectively and efficiently make predic-
tions on a large body of messages.
A seed set of example records constitutes our only
source of supervision; we do not observe alignment
between these seed records and individual messages,
nor any message-level field annotation. The output
of our model consists of an event-based clustering of
messages, where each cluster is represented by a sin-
gle multi-field record with a canonical value chosen
for each field.
We apply our technique to construct entertain-
ment event records for the city calendar section of
NYC.com using a stream of Twitter messages. Our
method yields up to a 63% recall against the city
table and up to 85% precision evaluated manually,
significantly outperforming several baselines.
2 Related Work
A large number of information extraction ap-
proaches exploit redundancy in text collections to
improve their accuracy and reduce the need for man-
ually annotated data (Agichtein and Gravano, 2000;
Yangarber et al, 2000; Zhu et al, 2009; Mintz
et al, 2009a; Yao et al, 2010b; Hasegawa et al,
2004; Shinyama and Sekine, 2006). Our work most
closely relates to methods for multi-document infor-
mation extraction which utilize redundancy in in-
put data to increase the accuracy of the extraction
process. For instance, Mann and Yarowsky (2005)
explore methods for fusing extracted information
across multiple documents by performing extraction
on each document independently and then merg-
ing extracted relations by majority vote. This idea
of consensus-based extraction is also central to our
method. However, we incorporate this idea into our
model by simultaneously clustering output and la-
beling documents rather than performing the two
tasks in serial fashion. Another important difference
is inherent in the input data we are processing: it is
not clear a priori which extraction decisions should
agree with each other. Identifying messages that re-
fer to the same event is a large part of our challenge.
Our work also relates to recent approaches for re-
lation extraction with distant supervision (Mintz et
al., 2009b; Bunescu and Mooney, 2007; Yao et al,
2010a). These approaches assume a database and a
collection of documents that verbalize some of the
database relations. In contrast to traditional super-
vised IE approaches, these methods do not assume
that relation instantiations are annotated in the input
documents. For instance, the method of Mintz et al
(2009b) induces the mapping automatically by boot-
strapping from sentences that directly match record
entries. These mappings are used to learn a classi-
fier for relation extraction. Yao et al (2010a) further
refine this approach by constraining predicted rela-
tions to be consistent with entity types assignment.
To capture the complex dependencies among assign-
ments, Yao et al (2010a) use a factor graph repre-
sentation. Despite the apparent similarity in model
structure, the two approaches deal with various types
of uncertainties. The key challenge for our method
is modeling message to record alignment which is
not an issue in the previous set up.
Finally, our work fits into a broader area of
text processing methods designed for social-media
streams. Examples of such approaches include
methods for conversation structure analysis (Ritter
et al, 2010) and exploration of geographic language
variation (Eisenstein et al, 2010) from Twitter mes-
sages. To our knowledge no work has yet addressed
record extraction from this growing corpus.
3 Problem Formulation
Here we describe the key latent and observed ran-
dom variables of our problem. A depiction of all
random variables is given in Figure 2.
Message (x): Each message x is a single posting to
Twitter. We use xj to represent the jth token of x,
and we use x to denote the entire collection of mes-
sages. Messages are always observed during train-
ing and testing.
Record (R): A record is a representation of the
canonical properties of an event. We use Ri to de-
note the ith record and R`i to denote the value of the
`th property of that record. In our experiments, each
record Ri is a tuple ?R1i , R2i ? which represents that
390
Mercury Lounge
Yonder Mountain 
String Band
Craig Ferguson Carnegie Hall
Artist Venue
1
2
k R?k R?+1k
.
.
.
 Really     excited      for    #CraigyAtCarnegie
Seeing     Yonder    Mountain        at             8
@YonderMountain  rocking  Mercury  Lounge
None None None Artist
None NoneArtist Artist None
Venue VenueNoneArtist
xi
yi
xi?1
yi?1
xi+1
yi+1
Ai?1
Ai+1
Ai
Figure 2: The key variables of our model. A collection ofK latent recordsRk, each consisting of a set ofL properties.
In the figure above, R11 =?Craig Ferguson? and R21 =?Carnegie Hall.? Each tweet xi is associated with a labeling
over tokens yi and is aligned to a record via the Ai variable. See Section 3 for further details.
record?s values for the schema ?ARTIST, VENUE?.
Throughout, we assume a known fixed number K
of records R1, . . . , RK , and we use R to denote this
collection of records. For tractability, we consider
a finite number of possibilities for each R`k which
are computed from the input x (see Section 5.1 for
details). Records are observed during training and
latent during testing.
Message Labels (y): We assume that each message
has a sequence labeling, where the labels consist of
the record fields (e.g., ARTIST and VENUE) as well
as a NONE label denoting the token does not corre-
spond to any domain field. Each token xj in a mes-
sage has an associated label yj . Message labels are
always latent during training and testing.
Message to Record Alignment (A): We assume
that each message is aligned to some record such
that the event described in the message is the one
represented by that record. Each message xi is as-
sociated with an alignment variable Ai that takes a
value in {1, . . . ,K}. We use A to denote the set of
alignments across all xi. Multiple messages can and
do align to the same record. As discussed in Sec-
tion 4, our model will encourage tokens associated
with message labels to be ?similar? to corresponding
aligned record values. Alignments are always latent
during training and testing.
4 Model
Our model can be represented as a factor graph
which takes the form,
P (R,A, y|x) ?
(
?
i
?SEQ(xi, yi)
)
(Seq. Labeling)
(
?
`
?UNQ(R`)
)
(Rec. Uniqueness)
?
?
?
i,`
?POP (xi, yi, R`Ai)
?
? (Term Popularity)
(
?
i
?CON (xi, yi, RAi)
)
(Rec. Consistency)
where R` denotes the sequence R`1, . . . , R`K of
record values for a particular domain field `. Each
of the potentials takes a standard log-linear form:
?(z) = ?T f(z)
where ? are potential-specific parameters and f(?)
is a potential-specific feature function. We describe
each potential separately below.
4.1 Sequence Labeling Factor
The sequence labeling factor is similar to a standard
sequence CRF (Lafferty et al, 2001), where the po-
tential over a message label sequence decomposes
391
XiYi
?SEQ
R?k
R?k+1
R?k?1?UNQ ?th field(across records)
?
?POP
R?k
Ai
Yi
Xi
Ai
Yi
Xi
R?k R?+1k
?CON
k
kth record
Figure 3: Factor graph representation of our model. Circles represent variables and squares represent factors. For
readability, we depict the graph broken out as a set of templates; the full graph is the combination of these factor
templates applied to each variable. See Section 4 for further details.
over pairwise cliques:
?SEQ(x, y) = exp{?TSEQfSEQ(x, y)}
=exp
?
?
?
?TSEQ
?
j
fSEQ(x, yj , yj+1)
?
?
?
This factor is meant to encode the typical message
contexts in which fields are evoked (e.g. going to see
X tonight). Many of the features characterize how
likely a given token label, such as ARTIST, is for a
given position in the message sequence conditioning
arbitrarily on message text context.
The feature function fSEQ(x, y) for this compo-
nent encodes each token?s identity; word shape2;
whether that token matches a set of regular expres-
sions encoding common emoticons, time references,
and venue types; and whether the token matches a
bag of words observed in artist names (scraped from
Wikipedia; 21,475 distinct tokens from 22,833 dis-
tinct names) or a bag of words observed in New
York City venue names (scraped from NYC.com;
304 distinct tokens from 169 distinct names).3 The
only edge feature is label-to-label.
4.2 Record Uniqueness Factor
One challenge with Twitter is the so-called echo
chamber effect: when a topic becomes popular, or
?trends,? it quickly dominates the conversation on-
line. As a result some events may have only a few
referent messages while other more popular events
may have thousands or more. In such a circum-
stance, the messages for a popular event may collect
to form multiple identical record clusters. Since we
2e.g.: xxx, XXX, Xxx, or other
3These are just features, not a filter; we are free to extract
any artist or venue regardless of their inclusion in this list.
fix the number of records learned, such behavior in-
hibits the discovery of less talked-about events. In-
stead, we would rather have just two records: one
with two aligned messages and another with thou-
sands. To encourage this outcome, we introduce a
potential that rewards fields for being unique across
records.
The uniqueness potential ?UNQ(R`) encodes the
preference that each of the values R`, . . . , R`K for
each field ` do not overlap textually. This factor fac-
torizes over pairs of records:
?UNQ(R`) =
?
k 6=k?
?UNQ(R`k, R`k?)
where R`k and R`k? are the values of field ` for two
records Rk and Rk? . The potential over this pair of
values is given by:
?UNQ(R`k, R`k?) = exp{??TSIMfSIM (R`k, R`k?)}
where fSIM is computes the likeness of the two val-
ues at the token level:
fSIM (R`k, R`k?) = |R
`
k ?R`k? |
max(|R`k|, |R`k? |)
This uniqueness potential does not encode any
preference for record values; it simply encourages
each field ` to be distinct across records.
4.3 Term Popularity Factor
The term popularity factor ?POP is the first of two
factors that guide the clustering of messages. Be-
cause speech on Twitter is colloquial, we would like
these clusters to be amenable to many variations of
the canonical record properties that are ultimately
learned. The ?POP factor accomplishes this by rep-
resenting a lenient compatibility score between a
392
message x, its labels y, and some candidate value
v for a record field (e.g., Dave Matthews Band).
This factor decomposes over tokens, and we align
each token xj with the best matching token vk in v
(e.g., Dave). The token level sum is scaled by the
length of the record value being matched to avoid a
preference for long field values.
?POP (x, y,R`A = v) =
?
j
max
k
?POP (xj , yj , R`A = vk)
|v|
This token-level component may be thought of as
a compatibility score between the labeled token xj
and the record field assignment R`A = v. Given that
token xj aligns with the token vk, the token-level
component returns the sum of three parts, subject to
the constraint that yj = `:
? IDF (xj)I[xj = vk], an equality indicator be-
tween tokens xj and vk, scaled by the inverse
document frequency of xj
? ?IDF (xj) (I[xj?1 = vk?1] + I[xj+1 = vk+1]),
a small bonus of ? = 0.3 for matches on adja-
cent tokens, scaled by the IDF of xj
? I[xj = vk and x contains v]/|v|, a bonus for a
complete string match, scaled by the size of the
value. This is equivalent to this token?s contri-
bution to a complete-match bonus.
4.4 Record Consistency Factor
While the uniqueness factor discourages a flood of
messages for a single event from clustering into mul-
tiple event records, we also wish to discourage mes-
sages from multiple events from clustering into the
same record. When such a situation occurs, the
model may either resolve it by changing inconsis-
tent token labelings to the NONE label or by reas-
signing some of the messages to a new cluster. We
encourage the latter solution with a record consis-
tency factor ?CON .
The record consistency factor is an indicator func-
tion on the field values of a record being present and
labeled correctly in a message. While the popular-
ity factor encourages agreement on a per-label basis,
this factor influences the joint behavior of message
labels to agree with the aligned record. For a given
record, message, and labeling, ?CON (x, y,RA) = 1
if ?POP (x, y,R`A) > 0 for all `, and 0 otherwise.
4.5 Parameter Learning
The weights of the CRF component of our model,
?SEQ, are the only weights learned at training time,
using a distant supervision process described in Sec-
tion 6. The weights of the remaining three factors
were hand-tuned4 using our training data set.
5 Inference
Our goal is to predict a set of records R. Ideally we
would like to compute P (R|x), marginalizing out
the nuisance variables A and y. We approximate
this posterior using variational inference.5 Con-
cretely, we approximate the full posterior over latent
variables using a mean-field factorization:
P (R,A,y|x) ? Q(R,A,y)
=
(
K?
k=1
?
`
q(R`k)
)(
n?
i=1
q(Ai)q(yi)
)
where each variational factor q(?) represents an ap-
proximation of that variable?s posterior given ob-
served random variables. The variational distribu-
tion Q(?) makes the (incorrect) assumption that the
posteriors amongst factors are independent. The
goal of variational inference is to set factors q(?) to
optimize the variational objective:
min
Q(?)
KL(Q(R,A,y)?P (R,A,y|x))
We optimize this objective using coordinate descent
on the q(?) factors. For instance, for the case of q(yi)
the update takes the form:
q(yi)? EQ/q(yi) logP (R,A,y|x)
where Q/q(yi) denotes the expectation under all
variables except yi. When computing a mean field
update, we only need to consider the potentials in-
volving that variable. The complete updates for each
of the kinds of variables (y, A, andR`) can be found
in Figure 4. We briefly describe the computations
involved with each update.
q(y) update: The q(y) update for a single mes-
sage yields an implicit expression in terms of pair-
wise cliques in y. We can compute arbitrary
4Their values are: ?UNQ = ?10, ?PhrasePOP = 5, ?
Token
POP = 10,
?CON = 2e8
5See Liang and Klein (2007) for an overview of variational
techniques.
393
Message labeling update:
ln q(y) ?
{
EQ/q(y) ln?SEQ(x, y) + ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]}
= ln?SEQ(x, y) + EQ/q(y) ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]
= ln?SEQ(x, y) +
?
z,v,`
q(A = z)q(yj = `)q(R`z = v) ln
[
?POP (x, y,R`z = v)?CON (x, y,R`z = v)
]
Mention record alignment update:
ln q(A = z) ? EQ/q(A)
{
ln?SEQ(x, y) + ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]}
? EQ/q(A)
{
ln
[
?POP (x, y,R`A)?CON (x, y,RA)
]}
= ?
z,v,`
q(R`z = v)
{
ln
[
?POP (x, y,R`z = v)?CON (x, y,R`z = v)
]}
= ?
z,v,`
q(R`z = v)q(yji = `) ln
[
?POP (x, y,R`z = v)?CON (x, y,R`z = v)
]
Record Field update:
ln q(R`k = v) ? EQ/q(R`k)
{
?
k?
ln?UNQ(R`k? , v) +
?
i
ln [?POP (xi, yi, v)?CON (xi, yi, v)]
}
= ?
k? 6=k,v?
(
q(R`k? = v?) ln?UNQ(v, v?)
+?
i
q(Ai = k)
?
j
q(yji = `) ln
[
?POP (x, y,R`z = v, j)?CON (x, y,R`z = v, j)
])
Figure 4: The variational mean-field updates used during inference (see Section 5). Inference consists of performing
updates for each of the three kinds of latent variables: message labels (y), record alignments (A), and record field
values (R`). All are relatively cheap to compute except for the record field update q(R`k) which requires looping
potentially over all messages. Note that at inference time all parameters are fixed and so we only need to perform
updates for latent variable factors.
marginals for this distribution by using the forwards-
backwards algorithm on the potentials defined in
the update. Therefore computing the q(y) update
amounts to re-running forward backwards on the
message where there is an expected potential term
which involves the belief over other variables. Note
that the popularity and consensus potentials (?POP
and ?CON ) decompose over individual message to-
kens so this can be tractably computed.
q(A) update: The update for individual record
alignment reduces to being log-proportional to the
expected popularity and consensus potentials.
q(R`k) update: The update for the record field
distribution is the most complex factor of the three.
It requires computing expected similarity with other
record field values (the ?UNQ potential) and looping
over all messages to accumulate a contribution from
each, weighted by the probability that it is aligned to
the target record.
5.1 Initializing Factors
Since a uniform initialization of all factors is a
saddle-point of the objective, we opt to initialize
the q(y) factors with the marginals obtained using
just the CRF parameters, accomplished by running
forwards-backwards on all messages using only the
394
?SEQ potentials. The q(R) factors are initialized
randomly and then biased with the output of our
baseline model. The q(A) factor is initialized to uni-
form plus a small amount of noise.
To simplify inference, we pre-compute a finite set
of values that each R`k is allowed to take, condi-
tioned on the corpus. To do so, we run the CRF
component of our model (?SEQ) over the corpus and
extract, for each `, all spans that have a token-level
probability of being labeled ` greater than ? = 0.1.
We further filter this set down to only values that oc-
cur at least twice in the corpus.
This simplification introduces sparsity that we
take advantage of during inference to speed perfor-
mance. Because each term in ?POP and ?CON in-
cludes an indicator function based on a token match
between a field-value and a message, knowing the
possible values v of each R`k enables us to precom-
pute the combinations of (x, `, v) for which nonzero
factor values are possible. For each such tuple, we
can also precompute the best alignment position k
for each token xj .
6 Evaluation Setup
Data We apply our approach to construct a database
of concerts in New York City. We used Twitter?s
public API to collect roughly 4.7 Million tweets
across three weekends that we subsequently filter
down to 5,800 messages. The messages have an av-
erage length of 18 tokens, and the corpus vocabu-
lary comprises 468,000 unique words6. We obtain
labeled gold records using data scraped from the
NYC.com music event guide; totaling 110 extracted
records. Each gold record had two fields of interest:
ARTIST and VENUE.
The first weekend of data (messages and events)
was used for training and the second two weekends
were used for testing.
Preprocessing Only a small fraction of Twitter mes-
sages are relevant to the target extraction task. Di-
rectly processing the raw unfiltered stream would
prohibitively increase computational costs and make
learning more difficult due to the noise inherent in
the data. To focus our efforts on the promising por-
tion of the stream, we perform two types of filter-
6Only considering English tweets and not counting user
names (so-called -mentions.)
ing. First, we only retain tweets whose authors list
some variant of New York as their location in their
profile. Second, we employ a MIRA-based binary
classifier (Ritter et al, 2010) to predict whether a
message mentions a concert event. After training on
2,000 hand-annotated tweets, this classifier achieves
an F1 of 46.9 (precision of 35.0 and recall of 71.0)
when tested on 300 messages. While the two-stage
filtering does not fully eliminate noise in the input
stream, it greatly reduces the presence of irrelevant
messages to a manageable 5,800 messages without
filtering too many ?signal? tweets.
We also filter our gold record set to include only
records in which each field value occurs at least once
somewhere in the corpus, as these are the records
which are possible to learn given the input. This
yields 11 training and 31 testing records.
Training The first weekend of data (2,184 messages
and 11 records after preprocessing) is used for train-
ing. As mentioned in Section 4, the only learned pa-
rameters in our model are those associated with the
sequence labeling factor ?SEQ. While it is possi-
ble to train these parameters via direct annotation of
messages with label sequences, we opted instead to
use a simple approach where message tokens from
the training weekend are labeled via their intersec-
tion with gold records, often called ?distant super-
vision? (Mintz et al, 2009b). Concretely, we auto-
matically label message tokens in the training cor-
pus with either the ARTIST or VENUE label if they
belonged to a sequence that matched a gold record
field, and with NONE otherwise. This is the only use
that is made of the gold records throughout training.
?SEQ parameters are trained using this labeling with
a standard conditional likelihood objective.
Testing The two weekends of data used for test-
ing totaled 3,662 tweets after preprocessing and 31
gold records for evaluation. The two weekends were
tested separately and their results were aggregated
across weekends.
Our model assumes a fixed number of records
K = 130.7 We rank these records according to
a heuristic ranking function that favors the unique-
ness of a record?s field values across the set and the
number of messages in the testing corpus that have
7Chosen based on the training set
395
0.2	 ?
0.25	 ?
0.3	 ?
0.35	 ?
0.4	 ?
0.45	 ?
0.5	 ?
0.55	 ?
0.6	 ?
0.65	 ?
0.7	 ?
1.00	 ? 1.5	 ? 2	 ? 2.5	 ? 3	 ? 3.5	 ? 4	 ? 4.5	 ? 5	 ?
Re
ca
ll	 ?a
ga
ins
t	 ?G
old
	 ?Ev
en
t	 ?R
ec
or
ds
	 ?
k,	 ?as	 ?a	 ?mul?ple	 ?of	 ?the	 ?number	 ?of	 ?gold	 ?records	 ?
Low	 ?Thresh	 ? CRF	 ? List	 ? Our	 ?Work	 ?
Figure 5: Recall against the gold records. The horizontal
axis is the number of records kept from the ranked model
output, as a multiple of the number of golds. The CRF
lines terminate because of low record yield.
token overlap with these values. This ranking func-
tion is intended to push garbage collection records
to the bottom of the list. Finally, we retain the top k
records, throwing away the rest. Results in Section
7 are reported as a function of this k.
Baseline We compare our system against three base-
lines that employ a voting methodology similar to
Mann and Yarowsky (2005). The baselines label
each message and then extract one record for each
combination of labeled phrases. Each extraction is
considered a vote for that record?s existence, and
these votes are aggregated across all messages.
Our List Baseline labels messages by finding
string overlaps against a list of musical artists and
venues scraped from web data (the same lists used as
features in our CRF component). The CRF Baseline
is most similar to Mann and Yarowsky (2005)?s CRF
Voting method and uses the maximum likelihood
CRF labeling of each message. The Low Thresh-
old Baseline generates all possible records from la-
belings with a token-level likelihood greater than
? = 0.1. The output of these baselines is a set of
records ranked by the number of votes cast for each,
and we perform our evaluation against the top k of
these records.
7 Evaluation
The evaluation of record construction is challeng-
ing because many induced music events discussed
in Twitter messages are not in our gold data set; our
gold records are precise but incomplete. Because
of this, we evaluate recall and precision separately.
Both evaluations are performed using hard zero-one
loss at record level. This is a harsh evaluation crite-
rion, but it is realistic for real-world use.
Recall We evaluate recall, shown in Figure 5,
against the gold event records for each weekend.
This shows how well our model could do at replac-
ing the a city event guide, providing Twitter users
chat about events taking place.
We perform our evaluation by taking the top
k records induced, performing a stable marriage
matching against the gold records, and then evalu-
ating the resulting matched pairs. Stable marriage
matching is a widely used approach that finds a bi-
partite matching between two groups such that no
pairing exists in which both participants would pre-
fer some other pairing (Irving et al, 1987). With
our hard loss function and no duplicate gold records,
this amounts to the standard recall calculation. We
choose this bipartite matching technique because it
generalizes nicely to allow for other forms of loss
calculation (such as token-level loss).
Precision To evaluate precision we assembled a list
of the distinct records produced by all models and
then manually determined if each record was cor-
rect. This determination was made blind to which
model produced the record. We then used this ag-
gregate list of correct records to measure precision
for each individual model, shown in Figure 6.
By construction, our baselines incorporate a hard
constraint that each relation learned must be ex-
pressed in entirety in at least one message. Our
model only incorporates a soft version of this con-
straint via the ?CON factor, but this constraint
clearly has the ability to boost precision. To show
it?s effect, we additionally evaluate our model, la-
beled Our Work + Con, with this constraint applied
in hard form as an output filter.
The downward trend in precision that can be seen
in Figure 6 is the effect of our ranking algorithm,
which attempts to push garbage collection records
towards the bottom of the record list. As we incor-
porate these records, precision drops. These lines
trend up for two of the baselines because the rank-
396
0.2	 ?
0.3	 ?
0.4	 ?
0.5	 ?
0.6	 ?
0.7	 ?
0.8	 ?
0.9	 ?
10	 ? 20	 ? 30	 ? 40	 ? 50	 ?
Pr
ec
isi
on
	 ?(M
an
ua
l	 ?E
ve
lua
?o
n)	 ?
Number	 ?of	 ?Records	 ?Kept	 ?
Low	 ?Thresh	 ? CRF	 ? List	 ? Our	 ?Work	 ? Our	 ?Work	 ?+	 ?Con	 ?
Figure 6: Precision, evaluated manually by cross-
referencing model output with event mentions in the in-
put data. The CRF and hard-constrained consensus lines
terminate because of low record yield.
ing heuristic is not as effective for them.
These graphs confirm our hypothesis that we gain
significant benefit by intertwining constraints on ex-
traction consistency in the learning process, rather
than only using this constraint to filter output.
7.1 Analysis
One persistent problem is a popular phrase appear-
ing in many records, such as the value ?New York?
filling many ARTIST slots. The uniqueness factor
?UNQ helps control this behavior, but it is a rela-
tively blunt instrument. Ideally, our model would
learn, for each field `, the degree to which dupli-
cate values are permitted. It is also possible that by
learning, rather than hand-tuning, the ?CON , ?POP ,
and ?UNQ parameters, our model could find a bal-
ance that permits the proper level of duplication for
a particular domain.
Other errors can be explained by the lack of con-
stituent features in our model, such as the selection
of VENUE values that do not correspond to noun
phrases. Further, semantic features could help avoid
learning syntactically plausible artists like ?Screw
the Rain? because of the message:
Screw the rainArtist! Grab an umbrella and head down to
Webster HallVenue for some American rock and roll.
Our model?s soft string comparison-based clus-
tering can be seen at work when our model uncov-
ers records that would have been impossible without
this approach. One such example is correcting the
misspelling of venue names (e.g. Terminal Five ?
Terminal 5) even when no message about the event
spells the venue correctly.
Still, the clustering can introduce errors by com-
bining messages that provide orthogonal field con-
tributions yet have overlapping tokens (thus escap-
ing the penalty of the consistency factor). An exam-
ple of two messages participating in this scenario is
shown below; the shared term ?holiday? in the sec-
ond message gets relabeled as ARTIST:
Come check out the holiday cheerArtist parkside is bursting..
Pls tune in to TV Guide NetworkVenue TONIGHT at 8 pm
for 25 Most Hilarious Holiday TV Moments...
While our experiments utilized binary relations,
we believe our general approach should be useful for
n-ary relation recovery in the social media domain.
Because short messages are unlikely to express high
arity relations completely, tying extraction and clus-
tering seems an intuitive solution. In such a sce-
nario, the record consistency constraints imposed by
our model would have to be relaxed, perhaps exam-
ining pairwise argument consistency instead.
8 Conclusion
We presented a novel model for record extraction
from social media streams such as Twitter. Our
model operates on a noisy feed of data and extracts
canonical records of events by aggregating informa-
tion across multiple messages. Despite the noise
of irrelevant messages and the relatively colloquial
nature of message language, we are able to extract
records with relatively high accuracy. There is still
much room for improvement using a broader array
of features on factors.
9 Acknowledgements
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C-0172. Any opin-
ions, findings, and conclusions expressed in this ma-
terial are those of the author(s) and do not necessar-
ily reflect the views of DARPA, AFRL, or the US
government. Thanks also to Tal Wagner for his de-
velopment assistance and the MIT NLP group for
their helpful comments.
397
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of DL.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In Proceedings of the ACL.
J Eisenstein, B O?Connor, and N Smith. . . . 2010. A
latent variable model for geographic lexical variation.
Proceedings of the 2010 . . . , Jan.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL.
Robert W. Irving, Paul Leather, and Dan Gusfield. 1987.
An efficient algorithm for the optimal stable marriage.
J. ACM, 34:532?543, July.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of International Conference of Machine
Learning (ICML), pages 282?289.
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tutorial).
In Association for Computational Linguistics (ACL).
Gideon S. Mann and David Yarowsky. 2005. Multi-field
information extraction and cross-document fusion. In
Proceeding of the ACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009a. Distant supervision for relation extraction
without labeled data. In Proceedings of ACL/IJCNLP.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009b. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the ACL,
pages 1003?1011.
A Ritter, C Cherry, and B Dolan. 2010. Unsupervised
modeling of twitter conversations. Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 172?180.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of HLT/NAACL.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition of
domain knowledge for information extraction. In Pro-
ceedings of COLING.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010a. Collective cross-document relation extraction
without labelled data. In Proceedings of the EMNLP,
pages 1013?1023.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010b. Cross-document relation extraction without la-
belled data. In Proceedings of EMNLP.
Jun Zhu, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW.
398
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 530?540,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
In-domain Relation Discovery with Meta-constraints
via Posterior Regularization
Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{harr, eob, tahira, regina} @csail.mit.edu
Abstract
We present a novel approach to discovering re-
lations and their instantiations from a collec-
tion of documents in a single domain. Our
approach learns relation types by exploiting
meta-constraints that characterize the general
qualities of a good relation in any domain.
These constraints state that instances of a
single relation should exhibit regularities at
multiple levels of linguistic structure, includ-
ing lexicography, syntax, and document-level
context. We capture these regularities via the
structure of our probabilistic model as well
as a set of declaratively-specified constraints
enforced during posterior inference. Across
two domains our approach successfully recov-
ers hidden relation structure, comparable to
or outperforming previous state-of-the-art ap-
proaches. Furthermore, we find that a small
set of constraints is applicable across the do-
mains, and that using domain-specific con-
straints can further improve performance. 1
1 Introduction
In this paper, we introduce a novel approach for the
unsupervised learning of relations and their instan-
tiations from a set of in-domain documents. Given
a collection of news articles about earthquakes, for
example, our method discovers relations such as the
earthquake?s location and resulting damage, and ex-
tracts phrases representing the relations? instantia-
tions. Clusters of similar in-domain documents are
1The source code for this work is available at:
http://groups.csail.mit.edu/rbg/code/relation extraction/
A strong earthquake rocked the Philippine island of Min-
doro early Tuesday, [destroying]ind [some homes]arg ...
A strong earthquake hit the China-Burma border early
Wednesday ... The official Xinhua News Agency said
[some houses]arg were [damaged]ind ...
A strong earthquake with a preliminary magnitude of 6.6
shook northwestern Greece on Saturday, ... [destroying]ind
[hundreds of old houses]arg ...
Figure 1: Excerpts from newswire articles about earth-
quakes. The indicator and argument words for the dam-
age relation are highlighted.
increasingly available in forms such as Wikipedia ar-
ticle categories, financial reports, and biographies.
In contrast to previous work, our approach learns
from domain-independent meta-constraints on rela-
tion expression, rather than supervision specific to
particular relations and their instances. In particular,
we leverage the linguistic intuition that documents
in a single domain exhibit regularities in how they
express their relations. These regularities occur both
in the relations? lexical and syntactic realizations as
well as at the level of document structure. For in-
stance, consider the damage relation excerpted from
earthquake articles in Figure 1. Lexically, we ob-
serve similar words in the instances and their con-
texts, such as ?destroying? and ?houses.? Syntacti-
cally, in two instances the relation instantiation is the
dependency child of the word ?destroying.? On the
discourse level, these instances appear toward the
beginning of their respective documents. In general,
valid relations in many domains are characterized by
these coherence properties.
We capture these regularities using a Bayesian
model where the underlying relations are repre-
530
sented as latent variables. The model takes as in-
put a constituent-parsed corpus and explains how the
constituents arise from the latent variables. Each re-
lation instantiation is encoded by the variables as
a relation-evoking indicator word (e.g., ?destroy-
ing?) and corresponding argument constituent (e.g.,
?some homes?).2 Our approach capitalizes on rela-
tion regularity in two ways. First, the model?s gen-
erative process encourages coherence in the local
features and placement of relation instances. Sec-
ond, we apply posterior regularization (Grac?a et
al., 2007) during inference to enforce higher-level
declarative constraints, such as requiring indicators
and arguments to be syntactically linked.
We evaluate our approach on two domains pre-
viously studied for high-level document structure
analysis, news articles about earthquakes and finan-
cial markets. Our results demonstrate that we can
successfully identify domain-relevant relations. We
also study the importance and effectiveness of the
declaratively-specified constraints. In particular, we
find that a small set of declarative constraints are
effective across domains, while additional domain-
specific constraints yield further benefits.
2 Related Work
Extraction with Reduced Supervision Recent
research in information extraction has taken large
steps toward reducing the need for labeled data. Ex-
amples include using bootstrapping to amplify small
seed sets of example outputs (Agichtein and Gra-
vano, 2000; Yangarber et al, 2000; Bunescu and
Mooney, 2007; Zhu et al, 2009), leveraging ex-
isting databases that overlap with the text (Mintz
et al, 2009; Yao et al, 2010), and learning gen-
eral domain-independent knowledge bases by ex-
ploiting redundancies in large web and news cor-
pora (Hasegawa et al, 2004; Shinyama and Sekine,
2006; Banko et al, 2007; Yates and Etzioni, 2009).
Our approach is distinct in both the supervision
and data we operate over. First, in contrast to boot-
strapping and database matching approaches, we
learn from meta-qualities, such as low variability in
syntactic patterns, that characterize a good relation.
2We do not use the word ?argument? in the syntactic sense?
a relation?s argument may or may not be the syntactic depen-
dency argument of its indicator.
We hypothesize that these properties hold across re-
lations in different domains. Second, in contrast to
work that builds general relation databases from het-
erogeneous corpora, our focus is on learning the re-
lations salient in a single domain. Our setup is more
germane to specialized domains expressing informa-
tion not broadly available on the web.
Earlier work in unsupervised information extrac-
tion has also leveraged meta-knowledge indepen-
dent of specific relation types, such as declaratively-
specified syntactic patterns (Riloff, 1996), frequent
dependency subtree patterns (Sudo et al, 2003), and
automatic clusterings of syntactic patterns (Lin and
Pantel, 2001; Zhang et al, 2005) and contexts (Chen
et al, 2005; Rosenfeld and Feldman, 2007). Our ap-
proach incorporates a broader range of constraints
and balances constraints with underlying patterns
learned from the data, thereby requiring more so-
phisticated machinery for modeling and inference.
Extraction with Constraints Previous work has
recognized the appeal of applying declarative con-
straints to extraction. In a supervised setting, Roth
and Yih (2004) induce relations by using linear pro-
gramming to impose global declarative constraints
on the output from a set of classifiers trained on lo-
cal features. Chang et al (2007) propose an objec-
tive function for semi-supervised extraction that bal-
ances likelihood of labeled instances and constraint
violation on unlabeled instances. Recent work has
also explored how certain kinds of supervision can
be formulated as constraints on model posteriors.
Such constraints are not declarative, but instead
based on annotations of words? majority relation la-
bels (Mann and McCallum, 2008) and pre-existing
databases with the desired output schema (Bellare
and McCallum, 2009). In contrast to previous work,
our approach explores a different class of constraints
that does not rely on supervision that is specific to
particular relation types and their instances.
3 Model
Our work performs in-domain relation discovery by
leveraging regularities in relation expression at the
lexical, syntactic, and discourse levels. These regu-
larities are captured via two components: a proba-
bilistic model that explains how documents are gen-
erated from latent relation variables and a technique
531
? ? ? ?
is_verb 0 1 0
earthquake 1 0 0
hit 0 1 0
? ? ? ?
has_proper 0 0 1
has_number 0 0 0
depth 1 3 2
Figure 2: Words w and constituents x of syntactic parses
are represented with indicator features ?i and argument
features ?a respectively. A single relation instantiation is
a pair of indicator w and argument x; we filter w to be
nouns and verbs and x to be noun phrases and adjectives.
for biasing inference to adhere to declaratively-
specified constraints on relation expression. This
section describes the generative process, while Sec-
tions 4 and 5 discuss declarative constraints.
3.1 Problem Formulation
Our input is a corpus of constituent-parsed docu-
ments and a number K of relation types. The output
is K clusters of semantically related relation instan-
tiations. We represent these instantiations as a pair
of indicator word and argument sequence from the
same sentence. The indicator?s role is to anchor a
relation and identify its type. We only allow nouns
or verbs to be indicators. For instance, in the earth-
quake domain a likely indicator for damage would
be ?destroyed.? The argument is the actual rela-
tion value, e.g., ?some homes,? and corresponds to
a noun phrase or adjective.3
Along with the document parse trees, we utilize
a set of features ?i(w) and ?a(x) describing each
potential indicator word w and argument constituent
x, respectively. An example feature representation
is shown in Figure 2. These features can encode
words, part-of-speech tags, context, and so on. Indi-
cator and argument feature definitions need not be
the same (e.g., has number is important for argu-
3In this paper we focus on unary relations; binary relations
can be modeled with extensions of the hidden variables and con-
straints.
ments but irrelevant for indicators).4
3.2 Generative Process
Our model associates each relation type k with a set
of feature distributions ?k and a location distribution
?k. Each instantiation?s indicator and argument, and
its position within a document, are drawn from these
distributions. By sharing distributions within each
relation, the model places high probability mass on
clusters of instantiations that are coherent in features
and position. Furthermore, we allow at most one in-
stantiation per document and relation, so as to target
relations that are relevant to the entire document.
There are three steps to the generative process.
First, we draw feature and location distributions for
each relation. Second, an instantiation is selected
for every pair of document d and relation k. Third,
the indicator features of each word and argument
features of each constituent are generated based on
the relation parameters and instantiations. Figure 3
presents a reference for the generative process.
Generating Relation Parameters Each relation k
is associated with four feature distribution param-
eter vectors: ?ik for indicator words, ?
bi
k for non-
indicator words, ?ak for argument constituents, and
?bak for non-argument constituents. Each of these is
a set of multinomial parameters per feature drawn
from a symmetric Dirichlet prior. A likely indica-
tor word should have features that are highly proba-
ble according to ?ik, and likewise for arguments and
?ak. Parameters ?
bi
k and ?
ba
k represent background dis-
tributions for non-relation words and constituents,
similar in spirit to other uses of background distri-
butions that filter out irrelevant words (Che, 2006).5
By drawing each instance from these distributions,
we encourage the relation to be coherent in local lex-
ical and syntactic properties.
Each relation type k is also associated with a pa-
rameter vector ?k over document segments drawn
from a symmetric Dirichlet prior. Documents are
divided into L equal-length segments; ?k states how
likely relation k is for each segment, with one null
outcome for the relation not occurring in the doc-
ument. Because ?k is shared within a relation, its
4We consider only categorical features here, though the ex-
tension to continuous or ordinal features is straightforward.
5We use separate background distributions for each relation
to make inference more tractable.
532
For each relation type k:
? For each indicator feature ?i draw feature distri-
butions ?ik,?i , ?
bi
k,?i ? Dir(?0)
? For each argument feature ?a draw feature dis-
tributions ?ak,?a , ?
ba
k,?a ? Dir(?0)
? Draw location distribution ?k ? Dir(?0)
For each relation type k and document d:
? Select document segment sd,k ? Mult(?k)
? Select sentence zd,k uniformly from segment
sd,k, and indicator id,k and argument ad,k uni-
formly from sentence zd,k
For each word w in every document d:
? Draw each indicator feature ?i(w) ?
Mult
(
1
Z
?K
k=1 ?k,?i
)
, where ?k,?i is ?
i
k,?i
if id,k = w and ?bik,?i otherwise
For each constituent x in every document d:
? Draw each argument feature ?a(x) ?
Mult
(
1
Z
?K
k=1 ?k,?a
)
, where ?k,?a is ?ak,?a
if ad,k = x and ?bak,?a otherwise
Figure 3: The generative process for model parameters
and features. In the above Dir and Mult refer respectively
to the Dirichlet distribution and multinomial distribution.
Fixed hyperparameters are subscripted with zero.
instances will tend to occur in the same relative po-
sitions across documents. The model can learn, for
example, that a particular relation typically occurs in
the first quarter of a document (if L = 4).
Generating Relation Instantiations For every rela-
tion type k and document d, we first choose which
portion of the document (if any) contains the instan-
tiation by drawing a document segment sd,k from
?k. Our model only draws one instantiation per pair
of k and d, so each discovered instantiation within a
document is a separate relation. We then choose the
specific sentence zd,k uniformly from within the seg-
ment, and the indicator word id,k and argument con-
stituent ad,k uniformly from within that sentence.
Generating Text Finally, we draw the feature val-
ues. We make a Na??ve Bayes assumption between
features, drawing each independently conditioned
on relation structure. For a word w, we want all re-
lations to be able to influence its generation. Toward
this end, we compute the element-wise product of
feature parameters across relations k = 1, . . . ,K,
using indicator parameters ?ik if relation k selected
w as an indicator word (if id,k = w) and background
parameters ?bik otherwise. The result is then normal-
ized to form a valid multinomial that produces word
w?s features. Constituents are drawn similarly from
every relations? argument distributions.
4 Inference with Constraints
The model presented above leverages relation reg-
ularities in local features and document placement.
However, it is unable to specify global syntactic
preferences about relation expression, such as indi-
cators and arguments being in the same clause. An-
other issue with this model is that different relations
could overlap in their indicators and arguments.6
To overcome these obstacles, we apply declara-
tive constraints by imposing inequality constraints
on expectations of the posterior during inference
using posterior regularization (Grac?a et al, 2007).
In this section we present the technical details
of the approach; Section 5 explains the specific
linguistically-motivated constraints we consider.
4.1 Inference with Posterior Regularization
We first review how posterior regularization impacts
the variational inference procedure in general. Let
?, z, and x denote the parameters, hidden struc-
ture, and observations of an arbitrary model. We
are interested in estimating the posterior distribution
p(?, z | x) by finding a distribution q(?, z) ? Q that
is minimal in KL-divergence to the true posterior:
KL(q(?, z) ? p(?, z | x))
=
?
q(?, z) log
q(?, z)
p(?, z, x)
d?dz + log p(x). (1)
For tractability, variational inference typically
makes a mean-field assumption that restricts the set
Q to distributions where ? and z are independent,
i.e., q(?, z) = q(?)q(z). We then optimize equa-
tion 1 by coordinate-wise descent on q(?) and q(z).
To incorporate constraints into inference, we fur-
ther restrict Q to distributions that satisfy a given
6In fact, a true maximum a posteriori estimate of the model
parameters would find the same most salient relation over and
over again for every k, rather than finding K different relations.
533
set of inequality constraints, each of the form
Eq[f(z)] ? b. Here, f(z) is a deterministic func-
tion of z and b is a user-specified threshold. Inequal-
ities in the opposite direction simply require negat-
ing f(z) and b. For example, we could apply a syn-
tactic constraint of the form Eq[f(z)] ? b, where
f(z) counts the number of indicator/argument pairs
that are syntactically connected in a pre-specified
manner (e.g., the indicator and argument modify the
same verb), and b is a fixed threshold.
Given a set C of constraints with functions fc(z)
and thresholds bc, the updates for q(?) and q(z) from
equation 1 are as follows:
q(?) = argmin
q(?)
KL
(
q(?) ? q?(?)
)
, (2)
where q?(?) ? expEq(z)[log p(?, z, x)], and
q(z) = argmin
q(z)
KL
(
q(z) ? q?(z)
)
s.t. Eq(z)[fc(z)] ? bc, ?c ? C, (3)
where q?(z) ? expEq(?)[log p(?, z, x)]. Equation 2
is not affected by the posterior constraints and is up-
dated by setting q(?) to q?(?). We solve equation 3
in its dual form (Grac?a et al, 2007):
argmin
?
?
c?C
?cbc + log
?
z
q?(z)e?
P
c?C ?cfc(z)
s.t. ?c ? 0, ?c ? C. (4)
With the box constraints of equation 4, a numerical
optimization procedure such as L-BFGS-B (Byrd
et al, 1995) can be used to find optimal dual pa-
rameters ??. The original q(z) is then updated to
q?(z) exp
(
?
?
c?C ?
?
cfc(z)
)
and renormalized.
4.2 Updates for our Model
Our model uses this mean-field factorization:
q(?, ?, z, a, i)
=
K?
k=1
q(?k; ??k)q(?
i
k; ??
i
k)q(?
bi
k ; ??
bi
k )q(?
a
k; ??
a
k)q(?
ba
k ; ??
ba
k )
?
?
d
q(zd,k, ad,k, id,k; c?d,k) (5)
In the above, ?? and ?? are Dirichlet distribution pa-
rameters, and c? are multinomial parameters. Note
that we do not factorize the distribution of z, i, and
a for a single document and relation, instead repre-
senting their joint distribution with a single set of
variational parameters c?. This is tractable because a
single relation occurs only once per document, re-
ducing the joint search space of z, i, and a. The
factors in equation 5 are updated one at a time while
holding the other factors fixed.
Updating ?? Due to the Na??ve Bayes assumption
between features, each feature?s q(?) distributions
can be updated separately. However, the product
between feature parameters of different relations in-
troduces a nonconjugacy in the model, precluding
a closed form update. Instead we numerically opti-
mize equation 1 with respect to each ??, similarly to
previous work (Boyd-Graber and Blei, 2008). For
instance, ??ik,? of relation k and feature ? is updated
by finding the gradient of equation 1 with respect to
??ik,? and applying L-BFGS. Parameters ??
bi, ??a, and
??ba are updated analogously.
Updating ?? This update follows the standard
closed form for Dirichlet parameters:
??k,` = ?0 + Eq(z,a,i)[C`(z, a, i)], (6)
whereC` counts the number of times z falls into seg-
ment ` of a document.
Updating c? Parameters c? are updated by first com-
puting an unconstrained update q?(z, a, i; c??):
c??d,k,(z,a,i) ? exp
?
?Eq(?k)[log p(z, a, i | ?k)]
+ Eq(?ik)[log p(i | ?
i
k)] +
?
w 6=i
Eq(?bik )[log p(w | ?
bi
k )]
+ Eq(?ak)[log p(a | ?
a
k)] +
?
x 6=a
Eq(?bak )[log p(x | ?
ba
k )]
?
?
We then perform the minimization on the dual in
equation 4 under the provided constraints to derive a
final update to the constrained c?.
Simplifying Approximation The update for ?? re-
quires numerical optimization due to the nonconju-
gacy introduced by the point-wise product in fea-
ture generation. If instead we have every relation
type separately generate a copy of the corpus, the ??
534
Quantity f(z, a, i) ? or ? b
Syntax ?k Counts i, a of relation k that match a pattern (see text) ? 0.8D
Prevalence ?k Counts instantiations of relation k ? 0.8D
Separation (ind) ?w Counts times w selected as i ? 2
Separation (arg) ?w Counts times w selected as part of a ? 1
Table 1: Each constraint takes the form Eq[f(z, a, i)] ? b or Eq[f(z, a, i)] ? b; D denotes the number of corpus
documents, ?k means one constraint per relation type, and ?w means one constraint per token in the corpus.
updates becomes closed-form expressions similar to
equation 6. This approximation yields similar pa-
rameter estimates as the true updates while vastly
improving speed, so we use it in our experiments.
5 Declarative Constraints
We now have the machinery to incorporate a va-
riety of declarative constraints during inference.
The classes of domain-independent constraints we
study are summarized in Table 1. For the propor-
tion constraints we arbitrarily select a threshold of
80% without any tuning, in the spirit of building a
domain-independent approach.
Syntax As previous work has observed, most rela-
tions are expressed using a limited number of com-
mon syntactic patterns (Riloff, 1996; Banko and Et-
zioni, 2008). Our syntactic constraint captures this
insight by requiring that a certain proportion of the
induced instantiations for each relation match one of
these syntactic patterns:
? The indicator is a verb and the argument?s
headword is either the child or grandchild of
the indicator word in the dependency tree.
? The indicator is a noun and the argument is a
modifier or complement.
? The indicator is a noun in a verb?s subject and
the argument is in the corresponding object.
Prevalence For a relation to be domain-relevant, it
should occur in numerous documents across the cor-
pus, so we institute a constraint on the number of
times a relation is instantiated. Note that the effect
of this constraint could also be achieved by tuning
the prior probability of a relation not occurring in a
document. However, this prior would need to be ad-
justed every time the number of documents or fea-
ture selection changes; using a constraint is an ap-
pealing alternative that is portable across domains.
Separation The separation constraint encourages
diversity in the discovered relation types by restrict-
ing the number of times a single word can serve as
either an indicator or part of the argument of a re-
lation instance. Specifically, we require that every
token of the corpus occurs at most once as a word
in a relation?s argument in expectation. On the other
hand, a single word can sometimes be evocative of
multiple relations (e.g., ?occurred? signals both date
and time in ?occurred on Friday at 3pm?). Thus, we
allow each word to serve as an indicator more than
once, arbitrarily fixing the limit at two.
6 Experimental Setup
Datasets and Metrics We evaluate on two datasets,
financial market reports and newswire articles about
earthquakes, previously used in work on high-level
content analysis (Barzilay and Lee, 2004; Lap-
ata, 2006). Finance articles chronicle daily mar-
ket movements of currencies and stock indexes, and
earthquake articles document specific earthquakes.
Constituent parses are obtained automatically us-
ing the Stanford parser (Klein and Manning, 2003)
and then converted to dependency parses using the
PennConvertor tool (Johansson and Nugues, 2007).
We manually annotated relations for both corpora,
selecting relation types that occurred frequently in
each domain. We found 15 types for finance and
9 for earthquake. Corpus statistics are summarized
below, and example relation types are shown in Ta-
ble 2.
Docs Sent/Doc Tok/Doc Vocab
Finance 100 12.1 262.9 2918
Earthquake 200 9.3 210.3 3155
In our task, annotation conventions for desired
output relations can greatly impact token-level per-
formance, and the model cannot learn to fit a par-
ticular convention by looking at example data. For
example, earthquakes times are frequently reported
in both local and GMT, and either may be arbitrar-
ily chosen as correct. Moreover, the baseline we
535
F
in
an
ce Bond 104.58 yen, 98.37 yen
Dollar Change up 0.52 yen, down 0.01 yen
Tokyo Index Change down 5.38 points or 0.41 percent, up 0.16 points, insignificant in percentage terms
E
ar
th
qu
ak
e Damage about 10000 homes, some buildings, no information
Epicenter
Patuca about 185 miles (300 kilometers) south of Quito, 110 kilometers (65 miles)
from shore under the surface of the Flores sea in the Indonesian archipelago
Magnitude 5.7, 6, magnitude-4
Table 2: Example relation types identified in the finance and earthquake datasets with example instance arguments.
compare against produces lambda calculus formulas
rather than spans of text as output, so a token-level
comparison requires transforming its output.
For these reasons, we evaluate on both sentence-
level and token-level precision, recall, and F-score.
Precision is measured by mapping every induced re-
lation cluster to its closest gold relation and comput-
ing the proportion of predicted sentences or words
that are correct. Conversely, for recall we map ev-
ery gold relation to its closest predicted relation and
find the proportion of gold sentences or words that
are predicted. This mapping technique is based on
the many-to-one scheme used for evaluating unsu-
pervised part-of-speech induction (Johnson, 2007).
Note that sentence-level scores are always at least as
high as token-level scores, since it is possible to se-
lect a sentence correctly but none of its true relation
tokens while the opposite is not possible.
Domain-specific Constraints On top of the cross-
domain constraints from Section 5, we study
whether imposing basic domain-specific constraints
can be beneficial. The finance dataset is heav-
ily quantitative, so we consider applying a single
domain-specific constraint stating that most rela-
tion arguments should include a number. Likewise,
earthquake articles are typically written with a ma-
jority of the relevant information toward the begin-
ning of the document, so its domain-specific con-
straint is that most relations should occur in the
first two sentences of a document. Note that these
domain-specific constraints are not specific to in-
dividual relations or instances, but rather encode a
preference across all relation types. In both cases,
we again use an 80% threshold without tuning.
Features For indicators, we use the word, part of
speech, and word stem. For arguments, we use the
word, syntactic constituent label, the head word of
the parent constituent, and the dependency label of
the argument to its parent.
Baselines We compare against three alternative un-
supervised approaches. Note that the first two only
identify relation-bearing sentences, not the specific
words that participate in the relation.
Clustering (CLUTO): A straightforward way of
identifying sentences bearing the same relation is
to simply cluster them. We implement a cluster-
ing baseline using the CLUTO toolkit with word and
part-of-speech features. As with our model, we set
the number of clusters K to the true number of rela-
tion types.
Mallows Topic Model (MTM): Another technique
for grouping similar sentences is the Mallows-based
topic model of Chen et al (2009). The datasets we
consider here exhibit high-level regularities in con-
tent organization, so we expect that a topic model
with global constraints could identify plausible clus-
ters of relation-bearing sentences. Again, K is set to
the true number of relation types.
Unsupervised Semantic Parsing (USP): Our fi-
nal unsupervised comparison is to USP, an unsuper-
vised deep semantic parser introduced by Poon and
Domingos (2009). USP induces a lambda calculus
representation of an entire corpus and was shown to
be competitive with open information extraction ap-
proaches (Lin and Pantel, 2001; Banko et al, 2007).
We give USP the required Stanford dependency for-
mat as input (de Marneffe and Manning, 2008). We
find that the results are sensitive to the cluster granu-
larity prior, so we tune this parameter and report the
best-performing runs.
We recognize that USP targets a different out-
put representation than ours: a hierarchical semantic
structure over the entirety of a dependency-parsed
text. In contrast, we focus on discovering a limited
numberK of domain-relevant relations expressed as
constituent phrases. Despite these differences, both
536
methods ultimately aim to capture domain-specific
relations expressed with varying verbalizations, and
both operate over in-domain input corpora supple-
mented with syntactic information. For these rea-
sons, USP provides a clear and valuable point of
comparison. For this comparison, we transform
USP?s lambda calculus formulas to relation spans as
follows. First, we group lambda forms by a combi-
nation of core form, argument form, and the parent?s
core form.7 We then filter to the K relations that
appear in the most documents. For token-level eval-
uation we take the dependency tree fragment corre-
sponding to the lambda form. For example, in the
sentence ?a strong earthquake rocked the Philippines
island of Mindoro early Tuesday,? USP learns that
the word ?Tuesday? has a core form corresponding
to words {Tuesday, Wednesday, Saturday}, a parent
form corresponding to words {shook, rock, hit, jolt},
and an argument form of TMOD; all phrases with
this same combination are grouped as a relation.
Training Regimes and Hyperparameters For each
run of our model we perform three random restarts
to convergence and select the posterior with lowest
final free energy. We fix K to the true number of
annotated relation types for both our model and USP
and L (the number of document segments) to five.
Dirichlet hyperparameters are set to 0.1.
7 Results
Table 3?s first two sections present the results of our
main evaluation. For earthquake, the far more diffi-
cult domain, our base model with only the domain-
independent constraints strongly outperforms all
three baselines across both metrics. For finance,
the CLUTO and USP baselines achieve performance
comparable to or slightly better than our base model.
Our approach, however, has the advantage of provid-
ing a formalism for seamlessly incorporating addi-
tional arbitrary domain-specific constraints. When
we add such constraints (denoted as model+DSC),
we achieve consistently higher performance than all
baselines across both datasets and metrics, demon-
strating that this approach provides a simple and ef-
fective framework for injecting domain knowledge
into relation discovery.
7This grouping mechanism yields better results than only
grouping by core form.
The first two baselines correspond to a setup
where the number of sentence clusters K is set to
the true number of relation types. This has the effect
of lowering precision because each sentence must be
assigned a cluster. To mitigate this impact, we exper-
imented with using K+N clusters, with N ranging
from 1 to 30. In each case, we then keep only the K
largest clusters. For the earthquake dataset, increas-
ing N improves performance until some point, after
which performance degrades. However, the best F-
Score corresponding to the optimal number of clus-
ters is 42.2, still far below our model?s 66.0 F-score.
For the finance domain, increasing the number of
clusters hurts performance.
Our results show a large gap in F-score between
the sentence and token-level evaluations for both the
USP baseline and our model. A qualitative analysis
of the results indicates that our model often picks up
on regularities that are difficult to distinguish with-
out relation-specific supervision. For earthquake, a
location may be annotated as ?the Philippine island
of Mindoro? while we predict just the word ?Min-
doro.? For finance, an index change can be anno-
tated as ?30 points, or 0.8 percent,? while our model
identifies ?30 points? and ?0.8 percent? as separate
relations. In practice, these outputs are all plausi-
ble discoveries, and a practitioner desiring specific
outputs could impose additional constraints to guide
relation discovery toward them.
The Impact of Constraints To understand the im-
pact of the declarative constraints, we perform an
ablation analysis on the constraint sets. We con-
sider removing the constraints on syntactic patterns
(no-syn) and the constraints disallowing relations to
overlap (no-sep) from the full domain-independent
model.8 We also try a version with hard syntac-
tic constraints (hard-syn), which requires that every
extraction match one of the three syntactic patterns
specified by the syntactic constraint.
Table 3?s bottom section presents the results of
this evaluation. The model?s performance degrades
when either of the two constraint sets are removed,
demonstrating that the constraints are in fact benefi-
cial for relation discovery. Additionally, in the hard-
syn case, performance drops dramatically for finance
8Prevalence constraints are always enforced, as otherwise
the prior on not instantiating a relation would need to be tuned.
537
Finance Earthquake
Sentence-level Token-level Sentence-level Token-level
Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Model 82.1 59.7 69.2 42.2 23.9 30.5 54.2 68.1 60.4 20.2 16.8 18.3
Model+DSC 87.3 81.6 84.4 51.8 30.0 38.0 66.4 65.6 66.0 22.6 23.1 22.8
CLUTO 56.3 92.7 70.0 ? ? ? 19.8 58.0 29.5 ? ? ?
MTM 40.4 99.3 57.5 ? ? ? 18.6 74.6 29.7 ? ? ?
USP 91.3 66.1 76.7 28.5 32.6 30.4 61.2 43.5 50.8 9.9 32.3 15.1
No-sep 97.8 35.4 52.0 86.1 8.7 15.9 42.2 21.9 28.8 16.1 4.6 7.1
No-syn 83.3 46.1 59.3 20.8 9.9 13.4 53.8 60.9 57.1 14.0 13.8 13.9
Hard-syn 47.7 39.0 42.9 11.6 7.0 8.7 55.0 66.2 60.1 20.1 17.3 18.6
Table 3: Top section: our model, with and without domain-specific constraints (DSC). Middle section: The three
baselines. Bottom section: ablation analysis of constraint sets for our model. For all scores, higher is better.
while remaining almost unchanged for earthquake.
This suggests that formulating constraints as soft in-
equalities on posterior expectations gives our model
the flexibility to accommodate both the underlying
signal in the data and the declarative constraints.
Comparison against Supervised CRF Our final
set of experiments compares a semi-supervised ver-
sion of our model against a conditional random field
(CRF) model. The CRF model was trained using
the same features as our model?s argument features.
To incorporate training examples in our model, we
simply treat annotated relation instances as observed
variables. For both the baselines and our model,
we experiment with using up to 10 annotated docu-
ments. At each of those levels of supervision, we av-
erage results over 10 randomly drawn training sets.
At the sentence level, our model compares very
favorably to the supervised CRF. For finance, it takes
at least 10 annotated documents (corresponding to
roughly 130 annotated relation instances) for the
CRF to match the semi-supervised model?s perfor-
mance. For earthquake, using even 10 annotated
documents (about 71 relation instances) is not suf-
ficient to match our model?s performance.
At the token level, the supervised CRF base-
line is far more competitive. Using a single la-
beled document (13 relation instances) yields su-
perior performance to either of our model variants
for finance, while four labeled documents (29 re-
lation instances) do the same for earthquake. This
result is not surprising?our model makes strong
domain-independent assumptions about how under-
lying patterns of regularities in the text connect to
relation expression. Without domain-specific super-
vision such assumptions are necessary, but they can
prevent the model from fully utilizing available la-
beled instances. Moreover, being able to annotate
even a single document requires a broad understand-
ing of every relation type germane to the domain,
which can be infeasible when there are many unfa-
miliar, complex domains to process.
In light of our strong sentence-level performance,
this suggests a possible human-assisted application:
use our model to identify promising relation-bearing
sentences in a new domain, then have a human an-
notate those sentences for use by a supervised ap-
proach to achieve optimal token-level extraction.
8 Conclusions
This paper has presented a constraint-based ap-
proach to in-domain relation discovery. We have
shown that a generative model augmented with
declarative constraints on the model posterior can
successfully identify domain-relevant relations and
their instantiations. Furthermore, we found that a
single set of constraints can be used across divergent
domains, and that tailoring constraints specific to a
domain can yield further performance benefits.
Acknowledgements
The authors gratefully acknowledge the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0172. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the view of the DARPA, AFRL, or
the US government. Thanks also to Hoifung Poon
and the members of the MIT NLP group for their
suggestions and comments.
538
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of DL.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
HLT/NAACL.
Kedar Bellare and Andrew McCallum. 2009. Gen-
eralized expectation criteria for bootstrapping extrac-
tors using record-text alignment. In Proceedings of
EMNLP.
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic
topic models. In Advances in NIPS.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In Proceedings of ACL.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou
Zhu. 1995. A limited memory algorithm for bound
constrained optimization. SIAM Journal on Scientific
Computing, 16(5):1190?1208.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of ACL.
2006. Modeling general and specific aspects of docu-
ments with a probabilistic topic model. In Advances
in NIPS.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and Zheng-
Yu Niu. 2005. Automatic relation extraction with
model order selection and discriminative label identi-
fication. In Proceedings of IJCNLP.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using la-
tent permutations. Journal of Artificial Intelligence
Research, 36:129?163.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-framework and Cross-domain Parser Evalu-
ation.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of NODALIDA.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):471?484.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
SIGKDD.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Proceedings of ACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of ACL/IJCNLP.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of EMNLP.
Ellen Riloff. 1996. Automatically generating extraction
patterns from untagged texts. In Proceedings of AAAI.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for unsupervised relation identification. In Pro-
ceedings of CIKM.
Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of HLT/NAACL.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-
ceedings of ACL.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition of
domain knowledge for information extraction. In Pro-
ceedings of COLING.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Cross-document relation extraction without la-
belled data. In Proceedings of EMNLP.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP.
539
Jun Zhu, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW.
540
