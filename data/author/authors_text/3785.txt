Deixis and Conjunction in Multimodal Systems 
Michael Johnston 
AT&T Labs - Research 
Shannon Laboratory, 180 Park Ave 
Florham Park, NJ 07932, USA 
j ohnston@research ,  att.  com 
Abstract 
In order to realize their full potential, multimodal 
interfaces need to support not just input from 
multiple modes, but single comnmnds optinmlly 
distributed across the available input modes. A 
multimodal anguage processing architecture is 
needed to integrate semantic content from the 
different modes. Johnston 1998a proposes a 
modular approach to multimodal language 
processing in which spoken language parsing is 
completed before lnultimodal parsing. In this 
paper, I will demonstrate the difficulties this 
approach faces as the spoken language parsing 
component is expanded to provide a compositional 
analysis of deictic expressions. I propose an 
alternative architecture in which spoken and 
multimodal parsing are tightly interleaved. This 
architecture greatly simplifies the spoken language 
parsing grm-nmar and enables predictive 
information fiom spoken language parsing to drive 
the application of multimodal parsing and gesture 
combination rules. I also propose a treatment of 
deictic numeral expressions that supports the 
broad range of pen gesture combinations that can 
be used to refer to collections of objects in the 
interface. 
Introduction 
Multimodal interfaces allow content to be 
conveyed between humans and machines over 
multiple different channels uch speech, graphics, 
pen, and hand gesture. This enables more natural 
and efficient interaction since different kinds of 
content are best suited to particular modes. For 
example, spatial information is effectively 
conveyed using gesture for input (Oviatt 1997) and 
2d or 3d graphics for output (Towns et al1998). 
Multimodal interfaces also stand to play a critical 
role in the ongoing migration of interaction onto 
wireless portable computing devices, such as 
PDAs and next generation phones, which have 
limited screen real estate and no keyboard. For 
such devices, complex graphical user interfaces 
are not feasible and speech and pen will be the 
primary input lnodes. I focus here on multimodal 
interfaces which support speech and pen input. 
Pen input consists of gestures and drawings which 
are made in electronic ink on the computer display 
and processed by a gesture recognizer. Speech 
input is transcribed using a speech recognizer. 
This paper is concerned with the 
relationship between spoken language parsing and 
nmltimodal parsing, specifically whether they 
should be separate modular components, and the 
related issue of determining the appropriate l vel 
of constituent structure at which nmltimodal 
integration should apply. Johnston 1998a 
proposes a modular approach in which the 
individual modes are parsed and assigned typed 
feature structures representing their combinatory 
properties and semantic content. A 
nmltidimensional chart parser then combines these 
structures in accordance with a unification-based 
lnultimodal grammar. This approach is outlined in 
Section 1. Section 2 addresses the compositional 
analysis of deictic expressions and their interaction 
with conjunction and other aspects of the 
gramnmr. In Section 3, a new architecture is
presented in which spoken and multimodal parsing 
are interleaved. Section 4 presents an analysis of 
deictic numeral expressions, and Section 5 
discusses certain constructions in which 
multimodal integration applies at higher levels of 
constituent structure than a simple deictic noun 
phrase. I will draw examples from a nmltimodal 
directory and messaging application, specifically a
multimodal variant of VPQ (Buntschuh et al
1998). 
1 Unification-based nmltimodal parsing 
Johnston 1998a presents an approach to language 
processing for multimodal systems in which 
multimodal integration strategies are specified 
declaratively in a unification-based grammar 
formalism. The basic architecture of the approach 
is given in Figure I. The results of speech 
recognition and gesture recognition are interpreted 
by spoken language processing (SLP) and gesture 
processing (GP) components respectively. These 
assign typed feature structure representations 
362 
(Carpenter 1992) to speech and gesture and pass 
those on to a nmltimodal parsing component (MP). 
Tim typed feature structure formalism is 
augmented with ftmctioual constraints (Wittenbnrg 
1993). MP uses a multidimensional chart parser to 
combine the interpretations of speech and gesture 
in accordance with a nmltimodal unil'ication-based 
grammar, determines the range of possible 
lnultimodal interpretations, selects the one with the 
highest joint probability, and passes it on for 
execution. 
~ Commands 
Figure 1 Modular architecture (Johnston 1998a) 
As an example of a multimodal command, in order 
to reposition an object, a user might say 'move this 
here' and make two gestures on the display. The 
spoken command 'move this here' needs to 
combine with the two gestures, the first indicating 
the entity to be moved and the second indicating 
where it should be moved to. In cases where the 
spoken string needs to combine with more than 
one gesture, it is assigned a multimodal 
subcategorization list indicating the gestures it 
needs, how they contribute to the meaning, and the 
constraints on their combination. For e.xample, 
SLI' assigns 'move this here' the feature structure 
in Figure 2. 
The mmsubcat: list indicates that this 
input needs to combine with two gestures. The 
spoken command is constrained to overlap with or 
follow within five seconds of the first gesture. 
The second gesture must follow within five 
seconds of the first. The first provides the entity to 
move and second the new location. GP assigns 
incolning estures feature structure representations 
specifying their semantic type and any object they 
select and passes these on to MP. MP uses general 
combinatory schelnata for nmltimodal 
subcategorization (Jolmston 1998a, p. 628) to 
combine the gestures with the speech, saturate the 
nmltilnodal subcategorization list, and yield an 
executable command. 
cal.:s tlbcat_COlllnlarKl limc:\[l\] 
\[-tyl~c:m!?vc \] 
? ) ....I ,? .,.\[typc:cnti b, \] / 
I / I ,  ? rtypc:localion\]/ 
\[ mcat'?n :\[sclcclion:\[3\] j J  
first: I |imc:\[4\] | 
conslraints:\[ovcrlap(\[1 ],\[4\])v fotlow(\[ 1 \],\[4\],5)\] 
mmsubcat: \[ \[cat:spatial gesture l\] 
i fa,.,,:l time:\[5\] // 
I c?nslraints:\[f?ll?v"(\[5\],\[4\],5)\] / 
\[rest:cud J 
Figure 2 Feature structure for 'move this here' 
Tiffs approach as many advantages. It allows for 
a great degree of expressivity. Combinations of 
speech with multiple gestures can be described as 
can visual parsing of collections of gestures. 
Unlike many previous multilaodal systems, the 
approach is not speech-driven, any piece of 
content can come fiom any mode. Another 
significant advantage is the lnoclularity of spoken 
hmguage parsing (SLP) and multimodal parsing 
(MP). More general rules regal'ding multimodal 
integration are in MP while the specific speech 
graMlllar used for an application is in SLP, 
enabling reuse of the multimodal parsing module 
for different applications. This modularity also 
enables plug-and-play of different kinds of spoken 
language parsers with the same multimodal 
parsing component. SLP can be a traditional chart 
parser, a robust parser, or a stochastic parser 
(Gorin et al1997). The modularity of SLP and 
MP also facilitates the adoption of a different 
strategy for string parsing t?om that used for 
multimodal parsing. Traditional approaches to 
string parsing, such as chart parsing (Kay 1980) 
assume the combining constituents o be discrete 
and in linear order. This imposes significant 
constraints on the combination of elelnents, greatly 
reduces the number of Colabinations that need to 
be considered, and facilitates prediction in parsing. 
In contrast, multimodal input is distributed over 
two or three spatial dimensions, peech, and time. 
Unlike words in a string, speech and gesture may 
overlap temporally and there is no singular 
dimension on which tim input is linear and 
discrete. The constraints that drive parsing are 
363 
specific to the combining elements and there is not 
the same general means for predictive parsing 
(Johnston 1998a). 
While the modularity of spoken language 
processing and multimodal parsing in Johnston 
1998a has many advantages, the assumption that 
all processing of the spoken string takes place 
before multimodal integration leads to significant 
difficulties as the spoken language processing 
component is expanded to handle more complex 
language and to provide a compositional nalysis 
of spoken language containing deictics. 
2 Composit ional  analysis of  de ic t i cs  
The basic problem the approach faces is to provide 
an analysis of spoken language in multimodal 
systems which enables the appropriate multilnodal 
subcategorization frame and associated constraints 
to be built compositionally in the course of parsing 
the spoken string. Whatever the syntactic 
structure of the spoken utterance, the essential 
constraint on the multimodal subcategorization is 
that the list of subcategorized gestures match the 
linear order of the deictic expressions in the 
utterance, and that the temporal constraints also 
reflect hat order. This can be thought of in terms 
of lambda abstraction. What we need to do is 
abstract over all of the unbound variables in the 
predicate that will be instantiated by gesture. For 
an expression like 'move tiffs here' we generate 
the abstraction. 2ge,,tio.2gh,catio,,.nlove(ge,,tio.,glocatio,,). 
In terms of the analysis above, this amounts to 
deriving the feature structure in Figure 2 
compositionally fi'om feature structures assigned 
to 'move', 'this', and 'here'. 
One way to accomplish this within the 
modular approach is to set up the spoken language 
processing component so that it manipulates two 
subcat lists: a regular spoken language subcat: list 
and a multimodal mmsubcat: list. Information 
about needed gestures percolates through the 
syntactic parse. The verb 'move' is assigned tim 
feature structure in Figure 3. It subcategorizes (in
the string) for an entity and for a location. If the 
arguments are not deictic, for example 'move the 
supplies to the island' the verb simply combines 
with its arguments to yield a complete command. 
Deictic expressions are assigned structures which 
subcategorize for phrases which subcategorize for 
NPs (the deictic expression is essentially type 
raised). The structure for 'this' is given in Figure 
4. Tim structure for 'here' is like that for 'this', 
except hat it selects for a verb subcategorizing for 
a location rather than an entity (subeat:f irst:  
subeat:first:eontent:type is location). 
-cat :v 
deictic : no 
time :114\] 
I-t: 'pc : move 
content : Io 9ject :\[1 \]\[tzpe : ntity\] 
Llocation : \[2\]\[type : location  c.t,p 1\] \/ :\[ oi.e.t :lU\] 
. uUca': / \[first : \[cat : np /,'est : / keontent : \[21 
L L rest :end 
\[list :\[31 \] 
mmsubcat :/end :\[3\] 
Llasttime : \[4\] 
Figure 3 Feature structure for 'move' 
-cat:v 
dcictic:yes 
content:\[ 1 ]
lime:\[9\] 
subcat first: 
-cat:v 
time:\[9\] 
deiclic:\[8\] 
content:\[ 1\] 
\[ \[cat:np \]\] 
, . \[fir,, t:/ . ? \[lype:entily \]// su,,c,,,\] \[co,,te,.:.selec,ion:\[2\]\]\]\[ 
krcst:\[31 - \] 
rlist:\[4\] 
/ \[cat:spatiaI geslurc \] 
\] firsl:llimc:\[5\] \] 
\[ \[ . \[lypc'a "e i \]\] 
I ICO l l tC I l l : |  i . ,  ? ' , ~, / I  
nlnlstlbcal:IclRl: rcst:16\]L ksciectmn.\[zlJj 
conslraints: \[is(\[8\],no)--> 
ove,lap(\[5\],\[7\])vfollow(\[7\] \[5\] 5) 
is(\[8\],yes)-->follow(\[5\] \[7\] 5)\] 
\[lasttime:\[7\] 
rest:\[3\] 
list:\[4\] \] 
mmsubcal end:\[6\] / 
lasttime:\[5\]\] 
Figure 4 Feature structure for 'this' 
In 'move this here', 'this' combines with the verb 
to its left, removing the first specification on the 
subcat:  list of 'move' and adding a gesture 
specification to the resulting mmsubcat:. Then 
'here' composes to the left with 'move this' 
relnoving the next specification on the subeat: and 
adding another gesture specification to the 
mmsubcat: I. The constraint on the first gesture 
i Directionality features in subeat: used to control the 
relative positions of combining phrases are omitted here 
to simplify tile exposition. 
364 
differs from that on the others. The t'irst must 
overlap o1 precede the speech, while tile others 
lnust follow the preceding gesture. This is 
achieved with the feature deictie: which is set to 
yes when composition with the first deictic takes 
place. The setting of this t'eature determines which 
of the temporal constraints applies (using 
conditional constraints). The lasttime: feature 
always provides the time of the last entity in the 
sequence o1' inputs. The mmsubcat:end: feature 
provides access to the end of the current 
mmsubcat: list. Once the subcat: feature has 
value end the mmsubcat:end: needs to be set to 
end and then the value of nunsubcat:list: is the 
same as lhe msubcat: in Figure 2 and can be 
passed on to the multimodal parser. 
So then, it is possible to set up tile speech 
parsing granlular so that it will build tile needed 
subcategorization for gestures and modularity 
between specch parsing and multimodal parsing 
can be maintained. However, as more complex 
phenomena re considered tile resulting gramlnar 
becomes more and more complex. In tile example 
above, the deictic NPs are pronouns ('lhis', 
'here'). The grammar of noun phrases needs to be 
set up so that tile presence of a deictic determiner 
makes the whole phrase subcategorize for a verb 
as in 'move this large green one here'. Matters 
becolne lnore complex as tile grammar is 
expanded to handle conjunction, for example 
'move this and this he,w'. An analysis of nolninal 
col\junction can be set up in which the multimodal 
subcategorization lists of conjuncts are combined 
and assigned constraints uch that gestures are 
required in the order in which the dcictic words (or 
other phrases requiring gestures) appear. If a 
deictic appears within a conjoined phrase, that 
phrase is assigned a representation which 
subcategorizes for a verb (just as 'this' does 
above). In 'move this and this there', 'this and 
this' combines with 'move' then 'there' combines 
with the result, yielding an expression which 
subcategorizes for three gestures. The treatment 
of possessives also needs to be expanded to handle 
deictics. For example, in 'call this pelwon's 
mmtber', 'this l)etwon 's number' needs to 
subcategorize for a verb which subcategorizes fox 
a nmnber while the multimodal subcategorization 
is for a gesture on a person. The possibility of 
larger phrases mapping onto single gestures 
further complicates matters. For example, to allow 
lk~r 'move.fi'om here to there' with a line gesture 
which connects tile start and elld points, SLP will 
need to assign multimodal subcategorization list 
with a single line element to the whole phrase 
'from here to there', in addition to the other 
analysis in which this expression multimodally 
subcategorizes for two gestures. An alternative is 
to have a rule that breaks down any line into its 
start and end points. The problem then is that you 
introduce subpart points into the muitimodal chart 
that could combine with other speech recognition 
results and lead to selection of the wrong parse of 
the multimodal input. Keeping the points together 
as a line avoids this difficulty but complicates tile 
SLP grammar. I return to these cases of larger 
phrases subcategorizing for single gestures in 
Section 5 below. 
If tile separation of natural language 
parsing and multimodal integration is to be 
maintained, the analysis of deictics 1 have shown, 
or one like it, has to permeate the whole of the 
natural language grammar so that appropriate 
nmltimodal subcategorization frames can be built 
in a general way. This can be done, but as the 
coverage of the natural anguage grammar grows, 
the analysis becomes increasingly baroque and 
hard to maintain. To overcome these difficulties, I 
propose here a new architecture in which spoken 
language parsing and multimodal parsing are 
interleaved and multilnodal integration takes place 
at the constituent structure level of simple deictic 
NPs. 
3 Interleaviug spoken language parsing 
and multimodal parsing 
There are a nmnber of different ways in which 
spoken language parsing (SLP) and multimodal 
parsing (MP) can be imerleaved: (1) SLP 
populates a chart with fragments, these are passed 
to MP which determines possible combinations 
with gesture, the resulting combinations are passed 
back to SLP which continues until a parse of the 
string is found, (2) SLP parses the incoming string 
into a series of fragments, these become edges in 
MP and are combined with gestures, MP is 
augmented with rules from SLP which operate in 
MP in order to complete the analysis of the phrase, 
(3) SLP and MP are merged and there is one single 
gralnmar covering both spoken language and 
multimodal parsing (cf. Johnston and Bangalore 
2000). 1 adopt here strategy (1) represented in 
Figure 5. 
365 
Commands 
Figure 5 Interleaved architecture 
A significant advantage of (1) is that it limits the 
number of elements and combinations that need to 
considered by the nmltimodal parser. The 
complexity of the inultidilnensional parsing 
algorithm is exponential in the worst case 
(Johnston 1998a) and so it is important to limit the 
number of elements that need to be considered. 
Another advantage of (1) over (2) and (3) is that as 
in the modular approach, the grammars are 
separated, facilitating reuse of the multimodal 
component for applications with different spoken 
COlnmands. Also, (2) has the problem that there is 
redundancy among the SLP and MP grammars, 
both need to have the grammars of verb 
subcategorization, conjunction etc. 
Returning now to the example above, 
'move this here'. The representation f 'move' is 
as before in Figure 3, except there is no 
mmsubcat: feature. The difference lies in the 
representation f the deictic expressions. In the 
first pass of SLP, the deictic NP 'this' is assigned 
the representation in Figure 6 (a). I have used < > 
to represent the list-wdued mmsubcat: feature and 
the constraints: feature is given in { }. The 
location deictic 'here' is assigned a similar 
representation except hat its content:type: feature 
has value location. All deictic expressions (those 
with deictic: yes) are passed to MP. MP uses a 
general subcategorization schema to combine 
'this' with an appropriate gesture, yielding the 
representation i  Figure 6 (b). The multimodal 
subcategorization schema changes the eat: featum 
from deictic_np to np when the mmsubcat: is 
saturated. Much the same happens for 'here' and 
both edges are passed back to SLP and added into 
the chart (the chart: feature keeps track of their 
location in the chart). Now that the deictic NPs 
have been combined with gestures and converted 
to NPs, spoken language parsing can proceed and 
'move' combines with 'this' and 'here' to yield an 
executable command which is then passed on to 
MP, which selects the optimal multimodal 
command and passes it on for execution. In 
examples with conjunction such as 'move this and 
this here', the deictic NPs am combined with 
gestures by MP belbre conjunction takes place in 
SLP, and so there is no need to complicate the 
analysis of conjunction. 
cat : dcictic_np 
deictic : yes 
time: \[1\[ 
\[type: entity \] 
c?ntent : \[selection :\[21J 
/\[cat: spatial_gesture \ ] \  
/ I .  ? . \[type:area 3/ \  
,:a> kso,o ,io,,. 
mmsubcat : tLtime :\[31 J 
\ / 
\{overlap(\[l\],\[3\]) v / 
\ fo l low( \ [1  \],\[3\]..5)} / 
chart : \[1,2\] 
\[cat : hi' \] 
\]deictic : no \] 
/L~/. . \[type : entity \ ] /  ""/":?"t?"t \[ o'ootior, : \[o ioc,'dg4 .  H 
|mm,~ubcat : ( ) / 
\[chart :\[1,2\] \] 
Figure 6 Representation of 'this' 
In this approach, the level of constituent structure 
at which multilnodal integration applies is the 
simple deictic NP. It is preferable to integrate at 
this level rather than the level of the deictic 
determiner, since other words in the simple NP 
will place constraints on the choice and 
interpretation of the gesture. For example, 'this 
petwon' is constrained to integrate with a gesture at 
a person while 'this number' is constrained to 
integrate with a gesture at a number. 
4 Deictic numerical expressions 
I turn now to the analysis of deictic expressions 
with numerals. An example command fi'om the 
multimodal messaging application domain is 
'email these four people'. This could be handled 
by developing an analysis that assigns 'these four 
people' a multimodal subcategorization which 
selects for four spatial gestures at people: <Gpe,..,.o,,, 
Gm,..,.o,,. Gp,.,,,.o,,. Gp ........ ,>. Similarly, 'these two 
organizations' would have tile following 
multimodal subcategorization: <Go,.~,,,,iz,tio,,, 
Go,.~,,,,iz,,~o,,>. The multilnodal subcategorization 
fiame will be saturated in MP through 
combination with the appropriate number of 
individual selection gestums. The problem with 
this approach is that it does not account for the 
wide range of different gesture patterns that can be 
366 
used to refer to a set of N objects on a disphty. 
Single objects may be selected using pointing 
gestures or circling (or underlining). Circling 
gestures can also be used to refer to sets of objects 
and combinations of circling and pointing can be 
used to enumerate a set of entities. Figure 7 shows 
some of the different ways that a set of four 
objects can be refened to using electronic ink. 
The graphical layout of objects on the 
screen plays an ilnportant role in deterlnining the 
kind of gesture combinations that are likely. If 
three objects are close together and another further 
away, the least effortl'ul gesture combination is to 
circle the three and then circle oi point at the 
remaining one. If all four are close together, then 
it is easiest to make a single area gesture 
containing all four. If other objects intervene 
between the objects to be selected, individual 
selections are lnore likely since there is less risk of 
accidentally selecting the intervening objects. It is 
desirable that multimodal systems be able to 
handle the broad range of ways to select 
collections of entities so that users can utilize the 
and most natural gesture least effortful 
combination. 
no@ 
030 
.63 
@ \ [ \ ]  m 
Figure 7 Gestures at collections of entities 
The range of possible,gesture combinations can be 
captured using multimodal subcategorization as 
above, but this vastly complicates the SLP 
grammar and leads to an explosion of ambiguity. 
Every time a numerical expression appears a 
multitude of alternative multimodal 
subcategorization fralnes would need to be 
assigned to it. 
To address this problem, my approach is to 
underspecify the particular configuration of 
gestures in the multilnodal subcategorization o1' 
the deictic uumeral expression. Instead of 
subcategorizing for a sequence of N gestures, 
'these N' subcategorizes for a collection of 
plurality N : <G\[number:N\]>. The expression 
'these fi~ttr people' has subcategorization 
<Gw.~.o,,\[mm,ber:4\]>. An independent set of roles 
for gesture combination are used to enumerate all 
of the different ways to refer to a collection of 
entities. In simplil'ied form, the basic gesture 
combination rule is as in Figure 8. 
G G G 
\[O,pc:lll \] \[~vt,~:lll -tvt, o:\[l\] \] \] 
mmtber : 12\] + 131/-> /'i"mhi": : I2\] / 1 liumber : 13\] / 
.sdeotio, i \[61 J \[.,'eleotio,, : 14\]_l \[.selection :\[ 51J 
{append(141, I51,161)} 
Figure 8 Gesture combination rule 
The rule is also constrained so that the combining 
gestures are adjacent in time and do not intersect 
with each other. The gesture combination rules 
will enumerate a broad range of possible gesture 
collections (though not as many combiuations as 
when they are enumerated in the mullimodal 
subcategorization frame). The over-application of
these rules can be prevented by using predictive 
information from SLP; that is, if SLP parses 'these 
.four people' then these rules are applied to the 
gesture input in order to construct candidate 
collections of four people. 
5 Integrat ion at higher levels of  const i tuent 
structure 
In the analysis developed above, multimodal 
inlegration takes place at the level of simple 
deictic nominal expressions. There are however 
nmltimodal utterances where a single gesture maps 
onto a higher level of constituent structure in the 
spoken language parse. For example, 'move from 
here to there' could appear with two pointing 
gestures, but could also very well appear with a 
line gesture indicating the start and end of the 
move. In this case, the integration coukt be kept at 
the level of 'here' and 'there' by introducing a rule 
which splits line gestures into their component 
start and end points (Gli,,e ---) Gi,oim Gl,,,i,,t). The 
problem with this approach is that it introduces 
points that MP could then attempt to combine with 
other recognition results leading to an erroneous 
parse of the utterance. To avoid this problem the 
SLP grammar can assign two possible analyses to 
this string. In one, both 'here' aud 'there' are 
passed to MP for integration with point gestures. 
In the other, 'fi'om here to there' is parsed in SLP 
367 
and passed to MP for integration with a line 
gesture. There are related examples with 
conjunction 'move this organization and this 
department here'. An encircling esture could be 
used to identify 'this organization and this 
department' (especially if the pen is close to each 
object as the corresponding deictic phrase is 
uttered). However, if in the general case we allow 
SLP to generate multiple analyzes of a 
conjunction, there will be an explosion of possible 
patterns generated, just as in the case of deictic 
numeral expressions. To overcome this difficulty, 
gesture decomposition rules can be used. In order 
to avoid errorful combinations with other 
recognition results, the application of these rules in 
MP needs to be driven by predictive information 
from SLP; that is, in our example, if single 
gestures cannot be found to combine with 'this 
organization' and 'this department', then the 
gesture decomposition rules are applied to 
temporally appropriate multiple selection gestures 
to extract the needed individual selections. A 
similar approach could be used to handle 'fi'om 
here to there' with a controlled GI,-,,. --~ @,o~,,t Gpoi, t
rule which only applies when required. 
Conclusion 
I have proposed an approach to nmltimodal 
language processing in which spoken language 
parsing and nmltimodal parsing are more tightly 
coupled than in the modular pipeliued approach 
taken in Johnston 1998. The spoken language 
parsing component and nmltilnodal parsing 
component cooperate in determining the 
interpretation of nmltimodal utterances. This 
enables multimodal integration to occur at a level 
of constituent s ructure below the verbal utterance 
level specifically, the simple deictic noun phrase. 
This greatly simplifies the development of the 
spoken language parsing grammar as it is no 
longer necessary construct a single multimodal 
subcategorization list for the whole utterance. 
Following the modular approach of Johnston 
! 998a, the treatment of multimodal 
subcategorization permeates the whole gramlnar 
complicating the analysis of verb 
subcategorization, conjunction, possessives and 
inany other phenomena. This new approach also 
enables more detailed inodeling of temporal 
constraints in multi-gesture multimodal utterances. 
I have also argued that a deictic numeral 
expression should multimodally subcategorize for 
a collection of entities and should be 
underspecified with respect to the particular 
combination of gestures used to pick out the 
collection. Possible combination patterns are 
enumerated by gesture composition rules. 
Communication between SLP and MP enables 
predictive application of rules for gesture 
composition and decomposition which might 
otherwise over-apply. 
References 
Buntschuh, B., Kamm, C., DiFabbrizio, G., Abella, A., 
Mohri, M., Narayanan, S., Zeljkovic, I., Sharp, R.D., 
Wright, J., Marcus, S., Shaffcr, J., Duncan, R. and 
Wilpon, J.G. 1998. VPQ: A spoken language 
interface to large scale directory information. Ill 
Proceedings of lCSLP 98 (Sydney, Australia). 
Carpenter, R. 1992. The logic of typed feature 
structures. Cambridge University Press, Cambridge, 
England. 
Gorin, A.L., Riccardi, G. and Wright, J.H. 1997. "How 
may I help you?". Speech Communication, vol. 23, p. 
113-127. 
Johnston, M. and S. Bangalore. 2000. Finite-state 
Multimodal Parsing and Understanding. In 
Proceedings of COLING-2000 (this volume). 
Johnston, M. 1998a. Unification-based multimodal 
parsing. In Proceedings of COLING-ACL 98, p. 624- 
630. 
Johnston, M. 1998b. Multimodal anguage processing. 
In Proceedings of lCSLP 98 (Sydney, Australia). 
Johnston, M., Cohen, P.R., McGee, D., Oviatt, S.L., 
Pittman, J.A., Smith, I. 1997. Unification-based 
multimodal integration. Proceedings of the 35th 
Ammal Meeting of the Associatiol~.\['or C mputational 
Linguistics'. Madrid, Spain. p. 281-288. 
Kay, M. 1980. Algoritlnn schemata and data structures 
in syntactic processing. In B. J. Grosz, K. S. Jones, 
and B. L. Webber (eds.) Readings in Natural 
Language P~vcessing, Morgan Kaufinann, 1986, p. 
35-70. 
Oviatt, S.L. 1997. Multimodal interactive maps: 
Designing for human performance. Human-Computer 
hzteraction, p. 93-129. 
Towns, S., Callaway, C., and Lester. J. 1998. 
Generating coordinated natural language and 3d 
animations for complex spatial explanations. 
Proceedings of the Fifteenth National Conference on 
Artificial httelligence, p. 112-119. 
Wittenburg, K. 1993. F-PATR: Functional constraints 
for unification-based grammars. In Proceedings of 
31 't Annual meeting of the Association for 
Computational Linguistics, p. 216-223. 
368 
Finite-state Multimodal Parsing and Understanding 
Michael Johnston 
AT&T Labs - Research 
Shannon Laboratory, 180 Park Ave 
FIorham Park, NJ 07932, USA 
j ohnston@research ,  a t t .  tom 
Srinivas Bangalore 
AT&T Labs - Research 
Shannon Laboratory, 180 Park Ave 
Florham Park, NJ 07932, USA 
s r in i@research ,  a r t .  tom 
Abstract 
Multimodal interfaces require effective parsing and 
nn(lerstanding of utterances whose content is dis- 
tributed across multiple input modes. Johnston 1998 
presents an approach in which strategies lbr mul- 
timodal integration are stated declaratively using a 
unification-based grammar that is used by a mnlti- 
dilnensional chart parser to compose inputs. This 
approach is highly expressive and supports a broad 
class of interfaces, but offers only limited potential 
for lnutual compensation among the input modes, is 
subject o signilicant concerns in terms o1' COml)uta- 
tional complexity, and complicates selection among 
alternative multimodal interpretations of the input. 
In tiffs papeh we l)resent an alternative approacla 
in which multimodal lmrsing and understanding are 
achieved using a weighted finite-state device which 
takes speech and gesture streams as inputs and out- 
puts their joint interpretation. This approach is sig- 
nificantly more efficienl, enables tight-coupling of 
multimodal understanding with speech recognition, 
and provides a general probabilistic fralnework for 
multimodal ambiguity resolution. 
1 Introduction 
Multimodal interfaces are systems that allow input 
and/or output o be conveyed over multiple different 
channels uch as speech, graphics, and gesture. They 
enable more natural and effective interaction since 
different kinds of content can be conveyed in the 
modes to which they are best suited (Oviatt, 1997). 
Our specific concern here is with multimodal inter- 
faces supporting input by speech, pen, and touch, but 
the approach we describe has far broader applicabil- 
ity. These interfaces stand to play a critical role in the 
ongoing migration of interaction fi'oln the desktop 
to wireless portable computing devices (PI)As, next- 
generation phones) that offer limited screen real es- 
tale, and other keyboard-less platforms uch as pub- 
lic information kiosks. 
To realize their full potential, multimodal inter- 
faces need to support not just input from multiple 
modes, but synergistic multimodal utterances opti- 
mally distributed over the available modes (John- 
ston et al, 1997). In order to achieve this, an e f  
fcctive method for integration of content fi'Oln dill 
ferent modes is needed. Johnston (1998b) shows 
how techniques from natural language processing 
(unification-based gramumrs and chart parsing) can 
be adapted to support parsing and interpretation of
utterances distributed over multiple modes. In that 
approach, speech and gesture recognition produce ~,- 
best lists of recognition results which are assigned 
typed feature structure representations (Carpenter, 
1992) and passed to a luultidimensioual chart parsel ? 
that uses a lnultimodal unification-based granunar to 
combine the representations assigned to the input el- 
ements. Possible multimodal interpretations are then 
ranked and the optimal interpretation is passed on 
for execution. This approach overcomes many of 
the limitations of previous approaches tomultimodal 
integration such as (Bolt, 1980; Neal and Shapiro, 
1991) (See (Johnston ct al., 1997)(1). 282)). It sup- 
ports speech with multiple gestures, visual parsing 
of unimodal gestures, and its dechu'ative nature fa- 
cilitates rapid l)rototyping and iterative develol)meut 
of multimodal systems. Also, the unification-based 
approach allows for mutual COlnpensatiou of recog- 
nition errors in the individual modalities (Oviatt, 
1999). 
However, the unification-based approach does not 
allow for tight-conpling of nmltimodal parsing with 
speech and gesture recognition. Compensation elL 
fects are dependent on the correct answer appear- 
ing in the ~;,-best list of interpretations a signed to 
each mode. Multimodal parsing cannot directly in- 
fluence the progress of speech or gesture recognition. 
The multidimensional parsing approach is also sub- 
ject to significant concerns in terms of computational 
complexity. In the worst case, the multidimensional 
parsing algorithm (Johnston, 1998b) (p. 626) is ex- 
ponential with respect o the number of input ele- 
ments. Also this approach does not provide a nat- 
ural fiamework for combining the probabilities of 
speech and gesture vents in order to select among 
multiple competing multimodal interpretations. Wu 
et.al. (1999) present a statistical approach for select- 
ing among multiple possible combinations of speech 
369 
and gesture. However; it is not clear how the ap- 
proach will scale to more complex verbal language 
and combinations of speech with multiple gestures. 
In this papm, we propose an alternative approach 
that addresses these limitations: parsing, understand- 
ing, and integration of speech and gesture am pe> 
formed by a single finite-state device. With certain 
simplifying assumptions, multidimensional parsing 
and understanding with multimodal grammars can 
be achieved using a weighted finite-state automa- 
ton (FSA) running on throe tapes which represent 
speech input (words), gesture input (gesture sym- 
bols and reference markers), and their combined in- 
terpretation. We have implemented our approach in 
the context of a multimodal messaging application 
in which users interact with a company directo W 
using synergistic ombinations of speech and pen 
input; a multimodal variant of VPQ (Buntschuh et 
al., 1998). For example, the user might say emai l  
this person  and this person and gesture 
with the pen on pictures of two people on a user inter- 
face display. In addition to the user interface client, 
the architecture contains peech and gesture recog- 
nition components which process incoming streams 
of speech and electronic ink, and a multimodal lan- 
guage processing component (Figure 1 ). 
u, \[ 
ASR ~ I ~esture Recognizer \[ 
Multimodal Parser/Understander \] 
Backend 
Figure 1: Multimodal alvhitecture 
Section 2 provides background on finite-state lan- 
guage processing. In Section 3, we define and exem- 
plify multimodal context-fiee grammars (MCFGS) 
and their approximation as multimodal FSAs. We 
describe our approach to finite-state representation 
of meaning and explain how the three-tape finite 
state automaton can be factored out into a number 
of finite-state transducers. In Section 4, we explain 
how these transducers can be used to enable tight- 
coupling of multimodal language processing with 
speech and gesture recognition. 
2 Finite-state Language Processing 
Finite-state transducers (FST) are finite-state au- 
tomata (FSA) where each transition consists of an 
input and an output symbol. The transition is tra- 
versed if its input symbol matches the current sym- 
bol in the input and generates the output symbol as- 
sociated with the transition. In other words, an FST 
can be regarded as a 2-tape FSA with an input tape 
from which the input symbols are read and an output 
tape where the output symbols are written. 
Finite-state machines have been extensively ap- 
plied to many aspects of language processing in- 
cluding, speech recognition (Pereira nd Riley, 1997; 
Riccardi et al, 1996), phonology (Kaplan and Kay, 
1994), morphology (Koskenniemi, 1984), chunk- 
ing (Abney, 1991; Joshi and Hopely, 1997; Ban- 
galore, 1997), parsing (Roche, 1999), and machine 
translation (Bangalore and Riccardi, 2000). 
Finite-state models are attractive n~echanisms for 
language processing since they are (a) efficiently 
learnable fiom data (b) generally effective for decod- 
ing and (c) associated with a calculus for composing 
machines which allows for straightforward integra- 
tion of constraints fl'om various levels of language 
processing. Furdmrmore, software implementing 
the finite-state calculus is available for research pur- 
poses (Mohri eta\[., 1998). Another motivation for 
our choice of finite-state models is that they enable 
tight integration of language processing with speech 
and gesture recognition. 
3 Finite-state MultimodalGrammars 
Multimodal integration involves merging semantic 
content fi'om multiple streams to build a joint inter- 
pretation for a inultimodal utterance. We use a finite- 
state device to parse multiple input strealns and to 
combine their content into a single semantic repre- 
sentation. For an interface with n inodes, a finite- 
state device operating over n+ 1 tapes is needed. The 
first n tapes represent the input streams and r~ + \] is 
an output stream representing their composition. In 
the case of speech and pen input there are three tapes, 
one for speech, one for pen gesture, and a third for 
their combined meaning. 
As an example, in the messaging application 
described above, users issue spoken commands 
such as emai l  this person and that 
organization and gestm'e on the appropriate 
person and organization on the screen. The struc- 
ture and interpretation of multimodal colnlnands of 
this kind can be captured eclaratively in a multi- 
modal context-free grammar. We present a fi'agment 
capable of handling such commands in Figure 2. 
370 
S .~ V NP g:c:\]) NP -+ I)ET N 
CONJ --4 and:E:, NP --+ I)ET N CONJ NP 
V -+ cmail:g:cmail(\[ DET --+ |his:g:c 
V -+ page:c:page(\[ I)ET --+ lhat:?:c 
N --:. person:Gp:person( ENTP, Y
N -4 organization:Go:org( ENTRY 
N --+ dcpartment:Gd:dept( ENTRY 
ENTRY -> C:el :el c:g:) 
ENTRY -> c:e2:e2 c:g:) 
ENTRY -4 c:ea:ea g:e:) 
ENTP, Y --+ ... 
Figure 2: Multimodal grammar fragment 
The non-terminals in the multimodal grammar are 
atomic symbols. The multimodal aspects el' the 
grammar become apparent in the terlninals. Each 
terminal contains three components W:G:M corre- 
sponding to the n q- 1 tapes, where W is for the spo- 
ken language stream, G is the gesture stream, and 
M is the combined meaning. The epsilon symbol is 
used to indicate when oue of these is empty in a given 
terminal. The symbols in W are woMs from the 
speech stream. The symbols in G are of two types. 
Symbols like Go indicate the presence of a particular 
kind of gesturc in the gesture stream, while those like 
et are used as references to entities referred to by the 
gesture (See Section 3.1). Simple deictic pointing 
gestures are assigned semantic types based on tl~e n- 
tities they are references to. Gp represents a gestural 
tel'erence to a person on the display, Go to an orga- 
nization, and Gd lo a department. Compared with 
a feature-based multimodal gralnlnar, these types 
constitute a set of atomic categories which make 
ltle relewmt dislinclions for gesture vents prcdicl- 
lug speech events and vice versa. For example, if 
the gesture is G,, then phrases like thLs  person  
aud him arc preferred speech events and vice versa. 
These categories also play a role in constraining the 
semantic representation when the speech is under- 
specified with respect o semantic type (e.g. emai l  
th i s  one). These gesture symbols can be orga- 
nized into a type hierarchy reflecting the ontology 
of the entities in the application domain. For exam- 
pie, there might be a general type G with subtypes 
Go and Gp, where G v has subtypes G,,,,~ and Gpf for 
male and female. 
A multimodal CFG (MCFG) can be defined fop 
really as quadruple < N, 7', P, S >. N is the set of 
nonterminals. 1 ~ is the set of productions of the form 
A -+ (~whereA E Nand,~, C (NUT)* .  S i s  
the start symbol for the grammar. 7' is the set ot' ter- 
minals of the l'orm (W U e) : (G U e) : M* where 
W is the vocabulary of speech, G is the vocabulary 
of gesture=GestureSymbols U EventSymbols; 
GcsturcSymbols ={G v, Go, Gpj', G~.., ...} and 
a finite collections of \],gventSymbols ={c,,c~, 
. . . ,  c,,}. M is the vocabulary to lel)rcsent meaning 
and includes event symbols (Evenl:Symbol.s C M). 
In general a context-free grammar can be approx- 
imated by an FSA (Pereira and Wright 1997, Neder- 
her 1997). The transition symbols of the approx- 
imated USA are the terminals of the context-fiee 
grammar and in the case of multimodal CFG as de- 
tined above, these terminals contain three compo- 
nents, W, G and M. The multimodal CFG fi'ag- 
merit in Figurc 2 translates into the FSA in Figure 3, 
a three-tape finite state device capable of composing 
two input streams into a single output semantic rep- 
resentation stream. 
Our approach makes certain simplil'ying assump- 
tions with respect o ternporal constraints. In multi- 
gesture utterances the primary flmction of tempo- 
ral constraints i to force an order on the gestures. 
If you say move th i s  here  and make two .ges- 
tures, the first corresponds toth i  s and the second to 
here. Our multimodal grammars encode order but 
do not impose explicit temporal constraints, ltow- 
ever, general temporal constraints between speech 
and the first gesture can be enforced belbrc the FSA 
is applied. 
3.1 Finite-state Meaning Representation 
A novel aspect of our approach is that in addition 
to capturing the structure of language with a finite 
state device, we also capture meaning. Tiffs is very 
important in nmltimodal language processing where 
the central goal is to capture how the multiple modes 
contribute to the combined interpretation. Ottr ba- 
sic approach is to write symbols onto the third tape, 
which when concatenated together yield the seman- 
tic representation l'or the multimodal utterance. It 
suits out" purposes here to use a simple logical repre- 
sentation with predicates pred(....) and lists la, b,...l. 
Many other kinds of semantic representation could 
be generated. In the fl'agment in Figure 2, the word 
ema?l contributes email(\[ to the semantics tape, 
and the list and predicate arc closed when the rule 
S --+ V NP e:z:\]) applies. The word person  
writes person( on the semantics tape. 
A signiiicant problem we face in adding mean- 
ing into the finite-state framework is how to reprc- 
sent all of the different possible specific values that 
can be contributed by a gesture. For deictic refer- 
ences a unique identitier is needed for each object in 
the interface that the user can gesture on. For ex- 
alnple, il' the interface shows lists of people, there 
needs to be a unique ideutilier for each person. As 
part of the composition process this identifier needs 
371 
departmcnl:Gd:dept( cps:cl :el 
or,mnization:Go:or-( tnat:eps:eps ~ z } ~ ~ . \ [  3 ~ eps:eZ:e2 ~_ 
/ / ~ e~q'e~ ~.\ " \]--.~cps:el~s:) 
+:?,,+. 
and:eps:, 
Figure 3: Multimodal three-tape FSA 
to be copied from the gesture stream into the seman- 
tic representation. In the unification-based approach 
to multimodal integration, this is achieved by fea- 
ture sharing (Johnston, 1998b). In the finite-state ap- 
proach, we would need to incorporate all of the dif- 
ferent possible IDs into the FSA. For a person with 
id objid345 you need an arc e:objid345:objid345 
to transfer that piece of information fiom the ges- 
ture tape to the lneaning tape. All of the arcs for 
different IDs would have to be repeated everywhere 
in the network where this transfer of information is 
needed. Furthermore, these arcs would have to be 
updated as the underlying database was changed or 
updated. Matters are even worse for more complex 
pen-based ata such as drawing lines and areas in an 
interactive map application (Cohen et al, 1998). In 
this case, the coordinate set from the gesture needs 
to be incorporated into the senmntic representation. 
It might not be practical to incorporate the vast nuln- 
bet of different possible coordinate sequences into an 
FSA. 
Our solution to this problem is to store these 
specific values associated with incoming gestures 
in a finite set of buffers labeled el,e,),ea . . . .  and 
in place of the specific content write in the nalne 
of the appropriate buffer on the gesture tape. In- 
stead of having the specific values in the FSA, we 
have the transitions E:C I :C \ ] ,  C :C2:C2 ,  s:e3:e:3.., in 
each location where content needs to be transferred 
from the gesture tape to the meaning tape (See Fig- 
ure 3). These are generated fi'om the ENTRY pro- 
ductions in the multilnodal CFG in Figure 2. The 
gesture interpretation module empties the buffers 
and starts back at el after each multimodal com- 
mand, and so we am limited to a finite set of ges- 
ture events in a single utterance. Returning to 
the example email this person and that 
organization, assume the user gestures on en- 
tities objid367 and objid893. These will be stored 
in buffers el and e2. Figure 4 shows the speech and 
gesture streams and the resulting combined meaning. 
The elements on the meaning tape are concate- 
nated and the buffer references are replaced to yield 
S: email this person and that organization 
G: Gp cl 'Go e2 
M: email(\[ person(ct) , org(c2) \]) 
Figure 4: Messaging domain example 
email(~)er.son(objid367), or.q(objidS93)\]). As 
more recursive semantic phenomena such as pos- 
sessives and other complex noun phrases are added 
to the grammar the resulting machines become 
larger. However, the computational consequences 
of this can be lessened by lazy ewfluation tech- 
niques (Mohri, 1997) and we believe that this finite- 
state approach to constructing semantic representa- 
tions is viable for a broad range of sophisticated lan- 
guage interface tasks. We have implemented a size- 
able multimodal CFG for VPQ (See Section 1): 417 
rules and a lexicon of 2388 words. 
3.2 Multimodal Finite-state Transducers 
While a three-tape finite-state automaton is feasi- 
ble in principle (Rosenberg, 1964), currently avail- 
able tools for finite-state language processing (Mohri 
et al, 1998) only support finite-state transducers 
(FSTs) (two tapes). Furthermore, speech recogniz- 
ers typically do not support ile use of a three-tape 
FSA as a language model. In order to implement our 
approach, we convert he three-tape FSA (Figme 3) 
into an FST, by decomposing the transition symbols 
into an input component (G x W) and output compo- 
nent M, thus resulting in a function, T:(G x W) --+ 
M. This corresponds to a transducer in which ges- 
ture symbols and words are on the :input ape and the 
meaning is on the output tape (Figure 6). The do- 
main of this function T can be further curried to re- 
sult in a transducer that maps 7~:G --> W (Figure 7). 
This transducer captures the constraints that gesture 
places on the speech stream and we use it as a Jan- 
guage model for constraining the speech recognizer 
based on the recognized gesture string. In the fop 
lowing section, we explain how "F and 7% are used in 
conjunction with the speech recognition engine and 
gesture recognizer and interpreter to parse and inter- 
372 
pret nmltimodal input. 
4 Applying Multimodal Transducers  
There arc number of different ways in which multi- 
modal finite-state transducers can be integrated with 
speech and gesture recognition. The best approach 
to take depends on the properties of the lmrticular 
interface to be supported. The approach we outline 
here involves recognizing esture ilrst then using the 
observed gestures to modify the language model for 
speech recognition. This is a good choice if there 
is limited ambiguity in gesture recognition, for ex- 
an@e, if lhe m~jority of gestures are unambiguous 
deictic pointing gestures. 
The first step is for the geslure recognition and 
interpretation module to process incoming pen ges- 
tures and construct a linite state machine GeslltVe 
corresponding tothe range of gesture interpretations. 
Ill our example case (Figure 4) tile gesture input is 
unambiguous and the Gestttre linite state machine 
will be as in Figure 5. \]f the gestural input involves 
gesture recognition or is otherwise ambiguous it is 
represented as a lattice indicating all of the possi- 
ble recognitions and interpretations o1' tile gesture 
stream. This allows speech to compensate for ges- 
ture errors and mutual compensation. 
Figure 5: (;eslttre linite-smte machine 
This Ge,s'lure linite state machine is then com- 
posed with the transducer "R, which represents the 
relationship between speech and gesture (Figure 7). 
The result of this composition is a transducer Gesl- 
Lang (Figure 8). This transducer represents the re- 
lationship between this particular sl.ream of gestures 
and all of the possible word sequences tlmt could co- 
occur with those oes" , rares. In order to use this in- 
lbnnation to guide the speech recognizer, we lhcn 
take a proiection on the output ape (speech) of Gesl- 
Lang to yield a finite-state machine which is used 
as a hmguage model for speech recognition (Fig- 
ure 9). Using this model enables the gestural in- 
formation to directly influence the speech recog- 
nizer's search. Speech recognition yields a lattice 
of possible word sequences. In our example case it 
yMds the wol~.t sequence mail this person 
and that organization (Figure 10). We 
now need to reintegrale the geslure inl'ormation that 
wc removed in the prqjection step before recog- 
nition. This is achieved by composing Gest- 
Lang (Figure 8) with the result lattice from speech 
recognition (Figure 10), yielding transducer Gesl~ 
&)eechFST (Figure 11). This transducer contains 
the information both from the speech stream and 
from the gesture stream. The next step is to gen- 
erate the Colnbined meaning representation. To 
achieve this Gest&)eechFST (G : W) is converted 
into an FSM GestSpeechFSM by combining out- 
put and input on one tape (G x W) (Figure 12). 
GestSk)eeckFSM is then composed with T (Fig- 
ure 6), which relates speech and gesture to mean- 
ing, yielding file result transducer Result (Figure 13). 
The meaning is lead from the output tape yield- 
ing cm,dl(\[perso,,,(ca), m'O(e2)\]). We have imple- 
mented lifts approach and applied it in a multimodal 
interface to VPQ on a wireless PDA. In prelilni- 
nary speech recognition experiments, our approach 
yielded an average o1' 23% relative sentence-level er- 
ror reduction on a corpus of 1000 utterances (John- 
ston and Bangalore, 2000). 
5 Conclusion 
We have presented here a novel approach to muI- 
timodal hmguage processing in which spoken lan- 
guage and gesture are parsed and integrated by a 
single weighted lhfite-state device. This device pro- 
vides language models for speech and gesture recog- 
nition alld colllposes content from speech and gcs- 
lure into a single semantic representalion. Our ap- 
proach is novel not just in addressing multimodal 
hmguage but also in the encoding of semantics as 
well as syntax in a finile-state device. 
Compared to previous al~proaches (Johnston el al., 
1997; Jolmston, 1998a; Wu et al, 1999) which com- 
pose elements from 'n.-best lists of recognition re- 
sults, our approach provides an unprecedenled po- 
tential for mutual compensation among the input 
modes. It enables gestural input to dynamically 
alter the hmguage model used tbr speech recogni- 
lion. Furthermore, our approach avoids the com- 
putational complexity of multidimensional multi- 
modal parsing and our system of weighted finite- 
stale transducers provides a well understood prob- 
abilistic framcwork for combining the probability 
distributions associated with speech and gesture in- 
put and selecting among multiple competing nmlti- 
modal interpretations. Since the finite-state approach 
is more lightweight in coml)utational needs, it can 
more readily be deployed on a broader ange of plat- 
forms. 
In ongoing research, we are collecting a corpus of 
multimodal data ill order to forlnally evahmte the ef- 
fectiveness of our approach and to train weights for 
1he multimodal inile-state transducers. While we 
have concentrated here on understanding, in princi- 
ple the same device could be applied to multimodal 
373 
Gd_dcpartnlcnt:dept( c I_cps:e 1
. ~ Go or,,anization:or,,( ells tnat:eps ~ z j - ~ ~ ~ a b__...______cz ps:cz 
, ~  op~y,:om~< ~._ :pg_*>~_>/  -_______/ -- - " 'W' : ' ,  
ells_and:, ~ -- 
ells:l) .? 
Figure 6: Transducer elating gesture and speech to meaning (7-':(G x W) - -  M) 
Gd:departmcnt e 1 :eps 
/f~'~/'~ Go:organization cps:that ~ z } "~ -~ }-........_.cz:cl)s 
~ . . " /~"MQI~S? ' - - J~- - J " JNN. .  e3:ep*-"-"'""~"s _.,,-((43) 
('7') ~p,:~ma,, ~ ~:y>. /  ~p,:a,,d " - - - - - - - - - -~ '~ 
-..,..j......__eps:pags_.......ac-..__J - __ 
Figure 7: Transducer elating gesture and speech (TE:G ---+ W) 
eps:elllail eps:lhal Gp:person 
Figure 8: GestLang Transducer 
(}o:lu'ganizalion 
u page ~ this - - 
Figure 9: Projection of Output tape of GestLang Transducer 
@ email " @  this . @  person .~@ and . @  that =@ 
Figure 10: Result from speech recognizer 
Figure 11: GestureSpeechFST 
Figure 12: GestureSpeech FSM 
organization ~ @  
organization _- 
Q ~,,se,l,a,l:Olll,i,~,~.q) ot,s_,,,is:e,,, >@ o,)-p~rso'l:,'e~go"~> G ~,?,,s:el >q)  
EllS_all(l:, 
Figure 13: Result Transducer 
eps:) ~, Q~ 
~i,.:) >(~) ep,:\]) >(~) 
374 
generation which we are currently investigating. We 
are also exploring teclmiques to extend compilation 
fi'om feature structures gralnnlars to FSTs (Johnson, 
19!)8) to nmltimodal unification-based grammars. 
References 
Steven Abney. 1991. Parsing by chunks. In Robert 
Berwick, Steven Abney, and Carol Tenny, editors, 
Principle-based palwing. Kluwer Academic Pub- 
lishers. 
Srinivas Bangalore and Giuseppe Riccardi. 2000. 
Stochastic lhfite-state models for spoken language 
machine translation. In Proceedings o/" the Work- 
shop on Embedded Machine Translation Systems. 
Srinivas Bangalore. 1997. ComplexiO, of Lexic.al 
Descriptions and its Relevance to Partial Pmw- 
ing. Ph.l). tlaesis, University of Pennsylwmia, 
t~hiladelphia, PA, August. 
Robert A. Bolt. 1980. "put-thal-there":voicc and 
gesture at the graphics interface. Computer 
Graphics, 14(3):262-270. 
Bruce Buntschuh, C. Kamm, G. DiFabbrizio, 
A. Abella, M. Mohri, S. Narayanan, I. Zel.ikovic, 
R.D. Sharp, J. Wright, S. Marcus, J. Shaffer, 
R. I)uncan, and J.G. Wilpon. 1998. Vpq: A 
spoken language interface to large scale directory 
information. In Proceedin,q,s o/' ICSLI', Sydney, 
Australia. 
Robert Carpenter. 1992. The logic qf OT)ed./~'alure 
structures. Cambridge University Press, England. 
Philip R. Cohen, M. Johnston, 1). McGee, S. L. 
Oviatt, J. Pittman, I. Smith, L. Chen, and 
J. Clew. 1998. Multimodal interaction for dis- 
tributed interactive simulation. In M. Maybury 
and W. Wahlster, editors, Readings itz Intelligent 
httelfiwes. Morgan Kaul'mann Publishers. 
Mark Jollnson. 1998. Finite-state approximation 
of constraint-based grammars using left-corner 
grammar transforms. In Proceedings q/'COLING- 
ACL, pages 619-623, Montreal, Canada. 
Michael Johnston and Srinivas Bangalore. 2000. 
Tight-coupling of multimodal language process- 
ing with speech recognition. Technical report, 
AT&T Labs - Reseamh. 
Michael Johnston, ER. Cohen, D. McGee, S.L. Ovi- 
att, J.A. Pittman, and 1. Smidl. 1997. Unilication- 
based multimodal integration. In Proceedings o/ 
lhe 35th ACL, pages 281-288, Madrid, Spain. 
Michael Johnston. 1998a. Mullimodal language 
processing. In Proceedings q/" ICSLP, Sydney, 
Australia. 
Michael Johnston. 1998b. Unification-based multi- 
modal parsing. In Proceedings of COLING-ACL, 
pages 624-630, Montreal, Canada. 
Aravind Joshi and Philip Hopely. 1997. A parser 
fiom antiquity. Natural Language Engilzeering, 
2(4). 
Ronald M. Kaplan and M. Kay. 1994. Regular mod- 
els of phonological rule systems. Computational 
Linguislics, 20(3):331-378. 
K. K. Koskenniemi. 1984. 7ire-level morphology: a
general computation model,for wordzform recog- 
nition and production. Ph.D. thesis, University of 
He\[sinki. 
Mehryar Mohri, Fernando C. N. Pereira, and 
Michael Riley. 1998. A rational design for a 
weighted .finite-state transducer librao,. Num- 
ber 1436 in Lecture notes in computer science. 
Springm; Berlin ; New York. 
Mehryar Mohri. 1997. Finite-state transducers in 
language and speech processing. (7Oml~utational 
Linguistics, 23(2):269-312. 
J. G. Neal and S. C. Shapiro. 1991. Intelligent multi- 
media interface technology. In J. W. Sulliwm and 
S. W. Tylm, editors, Intelligent User lnter\['aces, 
pages 45-68. ACM Press, Addison Wesley, New 
York. 
Sharon L. Oviatt. 1997. Multimodal interactive 
maps: l)esigning l'or human performance. In 
Hmmut-Computer Interaction, pages 93-129. 
Sharon L. Ovialt. 1999. Mutual disambiguation of
recognition errors in a inultimodal architecture. In 
Cltl '99, pages 576-583. ACM Press, New York. 
Fernando C.N. Pereira and Michael I). Riley. 1997. 
Speech recognition by composition of weighted fi- 
nite automata. In E. Roche and Schabes Y., ed- 
itors, Finite State Devices for Nalttral Language 
Processitlg, pages 431-456. MIT Press, Cam- 
bridge, Massachusetts. 
Giuseppe Riccardi, R. Pieraccini, and E. Bocchieri. 
1996. Stochastic Automata for Language Model- 
ing. Computer Speech and Language, 10(4):265- 
293. 
Emmanuel Roche. 1999.  Finite state transducers: 
parsing free and fl'ozen sentences. In Andrfis Ko- 
rnai, editol, Extended Finite State Models el'Lan- 
guage. Cambridge University Press. 
A.L .  Rosenberg. 1964. On n-tape finite state accep- 
ters. FOCS, pages 76-81. 
Lizhong Wu, Sharon L. Oviatt, and Philip R. Cohen. 
1999.  Multilnodal integration - a statistical view. 
IEEE Transactions on Multimedia, I (4):334-34 l,
I)ecember. 
375 
Edit Machines for Robust Multimodal Language Processing
Srinivas Bangalore
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
srini@research.att.com
Michael Johnston
AT&T Labs-Research
180 Park Ave
Florham Park, NJ 07932
johnston@research.att.com
Abstract
Multimodal grammars provide an expres-
sive formalism for multimodal integra-
tion and understanding. However, hand-
crafted multimodal grammars can be brit-
tle with respect to unexpected, erroneous,
or disfluent inputs. Spoken language
(speech-only) understanding systems have
addressed this issue of lack of robustness
of hand-crafted grammars by exploiting
classification techniques to extract fillers
of a frame representation. In this paper,
we illustrate the limitations of such clas-
sification approaches for multimodal in-
tegration and understanding and present
an approach based on edit machines that
combine the expressiveness of multimodal
grammars with the robustness of stochas-
tic language models of speech recognition.
We also present an approach where the
edit operations are trained from data using
a noisy channel model paradigm. We eval-
uate and compare the performance of the
hand-crafted and learned edit machines in
the context of a multimodal conversational
system (MATCH).
1 Introduction
Over the years, there have been several mul-
timodal systems that allow input and/or output
to be conveyed over multiple channels such as
speech, graphics, and gesture, for example, put
that there (Bolt, 1980), CUBRICON (Neal and
Shapiro, 1991), QuickSet (Cohen et al, 1998),
SmartKom (Wahlster, 2002), Match (Johnston et
al., 2002). Multimodal integration and interpre-
tation for such interfaces is elegantly expressed
using multimodal grammars (Johnston and Ban-
galore, 2000). These grammars support com-
posite multimodal inputs by aligning speech in-
put (words) and gesture input (represented as se-
quences of gesture symbols) while expressing the
relation between the speech and gesture input and
their combined semantic representation. In (Ban-
galore and Johnston, 2000; Johnston and Banga-
lore, 2005), we have shown that such grammars
can be compiled into finite-state transducers en-
abling effective processing of lattice input from
speech and gesture recognition and mutual com-
pensation for errors and ambiguities.
However, like other approaches based on hand-
crafted grammars, multimodal grammars can be
brittle with respect to extra-grammatical, erro-
neous and disfluent input. For speech recognition,
a corpus-driven stochastic language model (SLM)
with smoothing or a combination of grammar-
based and   -gram model (Bangalore and John-
ston, 2004; Wang et al, 2002) can be built in order
to overcome the brittleness of a grammar-based
language model. Although the corpus-driven lan-
guage model might recognize a user?s utterance
correctly, the recognized utterance may not be
assigned a semantic representation by the multi-
modal grammar if the utterance is not part of the
grammar.
There have been two main approaches to im-
proving robustness of the understanding compo-
nent in the spoken language understanding litera-
ture. First, a parsing-based approach attempts to
recover partial parses from the parse chart when
the input cannot be parsed in its entirety due to
noise, in order to construct a (partial) semantic
representation (Dowding et al, 1993; Allen et al,
2001; Ward, 1991). Second, a classification-based
approach views the problem of understanding as
extracting certain bits of information from the in-
put. It attempts to classify the utterance and iden-
tifies substrings of the input as slot-filler values
to construct a frame-like semantic representation.
Both approaches have shortcomings. Although in
the first approach, the grammar can encode richer
semantic representations, the method for combin-
ing the fragmented parses is quite ad hoc. In the
second approach, the robustness is derived from
training classifiers on annotated data, this data is
very expensive to collect and annotate, and the
semantic representation is fairly limited. Further-
more, it is not clear how to extend this approach to
apply on lattice input ? an important requirement
for multimodal processing.
361
An alternative to these approaches is to edit
the recognized string to match the closest string
that can be accepted by the grammar. Essentially
the idea is that, if the recognized string cannot
be parsed, then we determine which in-grammar
string it is most like. For example, in Figure 1, the
recognized string is mapped to the closest string in
the grammar by deletion of the words restaurants
and in.
ASR: show cheap restaurants thai places in in chelsea
Edits: show cheap  thai places in  chelsea
Grammar: show cheap thai places in chelsea
Figure 1: Editing Example
In this paper, we develop further this edit-based
approach to finite-state multimodal language un-
derstanding and show how when appropriately
tuned it can provide a substantial improvement in
concept accuracy. We also explore learning ed-
its from data and present an approach of model-
ing this process as a machine translation problem.
We learn a model to translate from out of grammar
or misrecognized language (such as ?ASR:? above)
to the closest language the system can understand
(?Grammar:? above). To this end, we adopt tech-
niques from statistical machine translation (Brown
et al, 1993; Och and Ney, 2003) and use statistical
alignment to learn the edit patterns. Here we eval-
uate these different techniques on data from the
MATCHmultimodal conversational system (John-
ston et al, 2002) but the same techniques are more
broadly applicable to spoken language systems in
general whether unimodal or multimodal.
The layout of the paper is as follows. In Sec-
tions 2 and 3, we briefly describe the MATCH
application and the finite-state approach to mul-
timodal language understanding. In Section 4,
we discuss the limitations of the methods used
for robust understanding in spoken language un-
derstanding literature. In Section 5 we present
our approach to building hand-crafted edit ma-
chines. In Section 6, we describe our approach to
learning the edit operations using a noisy channel
paradigm. In Section 7, we describe our experi-
mental evaluation.
2 MATCH: A Multimodal Application
MATCH (Multimodal Access To City Help) is a
working city guide and navigation system that en-
ables mobile users to access restaurant and sub-
way information for New York City and Washing-
ton, D.C. (Johnston et al, 2002). The user inter-
acts with an interface displaying restaurant list-
ings and a dynamic map showing locations and
street information. The inputs can be speech,
drawing/pointing on the display with a stylus, or
synchronous multimodal combinations of the two
modes. The user can ask for the review, cui-
sine, phone number, address, or other informa-
tion about restaurants and subway directions to lo-
cations. The system responds with graphical la-
bels on the display, synchronized with synthetic
speech output. For example, if the user says phone
numbers for these two restaurants and circles two
restaurants as in Figure 2 [A], the system will draw
a callout with the restaurant name and number and
say, for example Time Cafe can be reached at 212-
533-7000, for each restaurant in turn (Figure 2
[B]).
Figure 2: MATCH Example
3 Finite-state Multimodal Understanding
Our approach to integrating and interpreting mul-
timodal inputs (Johnston et al, 2002) is an exten-
sion of the finite-state approach previously pro-
posed in (Bangalore and Johnston, 2000; John-
ston and Bangalore, 2005). In this approach, a
declarative multimodal grammar captures both the
structure and the interpretation of multimodal and
unimodal commands. The grammar consists of
a set of context-free rules. The multimodal as-
pects of the grammar become apparent in the ter-
minals, each of which is a triple W:G:M, consist-
ing of speech (words, W), gesture (gesture sym-
bols, G), and meaning (meaning symbols, M). The
multimodal grammar encodes not just multimodal
integration patterns but also the syntax of speech
and gesture, and the assignment of meaning, here
represented in XML. The symbol SEM is used to
abstract over specific content such as the set of
points delimiting an area or the identifiers of se-
lected objects (Johnston et al, 2002). In Figure 3,
we present a small simplified fragment from the
MATCH application capable of handling informa-
tion seeking requests such as phone for these three
restaurants. The epsilon symbol (  ) indicates that
a stream is empty in a given terminal.
In the example above where the user says phone
for these two restaurants while circling two restau-
rants (Figure 2 [a]), assume the speech recognizer
returns the lattice in Figure 4 (Speech). The ges-
ture recognition component also returns a lattice
(Figure 4, Gesture) indicating that the user?s ink
362
CMD   :  :  cmd  INFO  :  :  /cmd 
INFO   :  :  type  TYPE  :  :  /type 
for:  :  :  :  obj  DEICNP  :  :  /obj 
TYPE  phone:  :phone  review:  :review
DEICNP  DDETPL  :area:  :sel:  NUM HEADPL
DDETPL  these:G: 	 those:G: 
HEADPL  restaurants:rest:  rest 
 :SEM:SEM
 :  :  /rest 
NUM  two:2:  three:3:  ... ten:10: 
Figure 3: Multimodal grammar fragment
Speech:
sel
locareaG
Gesture:
2
<rest>
Meaning:
<rest>
</type> <obj>
</cmd></info></obj></rest>r12,r15
phone
twotheseforphone
SEM(r12,r15)
restaurants
<type><info><cmd>
SEM(points...)
ten
Figure 4: Multimodal Example
is either a selection of two restaurants or a ge-
ographical area. In Figure 4 (Gesture) the spe-
cific content is indicated in parentheses after SEM.
This content is removed before multimodal pars-
ing and integration and replaced afterwards. For
detailed explanation of our technique for abstract-
ing over and then re-integrating specific gestural
content and our approach to the representation of
complex gestures see (Johnston et al, 2002). The
multimodal grammar (Figure 3) expresses the re-
lationship between what the user said, what they
drew with the pen, and their combined mean-
ing, in this case Figure 4 (Meaning). The mean-
ing is generated by concatenating the meaning
symbols and replacing SEM with the appropri-
ate specific content:  cmd  info  type 
phone  /type  obj  rest  [r12,r15]  /rest 
 /obj  /info  /cmd  .
For use in our system, the multimodal grammar
is compiled into a cascade of finite-state transduc-
ers (Johnston and Bangalore, 2000; Johnston et al,
2002; Johnston and Bangalore, 2005). As a result,
processing of lattice inputs from speech and ges-
ture processing is straightforward and efficient.
3.1 Meaning Representation for Concept
Accuracy
The hierarchically nested XML representation
above is effective for processing by the backend
application, but is not well suited for the auto-
mated determination of the performance of the
language understanding mechanism. We adopt an
approach, similar to (Ciaramella, 1993; Boros et
al., 1996), in which the meaning representation,
in our case XML, is transformed into a sorted flat
list of attribute-value pairs indicating the core con-
tentful concepts of each command. The example
above yields:
ffRobust Understanding in
Multimodal Interfaces
Srinivas Bangalore?
AT&T Labs ? Research
Michael Johnston??
AT&T Labs ? Research
Multimodal grammars provide an effective mechanism for quickly creating integration and
understanding capabilities for interactive systems supporting simultaneous use of multiple
input modalities. However, like other approaches based on hand-crafted grammars, multimodal
grammars can be brittle with respect to unexpected, erroneous, or disfluent input. In this article,
we show how the finite-state approach to multimodal language processing can be extended
to support multimodal applications combining speech with complex freehand pen input, and
evaluate the approach in the context of a multimodal conversational system (MATCH). We
explore a range of different techniques for improving the robustness of multimodal integration
and understanding. These include techniques for building effective language models for speech
recognition when little or no multimodal training data is available, and techniques for robust
multimodal understanding that draw on classification, machine translation, and sequence edit
methods. We also explore the use of edit-based methods to overcome mismatches between the
gesture stream and the speech stream.
1. Introduction
The ongoing convergence of the Web with telephony, driven by technologies such as
voice over IP, broadband Internet access, high-speed mobile data networks, and hand-
held computers and smartphones, enables widespread deployment of multimodal in-
terfaces which combine graphical user interfaces with natural modalities such as speech
and pen. The critical advantage of multimodal interfaces is that they allow user input
and system output to be expressed in the mode or modes to which they are best suited,
given the task at hand, user preferences, and the physical and social environment of
the interaction (Oviatt 1997; Cassell 2001; Andre? 2002; Wahlster 2002). There is also an
increasing body of empirical evidence (Hauptmann 1989; Nishimoto et al 1995; Cohen
et al 1998a; Oviatt 1999) showing user preference and task performance advantages of
multimodal interfaces.
In order to support effective multimodal interfaces, natural language processing
techniques, which have typically operated over linear sequences of speech or text,
? 180 Park Avenue, Florham Park, NJ 07932. E-mail: srini@research.att.com.
?? 180 Park Avenue, Florham Park, NJ 07932. E-mail: johnston@research.att.com.
Submission received: 26 May 2006; revised submission received: 6 May 2008; accepted for publication:
11 July 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 3
need to be extended in order to support integration and understanding of multimodal
language distributed over multiple different input modes (Johnston et al 1997; Johnston
1998b). Multimodal grammars provide an expressive mechanism for quickly creating
language processing capabilities for multimodal interfaces supporting input modes
such as speech and gesture (Johnston and Bangalore 2000). They support composite
multimodal inputs by aligning speech input (words) and gesture input (represented
as sequences of gesture symbols) while expressing the relation between the speech
and gesture input and their combined semantic representation. Johnston and Bangalore
(2005) show that such grammars can be compiled into finite-state transducers, enabling
effective processing of lattice input from speech and gesture recognition and mutual
compensation for errors and ambiguities.
In this article, we show how multimodal grammars and their finite-state imple-
mentation can be extended to support more complex multimodal applications. These
applications combine speech with complex pen input including both freehand gestures
and handwritten input. More general mechanisms are introduced for representation of
gestures and abstraction over specific content in the gesture stream along with a new
technique for aggregation of gestures. We evaluate the approach in the context of the
MATCH multimodal conversational system (Johnston et al 2002b), an interactive city
guide. In Section 2, we present the MATCH application, the architecture of the system,
and our experimental method for collection and annotation of multimodal data. In
Section 3, we evaluate the baseline approach on the collected data.
The performance of this baseline approach is limited by the use of hand-crafted
models for speech recognition and multimodal understanding. Like other approaches
based on hand-crafted grammars, multimodal grammars can be brittle with respect to
extra-grammatical, erroneous, and disfluent input. This is particularly problematic for
multimodal interfaces if they are to be used in noisy mobile environments. To overcome
this limitation we explore a broad range of different techniques for improving the
robustness of both speech recognition and multimodal understanding components.
For automatic speech recognition (ASR), a corpus-driven stochastic languagemodel
(SLM) with smoothing can be built in order to overcome the brittleness of a grammar-
based language model. However, for multimodal applications there is often very little
training data available and collection and annotation of realistic data can be very
expensive. In Section 5, we examine and evaluate various different techniques for rapid
prototyping of the language model for the speech recognizer, including transforma-
tion of out-of-domain data, grammar sampling, adaptation from wide-coverage gram-
mars, and speech recognition models built on conversational corpora (Switchboard).
Although some of the techniques presented have been reported in the literature, we
are not aware of work comparing the effectiveness of these techniques on the same
domain and using the same data sets. Furthermore, the techniques are general enough
that they can be applied to bootstrap robust gesture recognition models as well. The
presentation here focuses on speech recognitionmodels, partly due to the greater impact
of speech recognition performance compared to gesture recognition performance on the
multimodal application described here. However, in Section 7 we explore the use of
robustness techniques on gesture input.
Although the use of an SLM enables recognition of out-of-grammar utterances,
resulting in improved speech recognition accuracy, this may not help overall system
performance unless the multimodal understanding component itself is made robust
to unexpected inputs. In Section 6, we describe and evaluate several different tech-
niques for making multimodal understanding more robust. Given the success of dis-
criminative classification models in related applications such as natural language call
346
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
routing (Haffner, Tur, and Wright 2003; Gupta et al 2004) and semantic role label-
ing (Punyakanok, Roth, and Yih 2005), we first pursue a purely data-driven approach
where the predicate of a multimodal command and its arguments are determined by
classifiers trained on an annotated corpus of multimodal data. However, given the
limited amount of data available, this approach does not provide an improvement over
the grammar-based approach. We next pursue an approach combining grammar and
data where robust understanding is viewed as a statistical machine translation problem
where out-of-grammar or misrecognized language must be translated to the closest
language the system can understand. This approach provides modest improvement
over the grammar-based approach. Finally we explore an edit-distance approach which
combines grammar-based understanding with knowledge derived from the underlying
application database. Essentially, if a string cannot be parsed, we attempt to identify
the in-grammar string that it is most similar to, just as in the translation approach. This
is achieved by using a finite-state edit transducer to compose the output of the ASR
with the grammar-based multimodal alignment and understanding models. We have
presented these techniques as methods for improving the robustness of the multimodal
understanding by processing the speech recognition output. Given the higher chance of
error in speech recognition compared to gesture recognition, we focus on processing the
speech recognition output to achieve robustmultimodal understanding. However, these
techniques are also equally applicable to gesture recognition output. In Section 7, we
explore the use of edit techniques on gesture input. Section 8 concludes and discusses
the implications of these results.
2. The MATCH Application
Urban environments present a complex and constantly changing body of informa-
tion regarding restaurants, cinema and theater schedules, transportation topology, and
timetables. This information is most valuable if it can be delivered effectively while mo-
bile, since users? needs change rapidly and the information itself is dynamic (e.g., train
times change and shows get cancelled). MATCH (Multimodal Access To City Help) is a
working city guide and navigation system that enables mobile users to access restaurant
and subway information for urban centers such as New York City and Washington,
DC (Johnston et al 2002a, 2002b). MATCH runs stand-alone on a tablet PC (Figure 1) or
in client-server mode across a wireless network. There is also a kiosk version of the
system (MATCHkiosk) (Johnston and Bangalore 2004) which incorporates a life-like
talking head. In this article, we focus on the mobile version of MATCH, in which the
user interacts with a graphical interface displaying restaurant listings and a dynamic
map showing locations and street information. The inputs can be speech, drawings on
the display with a stylus, or synchronous multimodal combinations of the two modes.
The user can ask for reviews, cuisine, phone number, address, or other information
about restaurants and for subway directions to restaurants and locations. The system
responds with graphical callouts on the display, synchronized with synthetic speech
output.
For example, a user can request to see restaurants using the spoken command show
cheap italian restaurants in chelsea. The system will then zoom to the appropriate map
location and show the locations of restaurants on the map. Alternatively, the user could
give the same command multimodally by circling an area on the map and saying show
cheap italian restaurants in this neighborhood. If the immediate environment is too noisy or
public, the same command can be given completely using a pen stylus as in Figure 2,
by circling an area and writing cheap and italian.
347
Computational Linguistics Volume 35, Number 3
Figure 1
MATCH on tablet.
Similarly, if the user says phone numbers for these two restaurants and circles two
restaurants as in Figure 3(a) [A], the system will draw a callout with the restaurant
name and number and say, for example, Time Cafe can be reached at 212-533-7000, for
each restaurant in turn (Figure 3(a) [B]). If the immediate environment is too noisy or
public, the same command can be given completely in pen by circling the restaurants
and writing phone (Figure 3(b)).
The system also provides subway directions. For example, if the user says How do I
get to this place? and circles one of the restaurants displayed on the map the system will
askWhere do you want to go from?. The user can then respond with speech (for example,
25th Street and 3rd Avenue), with pen by writing (for example, 25th St & 3rd Ave), or
multimodally (for example, from here, with a circle gesture indicating the location).
The system then calculates the optimal subway route and generates a multimodal
presentation coordinating graphical presentation of each stage of the route with spoken
instructions indicating the series of actions the user needs to take (Figure 4).
Map-based systems have been a common application area for exploringmultimodal
interaction techniques. One of the reasons for this is the effectiveness and naturalness
of combining graphical input to indicate spatial locations with spoken input to specify
commands. See Oviatt (1997) for a detailed experimental investigation illustrating the
Figure 2
Unimodal pen command.
348
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 3
(a) Two area gestures. (b) Phone command in pen.
Figure 4
Multimodal subway route.
advantages of multimodal input for map-based tasks. Previous map-based multimodal
prototypes can be broken down into two main task domains: map annotation tasks and
information search tasks. Systems such as QuickSet (Cohen et al 1998b) focus on the use
of speech and pen input in order to annotate the location of features on a map. Other
systems use speech and pen input to enable users to search and browse for information
through direct interaction with a map display. In the ADAPT system (Gustafson et al
2000), users browse for apartments using combinations of speaking and pointing. In the
Multimodal Maps system (Cheyer and Julia 1998), users perform travel planning tasks
such as searching for hotels and points of interest. MATCH is an information search
application providing local search capabilities combined with transportation directions.
As such it is most similar to the Multimodal Maps application, though it provides
more powerful and robust language processing and multimodal integration capabili-
ties, while the language processing in the Multimodal Maps application is limited to
simple Verb Object Argument constructions (Cheyer and Julia 1998).
In the next section we explain the underlying architecture and the series of compo-
nents which enable the MATCH user interface.
2.1 MATCH Multimodal Architecture
The underlying architecture that supports MATCH consists of a series of re-usable
components which communicate over IP through a facilitator (MCUBE) (Figure 5).
Figure 6 shows the flow of information among components in the system. In earlier
349
Computational Linguistics Volume 35, Number 3
Figure 5
Multimodal architecture.
versions of the system, communication was over socket connections. In later versions of
the system communication between components uses HTTP.
Users interact with the system through a Multimodal User Interface client (MUI)
which runs in a Web browser. Their speech is processed by the WATSON speech recog-
nition server (Goffin et al 2005) resulting in aweighted lattice of word strings.When the
user draws on the map their ink is captured and any objects potentially selected, such as
currently displayed restaurants, are identified. The electronic ink is broken into a lattice
of strokes and sent to both gesture and handwriting recognition components which
Figure 6
Multimodal architecture flowchart.
350
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
enrich this stroke lattice with possible classifications of strokes and stroke combinations.
The gesture recognizer uses a variant of the template matching approach described
by Rubine (1991). This recognizes symbolic gestures such as lines, areas, points, arrows,
and so on. The stroke lattice is then converted into an ink lattice which represents all of
the possible interpretations of the user?s ink as either symbolic gestures or handwritten
words. The word lattice and ink lattice are integrated and assigned a combinedmeaning
representation by the multimodal integration and understanding component (Johnston
and Bangalore 2000; Johnston et al 2002b). Because we implement this component
using finite-state transducers, we refer to this component as the Multimodal Finite State
Transducer (MMFST). The approach used in the MMFST component for integrating
and interpreting multimodal inputs (Johnston et al 2002a, 2002b) is an extension of
the finite-state approach previously proposed (Bangalore and Johnston 2000; Johnston
and Bangalore 2000, 2005). (See Section 3 for details.) This provides as output a
lattice encoding all of the potential meaning representations assigned to the user?s
input. The meaning is represented in XML, facilitating parsing and logging by other
system components. MMFST can receive inputs and generate outputs using multiple
communication protocols, including the W3C EMMA standard for representation of
multimodal inputs (Johnston et al 2007). Themeaning lattice is flattened to an n-best list
and passed to a multimodal dialog manager (MDM) (Johnston et al 2002b), which re-
ranks the possible meanings in accordance with the current dialogue state. If additional
information or confirmation is required, the MDM enters into a short information
gathering dialogue with the user. Once a command or query is complete, it is passed
to the multimodal generation component (MMGEN), which builds amultimodal score
indicating a coordinated sequence of graphical actions and TTS prompts. This score is
passed back to the MUI. The MUI then coordinates presentation of graphical content
with synthetic speech output using the AT&T Natural Voices TTS engine (Beutnagel
et al 1999). The subway route constraint solver (SUBWAY) is a backend server built for
the prototype which identifies the best route between any two points in the city.
In the given example where the user says phone for these two restaurantswhile circling
two restaurants (Figure 3(a) [A]), assume the speech recognizer returns the lattice in
Figure 7 (Speech). The gesture recognition component also returns a lattice (Figure 7,
Gesture) indicating that the user?s ink is either a selection of two restaurants or a geo-
graphical area. The multimodal integration and understanding component (MMFST)
combines these two input lattices into a lattice representing their combined meaning
(Figure 7, Meaning). This is passed to the multimodal dialog manager (MDM) and from
there to the MUI where it results in the display in Figure 3(a) [B] and coordinated TTS
output.
The multimodal integration and understanding component utilizes a declarative
multimodal grammar which captures both the structure and the interpretation of mul-
timodal and unimodal commands. This formalism and its finite-state implementation
for the MATCH system are explained in detail in Section 3.
This multimodal grammar is in part derived automatically by reference to an un-
derlying ontology of the different kinds of objects in the application. Specific categories
in the ontology, such as located entity, are associated with templates and macros that
are used to automatically generate the necessary grammar rules for the multimodal
grammar and to populate classes in a class-based language model (Section 5). For
example, in order to add support for a new kind of entity, for example, bars, a category
bar is added to the ontology as a subtype of located entity along with specification of the
head nouns used for this new category, the attributes that apply to it, the symbol to use
for it in the gesture representation, and a reference to the appropriate table to find bars
351
Computational Linguistics Volume 35, Number 3
Figure 7
Multimodal example.
in the underlying application database. The appropriate multimodal grammar rules are
then derived automatically as part of the grammar compilation process. Because the
new entity type bar is assigned the ontology category located entity, the grammar will
automatically support deictic reference to bars with expressions such as this place in
addition to the more specific this bar.
In the next section, we explain the data collection procedure we employed in order
to evaluate the system and provide a test set for experimentingwith different techniques
for multimodal integration and understanding.
2.2 Multimodal Data Collection
A corpus of multimodal data was collected in a laboratory setting from a gender-
balanced set of 16 first-time novice users. The subjects were AT&T personnel with
no prior knowledge of the system and no experience building spoken or multimodal
systems. A total of 833 user interactions (218 multimodal/491 speech-only/124 pen-
only) resulting from six sample task scenarios involving finding restaurants of various
types and getting their names, phones, addresses, or reviews, and getting subway
directions between locations were collected and annotated.
Figure 8 shows the experimental set-up. Subjects interacted with the system in a
soundproof room separated from the experimenter by one-way glass. Two video feeds
were recorded, one from a scan converter connected to the system, the other from a
camera located in the subject room, which captured a side-on view of the subject and the
display. The system ran on a Fujitsu tablet computer networked to a desktop PC logging
server located next to the experimenter. The subject?s audio inputs were captured using
both a close-talking headset microphone and a desktop microphone (which captured
both user input and system audio).
As the user interacted with the system a multimodal log in XML format was
captured on the logging server (Ehlen, Johnston, and Vasireddy 2002). The log contains a
detailed record of the subject?s speech and pen inputs and the system?s internal process-
ing steps and responses, with links to the relevant audio files and speech recognition
lattices.
352
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 8
Experimenter and subject set-up.
The experimenter started out each subject with a brief tutorial on the system, show-
ing them the pen and how to click on the display in order to turn on themicrophone. The
tutorial was intentionally vague and broad in scope so the subjects might overestimate
the system?s capabilities and approach problems in new ways. The experimenter then
left the subject to complete, unassisted, a series of six sample task scenarios of vary-
ing complexity. These involved finding restaurants of various types and getting their
names, phones, addresses, or reviews, and getting subway directions between locations.
The task scenarios were presented in a GUI on the tablet next to the map display. In
our pilot testing, we presented users with whole paragraphs describing scenarios. We
found that users would often just rephrase the wording given in the paragraph, thereby
limiting the utility of the data collection. Instead, in this data collection we presented
what the user had to find as a table (Table 1). This approach elicited a broader range of
inputs from users.
After completing the scenarios the user then completed an online questionnaire on
the tablet regarding their experience with the system. This consisted of a series of Likert
scale questions to measure user satisfaction (Walker, Passonneau, and Boland 2001).
After the questionnaire the experimenter came into the experiment room and conducted
an informal qualitative post-experiment feedback interview.
The next phase of the data collection process was to transcribe and annotate the
users? input. Transcription is more complex for multimodal systems than for speech-
only systems because the annotator needs not just to hear what the user said but also
to see what they did. The browser-based construction of the multimodal user interface
enabled us to rapidly build a custom version of the system which serves as an online
multimodal annotation tool (Figure 9). This tool extends the approach described in
Ehlen, Johnston, and Vasireddy (2002) with a graphical interface for construction of
Table 1
Example scenario.
Use MATCH to find the name, address, and phone number of a restaurant matching
the following criteria:
Food Type Location
Vegetarian Union Square
353
Computational Linguistics Volume 35, Number 3
Figure 9
Multimodal log annotation tool.
gesture annotations and a tool for automatically deriving the meaning annotation for
out-of-grammar examples. This tool allows the annotator to dynamically replay the
users? inputs and system responses on the interactive map system itself, turn by turn,
and add annotations to a multimodal log file, encoded in XML. The annotation utilizes
the map component of the system (Figure 9(1)). It provides coordinated playback of
the subject?s audio with their electronic ink, enabling the user to rapidly annotate
multimodal data without having to replay video of the interaction. The user interface
of the multimodal log viewer provides fields for the annotator to transcribe the speech
input, the gesture input, and the meaning. A series of buttons and widgets are provided
to enable the annotator to rapidly and accurately transcribe the user?s gesture and the
appropriate meaning representation without having to remember the specifics of the
gesture and meaning representations (Figure 9(2)).
After transcribing the speech and gesture, the annotator hits a button to confirm
these, and they are recorded in the log and copied down to a second field used for
annotating the meaning of the input (Figure 9(3)). It would be both time consuming and
error-prone to have the annotator code in the meaning representation for each input by
hand. Instead the multimodal understanding system is integrated into the multimodal
annotation tool directly. The interface allows the annotator to adjust the speech and
gesture inputs and send them through the multimodal understander until they get the
meaning they are looking for (Figure 9(4)). When the multimodal understander returns
multiple possibilities an n-best list is presented and the annotator hits the button next
to the appropriate interpretation in order to select it as the annotated meaning. We
found this to be a very effective method of annotating meaning, although it does require
the annotator to have some knowledge of what inputs are acceptable to the system. In
addition to annotating the speech, gesture, and meaning, annotators also checked off a
series of flags indicating various properties of the exchange, such as whether the input
was partial, whether there was a user error, and so on. The result of this effort was a
354
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
corpus of 833 user interactions all fully annotated with speech, gesture, and meaning
transcriptions.
3. Multimodal Grammars and Finite-State Multimodal Language Processing
One of the most critical technical challenges in the development of effective multimodal
systems is that of enabling multimodal language understanding; that is, determining the
user?s intent by integrating and understanding inputs distributed over multiple modes.
In early work on this problem (Neal and Shapiro 1991; Cohen 1991, 1992; Brison and
Vigouroux 1993; Koons, Sparrell, and Thorisson 1993; Wauchope 1994), multimodal un-
derstanding was primarily speech-driven,1 treating gesture as a secondary dependent
mode. In these systems, incorporation of information from the gesture input into the
multimodal meaning is triggered by the appearance of expressions in the speech input
whose reference needs to be resolved, such as definite and deictic noun phrases (e.g.,
this one, the red cube). Multimodal integration was essentially a procedural add-on to a
speech or text understanding system.
Johnston et al (1997) developed a more declarative approach where multimodal
integration is modeled as unification of typed feature structures (Carpenter 1992) as-
signed to speech and gesture inputs. Johnston (1998a, 1998b) utilized techniques from
natural language processing (unification-based grammars and chart parsers) to extend
the unification-based approach and enable handling of inputs with more than one
gesture, visual parsing, and more flexible and declarative encoding of temporal and
spatial constraints. In contrast to the unification-based approaches, which separate
speech parsing and multimodal integration into separate processing stages, Johnston
and Bangalore (2000, 2005) proposed a one-stage approach to multimodal understanding
in which a single grammar specified the integration and understanding of multimodal
language. This avoids the complexity of interfacing between separate speech under-
standing and multimodal parsing components. This approach is highly efficient and
enables tight coupling with speech recognition, because the grammar can be directly
compiled into a cascade of finite-state transducers which can compose directly with
lattices from speech recognition and gesture recognition components.
In this section, we explain how the finite-state approach to multimodal language
understanding can be extended beyond multimodal input with simple pointing ges-
tures made on a touchscreen (as in Johnston and Bangalore [2000, 2005]) to applica-
tions such as MATCH with complex gesture input combining freeform drawings with
handwriting recognition. This involves three significant extensions to the approach: the
development of a gesture representation language for complex pen input combining
freehand drawing with selections and handwriting (Section 3.1); a new more scalable
approach to abstraction over the specific content of gestures within the finite-state
mechanism (Section 3.3); and a new gesture aggregation algorithmwhich enables robust
handling of the integration of deictic phrases with a broad range of different selection
gestures (Section 3.4). In Section 3.2, we illustrate the use of multimodal grammars for
this application with a fragment of the multimodal grammar for MATCH and illustrate
how this grammar is compiled into a cascade of finite-state transducers. Section 3.5
addresses the issue of temporal constraints onmultimodal integration. In Section 3.6, we
describe the multimodal dialog management mechanism used in the system and how
1 To be more precise, they are ?verbal language?-driven, in that either spoken or typed linguistic
expressions are the driving force of interpretation.
355
Computational Linguistics Volume 35, Number 3
Figure 10
Speech lattice.
contextual resolution of deictic expressions is accounted for. In Section 3.7, we evaluate
the performance of this approach to multimodal integration and understanding using
the multimodal data collected as described in Section 2.2.
3.1 Lattice Representations for Gesture and Meaning
One of the goals of our approach to multimodal understanding is to allow for am-
biguities and errors in the recognition of the individual modalities to be overcome
through combination with the other mode (Oviatt 1999; Bangalore and Johnston 2000).
To maximize the potential for error compensation, we maintain multiple recognition
hypotheses by representing input modes as weighted lattices of possible recognition
strings. For speech input, the lattice is a network of word hypotheses with associated
weights. Figure 10 presents a simplified speech lattice from the MATCH application.2
Representation of Gesture. Like speech, gesture input can also be represented as a token
stream, but unlike speech there is no pre-established tokenization of gestures (words
of a gesture language) other than for handwritten words. We have developed a gesture
representation language for pen input which enables representation of symbolic ges-
tures such as areas, lines, and arrows, selection gestures, and handwritten words. This
language covers a broad range of pen-based input for interactive multimodal applica-
tions and can easily be extended to new domains with different gesture symbols. Each
gesture is represented as a sequence of symbols indicating different characteristics of the
gesture. These symbol sequences can be concatenated in order to represent sequences
of gestures and assembled into a lattice representation in order to represent a range of
possible segmentations and interpretations of a sequence of ink strokes. In the MATCH
system, when the user draws on the map, their ink points are captured along with in-
formation about potentially selected items, and these are passed to a gesture processing
component. First, the electronic ink is rotated and scaled and broken into a lattice of
strokes. This stroke lattice is processed by both gesture and handwriting recognizers
to identify possible pen gestures and handwritten words in the ink stream. The results
are combined with selection information to derive the gesture lattice representations
presented in this section. The gesture recognizer uses a variant of the trained template
matching approach described in Rubine (1991). The handwriting recognizer is neural-
network based. Table 2 provides the full set of eight gestures supported and the symbol
sequences used to represent them in the gesture lattice.
2 The lattices in the actual system are weighted but for ease of exposition here we leave out weights in
the figures.
356
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Table 2
Gesture inputs supported.
For symbolic gestures and selections, the gesture symbol complexes have the basic
form: G FORM MEANING (NUMBER TYPE) SEM. FORM indicates the physical form
of the gesture, and has values such as area, point, line, and arrow. MEANING provides
a rough characterization of the specific meaning of that form; for example, an area can
be either a loc (location) or a sel (selection), indicating the difference between gestures
which delimit a spatial location on the screen and gestures which select specific dis-
played icons. NUMBER and TYPE are only found with sel. They indicate the number
of entities selected (1, 2, 3, many) and the specific type of entity (e.g., rest (restaurant) or
thtr (theater)). The TYPE value mix is used for selections of entities of different types.
Recognition of inputs as handwritten words is also encoded in the gesture lattice. These
are indicated by the sequence G hwWORD. For example, if the user wrote phone number
the gesture sequence would be G hw phone G hw number.
As an example, if the user draws an area on the screen which contains two restau-
rants (as in Figure 3(a) [A]), and the restaurants have associated identifiers id1 and id2,
357
Computational Linguistics Volume 35, Number 3
the gesture lattice will be as in Figure 11. The first two paths through this gesture lattice
represent the ambiguity between the use of the gesture to indicate a spatial location
versus a selection of objects on the screen. As defined in the subsequent multimodal
grammar, if the speech is show me chinese restaurants in this neighborhood then the first
pathwill be chosen. If the speech is tell me about these two restaurants then the second path
will be chosen. The third path represents the recognition hypothesis from handwriting
recognition that this is a handwritten O. If instead the user circles a restaurant and a
theatre, the lattice would be as in Figure 12. If they say tell me about this theater, the third
path will be taken. If they say tell me about these two, the fourth path will be taken. This
allows for cases where a user circles several entities and selects a specific one by type.
The underlying ontology of the application domain plays a critical role in the han-
dling of multimodal expressions. For example, if place in tell me about this place can refer
to either a restaurant or a theatre, then it can be aligned with both gesture symbols in
the multimodal grammar. The noun place is associated in the lexicon with a general type
in the ontology: located entity. When the multimodal grammar is compiled, by virtue of
this type assignment, the expression this place is associated with gesture representations
for all of the specific subtypes of located entity in the ontology, such as restaurant and
theater. The approach also extends to support deictic references to collections of objects
of different types. For example, the noun building is associated in the lexicon with the
type building. In the grammar these buildings is associated with the gesture type building.
If the user selects a collection of objects of different types they are assigned the type
building in the gesture lattice and so the expression these buildingswill pick out that path.
In the application domain of our prototype, where restaurants and theaters are the only
selectable object types, we use a simpler ontology with a single general object type mix
for collections of objects as in Figure 12, and this integrates with spoken phrases such as
these places.
Representation of Meaning. Understanding multimodal language is about extracting the
meaning from multimodal utterances. Although there continue to be endless debates in
Figure 11
Gesture lattice G: Selection of two restaurants.
Figure 12
Gesture lattice G: Restaurant and theater.
358
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 13
Meaning lattice.
Figure 14
XML meaning representation.
linguistics, philosophy, psychology, and neuroscience on what constitutes the meaning
of a natural language utterance (Jackendoff 2002), for the purpose of human?computer
interactive systems, ?meaning? is generally regarded as a representation that can be
executed by an interpreter in order to change the state of the system.
Similar to the input speech and gesture representations, in our approach the output
meaning is also represented in a lattice format. This enables compact representation
of multiple possible interpretations of the user?s inputs and allows for later stages
of processing, such as the multimodal dialog manager, to use contextual information
to rescore the meaning lattice. In order to facilitate logging and parsing by other
components (dialog manager, backend servers), the meaning representation language
is encoded in XML.3 The meaning lattice resulting from combination of speech and
gesture is such that for every path through the lattice, the concatenation of symbols
from that path will result in a well-formed XML expression which can be evaluated with
respect to the underlying application semantics. In the city information application this
includes elements such as<show>which contains a specification of a kind of restaurant
to show, with elements <cuis> (cuisine), <loc> (location), and so on. Figure 13 shows
the meaning lattice that would result when the speech lattice (Figure 10) combines with
the gesture lattice (Figure 11).
The first path through the lattice results from the combination of the speech string
show chinese restaurants here with an area gesture. Concatenating the symbols on this
path, we have the well-formed XML expression in Figure 14.
3.2 Multimodal Grammars and Finite-State Understanding
Context-free grammars have generally been used to encode the sequences of input
tokens (words) in a language which are considered grammatical or acceptable for pro-
cessing in a single input stream. In some cases grammar rules are augmented with oper-
ations used to simultaneously build a semantic representation of an utterance (Ades and
3 In our earlier work (Johnston and Bangalore 2000, 2005), we generated a predicate logic representation,
for example: email([person(id1), organization(id2)]).
359
Computational Linguistics Volume 35, Number 3
Steedman 1982; Pollard and Sag 1994; van Tichelen 2004). Johnston and Bangalore (2000,
2005) present a multimodal grammar formalism which directly captures the relation-
ship between multiple input streams and their combined semantic representation. The
non-terminals in the multimodal grammar are atomic symbols. The multimodal aspects
of the grammar become apparent in the terminals. Each terminal contains three compo-
nentsW:G:M corresponding to the two input streams and one output stream, whereW
is for the spoken language input stream, G is for the gesture input stream, andM is for
the combined meaning output stream. These correspond to the three representations
described in Section 3.1. The epsilon symbol () is used to indicate when one of these
is empty within a given terminal. In addition to the gesture symbols (G area loc ...), G
contains a symbol SEM used as a placeholder for specific content (see Section 3.3).
In Figure 15, we present a fragment of the multimodal grammar used for the
city information application described in this article. This grammar is simplified for
ease of exposition. The rules capture spoken, multimodal, and pen-only commands for
showing restaurants (SHOW), getting information about them (INFO), requesting subway
directions (ROUTE), and zooming the map (ZOOM).
As in Johnston and Bangalore (2000, 2005), this multimodal grammar is com-
piled into a cascade of finite-state transducers. Finite-state machines have been exten-
sively applied to many aspects of language processing, including speech recognition
(Riccardi, Pieraccini, and Bocchieri 1996; Pereira and Riley 1997), phonology (Kartunnen
1991; Kaplan and Kay 1994), morphology (Koskenniemi 1984), chunking (Abney 1991;
Joshi and Hopely 1997; Bangalore 1997), parsing (Roche 1999), and machine transla-
tion (Bangalore and Riccardi 2000). Finite-state models are attractive mechanisms for
language processing since they are (a) efficiently learnable from data; (b) generally
effective for decoding; and (c) associated with a calculus for composingmachines which
allows for straightforward integration of constraints from various levels of language
processing. Furthermore, software implementing the finite-state calculus is available
for research purposes (Noord 1997; Mohri, Pereira, and Riley 1998; Kanthak and Ney
2004; Allauzen et al 2007).
We compile the multimodal grammar into a finite-state device operating over two
input streams (speech and gesture) and one output stream (meaning). The transition
symbols of the FSA correspond to the terminals of the multimodal grammar. For the
sake of illustration here and in the following examples we will only show the portion of
the three-tape finite-state device which corresponds to theDEICNP rule in the grammar
in Figure 15. The corresponding finite-state device is shown in Figure 16. This three-tape
machine is then factored into two transducers: R:G ? W and T :(G?W)? M. The R
machine (e.g., Figure 17) aligns the speech and gesture streams through a composition
with the speech and gesture input lattices (G o (G:W oW)). The result of this operation
is then factored onto a single tape and composed with the T machine (e.g., Figure 18)
in order to map these composite gesture?speech symbols into their combined meaning
(G W:M). Essentially the three-tape transducer is simulated by increasing the alphabet
size by adding composite multimodal symbols that include both gesture and speech
information. A lattice of possible meanings is derived by projecting on the output of
G W:M.
Because the speech and gesture inputs to multimodal integration and under-
standing are represented as lattices, this framework enables mutual compensation
for errors (Johnston and Bangalore 2005); that is, it allows for information from one
modality to be used to overcome errors in the other. For example, a lower confidence
speech result may be selected through the integration process because it is semantically
compatible with a higher confidence gesture recognition result. It is even possible for
360
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 15
Multimodal grammar fragment.
Figure 16
Multimodal three-tape FSA.
the system to overcome errors in both modalities within a single multimodal utterance.
The multimodal composition process prunes out combinations of speech and gesture
which are not semantically compatible and through combination of weights from the
two different modalities it provides a ranking of the remaining semantically compatible
combinations. This aspect of the approach is not the focus of this article and for ease
361
Computational Linguistics Volume 35, Number 3
Figure 17
Gesture/speech alignment transducer.
Figure 18
Gesture/speech to meaning transducer.
of exposition we have left out weights from the examples given. For the sake of com-
pleteness, we provide a brief description of the treatment of weights in the multimodal
integration mechanism. The speech and gesture lattices contain weights. These weights
are combined through the process of finite-state composition, so the finite-state device
resulting from multimodal integration sums the weights from both the input lattices.
In order to account for differences in reliability between the speech lattice weights and
gesture lattice weights, the weights on the lattices are scaled according to a weighting
factor ? learned from held-out training data. The speech lattice is scaled by ? : 0 < ? < 1
and the gesture lattice by 1? ?. Potentially this scaling factor could be dynamically
adapted based on environmental factors and specific users? performance with the indi-
vidual modes, though in the system described here the scaling factor was fixed for the
duration of the experiment.
3.3 Abstraction over Specific Gesture Content
The semantic content associated with gesture inputs frequently involves specific infor-
mation such as a sequence of map coordinates (e.g., for area gestures) or the identities
of selected entities (e.g., restaurants or theaters). As part of the process of multimodal
integration and understanding this specific content needs to be copied from the gesture
stream into the resulting combined meaning. Within the finite-state mechanism, the
onlyway to copy content is to havematching symbols on the gesture input andmeaning
output tapes. It is not desirable and in some cases infeasible to enumerate all of the
different possible pieces of specific content (such as sequences of coordinates) so that
they can be copied from the gesture input tape to the meaning output tape. This will
significantly increase the size of themachine. In order to capturemultimodal integration
using finite-state methods, it is necessary to abstract over certain aspects of the gestural
content.
We introduce here an approach to abstraction over specific gesture content using
a number of additional finite-state operations. The first step is to represent the gesture
input as a transducer I:Gwhere the input side contains gesture symbols and the specific
content and the output side contains the same gesture symbols but a reserved symbol
SEM appears in place of any specific gestural content such as lists of points or entity
identifiers. The I:G transducer for the gesture lattice G in Figure 11 is as shown in
Figure 19.
362
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 19
I:G transducer: Two restaurants.
Figure 20
Gesture lattice G.
In any location in the multimodal grammar (Figure 15) and corresponding three-
tape finite-state device (Figure 16) where content needs to be copied from the gesture
input into the meaning, the transition :SEM:SEM is used. In the T :(G?W)? M
(Figure 17) transducer these transitions are labeled SEM :SEM.
For composition with the G:W gesture/speech alignment transducer (Figure 18) we
take a projection of the output of the I:G transducer. For the example I:G transducer
(Figure 19) the output projection G is as shown in Figure 20. This projection operation
provides the abstraction over the specific content.
After composing the G andWwith G:W, factoring this transducer into an FSA G W
and composing it with T :(G?W)? M, we are left with a transducer G W:M. This
transducer combines a meaning latticeMwith a specification of the gesture and speech
symbols and is used to determine the meaning of G W.
The next step is to factor out the speech information (W), resulting in a transducer
G:M which relates a meaning lattice M to the gestures involved in determining those
meaningsG. This machine can be composedwith the original I:G transducer (I:G o G:M),
yielding a transducer I:M. The final step is to read off meanings from the I:M transducer.
For each path through the meaning lattice we concatenate symbols from the output M
side, unless theM symbol is SEM in which case we take the input I symbol for that arc.
Essentially, the I:G transducer provides an index back from the gesture symbol sequence
associated with each meaning in the meaning lattice to the specific content associated
with each gesture.
For our example case, if the speech these two restaurants is aligned with the gesture
lattice (Figure 20) using R:G ? W (Figure 18) and the result is then factored and
composed with T :(G?W)? M (Figure 17), the resulting G W:M transducer is as in
Figure 21. This is then factored in the G:M transducer Figure 22 and composed with I:G
(Figure 19), yielding the I:M transducer shown in Figure 23.
Figure 21
G W:M transducer.
363
Computational Linguistics Volume 35, Number 3
Figure 22
G:M transducer.
Figure 23
I:M transducer.
The meaning is generated by reading off and concatenating meaning symbols from
the output of the I:M transducer, except for cases in which the output symbol is SEM,
where instead the input symbol is taken. Alternatively, for all arcs in the I:M transducer
where the output is SEM, the input and output symbols can be swapped (because the
input label represents the value of the SEM variable), and then all paths in M will be
the full meanings with the specific content. For our example case this results in the
following meaning representation: <rest> [r12,r15] </rest>. This example was only for
the DEICNP subgrammar. With the full string phone numbers for these two restaurants
the complete resulting meaning is:<cmd><info><type> phone</type><obj><rest>
[r12,r15] </rest> </obj> </info> </cmd>.
A critical advantage of this approach is that, because the gesture lattice itself is used
to store the specific contents, the retrieval mechanism scales as the size and complexity
of the gesture lattice increases. In the earlier approach more and more variable names
are required as lattices increase in size, and in all places in the grammar where content is
copied from gesture to meaning, arcs must be present for all of these variables. Instead
here we leverage the fact that the gesture lattice itself can be used as a data structure
from which the specific contents can be retrieved using the finite-state operation of
composing I:G and G:M. This has the advantage that the algorithms required for ab-
stracting over the specific contents and then reinserting the content are not required,
and these operations are instead captured within the finite-state mechanism. One of the
advantages of this representation of the abstraction is that it encodes not just the type of
each gesture but also its position within the gesture lattice.
3.4 Gesture Aggregation
Johnston (2000) identifies problems involved inmultimodal understanding and integra-
tion of deictic numeral expressions such as these three restaurants. The problem is that for
a particular spoken phrase there are a multitude of different lexical choices of gesture
and combinations of gestures that can be used to select the specified plurality of entities
and all of these need to be integrated with the spoken phrase. For example, as illustrated
in Figure 24, the user might circle all three restaurants with a single pen stroke, circle
each in turn, or circle a group of two and group of one.
In the unification-based approach to multimodal parsing (Johnston 1998b), captur-
ing all of these possibilities in the spoken language grammar significantly increases its
size and complexity and any plural expression is made massively ambiguous. The sug-
gested alternative in Johnston (2000) is to have the deictic numeral subcategorize for a
plurality of the appropriate number and predictively apply a set of gesture combination
rules in order to combine elements of gestural input into the appropriate pluralities.
364
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 24
Multiple ways to select.
In the finite-state approach described here this can be achieved using a process we
term gesture aggregation, which serves as a pre-processing phase on the gesture input
lattice. A gesture aggregation algorithm traverses the gesture input lattice and adds
new sequences of arcs which represent combinations of adjacent gestures of identical
type. The operation of the gesture aggregation algorithm is described in pseudo-code
in Algorithm 1. The function plurality() retrieves the number of entities in a selection
gesture; for example, for a selection of two entities g1, plurality(g1) = 2. The function
type() yields the type of the gesture; for example rest for a restaurant selection gesture.
The function specific content() yields the specific IDs.
Algorithm 1 Gesture aggregation.
P ? the list of all paths through the gesture lattice GL
while P = ? do
p ? pop(P)
G ?the list of gestures in path p
i ? 1
while i < length(G) do
if g[i] and g[i+ 1] are both selection gestures then
if type(g[i]) == type(g[i+ 1]) then
plurality ? plurality(g[i])+ plurality(g[i+ 1)
start ? start state(g[i])
end ? end state(g[i+ 1])
type ? type(g[i])
specific ? append(specific content(g[i]), specific content(g[i+ 1])
g? ? G area sel plurality type specific
Add g? to GL starting at state start and ending at state end
p? ? the path p but with the arcs from start to end replaced with g?
push p? onto P
i ? i+ 1
end if
end if
end while
end while
365
Computational Linguistics Volume 35, Number 3
Essentially what this algorithm does is perform closure on the gesture lattice of a
function which combines adjacent gestures of identical type. For each pair of adjacent
gestures in the lattice which are of identical type, a new gesture is added to the lattice.
This new gesture starts at the start state of the first gesture and ends at the end state of
the second gesture. Its plurality is equal to the sum of the pluralities of the combining
gestures. The specific content for the new gesture (lists of identifiers of selected objects)
results from appending the specific contents of the two combining gestures. This oper-
ation feeds itself so that sequences of more than two gestures of identical type can be
combined.
For our example case of three selection gestures on three different restaurants
as in Figure 24(2), the gesture lattice before aggregation is as in Figure 25(a). After
aggregation the gesture lattice is as in Figure 25(b). Three new sequences of arcs have
been added. The first, from state 3 to state 8, results from the combination of the first
two gestures; the second, from state 14 to state 24, from the combination of the last two
gestures; and the third, from state 3 to state 24, from the combination of all three ges-
tures. The resulting lattice after the gesture aggregation algorithm has applied is shown
in Figure 25(b). Note that minimization has been applied to collapse identical paths.
A spoken expression such as these three restaurants is aligned with the gesture
symbol sequence G area sel 3 rest SEM in the multimodal grammar. This will be able
to combine not just with a single gesture containing three restaurants but also with our
example gesture lattice, since aggregation adds the path: G area sel 3 rest [id1,id2,id3].
We term this kind of aggregation type specific aggregation. The aggregation process
can be extended to support type non-specific aggregation for cases where users refer to sets
of objects of mixed types and select them using multiple gestures. For example in the
case where the user says tell me about these two and circles a restaurant and then a theater,
non-type specific aggregation applies to combine the two gestures into an aggregate of
mixed type G area sel 2 mix [(id1,id2)] and this is able to combine with these two. For
applications with a richer ontology with multiple levels of hierarchy, the type non-specific
aggregation should assign to the aggregate to the lowest common subtype of the set
of entities being aggregated. In order to differentiate the original sequence of gestures
that the user made from the aggregate, paths added through aggregation are assigned
additional cost.
Figure 25(c) shows how these new processes of gesture abstraction and aggregation
integrate into the overall finite-state multimodal language processing cascade. Aggre-
gation applies to the I:G representation of the gesture. A projection G on the I:G is
composed with the gesture/speech alignment transducer R:G ? W, then the result is
composed with the speech lattice. The resulting G:W transducer is factored into an FSA
with a composite alphabet of symbols. This is then composed with the T :(G?W)? M
yielding a result transducer G W:M. The speech is factored out of the input yielding
G:M which can then be composed with I:G, yielding a transducer I:M from which the
final meanings can be read.
3.5 Temporal Constraints on Multimodal Integration
In the approach taken here, temporal constraints for speech and gesture alignment
are not needed within the multimodal grammar itself. Bellik (1995, 1997) provides
examples indicating the importance of precise temporal constraints for proper interpre-
tation of multimodal utterances. Critically, though, Bellik?s examples involve not single
366
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 25
(a) Three gestures. (b) Aggregated lattice. (c) Multimodal language processing cascade.
367
Computational Linguistics Volume 35, Number 3
multimodal utterances but sequences of two utterances.4 The multimodal integration
mechanism andmultimodal grammars described herein enumerate the content of single
turns of user input, be they unimodal or multimodal. The multimodal integration
component and multimodal grammars are not responsible for combination of content
from different modes that occur in separate dialog turns. This is treated as part of dialog
management and reference resolution. Temporal constraints do, however, play a role in
segmenting parallel multimodal input streams into single user turns. This is one of the
functions of the multimodal understanding component. In order to determine which
gestures and speech should be considered part of a single user utterance, a dynamic
timeout adaptation mechanismwas used. In initial versions of the system, fixed timeout
intervals were used on receipt of input from one modality to see if the input is in fact
unimodal or whether input in the other modality is forthcoming. In pilot studies we
determined that the system latency introduced by these timeouts could be significantly
reduced by making the timeouts sensitive to activity in the other mode. In addition
to messages containing the results of speech recognition and gesture processing, we
instrumented the multimodal understanding component (MMFST) to receive events
indicating when the pen first touches the screen (pen-down event) and when the click-
to-speak button is pressed (click-to-speak event). When theMMFST component receives
a speech lattice, if a gesture lattice has already been received then the two lattices are
processed immediately as a multimodal input. If gesture has not yet been received
and there is no pen-down event, the multimodal component waits for a short timeout
interval before interpreting the speech as a unimodal input. If gesture has not been
received, but there has been a pen-down event, the multimodal component will wait
for a longer timeout period for the gesture lattice message to arrive. Similarly, when
gesture is received, if the speech lattice has already been received the two are integrated
immediately. If speech has not yet arrived, and there was no click-to-speak event, then
the systemwill wait for a short timeout before processing the gesture lattice as unimodal
input. If speech has not yet arrived but the click-to-speak event has been received then
the component will wait for the speech lattice to arrive for a longer timeout period.
Longer timeouts are used instead of waiting indefinitely to account for cases where
the speech or gesture processing does not return a result. In pilot testing we determined
that with the adaptive mechanism the short timeouts could be kept as low as a second or
less, significantly reducing system latency for unimodal inputs. With the non-adaptive
mechanism we required timeouts of as much as two to three seconds. For the longer
timeouts we found 15 seconds to be an appropriate time period. A further extension of
this approach would be to make the timeout mechanism adapt to specific users, since
empirical studies have shown that users tend to fall into specific temporal integration
patterns (Oviatt, DeAngeli, and Kuhn 1997).
The adaptive timeout mechanism could also be used with other speech activation
mechanisms. In an ?open microphone? setting where there is no explicit click-to-speak
event, voice activity detection could be used to signal that a speech event is forthcom-
ing. For our application we chose a ?click-to-speak? strategy over ?open microphone?
because it is more robust to noise andmobile multimodal interfaces are intended for use
in environments subject to noise. The other alternative, ?click-and-hold,? where the user
has to hold down a button for the duration of their speech, is also problematic because
it limits the ability of the user to use pen input while they are speaking.
4 See Johnston and Bangalore (2005) for a detailed explanation.
368
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
3.6 Multimodal Dialog Management and Contextual Resolution
Themultimodal dialog manager (MDM) is based on previous work on speech-act based
models of dialog (Rich and Sidner 1998; Stent et al 1999). It uses a Java-based toolkit
for writing dialog managers that is similar in philosophy to TrindiKit (Larsson et al
1999). It includes several rule-based processes that operate on a shared state. The state
includes system and user intentions and beliefs, a dialog history and focus space, and
information about the speaker, the domain, and the available modalities. The processes
include interpretation, update, selection, and generation.
The interpretation process takes as input an n-best list of possible multimodal
interpretations for a user input from the MMFST. It rescores them according to a set
of rules that encode the most likely next speech act given the current dialogue context,
and picks the most likely interpretation from the result. The update process updates
the dialogue context according to the system?s interpretation of user input. It augments
the dialogue history, focus space, models of user and system beliefs, and model of user
intentions. It also alters the list of current modalities to reflect those most recently used
by the user.
The selection process determines the system?s next move(s). In the case of a com-
mand, request, or question, it first checks that the input is fully specified (using the
domain ontology, which contains information about required and optional roles for
different types of actions); if it is not, then the system?s next move is to take the
initiative and start an information-gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or answer the question; to do this,
MDM communicates directly with the UI.
The generation process performs template-based generation for simple responses
and updates the system?s model of the user?s intentions after generation. A text plan-
ning component (TEXTPLAN) is used for more complex generation, such as the gener-
ation of comparisons (Walker et al 2002, 2004).
In the case of a navigational query, such as the example in Section 2, MDM first
receives a route query in which only the destination is specified: How do I get to this
place?. In the selection phase it consults the domain ontology and determines that a
source is also required for a route. It adds a request to query the user for the source to the
system?s next moves. This move is selected and the generation process selects a prompt
and sends it to the TTS component. The system asks Where do you want to go from?. If
the user says or writes 25th Street and 3rd Avenue then the MMFST will assign this input
two possible interpretations: either this is a request to zoom the display to the specified
location or it is an assertion of a location. Because theMDMdialogue state indicates that
it is waiting for an answer of the type location, MDM reranks the assertion as the most
likely interpretation. A generalized overlay process (Alexandersson and Becker 2001)
is used to take the content of the assertion (a location) and add it into the partial route
request. The result is determined to be complete. The UI resolves the location to map
coordinates and passes on a route request to the SUBWAY component.
We found this traditional speech-act based dialogue manager worked well for our
multimodal interface. Critical in this was our use of a common semantic representation
across spoken, gestured, andmultimodal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users flexibility in the mode they choose
to advance the dialogue.
One of the roles of the multimodal dialog manager is to handle contextual res-
olution of deictic expressions. Because they can potentially be resolved either by in-
tegration with a gesture, or from context, deictic expressions such as this restaurant are
369
Computational Linguistics Volume 35, Number 3
ambiguous in the multimodal grammar. There will be one path through the grammar
where this expression is associated with a sequence of gesture symbols, such as G
area selection 1 rest r123, and another where it is not associated with any gesture sym-
bols and assigned a semantic representation which indicates that it must be resolved
from context: <rest><discourseref></discourseref></rest>. If at the multimodal un-
derstanding stage there is a gesture of the appropriate type in the gesture lattice,
then the first of these paths will be chosen and the identifier associated with the
gesture will be added to the semantics during the multimodal integration and under-
standing process: <rest>r123</rest>. If there is no gesture, then this restaurant will
be assigned the semantic representation <rest><discourseref></discourseref></rest>
and the dialog manager will attempt to resolve the gesture from the dialog context.
The update process in the multimodal dialog manager maintains a record in the fo-
cus space of the last mention of entities of each semantic type, and the last men-
tioned entity. When the interpretation process receives a semantic representation
containing the marker <rest><discourseref></discourseref></rest> it replaces <dis-
courseref></discourseref> with the identifier of the last-mentioned entity of the type
restaurant.
Cases where the gesture is a low-confidence recognition result, in fact, where
the gesture is spurious and not an intentional input, are handled using back-offs in
the multimodal grammar as follows: In the multimodal grammar, productions are
added which consume a gesture from the gesture lattice, but assign the semantics
<rest><discourseref></discourseref></rest>. Generally these are assigned a higher cost
than paths through the model where the gesture is meaningful, so that these back-
off paths will only be chosen if there is no alternative. In practice for speech and
pen systems of the kind described here, we have found that spurious gestures are
uncommon, though they are likely to be considerably more of a problem for other kinds
of modalities, such as freehand gesture recognized using computer vision.
3.7 Experimental Evaluation
To determine the baseline performance of the finite-state approach to multimodal in-
tegration and understanding, and to collect data for the experiments on multimodal
robustness described in this article, we collected and annotated a corpus of multimodal
data as described in Section 2.2. To enable this initial experiment and data collection,
because no corpus data had already been collected, to bootstrap the process we initially
used a handcrafted multimodal grammar using grammar templates combined with
data from the underlying application database. As shown in Figure 26, the multimodal
grammar can be used to create language models for ASR, align the speech and gesture
results from the respective recognizers, and transform the multimodal utterance to a
meaning representation. All these operations are achieved using finite-state transducer
operations.
For the 709 inputs that involve speech (491 unimodal speech and 218 multimodal)
we calculated the speech recognition accuracy (word and sentence level) for results
using the grammar-based language model projected from the multimodal grammar. We
also calculated a series of measures of concept accuracy on the meaning representations
resulting from taking the results from speech recognition and combining them with the
gesture lattice using the gesture speech alignment model, and then the multimodal un-
derstanding model. The concept accuracy measures: Concept Sentence Accuracy, Predicate
Sentence Accuracy, and Argument Sentence Accuracy are explained subsequently.
370
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 26
Multimodal grammar compilation for different processes of MATCH.
The hierarchically-nested XML representation described in Section 3.1 is effective
for processing by the backend application, but is not well suited for the automated
determination of the performance of the language understanding mechanism. We de-
veloped an approach, similar to Ciaramella (1993) and Boros et al (1996), in which
the meaning representation, in our case XML, is transformed into a sorted flat list of
attribute?value pairs indicating the core contentful concepts of each command. The
attribute?value meaning representation normalizes over multiple different XML rep-
resentations which correspond to the same underlying meaning. For example, phone
and address and address and phone receive different XML representations but the same
attribute?value representation. For the example phone number of this restaurant, the XML
representation is as in Figure 27, and the corresponding attribute?value representation
is as in Figure 28.
Figure 27
XML meaning representation.
cmd:info type:phone object:selection. (1)
Figure 28
Attribute?value meaning representation.
371
Computational Linguistics Volume 35, Number 3
Table 3
ASR and concept accuracy for the grammar-based finite-state approach (10-fold).
Speech recognition Word accuracy 41.6%
Sentence accuracy 38.0%
Understanding Concept sentence accuracy 50.7%
Predicate accuracy 67.2%
Argument accuracy 52.8%
This transformation of the meaning representation allows us to calculate the per-
formance of the understanding component using string-matching metrics parallel to
those used for speech recognition accuracy. Concept Sentence Accuracy measures
the number of user inputs for which the system got the meaning completely right.5
Predicate Sentence Accuracymeasures whether the main predicate of the sentence was
correct (similar to call type in call classification). Argument Sentence Accuracy is an
exact string match between the reference list of arguments and the list of arguments
identified for the command. Note that the reference and hypothesized argument se-
quences are lexicographically sorted before comparison so the order of the arguments
does not matter. We do not utilize the equivalent of word accuracy on the concept token
sequence. The concept-level equivalent of word accuracy is problematic because it can
easily be manipulated by increasing or decreasing the number of tokens used in the
meaning representation.
To provide a baseline for the series of techniques explored in the rest of the article,
we performed recognition and understanding experiments on the same 10 partitions
of the data as in Section 4. The numbers are all averages over all 10 partitions. Table 3
shows the speech recognition accuracy using the grammar-based language model pro-
jected from the multimodal grammar. It also shows the concept accuracy results for the
multimodal?grammar-based finite-state approach to multimodal understanding.
The multimodal grammars described here provide an expressive mechanism for
quickly creating language processing capabilities for multimodal interfaces support-
ing input modes such as speech and pen, but like other approaches based on hand-
crafted grammars, multimodal grammars are brittle with respect to extra-grammatical
or erroneous input. The language model directly projected from the speech portion of
the hand-crafted multimodal grammar is not able to recognize any strings that are not
encoded in the grammar. In our data, 62% of user?s utterances were out of the multi-
modal grammar, a major problem for recognition (as illustrated in Table 3). The poor
ASR performance has a direct impact on concept accuracy. The fact that the score for
concept sentence accuracy is higher than that for sentence accuracy is not unexpected
since recognition errors do not always result in changes in meaning and also to a certain
extent the grammar-based language model will force fit out-of-grammar utterances to
similar in-grammar utterances.
4. Robustness in Multimodal Language Processing
A limitation of grammar-based approaches to (multimodal) language processing is
that the user?s input is often not covered by the grammar and hence fails to receive
an interpretation. This issue is present in grammar-based speech-only dialog systems
5 This metric is called Sentence Understanding in Ciaramella (1993).
372
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
as well. The lack of robustness in such systems is due to limitations in (a) language
modeling and (b) understanding of the speech recognition output.
The brittleness of using a grammar as a language model is typically alleviated by
building SLMs that capture the distribution of the user?s interactions in an application
domain. However, such SLMs are trained on large amounts of spoken interactions
collected in that domain?a tedious task in itself, in speech-only systems, but an often
insurmountable task in multimodal systems. The problem we face is how to make
multimodal systems more robust to disfluent or unexpected multimodal language in
applications for which little or no training data is available. The reliance on multimodal
grammars as a source of data is inevitable in such situations. In Section 5, we explore
and evaluate a range of different techniques for building effective SLMs for spoken
and multimodal systems under constraints of limited training data. The techniques are
presented in the context of SLMs, since spoken language interaction tends to be a dom-
inant mode in our application and has higher perplexity than the gesture interactions.
However, most of these techniques can also be applied to improve the robustness of the
gesture recognition component in applications with higher gesture language perplexity.
The second source of brittleness in a grammar-based multimodal/unimodal inter-
active system is in the assignment of meaning to the multimodal output. The grammar
typically encodes the relation between the multimodal inputs and their meanings. The
assignment of meaning to a multimodal output is achieved by parsing the utterance
using the grammar. In a grammar-based speech-only system, if the language model of
ASR is derived directly from the grammar, then every ASR output can be parsed and
assigned a meaning by the grammar. However, using an SLM results in ASR outputs
that may not be parsable by the grammar and hence cannot be assigned a meaning by
the grammar. Robustness in such cases is achieved by either (a) modifying the parser to
accommodate for unparsable substrings in the input (Ward 1991; Dowding et al 1993;
Allen et al 2001) or (b) modifying the meaning representation to make it learnable as
a classification task using robust machine learning techniques as is done in large scale
human-machine dialog systems (e.g., Gorin, Riccardi, and Wright 1997).
In our grammar-based multimodal system, the grammar serves as the speech-
gesture alignment model and assigns a meaning representation to the multimodal
input. Failure to parse amultimodal input implies that the speech and gesture inputs are
not fused together and consequently may not be assigned a meaning representation. In
order to improve robustness in multimodal understanding, more flexible mechanisms
must be employed in the integration and the meaning-assignment phases. In Section 6,
we explore and evaluate approaches that transform the multimodal input so as to
be parsable by the multimodal grammar, as well as methods that directly map the
multimodal input to the meaning representation without the use of the grammar. We
again present these approaches in the context of transformation of the ASR output,
but they are equally applicable to gesture recognition outputs. Transformation of the
multimodal inputs so as to be parsable by the multimodal grammar directly improves
robustness of multimodal integration and understanding.
Although some of the techniques presented in the next two sections are known
in the literature, they have typically been applied in the context of speech-only dialog
systems and on different application domains. As a result, comparing the strengths and
weaknesses of these techniques is very difficult. By evaluating them on the MATCH
domain, we are able to compare and extend these techniques for robust multimodal
understanding. Other factors such as contextual information including dialog context,
graphical display context, geographical context, as well as meta-information such as
user preferences and profiles, can be used to further enhance the robustness of a
373
Computational Linguistics Volume 35, Number 3
multimodal application. However, here we focus on techniques for improving robust-
ness of multimodal understanding that do not rely on such factors.
5. Robustness of Language Models for Speech Recognition
The problem of speech recognition can be succinctly represented as a search for the
most likely word sequence (w?) through the network created by the composition of a
language of acoustic observations (O), an acoustic model which is a transduction from
acoustic observations to phone sequences (A), a pronunciation model which is a trans-
duction from phone sequences to word sequences (L), and a language model acceptor
(G) (Pereira and Riley 1997) (Equation 2). The language model acceptor encodes the
(weighted) word sequences permitted in an application.
w? = argmax
w
?2(O ? A ? L ? G)(w) (2)
Typically, G is built using either a hand-crafted grammar or using a statistical lan-
guagemodel derived from a corpus of sentences from the application domain. Although
a grammar could bewritten so as to be easily portable across applications, it suffers from
being too prescriptive and has no metric for the relative likelihood of users? utterances.
In contrast, in the data-driven approach a weighted grammar is automatically induced
from a corpus and the weights can be interpreted as a measure of the relative likeli-
hoods of users? utterances. However, the reliance on a domain-specific corpus is one
of the significant bottlenecks of data-driven approaches, because collecting a corpus
specific to a domain is an expensive and time-consuming task, especially formultimodal
applications.
In this section, we investigate a range of techniques for producing a domain-specific
corpus using resources such as a domain-specific grammar as well as an out-of-domain
corpus. We refer to the corpus resulting from such techniques as a domain-specific de-
rived corpus in contrast to a domain-specific collected corpus. We are interested in techniques
that would result in corpora such that the performance of language models trained on
these corpora would rival the performance of models trained on corpora collected for
a specific domain. We investigate these techniques in the context of MATCH. We use
the notation Cd for the corpus, ?d for the language model built using the corpus Cd, and
G?d for the language model acceptor representation of the model ?d which can be used
in Equation (2).
5.1 Language Model Using In-Domain Corpus
We used the MATCH domain corpus from the data collection to build a class-based
trigram language model (?MATCH) using the 709 multimodal and speech-only utterances
as the corpus (CMATCH). We used the names of cuisine types, areas of interest, points of
interest, and neighborhoods as classes when building the trigram language model. The
trigram language model is represented as a weighted finite-state acceptor (Allauzen,
Mohri, and Roark 2003) for speech recognition purposes. The performance of this model
serves as the point of reference to compare the performance of language models trained
on derived corpora.
374
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
5.2 Grammar as Language Model
The multimodal context-free grammar (CFG; a fragment is presented in Section 2 and a
larger fragment is shown in Section 3.2) encodes the repertoire of language and gesture
commands allowed by the system and their combined interpretations. The CFG can
be approximated by a finite state machine (FSM) with arcs labeled with language,
gesture, and meaning symbols, using well-known compilation techniques (Nederhof
1997). Selecting the language symbol of each arc (projecting the FSM on the speech
component) results in an FSM that can be used as the language model acceptor (Ggram)
for speech recognition. Note that the resulting languagemodel acceptor is unweighted if
the grammar is unweighted and suffers from not being robust to language variations in
users? input. However, due to the tight coupling of the grammars used for recognition
and interpretation, every recognized string can be assigned a meaning representation
(though it may not necessarily be the intended interpretation).
5.3 Grammar-Based n-gram Language Model
As mentioned earlier, a hand-crafted grammar typically suffers from the problem of
being too restrictive and inadequate to cover the variations and extra-grammaticality of
users? input. In contrast, an n-gram languagemodel derives its robustness by permitting
all strings over an alphabet, albeit with different likelihoods. In an attempt to provide
robustness to the grammar-based model, we created a corpus (Cgram) of k sentences by
randomly sampling the set of paths of the grammar (Ggram)
6 and built a class-based
n-gram language model (?gram) using this corpus. Although this corpus does not rep-
resent the true distribution of sentences in the MATCH domain, we are able to derive
some of the benefits of n-gram language modeling techniques. Similar approaches have
been presented in Galescu, Ringger, and Allen (1998) and Wang and Acero (2003).
5.4 Combining Grammar and Corpus
A straightforward extension of the idea of sampling the grammar in order to create a
corpus is to select those sentences out of the grammar which make the resulting corpus
?similar? to the corpus collected in the pilot studies. In order to create this corpus Cclose,
we choose the k most likely sentences as determined by a language model (?MATCH)
built using the collected corpus. A mixture model (?mix) with mixture weight (?) is
built by interpolating the model trained on the corpus of extracted sentences (?close)
and the model trained on the collected corpus (?MATCH). This method is summarized
in Equation (4), where L(M) represents the language recognized by the multimodal
grammar (M).
Cclose = {S1, . . . Sk|Si ? L(M) ? Pr?MATCH (Si) > Pr?MATCH (Si+1)
?  ? j Pr?MATCH (Si) > Pr?MATCH (Sj) > Pr?MATCH (Si+1) (3)
?mix = ? ? ?close + (1? ?) ? ?MATCH (4)
6 We can also randomly sample a sub-network without expanding the k paths.
375
Computational Linguistics Volume 35, Number 3
5.5 Class-Based Out-of-Domain Language Model
An alternative to using in-domain corpora for building language models is to ?migrate?
a corpus of a different domain to our domain. The process of migrating a corpus
involves suitably generalizing the corpus to remove information that is specific only
to the other domain and instantiating the generalized corpus to our domain. Although
there are a number of ways of generalizing the out-of-domain corpus, the generalization
we have investigated involved identifying linguistic units, such as noun and verb
chunks, in the out-of-domain corpus and treating them as classes. These classes are
then instantiated to the corresponding linguistic units from the MATCH domain. The
identification of the linguistic units in the out-of-domain corpus is done automatically
using a supertagger (Bangalore and Joshi 1999). We use a corpus collected in the context
of a software help-desk application as an example out-of-domain corpus. In cases where
the out-of-domain corpus is closely related to the domain at hand, a more semantically
driven generalization might be more suitable. Figure 29 illustrates the process of mi-
grating data from one domain to another.
5.6 Adapting the Switchboard Language Model
We investigated the performance of a large-vocabulary conversational speech recogni-
tion system when applied to a specific domain such as MATCH. We used the Switch-
board corpus (Cswbd) as an example of a large-vocabulary conversational speech corpus.
We built a trigrammodel (?swbd) using the 5.4-million-word corpus and investigated the
effect of adapting the Switchboard language model given k in-domain untranscribed
speech utterances ({OiM}). The adaptation is done by first recognizing the in-domain
speech utterances and then building a language model (?adapt) from the corpus of
recognized text (Cadapt). This bootstrapping mechanism can be used to derive a domain-
specific corpus and language model without any transcriptions. Similar techniques for
Figure 29
A method for migration of data from one domain to another domain.
376
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
unsupervised language model adaptation are presented in Bacchiani and Roark (2003)
and Souvignier and Kellner (1998).
Cadapt = {S1,S2, . . . ,Sk} (5)
Si = argmax
S
?2(O
i
M ? A ? L ? Gswbd)(S)
5.7 Adapting a Wide-Coverage Grammar
There have been a number of computational implementations of wide-coverage,
domain-independent, syntactic grammars for English in various grammar formalisms
(Flickinger, Copestake, and Sag 2000; XTAG 2001; Clark and Hockenmaier 2002). Here,
we describe a method that exploits one such grammar implementation in the Lexical-
ized Tree-Adjoining Grammar (LTAG) formalism, for deriving domain-specific corpora.
An LTAG consists of a set of elementary trees (supertags) (Bangalore and Joshi 1999)
each associated with a lexical item (the head). Supertags encode predicate?argument
relations of the head and the linear order of its arguments with respect to the head.
In Figure 30, we show the supertag associated with the word show in an imperative
sentence such as show the Empire State Building. A supertag can be represented as a
finite-state machine with the head and its arguments as arc labels (Figure 31). The set
of sentences generated by an LTAG can be obtained by combining supertags using
substitution and adjunction operations. In related work (Rambow et al 2002), it has
been shown that for a restricted version of LTAG, the combinations of a set of supertags
can be represented as an FSM. This FSM compactly encodes the set of sentences gen-
erated by an LTAG grammar. It is composed of two transducers, a lexical FST, and a
syntactic FSM.
The lexical FST transduces input words to supertags. We assume that as input to the
construction of the lexical machine we have a list of words with their parts-of-speech.
Once we have determined for each word the set of supertags they should be associated
Figure 30
Supertag tree for the word show. The NP nodes permit substitution of all supertags with root
node labeled NP.
Figure 31
FSM for the word show. The ?NP arc permits replacement with FSMs representing NP.
377
Computational Linguistics Volume 35, Number 3
with, we create a disjunctive finite-state transducer (FST) for all words which transduces
the words to their supertags.
For the syntactic FSM, we take the union of all the FSMs for each supertag which
corresponds to an initial tree (i.e., a tree which need not be adjoined). We then perform
a series of iterative replacements: In each iteration, we replace each arc labeled by a
supertag by its lexicalized version of that supertag?s FSM. Of course, in each iteration,
there are many more replacements than in the previous iteration. Based on the syntactic
complexity in our domain (such as number of modifiers, clausal embedding, and prepo-
sitional phrases), we use five rounds of iteration. The number of iterations restricts the
syntactic complexity but not the length of the input. This construction is in many ways
similar to constructions proposed for CFGs, in particular that of Nederhof (1997). One
difference is that, because we start from TAG, recursion is already factored, andwe need
not find cycles in the rules of the grammar.
We derive a MATCH domain-specific corpus by constructing a lexicon consisting
of pairings of words with their supertags that are relevant to this domain. We then
compile the grammar to build an FSM of all sentences up to a given depth of recursion.
We sample this FSM and build a language model as discussed in Section 5.3. Given
untranscribed utterances from a specific domain, we can also adapt the language model
as discussed in Section 5.6.
5.8 Speech Recognition Experiments
We describe a set of experiments to evaluate the performance of the language model
in the MATCH multimodal system. We use word accuracy and string accuracy for
evaluating ASR output. All results presented in this section are based on 10-fold cross-
validation experiments run on the 709 spoken andmultimodal exchanges collected from
the pilot study described in Section 2.2.
Table 4 presents the performance results for ASR word and sentence accuracy using
language models trained on the collected in-domain corpus as well as on corpora
derived using the different methods discussed in Sections 5.2?5.7. For the class-based
models mentioned in the table, we defined different classes based on areas of interest
(e.g., riverside park, turtle pond), points of interest (e.g., Ellis Island, United Nations Build-
ing), type of cuisine (e.g., Afghani, Indonesian), price categories (e.g., moderately priced,
expensive), and neighborhoods (e.g., Upper East Side, Chinatown).
It is immediately apparent that the hand-crafted grammar as a language model
performs poorly and a language model trained on the collected domain-specific corpus
performs significantly better than models trained on derived data. However, it is en-
couraging to note that a model trained on a derived corpus (obtained from combining
the migrated out-of-domain corpus and a corpus created by sampling the in-domain
grammar) is within 10% word accuracy as compared to the model trained on the col-
lected corpus. There are several other noteworthy observations from these experiments.
The performance of the languagemodel trained on data sampled from the grammar
is dramatically better as compared to the performance of the hand-crafted grammar.
This technique provides a promising direction for authoring portable grammars that can
be sampled subsequently to build robust language models when no in-domain corpora
are available. Furthermore, combining grammar and in-domain data, as described in
Section 5.4, outperforms all other models significantly.
For the experiment on the migration of an out-of-domain corpus, we used a corpus
from a software help-desk application. Table 4 shows that the migration of data using
378
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Table 4
Performance results for ASR word and sentence accuracy using models trained on data derived
from different methods of bootstrapping domain-specific data.
Scenario ASR Word Sentence
Accuracy Accuracy
1 Grammar-based Grammar as language model 41.6 38.0
(Section 5.2)
Class-based n-gram language 60.6 42.9
model (Section 5.3)
2 In-domain Data Class-based n-gram model 73.8 57.1
(Section 5.1)
3 Grammar+ Class-based n-gram model 75.0 59.5
In-domain Data (Section 5.4)
4 Out-of-domain n-gram model 17.6 17.5
(Section 5.5) Class-based n-gram model 58.4 38.8
Class-based n-gram model
with Grammar-based n-gram
Language Model 64.0 45.4
5 Switchboard n-gram model 43.5 25.0
(Section 5.6) Language model trained on
recognized in-domain data 55.7 36.3
6 Wide-coverage n-gram model 43.7 24.8
Grammar Language model trained on
(Section 5.7) recognized in-domain data 55.8 36.2
linguistic units as described in Section 5.5 significantly outperforms a model trained
only on the out-of-domain corpus. Also, combining the grammar sampled corpus with
the migrated corpus provides further improvement.
The performance of the Switchboard model on the MATCH domain is presented
in the fifth row of Table 4. We built a trigram model using a 5.4-million-word Switch-
board corpus and investigated the effect of adapting the resulting language model on
in-domain untranscribed speech utterances. The adaptation is done by first running
the recognizer on the training partition of the in-domain speech utterances and then
building a language model from the recognized text. We observe that although the
performance of the Switchboard language model on the MATCH domain is poorer than
the performance of a model obtained by migrating data from a related domain, the
performance can be significantly improved using the adaptation technique.
The last row of Table 4 shows the results of using the MATCH specific lexicon to
generate a corpus using a wide-coverage grammar, training a language model, and
adapting the resulting model using in-domain untranscribed speech utterances as was
done for the Switchboard model. The class-based trigrammodel was built using 500,000
randomly sampled paths from the network constructed by the procedure described in
Section 5.7. It is interesting to note that the performance is very similar to the Switch-
board model given that the wide-coverage grammar is not designed for conversational
speech unlike models derived from Switchboard data. The data from the domain has
some elements of conversational-style speech which the Switchboard model models
well, but it also has syntactic constructions that are adequately modeled by the wide-
coverage grammar.
379
Computational Linguistics Volume 35, Number 3
In this section, we have presented a range of techniques to build language models
for speech recognition which are applicable at different development phases of an
application. Although the utility of in-domain data cannot be obviated, we have shown
that there are ways to approximate this data with a combination of grammar and out-of-
domain data. These techniques are particularly useful in the initial phases of application
development when there is very little in-domain data. The technique of authoring a
domain-specific grammar that is sampled for n-gram model building presents a good
trade-off between time-to-create and the robustness of the resulting language model.
Thismethod can be extended by incorporating suitably generalized out-of-domain data,
in order to approximate the distribution of n-grams in the in-domain data. If time to
develop is of utmost importance, we have shown that using a large out-of-domain
corpus (Switchboard) or a wide-coverage domain-independent grammar can yield a
reasonable language model.
6. Robust Multimodal Understanding
In Section 3, we showed how multimodal grammars can be compiled into finite-state
transducers enabling effective processing of lattice input from speech and gesture
recognition and mutual compensation for errors and ambiguities. However, like other
approaches based on hand-crafted grammars, multimodal grammars can be brittle
with respect to extra-grammatical, erroneous, and disfluent input. Also, the primary
applications of multimodal interfaces include use in noisymobile environments and use
by inexperienced users (for whom they provide a more natural interaction); therefore it
is critical that multimodal interfaces provide a high degree of robustness to unexpected
or ill-formed inputs.
In the previous section, we presented methods for bootstrapping domain-specific
corpora for the purpose of training robust languagemodels used for speech recognition.
Thesemethods overcome the brittleness of a grammar-based languagemodel. Although
the corpus-driven language model might recognize a user?s utterance correctly, the
recognized utterance may not be assigned a semantic representation by the multimodal
grammar if the utterance is not part of the grammar.
In this section, we address the issue of robustness in multimodal understanding.
Robustness in multimodal understanding results from improving robustness to speech
recognition and gesture recognition errors. Although the techniques in this section are
presented as applying to the output of a speech recognizer, they are equally applicable to
the output of a gesture recognizer.We chose to focus on robustness to speech recognition
errors because the errors in a gesture recognizer are typically smaller than in a speech
recognizer due to smaller vocabulary and lower perplexity.
There have been two main approaches to improving robustness of the under-
standing component in the spoken language understanding literature. First, a parsing-
based approach attempts to recover partial parses from the parse chart when the
input cannot be parsed in its entirety due to noise, in order to construct a (partial)
semantic representation (Ward 1991; Dowding et al 1993; Allen et al 2001). Second,
a classification-based approach, adopted from the Information Extraction literature,
views the problem of understanding as extracting certain bits of information from the
input. It attempts to classify the utterance and identifies substrings of the input as slot-
filler values to construct a frame-like semantic representation. Both approaches have
limitations. Although in the first approach the grammar can encode richer semantic
representations, the method for combining the fragmented parses is quite ad hoc. In the
380
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
second approach, the robustness is derived from training classifiers on annotated data;
this data is very expensive to collect and annotate, and the semantic representation
is fairly limited. There is some more recent work on using structured classification
approaches to transduce sentences to logical forms (Papineni, Roukos, and Ward 1997;
Thompson and Mooney 2003; Zettlemoyer and Collins 2005). However, it is not clear
how to extend these approaches to apply to lattice input?an important requirement
for multimodal processing.
6.1 Evaluation Issue
Before we present the methods for robust understanding, we discuss the issue of data
partitions to evaluate these methods on. Due to the limited amount of data, we run
cross-validation experiments in order to arrive at reliable performance estimates for
these methods. However, we have a choice in terms of how the data is split into training
and test partitions for the cross-validation runs. We could randomly split the data for
an n-fold (for example, 10-fold) cross-validation test. However, the data contain several
repeated attempts by users performing the six scenarios. A random partitioning of
these data would inevitably have the same multimodal utterances in training and test
partitions. We believe that this would result in an overly optimistic estimate of the
performance. In order to address this issue, we run 6-fold cross-validation experiments
by using five scenarios as the training set and the sixth scenario as the test set. This way
of partitioning the data overly handicaps data-driven methods because the distribution
of data in the training and test partitions for each cross-validation run would be sig-
nificantly different. In the experiment results for each method, we present 10-fold and
6-fold cross-validation results where appropriate in order to demonstrate the strengths
and limitations of each method. For all the experiments in this section, we used a data-
driven language model for ASR. The word accuracy of the ASR is 73.8%, averaged over
all scenarios and all speakers.
6.2 Classification-Based Approach
In this approach we view robust multimodal understanding as a sequence of classifica-
tion problems in order to determine the predicate and arguments of an utterance. The
set of predicates are the same set of predicates used in the meaning representation.
The meaning representation shown in Figure 28 consists of a predicate (the command
attribute) and a sequence of one or more argument attributes which are the parame-
ters for the successful interpretation of the user?s intent. For example, in Figure 28,
cmd:info is the predicate and type:phone object:selection are the arguments to the
predicate.
We determine the predicate (c?) for a N token multimodal utterance (SN1 ) by
searching for the predicate (c) that maximizes the posterior probability as shown in
Equation (6).
c? = argmax
c
P(c | SN1 ) (6)
We view the problem of identifying and extracting arguments from a multimodal
input as a problem of associating each token of the input with a specific tag that encodes
381
Computational Linguistics Volume 35, Number 3
the label of the argument and the span of the argument. These tags are drawn from
a tagset which is constructed by extending each argument label by three additional
symbols I,O,B, following Ramshaw and Marcus (1995). These symbols correspond to
cases when a token is inside (I) an argument span, outside (O) an argument span, or at
the boundary of two argument spans (B) (See Table 5).
Given this encoding, the problem of extracting the arguments amounts to a search
for the most likely sequence of tags (T?) given the input multimodal utterance SN1
as shown in Equation (7). We approximate the posterior probability P(T | SN1 ) using
independence assumptions to include the lexical context in an n-word window and the
preceding two tag labels, as shown in Equation (8).
T? = argmax
T
P(T | SN1 ) (7)
? argmax
T
?
i
P(ti | Sii?n,S
i+n+1
i+1 , ti?1, ti?2) (8)
Owing to the large set of features that are used for predicate identification and
argument extraction, which typically result in sparseness problems for generative mod-
els, we estimate the probabilities using a classification model. In particular, we use
the Adaboost classifier (Schapire 1999) wherein a highly accurate classifier is built by
combining many ?weak? or ?simple? base classifiers fi, each of which may only be
moderately accurate. The selection of the weak classifiers proceeds iteratively, picking
the weak classifier that correctly classifies the examples that are misclassified by the
previously-selected weak classifiers. Each weak classifier is associated with a weight
(wi) that reflects its contribution towards minimizing the classification error. The pos-
terior probability of P(c | x) is computed as in Equation 9. For our experiments, we use
simple n-grams of the multimodal utterance to be classified as weak classifiers.
P(c | x) = 1
(1+ e?2?
?
i wi?fi(x) )
(9)
For the experiments presented subsequently we use the data collected from the
domain to train the classifiers. However, the data could be derived from an in-domain
grammar using techniques similar to those presented in Section 5.
Table 5
The {I,O,B} encoding for argument extraction.
User cheap thai upper west side
Utterance
Argument <price> cheap </price> <cuisine>
Annotation thai </cuisine> <place> upper west
side </place>
IOB cheap price<B> thai cuisine<B>
Encoding upper place<I> west place<I>
side place<I>
382
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
6.2.1 Experiments and Results. We used a total of 10 predicates such as help, assert,
inforequest, and 20 argument types such as cuisine, price, location for our experiments.
These were derived from our meaning representation language. We used unigrams,
bigrams, and trigrams appearing in the multimodal utterance as weak classifiers for
the purpose of predicate classification. In order to predict the tag of a word for ar-
gument extraction, we used the left and right trigram context and the tags for the
preceding two tokens as weak classifiers. The results are presented in Table 6. We
present the concept sentence accuracy and the predicate and argument string accuracy
of the grammar-based understandingmodel and the classification-based understanding
model. The corresponding accuracy results on the 10-fold cross-validation experiments
are shown in parentheses. As can be seen, the grammar-based model significantly out-
performs the classification-based approach on the 6-fold cross-validation experiments
and the classification-based approach outperforms the grammar-based approach on the
10-fold cross-validation experiments. This is to be expected since the classification-
based approach needs to generalize significantly from the training set to the test set,
and these have different distributions of predicates and arguments in the 6-fold cross-
validation experiments.
A significant shortcoming of the classification approach is that it does not exploit
the semantic grammar from the MATCH domain to constrain the possible choices
from the classifier. Also, the classifier is trained using the data that is collected in
this domain. However, the grammar is a rich source of distribution-free data. It is
conceivable to sample the grammar in order to increase the training examples for the
classifier, in the same spirit as was done for building a language model using the
grammar (Section 5.3). Furthermore, knowledge encoded in the grammar and data
can be combined by techniques presented in Schapire et al (2002) to improve classifier
performance.
Another limitation of this approach is that it is unclear how to extend it to apply
to speech and gesture lattices. As shown in earlier sections, multimodal understanding
receives ambiguous speech and gesture inputs encoded as lattices. Mutual disambigua-
tion between these two modalities needs to be exploited. Although the classification
approach can be extended to apply to n-best lists of speech and gesture inputs, we prefer
an approach that can apply to lattice inputs directly.
6.3 Noisy Channel Model for Error Correction
In order to address the limitations of the classification-based approach, we explore an
alternate method for robust multimodal understanding. We translate the user?s input
to a string that can be assigned a meaning representation by the grammar. We can
Table 6
Concept accuracy results from classification-based model using data-driven language model for
ASR. (Numbers are percent for 6-fold cross validation by scenario. Corresponding percent for
10-fold cross validation are given in parentheses.)
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy % Accuracy % Accuracy %
Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)
Classification-based 34.0 (58.3) 71.4 (85.5) 32.8 (61.4)
383
Computational Linguistics Volume 35, Number 3
apply this technique on a user?s gesture input as well in order to compensate for
gesture recognition errors.We couch the problem of error correction in the noisy channel
modeling framework. In this regard, we follow Ringger and Allen (1996) and Ristad
and Yianilos (1998); however, we encode the error correction model as a weighted FST
so we can directly edit speech/gesture input lattices. As mentioned earlier, we rely on
integrating speech and gesture lattices to avoid premature pruning of admissible so-
lutions for robust multimodal understanding. Furthermore, unlike Ringger and Allen,
the language grammar from our application filters out edited strings that cannot be
assigned an interpretation by the multimodal grammar. Also, whereas in Ringger and
Allen the goal is to translate to the reference string and improve recognition accuracy, in
our approach the goal is to translate the input in order to assign the reference meaning
and improve concept accuracy.
We let Sg be the string that can be assigned a meaning representation by the
grammar and Su be the user?s input utterance. If we consider Su to be the noisy version
of the Sg, we view the decoding task as a search for the string S
?
g as shown in Equa-
tion (10). Note we formulate this as a joint probability maximization as in Equation (11).
Equation (12) expands the sequence probability by the chain rule where Siu and S
i
g
are the ith tokens from Su and Sg respectively. We use a Markov approximation (limiting
the dependence on the history to the past two time steps: trigram assumption for our
purposes) to compute the joint probability P(Su,Sg), shown in Equation (13).
S?g = argmax
Sg
P(Sg|Su) (10)
= argmax
Sg
P(Sg,Su) (11)
= argmax
Sg
P(S0u,S
0
g) ? P(S
1
u,S
1
g|S
0
u,S
0
g) . . . ? P(S
n
u,S
n
g |S
0
u,S
0
g, . . . ,S
n?1
u ,S
n?1
g ) (12)
S?g = argmax
Sg
?
P(Siu,S
i
g|S
i?1
u ,S
i?2
u ,S
i?1
g ,S
i?2
g ) (13)
where Su = S
1
uS
2
u . . . S
n
u and Sg = S
1
gS
2
g . . . S
m
g .
In order to compute the joint probability, we need to construct an alignment
between tokens (Siu,S
i
g). We use the Viterbi alignment provided by the GIZA++
toolkit (Och and Ney 2003) for this purpose. We convert the Viterbi alignment into a
bilanguage representation that pairs words of the string Su with words of Sg. A few
examples of bilanguage strings are shown in Figure 32. We compute the joint n-gram
model using a language modeling toolkit (Goffin et al 2005). Equation (13) thus allows
us to edit a user?s utterance to a string that can be interpreted by the grammar.
Figure 32
A few examples of bilanguage strings.
384
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
6.3.1 Deriving a Translation Corpus. Because our multimodal grammar is implemented
as a finite-state transducer it is fully reversible and can be used not just to provide a
meaning for input strings but can also be run in reverse to determine possible input
strings for a given meaning. Our multimodal corpus was annotated for meaning using
themultimodal annotation tools described in Section 2.2. In order to train the translation
model we built a corpus that pairs the reference speech string for each utterance in
the training data with a target string. The target string is derived in two steps. First,
the multimodal grammar is run in reverse on the reference meaning yielding a lattice
of possible input strings. Second, the closest string (as defined by Levenshtein edit-
distance [Levenshtein 1966]) in the lattice to the reference speech string is selected as
the target string.
6.3.2 FST-Based Decoder. In order to facilitate editing of ASR lattices, we represent the
n-gram translation model as a weighted finite-state transducer (Bangalore and Riccardi
2002). We first represent the joint n-gram model as a finite-state acceptor (Allauzen
et al 2004). We then interpret the symbols on each arc of the acceptor as having two
components?a word from the user?s utterance (input) and a word from the edited
string (output). This transformation makes a transducer out of an acceptor. In doing
this, we can directly compose the editing model (?MT) with ASR lattices (?S) to produce
a weighted lattice of edited strings. We further constrain the set of edited strings to those
that are interpretable by the grammar. We achieve this by composing with the language
finite-state acceptor derived from the multimodal grammar (?G) and searching for the
best edited string, as shown in Equation (14).
S?MT = argmax
S
?S ? ?MT ? ?G (14)
If we were to apply this approach to input gesture lattices, then the translation
model would be built from pairings of the gesture recognition output and the corre-
sponding gesture string that would be interpretable by the grammar. Typical errors in
gesture input could include misrecognition of a spurious gesture that ought to have
been treated as noise (caused, for example, by improper detection of a pen-down event)
and non-recognition of pertinent gestures due to early end-pointing of ink input.
Figure 33 shows two examples. In the first example, the unimodal speech utterance
was edited by the model to produce a string that was correctly interpreted by the
multimodal grammar. In the second example, the speech and gesture integration failed
and resulted in an empty meaning representation. However, after the edit on the speech
string, the multimodal utterance was correctly interpreted.
6.3.3 Experiments and Results. Table 7 summarizes the results of the translation model
(TM) and compares its accuracy to a grammar-based model. We provide concept ac-
curacy and predicate and argument string accuracy of the translation models applied
to one-best and lattice ASR input. We also provide concept accuracy results on the
lattice of edited strings resulting from applying the translation models to the user?s
input. As can be seen from the table, the translation models outperform the grammar-
based models significantly in all cases. It is also interesting to note that there is some
improvement in concept accuracy using an ASR lattice over a one-best ASR output with
one-best translation output. However, the improvement is much more significant using
a lattice output from the translation model, suggesting that delaying selection of the
edited string until the grammar/domain constraints are applied is paying off.
385
Computational Linguistics Volume 35, Number 3
Figure 33
Sample inputs and the edited outputs from the translation model.
One of the limitations of the translation model is that the edit operations that are
learned are entirely driven by the parallel data. However, when the data are limited,
as is the case here, the edit operations learned are also restricted. We would like to
incorporate domain-specific edit operations in addition to the ones that are reflected in
the data. In the next section, we explore this approach.
6.4 Edit-Based Approach
In this section, we extend the approach of translating the ?noisy? version of the user?s
input to the ?clean? input to incorporate arbitrary editing operations. We encode the
possible edits on the input string as an edit FST with substitution, insertion, deletion,
and identity arcs. These operations could be either word-based or phone-based and
are associated with a cost. This allows us to incorporate, by hand, a range of edits
that may not have been observed in the data used in the noisy?channel-based error-
correction model. The edit transducer coerces the set of strings (S) encoded in the lattice
resulting from ASR (?S ) to the closest strings in the grammar that can be assigned
an interpretation. We are interested in the string with the least-cost sequence of edits
Table 7
Concept sentence accuracy of grammar-based and translation-based models with data-driven
language model for ASR. (Numbers are percent for 6-fold cross validation by scenario.
Corresponding percent for 10-fold cross validation are given in parentheses.)
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy (%) Accuracy (%) Accuracy (%)
Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)
ASR 1-best/1-best TM 46.1 (61.6) 68.0 (70.5) 47.0 (62.6)
ASR 1-best/Lattice TM 50.3 (61.6) 70.5 (70.5) 51.2 (62.6)
ASR Lattice/1-best TM 46.7 (61.9) 70.8 (69.9) 47.1 (63.3)
ASR Lattice/Lattice TM 54.2 (60.8) 81.9 (67.2) 54.5 (61.6)
386
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 34
Basic edit machine.
(argmin) that can be assigned an interpretation by the grammar.7 This can be achieved
by composition (?) of transducers followed by a search for the least-cost path through a
weighted transducer as shown in Equation (15).
s? = argmin
s?S
?S ? ?edit ? ?g (15)
We first describe the machine introduced in Bangalore and Johnston (2004) (Basic
edit) then go on to describe a smaller edit machine with higher performance (4-edit) and
an edit machine which incorporates additional heuristics (Smart edit).
6.4.1 Basic Edit. The edit machine described in Bangalore and Johnston (2004) is es-
sentially a finite-state implementation of the algorithm to compute the Levenshtein
distance. It allows for unlimited insertion, deletion, and substitution of any word for
another (Figure 34). The costs of insertion, deletion, and substitution are set as equal,
except for members of classes such as price (expensive), cuisine (turkish), and so on,
which are assigned a higher cost for deletion and substitution.
6.4.2 Limiting the Number of Edits. Basic edit is effective in increasing the number of
strings that are assigned an interpretation (Bangalore and Johnston 2004) but is quite
large (15Mb, 1 state, 978,120 arcs) and adds an unacceptable amount of latency (5 sec-
onds on average) in processing one-best input and is computationally prohibitive to use
on lattices. In order to overcome these performance limitations, we experimented with
revising the topology of the edit machine so that it allows only a limited number of edit
operations (e.g., at most four edits) and removed the substitution arcs, because they
give rise to O(|
?
|2) arcs, where
?
is the vocabulary. Substitution is still possible but
requires one delete and one insert. For the same grammar, the resulting edit machine is
about 300Kb with 4 states and 16,796 arcs. The topology of the 4-editmachine is shown
in Figure 35. In addition to 4-edit, we also investigated 6-edit and 8-edit machines whose
results we report in the subsequent sections.
There is a significant savings in bounding the number of edits on the number of
paths in the resulting lattice. After composing with the basic edit machine, the lattice
would contain O(n ? |
?
|) arcs where n is the length of the input being edited. For
the bounded k-edit machines this reduces to O(k ? |?|) arcs and O(|?|k) paths for a
constant k.
7 Note that the closest string according to the edit metric may not be the closest string in meaning.
387
Computational Linguistics Volume 35, Number 3
Figure 35
4-edit machine.
6.4.3 Smart Edit. We incorporate a number of additional heuristics and refinements to
tune the 4-edit machine based on the underlying application database.
i. Deletion of SLM-only wordsWe add arcs to the edit transducer to allow for free
deletion of words in the SLM training data which are not found in the grammar: for
example, listings in thai restaurant listings in midtown? thai restaurant in midtown.
ii. Deletion of doubled words A common error observed in SLM output was dou-
bling of monosyllabic words: for example, subway to the cloisters recognized as subway
to to the cloisters. We add arcs to the edit machine to allow for free deletion of any short
word when preceded by the same word.
iii. Extended variable weighting of words Insertion and deletion costs were further
subdivided from two to three classes: a low cost for ?dispensable? words, (e.g., please,
would, looking, a, the), a high cost for special words (slot fillers, e.g., chinese, cheap,
downtown), and a medium cost for all other words, (e.g., restaurant, find).
iv. Auto completion of place names It is unlikely that grammar authors will include
all of the different ways to refer to named entities such as place names. For example, if
the grammar includes metropolitan museum of art the user may just say metropolitan mu-
seum. These changes can involve significant numbers of edits. A capability was added
to the edit machine to complete partial specifications of place names in a single edit.
This involves a closed world assumption over the set of place names. For example, if
the onlymetropolitan museum in the database is themetropolitan museum of artwe assume
that we can insert of art after metropolitan museum. The algorithm for construction of
these auto-completion edits enumerates all possible substrings (both contiguous and
non-contiguous) for place names. For each of these it checks to see if the substring is
found in more than one semantically distinct member of the set. If not, an edit sequence
is added to the edit machine which freely inserts the words needed to complete the
place name. Figure 36 illustrates one of the edit transductions that is added for the place
name metropolitan museum of art. The algorithm which generates the autocomplete edits
also generates new strings to add to the place name class for the SLM (expanded class).
In order to limit over-application of the completion mechanism, substrings starting
in prepositions (of art ? metropolitan museum of art) or involving deletion of parts of
abbreviations are not considered for edits (b c building? n b c building).
Note that the application-specific structure and weighting of Smart edit (iii, iv) can
be derived automatically: We use the place-name list for auto completion of place names
and use the domain entities, as determined by which words correspond to fields in the
underlying application database, to assign variable costs to different entities.
Figure 36
Auto-completion edits.
388
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Table 8
Concept accuracy for different edit models on 6-fold cross-validation experiments using a
data-driven language model for ASR.
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy (%) Accuracy (%) Accuracy (%)
Grammar-based 38.9 40.3 40.7
(No edits)
Basic edit 51.5 63.1 52.6
4-edit 53.0 62.6 53.9
6-edit 58.2 74.7 59.0
8-edit 57.8 75.7 58.6
Smart 4-edit 60.2 69.9 60.9
Smart 6-edit 60.2 73.7 61.3
Smart 8-edit 60.9 76.0 61.9
Smart edit (exp) 59.7 70.8 60.5
Smart edit (exp, lattice) 62.0 73.1 63.0
Smart edit (lattice) 63.2 73.7 64.0
6.4.4 Experiments and Results.We summarize the concept accuracy results from the 6-fold
cross-validation experiments using the different edit machines previously discussed.
We also repeat the concept accuracy results from the grammar-based model with no
edits for a point of comparison. When compared to the baseline of 38.9% concept
accuracy without edits (No edits), Basic edit gave a relative improvement of 32%,
yielding 51.5% concept accuracy (Table 8). Interestingly, by limiting the number of edit
operations as in 4-edit, we improved the concept accuracy (53%) compared to Basic edit.
The reason for this improvement is that for certain input utterances, the Basic editmodel
creates a very large edited lattice and the composition with the grammar fails due to
memory restrictions.8 We also show improvement in concept accuracy by increasing
the number of allowable edit operations (up to 8-edit). The concept accuracy improves
with increasing number of edits but with diminishing relative improvements.
The heuristics in Smart edit clearly improve on the concept accuracy of the basic
edit models with a relative improvement of 55% over the baseline. Smart edit (exp)
shows the concept accuracy of Smart edit running on input from an ASR model with the
expanded classes required for auto completion of place names. Inspection of individual
partitions showed that, while the expanded classes did allow for the correction of
errors on place names, the added perplexity in the ASR model from expanding classes
resulted in errors elsewhere and an overall drop in concept accuracy of 0.5% compared
to Smart edit without expanded classes. Using ASR lattices as input to the edit models
further improved the accuracy to the best concept sentence accuracy score of 63.2%, a
relative improvement of 62.5% over the no-edit model. Lattice input also improved the
performance of Smart edit with the expanded classes from 59.7% to 62%.
To summarize, in Table 9 we tabulate the concept accuracy results from the best
performing configuration of each of the robust understandingmethods discussed in this
section. It is clear that the techniques such as translation-based edit and Smart edit that can
exploit the domain-specific grammar improve significantly over the classification-based
8 However, the concept accuracy for the 70% of utterances which are assigned a meaning using the basic
edit model was about 73%.
389
Computational Linguistics Volume 35, Number 3
Table 9
Summary of concept accuracy results from the different robust understanding techniques using
data-driven language models for ASR.
Model Concept Predicate Argument
Sentence Sentence Sentence
Accuracy (%) Accuracy (%) Accuracy (%)
Grammar-based 38.9 (41.5) 40.3 (43.1) 40.7 (43.2)
(No edits)
Classification-based 34.0 (58.3) 71.4 (85.5) 32.8 (61.4)
Translation-based edit 54.2 (60.8) 81.9 (67.2) 54.5 (61.6)
Smart edit 63.2 (68.4) 73.7 (73.8) 64.0 (69.4)
approach. Furthermore, the heuristics encoded in the smart edit technique that exploit
the domain constraints outperform the translation-based edit technique that is entirely
data-dependent.
We also show the results from the 10-fold cross-validation experiments in the
table. As can be seen there is a significant improvement in concept accuracy for data-
driven techniques (classification and translation-based edit) compared to the 6-fold
cross-validation experiments. This is to be expected because the distributions estimated
from the training set fit the test data better in the 10-fold experiments as against 6-fold
experiments.
Based on the results we have presented in this section, it would be pragmatic to
rapidly build a hand-crafted grammar-based conversational system that can be made
robust using stochastic language modeling techniques and edit-based understanding
techniques. Once the system is deployed and data collected, then a judicious balance
of data-driven and grammar-based techniques would maximize the performance of the
system.
7. Robust Gesture Processing
Gesture recognition has a lower error rate than speech recognition in this application.
Even so, gesture misrecognitions and incompleteness of the multimodal grammar in
specifying speech and gesture combinations contribute to the number of utterances not
being assigned a meaning. We address the issue of robustness to gesture errors in this
section.
We adopted the edit-based technique used on speech utterances to improve ro-
bustness of multimodal understanding. However, unlike a speech utterance, a gesture
string has a structured representation. The gesture string is represented as a sequence of
attribute?values (e.g., gesture type takes values from {area, line, point, handwriting}) and
editing a gesture representation implies allowing for replacements within the value set.
We adopted a simple approach that allows for substitution and deletion of values for
each attribute, in addition to the deletion of any gesture. We did not allow for insertions
of gestures as it is not clear what specific content should be assigned to an inserted
gesture. One of the problems is that if you have, for example, a selection of two items
and you want to increase it to three selected items, it is not clear a priori which entity to
add as the third item. We encoded the edit operations for gesture editing as a finite-state
transducer just as we did for editing speech utterances. Figure 37 illustrates the gesture
edit transducer with delc representing the delection cost and substc the substitution
cost. This method of manipulating the gesture recognition lattice is similar to gesture
390
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Figure 37
A finite-state transducer for editing gestures.
aggregation, introduced in Section 3.4. In contrast to substitution and deletion of ges-
tures, gesture aggregation involves insertion of new gestures into the lattice; however,
each introduced gesture has a well-definedmeaning based on the combination of values
of the gestures being aggregated.
We evaluated the effectiveness of the gesture edit machine on the MATCH data set.
The data consisted of 174 multimodal utterances that were covered by the grammar. We
used the transcribed speech utterance and the gesture lattice from the gesture recognizer
as inputs to the multimodal integration and understanding system. For 55.4% of the
utterances, we obtained the identical attribute?value meaning representation as the
human-transcribed meaning representation.
Applying the gesture edit transducer on the gesture recognition lattices, and then
integrating the result with the transcribed speech utterance produced a significant
improvement in the accuracy of the attribute?value meaning representation. For 68.9%
of the utterances, we obtained the identical attribute?value meaning representation
as the human-transcribed meaning representation, a 22.5% absolute improvement in
the robustness of the system that can be directly attributed to robustness in gesture
integration and understanding. In future work, we would like to explore learning from
data how to balance gesture editing and speech editing based on the relative reliabilities
of the two modalities.
8. Conclusion
We view the contributions of the research presented in this article from two perspec-
tives. First, we have shown how the finite-state approach to multimodal language
processing (Johnston and Bangalore 2005) can be extended to support applications
with complex pen input and how the approach can be made robust through coupling
with a stochastic speech recognition model using translation techniques or finite-state
edit machines. We have investigated the options available for bootstrapping domain-
specific corpora for language models by exploiting domain-specific and wide-coverage
grammars, linguistic generalization of out-of-domain data, and adapting domain-
independent corpora. We have shown that such techniques can closely approximate
the accuracy of speech recognizers trained on domain-specific corpora. For robust
391
Computational Linguistics Volume 35, Number 3
multimodal understanding we have presented and comparatively evaluated three dif-
ferent techniques based on discriminative classification, statistical translation, and edit
machines. We have investigated the strengths and limitations of these approaches
in terms of their ability to process lattice input, their ability to exploit constraints
from a domain-specific grammar, and their ability to utilize domain knowledge from
the underlying application database. The best performing multimodal understanding
system, using a stochastic ASR model coupled with the smart 4-edit transducer on
lattice input, is significantly more robust than the grammar-based system, achieving
68.4% concept sentence accuracy (10-fold) on data collected from novice first time users
of a multimodal conversational system. This is a substantial 35% relative improve-
ment in performance compared to 50.7% concept sentence accuracy (10-fold) using the
grammar-based language and multimodal understanding models without edits. In our
exploration of applying edit techniques to the gesture lattices we saw a 22.5% absolute
improvement in robustness.
The second perspective on the work views it as an investigation of a range of
techniques that balance the robustness provided by data-driven techniques and the
flexibility provided by grammar-based approaches. In the past four decades of speech
and natural language processing, both data-driven approaches and rule-based ap-
proaches have been prominent at different periods in time. Moderate-sized rule-based
spoken language models for recognition and understanding are easy to develop and
provide the ability to rapidly prototype conversational applications. However, scalabil-
ity of such systems is a bottleneck due to the heavy cost of authoring and maintenance
of rule sets and inevitable brittleness due to lack of coverage. In contrast, data-driven
approaches are robust and provide a simple process of developing applications given
availability of data from the application domain. However, this reliance on domain-
specific data is also one of the significant bottlenecks of data-driven approaches. Devel-
opment of conversational systems using data-driven approaches cannot proceed until
data pertaining to the application domain is available. The collection and annotation
of such data is extremely time-consuming and tedious, which is aggravated by the
presence of multiple modalities in the user?s input, as in our case. Also, extending an
existing application to support an additional feature requires adding additional data
sets with that feature. We have shown how a balanced approach where statistical lan-
guage models are coupled with grammar-based understanding using edit machines can
be highly effective in a multimodal conversational system. It is important to note that
these techniques are equally applicable for speech-only conversational systems as well.
Given that the combination of stochastic recognition models with grammar-based
understanding models provides robust performance, the question which remains is,
after the initial bootstrapping phase, as more data becomes available, should this
grammar-based approach be replaced with a data-driven understanding component?
There are a number of advantages to the hybrid approach we have proposed which
extend beyond the initial deployment of an application.
1. The expressiveness of the multimodal grammar allows us to specify
any compositional relationships and meaning that we want. The range
of meanings and their relationship to the input string can be arbitrarily
simple or complex.
2. The multimodal grammar provides an alignment between speech
and gesture input and enables multimodal integration of content
from different modes.
392
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
3. With the grammar-based approach it is straightforward to quickly add
support for new commands to the grammar or change the representation
of existing commands. The only retraining that is needed is for the ASR
model, and data for the ASR model can either be migrated from another
related domain or derived through grammar sampling.
4. Most importantly, this approach has the significant advantage that it
does not require annotation of speech data with meaning representations
and alignment of the meaning representations with word strings. This
can be complex and expensive, involving a detailed labeling guide and
instructions for annotators. In contrast in this approach, if data is used, all
that is needed is transcription of the audio, a far more straightforward
annotation task. If no data is used then grammar sampling can be used
instead and no annotation of data is needed whatsoever.
5. Although data-driven approaches to understanding are commonplace
in research, rule-based techniques continue to dominate in much of the
industry (Pieraccini 2004). See, for example, the W3C SRGS standard
(www.w3.org/TR/speech-grammar/).
Acknowledgments
We dedicate this article to the memory of
Candy Kamm whose continued support
for multimodal research made this work
possible. We thank Patrick Ehlen, Helen
Hastie, Preetam Maloor, Amanda Stent,
Gunaranjan Vasireddy, Marilyn Walker,
and Steve Whittaker for their contributions
to the MATCH system. We also thank
Richard Cox and Mazin Gilbert for their
ongoing support of multimodal research at
AT&T Labs?Research. We would also like
to thank the anonymous reviewers for
their many helpful comments and
suggestions for revision.
References
Abney, S. P. 1991. Parsing by chunks. In
R. Berwick, S. Abney, and C. Tenny,
editors, Principle-Based Parsing. IEEE,
Los Alamitos, CA, pages 257?278.
Ades, A. E. and M. Steedman. 1982. On the
order of words. Linguistics and Philosophy,
4:517?558.
Alexandersson, J. and T. Becker. 2001.
Overlay as the basic operation for
discourse processing in a multimodal
dialogue system. In Proceedings of the IJCAI
Workshop: Knowledge and Reasoning in
Practical Dialogue Systems, pages 8?14,
Seattle, WA.
Allauzen, C., M. Mohri, M. Riley, and
B. Roark. 2004. A generalized construction
of speech recognition transducers. In
International Conference on Acoustics, Speech,
and Signal Processing, pages 761?764,
Montreal.
Allauzen, C., M. Mohri, and B. Roark. 2003.
Generalized algorithms for constructing
statistical language models. In Proceedings
of the Association for Computational
Linguistics, pages 40?47, Sapporo.
Allauzen, C., M. Riley, J. Schalkwyk,
W. Skut, and M. Mohri. 2007. Openfst:
A general and efficient weighted
finite-state transducer library. In
Proceedings of the Ninth International
Conference on Implementation and Application
of Automata, (CIAA 2007), Lecture Notes in
Computer Science Vol. 4783, pages 11?23.
Springer, Berlin, Heidelberg.
Allen, J., D. Byron, M. Dzikovska,
G. Ferguson, L. Galescu, and A. Stent.
2001. Towards conversational
human-computer interaction.
AI Magazine, 22(4):27?38.
Andre?, E. 2002. Natural language in
multimedia/multimodal systems.
In R. Mitkov, editor, Handbook of
Computational Linguistics. Oxford
University Press, Oxford,
pages 650?669.
Bacchiani, M. and B. Roark. 2003.
Unsupervised language model
adaptation. In Proceedings of the
International Conference on Acoustics,
Speech, and Signal Processing,
pages 224?227, Hong Kong.
Bangalore, S. 1997. Complexity of Lexical
Descriptions and its Relevance to Partial
393
Computational Linguistics Volume 35, Number 3
Parsing. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Bangalore, S. and M. Johnston. 2000.
Tight-coupling of multimodal language
processing with speech recognition.
In Proceedings of the International Conference
on Spoken Language Processing,
pages 126?129, Beijing.
Bangalore, S. and M. Johnston. 2004.
Balancing data-driven and rule-based
approaches in the context of a
multimodal conversational system.
In Proceedings of the North American
Association for Computational Linguistics/
Human Language Technology, pages 33?40,
Boston, MA.
Bangalore, S. and A. K. Joshi. 1999.
Supertagging: An approach to almost
parsing. Computational Linguistics,
25(2):237?265.
Bangalore, S. and G. Riccardi. 2000.
Stochastic finite-state models for spoken
language machine translation. In
Proceedings of the Workshop on Embedded
Machine Translation Systems, pages 52?59,
Seattle, WA.
Bangalore, S. and G. Riccardi. 2002.
Stochastic finite-state models of spoken
language machine translation.Machine
Translation, 17(3):165?184.
Bellik, Y. 1995. Interface Multimodales:
Concepts, Mode`les et Architectures. Ph.D.
thesis, University of Paris XI (Orsay),
France.
Bellik, Y. 1997. Media integration in
multimodal interfaces. In Proceedings
of the IEEE Workshop on Multimedia
Signal Processing, pages 31?36,
Princeton, NJ.
Beutnagel, M., A. Conkie, J. Schroeter,
Y. Stylianou, and A. Syrdal. 1999. The
AT&T next-generation TTS. In Joint
Meeting of ASA; EAA and DAGA,
pages 18?24, Berlin.
Boros, M., W. Eckert, F. Gallwitz, G. Go?rz,
G. Hanrieder, and H. Niemann.
1996. Towards understanding
spontaneous speech: word accuracy
vs. concept accuracy. In Proceedings of
the International Conference on Spoken
Language Processing, pages 41?44,
Philadelphia, PA.
Brison, E. and N. Vigouroux. 1993.
Multimodal references: A generic fusion
process. Technical report, URIT-URA
CNRS, Universit Paul Sabatier, Toulouse.
Carpenter, R. 1992. The Logic of Typed Feature
Structures. Cambridge University Press,
Cambridge.
Cassell, J. 2001. Embodied conversational
agents: Representation and intelligence in
user interface. AI Magazine, 22:67?83.
Cheyer, A. and L. Julia. 1998. Multimodal
Maps: An Agent-Based Approach. Lecture
Notes in Computer Science, 1374:103?113.
Ciaramella, A. 1993. A prototype
performance evaluation report. Technical
Report WP8000-D3, Project Esprit 2218,
SUNDIAL.
Clark, S. and J. Hockenmaier. 2002.
Evaluating a wide-coverage CCG parser.
In Proceedings of the LREC 2002, Beyond
Parseval Workshop, pages 60?66,
Las Palmas.
Cohen, P. R. 1991. Integrated interfaces
for decision support with simulation.
In Proceedings of the Winter Simulation
Conference, pages 1066?1072, Phoenix, AZ.
Cohen, P. R. 1992. The role of natural
language in a multimodal interface.
In Proceedings of the User Interface Software
and Technology, pages 143?149,
Monterey, CA.
Cohen, P. R., M. Johnston, D. McGee, S. L.
Oviatt, J. Clow, and I. Smith. 1998a. The
efficiency of multimodal interaction: A
case study. In Proceedings of the International
Conference on Spoken Language Processing,
pages 249?252, Sydney.
Cohen, P. R., M. Johnston, D. McGee, S. L.
Oviatt, J. Pittman, I. Smith, L. Chen, and
J. Clow. 1998b. Multimodal interaction for
distributed interactive simulation. In
M. Maybury and W. Wahlster, editors,
Readings in Intelligent Interfaces. Morgan
Kaufmann Publishers, San Francisco, CA,
pages 562?571.
Dowding, J., J. M. Gawron, D. E. Appelt,
J. Bear, L. Cherny, R. Moore, and D. B.
Moran. 1993. GEMINI: A natural
language system for spoken-language
understanding. In Proceedings of the
Association for Computational Linguistics,
pages 54?61, Columbus, OH.
Ehlen, P., M. Johnston, and G. Vasireddy.
2002. Collecting mobile multimodal
data for MATCH. In Proceedings of the
International Conference on Spoken Language
Processing, pages 2557?2560, Denver, CO.
Flickinger, D., A. Copestake, and I. Sag.
2000. HPSG analysis of English.
In W. Wahlster, editor, Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer?Verlag, Berlin, pages 254?263.
Galescu, L., E. K. Ringger, and J. F. Allen.
1998. Rapid language model development
for new task domains. In Proceedings of the
ELRA First International Conference on
394
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Language Resources and Evaluation (LREC),
pages 807?812, Granada.
Goffin, V., C. Allauzen, E. Bocchieri,
D. Hakkani-Tur, A. Ljolje, S. Parthasarathy,
M. Rahim, G. Riccardi, and M. Saraclar.
2005. The AT&T WATSON speech
recognizer. In Proceedings of the
International Conference on Acoustics, Speech,
and Signal Processing, pages 1033?1036,
Philadelphia, PA.
Gorin, A. L., G. Riccardi, and J. H. Wright.
1997. How May I Help You? Speech
Communication, 23(1-2):113?127.
Gupta, N., G. Tur, D. Tur, S. Bangalore,
G. Riccardi, and M. Rahim. 2004. The
AT&T spoken language understanding
system. IEEE Transactions on Speech and
Audio Processing, 14(1):213?222.
Gustafson, J., L. Bell, J. Beskow, J. Boye,
R. Carlson, J. Edlund, B. Granstro?m,
D. House, and M. Wirn. 2000. Adapt?
a multimodal conversational dialogue
system in an apartment domain. In
International Conference on Spoken Language
Processing, pages 134?137, Beijing.
Haffner, P., G. Tur, and J. Wright. 2003.
Optimizing SVMs for complex call
classification. In International Conference on
Acoustics, Speech, and Signal Processing,
pages 632?635, Hong Kong.
Hauptmann, A. 1989. Speech and gesture
for graphic image manipulation. In
Proceedings of CHI ?89, pages 241?245,
Austin, TX.
Jackendoff, R. 2002. Foundations of Language:
Brain, Meaning, Grammar, and Evolution
Chapter 9. Oxford University Press,
New York.
Johnston, M. 1998a. Multimodal language
processing. In Proceedings of the
International Conference on Spoken Language
Processing, pages 893?896, Sydney.
Johnston, M. 1998b. Unification-based
multimodal parsing. In Proceedings of the
Association for Computational Linguistics,
pages 624?630, Montreal.
Johnston, M. 2000. Deixis and conjunction in
multimodal systems. In Proceedings of the
International Conference on Computational
Linguistics (COLING), pages 362?368,
Saarbru?cken.
Johnston, M., P. Baggia, D. C. Burnett,
J. Carter, D. Dahl, G. McCobb, and
D. Raggett. 2007. EMMA: Extensible
MultiModal Annotation markup language.
Technical report, W3C Candidate
Recommendation.
Johnston, M. and S. Bangalore. 2000.
Finite-state multimodal parsing and
understanding. In Proceedings of the
International Conference on Computational
Linguistics (COLING), pages 369?375,
Saarbru?cken.
Johnston, M. and S. Bangalore. 2004.
Matchkiosk: A multimodal interactive city
guide. In Proceedings of the Association of
Computational Linguistics (ACL) Poster and
Demonstration Session, pages 222?225,
Barcelona.
Johnston, M. and S. Bangalore. 2005.
Finite-state multimodal integration and
understanding. Journal of Natural Language
Engineering, 11(2):159?187.
Johnston, M., S. Bangalore, A. Stent,
G. Vasireddy, and P. Ehlen. 2002a.
Multimodal language processing for
mobile information access. In Proceedings
of the International Conference on Spoken
Language Processing, pages 2237?2240,
Denver, CO.
Johnston, M., S. Bangalore, G. Vasireddy,
A. Stent, P. Ehlen, M. Walker, S. Whittaker,
and P. Maloor. 2002b. MATCH: An
architecture for multimodal dialog
systems. In Proceedings of the Association of
Computational Linguistics, pages 376?383,
Philadelphia, PA.
Johnston, M., P. R. Cohen, D. McGee, S. L.
Oviatt, J. A. Pittman, and I. Smith.
1997. Unification-based multimodal
integration. In Proceedings of the
Association of Computational Linguistics,
pages 281?288, Madrid.
Joshi, A. and P. Hopely. 1997. A parser from
antiquity. Journal of Natural Language
Engineering, 2(4):6?15.
Kanthak, S. and H. Ney. 2004. FSA: An
Efficient and Flexible C++ Toolkit
for Finite State Automata Using
On-Demand Computation. In Proceedings
of the Association for Computational
Linguistics Conference, pages 510?517,
Barcelona.
Kaplan, R. M. and M. Kay. 1994. Regular
models of phonological rule systems.
Computational Linguistics, 20(3):331?378.
Kartunnen, L. 1991. Finite-state constraints.
In Proceedings of the International
Conference on Current Issues in
Computational Linguistics, Universiti
Sains Malaysia, Penang.
Koons, D. B., C. J. Sparrell, and K. R.
Thorisson. 1993. Integrating simultaneous
input from speech, gaze, and hand
gestures. In M. T. Maybury, editor,
Intelligent Multimedia Interfaces. AAAI
Press/MIT Press, Cambridge, MA,
pages 257?276.
395
Computational Linguistics Volume 35, Number 3
Koskenniemi, K. K. 1984. Two-level
Morphology: A General Computation Model
for Word-form Recognition and Production.
Ph.D. thesis, University of Helsinki.
Larsson, S., P. Bohlin, J. Bos, and D. Traum.
1999. TrindiKit manual. Technical report,
TRINDI Deliverable D2.2, Gothenburg
University, Sweden.
Levenshtein, V. I. 1966. Binary codes capable
of correcting deletions, insertion and
reversals. Soviet Physics Doklady,
10:707?710.
Mohri, M., F. C. N. Pereira, and M. Riley.
1998. A rational design for a weighted
finite-state transducer library. Lecture Notes
in Computer Science, 1436:144?158.
Neal, J. G. and S. C. Shapiro. 1991. Intelligent
multi-media interface technology. In J. W.
Sullivan and S. W. Tyler, editors, Intelligent
User Interfaces. Addison Wesley, New York,
pages 45?68.
Nederhof, M. J. 1997. Regular
approximations of CFLs: A grammatical
view. In Proceedings of the International
Workshop on Parsing Technology,
pages 159?170, Boston, MA.
Nishimoto, T., N. Shida, T. Kobayashi,
and K. Shirai. 1995. Improving human
interface in drawing tool using speech,
mouse, and keyboard. In Proceedings of the
4th IEEE International Workshop on Robot
and Human Communication, ROMAN95,
pages 107?112, Tokyo.
Noord, G. 1997. FSA utilities: A toolbox
to manipulate finite-state automata.
Lecture Notes in Computer Science,
1260:87?108.
Och, F. J. and H. Ney. 2003. A systematic
comparison of various statistical
alignment models. Computational
Linguistics, 29(1):19?51.
Oviatt, S., A. DeAngeli, and K. Kuhn. 1997.
Integration and synchronization of
input modes during multimodal
human-computer interaction. In CHI ?97:
Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems,
pages 415?422, New York, NY.
Oviatt, S. L. 1997. Multimodal interactive
maps: Designing for human performance.
Human-Computer Interaction, 12(1):93?129.
Oviatt, S. L. 1999. Mutual disambiguation
of recognition errors in a multimodal
architecture. In Proceedings of the
Conference on Human Factors in Computing
Systems: CHI?99, pages 576?583,
Pittsburgh, PA.
Papineni, K. A., S. Roukos, and T. R. Ward.
1997. Feature-based language
understanding. In Proceedings of
European Conference on Speech
Communication and Technology,
pages 1435?1438, Rhodes.
Pereira, F. C. N. and M. D. Riley. 1997.
Speech recognition by composition of
weighted finite automata. In E. Roche
and Y. Schabes, editors, Finite State
Devices for Natural Language Processing.
MIT Press, Cambridge, MA, USA,
pages 431?456.
Pieraccini, R. 2004. Spoken language
understanding: The research/industry
chasm. In HLT-NAACL 2004 Workshop
on Spoken Language Understanding for
Conversational Systems and Higher Level
Linguistic Information for Speech Processing,
Boston, MA.
Pollard, C. and I. A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Center
for the Study of Language and
Information, University of Chicago
Press, IL.
Punyakanok, V., D. Roth, and W. Yih. 2005.
Generalized inference with multiple
semantic role labeling systems shared
task paper. In Proceedings of the Annual
Conference on Computational Natural
Language Learning (CoNLL), pages 181?184,
Ann Arbor, MI.
Rambow, O., S. Bangalore, T. Butt, A. Nasr,
and R. Sproat. 2002. Creating a finite-state
parser with application semantics. In
Proceedings of the International Conference on
Computational Linguistics (COLING 2002),
pages 1?5, Taipei.
Ramshaw, L. and M. P. Marcus. 1995. Text
chunking using transformation-based
learning. In Proceedings of the Third
Workshop on Very Large Corpora,
pages 82?94, Cambridge, MA.
Riccardi, G., R. Pieraccini, and E. Bocchieri.
1996. Stochastic automata for language
modeling. Computer Speech and Language,
10(4):265?293.
Rich, C. and C. Sidner. 1998. COLLAGEN:
A collaboration manager for software
interface agents. User Modeling and
User-Adapted Interaction, 8(3?4):315?350.
Ringger, E. K. and J. F. Allen. 1996. A
fertility channel model for post-correction
of continuous speech recognition.
In International Conference on Spoken
Language Processing, pages 897?900,
Philadelphia, PA.
Ristad, E. S. and P. N. Yianilos. 1998.
Learning string-edit distance. IEEE
Transaction on Pattern Analysis and
Machine Intelligence, 20(5):522?532.
396
Bangalore and Johnston Robust Understanding in Multimodal Interfaces
Roche, E. 1999. Finite-state transducers:
parsing free and frozen sentences. In
A. Kornai, editor, Extended Finite-State
Models of Language. Cambridge University
Press, Cambridge, pages 108?120.
Rubine, D. 1991. Specifying gestures by
example. Computer Graphics, 25(4):329?337.
Schapire, R., M. Rochery, M. Rahim, and
N. Gupta. 2002. Incorporating prior
knowledge in boosting. In Proceedings
of the Nineteenth International Conference
on Machine Learning, pages 538?545,
Sydney.
Schapire, R. E. 1999. A brief introduction to
boosting. In Proceedings of International
Joint Conference on Artificial Intelligence,
pages 1401?1406, Stockholm.
Souvignier, B. and A. Kellner. 1998. Online
adaptation for language models in
spoken dialogue systems. In International
Conference on Spoken Language Processing,
pages 2323?2326, Sydney.
Stent, A., J. Dowding, J. Gawron, E. Bratt,
and R. Moore. 1999. The CommandTalk
spoken dialogue system. In Proceedings of
the 27th Annual Meeting of the Association for
Computational Linguistics, pages 183?190,
College Park, MD.
Thompson, C. A. and R. J. Mooney. 2003.
Acquiring word-meaning mappings for
natural language interfaces. Journal of
Artificial Intelligence Research, 18:1?44.
van Tichelen, L. 2004. Semantic
interpretation for speech recognition.
Technical Report W3C.
Wahlster, W. 2002. SmartKom: Fusion and
fission of speech, gestures, and facial
expressions. In Proceedings of the 1st
International Workshop on Man-Machine
Symbiotic Systems, pages 213?225, Kyoto.
Walker, M., R. Passonneau, and J. Boland.
2001. Quantitative and qualitative
evaluation of DARPA Communicator
spoken dialogue systems. In Proceedings
of the 39rd Annual Meeting of the
Association for Computational Linguistics
(ACL/EACL-2001), pages 515?522,
Toulouse.
Walker, M. A., S. Whittaker, A. Stent,
P. Maloor, J. D. Moore, M. Johnston, and
G. Vasireddy. 2004. Generation and
evaluation of user tailored responses in
multimodal dialogue. Cognitive Science,
28(5):811?840.
Walker, M. A., S. J. Whittaker, P. Maloor,
J. D. Moore, M. Johnston, and
G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in
spoken dialogue. In Proceedings of the
International Natural Language
Generation Conference, pages 73?80,
Ramapo, NY.
Wang, Y. and A. Acero. 2003. Combination of
CFG and n-gram modeling in semantic
grammar learning. In Proceedings of the
Eurospeech Conference, pages 2809?2812,
Geneva.
Ward, W. 1991. Understanding spontaneous
speech: The Phoenix system. In
Proceedings of the International Conference
on Acoustics, Speech, and Signal Processing,
pages 365?367, Washington, DC.
Wauchope, K. 1994. Eucalyptus: Integrating
natural language input with a graphical
user interface. Technical Report
NRL/FR/5510?94-9711, Naval
Research Laboratory, Washington, DC.
XTAG. 2001. A lexicalized tree-adjoining
grammar for English. Technical report,
University of Pennsylvania. Available at
www.cis.upenn.edu/?xtag/
gramrelease.html.
Zettlemoyer, L. S. and M. Collins. 2005.
Learning to map sentences to logical
form: Structured classification with
probabilistic categorial grammars.
In Proceedings of the Twenty-First
Conference on Uncertainty in Artificial
Intelligence (UAI-05), pages 658?66,
Arlington, VA.
397

Balancing Data-driven and Rule-based Approaches in the Context of a
Multimodal Conversational System
Srinivas Bangalore
AT&T Labs-Research
180 Park Avenue
Florham Park, NJ 07932
srini@research.att.com
Michael Johnston
AT&T Labs-Research
180 Park Avenue
Florham Park, NJ 07932
johnston@research.att.com
Abstract
Moderate-sized rule-based spoken language
models for recognition and understanding are
easy to develop and provide the ability to
rapidly prototype conversational applications.
However, scalability of such systems is a bot-
tleneck due to the heavy cost of authoring and
maintenance of rule sets and inevitable brittle-
ness due to lack of coverage in the rule sets.
In contrast, data-driven approaches are robust
and the procedure for model building is usu-
ally simple. However, the lack of data in a par-
ticular application domain limits the ability to
build data-driven models. In this paper, we ad-
dress the issue of combining data-driven and
grammar-based models for rapid prototyping
of robust speech recognition and understanding
models for a multimodal conversational sys-
tem. We also present methods that reuse data
from different domains and investigate the lim-
its of such models in the context of a particular
application domain.
1 Introduction
In the past four decades of speech and natural language
processing, both data-driven approaches and rule-based
approaches have been prominent at different periods in
time. In the recent past, rule-based approaches have
fallen into disfavor due to their brittleness and the sig-
nificant cost of authoring and maintaining complex rule
sets. Data-driven approaches are robust and provide a
simple process of developing applications given the data
from the application domain. However, the reliance on
domain-specific data is also one of the significant bottle-
necks of data-driven approaches. Development of a con-
versational system using data-driven approaches cannot
proceed until data pertaining to the application domain is
available. The collection and annotation of such data is
extremely time-consuming and tedious, which is aggra-
vated by the presence of multiple modalities in the user?s
input, as in our case. Also, extending an existing applica-
tion to support an additional feature requires adding ad-
ditional data sets with that feature.
In this paper, we explore various methods for combin-
ing rule-based and in-domain data for rapid prototyping
of speech recognition and understanding models that are
robust to ill-formed or unexpected input in the context
of a multimodal conversational system. We also investi-
gate approaches to reuse out-of-domain data and compare
their performance against the performance of in-domain
data-driven models.
We investigate these issues in the context of a multi-
modal application designed to provide an interactive city
guide: MATCH. In Section 2, we present the MATCH
application, the architecture of the system and the appa-
ratus for multimodal understanding. In Section 3, we dis-
cuss various approaches to rapid prototyping of the lan-
guage model for the speech recognizer and in Section 4
we present two approaches to robust multimodal under-
standing. Section 5 presents the results for speech recog-
nition and multimodal understanding using the different
approaches we consider.
2 The MATCH application
MATCH (Multimodal Access To City Help) is a work-
ing city guide and navigation system that enables mo-
bile users to access restaurant and subway information
for New York City (NYC) (Johnston et al, 2002b; John-
ston et al, 2002a). The user interacts with a graphical in-
terface displaying restaurant listings and a dynamic map
showing locations and street information. The inputs can
be speech, drawing on the display with a stylus, or syn-
chronous multimodal combinations of the two modes.
The user can ask for the review, cuisine, phone number,
address, or other information about restaurants and sub-
way directions to locations. The system responds with
graphical callouts on the display, synchronized with syn-
thetic speech output. For example, if the user says phone
numbers for these two restaurants and circles two restau-
rants as in Figure 1 [a], the system will draw a callout
with the restaurant name and number and say, for exam-
ple Time Cafe can be reached at 212-533-7000, for each
restaurant in turn (Figure 1 [b]). If the immediate en-
vironment is too noisy or public, the same command can
be given completely in pen by circling the restaurants and
writing phone.
Figure 1: Two area gestures
2.1 MATCH Multimodal Architecture
The underlying architecture that supports MATCH con-
sists of a series of re-usable components which commu-
nicate over sockets through a facilitator (MCUBE) (Fig-
ure 2). Users interact with the system through a Multi-
modal User Interface Client (MUI). Their speech and ink
are processed by speech recognition (Sharp et al, 1997)
(ASR) and handwriting/gesture recognition (GESTURE,
HW RECO) components respectively. These recognition
processes result in lattices of potential words and ges-
tures. These are then combined and assigned a mean-
ing representation using a multimodal finite-state device
(MMFST) (Johnston and Bangalore, 2000; Johnston et
al., 2002b). This provides as output a lattice encoding all
of the potential meaning representations assigned to the
user inputs. This lattice is flattened to an N-best list and
passed to a multimodal dialog manager (MDM) (John-
ston et al, 2002b), which re-ranks them in accordance
with the current dialogue state. If additional informa-
tion or confirmation is required, the MDM enters into a
short information gathering dialogue with the user. Once
a command or query is complete, it is passed to the mul-
timodal generation component (MMGEN), which builds
a multimodal score indicating a coordinated sequence of
graphical actions and TTS prompts. This score is passed
back to the Multimodal UI (MUI). The Multimodal UI
coordinates presentation of graphical content with syn-
thetic speech output using the AT&T Natural Voices TTS
engine (Beutnagel et al, 1999). The subway route con-
straint solver (SUBWAY) identifies the best route be-
tween any two points in New York City.
Figure 2: Multimodal Architecture
2.2 Multimodal Integration and Understanding
Our approach to integrating and interpreting multimodal
inputs (Johnston et al, 2002b; Johnston et al, 2002a) is
an extension of the finite-state approach previously pro-
posed (Bangalore and Johnston, 2000; Johnston and Ban-
galore, 2000). In this approach, a declarative multimodal
grammar captures both the structure and the interpreta-
tion of multimodal and unimodal commands. The gram-
mar consists of a set of context-free rules. The multi-
modal aspects of the grammar become apparent in the
terminals, each of which is a triple W:G:M, consisting
of speech (words, W), gesture (gesture symbols, G), and
meaning (meaning symbols, M). The multimodal gram-
mar encodes not just multimodal integration patterns but
also the syntax of speech and gesture, and the assignment
of meaning. The meaning is represented in XML, facil-
itating parsing and logging by other system components.
The symbol SEM is used to abstract over specific content
such as the set of points delimiting an area or the identi-
fiers of selected objects. In Figure 3, we present a small
simplified fragment from the MATCH application capa-
ble of handling information seeking commands such as
phone for these three restaurants. The epsilon symbol (   )
indicates that a stream is empty in a given terminal.
CMD    :   :  cmd  INFO   :   :  /cmd 
INFO    :   :  type  TYPE   :   :  /type 
for:   :    :   :  obj  DEICNP   :   :  /obj 
TYPE  phone:   :phone  review:   :review
DEICNP  DDETPL   :area:    :sel:   NUM HEADPL
DDETPL  these:G:    those:G:  
HEADPL  restaurants:rest:  rest  SEM:SEM:    :   :  /rest 
NUM  two:2:    three:3:   ... ten:10:  
Figure 3: Multimodal grammar fragment
Speech:    
Gesture:
<type><info><cmd>
SEM(points...)
phone
<rest>
Meaning: 
<rest>
<obj></type>
ten
2
sel
locareaG
SEM(r12,r15)
restaurantstwotheseforphone
</obj></rest>r12,r15 </info> </cmd>
Figure 4: Multimodal Example
In the example above where the user says phone for
these two restaurants while circling two restaurants (Fig-
ure 1 [a]), assume the speech recognizer returns the lat-
tice in Figure 4 (Speech). The gesture recognition com-
ponent also returns a lattice (Figure 4, Gesture) indicat-
ing that the user?s ink is either a selection of two restau-
rants or a geographical area. The multimodal grammar
(Figure 3) expresses the relationship between what the
user said, what they drew with the pen, and their com-
bined meaning, in this case Figure 4 (Meaning). The
meaning is generated by concatenating the meaning sym-
bols and replacing SEM with the appropriate specific con-
tent:  cmd  info  type  phone  /type  obj 
 rest  [r12,r15]  /rest 	 /obj 
 /info  /cmd  .
For the purpose of evaluation of concept accuracy, we
developed an approach similar to (Boros et al, 1996)
in which computing concept accuracy is reduced to com-
paring strings representing core contentful concepts. We
extract a sorted flat list of attribute value pairs that repre-
sents the core contentful concepts of each command from
the XML output. The example above yields the following
meaning representation for concept accuracy.
 
		MATCH: An Architecture for Multimodal Dialogue Systems
Michael Johnston, Srinivas Bangalore, Gunaranjan Vasireddy, Amanda Stent
Patrick Ehlen, Marilyn Walker, Steve Whittaker, Preetam Maloor
AT&T Labs - Research, 180 Park Ave, Florham Park, NJ 07932, USA
johnston,srini,guna,ehlen,walker,stevew,pmaloor@research.att.com
Now at SUNY Stonybrook, stent@cs.sunysb.edu
Abstract
Mobile interfaces need to allow the user
and system to adapt their choice of com-
munication modes according to user pref-
erences, the task at hand, and the physi-
cal and social environment. We describe a
multimodal application architecture which
combines finite-state multimodal language
processing, a speech-act based multimodal
dialogue manager, dynamic multimodal
output generation, and user-tailored text
planning to enable rapid prototyping of
multimodal interfaces with flexible input
and adaptive output. Our testbed appli-
cation MATCH (Multimodal Access To
City Help) provides a mobile multimodal
speech-pen interface to restaurant and sub-
way information for New York City.
1 Multimodal Mobile Information Access
In urban environments tourists and residents alike
need access to a complex and constantly changing
body of information regarding restaurants, theatre
schedules, transportation topology and timetables.
This information is most valuable if it can be de-
livered effectively while mobile, since places close
and plans change. Mobile information access devices
(PDAs, tablet PCs, next-generation phones) offer
limited screen real estate and no keyboard or mouse,
making complex graphical interfaces cumbersome.
Multimodal interfaces can address this problem by
enabling speech and pen input and output combining
speech and graphics (See (Andre?, 2002) for a detailed
overview of previous work on multimodal input and
output). Since mobile devices are used in different
physical and social environments, for different tasks,
by different users, they need to be both flexible in in-
put and adaptive in output. Users need to be able to
provide input in whichever mode or combination of
modes is most appropriate, and system output should
be dynamically tailored so that it is maximally effec-
tive given the situation and the user?s preferences.
We present our testbed multimodal application
MATCH (Multimodal Access To City Help) and the
general purpose multimodal architecture underlying
it, that: is designed for highly mobile applications;
enables flexible multimodal input; and provides flex-
ible user-tailored multimodal output.
Figure 1: MATCH running on Fujitsu PDA
Highly mobile MATCH is a working city guide
and navigation system that currently enables mobile
users to access restaurant and subway information for
New York City (NYC). MATCH runs standalone on
a Fujitsu pen computer (Figure 1), and can also run
in client-server mode across a wireless network.
Flexible multimodal input Users interact with a
graphical interface displaying restaurant listings and
a dynamic map showing locations and street infor-
mation. They are free to provide input using speech,
by drawing on the display with a stylus, or by us-
ing synchronous multimodal combinations of the two
modes. For example, a user might ask to see cheap
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 376-383.
                         Proceedings of the 40th Annual Meeting of the Association for
Italian restaurants in Chelsea by saying show cheap
italian restaurants in chelsea, by circling an area on
the map and saying show cheap italian restaurants
in this neighborhood; or, in a noisy or public envi-
ronment, by circling an area and writing cheap and
italian (Figure 2). The system will then zoom to the
appropriate map location and show the locations of
restaurants on the map. Users can ask for information
about restaurants, such as phone numbers, addresses,
and reviews. For example, a user might circle three
restaurants as in Figure 3 and say phone numbers for
these three restaurants (or write phone). Users can
also manipulate the map interface directly. For exam-
ple, a user might say show upper west side or circle
an area and write zoom.
Figure 2: Unimodal pen command
Flexible multimodal output MATCH provides
flexible, synchronized multimodal generation and
can take initiative to engage in information-seeking
subdialogues. If a user circles the three restaurants in
Figure 3 and writes phone, the system responds with
a graphical callout on the display, synchronized with
a text-to-speech (TTS) prompt of the phone number,
for each restaurant in turn (Figure 4).
Figure 3: Two area gestures
Figure 4: Phone query callouts
The system also provides subway directions. If the
user says How do I get to this place? and circles one
of the restaurants displayed on the map, the system
will ask Where do you want to go from? The user
can then respond with speech (e.g., 25th Street and
3rd Avenue), with pen by writing (e.g., 25th St & 3rd
Ave), or multimodally ( e.g, from here with a circle
gesture indicating location). The system then calcu-
lates the optimal subway route and dynamically gen-
erates a multimodal presentation of instructions. It
starts by zooming in on the first station and then grad-
ually zooms out, graphically presenting each stage of
the route along with a series of synchronized TTS
prompts. Figure 5 shows the final display of a sub-
way route heading downtown on the 6 train and trans-
ferring to the L train Brooklyn bound.
Figure 5: Multimodal subway route
User-tailored generation MATCH can also pro-
vide a user-tailored summary, comparison, or rec-
ommendation for an arbitrary set of restaurants, us-
ing a quantitative model of user preferences (Walker
et al, 2002). The system will only discuss restau-
rants that rank highly according to the user?s dining
preferences, and will only describe attributes of those
restaurants the user considers important. This per-
mits concise, targeted system responses. For exam-
ple, the user could say compare these restaurants and
circle a large set of restaurants (Figure 6). If the user
considers inexpensiveness and food quality to be the
most important attributes of a restaurant, the system
response might be:
Compare-A: Among the selected restaurants, the following
offer exceptional overall value. Uguale?s price is 33 dollars. It
has excellent food quality and good decor. Da Andrea?s price is
28 dollars. It has very good food quality and good decor. John?s
Pizzeria?s price is 20 dollars. It has very good food quality and
mediocre decor.
Figure 6: Comparing a large set of restaurants
2 Multimodal Application Architecture
The multimodal architecture supporting MATCH
consists of a series of agents which communicate
through a facilitator MCUBE (Figure 7).
Figure 7: Multimodal Architecture
MCUBE is a Java-based facilitator which enables
agents to pass messages either to single agents or
groups of agents. It serves a similar function to sys-
tems such as OAA (Martin et al, 1999), the use of
KQML for messaging in Allen et al(2000), and the
Communicator hub (Seneff et al, 1998). Agents may
reside either on the client device or elsewhere on the
network and can be implemented in multiple differ-
ent languages. MCUBE messages are encoded in
XML, providing a general mechanism for message
parsing and facilitating logging.
Multimodal User Interface Users interact with
the system through the Multimodal UI, which is
browser-based and runs in Internet Explorer. This
greatly facilitates rapid prototyping, authoring, and
reuse of the system for different applications since
anything that can appear on a webpage (dynamic
HTML, ActiveX controls, etc.) can be used in
the visual component of a multimodal user inter-
face. A TCP/IP control enables communication with
MCUBE.
MATCH uses a control that provides a dynamic
pan-able, zoomable map display. The control has ink
handling capability. This enables both pen-based in-
teraction (on the map) and normal GUI interaction
(on the rest of the page) without requiring the user to
overtly switch ?modes?. When the user draws on the
map their ink is captured and any objects potentially
selected, such as currently displayed restaurants, are
identified. The electronic ink is broken into a lat-
tice of strokes and sent to the gesture recognition
and handwriting recognition components which en-
rich this stroke lattice with possible classifications of
strokes and stroke combinations. The UI then trans-
lates this stroke lattice into an ink meaning lattice
representing all of the possible interpretations of the
user?s ink and sends it to MMFST.
In order to provide spoken input the user must tap
a click-to-speak button on the Multimodal UI. We
found that in an application such as MATCH which
provides extensive unimodal pen-based interaction, it
is preferable to use click-to-speak rather than pen-
to-speak or open-mike. With pen-to-speak, spurious
speech results received in noisy environments can
disrupt unimodal pen commands.
The Multimodal UI also provides graphical output
capabilities and performs synchronization of multi-
modal output. For example, it synchronizes the dis-
play actions and TTS prompts in the answer to the
route query mentioned in Section 1.
Speech Recognition MATCH uses AT&T?s Wat-
son speech recognition engine. A speech manager
running on the device gathers audio and communi-
cates with a recognition server running either on the
device or on the network. The recognition server pro-
vides word lattice output which is passed to MMFST.
Gesture and handwriting recognition Gesture
and handwriting recognition agents provide possible
classifications of electronic ink for the UI. Recogni-
tions are performed both on individual strokes and
combinations of strokes in the input ink lattice. The
handwriting recognizer supports a vocabulary of 285
words, including attributes of restaurants (e.g. ?chi-
nese?,?cheap?) and zones and points of interest (e.g.
?soho?,?empire?,?state?,?building?). The gesture rec-
ognizer recognizes a set of 10 basic gestures, includ-
ing lines, arrows, areas, points, and question marks.
It uses a variant of Rubine?s classic template-based
gesture recognition algorithm (Rubine, 1991) trained
on a corpus of sample gestures. In addition to classi-
fying gestures the gesture recognition agent also ex-
tracts features such as the base and head of arrows.
Combinations of this basic set of gestures and hand-
written words provide a rich visual vocabulary for
multimodal and pen-based commands.
Gestures are represented in the ink meaning lat-
tice as symbol complexes of the following form: G
FORM MEANING (NUMBER TYPE) SEM. FORM
indicates the physical form of the gesture and has val-
ues such as area, point, line, arrow. MEANING indi-
cates the meaning of that form; for example an area
can be either a loc(ation) or a sel(ection). NUMBER
and TYPE indicate the number of entities in a selec-
tion (1,2,3, many) and their type (rest(aurant), the-
atre). SEM is a place holder for the specific content
of the gesture, such as the points that make up an area
or the identifiers of objects in a selection.
When multiple selection gestures are present
an aggregation technique (Johnston and Bangalore,
2001) is employed to overcome the problems with
deictic plurals and numerals described in John-
ston (2000). Aggregation augments the ink meaning
lattice with aggregate gestures that result from com-
bining adjacent selection gestures. This allows a de-
ictic expression like these three restaurants to com-
bine with two area gestures, one which selects one
restaurant and the other two, as long as their sum is
three. For example, if the user makes two area ges-
tures, one around a single restaurant and the other
around two restaurants (Figure 3), the resulting ink
meaning lattice will be as in Figure 8. The first ges-
ture (node numbers 0-7) is either a reference to a
location (loc.) (0-3,7) or a reference to a restaurant
(sel.) (0-2,4-7). The second (nodes 7-13,16) is either
a reference to a location (7-10,16) or to a set of two
restaurants (7-9,11-13,16). The aggregation process
applies to the two adjacent selections and adds a se-
lection of three restaurants (0-2,4,14-16). If the user
says show chinese restaurants in this neighborhood
and this neighborhood, the path containing the two
locations (0-3,7-10,16) will be taken when this lat-
tice is combined with speech in MMFST. If the user
says tell me about this place and these places, then
the path with the adjacent selections is taken (0-2,4-
9,11-13,16). If the speech is tell me about these or
phone numbers for these three restaurants then the
aggregate path (0-2,4,14-16) will be chosen.
Multimodal Integrator (MMFST) MMFST re-
ceives the speech lattice (from the Speech Manager)
and the ink meaning lattice (from the UI) and builds
a multimodal meaning lattice which captures the po-
tential joint interpretations of the speech and ink in-
puts. MMFST is able to provide rapid response times
by making unimodal timeouts conditional on activity
in the other input mode. MMFST is notified when the
user has hit the click-to-speak button, when a speech
result arrives, and whether or not the user is inking on
the display. When a speech lattice arrives, if inking
is in progress MMFST waits for the ink meaning lat-
tice, otherwise it applies a short timeout (1 sec.) and
treats the speech as unimodal. When an ink meaning
lattice arrives, if the user has tapped click-to-speak
MMFST waits for the speech lattice to arrive, other-
wise it applies a short timeout (1 sec.) and treats the
ink as unimodal.
MMFST uses the finite-state approach to multi-
modal integration and understanding proposed by
Johnston and Bangalore (2000). Possibilities for
multimodal integration and understanding are cap-
tured in a three tape device in which the first tape
represents the speech stream (words), the second the
ink stream (gesture symbols) and the third their com-
bined meaning (meaning symbols). In essence, this
device takes the speech and ink meaning lattices as
inputs, consumes them using the first two tapes, and
writes out a multimodal meaning lattice using the
third tape. The three tape finite-state device is sim-
ulated using two transducers: G:W which is used to
align speech and ink and G W:M which takes a com-
posite alphabet of speech and gesture symbols as in-
put and outputs meaning. The ink meaning lattice
G and speech lattice W are composed with G:W and
the result is factored into an FSA G W which is com-
posed with G W:M to derive the meaning lattice M.
In order to capture multimodal integration using
finite-state methods, it is necessary to abstract over
specific aspects of gestural content (Johnston and
Bangalore, 2000). For example, all possible se-
quences of coordinates that could occur in an area
gesture cannot be encoded in the finite-state device.
We employ the approach proposed in (Johnston and
Bangalore, 2001) in which the ink meaning lattice is
converted to a transducer I:G, where G are gesture
symbols (including SEM) and I contains both gesture
symbols and the specific contents. I and G differ only
in cases where the gesture symbol on G is SEM, in
which case the corresponding I symbol is the specific
interpretation. After multimodal integration a pro-
jection G:M is taken from the result G W:M machine
and composed with the original I:G in order to rein-
corporate the specific contents that were left out of
the finite-state process (I:G o G:M = I:M).
The multimodal finite-state transducers used at
runtime are compiled from a declarative multimodal
context-free grammar which captures the structure
Figure 8: Ink Meaning Lattice
and interpretation of multimodal and unimodal com-
mands, approximated where necessary using stan-
dard approximation techniques (Nederhof, 1997).
This grammar captures not just multimodal integra-
tion patterns but also the parsing of speech and ges-
ture, and the assignment of meaning. In Figure 9 we
present a small simplified fragment capable of han-
dling MATCH commands such as phone numbers for
these three restaurants. A multimodal CFG differs
from a normal CFG in that the terminals are triples:
W:G:M, where W is the speech stream (words), G
the ink stream (gesture symbols) and M the meaning
stream (meaning symbols). An XML representation
for meaning is used to facilate parsing and logging
by other system components. The meaning tape sym-
bols concatenate to form coherent XML expressions.
The epsilon symbol (eps) indicates that a stream is
empty in a given terminal.
When the user says phone numbers for these
three restaurants and circles two groups of restau-
rants (Figure 3). The gesture lattice (Figure 8) is
turned into a transducer I:G with the same sym-
bol on each side except for the SEM arcs which are
split. For example, path 15-16 SEM([id1,id2,id3])
becomes [id1,id2,id3]:SEM. After G and the speech
W are integrated using G:W and G W:M. The G path
in the result is used to re-establish the connection
between SEM symbols and their specific contents
in I:G (I:G o G:M = I:M). The meaning read off
I:M is<cmd><phone><restaurant> [id1,id2,id3]
</restaurant> </phone> </cmd>. This is passed
to the multimodal dialog manager (MDM) and from
there to the Multimodal UI resulting in a display like
Figure 4 with coordinated TTS output. Since the
speech input is a lattice and there is also potential
for ambiguity in the multimodal grammar, the output
from MMFST to MDM is an N-best list of potential
multimodal interpretations.
Multimodal Dialog Manager (MDM) The MDM
is based on previous work on speech-act based mod-
els of dialog (Stent et al, 1999; Rich and Sidner,
1998). It uses a Java-based toolkit for writing dialog
managers that is similar in philosophy to TrindiKit
(Larsson et al, 1999). It includes several rule-based
S ! eps:eps:<cmd> CMD eps:eps:</cmd>
CMD ! phone:eps:<phone> numbers:eps:eps
for:eps:eps DEICTICNP
eps:eps:</phone>
DEICTICNP ! DDETPL eps:area:eps eps:selection:eps
NUM RESTPL eps:eps:<restaurant>
eps:SEM:SEM eps:eps:</restaurant>
DDETPL ! these:G:eps
RESTPL ! restaurants:restaurant:eps
NUM ! three:3:eps
Figure 9: Multimodal grammar fragment
processes that operate on a shared state. The state
includes system and user intentions and beliefs, a di-
alog history and focus space, and information about
the speaker, the domain and the available modalities.
The processes include interpretation, update, selec-
tion and generation processes.
The interpretation process takes as input an N-best
list of possible multimodal interpretations for a user
input from MMFST. It rescores them according to a
set of rules that encode the most likely next speech
act given the current dialogue context, and picks the
most likely interpretation from the result. The update
process updates the dialogue context according to the
system?s interpretation of user input. It augments the
dialogue history, focus space, models of user and sys-
tem beliefs, and model of user intentions. It also al-
ters the list of current modalities to reflect those most
recently used by the user.
The selection process determines the system?s next
move(s). In the case of a command, request or ques-
tion, it first checks that the input is fully specified
(using the domain ontology, which contains informa-
tion about required and optional roles for different
types of actions); if it is not, then the system?s next
move is to take the initiative and start an information-
gathering subdialogue. If the input is fully specified,
the system?s next move is to perform the command or
answer the question; to do this, MDM communicates
with the UI. Since MDM is aware of the current set
of preferred modalities, it can provide feedback and
responses tailored to the user?s modality preferences.
The generation process performs template-based
generation for simple responses and updates the sys-
tem?s model of the user?s intentions after generation.
The text planner is used for more complex genera-
tion, such as the generation of comparisons.
In the route query example in Section 1, MDM first
receives a route query in which only the destination
is specified How do I get to this place? In the se-
lection phase it consults the domain model and de-
termines that a source is also required for a route.
It adds a request to query the user for the source to
the system?s next moves. This move is selected and
the generation process selects a prompt and sends it
to the TTS component. The system asks Where do
you want to go from? If the user says or writes 25th
Street and 3rd Avenue then MMFST will assign this
input two possible interpretations. Either this is a re-
quest to zoom the display to the specified location or
it is an assertion of a location. Since the MDM dia-
logue state indicates that it is waiting for an answer
of the type location, MDM reranks the assertion as
the most likely interpretation. A generalized overlay
process (Alexandersson and Becker, 2001) is used to
take the content of the assertion (a location) and add
it into the partial route request. The result is deter-
mined to be complete. The UI resolves the location
to map coordinates and passes on a route request to
the SUBWAY component.
We found this traditional speech-act based dia-
logue manager worked well for our multimodal inter-
face. Critical in this was our use of a common seman-
tic representation across spoken, gestured, and multi-
modal commands. The majority of the dialogue rules
operate in a mode-independent fashion, giving users
flexibility in the mode they choose to advance the di-
alogue. On the other hand, mode sensitivity is also
important since user modality choice can be used to
determine system mode choice for confirmation and
other responses.
Subway Route Constraint Solver (SUBWAY)
This component has access to an exhaustive database
of the NYC subway system. When it receives a route
request with the desired source and destination points
from the Multimodal UI, it explores the search space
of possible routes to identify the optimal one, using a
cost function based on the number of transfers, over-
all number of stops, and the walking distance from
the station at each end. It builds a list of actions re-
quired to reach the destination and passes them to the
multimodal generator.
Multimodal Generator and Text-to-speech The
multimodal generator processes action lists from
SUBWAY and other components and assigns appro-
priate prompts for each action using a template-based
generator. The result is a ?score? of prompts and ac-
tions which is passed to the Multimodal UI. The Mul-
timodal UI plays this ?score? by coordinating changes
in the interface with the corresponding TTS prompts.
AT&T?s Natural Voices TTS engine is used to pro-
vide the spoken output. When the UI receives a mul-
timodal score, it builds a stack of graphical actions
such as zooming the display to a particular location
or putting up a graphical callout. It then sends the
prompts to be rendered by the TTS server. As each
prompt is synthesized the TTS server sends progress
notifications to the Multimodal UI, which pops the
next graphical action off the stack and executes it.
Text Planner and User Model The text plan-
ner receives instructions from MDM for execution
of ?compare?, ?summarize?, and ?recommend? com-
mands. It employs a user model based on multi-
attribute decision theory (Carenini and Moore, 2001).
For example, in order to make a comparison between
the set of restaurants shown in Figure 6, the text
planner first ranks the restaurants within the set ac-
cording to the predicted ranking of the user model.
Then, after selecting a small set of the highest ranked
restaurants, it utilizes the user model to decide which
restaurant attributes are important to mention. The
resulting text plan is converted to text and sent to TTS
(Walker et al, 2002). A user model for someone who
cares most highly about cost and secondly about food
quality and decor leads to a system response such as
that in Compare-A above. A user model for someone
whose selections are driven by food quality and food
type first, and cost only second, results in a system
response such as that shown in Compare-B.
Compare-B: Among the selected restaurants, the following of-
fer exceptional overall value. Babbo?s price is 60 dollars. It has
superb food quality. Il Mulino?s price is 65 dollars. It has superb
food quality. Uguale?s price is 33 dollars. It has excellent food.
Note that the restaurants selected for the user who
is not concerned about cost includes two rather more
expensive restaurants that are not selected by the text
planner for the cost-oriented user.
Multimodal Logger User studies, multimodal data
collection, and debugging were accomplished by in-
strumenting MATCH agents to send details of user
inputs, system processes, and system outputs to a log-
ger agent that maintains an XML log designed for
multimodal interactions. Our critical objective was
to collect data continually throughout system devel-
opment, and to be able to do so in mobile settings.
While this rendered the common practice of video-
taping user interactions impractical, we still required
high fidelity records of each multimodal interaction.
To address this problem, MATCH logs the state of
the UI and the user?s ink, along with detailed data
from other components. These components can in
turn dynamically replay the user?s speech and ink as
they were originally received, and show how the sys-
tem responded. The browser- and component-based
architecture of the Multimodal UI facilitated its reuse
in a Log Viewer that reads multimodal log files, re-
plays interactions between the user and system, and
allows analysis and annotation of the data. MATCH?s
logging system is similar in function to STAMP (Ovi-
att and Clow, 1998), but does not require multimodal
interactions to be videotaped and allows rapid re-
configuration for different annotation tasks since it
is browser-based. The ability of the system to log
data standalone is important, since it enables testing
and collection of multimodal data in realistic mobile
environments without relying on external equipment.
3 Experimental Evaluation
Our multimodal logging infrastructure enabled
MATCH to undergo continual user trials and evalu-
ation throughout development. Repeated evaluations
with small numbers of test users both in the lab and
in mobile settings (Figure 10) have guided the design
and iterative development of the system.
Figure 10: Testing MATCH in NYC
This iterative development approach highlighted
several important problems early on. For example,
while it was originally thought that users would for-
mulate queries and navigation commands primarily
by specifying the names of New York neighborhoods,
as in show italian restaurants in chelsea, early field
test studies in the city revealed that the need for
neighborhood names in the grammar was minimal
compared to the need for cross-streets and points of
interest; hence, cross-streets and a sizable list of land-
marks were added. Other early tests revealed the
need for easily accessible ?cancel? and ?undo? fea-
tures that allow users to make quick corrections. We
also discovered that speech recognition performance
was initially hindered by placement of the ?click-to-
speak? button and the recognition feedback box on
the bottom-right side of the device, leading many
users to speak ?to? this area, rather than toward the
microphone on the upper left side. This placement
also led left-handed users to block the microphone
with their arms when they spoke. Moving the but-
ton and the feedback box to the top-left of the device
resolved both of these problems.
After initial open-ended piloting trials, more struc-
tured user tests were conducted, for which we devel-
oped a set of six scenarios ordered by increasing level
of difficulty. These required the test user to solve
problems using the system. These scenarios were left
as open-ended as possible to elicit natural responses.
Sample scenario:You have plans to meet your aunt for dinner
later this evening at a Thai restaurant on the Upper West Side
near her apartment on 95th St. and Broadway. Unfortunately,
you forgot what time you?re supposed to meet her, and you can?t
reach her by phone. Use MATCH to find the restaurant and write
down the restaurant?s telephone number so you can check on the
reservation time.
Test users received a brief tutorial that was inten-
tionally vague and broad in scope so the users might
overestimate the system?s capabilities and approach
problems in new ways. Figure 11 summarizes re-
sults from our last scenario-based data collection for
a fixed version of the system. There were five sub-
jects (2 male, 3 female) none of whom had been in-
volved in system development. All of these five tests
were conducted indoors in offices.
exchanges 338 asr word accuracy 59.6%
speech only 171 51% asr sent. accuracy 36.1%
multimodal 93 28% handwritten sent. acc. 64%
pen only 66 19% task completion rate 85%
GUI actions 8 2% average time/scenario 6.25m
Figure 11: MATCH study
There were an average of 12.75 multimodal ex-
changes (pairs of user input and system response) per
scenario. The overall time per scenario varied from
1.5 to to 15 minutes. The longer completion times
resulted from poor ASR performance for some of the
users. Although ASR accuracy was low, overall task
completion was high, suggesting that the multimodal
aspects of the system helped users to complete tasks.
Unimodal pen commands were recognized more suc-
cessfully than spoken commands; however, only 19%
of commands were pen only. In ongoing work, we
are exploring strategies to increase users? adoption of
more robust pen-based and multimodal input.
MATCH has a very fast system response time.
Benchmarking a set of speech, pen, and multimodal
commands, the average response time is approxi-
mately 3 seconds (time from end of user input to sys-
tem response). We are currently completing a larger
scale scenario-based evaluation and an independent
evaluation of the functionality of the text planner.
In addition to MATCH, the same multimodal ar-
chitecture has been used for two other applications:
a multimodal interface to corporate directory infor-
mation and messaging and a medical application to
assist emergency room doctors. The medical proto-
type is the most recent and demonstrates the utility of
the architecture for rapid prototyping. System devel-
opment took under two days for two people.
4 Conclusion
The MATCH architecture enables rapid develop-
ment of mobile multimodal applications. Combin-
ing finite-state multimodal integration with a speech-
act based dialogue manager enables users to interact
flexibly using speech, pen, or synchronized combina-
tions of the two depending on their preferences, task,
and physical and social environment. The system
responds by generating coordinated multimodal pre-
sentations adapted to the multimodal dialog context
and user preferences. Features of the system such
as the browser-based UI and general purpose finite-
state architecture for multimodal integration facili-
tate rapid prototyping and reuse of the technology for
different applications. The lattice-based finite-state
approach to multimodal understanding enables both
multimodal integration and dialogue context to com-
pensate for recognition errors. The multimodal log-
ging infrastructure has enabled an iterative process
of pro-active evaluation and data collection through-
out system development. Since we can replay multi-
modal interactions without video we have been able
to log and annotate subjects both in the lab and in
NYC throughout the development process and use
their input to drive system development.
Acknowledgements
Thanks to AT&T Labs and DARPA (contract MDA972-99-3-
0003) for financial support. We would also like to thank Noemie
Elhadad, Candace Kamm, Elliot Pinson, Mazin Rahim, Owen
Rambow, and Nika Smith.
References
J. Alexandersson and T. Becker. 2001. Overlay as the ba-
sic operation for discourse processing in a multimodal
dialogue system. In 2nd IJCAI Workshop on Knowl-
edge and Reasoning in Practical Dialogue Systems.
J. Allen, D. Byron, M. Dzikovska, G. Ferguson,
L. Galescu, and A. Stent. 2000. An architecture for
a generic dialogue shell. JNLE, 6(3).
E. Andre?. 2002. Natural language in multime-
dia/multimodal systems. In Ruslan Mitkov, editor,
Handbook of Computational Linguistics. OUP.
G. Carenini and J. D. Moore. 2001. An empirical study of
the influence of user tailoring on evaluative argument
effectiveness. In IJCAI, pages 1307?1314.
M. Johnston and S. Bangalore. 2000. Finite-state mul-
timodal parsing and understanding. In Proceedings of
COLING 2000, Saarbru?cken, Germany.
M. Johnston and S. Bangalore. 2001. Finite-state meth-
ods for multimodal parsing and integration. In ESSLLI
Workshop on Finite-state Methods, Helsinki, Finland.
M. Johnston. 2000. Deixis and conjunction in mul-
timodal systems. In Proceedings of COLING 2000,
Saarbru?cken, Germany.
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999.
TrindiKit manual. Technical report, TRINDI Deliver-
able D2.2.
D. Martin, A. Cheyer, and D. Moran. 1999. The Open
Agent Architecture: A framework for building dis-
tributed software systems. Applied Artificial Intelli-
gence, 13(1?2):91?128.
M-J. Nederhof. 1997. Regular approximations of CFLs:
A grammatical view. In Proceedings of the Interna-
tional Workshop on Parsing Technology, Boston.
S. L. Oviatt and J. Clow. 1998. An automated tool for
analysis of multimodal system performance. In Pro-
ceedings of ICSLP.
C. Rich and C. Sidner. 1998. COLLAGEN: A collabora-
tion manager for software interface agents. User Mod-
eling and User-Adapted Interaction, 8(3?4):315?350.
D. Rubine. 1991. Specifying gestures by example. Com-
puter graphics, 25(4):329?337.
S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, and
V. Zue. 1998. Galaxy-II: A reference architecture for
conversational system development. In ICSLP-98.
A. Stent, J. Dowding, J. Gawron, E. Bratt, and R. Moore.
1999. The CommandTalk spoken dialogue system. In
Proceedings of ACL?99.
M. A. Walker, S. J. Whittaker, P. Maloor, J. D. Moore,
M. Johnston, and G. Vasireddy. 2002. Speech-Plans:
Generating evaluative responses in spoken dialogue. In
In Proceedings of INLG-02.
MATCHKiosk: A Multimodal Interactive City Guide
Michael Johnston
AT&T Research
180 Park Avenue
Florham Park, NJ 07932
johnston@research.att.com
Srinivas Bangalore
AT&T Research
180 Park Avenue
Florham Park, NJ 07932
srini@research.att.com
Abstract
Multimodal interfaces provide more flexible and
compelling interaction and can enable public infor-
mation kiosks to support more complex tasks for
a broader community of users. MATCHKiosk is
a multimodal interactive city guide which provides
users with the freedom to interact using speech,
pen, touch or multimodal inputs. The system re-
sponds by generating multimodal presentations that
synchronize synthetic speech with a life-like virtual
agent and dynamically generated graphics.
1 Introduction
Since the introduction of automated teller machines
in the late 1970s, public kiosks have been intro-
duced to provide users with automated access to
a broad range of information, assistance, and ser-
vices. These include self check-in at airports, ticket
machines in railway and bus stations, directions and
maps in car rental offices, interactive tourist and vis-
itor guides in tourist offices and museums, and more
recently, automated check-out in retail stores. The
majority of these systems provide a rigid structured
graphical interface and user input by only touch or
keypad, and as a result can only support a small
number of simple tasks. As automated kiosks be-
come more commonplace and have to support more
complex tasks for a broader community of users,
they will need to provide a more flexible and com-
pelling user interface.
One major motivation for developing multimodal
interfaces for mobile devices is the lack of a key-
board or mouse (Oviatt and Cohen, 2000; Johnston
and Bangalore, 2000). This limitation is also true of
many different kinds of public information kiosks
where security, hygiene, or space concerns make a
physical keyboard or mouse impractical. Also, mo-
bile users interacting with kiosks are often encum-
bered with briefcases, phones, or other equipment,
leaving only one hand free for interaction. Kiosks
often provide a touchscreen for input, opening up
the possibility of an onscreen keyboard, but these
can be awkward to use and occupy a considerable
amount of screen real estate, generally leading to a
more moded and cumbersome graphical interface.
A number of experimental systems have inves-
tigated adding speech input to interactive graphi-
cal kiosks (Raisamo, 1998; Gustafson et al, 1999;
Narayanan et al, 2000; Lamel et al, 2002). Other
work has investigated adding both speech and ges-
ture input (using computer vision) in an interactive
kiosk (Wahlster, 2003; Cassell et al, 2002).
We describe MATCHKiosk, (Multimodal Access
To City Help Kiosk) an interactive public infor-
mation kiosk with a multimodal interface which
provides users with the flexibility to provide in-
put using speech, handwriting, touch, or composite
multimodal commands combining multiple differ-
ent modes. The system responds to the user by gen-
erating multimodal presentations which combine
spoken output, a life-like graphical talking head,
and dynamic graphical displays. MATCHKiosk
provides an interactive city guide for New York
and Washington D.C., including information about
restaurants and directions on the subway or metro.
It develops on our previous work on a multimodal
city guide on a mobile tablet (MATCH) (Johnston
et al, 2001; Johnston et al, 2002b; Johnston et al,
2002a). The system has been deployed for testing
and data collection in an AT&T facility in Wash-
ington, D.C. where it provides visitors with infor-
mation about places to eat, points of interest, and
getting around on the DC Metro.
2 The MATCHKiosk
The MATCHKiosk runs on a Windows PC mounted
in a rugged cabinet (Figure 1). It has a touch screen
which supports both touch and pen input, and also
contains a printer, whose output emerges from a slot
below the screen. The cabinet alo contains speak-
ers and an array microphone is mounted above the
screen. There are three main components to the
graphical user interface (Figure 2). On the right,
there is a panel with a dynamic map display, a
click-to-speak button, and a window for feedback
on speech recognition. As the user interacts with
the system the map display dynamically pans and
zooms and the locations of restaurants and other
points of interest, graphical callouts with informa-
tion, and subway route segments are displayed. In
Figure 1: Kiosk Hardware
the top left there is a photo-realistic virtual agent
(Cosatto and Graf, 2000), synthesized by concate-
nating and blending image samples. Below the
agent, there is a panel with large buttons which en-
able easy access to help and common functions. The
buttons presented are context sensitive and change
over the course of interaction.
Figure 2: Kiosk Interface
The basic functions of the system are to enable
users to locate restaurants and other points of inter-
est based on attributes such as price, location, and
food type, to request information about them such
as phone numbers, addresses, and reviews, and to
provide directions on the subway or metro between
locations. There are also commands for panning and
zooming the map. The system provides users with
a high degree of flexibility in the inputs they use
in accessing these functions. For example, when
looking for restaurants the user can employ speech
e.g. find me moderately priced italian restaurants
in Alexandria, a multimodal combination of speech
and pen, e.g. moderate italian restaurants in this
area and circling Alexandria on the map, or solely
pen, e.g. user writes moderate italian and alexan-
dria. Similarly, when requesting directions they can
use speech, e.g. How do I get to the Smithsonian?,
multimodal, e.g. How do I get from here to here?
and circling or touching two locations on the map,
or pen, e.g. in Figure 2 the user has circled a loca-
tion on the map and handwritten the word route.
System output consists of coordinated presenta-
tions combining synthetic speech with graphical ac-
tions on the map. For example, when showing a
subway route, as the virtual agent speaks each in-
struction in turn, the map display zooms and shows
the corresponding route segment graphically. The
kiosk system also has a print capability. When a
route has been presented, one of the context sensi-
tive buttons changes to Print Directions. When this
is pressed the system generates an XHTML doc-
ument containing a map with step by step textual
directions and this is sent to the printer using an
XHTML-print capability.
If the system has low confidence in a user in-
put, based on the ASR or pen recognition score,
it requests confirmation from the user. The user
can confirm using speech, pen, or by touching on
a checkmark or cross mark which appear in the bot-
tom right of the screen. Context-sensitive graphi-
cal widgets are also used for resolving ambiguity
and vagueness in the user inputs. For example, if
the user asks for the Smithsonian Museum a small
menu appears in the bottom right of the map en-
abling them to select between the different museum
sites. If the user asks to see restaurants near a partic-
ular location, e.g. show restaurants near the white
house, a graphical slider appears enabling the user
to fine tune just how near.
The system also features a context-sensitive mul-
timodal help mechanism (Hastie et al, 2002) which
provides assistance to users in the context of their
current task, without redirecting them to separate
help system. The help system is triggered by spoken
or written requests for help, by touching the help
buttons on the left, or when the user has made sev-
eral unsuccessful inputs. The type of help is chosen
based on the current dialog state and the state of the
visual interface. If more than one type of help is ap-
plicable a graphical menu appears. Help messages
consist of multimodal presentations combining spo-
ken output with ink drawn on the display by the sys-
tem. For example, if the user has just requested to
see restaurants and they are now clearly visible on
the display, the system will provide help on getting
information about them.
3 Multimodal Kiosk Architecture
The underlying architecture of MATCHKiosk con-
sists of a series of re-usable components which
communicate using XML messages sent over sock-
ets through a facilitator (MCUBE) (Figure 3). Users
interact with the system through the Multimodal UI
displayed on the touchscreen. Their speech and
ink are processed by speech recognition (ASR) and
handwriting/gesture recognition (GESTURE, HW
RECO) components respectively. These recogni-
tion processes result in lattices of potential words
and gestures/handwriting. These are then com-
bined and assigned a meaning representation using a
multimodal language processing architecture based
on finite-state techniques (MMFST) (Johnston and
Bangalore, 2000; Johnston et al, 2002b). This pro-
vides as output a lattice encoding all of the potential
meaning representations assigned to the user inputs.
This lattice is flattened to an N-best list and passed
to a multimodal dialog manager (MDM) (Johnston
et al, 2002b) which re-ranks them in accordance
with the current dialogue state. If additional infor-
mation or confirmation is required, the MDM uses
the virtual agent to enter into a short information
gathering dialogue with the user. Once a command
or query is complete, it is passed to the multimodal
generation component (MMGEN), which builds a
multimodal score indicating a coordinated sequence
of graphical actions and TTS prompts. This score
is passed back to the Multimodal UI. The Multi-
modal UI passes prompts to a visual text-to-speech
component (Cosatto and Graf, 2000) which com-
municates with the AT&T Natural Voices TTS en-
gine (Beutnagel et al, 1999) in order to coordinate
the lip movements of the virtual agent with synthetic
speech output. As prompts are realized the Multi-
modal UI receives notifications and presents coordi-
nated graphical actions. The subway route server is
an application server which identifies the best route
between any two locations.
Figure 3: Multimodal Kiosk Architecture
4 Discussion and Related Work
A number of design issues arose in the development
of the kiosk, many of which highlight differences
between multimodal interfaces for kiosks and those
for mobile systems.
Array Microphone While on a mobile device a
close-talking headset or on-device microphone can
be used, we found that a single microphone had very
poor performance on the kiosk. Users stand in dif-
ferent positions with respect to the display and there
may be more than one person standing in front. To
overcome this problem we mounted an array micro-
phone above the touchscreen which tracks the loca-
tion of the talker.
Robust Recognition and Understanding is par-
ticularly important for kiosks since they have so
many first-time users. We utilize the techniques
for robust language modelling and multimodal
understanding described in Bangalore and John-
ston (2004).
Social Interaction For mobile multimodal inter-
faces, even those with graphical embodiment, we
found there to be little or no need to support so-
cial greetings and small talk. However, for a public
kiosk which different unknown users will approach
those capabilities are important. We added basic
support for social interaction to the language under-
standing and dialog components. The system is able
to respond to inputs such as Hello, How are you?,
Would you like to join us for lunch? and so on.
Context-sensitive GUI Compared to mobile sys-
tems, on palmtops, phones, and tablets, kiosks can
offer more screen real estate for graphical interac-
tion. This allowed for large easy to read buttons
for accessing help and other functions. The sys-
tem alters these as the dialog progresses. These but-
tons enable the system to support a kind of mixed-
initiative in multimodal interaction where the user
can take initiative in the spoken and handwritten
modes while the system is also able to provide
a more system-oriented initiative in the graphical
mode.
Printing Kiosks can make use of printed output
as a modality. One of the issues that arises is that
it is frequently the case that printed outputs such as
directions should take a very different style and for-
mat from onscreen presentations.
In previous work, a number of different multi-
modal kiosk systems supporting different sets of
input and output modalities have been developed.
The Touch-N-Speak kiosk (Raisamo, 1998) com-
bines spoken language input with a touchscreen.
The August system (Gustafson et al, 1999) is a mul-
timodal dialog system mounted in a public kiosk.
It supported spoken input from users and multi-
modal output with a talking head, text to speech,
and two graphical displays. The system was de-
ployed in a cultural center in Stockholm, enabling
collection of realistic data from the general public.
SmartKom-Public (Wahlster, 2003) is an interactive
public information kiosk that supports multimodal
input through speech, hand gestures, and facial ex-
pressions. The system uses a number of cameras
and a video projector for the display. The MASK
kiosk (Lamel et al, 2002) , developed by LIMSI and
the French national railway (SNCF), provides rail
tickets and information using a speech and touch in-
terface. The mVPQ kiosk system (Narayanan et al,
2000) provides access to corporate directory infor-
mation and call completion. Users can provide in-
put by either speech or touching options presented
on a graphical display. MACK, the Media Lab
Autonomous Conversational Kiosk, (Cassell et al,
2002) provides information about groups and indi-
viduals at the MIT Media Lab. Users interact us-
ing speech and gestures on a paper map that sits be-
tween the user and an embodied agent.
In contrast to August and mVPQ, MATCHKiosk
supports composite multimodal input combining
speech with pen drawings and touch. The
SmartKom-Public kiosk supports composite input,
but differs in that it uses free hand gesture for point-
ing while MATCH utilizes pen input and touch.
August, SmartKom-Public, and MATCHKiosk all
employ graphical embodiments. SmartKom uses
an animated character, August a model-based talk-
ing head, and MATCHKiosk a sample-based video-
realistic talking head. MACK uses articulated
graphical embodiment with ability to gesture. In
Touch-N-Speak a number of different techniques
using time and pressure are examined for enabling
selection of areas on a map using touch input. In
MATCHKiosk, this issue does not arise since areas
can be selected precisely by drawing with the pen.
5 Conclusion
We have presented a multimodal public informa-
tion kiosk, MATCHKiosk, which supports complex
unstructured tasks such as browsing for restaurants
and subway directions. Users have the flexibility to
interact using speech, pen/touch, or multimodal in-
puts. The system responds with multimodal presen-
tations which coordinate synthetic speech, a virtual
agent, graphical displays, and system use of elec-
tronic ink.
Acknowledgements Thanks to Eric Cosatto,
Hans Peter Graf, and Joern Ostermann for their help
with integrating the talking head. Thanks also to
Patrick Ehlen, Amanda Stent, Helen Hastie, Guna
Vasireddy, Mazin Rahim, Candy Kamm, Marilyn
Walker, Steve Whittaker, and Preetam Maloor for
their contributions to the MATCH project. Thanks
to Paul Burke for his assistance with XHTML-print.
References
S. Bangalore and M. Johnston. 2004. Balancing
Data-driven and Rule-based Approaches in the
Context of a Multimodal Conversational System.
In Proceedings of HLT-NAACL, Boston, MA.
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,
and A. Syrdal. 1999. The AT&T Next-
Generation TTS. In In Joint Meeting of ASA;
EAA and DAGA.
J. Cassell, T. Stocky, T. Bickmore, Y. Gao,
Y. Nakano, K. Ryokai, D. Tversky, C. Vaucelle,
and H. Vilhjalmsson. 2002. MACK: Media lab
autonomous conversational kiosk. In Proceed-
ings of IMAGINA02, Monte Carlo.
E. Cosatto and H. P. Graf. 2000. Photo-realistic
Talking-heads from Image Samples. IEEE Trans-
actions on Multimedia, 2(3):152?163.
J. Gustafson, N. Lindberg, and M. Lundeberg.
1999. The August spoken dialogue system. In
Proceedings of Eurospeech 99, pages 1151?
1154.
H. Hastie, M. Johnston, and P. Ehlen. 2002.
Context-sensitive Help for Multimodal Dialogue.
In Proceedings of the 4th IEEE International
Conference on Multimodal Interfaces, pages 93?
98, Pittsburgh, PA.
M. Johnston and S. Bangalore. 2000. Finite-
state Multimodal Parsing and Understanding. In
Proceedings of COLING 2000, pages 369?375,
Saarbru?cken, Germany.
M. Johnston, S. Bangalore, and G. Vasireddy. 2001.
MATCH: Multimodal Access To City Help. In
Workshop on Automatic Speech Recognition and
Understanding, Madonna di Campiglio, Italy.
M. Johnston, S. Bangalore, A. Stent, G. Vasireddy,
and P. Ehlen. 2002a. Multimodal Language Pro-
cessing for Mobile Information Access. In Pro-
ceedings of ICSLP 2002, pages 2237?2240.
M. Johnston, S. Bangalore, G. Vasireddy, A. Stent,
P. Ehlen, M. Walker, S. Whittaker, and P. Mal-
oor. 2002b. MATCH: An Architecture for Mul-
timodal Dialog Systems. In Proceedings of ACL-
02, pages 376?383.
L. Lamel, S. Bennacef, J. L. Gauvain, H. Dartigues,
and J. N. Temem. 2002. User Evaluation of
the MASK Kiosk. Speech Communication, 38(1-
2):131?139.
S. Narayanan, G. DiFabbrizio, C. Kamm,
J. Hubbell, B. Buntschuh, P. Ruscitti, and
J. Wright. 2000. Effects of Dialog Initiative and
Multi-modal Presentation Strategies on Large
Directory Information Access. In Proceedings of
ICSLP 2000, pages 636?639.
S. Oviatt and P. Cohen. 2000. Multimodal Inter-
faces That Process What Comes Naturally. Com-
munications of the ACM, 43(3):45?53.
R. Raisamo. 1998. A Multimodal User Interface
for Public Information Kiosks. In Proceedings of
PUI Workshop, San Francisco.
W. Wahlster. 2003. SmartKom: Symmetric Multi-
modality in an Adaptive and Reusable Dialogue
Shell. In R. Krahl and D. Gunther, editors, Pro-
ceedings of the Human Computer Interaction Sta-
tus Conference 2003, pages 47?62.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 376?383,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Multimodal Interface for Access to Content in the Home 
Michael Johnston 
AT&T Labs  
Research, 
Florham Park, 
New Jersey, USA 
johnston@ 
research. 
att.com 
Luis Fernando D?Haro 
Universidad Polit?cnica 
de Madrid,  
Madrid, Spain 
lfdharo@die. 
upm.es 
Michelle Levine 
AT&T Labs  
Research, 
Florham Park,  
New Jersey, USA 
mfl@research.
att.com 
Bernard Renger 
AT&T Labs  
Research, 
Florham Park,  
New Jersey, USA 
renger@ 
research. 
att.com 
 
Abstract 
In order to effectively access the rapidly 
increasing range of media content available 
in the home, new kinds of more natural in-
terfaces are needed.  In this paper, we ex-
plore the application of multimodal inter-
face technologies to searching and brows-
ing a database of movies.  The resulting 
system allows users to access movies using 
speech, pen, remote control, and dynamic 
combinations of these modalities. An ex-
perimental evaluation, with more than 40 
users, is presented contrasting two variants 
of the system: one combining speech with 
traditional remote control input and a sec-
ond where the user has a tablet display 
supporting speech and pen input. 
1 Introduction 
As traditional entertainment channels and the 
internet converge through the advent of technolo-
gies such as broadband access, movies-on-demand, 
and streaming video, an increasingly large range of 
content is available to consumers in the home.  
However, to benefit from this new wealth of con-
tent, users need to be able to rapidly and easily find 
what they are actually interested in, and do so ef-
fortlessly while relaxing on the couch in their liv-
ing room ? a location where they typically do not 
have easy access to the keyboard, mouse, and 
close-up screen display typical of desktop web 
browsing.  
Current interfaces to cable and satellite televi-
sion services typically use direct manipulation of a 
graphical user interface using a remote control. In 
order to find content, users generally have to either 
navigate a complex, pre-defined, and often deeply 
embedded menu structure or type in titles or other 
key phrases using an onscreen keyboard or triple 
tap input on a remote control keypad. These inter-
faces are cumbersome and do not scale well as the 
range of content available increases (Berglund, 
2004; Mitchell, 1999).  
 
Figure 1 Multimodal interface on tablet 
In this paper we explore the application of multi-
modal interface technologies (See Andr? (2002) 
for an overview) to the creation of more effective 
systems used to search and browse for entertain-
ment content in the home.  A number of previous 
systems have investigated the addition of unimodal 
spoken search queries to a graphical electronic 
program guide (Ibrahim and Johansson, 2002 
376
(NokiaTV); Goto et al, 2003; Wittenburg et al, 
2006). Wittenburg et alexperiment with unre-
stricted speech input for electronic program guide 
search, and use a highlighting mechanism to pro-
vide feedback to the user regarding the ?relevant? 
terms the system understood and used to make the 
query. However, their usability study results show 
this complex output can be confusing to users and 
does not correspond to user expectations. Others 
have gone beyond unimodal speech input and 
added multimodal commands combining speech 
with pointing (Johansson, 2003; Portele et al 
2006). Johansson (2003) describes a movie re-
commender system MadFilm where users can use 
speech and pointing to accept/reject recommended 
movies.  Portele et al(2006) describe the Smart-
Kom-Home system which includes multimodal 
electronic program guide on a tablet device. 
In our work we explore a broader range of inter-
action modalities and devices. The system provides 
users with the flexibility to interact using spoken 
commands, handwritten commands, unimodal 
pointing (GUI) commands, and multimodal com-
mands combining speech with one or more point-
ing gestures made on a display. We compare two 
different interaction scenarios. The first utilizes a 
traditional remote control for direct manipulation 
and pointing, integrated with a wireless micro-
phone for speech input. In this case, the only 
screen is the main TV display (far screen). In the 
second scenario, the user also has a second graphi-
cal display (close screen) presented on a mobile 
tablet which supports speech and pen input, includ-
ing both pointing and handwriting (Figure 1).  Our 
application task also differs, focusing on search 
and browsing of a large database of movies-on-
demand and supporting queries over multiple si-
multaneous dimensions.  This work also differs in 
the scope of the evaluation. Prior studies have pri-
marily conducted qualitative evaluation with small 
groups of users (5 or 6). A quantitative and qualita-
tive evaluation was conducted examining the inter-
action of 44 na?ve users with two variants of the 
system.  We believe this to be the first broad scale 
experimental evaluation of a flexible multimodal 
interface for searching and browsing large data-
bases of movie content.  
In Section 2, we describe the interface and illus-
trate the capabilities of the system. In Section 3, 
we describe the underlying multimodal processing 
architecture and how it processes and integrates 
user inputs.  Section 4 describes our experimental 
evaluation and comparison of the two systems. 
Section 5 concludes the paper. 
2 Interacting with the system 
The system described here is an advanced user in-
terface prototype which provides multimodal ac-
cess to databases of media content such as movies 
or television programming.  The current database 
is harvested from publicly accessible web sources 
and contains over 2000 popular movie titles along 
with associated metadata such as cast, genre, direc-
tor, plot, ratings, length, etc. 
The user interacts through a graphical interface 
augmented with speech, pen, and remote control 
input modalities. The remote control can be used to 
move the current focus and select items.  The pen 
can be used both for selecting items (pointing at 
them) and for handwritten input. The graphical 
user interface has three main screens. The main 
screen is the search screen (Figure 2). There is also 
a control screen used for setting system parameters 
and a third comparison display used for showing 
movie details side by side (Figure 4).  The user can 
select among the screens using three icons in the 
navigation bar at the top left of the screen. The ar-
rows provide ?Back? and ?Next? for navigation 
through previous searches.  Directly below, there is 
a feedback window which indicates whether the 
system is listening and provides feedback on 
speech recognition and search.  In the tablet vari-
ant, the microphone and speech recognizer are ac-
tivated by tapping on ?CLICK TO SPEAK? with 
the pen. In the remote control version, the recog-
nizer can also be activated using a button on the 
remote control.  The main section of the search 
display (Figure 2) contains two panels.  The right 
panel (results panel) presents a scrollable list of 
thumbnails for the movies retrieved by the current 
search.  The left panel (details panel) provides de-
tails on the currently selected title in the results 
panel.  These include the genre, plot summary, 
cast, and director.  
The system supports a speech modality, a hand-
writing modality, pointing (unimodal GUI) modal-
ity, and composite multimodal input where the user 
utters a spoken command which is combined with 
pointing ?gestures? the user has made towards 
screen icons using the pen or the remote control.  
 
377
 
 
 
 
 
Figure 2 Graphical user interface 
Speech: The system supports speech search over 
multiple different dimensions such as title, genre, 
cast, director, and year. Input can be more tele-
graphic with searches such as ?Legally Blonde?, 
?Romantic comedy?, and ?Reese Witherspoon?, or 
more verbose natural language queries such as 
?I?m looking for a movie called Legally Blonde? 
and ?Do you have romantic comedies?.  An impor-
tant advantage of speech is that it makes it easy to 
combine multiple constraints over multiple dimen-
sions within a single query (Cohen, 1992). For ex-
ample, queries can indicate co-stars: ?movies star-
ring Ginger Rogers and Fred Astaire?, or constrain 
genre and cast or director at the same time: ?Meg 
Ryan Comedies?, ?show drama directed by Woody 
Allen? and ?show comedy movies directed by 
Woody Allen and starring Mira Sorvino?.  
Handwriting: Handwritten pen input can also be 
used to make queries.  When the user?s pen ap-
proaches the feedback window, it expands allow-
ing for freeform pen input. In the example in Fig-
ure 3, the user requests comedy movies with Bruce 
Willis using unimodal handwritten input. This is an 
important input modality as it is not impacted by 
ambient noise such as crosstalk from other viewers 
or currently playing content. 
 
Figure 3 Handwritten query 
 Navigation Bar Feedback Window 
Pointing/GUI:  In addition to the recognition-
based modalities, speech and handwriting, the in-
terface also supports more traditional graphical 
user interface (GUI) commands. In the details 
panel, the actors and directors are presented as but-
tons. Pointing at (i.e., clicking on) these buttons 
results in a search for all of the movies with that 
particular actor or director, allowing users to 
quickly navigate from an actor or director in a spe-
cific title to other material they may be interested 
in. The buttons in the results panel can be pointed 
at (clicked on) in order to view the details in the 
left panel for that particular title.   
 
Actor/Director Buttons Details Results 
Figure 4 Comparison screen 
Composite multimodal input: The system also 
supports true composite multimodality when spo-
ken or handwritten commands are integrated with 
pointing gestures made using the pen (in the tablet 
version) or by selecting items (in the remote con-
trol version).  This allows users to quickly execute 
more complex commands by combining the ease 
of reference of pointing with the expressiveness of 
spoken constraints.  While by unimodally pointing 
at an actor button you can search for all of the ac-
tor?s movies, by adding speech you can narrow the 
search to, for example, all of their comedies by 
saying: ?show comedy movies with THIS actor?.  
Multimodal commands with multiple pointing ges-
tures are also supported, allowing the user to ?glue? 
together references to multiple actors or directors 
in order to constrain the search.  For example, they 
can say ?movies with THIS actor and THIS direc-
tor? and point at the ?Alan Rickman? button and 
then the ?John McTiernan? button in turn (Figure 
2). Comparison commands can also be multimo-
378
dal; for example, if the user says ?compare THIS 
movie and THIS movie? and clicks on the two but-
tons on the right display for ?Die Hard? and the 
?The Fifth Element? (Figure 2), the resulting dis-
play shows the two movies side-by-side in the 
comparison screen (Figure 4).  
3 Underlying multimodal architecture 
The system consists of a series of components 
which communicate through a facilitator compo-
nent (Figure 5). This develops and extends upon 
the multimodal architecture underlying the 
MATCH system (Johnston et al, 2002). 
 
Multimodal UI ASR
Server
ASR
Server
Multimodal
NLU
Multimodal
NLU
Movie DB
(XML)
NLU 
Model
Grammar Template
ASR 
Model
WordsGestures
Speech
Client
Speech
Client
Meaning
Grammar
Compiler
Grammar
Compiler
F
A
C
I
L
I
T
A
T
O
R
Handwriting
Handwriting
Recognition
 
Figure 5 System architecture 
The underlying database of movie information is 
stored in XML format.  When a new database is 
available, a Grammar Compiler component ex-
tracts and normalizes the relevant fields from the 
database. These are used in conjunction with a pre-
defined multimodal grammar template and any 
available corpus training data to build a multimo-
dal understanding model and speech recognition 
language model.   
The user interacts with the multimodal user in-
terface client (Multimodal UI), which provides the 
graphical display.  When the user presses ?CLICK 
TO SPEAK? a message is sent to the Speech Cli-
ent, which activates the microphone and ships au-
dio to a speech recognition server.  Handwritten 
inputs are processed by a handwriting recognizer 
embedded within the multimodal user interface 
client. Speech recognition results, pointing ges-
tures made on the display, and handwritten inputs, 
are all passed to a multimodal understanding server 
which uses finite-state multimodal language proc-
essing techniques (Johnston and Bangalore, 2005) 
to interpret and integrate the speech and gesture. 
This model combines alignment of multimodal 
inputs, multimodal integration, and language un-
derstanding within a single mechanism. The result-
ing combined meaning representation (represented 
in XML) is passed back to the multimodal user 
interface client, which translates the understanding 
results into an XPATH query and runs it against 
the movie database to determine the new series of 
results.  The graphical display is then updated to 
represent the latest query. 
The system first attempts to find an exact match 
in the database for all of the search terms in the 
user?s query.  If this returns no results, a back off 
and query relaxation strategy is employed. First the 
system tries a search for movies that have all of the 
search terms, except stop words, independent of 
the order (an AND query). If this fails, then it 
backs off further to an OR query of the search 
terms and uses an edit machine, using Levenshtein 
distance, to retrieve the most similar item to the 
one requested by the user.  
4 Evaluation 
After designing and implementing our initial proto-
type system, we conducted an extensive multimo-
dal data collection and usability study with the two 
different interaction scenarios: tablet versus remote 
control.  Our main goals for the data collection and 
statistical analysis were three-fold: collect a large 
corpus of natural multimodal dialogue for this me-
dia selection task, investigate whether future sys-
tems should be paired with a remote control or tab-
let-like device, and determine which types of 
search and input modalities are more or less desir-
able. 
4.1 Experimental set up 
The system evaluation took place in a conference 
room set up to resemble a living room (Figure 6). 
The system was projected on a large screen across 
the room from a couch. 
An adjacent conference room was used for data 
collection (Figure 7). Data was collected in sound 
files, videotapes, and text logs. Each subject?s spo-
ken utterances were recorded by three micro-
phones: wireless, array and stand alone. The wire-
less microphone was connected to the system 
while the array and stand alone microphones were 
379
around 10 feet away.1 Test sessions were recorded 
with two video cameras ? one captured the sys-
tem?s screen using a scan converter while the other 
recorded the user and couch area. Lastly, the user?s 
interactions and the state of the system were cap-
tured by the system?s logger. The logger is an addi-
tional agent added to the system architecture for 
the purposes of the evaluation.  It receives log mes-
sages from different system components as interac-
tion unfolds and stores them in a detailed XML log 
file. For the specific purposes of this evaluation, 
each log file contains: general information about 
the system?s components, a description and time-
stamp for each system event and user event, names 
and timestamps for the system-recorded sound 
files, and timestamps for the start and end of each 
scenario. 
 
Figure 6 Data collection environment 
Forty-four subjects volunteered to participate in 
this evaluation.  There were 33 males and 11 fe-
males, ranging from 20 to 66 years of age.  Each 
user interacted with both the remote control and 
tablet variants of the system, completing the same 
two sets of scenarios and then freely interacting 
with each system.  For counterbalancing purposes, 
half of the subjects used the tablet and then the re-
mote control and the other half used the remote 
                                                 
1 Here we report results for the wireless microphone only. 
Analysis of the other microphone conditions is ongoing. 
control and then the tablet.  The scenario set as-
signed to each version was also counterbalanced.   
 
Figure 7 Data collection room 
Each set of scenarios consisted of seven defined 
tasks, four user-specialized tasks and five open-
ended tasks. Defined tasks were presented in chart 
form and had an exact answer, such as the movie 
title that two specified actors/actresses starred in. 
For example, users had to find the movie in the 
database with Matthew Broderick and Denzel 
Washington. User-specialized tasks relied on the 
specific user?s preferences, such as ?What type of 
movie do you like to watch on a Sunday evening?  
Find an example from that genre and write down 
the title?. Open-ended tasks prompted users to 
search for any type of information with any input 
modality. The tasks in the two sets paralleled each 
other. For example, if one set of tasks asked the 
user to find the highest ranked comedy movie with 
Reese Witherspoon, the other set of tasks asked the 
user to find the highest ranked comedy movie with 
Will Smith. Within each task set, the defined tasks 
appeared first, then the user-specialized tasks and 
lastly the open-ended tasks. However, for each par-
ticipant, the order of defined tasks was random-
ized, as well as the order of user-specialized tasks. 
At the beginning of the session, users read a 
short tutorial about the system?s GUI, the experi-
ment, and available input modalities. Before inter-
acting with each version, users were given a man-
ual on operating the tablet/remote control. To 
minimize bias, the manuals gave only a general 
overview with few examples and during the ex-
periment users were alone in the room.  
At the end of each session, users completed a 
user-satisfaction/preference questionnaire and then 
a qualitative interview. The questionnaire consisted 
380
of 25 statements about the system in general, the 
two variants of the system, input modality options 
and search options. For example, statements 
ranged from ?If I had [the system], I would use the 
tablet with it? to ?If my spoken request was mis-
understood, I would want to try again with speak-
ing?.  Users responded to each statement with a 5-
point Likert scale, where 1 = ?I strongly agree?, 2 = 
?I mostly agree?, 3 = ?I can?t say one way or the 
other?, 4 = ?I mostly do not agree? and 5 = ?I do not 
agree at all?. The qualitative interview allowed for 
more open-ended responses, where users could 
discuss reasons for their preferences and their likes 
and dislikes regarding the system. 
4.2 Results 
Data was collected from all 44 participants. Due to 
technical problems, five participants? logs or sound 
files were not recorded in parts of the experiment.  
All collected data was used for the overall statistics 
but these five participants had to be excluded from 
analyses comparing remote control to tablet. 
Spoken utterances: After removing empty 
sound files, the full speech corpus consists of 3280 
spoken utterances.  Excluding the five participants 
subject to technical problems, the total is 3116 ut-
terances (1770 with the remote control and 1346 
with the tablet).   
The set of 3280 utterances averages 3.09 words 
per utterance.  There was not a significant differ-
ence in utterance length between the remote con-
trol and tablet conditions.  Users? averaged 2.97 
words per utterance with the remote control and 
3.16 words per utterance with the tablet, paired t 
(38) = 1.182, p = n.s.  However, users spoke sig-
nificantly more often with the remote control.  On 
average, users spoke 34.51 times with the tablet 
and 45.38 times with the remote control, paired t 
(38) = -3.921, p < .01. 
ASR performance: Over the full corpus of 
3280 speech inputs, word accuracy was 44% and 
sentence accuracy 38%.  In the tablet condition, 
word accuracy averaged 46% and sentence accu-
racy 41%.  In the remote control condition, word 
accuracy averaged 41% and sentence accuracy 
38%.  The difference across conditions was only 
significant for word accuracy, paired t (38) = 
2.469, p < .02.  In considering the ASR perform-
ance, it is important to note that 55% of the 3280 
speech inputs were out of grammar, and perhaps 
more importantly 34% were out of the functional-
ity of the system entirely.  On within functionality 
inputs, word accuracy is 62% and sentence accu-
racy 57%.  On the in grammar inputs, word accu-
racy is 86% and sentence accuracy 83%. The vo-
cabulary size was 3851 for this task. In the corpus, 
there are a total of 356 out-of-vocabulary words.  
Handwriting recognition: Performance was de-
termined by manual inspection of screen capture 
video recordings. 2   There were a total of 384 
handwritten requests with overall 66% sentence 
accuracy and 76% word accuracy. 
Task completion:  Since participants had to re-
cord the task answers on a paper form, task com-
pletion was calculated by whether participants 
wrote down the correct answer.  Overall, users had 
little difficulty completing the tasks.  On average, 
participants completed 11.08 out of the 14 defined 
tasks and 7.37 out of the 8 user-specialized tasks.  
The number of tasks completed did not differ 
across system variants. 3  For the seven defined 
tasks within each condition, users averaged 5.69 
with the remote control and 5.40 with the tablet, 
paired t (34) = -1.203, p = n.s.  For the four user-
specialized task within each condition, users aver-
aged 3.74 on the remote control and 3.54 on the 
tablet, paired t (34) = -1.268, p = n.s. 
Input modality preference: During the inter-
view, 55% of users reported preferring the pointing 
(GUI) input modality over speech and multimodal 
input. When asked about handwriting, most users 
were hesitant to place it on the list.  They also dis-
cussed how speech was extremely important, and 
given a system with a low error speech recognizer, 
using speech for input probably would be their first 
choice. In the questionnaire, the majority of users 
(93%) ?strongly agree? or ?mostly agree? with the 
importance of making a pointing request. The im-
portance of making a request by speaking had the 
next highest average, where 57% ?strongly agree? 
or ?mostly agree? with the statement. The impor-
tance of multimodal and handwriting requests had 
the lowest averages, where 39% agreed with the 
former and 25% for the latter.  However, in the 
open-ended interview, users mentioned handwrit-
ing as an important back-up input choice for cases 
when the speech recognizer fails. 
                                                 
2 One of the 44 participants videotape did not record and so is 
not included in the statistics.     
3 Four participants did not properly record their task answers 
and had to be eliminated from the 39 participants being used 
in the remote control versus tablet statistics.   
381
Further support for input modality preference was 
gathered from the log files, which showed that par-
ticipants mostly searched using unimodal speech 
commands and GUI buttons.  Out of a total of 
6082 user inputs to the systems, 48% were unimo-
dal speech and 39% were unimodal GUI (pointing 
and clicking). Participants requested information 
with composite multimodal commands 7% of the 
time and with handwriting 6% of the time. 
Search preference: Users most strongly agreed 
with movie title being the most important way to 
search. For searching by title, more than half the 
users chose ?strongly agree? and 91% of users 
chose ?strongly agree? or ?mostly agree?.  Slightly 
more than half chose ?strongly agree? with search-
ing by actor/actress and slightly less than half 
chose ?strongly agree? with the importance of 
searching by genre. During the open ended inter-
view, most users reported title as the most impor-
tant means for searching. 
Variant preference:  Results from the qualita-
tive interview indicate that 67% of users preferred 
the remote control over the tablet variant of the 
system. The most common reported reasons were 
familiarity, physical comfort and ease of use. Re-
mote control preference is further supported from 
the user-preference questionnaire, where 68% of 
participants ?mostly agree? or ?strongly agree? with 
wanting to use the remote control variant of the 
system, compared to 30% of participants choosing 
?mostly agree? or ?strongly agree? with wanting to 
use the tablet version of the system. 
5 Conclusion  
With the range of entertainment content available 
to consumers in their homes rapidly expanding, the 
current access paradigm of direct manipulation of 
complex graphical menus and onscreen keyboards, 
and remote controls with way too many buttons is 
increasingly ineffective and cumbersome. In order 
to address this problem, we have developed a 
highly flexible multimodal interface that allows 
users to search for content using speech, handwrit-
ing, pointing (using pen or remote control), and 
dynamic multimodal combinations of input modes. 
Results are presented in a straightforward graphical 
interface similar to those found in current systems 
but with the addition of icons for actors and direc-
tors that can be used both for unimodal GUI and 
multimodal commands. The system allows users to 
search for movies over multiple different dimen-
sions of classification (title, genre, cast, director, 
year) using the mode or modes of their choice. We 
have presented the initial results of an extensive 
multimodal data collection and usability study with 
the system. 
Users in the study were able to successfully use 
speech in order to conduct searches. Almost half of 
their inputs were unimodal speech (48%) and the 
majority of users strongly agreed with the impor-
tance of using speech as an input modality for this 
task. However, as also reported in previous work 
(Wittenburg et al2006), recognition accuracy re-
mains a serious problem. To understand the per-
formance of speech recognition here, detailed error 
analysis is important. The overall word accuracy 
was 44% but the majority of errors resulted from 
requests from users that lay outside the functional-
ity of the underlying system, involving capabilities 
the system did not have or titles/cast absent from 
the database (34% of the 3280 spoken and multi-
modal inputs). No amount of speech and language 
processing can resolve these problems. This high-
lights the importance of providing more detailed 
help and tutorial mechanisms in order to appropri-
ately ground users? understanding of system capa-
bilities. Of the remaining 66% of inputs (2166) 
which were within the functionality of the system, 
68% were in grammar. On the within functionality 
portion of the data, the word accuracy was 62%, 
and on in grammar inputs it is 86%.  Since this was 
our initial data collection, an un-weighted finite-
state recognition model was used. The perform-
ance will be improved by training stochastic lan-
guage models as data become available and em-
ploying robust understanding techniques. One in-
teresting issue in this domain concerns recognition 
of items that lie outside of the current database. 
Ideally the system would have a far larger vocabu-
lary than the current database so that it would be 
able to recognize items that are outside the data-
base. This would allow feedback to the user to dif-
ferentiate between lack of results due to recogni-
tion or understanding problems versus lack of 
items in the database. This has to be balanced 
against degradation in accuracy resulting from in-
creasing the vocabulary.  
In practice we found that users, while acknowl-
edging the value of handwriting as a back-up 
mode, generally preferred the more relaxed and 
familiar style of interaction with the remote con-
trol. However, several factors may be at play here. 
382
The tablet used in the study was the size of a small 
laptop and because of cabling had a fixed location 
on one end of the couch. In future, we would like 
to explore the use of a smaller, more mobile, tablet 
that would be less obtrusive and more conducive to 
leaning back on the couch. Another factor is that 
the in-lab data collection environment is somewhat 
unrealistic since it lacks the noise and disruptions 
of many living rooms. It remains to be seen 
whether in a more realistic environment we might 
see more use of handwritten input. Another factor 
here is familiarity. It may be that users have more 
familiarity with the concept of speech input than 
handwriting. Familiarity also appears to play a role 
in user preferences for remote control versus tablet. 
While the tablet has additional capabilities such 
handwriting and easier use of multimodal com-
mands, the remote control is more familiar to users 
and allows for a more relaxed interaction since 
they can lean back on the couch. Also many users 
are concerned about the quality of their handwrit-
ing and may avoid this input mode for that reason.   
Another finding is that it is important not to un-
derestimate the importance of GUI input. 39% of 
user commands were unimodal GUI (pointing) 
commands and 55% of users reported a preference 
for GUI over speech and handwriting for input. 
Clearly, the way forward for work in this area is to 
determine the optimal way to combine more tradi-
tional graphical interaction techniques with the 
more conversational style of spoken interaction. 
Most users employed the composite multimodal 
commands, but they make up a relatively small 
proportion of the overall number of user inputs in 
the study data (7%). Several users commented that 
they did not know enough about the multimodal 
commands and that they might have made more 
use of them if they had understood them better. 
This, along with the large number of inputs that 
were out of functionality, emphasizes the need for 
more detailed tutorial and online help facilities. 
The fact that all users were novices with the sys-
tem may also be a factor. In future, we hope to 
conduct a longer term study with repeat users to 
see how previous experience influences use of 
newer kinds of inputs such as multimodal and 
handwriting.   
Acknowledgements Thanks to Keith Bauer, Simon Byers, 
Harry Chang, Rich Cox, David Gibbon, Mazin Gilbert, 
Stephan Kanthak, Zhu Liu, Antonio Moreno, and Behzad 
Shahraray for their help and support.  Thanks also to the Di-
recci?n General de Universidades e Investigaci?n - Consejer?a 
de Educaci?n - Comunidad de Madrid, Espa?a for sponsoring 
D?Haro?s visit to AT&T. 
References 
Elisabeth Andr?. 2002. Natural Language in Multimodal 
and Multimedia systems. In Ruslan Mitkov (ed.) Ox-
ford Handbook of Computational Linguistics. Oxford 
University Press. 
Aseel Berglund. 2004. Augmenting the Remote Control: 
Studies in Complex Information Navigation for Digi-
tal TV. Link?ping Studies in Science and Technol-
ogy, Dissertation no. 872. Link?ping University. 
Philip R. Cohen. 1992. The Role of Natural Language in 
a Multimodal Interface. In Proceedings of ACM 
UIST Symposium on User Interface Software and 
Technology. pp. 143-149. 
Jun Goto, Kazuteru Komine, Yuen-Bae Kim and Nori-
yoshi Uratan. 2003. A Television Control System 
based on Spoken Natural Language Dialogue. In 
Proceedings of 9th International Conference on Hu-
man-Computer Interaction. pp. 765-768. 
Aseel Ibrahim and Pontus Johansson. 2002. Multimodal 
Dialogue Systems for Interactive TV Applications. In 
Proceedings of 4th IEEE International Conference 
on Multimodal Interfaces. pp. 117-222. 
Pontus Johansson. 2003. MadFilm - a Multimodal Ap-
proach to Handle Search and Organization in a 
Movie Recommendation System. In Proceedings of 
the 1st Nordic Symposium on Multimodal Communi-
cation. Helsing?r, Denmark. pp. 53-65. 
Michael Johnston, Srinivas Bangalore, Guna Vasireddy, 
Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve 
Whittaker, Preetam Maloor. 2002. MATCH: An Ar-
chitecture for Multimodal Dialogue Systems. In Pro-
ceedings of the 40th ACL. pp. 376-383. 
Michael Johnston and Srinivas Bangalore. 2005. Finite-
state Multimodal Integration and Understanding. 
Journal of Natural Language Engineering 11.2. 
Cambridge University Press. pp. 159-187. 
Russ Mitchell. 1999. TV?s Next Episode. U.S. News 
and World Report. 5/10/99. 
Thomas Portele, Silke Goronzy, Martin Emele, Andreas 
Kellner, Sunna Torge, and J?ergen te Vrugt. 2006. 
SmartKom?Home: The  Interface to Home Enter-
tainment. In Wolfgang Wahlster (ed.) SmartKom: 
Foundations of Multimodal Dialogue Systems. 
Springer.  pp. 493-503. 
Kent Wittenburg, Tom Lanning, Derek Schwenke, Hal 
Shubin and Anthony Vetro. 2006. The Prospects for 
Unrestricted Speech Input for TV Content Search. In 
Proceedings of  AVI?06. pp. 352-359. 
383
Bridging the Gap: Academic and Industrial Research in Dialog Technologies Workshop Proceedings, pages 17?24,
NAACL-HLT, Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
The Multimodal Presentation Dashboard 
 
Michael Johnston 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
johnston 
@research. 
att.com 
Patrick Ehlen 
CSLI 
 Stanford University 
Palo Alto, CA  
ehlen@csli. 
stanford.edu 
David Gibbon 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
dcg@research. 
att.com 
Zhu Liu 
AT&T Labs Research 
180 Park Ave 
Florham Park, NJ 
zliu@research. 
att.com 
 
Abstract 
The multimodal presentation dashboard al-
lows users to control and browse presenta-
tion content such as slides and diagrams 
through a multimodal interface that sup-
ports speech and pen input. In addition to 
control commands (e.g. ?take me to slide 
10?), the system allows multimodal search 
over content collections. For example, if 
the user says ?get me a slide about internet 
telephony,? the system will present a 
ranked series of candidate slides that they 
can then select among using voice, pen, or 
a wireless remote. As presentations are 
loaded, their content is analyzed and lan-
guage and understanding models are built 
dynamically. This approach frees the user 
from the constraints of linear order allow-
ing for a more dynamic and responsive 
presentation style. 
1 Introduction 
Anthropologists have long informed us that the 
way we work?whether reading, writing, or giving 
a presentation?is tightly bound to the tools we 
use. Web browsers and word processors changed 
the way we read and write from linear to nonlinear 
activities, though the linear approach to giving a 
presentation to a roomful of people has evolved 
little since the days of Mylar sheets and notecards, 
thanks to presentation software that reinforces?or 
even further entrenches?a linear bias in our no-
tion of what ?giving a presentation? means to us. 
While today?s presentations may be prettier and 
flashier, the spontaneity once afforded by holding a 
stack of easily re-arrangeable sheets has been lost. 
 
 
 
Figure 1 Presentation dashboard in action 
Instead, a question from the audience or a change 
in plan at the podium results in a whizzing-by of 
all the wrong slides as the presenter sweats through 
an awkward silence while hammering an arrow 
key to track down the right one. In theory there are 
?search? functions that presenters could use to find 
another slide in the same presentation, or even in 
another presentation on the same machine, though 
none of the authors of this paper has ever seen a 
presenter do this. A likely reason is that these 
search functions are designed for desktop ergo-
17
nomics rather than for standing at a podium or 
walking around the room, making them even more 
disruptive to the flow of a presentation than frantic 
arrow key hammering. 
In some utopian future, we envision presenters 
who are unhindered by limitations imposed by 
their presentation tools, and who again possess, as 
Aristotle counseled, ?all available means of per-
suasion? at the tips of their fingers?or their 
tongues. They enjoy freeform interactions with 
their audiences, and benefit from random access to 
their own content with no arrow hammering and no 
disruption in flow. Their tools help to expand their 
possible actions rather than limiting them. We are 
hardly alone in this vision. 
In that spirit, many tools have been developed of 
late?both within and outside of research labs?
with the aim of helping people work more effec-
tively when they are involved in those assemblies 
of minds of mutual interest we often call ?meet-
ings.? Tools that capture the content of meetings, 
perform semantic understanding, and provide a 
browsable summary promise to free meeting par-
ticipants from the cognitive constraints of worrying 
about trying to record and recall what happened 
when a meeting takes place (e.g., Ehlen, Purver & 
Niekrasz, 2007; Tucker & Whittaker, 2005).  
Presentations are a kind of meeting, and several 
presentation tools have also sought to free present-
ers from similar constraints. For example, many 
off-the-shelf products provide speech interfaces to 
presentation software. These often replace the lin-
ear arrow key with the voice, offering command-
based navigation along a one-dimensional vector 
of slides by allowing a presenter to say ?next slide 
please? or ?go to the last slide.?  
A notable exception is the Jabberwocky inter-
face to PowerPoint (Franklin, Bradshaw & 
Hammond, 1999; 2000), which aims to follow 
along with a presenter?s talk?like a human assis-
tant might do?and switch to the appropriate slide 
when the presenter seems to be talking about it. 
Using a method similar to topic modeling, words 
spoken by the presenter are compared to a prob-
ability distribution of words across slides. Jabber-
wocky changes to a different slide when a 
sufficient probability mass has been reached to 
justify the assumption that the speaker is now talk-
ing about a different slide from the one that?s al-
ready showing. 
A similar effort (Rogina & Schaaf, 2002) uses 
words extracted from a presentation to augment a 
class-based language model and attempt automatic 
tracking of a presentation as it takes place. This 
intelligent meeting room system then aligns the 
presenter?s spoken words with parts of a presenta-
tion, hoping to determine when a presenter has 
moved on to a new slide. 
A major drawback of this ?machine-initiative? 
approach to presentation assistance is that a pre-
senter must speak enough words associated with a 
new slide for a sufficient probability mass to be 
reached before the slide is changed. The resulting 
delay is likely to make an audience feel like the 
presentation assistant is rather dim-witted. And any 
errors that change slides before the presenter is 
ready can be embarrassing and disruptive in front 
of potentially important audiences. 
So, in fashioning our own presentation control 
interface, we chose to allow the presenter to retain 
full initiative in changing slides, while offering a 
smarter and more flexible way to navigate through 
a presentation than the single degree of freedom 
afforded by arrow keys that simply traverse a pre-
determined order. The result is the Multimodal 
Presentation Dashboard, a presentation interface 
that integrates command-based control with prob-
abilistic, content-based search. Our method starts 
with a context-free grammar of speech commands, 
but embeds a stochastic language model generated 
from the presenter?s slide deck content so a pre-
senter can request any slide from the deck?or 
even a large set of decks?just by asking for its 
contents. Potentially ambiguous results are re-
solved multimodally, as we will explain. 
2 Multimodal interface for interactive 
presentations  
The presentation dashboard provides presenters 
with the ability to control and adapt their presenta-
tions on the fly in the meeting room. In addition to 
the traditional next/previous approach to navigat-
ing a deck of slides, they can access slides by posi-
tion in the active deck (e.g., ?show slide 10? or 
?last slide please?) or they can multimodally com-
bine voice commands with pen or remote control 
to browse for slides by content, saying, for in-
stance, ?show the slide on internet telephony,? and 
then using the pen to select among a ranked list of 
alternatives. 
18
2.1 Setup configuration 
Though the dashboard offers many setup configu-
rations, the preferred arrangement uses a single PC 
with two displays (Figure 1). Here, the dashboard 
is running on a tablet PC with a large monitor as a 
second external display. On the tablet, the 
dashboard UI is visible only to the presenter. On 
the external display, the audience sees the current 
slide, as they would with a normal presentation. 
The presenter can interact with the dashboard 
using either the microphone onboard the tablet PC, 
or, preferably, a wireless microphone. A wireless 
remote functions as a presentation control, which 
can be used to manually change slides in the tradi-
tional manner, and also provides a ?push to talk? 
button to tell the dashboard when to listen. A wire-
less microphone combined with the wireless pres-
entation control and voice selection mode (see 
Section 2.3) allows a presenter to stroll around the 
room or stage completely untethered.  
2.2 Presenter UI 
The presenter?s primary control of the system is 
through the presenter UI, a graphical user interface 
augmented with speech and pen input. The inter-
face has three main screens: a presentation panel 
for controlling an ongoing presentation (Figure 2), 
a loader panel for selecting a set of presentations to 
load (Figure 4), and a control panel for adjusting 
system settings and bundling shareable index and 
grammar models. The user can select among the 
panels using the tabs at the top left.  
 
  
Figure 2 The presentation panel 
 
The presentation panel has three distinct functional 
areas from top to bottom. The first row shows the 
current slide, along with thumbnails of the previ-
ous and next slides to provide context. The user 
can navigate to the next or previous slide by click-
ing on these thumbnails. The next row shows a 
scrolling list of search results from content-based 
queries. The last row contains interaction informa-
tion. There is a click & speak button for activating 
the speech recognizer and a feedback window that 
displays recognized speech.  
Some user commands are independent of the 
content of slide decks, as with basic commands for 
slide navigation: 
- ?next slide please? 
- ?go back? 
- ?last slide? 
In practice, however, navigation to next and previ-
ous slides is much easier using buttons on the wire-
less control. The presenter can also ask for slides 
by position number, allowing random access: 
- ?take me to slide 10? 
- ?slide 4 please? 
But not many presenters can remember the posi-
tion numbers of some 40 or 50 slides, we?d guess, 
so we added content-based search, a better method 
of random access slide retrieval by simply saying 
key words or phrases from the desired slide, e.g.:  
- ?slides about internet telephony? 
- ?get me the slide with the 
  system architecture? 
- ?2006 highlights? 
- ?budget plan, please? 
When the presenter gives this kind of request, the 
system identifies any slides that match the query 
and displays them in a rank ordered list in the mid-
dle row of the presenter?s panel. The presenter can 
then scroll through the list of thumbnails and click 
one to display it to the audience. 
This method of ambiguity resolution offers the 
presenter some discretion in selecting the correct 
slide to display from multiple search results, since 
search results appear first on the presenter?s private 
interface rather than being displayed to the audi-
ence. However, it requires the presenter to return to 
the podium (or wherever the tablet is located) to 
select the correct slide.  
 
19
2.3 Voice selection mode 
Alternatively, the presenter may sacrifice discre-
tion for mobility and use a ?voice selection mode,? 
which lets the presenter roam freely throughout the 
auditorium while making and resolving content-
based queries in plain view of the audience. In this 
mode, if a presenter issues a content-based query 
(e.g., ?shows slides about multimodal access?), 
thumbnails of the slides returned by the query ap-
pear as a dynamically-generated interactive 
?chooser? slide (Figure 3) in the main presentation 
viewed by the audience. The presenter can then 
select the desired slide by voice (e.g., ?slide three?) 
or by using the previous, next, and select controls 
on the wireless remote. If more than six slides are 
returned by the query, multiple chooser slides are 
generated with six thumbnails to each slide, which 
can be navigated with the remote.  
While voice selection mode allows the presenter 
greater mobility, it has the drawback of allowing 
the audience to see thumbnails of every slide re-
turned by a content-based query, regardless of 
whether the presenter intended for them to be seen. 
Hence this mode is more risky, but also more im-
pressive! 
 
 
Figure 3 Chooser slide for voice selection mode 
2.4 Compiling deck sets 
Sometimes a presenter wishes to have access to 
more than one presentation deck at a time, in order 
to respond to unexpected questions or comments, 
or to indulge in a whimsical tangent. We respond 
to this wish by allowing the presenter to compile a 
deck set, which is, quite simply, a user-defined 
bundle of multiple presentations that can all be 
searched at once, with their slides available for 
display when the user issues a query. In fact, this 
option makes it easy for a presenter to follow spon-
taneous tangents by switching from one presenta-
tion to another, navigating through the alternate 
deck for a while, and then returning to the original 
presentation, all without ever walking to the po-
dium or disrupting the flow of a presentation by 
stopping and searching through files. 
Deck sets are compiled in the loader panel (Fig-
ure 4), which provides a graphical browser for se-
lecting a set of active decks from the file system. 
When a deck set is chosen, the system builds ASR 
and language understanding models and a retrieval 
index for all the slides in the deck set. A compiled 
deck set is also portable, with all of the grammar 
and understanding model files stored in a single 
archive that can be transferred via e-mail or thumb 
drive and speedily loaded on another machine.  
A common use of deck sets is to combine a 
main presentation with a series of other slide decks 
that provide background information and detail for 
answering questions and expanding points, so the 
presenter can adapt to the interests of the audience. 
 
Figure 4 The loader panel 
3 Multimodal architecture  
The Multimodal Presentation Dashboard uses an 
underlying multimodal architecture that inherits 
core components from the MATCH architecture 
(Johnston et al2002). The components communi-
cate through a central messaging facilitator and 
include a speech recognition client, speech recog-
nition server (Goffin et al2005), a natural lan-
guage understanding component (Johnston & 
Bangalore 2005), an information retrieval engine, 
20
and a graphical user interface client.  The graphical 
UI runs in a web browser and controls PowerPoint 
via its COM interface.  
We first describe the compilation architecture, 
which builds models and performs indexing when 
the user selects a series of decks to activate. We 
then describe the runtime architecture that operates 
when the user gives a presentation using the sys-
tem. In Section 3.3, we provide more detail on the 
slide indexing mechanism and in Section 3.4 we 
describe a mechanism used to determine key-
phrases from the slide deck that are used on a drop 
down menu and for determining relevancy.  
3.1 Compilation architecture 
In a sense, the presentation dashboard uses neither 
static nor dynamic grammars; the grammars com-
piled with each deck set lie somewhere in-between 
those two concepts. Command-based speech inter-
faces often fare best when they rely on the predict-
ability of a fixed, context-free grammar, while 
interfaces that require broader vocabulary coverage 
and a wider range of syntax are better off leverag-
ing the flexibility of stochastic language models. 
To get the best of both worlds for our ASR model, 
we use a context-free command ?wrapper? to a 
stochastic language model (c.f. Wang & Acero 
2003). This is coupled to the understanding 
mechanism using a transducer with a loop over the 
content words extracted from the slides.  
This combined grammar is best thought of as a 
fixed, context-free template which contains an em-
bedded SLM of dynamic slide contents. Our 
method allows a static background grammar and 
understanding model to happily co-exist with a 
dynamic grammar component which is compiled 
on the fly when presentations are loaded, enabling 
custom, content-based queries.  
When a user designates a presentation deck set 
and compiles it, the slides in the set are processed 
to create the combined grammar by composing an 
SLM training corpus based on the slide content.  
First, a slide preprocessor extracts sentences, ti-
tles, and captions from each slide of each deck, and 
normalizes the text by converting numerals and 
symbols to strings, Unicode to ASCII, etc. These 
content phrases are then used to compose (1) a 
combined corpus to use for training an SLM for 
speech recognition, and (2) a finite-state transducer 
to use for multimodal natural language understand-
ing (Johnston & Bangalore 2005). 
Combined Corpus
Presentations
Slide 
index
Keyphrases
Slide PreprocessorSlide Preprocessor
Sentences
Index 
Server
Index 
Server
SLM 
for ASR
SLM 
for ASR NLU
MODEL
NLU
MODEL
GUI 
Menu
GUI 
Menu
Grammar
Template
Class-tagged
Corpus
Grammar
Words
 
Figure 5 Compilation architecture 
To create a combined corpus for the SLM, the con-
tent phrases extracted from slides are iterated over 
and folded into a static template of corpus classes. 
For instance, the template entry, 
<POLITE> <SHOWCON> <CONTENT_PHRASE> 
could generate the phrase ?please show the slide 
about <CONTENT_PHRASE>? for each content 
phrase?as well as many others.  These templates 
are currently manually written but could poten-
tially be induced from data as it becomes available. 
The content corpus is appended to a command 
corpus of static command classes that generate 
phrases like ?next slide please? or ?go back to the 
last one.? Since the number of these command 
phrases remains constant for every grammar while 
the number of content phrases depends on how 
many phrases are extracted from the deck set, a 
weighting factor is needed to ensure the number of 
examples of both content and command phrases is 
balanced in the SLM training data. The resulting 
combined corpus is used to build a stochastic lan-
guage model that can handle variations on com-
mands and slide content.  
In parallel to the combined corpus, a stack of 
slide content words is compiled for the finite state 
understanding machine. Phrases extracted for the 
combined corpus are represented as a terminal 
_CWORD class. (Terminals for tapes in each gram-
mar class are separated by colons, in the format 
speech:meaning, with empty transitions repre-
21
sented as ?) For example, the phrase ?internet 
telephony? on a slide would appear in the under-
standing grammar like so: 
_CWORD internet:internet 
_CWORD telephony:telephony 
These content word classes are then ?looped? in 
the FSM (Figure 6) into a flexible understanding 
model of potential slide content results using only 
a few grammar rules, like: 
_CONTENT _CWORD _CONTENT 
_CONTENT _CWORD 
The SLM and the finite-state understanding ma-
chine now work together to extract plausible mean-
ings from dynamic and inexact speech queries. 
 
 
Figure 6 Understanding FSM 
To provide an example of how this combined ap-
proach to understanding comes together in the run-
ning system, let?s say a presenter?s slide contains 
the title ?Report for Third Quarter? and she asks 
for it by saying, ?put up the third quarter report 
slide.? Though she asks for the slide with language 
that doesn?t match the phrase on the slide, our for-
giving stochastic model might return a speech re-
sult like, ?put up third quarter report mine.? The 
speech result is then mapped to the finite-state 
grammar, which catches ?third quarter report 
mine? as a possible content phrase, and returns, 
?third,quarter,report,mine? as a con-
tent-based meaning result. That result is then used 
for information retrieval and ranking to determine 
which slides best match the query (Section 3.3). 
3.2 Runtime architecture  
A primary goal of the presentation dashboard was 
that it should run standalone on a single laptop. A 
tablet PC works best for selecting slides with a 
pen, though a mouse or touch screen can also be 
used for input. We also developed a networked 
version of the dashboard system where indexing, 
compilation, speech recognition, and understand-
ing are all network services accessed over HTTP 
and SIP, so any web browser-based client can log 
in, upload a presentation, and present without in-
stalling software aside from PowerPoint and a SIP 
plug-in. However, our focus in this paper is on the 
tablet PC standalone version. 
ASR SERVERASR SERVER
Multimodal Dashboard 
UI (Browser)
Multimodal Dashboard 
UI (Browser)
NLUNLU
Powerpoint
Application
Powerpoint
Application
Index Server (http)Index Server (http)
Language
Model
Slide index
HTTP
Commands
Images
FACILITATORFACILITATOR
SPEECH
CLIENT
SPEECH
CLIENT
Understanding
Model
 
Figure 7 Multimodal architecture 
The multimodal user interface client is browser-
based, using dynamic HTML and Javascript. Inter-
net Explorer provides COM access to the Power-
Point object model, which reveals slide content and 
controls the presentation. Speech recognition, un-
derstanding, and compilation components are ac-
cessed through a java-based facilitator via a socket 
connection provided by an ActiveX control on the 
client page (Figure 7). When the user presses or 
taps the click & speak button, a message is sent to 
the Speech client, which sends audio to the ASR 
Server. The recognizer?s speech result is processed 
by the NLU component using a finite-state trans-
ducer to translate from the input string to an XML 
meaning representation. When the multimodal UI 
receives XML for simple commands like ?first 
slide? or ?take me to slide ten,? it calls the appro-
priate function through the PowerPoint API. For 
content-based search commands, an SQL query is 
constructed and issued to the index server as an 
HTTP query. When the results are returned, mul-
timodal thumbnail images of each slide appear in 
the middle row of the UI presenter panel. The user 
can then review the choices and switch to the ap-
propriate slide by clicking on it?or, in voice se-
lection mode, by announcing or selecting a slide 
shown in the dynamically-generated chooser slide.  
The system uses a three stage strategy in search-
ing for slides. First it attempts an exact match by 
looking for slides which have the words of the 
query in the same order on the same slide in a sin-
gle phrase. If no exact matches are found, the sys-
tem backs off to an AND query and shows slides 
which contain all of the words, in any order. If that 
22
fails, the system resorts to an OR query and shows 
slides which have any of the query terms.  
3.3 Information retrieval 
When the slide preprocessor extracts text from a 
presentation, it retains the document structure as 
much as possible and stores this in a set of hier-
archal XML documents. The structure includes 
global document metadata such as creation date 
and title, as well as more detailed data such as slide 
titles. It also includes information about whether 
the text was part of a bullet list or text box. With 
this structure, queries can be executed against the 
entire text or against specified textual attributes 
(e.g. ?show me the chart titled ?project budget??). 
For small document collections, XPath queries 
can search the entire collection with good response 
time, providing a stateless search method. But as 
the collection of presentation decks to be searched 
grows, a traditional inverted index information re-
trieval system achieves better response times. We 
use a full text retrieval system that employs stem-
ming, proximity search, and term weighting, and 
supports either a simplified query syntax or SQL. 
Global metadata can also constrain queries. Incre-
mental indexing ensures that new presentation 
decks cause the index to update automatically 
without being rebuilt from scratch. 
3.4 Key phrase extraction 
Key phrases and keywords are widely used for in-
dexing and retrieving documents in large data-
bases. For presentation slides, they can also help 
rank a slide?s relevance to a query. We extract a 
list of key phrases with importance scores for each 
slide deck, and phrases from a set of decks are 
merged and ranked based on their scores. 
A popular approach to selecting keywords from 
a document within a corpus is to find keywords 
that frequently occur in one document but seldom 
occur in others, based on term frequency-inverse 
document frequency (TF-IDF). Our task is slightly 
different, since we wish to choose key phrases for 
a single document (the slide deck), independent of 
other documents. So our approach uses term fre-
quency-inverse term probability (TF-ITP), which 
expresses the probability of a term calculated over 
a general language rather than a set of documents. 
Assuming a term Tk occurs tfk times in a docu-
ment, and its term probability is tpk, the TF-ITP of 
Tk is defined as, wTk = tfk / tpk. This method can be 
extended to assign an importance score to each 
phrase. For a phrase Fk = {T1 T2 T3 ? TN}, which 
contains a sequence of N terms, assuming it ap-
pears ffk times in a document, its importance score, 
ISk, is defined as, 
?
=
=
N
i i
k
k T
ff
IS
1
. 
To extract a set of key phrases, we first segment 
the document into sentences based on punctuation 
and some heuristics. A Porter stemming algorithm 
(Porter 1980) eliminates word variations, and 
phrases up to N=4 terms long are extracted, remov-
ing any that start or end with noise words. An im-
portance score ranks each phrase, where term 
probabilities are estimated from transcripts of 600 
hours of broadcast news data. A term that is out of 
the vocabulary with a term frequency of more than 
2 is given a default term probability value, defined 
as the minimum term probability in the vocabulary. 
Phrases with high scores are chosen as key 
phrases, eliminating any phrases that are contained 
in other phrases with higher scores. For an overall 
list of key phrases in a set of documents, we merge 
individual key phrase lists and sum the importance 
scores for key phrases that recur in different lists, 
keeping the top 10 phrases. 
4 Performance and future work 
The dashboard is fully implemented, and has been 
used by staff and management in our lab for inter-
nal presentations and talks. It can handle large 
decks and collections (100s to 1000s of slides). A 
tablet PC with a Pentium M 1.6Ghz processor and 
1GB of RAM will compile a presentation of 50 
slides?with ASR, understanding models, and 
slide index?in under 30 seconds.  
In ongoing work, we are conducting a usability 
test of the system with users in the lab. Effective 
evaluation of a tool of this kind is difficult without 
fielding the system to a large number of users. An 
ideal evaluation would measure how users fare 
when giving their own presentations, responding to 
natural changes in narrative flow and audience 
questions. Such interaction is difficult to simulate 
in a lab, and remains an active area of research. 
23
We also hope to extend current retrieval meth-
ods to operate at the level of concepts, rather than 
words and phrases, so a request to show ?slides 
about mortgages? might return a slide titled ?home 
loans.? Thesauri, gazetteers, and lexicons like 
WordNet will help achieve this. Analyzing non-
textual elements like tables and charts could also 
allow a user to say, ?get the slide with the network 
architecture diagram.? And, while we now use a 
fixed lexicon of common abbreviations, an auto-
mated analysis based on web search and other 
techniques could identify likely expansions. 
5 Conclusion 
Our goal with the multimodal presentation 
dashboard was to create a meeting/presentation 
assistance tool that would change how people be-
have, inspiring presenters to expand the methods 
they use to interact with audiences and with their 
own material. To this end, our dashboard runs on a 
single laptop, leaves the initiative in the hands of 
the presenter, and allows slides from multiple pres-
entations to be dynamically retrieved from any-
where in the room. Our assistant requires no 
?intelligent room?; only an intelligent presenter, 
who may now offer the audience a presentation 
that is as dynamic or as dull as imagination allows. 
As Tufte (2006) reminds us in his analysis of 
how PowerPoint presentations may have precipi-
tated the Columbia shuttle tragedy, the way infor-
mation is presented can have a profound?even 
life-threatening?impact on the decisions we 
make. With the multimodal presentation 
dashboard, we hope to free future presenters from 
that single, arrow-key dimension, offering access 
to presentation slides and diagrams in any order, 
using a diverse combination of modes. Presenters 
can now pay more attention to the needs of their 
audiences than to the rigid determinism of a fixed 
presentation. Whether they will break free of the 
linear presentation style imposed by current tech-
nology if given a chance remains to be seen. 
References  
Patrick Ehlen, Matthew Purver, and John Niekrasz. 
2007. A meeting browser that learns. In Proceedings 
of the AAAI Spring Symposium on Interaction Chal-
lenges for Intelligent Assistants. 
David Franklin, Shannon Bradshaw, and Kristian 
Hammond. 1999. Beyond ?Next slide, please?: The 
use of content and speech in multi-modal control. In 
Working Notes of the AAAI-99 Workshop on Intelli-
gent Information Systems. 
David Franklin, Shannon Bradshaw, and Kristian 
Hammond. 2000. Jabberwocky: You don't have to be 
a rocket scientist to change slides for a hydrogen 
combustion lecture. In Proceedings of Intelligent 
User Interfaces 2000 (IUI-2000). 
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri, Dilek 
Hakkani-T?r, Andrej Ljolje, Sarangarajan Partha-
sarathy, Mazin Rahim, Giuseppe Riccardi, and Murat 
Saraclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP.   
Michael Johnston, Srinivas Bangalore, Guna Vasireddy, 
Amanda Stent, Patrick Ehlen, Marilyn Walker, Steve 
Whittaker, Preetam Maloor. 2002. MATCH: An Ar-
chitecture for Multimodal Dialogue Systems. In Pro-
ceedings of the 40th ACL. 376-383. 
Michael Johnston  and Srinivas Bangalore. 2005. Finite-
state multimodal integration and understanding. 
Journal of Natural Language Engineering. 11.2, pp. 
159-187, Cambridge University Press. 
Martin F. Porter. 1980. An algorithm for suffix strip-
ping, Program, 14, 130-137. 
Ivica Rogina and Thomas Schaaf. 2002. Lecture and 
presentation tracking in an intelligent meeting room. 
In Proceedings of the 4th IEEE International Confer-
ence on Multimodal Interfaces. 47-52. 
Simon Tucker and Steve Whittaker. 2005. Accessing 
multimodal meeting data: Systems, problems and 
possibilities. In Samy Bengio and Herv? Bourlard 
(Eds.) Lecture Notes in Computer Science, 3361, 1-
11 
Edward Tufte. 2006. The Cognitive Style of PowerPoint. 
Graphics Press, Cheshire, CT. 
Ye-Yi Wang and Alex Acero. 2003. Combination of 
CFG and N-gram Modeling in Semantic Grammar 
Learning. Proceedings of Eurospeech conference, 
Geneva, Switzerland. 
Acknowledgements We would like to thank Srinivas Banga-
lore, Rich Cox, Mazin Gilbert, Vincent Goffin, and Behzad 
Shahraray for their help and support.  
24
Proceedings of the SIGDIAL 2013 Conference, pages 329?333,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Spoken Dialog Systems for Automated Survey Interviewing 
Michael Johnston1, Patrick Ehlen2, Frederick G. Conrad3, Michael F. Schober4,  Christopher Antoun3, Stefanie Fail4, Andrew Hupp3, Lucas Vickers4,  Huiying Yan3, Chan Zhang3 AT&T Labs Research, Florham Park, NJ, USA1, AT&T, San Francisco, CA, USA2 Survey Research Center, University of Michigan, Ann Arbor, USA3 The New School, New York, NY, USA4 johnston@research.att.com, ehlen@research.att.com,  fconrad@umich.edu, schober@newschool.edu,  antoun@umich.edu, stefaniefail@gmail.com, ahupp@umich.edu,  lucasvickers@gmail.com, yanhuier@umich.edu, chanzh@umich.edu 
Abstract We explore the plausibility of using automated spoken dialog systems (SDS) for administer-ing survey interviews. Because the goals of a survey dialog system differ from more tradi-tional information-seeking and transactional applications, different measures of task accu-racy and success may be warranted. We report a large-scale experimental evaluation of an SDS that administered survey interviews with questions drawn from government and social scientific surveys. We compare two dialog confirmation strategies: (1) a traditional strate-gy of explicit confirmation on low-confidence recognition; and (2) no confirmation. With ex-plicit confirmation, the small percentage of re-sidual errors had little to no impact on survey data measurement. Even without confirmation, while there are significantly more errors, im-pact on the substantive conclusions of the sur-vey is still very limited. 1 Introduction Survey interviews play a critical role in the oper-ation of government and commerce. Large-scale social scientific surveys provide key indicators of the success or failure of economic and social pol-icies, driving critical policy and funding deci-sions. Market research surveys are key in evalu-ating products and services for business. Survey interviews are typically conducted ei-ther via telephone or face-to-face by skilled hu-man interviewers. But ongoing changes in com-munication technology threaten the viability of these methods. As people migrate from landline telephony to mobile-only (Ehlen and Ehlen 2007) and Voice-over-IP (Fuchs 2008) as prima-ry modes of communication, they undermine the effectiveness of traditional survey sampling techniques that rely on random selection of num-
bers within a dial code. Telephone respondents were once reachable at a fixed geographic loca-tion in a largely predictable conversational envi-ronment. Now they are increasingly mobile, and more apt to prefer asynchronous communication. Thus it is imperative to understand how these changing behaviors affect survey results. The work described here is part of a larger re-search project (see Schober et al 2012; Conrad et al 2013) that investigates the viability of four different modes for administering a survey inter-view over a smartphone: automated voice, hu-man voice, automated SMS text, and human SMS text. Here we focus specifically on the au-tomated voice mode and explore the use of a spoken dialog system for survey administration. Spoken dialog systems are widely used in te-lephony applications such as customer service, information access, and transaction fulfillment. They are also now common in virtual assistant applications for smartphones and mobile devices. But survey designers seeking automation have mostly eschewed spoken dialog in favor of tex-tual web surveys or touchtone DTMF response systems. A preliminary comparison of spoken dialog and touchtone survey systems is available in Bloom (2008), and Stent et al (2007) offer an evaluation of a spoken dialog system for aca-demic course ratings. The work presented here describes the first large-scale investigation into spoken dialog technology as a viable means of administering the kinds of surveys that produce official statistics and social scientific data.  Survey interview designers should be interest-ed in using spoken dialog systems for several reasons. The most obvious reason is to curtail the error and bias that human interviewers are known to introduce to survey results data. Dec-ades of research and investment led to ?standard-ized interviewing techniques? to reduce this error (Fowler and Mangione 1990), and limit a survey 
329
interviewer?s ability to offer help or clarification in ways that might affect results. Automated dia-log systems can be thought of as the ultimate in standardization, as they can be designed to pro-vide exactly the same interaction possibilities to all respondents. In effect, everyone can be inter-viewed by the same ?interviewer.? Or, if survey designers want to allow clarification in an inter-view, an automated spoken dialog system can ensure that the same possibilities are available to all respondents (Schober and Conrad 1997). Unlike systems that use human interviewers, there is marginal additional cost per interview after the initial investment of building a system. This offers significant potential for cost savings in large cross-sectional samples or repeated panel surveys, such as the U.S. Current Population Survey or the American Community Survey. Re-peated data collection allows refinement and re-training of speech models to improve perfor-mance. Spoken dialog system surveys can be administered on demand at any time of day, al-lowing a better fit with respondents? circum-stances and schedules. Compared to asynchro-nous text-based interviews like web or paper-and-pencil surveys, spoken dialog systems can capture richer verbal paradata (Couper 2009) or process data like pauses, disfluencies and proso-dy (Ehlen et al 2007). Finally, survey tasks fit nicely within the limitations of current recogni-tion and dialog technology, since they tend to have a purposefully structured and controlled interaction flow and generally require only a lim-ited number of responses to each question. While spoken dialog systems have the poten-tial to remove data error that is introduced by variation in human interviewer behaviors, they also introduce risks to survey data quality due to speech recognition and understanding error.  Numerous strategies for mitigating error have been explored in research on dialog systems (Bohus and Rudnicky 2005, Litman et al 2006). One approach is to use either an explicit or im-plicit confirmation of the user's input. Following previous research showing that explicit confir-mation is less confusing for users (Shin et al 2002), we adopt an explicit confirmation strate-gy, which is also more in keeping with standard-ized interview techniques. The effects of speech recognition and under-standing errors may be different in a survey dia-log system than in most current spoken dialog applications. One consideration is speaker initia-tive, and the stake of the user in the interaction. In systems for customer service, information ac-
cess, or transactions, the user generally initiates contact with the system and seeks to accomplish a task where the system?s recognition accuracy will affect success of the user?s own goal. But in a survey dialog, the system initiates contact, and most respondents do not have a stake in whether the designers of the survey system succeed at collecting high quality data from them.  This is a key point where a survey interview-ing system might differ from traditional SDS: From the survey researchers? perspective, the critical question is not whether individual users achieve some goal, but rather the extent to which individual errors in system recognition and un-derstanding affect the distribution of responses across the population sample, affecting the quali-ty of the estimates produced. If recognition errors do not affect the substantive conclusions based on the survey data, then survey researchers should be able to tolerate the imprecision of recognition error. This situation makes survey system evaluation rather different from how one would expect to evaluate the task success of a traditional SDS, like a customer service system.  In Section 2, we characterize the content of the survey items, describe the dialog strategy, and provide examples of interaction. Section 3 de-scribes the technical architecture of the survey dialog system. We provide experimental evalua-tion in Section 4, and conclusions in Section 5. 2 Survey interview dialogs After an initial question assessing whether the respondent is in an environment where it is safe for them to talk, our system administers a series of 32 questions drawn from major U.S. social surveys, including the Behavioral Risk Factor Surveillance System (BRFSS), National Survey of Drug Use and Health (NSDUH), General So-cial Survey (GSS), and the Pew Internet and American Life Project. The sample transcribed dialogs in Appendix 1 illustrate various features of interaction with the system. Question types include Yes/No, categorical (where users pick from a specified set of response options), and numerical questions. Some categorical items are grouped into battery questions with the same re-sponse options for all the items. The system supports explicit requests to repeat the question or ask for help, and mimics a ?standardized interviewing? style of interaction that trained interviewers would use to repeat or clarify a question when the answer is rejected or requires confirmation. Thresholds set on acoustic and language confidence scores are used to de-
330
cide whether to reject, explicitly confirm, or ac-cept a response. The final question in the dialog in Appendix A (?Thinking about ??) illustrates the importance of confirmation in ensuring the correct survey response is recorded. In this case, the system misrecognized ?None? for ?Nine,? but this was caught by the explicit confirmation prompt. Two terms are introduced in the final example that we will return to in the evaluation. First hypothesis indicates the speech recognition and semantic result produced by the system the first time the question is asked. Last hypothesis indicates the speech recognition and semantic result that the system produced the last time the question was asked within the segment.  3 System Architecture The survey dialog system is directly integrated with a custom-built survey data case collection management system (PAMSS). When a survey case is administered, the case management sys-tem makes an HTTP request to a voice gateway, which initiates a call to the respondent. When the respondent answers, it bridges the call to a spo-ken dialog system running within the AT&T WatsonSM speech platform. The system uses pre-recorded prompts for survey questions and re-prompts. Confirmations for numeric responses combine prompts with TTS output. 
 Figure 1: Survey Dialog System Architecture Users? spoken inputs are recognized using state-specific grammars for each question. Data were not initially available for training statistical mod-els, so SRGS (Hunt and McGlashan 2004) grammars were built for each answer. These were tuned in an initial pilot phase. The gram-mars included standard responses for the ques-tion, along with common paraphrases and fram-ing words from the question. In the Watson plat-form, a dialog manager  (built in Python) is inte-grated with ASR and TTS engines. Questions to be administered are represented in a declarative format in a survey item specification along with 
references to the appropriate prompts and gram-mars. The dialog manager interprets this specifi-cation to administer the survey and control the interaction flow. As the user responds to ques-tions, the answers are posted back to the survey case management system.  4 Experimental Evaluation We evaluated the survey dialog system as part of the first phase of a larger experiment comparing different survey interaction modes (Schober et al 2012). In this phase, 642 subjects were recruited from Craigslist, Facebook, Google Ads, and Amazon Mechanical Turk. A web-based screener application verified respondents to be over 21 and collected their zip code. Of these, 158 re-spondents were randomly assigned to the auto-mated voice condition. A $20 iTunes gift card was given as an incentive after completion of a post-interview web questionnaire. This included multiple-choice questions examining user satis-faction with their experience. In total there were 8,228 spoken inputs over the 158 respondent dia-logs. These responses were transcribed, coded, and annotated for semantic content. The questions we sought to answer were: What is the performance of a spoken dialog sys-tem on a typical survey task? What impact does speech recognition and concept error have on overall survey estimates? Does an automated survey system benefit from implementing a tradi-tional confirmation strategy, where responses with low confidence scores are verified with con-firmation dialog? We also examine the impact of dialog length and confirmation prompts on a qualitative measure of user satisfaction. 4.1 ASR and concept accuracy We evaluated overall word, sentence, and con-cept accuracy for all 8,228 spoken utterances to the system, shown in the first row of Table 1. Accuracy: Word  Sentence  Concept  All  80% 78.2% 90.3% First  81.2% 78% 88.9% Last 88.5% 85.4% 95.6% Table 1: System Performance An input is ?concept accurate? if the semantic value assigned by the system exactly matches that assigned by the annotator. First shows the performance on the first response made by a user to each question before any confirmation dialog. Last shows performance on the last time each question was asked. Concept accuracy on last responses is 95.6%, showing that the confirma-
AT&T$WATSONSM$SPEECH$PLATFORM$SURVEY$CASE$$MANAGEMENT$$SYSTEM$$(ISR,$PARSONS)$
ASR$NLU$
DIALOG$MANAGER$
GRAMMARS$ PROMPTS$
AUDIO/TTS$
SURVEY$$ITEMS$SPEC$(XML)$
CASE$DB$
SURVEY$RESPONSE$DB$
VOICE$GATEWAY$(ASTERISK)$
h9p$$request:$iniDate$voice$survey$ Request$handler$
calls$$respondent$
Telephony$$Gateway$
bridges$to$$Watson$
?? Manages$cases$?? Collects$survey$results$
survey$results$returned$to$case$management$as$quesDons$completed$
LOGS$
331
tion strategy resulted in a 60% relative reduction in error compared to the first response. 4.2 Impact of Errors on Survey Estimates Recognition error is undoubtedly a key factor in overall user experience. But unlike dialog sys-tems for information access, search, and transac-tions, the most important factor in a survey dia-log system is the impact of errors on the quality of the estimates derived from the survey. To ex-amine the impact of the residual 4.4% concept error on overall survey error, we compared an-swer distributions derived from the system hy-pothesis for the last response versus the annota-tion of the last response using paired t-tests. For the 18 categorical questions, we conducted t-tests comparing the counts for each response option of each question. For all 18 questions (a total of 77 response categories) none of the dif-ferences were statistically significant (p<0.05). For the 14 numerical questions, for only one (?Number of times shopping in a grocery store in the last month?) did the interpretations differ significantly (Annotated: 7.8 times, Hypothesis: 7.6 times, p=0.04).1 This is strong evidence that speech recognition errors in this system did not have a major effect on survey estimates. How much survey error would have occurred without the dialog strategy?  To test this, we compared the annotated last response to the sys-tem hypothesis for the first response, simulating an interaction without confirmation dialog, and thus lower recognition accuracy?see Table 1 (This is not a perfect simulation, as we have no independent evidence on whether the first or fi-nal response is true). There would indeed have been more survey error without dialog, although the overall level was still surprisingly low. For the 18 categorical questions, 14 of the 77 re-sponse categories show significant differences (p<0.05). For the 14 numerical questions, two showed significant differences.  4.3 User Satisfaction One of the post-interview questionnaire items provided a qualitative measure of user satisfac-tion: ?Overall, how satisfied were you with the interview?? The results were: Very satisfied (47.3%), Somewhat satisfied (41.8%), Somewhat dissatisfied (7.1%), and Very dissatisfied (0.6%).  We examined the impact of various dialog fea-tures that seemed on intuitive grounds plausibly                                                 1 If we treat the two interpretations as independent samples, the response distributions did not differ significantly at all. 
connected with satisfaction: average number of turns per question, average number of clarifica-tion prompts per session, and average number of no input response prompts. We conducted a se-ries of logistic regressions with one variable con-trolled at a time to see the extent to which each of these features affected satisfaction. A Chi-squared test was used to measure significance. All three features were significant predictors when comparing Somewhat/Very Dissatisfied to Very/Somewhat satisfied (Table 2). Feature Odds ratio  SE p # turns per Q 10.411 0.787 0.003 # clarifications  1.043 0.033 0.024 # no input  2.001 0.176 <0.001 Table 2: User satisfaction regression 5 Conclusion Our results demonstrate the viability of conduct-ing survey interviews of the sort from which im-portant national statistics are derived with spoken dialog systems.  In our system, the speech recog-nition errors (with an overall concept recognition rate of 95.6%) did not substantially affect the error of the survey estimates; for only one of 32 questions was there a significant difference in the survey estimate determined by the automated spoken dialog system compared to the annotated result. Of course, we don?t know whether these results generalize to dialog systems with other features, different questions, or different re-spondents; much remains to be learned.  Nonetheless, our results provide some guid-ance for improving respondent satisfaction and minimizing survey error in future development of survey dialog systems.  For example, for nu-merical questions, which generally involve larger numbers of response options, recognition errors may be reduced by adopting the strategy of ask-ing the respondent to select among categories representing ranges (e.g. ?none?, ?1 to 5 times?, ?6 to 10 times?).  Recognition performance could be improved by tuning confirmation strategies, e.g. applying a tighter confidence threshold for numerical vs. categorical questions. In a broad scale application of a repeated spoken dialog survey, greater amounts of data could be availa-ble for training statistical models for the respons-es, for improved recognition accuracy and fur-ther reduced concept error. Finally, it is worth exploring the trade-offs for survey error and re-spondent satisfaction between adding potentially frustrating confirmation dialog and accepting lower-confidence recognition for subsequent human annotation and processing.  
332
Acknowledgments: NSF #SES-1025645 and SES-1026225 to Conrad and Schober. References  Jonathan Bloom. 2008. The Speech IVR as a Survey Interviewing Methodology. In Conrad and Schober (eds.), Envisioning the Survey Interview of the Fu-ture. Wiley, New York. Dan Bohus and Alex Rudnicky. 2005. Sorry, I didn?t Catch That: An Investigation of Non-Understanding Errors and Recovery Strategies. Proceedings of SIGdial-2005, Lisbon, Portugal. Frederick G. Conrad, Michael F. Schober, Chan Zhang, Huiying Yan, Lucas Vickers, Michael Johnston, Andrew L. Hupp, Lloyd Hemingway, Stefanie Fail, Patrick Ehlen, and Chris Antoun. 2013. Mode Choice on an iPhone Increases Survey Data Quality. 68th Annual Conference of the American Association for Public Opinion Research (AAPOR), Boston, MA. Mick P. Couper, 2009. The Role of Paradata in Meas-uring and Reducing Measurement Error in Surveys. NCRM Network for Methodological Innovation 2009: The Use of Paradata in UK Social Surveys. John Ehlen and Patrick Ehlen. 2007. Cellular-Only Substitution in the United States as Lifestyle Adop-tion. Public Opinion Quarterly: Special Issue Vol 71 (5), pp. 717-733.  Patrick Ehlen, Michael Schober, and Frederick G. Conrad. 2007. Modeling Speech Disfluency to Predict Conceptual Misalignment in Speech Sur-vey Interfaces. Discourse Processes 44:3, pp. 245?265. Floyd J. Fowler and Thomas W. Mangione 1990. Standardized Survey Interviewing; Minimizing In-terviewer Related Error. Sage Publications, CA. Marek Fuchs, 2008. Mobile Web Surveys: A Prelimi-nary Discussion of Methodological Implications. In Conrad and Schober (eds.), Envisioning the Survey Interview of the Future. Wiley, New York. Andrew Hunt and Scott McGlashan. 2004. Speech Recognition Grammar Specification Version 1.0. W3C Recommendation 16 March 2004. http://www.w3.org/TR/speech-grammar/. Diane Litman, Julia Hirschberg, and M. G. J. Swerts. 2006. Characterizing and Predicting Corrections in Spoken Dialogue Systems. Computational Linguis-tics 32:3, pp. 417-438. Michael F. Schober and Frederick G. Conrad. 1997. Does conversational interviewing reduce survey measurement error? Public Opinion Quar-terly, 61, pp. 576-602.  Michael F. Schober, Frederick G. Conrad, Chris An-toun, Carroll, Patrick Ehlen, Stefanie Fail, Andrew 
L. Hupp, Michael Johnston, Courtney Kellner, Kelly Nichols, Leif Percifield, Lucas Vickers, Hui-ying Yan, and Chan Zhang. 2012. Disclosure and Quality of Answers in Text and Voice Interviews on iPhones. 67th Annual Conference of the Ameri-can Association for Public Opinion Research (AAPOR) Orlando, FL. Jongho Shin, Shrikanth Narayanan, Laurie Gerber, Abe Kazemzadeh, and Dani Byrd. 2002. Analysis of User Behavior under Error Conditions in Spo-ken Dialogs. Proceedings of ICSLP-2002. Amanda Stent, Svetlana Stenchikova, and Matthew Marge. 2006. Dialog systems for surveys: The Rate-a-Course system. Proceedings of the IEEE/ACL 2006 Workshop on Spoken Language Technology. pp. 210-213. Appendix ? Sample Survey Dialogs I: Have you smoked at least 100 cigarettes in your entire life? Yes or no. R: No I: Thanks  I: Now I'd like to ask you some questions specific to your diet. For the next group of questions, I'll ask you how much you favor or oppose engaging in certain behaviors. After I say the behavior, please say one of the following: strongly favor, somewhat favor, neither favor nor oppose, somewhat oppose or strongly op-pose. The first behavior...avoiding fast food. R: Strongly oppose I: You said strongly oppose, right? R: Yes I: Got it I: The next behavior: Maintaining a healthy diet R: Repeat I: The next behavior: Maintaining a healthy diet R: Strongly agree I: Sorry, I didn't hear you. When thinking about 'maintaining a healthy diet', please say one of the fol-lowing: strongly favor, somewhat favor, neither favor nor oppose, somewhat oppose or strongly oppose.  R: Somewhat favor I: You said somewhat favor, right?  R: Yes  I: Thanks ? I: Thinking about the time since your eighteenth birthday (including the recent past that you've already told us about), how many male partners have you had sex with?  R: None  (First hypothesis: ?Nine?) I: I think you said '9'. Is that right? Yes or No.  R: No  I: Thinking about the time since your eighteenth birthday (including the recent past that you've already told us about), how many male partners have you had sex with?  R: Zero (Last hypothesis: ?Zero?) I: Thanks 
333
Proceedings of the SIGDIAL 2014 Conference, pages 257?259,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
MVA: The Multimodal Virtual Assistant
Michael Johnston
1
, John Chen
1
, Patrick Ehlen
2
, Hyuckchul Jung
1
, Jay Lieske
2
, Aarthi Reddy
1
,
Ethan Selfridge
1
, Svetlana Stoyanchev
1
, Brant Vasilieff
2
, Jay Wilpon
1
AT&T Labs Research
1
, AT&T
2
{johnston,jchen,ehlen,hjung,jlieske,aarthi,
ethan,sveta,vasilieff,jgw}@research.att.com
Abstract
The Multimodal Virtual Assistant (MVA)
is an application that enables users to plan
an outing through an interactive multi-
modal dialog with a mobile device. MVA
demonstrates how a cloud-based multi-
modal language processing infrastructure
can support mobile multimodal interac-
tion. This demonstration will highlight in-
cremental recognition, multimodal speech
and gesture input, contextually-aware lan-
guage understanding, and the targeted
clarification of potentially incorrect seg-
ments within user input.
1 Introduction
With the recent launch of virtual assistant appli-
cations such as Siri, Google Now, S-Voice, and
Vlingo, spoken access to information and services
on mobile devices has become commonplace. The
Multimodal Virtual Assistant (MVA) project ex-
plores the application of multimodal dialog tech-
nology in the virtual assistant landscape. MVA de-
parts from the existing paradigm for dialog-based
mobile virtual assistants that display the unfold-
ing dialog as a chat display. Instead, the MVA
prototype situates the interaction directly within a
touch-based interface that combines a map with
visual information displays. Users can interact
using combinations of speech and gesture inputs,
and the interpretation of user commands depends
on both map and GUI display manipulation and
the physical location of the device.
MVA is a mobile application that allows users
to plan a day or evening out with friends using
natural language and gesture input. Users can
search and browse over multiple interconnected
domains, including music events, movie show-
ings, and places to eat. They can specify multi-
ple parameters in natural language, such as ?Jazz
concerts around San Francisco next Saturday?. As
users find interesting events and places, they can
be collected together into plans and shared with
others. The central components of the graph-
ical user interface are a dynamic map showing
business and event locations, and an information
display showing the current recognition, system
prompts, search result listing, or plans (Figure 1).
Figure 1: MVA User Interface
Spoken input begins when the user taps a micro-
phone button on the display. As the user speaks,
incremental speech recognition results appear. In
addition to enabling voice input, the microphone
button also activates the map as a drawing can-
vas, and enables the user to combine speech with
drawing in coordinated multimodal commands.
For example, a user might say, ?Movies playing
tonight in this area? while simultaneously outlin-
ing a relevant area on the map. Or a user may say,
?Restaurants? while drawing a line down a spe-
cific street. MVA determines the intent and dis-
ambiguates concepts in the input in order to re-
turn relevant results. MVA then responds to user
input multimodally, by updating the display and
using speech synthesis to summarize results, pro-
vide feedback, or make requests for clarification
and additional information.
257
2 Sample Interaction
In Figure 2 we present a sample of interaction
from MVA that illustrates some of its capabilities.
The user starts with a spoken natural language
query where they specify some constraints: the
type of music (jazz), location (San Francisco), and
time (tomorrow). The system gets low confidence
on the location, so it constructs a targeted clarifi-
cation for clarifying only that constraint. The user
repeats the location, and then the system searches
for events meeting the user?s constraints. The user
then reviews the results, and follows on with a
refinement: ?What about blues??. Even though
many parameters in this query are underspecified,
the system applies contextually-aware natural lan-
guage understanding and interprets this as ?Blues
concerts near San Francisco tomorrow?. After
selecting a concert, the user then searches for a
restaurant nearby. The location of the concert re-
mains salient. The user follows up with a mul-
timodal query combining speech and gesture to
search for similar restaurants in an adjoining area.
U: ?Jazz concerts near San Francisco tomorrow.?
S: ?Where did you want to see jazz tomorrow??
U: ?San Francisco.?
S: ?I found 20 jazz concerts in San
Francisco tomorrow.?
[Zooms map to San Francisco and displays
pins on map and list of results]
U: ?What about blues??
S: ?I found 20 blues concerts in
San Francisco tomorrow.?
U: [Clicks on a concert listing and adds it
to the plan]
U: ?Sushi restaurants near there.?
S: ?I found 10 sushi restaurants.?
U: ?What about here??
[Circles adjoining area on map]
S: ?I found 5 sushi restaurants in
the area you indicated?
Figure 2: Sample Interaction
3 System Architecture
Figure 3 shows the underlying multimodal assis-
tant architecture supporting the MVA app. The
user interacts with a native iOS client. When the
user taps the microphone icon, this initiates the
flow of audio interleaved with gesture and context
information streamed over a WebSocket connec-
tion to the platform.
This stream of interleaved data is handled at
the server by a multimodal natural language pro-
cessing pipeline. This fields incoming packets of
Figure 3: MVA Multimodal assistant Architecture
data from the client, demuxes the incoming data
stream, and sends audio, ink traces, and context
information to three modules that operate in par-
allel. The audio is processed using the AT&T
Watson
SM
speech recognition engine (Goffin et
al., 2005). Recognition is performed using a dy-
namic hierarchical language model (Gilbert et al.,
2011) that combines a statistical N-gram language
model with weighted sub-grammars. Ink traces
are classified into gestures using a linear classi-
fier. Speech recognition results serve as input to
two NLUmodules. A discriminative stochastic se-
quence tagger assigns tags to phrases within the
input, and then the overall string with tags is as-
signed by a statistical intent classifier to one of
a number of intents handled by the system e.g.
search(music event), refine(location).
The NLU results are passed along with gesture
recognition results and the GUI and device context
to a multimodal dialog manager. The contextual
resolution component determines if the input is a
query refinement or correction. In either case, it
retrieves the previous command from a user con-
text store and combines the new content with the
context through destructive unification (Ehlen and
Johnston, 2012). A location salience component
then applies to handle cases where a location is
not specified verbally. This component uses a su-
pervised classifier to select from among a series
of candidate locations, including the gesture (if
present), the current device location, or the current
map location (Ehlen and Johnston, 2010).
The resolved semantic interpretation of the ut-
terance is then passed to a Localized Error Detec-
tion (LED) module (Stoyanchev et al., 2012). The
LEDmodule contains two maximum entropy clas-
sifiers that independently predict whether a con-
258
cept is present in the input, and whether a con-
cept?s current interpretation is correct. These clas-
sifiers use word scores, segment length, confu-
sion networks and other recognition and context
features. The LED module uses these classifiers
to produce two probability distributions; one for
presence and one for correctness. These distri-
butions are then used by a Targeted Clarification
component (TC) to either accept the input as is,
reject all of the input, or ask a targeted clarifica-
tion question (Stoyanchev et al., 2013). These de-
cisions are currently made using thresholds tuned
manually based on an initial corpus of user inter-
action withMVA. In the targeted clarification case,
the input is passed to the natural language gen-
eration component for surface realization, and a
prompt is passed back to the client for playback
to the user. Critically, the TC component decides
what to attempt to add to the common ground
by explicit or implicit confirmation, and what to
explicitly query from the user; e.g. ?Where did
you want to see jazz concerts??. The TC com-
ponent also updates the context so that incoming
responses from the user can be interpreted with re-
spect to the context set up by the clarification.
Once a command is accepted by the multimodal
dialog manager, it is passed to the Semantic Ab-
straction Layer (SAL) for execution. The SAL in-
sulates natural language dialog capabilities from
the specifics of any underlying external APIs that
the system may use in order to respond to queries.
A general purpose time normalization component
projects relative time expressions like ?tomorrow
night? or ?next week? onto a reference timeframe
provided by the client context and estimates the
intended time interval. A general purpose location
resolution component maps from natural language
expressions of locations such as city names and
neighborhoods to specific geographic coordinates.
These functions are handled by SAL?rather than
relying on any time and location handling in the
underlying information APIs?to provide consis-
tency across application domains.
SAL also includes mechanisms for category
mapping; the NLU component tags a portion
of the utterance as a concept (e.g., a mu-
sic genre or a cuisine) and SAL leverages
this information to map a word sequence to
generic domain-independent ontological represen-
tations/categories that are reusable across different
backend APIs. Wrappers in SAL map from these
categories, time, and location values to the spe-
cific query language syntax and values for each
specific underlying API. In some cases, a single
natural language query to MVA may require mul-
tiple API calls to complete, and this is captured
in the wrapper. SAL also handles API format dif-
ferences by mapping all API responses into a uni-
fied format. This unified format is then passed to
our natural language generation component to be
augmented with prompts, display text, and instruc-
tions to the client for updating the GUI. This com-
bined specification of a multimodal presentation is
passed to the interaction manager and routed back
to the client to be presented to the user.
In addition to testing the capabilities of our mul-
timodal assistant platform, MVA is designed as a
testbed for running experiments with real users.
Among other topics, we are planning experiments
with MVA to evaluate methods of multimodal in-
formation presentation and natural language gen-
eration, error detection and error recovery.
Acknowledgements
Thanks to Mike Kai and to Deepak Talesra for
their work on the MVA project.
References
Patrick Ehlen and Michael Johnston. 2010. Location
grounding in multimodal local search. In Proceed-
ings of ICMI-MLMI, pages 32?39.
Patrick Ehlen and Michael Johnston. 2012. Multi-
modal dialogue in mobile local search. In Proceed-
ings of ICMI, pages 303?304.
Mazin Gilbert, Iker Arizmendi, Enrico Bocchieri, Dia-
mantino Caseiro, Vincent Goffin, Andrej Ljolje,
Mike Phillips, Chao Wang, and Jay G. Wilpon.
2011. Your mobile virtual assistant just got smarter!
In Proceedings of INTERSPEECH, pages 1101?
1104. ISCA.
Vincent Goffin, Cyril Allauzen, Enrico Bocchieri,
Dilek Hakkani-Tur, Andrej Ljolje, S. Parthasarathy,
Mazim Rahim, Giuseppe Riccardi, and Murat Sar-
aclar. 2005. The AT&T WATSON speech recog-
nizer. In Proceedings of ICASSP, pages 1033?1036,
Philadelphia, PA, USA.
Svetlana Stoyanchev, Philipp Salletmayer, Jingbo
Yang, and Julia Hirschberg. 2012. Localized de-
tection of speech recognition errors. In Proceedings
of SLT, pages 25?30.
Svetlana Stoyanchev, Alex Liu, and Julia Hirschberg.
2013. Modelling human clarification strategies. In
Proceedings of SIGDIAL 2013, pages 137?141.
259
