Automated Generalization of Phrasal Paraphrases from the Web*
Weigang Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lee@ir.hit.edu
.cn
Ting Liu 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
tliu@ir.hit.ed
u.cn
Yu Zhang
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
zhangyu@ir.hit
.edu.cn
Sheng Li 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
lis@ir.hit.edu
.cn
Wei He 
School of Computer
Science and Tech-
nology, Box 321,
Harbin Institute of
Technology, Harbin,
 P.R. China, 150001
truman@ir.hit.
edu.cn
Abstract
Rather than creating and storing thou-
sands of paraphrase examples, para-
phrase templates have strong 
representation capacity and can be used
to generate many paraphrase examples.
This paper describes a new template
representation and generalization
method. Combing a semantic diction-
ary, it uses multiple semantic codes to
represent a paraphrase template. Using
an existing search engine to extend the
word clusters and generalize the exam-
ples.  We also design three metrics to
measure our generalized templates. The 
experimental results show that the rep-
resentation method is reasonable and 
the generalized templates have a higher 
precision and coverage.
1 Introduction
Paraphrases are alternative ways to convey the 
same information (Barzilay and McKeown,
2001) and they have been applied in many fields
of natural language processing. There are many
previous work on paraphrase examples extrac-
tion or combining them with some applications
such as information retrieval and question an-
swering (Agichtein et al, 2001; Florence et al, 
2003; Rinaldi et al, 2003; Tomuro, 2003; Lin
and Pantel, 2001;), information extraction 
(Shinyama et al, 2002; Shinyama and Sekine, 
2003), machine translation (Hiroshi et al, 2003;
Zhang and Yamamoto, 2003), multi-document
(Barzilay et al, 2003).
There is also some other research about 
paraphrase. (Wu and Zhou, 2003) just extract 
the synonymy collocation, such as <turn on, 
OBJ, light> and <switch on, OBJ, light> using
both monolingual corpora and bilingual corpora 
to get an optimal result, but do not generalize
them. (Glickman and Dagan, 2003) detects verb
paraphrases instances within a single corpus
without relying on any priori structure and in-
formation. Generation of paraphrase examples
was also investigated (Barzilay and Lee, 2003;
Quirk et al, 2004).
Rather than creating and storing thousands of 
paraphrases, paraphrase templates have strong 
representation capacity and can be used to gen-
erate many paraphrase examples. As (Hirst, 
2003) said, for each aspect of paraphrase there 
are two main challenges: representation of 
knowledge and acquisition of knowledge. Cor-
responding to the problem of generalization of 
paraphrase templates, there are also two prob-
lems: the first is the representation of paraphrase
templates and the second is acquisition of para-
phrase templates.
There are several methods about paraphrase
templates representation. The first method is 
using the Part-of-Speech (Barzilay and McKe-
own, 2001; Daum? and Marcu, 2003; Zhang and 
Yamamoto, 2003), the second uses name entity 
as the variable (Shinyama et al, 2002; Shinyama
and Sekine, 2003), the third method is similar to 
the second method which is called the inference 
rules extraction (Lin and Pantel, 2001).
A paraphrases template is a pair of natural
language phrases with variables standing in for
certain grammatical constructs in (Daum? and 
*: Supported by the Key Project of National Natural Sci-
ence Foundation of China under Grant No. 60435020
49
Marcu, 2003). He used Part-of-Speech to repre-
sent templates. But for some cases, the POS will 
be very limited and for some other cases will be 
over generalized. For example:
?????????
(In my view/mind ----I feel)
The above pair of phrases is a paraphrase, it 
can be generalized using POS information: 
? [pronoun]??
(In [pronoun] view/mind)
[pronoun]??
( [pronoun] feel)
But for this template many noun words will
be excluded. From this point of view, the tem-
plate representation capacity is limited. But for 
other examples, the POS information will be 
over generally. For example:
?????????
(What's the price for the apples?)
????????
(How much is the apples per Jin?)
Here, we just generalize one variable ????.
Then, the template becomes:
[noun]???????
(What's the price for the [noun]?)
[noun]??????
(How much is the [noun] per Jin?)
If there is a sentence ??????????
(What's the price for the notebook?)?, its? para-
phrase will be ?????????(How much 
is the notebook per Jin?)? according to this tem-
plate. Obviously, the result is unreasonable.
(Shinyama et al, 2002) tried to find para-
phrases assuming that two sentences sharing
many Named Entities and a similar structure are 
likely to be paraphrases of each other. But just 
name entities are limited, too. And (Lin and 
Pantel, 2001) present an unsupervised algorithm 
for discovering inference rules from text such as 
?X writes Y? and ?X is the author of Y?. This 
generalized method has good ability. But it also
has some limited aspect. For example:
[Jack] writes [his homework].
According to the paraphrase template, the
target sentence will be transformed into ?[Jack]
is the author of [his homework]?. It?s obviously
that the generated sentence is not standard.
So how to represent paraphrase templates
and generalize the paraphrase examples is a very 
interesting task. In this paper, we present a novel
approach to represent paraphrase template with 
semantic code of words and using an existing
search engine to get the paraphrase template.
The remainder of this paper is organized as 
follows. In the next section, we give the over-
view of our method. In section 3, we define the 
representation method in details. Section 4 pre-
sents the generalization method. Some experi-
ments and discussions are shown in Section 5. 
Finally, we draw a conclusion of this method
and give some suggestions about future work. 
2 Overview of Generalization Method
The origin input of our system is a seed phrasal
paraphrase example. And the output is the gen-
eralized paraphrase templates from the given 
examples. The overall architecture of our para-
phrase generalization is represented on figure 1. 
A seed phrasal
paraphrase examples
Getting the slot word
Extend the slot word
using Search Engine
on every example
Mapping two word
sets to their semantic
code sets
Intersection operation
on the two semantic
code sets
Generalizing a
template
Figure 1: Sketch Map of Paraphrase example
Generalization
We also use the example (1) to illustrate the 
representation. Here a semantic dictionary called 
?TongYiCiCiLin? (Extension Version)1 is used. 
The pair of phrases is a phrasal paraphrase. At
first, after preprocessing which includes word
segment, POS tagging and word sense disam-
biguation, we get the slot word in the paraphrase.
In this example, the slot word is ??(I)?. Then
we search the web using the context of the slot 
word. Every phrase in the phrasal pair derives a
set of sentences which include the original 
phrase context. A dependency parser on these 
sentences is used to extract the corresponding
word with the slot word. Two word sets can be 
obtained through the two sentence sets. Then,
we map word sets to their semantic code sets
1 TongYiCiCiLin (Extended Version) can be downloaded
from the website of HIT-IRLab (Http://ir.hit.edu.cn). In the 
past section, we abbreviate the TongYiCiCiLin (Extended
Version) to Cilin (EV) 
50
according to Cilin(EV). Then an intersection 
operation is conducted on the two sets. We use 
the intersection set to replace the slot word and 
generate the final paraphrase template. 
In order to verify the validation of the gener-
alized paraphrase template, we also design an 
automatic algorithm to confirm whether the 
template is reasonable using the existing search 
engine.
3 Representation of Template 
In the section of introduction, some representa-
tion methods of paraphrase template have been 
introduced. And we proposed a new method us-
ing word semantic codes to represent the vari-
able in a template. Before we introduce the 
representation method, Firstly, we give some 
general introduction about the semantic diction-
ary of Cilin(EV). 
3.1 TongYiCiCiLin (Extended Version) 
Cilin (EV) is derived from original TongY-
iCiCilin in which word senses are decomposed 
to 12 large categories, 94 middle categories, 
1,428 small categories. Cilin (EV) removes 
some outdated words and updates many new 
words. More fine-grained categories are added 
on the base of original classification system to 
satisfy the more complex natural language ap-
plications. The encoding criterion is shown in 
the table 1:
Table 1 Encoding table of dictionary
Encoding
bit 1 2 3 4 5 6 7 8
Example D a 1 5 B 0 2 =
Attribute Big Middle Small groups Atom groups
Layer 1 2 3 4 5
The encoding bits are arranged from left to 
right. The first three layers are same with Cilin. 
The fourth layer is represented by capital letters 
and the fifth layer is two-bit decimal digit. The 
last bit is some more detailed information about 
the atom groups. 
3.2 An Example of a Paraphrase Template 
For simplicity, we just select one slot word in 
every paraphrase. And we stipulated that only 
content word can be slot word. We also use the 
above paraphrase example (1). 
?????????
(In my view/mind ----I feel)
Here, we get the slot word ??(I)?. Through 
the Word Sense Disambiguation processing, we 
get its semantic code ?Aa02A01=? according to 
the fifth layer in Cilin(EV). If we just use the 
semantic code of the slot word, we can get a 
simple paraphrase template as follows:  
? [Aa02A01=] ??
(In [Aa02A01=]  view/mind)
[Aa02A01=] ??
([Aa02A01=]  feel)
But it is obviously that the template is very 
limited. Its? representation ability is also limited. 
So how to extend the ability of a paraphrase 
template is a challenging work.  
3.3 Extending the Template Abstract Ability 
According to the feature of Cilin(EV) architec-
ture, we can use the higher layer?s semantic 
code instead of the slot word to generalize the 
paraphrase template naturally. Of course it?s a 
very simple method to extend the template abil-
ity, but it also brings more redundancy of a 
paraphrase template and it will be proven in the 
later section. 
So we use multiple semantic codes of the dif-
ferent layer instead of only one semantic code of 
slot word in Cilin (EV). The later experimental 
results prove this representation has a good per-
formance with a good precision and coverage. 
4 Generalizing to Templates 
As mentioned above, we can use multiple se-
mantic codes to generalize paraphrase examples. 
So the problem of how to generalize paraphrase 
examples is transformed into the problem of 
how to get the multiple semantic codes set. We 
proposed a new method which uses the existing 
search engine to reach the target.  
4.1 Getting the Candidate Sentences 
After we removed the slot word in the para-
phrase examples, two phrasal contexts of the 
original paraphrase phrases were obtained. Each 
phrase without slot word is used as a search 
query for an existing search engine and achiev-
ing many sentences which include the query 
word. For this example, the two queries are ??
??(in?view)? and ???(feel)?. Each query 
gets one sentence set respectively. Part of the 
two result sentence sets are shown in figure 2 
and figure 3: 
51
Figure 2. Sentence Set 1 
Figure 3. Sentence Set 2 
From the above two sentence sets, we can
find that there is some noisy information in the 
sentences. In order to extend the correspondent
words of the slot word, it is not enough that we 
just use the position information or POS tagging
information of the slot word. Even if we extract
these words, many of them can?t be found in the
dictionary because they are not simple words.
Benefiting from the idea of (Lin and Pantel, 
2001), we use a dependency parser to determine
the correspondent extended words. 
4.2 Dependency Parser 
In this paper, we use a dependency parser (Ma et 
al., 2004) to extract the candidate slot word. For 
example, the dependency parsing result of the 
phrase of ?????? is shown in figure 4. 
Figure 4. Dependency parsing result 
The arcs in the figure represent dependency
relationships. The direction of an arc is from the 
head to the modifier in the relationship. Labels 
associated with the arcs represent types of de-
pendency relations. Table 2 lists a subset of the
dependency relations in the HIT-IRLab depend-
ency parser2.
Table 2. A subset of the dependency relations 
Relation Description
ATT ????(attribute)
HED ??(head)
SBJ ??(subject)
ADV ????(adverbial)
VOB ????(verb-object)
???????????????????
??????????????????
??????????????
,7 ????????????"
??????????????????
2 More information about the dependency parser can be got
from http://ir.hit.edu.cn/cuphelp.htm
4.3 Extracting the extended words 
We just use a very simple method to get the ex-
tended words from the parsed sentences. At first, 
we record the relations of the original parsed 
phrasal examples. And then we use these rela-
tions to matched similar part in the candidate 
parsed sentence except slot word. And we omit
these unseen relations and content words which
don?t appear in the original parsed phrasal ex-
amples. Then we can get the extended words. 
????????????
?????????
???????????????
??????????
???????????B720 ????
Figure 5. Dependency parsing result 
Figure 5 shows the dependency parsing result 
of the phrase of ???????????(In for-
eign capital fund manager view). We can easily
find that the extended word of the slot word
?? ?(I) is ??? ?(manager). Two extended
word sets can be extracted from two sentence
sets. Then we map each word to their semantic
code to get two semantic code sets. Intersection
operation is conducted on these two semantic
code sets to obtain their intersection set. Finally, 
we use the semantic code set instead of the slot 
word to generate the paraphrase template.
4.4 Some tricks 
Because the precision of the current dependency
parser on Chinese is not very high, we just ex-
tract a part of the candidate sentences to parse. 
There are three patterns to segment the long 
candidate sentences according to position of slot 
word in paraphrase examples. They are called
FRONT, MIDDLE and BACK. Here we use an
example to illustrate it as shown in table 3: 
Table 3 Examples of sentence segmentation
Pattern Origin Phrase Segment examples
FRONT (SW)?? ????????
?????
MIDDLE ?(SW)?? ????????
????????
????
The bold section in the sentence will be ex-
tracted to parse. Pattern type can be decided by 
52
the position relation between slot word and con-
text words. And these patterns can reduce the 
relative error rate of the dependency parser. That 
is to say, if the original phrase is parsed wrongly, 
the extracted segments may be parsed wrongly 
with the similar error. But according to our 
method, this kind of parser error has little influ-
ence on the final extracting result. 
5 Experiments and Discussions 
5.1 Setting 
We extract about 510 valid paraphrase examples 
from a Chinese paraphrase corpus (Li et al, 
2004). For simplicity, we just select those 
phrasal paraphrase examples which own same 
word. And we stipulate only content word can 
be as slot word. We just use four seed phrasal 
paraphrases as the original paraphrases in this 
paper. And the generalized paraphrase templates 
represented by semantic codes of the fifth layer 
in Cinlin (EV) are also shown in the Table 4: 
Table 4: Examples of the generalized template 
Origin 
Phrases 
Generalized Paraphrase  
templates 
??? [Aa01A01=,Aa01A05=,   
Aa01C03=,Aa02A01=,  ?]??
1 ? ? ?
?
?[Aa01A01=,Aa01A05=, 
  Aa01C03=,Aa02A01=,...  ]??
??? ? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]?2 ??? [Ac03A01=,Ah04A01=, 
Ah05A01=,Am03D01@,?]??
? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,? ]??
3 ? ? ?
?
?[Fb01A01=,Gb07B01=, 
Hb06A01=,He15B01=,?  ]??
? ? ?
? ? ?
??
[Aa03A01=,Ac03A01=, 
Ba05A10#,Bb02A01=,?]???
???4
? ? ?
???
[Aa03A01=,Ac03A01=,Ba05A10#,
Bb02A01=,?]?????
5.2 Evaluation on Templates 
The goal of the evaluations is to confirm how 
reasonable this kind of representation method of 
paraphrase templates is and how well the tem-
plate is. We evaluated the generalized para-
phrase template in three ways. They are listed in 
the following three categories: 1) Reasonability; 
2) Precision; 3) Coverage. 
1) Reasonability 
The reasonability of a paraphrase template aims 
to measure the reasonable extent of the presenta-
tion method with multiple semantic codes. For 
example, if we use POS to generalize a para-
phrase template, its reasonability is very lower; 
that is to say, POS is not suitable to represent 
paraphrase template in some extent.  
We use an existing search engine to calcu-
late the reasonability of every paraphrase tem-
plate. Firstly, we instantiate all paraphrase 
examples from a template. Then all these exam-
ples are as the queries of the search engine. If 
two phrases in one paraphrase can be matched 
completely from the search engine, it also means 
that one or more examples are found on the Web 
via search engine, we then consider this para-
phrase is reasonable. Using this method we can 
get the approximate evaluation of all the exam-
ples. We define two metrics: 
Strict_Reasonability = S / N 
Loose_Reasonability = (L + S) / N 
Where N is the total number of the instanti-
ated examples; S is the number of the para-
phrase examples which two phrases in it can be 
matched all; L is the number of paraphrase ex-
amples only one phrase in a paraphrase can be 
matched.
2) Precision 
Every template is correspondent to the examples 
number with the semantic code of different layer 
in Cilin (EV) as shown in table 5.  
Table 5 Templates and their correspond exam-
ples number 
Instantiated examples 
number
Number of 
Paraphrase
templates Cilin3 Cilin4 Cilin5
1 2696 1815 478
2 13032 6354 3011
3 1057 587 177
4 3004 2229 429
From the above table, we can find that every 
template can instantiate many examples. If 
manually judging all of these examples will 
spend plenty of time. So we just sample part of 
all instantiate examples, 200 paraphrase exam-
ples for each template in this paper. For each 
53
phrase in a sample paraphrase example, it is as 
search query to get the first two matched sen-
tences. Evaluators would be asked whether it is
semantically okay to replace the query in the
sentence by the correspondent phrase in a para-
phrase. They were given only two options: Yes
or No. If search query have no matched results, 
we consider that this phrase cannot be replace 
with its correspondent paraphrase. According to 
the above regulations, we know that every para-
phrase examples correspondent to 4 sentences. If 
we sample n examples from a template, the pre-
cision of a paraphrase template can be calculated
by:
Precision = R / (4 * n) 
Where, R is the number of sentences which
is considered to be correct by the evaluator.
3) Coverage 
Evaluating directly the coverage of a paraphrase 
template is difficult because humans can?t enu-
merate all the words to be suitable to the tem-
plate. We use an approximate method to get the
coverage of a template. At first we use another 
search engine to get candidate sentences with 
similar method for generalization of a para-
phrase template. From these retrieved sentences
we can get many different words with the 
known generalized words because more than
85% of search results from different search en-
gine are different. Evaluators extract every sen-
tence which can be replaced with the 
correspondent phrase in a paraphrase and the
new sentences retain the origin meaning. We 
know each sentence is correspondent to a word. 
Then we define two metrics: 
Surface_Coverage = M / NS
Semantic_Coverage =
Map(K) / (Map(NS-M) + Map(K)) 
Where, NS is the number of all manually
tagged right words, M is the number of words 
which can be instantiated from a paraphrase
template, K is the number of all the words that 
generalized the template at the front. Map(X) is 
the total word number of the word clusters 
which derived from X word in the semantic dic-
tionary of Cilin(EV).
5.3 Result 
In order to exhibit the merit of our method, we 
conduct four groups of experiment. They are
POS-Tag, Cilin3, Cilin4 and Ciln5, respectively.
Especially, we just randomly select 400 words 
to satisfy the POS information.
Table 6: Experiment Results 
Reasonability
(%)
Coverage
(%)
St_R Lo_R Su_C Se_C
Preci-
sion
(%)
POS 10.50 17.00 90.00 ---- 11.75
Cilin3 45.57 84.50 27.55 38.71 45.75
Cilin4 46.89 84.54 23.87 44.48 64.13
Cilin5 46.24 83.12 20.39 39.47 69.88
Every value in table 6 is a average value of 
four values correspondent to four templates.
From the table we can find that the reasonability
of the Cilin-based representation template
changes little, and that of POS-based representa-
tion is very lower. We find that the longer origi-
nal phrases are, the lower the coverage of the
generalized template is. Although the average 
coverage of generalized template is relatively
low, we can draw a conclusion that using multi-
ple semantic codes to generalize phrasal para-
phrase examples is reasonable.
The column of the coverage shows that the 
coverage rates of Cilin-based templates are all
not more than 50%. And the POS-based tem-
plate has a very high coverage rate. And we 
know that the extended information is not
enough only depending on one search engine. 
We will combine several different search en-
gines with together to solve this problem in the 
future work. 
1.0 1.5 2.0 2.5 3.0 3.5 4.0
0
10
20
30
40
50
60
70
80
90
100
 strict_Reasonability  loose_Reasonability
 surface_Coverage  semantic_Coverage
 Precision
Va
lu
es
 o
f P
er
ce
nt
Different Template Representation Method
Figure 6. Experimental Results
The numbers from one to four on the X-axis
are correspondent to POS, Cilin3, Cilin4 and 
Cilin5 in figure 6. We can see the features
clearly of different representation methods of 
template from the figure 6. We can find that
54
Cilin5-based template has the highest precision, 
but its coverage is lower. And Cilin3-based 
template has opposite feature. This is because 
that one semantic code of Cilin3 includes more 
words than that of Cilin5. At the same time, 
more words bring more redundant information. 
And Cilin4-based template has a good tradeoff 
between coverage and precision. So we con-
clude that the semantic code of fourth layer in 
Cilin (EV) is more suitable to represent para-
phrase template.  
Some additional information can be extracted 
from the generalized template. Such as, the col-
location information between the slot word and 
the context words can be extract. For example, 
in the fourth template, we can get the informa-
tion about which words can be collocated with 
??(Jin)?.
Although this kind of representation of para-
phrase template has a good performance, it is 
weak for those words or structures that don?t 
exist in dictionary. Also, this method is not suit-
able to the named entities representation. 
6 Conclusion
In this paper, a novel method for automated 
generalization of paraphrase examples is pro-
posed. This method is not dependent on the tra-
ditional limited texts instead it is based on the 
richness of the Web. It uses the multiple seman-
tic codes to generalize a paraphrase example 
combing a semantic dictionary (Cilin (EV)). The 
experimental results proved that this representa-
tion method is reasonable and the generalized 
templates have a good precision and coverage.  
But this is just the beginning of the para-
phrase examples generalization. And we sim-
plify the problem in some aspects, such as we 
limited the number of the slot word in a para-
phrase example, and we stipulate only the same 
word can be slot word. Also, we find that our 
templates are weak for those words or structures 
that don?t exist in dictionary. Some methods in 
information extraction about named entities 
generalization can be used for reference in the 
future. Moreover, how to combine the semantic 
code with other representation forms together is 
also an interesting work. 
References
[1] Chris Quirk, Chris Brockett, and William Dolan. 
Monolingual Machine Translation for Para-
phrase Generation. editors, Dekang Lin and 
Dekai Wu, In Proceedings of EMNLP 2004, 
Barcelona, pages 142-149  
[2] Dekang Lin and Patrick Pantel. 2001. Discovery 
of Inference Rules for Question Answering. 
Natural Language Engineering 7(4):343-360 
[3] Dekang Lin and Patrick Pantel. Discovery of 
inference rules for question answering. Natural 
Language Engineering, 1, 2001.  
[4] E. Agichtein, S. Lawrence, and L. Gravano. 
Learning search engine specific query transfor-
mations for question answering. In Proceedings 
of the 10th International World-Wide Web Con-
ference (WWW10), 2001 
[5] Fabio Rinaldi, James Dowdall, Kaarel Kalju-
rand, Michael Hess, Diego Molla. 2003. Ex-
ploiting Paraphrases in a Question Answering 
System. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[6] Florence Duclaye France. Learning paraphrases 
to improve a question-answering system. In 
EACL Natural Language Processing for Ques-
tion Answering, 2003 
[7] Graeme Hirst. Paraphrasing Paraphrased. In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003 
[8] Hal Daum? III and Daniel Marcu. Acquiring 
paraphrase templates from document/abstract 
pairs. In NL Seminar in ISI, 2003 
[9] Hua Wu, Ming Zhou. Optimizing Synonym 
Extraction Using Monolingual and Bilingual 
Resources. In Proceedings of the Second Inter-
national Workshop on Paraphrasing, 2003 
[10] Hua Wu, Ming Zhou. Synonymous Collocation 
Extraction Using Translation Information. In 
Proceedings of the 41st Annual Meeting of the 
Association for Computational Linguistics, 
2003 
[11] Jinshan Ma, Yu Zhang, Ting Liu, and Sheng Li. 
A Statistical Dependency Parser of Chinese un-
der Small Training Data. Workshop: Beyond 
shallow analyses - Formalisms and statistical 
modeling for deep analyses, IJCNLP-04, 4 2004. 
[12] Noriko Tomuro. 2003. Interrogative Reformula-
tion Patterns and Acquisition of Question Para-
phrases. The Second International Workshop on 
Paraphrasing: Paraphrase Acquisition and Ap-
plications 
[13] Oren Glickman and Ido Dagan. Identifying 
lexical paraphrases from a single corpus: A case 
study for verbs. In Proceedings of Recent Ad-
vantages in Natural Language Processing, Sep-
tember 2003 
55
[14] Regina Barzilay and Kathleen McKeown. Ex-
tracting paraphrases from a parallel corpus. In 
Proceedings of the ACL/EACL, Toulouse, 2001 
[15] Regina Barzilay and Lillian Lee. Learning to 
Paraphrase: An Unsupervised Approach Using 
Multiple-Sequence Alignment. In Proceedings 
of HLT-NAACL 2003, pages 16-23  
[16] Regina Barzilay, Noemie Elhadad, Kathleen R. 
McKeown. 2003. Inferring Strategies for Sen-
tence Ordering in Multidocument News Sum-
marization. The Second International Workshop 
on Paraphrasing: Paraphrase Acquisition and 
Applications 
[17] Weigang Li, Ting Liu, Sheng Li. Combining 
Sentence Length with Location Information to 
Align Monolingual Parallel Texts. AIRS, 2004, 
pages 71-77 
[18] Yusuke Shinyama and Satoshi Sekine. Para-
phrase acquisition for information extraction. 
editors, Kentaro Inui and Ulf Hermjakob, In 
Proceedings of the Second International Work-
shop on Paraphrasing, 2003, pages 65-71 
[19] Yusuke Shinyama, Satoshi Sekine, Kiyoshi 
Sudo, and Ralph Grishman. Automatic para-
phrase acquisition from news articles, In Pro-
ceedings of Human Language Technology 
Conference (HLT2002), San Diego, USA, Mar. 
15, 2002 
[20] Zhang Yujie, Kazuhide Yamamoto. Automatic 
Paraphrasing of Chinese Utterances. Journal of 
Chinese Information Processing. Vol. 117 No. 
16: 31-38(Chinese) 
56
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 809?816,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Dependency Based Chinese Sentence Realization 
 
 
Wei He1, Haifeng Wang2, Yuqing Guo2, Ting Liu1 
1Information Retrieval Lab, Harbin Institute of Technology, Harbin, China 
{whe,tliu}@ir.hit.edu.cn 
2Toshiba (China) Research and Development Center, Beijing, China 
{wanghaifeng,guoyuqing}@rdc.toshiba.com.cn 
 
  
Abstract 
This paper describes log-linear models for a 
general-purpose sentence realizer based on de-
pendency structures. Unlike traditional realiz-
ers using grammar rules, our method realizes 
sentences by linearizing dependency relations 
directly in two steps. First, the relative order 
between head and each dependent is deter-
mined by their dependency relation. Then the 
best linearizations compatible with the relative 
order are selected by log-linear models. The 
log-linear models incorporate three types of 
feature functions, including dependency rela-
tions, surface words and headwords. Our ap-
proach to sentence realization provides sim-
plicity, efficiency and competitive accuracy. 
Trained on 8,975 dependency structures of a 
Chinese Dependency Treebank, the realizer 
achieves a BLEU score of 0.8874. 
1 Introduction 
Sentence realization can be described as the 
process of converting the semantic and syntactic 
representation of a sentence or series of sen-
tences into meaningful, grammatically correct 
and fluent text of a particular language. 
Most previous general-purpose realization sys-
tems are developed via the application of a set of 
grammar rules based on particular linguistic 
theories, e.g. Lexical Functional Grammar (LFG), 
Head Driven Phrase Structure Grammar (HPSG), 
Combinatory Categorical Grammar (CCG), Tree 
Adjoining Grammar (TAG) etc. The grammar 
rules are either developed by hand, such as those 
used in LinGo (Carroll et al, 1999), OpenCCG 
(White, 2004) and XLE (Crouch et al, 2007), or 
extracted automatically from annotated corpora, 
like the HPSG (Nakanishi et al, 2005), LFG 
(Cahill and van Genabith, 2006; Hogan et al, 
2007) and CCG (White et al, 2007) resources 
derived from the Penn-II Treebank. 
Over the last decade, there has been a lot of in-
terest in a generate-and-select paradigm for sur-
face realization. The paradigm is characterized 
by a separation between realization and selection, 
in which rule-based methods are used to generate 
a space of possible paraphrases, and statistical 
methods are used to select the most likely reali-
zation from the space. Usually, two statistical 
models are used to rank the output candidates. 
One is n-gram model over different units, such as 
word-level bigram/trigram models (Bangalore 
and Rambow, 2000; Langkilde, 2000), or fac-
tored language models integrated with syntactic 
tags (White et al 2007). The other is log-linear 
model with different syntactic and semantic fea-
tures (Velldal and Oepen, 2005; Nakanishi et al, 
2005; Cahill et al, 2007). 
However, little work has been done on proba-
bilistic models learning direct mapping from in-
put to surface strings, without the effort to con-
struct a grammar. Guo et al (2008) develop a 
general-purpose realizer couched in the frame-
work of Lexical Functional Grammar based on 
simple n-gram models. Wan et al (2009) present 
a dependency-spanning tree algorithm for word 
ordering, which first builds dependency trees to 
decide linear precedence between heads and 
modifiers then uses an n-gram language model to 
order siblings. Compared with n-gram model, 
log-linear model is more powerful in that it is 
easy to integrate a variety of features, and to tune 
feature weights to maximize the probability. A 
few papers have presented maximum entropy 
models for word or phrase ordering (Ratnaparkhi, 
2000; Filippova and Strube, 2007). However, 
those attempts have been limited to specialized 
applications, such as air travel reservation or or-
dering constituents of a main clause in German.  
This paper presents a general-purpose realizer 
based on log-linear models for directly lineariz-
ing dependency relations given dependency 
structures. We reduce the generation space by 
809
two techniques: the first is dividing the entire 
dependency tree into one-depth sub-trees and 
solving linearization in sub-trees; the second is 
the determination of relative positions between 
dependents and heads according to dependency 
relations. Then the best linearization for each 
sub-tree is selected by the log-linear model that 
incorporates three types of feature functions, in-
cluding dependency relations, surface words and 
headwords. The evaluation shows that our realiz-
er achieves competitive generation accuracy. 
The paper is structured as follows. In Section 
2, we describe the idea of dividing the realization 
procedure for an entire dependency tree into a 
series of sub-procedures for sub-trees. We de-
scribe how to determine the relative positions 
between dependents and heads according to de-
pendency relations in Section 3. Section 4 gives 
details of the log-linear model and the feature 
functions used for sentence realization. Section 5 
explains the experiments and provides the results. 
2 Sentence Realization from Dependen-
cy Structure  
2.1 The Dependency Input  
The input to our sentence realizer is a dependen-
cy structure as represented in the HIT Chinese 
Dependency Treebank (HIT-CDT)1. In our de-
pendency tree representations, dependency rela-
tions are represented as arcs pointing from a head 
to a dependent. The types of dependency arcs 
indicate the semantic or grammatical relation-
ships between the heads and the dependents, 
which are recorded in the dependent nodes. Fig-
ure 1 gives an example of dependency tree repre-
sentation for the sentence: 
 
(1) ? ? ?? ?? 
 this is Wuhan Airline 
 ?? ?? ?? ?? 
 first time buy Boeing airliner 
?This is the first time for Airline Wuhan to buy 
Boeing airliners.? 
In a dependency structure, dependents are un-
ordered, i.e. the string position of each node is 
not recorded in the representation. Our sentence 
realizer takes such an unordered dependency tree 
as input, determines the linear order of the words 
                                                 
1 HIT-CDT (http://ir.hit.edu.cn) includes 10,000 sentences 
and 215,334 words, which are manually annotated with 
part-of-speech tags and dependency labels. (Liu et al, 
2006a) 
as encoded in the nodes of the dependency struc-
ture and produces a grammatical sentence. As the 
dependency structures input to our realizer have 
been lexicalized, lexical selection is not involved 
during the surface realization. 
2.2 Divide and Conquer Strategy for Linea-
rization 
For determining the linear order of words 
represented by nodes of the given dependency 
structure, in principle, the sentence realizer has 
to produce all possible sequences of the nodes 
from the input tree and selects the most likely 
linearization among them. If the dependency tree 
consists of a considerable number of nodes, this 
procedure would be very time-consuming.  To 
reduce the number of possible realizations, our 
generation algorithm adopts a divide-and-
conquer strategy, which divides the whole tree 
into a set of sub-trees of depth one and recursive-
ly linearizes the sub-trees in a bottom-up fashion. 
As illustrated in Figure 2, sub-trees c and d, 
which are at the bottom of the tree, are linearized 
first, then sub-tree b is processed, and finally 
sub-tree a.  
The procedure imposes a projective constraint 
on the dependency structures, viz. each head 
dominates a continuous substring of the sentence 
realization. This assumption is feasible in the 
application of the dependency-based generation, 
because: (i) it has long been observed that the 
dependency structures of a vast majority of sen-
tences in the languages of the world are projec-
tive (Igor, 1988) and (ii) non-projective depen-
dencies in Chinese, for the most part, are used to 
account for non-local dependency phenomena. 
Figure 1: The dependency tree for the sentence 
???????????????? 
??(HED)
is 
??(SBV)
this 
???(VOB) 
buy 
???(ADV)
first time 
???(VOB) 
airliner 
???(SBV)
airline 
???(ATT)
Wuhan 
???(ATT) 
Boeing 
810
Though non-local dependencies are important for 
accurate semantic analysis, they can be easily 
converted to local dependencies conforming to 
the projective constraint. In fact, we find that the 
10, 000 manually-build dependency trees of the 
HIT-CDT do not contain any non-projective de-
pendencies. 
3 Relative Position Determination 
In dependency structures, the semantic or gram-
matical roles of the nodes are indicated by types 
of dependency relations. For example, the VOB 
dependency relation, which stands for the verb-
object structure, means that the head is a verb 
and the dependent is an object of the verb; the 
ATT relation, means that the dependent is an 
attribute of the head. In languages with fairly 
rigid word order, the relative position between 
the head and dependent of a certain relation is in 
a fixed order. For example in Chinese, the object 
almost always occurs behind its dominating verb; 
the attribute modifier always occurs in front of 
its head word. Therefore, we can draw a conclu-
sion that the relative positions between head and 
dependent of VOB and ATT can be determined 
by the types of dependency relations. 
We make a statistic on the relative positions 
between head and dependent for each dependen-
cy relation type. Following (Covington, 2001), 
we call a dependent that precedes its head prede-
pendent, a dependent that follows its head post-
dependent. The corpus used to gather appropriate 
statistics is HIT-CDT. Table 1 gives the numbers 
??(HED) 
is 
??(SBV) 
this 
?   ?   ???????????? 
?  
???(VOB) 
buy 
???(ADV)
first time 
?  ?  
????   ??   ??   ???? 
???(VOB)
airliner 
???(ATT)
Boeing 
??   ??
???(SBV) 
Airline 
???(ATT) 
Wuhan 
??   ?? 
sub-tree a 
sub-tree b 
sub-tree c sub-tree d 
Figure 2: Illustration of the linearization procedure 
Relation Description Postdep. Predep.
ADV adverbial 1 25977
APP appositive 807 0
ATT attribute 0 47040
CMP complement 2931 3
CNJ conjunctive 0 2124
COO coordinate 6818 0
DC dep. clause 197 0
DE DE phrase 0 10973
DEI DEI phrase 131 3
DI DI phrase 0 400
IC indep.clause 3230 0
IS indep.structure 125 794
LAD left adjunct 0 2644
MT mood-tense 3203 0
POB prep-obj 7513 0
QUN quantity 0 6092
RAD right adjunct 1332 1
SBV subject-verb 6 16016
SIM similarity 0 44
VOB verb-object 23487 21
VV verb-verb 6570 2
Table 1: Numbers of pre/post-dependents for each 
dependency relation 
811
of predependent/postdependent for each type of 
dependency relations and its descriptions. 
Table 1 shows that 100% dependents of ATT 
relation are predependents and 23,487(99.9%) 
against 21(0.1%) VOB dependents are postde-
pendents. Almost all the dependency relations 
have a dominant dependent type?predependent 
or postdependent. Although some dependency 
relations have exceptional cases (e.g. VOB), the 
number is so small that it can be ignored. The 
only exception is the IS relation, which has 
794(86.4%) predependents and 125(13.6%) 
postdependents. The IS label is an abbreviation 
for independent structure. This type of depen-
dency relation is usually used to represent inter-
jections or comments set off by brackets, which 
usually has little grammatical connection with 
the head. Figure 3 gives an example of indepen-
dent structure. This example is from a news re-
port, and the phrase ??????? (set apart by 
brackets in the original text) is a supplementary 
explanation for the source of the news. The con-
nection between this phrase and the main clause 
is so weak that either it precedes or follows the 
head verb is acceptable in grammar. However, 
this kind of news-source-explanation is customa-
ry to place at the beginning of a sentence in Chi-
nese. This can probably explain the majority of 
the IS-tagged dependents are predependents. 
If we simply treat all the IS dependents as pre-
dependents, we can assume that every dependen-
cy relation has only one type of dependent, either 
predependent or postdependent. Therefore, the 
relative position between head and dependent 
can be determined just by the types of dependen-
cy relations. 
In the light of this assumption, all dependents 
in a sub-tree can be classified into two groups?
predependents and postdependents. The prede-
pendents must precede the head, and the postde-
pendents must follow the head. This classifica-
tion not only reduces the number of possible se-
quences, but also solves the linearization of a 
sub-tree if the sub-tree contains only one depen-
dent, or two dependents of different types, viz. 
one predependent and one postdependent. In sub-
tree c of Figure 2, the dependency relation be-
tween the only dependent and the head is ATT, 
which indicates that the dependent is a prede-
pendent. Therefore, node 7 is bound to precede 
node 5, and the only linearization result is ???
???. In sub-tree a of the same figure, the clas-
sification for SBV is predependent, and for VOB 
is postdependent, so the only linearization is 
<node 2, node 1, node 3>.  
In HIT-CDT, there are 108,086 sub-trees in 
the 10,000 sentences, 65% sub-trees have only 
one dependent, and 7% sub-trees have two de-
pendents of different types (one predependent 
and one postdependent). This means that the 
relative position classification can deterministi-
cally linearize 72% sub-trees, and only the rest 
28% sub-trees with more than one predependent 
or postdependent need to be further determined. 
4 Log-linear Models 
We use log-linear models for selecting the se-
quence with the highest probability from all the 
possible linearizations of a sub-tree. 
4.1 The Log-linear Model 
Log-linear models employ a set of feature func-
tions to describe properties of the data, and a set 
of learned weights to determine the contribution 
of each feature. In this framework, we have a set 
of M feature functions Mmtrhm ,...,1),,( = . 
For each feature function, there exists a model 
parameter Mmtrm ,...,1),,( =?  that is fitted to 
optimize the likelihood of the training data. A 
conditional log-linear model for the probability 
of a realization r given the dependency tree t, has 
the general parametric form 
)],(exp[
)(
1
)|(
1
trh
tZ
trp m
M
m
m?
=
= ?
?
?            (1) 
where )(tZ?  is a normalization factor defined as 
? ?
? =
=
)(' 1
)],'(exp[)(
tYr
m
M
m
m trhtZ ??                    (2) 
And Y(t) gives the set of all possible realizations 
of the dependency tree t. 
4.2 Feature Functions 
We use three types of feature functions for cap-
turing relations among nodes on the dependency 
tree. In order to better illustrate the feature func-
tions used in the log-linear model, we redraw 
sub-tree b of Figure 2 in Figure 4. Here we as-
sume the linearizations of sub-tree c and d have 
Figure 3: Example of independent structure 
???(HED) 
serious 
??????(IS) 
Xinhua news 
?????(SBV) 
southern snowstorm 
812
been finished, and the strings of linearizing re-
sults are recorded in nodes 5 and 6. 
The sub-tree in Figure 4 has two predepen-
dents (SBV and ADV) and one postdependent 
(VOB). As a result of this classification, the only 
two possible linearizations of the sub-tree are 
<node 4, node 6, node 3, node 5> and <node 6, 
node 4, node 3, node 5>. Then the log-linear 
model that incorporates three types of feature 
functions is used to make further selection. 
Dependency Relation Model: For a particular 
sub-tree structure, the task of generating a string 
covered by the nodes on the sub-tree is equiva-
lent to linearizing all the dependency relations in 
that sub-tree. We linearize the dependency rela-
tions by computing n-gram models, similar to 
traditional word-based language models, except 
using the names of dependency relations instead 
of words. For the two linearizations of Figure 4, 
the corresponding dependency relation sequences 
are ?ADV SBV VOB VOB? and ?SBV ADV 
VOB VOB?. The dependency relation model 
calculates the probability of dependency relation 
n-gram P(DR) according to Eq.(3). The probabil-
ity score is integrated into the log-linear model as 
a feature.  
)...()( 11 m
m DRDRPDRP =  (3) 
       )|( 1 1
1
?
+?
=
?= k nkm
k
k DRDRP  
Word Model: We integrate an n-gram word 
model into the log-linear model for capturing the 
relation between adjacent words. For a string of 
words generated from a possible sequence of 
sub-tree nodes, the word models calculate word-
based n-gram probabilities of the string. For ex-
ample, in Figure 4, the strings generated by the 
two possible sequences are ????? ?? ?
? ????? and ??? ???? ?? ???
??. The word model takes these two strings as 
input, and calculates the n-gram probabilities. 
Headword Model: 2 In dependency representa-
tions, heads usually play more important roles 
than dependents. The headword model calculates 
the n-gram probabilities of headwords, without 
regard to the words occurring at dependent nodes, 
in that dependent words are usually less impor-
tant than headwords. In Figure 4, the two possi-
ble sequences of headwords are ??? ?? ?
?  ??? and ???  ??  ??  ???. The 
headword strings are usually more generic than 
the strings including all words, and thus the 
headword model is more likely to relax the data 
sparseness. 
   Table 2 gives some examples of all the features 
used in the log-linear model. The examples listed 
in the table are features of the linearization 
<node 6, node 4, node 3, node 5>, extracted from 
the sub-tree in Figure 4. 
In this paper, all the feature functions used in 
the log-linear model are n-gram probabilities. 
However, the log-linear framework has great 
potential for including other types of features. 
4.3 Parameter Estimation 
BLEU score, a method originally proposed to 
automatically evaluate machine translation quali-
ty (Papineni et al, 2002), has been widely used 
as a metric to evaluate general-purpose sentence 
generation (Langkilde, 2002; White et al, 2007; 
Guo et al 2008, Wan et al 2009). The BLEU 
measure computes the geometric mean of the 
precision of n-grams of various lengths between 
a sentence realization and a (set of) reference(s).  
To estimate the parameters ),...,( 1 M??  for the 
feature functions ),...,( 1 Mhh , we use BLEU
3 as 
optimization objective function and adopt the 
approach of minimum error rate training 
                                                 
2 Here the term ?headword? is used to describe the word 
that occurs at head nodes in dependency trees.  
3 The BLEU scoring script is supplied by NIST Open Ma-
chine Translation Evaluation at 
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 
Feature function Examples of features 
Dependency Relation ?SBV ADV VOB?  ?ADV VOB VOB? 
Word Model ???????? ???????? ????????????????
Headword Model ?????? ?????? ?????? 
Table 2: Examples of feature functions 
???(VOB) 
buy 
???(ADV) 
first time 
???(VOB) 
airliner 
?????? 
airliners of Boeing 
???(SBV)
Airline 
?????? 
Airline Wuhan
Figure 4: Sub-tree with multiple predependents
813
(MERT), which is popular in statistical machine 
translation (Och, 2003).   
4.4 The Realization Algorithm 
The realization algorithm is a recursive proce-
dure that starts from the root node of the depen-
dency tree, and traverses the tree by depth-first 
search. The pseudo code of the realization algo-
rithm is shown in Figure 5. 
5 Experiments 
5.1 Experimental Design 
Our experiments are carried out on HIT-CDT. 
We randomly select 526 sentences as the test set, 
and 499 sentences as the development set for 
optimizing the model parameters. The rest 8,975 
sentences of the HIT-CDT are used for training 
of the dependency relation model. For training of 
word models, we use the Xinhua News part 
(6,879,644 words) of Chinese Gigaword Second 
Edition (LDC2005T14), segmented by the Lan-
guage Technology Platform (LTP) 4 . And for 
training the headword model, we use both the 
HIT-CDT and the HIT Chinese Skeletal Depen-
dency Treebank (HIT-CSDT). HIT-CSDT is a 
                                                 
4 http://ir.hit.edu.cn/demo/ltp  
component of LTP and contains 49,991 sen-
tences in dependency structure representation 
(without dependency relation labels). 
As the input dependency representation does 
not contain punctuation information, we simply 
remove all punctuation marks in the test and de-
velopment sets. 
5.2 Evaluation Metrics 
In addition to BLEU score, percentage of exactly 
matched sentences and average NIST simple 
string accuracy (SSA) are adopted as evaluation 
metrics. The exact match measure is percentage 
of the generated string that exactly matches the 
corresponding reference sentence. The average 
NIST simple string accuracy score reflects the 
average number of insertion (I), deletion (D), and 
substitution (S) errors between the output sen-
tence and the reference sentence. Formally, SSA 
= 1 ? (I + D + S) / R, where R is the number of 
tokens in the reference sentence. 
5.3 Experimental Results 
All the evaluation results are shown in Table 3. 
The first experiment, which is a baseline experi-
ment, ignores the tree structure and randomly 
chooses position for every word. From the 
second experiment, we begin to utilize the tree 
structure and apply the realization algorithm de-
scribed in Section 4.4. In the second experiment, 
predependents are distinguished from postdepen-
dents by the relative position determination me-
thod (RPD), then the orders inside predependents 
and postdependents are chosen randomly. From 
the third experiments, the log-linear models are 
used for scoring the generated sequences, with 
the aid of three types of feature functions as de-
scribed in Section 4.2. First, the feature functions 
of trigram dependency relation model (DR), bi-
gram word model (Bi-WM), trigram word model 
(Tri-WM) (with Katz backoff) and trigram 
headword model (HW) are used separately in 
experiments 3-6. Then we combine the feature 
1:procedure SEARCH 
2:input: sub-tree T {head:H dep.:D1?Dn} 
3:  if n = 0 then return 
4:  for i := 1 to n 
5:    SEARCH(Di) 
6:  Apre := {} 
7:  Apost := {} 
8:  for i := 1 to n 
9:    if PRE-DEP(Di) then Apre:=Apre?{Di} 
10:   if POST-DEP(Di) then Apost:=Apost?{Di} 
11: for all permutations p1 of Apre 
12:   for all permutations p2 of Apost 
13:     sequence s := JOIN(p1,H,p2) 
14:     score r := LOG-LINEAR(s) 
15:     if best-score(r) then RECORD(r,s) 
Figure 5: The algorithm for linearizations of sub-
trees 
 Model BLEU ExMatch SSA 
1 Random 0.1478 0.0038 0.2044 
2 RPD + Random 0.5943 0.1274 0.6369 
3 RPD + DR 0.7204 0.2167 0.7683 
4 RPD + Bi-WM 0.8289 0.4125 0.8270 
5 RPD + Tri-WM 0.8508 0.4715 0.8415 
6 RPD + HW 0.7592 0.2909 0.7638 
7 RPD + DR + Bi-WM 0.8615 0.4810 0.8723 
8 RPD + DR + Tri-WM 0.8772 0.5247 0.8817 
9 RPD + DR + Tri-WM + HW 0.8874 0.5475 0.8920 
Table 3: BLEU, ExMatch and SSA scores on the test set 
814
functions incrementally based on the RPD and 
DR model. 
The relative position determination plays an 
important role in the realization algorithm. We 
observe that the BLEU score is boosted from 
0.1478 to 0.5943 by using the RPD method. This 
can be explained by the reason that the lineariza-
tions of 72% sub-trees can be definitely deter-
mined by the RPD method. All of the four fea-
ture functions we have tested achieve considera-
ble improvement in BLEU scores. The depen-
dency relation model achieves 0.7204, the bi-
gram word model 0.8289, the trigram word mod-
el 0.8508 and the headword model achieves 
0.7592. While the combined models perform bet-
ter than any of their individual component mod-
els. On the foundation of relative position deter-
mination method, the combination of dependen-
cy relation and bigram word model achieves a 
BLEU score of 0.8615, and the combination of 
dependency relation and trigram word model 
achieves a BLEU score of 0.8772. Finally the 
combination of dependency relation model, tri-
gram word model and headword model achieves 
the best result 0.8874.  
5.4 Discussion 
We first inspected the errors made by the relative 
position determination method. In the treebank-
tree test set, there are 7 predependents classified 
as postdependents and 3 postdependents classi-
fied as predependents by error. Among the 9,384 
dependents, the error rate of the relative position 
determination method is very small (0.1%). 
Then we make a classification on the errors in 
the experiment of dependency relation model 
(with relative position determination method). 
Table 4 shows the distribution of the errors. 
The first type of errors is caused by duplicate 
dependency relations, i.e. a head with two or 
more dependents that have the same dependency 
relations. In this situation, only using the depen-
dency relation model cannot generate the right 
linearization. However, word models, which util-
ize the word information, can make distinctions 
between the dependencies. The reason for the 
errors of SBV-ADV and ATT-QUN is probably 
because the order of these pairs of grammar roles 
is somewhat flexible. For example, the strings of 
???(ADV)/today ?(SBV)/I? and ??(SBV)/I 
??(ADV)/today? are both very common and 
acceptable in Chinese. 
The word models tend to combine the nodes 
that have strong correlation together. For exam-
ple in Figure 6, node 2 is more likely to precede 
node 3 because the words ??? /protect? and 
??? /future? have strong correlation, but the 
correct order is <node 3, node 2>. 
Headword model only consider the words oc-
cur at head nodes, which is helpful in the situa-
tion like Figure 6. In our experiments, the head-
word model gets a relatively low performance by 
itself, however, the addition of headword model 
to the combination of the other two feature func-
tions improves the result from 0.8772 to 0.8874. 
This indicates that the headword model is com-
plementary to the other feature functions. 
6 Conclusions 
We have presented a general-purpose realizer 
based on log-linear models, which directly maps 
dependency relations into surface strings. The 
linearization of a whole dependency tree is di-
vided into a series of sub-procedures on sub-trees. 
The dependents in the sub-trees are classified 
into two groups, predependents or postdepen-
dents, according to their dependency relations. 
The evaluation shows that this relative position 
determination method achieves a considerable 
result. The log-linear model, which incorporates 
three types of feature functions, including de-
pendency relation, surface words and headwords, 
successfully captures factors in sentence realiza-
tion and demonstrates competitive performance.  
 
References  
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a Probabilistic Hierarchical Model for 
Generation. In Proceedings of the 18th Interna-
tional Conference on Computational Linguistics, 
pages 42-48. Saarbr?cken, Germany. 
 Error types Proportion
1 Duplicate dependency relations 60.0% 
2 SBV-ADV 20.3% 
3 ATT-QUN 6.3% 
4 Other 13.4% 
Table 4: Error types in the RPD+DR experiment 
Figure 6: Sub-tree for ??????????? 
??? 
work
???(ATT) 
protect 
??? ??? 
?birds protecting?
??(SBV) 
of 
??? ?? 
future 
815
Aoife Cahill and Josef van Genabith. 2006. Robust 
PCFG-Based Generation Using Automatically Ac-
quired LFG Approximations. In Proceedings of the 
21st International Conference on Computational 
Linguistics and 44th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1033-
1040. Sydney, Australia. 
Aoife Cahill, Martin Forst and Christian Rohrer. 2007. 
Stochastic Realisation Ranking for a Free Word 
Order language. In Proceedings of 11th European 
Workshop on Natural Language Generation, pages 
17-24. Schloss Dagstuhl, Germany. 
John Carroll, Ann Copestake, Dan Flickinger, and 
Victor Poznanski. 1999. An Efficient Chart Gene-
rator for (Semi-)Lexicalist Grammars. In Proceed-
ings of the 7th European Workshop on Natural 
Language Generation, pages 86-95, Toulouse. 
Michael A. Covington. 2001. A Fundamental Algo-
rithm for Dependency Parsing. In Proceedings of 
the 39th Annual ACM Southeast Conference, pages 
95?102. 
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy 
King, John Maxwell, and Paula Newman. 2007. 
XLE documentation. Palo Alto Research Center, 
CA. 
Katja Filippova and Michael Strube. 2007. Generating 
Constituent Order in German Clauses. In Proceed-
ings of the 45th Annual Meeting of the Association 
of Computational Linguistics, pages 320-327. Pra-
gue, Czech Republic. 
Yuqing Guo, Haifeng Wang and Josef van Genabith. 
2008. Dependency-Based N-Gram Models for 
General Purpose Sentence Realisation. In Proceed-
ings of the 22th International Conference on Com-
putational Linguistics, pages 297-304. Manchester, 
UK. 
Deirdre Hogan, Conor Cafferkey, Aoife Cahill and 
Josef van Genabith. 2007. Exploiting Multi-Word 
Units in History-Based Probabilistic Generation. In 
Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing 
and CoNLL, pages 267-276. Prague, Czech Repub-
lic. 
Mel'?uk Igor. 1988. Dependency syntax: Theory and 
practice. In Suny Series in Linguistics. State Uni-
versity of New York Press, New York, USA. 
Irene Langkilde. 2000. Forest-Based Statistical Sen-
tence Generation. In Proceedings of 1st Meeting of 
the North American Chapter of the Association for 
Computational Linguistics, pages 170-177. Seattle, 
WA. 
Irene Langkilde. 2002. An Empirical Verification of 
Coverage and Correctness for a General-Purpose 
Sentence Generator. In Proceedings of the Second 
International Conference on Natural Language 
Generation, pages 17-24. New York, USA. 
Ting Liu, Jinshan Ma, and Sheng Li. 2006a. Building 
a Dependency Treebank for Improving Chinese 
Parser. Journal of Chinese Language and Compu-
ting, 16(4): 207-224. 
Ting Liu, Jinshan Ma, Huijia Zhu, and Sheng Li. 
2006b. Dependency Parsing Based on Dynamic 
Local Optimization. In Proceedings of CoNLL-X, 
pages 211-215, New York, USA. 
Hiroko Nakanishi, Yusuke Miyao and Jun?ichi Tsujii. 
2005. Probabilistic Models for Disambiguation of 
an HPSG-Based Chart Generator. In Proceedings 
of the 9th International Workshop on Parsing 
Technology, pages 93-102. Vancouver, British Co-
lumbia. 
Franz Josef Och. 2003. Minimum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
of the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160-167, Sappo-
ro, Japan. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 311-
318. Philadelphia, PA. 
Adwait Ratnaparkhi. 2000. Trainable Methods for 
Natural Language Generation. In Proceedings of 
North American Chapter of the Association for 
Computational Linguistics, pages 194-201. Seattle, 
WA. 
Erik Velldal and Stephan Oepen. 2005. Maximum 
Entropy Models for Realization Ranking. In Pro-
ceedings of the 10th Machine Translation Summit, 
pages 109-116. Phuket, Thailand,  
Stephen Wan, Mark Dras, Robert Dale, C?cile Paris. 
2009. Improving Grammaticality in Statistical Sen-
tence Generation: Introducing a Dependency Span-
ning Tree Algorithm with an Argument Satisfac-
tion Model. In Proceedings of the 12th Conference 
of the European Chapter of the ACL, pages 852-
860. Athens, Greece. 
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proceedings of the third International Nat-
ural Language Generation Conference, pages 182-
191. Hampshire, UK. 
Michael White, Rajakrishnan Rajkumar and Scott 
Martin. 2007. Towards Broad Coverage Surface 
Realization with CCG. In Proceedings of the Ma-
chine Translation Summit XI Workshop, pages 22-
30. Copenhagen, Danmark. 
816
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 142?146,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Improve Statistical Machine Translation with Context-Sensitive
Bilingual Semantic Embedding Model
Haiyang Wu
1
Daxiang Dong
1
Wei He
1
Xiaoguang Hu
1
Dianhai Yu
1
Hua Wu
1
Haifeng Wang
1
Ting Liu
2
1
Baidu Inc., No. 10, Shangdi 10th Street, Beijing, 100085, China
2
Harbin Institute of Technology, Harbin, China
wuhaiyang,dongdaxiang,hewei,huxiaoguang,yudianhai,
wu hua,wanghaifeng@baidu.com
tliu@ir.hit.edu.cn
Abstract
We investigate how to improve bilingual
embedding which has been successfully
used as a feature in phrase-based sta-
tistical machine translation (SMT). De-
spite bilingual embedding?s success, the
contextual information, which is of criti-
cal importance to translation quality, was
ignored in previous work. To employ
the contextual information, we propose
a simple and memory-efficient model for
learning bilingual embedding, taking both
the source phrase and context around the
phrase into account. Bilingual translation
scores generated from our proposed bilin-
gual embedding model are used as features
in our SMT system. Experimental results
show that the proposed method achieves
significant improvements on large-scale
Chinese-English translation task.
1 Introduction
In Statistical Machine Translation (SMT) sys-
tem, it is difficult to determine the translation of
some phrases that have ambiguous meanings.For
example, the phrase??? jieguo? can be trans-
lated to either ?results?, ?eventually? or ?fruit?,
depending on the context around it. There are two
reasons for the problem: First, the length of phrase
pairs is restricted due to the limitation of model
size and training data. Another reason is that SMT
systems often fail to use contextual information
in source sentence, therefore, phrase sense disam-
biguation highly depends on the language model
which is trained only on target corpus.
To solve this problem, we present to learn
context-sensitive bilingual semantic embedding.
Our methodology is to train a supervised model
where labels are automatically generated from
phrase-pairs. For each source phrase, the aligned
target phrase is marked as the positive label
whereas other phrases in our phrase table are
treated as negative labels. Different from previ-
ous work in bilingual embedding learning(Zou et
al., 2013; Gao et al., 2014), our framework is a
supervised model that utilizes contextual informa-
tion in source sentence as features and make use
of phrase pairs as weak labels. Bilingual seman-
tic embeddings are trained automatically from our
supervised learning task.
Our learned bilingual semantic embedding
model is used to measure the similarity of phrase
pairs which is treated as a feature in decoding. We
integrate our learned model into a phrase-based
translation system and experimental results indi-
cate that our system significantly outperform the
baseline system. On the NIST08 Chinese-English
translation task, we obtained 0.68 BLEU improve-
ment. We also test our proposed method on much
larger web dataset and obtain 0.49 BLEU im-
provement against the baseline.
2 Related Work
Using vectors to represent word meanings is
the essence of vector space models (VSM). The
representations capture words? semantic and syn-
tactic information which can be used to measure
semantic similarities by computing distance be-
tween the vectors. Although most VSMs represent
one word with only one vector, they fail to cap-
ture homonymy and polysemy of word. Huang
et al. (2012) introduced global document context
and multiple word prototypes which distinguishes
and uses both local and global context via a joint
training objective. Much of the research focus
on the task of inducing representations for sin-
gle languages. Recently, a lot of progress has
142
been made at representation learning for bilin-
gual words. Bilingual word representations have
been presented by Peirsman and Pad?o (2010) and
Sumita (2000). Also unsupervised algorithms
such as LDA and LSA were used by Boyd-Graber
and Resnik (2010), Tam et al. (2007) and Zhao and
Xing (2006). Zou et al. (2013) learn bilingual em-
beddings utilizes word alignments and monolin-
gual embeddings result, Le et al. (2012) and Gao et
al. (2014) used continuous vector to represent the
source language or target language of each phrase,
and then computed translation probability using
vector distance. Vuli?c and Moens (2013) learned
bilingual vector spaces from non-parallel data in-
duced by using a seed lexicon. However, none
of these work considered the word sense disam-
biguation problem which Carpuat and Wu (2007)
proved it is useful for SMT. In this paper, we learn
bilingual semantic embeddings for source content
and target phrase, and incorporate it into a phrase-
based SMT system to improve translation quality.
3 Context-Sensitive Bilingual Semantic
Embedding Model
We propose a simple and memory-efficient
model which embeds both contextual information
of source phrases and aligned phrases in target cor-
pus into low dimension. Our assumption is that
high frequent words are likely to have multiple
word senses; therefore, top frequent words are se-
lected in source corpus. We denote our selected
words as focused phrase. Our goal is to learn a
bilingual embedding model that can capture dis-
criminative contextual information for each fo-
cused phrase. To learn an effective context sensi-
tive bilingual embedding, we extract context fea-
tures nearby a focused phrase that will discrimi-
nate focused phrase?s target translation from other
possible candidates. Our task can be viewed as
a classification problem that each target phrase is
treated as a class. Since target phrases are usu-
ally in very high dimensional space, traditional
linear classification model is not suitable for our
problem. Therefore, we treat our problem as a
ranking problem that can handle large number of
classes and optimize the objectives with scalable
optimizer stochastic gradient descent.
3.1 Bilingual Word Embedding
We apply a linear embedding model for bilin-
gual embedding learning. Cosine similarity be-
tween bilingual embedding representation is con-
sidered as score function. The score function
should be discriminative between target phrases
and other candidate phrases. Our score function
is in the form:
f(x,y; W,U) = cos(W
T
x,U
T
y) (1)
where x is contextual feature vector in source sen-
tence, and y is the representation of target phrase,
W ? R
|X|?k
,U ? R
|Y|?k
are low rank ma-
trix. In our model, we allow y to be bag-of-words
representation. Our embedding model is memory-
efficient in that dimensionality of x and y can be
very large in practical setting. We use |X| and |Y|
means dimensionality of random variable x and y,
then traditional linear model such as max-entropy
model requires memory space of O(|X||Y|). Our
embedding model only requires O(k(|X|+ |Y|))
memory space that can handle large scale vocabu-
lary setting. To score a focused phrase and target
phrase pair with f(x,y), context features are ex-
tracted from nearby window of the focused phrase.
Target words are selected from phrase pairs. Given
a source sentence, embedding of a focused phrase
is estimated from W
T
x and target phrase embed-
ding can be obtained through U
T
y.
3.2 Context Sensitive Features
Context of a focused phrase is extracted from
nearby window, and in our experiment we choose
window size of 6 as a focused phrase?s con-
text. Features are then extracted from the focused
phrase?s context. We demonstrate our feature
extraction and label generation process from the
Chinese-to-English example in figure 1. Window
size in this example is three. Position features
and Part-Of-Speech Tagging features are extracted
from the focused phrase?s context. The word fruit
Figure 1: Feature extraction and label generation
143
is the aligned phrase of our focused phrase and is
treated as positive label. The phrase results is a
randomly selected phrase from phrase table results
of ??. Note that feature window is not well de-
fined near the beginning or the end of a sentence.
To conquer this problem, we add special padding
word to the beginning and the end of a sentence to
augment sentence.
3.3 Parameter Learning
To learn model parameter W and U, we ap-
ply a ranking scheme on candidates selected from
phrase table results of each focused phrase. In par-
ticular, given a focus phrase w, aligned phrase is
treated as positive label whereas phrases extracted
from other candidates in phrase table are treated
as negative label. A max-margin loss is applied in
this ranking setting.
I(?) =
1
m
m
?
i=1
(? ? f(x
i
, y
i
; ?)? f(x
i
, y
?
i
; ?))+
(2)
Where f(x
i
,y
i
) is previously defined, ? =
{W,U} and + means max-margin hinge loss. In
our implementation, a margin of ? = 0.15 is used
during training. Objectives are minimized through
stochastic gradient descent algorithm. For each
randomly selected training example, parameters
are updated through the following form:
? := ?? ?
?l(?)
??
(3)
where ? = {W,U}. Given an instance with pos-
itive and negative label pair {x,y,y
?
}, gradients
of parameter W and U are as follows:
?l(W,U)
?W
= qsx(W
T
x)
T
? pqs
3
x(U
T
y) (4)
?l(W,U)
?U
= qsy(U
T
y)
T
? pqs
3
y(W
T
x) (5)
Where we set p = (W
T
x)
T
(U
T
y), q =
1
||W
T
x||
2
and s =
1
||U
T
y||
2
. To initialize our model param-
eters with strong semantic and syntactic informa-
tion, word vectors are pre-trained independently
on source and target corpus through word2vec
(Mikolov et al., 2013). And the pre-trained word
vectors are treated as initial parameters of our
model. The learned scoring function f(x,y) will
be used during decoding phase as a feature in log-
linear model which we will describe in detail later.
4 Integrating Bilingual Semantic
Embedding into Phrase-Based SMT
Architectures
To incorporate the context-sensitive bilingual
embedding model into the state-of-the-art Phrase-
Based Translation model, we modify the decoding
so that context information is available on every
source phrase. For every phrase in a source sen-
tence, the following tasks are done at every node
in our decoder:
? Get the focused phrase as well as its context in the
source sentence.
? Extract features from the focused phrase?s context.
? Get translation candidate extracted from phrase pairs of
the focused phrase.
? Compute scores for any pair of the focused phrase and
a candidate phrase.
We get the target sub-phrase using word align-
ment of phrase, and we treat NULL as a common
target word if there is no alignment for the focused
phrase. Finally we compute the matching score for
source content and target word using bilingual se-
mantic embedding model. If there are more than
one word in the focus phrase, then we add all score
together. A penalty value will be given if target is
not in translation candidate list. For each phrase in
a given SMT input sentence, the Bilingual Seman-
tic score can be used as an additional feature in
log-linear translation model, in combination with
other typical context-independent SMT bilexicon
probabilities.
5 Experiment
Our experiments are performed using an in-
house phrase-based system with a log-linear
framework. Our system includes a phrase trans-
lation model, an n-gram language model, a lexi-
calized reordering model, a word penalty model
and a phrase penalty model, which is similar to
Moses (Koehn et al., 2007). The evaluation metric
is BLEU (Papineni et al., 2002).
5.1 Data set
We test our approach on LDC corpus first. We
just use a subset of the data available for NIST
OpenMT08 task
1
. The parallel training corpus
1
LDC2002E18, LDC2002L27, LDC2002T01,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005E83,
LDC2005T06, LDC2005T10, LDC2005T34, LDC2006E24,
LDC2006E26, LDC2006E34, LDC2006E86, LDC2006E92,
LDC2006E93, LDC2004T08(HK News, HK Hansards )
144
Method
OpenMT08 WebData
BLEU BLEU
Our Baseline 26.24 29.32
LOC 26.78** 29.62*
LOC+POS 26.82** 29.81*
Table 1: Results of lowercase BLEU on NIST08
task. LOC is the location feature and POS is
the Part-of-Speech feature * or ** equals to sig-
nificantly better than our baseline(? < 0.05 or
? < 0.01, respectively)
contains 1.5M sentence pairs after we filter with
some simple heuristic rules, such as sentence be-
ing too long or containing messy codes. As mono-
lingual corpus, we use the XinHua portion of the
English GigaWord. In monolingual corpus we fil-
ter sentence if it contain more than 100 words
or contain messy codes, Finally, we get mono-
lingual corpus containing 369M words. In order
to test our approach on a more realistic scenario,
we train our models with web data. Sentence
pairs obtained from bilingual website and com-
parable webpage. Monolingual corpus is gained
from some large website such as WiKi. There are
50M sentence pairs and 10B words monolingual
corpus.
5.2 Results and Analysis
For word alignment, we align all of the train-
ing data with GIZA++ (Och and Ney, 2003), us-
ing the grow-diag-final heuristic to improve recall.
For language model, we train a 5-gram modified
Kneser-Ney language model and use Minimum
Error Rate Training (Och, 2003) to tune the SMT.
For both OpenMT08 task and WebData task, we
use NIST06 as the tuning set, and use NIST08 as
the testing set. Our baseline system is a standard
phrase-based SMT system, and a language model
is trained with the target side of bilingual corpus.
Results on Chinese-English translation task are re-
ported in Table 1. Word position features and part-
of-speech tagging features are both useful for our
bilingual semantic embedding learning. Based on
our trained bilingual embedding model, we can
easily compute a translation score between any
bilingual phrase pair. We list some cases in table
2 to show that our bilingual embedding is context
sensitive.
Contextual features extracted from source sen-
tence are strong enough to discriminate different
Source Sentence
4 Nearest Neighbor from
bilingual embedding
??????????
?????????
?????(Investors
can only get down to
business in a stable so-
cial environment)
will be, can only, will, can
??????????
?????????
?????(In compe-
titions, the Chinese Dis-
abled have shown ex-
traordinary athletic abil-
ities)
skills, ability, abilities, tal-
ent
??????????
?????????
????(In the natu-
ral environment of Costa
Rica, grapes do not nor-
mally yield fruit.)
fruit, outcome of, the out-
come, result
? ? ??????
???????(As
a result, Eastern District
Council passed a pro-
posal)
in the end, eventually, as a
result, results
Table 2: Top ranked focused phrases based on
bilingual semantic embedding
word senses. And we also observe from the word
??? jieguo? that Part-Of-Speech Tagging fea-
tures are effective in discriminating target phrases.
6 Conlusion
In this paper, we proposed a context-sensitive
bilingual semantic embedding model to improve
statistical machine translation. Contextual infor-
mation is used in our model for bilingual word
sense disambiguation. We integrated the bilingual
semantic model into the phrase-based SMT sys-
tem. Experimental results show that our method
achieves significant improvements over the base-
line on large scale Chinese-English translation
task. Our model is memory-efficient and practical
for industrial usage that training can be done on
large scale data set with large number of classes.
Prediction time is also negligible with regard to
SMT decoding phase. In the future, we will ex-
plore more features to refine the model and try to
utilize contextual information in target sentences.
Acknowledgments
We thank the three anonymous reviewers for
their valuable comments, and Niu Gang and Wu
Xianchao for discussions. This paper is supported
by 973 program No. 2014CB340505.
145
References
Jordan Boyd-Graber and Philip Resnik. 2010. Holis-
tic sentiment analysis across languages: Multilin-
gual supervised latent dirichlet allocation. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 45?55,
Cambridge, MA, October. Association for Compu-
tational Linguistics.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 61?72, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2014. Learning continuous phrase rep-
resentations for translation modeling. In Proc. ACL.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 873?882, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with
neural networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the As-
sociation for Computational Linguistics: Human
Language Technologies, pages 39?48, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.
Corrado, and Jeffrey Dean. 2013. Distributed rep-
resentations of words and phrases and their compo-
sitionality. In NIPS, pages 3111?3119.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. In Computational Linguistics, Volume 29,
Number 1, March 2003. Computational Linguistics,
March.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Yves Peirsman and Sebastian Pad?o. 2010. Cross-
lingual induction of selectional preferences with
bilingual vector spaces. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 921?929, Los Ange-
les, California, June. Association for Computational
Linguistics.
Eiichiro Sumita. 2000. Lexical transfer using a vector-
space model. In Proceedings of the 38th Annual
Meeting of the Association for Computational Lin-
guistics. Association for Computational Linguistics,
August.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual-lsa based lm adaptation for spoken lan-
guage translation. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 520?527, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Ivan Vuli?c and Marie-Francine Moens. 2013. Cross-
lingual semantic similarity of words as the similarity
of their semantic word responses. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, pages 106?116, At-
lanta, Georgia, June. Association for Computational
Linguistics.
Bing Zhao and Eric P. Xing. 2006. Bitam: Bilingual
topic admixture models for word alignment. In Pro-
ceedings of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 969?976, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Will Y. Zou, Richard Socher, Daniel Cer, and Christo-
pher D. Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1393?
1398, Seattle, Washington, USA, October. Associa-
tion for Computational Linguistics.
146
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 979?987,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Improve SMT Quality with Automatically Extracted Paraphrase Rules 
 
 
Wei He1, Hua Wu2, Haifeng Wang2, Ting Liu1* 
1Research Center for Social Computing and Information 
Retrieval, Harbin Institute of Technology 
{whe,tliu}@ir.hit.edu.cn 
2Baidu 
{wu_hua,wanghaifeng}@baidu.com 
 
 
 
Abstract1 
We propose a novel approach to improve 
SMT via paraphrase rules which are 
automatically extracted from the bilingual 
training data. Without using extra 
paraphrase resources, we acquire the rules 
by comparing the source side of the parallel 
corpus with the target-to-source 
translations of the target side. Besides the 
word and phrase paraphrases, the acquired 
paraphrase rules mainly cover the 
structured paraphrases on the sentence 
level. These rules are employed to enrich 
the SMT inputs for translation quality 
improvement. The experimental results 
show that our proposed approach achieves 
significant improvements of 1.6~3.6 points 
of BLEU in the oral domain and 0.5~1 
points in the news domain. 
1 Introduction 
The translation quality of the SMT system is 
highly related to the coverage of translation models. 
However, no matter how much data is used for 
training, it is still impossible to completely cover 
the unlimited input sentences. This problem is 
more serious for online SMT systems in real-world 
applications. Naturally, a solution to the coverage 
problem is to bridge the gaps between the input 
sentences and the translation models, either from 
the input side, which targets on rewriting the input 
sentences to the MT-favored expressions, or from 
                                                          
This work was done when the first author was visiting Baidu. 
*Correspondence author: tliu@ir.hit.edu.cn 
the side of translation models, which tries to enrich 
the translation models to cover more expressions.  
In recent years, paraphrasing has been proven 
useful for improving SMT quality. The proposed 
methods can be classified into two categories 
according to the paraphrase targets: (1) enrich 
translation models to cover more bilingual 
expressions; (2) paraphrase the input sentences to 
reduce OOVs or generate multiple inputs. In the 
first category, He et al (2011), Bond et al (2008) 
and Nakov (2008) enriched the SMT models via 
paraphrasing the training corpora. Kuhn et al 
(2010) and Max (2010) used paraphrases to 
smooth translation models. For the second 
category, previous studies mainly focus on finding 
translations for unknown terms using phrasal 
paraphrases. Callison-Burch et al (2006) and 
Marton et al (2009) paraphrase unknown terms in 
the input sentences using phrasal paraphrases 
extracted from bilingual and monolingual corpora. 
Mirkin et al (2009) rewrite OOVs with 
entailments and paraphrases acquired from 
WordNet. Onishi et al (2010) and Du et al (2010) 
use phrasal paraphrases to build a word lattice to 
get multiple input candidates. In the above 
methods, only word or phrasal paraphrases are 
used for input sentence rewriting. No structured 
paraphrases on the sentence level have been 
investigated. However, the information in the 
sentence level is very important for disambiguation.  
For example, we can only substitute play with 
drama in a context related to stage or theatre. 
Phrasal paraphrase substitutions can hardly solve 
such kind of problems.  
In this paper, we propose a method that rewrites 
979
the input sentences of the SMT system using 
automatically extracted paraphrase rules which can 
capture structures on sentence level in addition to 
paraphrases on the word or phrase level. Without 
extra paraphrase resources, a novel approach is 
proposed to acquire paraphrase rules from the 
bilingual training corpus based on the results of 
Forward-Translation and Back-Translation. The 
rules target on rewriting the input sentences to an 
MT-favored expression to ensure a better 
translation. The paraphrase rules cover all kinds of 
paraphrases on the word, phrase and sentence 
levels, enabling structure reordering, word or 
phrase insertion, deletion and substitution. The 
experimental results show that our proposed 
approach achieves significant improvements of 
1.6~3.6 points of BLEU in the oral domain and 
0.5~1 points in the news domain. 
The remainder of the paper is organized as 
follows: Section 2 makes a comparison between 
the Forward-Translation and Back-Translation. 
Section 3 introduces our methods that extract 
paraphrase rules from the bilingual corpus of SMT. 
Section 4 describes the strategies for constructing 
word lattice with paraphrase rules. The 
experimental results and some discussions are 
presented in Section 5 and Section 6. Section 7 
compares our work to the previous researches. 
Finally, Section 8 concludes the paper and suggests 
directions for future work. 
2 Forward-Translation vs. Back-
Translation 
The Back-Translation method is mainly used for 
automatic MT evaluation (Rapp 2009). This 
approach is very helpful when no target language 
reference is available. The only requirement is that 
the MT system needs to be bidirectional. The 
procedure includes translating a text into certain 
foreign language with the MT system (Forward-
Translation), and translating it back into the 
original language with the same system (Back-
Translation). Finally the translation quality of 
Back-Translation is evaluated by using the original 
source texts as references. 
Sun et al (2010) reported an interesting 
phenomenon: given a bilingual text, the Back-
Translation results of the target sentences is better 
than the Forward-Translation results of the source 
sentences. Clearly, let (S0, T0) be the initial pair of 
bilingual text. A source-to-target translation system 
SYS_ST and a target-to-source translation system 
SYS_TS are trained using the bilingual corpus. 
?????  is a Forward-Translation function, and 
????? is a function of Back-Translation which can 
be deduced with two rounds of translations: 
????? ? ???_??????_??????. In the first round 
of translation, S0 and T0 are fed into SYS_ST and 
SYS_TS, and we get T1 and S1 as translation results. 
In the second round, we translate S1 back into the 
target side with SYS_ST, and get the translation T2. 
The procedure is illustrated in Figure 1, which can 
also formally be described as: 
1. T1 = FT(S0) = SYS_ST(S0). 
2. T2 = BT(T0), which can be decomposed into 
two steps: S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Using T0 as reference, an interesting result is 
reported in Sun et al (2010) that T2 achieves a 
higher score than T1 in automatic MT evaluation. 
This outcome is important because T2 is translated 
Figure 1: Procedure of Forward-Translation and Back-Translation. 
S0 T0 
S1 T1 
T2 
Source Language Target Language 
Initial Parallel Text 
1st Round Translation 
2nd Round Translation 
Forward- 
Translation
Back- 
Translation 
980
from a machine-generated text S1, but T1 is 
translated from a human-write text S0. Why the 
machine-generated text results in a better 
translation than the human-write text? Two 
possible reasons may explain this phenomenon: (1) 
in the first round of translation T0 ? S1, some 
target word orders are reserved due to the 
reordering failure, and these reserved orders lead to 
a better result in the second round of translation; (2) 
the text generated by an MT system is more likely 
to be matched by the reversed but homologous MT 
system.  
Note that all the texts of S0, S1, S2, T0 and T1 are 
sentence aligned because the initial parallel corpus 
(S0, T0) is aligned in the sentence level. The aligned 
sentence pairs in (S0, S1) can be considered as 
paraphrases. Since S1 has some MT-favored 
structures which may result in a better translation, 
an intuitive idea is whether we can learn these 
structures by comparing S1 with S0. This is the 
main assumption of this paper. Taking (S0, S1) as 
paraphrase resource, we propose a method that 
automatically extracts paraphrase rules to capture 
the MT-favored structures. 
3 Extraction of Paraphrase Rules 
3.1 Definition of Paraphrase Rules 
We define a paraphrase rule as follows: 
1. A paraphrase rule consists of two parts, left-
hand-side (LHS) and right-hand-side (RHS). 
Both of LHS and RHS consist of non-
terminals (slot) and terminals (words). 
2. LHS must start/end with a terminal. 
3. There must be at least one terminal between 
two non-terminals in LHS. 
A paraphrase rule in the format of:  
LHS ? RHS 
which means the words matched by LHS can be 
paraphrased to RHS. Taking Chinese as a case 
study, some examples of paraphrase rules are 
shown in Table 1. 
3.2  Selecting Paraphrase Sentence Pairs 
Following the methods in Section 2, the initial 
bilingual corpus is (S0, T0). We train a source-to-
target PBMT system (SYS_ST) and a target-to-
source PBMT system (SYS_TS) on the parallel 
corpus. Then a Forward-Translation is performed 
on S0 using SYS_ST, and a Back-Translation is 
performed on T0 using SYS_TS and SYS_ST. As 
mentioned above, the detailed procedure is: T1 = 
SYS_ST(S0), S1 = SYS_TS(T0), T2 = SYS_ST(S1). 
Finally we compute BLEU (Papineni et al 2002) 
score for every sentence in T2 and T1, using the 
corresponding sentence in T0 as reference. If the 
sentence in T2 has a higher BLEU score than the 
aligned sentence in T1, the corresponding sentences 
in S0 and S1 are selected as candidate paraphrase 
sentence pairs, which are used in the following 
steps of paraphrase extractions. 
3.3 Word Alignments Filtering 
We can construct word alignment between S0 and 
S1 through T0. On the initial corpus of (S0, T0), we 
conduct word alignment with Giza++ (Och and 
Ney, 2000) in both directions and then apply the 
grow-diag-final heuristic (Koehn et al, 2005) for 
symmetrization. Because S1 is generated by 
feeding T0 into the PBMT system SYS_TS, the 
word alignment between T0 and S1 can be acquired 
from the verbose information of the decoder. 
The word alignments of S0 and S1 contain noises 
which are produced by either wrong alignment of 
GIZA++ or translation errors of SYS_TS. To ensure 
the alignment quality, we use some heuristics to 
filter the alignment between S0 and S1: 
1. If two identical words are aligned in S0 and 
S1, then remove all the other links to the two 
words. 
No. LHS RHS 
1 ??/ride   X1   ????/bus ??/ride    X1   ??/bus 
2    ?/at   X1  ?/location   ???/turn left  ???/turn left   ?/at   X1  ?/location 
3 ?/NULL   X1    ?/give    ?/me ?/give    ?/me    X1 
4 ?/from  X1  ?/to  X2  ?/need ??/how long??/time 
?/need   ?/spend  ??/how long  ??/time 
?/from X1?/to X2 
Table 1: Examples of Chinese Paraphrase rules, together with English translations for every word 
981
2. Stop words (including some function words 
and punctuations) can only be aligned to 
either stop words or null. 
Figure 2 illustrates an example of using the 
heuristics to filter alignment. 
3.4 Extracting Paraphrase Rules 
From the word-aligned sentence pairs, we then 
extract a set of rules that are consistent with the 
word alignments. We use the rule extracting 
methods of Chiang (2005). Take the sentence pair 
in Figure 2 as an example, two initial phrase pairs 
PP1 = ?? ? ?? ??? ||| ? ? ?? ????  
and  PP2 = ?? ? ? ?? ??? ? ?? ||| ? 
? ?? ? ? ?? ???? are identified, and 
PP1 is contained by PP2, then we could form the 
rule: 
? X1 ? ?? ? ? ? ?? X1
to  have interest  very feel interest  
4 Paraphrasing the Input Sentences 
The extracted paraphrase rules aim to rewrite the 
input sentences to an MT-favored form which may 
lead to a better translation. However, it is risky to 
directly replace the input sentence with a 
paraphrased sentence, since the errors in automatic 
paraphrase substitution may jeopardize the 
translation result seriously. To avoid such damage, 
for a given input sentence, we first transform all 
paraphrase rules that match the input sentences to 
phrasal paraphrases, and then build a word lattice 
for SMT decoder using the phrasal paraphrases. In 
this case, the decoder can search for the best result 
among all the possible paths. 
The input sentences are first segmented into sub-
sentences by punctuations. Then for each sub-
sentence, the matched paraphrase rules are ranked 
according to: (1) the number of matched words; (2) 
the frequency of the paraphrase rule in the training 
data. Actually, the ranking strategy tends to select 
paraphrase rules that have more matched words 
(therefore less ambiguity) and higher frequency 
(therefore more reliable). 
4.1 Applying Paraphrase Rules 
Given an input sentence S and a paraphrase rule R 
<RLHS, RRHS>, if S matches RLHS, then the matched 
part can be replaced by RRHS. An example for 
applying the paraphrase rules is illustrated in 
Figure 3.  
From Figure 3, we can see that the words of 
position 1~3 are replaced to ??? 10 ? ???. 
Actually, only the words at position 3 and 4 are 
paraphrased to the word ????, other words are 
left unchanged. Therefore, we can use a triple, 
<MIN_RP_TEXT, COVER_START, COVER_LEN> 
(<?? , 3, 1> in this example) to denote the 
paraphrase rule, which means the minimal text to 
replace is ????, and the paraphrasing starts at 
position 3 and covers 1 words. 
In this manner, all the paraphrase rules matched 
for a certain sentence can be converted to the 
format of <MIN_RP_TEXT, COVER_START, 
COVER_LEN>, which can also be considered as 
phrasal paraphrases. Then the methods of building 
phrasal paraphrases into word lattice for SMT 
inputs can be used in our approaches. 
??    ??     [10?]   ????
??     [10?]      ?? 
Rule 
LHS:??/ride  X1 ????/bus 
RHS:??/ride  X1  ??/bus 
Figure 3: Example for Applying Paraphrase Rules 
0         1            2                3
welcome  ride     No.10         bus
ride       No.10        bus 
I  very feel interest that N/A  blue   handbag  
I     to   that   N/A  blue  handbag have interest    
?   ?   ?    ??   ?    ?  ??   ???     ? 
?   ?    ?     ?    ??   ???  ?  ??     ? 
Figure 2: Example for Word Alignment 
Filtration 
I     to   that   N/A  blue  handbag have interest    
?   ?    ?     ?    ??   ???  ?  ??     ? 
I  very feel interest that N/A  blue   handbag  
?   ?   ?    ??   ?    ?  ??   ???      ? 
982
4.2 Construction of Paraphrase Lattice 
Given an input sentence, all the matched 
paraphrase rules are converted to phrasal 
paraphrases first. Then we build the phrasal 
paraphrases into word lattices using the methods 
proposed by Du et al (2010). The construction 
process takes advantage of the correspondence 
between detected phrasal paraphrases and positions 
of the original words in the input sentence, and 
then creates extra edges in the lattices to allow the 
decoder to consider paths involving the paraphrase 
words. An example is illustrated in Figure 4: given 
a sequence of words {w1,?,wN} as the input, two 
phrases ? ={?1,??p} and ? = {?1,?, ?q} are 
detected as paraphrases for P1 = {wx,?, wy} (1 ? x 
? y ? N) and P2 = {wm,?,wn} (1 ? m ? n ? N) 
respectively. The following steps are taken to 
transform them into word lattices: 
1. Transform the original source sentence into 
word lattice. N + 1 nodes (?k, 0 ? k ? N) are 
created, and N edges labeled with wi (1 ? i ? 
N) are generated to connect them 
sequentially. 
2. Generate extra nodes and edges for each of 
the paraphrases. Taking ? as an example, 
firstly, p ? 1 nodes are created, and then p 
edges labeled with ?j (1 ? j ? p) are 
generated to connect node ?x-1, p-1 nodes 
and ?y-1. 
Via step 2, word lattices are generated by adding 
new nodes and edges coming from paraphrases. 
4.3  Weight Estimation 
The weights of new edges in the lattices are 
estimated by an empirical method base on ranking 
positions. Following Du et al (2010), supposing 
that E = {e1,?,ek} are a set of new edges 
constructed from k paraphrase rules, which are 
sorted in a descending order. Then the weight for 
an edge ei is calculated as: 
??e?? ? 1? ? ? ???1 ? ? ? ?? where k is a predefined tradeoff parameter between 
decoding speed and the number of potential 
paraphrases being considered. 
5  Experiments 
5.1  Experimental Data 
In our experiments, we used Moses (Koehn et al, 
2007) as the baseline system which can support 
lattice decoding. The alignment was obtained using 
GIZA++ (Och and Ney, 2003) and then we 
symmetrized the word alignment using the grow-
diag-final heuristic. Parameters were tuned using 
Minimum Error Rate Training (Och, 2003). To 
comprehensively evaluate the proposed methods in 
different domains, two groups of experiments were 
carried out, namely, the oral group (Goral) and the 
news group (Gnews). The experiments were 
conducted in both Chinese-English and English-
Chinese directions for the oral group, and Chinese-
English direction for the news group. The English 
sentences were all tokenized and lowercased, and 
the Chinese sentences were segmented into words 
by Language Technology Platform (LTP) 1 . We 
used SRILM2 for the training of language models 
(5-gram in all the experiments). The metrics for 
automatic evaluation were BLEU 3  and TER 4 
(Snover et al, 2005). 
The detailed statistics of the training data in Goral 
are showed in Table 2. For the bilingual corpus, we 
used the BTEC and PIVOT data of IWSLT 2008, 
HIT corpus 5  and other Chinese LDC (CLDC) 
                                                          
1 http://ir.hit.edu.cn/ltp/ 
2 http://www.speech.sri.com/projects/srilm/ 
3 ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl 
4 http://www.umiacs.umd.edu/~snover/terp/ 
5 The HIT corpus contains the CLDC Olympic corpus (2004-
863-008) and the other HIT corpora available at 
http://mitlab.hit.edu.cn/index.php/resources/29-the-
resource/111-share-bilingual-corpus.html. 
Figure 4: An example of lattice-based 
paraphrases for an input sentence. 
983
corpora, including the Chinese-English Sentence 
Aligned Bilingual Corpus (CLDC-LAC-2003-004) 
and the Chinese-English Parallel Corpora (CLDC-
LAC-2003-006). We trained a Chinese language 
model for the E-C translation on the Chinese part 
of the bi-text. For the English language model of 
C-E translation, an extra corpus named Tanaka was 
used besides the English part of the bilingual 
corpora. For testing and developing, we used six 
Chinese-English development corpora of IWSLT 
2008. The statistics are shown in Table 3.  
In detail, we chose CSTAR03-test and 
IWSLT06-dev as the development set; and used 
IWSLT04-test, IWSLT05-test, IWSLT06-dev and 
IWSLT07-test for testing. For English-Chinese 
evaluation, we used IWSLT English-Chinese MT 
evaluation 2005 as the test set. Due to the lacking 
of development set, we did not tune parameters on 
English-Chinese side, instead, we just used the 
default parameters of Moses. 
In the experiments of the news group, we used 
the Sinorama and FBIS corpora (LDC2005T10 and 
LDC2003E14) for bilingual corpus. After 
tokenization and filtering, this bilingual corpus 
contained 319,694 sentence pairs (7.9M tokens on 
Chinese side and 9.2M tokens on English side). 
We trained a 5-gram language model on the 
English side of the bi-text. The system was tested 
using the Chinese-English MT evaluation sets of 
NIST 2004, NIST 2006 and NIST 2008. For 
development, we used the Chinese-English MT 
evaluation sets of NIST 2002 and NIST 2005. 
Table 4 shows the statistics of test/development 
sets used in the news group. 
5.2 Results 
We extract both Chinese and English rules in Goral, 
and Chinese paraphrase rules in Gnews by 
comparing the results of Forward-Translation and 
Back-Translation as described in Section 3. During 
the extraction, some heuristics are used to ensure 
the quality of paraphrase rules. Take the extraction 
of Chinese paraphrase rules in Goral as a case study. 
Suppose (C0, E0) are the initial bilingual corpus of 
Goral. A Chinese-English and an English-Chinese 
MT system are trained on (C0, E0). We perform 
Back-Translation on E0 (?? ???????? ?? ?? ???????? ?? ??), and 
Forward-Translation on C0 (?? ???????? ?? ??). Suppose e1i and e2i are two aligned sentences in E1 and E2, 
c0i and c1i are the corresponding sentences in C0 
and C1. (c0i, c1i) are selected for the extraction of 
paraphrase rules if two conditions are satisfied: (1) 
BLEU(e2i) ? BLEU(e1i) > ?1, and (2) BLEU(e2i) > 
?2, where BLEU???  is a function for computing 
BLEU score; ?1 and ?2 are thresholds for balancing 
the rules number and the quality of paraphrase 
rules. In our experiment, ?1 and ?2 are empirically 
set to 0.1 and 0.3. 
As a result, we extract 912,625 Chinese and 
1,116,375 English paraphrase rules for Goral, and 
for Gnews the number of Chinese paraphrase rules is 
2,877,960. Then we use the extracted paraphrase 
rules to improve SMT by building word lattices for 
the input sentences. 
The Chinese-English experimental results of 
Goral and Gnews are shown in Table 5 and Table 6, 
respectively. It can be seen that our method 
outperforms the baselines in both oral and news 
domains. Our system gains significant 
improvements of 1.6~3.6 points of BLEU in the 
oral domain, and 0.5~1 points of BLEU in the 
news domain. Figure 5 shows the effect of 
considered paraphrases (k) in the step of building  
Corpus #Sen. pairs #Ch. words #En words
BETC 19,972 174k 190k 
PIVOT 20,000 162k 196k 
HIT 80,868 788k 850k 
CLDC 190,447 1,167k 1,898k 
Tanaka 149,207 - 1,375k 
Table 2: Statistics of training data in Goral 
 Corpus #Sen.  #Ref.  
develop CSTAR03 test set 506 16 IWSLT06 dev set 489 7 
test 
IWSLT04 test set 500 16 
IWSLT05 test set 506 16 
IWSLT06 test set 500 7 
IWSLT07 test set 489 6 
Table 3: Statistics of test/develop sets in Goral 
 Corpus #Sen.  #Ref.  
develop NIST 2002 878 10 NIST 2005 1,082 4 
test 
NIST 2004 1,788 5 
NIST 2006 1,664 4 
NIST 2008 1,357 4 
Table 4: Statistics of test/develop sets in Gnews 
984
word lattices. The result of English-Chinese 
experiments in Goral is shown in Table 7.  
6 Discussion 
We make a detailed analysis on the Chinese-
English translation results that are affected by our 
paraphrase rules. The aim is to investigate what 
kinds of paraphrases have been captured in the 
rules. Firstly the input path is recovered from the 
translation results according to the tracing 
information of the decoder, and therefore we can 
examine which path is selected by the SMT 
decoder from the paraphrase lattice. A human 
annotator is asked to judge whether the recovered 
paraphrase sentence keeps the same meaning as the 
original input. Then the annotator compares the 
baseline translation with the translations proposed 
by our approach. The analysis is carried out on the 
IWSLT 2007 Chinese-English test set, 84 out of 
489 input sentences have been affected by 
paraphrases, and the statistic of human evaluation 
is shown in Table 8.  
It can be seen in Table 8 that the paraphrases 
achieve a relatively high accuracy, 60 (71.4%) 
paraphrased sentences retain the same meaning, 
and the other 24 (28.6%) are incorrect. Among the 
60 correct paraphrases, 36 sentences finally result 
in an improved translation. We further analyze 
these paraphrases and the translation results to 
investigate what kinds of transformation finally 
lead to the translation quality improvement. The 
paraphrase variations can be classified into four 
categories: 
(1) Reordering: The original source sentences 
are reordered to be similar to the order of 
the target language. 
(2) Word substitution: A phrase with multi-
word translations is replaced by a phrase 
with a single-word translation.  
(3) Recovering omitted words: Ellipsis occurs 
frequently in spoken language. Recovering 
the omitted words often leads to a better 
translation. 
(4) Removing redundant words: Mostly, 
translating redundant words may confuse 
the SMT system and would be unnecessary. 
Removing redundant words can mitigate 
this problem. 
44.2?
44.4?
44.6?
44.8?
45.0?
45.2?
45.4?
0 10 20 30 40
BLE
U?s
cor
e?(
%)
Considered?paraprhases?(k)
Figure 5: Effect of considered paraphrases (k) 
on BLEU score
Model BLEU TER iwslt 04 iwslt 05 iwslt 06 iwslt 07 iwslt 04 iwslt 05 iwslt 06 iwslt 07
baseline 0.5353 0.5887 0.2765 0.3977 0.3279 0.2874 0.5559 0.4390 
para. improved 0.5712 0.6107 0.2924 0.4193 0.3055 0.2722 0.5374 0.4217 
Model BLEU TER nist 04 nist 06 nist 08 nist 04 nist 06 nist 08 
baseline 0.2795 0.2389 0.1933 0.6554 0.6515 0.6652 
para. improved 0.2891 0.2485 0.1978 0.6451 0.6407 0.6582 
 model IWSLT 2005  BLEU TER 
 baseline 0.4644 0.4164 
 para. improved  0.4853 0.3883 
trans. 
para. improve comparable worsen total
correct 36 20 4 60 
incorrect 1 9 14 24 
Table 8: Human analysis of the paraphrasing 
results in IWSLT 2007 CE translation 
Table 5: Experimental results of Goral in Chinese-English direction 
Table 6: Experimental results of Gnews in Chinese-English direction 
Table 7: Experimental results of Goral in 
English-Chinese direction 
985
Four examples for category (1), (2), (3) and (4) 
are shown in Table 9, respectively. The numbers in 
the second column indicates the number of the 
sentences affected by the rules, among the 36 
sentences with improved paraphrasing and 
translation. A sentence can be classified into 
multiple categories. Except category (2), the other 
three categories cannot be detected by the previous 
approaches, which verify our statement that our 
rules can capture structured paraphrases on the 
sentence level in addition to the paraphrases on the 
word or phrase level. 
Not all the paraphrased results are correct. 
Sometimes an ill paraphrased sentence can produce 
better translations. Take the first line of Table 9 as 
an example, the paraphrased sentence ???/How 
many ??/cigarettes ??/can ??/duty-free ?
/take ?/NULL? is not a fluent Chinese sentence, 
however, the rearranged word order is closer to 
English, which finally results in a much better 
translation. 
7 Related Work 
Previous studies on improving SMT using 
paraphrase rules focus on hand-crafted rules. 
Nakov (2008) employs six rules for paraphrasing 
the training corpus. Bond et al (2008) use 
grammars to paraphrase the source side of training 
data, covering aspects like word order and minor 
lexical variations (tenses etc.) but not content 
words. The paraphrases are added to the source 
side of the corpus and the corresponding target 
sentences are duplicated. 
A disadvantage for hand-crafted paraphrase 
rules is that it is language dependent. In contrast, 
our method that automatically extracted paraphrase 
rules from bilingual corpus is flexible and suitable 
for any language pairs. 
Our work is similar to Sun et al (2010). Both 
tried to capture the MT-favored structures from 
bilingual corpus. However, a clear difference is 
that Sun et al (2010) captures the structures 
implicitly by training an MT system on (S0, S1) and 
?translates? the SMT input to an MT-favored 
expression. Actually, the rewriting process is 
considered as a black box in Sun et al (2010). In 
this paper, the MT-favored expressions are 
captured explicitly by automatically extracted 
paraphrase rules. The advantages of the paraphrase 
rules are: (1) Our method can explicitly capture the 
structure information in the sentence level, 
enabling global reordering, which is impossible in 
Sun et al (2010). (2) For each rule, we can control 
its quality automatically or manually. 
8 Conclusion 
In this paper, we propose a novel method for 
extracting paraphrase rules by comparing the 
source side of bilingual corpus to the target-to-
source translation of the target side. The acquired 
paraphrase rules are employed to enrich the SMT 
inputs, which target on rewriting the input 
sentences to an MT-favored form. The paraphrase 
rules cover all kinds of paraphrases on the word, 
phrase and sentence levels, enabling structure 
reordering, word or phrase insertion, deletion and 
substitution. Experimental results show that the 
paraphrase rules can improve SMT quality in both 
the oral and news domains. The manual 
investigation on oral translation results indicate 
that the paraphrase rules capture four kinds of MT-
favored transformation to ensure translation quality 
improvement. 
Cate. Num Original Sentence/Translation Paraphrased Sentence/Translation 
(1) 11 
??/cigarette ??/can ??/duty-free ?
/take ??/how much ?/N/A ?  
??/how much ??/cigarettes ??/can ??
/duty-free ?/take ?/N/A ? 
what a cigarette can i take duty-free ? how many cigarettes can i take duty-free  one ? 
(2) 18 
?/you  ?/have  ??/how long  ?/N/A  
??/teaching ??/experience ? 
?/you  ?/have  ??/how much  ??/teaching  
??/experience ? 
you have how long teaching experience ? how much teaching experience you have ? 
(3) 10 ??/need  ??/deposit  ?/N/A ? ?/you  ??/need  ??/deposit  ?/N/A ? you need a deposit ? do you need a deposit ? 
(4) 4 
??/ring ?/fall ?/into ???/washbasin 
?/in ?/N/A ?  
ring off into the washbasin is in . 
??/ring  ?/fall  ?/into  ???/washbasin ?
/N/A ? 
ring off into the washbasin . 
Table 9: Examples for classification of paraphrase rules 
986
Acknowledgement 
This work was supported by National Natural 
Science Foundation of China (NSFC) (61073126, 
61133012), 863 High Technology Program 
(2011AA01A207). 
References  
Francis Bond, Eric Nichols, Darren Scott Appling, and 
Michael Paul. 2008. Improving Statistical Machine 
Translation by Paraphrasing the Training Data. In 
Proceedings of the IWSLT, pages 150?157. 
Chris Callison-Burch, Philipp Koehn, and Miles 
Osborne. 2006. Improved Statistical Machine 
Translation Using Paraphrases. In Proceedings of 
NAACL, pages 17-24. 
David Chiang. 2005. A hierarchical phrase-based model 
for statistical machine translation. In Proceedings of 
ACL, pages 263?270. 
Jinhua Du, Jie Jiang, Andy Way. 2010. Facilitating 
Translation Using Source Language Paraphrase 
Lattices. In Proceedings of EMNLP, pages 420-429. 
Wei He, Shiqi Zhao, Haifeng Wang and Ting Liu. 2011. 
Enriching SMT Training Data via Paraphrasing. In 
Proceedings of IJCNLP, pages 803-810. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In 
Proceedings of HLT/NAACL, pages 48?54 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of 
EMNLP, pages 388-395. 
Philipp Koehn, Amittai Axelrod, Alexandra Birch 
Mayne, Chris Callison-Burch, Miles Osborne, and 
David Talbot. 2005. Edinburgh System Description 
for the 2005 IWSLT Speech Translation Evaluation. 
In Proceedings of IWSLT. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, Wade Shen, Christine Moran 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proceedings of the ACL Demo and Poster Sessions, 
pages 177?180. 
Roland Kuhn, Boxing Chen, George Foster and Evan 
Stratford. 2010. Phrase Clustering for Smoothing TM 
Probabilities-or, How to Extract Paraphrases from 
Phrase Tables. In Proceedings of COLING, pages 
608?616. 
Yuval Marton, Chris Callison-Burch, and Philip Resnik. 
2009. Improved Statistical Machine Translation 
Using Monolingually-Dervied Paraphrases. In 
Proceedings of EMNLP, pages 381-390. 
Aur?lien Max. 2010. Example-Based Paraphrasing for 
Improved Phrase-Based Statistical Machine 
TranslationIn Proceedings of EMNLP, pages 656-
666. 
Shachar Mirkin, Lucia Specia, Nicola Cancedda, Ido 
Dagan, Marc Dymetman, Idan Szpektor. 2009. 
Source-Language Entailment Modeling for 
Translation Unknown Terms. In Proceedings of ACL, 
pages 791-799. 
Preslav Nakov. 2008. Improved Statistical Machine 
Translation Using Monolingual Paraphrases. In 
Proceedings of ECAI, pages 338-342. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. In Proceedings of ACL, 
pages 440-447. 
Franz Josef Och. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
ACL, pages 160-167. 
Takashi Onishi, Masao Utiyama, Eiichiro Sumita. 2010. 
Paraphrase Lattice for Statistical Machine 
Translation. In Proceedings of ACL, pages 1-5. 
Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing 
Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. In Proceedings of 
ACL, pages 311-318. 
Reinhard Rapp. 2009. The Back-translation Score: 
Automatic MT Evaluation at the Sentence Level 
without Reference Translations. In Proceedings of 
ACL-IJCNLP, pages 133-136. 
Matthew Snover, Bonnie J. Dorr, Richard Schwartz, 
John Makhoul, Linnea Micciulla, and Ralph 
Weischedel. 2005. A study of translation error rate 
with targeted human annotation. Technical Report 
LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-
58, University of Maryland, July, 2005. 
Yanli Sun, Sharon O?Brien, Minako O?Hagan and Fred 
Hollowood. 2010. A Novel Statistical Pre-Processing 
Model for Rule-Based Machine Translation System. 
In Proceedings of EAMT, 8pp. 
987
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 407?410,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
HIT-CIR: An Unsupervised WSD System Based on Domain Most
Frequent Sense Estimation
Yuhang Guo, Wanxiang Che, Wei He, Ting Liu, Sheng Li
Harbin Institute of Technolgy
Harbin, Heilongjiang, PRC
yhguo@ir.hit.edu.cn
Abstract
This paper presents an unsupervised sys-
tem for all-word domain specific word
sense disambiguation task. This system
tags target word with the most frequent
sense which is estimated using a thesaurus
and the word distribution information in
the domain. The thesaurus is automati-
cally constructed from bilingual parallel
corpus using paraphrase technique. The
recall of this system is 43.5% on SemEval-
2 task 17 English data set.
1 Introduction
Tagging polysemous word with its most frequent
sense (MFS) is a popular back-off heuristic in
word sense disambiguation (WSD) systems when
the training data is inadequate. In past evalua-
tions, MFS from WordNet performed even bet-
ter than most of the unsupervised systems (Snyder
and Palmer, 2004; Navigli et al, 2007).
MFS is usually obtained from a large scale
sense tagged corpus, such as SemCor (Miller et al,
1994). However, some polysemous words have
different MFS in different domains. For example,
in the Koeling et al (2005) corpus, target word
coach means ?manager? mostly in the SPORTS
domain but means ?bus? mostly in the FINANCE
domain. So when the MFS is applied to specific
domains, it needs to be re-estimated.
McCarthy et al (2007) proposed an unsuper-
vised predominant word sense acquisition method
which obtains domain specific MFS without sense
tagged corpus. In their method, a thesaurus, in
which words are connected with their distribu-
tional similarity, is constructed from the domain
raw text. Word senses are ranked by their preva-
lence score which is calculated using the thesaurus
and the sense inventory.
In this paper, we propose another way to con-
struct the thesaurus. We use statistical machine
Figure 1: The architecture of HIT-CIR
translation (SMT) techniques to extract paraphrase
pairs from bilingual parallel text. In this way, we
avoid calculating similarities between every pair
of words and could find semantic similar words or
compounds which have dissimilar distributions.
Our system is comprised of two parts: the word
sense ranking part and the word sense tagging part.
Senses are ranked according to their prevalence
score in the target domain, and the predominant
sense is used to tag the occurrences of the target
word in the test data. The architecture of this sys-
tem is shown in Figure 1.
The word sense ranking part includes following
steps.
1. Tag the POS of the background text, count
the word frequency in each POS, and get the
polysemous word list of the POS.
2. Using SMT techniques to extract phrase table
407
Figure 2: Word sense ranking for the noun backbone
from the bilingual corpus. Extract the para-
phrases (called as neighbor words) with the
phrase table for each word in the polysemous
word list.
3. Calculate the prevalence score of each sense
of the target words, rank the senses with the
score and obtain the predominant sense.
We applied our system on the English data set
of SemEval-2 specific domain WSD task. This
task is an all word WSD task in the environ-
mental domain. We employed the domain back-
ground raw text provided by the task organizer as
well as the English WordNet 3.0 (Fellbaum, 1998)
and the English-Spanish parallel corpus from Eu-
roparl (Koehn, 2005).
This paper is organized as follows. Section 2
introduces how to rank word senses. Section 3
presents how to obtain the most related words of
the target words. We describe the system settings
in Section 4 and offer some discussions in Sec-
tion 5.
2 Word Sense Ranking
In our method, word senses are ranked according
to their prevalence score in the specific domain.
According to the assumption of McCarthy et al
(2007), the prevalence score is affected by the fol-
lowing two factors: (1) The relatedness score be-
tween a given sense of the target word and the
target word?s neighbor word. (2) The similarity
between the target word and its neighbor word.
In addition, we add another factor, (3) the impor-
tance of the neighbor word in the specific domain.
In this paper, ?neighbor words? means the words
which are most semantically similar to the target
word.
Figure 2 illustrates the word sense ranking pro-
cess of noun backbone. The contribution of a
neighbor word to a given word sense is measured
by the similarity between them and weighted by
the importance of the neighbor word in the tar-
get domain and the relatedness between the neigh-
bor word and the target word. Sum up the con-
tributions of each neighbor words, and we get the
prevalence score of the word sense.
Formally, the prevalence score of sense s
i
of a
target word w is assigned as follows:
ps(w, s
i
) =
?
n
j
?N
w
rs(w, n
j
) ? ns(s
i
, n
j
) ? dw(n
j
)
(1)
where
ns(s
i
, n
j
) =
sss(s
i
, n
j
)
?
s
i
?
?senses(w)
sss(s
i
?
, n
j
)
, (2)
sss(s
i
, n
j
) = max
s
x
?senses(n
j
)
sss
?
(s
i
, s
x
). (3)
rs(w, n
j
) is the relatedness score between w and
a neighbor word n
j
. N
w
= {n
1
, n
2
, . . . , n
k
}
is the top k relatedness score neighbor word set.
ns(s
i
, n
j
) is the normalized form of the sense sim-
ilarity score between sense s
i
and the neighbor
word n
j
(i.e. sss(s
i
, n
j
)). We define this score
with the maximum WordNet similarity score be-
tween s
i
and the senses of n
j
(i.e. sss
?
(s
i
, n
j
)).
In our system, lesk algorithm is used to measure
the sense similarity score between word senses.
408
Figure 3: Finding the neighbor words of noun backbone
The similarity of this algorithm is the count of
the number of overlap words in the gloss or the
definition of the senses (Banerjee and Pedersen,
2002). The domain importance weight dw(n
j
) is
assigned with the count of n
j
in the domain back-
ground corpus. For the neighbor word that does
not occur in the domain background text, we use
the add-one strategy. We will describe how to ob-
tain n
j
and rs in Section 3.
3 Thesaurus Construction
The neighbor words of the target word as well as
the relatedness score are obtained by extracting
paraphrases from bilingual parallel texts. When
a word is translated from source language to tar-
get language and then translated back to the source
language, the final translation may have the same
meaning to the original word but with different ex-
pressions (e.g. different word or compound). The
translation in the same language could be viewed
as a paraphrase term or, at least, related term of the
original word.
For example, in Figure 3, English noun back-
bone can be translated to columna, columna verte-
bral, pilar and convicciones etc. in Spanish, and
these words also have other relevant translations
in English, such as vertebral column, column, pil-
lar and convictions etc., which are semantically re-
lated to the target word backbone.
We use a statistical machine translation sys-
tem to calculate the translation probability from
English to another language (called as pivot lan-
guage) as well as the translation probability from
that language to English. By multiplying these
two probabilities, we get a paraphrase probabil-
ity. This method was defined in (Bannard and
Callison-Burch, 2005).
In our system, we choose the top k paraphrases
as the neighbor words of the target word, which
have the highest paraphrase probability. Note that
there are two directions of the paraphrase, from
target word to its neighbor word and from the
neighbor word to the target word. We choose
the paraphrase score of the former direction as
the relatedness score (rs). Because the higher
of the score in this direction, the target word is
more likely paraphrased to that neighbor word,
and hence the prevalence of the relevant target
word sense will be higher than other senses. For-
mally, the relatedness score is given by
rs(w, n
j
) =
?
f
p(f |w)p(n
j
|f), (4)
where f is the pivot language word.
We use the English-Spanish parallel text from
Europarl (Koehn, 2005). We choose Spanish as
the pivot language because in the both directions
the BLEU score of the translation between English
and Spanish is relatively higher than other English
and other languages (Koehn, 2005).
4 Data set and System Settings
The organizers of the SemEval-2 specific domain
WSD task provide no training data but raw back-
ground data in the environmental domain. The En-
glish background data is obtained from the offi-
cial web site of World Wide Fund (WWF), Euro-
pean Centre for Nature Conservation (ECNC), Eu-
ropean Commission and the United Nations Eco-
nomic Commission for Europe (UNECE). The
size of the raw text is around 15.5MB after sim-
ple text cleaning. The test data is from WWF and
ECNC, and contains 1398 occurrence of 436 tar-
get words.
For the implementation, we used bpos (Shen et
al., 2007) for the POS tagging. The maximum
409
number of the neighbor word of each target word k
was set to 50. We employed Giza++
1
and Moses
2
to get the phrase table from the bilingual paral-
lel corpus. TheWordNet::Similarity package
3
was
applied for the implement of the lesk word sense
similarity algorithm.
For the target word that is not in the polysemous
word list, we use the MFS from WordNet as the
back-off method.
5 Discussion and Future Work
The recall of our system is 43.5%, which is lower
than that of the MFS baseline, 50.5% (Agirre et
al., 2010). The baseline uses the most frequent
sense from the SemCor corpus (i.e. the MFS of
WordNet). This means that for some target words,
the MFS from SemCor is better than the domain
MFS we estimated in the environmental domain.
In the future, we will analysis errors in detail to
find the effects of the domain on the MFS.
For the domain specific task, it is better to use
parallel text in the domain of the test data in our
method. However, we didn?t find any available
parallel text in the environmental domain yet. In
the future, we will try some parallel corpus acqui-
sition techniques to obtain relevant corpus for en-
vironmental domain for our method.
Acknowledgments
This work was supported by National Natural
Science Foundation of China (NSFC) via grant
60803093, 60975055, the ?863? National High-
Tech Research and Development of China via
grant 2008AA01Z144, and Natural Scientific Re-
search Innovation Foundation in Harbin Institute
of Technology (HIT.NSRIF.2009069).
References
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Shu kai Hsieh, Maurizio Tesconi, Mon-
ica Monachini, Piek Vossen, and Roxanne Segers.
2010. Semeval-2010 task 17: All-words word sense
disambiguation on a specific domain. In Proceed-
ings of the 5th International Workshop on Semantic
Evaluations (SemEval-2010), Association for Com-
putational Linguistics.
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted lesk algorithm for word sense disambigua-
tion using wordnet. In CICLing ?02: Proceedings
1
http://www.fjoch.com/GIZA++.html
2
http://www.statmt.org/moses/
3
http://wn-similarity.sourceforge.net/
of the Third International Conference on Compu-
tational Linguistics and Intelligent Text Processing,
pages 136?145, London, UK. Springer-Verlag.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL
?05: Proceedings of the 43rd Annual Meeting on As-
sociation for Computational Linguistics, pages 597?
604, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In The Tenth Ma-
chine Translation Summit, Phuket, Thailand.
Rob Koeling, Diana McCarthy, and John Carroll.
2005. Domain-specific sense distributions and pre-
dominant sense acquisition. In Proceedings of Hu-
man Language Technology Conference and Confer-
ence on Empirical Methods in Natural Language
Processing, pages 419?426, Vancouver, British
Columbia, Canada, October. Association for Com-
putational Linguistics.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2007. Unsupervised acquisition of pre-
dominant word senses. Computational Linguistics,
33(4):553?590, December.
G. A. Miller, C. Leacock, R. Tengi, and R. Bunker.
1994. A semantic concordance. In Proc. ARPA
Human Language Technology Workshop ?93, pages
303?308, Princeton, NJ, March. distributed as Hu-
man Language Technology by San Mateo, CA: Mor-
gan Kaufmann Publishers.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. Semeval-2007 task 07: Coarse-
grained english all-words task. In Proceedings of the
Fourth International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 30?35, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Rada Mihalcea and Phil
Edmonds, editors, Senseval-3: Third International
Workshop on the Evaluation of Systems for the Se-
mantic Analysis of Text, pages 41?43, Barcelona,
Spain, July. Association for Computational Linguis-
tics.
410
CMDMC: A Diachronic Digital Museum of Chinese Mandarin
Hou Min1, Zou Yu1, Teng Yonglin1, He Wei
Wang Yan
1
1,2, Liu Jun1,2, and Wu Jiyuan1,2
1
Monitoring and Research Center at Communication University of China
Broadcast Media Language Branch, National Language Resources
2
Beijing 100024, China
School of Literature, Communication University of China
{houmin, zouiy, tengyonglin, hewei}@cuc.edu.cn?
forget1812@sina.com, {aaa_0119, wjy__00}@163.com
Abstract
Modern Chinese Mandarin has gone 
through near a hundred years, it is very 
important to store its representative 
sample in digital form permanently. In 
this paper, we propose a Chinese Man-
darin Digital Multi-modal Corpus 
(CMDMC), which is a digital speech 
museum with diachronic, opened, cross-
media and sharable features. It has over 
3460 hours video and audio files with 
metadata tagging. The materials, which 
were generated by the authoritative 
speakers (e.g. announcers at TV or radio 
station) with normality, are required 
samples if we can get them. Based on 
this resource, we also intend to analyze 
the syntactic correlations of prosodic 
phrase in broadcasting news speech, and 
compare the phonetic and prosodic fea-
tures in movie dialogues among several 
same-name movies in different histori-
cal eras.
1 Introduction
Modern Chinese Mandarin has gone through 
near a hundred years. As language changes as 
society develops, Mandarin must be periodically 
marked with the different features of different 
historical eras. It is very important to design and 
construct a Chinese Mandarin Digital Multi-
modal Corpus (CMDMC), and store its repre-
sentative sample in digital form permanently.
It?s international trend to establish large-scale 
natural language corpus, and many countries 
pay more attention to research and preserve 
their national language. For instance, the Lin-
guistic Data Consortium (LDC) is an open con-
sortium of universities, companies and govern-
ment research laboratories. It creates, collects 
and distributes speech and text databases, lex-
icons, and other resources for research and de-
velopment purposes1. Since its foundation, the 
LDC has delivered data to 197 member institu-
tions and 458 non-member institutions. Moreo-
ver, European Language Resources Association 
(ELRA)2
The paper is organized as follows: Section 2 
describes the resources and data processing of 
our CMDMC. The experiment and evaluation is 
designed and carried out in section 3. Section 4 
is dedicated to analyze the syntactic correlations 
of prosodic phrase in broadcast news speech on 
CNR (China National Radio), and compare the 
is the driving force to make available 
the language resources for language engineering 
and to evaluate language engineering technolo-
gies. In order to achieve this goal, ELRA is ac-
tive in identification, distribution, collection, 
validation, standardization, improvement, in 
promoting the production of language resources, 
in supporting the infrastructure to perform eval-
uation campaigns and in developing a scientific 
field of language resources and evaluation. In
this paper, we intend to establish the CMDMC 
with the goal of showing the history of the de-
velopment of Chinese Mandarin, and represen-
tation the real character in different historical 
eras.
1 The Linguistic Data Consortium (LDC),
http://www.ldc.upenn.edu.
2 European Language Resources Association (EL-
RA), http://www.elra.info/.
phonetic and prosodic features in movie dialo-
gues. Finally, some conclusions and outlines of 
our future work are given in section 5.
2 General Description of CMDMC
In order to show the history of the development 
of Chinese Mandarin, and representation the 
real character in different historical periods, the 
CMDMC, which is a dynamic miniature model 
(or speech museum) with diachronic, opened, 
cross-media and sharable features, is designed
and constructed by Broadcast Media Language 
Branch of National Language Resources 
Monitor & Research Center at Communication 
University of China.
In China, announcers in Radio & TV stations, 
as well as movie or stage actors, are the authori-
ty of the national language standardization. 
Therefore, the speech in radio, television and 
movie can be taken as the paradigm and repre-
sentative of Mandarin. They can reflect the 
phonetic situation of that era. All of these are 
the source of the sample data for CMDMC.
2.1 Description of Resources
In order to fully demonstrate the development 
of Chinese Mandarin by the past 100 years, we 
try to collect all the video or audio materials in 
different periods. Therefore, a state-of-the-art 
classification is defined based on the corpora 
that we got.
Language styles: According to characteristic 
speaking styles of different media, there are 
three categories was defined, such as broadcast 
media language, movie or drama dialogue, and 
the dialogue in folk art (e.g. xiangsheng, ping-
shu etc.) and so on. To sum up, the three speak-
ing styles accounted for about 64.9%, 27.2%
and 7.9% of total corpora, respectively.
Mediums: The materials can be divided into 
audio, video, text and image/picture. The audio 
or video files are the main materials in our cor-
pus, and the aligned texts are transcribed based 
on the audio or video. The documents of image 
are subsidiary corpora.
Historical eras: Based on the characteristics 
of social and language changes, we also define 
six historical stages of Chinese Mandarin: 1) 
Before1949 (or 1919-1949), it is a theoretical
stage for corpora collection. In fact, the earliest 
speech materials, which we can collect, is re-
leased in 1932; 2) 1949-1965; 3) 1966-1977; 4) 
1978-1989; 5) 1990-1999; 6) 2000 to today.
Table 1 shows the distribution of detailed data
in different eras.
Eras
Broadcast 
media
(hours)
Movie 
/drama
(hours)
Folk 
art
(hours)
Percent
of total
(%)
1932-49 39.3 1.1 
1949-65 5.2 191.4 20 6.2 
1966-77 17.5 93.0 3.2 
1978-89 52.4 145.9 75.5 7.9 
1990-99 43.5 137.5 11.5 5.6 
2000-- 2131.5 337.0 167.1 76.0 
Total 2250.1 944.1 274.1
Table 1: The distribution of video and audio 
materials in different eras.
2.2 Data Processing
The data processing includes metadata tagging, 
text transcription and aligning, phonetic and 
prosodic annotation, POS and syntactic tagging
and so on.
As for labeling prosodic phrase boundaries, 
we strictly dependent on the prosodic criteria 
and perception by using the wave files and their 
transcriptions, which use many prosodic fea-
tures such as F0 contour, energy contour etc. At 
the same time, some spoken phenomena are 
considered.
3 Experiment and Evaluation
Firstly, in order to investigate the correlations 
between prosody and syntax, about 13 hours 
speech materials were selected to segment and 
label, including break index, stress index and 
summary of emotional tendentiousness etc.
Before the real annotation, six transcribers have 
been trained in accordance with the prosodic 
labeling conventions, until a high consistency of 
prosodic annotation can be carried out.
According to above experiment and annota-
tion, the number of occurrences of the various 
boundaries was calculated in table 2.
Secondly, we also designed a perception ex-
periment to determine phonetic diversification
for elimination as much as possible the subjec-
tivity which could be caused by the different 
personal intuition of language. Ten people at-
tended the perception experiment of this study:
3 men and 7 women. The average age is 25 
years. Nearly all of them were graduates major-
ing in linguistics. During the experiment, the 
participants were asked to discriminate 12 para-
graphs of random materials and judge the natu-
ralness, pitch, and speech rate of the sentences 
produced in each paragraph. These 12 para-
graphs consisted of 4 from 21 paragraphs of the 
1995 version, 4 from 21 paragraphs of the 1975 
version and 4 from modern materials. 
Boundaries
Types Index Marker Frequency
PW 1 /1, /1+ 55237
PP 2 /2 28867
C-PP 2 /2* 5976
IP 3 /3 7147
IG 4 /4 2781
MEC 5 /5 1770
Table 2: Distribution of all boundaries. The PW, 
PP, C-PP, IP, IG and MEC are the abbreviation of 
prosodic word, normal prosodic phrase, complex 
prosodic phrase, intonational phrase, intonational 
group and meaning expression cluster respectively.
In the perceptive procedure, we disordered all 
these materials for experiment, and three choic-
es were given to these ten people: 1) natural, in 
conformity with the standard of modern Manda-
rin; 2) fairly natural, close to the standard of 
modern Mandarin; 3) unnatural, a little stagy. 
Every paragraph was released twice with an 
interval of 10 seconds. After one hour of conti-
nuous work, a 10-minute break was given.
Only the results with at least a 90% agree-
ment rate were considered for analysis. 
4 Related Works
Based on this resource, we intend to analyze the 
syntactic correlations of prosodic phrase in 
broadcasting news speech on CNR, and com-
pare the phonetic and prosodic features in 
movie dialogues among several same-name 
movies in different historical eras.
4.1 Correlation between Syntax & Prosody
In English, there is a strong correlation between 
prosodic phrase boundaries and syntactic phrase 
boundaries (Price et al 1991). That is to say, 
prosodic phrase boundaries can play an impor-
tant role in understanding utterance as punctua-
tion marks do in written language. An investiga-
tion propose that boundary strength according to 
the measure, which the boundary strength is 
applied to syntactic structures and the phrase 
structure is viewed as an immediate constituen-
cy tree exclusively, corresponds much more 
closely to empirical prosodic boundary strength 
than does syntactic boundary strength according 
to a standard measure (Abney, 1992). In Greek, 
some study indicated that prosodic phrasing has 
a 95% identification rate, and a major effect on 
final tonal boundaries (Botinis et al 2004).
In Chinese, some researchers also proposed a 
statistical model to predict prosodic words from 
lexical words. In their model, both length of the 
word and the tagging from POS are two essen-
tial features to predict prosodic words, and the 
results showed approximately 90% of prediction 
for prosodic words (Chen at el. 2004).
What the correlation between syntax and 
prosody is in Chinese broadcasting news speech?
In order to investigate the syntactic correlations 
of prosodic phrase in real read speech on radio, 
we chose the representative speech materials 
from Xinwen he Baozhi Zhaiyao (News and 
Newspapers Summary) from CMDMC, which is 
a very famous broadcast news program of CNR.
This news program contains more syntactic, 
semantic and prosodic information, speaking
styles and high quality voice in real context. 
Therefore, 908 programs, which contain 454 
hours speech data from January 2006 to June 
2008, were selected for pre-processing. After 
the pre-processing step, we selected two fe-
male?s 13 hours speech materials (one female 
announcer?s material forms the main data, and 
another one?s is supplemented for comparable 
data) as a core database, which segmentation, 
transcription and prosodic annotation (including 
break index, stress index and summary of emo-
tional tendentiousness etc) was made by six 
transcribers. 
According to the characteristic of broadcast-
ing news speech, a new prosodic hierarchical 
structure (Zou et al 2009) and two different 
types of prosodic phrase (i.e. the normal prosod-
ic phrase and the complex prosodic phrase) 
boundaries were defined and used in our data 
labeling.
Top pitch value Bottom pitch value
Categories Location N SD Mean N SD Mean
PW Left 3478 3.917 16.1 3253 4.761 8.5
Right 3701 4.894 14.7 3165 5.457 9.9
PP Left 1741 3.891 14.7 1718 4.302 6.2
Right 627 3.481 16.5 492 5.077 9.3
C-PP Left 314 4.085 13.5 317 4.135 4.8
Right 361 3.616 17.9 285 5.092 10.0
IP Left 536 4.817 12.9 456 5.575 3.9
Right 531 3.019 18.8 473 3.720 13.8
IG Left 211 4.363 11.4 203 6.055 4.7
Right 229 2.377 19.4 185 2.927 15.0
MEC Left 104 4.238 8.1 95 4.937 2.6
Right 22 2.178 18.7 12 2.893 16.2
Table 3: The distribution of pitch on different boundaries. The phonetic acoustic data of each 
syllable was extracted by Praat script, and the foundational frequency was normalized by semi-
tones, the normalization formula is ST=12*log (F0/Fref)/log2 (the female?s reference frequency 
is 100Hz). (?top? is the mean of the highest pitch value at the first tone and the fourth tone; 
?bottom? is the mean of the lowest pitch value at the third tone and the fourth tone; ?N? refers 
the number of samples; ?SD? is the abbreviation of standard deviation)
In the further step, we selected 100 minutes 
speech materials from core annotated data, and 
investigated its features of pitch and duration at 
boundary (Zou et al 2010). The detailed data 
are shown in table 3 and 4 respectively.
Boundaries
Types Marker N Mean SD
PW /1 or/1+ 118 65.2 61.714
PP /2 659 97.6 84.140
C-PP /2* 193 108.7 82.483
IP /3 877 343.2 138.906
IG /4 375 699.2 254.287
MEC /5 31 771.0 208.580
Table 4: The mean of silent pause duration at
boundaries.
There are two ways of representation to pitch 
feature at prosodic boundary: Firstly, the pitch 
contour is un-continuity; secondly, the pitch 
resetting of the declination contour (de Pijper et 
al 1994). According to Table 3, we can find that 
there is a few resetting of bottom pitch value at 
PW boundary, that is to say, the bottom of the
PW boundary right is 1.4 semitones higher than 
that of its left. At other boundaries, the bottom 
pitch values at right side are much higher than 
that at left side, for instance, there is 3.1, 5.2, 
9.9, 11.3 and 13.6 semitones resetting from PP 
to MEC boundary successively. Especially, at 
the IP boundary its resetting has about two 
times than that of C-PP boundary. This shows 
that there are very obvious prosodic feature at 
various boundaries in broadcasting news speech.
Generally, we know that 90ms is the floor of 
threshold for perceiving the silent pause. From 
Table 4, the mean of silent pause duration from 
long to short followed by MEC > IG > IP > C-
PP > PP > PW. Except there is no perceived 
silent pause at PW boundary, the other bounda-
ries have obvious silent pause that can be per-
ceived. The length of silent pause at PP and C-
PP are 97.6ms and 108.7ms respectively, and 
the length at IP has over three times longer than 
that at C-PP. According to this, we propose that 
the PP and C-PP lie in the same position at the 
prosodic hierarchical structure, and the C-PP is 
a special prosodic phrase.
From our core data we got 6728 C-PPs. Ac-
cording to the C-PP that contains the number of 
PW, we divided them into four categories, such 
as three-PW, four-PW, five-PW and six-PW. 
The distribution of them is shown in Table 5.
After preliminary analysis we found that the 
C-PP, which contains three PWs, has a simple 
syntactic structure although it is absolute major-
ity in the number, and that is compose of four 
PWs should be done for correlations of prosody 
and syntax. There are about 6 types of prosodic 
structure if the C-PP contains four PWs. The 
detail data of this type C-PP followed in table 6.
From the data, we know that the fourth type, 
which is (A+B) +(C+D), is the most, and that is 
composed by (A+B) +C+D is the least in all of 
the six types. Although there are just six types 
of prosodic structure that can be found, there are 
more than 985 syntactic categories in this 1835 
C-PPs. There are 23 types which occur more 
than 10 times, and most of them occur only one 
time. To some extent, it can explain that the 
syntactic structure is more complex than the 
prosodic one.
An example of prosodic and syntactic struc-
tures in the utterance, which is ou1 yang2 yu3 
hang2 yi4 zhi1 shou3 jin3 jin0 bao4 zhu4 lou2 
ti1 de0 lan2 gan1 (Ouyang Yuhang held fast to 
the staircase railing with one hand), is given in 
figure 1. The left side of figure is the prosodic 
structure, and the syntactic one lies at the right 
side.
In figure 1, there is a little difference of jin3 
jin0 bao4 zhu4 lou2 ti1 de0 lan2 gan1 (???
??????) between its prosodic structure 
?A+(B+C+D)? and its syntactic structure ?[VP 
[VP jin3jin0/adv bao4zhu4/v] [NP [AP 
lou2ti1/n de0/u] [NP lan2gan1/n]]]?, but the 
differences between its prosodic and syntactic 
structure are obvious because the jin3 jin0 is 
stressed in speech for semantic expression.
Categories Example Num.
Three-PW ??/1+ ??/1 ??/2* 4433
Four-PW ???/1+ ??/1 ??/1 ??/2* 1835
Five-PW ??/1 ??/2 ??/1 ??/1 ??/2* 414
Six-PW ?/1+ ??/1 ??/2 ??/1 ??/2 ??/2* 46
Total 6728
Table 5: The distribution of four kinds of C-PP
Types Example Num. Percent (%)
A+(B+C)+D ?/1+ ??/1 ??/2 ??/2* 441 24.03
A+(B+C+D) ??/1+ ?/1 ?/1 ???/2* 495 26.98
A+B+(C+D) ??/1+ ??/1+ ??/1 ??/2* 97 5.29
(A+B)+(C+D) ?/1 ??/2 ??/1 ??/2* 529 28.83
(A+B+C)+D ??/1 ??/1 ??/2 ??/2* 259 14.11
(A+B)+C+D ?/1 ??/2 ??/2 ??/2* 14 0.76
Total 1835 100
Table 6: The distribution of prosodic type in C-PP of four-PW
Figure 1: An example of (a) prosodic structure vs. (b) syntactic structure in an utterance: ou1 yang2 
yu3 hang2 yi4 zhi1 shou3 jin3 jin0 bao4 zhu4 lou2 ti1 de0 lan2 gan1 (Ouyang Yuhang held fast to 
the staircase railing with one hand).
Figure 2: The pitch contour of the same utterance.
Figure 2 shows the pitch contour of the same 
utterance. In this utterance, there is a nesting 
structure at jin3 jin0 bao4 zhu4 lou2 ti1 de0 
lan2 gan1 (held fast to the staircase railing)
based on the length of perceived silent pause. 
Furthermore, the pitch declination trend within 
the C-PP is obvious despite small resetting be-
tween zhu4 and lou2. So we suggest that there is 
a stable prosodic pattern within a C-PP in 
broadcasting news speech.
Conversely, what is the correlation between 
the prosody and syntax? From above analysis, 
we know that the conjunction and particle, such
as?(de0), ?(deng3),?(he2), ?(dan4) and so 
on, more likely attached to the end of left struc-
ture or the beginning of right one and form a 
prosodic word. If it has just four lexical words 
including the conjunction or particle they form a 
prosodic word by itself. That is to say, it has 
very great flexibility in prosodic structures for 
conjunctions and particles, such as ? ?
(zhan4)/1+ ??(quan2guo2)/1 ??(shi1di4)/1 
???(mian4ji1 de0)/2* (occupy/1+ country-
wide/1everglade/1 acreage/2*)?, ??(he2)/1 ?
? (she4hui4)/2 ? ? (jiu4zhu4)/1 ? ?
(zhi4du4)/2* (and/1 social/2 assistance/1 sys-
tem/2*)? and so on.
4.2 Diachronic Comparative Phonetic and 
Prosodic Analysis in Movie Dialogues
Which diachronic phonetic changes happened in 
Mandarin by the past 100 years? We also ana-
lyze and compare the phonetic features of Chi-
nese Mandarin among several same-name mov-
ies in different historical eras from CMDMC
(Wang et al 2010). In order to minimize the 
divergence of the variables and maximize the 
reliability of conclusions, we chose two pairs of 
same-name movies screened in different histori-
cal periods. These movies are: Pingyuan Youji-
dui (The Plains Guerrillas) shot in 1955 and 
1975, Dujiang Zhencha Ji (Reconnaissance 
across the Yangtze River) shot in 1954 and 1974 
respectively.
Pitch Feature: In the analysis of pitch, we 
put aside the stresses and the neutral tone syl-
lables, and make the statistical investigations on 
the top pitch value and the bottom pitch value of 
the syllables.
Figure 3: The pitch data of 1955 and 1975 ver-
sion in the Plains Guerrillas. The fundamental 
frequency also was normalized by semitones;
the male?s reference frequency is 50Hz.
Figure 3 shows that the mean of the top pitch 
value in the 1950s? materials is lower than that 
of 1970s?. In the 1955 version, the leading cha-
racter, Speaker A, possesses a mean value of the 
top pitch value which is 20.9 semitones. This 
value is lower than that of 1975s? by a differ-
ence of 0.9 semitones. The negative character, 
Speaker B, has a mean value of the top pitch 
value which is 24.5 semitones in the 1955 ver-
sion. The value in the 1975 version is 27 semi-
tones, with a difference of 2.5 semitones left, 
also showing that the value in the 1975 version 
is comparatively high. Comparing the data of 
the bottom pitch value in the 1955 version with 
that in the 1975 version, we know that these 
data seem closer than the top pitch value, but 
still the higher ones belong to the 1975 version. 
That the bottom pitch value is higher tells us 
that the whole pitch register is raised.
Furthermore, we can easily see from Figure 3
that the pitch range of the same character in the 
1975 version is wider. Speaker A of the 1955 
version has a pitch range of 4.8 semitones. In 
contrast, the same character in the 1975 version 
has a pitch range of 6 semitones. Speaker C of
the 1955 version has 4 semitones pitch range, 
but in the 1975 version, he has 5.9 semitones 
pitch range. The gap between them is 1.9 semi-
tones. Through this comparison, we find that the 
pitch range in the 1975 version is wider than 
that in the 1955 version in the whole. 
To some extent, the speaking, both the top
pitch value and the bottom pitch value in the 
1975 version are higher. This proves that, on the 
whole, the pitch of the 1970s? materials is high-
er and more unnatural than that of 50s? because 
of the effect by the Cultural Revolution era.
And this also proves the feeling of the partici-
pants in the perceptional experiment at section 3 
about the 1970s? materials, that is, the 1970s?
Mandarin has a loud and sonorous voice; the 
characters pronounce harder; the general pitch is 
higher.
Duration feature: In the respect of duration, 
we also compared and analyzed the presenters?
speech on TV in 20053
According to table 7, there is a little differ-
ence of the durations mean among them (fol-
lowing four tones), especially it?s very closely 
between the 1975 and the 2005, and those of the 
1975 version are a few longer than those of the 
1955 version. But, except the first tone (Sig. 
=.077), the differences of the duration means 
between the others, which is in the 1955, the 
with the materials ex-
tracted from the movie dialogues the 1955 and 
the 1975. Table 7 is the relevant data.
3 In this work, we just chose the male?s speech data 
from Zou (2007).
1975 and the 2005, are significant (Sig. 
=.000, .000, .002?.05 respectively).
mean SD N
Movie:1955 T1 153.6 69.5 243
T2 136.8 58.1 242
T3 132.8 58.7 321
T4 133.5 52.0 539
Movie:1975 T1 177.8 72.1 258
T2 155.5 52.0 263
T3 152.5 57.6 289
T4 156.7 59.9 505
TV: 2005 T1 163.1 65.7 1471
T2 156.0 66.5 1743
T3 156.8 67.6 1054
T4 145.9 62.3 2652
Table 7: The duration mean of four tones in 
movie dialogues (1955 and 1975) vs. that of
presenters? spoken language on TV in 2005(ms).
Demonstrations of the four-syllable pro-
sodic words: The comparative pitch contour of 
two four-syllable prosodic words, which are 
?bu2 yao4 lu4 mian4? (don?t appear) and ?gan4
shen2 me0 de0? (What are you doing?), are 
shown in Figure 4 and 5, respectively.
Figure 4: The pitch contour of ?bu2 yao4 lu4
mian4? (don?t appear)
Figure 5: The pitch contour of ?gan4 shen2 me0
de0? (What are you doing?)
By observing the above two figures, we find 
that the pitch contour of the 1975 and that of the 
1955 are almost identical except the latter is 
always lower than the former. This may explain 
that although the Mandarin has gone through a 
hundred years, the pitch pattern is relatively 
stable.
5 Conclusions and Future Work
This paper proposes to design a Chinese 
Mandarin Digital Multi-modal Corpus
(CMDMC). Through this corpus, the historical 
trace of Mandarin development can be followed;
the fresh and alive data and material resources 
can be drawn up for the modern researchers and 
successors. We also intend to analyze the syn-
tactic correlations of prosodic phrase in broad-
casting news speech, and compare the phonetic 
and prosodic features in movie dialogues among 
several same-name movies in different histori-
cal eras. The contributions are as follows.
Firstly, the syntactic structure is more com-
plex than the prosodic structure, some conjunc-
tion and particle, such as de0, deng3, he2, dan4
and so on, more likely attached to the end of left 
structure or the beginning of right one and form 
a prosodic word, if the number of lexical words 
mismatch the prosodic words. Otherwise, they 
have almost similar structure.
Secondly, the speech of 1970s in last century 
is greatly influenced by the special era. People 
usually use exaggerated voice, pronounce hard 
and raise the pitch unnaturally, giving others a 
taste of lecturing and ordering. In contrast, the 
speech of Mandarin in 1950s is more natural 
and close to the daily life pronunciation and 
intonation. Even so, the pitch patterns have no 
big changes, and this may explain that the pitch
patterns are comparatively stable in Chinese 
Mandarin.
Future research will include treatment of cor-
relation between syntax and prosody within IP
or IG, ideally comparing the diachronic phonet-
ic or prosodic changes in Mandarin by the past 
100 years. Additionally, we would like to tackle 
the problem of data management, update and 
periodical increasing as time passes.
6 Acknowledgements
This work was supported by the Department of 
Science and Technology at Ministry of Educa-
tion (No. 107118), and ?211? Key Projects of 
Communication University of China (No. 
21103010105, 21103010106). We would like to 
thank the anonymous reviewers for their in-
sightful comments.
References
Abney, S. 1992. Prosodic Structure, Performance 
Structure and Phrase Structure. Proceedings of 5th 
Darpa Workshop on Speech & Natural Language.
Botinis, A., Ganetsou, S., Griva, M., and Bizani, H.
2004. Prosodic Phrasing and Syntactic Structure 
in Greek. Proceedings of FONETIK 2004, Dept. 
of Linguistics, Stockholm University.
Chen, Keh-jiann, Tseng, Chiu-yu, Peng, Hua-jiu and 
Chen, Chi-ching. 2004. Predicting Prosodic 
Words from Lexical Words -- A First Step to-
wards Predicting Prosody from Text . Proceed-
ings of the 4th International Symposium on Chi-
nese Spoken Language Processing (ISCSLP 2004).
Hong Kong, 173-176.
de Pijper, J. R., and Sanderman, A. A. 1994. On the 
Perceptual Strength of Prosodic Boundaries and 
its Relation to Suprasegmental Cues. Journal of 
the Acoustical Society of America, 96(4), 2037-
2047.
Price, P., Ostendorf, M., Shattuck-Hufnagel, S., and 
Fong, C. 1991. The Use of Prosody in Syntactic 
Disambiguation. Journal of the Acoustic Society 
of American, 90, 2956-2970.
Wang Yan, Liu Jun, Kan Minggang, Hou Min, Zou 
Yu. 2010. Phonetic Diachronic Diversification in 
Mandarin: A Case of the Same Movie?s Dialogue 
in 1950s and 1970s. Proc. of YWCL 2010, Wuhan, 
Hubei, Oct 10-13. (Accepted)
Zou Yu. 2007. A Formal Study on Prosody of Pre-
senter's Spoken Language Based on Broadcast 
Speech Corpus. PhD thesis, Communication Uni-
versity of China.
Zou Yu, He Wei, Zhang Yuqiang, Hou Min and Zhu 
Weibin. 2009. A Special Prosodic Phrasing in 
Broadcasting News Programs. Computational 
Sciences and Optimization: Theory, Simulation 
and Experiment (Vol. 2), Sanya, Hainan, China, 
24-26 April, 406-408.
Zou Yu, Wu Jiyuan, He Wei, Hou Min, Teng Yon-
glin. 2010. Syntactic Correlations of Prosodic 
Phrase in Broadcasting News Speech. The 6th 
IEEE International Conference on Natural Lan-
guage Processing and Knowledge Engineering 
(NLP-KE 2010), Beijing, China, Aug. 21-23.
