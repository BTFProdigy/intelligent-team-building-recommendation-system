Language Model Adaptation for Statistical Machine Translation 
with Structured Query Models 
Bing Zhao    Matthias Eck     Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
{bzhao, matteck, vogel+}@cs.cmu.edu 
 
 
Abstract 
We explore unsupervised language model 
adaptation techniques for Statistical Machine 
Translation.  The hypotheses from the 
machine translation output are converted into 
queries at different levels of representation 
power and used to extract similar sentences 
from very large monolingual text collection.  
Specific language models are then build from 
the retrieved data and interpolated with a 
general background model.  Experiments 
show significant improvements when 
translating with these adapted language 
models. 
1 Introduction 
Language models (LM) are applied in many 
natural language processing applications, such as 
speech recognition and machine translation, to 
encapsulate syntactic, semantic and pragmatic 
information.  For systems which learn from given 
data we frequently observe a severe drop in 
performance when moving to a new genre or new 
domain.  In speech recognition a number of 
adaptation techniques have been developed to cope 
with this situation.  In statistical machine 
translation we have a similar situation, i.e. estimate 
the model parameter from some data, and use the 
system to translate sentences which may not be 
well covered by the training data.  Therefore, the 
potential of adaptation techniques needs to be 
explored for machine translation applications. 
Statistical machine translation is based on the 
noisy channel model, where the translation 
hypothesis is searched over the space defined by a 
translation model and a target language (Brown et 
al, 1993).  Statistical machine translation can be 
formulated as follows: 
)()|(maxarg)|(maxarg* tPtsPstPt
tt
?==  
where t is the target sentence, and s is the source 
sentence. P(t) is the target language model and 
P(s|t) is the translation model.  The argmax 
operation is the search, which is done by the 
decoder. 
In the current study we modify the target 
language model P(t), to represent the test data 
better, and thereby improve the translation quality.  
(Janiszek, et al 2001) list the following approaches 
to language model adaptation: 
? Linear interpolation of a general and a domain 
specific model (Seymore, Rosenfeld, 1997). 
? Back off of domain specific probabilities with 
those of a specific model (Besling, Meier, 
1995). 
? Retrieval of documents pertinent to the new 
domain and training a language model on-line 
with those data (Iyer, Ostendorf, 1999, 
Mahajan et. al. 1999). 
? Maximum entropy, minimum discrimination 
adaptation (Chen, et. al., 1998). 
? Adaptation by linear transformation of vectors 
of bigram counts in a reduced space (DeMori, 
Federico, 1999). 
? Smoothing and adaptation in a dual space via 
latent semantic analysis, modeling long-term 
semantic dependencies, and trigger 
combinations.  (J. Bellegarda, 2000). 
Our approach can be characterized as 
unsupervised data augmentation by retrieval of 
relevant documents from large monolingual 
corpora, and interpolation of the specific language 
model, build from the retrieved data, with a 
background language model.  To be more specific, 
the following steps are carried out to do the 
language model adaptation.  First, a baseline 
statistical machine translation system, using a large 
general language model, is applied to generate 
initial translations.  Then these translations 
hypotheses are reformulated as queries to retrieve 
similar sentences from a very large text collection.  
A small domain specific language model is build 
using the retrieved sentences and linearly 
interpolated with the background language model.  
This new interpolated language model in applied in 
a second decoding run to produce the final 
translations.  
There are a number of interesting questions 
pertaining to this approach: 
? Which information can and should used to 
generate the queries: the first-best translation 
only, or also translation alternatives. 
? How should we construct the queries, just as 
simple bag-of-words, or can we incorporate more 
structure to make them more powerful. 
? How many documents should be retrieved to 
build the specific language models, and on what 
granularity should this be done, i.e. what is a 
document in the information retrieval process. 
 
The paper is structured as follows:  section 2 
outlines the sentence retrieval approach, and three 
bag-of-words query models are designed and 
explored; structured query models are introduced 
in section 3.  In section 4 we present translation 
experiments are presented for the different query.  
Finally, summary is given in section 5. 
2 LM Adaptation via Sentence Retrieval 
Our language model adaptation is an unsupervised 
data augmentation approach guided by query 
models.  Given a baseline statistical machine 
translation system, the language model adaptation 
is done in several steps shown as follows: 
 
? Generate a set of initial translation 
hypotheses H = {h1 ?hn} for source 
sentences s, using either the baseline MT 
system with the background language 
model or only the translation model 
? Use H  to build query 
? Use query to retrieve relevant sentences 
from the large corpus  
? Build specific language models from 
retrieved sentences 
? Interpolate the specific language model 
with the background language 
? Re-translate sentences s with adapted 
language model 
 
Figure-1: Adaptation Algorithm 
 
The specific language model )|( hwP iA  and the 
general background model )|( hwP iB  are combined 
using linear interpolation: 
)|()1()|()|(? hwPhwPhwP iAiBi ?? ?+=  (1)
The interpolation factor ?  can be simply 
estimated using cross validation or a grid search. 
As an alternative to using translations for the 
baseline system, we will also describe an approach, 
which uses partial translations of the source 
sentence, using the translation model only.  In this 
case, no full translation needs to be carried out in 
the first step; only information from the translation 
model is used.  
Our approach focuses on query model building, 
using different levels of knowledge representations 
from the hypothesis set or from the translation 
model itself.  The quality of the query models is 
crucial to the adapted language model?s 
performance.  Three bag-of-words query models 
are proposed and explained in the following 
sections. 
2.1 Sentence Retrieval Process 
In our sentence retrieval process, the standard tf/idf 
(term frequency and inverse document frequency) 
term weighting scheme is used.  The queries are 
built from the translation hypotheses.  We follow 
(Eck, et al, 2004) in considering each sentence in 
the monolingual corpus as a document, as they 
have shown that this gives better results compared 
to retrieving entire news stories. 
Both the query and the sentences in the text 
corpus are converted into vectors by assigning a 
term weight to each word.  Then the cosine 
similarity is calculated proportional to the inner 
product of the two vectors.  All sentences are 
ranked according to their similarity with the query, 
and the most similar sentences are used as the data 
for building the specific language model.  In our 
experiments we use different numbers of similar 
sentences, ranting from one to several thousand. 
2.2 Bag-of-words Query Models 
Different query models are designed to guide the 
data augmentation efficiently.  We first define  
?bag-of-words? models, based on different levels 
of knowledge collected from the hypotheses of the 
statistical machine translation engine. 
2.2.1 First-best Hypothesis as a Query Model 
The first-best hypothesis is the Viterbi path in the 
search space returned from the statistical machine 
translation decoder.  It is the optimal hypothesis 
the statistical machine translation system can 
generate using the given translation and language 
model, and restricted by the applied pruning 
strategy.  Ignoring word order, the hypothesis is 
converted into a bag-of-words representation, 
which is then used as a query: 
}|),{(),,( 1211 TiiilT VwfwwwwQ ?== L  
where iw is a word in the vocabulary 1TV of the Top-
1 hypothesis. if  is the frequency of iw ?s 
occurrence in the hypothesis.  
The first-best hypothesis is the actual translation 
we want to improve, and usually it captures 
enough correct word translations to secure a sound 
adaptation process.  But it can miss some 
informative translation words, which could lead to 
better-adapted language models.  
2.2.2  N-Best Hypothesis List as a Query Model 
Similar to the first-best hypothesis, the n-best 
hypothesis list is converted into a bag-of-words 
representation.  Words which occurred in several 
translation hypotheses are simply repeated in the 
bag-of-words representations.  
}|),{(
),,;;,,( ,2,1,,12,11,1 1
TNiii
lNNNlTN
Vwfw
wwwwwwQ
N
?=
= LLL   
where TNV  is the combined vocabulary from all n-
best hypotheses and if  is the frequency of iw ?s 
occurrence in the n-best hypothesis list. 
TNQ  has several good characteristics:  First it 
contains translation candidates, and thus is more 
informative than 1TQ .  In addition, the confidently 
translated words usually occur in every hypothesis 
in the n-best list, therefore have a stronger impact 
on the retrieval result due to the higher term 
frequency (tf) in the query.  Thirdly, most of the 
hypotheses are only different from each other in 
one word or two.  This means, there is not so much 
noise and variance introduced in this query model. 
2.2.3 Translation Model as a Query Model 
To fully leverage the available knowledge from the 
translation system, the translation model can be 
used to guide the language model adaptation 
process.  As introduced in section 1, the translation 
model represents the full knowledge of translating 
words, as it encodes all possible translations 
candidates for a given source sentence.  Thus the 
query model based on the translation model, has 
potential advantages over both 1TQ  and TNQ . 
To utilize the translation model, all the n-grams 
from the source sentence are extracted, and the 
corresponding candidate translations are collected 
from the translation model.  These are then 
converted into a bag-of-words representation as 
follows: 
}|),{(
),,;;,,( ,2,1,,2,1, 1111
TMiii
nsssnsssTM
Vwfw
wwwwwwQ
IIII
?=
= LLL   
where is  is a source n-gram, and I is the number of 
n-grams in the source sentence.  jsiw ,  is a candidate 
target word as translation of is .  Thus the 
translation model is converted into a collection of 
target words as a bag-of-word query model. 
There is no decoding process involved to build 
TMQ .  This means TMQ  does not incorporate any 
background language model information at all, 
while both 1TQ  and TNQ  implicitly use the 
background language model to prune the words in 
the query.  Thus TMQ  is a generalization, and 1TQ  
and TNQ  are pruned versions.  This also means TMQ  
is subject to more noise. 
3 Structured Query Models 
Word proximity and word order is closely related 
to syntactic and semantic characteristics.  
However, it is not modeled in the query models 
presented so far, which are simple bag-of-words 
representations.  Incorporating syntactic and 
semantic information into the query models can 
potentially improve the effectiveness of LM 
adaptation. 
The word-proximity and word ordering 
information can be easily extracted from the first-
best hypothesis, the n-best hypothesis list, and the 
translation lattice built from the translation model.  
After extraction of the information, structured 
query models are proposed using the structured 
query language, described in the Section 3.1. 
3.1 Structured Query Language 
This query language essentially enables the use of 
proximity operators (ordered and unordered 
windows) in queries, so that it is possible to model 
the syntactic and semantic information encoded in 
phrases, n-grams, and co-occurred word pairs.  
The InQuery implementation (Lemur 2003) is 
applied.  So far 16 operators are defined in 
InQuery to model word proximity (ordered, 
unordered, phrase level, and passage level).  Four 
of these operators are used specially for our 
language model adaptation: 
Sum Operator: #sum( 1t ? nt ) 
The terms or nodes ( 1t ? nt ) are treated as 
having equal influence on the final retrieval result.  
The belief values provided by the arguments of the 
sum are averaged to produce the belief value of the 
#sum node. 
Weighted Sum Operator: #wsum( 11 : tw , ?) 
The terms or nodes ( 1t ? nt ) contribute 
unequally to the final result according to the 
weight ( iw ) associated with each it .  
Ordered Distance Operator: #N( 1t ? nt ) 
The terms must be found within N words of 
each other in the text in order to contribute to the 
document's belief value.  An n-gram phrase can be 
modeled as an ordered distance operator with N=n. 
Unordered Distance Operator: #uwN( 1t ? nt ) 
The terms contained must be found in any order 
within a window of N words in order for this 
operator to contribute to the belief value of the 
document. 
3.2 Structured Query Models 
Given the representation power of the structured 
query language, the Top-1 hypothesis, Top-N Best 
hypothesis list, and the translation lattice can be 
converted into three Structured Query Models 
respectively. 
For first-best and n-best hypotheses, we collect 
related target n-grams of a given source word 
according to the alignments generated in the 
Viterbi decoding process.  While for the translation 
lattice, similar to the construction of TMQ , we 
collect all the source n-grams, and translate them 
into target n-grams.  In either case, we get a set of 
target n-grams for each source word. The 
structured query model for the whole source 
sentence is a collection of such subsets of target n-
grams. 
},,,{
21 Isssst
tttQ
vLvv=  
is
t
v
is a set of target n-grams for the source word is : 
}}{;},{;},{{ 311211 LLL
v
gramiiigramiigramis ttttttt i ?+??+?=  
In our experiments, we consider up to trigram for 
better retrieval efficiency, but higher order n-grams 
could be used as will.  The second simplification is 
that every source word is equally important, thus 
each n-gram subset 
is
t
v
will have an equal 
contribution to the final retrieval results.  The last 
simplification is each n-gram within the set of 
is
t
v
 
has an equal weight, i.e. we do not use the 
translation probabilities of the translation model.  
If the system is a phrase-based translation system, 
we can encode the phrases using the ordered 
distance operator (#N) with N equals to the number 
of the words of that phrase, which is denoted as the 
#phrase operator in InQuery implementation.  The 
2-grams and 3-grams can be encoded using this 
operator too. 
Thus our final structured query model is a sum 
operator over a set of nodes.  Each node 
corresponds to a source word.  Usually each source 
word has a number of translation candidates 
(unigrams or phrases).  Each node is a weighted 
sum over all translation candidates weighted by 
their frequency in the hypothesis set.  An example 
is shown below, where #phrase indicates the use of 
the ordered distance operator with varying n: 
 
#q=#sum( #wsum(2 eu  2 #phrase(european union) ) 
   #wsum(12 #phrase(the united states) 
1 american 1 #phrase(an american) ) 
   #wsum(4 are 1 is ) 
   #wsum(8 markets  3 market)) 
   #wsum(7 #phrase(the main)  5 primary ) ); 
4 Experiments 
Experiments are carried out on a standard 
statistical machine translation task defined in the 
NIST evaluation in June 2002.  There are 878 test 
sentences in Chinese, and each sentence has four 
human translations as references.  NIST score 
(NIST 2002) and Bleu score (Papineni et. al. 2002) 
of mteval version 9 are reported to evaluate the 
translation quality. 
4.1  Baseline Translation System 
Our baseline system (Vogel et al, 2003) gives 
scores of 7.80 NIST and 0.1952 Bleu for Top-1 
hypothesis, which is comparable to the best results 
reported on this task.  
For the baseline system, we built a translation 
model using 284K parallel sentence pairs, and a 
trigram language model from a 160 million words 
general English news text collection.  This LM is 
the background model to be adapted.  
With the baseline system, the n-best hypotheses 
list and the translation lattice are extracted to build 
the query models.  Experiments are carried out on 
the adapted language model using the three bag-of-
words query models: 1TQ , TNQ  and TMQ , and the 
corresponding structured query models. 
4.2 Data: GigaWord Corpora 
The so-called GigaWord corpora (LDC, 2003) are 
very large English news text collections.  There are 
four distinct international sources of English 
newswire: 
 
AFE Agence France Press English Service 
APW Associated Press Worldstream English Service 
NYT The New York Times Newswire Service 
XIE The Xinhua News Agency English Service 
 
Table-1 shows the size of each part in word counts. 
 
AFE APW NYT XIE 
170,969K 539,665K 914,159K 131,711K 
Table-1: Number of words in the different 
GigaWord corpora 
 
As the Lemur toolkit could not handle the two 
large corpora (APW and NYT) we used only 200 
million words from each of these two corpora. 
In the preprocessing all words are lowercased 
and punctuation is separated.  There is no explicit 
removal of stop words as they usually fade out by 
tf.idf weights, and our experiments showed not 
positive effects when removing stop words. 
4.3 Bag-of-Words Query Models 
Table-2 shows the size of 1TQ , TNQ  and TMQ  in 
terms of number of tokens in the 878 queries: 
 
 1TQ  TNQ  TMQ  
|| Q  25,861 231,834 3,412,512 
Table-2: Query size in number of tokens 
 
As words occurring several times are reduced to 
word-frequency pairs, the size of the queries 
generated from the 100-best translation lists is only 
9 times as big as the queries generated from the 
first-best translations.  The queries generated from 
the translation model contain many more 
translation alternatives, summing up to almost 3.4 
million tokens.  Using the lattices the whole 
information of the translation model is kept.  
4.3.1 Results for Query 1TQ  
In the first experiment we used the first-best 
translations to generate the queries.  For each of 
the 4 corpora different numbers of similar 
sentences (1, 10, 100, and 1000) were retrieved to 
build specific language models.  Figure-2 shows 
the language model adaptation after tuning the 
interpolation factor ?  by a grid search over [0,1]. 
Typically ?  is around 0.80. 
 
1-Best/NIST Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
1-Best/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-2: NIST and Bleu scores 
1TQ  
 
We see that each corpus gives an improvement 
over the baseline.  The best NIST score is 7.94, 
and the best Bleu score is 0.2018.  Both best scores 
are realized using top 100 relevant sentences 
corpus per source sentence mined from the AFE. 
4.3.2 Results for Query TNQ  
Figure-3 shows the results for the query model TNQ .  
The best results are 7.99 NIST score, and 0.2022 
Bleu score.  These improvements are statistically 
significant.  Both scores are achieved at the same 
settings as those in 1TQ , i.e. using top 100 retrieved 
relevant sentences mined from the AFE corpus. 
 
100-Best/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
100-Best/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-3: NIST and Bleu scores from TNQ  
 
Using the translation alternatives to retrieve the 
data for language model adaptation gives an 
improvement over using the first-best translation 
only for query construction.  Using only one 
translation hypothesis to build an adapted language 
model has the tendency to reinforce that 
translation. 
4.3.3 Results for Query TMQ  
The third bag-of-words query model uses all 
translation alternatives for source words and source 
phrases.  Figure-4 shows the results of this query 
model TMQ .  The best results are 7.91 NIST score 
and 0.1995 Bleu.  For this query model best results 
were achieved using the top 1000 relevant 
sentences mined from the AFE corpus per source 
sentence. 
The improvement is not as much as the other 
two query models.  The reason is probably that all 
translation alternatives, even wrong translations 
resulting from errors in the word and phrase 
alignment, contribute alike to retrieve similar 
sentences.  Thereby, an adapted language model is 
built, which reinforces not only good translations, 
but also bad translations. 
All the three query models showed 
improvements over the baseline system in terms of 
NIST and Bleu scores.  The best bag-of-words 
query model is TNQ  built from the N-Best list.  It 
provides a good balance between incorporating 
translation alternatives in the language model 
adaptation process and not reinforcing wrong 
translations. 
 
Lattice/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Lattice/BLEU-Scores
0.1900
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
AFE APW NYT XIE
Top1
Top10
Top100
Top1000
Baseline
 
Figure-4: NIST and Bleu scores from TMQ  
 
4.4 Structured Query Models 
The next series of experiments was done to 
study if using word order information in 
constructing the queries could help to generate 
more effective adapted language models.  By using 
the structured query language we converted the 
same first-best hypothesis, the 100-best list, and 
the translation lattice into structured query models.  
Results are reported for the AFE corpus only, as 
this corpus gave best translation scores. 
Figure-5 shows the results for all three structured 
query models, built from the first-best hypothesis 
(?1-Best?), the 100 best hypotheses list (?100-
Best?), and translation lattice (?TM-Lattice?).  
Using these query models, different numbers of 
most similar sentences, ranging from 100 to 4000, 
where retrieved from the AFE corpus.  The given 
baseline results are the best results achieved from 
the corresponding bag-of-words query models. 
Consistent improvements were observed on 
NIST and Bleu scores.  Again, optimal 
interpolation factors to interpolate the specific 
language models with the background language 
model were used, which typically were in the 
range of [0.6, 0.7].  Structured query models give 
most improvements when using more sentences for 
language model adaptation.  The effect is more 
pronounced for Bleu then for NIST score. 
 
Structured query/NIST-Scores
7.7500
7.8000
7.8500
7.9000
7.9500
8.0000
8.0500
8.1000
8.1500
Baseline Top100 Top500 Top1000 Top2000 Top4000
1-Best
100-Best
TM-Lattice
Structured query/BLEU-Scores
0.1920
0.1940
0.1960
0.1980
0.2000
0.2020
0.2040
0.2060
0.2080
Baseline Top100 Top500 Top1000 Top2000 Top4000
1-Best
100-Best
TM-Lattice
 
Figure-5: NIST and Bleu scores from the  
structured query models 
 
The really interesting result is that the structured 
query model TMQ gives now the best translation 
results.  Adding word order information to the 
queries obviously helps to reduce the noise in the 
retrieved data by selecting sentences, which are 
closer to the good translations,  
The best results using the adapted language 
models are NIST score 8.12 for using the 2000 
most similar sentences, whereas Bleu score goes 
up to 0.2068 when using 4000 sentences for 
language model adaptation. 
4.5 Example 
Table-3 shows translation examples for the 17th 
Chinese sentence in the test set. We applied the 
baseline system (Base), the bag-of-word query 
model (Hyp1), and the structured query model 
(Hyp2) using AFE corpus. 
 
Ref The police has already blockade the scene of the explosion. 
Base At present, the police had cordoned off the explosion. 
Hyp1 At present, police have sealed off the explosion.  
Hyp2 Currently, police have blockade on the scene of the explosion. 
Table-3 Translation examples 
 4.6 Oracle Experiment 
Finally, we run an oracle experiments to see 
how much improvement could be achieved if we 
only selected better data for the specific language 
models. We converted the four available reference 
translations into structured query models and 
retrieved the top 4000 relevant sentences from 
AFE corpus for each source sentence.  Using these 
language models, interpolated with the background 
language model gave a NIST score of 8.67, and a 
Bleu score of 0.2228.  This result indicates that 
there is room for further improvements using this 
language model adaptation technique. 
The oracle experiment suggests that better initial 
translations lead to better language models and 
thereby better 2nd iteration translations.  This lead 
to the question if we can iterate the retrieval 
process several times to get further improvement, 
or if the observed improvement results form using 
for (good) translations, which have more diversity 
than the translations in an n-best list. 
On the other side the oracle experiment also 
shows that the optimally expected improvement is 
limited by the translation model and decoding 
algorithm used in the current SMT system. 
 
5 Summary 
In this paper, we studied language model 
adaptation for statistical machine translation.  
Extracting sentences most similar to the initial 
translations, building specific language models for 
each sentence to be translated, and interpolating 
those with the background language models gives 
significant improvement in translation quality.  
Using structured query models, which capture 
word order information, leads to better results that 
plain bag of words models. 
The results obtained suggest a number of 
extensions of this work:  The first question is if 
more data to retrieve similar sentences from will 
result in even better translation quality.  A second 
interesting question is if the translation 
probabilities can be incorporated into the queries.  
This might be especially useful for structured 
query models generated from the translation 
lattices.  
References  
J. Bellegarda. 2000, Exploiting Latent Semantic 
Information in Statistical Language Modeling. In 
Proceedings of the IEEE, 88(8), pp. 1279-1296. 
S. Besling and H.G. Meier 1995. Language Model 
Speaker Adaptation, Eurospeech 1995, Madrid, 
Spain. 
Peter F Brown., Stephen A Della Pietra., Vincent J. 
Della Pietra and Mercer Robert L., 1993.  The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2), pp. 263?311.  
S.F Chen., K. Seymore, and R. Rosenfeld 1998. Topic 
Adaptation for Language Modeling using 
Unnormalized Exponential Models. IEEE 
International Conference on Acoustics, Speech and 
Signal Processing 1998, Seattle WA.  
Renato DeMori and Marcello Federico 1999. Language 
Model Adaptation, In Computational Models of 
Speech Pattern Processing, Keith Pointing (ed.), 
NATO ASI Series, Springer Verlag.  
Matthias Eck, Stephan Vogel, and Alex Waibel, 2004. 
Language Model Adaptation for Statistical 
Machine Translation based on Information Retrieval, 
International Conference on Language Resources and 
Evaluation, Lisbon, Portugal. 
R. Iyer and M. Ostendorf, 1999. Modeling Long 
Distance Dependence in Language: Topic Mixtures 
vs. Dynamic Cache Models, IEEE Transactions on 
Speech and Audio Processing, SAP-7(1): pp. 30-39. 
David Janiszek, Renato DeMori and Frederic Bechet, 
2001.  Data Augmentation and Language Model 
adaptation, IEEE International Conference on 
Acoustics, Speech and Signal Processing 2001, Salt 
Lake City, UT.  
LDC, Gigaword Corpora. http://wave.ldc.upenn.edu/ 
Catalog/CatalogEntry.jsp?catalogId=LDC2003T05 
Lemur, The Lemur Toolkit for Language Modeling and 
Information Retrieval, http://www.cs.cmu.edu/~ 
lemur/ 
Milind Mahajan, Doug Beeferman and X.D. Huang, 
1999. Improved Topic-Dependent Language 
Modeling Using Information Retrieval Techniques, 
IEEE International Conference on Acoustics, Speech 
and Signal Processing 1999, Phoenix, AZ. 
NIST Report: 2002, Automatic Evaluation of Machine 
Translation Quality Using N-gram Co-Occurrence 
Statistics.  http://www.nist.gov/speech/tests/mt/doc/ 
ngram-study.pdf . 
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei- 
Jing Zhu, 2002. BLEU: A Method for Automatic 
Evaluation of Machine Translation. In Proc of the 
40th Annual Meeting of the Association for 
Computational Linguistics. 2002, Philadelphia, PA. 
Kristie Seymore and Ronald Rosenfeld, 1997. Using 
Story Topics for Language Model Adaptation. In 
Proc. Eurospeech 1997, Rhodes, Greece.  
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Tribble, 
Ashish Venogupal, Bing Zhao, Alex Waibel, 2003. 
The CMU Statistical Translation System, Proceedings 
of MT-Summit IX, 2003, New Orleans, LA. 
 
Improving Statistical Machine Translation in the Medical Domain  
using the Unified Medical Language System 
Matthias Eck 
 
 
 
matteck@cs.cmu.edu 
Stephan Vogel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
vogel+@cs.cmu.edu 
Alex Waibel 
 
 
 
ahw@cs.cmu.edu 
 
Abstract 
Texts from the medical domain are an 
important task for natural language 
processing. This paper investigates the 
usefulness of a large medical database (the 
Unified Medical Language System) for the 
translation of dialogues between doctors and 
patients using a statistical machine translation 
system. We are able to show that the 
extraction of a large dictionary and the usage 
of semantic type information to generalize the 
training data significantly improves the 
translation performance. 
1 Introduction 
Hospitals in the United States have to deal with 
an increasing number of patients who have no 
knowledge of the English language. It is not 
surprising that in this area translation errors can 
lead to severe problems (Neergard, 2003; Flores et 
al. 2003). This is one of the main reasons why the 
medical domain plays an important role in many of 
the current projects involving natural language 
processing. Especially many text or speech 
translation projects include tasks to translate texts 
or dialogues with medical topics.  
The goal of this research was the improvement 
of translation quality in the medical domain using 
a statistical machine translation system. A 
statistical machine translation system deduces 
translation rules from large amounts of parallel 
texts in the source and target language.  
The general approach to gather as much training 
data as possible is usually complicated and 
expensive. So it is necessary to make use of 
already available data and databases and it is 
reasonable to hope that some ideas and special 
methods could actually improve the performance 
in limited domains, like the medical domain.  
The Internet and especially the WWW offers a 
lot of data related to medical topics. Especially 
interesting and promising for us was the Unified 
Medical Language System? (UMLS, 1986-2004) 
available from the US National Library of 
Medicine. It provides a vast amount of information 
concerning medical terms and we extracted 
information from this database to improve an 
existent translation system.  
The paper will first give an introduction into the 
Unified Medical Language system. We will then 
point out which parts could be useful for statistical 
machine translation and later show how the 
baseline system was actually significantly 
improved using this data. 
2 The Unified Medical Language System 
2.1 Introduction 
The Unified Medical Language System (UMLS, 
1986-2004) project was initiated in 1986 by the 
U.S. National Library of Medicine. It integrates 
different knowledge sources into one database (e.g. 
biomedical vocabularies, dictionaries). 
The goal is to help health professionals and 
researchers to use biomedical information from 
these different sources. It is usually updated about 
3 or 4 times per year. 
It consists of three main knowledge repositories, 
the UMLS Metathesaurus, the UMLS Semantic 
Network and the SPECIALIST lexicon. 
Interesting facts about the UMLS, related work 
and further information can be found in 
(Lindbergh, 1990; Kashyap, 2003; Brown et al, 
2003; Friedman et al, 2001; Zweigenbaum et al, 
2003). 
2.2 The UMLS Metathesaurus 
The UMLS Metathesaurus provides a common 
structure for approximately 100 source biomedical 
vocabularies.  
The 2003AB1 version of the Metathesaurus 
contains exactly 900,551 concepts named by 
2,247,457 terms.  It is organized by concept, which 
is a cluster of terms (i.e. synonyms, lexical variants 
                                                    
1
 2003AB was the actual release when the 
experiments described in this paper were executed. The 
most recent version now is 2004AA, which contains 
certain additional and updated information. All numbers 
given in this paper are according to the 2003AB 
version. 
and translations) with the same meaning. 
Translations are present for up to 14 additional 
languages besides English. It is very likely that 
other languages will be added in later releases. 
 
Table 1 shows the distribution of the terms 
according to the 15 different languages. 
 
Language Number of Terms 
English 1860683 
Spanish 73136 
German 71316 
Portuguese 69127 
Russian 44907 
Dutch  38600 
French 38249 
Italian 24992 
Finnish 22382 
Danish 723 
Swedish 723 
Norwegian 722 
Hungarian 718 
Basque 695 
Hebrew 484 
Table 1: Languages in the UMLS 
 
For example the concept ?arm? includes the 
English lexical variant, its plural form, ?arms? and 
with ?bras?, ?arm?, ?braccio?, ?braco?, ?ruka? and 
?brazo? the French, German, Italian, Portuguese, 
Russian and Spanish translations.  
 
Some entries contain case information, too, and 
the entries are not limited to words but some terms 
are also longer phrases like  ?third degree burn of 
lower leg? or ?loss of consciousness?. 
 
It also includes inter-concept relationships 
across the multiple vocabularies. The main 
relationship types are shown in Table 2: 
 
Relationship types 
broader  
narrower 
other related 
like 
parent 
child 
sibling 
is allowed qualifier 
can be qualified by 
is co-occurring with 
Table 2: Relationship types 
 
 
The synonym-relationship is implicitly realized 
by different terms that are affiliated with the same 
concept. 
The co-occurrence relationship refers to 
concepts co-occurring in the MEDLINE-
publications. 
 
In addition each concept is categorized into 
semantic types according to the UMLS Semantic 
Network. 
2.3 The UMLS Semantic Network 
The UMLS Semantic Network categorizes the 
concepts of the UMLS Metathesaurus through 
semantic types and relationships.  
Every concept in the Metathesaurus is part of 
one or more semantic types. 
There are 135 semantic types arranged in a 
generalization hierarchy with the two roots 
?Entity? and ?Event?. This hierarchy is still rather 
abstract (e.g. not deeper than six).  
A more detailed generalization hierarchy is 
realized with the child, parent and sibling 
relationships of the UMLS Metathesaurus. 
 
Figure 1 shows some examples for semantic types. 
 
Entity 
    Physical Object 
        Organism 
        Anatomical Structure 
            Fully Formed Anatomical Structure 
                Body Part, Organ or Organ Component 
        Manufactured Object 
            Medical Device 
                Drug Delivery Device 
            Clinical Drug 
Event 
    Activity 
        Behavior 
            Social Behavior 
        Occupational Activity 
            Health Care Activity 
                Laboratory Procedure 
    Phenomenon or Process 
        Human caused Phenomenon or Process 
Figure 1: Some semantic types 
2.4 The SPECIALIST lexicon 
The SPECIALIST lexicon contains over 30,000 
English words. It is intended to be a general 
English lexicon including many biomedical terms.  
The lexicon entry for each word or term records 
the syntactic, morphological and orthographic 
information. 
 
{base=anesthetic 
spelling_variant=anaesthetic 
entry=E0330018 
        cat=noun 
        variants=reg 
        variants=uncount 
} 
Figure 2: Example entry from the  
Specialist Lexicon 
 
Figure 2 shows the entry for ?anesthetic?. There 
is a spelling variant ?anaesthetic? and an entry 
number. The category in this case is noun (there is 
another entry for ?anesthetic? as an adjective). The 
variants-slot contains a code indicating the 
inflectional morphology of the entry. ?anesthetic? 
can either be a regular count noun (with regular 
plural ?anesthetics?) or an uncountable noun. 
 
3 Machine Translation Experiments 
3.1 The Baseline System 
The Baseline system, which we used to test 
different approaches to improve the translation 
performance, is a statistical machine translation 
system. The task was to facilitate doctor-patient 
dialogues across languages. In this case we chose 
translation from Spanish to English. 
 
The Baseline system was trained using 9,227 
lines of training data (90,012 English words, 
89,432 Spanish words). 3,227 lines of this data are 
?in-domain? data. We collected doctor patient 
dialogues during ongoing research projects in our 
group and used this data as training data. The 
6,000 other lines of training data are out of domain 
data from the C-Star Project. This data also 
consists of dialogues but not from the medical 
domain.  
 
The test data consists of 500 lines with 6,886 
words. The test data was also taken from medical 
dialogues between a doctor and a patient and 
contains a reasonable number of medical terms but 
the language is not very complex. Figure 3 shows 
some example test sentences (from the reference 
data). 
  
 (?) 
Doctor: The symptoms you are describing and 
given your recent change in diet, I believe 
you may be anemic. 
Patient: Anemic? Really? Is that serious? 
Doctor: Anemia can be very serious if left 
untreated. Being anemic means your body 
lacks a sufficient amount of red blood cells 
to carry oxygen through your body. 
 (?) 
Figure 3: Example test sentences (reference) 
 
The Baseline system uses IBM1 lexicon 
transducers and different types of phrase 
transducers (Zhang et al 2003, Vogel et al 1996, 
Vogel et al 2003). The Language model is a 
trigram language model with Good-Turing-
Smoothing built with the SRI-Toolkit (SRI, 1995-
2004) using only the English part of the training 
data. 
 
The Baseline system scores a 0.171 BLEU and 
4.72 NIST. [BLEU and NIST are well known 
scoring methods for measuring machine translation 
quality. Both calculate the precision of a 
translation by comparing it to a reference 
translation and incorporating a length penalty 
(Doddington, 2001; Papineni et al, 2002).] 
3.2 Extracting dictionaries from the UMLS 
The first way to exploit the UMLS database for a 
statistical machine translation system naturally is 
to extract additional Spanish-English lexicons or 
phrasebooks.  
The UMLS Metathesaurus provides translation 
information as we can assume that Spanish and 
English terms that are associated with the same 
concept are respective translations.  For example 
as the English term ?arm? is associated with the 
same concept as the Spanish term ?brazo? we can 
deduce that ?arm? is the English translation of 
?brazo?.  
Unfortunately the UMLS does not contain 
morphological information about languages other 
than English. This means it cannot be 
automatically detected that ?brazo? is the singular 
form and thus the translation of ?arm? and not the 
translation of ?arms?. 
As most of the entries are in singular form we 
just extracted every possible combination of 
Spanish and English terms regardless of possible 
errors like combining the singular ?brazo? and the 
plural ?arms?. 
The resulting (lower-cased) Spanish-English 
lexicon/phrasebook contains 495,248 pairs of 
words and phrases. This means each Spanish term 
is combined with seven English terms on average. 
This seems to be an extremely huge amount but 
it has to be considered that there are terms in the 
UMLS and the resulting lexicon that are probably 
too special to be really useful for the translation of 
dialogues  (e.g. ?1,1,1-trichloropropene-2,3-oxide? 
translating to ?oxido de tricloropropeno?). 
Nevertheless there are lots of meaningful entries 
as the following experiments show.  
 
Applying the dictionaries to the Baseline system 
In the first step we just added this 
lexicon/phrasebook as an additional transducer and 
did not change the language model.  
The experiment showed a nice increase in BLEU 
and NIST performances and scored at 0.180 BLEU 
and 4.86 NIST. 
This system especially has a higher coverage, as 
only 302 words (types) are not covered by the 
training data compared to 411 for the baseline 
system. 
 
Adding the English side to the Language Model 
As the extracted dictionary contained many 
phrases it seemed reasonable to add the English 
side to the language modeling data. This also 
prevents words from the extracted dictionary to be 
treated as ?unknown? by the language model if 
they were not in the language model training data. 
This further improved the BLEU and NIST scores 
to 0.182 BLEU and 4.92 NIST. 
 
It should not be surprising to get an improvement 
in these first two experiments because basically 
just more data was used to train the systems. The 
really interesting ideas will be presented in the 
next sections. 
3.3 Using the Semantic Type Information 
The overall idea to use the semantic type 
information is to generalize the training data. 
The training data contains for example sentence 
pairs like: 
 
Necesito examinar su cabeza. 
I need to examine your head. 
Necesito examinar su brazo. 
I need to examine your arm. 
Necesito examinar su rodilla. 
I need to examine your knee. 
 
If we could generalize these sentences by 
replacing the special body parts like ?head?, ?arm? 
and ?knee? with a general tag e.g. 
?@BODYPART? and especially treat this tag we 
could use one sentence of training data for every 
body part imaginable in this sentence.  
We would just need an additional lexicon that just 
translates body parts. 
 
Necesito examinar su @BODYPART. 
I need to examine your @BODYPART. 
 
 
We could additionally correctly translate possibly 
unseen sentences like ?Necesito examinar su 
antebrazo? (?I need to examine your forearm?) if 
we could automatically deduce that 
?antebrazo/forearm? is a body part and if we just 
knew this translation pair. 
 
Some additional similar sentences in which we 
could apply the same ideas are: 
 
Enseneme que @BODYPART es. 
Show me which @BODYPART. 
?Que @BODYPART le/la duele? 
Which @BODYPART hurts? 
 
(In the last sentence it actually depends on the 
gender of the body part on the Spanish side if the 
sentence is ??Que @BODYPART la duele?? or 
??Que @BODYPART le duele??. But as we are 
translating from Spanish to English this did not 
seem to be a big problem.) 
 
As stated before every concept in the UMLS 
Metathesaurus is categorized into one or more 
semantic types defined in the UMLS Semantic 
Network. 
The two semantic types ?Body Part, Organ, or 
Organ Component? and ?Body Location or 
Region? from the UMLS Semantic Network cover 
pretty closely what we usually affiliate with the 
colloquial meaning of body part. 
[The terminological difference is that the 
semantic type ?Body Part, Organ, or Organ 
Component? is defined by a certain function. For 
example ?liver? and ?eye? are part of this semantic 
type, whereas the semantic type ?Body Location or 
Region? is defined by the topographical location of 
the respective body part. Examples are ?head? and 
?arm?. The function in this case is not as clearly 
defined as the function of a ?liver?.] 
 
This information was used in the next 
experiment. We first filtered the general Spanish-
English dictionary, we had extracted from the 
UMLS, to contain only words and phrases from the 
two semantic types ?Body Part, Organ, or Organ 
Component? and ?Body Location or Region?. This 
gave a dictionary of 11,260 translation entries for 
body parts. Again each Spanish term is combined 
with about seven English terms on average. 
In the next step we replaced every occurrence of a 
word or phrase pair from this new dictionary in the 
training data (i.e. if it occurred on the Spanish and 
English side) with a general body-part-tag. 
527 sentence pairs of the original 9,227 sentence 
pairs contained a word or phrase pair from this 
dictionary. 
 A retraining of the translation system with this 
changed training data resulted in transducer rules 
containing this body-part-tag. 
By using cascaded transducers (Vogel and Ney, 
2000) in the actual translation the first transducer, 
that is applied (in this case the body-part 
dictionary) replaces the Spanish body part with its 
translation pair and the body-part tag.  
The following transducers can apply their 
generalized rules containing the body-part-tag 
instead of the real body part. 
 
E.g. translation of the sentence:  
 
Necesito examinar su antebrazo. 
 
First step apply body-part dictionary rule  
(antebrazo?forearm) 
 
Necesito examinar su @BODYPART(antebrazo?forearm). 
 
Apply generalized transducer rule: (a rule could 
be: Necesito examinar su @BODYPART ? I need 
to examine your @BODYPART) 
 
I need to examine your @BODYPART(antebrazo?forearm). 
 
Resolve tags: 
 
I need to examine your forearm. 
 
By applying this to the whole translation system 
the score improved to 0.188 BLEU/4.94 NIST. 
Using other semantic types  
As the body-part lexicon and the replacement of 
body-parts proved to be helpful we applied two 
more of these replacement strategies. Consider the 
following 4 sentence pairs from the training data. 
 
?Siente dolor cuando respira? 
Do you feel pain when you breathe? 
?Cuando le empezo la fiebre? 
When did the fever start? 
?Podria ser artritis? 
Could this be arthritis? 
?Es grave la anemia, doctor? 
Is anemia serious, doctor? 
The first two sentences contain findings or 
symptoms with the terms ?dolor/pain? and 
?fiebre/fever?. The second two sentences contain 
diseases with ?artritis/arthritis? and 
?anemia/anemia?. The appropriate semantic types 
from the UMLS Semantic Network for these terms 
are ?Finding? and ?Sign or Symptom? for ?pain? 
and ?fever? and ?Disease or Syndrome? for 
?arthritis? and ?anemia? 
Filtering the Spanish-English dictionary resulted 
in 25,987 ?Finding/Sign or Symptom? translation 
pairs (approximately three English terms per 
Spanish term) and 116,793 ?Disease or Syndrome? 
translation pairs (approximately five English terms 
per Spanish term). 
198 sentence pairs from the training data 
contained a ?Finding/Sign or Symptom?-pair and 
127 sentence pairs contained a ?Disease or 
Syndrome?-pair from these dictionaries. 
 
The final translation with those three semantic 
types replaced in the training data and using the 
three filtered dictionaries with the cascaded 
transducer application gave a translation 
performance of 0.190 BLEU/5.02 NIST.  
This shows that although less than 10% of the 
sentences were affected by the replacement with 
the appropriate tags we could nicely improve the 
overall translation performance. 
 Example translations 
Some example translations comparing the baseline 
and the best system with the reference are listed in 
table 3. 
 
1. Sentence  
Reference 
the condition is called tenosynovitis, which 
is an inflammation of the tendon sheath. 
 
Baseline 
this condici?n diagnostic, which is a 
inflammation from the of the tendon. 
 
Best System 
this condition is called tenosynovitis, which 
is a inflammation of tendon sheath. 
2. Sentence  
Reference 
i guess your work involves a lot of repetitive 
movement, huh? 
 
Baseline 
do you I guess your work require plenty 
baby?s, no? 
 
Best System 
i guess you your work require plenty 
repetitive movements, not? 
3. Sentence 
Reference 
you need vitamin c and iron in your blood 
to help your body 
 
Baseline 
you need vitamin c and iron in your blood 
help rescue to  
 
Best System 
you need vitamin c and iron in your blood 
to help the body 
4. Sentence  
Reference 
did you take anything for the pain? 
 
Baseline 
did you sleep taken anything for the pain? 
 
Best System 
did you taken anything for the pain? 
5. Sentence  
Reference 
i can feel it here, behind my breastbone. 
 
Baseline 
i here, behind of the estern?n. 
 
Best System 
i here, behind of sternum. 
Table 3: Example translations 
 
The last example sentence is an interesting case. 
The best system does not get more words right 
compared to the baseline system and so the 
BLEU/NIST-score does not improve. But 
?sternum? is a synonym of the correct 
?breastbone? and a more technical term. This 
supports the claim that the UMLS tends to contain 
more technical terms (like ?tenosynovitis? in the 
first sentence).  
4 Future work 
It is surely possible to use every semantic type 
from the semantic network in the same way like 
the overall five semantic types, which were used in 
the experiments. We did not do this here because 
further semantic types occurred extremely rarely in 
the test and training data. But this could easily be 
done for other test and training data and it is 
reasonable to expect similar improvements. 
Another idea is to use a more specialized 
approach and to make use of the relationships in 
the UMLS Metathesaurus. Each concept could be 
generalized by its parent-concepts instead of its 
semantic type. The generalization hierarchy for the 
concept ?leg? is for example: leg ? lower extremity 
? extremity ? body region ? anatomy. 
This could be especially helpful when translating 
to morphologically richer languages than English 
because the usage of extremities could differ from 
other body parts for example. 
 
In the extracted dictionaries every translation 
pair was given the same translation probability. It 
might be helpful to re-score these probabilities by 
using information from bilingual or monolingual 
texts to improve the translation probabilities for 
usually frequently used terms compared to rarely 
used terms. 
 
As the example translations showed, the 
extracted dictionaries from the UMLS tend to 
contain technical terms instead of colloquial terms 
(translation ?sternum? instead of ?breastbone?). 
We can further assume that a doctor prefers to use 
the more technical terms and a patient prefers the 
more colloquial terms.  Therefore it could be 
interesting to examine if having two different 
translation systems for sentences uttered by a 
doctor and a patient would improve the overall 
translation performance. 
5 Conclusion 
We carried out four different experiments in order 
to improve a Spanish-English medical domain 
translation system. After sequentially applying 
different ideas the final system shows an 11% 
improvement in BLEU and 6% improvement in 
NIST score.  
Table 4 compares the different experiments and 
scores (the 500k dictionary refers to the dictionary 
that was first extracted from the UMLS with 
495,248 word pairs). 
 
System BLEU NIST 
Baseline system 0.171 4.72 
+500k dictionary 0.180 4.86 
+LM improvement 0.182 4.92 
+body part-tags 0.188 4.94 
+sign/symptom/finding 
+disease/syndrome 
0.190 5.02 
Table 4: Experiments and improvements 
 
With more investigation and the ongoing effort 
of the National Library of Medicine to extent the 
UMLS databases it will hopefully be possible to 
further improve the translation performance.  
 
References 
Allen C. Browne, Guy Divita, Alan R. Aronson, 
Alexa T. McGray, 2003.  UMLS Language and 
Vocabulary Tools, Proceedings of the American 
Medical Informatics Association (AMIA) 2003 
Symposium, Washington, DC, USA. 
George Doddington. 2001. Automatic Evaluation 
of Machine Translation Quality using n-Gram 
Cooccurrence Statistics. NIST Washington, DC, 
USA. 
Glenn Flores, M. Barton Laws, Sandra J. Mayo, 
Barry Zuckerman, Milagros Abreu, Leonardo 
Medina,  
Eric J. Hardt, 2003. Errors in medical 
interpretation and their potential clinical 
consequences in pediatric encounters, Pediatrics, 
Jan 2003. 
Carol Friedman, Hongfang Liu, Lyuda Shagina, 
Stephen Johnson, George Hripcsak, 2001. 
Evaluating the UMLS as a Source of Lexical 
Knowledge for Medical Language Processing, 
Proceedings of the AMIA 2001 Symposium, 
Washington, DC, USA. 
Vipul Kashyap, 2003. The UMLS semantic 
network and the semantic web, Proceedings of 
the AMIA 2003 Symposium,  Washington, DC, 
USA. 
C. Lindberg, 1990. The Unified Medical Language 
System (UMLS) of the National Library of 
Medicine, Journal of the American Medical 
Record Association, 1990;61(5):40-42. 
Lauren Neergard, 2003. Hospitals struggle with 
growing language barrier, Associated Press, The 
Charlotte Observer Sept. 2, 2003 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu, 2002. BLEU: a Method for 
Automatic Evaluation of Machine Translation, 
Proceedings of the ACL 2002, Philadelphia, 
USA. 
SRI Speech Technology and Research Laboratory, 
SRI Language Modeling Toolkit, 1995-2004 
(ongoing) 
http://www.speech.sri.com/projects/srilm/ 
UMLS Unified Medical Language System, 
National Library of Medicine, 1986-2004 
(ongoing) 
http://www.nlm.nih.gov/research/umls/ 
Stephan Vogel and Hermann Ney, 2000. 
Translation with Cascaded Finite State 
Transducers. Proceedings of the 38th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2000), pp. 23-30. Hongkong, 
China, October 2000. 
Stephan Vogel, Hermann Ney, and Christoph Till-
mann, 1996. HMM-based Word Alignment in 
Statistical Translation, Proceedings of COLING 
1996: The 16th International Conference on 
Computational Linguistics, pp. 836-841. 
Copenhagen, August 1996. 
Stephan Vogel, Ying Zhang, Fei Huang, Alicia 
Tribble, Ashish Venogupal, Bing Zhao, Alex 
Waibel, 2003. The CMU Statistical Translation 
System, Proceedings of MT-Summit IX. New 
Orleans, LA. Sep 2003. 
Ying Zhang, Stephan Vogel, Alex Waibel, 2003.  
Integrated Phrase Segmentation and Alignment 
Algorithm for Statistical Machine Translation, 
Proceedings of International Conference on 
Natural Language Processing and Knowledge 
Engineering 2003, Beijing, China, Oct 2003. 
Pierre Zweigenbaum, Robert Baud, Anita Burgun, 
Fiammetta Namer, ?ric Jarrousse, Natalia 
Grabar, Patrick Ruch, Franck Le Duff, Beno?t 
Thirion, St?fan Darmoni, 2003. UMLF: a 
Unified Medical Lexicon for French, 
Proceedings of the AMIA 2003 Symposium, 
Washington, DC, USA. 
 
 
 
 
Proceedings of NAACL HLT 2007, Companion Volume, pages 21?24,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Translation Model Pruning via Usage Statistics  
for Statistical Machine Translation 
 
 
Matthias Eck, Stephan Vogel, and Alex Waibel 
InterACT Research 
Carnegie Mellon University, Pittsburgh, USA 
matteck@cs.cmu.edu, vogel+@cs.cmu.edu, ahw@cs.cmu.edu 
 
 
 
Abstract 
We describe a new pruning approach to 
remove phrase pairs from translation mod-
els of statistical machine translation sys-
tems. The approach applies the original 
translation system to a large amount of text 
and calculates usage statistics for the 
phrase pairs. Using these statistics the rele-
vance of each phrase pair can be estimated. 
The approach is tested against a strong 
baseline based on previous work and shows 
significant improvements.  
1 Introduction 
A relatively new device for translation systems are 
small portable devices like cell phones, PDAs and 
handheld game consoles. The idea here is to have a 
lightweight and convenient translation device e.g. 
for tourists that can be easily carried. Other appli-
cations include medical, relief, and military scenar-
ios.  
Preferably such a device will offer speech-to-
speech translation for both (or multiple) translation 
directions. These devices have been researched and 
are starting to become commercially available (e.g. 
Isotani et al, 2003). The main challenges here are 
the severe restrictions regarding both memory and 
computing power on such a small portable device. 
1.1 Statistical Machine Translation  
Generally statistical machine translation systems 
have recently outperformed other translation ap-
proaches so it seems natural to also apply them in 
these scenarios.  
A main component of every statistical machine 
translation system is the translation model. The 
translation model assigns translation probabilities 
to phrase1 pairs of source and target phrases ex-
tracted from a parallel bilingual text. These phrase 
pairs are applied during the decoding process and 
their target sides are combined to form the final 
translation. A variety of algorithms to extract 
phrase pairs has been proposed. (e.g. Och and Ney, 
2000 and Vogel, 2005). 
Our proposed approach now tries to remove 
phrase pairs, which have little influence on the fi-
nal translation performance, from a translation sys-
tem (pruning of the translation model2). The goal 
is to reduce the number of phrase pairs and in turn 
the memory requirement of the whole translation 
system, while not impacting the translation per-
formance too heavily.  
The approach does not depend on the actual al-
gorithm used to extract the phrase pairs and can be 
applied to every imaginable method that assigns 
probabilities to phrase pairs. We assume that the 
phrase pairs were pre-extracted before decoding. 
(in contrast to the proposed approaches to ?online 
phrase extraction? (Zhang and Vogel, 2005; Calli-
son-Burch et al, 2005)). 
The task now is to remove enough pre-extracted 
phrase pairs in order to accommodate the possibly 
strict memory limitations of a portable device 
while restricting performance degradation as much 
as possible.  
We will not specifically address the computing 
power limitations of the portable devices in this 
paper.  
                                                          
1
 A ?phrase? here can also refer to a single word. 
2
 Small language models are also desirable and the approaches 
could be applied as well but this was not investigated yet. 
21
2 Previous work 
Previous work mainly introduced two natural ideas 
to prune phrase pairs. Both are for example di-
rectly available in the Pharaoh decoder (Koehn, 
2004). 
Probability threshold 
A very simple way to prune phrase pairs from a 
translation model is to use a probability threshold 
and remove all pairs for which the translation 
probability is below the threshold. The reasoning 
for this is that it is very unlikely that a translation 
with a very low probability will be chosen (over 
another translation candidate with a higher prob-
ability).  
Translation variety threshold 
Another way to prune phrase pairs is to impose a 
limit on the number of translation candidates for a 
certain phrase. That means the pruned translation 
model can only have equal or fewer possible trans-
lations for a given source phrase than the thresh-
old. This is accomplished by sorting the phrase 
pairs for each source phrase according to their 
probability and eliminating low probability ones 
until the threshold is reached. 
3 Pruning via Usage Statistics  
The approach presented here uses a different idea 
inspired by the Optimal Brain Damage algorithm 
for neural networks (Le Cun et al, 1990). 
The Optimal Brain Damage algorithm for neural 
networks computes a saliency for each network 
element. The saliency is the relevance for the per-
formance of the network. In each pruning step the 
element with the smallest saliency is removed, and 
the network is re-trained and all saliencies are re-
calculated etc. 
We can analogously view each phrase pair in the 
translation system as such a network element. The 
question is of course how to calculate the relevance 
for the performance for each phrase pair.  
A simple approximation was already done in the 
previous work using a probability or variety 
threshold. Here the relevance is estimated using the 
phrase pair probability or the phrase pair rank as 
relevance indicators.  
But these are not the only factors that influence 
the final selection of a phrase pair and most of 
these factors are not established during the training 
and phrase extraction process. Especially the fol-
lowing two additional factors play a major role in 
the importance of a phrase pair. 
Frequency of the source phrase  
We can clearly say that a phrase pair with a very 
common source phrase will be much more impor-
tant than a phrase pair where the source phrase oc-
curs only very rarely. 
Actual use of the phrase-pair 
But even phrase-pairs with very common source 
phrases might not be used for the final translation 
hypothesis. It is for example possible that it is part 
of a longer phrase pair that gets a higher probabil-
ity so that the shorter phrase pair is not used.  
 
Generally there are a lot of different factors influ-
encing the estimated importance of a phrase pair 
and it seems hard to consider every influence sepa-
rately. Hence the proposed idea does not use a 
combination of features to estimate the phrase pair 
importance. Instead the idea is to just apply the 
translation system to a large amount of text and see 
how often a phrase pair is actually used (i.e. influ-
ences the translation performance). If the translated 
text is large enough this will give a good statistics 
of the relevance of this respective phrase pair. This 
leads to the following algorithm: 
Algorithm 
Translate a large amount of (in-domain) data with 
the translation system (tuned on a development set) 
and collect the following two statistics for each 
phrase pair in the translation model. 
? c(phrase pair) = Count how often a phrase pair 
was considered during decoding (i.e. was 
added to the translation lattice) 
? u(phrase pair) = Count how often a phrase pair 
was used in the final translation (i.e. in the 
chosen path through the lattice). 
The overall score for a phrase pair with simple 
smoothing (+1) is calculated as:  
 
[ ] [ ]1)(*)1)(log(
)(
pair phrasepair phrase
pair phrase
++
=
uc
score
 
 
We use the logarithm function to limit the influ-
ence of the c value. The u value is more important 
as this measures how often a phrase was actually 
used in a translation hypothesis. This scoring func-
22
tion was empirically found after experimenting 
with a variety of possible scoring terms. 
The phrase pairs can then be sorted according to 
this score and the top n phrase pairs can be selected 
for a smaller phrase translation model. 
4 Data and Experiments 
4.1 Experimental Setup & Baseline 
Translation system 
The translation system that was used for the ex-
periments is a state-of-the-art statistical machine 
translation system (Eck et al 2006). The system 
uses a phrase extraction method described in Vogel 
(2005) and a 6-gram language model.  
Training and testing data 
The training data for all experiments consisted of 
the BTEC corpus (Takezawa et al, 2002) with 
162,318 lines of parallel Japanese-English text. All 
translations were done from Japanese to English. 
The language model was trained on the English 
part of the training data.   
The test set from the evaluation campaign of 
IWSLT 2004 (Akiba et al, 2004) was used as test-
ing data. This data consists of 500 lines of tourism 
data. 16 reference translations to English were 
available.  
Extracted phrases 
Phrase pairs for n-grams up to length 10 were ex-
tracted (with low frequency thresholds for higher 
n-grams). This gave 4,684,044 phrase pairs 
(273,459 distinct source phrases). The baseline 
score using all phrase pairs was 59.11 (BLEU, 
Papineni et al, 2002) with a 95% confidence inter-
val of [57.13, 61.09].  
Baseline pruning 
The approaches presented in previous work served 
as a baseline. The probability threshold was tested 
for 8 values (0 (no pruning), 0.0001, 0.0005, 0.001, 
0.005, 0.01, 0.05, 0.1) while the variety threshold 
tested for 14 values (1, 2, 3, 4, 5, 6, 8, 10, 15, 20, 
50, 100, 200, 500 (no pruning in this case)) and all 
combinations thereof. The final translation scores 
for different settings are very fluctuating. For that 
reason we defined the baseline score for each pos-
sible size as the best score that was reached with 
equal or less phrase pairs than the given size in any 
of the tested combinations.  
4.2 Results for  
Pruning via Usage Statistics 
For the proposed approach ?Pruning via Usage 
Statistics?, the translation system was applied to 
the 162,318 lines of Japanese training data. 
As explained in section 3 it was now counted for 
each phrase pair how often it occurred in a transla-
tion lattice and how often it was used for the final 
translation. The phrase pairs were then sorted ac-
cording to their relevance estimation and the top n 
phrase pairs were chosen for different values of n. 
The pruned phrase table was then used to translate 
the IWSLT 2004 test set. Table 1 shows the results 
comparing the baseline scores with the results us-
ing the described pruning. Figure 1 illustrates the 
scores. The plateaus in the baseline graph are due 
to the baseline definition as stated above. 
 
 BLEU scores  
# of Phrase  
Pairs (n) 
Baseline 
 
Pruning 
 
Relative score  
improvement 
100,000 - 0.4735 - 
200,000 0.3162 0.5008 58.38% 
300,000 0.4235 0.5154 21.70% 
400,000 0.4743 0.5241 10.50% 
500,000 0.4743 0.5269 11.09% 
600,000 0.4890 0.5359 9.59% 
800,000 0.5194 0.5394 3.85% 
1,000,000 0.5355 0.5442 1.62% 
1,500,000 0.5413 0.5523 2.03% 
2,000,000 0.5630 0.5749 2.11% 
3,000,000 0.5778 0.5798 0.35% 
4,000,000 0.5855 0.5865 0.17% 
4,684,044 0.5911 0.5911 0.00% 
Table 1: BLEU scores at different levels of pruning 
(Baseline: Best score with equal or less phrase 
pairs) 
 
For more than 1 million phrase pairs the differ-
ences are not very pronounced. However the trans-
lation score for the proposed pruning algorithm is 
still not significantly lower than the 59.11 score at 
2 million phrase pairs while the baseline drops 
slightly faster. For less than 1 million phrase pairs 
the differences become much more pronounced 
with relative improvements of up to 58% at 
200,000 phrase pairs. It is interesting to note that 
the improved pruning removes infrequent source 
23
phrases and to a lesser extent source vocabulary 
even for larger numbers of phrase pairs. 
 
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0 1,000,000 2,000,000 3,000,000 4,000,000
phrase pairs
B
LE
U 
sc
o
re
Baseline Pruning
 
Figure 1: Pruning and baseline comparison 
5 Conclusions and Future Work 
The proposed pruning algorithm is able to outper-
form a strong baseline based on previously intro-
duced threshold pruning ideas. Over 50% of phrase 
pairs can be pruned without a significant loss of 
performance. Even for very low memory situations 
the improved pruning remains a viable option 
while the baseline pruning performance drops 
heavily.  
One idea to improve this new pruning approach 
is to exchange the used count with the count of the 
phrase occurring in the best path of the lattice ac-
cording to a scoring metric. This would require 
having a reference translation available to be able 
to tell which path is the actual best one (metric-
best path). It would be interesting to compare the 
performance if the statistics is done using the met-
ric-best path on a smaller amount of data to the 
performance if the statistics is done using the 
model-best path on a larger amount (as there is no 
reference translation necessary). 
The Optimal Brain Damage algorithm recalcu-
lates the saliency after removing each network 
element. It could also be beneficial to sequentially 
prune the phrase pairs and always re-calculate the 
statistics after removing a certain number of phrase 
pairs. 
6 Acknowledgements 
This work was partly supported by the US DARPA 
under the programs GALE and TRANSTAC. 
7 References  
Yasuhiro Akiba, Marcello Federico, Noriko Kando, 
Hiromi Nakaiwa, Michael Paul, and Jun'ichi Tsujii}. 
2004. Overview of the IWSLT04 Evaluation Cam-
paign. Proceedings of IWSLT 2004, Kyoto, Japan. 
Chris Callison-Burch, Colin Bannard, and Josh Schroe-
der. 2005. Scaling Phrase-Based Statistical Machine 
Translation to Larger Corpora and Longer Phrases. 
Proceedings of ACL 2005, Ann Arbor, MI, USA. 
Yann Le Cun, John S. Denker, and Sara A. Solla. 1990. 
Optimal brain damage. In Advances in Neural In-
formation Processing Systems 2, pages 598-605. 
Morgan Kaufmann, 1990. 
Matthias Eck, Ian Lane, Nguyen Bach, Sanjika He-
wavitharana, Muntsin Kolss, Bing Zhao, Almut Silja 
Hildebrand, Stephan Vogel, and Alex Waibel. 2006. 
The UKA/CMU Statistical Machine Translation Sys-
tem for IWSLT 2006. Proceedings of IWSLT 2006, 
Kyoto, Japan.  
Ryosuke Isotani, Kyoshi Yamabana, Shinichi Ando, 
Ken Hanazawa, Shin-ya Ishikawa and Ken.ichi Iso. 
2003. Speech-to-speech translation software on 
PDAs for travel conversation. NEC research & de-
velopment, Tokyo, Japan. 
Philipp Koehn. 2004. A Beam Search Decoder for Sta-
tistical Machine Translation Models. Proceedings of 
AMTA 2004, Baltimore, MD, USA. 
Franz Josef Och and Hermann Ney, 2000. Improved 
statistical alignment models, Proceedings of ACL 
2000, Hongkong, China. 
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic 
Evaluation of Machine Translation. Proceedings of 
ACL 2002, Philadelphia, PA, USA. 
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, 
Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. 
Toward a Broad-coverage Bilingual Corpus for 
Speech Translation of Travel Conversation in the 
Real World. Proceedings of LREC 2002, Las Palmas, 
Spain. 
Stephan Vogel. 2005. PESA: Phrase Pair Extraction as 
Sentence Splitting. Proceedings of MTSummit X, 
Phuket, Thailand. 
Ying Zhang and Stephan Vogel. 2005. An Efficient 
Phrase-to-Phrase Alignment Model for Arbitrarily 
Long Phrases and Large Corpora. Proceedings of 
EAMT 2005, Budapest, Hungary. 
 
24
Proceedings of NAACL HLT 2009: Short Papers, pages 149?152,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incremental Adaptation of Speech-to-Speech Translation
Nguyen Bach, Roger Hsiao, Matthias Eck, Paisarn Charoenpornsawat, Stephan Vogel,
Tanja Schultz, Ian Lane, Alex Waibel and Alan W. Black
InterACT, Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
{nbach, wrhsiao, matteck, paisarn, stephan.vogel, tanja, ianlane, ahw, awb}@cs.cmu.edu
Abstract
In building practical two-way speech-to-speech
translation systems the end user will always wish
to use the system in an environment different from
the original training data. As with all speech sys-
tems, it is important to allow the system to adapt
to the actual usage situations. This paper investi-
gates how a speech-to-speech translation system can
adapt day-to-day from collected data on day one to
improve performance on day two. The platform is
the CMU Iraqi-English portable two-way speech-
to-speech system as developed under the DARPA
TransTac program. We show how machine transla-
tion, speech recognition and overall system perfor-
mance can be improved on day 2 after adapting from
day 1 in both a supervised and unsupervised way.
1 Introduction
As speech-to-speech translation systems move from the
laboratory into field deployment, we quickly see that mis-
match in training data with field use can degrade the per-
formance of the system. Retraining based on field us-
age is a common technique used in all speech systems
to improve performance. In the case of speech-to-speech
translation we would particularly like to be able to adapt
the system based on its usage automatically without hav-
ing to ship data back to the laboratory for retraining. This
paper investigates the scenario of a two-day event. We
wish to improve the system for the second day based on
the data collected on the first day.
Our system is designed for eyes-free use and hence
provides no graphical user interface. This allows the user
to concentrate on his surrounding environment during an
operation. The system only provides audio control and
feedback. Additionally the system operates on a push-to-
talk method. Previously the system (Hsiao et al, 2006;
Bach et al, 2007) needed 2 buttons to operate, one for the
English speaker and the other one for the Iraqi speaker.
W i i c o n t r o l l e r
M i c & L i g h t
L o u d  s p e a k e r
Figure 1: The users interact with the system
To make the system easier and faster to use, we propose
to use a single button which can be controlled by the En-
glish speaker. We mounted a microphone and a Wii re-
mote controller together as shown in 1.
Since the Wii controller has an accelerometer which
can be used to detect the orientation of the controller, this
feature can be applied to identify who is speaking. When
the English speaker points towards himself, the system
will switch to English-Iraqi translation. However, when
the Wii is pointed towards somebody else, the system will
switch to Iraqi-English translation. In addition, we attach
a light on the Wii controller providing visual feedback.
This can inform an Iraqi speaker when to start speaking.
The overall system is composed of five major compo-
nents: two automatic speech recognition (ASR) systems,
a bidirectional statistical machine translation (SMT) sys-
tem and two text-to-speech (TTS) systems.
2 Data Scenario
The standard data that is available for the TransTac
project was collected by recording human interpreter
mediated dialogs between war fighters and Iraqi native
speakers in various scenarios. The dialog partners were
aware that the data was being collected for training ma-
chine based translation devices, but would often talk di-
rectly to the human interpreter rather than pretending it
was an automatic device. This means that the dialog
149
partners soon ignored the recording equipment and used
a mostly natural language, using informal pronunciation
and longer sentences with more disfluencies than we find
in machine mediated translation dialogs.
Most users mismatch their language when they com-
municate using an automatic speech-to-speech transla-
tion system. They often switch to a clearer pronuncia-
tion and use shorter and simpler sentences with less dis-
fluency. This change could have a significant impact on
speech recognition and machine translation performance
if a system was originally trained on data from the inter-
preter mediated dialogs.
For this reason, additional data was collected during
the TransTac meeting in June of 2008. This data was
collected with dialog partners using the speech-to-speech
translation systems from 4 developer participants in the
TransTac program. The dialog partners were given a de-
scription of the specific scenario in form of a rough script
and had to speak their sentences into the translation sys-
tems. The dialog partners were not asked to actually react
to the potentially incorrect translations but just followed
the script, ignoring the output of the translation system.
This has the effect that the dialog partners are no longer
talking to a human interpreter, but to a machine, press-
ing push-to-talk buttons etc. and will change their speech
patterns accordingly.
The data was collected over two days, with around 2
hours of actual speech per day. This data was transcribed
and translated, resulting in 864 and 824 utterance pairs
on day 1 and 2, respectively.
3 ASR LM Adaptation
This section describes the Iraqi ASR system and how we
perform LM adaptation on the day 1 data to improve ASR
performance on day 2. The CMU Iraqi ASR system is
trained with around 350 hours of audio data collected un-
der the TransTac program. The acoustic model is speaker
independent but incremental unsupervised MLLR adap-
tation is performed to improve recognition. The acous-
tic model has 6000 codebooks and each codebook has
at most 64 Gaussian mixtures determined by merge-and-
split training. Semi-tied covariance and boosted MMI
discriminative training is performed to improve the model
(Povey et al, 2009). The features for the acoustic model
is the standard 39-dimension MFCC and we concatenate
adjacent 15 frames and perform LDA to reduce the di-
mension to 42 for the final feature vectors. The language
model of the ASR system is a trigram LM trained on the
audio transcripts with around three million words with
Kneser-Ney smoothing (Stolcke, 2002).
To perform LM adaptation for the ASR system, we use
the ASR hypotheses from day 1 to build a LM. This LM
is then interpolated with the original trigram LM to pro-
duce an adapted LM for day 2. We also evaluate the effect
of having transcribers provide accurate transcription ref-
erences for day 1 data, and see how it may improve the
performance on day 2. We compare unigram, bigram and
trigram LMs for adaptation. Since the amount of day 1
data is much smaller than the whole training set and we
do not assume transcription of day 1 is always available,
the interpolation weight is chosen of be 0.9 for the orig-
inal trigram LM and 0.1 for the new LM built from the
day 1 data. The WER of baseline ASR system on day 1
is 32.0%.
Base 1-g hypo 2-g hypo 3-g hypo 1-g ref 2-g ref 3-g ref
31.3 30.9 31.2 31.1 30.6 30.5 30.4
Table 1: Iraqi ASR?s WER on day 2 using different adaptation
schemes for day 1 data
The results in Table 1 show that the ASR benefits from
LM adaptation. Adapting day 1 data can slightly improve
the performance of day 2. The improvement is larger
when day 1 transcript is available which is expected. The
result also shows that the unigram LM is the most robust
model for adaptation as it works reasonably well when
transcripts are not available, whereas bigram and trigram
LM are more sensitive to the ASR errors made on day 1.
Day 1 Day 2
No ASR adaptation 29.39 27.41
Unsupervised ASR adaptation 31.55 27.66
Supervised ASR adaptation 32.19 27.65
Table 2: Impact of ASR adaptation to SMT
Table 2 shows the impact of ASR adaptation on the
performance of the translation system in BLEU (Papineni
et al, 2002). In these experiments we only performed
adaptation on ASR and still using the baseline SMT com-
ponent. There is no obvious difference between unsuper-
vised and supervised ASR adaptation on performance of
SMT on day 2. However, we can see that the difference
in WER on day 2 of unsupervised and supervised ASR
adaptation is relatively small.
4 SMT Adaptation
The Iraqi-English SMT system is trained with around
650K sentence pairs collected under the TransTac pro-
gram. We used PESA phrase extraction (Vogel, 2005)
and a suffix array language model (Zhang and Vogel,
2005). To adapt SMT components one approach is to op-
timize LM interpolation weights by minimizing perplex-
ity of the 1-best translation output (Bulyko et al, 2007).
Related work including (Eck et al, 2004) attempts to use
information retrieval to select training sentences similar
to those in the test set. To adapt the SMT components
we use a domain-specific LM on top of the background
150
language models. This approach is similar to the work
in (Chen et al, 2008). sThe adaptation framework is 1)
create a domain-specific LM via an n-best list of day 1
machine translation hypothesis, or day 1 translation ref-
erences; 2) re-tune the translation system on day 1 via
minimum error rate training (MERT) (Venugopal and Vo-
gel, 2005).
Use Day 1 Day 2
Baseline 29.39 27.41
500 Best 1gramLM 29.18 27.23
MT Hypos 2gramLM 29.53 27.50
3gramLM 29.36 27.23
Table 3: Performance in BLEU of unsupervised adaptation.
The first question we would like to address is whether
our adaptation obtains improvements via an unsupervised
manner. We take day 1 baseline ASR hypothesis and use
the baseline SMT to get the MT hypothesis and a 500-
best list. We train a domain LM using the 500-best list
and use the MT hypotheses as the reference in MERT. We
treat day 1 as a development set and day 2 as an unseen
test set. In Table 3 we compare the performance of four
systems: the baseline which does not have any adaptation
steps; and 3 adapted systems using unigram, bigram and
trigram LMs build from 500-best MT hypotheses.
Use Day 1 Day 2
Baseline (no tune) 29.39 27.41
Baseline (tune) 29.49 27.30
500 Best 1gramLM 30.27 28.29
MT Hypos 2gramLM 30.39 28.30
3gramLM 28.36 24.64
MT Ref 1gramLM MT Ref 30.53 28.35
Table 4: Performance in BLEU of supervised adaptation.
Experimental results from unsupervised adaptation did
not show consistent improvements but suggest we may
obtain gains via supervised adaptation. In supervised
adaptation, we assume we have day 1 translation refer-
ences. The references are used in MERT. In Table 4 we
show performances of two additional systems which are
the baseline system without adaptation but tuned toward
day 1, and the adapted system which used day 1 trans-
lation references to train a unigram LM (1gramLM MT
Ref). The unigram and bigram LMs from 500-best and
unigram LM from MT day 1 references perform rela-
tively similar on day 2. Using a trigram 500-best LM
returned a large degradation and this LM is sensitive to
the translation errors on day1
5 Joint Adaptation
In Sections 3 and 4 we saw that individual adaptation
helps ASR to reduce WER and SMT to increase BLEU
ASR SMT Day 1 Day 2
No adaptation No adaptation 29.39 27.41
Unsupervised ASR 1gramLM 500-Best 32.07 28.65
adaptation with MT Hypo
1gramLM ASR hypo 1gramLM MT Ref 31.76 28.83
Supervised ASR 1gramLM 500-Best 32.48 28.59
adaptation with MT Hypo
1gramLM transcription 1gramLM MT Ref 32.68 28.60
Table 5: Performance in BLEU of joint adaptation.
score. The next step in validating the adaptation frame-
work was to check if the joint adaptation of ASR and
SMT on day 1 data will lead to improvements on day
2. Table 5 shows the combination of ASR and SMT
adaptation methods. Improvements are obtained by us-
ing both ASR and SMT adaptation. Joint adaptation con-
sistently gained more than one BLEU point improvement
on day 2. Our best system is unsupervised ASR adapta-
tion via 1gramLM of ASR day 1 transcription coupled
with supervised SMT adaptation via 1gramLM of day
1 translation references. An interesting result is that to
have a better result on day 2 our approach only requires
translation references on day 1. We selected 1gramLM
of 500-best MT hypotheses to conduct the experiments
since there is no significant difference between 1gramLM
and 2gramLM on day 2 as showed in Table 3.
6 Selective Adaptation
The previous results indicate that we require human
translation references on day 1 data to get improved per-
formance on day 2. However, our goal is to make a better
system on day 2 but try to minimize human efforts on day
1. Therefore, we raise two questions: 1) Can we still ob-
tain improvements by not using all of day 1 data? and 2)
Can we obtain more improvements?
To answer these questions we performed oracle exper-
iments when we take the translation hypotheses on day
1 of the baseline SMT and compare them with transla-
tion references, then select sentences which have BLEU
scores higher than a threshold. The subset of day 1 sen-
tences is used to perform supervised adaptation in a sim-
ilar way showed in section 5. These experiments also
simulate the situation when we have a perfect confidence
score for machine translation hypothesis selection. Table
6 shows results when we use various portions of day 1 to
perform adaptation. By using day 1 sentences which have
smoothed sentence BLEU scores higher than 10 or 20 we
have very close performance with adaptation by using all
day 1 data. The results also show that by using 416 sen-
tences which have sentence BLEU score higher than 40
on day 1, our adapted translation components outperform
the baseline. Performance starts degrading after 50. Ex-
perimental results lead to the answer for question 1) that
151
by using less day 1 data our adapted translation compo-
nents still obtain improvements compare with the base-
line, and 2) we did not see that using less data will lead
us to a better performance compare with using all day 1
data.
No. sents Day 1 Day 2
Baseline 29.39 27.41
? 0 864 30.27 28.29
? 10 797 31.15 28.27
? 20 747 30.81 28.24
? 30 585 30.04 27.71
? 40 416 29.72 27.65
? 50 296 30.06 27.04
Correct 98 29.18 27.19
Table 6: Performance in BLEU of selective adaptation
W i c o n t r o l e M & L g
h r n u
d
r c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  
h r n u c
d 
o s p t c o a
k   i a i
 
i

t  r l
e M &  Phrase Pair Rescoring with Term Weightings for  
Statistical Machine Translation 
Bing Zhao   Stephan Vogel   Alex Waibel 
Carnegie Mellon University 
5000 Forbes Avenue 
Pittsburgh, PA, 15213, USA 
{bzhao, vogel+, ahw}@cs.cmu.edu 
 
Abstract 
We propose to score phrase translation 
pairs for statistical machine translation 
using term weight based models.  These 
models employ tf.idf to encode the 
weights of content and non-content words 
in phrase translation pairs.  The transla-
tion probability is then modeled by simi-
larity functions defined in a vector space.  
Two similarity functions are compared.  
Using these models in a statistical ma-
chine translation task shows significant 
improvements. 
1 Introduction 
Words can be classified as content and func-
tional words.  Content words like verbs and 
proper nouns are more informative than function 
words like "to'' and "the''.  In machine translation, 
intuitively, the informative content words should 
be emphasized more for better adequacy of the 
translation quality.  However, the standard statis-
tical translation approach does not take account 
how informative and thereby, how important a 
word is, in its translation model.  One reason is 
the difficulty to measure how informative a word 
is. Another problem is to integrate it naturally 
into the existing statistical machine translation 
framework, which typically is built on word 
alignment models, like the well-known IBM 
alignment models (Brown et al1993).  
In recent years there has been a strong ten-
dency to incorporate phrasal translation into sta-
tistical machine translation.  It directly translates 
an n-gram from the source language into an m-
gram in the target language.  The advantages are 
obvious:  It has built-in local context modeling, 
and provides reliable local word reordering.  It 
has multi-word translations, and models a word?s 
conditional fertility given a local context.  It cap-
tures idiomatic phrase translations and can be 
easily enriched with bilingual dictionaries. In 
addition, it can compensate for the segmentation 
errors made during preprocessing, i.e. word seg-
mentation errors of Chinese.  The advantage of 
using phrase-based translation in a statistical 
framework has been shown in many studies such 
as (Koehn et al 2003; Vogel et al 2003; Zens et 
al. 2002; Marcu and Wong, 2002).  However, the 
phrase translation pairs are typically extracted 
from a parallel corpus based on the Viterbi align-
ment of some word alignment models.  The leads 
to the question what probability should be as-
signed to those phrase translations.  Different 
approaches have been suggested as using relative 
frequencies (Zens et al 2002), calculate prob-
abilities based on a statistical word-to-word dic-
tionary (Vogel et al 2003) or use a linear 
interpolation of these scores (Koehn et al 2003).  
In this paper we investigate a different ap-
proach with takes the information content of 
words better into account.  Term weighting based 
vector models are proposed to encode the transla-
tion quality.  The advantage is that term weights, 
such as tf.idf, are useful to model the informa-
tiveness of words.  Highly informative content 
words usually have high tf.idf scores.  In informa-
tion retrieval this has been successfully applied to 
capture the relevance of a document to a query, 
by representing both query and documents as 
term weight vectors and use for example the 
cosine distance to calculate the similarity be-
tween query vector and document vector.  The 
idea now is to consider the source phrase as a 
?query?, and the different target phrases ex-
tracted from the bilingual corpus as translation 
candidates as a relevant ?documents?.  The co-
sine distance is then a natural choice to model the 
translation probability.  
Our approach is to apply term weighting 
schemes to transform source and target phrases 
into term vectors.  Usually content words in both 
source and target languages will be emphasized 
by large term weights.  Thus, good phrase trans-
lation pairs will share similar contours, or, to ex-
press it in a different way, will be close to each 
other in the term weight vector space.  A similar-
ity function is then defined to approximate trans-
lation probability in the vector space. 
The paper is structured as follows:  in Section 
2, our phrase-based statistical machine translation 
system is introduced; in Section 3, a phrase trans-
lation score function based on word translation 
probabilities is explained, as this will be used as a 
baseline system;  in Section 4, a vector model 
based on tf.idf is proposed together with two 
similarity functions;  in Section 5, length regu-
larization and smoothing schemes are explained 
briefly;  in Section 6, the translation experiments 
are presented; and Section 7 concludes with a 
discussion.  
2 Phrase-based Machine Translation 
In this section, the phrase-based machine transla-
tion system used in the experiments is briefly 
described: the phrase based translation models 
and the decoding algorithm, which allows for 
local word reordering.  
2.1 Translation Model 
The phrase-based statistical translation systems 
use not only word-to-word translation, extracted 
from bilingual data, but also phrase-to phrase 
translations. . Different types of extraction ap-
proaches have been described in the literature: 
syntax-based, word-alignment-based, and genu-
ine phrase alignment models.  The syntax-based 
approach has the advantage to model the gram-
mar structures using models of more or less 
structural richness, such as the syntax-based 
alignment model in (Yamada and Knight, 2001) 
or the Bilingual Bracketing in (Wu, 1997).  Popu-
lar word-alignment-based approaches usually 
rely on initial word alignments from the IBM and 
HMM alignment models (Och and Ney, 2000), 
from which the phrase pairs are then extracted.  
(Marcu and Wong 2002) and (Zhang et al 2003) 
do not rely on word alignment but model directly 
the phrase alignment. 
Because all statistical machine translation sys-
tems search for a globally optimal translation 
using the language and translation model, a trans-
lation probability has to be assigned to each 
phrase translation pair.  This score should be 
meaningful in that better translations have a 
higher probability assigned to them, and balanced 
with respect to word translations.  Bad phrase 
translations should not win over better word for 
word translations, only because they are phrases. 
Our focus here is not phrase extraction, but 
how to estimate a reasonable probability (or 
score) to better represent the translation quality 
of the extracted phrase pairs.  One major problem 
is that most phrase pairs are seen only several 
times, even in a very large corpus.  A reliable and 
effective estimation approach is explained in sec-
tion 3, and the proposed models are introduced in 
section 4.   
In our system, a collection of phrase transla-
tions is called a transducer.  Different phrase ex-
traction methods result in different transducers.  
A manual dictionary can be added to the system 
as just another transducer.  Typically, one source 
phrase is aligned with several candidate target 
phrases, with a score attached to each candidate 
representing the translation quality.  
2.2 Decoding Algorithm 
Given a set of transducers as the translation 
model (i.e. phrase translation pairs together with 
the scores of their translation quality), decoding 
is divided into several steps. 
The first step is to build a lattice by applying 
the transducers to the input source sentence.  We 
start from a lattice, which has as its only path the 
source sentence.  Then for each word or sequence 
of words in the source sentence for which we 
have an entry in the transducer new edges are 
generated and inserted into the lattice, spanning 
over the source phrase.  One new edge is created 
for each translation candidate, and the translation 
score is assigned to this edge.  The resulting lat-
tice has then all the information available from 
the translation model. 
The second step is search for a best path 
through this lattice, but not only based on the 
translation model scores but applying also the 
language model.  We start with an initial special 
sentence begin hypothesis at the first node in the 
lattice.  Hypotheses are then expanded over the 
edges, applying the language model to the partial 
translations attached to the edges.  The following 
algorithm summarizes the decoding process 
when not considering word reordering:  
 
Current node n, previous node n?; edge e 
Language model state L, L? 
Hypothesis h, h? 
Foreach node n in the lattice 
  Foreach incoming edge e in n 
      phrase = word sequence at e 
      n?     = FromNode(e) 
      foreach L in n? 
         foreach h with LMstate L 
           LMcost = 0.0 
           foreach word w in phrase 
             LMcost += -log p(w|L) 
             L? = NewState(L,w) 
             L  = L? 
           end 
           Cost= LMcost+TMcost(e) 
           TotalCost=TotalCost(h)+Cost 
          h? = (L,e,h,TotalCost) 
          store h?in Hypotheses(n,L) 
 
The updated hypothesis h? at the current node 
stores the pointer to the previous hypothesis and 
the edge (labeled with the target phrase) over 
which it was expanded.  Thus, at the final step, 
one can trace back to get the path associated with 
the minimum cost, i.e. the best hypothesis. 
Other operators such as local word reordering 
are incorporated into this dynamic programming 
search (Vogel, 2003). 
3 Phrase Pair Translation Probability  
As stated in the previous section, one of the 
major problems is how to assign a reasonable 
probability for the extracted phrase pair to repre-
sent the translation quality. 
Most of the phrase pairs are seen only once or 
twice in the training data.  This is especially true 
for longer phrases.  Therefore, phrase pair co-
occurrence counts collected from the training 
corpus are not reliable and have little discrimina-
tive power.  In (Vogel et al 2003) a different es-
timation approach was proposed.  Similar as in 
the IBM models, it is assumed that each source 
word si in the source phrase ),,( 21 Issss L
v =  is 
aligned to every target word tj in the target phrase 
),,( 21 Jtttt L
v =  with probability )|Pr( ij st .  The 
total phrase translation probability is then calcu-
lated according to the following generative 
model:  
? ?
= =
=
I
i
J
j
ji tsts
1 1
))|Pr(()|Pr(
vv  (1) 
 
This is essentially the lexical probability as 
calculated in the IBM1 alignment model, without 
considering position alignment probabilities.  
Any statistical translation can be used in (1) to 
calculate the phrase translation probability.  
However, in our experiment we typically see now 
significant difference in translation results when 
using lexicons trained from different alignment 
models. 
Also Equation (1) was confirmed to be robust 
and effective in parallel sentence mining from a 
very large and noisy comparable corpus (Zhao 
and Vogel, 2002).  
Equation (1) does not explicitly discriminate 
content words from non-content words.  As non-
content words such as high frequency functional 
words tend to occur in nearly every parallel sen-
tence pair, they co-occur with most of the source 
words in the vocabulary with non-trivial transla-
tion probabilities.  This noise propagates via (1) 
into the phrase translations probabilities, increas-
ing the chance that non-optimal phrase transla-
tion candidates get high probabilities and better 
translations are often not in the top ranks.  
We propose a vector model to better distin-
guish between content words and non-content 
words with the goal to emphasize content words 
in the translation.  This model will be used to 
rescore the phrase translation pairs, and to get a 
normalized score representing the translation 
probability.  
4 Vector Model for Phrase Translation 
Probability 
Term weighting models such as tf.idf are ap-
plied successfully in information retrieval.  The 
duality of term frequency (tf) and inverse docu-
ment frequency (idf), document space and collec-
tion space respectively, can smoothly predict the 
probability of terms being informative (Roelleke, 
2003).  Naturally, tf.idf is suitable to model con-
tent words as these words in general have large 
tf.idf weights. 
4.1 Phrase Pair as Bag-of-Words 
Our translation model: (transducer, as defined 
in 2.1), is a collection of phrase translation pairs 
together with scores representing the translation 
quality.  Each phrase translation pair, which can 
be represented as a triple },{ pts vv? , is now con-
verted into a ?Bag-of-Words? D consisting of a 
collection of both source and target words ap-
pearing in the phrase pair, as shown in (2):  
},,,,,{},{ 2121 JI tttsssDpts LL
vv =??  (2) 
 
Given each phrase pair as one document, the 
whole transducer is a collection of such docu-
ments.  We can calculate tf.idf for each is  and jt , 
and represent source and target phrases by vec-
tors of sv
v  and tv
v   as in Equation (3):  
},,,{
21 Issss
wwwv Lv =  
},,,{
21 Jtttt
wwwv Lv =  (3) 
where
is
w and
jt
w are tf.idf for is or jt respectively.  
This vector representation can be justified by 
word co-occurrence considerations.  As the 
phrase translation pairs are extracted from paral-
lel sentences, the source words is  and target 
words jt  in the source and target phrases must 
co-occur in the training data.  The co-occurring 
words should share similar term frequency and 
document frequency statistics.  Therefore, the 
vectors  sv
v and tv
v  have similar term weight con-
tours corresponding to the co-occurring word 
pairs.  So the vector representations of a phrase 
translation pair can reflect the translation quality.  
In addition, the content words and non-content 
words are modeled explicitly by using term 
weights.  An over-simplified example would be 
that a rare word in the source language usually 
translates into a rare word in the target language. 
4.2 Term Weighting Schemes 
Given the transducer, it is straightforward to 
calculate term weights for source and target 
words.  There are several versions of tf.idf.  The 
smooth ones are preferred, because phrase trans-
lation pairs are rare events collected from train-
ing data.  
The idf model selected is as in Equation (4): 
)
5.0
5.0log( +
+?=
df
dfNidf  (4)
where N is the total number of documents in the 
transducer, i.e. the total number of translation 
pairs, and df is the document frequency, i.e. in 
how many phrase pairs a given word occurs.  The 
constant of 0.5 acts as smoothing. 
Because most of the phrases are short, such as 
2 to 8 words, the term frequency in the bag of 
words representation is usually 1, and some times 
2.  This, in general, does not bring much dis-
crimination in representing translation quality.  
The following version of tf is chosen, so that 
longer target phrases with more words than aver-
age will be slightly down-weighted: 
)(/)(5.15.0
'
vavglenvlentf
tftf vv?++=  (5)
where tf is the term frequency, )(vlen v  is the 
length in words of the phrase vv , and )(vavglen v  is 
the average length of source or target phrase cal-
culated from the transducer.  Again, the values of 
0.5 and 1.5 are constants used in IR tasks acting 
as smoothing.  
Thus after a transducer is extracted from a par-
allel corpus, tf and df are counted from the collec-
tion of the ?bag-of-words'' phrase alignment 
representations.  For each word in the phrase pair 
translation its tf.idf weight is assigned and the 
source and target phrase are transformed into 
vectors as shown in Equation (3).  These vectors 
reserve the translation quality information and 
also model the content and non-content words by 
the term weighting model of tf.idf. 
4.3 Vector Space Alignment 
Given the vector representations in Equation 
(3), a similarity between the two vectors can not 
directly be calculated.  The dimensions I and J 
are not guaranteed to be the same.  The goal is to 
transform the source vector into a vector having 
the same dimensions as the target vector, i.e. to 
map the source vector into the space of the target 
vector, so that a similarity distance can be calcu-
lated.  Using the same reasoning as used to moti-
vate Equation (1), it is assumed that every source 
word is  contributes some probability mass to 
each target word jt .  That is to say, given a term 
weight for jt , all source term weights are aligned 
to it with some probability.  So we can calculate 
a transformed vector from the source vectors by 
calculating weights jtaw  using a translation lexi-
con )|Pr( st  as in Equation (6): 
?
=
?=
I
i
sij
t
a i
j wstw
1
)|Pr(  (6) 
 
Now the target vector and the mapped vector 
av
v  have the same dimensions as shown in (7):  
},,,{ 21 Jta
t
a
t
aa wwwv L
v =  
},,,{
21 Jtttt
wwwv Lv =  (7) 
 
4.4 Similarity Functions 
As explained in section 4.1, intuitively, if sv  
and t
v  is a good translation pair, then the corre-
sponding vectors of av
v  and tv
v  should be similar 
to each other in the vector space.   
Cosine distance 
The standard cosine distance is defined as the 
inner product of the two vectors av
v  and tv
v  nor-
malized by their norms.  Based on Equation (6), 
it is easy to derive the similarity as follows:  
)()(
)|(
)|(1
1),(),(
1
2
1
2
1 1
1 1
1
cos
??
? ?
? ?
?
==
= =
= =
=
=
=
==
J
j
t
a
J
j
t
J
j
I
i
sijt
J
j
I
i
sijt
t
t
a
J
j
t
t
a
t
t
at
t
a
t
t
a
t
t
a
j
j
ij
ij
j
j
wsqrtwsqrt
wstPw
wstPw
vv
ww
vvvv
vvvvd
 
(8)
where I and J are the length of the source and 
target phrases; 
is
w  and 
jt
w  are term weights for 
source word and target words;  jtaw  is the trans-
formed weight mapped from all source words to 
the target dimension at word jt .   
BM25 distance 
TREC tests show that bm25 (Robertson and 
Walker, 1997) is one of the best-known distance 
schemes.  This distance metric is given in Equa-
tion (9). The constants of 31 ,, kbk are set to be 1, 1 
and 1000 respectively.  
)(
)1(
)(
)1(
3
3
1
1
25 j
j
j
j
t
a
t
a
J
j t
t
bm wk
wk
wK
wk
wd +
+
+
+=?
=
 
)5.0/()5.0( ++?==
jjj ttt
dfdfNidfw  
))(/)1((1 lavgJbkK +?=  
(9)
where avg(l) is the average target phrase length 
in words given the same source phrase. 
Our experiments confirmed the bm25 distance 
is slightly better than the cosine distance, though 
the difference is not really significant.  One ad-
vantage of bm25 distance is that the set of free 
parameters 31 ,, kbk can be tuned to get better per-
formance e.g. via n-fold cross validation.  
4.5 Integrated Translation Score 
Our goal is to rescore the phrase translation 
pairs by using additional evidence of the transla-
tion quality in the vector space.   
The vector based scores (8) & (9) provide a 
distinct view of the translation quality in the vec-
tor space.  Equation (1) provides a evidence of 
the translation quality based on the word align-
ment probability, and can be assumed to be dif-
ferent from the evidences in vector space.  Thus, 
a natural way of integrating them together is a 
geometric interpolation shown in (10) or equiva-
lently a linear interpolation in the log domain.  
)|(Pr),( 1int tsstdd vec
vvvv ?? ??=  (10)
where ),( stdvec
vv is the score from the cosine or 
bm25 vector distance, normalized within [0, 1], 
like a probability. 
0.1),( =?
t
vec stdv
vv   
The parameter ? can be tuned using held-out 
data.  In our cross validation experiments 5.0=?  
gave the best performance in most cases.  There-
fore, Equation (10) can be simplified into: 
)|Pr(),(int tsstdd vec
vvvv ?=  (11)
 
The phrase translation score functions in (1) 
and (11) are non-symmetric.  This is because the 
statistical lexicon Pr(s|t) is non-symmetric.  One 
can easily re-write all the distances by using 
Pr(t|s).  But in our experiments this reverse di-
rection of using Pr(t|s) gives trivially difference.  
So in all the experimental results reported in this 
paper, the distances defined in (1) and (11) are 
used. 
5 Length Regularization  
Phrase pair extraction does not work perfectly 
and sometimes a short source phrase is aligned to 
a long target phrase or vice versa.  Length regu-
larization can be applied to penalize too long or 
too short candidate translations.  Similar to the 
sentence alignment work in (Gale and Church, 
1991), the phrase length ratio is assumed to be a 
Gaussian distribution as given in Equation (12):  
)))(/)((5.0exp(),( 2
2
?
????? sltlstl
vvvv  (12)
where l(t) is the target sentence  length.  Mean ?  
and variance ?  can be estimated using a parallel 
corpus using a Maximum Likelihood criteria. 
The regularized score is the product of (11) and 
(12).  
6 Experiments  
Experiments were carried out on the so-called 
large data track Chinese-English TIDES transla-
tion task, using the June 2002 test data.  The 
training data used to train the statistical lexicon 
and to extract the phrase translation pairs was 
selected from a 120 million word parallel corpus 
in such a way as to cover the phrases in test sen-
tences.  The restricted training corpus contained 
then approximately 10 million words..  A trigram 
model was built on 20 million words of general 
newswire text, using the SRILM toolkit (Stolcke, 
2002).  Decoding was carried out as described in 
section 2.2.  The test data consists of 878 Chinese 
sentences or 24,337 words after word segmenta-
tion.  There are four human translations per Chi-
nese sentence as references.  Both NIST score 
and Bleu score (in percentage) are reported for 
adequacy and fluency aspects of the translation 
quality. 
6.1 Transducers 
Four transducers were used in our experi-
ments: LDC, BiBr, HMM, and ISA.  
LDC was built from the LDC Chinese-English 
dictionary in two steps: first, morphological 
variations are created.  For nouns and noun 
phrases plural forms and entries with definite and 
indefinite determiners were generated.  For verbs 
additional word forms with -s -ed and -ing were 
generated, and the infinitive form with 'to'.  Sec-
ond, a large monolingual English corpus was 
used to filter out the new word forms.  If they did 
not appear in the corpus, the new entries were not 
added to the transducer (Vogel, 2004). 
BiBr extracts sub-tree mappings from Bilin-
gual Bracketing alignments (Wu, 1997);  HMM 
extracts partial path mappings from the Viterbi 
path in the Hidden Markov Model alignments 
(Vogel et. al., 1996).  ISA is an integrated seg-
mentation and alignment for phrases (Zhang et.al, 
2003), which is an extension of (Marcu and 
Wong, 2002).  
 LDC BiBr HMM ISA 
)(KN  425K 137K 349K 263K 
)/( srctgt llavg  1.80 1.11 1.09 1.20 
Table-1 statistics of transducers 
 
Table-1 shows some statistics of the four 
transducers extracted for the translation task. N  
is the total number of phrase pairs in the trans-
ducer.  LDC is the largest one having 425K en-
tries, as the other transducers are restricted to 
?useful? entries, i.e. those translation pairs where 
the source phrase matches a sequence of words in 
one of the test sentence.  Notice that the LDC 
dictionary has a large number of long transla-
tions, leading to a high source to target length 
ratio. 
6.2 Cosine vs BM25 
The normalized cosine and bm25 distances de-
fined in (8) and (9) respectively, are plugged into 
(11) to calculate the translation probabilities.  
Initial experiments are reported on the LDC 
transducer, which gives already a good transla-
tion, and therefore allows for fast and yet mean-
ingful experimentation.  
Four baselines (Uniform, Base-m1, Base-m4, 
and Base-m4S) are presented in Table-2.   
 
NIST Bleu 
Uniform 6.69 13.82 
Base-m1 7.08 14.84 
Base-m4 7.04 14.91 
Base-m4S 6.91 14.44 
cosine 7.17 15.30 
bm25 7.19 15.51 
bm25-len 7.21 15.64 
Table-2 Comparisons of different score functions 
 
In the first uniform probabilities are assigned 
to each phrase pair in the transducer.  The second 
one (Base-m1) is using Equation (1) with a statis-
tical lexicon trained using IBM Model-1, and 
Base-m4 is using the lexicon from IBM Model-4.  
Base-m4S is using IBM Model-4, but we skipped 
194 high frequency English stop words in the 
calculation of Equation (1). 
Table-2 shows that the translation score de-
fined by Equation (1) is much better than a uni-
form model, as expected.  Base-m4 is slightly 
worse than Base-m1.on NIST score, but slightly 
better using the Bleu metric.  Both differences 
are not statistically significant.  The result for 
Base-m4S shows that skipping English stop 
words in Equation (1) gives a disadvantage.  One 
reason is that skipping ignores too much non-
trivial statistics from parallel corpus especially 
for short phrases.  These high frequency words 
actually account already for more than 40% of 
the tokens in the corpus.  
Using the vector model, both with the cosine 
cosd  and the bm25 25bmd  distance, is significantly 
better than Base-m1 and Base-m4 models, which 
confirms our intuition of the vector model as an 
additional useful evidence for translation quality. 
The length regularization (12) helps only slightly 
for LDC.  Since bm25?s parameters could be 
tuned for potentially better performance, we se-
lected bm25 with length regularization as the 
model tested in further experiments.  
A full-loaded system is tested using the 
LM020 with and without word-reordering in de-
coding.  The results are presented in Table-3.  
Table-3 shows consistent improvements on all 
configurations: the individual transducers, com-
binations of transducers, and different decoder 
settings of word-reordering. Because each phrase 
pair is treated as a ?bag-of-words?, the grammar 
structure is not well represented in the vector 
model.  Thus our model is more tuned towards 
the adequacy aspect, corresponding to NIST 
score improvement. 
Because the transducers of BiBr, HMM, and 
ISA are extracted from the same training data, 
they have significant overlaps with each other.  
This is why we observe only small improvements 
when adding more transducers.   
The final NIST score of the full system is 8.24, 
and the Bleu score is 22.37.  This corresponds to 
3.1% and 11.8% relative improvements over the 
baseline.  These improvements are statistically 
significant according to a previous study (Zhang 
et.al., 2004), which shows that a 2% improve-
ment in NIST score and a 5% improvement in 
Bleu score is significant for our translation sys-
tem on the June 2002 test data. 
6.3 Mean Reciprocal Rank 
To further investigate the effects of the rescor-
ing function in (11), Mean Reciprocal Rank 
(MRR) experiments were carried out.  MRR for a 
labeled set is the mean of the reciprocal rank of 
the individual phrase pair, at which the best can-
didate translation is found (Kantor and Voorhees, 
1996).  
Totally 9,641 phrase pairs were selected con-
taining 216 distinct source phrases.  Each source 
phrase was labeled with its best translation can-
didate without ambiguity.  The rank of the la-
beled candidate is calculated according to 
translation scores. The results are shown in Ta-
ble-4. 
 baseline cosine bm25 
MRR 0.40 0.58 0.75 
Table-4 Mean Reciprocal Rank 
 
The rescore functions improve the MRR from 
0.40 to 0.58 using cosine distance, and to 0.75 
using bm25.  This confirms our intuitions that 
good translation candidates move up in the rank 
after the rescoring.  
Decoder settings without word reordering with word reordering 
baseline bm25 baseline bm25 Scores (%) NIST Bleu NIST Bleu NIST Bleu NIST Bleu 
LDC 7.08 14.84 7.21 15.64 7.13 15.10 7.26 15.98 
LDC+ISA 7.73 19.60 7.99 19.58 7.86 20.80 8.13 20.93 
LDC+ISA+HMM 7.86 19.08 8.14 20.70 7.95 19.84 8.19 21.60 
LDC+ISA+HMM+BiBr 7.87 19.23 8.14 21.48 7.99 20.01 8.24 22.37 
Table-3 Translation using bm25 rescore function with different decoder settings 
 
7 Conclusion and Discussion  
In this work, we proposed a way of using term 
weight based models in a vector space as addi-
tional evidences for translation quality, and inte-
grated the model into an existing phrase-based 
statistical machine translation system.  The mod-
el shows significant improvements when using it 
to score a manual dictionary as well as when us-
ing different phrase transducers or a combination 
of all available translation information.  Addi-
tional experiments also confirmed the effective-
ness of the proposed model in terms of of 
improved Mean Reciprocal Rank of good transla-
tions. 
Our future work is to explore alternatives such 
as the reranking work in (Collins, 2002) and in-
clude more knowledge such as syntax informa-
tion in rescoring the phrase translation pairs.  
References 
A. Stolcke. 2002. SRILM -- An Extensible Language 
Modeling Toolkit. In 2002 Proc. Intl. Conf. on 
Spoken Language Processing, Denver. 
Peter F. Brown, Stephen A. Della Pietra, Vincent 
J.Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation, Computational Linguistics, 
vol. 19, no. 2, pp. 263?311. 
Michael Collins. 2000. Discriminative Reranking for 
Natural Language Parsing. Proc. 17th International 
Conf. on Machine Learning. pp. 175-182. 
William A. Gale and Kenneth W. Church. 1991. A 
Program for Aligning Sentences in Bilingual Cor-
pora.  In Computational Linguistics, vol.19 pp. 75-
102.  
Paul B. Kantor, Ellen Voorhees. 1996. Report on the 
TREC-5 Confusion Track. The Fifth Text Retrieval 
Conference. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. Pro-
ceedings of HLT-NAACL. Edmonton, Canada.  
Daniel Marcu and William Wong. 2002. A Phrase-
Based, Joint Probability Model for Statistical Ma-
chine Translation. Proceedings of EMNLP-2002, 
Philadelphia, PA. 
Franz Josef Och and Hermann Ney. 2000. Improved 
Statistical Alignment Models. Proceedings of ACL-
00, pp. 440-447, Hongkong, China. 
Thomas Roelleke. 2003. A Frequency-based and a 
Poisson-based Definition of the Probability of Be-
ing Informative. Proceedings of the 26th annual in-
ternational ACM SIGIR. pp. 227-234.  
S.E. Robertson, and S. Walker. 1997. On relevance 
weights with little relevance information. In 1997 
Proceedings of ACM SIGIR. pp. 16-24. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora, 
Computational Linguistics 23(3): pp.377-404.  
K. Yamada and K. Knight. 2001. A Syntax-Based 
Statistical Translation Model. Proceedings of the 
39th Annual Meeting of the Association for Compu-
tational Linguistics. pp. 523-529. Toulouse, France.  
Richard Zens, Franz Josef Och and Hermann Ney. 
2002. Phrase-Based Statistical Machine Transla-
tion. Proceedings of the 25th Annual German Con-
ference on AI: Advances in Artificial Intelligence. 
pp. 18-22. 
Stephan Vogel, Hermann Ney, Christian Tillmann. 
1996. HMM-based word alignment in statistical 
translation. In: COLING '96: The 16th Int. Conf. on 
Computational Linguistics, Copenhagen, Denmark 
(1996) pp. 836-841. 
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venogupal, Bing Zhao, Alex Waibel. 
2003. The CMU Statistical Translation System, 
Proceedings of MT-Summit IX. New Orleans, LA. 
Stephan Vogel. 2003. SMT decoder dissected: word 
reordering, In 2003 Proceedings of Natural Lan-
guage Processing and Knowledge Engineering, 
(NLP-KE'03) Beijing, China.  
Stephan Vogel. 2004. Augmenting Manual Dictionar-
ies for Statistical Machine Translation Systems, In 
2003 Proceedings of LREC, Lisbon, Portugal.  pp. 
1593-1596. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2004. In-
terpreting BLEU/NIST Scores : How much Im-
provement Do We Need to Have a Better System? 
In 2004 Proceedings of LREC, Lisbon, Portugal. 
pp. 2051-2054. 
Ying Zhang, Stephan Vogel, Alex Waibel. 2003. "In-
tegrated Phrase Segmentation and Alignment Algo-
rithm for Statistical Machine Translation," in the 
Proceedings of NLP-KE'03, Beijing, China. 
Bing Zhao, Stephan Vogel. 2002. Adaptative Parallel 
Sentences Mining from web bilingual news collec-
tion. In 2002 IEEE International Conference on 
Data Mining, Maebashi City, Japan.  
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 184?187,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Tools for Collecting Speech Corpora via Mechanical-Turk 
  
Ian Lane1,2, Alex Waibel1,2 
1Language Technologies Institute 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
{ianlane,ahw}@cs.cmu.edu 
Matthias Eck2, Kay Rottmann2 
2Mobile Technologies LLC 
Pittsburgh, PA, USA 
matthias.eck@jibbigo.com 
kay.rottmann@jibbigo.com
 
 
Abstract 
To rapidly port speech applications to 
new languages one of the most difficult 
tasks is the initial collection of sufficient 
speech corpora. State-of-the-art automatic 
speech recognition systems are typical 
trained on hundreds of hours of speech 
data. While pre-existing corpora do exist 
for major languages, a sufficient amount 
of quality speech data is not available for 
most world languages. While previous 
works have focused on the collection of 
translations and the transcription of audio 
via Mechanical-Turk mechanisms, in this 
paper we introduce two tools which ena-
ble the collection of speech data remotely. 
We then compare the quality of audio col-
lected from paid part-time staff and unsu-
pervised volunteers, and determine that 
basic user training is critical to obtain us-
able data.  
1 Introduction 
In order to port a spoken language application to a 
new language, first an automatic speech recogni-
tion (ASR) system must be developed. For many 
languages pre-existing corpora do not exist and 
thus speech data must be collected before devel-
opment can begin. The collection of speech corpo-
ra is an expensive undertaking and obtaining this 
data rapidly, for example in response to a disaster, 
cannot be done using the typical methodology in 
which corpora are collected in controlled environ-
ments. 
 
To build an ASR system for a new language, two 
sets of data are required; first, a text corpus con-
sisting of written transcriptions of utterances users 
are likely to speak to the system, this is used to 
train the language model (LM) applied during 
ASR; and second, a corpora of recordings of 
speech, which are used to train an acoustic model 
(AM). Text corpora for a new language can be 
created by manually translating a pre-existing cor-
pus (or a sub-set of that corpus) into the new lan-
guage and crowd-sourcing methodologies can be 
used to rapidly perform this task. Rapidly creating 
corpora of speech data, however, is not trivial. 
Generally speech corpora are collected in con-
trolled environments where speakers are super-
vised by experts to ensure the equipment is setup 
correctly and recordings are performed adequately. 
However, for most languages performing this task 
on-site, where developers are located, is impractic-
al as there may not be a local community of speak-
ers of the required language. An alternative is to 
perform the data collection remotely, allowing 
speakers to record speech on their own PCs or mo-
bile devices in their home country or wherever 
they are located. While previous works have fo-
cused on the generation of translations (Razavian, 
2009) and transcribing of audio (Marge, 2010) via 
Mechanical-Turk, in this paper we focus on the 
collection of speech corpora using a Mechanical-
Turk type framework. 
 
Previous works (Voxforge), (Gruenstein, 2009), 
(Schultz, 2007) have developed solutions for col-
lecting speech data remotely via web-based inter-
faces. A web-based system for the collection of 
open-source speech corpora has been developed by 
the group at www.voxforge.org. Speech recordings 
are collected for ten major European languages and 
speakers can either record audio directly on the 
website or they can call in on a dedicated phone 
line. In (Gruenstein, 2009) spontaneous speech 
(US English) was collected via a web-based mem-
ory game. In this system speech prompts were not 
provided, but rather a voice-based memory game 
was used to gather and partially annotate 
184
             
Figure 1: Screenshots from Speech Collection iPhone App 
 
spontaneous speech. In comparison to the above 
works which focus on the collection of data for 
major languages, the SPICE project (Schultz, 
2007) provides a set of web-based tools to enable 
developers to create voice-based applications for 
less-common languages. In addition to tools for 
defining the phonetic units of a language and creat-
ing pronunciation dictionaries, this system also 
includes tools to create prompts and collect speech 
data from volunteers over the web. 
 
In this paper, we describe two tools we have de-
veloped to collect speech corpora remotely. The 
first, a Mobile smart-phone based system which 
allows speakers to record prompted speech directly 
on their phones and second, a web-based system 
which allows recordings to be collected remotely 
on PCs. We compare the quality of audio collected 
from paid part-time staff and unsupervised volun-
teers and determine that basic user training and 
automatic feedback mechanisms are required to 
obtain usable data. 
2 Collection of Speech on Mobile Devices 
Today?s smart-phones are able to record quality 
audio onboard and generally have the ability to 
connect to the internet via a fast wifi-connection. 
This makes them an ideal platform for collecting 
speech data in the field. Speech data can be col-
lected by a user at any time in any location, and the 
data can be uploaded at a later time when a wire-
less connection is available. At Mobile Technolo-
gies we have developed an iPhone application to 
perform this task. 
 
The collection procedure consists of three steps. 
First, on start-up a small amount of personal in-
formation, namely, gender and age, are requested 
from the user. They then select the language for 
which they intend to provide speech data. The mo-
bile-device ID, personal information and language 
selected is used as an identifier for individual 
speakers. Next, collection of speech data is per-
formed. Collection is performed offline, enabling 
data to be collected in the field where there may 
not be a persistent internet connection. A prompt is 
randomly selected from an onboard database of 
sentences and is presented to the user, who reads 
the sentence aloud holding down a push-to-talk 
button while speaking. During the speech collec-
tion stage, the system automatically proceeds to the 
following prompt when the current recording is 
complete. The user however has the ability to go 
back to previous recordings, listen to it and re-
speak the sentence if any issues are found. Finally, 
the speech data is uploaded using a wireless collec-
tion. Data is uploaded one utterance at a time to an 
FTP server. Uploading each utterance individually 
allows the user to halt the upload and continue it at 
a later time if required. 
  
185
 
Figure 2: Java applet for Web-based recording 
3 Collection via Web-based Recording 
One of the most popular websites for crowd-
sourcing is Amazon Mechanical Turk (AMT). 
?Requesters? post Human Intelligence Tasks 
(HITs) to this website and ?Workers? browse the 
HITs, perform tasks and get paid a predefined 
amount after submitting their work. It has been 
reported that over 100,000 workers from 100 coun-
tries are using AMT (Pontin, 2007). 
 
AMT allows two general types of HITs. A Ques-
tion Form HIT is based on a provided XML tem-
plate and only allows certain elements in the HIT. 
However, it is possible to integrate an external 
JAVA applet within a Question Form HIT which 
allows for some flexibility. Questions can also be 
hosted on an external website which increases flex-
ibility for the HIT developer while remaining 
tightly integrated in the AMT environment. 
 
For collection of audio data Amazon does not offer 
any integrated tools. We thus designed and imple-
mented a Java applet for web based speech collec-
tion. The Java applet can easily be incorporated in 
the AMT Question-Form mechanism and could 
also be used as part of an External-Question HIT.  
Currently the Java applet provides the same basic 
functionality as outlined for the iPhone application. 
The applet sequentially shows a number of 
prompts to record. The user can skip a sentence, 
playback a recording to check the quality and also 
redo the recording for the current sentence (see 
screenshot in Figure 2).  
 
After the user is finished, the recorded sentences 
are uploaded to a web-server using an HTTP Post 
request. An important difference is the necessity to 
be online during the speech recordings. 
4 Evaluation of Recorded Audio 
One issue when collecting speech data remotely is 
the quality of the resulting audio. When collection  
Table 1: Details of Evaluated Corpora 
 
Table 2: Annotations used to label poor quality 
recordings 
is performed in a controlled environment, the de-
veloper can ensure that the recording equipment is 
setup correctly, background noise is kept to a min-
imum and the speaker is adequately trained to use 
the recording equipment. However, the same is not 
guaranteed when collecting speech remotely via 
mechanical-turk frameworks.  
When recording prompted speech there are three 
types of issues that result in unsuitable data: 
? Garbage Audio: recordings that are emp-
ty, clipped, have insufficient power, or are 
incorrectly segmented. 
? Low quality recordings: low Signal-to-
Noise recordings due to poor equipment or 
large background noise 
? Speaker errors: Misspeaking of prompts, 
both accidental and malicious 
To verify the quality of audio recorded in unsuper-
vised environments we compared two sets of 
speech data. First, in an earlier data collection task 
we collected 445 prompted utterances from 10 US-
English speakers. This data collection was per-
formed in a quiet office environment with technic-
al supervision. Speakers were paid a fee for their 
time. As a comparison a similar collection of Hai-
tian Creole was performed. In this case data was 
collected on a volunteer basis and supervision was 
limited. Details of the collected data are shown in 
Table 1.  
  
Paid Employees 
Language English 
Number of Speakers 10  
Utterances Evaluated 445 
  
Volunteers 
Language Haitian Creole 
Number of Speakers 3 
Utterances Evaluated 167 
1 Recorded utterance is empty 
2 Utterance is not segmented correctly 
3 Recording is clipped 
4 Recording contains audible echo 
5 Recording contains audible noise 
186
 Figure 3: Percentage of recorded utterances de-
termined to be inadequate for acoustic model 
training. Annotations limited to five issues 
listed in Table 1. 
To determine the frequency of the quality issues 
listed above, we manually verified the two sets of 
collected speech. The recording of each utterance 
was listened to and if the audio file was determined 
to be of low quality it was annotated with one of 
the tags listed in Table 2. The percentage of utter-
ances labeled with each annotation is shown for the 
English and volunteer Haitian Creole cases in Fig-
ure 3. 
 
Around 10% of the English recordings were found 
to have issues. Clipping occurred in approximately 
5% and a distinct echo was present in the record-
ings for one speaker. For the Haitian Creole case 
the yield of useable audio was significantly lower 
than that obtained for English. For all three speak-
ers clipping was more prevalent and the level of 
background noise was higher. We discovered that 
due to lack of training, one of the volunteers had 
significant issues with the push-to-talk interface in 
our system. This led to many empty or incorrectly 
segmented recordings. In both cases, prompts were 
generally spoken accurately and technical prob-
lems caused poor quality recordings. 
 
We believe the large difference in the yield of high 
quality recordings, 90% for English compared to 
65% for Haitian Creole case, is directly due to the 
lack of training speakers received and the volun-
teer nature of the Haitian Creole task. By incorpo-
rating a basic tutorial when users first start our 
tools and an explicit feedback mechanism which 
automatically detects quality issues and prompts 
users to correct them we expect the yield of high 
quality recordings to increase significantly. In the 
near future we plan to use the tools to collect data 
from large communities of remote users.  
5 Conclusions and Future Work 
In this work, we have described two applications 
that allow speech corpora to be collected remotely, 
either directly on Mobile smart-phones or on a PC 
via a web-based interface. We also investigated the 
quality of recordings made by unsupervised volun-
teers and found that although prompts were gener-
ally read accurately, lack of training led to a 
significantly lower yield of high quality record-
ings. 
 
In the near future we plan to use the tools to collect 
data from large communities of remote users. We 
will also investigate the user of tutorials and feed-
back to improve the yield of high quality data. 
 
Acknowledgements 
 
We would like to thank the Haitian volunteers who 
gave their time to help with this data collection. 
References  
N. S. Razavian, S Vogel, "The Web as a Platform 
to Build Machine Translation Resources", 
IWIC2009 
M. Marge, S. Banerjee and A. Rudnicky, "Using 
the Amazon Mechanical Turk for Transcription 
of Spoken Language", IEEE-ICASSP, 2010 
Voxforge, www.voxforge.org 
A. Gruenstein, I. McGraw, and A. Sutherland, "A 
self-transcribing speech corpus: collecting con-
tinuous speech with an online educational 
game," Submitted to the Speech and Language 
Technology in Education (SLaTE) Workshop, 
2009. 
T. Schultz, et. al, "SPICE: Web-based Tools for 
Rapid Language Adaptation in Speech 
Processing Systems", In the Proceedings of 
INTERSPEECH, Antwerp, Belgium, 2007. 
J. Pontin, ?Artificial Intelligence, With Help From 
the Humans?, The New York Times, 25 March 
2007 
187
