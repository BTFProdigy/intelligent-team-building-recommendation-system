Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?672,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Encoding Semantic Resources in Syntactic Structures
for Passage Reranking
Kateryna Tymoshenko
Trento RISE
38123 Povo (TN), Italy
k.tymoshenko@trentorise.eu
Alessandro Moschitti
Qatar Computing Research Instit.
5825 Doha, Qatar
amoschitti@qf.org.qa
Aliaksei Severyn
University of Trento
38123 Povo (TN), Italy
severyn@disi.unitn.it
Abstract
In this paper, we propose to use seman-
tic knowledge from Wikipedia and large-
scale structured knowledge datasets avail-
able as Linked Open Data (LOD) for
the answer passage reranking task. We
represent question and candidate answer
passages with pairs of shallow syntac-
tic/semantic trees, whose constituents are
connected using LOD. The trees are pro-
cessed by SVMs and tree kernels, which
can automatically exploit tree fragments.
The experiments with our SVM rank algo-
rithm on the TREC Question Answering
(QA) corpus show that the added relational
information highly improves over the state
of the art, e.g., about 15.4% of relative im-
provement in P@1.
1 Introduction
Past work in TREC QA, e.g. (Voorhees, 2001),
and more recent work (Ferrucci et al., 2010) in
QA has shown that, to achieve human perfor-
mance, semantic resources, e.g., Wikipedia
1
,
must be utilized by QA systems. This requires
the design of rules or machine learning features
that exploit such knowledge by also satisfying
syntactic constraints, e.g., the semantic type of
the answer must match the question focus words.
The engineering of such rules for open domain
QA is typically very costly. For instance, for
automatically deriving the correctness of the
answer passage in the following question/answer
passage (Q/AP) pair (from the TREC corpus
2
):
Q: What company owns the soft drink brand ?Gatorade??
A: Stokely-Van Camp bought the formula and started
marketing the drink as Gatorade in 1967. Quaker Oats Co.
took over Stokely-Van Camp in 1983.
1
http://www.wikipedia.org
2
It will be our a running example for the rest of the paper.
we would need to write the following complex
rules:
is(Quaker Oats Co.,company),
own(Stokely-Van Camp,Gatorade),
took over(Quaker Oats Co.,Stokely-Van Camp),
took over(Y, Z)?own(Z,Y),
and carry out logic unification and resolution.
Therefore, approaches that can automatically
generate patterns (i.e., features) from syntactic
and semantic representations of the Q/AP are
needed. In this respect, our previous work, e.g.,
(Moschitti et al., 2007; Moschitti and Quarteroni,
2008; Moschitti, 2009), has shown that tree
kernels for NLP, e.g., (Moschitti, 2006), can
exploit syntactic patterns for answer passage
reranking significantly improving search engine
baselines. Our more recent work, (Severyn and
Moschitti, 2012; Severyn et al., 2013b; Severyn
et al., 2013a), has shown that using automatically
produced semantic labels in shallow syntactic
trees, such as question category and question
focus, can further improve passage reranking and
answer extraction (Severyn and Moschitti, 2013).
However, such methods cannot solve the class
of examples above as they do not use background
knowledge, which is essential to answer com-
plex questions. On the other hand, Kalyanpur
et al. (2011) and Murdock et al. (2012) showed
that semantic match features extracted from large-
scale background knowledge sources, including
the LOD ones, are beneficial for answer rerank-
ing.
In this paper, we tackle the candidate answer
passage reranking task. We define kernel func-
tions that can automatically learn structural pat-
terns enriched by semantic knowledge, e.g., from
LOD. For this purpose, we carry out the follow-
ing steps: first, we design a representation for the
Q/AP pair by engineering a pair of shallow syn-
tactic trees connected with relational nodes (i.e.,
664
  
NLPAnnotatorsFocus and Question classifiers
NLPAnnotatorsFocus and Question classifiers
syntactic/semantic graphsyntactic/semantic graph train/testdata
Kernel-based rerankerKernel-based reranker
RerankedAP
EvaluationEvaluation
CandidateAPQuestion
UIMA pipeline
Search engineSearch engine
q/a similarity featuresq/a similarity features
Wikipedia link annotatorWikipedia link annotator
WikipediaWikipedia
LOD type annotatorLOD type annotator
LOD datasetsLOD datasets
Figure 1: Kernel-based Answer Passage Reranking System
those matching the same words in the question and
in the answer passages).
Secondly, we use YAGO (Suchanek et al.,
2007), DBpedia (Bizer et al., 2009) and Word-
Net (Fellbaum, 1998) to match constituents from
Q/AP pairs and use their generalizations in our
syntactic/semantic structures. We employ word
sense disambiguation to match the right entities in
YAGO and DBpedia, and consider all senses of an
ambiguous word from WordNet.
Finally, we experiment with TREC QA and sev-
eral models combining traditional feature vectors
with automatic semantic labels derived by statis-
tical classifiers and relational structures enriched
with LOD relations. The results show that our
methods greatly improve over strong IR baseline,
e.g., BM25, by 96%, and on our previous state-
of-the-art reranking models, up to 15.4% (relative
improvement) in P@1.
2 Reranking with Tree Kernels
In contrast to ad-hoc document retrieval, struc-
tured representation of sentences and paragraphs
helps to improve question answering (Bilotti et al.,
2010). Typically, rules considering syntactic and
semantic properties of the question and its candi-
date answer are handcrafted. Their modeling is in
general time-consuming and costly. In contrast,
we rely on machine learning and automatic fea-
ture engineering with tree kernels. We used our
state-of-the-art reranking models, i.e., (Severyn et
al., 2013b; Severyn et al., 2013a) as a baseline.
Our major difference with such approach is that
we encode knowledge and semantics in different
ways, using knowledge from LOD. The next sec-
tions outline our new kernel-based framework, al-
though the detailed descriptions of the most inno-
vative aspects such as new LOD-based representa-
tions are reported in Section 3.
2.1 Framework Overview
Our QA system is based on a rather simple rerank-
ing framework as displayed in Figure 1: given a
question Q, a search engine retrieves a list of can-
didate APs ranked by their relevancy. Next, the
question together with its APs are processed by
a rich NLP pipeline, which performs basic tok-
enization, sentence splitting, lemmatization, stop-
word removal. Various NLP components, em-
bedded in the pipeline as UIMA
3
annotators, per-
form more involved linguistic analysis, e.g., POS-
tagging, chunking, NE recognition, constituency
and dependency parsing, etc.
Each Q/AP pair is processed by a Wikipedia
link annotator. It automatically recognizes n-
grams in plain text, which may be linked to
Wikipedia and disambiguates them to Wikipedia
URLs. Given that question passages are typically
short, we concatenate them with the candidate an-
swers to provide a larger disambiguation context
to the annotator.
These annotations are then used to produce
computational structures (see Sec. 2.2) input to the
reranker. The semantics of such relational struc-
tures can be further enriched by adding links be-
tween Q/AP constituents. Such relational links
can be also generated by: (i) matching lemmas
as in (Severyn and Moschitti, 2012); (ii) match-
ing the question focus type derived by the ques-
tion classifiers with the type of the target NE as
in (Severyn et al., 2013a); or (iii) by matching the
constituent types based on LOD (proposed in this
paper). The resulting pairs of trees connected by
semantic links are then used to train a kernel-based
reranker, which is used to re-order the retrieved
answer passages.
2.2 Relational Q/AP structures
We use the shallow tree representation that we
proposed in (Severyn and Moschitti, 2012) as a
baseline structural model. More in detail, each Q
and its candidate AP are encoded into two trees,
where lemmas constitute the leaf level, the part-
of-speech (POS) tags are at the pre-terminal level
and the sequences of POS tags are organized into
the third level of chunk nodes. We encoded struc-
tural relations using the REL tag, which links the
related structures in Q/AP, when there is a match
3
http://uima.apache.org/
665
Figure 2: Basic structural representations using a shallow chunk tree structure for the Q/AP in the running example. Curved
line indicates the tree fragments in the question and its answer passage linked by the relational REL tag.
between the lemmas in Q and AP. We marked the
parent (POS tags) and grand parent (chunk) nodes
of such lemmas by prepending a REL tag.
However, more general semantic relations, e.g.,
derived from the question focus and category, can
be encoded using the REL-FOCUS-<QC> tag,
where <QC> stands for the question class. In
(Severyn et al., 2013b; Severyn et al., 2013a), we
used statistical classifiers to derive question focus
and categories of the question and of the named
entities in the AP. We again mark (i) the focus
chunk in the question and (ii) the AP chunks con-
taining named entities of type compatible with the
question class, by prepending the above tags to
their labels. The compatibility between the cat-
egories of named entities and questions is evalu-
ated with a lookup to a manually predefined map-
ping (see Table 1 in (Severyn et al., 2013b)). We
also prune the trees by removing the nodes beyond
a certain distance (in terms of chunk nodes) from
the REL and REL-FOCUS nodes. This removes
irrelevant information and speeds up learning and
classification. We showed that such model outper-
forms bag-of-words and POS-tag sequence mod-
els (Severyn et al., 2013a).
An example of a Q/AP pair encoded using shal-
low chunk trees is given in Figure 2. Here, for ex-
ample, the lemma ?drink? occurs in both Q and AP
(we highlighted it with a solid line box in the fig-
ure). ?Company? was correctly recognized as a fo-
cus
4
, however it was misclassified as ?HUMAN?
(?HUM?). As no entities of the matching type
?PERSON? were found in the answer by a NER
system, no chunks were marked as REL-FOCUS
on the answer passage side.
We slightly modify the REL-FOCUS encod-
ing into the tree. Instead of prepending REL-
FOCUS-<QC>, we only prepend REL-FOCUS
to the target chunk node, and add a new node
QC as the rightmost child of the chunk node, e.g,
in Figure 2, the focus node would be marked as
REL-FOCUS and the sequence of its children
would be [WP NN HUM]. This modification in-
4
We used the same approach to focus detection and ques-
tion classification used in (Severyn et al., 2013b)
tends to reduce the feature sparsity.
3 LOD for Semantic Structures
We aim at exploiting semantic resources for build-
ing more powerful rerankers. More specifically,
we use structured knowledge about properties of
the objects referred to in a Q/AP pair. A large
amount of knowledge has been made available as
LOD datasets, which can be used for finding addi-
tional semantic links between Q/AP passages.
In the next sections, we (i) formally define novel
semantic links between Q/AP structures that we
introduce in this paper; (ii) provide basic notions
of Linked Open Data along with three of its most
widely used datasets, YAGO, DBpedia and Word-
Net; and, finally, (iii) describe our algorithm to
generate linked Q/AP structures.
3.1 Matching Q/AP Structures: Type Match
We look for token sequences (e.g., complex nomi-
nal groups) in Q/AP pairs that refer to entities and
entity classes related by isa (Eq. 1) and isSubclas-
sOf (Eq. 2) relations and then link them in the
structural Q/AP representations.
isa : entity ? class? {true, false} (1)
isSubclassOf : class? class? {true, false} (2)
Here, entities are all the objects in the world
both real or abstract, while classes are sets of en-
tities that share some common features. Informa-
tion about entities, classes and their relations can
be obtained from the external knowledge sources
such as the LOD resources. isa returns true if an
entity is an element of a class (false otherwise),
while isSubclassOf(class1,class2) returns true if
all elements of class1 belong also to class2.
We refer to the token sequences introduced
above as to anchors and the entities/classes they
refer to as references. We define anchors to be in
a Type Match (TM) relation if the entities/classes
they refer to are in isa or isSubclassOf relation.
More formally, given two anchors a
1
and a
2
be-
longing to two text passages, p
1
and p
2
, respec-
tively, and given an R(a, p) function, which re-
turns a reference of an anchor a in passage p, we
define TM (r
1
, r
2
) as
666
{
isa (r
1
, r
2
) : if isEntity (r
1
) ? isClass (r
2
)
subClassOf (r
1
, r
2
) : if isClass (r
1
) ? isClass (r
2
)
(3)
where r
1
= R(a
1
, p
1
), r
2
= R(a
2
, p
2
) and isEn-
tity(r) and isClass(r) return true if r is an entity or
a class, respectively, and false otherwise. It should
be noted that, due to the ambiguity of natural lan-
guage, the same anchor may have different refer-
ences depending on the context.
3.2 LOD for linking Q/A structures
LOD consists of datasets published online accord-
ing to the Linked Data (LD) principles
5
and avail-
able in open access. LOD knowledge is repre-
sented following the Resource Description Frame-
work (RDF)
6
specification as a set of statements.
A statement is a subject-predicate-object triple,
where predicate denotes the directed relation, e.g.,
hasSurname or owns, between subject and object.
Each object described by RDF, e.g., a class or
an entity, is called a resource and is assigned a
Unique Resource Identifier (URI).
LOD includes a number of common schemas,
i.e., sets of classes and predicates to be reused
when describing knowledge. For example, one
of them is RDF Schema (RDFS)
7
, which contains
predicates rdf:type and rdfs:SubClassOf
similar to the isa and subClassOf functions above.
LOD contains a number of large-scale cross-
domain datasets, e.g., YAGO (Suchanek et al.,
2007) and DBpedia (Bizer et al., 2009). Datasets
created before the emergence of LD, e.g., Word-
Net, are brought into correspondence with the LD
principles and added to the LOD as well.
3.2.1 Algorithm for detecting TM
Algorithm 1 detects n-grams in the Q/AP struc-
tures that are in TM relation and encodes TM
knowledge in the shallow chunk tree representa-
tions of Q/AP pairs. It takes two text passages, P
1
and P
2
, and a LOD knowledge source, LOD
KS
,
as input. We run the algorithm twice, first with
AP as P
1
and Q as P
2
and then vice versa. For
example, P
1
and P
2
in the first run could be, ac-
cording to our running example, Q and AP candi-
date, respectively, and LOD
KS
could be YAGO,
DBpedia or WordNet.
Detecting anchors. getAnchors(P
2
,LOD
KS
)
in line 1 of Algorithm 1 returns all anchors in the
5
http://www.w3.org/DesignIssues/
LinkedData.html
6
http://www.w3.org/TR/rdf-concepts/
7
http://www.w3.org/TR/rdf-schema/
Algorithm 1 Type Match algorithm
Input: P
1
, P
2
- text passages; LOD
KS
- LOD knowledge
source.
1: for all anchor ? getAnchors(P
2
,LOD
KS
) do
2: for all uri ? getURIs(anchor,P
2
,LOD
KS
) do
3: for all type ? getTypes(uri,LOD
KS
) do
4: for all ch ? getChunks(P
1
) do
5: matchedTokens ? checkMatch(ch,
type.labels)
6: if matchedTokens 6= ? then
7: markAsTM(anchor,P
2
.parseTree)
8: markAsTM(matchedTokens,
P
1
.parseTree)
given text passage, P
2
. Depending on LOD
KS
one may have various implementations of this pro-
cedure. For example, when LOD
KS
is Word-
Net, getAnchor returns token subsequences of the
chunks in P
2
of lengths n-k, where n is the number
of tokens in the chunk and k = [1, .., n? 1).
In case when LOD
KS
is YAGO or DBpedia,
we benefit from the fact that both YAGO and DB-
pedia are aligned with Wikipedia on entity level by
construction and we can use the so-called wikifica-
tion tools, e.g., (Milne and Witten, 2009), to detect
the anchors. The wikification tools recognize n-
grams that may denote Wikipedia pages in plain
text and disambiguate them to obtain a unique
Wikipedia page. Such tools determine whether
a certain n-gram may denote a Wikipedia page(s)
by looking it up in a precomputed vocabulary cre-
ated using Wikipedia page titles and internal link
network (Csomai and Mihalcea, 2008; Milne and
Witten, 2009).
Obtaining references. In line 2 of Algorithm 1
for each anchor, we determine the URIs of enti-
ties/classes it refers to in LOD
KS
. Here again,
we have different strategies for different LOD
KS
.
In case of WordNet, we use the all-senses strat-
egy, i.e., getURI procedure returns a set of URIs
of synsets that contain the anchor lemma.
In case when LOD
KS
is YAGO or DBpedia,
we use wikification tools to correctly disambiguate
an anchor to a Wikipedia page. Then, Wikipedia
page URLs may be converted to DBpedia URIs by
substituting the en.wikipedia.org/wiki/
prefix to the dbpedia.org/resource/; and
YAGO URIs by querying it for subjects of the
RDF triples with yago:hasWikipediaUrl
8
as a predicate and Wikipedia URL as an object.
For instance, one of the anchors detected in
the running example AP would be ?Quaker oats?,
8
yago: is a shorthand for the http prefix http://
yago-knowledge.org/resource/
667
a wikification tool would map it to wiki:
Quaker_Oats_Company
9
, and the respective
YAGO URI would be yago:Quaker_Oats_
Company.
Obtaining type information. Given a uri, if it
is an entity, we look for all the classes it belongs
to, or if it is a class, we look for all classes for
which it is a subclass. This process is incorpo-
rated in the getTypes procedure in line 3 of Algo-
rithm 1. We call such classes types. If LOD
KS
is WordNet, then our types are simply the URIs of
the hypernyms of uri. If LOD
KS
is DBpedia or
YAGO, we query these datasets for the values of
the rdf:type and rdfs:subClassOf prop-
erties of the uri (i.e., objects of the triples with uri
as subject and type/subClassOf as predicates) and
add their values (which are also URIs) to the types
set. Then, we recursively repeat the same queries
for each retrieved type URI and add their results to
the types. Finally, the getTypes procedure returns
the resulting types set.
The extracted URIs returned by getTypes are
HTTP ids, however, frequently they have human-
readable names, or labels, specified by the rdfs:
label property. If no label information for a
URI is available, we can create the label by re-
moving the technical information from the type
URI, e.g., http prefix and underscores. type.labels
denotes a set of type human-readable labels for
a specific type. For example, one of the types
extracted for yago:Quaker_Oats_Company
would have label ?company?.
Checking for TM. Further, the checkMatch
procedure checks whether any of the labels in the
type.labels matches any of the chunks in P
1
re-
turned by getChunks, fully or partially (line 5 of
Algorithm 1). Here, getChunks procedure returns
a list of chunks recognized in P
1
by an external
chunker.
More specifically, given a chunk, ch, and a type
label, type.label, checkMatch checks whether the
ch string matches
10
type.label or its last word(s).
If no match is observed, we remove the first to-
ken from ch and repeat the procedure. We stop
when the match is observed or when no tokens
in ch are left. If the match is observed, check-
Match returns all the tokens remaining in ch as
matchedTokens. Otherwise, it returns an empty
set. For example, the question of the running ex-
9
wiki: is a shorthand for the http prefix http://en.
wikipedia.org/wiki/
10
case-insensitive exact string match
ample contains the chunk ?what company?, which
partially matches the human readable ?company?
label of one of the types retrieved for the ?Quaker
oats? anchor from the answer. Our implemen-
tation of the checkMatch procedure would re-
turn ?company? from the question as one of the
matchedTokens.
If the matchedTokens set is not empty,
this means that TM
(
R
(
anchor, P
2
)
, R
(
matchedTokens, P
1
))
in Eq. 3 returns true.
Indeed, a
1
is an anchor and a
2
is the matched-
Tokens sequence (see Eq. 3), and their respective
references, i.e., URI assigned to the anchor and
URI of one of its types, are either in subClassOf
or in isa relation by construction. Naturally, this
is only one of the possible ways to evaluate the
TM function, and it may be noise-prone.
Marking TM in tree structures. Finally,
if the TM match is observed, i.e., matchedTo-
kens is not an empty set, we mark tree substruc-
tures corresponding to the anchor in the struc-
tural representation of P
2
(P
2
.parseTree) and
those corresponding to matchedTokens in that of
P
1
(P
1
.parseTree) as being in a TM relation. In
our running example, we would mark the substruc-
tures corresponding to ?Quaker oats? anchor in the
answer (our P
2
) and the ?company? matchedTo-
ken in the question (our P
1
) shallow syntactic tree
representations. We can encode TM match infor-
mation into a tree in a variety of ways, which we
describe below.
3.2.2 Encoding TM knowledge in the trees
a
1
and a
2
from Eq. 3 are n-grams, therefore they
correspond to the leaf nodes in the shallow syn-
tactic trees of p
1
and p
2
. We denote the set of
their preterminal parents as N
TM
. We consid-
ered the following strategies of encoding TM re-
lation in the trees: (i) TM node (TM
N
). Add leaf
sibling tagged with TM to all the nodes in N
TM
.
(ii) Directed TM node (TM
ND
). Add leaf sib-
ling tagged with TM-CHILD to all the nodes in
N
TM
corresponding to the anchor, and leaf sib-
lings tagged with TM-PARENT to the nodes cor-
responding to matchedTokens. (iii) Focus TM
(TM
NF
). Add leaf siblings to all the nodes in
N
TM
. If matchedTokens is a part of a question
focus label them as TM-FOCUS. Otherwise, la-
bel them as TM. (iv) Combo TM
NDF
. Encode
using the TM
ND
strategy. If matchedTokens is a
part of a question focus label then also add a child
labeled FOCUS to each of the TM labels. Intu-
668
Figure 3: Fragments of a shallow chunk parse tree anno-
tated in TM
ND
mode.
itively, TM
ND
, TM
NF
, TM
NDF
are likely to re-
sult in more expressive patterns. Fig. 3 shows an
example of the TM
ND
annotation.
3.3 Wikipedia-based matching
Lemma matching for detecting REL may result in
low coverage, e.g., it is not able to match differ-
ent variants for the same name. We remedy this
by using Wikipedia link annotation. We consider
two word sequences (in Q and AP, respectively)
that are annotated with the same Wikipedia link
to be in a matching relation. Thus, we add new
REL tags to Q/AP structural representations as de-
scribed in Sec. 2.2.
4 Experiments
We evaluated our different rerankers encoding sev-
eral semantic structures on passage retrieval task,
using a factoid open-domain TREC QA corpus.
4.1 Experimental Setup
TREC QA 2002/2003. In our experiments, we
opted for questions from years 2002 and 2003,
which totals to 824 factoid questions. The
AQUAINT corpus
11
is used for searching the sup-
porting passages.
Pruning. Following (Severyn and Moschitti,
2012) we prune the shallow trees by removing the
nodes beyond distance of 2 from the REL, REL-
FOCUS or TM nodes.
LOD datasets. We used the core RDF distribu-
tion of YAGO2
12
, WordNet 3.0 in RDF
13
, and the
datasets from the 3.9 DBpedia distribution
14
.
Feature Vectors. We used a subset of the sim-
ilarity functions between Q and AP described in
(Severyn et al., 2013b). These are used along
with the structural models. More explicitly: Term-
overlap features: i.e., a cosine similarity over
question/answer, sim
COS
(Q,AP ), where the in-
put vectors are composed of lemma or POS-tag
11
http://catalog.ldc.upenn.edu/
LDC2002T31
12
http://www.mpi-inf.mpg.de/yago-naga/
yago1_yago2/download/yago2/yago2core_
20120109.rdfs.7z
13
http://semanticweb.cs.vu.nl/lod/wn30/
14
http://dbpedia.org/Downloads39
n-grams with n = 1, .., 4. PTK score: i.e., out-
put of the Partial Tree Kernel (PTK), defined in
(Moschitti, 2006), when applied to the structural
representations of Q and AP, sim
PTK
(Q,AP ) =
PTK(Q,AP ) (note that, this is computed within
a pair). PTK defines similarity in terms of the
number of substructures shared by two trees.
Search engine ranking score: the ranking score of
our search engine assigned to AP divided by a nor-
malizing factor.
SVM re-ranker. To train our models, we use
SVM-light-TK
15
, which enables the use of struc-
tural kernels (Moschitti, 2006) in SVM-light
(Joachims, 2002). We use default parameters and
the preference reranking model described in (Sev-
eryn and Moschitti, 2012; Severyn et al., 2013b).
We used PTK and the polynomial kernel of degree
3 on standard features.
Pipeline. We built the entire processing pipeline
on top of the UIMA framework.We included many
off-the-shelf NLP tools wrapping them as UIMA
annotators to perform sentence detection, tok-
enization, NE Recognition, parsing, chunking and
lemmatization. Moreover, we used annotators
for building new sentence representations starting
from tools? annotations and classifiers for question
focus and question class.
Search engines. We adopted Terrier
16
using the
accurate BM25 scoring model with default param-
eters. We trained it on the TREC corpus (3Gb),
containing about 1 million documents. We per-
formed indexing at the paragraph level by splitting
each document into a set of paragraphs, which are
then added to the search index. We retrieve a list of
50 candidate answer passages for each question.
Wikipedia link annotators. We use the
Wikipedia Miner (WM) (Milne and Witten,
2009)
17
tool and the Machine Linking (ML)
18
web-service to annotate Q/AP pairs with links to
Wikipedia. Both tools output annotation confi-
dence. We use all WM and ML annotations with
confidence exceeding 0.2 and 0.05, respectively.
We obtained these figures heuristically, they are
low because we aimed to maximize the Recall of
the Wikipedia link annotators in order to maxi-
15
http://disi.unitn.it/moschitti/
Tree-Kernel.htm
16
http://terrier.org
17
http://sourceforge.net/projects/
wikipedia-miner/files/wikipedia-miner/
wikipedia-miner_1.1, we use only topic detector
module which detects and disambiguates anchors
18
http://www.machinelinking.com/wp
669
System MRR MAP P@1
BM25 28.02?2.94 0.22?0.02 18.17?3.79
CH+V (CoNLL, 2013) 37.45 0.3 27.91
CH+V+QC+TFC
(CoNLL, 2013)
39.49 0.32 30
CH + V 36.82?2.68 0.30?0.02 26.34?2.17
CH + V+ QC+TFC 40.20?1.84 0.33?0.01 30.85?2.35
CH+V+QC+TFC* 40.50?2.32 0.33?0.02 31.46?2.42
Table 1: Baseline systems
mize the number of TMs. In all the experiments,
we used a union of the sets of the annotations pro-
vided by WM and ML.
Metrics. We used common QA metrics: Precision
at rank 1 (P@1), i.e., the percentage of questions
with a correct answer ranked at the first position,
and Mean Reciprocal Rank (MRR). We also report
the Mean Average Precision (MAP). We perform
5-fold cross-validation and report the metrics aver-
aged across all the folds together with the std.dev.
4.2 Baseline Structural Reranking
In these experiments, we evaluated the accuracy
of the following baseline models: BM25 is the
BM25 scoring model, which also provides the ini-
tial ranking; CH+V is a combination of tree struc-
tures encoding Q/AP pairs using relational links
with the feature vector; and CH+V+QC+TFC is
CH+V extended with the semantic categorial links
introduced in (Severyn et al., 2013b).
Table 1 reports the performance of our base-
line systems. The lines marked with (CoNLL,
2013) contain the results we reported in (Sev-
eryn et al., 2013b). Lines four and five report
the performance of the same systems, i.e., CH+V
and CH+V+QC+TFC, after small improvement
and changes. Note that in our last version, we
have a different set of V features than in (CoNLL,
2013). Finally, CH+V+QC+TFC* refers to the
performance of CH+V+QC+TFC with question
type information of semantic REL-FOCUS links
represented as a distinct node (see Section 2.2).
The results show that this modification yields a
slight improvement over the baseline, thus, in
the next experiments, we add LOD knowledge to
CH+V+QC+TFC*.
4.3 Impact of LOD in Semantic Structures
These experiments evaluated the accuracy of the
following models (described in the previous sec-
tions): (i) a system using Wikipedia to establish
the REL links; and (ii) systems which use LOD
knowledge to find type matches (TM).
The first header line of the Table 2 shows which
baseline system was enriched with the TM knowl-
edge. Type column reports the TM encoding strat-
egy employed (see Section 3.2.2). Dataset column
reports which knowledge source was employed to
find TM relations. Here, yago is YAGO2, db is
DBpedia, and wn is WordNet 3.0. The first re-
sult line in Table 2 reports the performance of
the strong CH+V and CH+V+QC+TFC* base-
line systems. Line with the ?wiki? dataset re-
ports on CH+V and CH+V+QC+TFC* using
both Wikipedia link annotations provided by ML
and MW and hard lemma matching to find the re-
lated structures to be marked by REL (see Sec-
tion 3.3 for details of the Wikipedia-based REL
matching). The remainder of the systems is built
on top of the baselines using both hard lemma and
Wikipedia-based matching. We used bold font to
mark the top scores for each encoding strategy.
The tables show that all the systems ex-
ploiting LOD knowledge, excluding those us-
ing DBpedia only, outperform the strong CH+V
and CH+V+QC+TFC* baselines. Note that
CH+V enriched with TM tags performs com-
parably to, and in some cases even outper-
forms, CH+V+QC+TFC*. Compare, for exam-
ple, the outputs of CH+V+TM
NDF
using YAGO,
WordNet and DBpedia knowledge and those of
CH+V+QC+TFC* with no LOD knowledge.
Adding TM tags to the top-performing base-
line system, CH+V+QC+TFC*, typically re-
sults in further increase in performance. The
best-performing system in terms of MRR and
P@1 is CH+V+QC+TFC*+TM
NF
system us-
ing the combination of WordNet and YAGO2 as
source of TM knowledge and Wikipedia for REL-
matching. It outperforms the CH+V+QC+TFC*
baseline by 3.82% and 4.15% in terms of MRR
and P@1, respectively. Regarding MAP, a num-
ber of systems employing YAGO2 in combina-
tion with WordNet and Wikipedia-based REL-
matching obtain 0.37 MAP score thus outperform-
ing the CH+V+QC+TFC* baseline by 4%.
We used paired two-tailed t-test for evaluating
the statistical significance of the results reported in
Table 2. ? and ? correspond to the significance lev-
els of 0.05 and 0.1, respectively. We compared (i)
the results in the wiki line to those in the none line;
and (ii) the results for the TM systems to those in
the wiki line.
The table shows that we typically obtain bet-
ter results when using YAGO2 and/or WordNet.
In our intuition this is due to the fact that these
resources are large-scale, have fine-grained class
670
Type Dataset CH + V CH + V + QC + TFC*
MRR MAP P@1 MRR MAP P@1
- none 36.82?2.68 0.30?0.02 26.34?2.17 40.50?2.32 0.33?0.02 31.46?2.42
- wiki 39.17?1.29? 0.31?0.01? 28.66?1.43? 41.33?1.17 0.34?0.01 31.46?1.40
TM
N
db 40.60?1.88 0.33?0.01? 31.10?2.99? 40.80?1.01 0.34?0.01 30.37?1.90
TM
N
wn 41.39?1.96? 0.33?0.01? 31.34?2.94 42.43?0.56 0.35?0.01 32.80?0.67
TM
N
wn+db 40.85?1.52? 0.33?0.01? 30.37?2.34 42.37?1.12 0.35?0.01 32.44?2.64
TM
N
yago 40.71?2.07 0.33?0.03? 30.24?2.09? 43.28?1.91? 0.36?0.01? 33.90?2.75
TM
N
yago+db 41.25?1.57? 0.34?0.02? 31.10?1.88? 42.39?1.83 0.35?0.01 32.93?3.14
TM
N
yago+wn 42.01?2.26? 0.34?0.02? 32.07?3.04? 43.98?1.08? 0.36?0.01? 35.24?1.46?
TM
N
yago+wn+db 41.52?1.85? 0.34?0.02? 30.98?2.71? 43.13?1.38 0.36?0.01 33.66?2.77
TM
NF
db 40.67?1.94? 0.33?0.01? 30.85?2.22? 41.43?0.70 0.35?0.01 31.22?1.09
TM
NF
wn 40.95?2.27? 0.33?0.01? 30.98?3.74 42.37?0.98 0.35?0.01 32.56?1.76
TM
NF
wn+db 40.84?2.18? 0.34?0.01? 30.73?3.04 43.08?0.83? 0.36?0.01? 33.54?1.29?
TM
NF
yago 42.01?2.44? 0.34?0.02? 32.07?3.01? 43.82?2.36? 0.36?0.02? 34.88?3.35
TM
NF
yago+db 41.32?1.70? 0.34?0.02? 31.10?2.48? 43.19?1.17? 0.36?0.01? 33.90?1.86
TM
NF
yago+wn 41.69?1.66? 0.34?0.02? 31.10?2.44? 44.32?0.70? 0.36?0.01? 35.61?1.11?
TM
NF
yago+wn+db 41.56?1.41? 0.34?0.02? 30.85?2.22? 43.79?0.73? 0.37?0.01? 34.88?1.69?
TM
ND
db 40.37?1.87 0.33?0.01? 30.37?2.17 41.58?1.02 0.35?0.01? 31.46?1.59
TM
ND
wn 41.13?2.14? 0.33?0.01? 30.73?2.75 42.19?1.39 0.35?0.01 32.32?1.36
TM
ND
wn+db 41.28?1.03? 0.34?0.01? 30.73?0.82? 42.37?1.16 0.36?0.01 32.44?2.71
TM
ND
yago 42.11?3.24? 0.34?0.02? 32.07?4.06? 44.04?2.05? 0.36?0.01? 34.63?2.17?
TM
ND
yago+db 42.28?2.01? 0.35?0.01? 32.44?1.99? 43.77?2.02? 0.37?0.01? 34.27?2.42
TM
ND
yago+wn 42.96?1.45? 0.35?0.01? 33.05?2.04? 44.25?1.32? 0.37?0.00? 34.76?1.61?
TM
ND
yago+wn+db 42.56?1.25? 0.35?0.01? 32.56?1.91? 43.91?1.01? 0.37?0.01? 34.63?1.32?
TM
NDF
db 40.40?1.93? 0.33?0.01? 30.49?1.78? 41.85?1.05 0.35?0.01? 31.83?0.80
TM
NDF
wn 40.84?1.69? 0.33?0.01? 30.49?2.24 41.89?0.99 0.35?0.01 31.71?0.86
TM
NDF
wn+db 41.14?1.29? 0.34?0.01? 30.73?1.40? 42.31?0.92 0.36?0.01 32.32?2.36
TM
NDF
yago 42.31?2.57? 0.35?0.02? 32.68?3.01? 44.22?2.38? 0.37?0.02? 35.00?2.88?
TM
NDF
yago+db 41.96?1.82? 0.35?0.01? 32.32?2.24? 43.82?1.95? 0.37?0.01? 34.51?2.39?
TM
NDF
yago+wn 42.80?1.19? 0.35?0.01? 33.17?1.86? 43.91?0.98? 0.37?0.01? 34.63?0.90?
TM
NDF
yago+wn+db 43.15?0.93? 0.35?0.01? 33.78?1.59? 43.96?0.94? 0.37?0.01? 34.88?1.69?
Table 2: Results in 5-fold cross-validation on TREC QA corpus
taxonomy and contain many synonymous labels
per class/entity thus allowing us to have a good
coverage with TM-links. DBpedia ontology that
we employed in the db experiments is more shal-
low and contains fewer labels for classes, there-
fore the amount of discovered TM matches is
not always sufficient for increasing performance.
YAGO2 provides better coverage for TM relations
between entities and their classes, while Word-
Net contains more relations between classes
19
.
Note that in (Severyn and Moschitti, 2012), we
also used supersenses of WordNet (unsuccess-
fully) whereas here we use hypernymy relations
and a different technique to incorporate semantic
match into the tree structures.
Different TM-knowledge encoding strategies,
TM
N
, TM
ND
, TM
NF
, TM
NDF
produce small
changes in accuracy. We believe, that the differ-
ence between them would become more signifi-
cant when experimenting with larger corpora.
5 Conclusions
This paper proposes syntactic structures whose
nodes are enriched with semantic information
from statistical classifiers and knowledge from
LOD. In particular, YAGO, DBpedia and Word-
Net are used to match and generalize constituents
from QA pairs: such matches are then used in
19
We consider the WordNet synsets to be classes in the
scope of our experiments
syntactic/semantic structures. The experiments
with TREC QA and the above representations
also combined with traditional features greatly im-
prove over a strong IR baseline, e.g., 96% on
BM25, and on previous state-of-the-art rerank-
ing models, up to 15.4% (relative improvement)
in P@1. In particular, differently from previous
work, our models can effectively use semantic
knowledge in statistical learning to rank methods.
These promising results open interesting future
directions in designing novel semantic structures
and using innovative semantic representations in
learning algorithms.
Acknowledgments
This research is partially supported by the EU?s 7
th
Framework Program (FP7/2007-2013) (#288024
LIMOSINE project) and by a Shared University
Research award from the IBM Watson Research
Center - Yorktown Heights, USA and the IBM
Center for Advanced Studies of Trento, Italy. The
third author is supported by the Google Europe
Fellowship 2013 award in Machine Learning.
References
Matthew W. Bilotti, Jonathan L. Elsas, Jaime Car-
bonell, and Eric Nyberg. 2010. Rank learning
for factoid question answering with linguistic and
semantic constraints. In Proceedings of the 19th
671
ACM international Conference on Information and
Knowledge Management (CIKM), pages 459?468.
Christian Bizer, Jens Lehmann, Georgi Kobilarov,
S?oren Auer, Christian Becker, Richard Cyganiak,
and Sebastian Hellmann. 2009. Dbpedia - a crys-
tallization point for the web of data. Web Seman-
tics: Science, Services and Agents on the World Wide
Web, 7(3):154?165, September.
Andras Csomai and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll,
James Fan, David Gondek, Aditya Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John
Prager, Nico Schlaefer, and Chris Welty. 2010.
Building watson: An overview of the deepqa
project. AI Magazine, 31(3).
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining
(KDD), pages 133?142. ACM.
Aditya Kalyanpur, J William Murdock, James Fan, and
Christopher Welty. 2011. Leveraging community-
built knowledge for type coercion in question an-
swering. In The Semantic Web?ISWC 2011, pages
144?156. Springer.
David Milne and Ian H Witten. 2009. An open-source
toolkit for mining wikipedia. In New Zealand Com-
puter Science Research Student Conference (NZC-
SRSC).
Alessandro Moschitti and Silvia Quarteroni. 2008.
Kernels on linguistic structures for answer extrac-
tion. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics on Human Language Technologies: Short Pa-
pers (ACL), pages 113?116.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
syntactic and shallow semantic kernels for ques-
tion/answer classification. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL), pages 776?783.
Alessandro Moschitti. 2006. Efficient convolution
kernels for dependency and constituent syntactic
trees. In Proceedings of the 17th European Confer-
ence on Machine Learning (ECML), pages 318?329.
Springer.
Alessandro Moschitti. 2009. Syntactic and seman-
tic kernels for short text pair categorization. In
Proceedings of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL), pages 576?584. Association for
Computational Linguistics.
J William Murdock, Aditya Kalyanpur, Chris Welty,
James Fan, David A Ferrucci, DC Gondek, Lei
Zhang, and Hiroshi Kanayama. 2012. Typing can-
didate answers using type coercion. IBM Journal of
Research and Development, 56(3.4):7?1.
Aliaksei Severyn and Alessandro Moschitti. 2012.
Structural relationships for large-scale learning of
answer re-ranking. In Proceedings of the 35th in-
ternational ACM SIGIR conference on Research and
development in information retrieval (SIGIR), pages
741?750. ACM.
Aliaksei Severyn and Alessandro Moschitti. 2013. Au-
tomatic feature engineering for answer selection and
extraction. In Proceedings of the 2013 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 458?467.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013a. Building structures from clas-
sifiers for passage reranking. In Proceedings of the
22nd ACM international conference on Conference
on information & knowledge management (CIKM),
pages 969?978. ACM.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013b. Learning adaptable patterns for
passage reranking. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL).
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In Proceedings of the 16th international con-
ference on World Wide Web (WWW), pages 697?706.
ACM Press.
Ellen M Voorhees. 2001. Overview of the TREC
2001 Question Answering Track. In Proceedings of
TREC, pages 42?51.
672
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 214?217,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
FBK-IRST: Semantic Relation Extraction using Cyc
Kateryna Tymoshenko and Claudio Giuliano
FBK-IRST
I-38050, Povo (TN), Italy
tymoshenko@fbk.eu, giuliano@fbk.eu
Abstract
We present an approach for semantic re-
lation extraction between nominals that
combines semantic information with shal-
low syntactic processing. We propose to
use the ResearchCyc knowledge base as
a source of semantic information about
nominals. Each source of information
is represented by a specific kernel func-
tion. The experiments were carried out
using support vector machines as a clas-
sifier. The system achieves an overall F
1
of 77.62% on the ?Multi-Way Classifica-
tion of Semantic Relations Between Pairs
of Nominals? task at SemEval-2010.
1 Introduction
The SemEval-2010 Task 8 ?Multi-Way Classifi-
cation of Semantic Relations Between Pairs of
Nominals? consists in identifying which seman-
tic relation holds between two nominals in a sen-
tence (Hendrickx et al, 2010). The set of rela-
tions is composed of nine mutually exclusive se-
mantic relations and the Other relation. Specifi-
cally, the task requires to return the most informa-
tive relation between the specified pair of nomi-
nals e
1
and e
2
taking into account their order. An-
notation guidelines show that semantic knowledge
about e
1
and e
2
plays a very important role in dis-
tinguishing among different relations. For exam-
ple, relations Cause-Effect and Product-Producer
are closely related. One of the restrictions which
might help to distinguish between them is that
products must be concrete physical entities, while
effects must not.
Recently, there has emerged a large number of
freely available large-scale knowledge bases. The
ground idea of our research is to use them as
source of semantic information. Among such re-
sources there are DBpedia,
1
YAGO,
2
and Open-
Cyc.
3
On the one hand, DBpedia and YAGO have
been automatically extracted from Wikipedia.
They have a good coverage of named entities, but
their coverage of common nouns is poorer. They
seem to be more suitable for relation extraction be-
tween named entities. On the other hand, Cyc is
a manually designed knowledge base, which de-
scribes actions and entities both in common life
and in specific domains (Lenat, 1995). Cyc has
a good coverage of common nouns, making it in-
teresting for our task. The full version of Cyc is
freely available to the research community as Re-
searchCyc.
4
We approached the task using the system intro-
duced by Giuliano et al (2007) as a basis. They
exploited two information sources: the whole sen-
tence where the relation appears, and WordNet
synonymy and hyperonymy information. In this
paper, we (i) investigate usage of Cyc as a source
of semantic knowledge and (ii) linguistic infor-
mation, which give useful clues to semantic re-
lation extraction. From Cyc, we obtain informa-
tion about super-classes (in the Cyc terminology
generalizations) of the classes which correspond
to nominals in a sentence. The sentence itself
provides linguistic information, such as local con-
texts of entities, bag of verbs and distance between
nominals in the context.
The different sources of information are rep-
resented by kernel functions. The final system
is based on four kernels (i.e., local context ker-
nel, distance kernel, verbs kernel and generaliza-
tion kernel). The experiments were carried out us-
ing support vector machines (Vapnik, 1998) as a
classifier. The system achieves an overall F
1
of
1
http://dbpedia.org/
2
http://www.mpi-inf.mpg.de/yago-naga/
yago/
3
http://www.cyc.com/opencyc
4
http://research.cyc.com/
214
77.62%.
2 Kernel Methods for Relation
Extraction
In order to implement the approach based on shal-
low syntactic and semantic information, we em-
ployed a linear combination of kernels, using the
support vector machines as a classifier. We de-
veloped two types of basic kernels: syntactic and
semantic kernels. They were combined by exploit-
ing the closure properties of kernels. We define the
composite kernelK
C
(x
1
, x
2
) as follows.
n
?
i=1
K
i
(x
1
, x
2
)
?
K
i
(x
1
, x
1
)K
i
(x
2
, x
2
)
. (1)
Each basic kernelK
i
is normalized.
All the basic kernels are explicitly calculated as
follows
K
i
(x
1
, x
2
) = ??(x
1
), ?(x
2
)? , (2)
where ?(?) is the embedding vector. The resulting
feature space has high dimensionality. However,
Equation 2 can be efficiently computed explicitly
because the representations of input are extremely
sparse.
2.1 Local context kernel
Local context is represented by terms, lemmata,
PoS tags, and orthographic features extracted
from a window around the nominals considering
the token order. Formally, given a relation ex-
ample R, we represent a local context LC =
t
?w
, ..., t
?1
, t
0
, t
+1
, ..., t
+w
as a row vector
?
LC
(R) = (tf
1
(LC), tf
2
(LC), ..., tf
m
(LC) ) ? {0, 1}
m
,
(3)
where tf
i
is a feature function which returns 1
if the feature is active in the specified position
of LC; 0 otherwise. The local context kernel
K
LC
(R
1
, R
2
) is defined as
K
LC e1
(R
1
, R
2
) +K
LC e2
(R
1
, R
2
), (4)
where K
LC e1
and K
LC e2
are defined by substi-
tuting the embedding of the local contexts of e
1
and e
2
into Equation 2, respectively.
2.2 Verb kernel
The verb kernel operates on the verbs present in
the sentence,
5
representing it as a bag-of-verbs.
5
On average there are 2.65 verbs per sentence
More formally, given a relation example R, we
represent the verbs from it as a row vector
?
V
(R) = (vf(v
1
, R), ..., vf(v
l
, R)) ? {0, 1}
l
, (5)
where the binary function vf(v
i
, R) shows if a
particular verb is used in R. By substituting
?
V
(R) into Equation 2 we obtain the bag-of-verbs
kernelK
V
.
2.3 Distance kernel
Given a relation example R(e
1
, e
2
), we repre-
sent the distance between the nominals as a one-
dimensional vector
?
D
(R) =
1
dist(e
1
, e
2
)
? <
1
, (6)
where dist(e
1
, e
2
) is number of tokens between
the nominals e
1
and e
2
in a sentence. By substitut-
ing ?
D
(R) into Equation 2 we obtain the distance
kernelK
D
.
2.4 Cyc-based kernel
Cyc is a comprehensive, manually-build knowl-
edge base developed since 1984 by CycCorp. Ac-
cording to Lenat (1995) it can be considered as
an expert system with domain spanning all ev-
eryday actions and entities, like Fish live in wa-
ter. The open-source version of Cyc named Open-
Cyc, which contains the full Cyc ontology and re-
stricted number of assertions, is freely available
on the web. Also the full power of Cyc has been
made available to the research community via Re-
searchCyc. Cyc knowledge base contains more
than 500,000 concepts and more than 5 million as-
sertions about them. They may refer both to com-
mon human knowledge like food or drinks and to
specialized knowledge in domains like physics or
chemistry. The knowledge base has been formu-
lated using CycL language. A Cyc constant repre-
sents a thing or a concept in the world. It may be
an individual, e.g. BarackObama, or a collection,
e.g. Gun, Screaming.
2.4.1 Generalization kernel
Given a nominal e, we map it to a set of Cyc
constants EC = {c
i
}, using the Cyc function
denotation-mapper. Nominals in Cyc usually de-
note constants-collections. Notice that we do not
performword sense disambiguation. For each c
i
?
EC, we query Cyc for collections which general-
ize it. In Cyc collection X generalizes collection
215
Y if each element of Y is also an element of col-
lectionX . For instance, collection Gun is general-
ized by Weapon, ConventionalWeapon, Mechani-
calDevice and others.
The semantic kernel incorporates the data from
Cyc described above. More formally, given a rela-
tion example R each nominal e is represented as
?
EC
(R) = (fc(c
1
, e), ..., fc(c
k
, e)) ? {0, 1}
k
, (7)
where the binary function fc(c
i
, e) shows if a par-
ticular Cyc collection c
i
is a generalization of e.
The bag-of-generalizations kernel
K
genls
(R
1
, R
2
) is defined as
K
genls e1
(R
1
, R
2
) +K
genls e2
(R
1
, R
2
) , (8)
whereK
genls e1
andK
genls e2
are defined by sub-
stituting the embedding of generalizations e
1
and
e
2
into Equation 2 respectively.
3 Experimental setup and Results
Sentences have been tokenized, lemmatized and
PoS tagged with TextPro.
6
Information for gener-
alization kernel has been obtained from Research-
Cyc. All the experiments were performed using
jSRE customized to embed our kernels.
7
jSRE
uses the SVM package LIBSVM (Chang and Lin,
2001). The task is casted as multi-class classifica-
tion problem with 19 classes (2 classes for each
relation to encode the directionality and 1 class
to encode Other). The multiple classification task
is handled with One-Against-One technique. The
SVM parameters have been set as follows. The
cost-factor W
i
for a given class i is set to be the
ratio between the number of negative and positive
examples. We used two values of regularization
parameter C: (i) C
def
=
1
?
K(x,x)
where x are
all examples from the training set, (ii) optimized
C
grid
value obtained by brute-force grid search
method. The default value is used for the other
parameters.
Table 1 shows the performance of different ker-
nel combinations, trained on 8000 training exam-
ples, on the test set. The system achieves the
best overall macro-average F
1
of 77.62% using
K
LC
+K
V
+K
D
+K
genls
. Figure 1 shows the
learning curves on the test set. Our experimen-
tal study has shown that the size of the training
6
http://textpro.fbk.eu/
7
jSRE is a Java tool for relation extraction avail-
able at http://tcc.itc.it/research/textec/
tools-resources/jsre.html.
1000 2000 4000 80000.40
0.450.50
0.550.60
0.650.70
0.750.80
0.850.90
Cause-EffectComponent-Whole Content-ContainerEntity-Destination Entity-Origin Instrument-Agency Member-CollectionMessage-TopicProduct-ProducerAll
Number of training examples
F1
Figure 1: Learning curves on the test set per rela-
tion
Kernels P R F
1
K
LC
+K
V
+K
D
+K
genls
74.98 80.69 77.62
K
LC
+K
V
+K
D
+K
genls
* 78.51 76.03 77.11
K
LC
+K
D
+K
genls
* 78.14 75.93 76.91
K
LC
+K
genls
* 78.19 75.70 76.81
K
LC
+K
D
+K
genls
72.98 80.28 76.39
K
LC
+K
genls
73.05 79.98 76.28
Table 1: Performance on the test set. Combina-
tions marked with * were run with C
grid
, others
with C
def
.
set influences the performance of the system. We
observe that when the system is trained on 8000
examples the overall F
1
increases for 14.01% as
compared to the case of 1000 examples.
4 Discussion and error analysis
The experiments have shown thatK
LC
is the core
kernel of our approach. It has good performance
on its own. For instance, it achieves precision of
66.16%, recall 72.67% and F
1
of 69.13% evalu-
ated using 10-fold cross-validation on the training
set.
Relation K
LC
K
LC
+K
genls
?F
1
Cause-Effect 74.29 76.41 2.12
Component-Whole 61.24 66.13 4.89
Content-Container 76.36 79.12 2.76
Entity-Destination 82.85 83.95 1.10
Entity-Origin 72.09 74.13 2.04
Instrument-Agency 57.71 65.51 7.80
Member-Collection 81.30 83.40 2.10
Message-Topic 60.41 69.09 8.68
Product-Producer 55.95 63.52 7.57
Table 2: The contribution of Cyc evaluated on the
training set.
216
Generalization kernel combined with local con-
text kernel gives precision of 70.38%, recall of
76.96%, and F
1
73.47% with the same exper-
imental setting. The increase of F
1
per re-
lation is shown in the Table 2 in the col-
umn ?F
1
. The largest F
1
increase is ob-
served for Instrument-Agency (+7.80%),Message-
Topic (+8.68%) and Product-Producer (+7.57%).
K
genls
reduces the number of misclassifications
between the two directions of the same rela-
tion, like Product-Producer(artist,design). It
also captures the differences among relations,
specified in the annotation guidelines. For in-
stance, the system based only on K
LC
misclass-
fied ?The<e1>species</e1>makes a squelching
<e2>noise</e2>? as Product-Producer(e2,e1).
Generalizations for <e2>noise</e2> provided
by Cyc include Event, MovementEvent, Sound.
According to the annotation guidelines a product
must not be an event. A system based on the com-
bination of K
LC
and K
genls
correctly labels this
example as Cause-Effect(e1,e2).
K
genls
improves the performance in general.
However, in some cases using Cyc as a source of
semantic information is a source of errors. Firstly,
sometimes the set of constants for a given nom-
inal is empty (e.g., disassembler, babel) or does
not include the correct one (noun surge is mapped
to the constant IncreaseEvent). In other cases,
an ambiguous nominal is mapped to many con-
stants at once. For instance, notes is mapped
to a set of constants, which includes Musical-
Note, Note-Document and InformationRecording-
Process. Word sense disambiguation should help
to solve this problem. Other knowledge bases like
DBpedia and FreeBase
8
can be used to overcome
the problem of lack of coverage.
Bag-of-word kernel with all words from the
sentence did not impact the final result.
9
However,
the information about verbs present in the sentence
represented by K
V
helped to improve the perfor-
mance. A preliminary error analysis shows that a
deeper syntactic analysis could help to further im-
prove the performance.
For comparison purposes, we also exploited
WordNet information by means of the supersense
kernel K
SS
(Giuliano et al, 2007). In all exper-
iments, K
SS
was outperformed by K
genls
. For
instance, K
LC
+ K
SS
gives overall F
1
measure
8
http://www.freebase.com/
9
This kernel has been evaluated only on the training data.
of 70.29% with the same experimental setting as
described in the beginning of this section.
5 Conclusion
The paper describes a system for semantic rela-
tions extraction, based on the usage of semantic
information provided by ResearchCyc and shal-
low syntactic features. The experiments have
shown that the external knowledge, encoded as
super-class information from ResearchCyc with-
out any word sense disambiguation, significantly
contributes to improve overall performance of the
system. The problem of the lack of coverage may
be overcome by the usage of other large-scale
knowledge bases, such as DBpedia. For future
work, we will try to use the Cyc inference en-
gine to obtain implicit information about nominals
in addition to the information about their super-
classes and perform word sense disambiguation.
Acknowledgments
The research leading to these results has received funding
from the ITCH project (http://itch.fbk.eu), spon-
sored by the Italian Ministry of University and Research and
by the Autonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint Research
Project under Future Internet - Internet of Content program
of the Information Technology Center, Fondazione Bruno
Kessler.
References
Chih-Chung Chang and Chih-Jen Lin, 2001. LIB-
SVM: a library for support vector machines. Soft-
ware available at http://www.csie.ntu.edu.tw/
?cjlin/libsvm.
Claudio Giuliano, Alberto Lavelli, Daniele Pighin, and
Lorenza Romano. 2007. Fbk-irst: Kernel methods
for semantic relation extraction. In Proceedings of the
Fourth International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
Nakov, Diarmuid
?
O S?eaghdha, Sebastian Pad?o, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz.
2010. Semeval-2010 task 8: Multi-way classification of
semantic relations between pairs of nominals. In Proceed-
ings of the 5th SIGLEXWorkshop on Semantic Evaluation,
Uppsala, Sweden.
Douglas B. Lenat. 1995. CYC: A large-scale investment in
knowledge infrastructure. Communications of the ACM,
38(11):33?38.
Vladimir N. Vapnik. 1998. Statistical Learning Theory.
Wiley-Interscience, September.
217
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 19?27,
Beijing, August 2010
Extending English ACE 2005 Corpus Annotation with Ground-truth
Links to Wikipedia
Luisa Bentivogli
FBK-Irst
bentivo@fbk.eu
Pamela Forner
CELCT
forner@celct.it
Claudio Giuliano
FBK-Irst
giuliano@fbk.eu
Alessandro Marchetti
CELCT
amarchetti@celct.it
Emanuele Pianta
FBK-Irst
pianta@fbk.eu
Kateryna Tymoshenko
FBK-Irst
tymoshenko@fbk.eu
Abstract
This paper describes an on-going annota-
tion effort which aims at adding a man-
ual annotation layer connecting an exist-
ing annotated corpus such as the English
ACE-2005 Corpus to Wikipedia. The an-
notation layer is intended for the evalua-
tion of accuracy of linking to Wikipedia in
the framework of a coreference resolution
system.
1 Introduction
Collaboratively Constructed Resources (CCR)
such as Wikipedia are starting to be used for a
number of semantic processing tasks that up to
few years ago could only rely on few manually
constructed resources such as WordNet and Sem-
Cor (Fellbaum, 1998). The impact of the new re-
sources can be multiplied by connecting them to
other existing datasets, e.g. reference corpora. In
this paper we will illustrate an on-going annota-
tion effort which aims at adding a manual anno-
tation layer connecting an existing annotated cor-
pus such as the English ACE-2005 dataset1 to a
CCR such as Wikipedia. This effort will produce
a new integrated resource which can be useful for
the coreference resolution task.
Coreference resolution is the task of identify-
ing which mentions, i.e. individual textual de-
scriptions usually realized as noun phrases or pro-
nouns, refer to the same entity. To solve this
task, especially in the case of non-pronominal co-
reference, researchers have recently started to ex-
ploit semantic knowledge, e.g. trying to calculate
1http://projects.ldc.upenn.edu/ace/
the semantic similarity of mentions (Ponzetto and
Strube, 2006) or their semantic classes (Ng, 2007;
Soon et al, 2001). Up to now, WordNet has been
one of the most frequently used sources of se-
mantic knowledge for the coreference resolution
task (Soon et al, 2001; Ng and Cardie, 2002). Re-
searchers have shown, however, that WordNet has
some limits. On one hand, although WordNet has
a big coverage of the English language in terms
of common nouns, it still has a limited coverage
of proper nouns (e.g. Barack Obama is not avail-
able in the on-line version) and entity descrip-
tions (e.g. president of India). On the other hand
WordNet sense inventory is considered too fine-
grained (Ponzetto and Strube, 2006; Mihalcea and
Moldovan, 2001). In alternative, it has been re-
cently shown that Wikipedia can be a promising
source of semantic knowledge for coreference res-
olution between nominals (Ponzetto and Strube,
2006).
Consider some possible uses of Wikipedia.
For example, knowing that the entity men-
tion ?Obama? is described on the Wikipedia
page Barack_Obama2, one can benefit from
the Wikipedia category structure. Categories as-
signed to the Barack_Obama page can be used
as semantic classes, e.g. ?21st-century presidents
of the United States?. Another example of a
useful Wikipedia feature are the links between
Wikipedia pages. For instance, some Wikipedia
pages contain links to the Barack_Obama page.
Anchor texts of these links can provide alterna-
2The links to Wikipedia pages are given displaying only
the last part of the link which corresponds to the title of the
page. The complete link can be obtained adding this part to
http://en.wikipedia.org/wiki/.
19
tive names of this entity, e.g. ?Barack Hussein
Obama? or ?Barack Obama Junior?.
Naturally, in order to obtain semantic knowl-
edge about an entity mention from Wikipedia
one should link this mention to an appropriate
Wikipedia page, i.e. to disambiguate it using
Wikipedia as a sense inventory. The accuracy
of linking entity mentions to Wikipedia is a very
important issue. For example, such linking is a
step of the approach to coreference resolution de-
scribed in (Bryl et al, 2010). In order to evaluate
this accuracy in the framework of a coreference
resolution system, a corpus of documents, where
entity mentions are annotated with ground-truth
links to Wikipedia, is required.
The possible solution of this problem is to ex-
tend the annotation of entity mentions in a corefer-
ence resolution corpus. In the recent years, coref-
erence resolution systems have been evaluated on
various versions of the English Automatic Content
Extraction (ACE) corpus (Ponzetto and Strube,
2006; Versley et al, 2008; Ng, 2007; Culotta et
al., 2007; Bryl et al, 2010). The latest publicly
available version is ACE 20053.
In this paper we present an extension of ACE
2005 non-pronominal entity mention annotations
with ground-truth links to Wikipedia. This exten-
sion is intended for evaluation of accuracy of link-
ing entity mentions to Wikipedia pages. The an-
notation is currently in progress. At the moment
of writing this paper we have completed around
55% of the work. The extension can be exploited
by coreference resolution systems, which already
use ACE 2005 corpus for development and testing
purposes, e.g. (Bryl et al, 2010). Moreover, En-
glish ACE 2005 corpus is multi-purpose and can
be used in other information extraction (IE) tasks
as well, e.g. relation extraction. Therefore, we
believe that our extension might also be useful for
other IE tasks, which exploit semantic knowledge.
In the following we start by providing a brief
overview of the existing corpora annotated with
links to Wikipedia. In Section 3 we describe some
characteristics of the English ACE 2005 corpus,
which are relevant to the creation of the extension.
Next, we describe the general annotation princi-
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
ples and the procedure adopted to carry out the
annotation. In Section 4 we present some anal-
yses of the annotation and statistics about Inter-
Annotator Agreement.
2 Related work
Recent approaches to linking terms to Wikipedia
pages (Cucerzan, 2007; Csomai and Mihalcea,
2008; Milne and Witten, 2008; Kulkarni et al,
2009) have used two kinds of corpora for eval-
uation of accuracy: (i) sets of Wikipedia pages
and (ii) manually annotated corpora. In Wikipedia
pages links are added to terms ?only where
they are relevant to the context?4. Therefore,
Wikipedia pages do not contain the full annotation
of all entity mentions. This observation applies
equally to the corpus used by (Milne and Wit-
ten, 2008), which includes 50 documents from the
AQUAINT corpus annotated following the same
strategy5. The corpus created by (Cucerzan, 2007)
contains annotation of named entities only6. It
contains 756 annotations, therefore for our pur-
poses it is limited in terms of size.
Kulkarni et al (2009) have annotated 109 doc-
uments collected from homepages of various sites
with as many links as possible7. Their annotation
is too extensive for our purposes, since they do not
limit annotation to the entity mentions. To tackle
this issue, one can use an automatic entity mention
detector, however it is likely to introduce noise.
3 Creating the extension
The task consists of manually annotating the
non-pronominal mentions contained in the En-
glish ACE 2005 corpus with links to appropriate
Wikipedia articles. The objective of the work is
to create an extension of ACE 2005, where all the
mentions contained in the ACE 2005 corpus are
disambiguated using Wikipedia as a sense reposi-
tory to point to. The extension is intended for the
4http://en.wikipedia.org/wiki/
Wikipedia:Manual_of_Style
5http://www.nzdl.org/wikification/
docs.html
6http://research.microsoft.com/en-us/
um/people/silviu/WebAssistant/TestData/
7http://soumen.cse.iitb.ac.in/?soumen/
doc/CSAW/
20
evaluation of accuracy of linking to Wikipedia in
the framework of a coreference resolution system.
3.1 The English ACE 2005 Corpus
The English ACE 2005 corpus is composed of
599 articles assembled from a variety of sources
selected from broadcast news programs, newspa-
pers, newswire reports, internet sources and from
transcribed audio. It contains the annotation of a
series of entities (person, location, organization)
for a total of 15,382 different entities and 43,624
mentions of these entities. A mention is an in-
stance of a textual reference to an object, which
can be either named (e.g. Barack Obama), nom-
inal (e.g. the president), or pronominal (e.g. he,
his, it). An entity is an aggregate of all the men-
tions which refer to one conceptual entity. Beyond
the annotation of entities and mentions, ACE 05
contains also the annotation of local co-reference
for the entities; this means that mentions which
refer to the same entity in a document have been
marked with the same ID.
3.2 Annotating ACE 05 with Wikipedia
Pages
For the purpose of our task, not all the
ACE 05 mentions are annotated, but only the
named (henceforth NAM) and nominal (hence-
forth NOM) mentions. The resulting additional
annotation layer will contain a total of 29,300
mentions linked to Wikipedia pages. As specif-
ically regards the annotation of NAM mentions,
information about local coreference contained in
ACE 05 has been exploited in order to speed up
the annotation process. In fact, only the first
occurrence of the NAM mentions in each doc-
ument has been annotated and the annotation is
then propagated to all the other co-referring NAM
mentions in the document.
Finally, it must be noted that in ACE 05, given
a complex entity description, both the full ex-
tent of the mention (e.g. president of the United
States) and its syntactic head (e.g. ?president?)
are marked. In our Wikipedia extension only the
head of the mention is annotated, while the full ex-
tent of the mention is available from the original
ACE 05 corpus.
3.3 General Annotation Principles
Depending on the mention type to be annotated,
i.e. NAM or NOM, a different annotation strategy
has been followed. Each mention of type NAM
is annotated with a link to a Wikipedia page de-
scribing the referred entity. For instance, ?George
Bush? is annotated with a link to the Wikipedia
page George_W._Bush.
NOM mentions are annotated with a link to the
Wikipedia page which provides a description of
its appropriate sense. For instance, in the exam-
ple ?I was driving Northwest of Baghdad and I
bumped into these guys going around the capi-
tal? the mention ?capital? is linked to the page
which provides a description of its meaning, i.e.
Capital_(political). Note that the object
of linking is the textual description of an entity,
and not the entity itself. In the example, even
though from the context it is clear that the mention
?capital? refers to Baghdad, we provide a link to
the concept of capital and not to the entity Bagdad.
As a term can have both a more generic sense
and a more specific one, depending on the context
in which it occurs, mentions of type NOM can of-
ten be linked to more than one Wikipedia page.
Whenever possible, the NOM mentions are anno-
tated with a list of links to appropriate Wikipedia
pages in the given context. In such cases, links
are sorted in order of relevance, where the first
link corresponds to the most specific sense for that
term in its context, and therefore is regarded as the
best choice. For instance, for the NOM mention
head ?President? which in the context identifies
the United States President George Bush the an-
notation?s purpose is to provide a description of
the item ?President?, so the following links are
selected as appropriate: President_of_the_
United_States and President.
The correct interpretation of the term is strictly
related to the context in which the term occurs.
While performing the annotation, the context of
the entire document has always been exploited in
order to correctly identify the specific sense of the
mention.
3.4 Annotation Procedure
The annotation procedure requires that the men-
tion string is searched in Wikipedia in order to
21
find the appropriate page(s) to be used for anno-
tating the mention. In the annotation exercise, the
annotators have always taken into consideration
the context where a mention occurs, searching for
both the generic and the most specific sense of the
mention disambiguated in the context. In fact, in
the example provided above, not only ?President?,
but also ?President of the United States? has been
queried in Wikipedia as required by the context.
Not only the context, but also some features of
Wikipedia must be mentioned as they affect the
annotation procedure:
a. One element which contributes to the choice
of the appropriate Wikipedia page(s) for
one mention is the list of links proposed in
Wikipedia?s Disambiguation pages. Disam-
biguation pages are non-article pages which
are intended to allow the user to choose from
a list of Wikipedia articles defining different
meanings of a term, when the term is am-
biguous. Disambiguation pages cannot be
used as links for the annotation as they are
not suitable for the purposes of this task. In
fact, the annotator?s task is to disambiguate
the meaning of the mention, so one link,
pointing to a specific sense, is to be cho-
sen. Disambiguation pages should always be
checked as they provide useful suggestions
in order to reach the appropriate link(s).
b. In the same way as Disambiguation pages,
Wikitionary cannot be used as linking page,
as it provides a list of possible senses for a
term and not only one specific sense which is
necessary to disambiguate the mention.
c. In Wikipedia, terms may be redirected to
other terms which are related in terms of
morphological derivation; i.e. searching for
the term ?Senator? you are automatically
redirected to ?Senate?; or querying ?citizen?
you are automatically redirected to ?citizen-
ship?. Redirections have always been con-
sidered appropriate links for the term.
Some particular rules have been followed in order
to deal with specific cases in the annotation, which
are described below:
1. As explained before in Section 3.2, as a gen-
eral rule the head of the ACE 05 mention
is annotated with Wikipedia links. In those
cases where the syntactic head of the men-
tion is a multiword lexical unit, the ACE 05
practice is to mark as head only the rightmost
item of the multiword. For instance, in the
case of the multiword ?flight attendant? only
?attendant? is marked as head of the men-
tion, although ?flight attendant? is clearly a
multiword lexical unit that should be anno-
tated as one semantic whole. In our anno-
tation we take into account the meaning of
the whole lexical unit; so, in the above exam-
ple, the generic sense of ?attendant? has not
been given, whereas Flight_attendant
is considered as the appropriate link.
2. In some cases, in ACE 2005 pronouns like
?somebody?, ?anybody?, ?anyone?, ?one?,
?others?, were incorrectly marked as NOM
(instead of PRO). Such cases, which amount
to 117, have been marked with the tag ?No
Annotation?.
3. When a page exists in Wikipedia for a given
mention but not for the specific sense in that
context the ?Missing sense? annotation has
been used. One example of ?Missing sense?
is for instance the term ?heart? which has 29
links proposed in the ?Disambiguation page?
touching different categories (sport, science,
anthropology, gaming, etc.), but there is no
link pointing to the sense of ?center or core of
something?; so, when referring to the heart
of a city, the term has been marked as ?Miss-
ing sense?.
4. When no article exists in Wikipedia for a
given mention, the tag ?No page? has been
adopted.
5. Nicknames, i.e. descriptive names used
in place of or in addition to the official
name(s) of a person, have been treated as
NAM. Thus, even if nicknames look like de-
scriptions of individuals (and their reference
should not be solved, following the general
rule), they are actually used and annotated as
22
Number of annotated mentions 16310
Number of single link mentions 13774
Number of multi-link mentions 1458
Number of ?No Page? annotations 481
Number of ?Missing Sense? 480
annotations
Number of ?No Annotation? 117
annotations
Total number of links 16851
Total number of links in multi-link 3077
mentions
Table 1: Annotation data
proper names aliases. For example, given the
mention ?Butcher of Baghdad?, whose head
?Butcher? is to be annotated, the appropriate
Wikipedia link is Saddam_Hussein, auto-
matically redirected from the searched string
?Butcher of Baghdad?. The link Butcher
is not appropriate as it provides a description
of the mention. It is interesting the fact that
Wikipedia itself redirects to the page of Sad-
dam Hussein.
4 The ACE05-WIKI Extension
Up to now, the 55% of the markable men-
tions have been annotated by one annotator,
amounting to 16,310 mentions. This annotation
has been carried out by CELCT in a period
of two months from February 22 to April 30,
2010, using the on-line version of Wikipedia,
while the remaining 45% of the ACE mentions
will be annotated during August 2010. The
complete annotation will be freely available
at: http://www.celct.it/resources.
php?id_page=acewiki2010, while the
ACE 2005 corpus is distributed by LDC8.
4.1 Annotation Data Analysis
Table 1 gives some statistics about the overall
annotation. In the following sections, mentions
annotated with one link are called ?single link?,
whereas, mentions annotated with more than one
link are named ?multi-link?.
These data refer to the annotation of each sin-
gle mention. It is not possible to give statis-
tics at the entity level, as mentions have differ-
8http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
Annotation Mention Type
NAM NOM
Single link mentions 6589 7185
Multi-link mentions 79 1379
Missing sense 96 384
No Page 440 41
Table 2: Distinction of NAM and NOM in the an-
notation
ent ID depending on the documents they belong
to, and the information about the cross-document
co-reference is not available. Moreover, mentions
of type NOM are annotated with different links
depending on their disambiguated sense, making
thus impossible to group them together.
Most mentions have been annotated with only
one link; if we consider multi-link mentions, we
can say that each mention has been assigned an
average of 2,11 links (3,077/1,458).
Data about ?Missing sense? and ?No page?
are important as they provide useful information
about the coverage of Wikipedia as sense in-
ventory. Considering both ?Missing sense? and
?No page? annotations, the total number of men-
tions which have not been linked to a Wikipedia
page amounts to 6%, equally distributed between
?Missing sense? and ?No page? annotations. This
fact proves that, regarded as a sense inventory,
Wikipedia has a broad coverage. As Table 2
shows, the mentions for which more than one link
was deemed appropriate are mostly of type NOM,
while NAM mentions have been almost exclu-
sively annotated with one link only. The very few
cases in which a NAM mention is linked to more
than one Wikipedia page are primarily due to (i)
mistakes in the ACE 05 annotation (for example,
the mention ?President? was erroneously marked
as a NAM); (ii) or to cases where nouns marked
as NAM could also be considered as NOMs (see
for instance the mention ?Marine?, to mean the
Marine Corps).
Table 2 provides also statistics about the ?Miss-
ing sense? and ?No page? cases provided on men-
tions divided among the NAM and NOM type.
The ?missing sense? annotation concerns mostly
the NOM category, whereas the NAM category
is hardly affected. This attests the fact that per-
sons, locations and organizations are well repre-
23
sented in Wikipedia. This is mainly due to the
encyclopedic nature of Wikipedia where an arti-
cle may be about a person, a concept, a place,
an event, a thing etc.; instead, information about
nouns (NOM) is more likely to be found in a
dictionary, where information about the meanings
and usage of a term is provided.
4.2 Inter-Annotator Agreement
About 3,100 mentions, representing more than
10% of the mentions to be annotated, have been
annotated by two annotators in order to calculate
Inter-Annotator Agreement.
Once the annotations were completed, the
two annotators carried out a reconciliation phase
where they compared the two sets of links pro-
duced. Discrepancies in the annotation were
checked with the aim of removing only the more
rough errors and oversights. No changes have
been made in the cases of substantial disagree-
ment, which has been maintained.
In order to measure Inter-Annotator Agree-
ment, two metrics were used: (i) the Dice coeffi-
cient to measure the agreements on the set of links
used in the annotation9 and (ii) two measures of
agreement calculated at the mention level, i.e. on
the group of links associated to each mention.
The Dice coefficient is computed as follows:
Dice = 2C/(A + B)
where C is the number of common links chosen by
the two annotators, while A and B are respectively
the total number of links selected by the first and
the second annotator. Table 3 shows the results
obtained both before and after the reconciliation
9The Dice coefficient is a typical measure used to com-
pare sets in IR and is also used to calculate inter-annotator
agreement in a number of tasks where an assessor is allowed
to select a set of labels to apply to each observation. In fact,
in these cases measures such as the widely used K are not
good to calculate agreement. This is because K only offers
a dichotomous distinction between agreement and disagree-
ment, whereas what is needed is a coefficient that also allows
for partial disagreement between judgments. In fact, in our
case we often have a partial agreement on the set of links
given for each mention. Also considering only the mentions
for which a single link has been chosen, it is not possible
to calculate K statistics in a straightforward way as the cate-
gories (i.e. the possible Wikipedia pages) in some cases can-
not be determined a priori and are different for each mention.
Due to these factors chance agreement cannot be calculated
in an appropriate way.
BEFORE AFTER
reconciliation reconciliation
DICE 0.85 0.94
Table 3: Statistics about Dice coefficient
BEFORE AFTER
reconciliation reconciliation
Complete 77.98% 91.82%
On first link 84.41% 95.58%
Table 4: Agreement at the mention level
process. Agreement before reconciliation is satis-
factory and shows the feasibility of the annotation
task and the reliability of the annotation scheme.
Two measures of agreement at the mention
level are also calculated. To this purpose, we
count the number of mentions where annotators
agree, as opposed to considering the agreement on
each link separately. Mention-level agreement is
calculated as follows:
Number of mentions with annotation in agreement
Total number of annotated mentions
We calculate both ?complete? agreement and
agreement on the first link. As regards the first
measure, a mention is considered in complete
agreement if (i) it has been annotated with the
same link(s) and (ii) in the case of multi-link men-
tions, links are given in the same order. As for the
second measure, there is agreement on a mention
if both the annotators chose the same first link (i.e.
the one judged as the most appropriate), regard-
less of other possible links assigned to that men-
tion. Table 4 provides data about both complete
agreement and first link agreement, calculated be-
fore and after the annotators reconciliation.
4.3 Disagreement Analysis
Considering the 3,144 double-annotated men-
tions, the cases of disagreements amount to 692
(22,02%) before the reconciliation while they are
reduced to 257 (8,18%) after that process. It is in-
teresting to point out that the disagreements affect
the mentions of type NOM in most of the cases,
whereas mentions of type NAM are involved only
in 3,8% of the cases.
Examining the two annotations after the recon-
ciliation, it is possible to distinguish three kinds
of disagreement which are shown in Table 5 to-
24
Number of
Disagreement type Disagreements
1) No matching in the link(s)
proposed
105 (40,85%)
2) No matching on the first link,
but at least one of the other links
is the same
14 (5,45%)
3) Matching on the first link and
mismatch on the number of ad-
ditional links
138 (53,70%)
Total Disagreements 257
Table 5: Types of disagreements
gether with the data about their distribution. An
example of disagreement of type (1) is the anno-
tation of the mention ?crossing?, in the following
context: ?Marines from the 1st division have se-
cured a key Tigris River Crossing?. Searching for
the word ?river crossing? in the Wikipedia search-
box, the Disambiguation Page is opened and a
list of possible links referring to more specific
senses of the term are offered, while the generic
?river crossing? sense is missing. The annota-
tors are required to choose just one of the possi-
ble senses provided and they chose two different
links pointing to pages of more specific senses:
{Ford_%28river%29} and {Bridge}.
Another example is represented by the annota-
tion of the mention ?area? in the context : ?Both
aircraft fly at 125 miles per hour gingerly over en-
emy area?. In Wikipedia no page exists for the
specific sense of ?area? appropriate in the con-
text. Searching for ?area? in Wikipedia, the page
obtained is not suitable, and the Disambiguation
page offers a list of various possible links to either
more specific or more general senses of the term.
One annotator judged the more general Wikipedia
page Area_(subnational_entity) as ap-
propriate to annotate the mention, while the sec-
ond annotator deemed the page not suitable and
thus used the ?Missing sense? annotation.
Disagreement of type (2) refers to cases where
at least one of the links proposed by the annota-
tors is the same, but the first (i.e. the one judged
as the most suitable) is different. Given the fol-
lowing context: ?Tom, You know what Liber-
als want?, the two annotation sets provided for
the mention ?Liberal? are: {Liberalism} and
{Liberal_Party, Modern_liberalism_
in_the_United_States, Liberalism}.
The first annotator provided only one link for
the mention ?liberal?, which is different from the
first link provided by second annotator. However,
the second annotator provided also other links,
among which there is the link provided by the first
annotator.
Another example is represented by the annota-
tion of the mention ?killer?. Given the context:
?He?d be the 11th killer put to death in Texas?, the
two annotators provided the following link sets:
{Assassination, Murder} and {Murder}.
Starting from the Wikipedia disambiguation page,
the two annotators agreed on the choice of one of
the links but not on the first one.
Disagreement of type (3) refers to cases where
both annotators agree on the first link, correspond-
ing to the most specific sense, but one of them
also added link(s) considered appropriate to an-
notate the mention. Given the context: ?7th Cav-
alry has just taken three Iraqi prisoners?, the an-
notations provided for the term ?prisoners? are:
{Prisoner_of_war} and {Prisoner_of_
war, Incarceration}. This happens when
more than one Wikipedia pages are appropriate to
describe the mention.
As regards the causes of disagreement, we see
that the cases of disagreement mentioned above
are due to two main reasons:
a. The lack of the appropriate sense in
Wikipedia for the given mention
b. The different interpretation of the context in
which the mention occurs.
In cases of type (a) the annotators adopted differ-
ent strategies to perform their task, that is:
i. they selected a more general sense (i.e.
?area? which has been annotated with
Area_(subnational_entity)),
ii. they selected a more specific sense (see for
example the annotations of the mentions
?river crossing?).
iii. they selected the related senses proposed by
the Wikipedia Disambiguation page (as in
the annotation of ?killer? in the example
above).
25
Disagreement Reas. a Reas. b Tot
type (see above)
1) No match 95 10 105
2) No match on 4 10 14
first link
3) Mismatch on 138 138
additional links
Total 99 158 257
(38,5%) (61,5%)
Table 6: Distribution of disagreements according
to their cause
iv. they used the tag ?Missing sense?.
As Wikipedia is constantly evolving, adding
new pages and consequently new senses, it is
reasonable to think that the considered elements
might find the appropriate specific/general link as
time goes by.
Case (b) happens when the context is ambigu-
ous and the information provided in the text al-
lows different possible readings of the mention
to be annotated, making thus difficult to disam-
biguate its sense. These cases are independent
from Wikipedia sense repository but are related to
the subjectivity of the annotators and to the inher-
ent ambiguity of text.
Table 6 shows the distribution of disagreements
according to their cause. Disagreements of type 1
and 2 can be due to both a and b reasons, while
disagreements of type 3 are only due to b.
The overall number of disagreements shows
that the cases where the two annotators did not
agree are quite limited, amounting only to 8%.
The analyses of the disagreements show some
characteristics of Wikipedia considered as sense
repository. As reported in Table 8, in the 61,5%
of the cases of disagreement, the different anno-
tations are caused by the diverse interpretation
of the context and not by the lack of senses in
Wikipedia. It is clear that Wikipedia has a good
coverage and it proves to be a good sense disam-
biguation tool. In some cases it reveals to be too
fine-grained and in other cases it remains at a more
general level.
5 Conclusion
This paper has presented an annotation work
which connects an existing annotated corpus such
as the English ACE 2005 dataset to a Collabo-
ratively Constructed Semantic Resource such as
Wikipedia. Thanks to this connection Wikipedia
becomes an essential semantic resource for the
task of coreference resolution. On one hand, by
taking advantage of the already existing annota-
tions, with a relatively limited additional effort,
we enriched an existing corpus and made it useful
for a new NLP task which was not planned when
the corpus was created. On the other hand, our
work allowed us to explore and better understand
certain characteristics of the Wikipedia resource.
For example we were able to demonstrate in quan-
titative terms that Wikipedia has a very good cov-
erage, at least as far as the kind of entity men-
tions which are contained in the ACE 2005 dataset
(newswire) is concerned.
Acknowledgments
The research leading to these results has re-
ceived funding from the ITCH project (http://
itch.fbk.eu), sponsored by the Italian Min-
istry of University and Research and by the Au-
tonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint
Research Project under Future Internet - Internet
of Content program of the Information Technol-
ogy Center, Fondazione Bruno Kessler.
We thank Giovanni Moretti from CELCT for
technical assistance.
References
Bryl, Volha, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In
Proceedings of the 19th European Conference on
Artificial Intelligence (ECAI 2010), August.
Csomai, Andras and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Cucerzan, Silviu. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June. Association for Computational Linguistics.
26
Culotta, Aron, Michael L. Wick, and Andrew McCal-
lum. 2007. First-order probabilistic models for
coreference resolution. In Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 81?88.
Fellbaum, Christiane, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Kulkarni, Sayali, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of wikipedia entities in web text. In KDD
?09: Proceedings of the 15th ACM SIGKDD inter-
national conference on Knowledge discovery and
data mining, pages 457?466, New York, NY, USA.
ACM.
Mihalcea, Rada and Dan I. Moldovan. 2001.
Ez.wordnet: Principles for automatic generation of
a coarse grained wordnet. In Russell, Ingrid and
John F. Kolen, editors, FLAIRS Conference, pages
454?458. AAAI Press.
Milne, David and Ian H. Witten. 2008. Learning
to link with wikipedia. In CIKM ?08: Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 509?518, New York,
NY, USA. ACM.
Ng, Vincent and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104?111.
Ng, Vincent. 2007. Semantic class induction and
coreference resolution. In ACL 2007, Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, June 23-30, 2007,
Prague, Czech Republic, pages 536?543.
Ponzetto, S. P. and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 192?
199.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistic, 27(4):521?544.
Versley, Yannick, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
Bart: a modular toolkit for coreference resolution.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies, pages 9?12.
27
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 55?62,
Beijing, August 2010
Identifying and Ranking Topic Clusters in the Blogosphere 
M. Atif Qureshi 
Korea Advanced Institute of  
Science and Technology  
atifms@kaist.ac.kr 
Arjumand Younus                    
Korea Advanced Institute of 
Science and Technology  
arjumandms@kaist.ac.kr 
Muhammad Saeed 
University of Karachi  
saeed@uok.edu.pk 
Nasir Touheed                    
Institute of Business Administra-
tion 
ntouheed@iba.edu.pk 
Abstract 
The blogosphere is a huge collaboratively 
constructed resource containing diverse 
and rich information. This diversity and 
richness presents a significant research 
challenge to the Information Retrieval 
community. This paper addresses this 
challenge by proposing a method for 
identification of ?topic clusters? within 
the blogosphere where topic clusters 
represent the concept of grouping togeth-
er blogs sharing a common interest i.e. 
topic, the algorithm takes into account 
both the hyperlinked social network of 
blogs along with the content in the blog 
posts. Additionally we use various forms 
and parts-of-speech of the topic to pro-
vide a broader coverage of the blogos-
phere. The next step of the method is to 
assign topic-specific ranks to each blog 
in the cluster using a metric called ?Topic 
Discussion Rank,? that helps in identify-
ing the most influential blog for a specif-
ic topic. We also perform an experimen-
tal evaluation of our method on real blog 
data and show that the proposed method 
reaches a high level of accuracy. 
1 Introduction 
With a proliferation of Web 2.0 services and ap-
plications there has been a major paradigm shift 
in the way we envision the World Wide Web 
(Anderson, 2007; O?Reilly, 2005). Previously the 
Web was considered as a medium to access in-
formation in a read-only fashion. Weblogs or 
blogs is one such application that has played an 
effective role in making the Web a social gather-
ing point for masses. The most appealing aspect 
of blogs is the empowerment they provide to 
people on the World Wide Web by enabling 
them to publish their own opinions, ideas, and 
thoughts on many diverse topics of their own 
interest generally falling into politics, economics, 
sports, technology etc.  A blog is usually like a 
personal diary (Sorapure, 2003) with the differ-
ence that it's now online and accessible to remote 
people, it consists of posts arranged chronologi-
cally by date and it can be updated on a regular 
basis by the author of the blog known as blogger. 
Moreover bloggers have the option to link to 
other blogs thereby creating a social network 
within the world of blogs called the blogos-
phere ? in short the blogosphere is a collabora-
tively constructed resource with rich information 
on a wide spectrum of topics having characteris-
tics very different from the traditional Web. 
   However with these differing characteristics of 
blogs arise many research challenges and this is 
in particular the case for the Information Retriev-
al domain. One important problem that arises 
within this huge blogosphere (Sifry, 2009) is 
with respect to identification of topic clusters. 
Such a task involves identification of the key 
blog clusters that share a common interest point 
(i.e., topic) reflected quite frequently through 
their blog posts. This is a special type of cluster-
55
ing problem with useful applications in the do-
main of blog search as Mishne and de Rijke 
(2006) point out in their study of blog search 
about the concept queries submitted by users of 
blog search systems.  
   Moreover ranking these bloggers with respect 
to their interest in the topic is also a crucial task 
in order to recognize the most influential blogger 
for that specific topic. However the blog ranking 
problem has a completely different nature than 
the web page ranking problem and link populari-
ty based algorithms cannot be applied for ranking 
blogs. The reasons for why link based methods 
cannot be used for blog ranking are as follows:  
 
Blogs have very few links when com-
pared to web pages; Leskovec et al 
report that average number of links 
per blog post is only 1.6 links (2007). 
This small number of links per blog 
results in formation of very sparse 
network especially when trying to find 
blogs relevant to a particular topic. 
Blog posts are associated with a time-
stamp and they need some time for 
getting in-links. In most of the cases 
when they receive the links the topics 
which they talk about die out. 
When link based ranking techniques are 
used for blogs, bloggers at times as-
sume the role of spammers and try to 
exploit the system to boost rank of 
their blogs.  
   In this paper we propose a solution for identifi-
cation of topic clusters from within the blogos-
phere for any topic of interest. We also devise a 
way to assign topic-specific ranks for each iden-
tified blog within the topic cluster. The cluster is 
identified by the calculation of a metric called 
?Topic Discussion Isolation Rank (TDIR).? Each 
blog in the cluster is also assigned a topic rank 
by further calculation of another metric ?Topic 
Discussion Rank (TDR).? The first metric 
"TDIR" is applied to a blog in isolation for the 
topic under consideration and the second metric 
"TDR" takes into account the blog?s role in its 
neighborhood for that specific topic. Our work 
differs from past approaches (Kumar et al, 2003; 
Gruhl et al, 2004; Chi et al, 2007; Li et al, 2009) 
in that it takes into consideration both the links 
between the blogs as well as the content in the 
blog posts whereas a majority of the past me-
thods follow only link structure. Furthermore we 
make use of some natural language processing 
techniques to ensure better coverage of our clus-
ter-finding and ranking methodology. We also 
perform an experimental evaluation of our pro-
posed solution and release the resultant data of 
blog clusters and the ranks as an XML corpus.  
   The remainder of this paper is organized as 
follows. Section 2 presents a brief summary of 
related work in this dimension and explains how 
our proposed methodology differs from these 
works. Section 3 explains the concept of ?topic 
clusters? in detail along with a description of our 
solution for clustering and ranking blogs on basis 
of topics. Section 4 explains our experimental 
methodology and presents our experimental 
evaluations on a corpus of 50,471 blog posts ga-
thered from 102 blogs. Section 5 concludes the 
paper with a discussion of future work in this 
direction.  
2 Related Work 
Given the vast amount of useful information in 
the blogosphere there have been many research 
efforts for mining and analysis of the 
blogosphere. This section reviews some of the 
works that are relevant to our study.  
   There have been several works with respect to 
community detection in the blogosphere: one of 
the oldest works in this dimension is by Kumar 
et al who studied the bursty nature of the 
blogosphere by extracting communities using the 
hyperlinks between the blogs (2003). Gruhl et al 
proposed a transmission graph to study the flow 
of information in the blogosphere and the 
proposed model is based on disease-propagation 
model in epidemic studies (2004). Chi et al 
studied the evolution of blog communities over 
time and introduced the concept of community 
factorization (2007). A fairly recent work is by 
Li et al that studies the information propagation 
pattern in the blogosphere through cascade 
affinity which is an inclination of a blogger to 
join a particular blog community (2009). Apart 
from detection of communities within the 
blogosphere another related study which has 
recently attracted much interest is of identifying 
influentials within a ?blog community? 
(Nakajima et al, 2005; Agarwal et al, 2008). All 
56
these works base their analysis on link structure 
of the blogosphere whereas our analytical model 
differs from these works in that it assigns topic 
based ranks to the blogs by taking into account 
both links and blog post?s contents.  
Along with the community detection problem 
in the blogosphere there has also been an increas-
ing interest in ranking blogs. Fujimura et al 
point out the weak nature of hyperlinks in the 
web blogs and due to that nature they devise a 
ranking algorithm for blog entries that uses the 
structural characteristic of blogs; the algorithm 
enables a new blog entry or other entries that 
have no in-links to be rated according to the past 
performance of the blogger (2005). There is a 
fairly recent work closely related to ours per-
formed by Hassan et al(2009) and this work 
identifies the list of particularly important blogs 
with recurring interest in a specific topic; their 
approach is based on lexical similarity and ran-
dom walks. 
3 Cluster Finding and Ranking Metho-
dology 
In this section we explain the concept of ?topic 
clusters? in detail and go into the details of why 
we deviate from the traditional term of ?blog 
community? in the literature. After this signifi-
cant discussion we then move on to explain our 
proposed method for identification and ranking 
of the ?topic clusters? in the blogosphere: two 
metrics ?topic discussion isolation rank? and 
?topic discussion rank? are used for this purpose.  
3.1 Topic Clusters 
As explained in section 2 the problem of group-
ing together blogs has been referred to as the 
?community detection problem? in the literature. 
However an aspect ignored by most of these 
works is the contents of the blogs. Additionally 
most of the works in this dimension find a blog 
community by following blog threads? discus-
sions/conversations (Nakajima et al, 2005; 
Agarwal et al, 2008) which may not always be 
the case as blogs linking to each other are not 
necessarily part of communications or threads. 
With the advent of micro blogging tools such 
as Twitter (Honeycutt and Herring, 2009) the 
role of blogs as a conversational medium has 
diminished and bloggers link to each other as a 
socially networked cluster by linking to their 
most favorite blogs on their home page as is 
shown in the snapshot of a blog in Figure 1:     
Normally those bloggers link to each other 
that have similar interests and importantly talk 
about same topics. Hence the idea of topic cluster 
is used to extract those clusters from the blogos-
phere that have  strong interest in some specific 
topics which they mention frequently in their 
blog posts and additionally they form a linked 
cluster of blogs. As pointed out by Hassan et al 
the ?task of providing users with a list of particu-
larly important blogs with a recurring interest in 
a specific topic is a problem that is very signifi-
cant in the Information Retrieval domain? 
(2009). For the purpose of solving this problem 
we propose the notion of ?topic clusters.? The 
task is much different from traditional communi-
Figure 1: Blog Showing the List of Blogs it Follows 
57
ty detection in the blogosphere as it utilizes both 
content and link based analysis. The process of 
finding topic clusters is carried out by calculating 
a metric ?Topic Discussion Isolation Rank? 
which we explain in detail in section 3.3. 
3.2 Rank Assignment to Topic Clusters 
As we explained in section 1, due to the unique 
nature of the blogosphere, traditional link-based 
methods such as PageRank (Page et al, 1998) 
may not be appropriate for the ranking task in 
blogs. This is the main reason that we use the 
content of blog posts and lexical similarity in 
blog posts along with links for the rank assign-
ment function that we propose. Furthermore we 
take a blog as aggregate of all its posts for the 
retrieval task.  
3.3 Topic Discussion Isolation Rank 
Topic Discussion Isolation Rank is a metric that 
is used to find the cluster of blogs for a specific 
topic. It takes each blog in isolation and analyses 
the contents of its posts to discover its interest in 
a queried topic. We consider a blog along three 
dimensions as Figure 2 shows:                
As mentioned in section 1 of this paper we 
utilize some natural language processing tech-
niques to ensure better coverage of our cluster-
finding and ranking methodology: those tech-
niques are applied along the part of speech di-
mension shown in Figure 1, for a given topic we 
analyze blog post contents not only for that par-
ticular topic but also for its associated adjectives 
and adverbs i.e. the topic itself is treated as a 
noun and its adjectives and adverbs are also used. 
For example if the topic of interest is ?democra-
cy? we will also analyze the blog post contents 
for adjective ?democratic? and adverb ?demo-
cratically.? Furthermore, a weight in descending 
order is assigned to the noun (denoted as w
n
), 
adjective (denoted as w
adj
) and adverb (denoted 
as w
adv
) of the queried topic where w
n
>w
adj
>w
adv
.  
This approach guarantees better coverage of the 
blogosphere and the chances of missing out blogs 
that have interest in the queried topic are minim-
al. The blog post number denotes the number of 
the post in which the word is found and occur-
rence is a true/false parameter denoting whether 
or not the word exists in the blog post. Based on 
these three dimensions we formulated the TDIR 
metric as follows:  
1+ (n
noun
x w
n
)+(n
adjective 
x w
adj
)+(n
adverb
x w
adv
) 
Number of total posts  
   Here w
n, 
w
adj and 
w
adv 
are as explained pre-
viously in this section and n
noun 
denotes the num-
ber of times nouns are found in all the blog posts, 
n
adjective 
denotes the number of times adjectives 
are found in all the blog posts and n
adverb 
denotes 
the number of times adverbs are found in all the 
blog posts. This metric is calculated for each 
blog in isolation and the blogs that have TDIR 
value of greater than 1 are considered part of the 
topic cluster. 
Additionally we also use various forms of the 
queried topic in the calculation of TDIR as this 
also ensures better coverage during the cluster 
detection process. In the world of the blogos-
phere, bloggers have all the freedom to use what-
ever terms they want to use for a particular topic 
and it is this freedom which adds to the difficulty 
of the Information Retrieval community. Within 
the TDIR metric we propose use of alternate 
terms/spellings/phrases for a given topic ? an 
example being the use of ?Obama? by some 
bloggers and ?United States first Black Presi-
dent? or ?United States? Black President? by oth-
ers. Such ambiguity with respect to posts talking 
about same topic but using different phras-
es/spellings/terms can be resolved by using a 
corpus-based approach with listing of alternate 
phrases and terms for the broad topics. Moreover 
the weights used for each of the part of speech 
?noun?, ?adjective? and ?adverb? in the TDIR 
metric can be adjusted differently for different 
topics with some topics having a stronger indica-
Figure 2: Blog TDIR Dimensions 
58
tion of discussion of that topic through occur-
rence of noun and some through occurrence of 
adjective or adverb. Some examples of these var-
ious measures are shown in our experimental 
evaluations that are explained in section 4. 
3.4 Topic Discussion Rank 
After the cluster-finding phase we perform the 
ranking step by means of Topic Discussion Rank. 
It is in this phase that the socially networked and 
linked blogs play a role in boosting each other?s 
ranks. It is reasonable to assign a higher topic 
rank to a blog that has interest in the specific top-
ic and is also a follower of many blogs with simi-
lar topic discussions than one that mentions the 
topic under consideration but does not link to 
other similar blogs: Topic Discussion Rank does 
that by taking into account both link structure 
and TDIR explained in previous section. This has 
the advantage of taking into account both factors: 
the content of the blog posts and the link struc-
ture of its neighborhood. 
The following piecewise function shows how 
the metric Topic Discussion Rank is calculated:       
Explanation of notations used:  
b - blog  
o : (o,b) ? outlinks from blog b  
The TDR is same as the TDIR in case of the 
blog having zero outlinks as such a blog exists in 
isolation and does not have a strong participation 
within the social network of the blogosphere. In 
the case of a blog having one or more outlinks to 
other blogs we add its own TDIR to the factor  
.   
Here matching links represent blogs that are 
part of topic cluster for a given topic (i.e. those 
having TDIR greater than 1 as explained in sec-
tion 3.3) and each matching link?s TDIR is 
summed up and multiplied by a factor called 
damp. Note that summation of TDIR is used in 
the first iteration only, in the other iterations it is 
replaced by TDR of the blogs.  
Furthermore it is important to note that the 
process of TDR computation is an iterative one 
similar to PageRank (Page et al, 1998) computa-
tion, however the termination condition is unlike 
PageRank in that PageRank terminates when 
rank values are normalized whereas our approach 
uses the blog depth as a termination condition 
which is an adjustable parameter. Due to the 
changed termination condition the role of spam 
blogs is minimized. 
The damping factor damp is introduced to mi-
nimize biasness as is explained below. Consider 
the two blogs as shown with the link structure 
represented by arrows:      
In this case let?s assume the TDIR of blog A is 
2 and the TDIR of blog B is 1. Using the formu-
lation for TDR without the damping factor we 
would have 2+(1/1x1)=3 for blog A and 
1+(1/1x2)=3 for blog B which is not the true ref-
lection of their topic discussion ranks. However 
when we use the damping factor the resultant 
TDR?s are 2+(1/1x1x0.9)=2.9 for blog A and 
1+(1/1x2x0.9)=2.8 for blog B and this more cor-
rectly represents the topic discussion ranks of 
both the blogs. 
4 Experimental Evaluations 
This section presents details of our experiments 
on real blog data. We use precision and recall to 
measure the effectiveness of our approach of 
cluster-finding. The experimental data is released 
as an XML corpus which can be downloaded 
from: 
http://unhp.com.pk/blogosphereResearch/data.tar
.gz. 
Figure 3: Example for Damping Factor Explanation  
 
59
4.1 Data and Methodology 
The data used in the experiments was gathered 
from 102 blog sites which comprised of 50,471 
blog posts. Currently we have restricted the data 
set to only the blogspot domain (blogger.com 
service by Google).We used four blog sites as 
seeds and from them the link structure of the 
blogs was extracted after which the crawl (Qure-
shi et al, 2010) was performed using the XML 
feeds of the blogs to retrieve all the posts in each 
blog. Each blog had an average of 494 posts. 
The topics for which we perform the experi-
ments of finding TDIR and TDR were taken to 
be ?compute?, ?democracy?, ?secularism?, 
?bioinformatics?, ?haiti? and ?obama.?  
The measures that we use to assess the accura-
cy of our method are precision and recall which 
are widely used statistical classification measures 
for the Information Retrieval domain. The two 
measures are calculated using equations 4.1 and 
4.2:  
Precision =    |Ct nCa|            (4.1) 
  |Ca|   
Recall  =      |Ct nCa|              (4.2)  
|Ct| 
Here Ca represents the topic cluster set found 
using our algorithm i.e. the set of blogs that have 
interest in the queried topic, in other words it is 
the set of the blogs that have TDIR greater than 1. 
Ct represents the true topic cluster set meaning 
the set of those blogs that not just mention the 
topic but are really interested in it. The reason for 
distinguishing between true cluster set Ct and 
algorithmic cluster set Ca is that our method just 
searches for the given keyword i.e. topic in all 
the posts and since natural language is so rich 
that just mentioning the topic does not represent 
the fact that the blog is a part of that topic cluster. 
Hence we use a human annotator/labeler for 
identification of the true cluster set from the set 
of the 102 blogs for each of the 6 topics that we 
used in our experiments.  
4.2 Results 
We plot the precision and recall graphs for the 
topics chosen. Figure 4 shows the graph for pre-
cision: 
       
The average precision was found to be 0.87 
which reflects the accurate relevance of our me-
thod. As can be seen from the graph in figure 4 
the precision falls below the 0.8 mark only for 
the topics compute and secularism ? the reason 
for this is that for these two topics a higher pro-
portion of false positives were discovered. Not 
all the posts having the word ?compute? were 
actually related to computing as found by human 
annotator. Same was the case for the word secu-
larism ? since our method searches for adjective 
secular and adverb secularly in case of secular-
ism not being found hence there were some blogs 
in which secular was used but the blog?s focus 
was not in secularism as an idea. On the other 
hand precision measures for the topics ?democ-
racy?, ?obama?, ?haiti? and ?bioinformatics? 
were quite good because these words are likely 
to be found in the blogs that actually focus on 
them as a topic hence reducing the chances of 
false positives. 
Figure 5 shows the graph for recall:  
    The average recall was found to be 0.971 
which reflects the high coverage of our method. 
As the graph in figure 5 shows the recall value is 
Figure 4: Precision Graph for Chosen Topics  
 
Figure 5: Recall Graph for Chosen Topics  
60
mostly close to 1 for the chosen topics. This high 
coverage is attributed to the part of speech di-
mension as discussed in section 3.3; this tech-
nique rules out the chances of false negatives and 
hence we obtain a high recall for our method. 
4.3 Additional Experiments 
In addition to experiments on the six coarse-
grained topics mentioned above we performed 
some additional experiments on two fine-grained 
topics and also repeated the experiment per-
formed on topic ?Obama? with an additional 
term ?Democrats.? On formulating the cluster 
with these two terms the precision increased 
from 0.907 to 0.95 which clearly shows that in-
corporation of extra linguistic features into the 
TDIR formulation ensures better results. Moreo-
ver the ranks of some blogs were found to be 
higher than the ranks obtained previously and 
this increase in rank was due to the fact that 
many posts had subject theme ?Obama? but they 
used the term ?Democrats? ? when we used this 
alternate term the ranks i.e. TDR more correctly 
represented the role of the blogs in the cluster. 
The two fine grained topics for which we re-
peated our experiments were: healthcare bill and 
avatar. Additional terms were also included in 
the TDIR and TDR computation process which 
were as follows:  
healthcare bill ? obamacare 
avatar- sky people,  jake sully  
These alternate terms were chosen as these are 
the commonly associated terms when these top-
ics are discussed. At this point we provided them 
as query topics but for future work our plan is to 
use a machine learning approach for learning 
these alternate phrases for each topic, and know-
ledge bases such as Wikipedia may also be used 
to gather the alternate terms for different topics. 
The precision for the topic healthcare bill was 
found to be 0.857 which had a negligible effect 
on excluding ?obamacare?; however recall suf-
fered more on exclusion of alternate term ?ob-
amacare? as it fell from 1 to 0.667. Results for 
the topic ?avatar? however were quite different 
with a precision of 0.47 and a recall of 1; this 
was due to the large number of false positives 
that were retrieved for the term avatar and we 
found reason for this to be that our approach does 
not take into consideration case-sensitivity at this 
point hence it failed to distinguish between the 
term ?avatar? and movie ?Avatar?. Also in the 
case of topic ?avatar? the alternate phrases did 
not have any effect and hence there is a need to 
refine the approach for fine-grained topics such 
as this one ? we present future directions for re-
finement of our approach in section 5. 
5 Conclusions and Future Work 
In this paper we proposed the concept of ?topic 
clusters? to solve the blog categorization task for 
the Information Retrieval domain. The proposed 
method offers a new dimension in blog commu-
nity detection and blog ranking by taking into 
account both link structure and contents of blog 
posts. Furthermore the natural language 
processing techniques we use provide a higher 
coverage thereby leading to a high average recall 
value of 0.971 in the experiments we performed. 
At the same time we achieved a good accuracy as 
was reflected by an average precision of 0.87. 
For future work we aim to combine our pro-
posed solution into a framework for auto genera-
tion of useful content on a variety of topics such 
as ?blogopedia?; the content can be obtained au-
tomatically from the blog posts and in this way 
manual effort may be saved. We also plan to re-
fine our approach by taking into account the 
temporal aspects of blog posts such as time in-
terval between blog posts, start post date and 
time, end post data and time into our formulation 
for ?Topic Discussion Isolation Rank? and ?Top-
ic Discussion Rank?. Moreover as future direc-
tions of this work we plan to incorporate a ma-
chine learning framework for the assignment of 
the weights corresponding to each topic and for 
the additional phrases to use for each of the top-
ics that we wish to cluster.         
61
References 
Agarwal, Nitin, Huan Liu, Lei Tang, and Philip S. Yu, 
2008. Identifying the influential bloggers in a 
community. In Proceedings of the international 
Conference on Web Search and Web Data Mining 
(Palo Alto, California, USA, February 11 - 12, 
2008). WSDM '08. ACM. 
Anderson, Paul, 2007. What is Web 2.0? Ideas, tech-
nologies and implications for education. Technical 
report, JISC. 
Chi, Yun, Shenghuo Zhu, Xiaodan Song, Junichi Ta-
temura, and Belle L. Tseng,  2007. Structural and 
temporal analysis of the blogosphere through 
community factorization. In Proceedings of the 
13th ACM SIGKDD international Conference on 
Knowledge Discovery and Data Mining (San Jose, 
California, USA, August 12 - 15, 2007). KDD '07. 
ACM. 
Fujimura,Ko, Takafumi Inoue, and Masayuki Sugiza-
ki, 2005. The EigenRumor Algorithm for Ranking 
Blogs. In Proceedings of the WWW 2005 Work-
shop on the Weblogging Ecosystem: Aggregation, 
Analysis and Dynamics. 
Gruhl, Daniel, R. Guha, David Liben-Nowell, and 
Andrew Tomkins, 2004. Information diffusion 
through blogspace. In Proceedings of the 13th in-
ternational Conference on World Wide Web (New 
York, NY, USA, May 17 - 20, 2004). WWW '04. 
ACM. 
Hassan, Ahmed, Dragomir Radev, Junghoo Cho and 
Amruta Joshi, 2009. Content Based Recommenda-
tion and Summarization in the Blogosphere. Third 
International AAAI Conference on Weblogs and 
Social Media, AAAI Publications. 
Honeycutt, Courtenay, and Susan C. Herring, 2009. 
Beyond microblogging: Conversation and collabo-
ration via Twitter. In Proceedings Hawaii Interna-
tional Conference on System Sciences, IEEE Press 
Kumar, Ravi, Jasmine Novak, Prabhakar Raghavan, 
and Andrew Tomkins, 2003. On the bursty evolu-
tion of blogspace. In Proceedings of the 12th inter-
national Conference on World Wide Web (Budap-
est, Hungary, May 20 - 24, 2003). WWW '03. 
ACM. 
Leskovec, Jure, Andreas Krause, Carlos Guestrin, 
Christos Faloutsos, Jeanne Van-Briesen, and Nata-
lie  Glance, 2007. Costeffective outbreak detection 
in networks. In The 13th International Conference 
on Knowledge Discovery and Data Mining (KDD) 
2007. ACM.   
Li, Hui,  Sourav S. Bhowmick,  and Aixin Sun, 2009. 
Blog cascade affinity: analysis and prediction. In 
Proceeding of the 18th ACM Conference on infor-
mation and Knowledge Management (Hong Kong, 
China, November 02 - 06, 2009). CIKM '09. ACM. 
Mishne, G. and Maarten de Rijke, 2006. A Study of 
Blog Search. In Proceedings of ECIR-2006. LNCS 
vol 3936. Springer. 
Nakajima,Shinsuke, Junichi Tatemura, Yoichiroara 
Hino, Yoshinori Hara and Katsumi Tanaka, 2005. 
Discovering Important Bloggers based on Analyz-
ing Blog Threads. In Proceedings of the 14th inter-
national Conference on World Wide Web (Chiba, 
Japan, May 10 - 14, 2005). WWW '05. ACM. 
O'Reilly, Tim, 2005. What is Web 2.0: Design Pat-
terns and Business Models for the next generation 
of software.  
Page, Larry, Sergey Brin, Rajeev Motwani and Terry 
Winograd, 1999. The PageRank citation ranking: 
Bringing order to the Web, Technical Report, Stan-
ford University. 
Qureshi, M. Atif, Arjumand Younus and Francisco 
Rojas, 2010. Analyzing Web Crawler as Feed For-
ward Engine for Efficient Solution to Search Prob-
lem in the Minimum Amount of Time through a 
Distributed Framework. In Proceedings of 1
st 
In-
ternational Conference on Information Science and 
Applications, IEEE Publications. 
Sifry, David, 2009 Sifry?s Alerts. 
http://www.sifry.com/alerts/
 
Sorapure, Madeleine. 2003. Screening moments, 
scrolling lives: Diary writing on the web. Biogra-
phy: An Interdisciplinary Quarterly, 26(1), 1-23.  
62
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 23?33,
Dublin, Ireland, August 23rd 2014.
Towards Model Driven Architectures for Human Language Technologies
Alessandro Di Bari
IBM Center for
Advanced Studies of Trento
Povo di Trento
Piazza Manci 12
alessandro dibari@it.ibm.com
Kateryna Tymoshenko
Trento RISE
Povo di Trento
Via Sommarive 18
k.tymoshenko@trentorise.eu
Guido Vetere
IBM Center for
Advanced Studies of Trento
Povo di Trento
Piazza Manci 12
guido vetere@it.ibm.com
Abstract
Developing multi-purpose Human Language Technologies (HLT) pipelines and integrating them
into the large scale software environments is a complex software engineering task. One needs
to orchestrate a variety of new and legacy Natural Language Processing components, language
models, linguistic and encyclopedic knowledge resources. This requires working with a variety
of different APIs, data formats and knowledge models. In this paper, we propose to employ
the Model Driven Development (MDD) approach to software engineering, which provides rich
structural and behavioral modeling capabilities and solid software support for model transforma-
tion and code generation. These benefits help to increase development productivity and quality
of HLT assets. We show how MDD techniques and tools facilitate working with different data
formats, adapting to new languages and domains, managing UIMA type systems, and accessing
the external knowledge bases.
1 Introduction
Modern architectures of knowledge-based computing (cognitive computing) require HLT components
to interact with increasingly many sources and services, such as Open Data and APIs, which may not
be known before the system is designed. IBM?s Watson
1
, for instance, works on textual documents to
provide question answering and other knowledge-based services by integrating lexical resources, ontolo-
gies, encyclopaedic data, and potentially any available information source. Also, they combine a variety
of analytical procedures, which may use search, reasoning services, database queries, to provide answers
based on many kinds of evidence (IJRD, 2012). Development platforms such as UIMA
2
or GATE
3
facil-
itate the development of HLT components to a great extent, by providing tools for annotating texts, based
on vocabularies and ontologies, training and evaluating pipeline components, etc. However, in general,
they focus on working with specific analytical structures (annotations), rather than integrating distributed
services and heterogeneous resources. Such integration requires great flexibility in the way linguistic and
conceptual data are encoded and exchanged, since each independent service or resource may adopt dif-
ferent representations for notions whose standardization is still in progress. Therefore, developing HLT
systems and working with them in such environments requires modeling, representing, mapping, manip-
ulating, and exchanging linguistic and conceptual data in a robust and flexible way. Supporting these
tasks with mature methodologies, representational languages, and development environments appears to
be of paramount importance.
Since the early 90s, Computer Sciences have envisioned methodologies, languages, practices, and
tools for driving the development of software architectures by models (Model Driven Architecture,
MDA)
4
. The MDA approach starts from providing formal descriptions (models) of requirements, inter-
actions, data structures, protocols and many other aspects of the desired system. Then, models are turned
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://www.ibm.com/innovation/us/watson/
2
http://uima.apache.org/
3
https://gate.ac.uk/
4
http://www.omg.org/mda/
23
into technical resources (e.g. schemes and software modules) by means of programmable transforma-
tion procedures. Model-to-model transformations, both at schema and instance level, are also supported,
based on model correspondence rules (mappings) that can be programmed in a declarative way.
As part of the development of UIMA-based NLP components, easily pluggable in services ecosystems,
we are working on an open, flexible and interoperable pipeline, based on MDA. We want our platform
to be language agnostic and domain independent, to facilitate its use across projects and geographies.
To achieve this, we adopted the Eclipse Modeling Framework
5
as modeling and developing platform,
and we generate UIMA resources (Type System) as a Platform Specific Model, by means of a specific
transformation. We started by designing models of all the required components, and analyzed the way to
improve usability and facilitate interoperability by providing appropriate abstraction and layering.
In the present paper we provide the motivation of our architectural choices, we illustrate the basic
features, and discuss relevant issues. Also, we provide some example of how MDA facilitates the design
and the implementation of open and easily adaptable HLT functionalities.
2 Model-driven Development and NLP
2.1 Model-driven Development
Generally speaking, we talk about a ?modeling? language when it is possible to visually represent (or
model) objects under construction (such as services and objects) from both a structural and behav-
ioral point of view. The most popular (software) modeling language is the Unified Modeling Language
(UML)(Rumbaugh et al., 2005). One of the strengths of such language is that it allows clear and trans-
parent communication among different stakeholders and roles such as developers, architects, analyst,
project managers and testers.
Model Driven Development (MDD) is a software development approach aimed at improving quality
and productivity by raising the level of abstraction of services and related objects under development.
Given an application domain, we usually have a ?business? model that allows for fast and highly expres-
sive representation of specific domain objects. Based on business models, the MDD tooling provides
facilities to generate a variety of platform specific ?executable models?
6
2.2 Model-driven Architecture
Model Driven Architecture (MDA)(Miller and Mukerji, 2003) is a development approach, strictly based
on formal specifications of information structures and behaviors, and their semantics. MDA is promoted
by the Object Management Group (OMG)
7
based on several modeling standards such as: Unified Mod-
eling Language (UML)
8
, Meta-Object Facility (MOF
9
), XML Metadata Interchange (XMI) and others.
The aim is to provide complex software environments with a higher level of abstraction, so that the
application (or service) can be completely designed independently from the underlying technological
platform. To this end, MDA defines three macro modeling layers:
? Computation Independent Model (CIM) abstract from any possible (software) automation; usu-
ally business processes are represented at this layer
? Platform Independent Model (PIM) represents any software automation (that supports the CIM)
but this representation is really independent from any technological constraint such as the running
platform or related middleware, or any third party software. Usually, the skeleton or structure of
such a model is automatically generated from the CIM but it is expected to be further expanded for
5
http://www.eclipse.org/modeling/emf/
6
For a typical MDD approach, there are two different possible implementation strategies: the first one is to customize
generic modeling languages (such as UML) by providing specific profiles (representing the business domain) for the language
(UML is very generic and it offers several extensibility mechanisms such as profiles); the second one is a stronger approach
that leads to what we call a DSL (Domain Specific Language), a fully specified, vertical language for a particular domain. Such
a language is usually built in order to allow business experts to directly work on it, without the need of technological skills.
7
http://omg.org/
8
http://www.uml.org/
9
http://www.omg.org/mof/
24
a fully specification. PIM can be expressed in UML (Rumbaugh et al., 2005) or EMF
10
but also in
any peculiar DSL (domain specific language).
? Platform Specific Model (PSM) can be completely generated from the PIM. Among other things,
this requires PIM to be able to represent every aspect of the solution under development: both
structural and behavioral parts have to be fully specified at this level.
2.3 Eclipse Modeling Framework (EMF)
We chose to adopt EMF as the underlying modeling framework and tooling for our model-driven ap-
proach. EMF is an Eclipse
11
-based modeling framework and code generation facility. Ecore is the core
meta-model for EMF. Ecore represents the reference implementation of OMG?s EMOF (Essential Meta-
Object Facility). EMF is at the heart of several important tools and applications and it is often used
to represent meta-models and their instances (models) management. Just as an example, UML open
source implementation defines its meta-model in terms of EMF-Ecore. Applications or tools that use
Ecore to represent their meta model can leverage the EMF tooling to accomplish several goals such as
the code generation for model management, diagramming and editing of models, transformations visual
development and so on.
2.4 Model-driven NLP
We considered the main features of the Model Driven approach as a powerful way to handle the com-
plexity of modern HLT use cases (Di Bari et al., 2013). In adopting this approach, we chose UIMA
12
as
a standardized and well supported tool to deploy our Platform Specific models.
With respect to the basic features of the UIMA platform, we wanted to enhance:
? The representation, visualization and management of complex linguistic and conceptual models
? The use of many kinds of data specifications
? The interaction with many kinds of platforms and services
Therefore, we decided to design a set of models in EMF, considering this as our PIM layer. From these
models, we can generate PSM components to support a variety of tasks, including:
? A UIMA Type System for implementing NLP pipelines
? A set of data format transformations for working with linguistic corpora
? A set of basic interfaces to access linguistic and knowledge services
? A Business Object Model (BOM) to work with rule engines (business rules management systems)
3 Modeling NLP with EMF and UIMA
3.1 Working with data formats
In order to train our statistical parser based on OpenNLP
13
and UIMA, we had to adapt different corpora
formats, (such as PENN
14
and CONLL
15
), because OpenNLP requests them for different analysis classes
(such as named entity, part-of-speech, chunking, etc.). In fact, we had to transform available corpora
from standard formats to OpenNLP specific ones. We represented standard formats with EMF (an Ecore
model for each one) and we created specific transformations using Java Emitter Templates (JET)
16
, a
10
http://www.eclipse.org/modeling/emf/
11
http://eclipse.org/
12
http://uima.apache.org/
13
http://opennlp.apache.org/
14
http://www.cis.upenn.edu/
?
treebank/
15
http://ilk.uvt.nl/conll/#dataformat
16
http://www.eclipse.org/modeling/m2t/?project=jet
25
framework for fast code generation based on EMF. This solution gives us a lot of flexibility: if the parser
or corpora formats change, we just have to adapt EMF models and/or JET templates consistently. For a
better understanding, we show here an example of the JET template used for transforming from CONLL
to OpenNLP POSTag format:
<c:setVariable select="/contents" var="root"/>
<c:iterate select="$root/sentence" var="sentence">
<c:iterate select="$sentence/token" var="token">
<c:get select="$token/@FORM"/>_<c:get select="$token/@CPOSTAG"/>
</c:iterate>
</c:iterate>
Having the simple CONLL Ecore model (in Figure 1) available, this template is the only artifact realizing
the needed transformation. This template simply iterates over all sentences of a document (root),
then over all tokens of a sentence and print out the form and its postag in the requested format.
Other format conversions are done with the same technique. Notice that this tool allows to write (even
complex) transformation without requiring programming skills, thus ensuring a greater maintainability
and lowering the overall cost of the project.
Figure 1: A simple EMF model representing CONLL format
Figure 2: A sentence represented in PENN format (EMF editor on the left)
Further, as we can notice from the Figure 2, EMF provides very powerful MDD capabilities also from
the modeling instantiation and editing point of view. Notice that these are two view of the very same
resource (a PENN file). This is done by ?teaching? the EMF framework how to parse the underlying per-
sisted data, by providing an implementation (that can be done manually or using some parser generation
tool in its turn) through a specific EMF extension point. From that point on, the framework will always
26
be able to open and manage PENN resources as instances (with all semantic constraints and validation
available) of the PENN Ecore model. Furthermore, the editor and all the specific model management
tooling are automatically generated by the EMF platform. This way, all instance management (and edit-
ing) tasks are transferred to the EMF framework, which reduces costs and helps developers focusing on
NLP tasks.
3.2 Managing the UIMA Type System
In order to manage a (complex enough) UIMA Type System, we fully leveraged the EMF/UIMA in-
teroperability provided by the UIMA framework. UIMA already provides simple transformations from
EMF to a UIMA type system and viceversa; furthermore, UIMA provides the XMI serialization so that
an instance of a type system can be read as an instance of the corresponding EMF model. However,
we had to modify the transformation from EMF to UIMA (type system) in order to reach our ?model
driven? goals and also to handle several dependent, but separated EMF modules. Specifically, our model
is organized around the following modules (packages):
? Text. A model of linguistic occurrences in their context, which are given to the parser (token,
sentences, named entities, parse) and get annotated by the underlying document analysis framework
(e.g. UIMA).
? Linguistics A model that abstracts the linguistic apparatus, including categories, morphological and
syntactic features, and relationships. This is the key for the multilinguality we walk through in the
next section. We can see a fragment of this model in Figure 3
? Ontology An abstract representation of any entity and concept, along with a uniform interface to a
variety of existing knowledge bases
? Mapping A model that allows bridging any tag-set, also explained in next section.
Figure 3: A fragment of the Linguistic model
3.3 Adapting to new languages and domains
Given this basis, we can now illustrate how we are able to incorporate new linguistic resources (such as
vocabularies, ontologies, training corpora with different formats, tag-sets, etc) obtaining UIMA-based
pipeline components. Given an (untrained) statistical library for parsing (such as OpenNLP or others)
and a new natural language to represent, we do the following:
1. Analyze the format requested for the training corpora by the NLP parsing library versus the available
training data for the new language. Most likely the training data will be in some standard format
(e.g. CONLL, PENN, IOB) that is already represented by a suitable Ecore (EMF) model. If not yet
present, we have to write a transformation (usually a simple JET template) from the standard to the
specific one requested by the parser.
27
2. Train the parser for the specific language and tag-set.
3. Given the linguistic knowledge for the new language, and the task at hand, we adopt/modify/extend
a suitable Linguistics model. If needed, we generate the UIMA Type System portion corre-
sponding to the requested features.
17
4. In case of a new business domain also, we adopt/modify/extend the ontology. If needed, we wrap
business knowledge bases where the ontology is instantiated.
18
5. Given the tag-set in use, we represent the mapping with the linguistic model, by instantiating a
suitable Mapping model. At run-time, the (UIMA-based) parser asks (the mapping manager) to
get a Linguistic Category (see Figure 4) of a given tag (the key feature in the Mapping model).
Given the workflow above, we can figure out how the Linguistics and Mapping model are play-
ing a key role for achieving a multi-language solution. Just as an example, for the Italian language, the
Linguistics model defines 55 Word Classes (part of speech), 6 morpho-syntactic attributes com-
posed by 22 morpho-syntactic features and 27 syntactic dependencies. The mapping model for the
EVALITA
19
tagset, contains 115 mapping elements.
Figure 4: A fragment of the mapping between tag-set and the Linguistic model
We can now prove how the specific language knowledge has been encapsulated in the suitable model so
that for example, the code that manages the parser results and creates UIMA annotations does not change
for a new language. Correspondingly, no programming skills are needed to represent this linguistic
knowledge.
3.4 Benefits for NLP development
To show the benefits of our approach, we summarize here the following remarks:
? Whereas we consider UIMA the reference framework for document management and annotation,
we also leveraged the features of a mature and stable modeling framework such as EMF. For in-
stance, we could exploit diagramming, model and instance management and code generation. In
sum, we saved significant development time by means of higher level of abstraction that allowed us
to easily create transformations, create mapping models, smoothly use different format for the same
data and so on.
? Having an additional level of abstraction can lead in thinking there is an additional cost; however, as
stated above, this is not the case for transforming from EMF to UIMA. The only, real additional cost
is represented by the effort of developing transformations. Nevertheless, this is quickly absorbed as
soon as the transformation is re-used. In our case, just for data conversions, we re-used the same
17
Steps 2, 3, 4 are done through our Eclipse-based tooling.
18
The benefit of abstracting semantic information from the Type System has been illustrated in (Verspoor et al., 2009)
19
http://www.evalita.it/
28
EMF models (and their underlying data parsing capabilities we implemented) several times. Each
time we wanted to change a parser library, or new corpora were available, we reused the same EMF
models and techniques.
? The abstraction introduced by the Linguistic model, allowed us to create a language-
independent parsing software layer. Furthermore, the same model was leveraged in order to allow
interoperability between different parsers that were using different tagsets.
4 Using an External Knowledge Base
State-of-the-art NLP algorithms frequently employ semantic knowledge about entities or concepts, men-
tioned in the text being processed. This kind of knowledge is typically extracted from the external
lexicons and knowledge bases (KB)
20
, e.g. WordNet (Fellbaum, 1998) or Wikipedia
21
. Therefore, in
our NLP stack, we have to model extraction, usage and representation of semantic knowledge from the
external KBs each of which might have different structure and access protocols. Moreover, we may
need to plug new KBs into our software environment, with the least effort. Finally, in complex services
ecosystems, we might also need to use the KB outside of the NLP system, e.g. in a remote reasoning
system, and we should be able to reuse the same model for these purposes.
MDD allows us to define the a single ontology/knowledge base (KB) abstraction, i.e. KB PIM, based
on the high-level requirements of our software system, regardless of the actual implementation details of
the KBs to be used. In contrast to UIMA type systems, which are limited to type hierarchies and type
feature structures with the respective getters and setters, MDD allows to specify also functionalities, i.e.
methods, such as querying and updating the KB. Figure 5 demonstrates a diagram of a KB abstraction that
we employ in the Ontology module. In this abstraction KnowledgeBase is a collection of Individuals
and Concepts, Relationships and Roles, which all are subclasses of the Entity abstraction. The figure
contains also the visualizations of some components from the Text module, which show how we model
annotating text with references to the KB elements. For this purpose we have a special kind of TextUnit
called EntityToken used to encode the fact that a certain span in a text denotes a specific Entity. Note that
this is a high-level abstract schema without any platform-related details. This KB PIM may be invoked
in the various parts of the full system PIM model, not necessarily within the NLP stack. We can use this
schema to generate different PSMs depending on our needs.
One of the core benefits of MDD are the transformations. After we have defined our high-level KB
PIM conceptual model we may define a set of transformations which will convert it to the PSM mod-
els, which contain the implementation details of the conceptual model within a specific platform. For
example, it can be Jena TDB
22
, a framework for storing and managing structured knowledge models, or
a SQL relational database. PIM-to-PSM transformations can be defined programmatically, by means of
special tools such as IBM Rational Software Architect
23
, or by means of a specific modeling language,
e.g. ATL Transformation Language (Jouault et al., 2006). Finally, we can use a number of tools for code
generation from the PSM model, thus facilitating and speeding up the software development process.
Depending on the task, our Ontology PIM may be instantiated both as an UIMA PSM or as a PSM
for another platform. More specifically, we have the following scenarios for the KB usage.
Within NLP stack. This may be required, for example, if some annotators in the NLP UIMA pipeline,
such as a relation extractor or a coreference resolver, require knowledge about types of the individuals
(entities) mentioned in a text. In such case, in order to annotate the text with information about individu-
als and their classes from an external KB we can transform the PIM model to a UIMA Type System (TS).
We define a transformation which converts TextUnit to a subclass of UIMA Annotation
24
, and the Entity
element in Ontology to a direct subclass of UIMA TOP
25
. Therefore, for example, within our model
20
Here we use the term ?knowledge base? to refer to any external resource containing structured semantic knowledge
21
http://en.wikipedia.org/
22
http://jena.apache.org/documentation/tdb/index.html
23
http://www-03.ibm.com/software/products/en/ratisoftarch
24
https://uima.apache.org/d/uimaj-2.6.0/apidocs/org/apache/uima/jcas/tcas/
Annotation.html
25
https://uima.apache.org/d/uimaj-2.6.0/apidocs/org/apache/uima/jcas/cas/TOP.html
29
Figure 5: Fragments of the Ontology and Text models
illustrated in Figure 5, Individual abstraction, which is a subclass of Entity, becomes a subclass of UIMA
TOP. The EntityToken annotations from Text are converted to a subclass of UIMA Annotation and are
used to mark the spans of the Individual?s mentions in a text. In the original PIM model EntityTokens
have property references which is a collection of pointers to the respective entities. In UIMA PSM this
property is converted to a UIMA TS feature.
KnowledgeBase is an abstraction for an external KB resource to be plugged into the UIMA NLP stack.
UIMA contains a mechanism for plugging in external resources
26
such as KBs. Each resource is defined
by its name, location and a name of the class which implements handling this resource. MDA and PIM-
to-PSM transformations can simplify modeling the resource implementations for different platforms,
and EMF tools can further help with automatic code generation. We can define a transformation which
would convert platform independent KnowledgeBase model elements to a platform-specific models, for
instance a class diagram for implementing a KnowledgeBase within Jena TDB platform. Then we can
use code generation tools for further facilitation of software development.
Within a reasoning component of a QA system. We may need to use both the output of the NLP
stack and information stored in a KB within some reasoning platform. For instance, this could be needed
within a Question Answering system which provides an answer to the input question based on both text
evidence coming from the UIMA pipeline, e.g. syntactic parse information, and a KB. In this case, the
PIM representations of linguistic annotations (Text and Linguistics packages) and KB knowledge
(Ontology package) may be instantiated as PSMs relative to the specific reasoning (rule- or statistic-
based) framework. UIMA annotations previously obtained within the UIMA can be converted to the
format required by the reasoning framework by means of model-to-model transformations.
5 Related Work
In the recent years, a number of approaches to modeling annotations and language in NLP software
systems and increasing interoperability of distinct NLP components have been proposed (Hellmann et
al., 2013; Ide and Romary, 2006; McCrae et al., 2011).
The most widely-accepted solutions for assembling NLP pipelines are UIMA and the GATE (Cun-
ningham et al., 2011) frameworks. They both use annotation models based on referential and feature
structures and allow to define custom language models called UIMA type system (TS) or GATE annota-
tion schema in an XML descriptor file. There are ongoing efforts to develop all-purpose UIMA-based
26
http://uima.apache.org/downloads/releaseDocs/2.1.0-incubating/docs/html/
tutorials_and_users_guides/tutorials_and_users_guides.html#ugr.tug.aae.accessing_
external_resource_files
30
NLP toolkits , such as DKPro
27
or ClearTK (Ogren et al., 2009), with TSs describing a variety of lin-
guistic phenomena. UIMA is accompanied with a UIMAfit toolkit (Ogren and Bethard, 2009), which
contains a set of utilities that facilitate construction and testing of UIMA pipelines. The two frameworks
are compatible, and GATE provides means to integrate GATE with UIMA and vice versa by using XML
mapping files to reconcile the annotation schemes and software wrappers to integrate the components
28
.
From the MDA perspective, UIMA and GATE type/annotation systems are platform specific models.
Defining a language model as a platform independent EMF model results in greater expressivity. For
example, in UIMA TS one can model only type class hierarchy and type features, while in EMF model
we can also encode the types behavior, i.e. their methods. Moreover, MDA allows to model the usage of
the annotation produced by an NLP pipeline within a larger system. For instance, this could be a question
answering system which uses both information extracted from text (e.g. question parse) and information
extracted from a knowledge base (e.g. query results) and provides tools to facilitate the code generation.
Certain effort has been made on reconciling the different annotation formats. For example, Ide et al.
(2003) and Ide and Suderman (2007) proposed Linguistic Annotation Framework (LAF), a graph-based
annotation model, and its extension, Graph Annotation Format (GrAF), an XML serialization of LAF, for
representing various corpora annotations. It can be used as a pivot format to run the transformations be-
tween different annotation models that adhere to the abstract data model. The latter contains referential
structures for associating annotations with the original data and uses feature structure graphs to describe
the annotations. Ide and Suderman (2009) show how one may perform GrAF-UIMA-GrAF and GrAF-
GATE-GrAF transformations and provide the corresponding software. However, in GrAF representation
feature values are strings, and additionally, it is not possible to derive information about annotations
types hierarchy from the GrAF file only, therefore, the resulting UIMA TS would be shallow and with all
feature values types being string, unless additional information is provided (Ide and Suderman, 2009).
At the same time, MDD tools like EMF provide a both a modeling language with high expressiveness
and solid support to any transformation. We show how MDD facilitates conversion between different
formats in Section 3.1. Additionaly, MDD tools like EMF have a solid sofware support for complex
model visualization, this helps to facilitate understanding and managing large models.
After the emergence of the Semantic Web, RDF/OWL formalisms have been used to describe language
and annotation models (Hellmann et al., 2013; McCrae et al., 2011; Liu et al., 2012). For instance, NLP
Interchange Format, NIF, (Hellmann et al., 2013) is intended to allow various NLP tools to interact on
web in a decentralized manner. Its annotation model, NIF Core Ontology, encoded in OWL, provides
means to describe strings, documents, spans, and their relations. Choice of a language model depends
on the developers of the NLP tools, however, they are recommended to reuse existing ontologies, e.g.
Ontologies of Linguistic Annotations (OLiA) (Chiarcos, 2012). Another effort, Lemon (McCrae et al.,
2011), is a common RDF meta-model for describing lexicons for ontologies and linking them with
ontologies. Its core element is a lexical entry which has a number of properties, e.g. semantic roles
or morpho-syntactic properties. There has also been research on converting UIMA type systems to the
OWL format (Liu et al., 2012). OWL is highly expressive, and a number of tools exists for reasoning
upon OWL/RDF models or visualizing them, e.g. Prot?eg?e
29
, or for aligning them (Volz et al., 2009).
Expressiveness of UML, which is typically used to encode the PIM models in MDD, is comparable to that
of the ontology description languages (Guizzardi et al., 2004), and it may be reasoned upon (Calvanese et
al., 2005). However, differently from the OWL models, there is a number of software solutions which are
able to generate code on top of UML representations. Therefore, when using MDD we benefit both from
the high expressiveness of the modeling language and the solid software engineering support provided
by the MDD tools.
27
http://www.ukp.tu-darmstadt.de/software/dkpro-core/
28
http://gate.ac.uk/sale/tao/splitch21.html
29
http://protege.stanford.edu/
31
6 Conclusion
Model Driven Development and Architecture is a successful paradigm for tackling the complexity of
modern software infrastructures, well supported by tools and standards. We have discussed how the
basic principles behind MDD/A are relevant for Human Language Technologies, when approaching the
coming era of high-level cognitive functionalities delivered by interconnected software services, and
grounded on open data. Model-to-model transformations, supported by specific tools, offer the possi-
bility to rapidly integrate different platforms, and to work with many kinds of data representations. To
get the best of this approach, it is important to carefully analyze the layering and interconnections of
different models, and to provide them with a suitable design. In our work, we are learning the benefit
of modeling aspects such as morpho-syntax and semantics separately, to foster adaptability across lan-
guages and domains. A complete and principled analysis of such general design is yet to come. Here we
have presented some preliminary result of our experiences, and shared what we achieved so far.
Acknowledgements
This research is partially supported by the EU FP7 / Marie Curie Industry-Academia Partnerships and
Pathways schema / PEOPLE Work Programme (Grant agreement no.: 286348, K-Drive project) and by
the IBM Center for Advanced Studies of Trento, Italy.
References
D. Calvanese, G. De Giacomo, D. Lembo, M. Lenzerini, and R. Rosati. 2005. DL-Lite: Tractable description
logics for ontologies. In AAAI, pages 602?607.
C. Chiarcos. 2012. Ontologies of linguistic annotation: Survey and perspectives. In LREC, pages 303?310.
H. Cunningham, D. Maynard, K. Bontcheva, V. Tablan, N. Aswani, I. Roberts, G. Gorrell, A. Funk, A. Roberts,
D. Damljanovic, T. Heitz, M. A. Greenwood, H. Saggion, J. Petrak, Y. Li, and W. Peters. 2011. Text Processing
with GATE (Version 6). University of Sheffield Department of Computer Science.
A. Di Bari, A. Faraotti, C. Gambardella, and G. Vetere. 2013. A Model-driven approach to NLP programming
with UIMA. In 3rd Workshop on Unstructured Information Management Architecture, pages 2?9.
C. Fellbaum. 1998. WordNet: An electronic lexical database. The MIT press.
G. Guizzardi, G. Wagner, and H. Herre. 2004. On the foundations of uml as an ontology representation language.
In E. Motta, N. Shadbolt, A. Stutt, and N. Gibbins, editors, EKAW, volume 3257 of Lecture Notes in Computer
Science, pages 47?62. Springer.
S. Hellmann, J. Lehmann, S. Auer, and M. Br?ummer. 2013. Integrating NLP using Linked Data. In ISWC.
N. Ide and L. Romary. 2006. Representing linguistic corpora and their annotations. In LREC.
N. Ide and K. Suderman. 2007. GrAF: A graph-based format for linguistic annotations. In Proceedings of the
Linguistic Annotation Workshop, pages 1?8. Association for Computational Linguistics.
N. Ide and K. Suderman. 2009. Bridging the gaps: interoperability for GrAF, GATE, and UIMA. In Third
Linguistic Annotation Workshop, pages 27?34. Association for Computational Linguistics.
N. Ide, L. Romary, and E. de la Clergerie. 2003. International standard for a linguistic annotation framework. In
HLT-NAACL workshop on Software engineering and architecture of language technology systems, pages 25?30.
Association for Computational Linguistics.
IJRD. 2012. This is Watson [Special issue]. IBM Journal of Research and Development, editor Clifford A.
Pickover, 56(3.4).
F. Jouault, F. Allilaire, J. B?ezivin, I. Kurtev, and P. Valduriez. 2006. ATL: a QVT-like transformation language. In
Companion to the 21st ACM SIGPLAN symposium on Object-oriented programming systems, languages, and
applications, pages 719?720. ACM.
32
H. Liu, S. Wu, C. Tao, and C. Chute. 2012. Modeling UIMA type system using web ontology language: towards
interoperability among UIMA-based NLP tools. In 2nd international workshop on Managing interoperability
and compleXity in health systems, pages 31?36. ACM.
J. McCrae, D. Spohr, and P. Cimiano. 2011. Linking lexical resources and ontologies on the semantic web with
lemon. In The Semantic Web: Research and Applications, pages 245?259. Springer.
J. Miller and J. Mukerji. 2003. MDA Guide Version 1.0.1. Technical report, Object Management Group (OMG).
P. Ogren and S. Bethard. 2009. Building test suites for UIMA components. In Workshop on Software Engineering,
Testing, and Quality Assurance for Natural Language Processing, pages 1?4. Association for Computational
Linguistics, June.
P. V. Ogren, P.G. Wetzler, and S. J. Bethard. 2009. ClearTK: a framework for statistical natural language process-
ing. In Unstructured Information Management Architecture Workshop at the Conference of the German Society
for Computational Linguistics and Language Technology.
J. Rumbaugh, I. Jacobson, and G. Booch. 2005. The Unified Modeling Language Reference Manual. Addison-
Wesley, Boston, MA, 2 edition.
K. Verspoor, W. Baumgartner Jr, C. Roeder, and L. Hunter. 2009. Abstracting the types away from a UIMA type
system. From Form to Meaning: Processing Texts Automatically. T?ubingen:Narr, pages 249?256.
J. Volz, C. Bizer, M. Gaedke, and G. Kobilarov. 2009. Silk - A Link Discovery Framework for the Web of Data.
In LDOW.
33
