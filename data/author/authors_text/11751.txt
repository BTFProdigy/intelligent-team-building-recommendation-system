Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 173?181,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Improving Arabic-Chinese Statistical Machine Translation  
using English as Pivot Language 
 
 
Nizar Habash Jun Hu 
Center for Computational Learning Systems Computer Science Department 
Columbia University Columbia University 
New York, NY 10115, USA New York, NY 10115, USA 
habash@ccls.columbia.edu jh2740@columbia.edu 
 
  
Abstract 
We present a comparison of two approaches for 
Arabic-Chinese machine translation using Eng-
lish as a pivot language: sentence pivoting and 
phrase-table pivoting. Our results show that 
using English as a pivot in either approach out-
performs direct translation from Arabic to Chi-
nese.  Our best result is the phrase-pivot system 
which scores higher than direct translation by 
1.1 BLEU points. An error analysis of our best 
system shows that we successfully handle many 
complex Arabic-Chinese syntactic variations. 
1 Introduction 
Arabic and Chinese are two languages with a 
very large global presence; however, there has 
not been, to our knowledge, any work on MT 
for this pair.  Given the cost involved in creat-
ing parallel corpora for Arabic and Chinese and 
given that there are lots of available resources 
(in particular parallel corpora) for Arabic and 
English and for Chinese and English, we are 
interested in exploring the role English might 
serve as a pivot (or bridge) language.  In this 
paper we explore different ways of pivoting 
through English to translate Arabic to Chinese. 
Our work is similar to previous research on 
pivot languages except in that our three lan-
guages (source, pivot and target) are very dif-
ferent and from completely unrelated families.  
We focus our experiments on a trilingual paral-
lel corpus to keep all conditions experimentally 
clean. Our results show that using English as a 
pivot language for translating Arabic to Chinese 
actually outperforms direct translation. We be-
lieve this may be a result of English being a sort 
of middle ground between Arabic and Chinese 
in terms of different linguistic features and, in 
particular, word order. 
 
Section  2 describes previous work. Section 3 
discusses relevant linguistic issues of Arabic, 
Chinese and English. Section  4 describes our 
system and different pivoting techniques. And 
Section  5 presents our experimental results. 
2 Previous Work 
There has been a lot of work on translation 
from Chinese to English (Wang et al, 2007; 
Crego and Mari?o, 2007; Carpuat and Wu, 
2007; among others) and from Arabic to Eng-
lish (Sadat and Habash, 2006, Al-Onaizan and 
Papineni, 2006; among others).  There is also a 
fair amount of work on translation into Chinese 
from Japanese, Korean and English (Isahara et 
al., 2007; Kim et al, 2002; Ye et al, 2007; 
among others).  In 2008, the National Institute 
of Standards and Technology (NIST) MT 
Evaluation competition introduced English-
Chinese as a new evaluation track.1 
 
Much work has been done on exploiting multi-
lingual corpora for MT or related tasks such as 
lexical induction or word alignment.  Schafer 
and Yarowsky (2002) induced translation lexi-
cons for languages without common parallel 
corpora using a bridge language that is related 
to the target languages.  Simard (1999) de-
scribed a sentence aligner that makes simulta-
neous decisions in a trilingual parallel text. 
Kumar et al (2007) improved Arabic-English 
MT by using available parallel data in other 
languages. Callison-Burch et al(2006) ex-
ploited the existence of multiple parallel cor-
pora to learn paraphrases for Phrase-based MT. 
Filali and Bilmes (2005) improved word align-
ment by leveraging multilingual parallel trans-
lations. 
 
Most related to our work on pivoting are the 
following: Utiyama and Isahara (2007) studied 
                                                           
1 http://www.nist.gov/speech/tests/mt/2008/doc/ 
173
sentence and phrase pivoting strategies using 
three European languages (Spanish, French and 
German). Their results showed that pivoting 
does not work as well as direct translation.  Wu 
and Wang (2007) focused on phrase pivoting. 
They proposed an interpolated scheme that em-
ploys two phrase tables: one extracted from a 
small amount of direct parallel data; and the 
other extracted from large amounts of indirect 
data with a third pivoting language. They com-
pared results for different European language as 
well as Chinese-Japanese translation using Eng-
lish as a pivoting language. Their results show 
that simple pivoting does not improve over di-
rect MT; however, extending the direct MT sys-
tem with phrases learned through pivoting 
helps. Babych et al (2007) compared two 
methods for translating into English from 
Ukrainian: direct Ukrainian-English MT versus 
translation via a cognate language, Russian. 
Their comparison showed that it is possible to 
achieve better translation quality via pivoting. 
 
In this paper we use a standard phrase-based 
MT approach (Koehn, 2004) that is in the same 
spirit of most statistical MT nowadays.  We be-
lieve that we are the first to explore the Arabic-
Chinese language pair in MT. We differ from 
previous pivoting research in showing that piv-
oting can outperform direct translation even 
when the source, target and pivot languages are 
all linguistically unrelated. 
 
3 Linguistic Issues  
In this section we discuss different linguistic 
phenomena in which Arabic, English and Chi-
nese are divergent. We consider orthography, 
morphology and syntax. We also present a new 
metric for quantifying linguistic differences.  
3.1 Orthography 
Arabic is written from right-to-left using an al-
phabet of 36 letters and eight optional diacriti-
cal marks. Arabic is written in a cursive mostly 
word-internal connected form, but words are 
separated by white spaces.  The absence of Ara-
bic diacritics adds a lot of ambiguity.    
 
Chinese uses a complex orthography that in-
cludes around 10,000 characters in common 
use.  Characters convey semantic rather than 
phonological information. Chinese is written 
from left-to-right or top-down.  Chinese words 
can be made out of one, two or more charac-
ters. However, words are written without sepa-
rating spaces.  Word segmentation is a major 
challenge for processing Chinese (Wu, 1998).   
 
English uses the Roman alphabet and its words 
are written with separating white spaces. Eng-
lish orthography is much closer to Arabic than 
it is to Chinese. 
3.2 Morphology 
Arabic is a morphologically rich language with 
a large set of morphological features such per-
son, number, gender, voice, aspect, mood, case, 
and state. Arabic features are realized using 
both concatenative (affixes and stems) and 
templatic (root and patterns) morphology with 
a variety of morphological, phonological and 
orthographic adjustments. In addition, Arabic 
has a set of very common clitics that are writ-
ten attached to the word, e.g., the conjunction 
?+  w+ ?and?, the preposition ?+  b+2  ?with/in?, 
the definite article ?? +  Al+ ?the? and a range of 
pronominal clitics that can attach to nouns (as 
possessives) or verbs and prepositions (as ob-
jects).   
 
In stark contrast to Arabic, Chinese is an isolat-
ing language with no morphology to talk of. 
However, what Chinese lacks in morphology it 
replaces with a complex system of nominal 
quantifiers and verbal aspects.  For example, in 
Figure 1 (at the end of this paper),  Chinese 
marks the definiteness and humanness of the 
word?? Xue Sheng ?student? using  the two 
characters ?? Zhe Wei  ?this person?, while 
the indefiniteness and book-ness of the word?
Shu ?book? are indicated through the characters
?? Yi Ben ?one book-type?.  
 
English has a simple limited morphology pri-
marily indicating number and tense. English 
stands in the middle between Arabic and Chi-
nese in terms of morphological complexity.  
3.3 Syntax 
Arabic is morpho-syntactically complex with 
many differences from Chinese and English. 
We describe here three prominent syntactic 
issues in which Arabic, Chinese and English 
vary widely: subject-verb order, verb-
                                                           
2  Arabic transliteration is in the Habash-Soudi-Buckwalter 
scheme (Habash et al 2007). 
174
prepositional phrase order and nominal modifi-
cation. 
 
First, Arabic verb subjects may be: (a.) pro-
dropped (verb conjugated), (b.)  pre-verbal, or 
(c.)  post-verbal. The morphology of the Arabic 
verb varies in the three cases. By contrast Eng-
lish and Chinese are both generally subject-verb 
languages. When translating from Arabic, the 
challenge is to determine whether there is an 
explicit subject and, if there is, whether it is pre- 
or post-verbal.  Since Arabic objects also follow 
the verb, a sequence of Verb NounPhrase may 
be a verb subject or a pro-drop-verb object. In 
the example in Figure 1, the subject (student) 
appears after the sentence initial verb in Arabic, 
but at the beginning of the sentence in Chinese 
and English. 
 
Secondly, as for the word order of prepositional 
phrases (PP), Arabic and English are similar in 
that PPs generally appear at the end of the sen-
tence (after all the verbal arguments) and to a 
lesser extent at its beginning.  In Chinese, how-
ever, some PPs (in particular locatives and tem-
porals) must appear between subject and verb.  
Other PPs may appear at end of sentence.  In the 
example in Figure 1, the location of the reading, 
?in the classroom? appears at the end of the 
Arabic and English sentences; however, it is 
between subject and verb in Chinese. 
 
Finally, we distinguish three types of nominal 
modification: adjectival (as in ?red book?),  pos-
sessive (as in ?John?s book?) and relative (as in 
?the book [which] John gave me?).  All of these 
modification types are handled in a similar 
manner in Chinese: using the particle? De to 
connect modifier with modified.  Modifiers al-
ways precede the modified. For example, in 
Figure 1, ?a book about China? appears as ?? 
?? ?? Guan Yu Zhong Guo De Shu ?about 
China De book?.    Similarly, ?the student?s 
book? would be translated as ?? ? ? Xue 
Sheng De Shu ?student DE book?. Like Chinese, 
English adjectival modifiers precede what they 
modify. However, relative modifiers follow.  
Possessive modifiers in English can appear be-
fore or after: ?the student?s book? or ?the book 
of the student?.  Unlike English and Chinese, 
Arabic adjectival modifiers typically follow 
their nouns (with a small exception of some su-
perlative adjectives).  However, similar to Eng-
lish but not Chinese, Arabic relative modifiers 
follow what they modify.  As for possessive 
modifiers, Arabic has a special construction 
called Idafa, in which modifiers immediately 
follow what they modify without connecting 
particles. For example, ?the student?s book? can 
only be translated in Arabic as NOPQO? ?PR? ktAb 
AlTAlb ?book the-student?.3 
 
These different phenomena are summarized in 
Table 1.  It is interesting to point out that Eng-
lish phenomena are a middle ground for Arabic 
and Chinese: in some cases English is closer to 
Arabic and in others to Chinese.  
 
 Arabic English Chinese 
Orthography reduced 
alphabet 
alphabet Characters 
Morphology Rich Poor Very Poor 
Subject-Verb V Subj 
Subj V 
Vsubj 
Subj V Subj ? V 
Verb-PP V?PP V?PP PP V 
V PP 
Adjectival 
Modifier 
N Adj Adj N Adj DE N 
Possessive 
Modifier 
N Poss N of Poss 
Poss ?s N 
Poss DE N 
Relative 
Modifier 
N Rel N Rel Rel DE N 
Table 1: Comparing different linguistic phenomena 
in Arabic, English and Chinese 
3.4 Quantifying Linguistic Differences 
The previous section described specific types 
of linguistic phenomena without distinguishing 
them in terms of frequency or effect distance. 
For example, Arabic nominals (nouns, adjec-
tives and adverbs) are seven times as frequent 
as verbs; and nominal modification phenomena 
are more likely local than long distance com-
pared to verb-subject order.  A proper quantifi-
cation of these different phenomena requires 
trilingual parallel treebanks, which are not 
available. As such, we propose a simple metric 
to quantify linguistic differences by measuring 
the translation complexity of different language 
pairs. The metric is Average Relative Align-
ment Length (ARAL): 
 
ARAL = 1| L |
pa
Salab ?L
? ? pbSb  
 
                                                           
3  Arabic dialects allow an additional construction. We 
focus here on Modern Standard Arabic. 
175
We define L as the set of all alignment links 
linking words in a parallel corpus of languages 
A and B. For each alignment link, lab, linking 
words a and b, we define pa and pb as the posi-
tion of words a and b in their respective sen-
tences. We also define Sa and Sb as the lengths 
of the sentences in which a and b appear, re-
spectively. ARAL is the mean of the absolute 
difference in relative word position (pi/Si) of the 
words of every alignment link. The larger 
ARAL is, the more reordering and inser-
tions/deletions we expect, and the more com-
plexity and difference. ARAL is a harsh metric 
since it ignores syntactic structure facts that ex-
plain how clusters of words move together.   
 
A-C A-E E-C 
0.1679 0.0846 0.1531 
Table 2: Average Relative Alignment Length for  
pairs of Arabic (A), English (E) and Chinese (C) 
 
Table 2 presents the ARAL scores for each lan-
guage pair. These scores are computed over the 
grow-diag-final symmetrized alignment we use 
in our system (Koehn, 2004). ARALAC is the 
highest and ARALAE is the lowest. The average 
length of sentences is generally close among 
these languages (given the segmentation we 
use): Arabic is ~32 words, English is ~31 and 
Chinese is ~29.  Arabic and English are much 
closer to each other than either to Chinese. This 
may be the result of Arabic tokenization and 
Chinese segmentation technologies which have 
been developed for translation into English. We 
address this issue in section  4.1. The ARAL 
scores agree with our assessment that English is 
closer to Arabic and to Chinese than Arabic is 
to Chinese.  As a result, we believe it may serve 
as a good pivot language for translating Arabic 
to Chinese.    
 
4 System Description 
In this section, we describe the different sys-
tems we compare. 
4.1 Data 
Our data collection is the United Nations (UN) 
multilingual corpus, provided by the LDC 4 
(catalog no. LDC2004E12).  The UN corpus has 
in principle parallel sentences for Arabic, Eng-
lish and Chinese. However, the Arabic-English 
                                                           
4 http://www.ldc.upenn.edu 
(A-E) data and Chinese-English (C-E) data sets 
were not in synch. The A-E data set has 3.2M 
lines while the C-E data set has 5.0M lines. We 
used the document ID provided in the data to 
match sentences from A-E against those in C-E 
to generate a three-way parallel corpus with 
2.6M lines.  
 
We tokenized the Arabic data in the Arabic 
Treebank scheme (Sadat and Habash, 2006). 
Chinese was segmented into words using a 
segmenter developed by Howard Johnson for 
the Portage Chinese-English MT system.5 So a 
sentence consists of multiple words with spaces 
between them and each word is comprised of 
one or more characters. English was simply 
processed to split punctuation and ??s?. The 
same preprocessing was used in all systems 
compared. 
 
We are aware of two potentially biased aspects 
of our experimental setting.  First, the Arabic 
and Chinese portions of our data collection, the 
UN corpus, are known to be generated from 
English originals. And secondly, the preproc-
essing techniques we used on Arabic and Chi-
nese were developed for translation from these 
languages into English. These two aspects 
make English potentially more central to our 
experiments than if the data collection and pre-
processing were done on Arabic and Chinese 
independent of English. Of course, it must be 
noted that the data bias is not unique to our 
work but rather a challenge for any bilingual 
corpus, in which translation is done from one 
language to another. Additionally, we can ar-
gue that the English bias in data and preproc-
essing does not only affect the Arabic-English 
and English-Chinese pipelines, but it also 
makes the Arabic and Chinese data potentially 
closer.  Finally, given the expense involved in 
creating direct Arabic-Chinese parallel text and 
given the large amounts of Arabic-English and 
English-Chinese data, we think our results 
(with English bias) are still valid and interest-
ing. That said, we leave the question of Arabic-
Chinese optimization to future work.  
4.2 Direct A-C MT System 
In our baseline direct A-C system, we used the 
Arabic and Chinese portions of our parallel 
corpus to train a direct phrase-based MT sys-
tem. We use GIZA++ (Och and Ney, 2003) for 
                                                           
5 http://iit-iti.nrc-cnrc.gc.ca/projects-projets/portage_e.html 
176
word alignment, and the Pharaoh system suite 
to build the phrase table and decode (Koehn, 
2004). The Chinese language model (LM) used 
200M words from the UN corpus segmented in 
a manner consistent with our training.  The tri-
gram LM was built using the SRILM toolkit 
(Stolcke, 2002).   
4.3 Sentence Pivoting MT System 
The sentence pivoting system (A-s-C) used 
English as an interface between two separate 
pharse-based MT systems: an Arabic-English 
direct system and an English-Chinese direct 
system. When translating Arabic to Chinese, the 
English top-1 output of the Arabic-English sys-
tem was passed as input to the English-Chinese 
system. The English LM used to train the Ara-
bic-English system is built from the counterpart 
of the Chinese data used to build the Chinese 
LM in our parallel corpus. We use 210M Eng-
lish words in total.  
4.4 Phrase Pivoting MT System  
The phrase pivoting system (A-p-C) extracts a 
new Arabic to Chinese phrase table using the 
Arabic-English phrase table and the English-
Chinese phrase table. We consider a Chinese 
phrase a translation of an Arabic phrase only if 
some English phrase can bridge the two. We use 
the following formulae to compute the lexical 
and phrase probabilities in the new phrase table 
in a similar manner to Utiyama and Isahara 
(2007). Here, ?  is the lexical probability and 
wp is the phrase probability.  
  
'( | ) ( | ) ( | )
e
a c a e e c? ? ?=?  
'( | ) ( | ) ( | )
e
c a c e e a? ? ?=?  
'( | ) ( | ) ( | )w w w
e
p a c p a e p e c=?  
'( | ) ( | ) ( | )w w w
e
p c a p c e p e a=?  
 
The left hand side of the formulae represents the 
four required probabilities in a Pharaoh Arabic-
Chinese phrase table. 
 
5 Evaluation 
For each of the direct system, the sentence-
pivoting system and the phrase-pivoting system, 
we conduct four sets of experiments with dif-
ferent data sizes. Table 3 illustrates the training 
data size for each experiment. The training data 
is collected from the beginning of the same 
parallel corpus, so the larger training sets in-
clude the smaller ones. 
 
 Lines Words (Arabic) 
S 32500 1 Million 
M 65000 2 Million 
L 130000 4 Million 
XL 260000 8 Million 
Table 3: Training Data Size 
 
We use two other data sets (1K lines each) for 
tuning and testing. Each sentence in these sets 
has only one reference. Tuning and testing data 
sets are the same across all experiments and 
systems.  In all our experiments, we decode 
using Pharaoh (Koehn, 2004) with a distortion 
limit of 4 and a maximum phrase length of 7. 
Tuning is done for each experimental condition 
using Och?s Minimum Error Training (Och, 
2003). 
 
Note that for each set of experiments with the 
same data size, we draw Chinese, Arabic and 
English from the same chunk of three way par-
allel corpus. For example, in S size experi-
ments, the two phrase tables used to build a 
new table in the phrase-pivoting approach are 
extracted respectively from the A-E and E-C 
systems built in the sentence-pivoting approach 
with size S corpora. 
 
5.1 Direct System Results 
Table 4 shows the results of the direct transla-
tion system A-C. It also includes the result for 
A-E and E-C direct translation. As expected, as 
we double the size of the data, the BLEU score 
(Papineni et al, 2002) increases.  However, the 
rate of increase is not always consistent. In par-
ticular, the M and L conditions vary highly in 
A-E compared to A-C. This is odd especially 
given that we are comparing the same set of 
data from the three parallel corpora.  We specu-
late that this may have to do with an oddity in 
that portion of the data set that may have a dif-
ferent quality than the rest. We see the effect of 
this drop in A-E in the next section.  BLEU is 
measured on English case-insensitively. BLEU 
is measured on Chinese using segmented words 
not characters. 
177
 
 
 A-C A-E E-C 
S 11.17 21.89 19.29 
M 13.43 
(+20.2%) 
23.86 
(+9.0%) 
20.85 
(+8.1%) 
L 14.62 
(+8.9%) 
24.86 
(+4.2%) 
22.42 
(+7.5%) 
XL 16.17 
(+10.6%) 
27.96 
(+12.5%) 
24.11 
(+7.5%) 
Table 4: BLEU-4 scores comparing performance of 
direct translation of Arabic-Chinese (A-C), Arabic-
English (A-E) and English-Chinese (E-C) for four 
training data sizes. The percentage increases are 
against the immediately lower data size. 
5.2 Pivoting System Results 
In Table 5, we present the results of the sen-
tence pivoting system (A-s-C) and the phrase 
pivoting system (A-p-C).  Under all conditions, 
A-s-C and A-p-C outperform A-C. A-p-C gen-
erally outperforms A-s-C except in the M data 
condition. The effect in the S conditions is big-
ger than the XL condition.  In our best result 
(XL), we increase the BLEU score by over 1.12 
points. Furthermore, the relative BLEU score 
increase from the L condition for A-p-C is 
15.5% as opposed to A-C?s 10.6%. The A-s-C 
relative increase from L to XL is 12.8%. This 
suggests that we are making better use of the 
available resources. The differences between A-
s-C and A-C and between A-p-C and A-C are 
statistically significant at the 95% confidence 
level (Zhang et al, 2004).   The differences be-
tween the two pivoting systems are not statisti-
cally significant. Examples from our best 
performing system are shown in Figure 2. 
 
 A-C A-s-C A-p-C 
S 11.17 12.24 9.6% 13.12 17.5% 
M 13.43 14.10 5.0% 13.75 2.4% 
L 14.62 14.96 2.3% 14.97 2.4% 
XL 16.17 16.88 4.4% 17.29 6.9% 
Table 5: Word-based BLEU-4 scores. A-C is direct 
translation. A-s-C is indirect translation through sen-
tence pivoting and A-p-C is indirect translation 
through phrase pivoting. The percentages indicate 
relative improvement over A-C. 
 
Our results are consistent with (Utiyama and 
Isahara, 2007) in that phrase-pivoting generally 
does better than sentence pivoting. However, 
we disagree with them in that, for us, direct 
translation is not the best system to use. We be-
lieve that this effect is caused by the combina-
tion of the very different languages we use. 
English is truly bridging between Arabic and 
Chinese in many linguistic dimensions.  We 
think it?s English?s middle-ground-ness that 
makes these results possible. 
 
 A-C A-s-C A-p-C 
S 53.75 54.38 1.2% 54.64 1.7% 
M 56.65 57.00 0.6% 55.88 -1.4% 
L 58.37 57.69 -1.2% 58.79 0.7% 
BLEU
-1 
XL 59.90 60.34 0.7% 60.28 0.6% 
S 21.32 21.80 2.3% 22.88 7.3% 
M 23.84 24.22 1.6% 23.76 -0.3% 
L 24.98 25.14 0.6% 25.87 3.6% 
BLEU
-4 
XL 25.95 27.11 4.5% 27.70 6.7% 
S 9.82 10.02 2.0% 11.42 16.3% 
M 11.56 11.84 2.4% 11.64 0.7% 
L 12.23 12.52 2.4% 13.09 7.0% 
BLEU
-7 
XL 12.69 13.52 6.5% 14.57 14.8% 
Table 6: Character-based BLEU scores for n-grams 
of maximum size 1, 4, and 7.  The percentages are 
relative to the direct system. 
 
In Table 6, we present additional scores using 
BLEU-1, BLEU-4 and BLEU-7 measured at 
the character level as opposed to the harsher 
measure at word level. Ignoring the odd behav-
ior in M and L conditions, the sentence-pivot 
and phrase-pivot approaches improve over the 
direct translation baseline in terms of fluency 
(BLEU-7) and accuracy (BLEU-1). Under the 
small data condition, the phrase-pivot approach 
increases the BLEU-4 score three times the 
increase of the sentence-pivot approach. That 
ratio reduces to 1.5 times in the XL condition. 
The relative improvements of the pivoting sys-
tems over the direct system are small at BLEU-
1 and much bigger at higher BLEU scores.  
This suggests that differences between the piv-
oting systems and the direct system are not in 
terms of lexical coverage but rather in terms of 
better reordering. 
 
The lengths of the outputs of all the systems 
(direct and pivoting) are larger than the refer-
ence length which means no brevity penalty 
was applied in BLEU calculation. Also, no 
BLEU-gaming was done by OOV deletion: all 
OOV words were left in the output. 
5.3 Error Analysis 
We conducted an error analysis of our best per-
forming system (Phrase Pivot XL) to under-
stand what issues need to be addressed in the 
178
future. We took a sample of 50 sentences re-
stricted in length to be between 15 and 35 Chi-
nese words. A Chinese native speaker compared 
our output to the reference translation and 
judged its quality in terms of two categories: 
syntax and lexical choice.   
 
In terms of syntax, our judge identified all the 
occurrences of (a) subjects and verbs, (b) prepo-
sitional phrases and verbs and (c) modified 
nouns.  Each case was judged as acceptable or 
wrong.  Placing a verb before its subject, a pre-
verbal prepositional phrase after its verb, or a 
modifier after the noun it modifies are all con-
sidered wrong.  We correctly produce subject-
verb order 73% of the time; and we produce 
nominal modification order correctly 64% of 
the time.  Our biggest weakness in terms of syn-
tax is prepositional phrase order.  It is worth 
noting that the two phenomena we do better on 
are addressed in translation from Arabic to Eng-
lish, unlike prepositional phrase order which is 
where Chinese is different from both Arabic and 
English. 
 
In terms of lexical choice our judge considered 
the translation quality of three classes of words: 
Nominals (nouns, pronouns, adjectives and ad-
verbs), Verbs, and other particles (prepositions, 
conjunctions and quantifiers).  An incorrectly 
translated or deleted word is considered wrong.  
We perform on nominals and particles at about 
the same level of 90%. Verbs are our biggest 
challenge with accuracy below 80%.  The ratio 
of deleted words among all wrong words is 
rather high at about 30% (for nominals and for 
verbs). The detailed results of the error analysis 
are shown in Table 7. 
 
Finally, there are 27 instances of Arabic Out-of-
Vocabulary (OOV) words (1.93% of all words) 
that are not handled. Ten (37%) of these are 
proper nouns. The rest belong to mostly nouns 
and adjectives. Orthogonally, 19 (70%) of all 
OOV words belong to the genre of science re-
ports, which is quite different from the data we 
train on. The OOVs include complex terms like 
_`aPb?cde?fg`bO? AlsybrwflwksAsyn ?ciproflox-
acin? and hi??kl ?PnPn? rjAjAt mdAry? ?[chemi-
cal] orbital shakers?. Other less frequent OOV 
cases involve bad tokenization and less com-
mon morphological constructions. 
 
 Total Acceptable Wrong 
Subj-Verb 48 35(73%) 13 (27%) 
Verb-PP 46 17 (37%) 29 (63%) 
Syntax 
Noun-Mod 97 62 (64%) 35 (36%) 
Nominal 408 368 (90%) 40 (10%) 
Verb 124 98 (79%) 26 (21%) 
Lexical 
Choice 
Particle 116 106 (91%) 10 (9%) 
Table 7: Results of human error analysis on a 
sample from the A-p-C system (XL) 
6 Conclusion and Future Work  
We presented a comparison of two approaches 
for Arabic-Chinese MT using English as a piv-
ot language against direct MT. Our results 
show that using English as a pivot in either ap-
proach outperforms direct translation from 
Arabic to Chinese. We believe that this is a 
result of English being a sort of middle ground 
between Arabic and Chinese in terms of differ-
ent linguistic features (in particular word or-
der). Our best result is the phrase-pivot system 
which scores higher than direct translation by 
1.1 BLEU points. An error analysis of our sys-
tem shows that we successfully handle many 
complex Arabic-Chinese syntactic variations 
although there is a large space for improvement 
still.   
 
In the future, we plan on exploring tighter cou-
pling of Arabic and Chinese through compar-
ing different methods of preprocessing Arabic 
for Arabic-Chinese MT, in a similar manner to 
Sadat and Habash (2006).  We also plan to 
study how well these results carry on to differ-
ent corpora (bilingual Arabic-English and Eng-
lish-Chinese) as opposed to the trilingual 
corpus used in this paper.  We also plan to in-
vestigate whether our findings in Arabic-
English-Chinese can be used for other different 
language triples.  
Acknowledgements 
We would like to thank Roland Kuhn, George 
Foster and Howard Johnson of the National 
Research Council Canada for helpful advice 
and discussions and for providing us with the 
Chinese preprocessing tools. 
 
179
 
Figure 1: An example highlighting Arabic-English-Chinese syntactic differences 
 
 
Figure 2: Examples of Arabic-Chinese MT output. English references and English  
glosses for Arabic and Chinese are provided to ease readability.  
 
Arabic k`op kq rO?? ?PuvO?? ?kox? ??PbudO hzfol h{`gO? ?{| ?Pe ? ?O? rd? ?P?p? .  
and-building upon this , therefore  this environment susceptible to-corruption and-lack qualifica-
tion to extent big . 
Eng-Ref Consequently , this environment lends itself to significant degrees of corruption and inefficiency . 
Chn-Ref ??,????????????????? 
Therefore, this kind environment caused have high-degree corruption and efficiency low. 
Chn-Out ??,??????????? ???????? ? 
Therefore , this kind environment inside DE corruption and lack efficiency on big degree top. 
 
Arabic  ?c?? ?e hpcdQ?O? ?Plcdo?O? ?f? ?O 90???? NdQO? ??bi ? ?f?? Plci . 
and-if did-not arrive information requested in period 90 day other , lapse application . 
Eng-Ref If the requested information is not received within a further 90 days ,  the application will lapse. 
Chn-Ref ???? 90?????? ?????,?????? 
If again pass 90 day yet not received requested DE information , then application loose validity. 
Chn-Out ??????? ????? 90??????????  
If not receive requested DE information 90 day within provide more DE request. 
  
Arabic ... h`lcv?O? ???n?? _`p ???c?O? ?e ??P?RO? ??Plcdo?O?? ?Pg? f`b`? . 
? facilitation exchanging the-information and-the-sharing in the-resources between the-agencies 
the-governmental . 
Eng-Ref ? to facilitate the sharing of information and resources between government agencies . 
Chn-Ref ???????????????????? ? 
?for all government agencies among exchanging information and resource offer convenience. 
Chn-Out ???????????????????? 
...purpose in facilitate information exchanging and sharing resource governments among agency . 
  
Arabic  ?PbuO? ??P?Rq? _l rx??? k?O? rO? ?`d?RdO hOPoe? hgaP?l f`p?k? ??k?Ra? ?e f??? ? ??Plcv?dO ??g?i?. 
and-should to-government that look in introducing measures appropriate and-effective to-reduce 
to extent least from possibilities the-corruption 
Eng-Ref Governments should consider introducing appropriate and effective measures to minimize the 
potential for corruption. 
Chn-Ref ????????????????,?? ??? ?????????? ? 
all countries governments should consider adopt appropriate DE effective methods , to-biggest-
extent DE reduce producing corruption DE possibility. 
Chn-Out ? ???????????????,????????????? ? 
all countries governments should consider build appropriate DE effective methods , to-biggest-
extent DE reduce corruption DE possibility. 
 
?f?i1NOPQO2 ?k?R??O3 ?PpPR5 ?_4 ?_`?O6 ??e 7??O8  ?. 
yqr?1 AlTAlb2 Almjthd3 ktAbA4 ?n5 AlSyn6 fy7 AlSf8 . 
read1 the-student2 the-diligent3 a-book4 about5 china6 in7 the-classroom8 . 
  
? 1? 2?? 3              ? 4  ?? 5        ? 6?? 7            ? 8     ? 9  ? 10      ?? 11      ?? 12          ? 13? 14? 
this1 quant2 diligent3 de4  student5     in6    classroom7 read8 one9 quant10 about11     china12           de13    book14 
Zhe1 Wei2 Qin Fen3        De4   Xue Sheng5 Zai6  Jiao Shi7       Du8     Yi9    Ben10      Guan Yu11 Zhong Guo12 De13   Shu14 
  
The diligent student is reading a book about China in the classroom. 
 
180
References  
Yaser Al-Onaizan and Kishore Papineni. 2006 Dis-
tortion models for statistical machine transla-
tion. In Proceedings of Coling-ACL?06. 
Sydney, Australia. 
Bogdan Babych, Anthony Hartley, and Serge 
Sharoff. 2007. Translating from under-
resourced languages: comparing direct transfer 
against pivot translation. In Proceedings of MT 
Summit XI, Copenhagen, Denmark.  
Tim Buckwalter. 2002. Buckwalter Arabic morpho-
logical analyzer version 1.0.  
Chris Callison-Burch, Philipp Koehn, and Miles 
Osborne. 2006. Improved statistical machine 
translation using paraphrases. In Proceedings 
of HLT-NAACL?06. New York, NY, USA. 
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for sta-
tistical machine translation. In Proceedings of 
MT Summit XI, Copenhagen, Denmark.  
Josep M. Crego and Jos? B. Mari?o. 2007. Syntax-
enhanced n-gram-based SMT. In Proceedings 
of MT Summit XI, Copenhagen, Denmark.  
Karim Filali and Jeff Bilmes. Leveraging Multiple 
Languages to Improve Statistical MT Word 
Alignments. In Proceedings of ASRU?05, Can-
cun, Mexico. 
Nizar Habash. 2007. Syntactic preprocessing for 
statistical machine translation. In Proceedings 
of MT Summit XI, Copenhagen, Denmark. 
Nizar Habash, Abdelhadi Soudi, and Tim Buckwal-
ter. 2007. On Arabic Transliteration. In A. van 
den Bosch and A. Soudi, editors, Arabic Com-
putational Morphology: Knowledge-based and 
Empirical Methods. Springer.  
Hitoshi Isahara, Sadao Kurohashi, Jun?ichi Tsujii, 
Kiyotaka Uchimoto, Hiroshi Nakagawa, Hiro-
yuki Kaji, and Shun?ichi Kikuchi. 2007. De-
velopment of a Japanese-Chinese machine 
translation system.  In Proceedings of MT 
Summit XI, Copenhagen, Denmark. 
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-based Statistical Machine 
Translation Models. In Proceedings of 
AMTA?04, Washington, DC, USA. 
Shankar Kumar, Franz Och, and Wolfgang Ma-
cherey. 2007. Improving word alignment with 
bridge languages. In Proceedings of EMNLP-
CoNLL?07, Prague, Czech Republic.   
Young-Suk Lee. 2004. Morphological Analysis for 
Statistical Machine Translation. In Proceedings 
of HLT-NAACL?04, Boston, MA, USA.  
Franz Josef Och and Hermann Ney. 2003. A Sys-
tematic Comparison of Various Statistical 
Alignment Models. Computational Linguistics, 
29 (1):19?52.  
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Pro-
ceedings of ACL?03, Sapporo, Japan. 
Fatiha Sadat and Nizar Habash. 2006. Combination 
of Arabic preprocessing schemes for statistical 
machine translation. In Proceedings of Coling-
ACL?06. Sydney, Australia. 
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for 
Automatic Evaluation of Machine Translation. 
In Proceedings of ACL?02, Philadelphia, PA, 
USA.  
Charles Schafer & David Yarowsky. 2002. Induc-
ing translation lexicons via diverse similarity 
measures and bridge languages. In Proceedings 
of CoNLL?02, Taipei, Taiwan. 
Micheal. Simard. 1999. Text translation alignment: 
Three languages are better than two. In Pro-
ceedings of EMNLP-VLC?99, College Park, 
MD, USA. 
Michel Simard, Nicola Ueffing, Pierre Isabelle, and 
Roland Kuhn. 2007. Rule-based translation 
with statistical phrase-based post-editing. In 
Proceedings of the workshop on Statistical 
Machine Translation, ACL?07, Prague, Czech 
Republic.   
Andreas Stolcke. 2002. SRILM - an Extensible 
Language Modeling Toolkit. In Proceedings of 
ICSLP?02, Denver, CO, USA. 
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based sta-
tistical machine translation. In Proceedings of 
NAACL-HLT?07, Rochester, NY, USA. 
Chao Wang, Michael Collins, and Philipp Koehn. 
2007. Chinese syntactic reordering for statisti-
cal machine translation.  In Proceedings of 
EMNLP-CoNLL?07, Prague, Czech Republic.   
Dekai Wu. 1998. A Position Statement on Chinese 
Segmentation.  Presented at the Chinese Lan-
guage Processing Workshop. http://www. 
cs.ust.hk/~dekai/papers/segmentation.html. 
Hua Wu and Haifeng Wang. 2007. Pivot language 
aproach for phrase-based statistical machine 
translation. In Proceedings of ACL?07, Prague, 
Czech Republic.   
Yang Ye, Karl-Michael Schneider, and Steven 
Abney. 2007. Aspect marker generation in 
English-to-Chinese machine translation. In 
Proceedings of MT Summit XI, Copenhagen, 
Denmark. 
Ying Zhang, Stephan Vogel and Alex Waibel, In-
terpreting Bleu/NIST scores: How much im-
provement do we need to have a better system?, 
In Proceedings of LREC?04, Lisbon, Portugal.  
 
 
181
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 357?366,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Contrasting the Interaction Structure of an Email and a Telephone
Corpus: A Machine Learning Approach to Annotation of Dialogue
Function Units
Jun Hu
Department of Computer Science
Columbia University
New York, NY, USA
jh2740@columbia.edu
Rebecca J. Passonneau
CCLS
Columbia University
New York, NY, USA
becky@cs.columbia.edu
Owen Rambow
CCLS
Columbia University
New York, NY, USA
rambow@ccls.columbia.edu
Abstract
We present a dialogue annotation scheme
for both spoken and written interaction,
and use it in a telephone transaction cor-
pus and an email corpus. We train classi-
fiers, comparing regular SVM and struc-
tured SVM against a heuristic baseline.
We provide a novel application of struc-
tured SVM to predicting relations between
instance pairs.
1 Introduction
We present an annotation scheme for verbal inter-
action which can be applied to corpora that vary
across many dimensions: modality of signal (oral,
textual), medium (e.g., email, voice alone, voice
over electronic channel), register (such as infor-
mal conversation versus formal legal interroga-
tion), number of participants, immediacy (online
versus offline), and so on.1 We test it by anno-
tating transcribed phone conversations and email
threads. We then use three algorithms, two of
which use machine learning (including a novel ap-
proach to using Structured SVM), to predict labels
and links (a generalization of adjacency pairs) on
unseen data. We conclude that we can indeed use
a common annotation scheme, and that the email
modality is easier to tag for dialogue acts, but that
it is harder in email to find the links.
2 Related Work
Annotation for dialogue acts (DAs), inspired by
Searle and Austin?s work on speech acts, arose
largely as a means to understand, evaluate and
1This research was supported in part by the National Sci-
ence Foundation under grants IIS-0745369 and IIS-0713548,
and by the Human Language Technology Center of Excel-
lence. Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of the sponsors.
We would like to thank three anonymous reviewers for their
thoughtful comments.
model human-human and human-machine com-
munication. The need for the enterprise derives
from the fact that the relationship between lexico-
grammatical form (including mood, e.g., interrog-
ative) and communicative actions cannot be enu-
merated; there are complex dependencies on the
linguistic and situational contexts of use. Many
DA schemes exist: they can be hierarchical or
flat (Popescu-Belis, 2008), can comprise a large
(Devillers et al, 2002; Hardy et al, 2003) or small
repertoire (Komatani et al, 2005), or can be ori-
ented towards human-human dialogue (Allen and
Core, 1997; Devillers et al, 2002; Thompson et
al., 1993; Traum and Heeman, 1996; Stolcke et
al., 2000) or multi-party interactions (Galley et al,
2004), or human-computer interaction (Walker
and Passonneau, 2001; Hardy et al, 2003), in-
cluding multimodal ones (Thompson et al, 1993;
Kruijff-Korbayova? et al, 2006).
A major focus of the cited work is on how to
recognize or generate speech acts for interactive
systems, or how to classify speech acts for dis-
tributional analyses. The focus can be on a spe-
cific type of speech act (e.g., grounding and re-
pairs (Traum and Heeman, 1996; Frampton and
Lemon, 2008)), or on more general comparisons,
such as the contrast between human-human and
human-computer dialogues (Doran et al, 2001).
While there is a large degree of overlap across
schemes, the set of DA types will differ due to dif-
ferences in the nature of the communicative goals;
thus information-seeking versus task-oriented di-
alogues differ in the set of speech acts and their
relative frequencies.
Our motivation in providing a new DA annota-
tion scheme is that our focus differs from much
of this prior work. We aim for a relatively ab-
stract annotation scheme in order to make compar-
isons across interactions of widely differing prop-
erties. Our initial focus is less on speech act types
and more on the patterns of local alternation be-
357
tween an initiating speech act and a responding
one?the analog of adjacency pairs (Sacks et al,
1974). The most closely related effort is (Gal-
ley et al, 2004), which aims to automatically
identify adjacency pairs in the ICSI Meeting cor-
pus, a large corpus of 75 meetings, using a small
tagset. Their maximum entropy ranking approach
achieved 90% accuracy on the 4-way classifica-
tion into agreement, disagreement, backchannel
and other. Using the switchboard corpus, (Stolcke
et al, 2000) achieved good dialogue act labeling
accuracy (71% on manual transcriptions) for a set
of 42 dialogue act types, and constructed proba-
bilistic models of dialogue act sequencing in order
to test the hypothesis that dialogue act sequence
information could boost speech recognition per-
formance.
There has been far less work on developing
manual and automatic dialogue act annotation
schemes for email. We summarize some salient
recent work. Carvalho and Cohen (2006) use word
n-grams (with extensive preprocssing) to classify
entire emails into a complex ontology of speech
acts. However, in their experiments, they con-
centrate on detecting only a subset of speech acts,
which is comparable in size to ours. Speech acts
are assigned for entire emails, but several speech
acts can be assigned to one email. Apparently,
they develop separate binary classifiers for each
speech act. Corston-Oliver et al (2004) are in-
terested in identifying tasks in email. They label
each sentence in email with tags from a set which
describes the type of content of the sentence (de-
scribing a task, scheduling a meeting), but are less
interested in the interactive aspect of email com-
munication (creating an obligation to respond).
There has been some work which relates to find-
ing links, but limited to finding question-answer
pairs. Shrestha and McKeown (2004) first de-
tect questions using lexical and part-of-speech fea-
tures, and then find the paragraph that answers the
question. They use features related to the structure
of the email thread, as well as lexical features. As
do we, they find that classifying is easier than link-
ing.
Ding et al (2008) argue that in order to do
well at finding answers to questions, one must
also find the context of the question, since it of-
ten contains the information needed to identify the
answer. They use a corpus of online discussion
forums, and use slip-CRFs and two-dimensional
CRFs, models related to those we use. We will
investigate their proposal to consider the question
context in future work.
While they do not use dialogue act tagging
to compare modalities, as we do, Murray and
Carenini (2008) compare spoken conversation
with email by comparing a common summariza-
tion architecture across both modalities. They get
similar performance, but the features differ.
Table 1: DFU speech act labels
Request-Information (R-I)
Request-Action (R-A)
Inform (Inf)
Commit (Comm)
Conventional (Conv)
Perform (Perf)
Backchannel (Bch) (+/- Grounding)
Other
3 Annotation Scheme
Figure 1: Example DFU illustrating the relation of
extent (segmentation) to speech act type
M1.2 I have completed the invoices for April,
May and June
M1.3 and we owe Pasadena each month for a to-
tal of $3,615,910.62.
M1.4 I am waiting to hear back from Patti on May
and June to make sure they are okay with her.
[Inform(1.2-1.4): status of Pasadena invoicing-
completed & pending approval ? versus amount
due]
Sflink(1.2-1.4)
M2.1 That?s fine.
[Inform(2.1): acknowledgement of status of
Pasadena invoicing]
Blink(1.2-1.4)
The annotation scheme presented here consists
of Dialogue Function Units (DFUs), which are
intended to represent abstract units of interac-
tion. The last two authors developed the annota-
tion on three contrasting corpora: email threads,
telephone conversations, and court transcripts. It
builds on our previous work in intention-based
segmentation (Passonneau and Litman, 1997),
and on mixing a formal schema with natural lan-
guage descriptions (Nenkova et al, 2007). In this
358
paper, we investigate the modalities of telephone
two-person conversation in a library setting, and
multi-party email in a workplace setting. Our ini-
tial focus is on the structure of turn-taking. By
using a relatively abstract annotation scheme, we
can compare and contrast this behavior across dif-
ferent types of interaction.
Our unit of annotation is the DFU. DFUs have
an extent, a dialogue act (DA) label along with
a description, and possibly one or more forward
and/or backward links. We explain each compo-
nent of the annotation in turn. We use the exam-
ple in Figure 1; the example is drawn from actual
messages, but has been modified to yield a more
succinct example.
The extent of a DFU roughly corresponds to that
portion of a turn (conversational turn; email mes-
sage; etc.) that corresponds to a coherent com-
municative intention. Because we do not address
automatic identification of the segmentation into
DFU units in this paper, we do not discuss how
annotators are instructed to identify extent.
As illustrated in Figure 1, the communicative
function of a DFU is captured by a speech act
type, and a natural language description. This is
somewhat analogous to the natural language de-
scriptions associated with Summary Content Units
(SCUs) in pyramid annotation (Nenkova et al,
2007), or with the intention-based segmentation
of (Passonneau and Litman, 1997). The pur-
pose in all cases is to require annotators to artic-
ulate briefly but specifically the unifying intention
(Passonneau and Litman, 1997), semantic content
(Nenkova et al, 2007), or speech act. We use the
eight dialogue act types listed in the upper left of
Table 1. To accommodate discontinuous speech
acts, due to the interruptions that are common to
conversation, each speech act can have an oper-
ator affix such as ?-Continue?. We have previ-
ously shown (Passonneau and Litman, 1997) that
intention-based segmentation can be done reliably
by multiple annotators. For twenty narratives each
segmented by the same seven annotators, using
Cochran?sQ (Cochran, 1950), we found the prob-
abilities associated with the null hypothesis that
the observed distributions could have arisen by
chance to be at or below p=0.1 ?10?6. Partition-
ingQ by number of annotators gave significant re-
sults for all values of A ranging over the number
of annotators apart from A = 2. We would expect
similar patterns of agreement on DFU segmen-
tation, but have not collected segmentation data
from multiple annotators on the two corpora pre-
sented here.
DFU Links, or simply Links, correspond to ad-
jacency pairs, but need not be adjacent. A forward
link (Flink) is the analog of a ?first pair-part? of
an adjacency pair (Sacks et al, 1974), and is sim-
ilarly restricted to specific speech act types. All
Request-Information and Request-Action DFUs
are assigned Flinks. The responses to such re-
quests are assigned a backward link (Blink). In
principle, a response can be any of the speech act
types, thus it can be an answer to a question (In-
form), a rejection of a Request-Action or a com-
mitment to take the requested action (Commit),
a request for clarification (Request-Information),
and so on. In most but not all cases, requests are
responded to, thus most Flinks and Blinks come in
pairs. We refer to Flinks with no matching Blink
as dangling links. If an utterance can be inter-
preted as a response to a preceding DFU, it will
get a Blink even where the preceding DFU has no
Flink. The preceding DFU taken to be the ?first
pair-part? of the Link will be assigned a secondary
forward link (Sflink). All links except dangling
links are annotated with the address of the DFU
from which they originate. Figure 1 illustrates an
email message (M2) containing a single sentence
(?That?s fine?) that is a response to a DFU in a
prior email (M1), where the prior email had no
Flink because it only contains Inform DAs; thus
M1 gets an Sflink.
4 Corpora
The Loqui corpus consists of 82 transcribed dia-
logues from a larger set of 175 dialogues that were
recorded at New York City?s Andrew Heiskell
Braille and Talking Book Library during the sum-
mer of 2005. All of the transcribed dialogues per-
tain to one or more book requests. Forty-eight
dialogues were annotated; the annotators worked
from a combination of the transcription and the au-
dio. Three annotators were trained together, anno-
tated up to a dozen dialogues independently, then
discussed, adjudicated and merged ten of them.
During this phase, the annotation guidelines were
refined and revised. One of the three annotators
subsequently annotated 38 additional dialogues.
We also annotated 122 email threads of the En-
ron email corpus, consisting of email messages
in the inboxes and outboxes of Enron corporation
359
Table 2: Distributional Characteristics of Dialogue
Acts in Enron and Loqui
Loqui Enron
Words 21097 17924
DFUs 3845 1400
Speech Act Labels
Inform 1928 50% 853 61%
Request-Inf. 761 20% 149 11%
Request-Action 39 1% 37 3%
Commit 338 9% 3 0%
Conventional 254 7% 356 25%
Backchannel 507 13% 0 0
Other 18 0% 2 0%
Total 3845 100% 1400 100%
Links
Paired Links 1204 63% 193 28%
Flink/Blink 702 58% 83 43%
Sflink/Blink 502 42% 110 57%
Dangling Links 90 2% 97 7%
Mutliple Blinks 4 0% 4 0%
Links by Speech Act Labels
Inform 1003 83% 142 74%
Request-Inf. 170 14% 44 23%
Request-Action 1 0% 5 3%
Commit 13 1% 2 1%
Conventional 2 0% 0 0
Backchannel 15 1% 0 0
1204 100% 193 100%
employees. Most of the emails are concerned with
exchanging information, scheduling meetings, and
solving problems, but there are also purely social
emails. We used a version of the corpus with some
missing messages restored from other emails in
which they were quoted (Yeh and Harnly, 2006).
The annotator of the majority of the Loqui corpus
also annotated the Enron corpus. She received ad-
ditional training and guidance based on our experi-
ence with a pilot annotator who helped us develop
the initial guidelines.
Table 2 illustrates differences between the two
corpora. The DFUs in the Loqui data are much
shorter, with 5.5 words on average compared with
12.8 words in Enron. The distribution of DFU la-
bels shows a similarly high proportion of Inform
acts, comprising 50% of all Loqui DFUs and 61%
of all Enron DFUs. Otherwise, the distributions
are quite distinct. The Loqui interactions are all
two party telephone dialogues where the callers
(library patrons) tend to have limited goals (re-
questing books). The Enron threads consist of
two or more parties, and exhibit a much broader
range of communicative goals. In the Loqui data,
backchannels are relatively frequent (13%) but do
not occur in the email corpus for obvious reasons.
There are some Commits (9%), typically reflect-
ing cases where the librarian indicates she will
send requested items to the caller by mail, or place
them on reserve. There are no Commits in the
Enron data. Neither corpus has many Request-
Actions; the Loqui corpus has many more requests
for information, which includes requests made by
the librarian, e.g., for the patrons? identifying in-
formation, or by the caller.
The most striking differences between the two
corpora pertain to the distribution of DFU Links.
In Loqui, 63% of the DFUs are the first pair-part
or the second pair-part of a Link compared with
28% in Enron. In Loqui, the majority of Links
are initiated by overt requests (58% of Links are
Flink/Blink pairs), whereas in Enron, the major-
ity of Links involve SFlinks (57%). There are
relatively few dangling Links in either dataset,
with more than three times as many in Enron (7%
versus 2% in Loqui). Most of the DFU types
in the second pair-part of Links are Informs and
Request-Information, with a different proportion
in each dataset. In Loqui, 83% of DFUs that are
second pair-part of a Link are Informs compared
with 74% in Enron; correspondingly, only 14% of
DFUs in Links are Request-Information in Loqui
versus 23% in Enron.
5 Dialogue Act Tagging and Link
Prediction
There are two machine learning tasks in our prob-
lem. The first is Dialogue Act (DA) Tagging, in
which we assign DAs to every Dialogue Func-
tional Unit (DFU). The second is Link predic-
tion, in which we predict if two DFUs form a link
pair. In this paper, we assume that the DFUs are
given. We propose three systems to tackle the
problem. The first system is a non-strawman Base-
line Heuristics system, which uses the structural
characteristics of dialogue. The second is Regu-
lar SVM. The third is Structured SVM. Structured
SVM is a discriminative method that can predict
complex structured output. Recently, discrimi-
native Probabilistic Graphical Models have been
widely applied in structural problems (Getoor and
360
Taskar, 2007) such as link prediction. However,
Structured SVM (Taskar et al, 2003; Tsochan-
taridis et al, 2005) is also a compelling method
which has the potential to handle the interdepen-
dence between labeling and sequencing, due to its
ability to handle dependencies among features and
prediction results within the structure. sequence
labeling (Tsochantaridis et al, 2005). We have
adapted Structured SVM to our problem, provided
a novel method for link prediction, and shown that
it is superior in some aspects to Regular SVM.
5.1 Features
We have two sets of features. DFU features are as-
sociated with a particular DFU, and link features
describe the relationship between two DFUs. DFU
features are used in both tasks. Link features are
only used in link prediction. The feature vector of
a link contains two sets of DFU features and the
link features that are defined over the two DFUs.
Table 3 gives the features we used, which are al-
most identical for both corpora, so we could com-
pare the performance.
Because a lot of Flinks are questions, we
chose some features that are tailored to Question-
Answer detection, such as presence of a question
mark. Dialogue fillers and acceptance words af-
fect the accuracy of Part-Of-Speech tagging. On
the other hand, they are helpful indicators of dis-
fluency or confirmation. So we hand-picked a list
of filler and acceptance words, removed them from
the sentence, and added features counting their oc-
currences.
5.2 Baseline Heuristics
Dialogue Act Tagging We use the most frequent
DA as the heuristic for prediction. In both Enron
and Loqui, this DA is Inform.
Link Prediction In link prediction, the heuris-
tics for Enron and Loqui corpora are different due
to structural differences. In Loqui, whenever we
see a DFU with a Forward Link (DA is Request-
Information or Request-Action), we predict that
the target of the link is the first following DFU that
is available and acceptable. ?Available? means
that the second DFU has not been assigned a Back-
ward Link yet. ?Acceptable? means that the sec-
ond DFU has a DA that is very frequent in a Back-
ward Link and it is of a different speaker to the
first DFU. We enforce similar constraints in Enron
corpus for link prediction, except that the second
Table 3: DFU features (E: Enron, L: Loqui)
Structural for DA prediction
E,L First three POS
E,L Relative Position in the Dialogue
E Existence of Question Mark
E,L Does the first POS start with ?w? or ?v?
E,L Length of the DFU
E Head, body, tail of the Message
E,L Dialogue Act (Only used in link prediction)
Lexical for DA prediction
E,L Bag of Words
E,L Number of Content Words
L Number of Filler Words, as ?uh?, ?hmm?
E,L Number of Acceptance Words, as ?yes?
Structural for Link prediction
E,L The distance between two DFUs
Lexical for Link prediction
E,L Overlapping number of content words
DFU not only has to be from a different author,
but also has to be in a message which is a direct
descendant in the reply chain of the message that
contains the first DFU. The baseline link predic-
tion algorithm uses the DAs as predicted by the
Regular SVM. If we used the baseline DA predic-
tion, the result would be too low to make a valid
comparison against other systems in terms of link
prediction because all DAs would be identical.
5.3 Regular SVM
We have used the Yamcha support vector machine
package (chasen.org/?taku/software/yamcha/).
The advantage of Yamcha is that it extends the
traditional SVM by enabling using dynamically
generated features such as preceding labels.
Dialogue Act Tagging We use the feature vector
of the current DFU as well as the predicted DA of
the preceding DFU as features to predict the DA
of the current DFU.
Link Prediction First, in order to limit search
space, we specify a certain window size to produce
a space S of DFU pairs under consideration. For
a particular DFU, we look at all succeeding DFUs
and check if these two DFUs satisfy the follow-
ing constraint: in Loqui, they must be of different
speakers; in Email, one must be another?s ancestor
and they must be of different authors. We consider
all valid pairs starting from the current DFU until
361
the number of considered valid pairs reaches the
window size. Then we proceed to the next DFU
and collect more DFU pairs into our consideration
space.
Second, we train a link binary classifier with all
DFU pairs in this consideration space along with a
binary classification correct/not correct as training
data. This classifier takes the feature vectors of the
two DFUs as well as the link features such as the
distance between these two DFUs as features.
Third, we apply a greedy algorithm to gener-
ate links in the test data with the binary classifier.
The algorithm firstly uses the classifier to generate
scores for all DFU pairs in the consideration space
of the test data, then it scans the dialogue sequen-
tially, checks all preceding DFUs that are allowed
to link to the current DFU (i.e., the DFU pair is in
the consideration space), and assigns correspond-
ing links to the most likely DFU pair. We impose a
restriction that there can be at most one Flink, one
Sflink and one Blink for any given DFU.
5.4 Structured SVM
A Structured SVM is able to predict com-
plex output instead of simply a binary result
as in a regular SVM. There are several vari-
ants. We have followed the margin-rescaling ap-
proach (Tsochantaridis et al, 2005), and im-
plemented our systems using SVMpython, which
is a python interface to the SVMstruct package
(svmlight.joachims.org/svm struct.html). Gener-
ally, Structured SVM learns a discriminant func-
tion F : X?Y ? R, which estimates a score of
how likely the output y is given the input x. Cru-
cially, y can be a complex structure. Section A in
the appendix; here, we summarize the main intu-
itions.
Dialogue Act Tagging The input x is a sequence
of DFUs, and y is the corresponding sequence of
DAs to predict. Compared to Regular SVM, in-
stead of predicting yt one at a time, Structured
SVM optimizes the sequence as a whole and pre-
dicts all labels simultaneously. Due to the similar-
ity to HMM, the maximization problem is solved
by the Viterbi algorithm (Tsochantaridis et al,
2005).
Link Prediction The input now contains the DFU
sequence, a link consideration space, as well as
a label sequence, which we get from the previ-
ous stage. The output structure chooses among
the possible links in the link consideration space,
such that there is at most one Flink/SFlink or Blink
for any given DFU, and that there are no crossing
links. (Note that all the constraints are only en-
forced in training and prediction; in testing, we
compare results against the complete manual an-
notations which do not follow these constraints.)
Then the maximization problem can be solved by a
straightforward dynamic programming algorithm.
Table 4: Result of DA prediction
Baseline Regular Struct
Loqui 50.14% 68.30% 70.26%
Enron 60.93% 88.34% 88.71%
Note: Structured SVM parameters for Loqui are C =
300, ? = 1; Structured SVM parameters for Enron
are C = 1000, ? = 1.
6 Experiments
We have three hypotheses for our experiments:
Hypothesis 1 Link prediction is harder than Dia-
logue Act prediction.
Hypothesis 2 Enron is harder than Loqui.
Hypothesis 3 Structured SVM is better than Reg-
ular SVM, and Baseline is the worst.
We have applied the algorithm described in Sec-
tion 5 to both the Enron and Loqui corpora. The
data set is annotated with DFUs; we focus on the
DA labels and Links. As discussed before, every
system is a pipeline that would preprocess the data
into separate DFUs, predict the Dialogue Acts,
and then feed the Dialogue Acts into the link pre-
diction algorithm. The size of the data set is shown
in Table 2. We do five-fold cross-validation.
Table 4 shows the accuracy of three systems on
Enron and Loqui. Structured SVM has a clear lead
to Regular SVM in Loqui; but the advantage is less
clear in Enron. Tables 6 and 7 give detailed results
of DA prediction.We do not show DAs that do not
exist in the corpora, or that were not predicted by
the algorithms. Both Regular SVM and Structured
SVM performed consistently for the two corpora.
Table 5 gives Link prediction results. Note that
when we compute the combined result for both
types of links, we are only concerned with the
Link position. The seperate results for Flink/Blink
and Sflink/Blink require us to identify the types of
links first, so here we not only compare the posi-
tion of predicted links against the gold, but also
require predicted DAs to indicate the link type
(e.g., the DA of the first DFU must be Request-
362
Table 5: Link Prediction for Enron and Loqui
Baseline Regular Struct
Enron R P F R P F R P R
Paired Links 16.66% 40% 23.52% 18.75% 55.38% 28.01% 31.25% 39.47% 34.88%
Flink/Blink 32.53% 33.75% 33.13% 26.50% 61.11% 36.97% 34.93% 47.54% 40.27%
Sflink/Blink 0.0% 0.0% 0.0% 11.92% 44.82% 18.83% 22.93% 27.47% 25.00%
Loqui
Paired Links 30% 56.15% 39.11% 43.59% 60.60% 50.71% 44.15% 56.02% 49.38%
Flink/Blink 43.30% 46.47% 44.83% 40.58% 57.73% 47.66% 43.55% 60.04% 50.48%
Sflink/Blink 0.0% 0.0% 0.0% 21.76% 29.36% 25.00% 22.88% 26.24% 24.45%
Note: Structured SVM parameters for Enron are C = 2000, ? = 2., for Loqui C = 1000, ? = 4.
Information or Request-Action to qualify as a
Flink/Blink).
Table 6: Recall/Precision/F-measure of DA pre-
diction for Loqui (in %)
Regular Struct
P R F P R F
R-A 50.0 51.7 50.9 43.3 43.3 43.3
R-I 51.3 61.1 55.8 52.3 71.2 60.3
Inf 73.9 73.0 73.5 76.9 74.1 75.5
Bch 65.3 51.7 57.7 65.1 53.6 58.8
Com 5.6 33.3 9.5 5.6 33.3 9.5
Conv 81.2 84.0 82.6 83.7 83.3 83.5
Table 7: Recall/Precision/F-measure of DA pre-
diction for Enron (in %)
Regular Struct
R P F R P F
R-A 27.8 55.6 37.0 25.0 75.0 37.5
R-I 77.9 82.3 80.0 77.2 83.3 80.1
Inf 92.5 90.6 91.5 92.1 91.2 91.7
Conv 90.5 87.3 88.9 93.4 85.6 89.3
7 Discussion
Hypothesis 1 The result of DA prediction is dras-
tically better than link prediction. There are usu-
ally indicators of DA types such as ?thank you? for
Conventional, so learning algorithms could easily
capture them. But in link prediction, we frequently
need to handle deep semantic inference and some-
times useful information exists in the surrounding
context rather than the DFU itself. Both of these
scenarios imply that in order to predict links or re-
lationships better, we need more sophisticated fea-
tures.
Hypothesis 2 This hypothesis turns out to be half-
correct. The DA prediction accuracy for Enron
is better than that of Loqui. The higher percent-
age of Inform and less diversity of DAs in Enron
(See Appendix for statistics) may be part of the
reason. Another possible explanation is that as a
set of spoken dialogue data, Loqui is inherently
more difficult to process than written form, since
some common tasks such Part-Of-Speech tagging
have lower accuracy for spoken data. On the other
hand, the result of link prediction did confirm our
hypothesis. The first reason is that there are far
fewer links in Enron than in Loqui, so we have less
training data. The tree structure of the reply chain
in the email threads also makes prediction more
difficult. And the link distance is longer, because
in email, people can respond to a very early mes-
sage, while in a phone conversation, people tend
to respond to immediate requests.
Hypothesis 3 Both SVM models perform better
than the baseline. Generally, Structured SVM per-
forms better than Regular SVM, especially in link
prediction for Enron. This confirms the advan-
tage of using Structured SVM for output involv-
ing inter-dependencies. The only exception is the
Sflink prediction in Loqui, which in turn affects
the overall accuracy of link prediction.
References
James Allen and Mark Core. 1997. Damsl:
Dialogue act markup in several layers.
http://www.cs.rochester.edu/research/cisd/resources/damsl.
Vitor Carvalho and William Cohen. 2006. Improving ?email
speech acts? analysis via n-gram selection. In Proceed-
ings of the Analyzing Conversations in Text and Speech.
William G. Cochran. 1950. The comparison of percentages
in matched samples. Biometrika, 37:256?266.
363
Simon Corston-Oliver, Eric Ringger, Michael Gamon, and
Richard Campbell. 2004. Task-focused summarization of
email. In Stan Szpakowicz Marie-Francine Moens, edi-
tor, Text Summarization Branches Out: Proceedings of the
ACL-04 Workshop.
Laurence Devillers, Sophie Rosset, Bonneau-Helene May-
nard, and Lamel Lori. 2002. Annotations for dynamic
diagnosis of the dialog state. In LREC.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract contexts
and answers of questions from online forums. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio.
Christine Doran, John Aberdeen, Laurie Damianos, and
Lynette Hirschman. 2001. Comparing several aspects of
human-computer and human-human dialogues. In Pro-
ceedings of the 2nd SIGDIAL Workshop on Discourse and
Dialogue.
Matthew Frampton and Oliver Lemon. 2008. Using dialogue
acts to learn better repair strategies for spoken dialogue
systems. In ICASSP.
Michel Galley, Kathleen McKeown, Julia Hirschberg, and
Elizabeth Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In Proceedings
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 669?676.
Lise Getoor and Ben Taskar, editors. 2007. Introduction to
Statistical Relational Learning. The MIT Press.
Hilda Hardy, Kirk Baker, Bonneau-Helene Maynard, Lau-
rence Devillers, Sophie Rosset, and Tomek Strza-
lkowski. 2003. Semantic and dialogic annotation
for automated multilingual customer service. In Eu-
rospeech/Interspeech.
Kazunori Komatani, Nayouki Kanda, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2005. Contextual constraints based on
dialogue models in database search task for spoken dia-
logue systems. In Eurospeech.
Ivana Kruijff-Korbayova?, Tilman Becker, Nate Blaylock,
Ciprian Gerstenberger, Michael Kaisser, Peter Poller, Ver-
ena Rieser, and Jan Schehl. 2006. The Sammie corpus of
multimodal dialogues with an mp3 player. In LREC.
Gabriel Murray and Giuseppe Carenini. 2008. Summarizing
spoken and written conversations. In EMNLP.
Ani Nenkova, Rebecca J. Passonneau, and Kathleen McKe-
own. 2007. The pyramid method: incorporating human
content selection variation in summarization evaluation.
ACM Transactions on Speech and Language Processing,
4(2).
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1).
Andrei Popescu-Belis. 2008. Dimensionality of dialogue act
tagsets: An empirical analysis of large corpora. LREC,
42(1).
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systemics for the organization of turn-
taking for conversation. Language, 50(4).
Lokesh Shrestha and Kathleen McKeown. 2004. Detection
of question-answer pairs in email conversations. In COL-
ING.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor,
Rachel Martin, Carol Van Ess-Dykena, and Meteer Marie.
2000. Dialogue act modeling for automatic tagging and
recognition of conversational speech. International Jour-
nal of Computational Linguistics, 26(3).
Ben Taskar, Crlos Guestrin, and Daphne Koller. 2003. Max-
margin markov networks. In NIPS.
Henry S. Thompson, Anne H. Anderson, Ellen Gurman Bard,
Gwyneth Doherty-Sneddon, Alison Newlands, and Cathy
Sotillo. 1993. The HCRC map task corpus: Natural
dialogue for speech recognition. In Proceedings of the
DARPA Human Language Technology Workshop.
David Traum and Peter Heeman. 1996. Utterance units and
grounding in spoken dialogue. In Interspeech/ICSLP.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. JMLR,
6.
Marilyn A. Walker and Rebecca Passonneau. 2001. Date:
A dialogue act tagging scheme for evaluation of spoken
dialogue systems. In HLT.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread re-
assembly using similarity matching. In Conference on
Email and Anti-Spam.
364
A Appendix: Structured SVM
This section provides mathematical background
for Secton 5.4. The hypothesis function is given
by:
f(x,w) = argmaxy?YF (x,y : w)
And in addition, we assume F to be linear to a
joint feature map ?(x,y).
F (x,y : w) = ?w,?(x,y)?
We also define a loss function ?(y,y) which de-
fines the deviation of the predicted output y to the
correct output.
As a result, given a sequence of training
examples,(x1,y1) ? ? ? (xn,yn) ? X ? Y, the
function we need to optimize becomes:
minw,? 12 ?w?
2 + Cn
?n
i=1 ?i
s.t. ?i?y ? Y\y(i) : ?w, ??i(y)? >
?(y(i),y)? ?i where,
?w, ??i(y)? =
?
w,?(x(i),y(i))??(x(i),y)
?
w is optimized towards maximizing the margin
between the true structured output y and any
other suboptimal configurations for all training in-
stances.
A cutting plane optimization algorithm is im-
plemented in SVMstruct. However, for any prob-
lem, we need to implement the feature map
?(x,y), the loss function ?(y,y), and a maxi-
mization problem which enables the cutting plane
optimization, i.e.
y = argmaxy?Y?(y(i),y)? ?w, ??i(y)?
Only certain feature maps that would make
solving this maximization effectively, usually by
dynamic programming, could be handled this way.
For Dialogue Act Tagging, let x =(
x1,x2 . . .xT
)
be the sequence of DFUs,
and y =
(
y1,y2 . . .yT
)
the corresponding se-
quence of dialogue acts. ?(xt) represents the DFU
features and ?(xt) ? RD. yt ? L = {l1, . . . , lK}
where L contains the set of available DAs. The
feature map is (Tsochantaridis et al, 2005):
?(x,y) =
( ?T
t=1 ?(x
t)??(yt)
?(yt?1)??(yt)
)
where ?(yt) = [?(l1,y), . . . , ?(lk,y)] and ? is
an indicator function that returns 1 if two parame-
ters are equal. ?-operator is defined as:
RD ?RK ? RD?K , (a? b)i+(j?1)D ? ai ? bj
In analogy to an HMM, the lower part in
?(x,y) encodes the histogram of adjacent DA
transitions in y ; the upper part encodes the DA
emissions from a specific label to one dimension
in the DFU feature vector. Hence, the total num-
ber of dimensions in ?(x,y) is K2 + DK. As
a result, F (x,y : w) = ?w,?(x,y)? gives a
global score based on all transitions and emissions
in the sequence, which captures the dependecies
among nearby labels and mimics the behaviour of
an HMM. Figure 2 gives an example of how to
compute the feature map.
The loss function is the sum of all zero-one
losses across the sequence, i.e.
?(y,y) = ?
?T
t=1 ?(y
t,yt)
? denotes a cost assigned to every DA loss.
For Link Prediction, the input contains the
DFU sequence x, a link consideration space
s = {(i, j) :,DFU i and j is being considered},
as well as label sequence y which we get from
the previous stage. ?(xi,xj) is the link feature
defined over two DFUs. Let the dimension of
link feature be B. The output structure u ={
u1,u2 . . .uT
}
specifies the link plan. ut denotes
that there is a link from DFU t ? ut to t with the
exception that ut being zero denotes there is no
link pointing to t. The setup of u constraints that
there can be at most one Flink/SFlink or Blink for
any given DFU. In addition u is also subject to the
constraint that all specified links must be in the
link consideration space.
The discriminant function becomes F : X ?
Y?S?U? R. Similar to structured DA predic-
tion, the discriminant function should give a global
evaluation as to how likely is the link plan spec-
ified by U with respect to all the input vectors.
Our solution is to decompose the score, and cor-
respondingly, the feature representation into two
components, link emission and no-link emission;
the details can be found in Figure 3 in the appendix
and an example is in Figure 2.
Similarly, we could define the loss function as
the sum of all zero-one losses across the sequence
, i.e.
?(u,u) = ?
?T
t=1 ?(u
t,ut)
? denotes a cost assigned to every Link loss.
365
Figure 2: A full example of feature map for Structured SVM
x1 = ?are you you sure?
x2 = ?sure?
y1 = ?Req-Info?
y2 = ?Inform?
u1 = 0
u2 = 1
?(x1) = (1, 2, 1)
?(x2) = (0, 0, 1)
?(x1,x2) = (1, 1)
?da =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0
0
0
1
0
0
1
1
2
1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Inform to Inform
Inform to Req-Info
Req-Info to Inform
Req-Info to Inform
Inform with ?are?
Inform with ?you?
Inform with ?sure?
Req-Info with ?are?
Req-Info with ?you?
Req-Info with ?sure?
?link =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1
2
1
0
1
0
0
1
1
0
1
1
1
2
1
0
1
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
1st link pair-part with?are?
1st link pair-part with?you?
1st link pair-part with?sure?
1st link pair-part with Inform
1st link pair-part with Req-Info
2nd link pair-part with?are?
2nd link pair-part with?you?
2nd link pair-part with?sure?
2nd link pair-part with Inform
2nd link pair-part with Req-Info
distance of link
overlap of link
No-Link with?are?
No-Link with?you?
No-Link with?sure?
No-Link with Inform
No-Link with Req-Info
Note: In this example, ?(xt) extracts the bag-of-words features from xt. ?are?,?you?,?sure? are the 1st, 2nd
and 3rd DFU feature respectively. ?(xi,xj) extracts the distance and number of the overlap content, which are
the link features, from the 1st and 2nd pair-part in a DFU link pair. There is a link from DFU 1 to DFU 2 as
specified by j ? uj = i, but there is no link pointing to DFU 1.
Figure 3: The feature map of link prediction for
the structured SVM
?L =?
?
?
?
?
?
?
?
?T?1
i=1
?T
j=i+1 ?(x
i)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(y
i)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(x
j)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(y
j)?(i, j ? uj)
?T?1
i=1
?T
j=i+1 ?(x
i,xj)?(i, j ? uj)
?
?
?
?
?
?
?
?
?NL =
( ?T
i=1 ?(x
i)?(0,ui)
?T
i=1 ?(y
i)?(0,ui)
)
?(x,y, s,u) =
(
?L
?NL
)
Note: ?L and ?NL correspond to the link and no-
link emissions in the feature map ?(x,y, s,u) re-
spectively as shown in the equations. The total di-
mension of the feature map is 3D + 3K +B.
366
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1557?1567,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Bootstrapped Named Entity Recognition for Product Attribute Extraction
Duangmanee (Pew) Putthividhya
eBay Inc.
2065 Hamilton Ave
San Jose, CA 95125
dputthividhya@ebay.com
Junling Hu
eBay Inc.
2065 Hamilton Ave
San Jose, CA 95125
juhu@ebay.com
Abstract
We present a named entity recognition (NER)
system for extracting product attributes and
values from listing titles. Information extrac-
tion from short listing titles present a unique
challenge, with the lack of informative con-
text and grammatical structure. In this work,
we combine supervised NER with bootstrap-
ping to expand the seed list, and output nor-
malized results. Focusing on listings from
eBay?s clothing and shoes categories, our
bootstrapped NER system is able to identify
new brands corresponding to spelling variants
and typographical errors of the known brands,
as well as identifying novel brands. Among
the top 300 new brands predicted, our system
achieves 90.33% precision. To output normal-
ized attribute values, we explore several string
comparison algorithms and found n-gram sub-
string matching to work well in practice.
1 Introduction
Traditional named entity recognition (NER) task has
expanded beyond identifying people, location, and
organization to book titles, email addresses, phone
numbers, and protein names (Nadeau and Sekine
2007). Recently there has been a surge of interest
in extracting product attributes from online data due
to the rapid growth of E-Commerce. Current work
in this domain focuses on mining product reviews
and descriptions from retailer websites. Such text
data tend to be long and generate enough context for
the target task (Brody and Elhadad 2010; Liu et al
2005; Popescu and Etzioni 2005). In this paper, we
focus on mining short product listing titles, which
poses unique challenges.
Short listings are typical in classified ads where
each seller is given limited space (in terms of words)
to describe the product. On eBay, product listing ti-
tles cannot exceed 55 characters in length. Similarly,
on Craigslist and newspaper ads, the length of a list-
ing title is restricted. Extracting product attributes
from such short titles faces the following challenges:
? Loss of grammatical structure in short listings
where many nouns are piled together.
? Typographical errors, abbreviations, and
acronyms that must be normalized to the
standardized values.
? Lack of contextual information to infer product
attribute value.
It can be argued that the use of short listings simpli-
fies the problem of attribute extraction, since short
listings can be easily annotated and one can apply
supervised learning approach to extract product at-
tributes. However, as the size of the data grows, ob-
taining labeled training set on the scale of millions
of listings becomes very expensive. In such a sce-
nario, incorporating unlabeled examples in a semi-
supervised fashion to scale up the solution becomes
a necessity rather than a luxury.
We formulate the product attribute extraction
problem as a named entity recognition (NER) task
and investigate supervised and semi-supervised ap-
proaches to this problem. In addition, we have in-
vestigated attribute discovery, and normalization to
standardized values. We use listings from eBay?s
clothing and shoes categories and develop an at-
tribute extraction system for 4 attribute types. We
have 105, 335 listings from men?s clothing category
and 72, 628 listings from women?s clothing category1557
on eBay, constituting a dataset of 1, 380, 337 word
tokens.
In the first part of this work, we outline a super-
vised learning approach to attribute value extraction
where we train a sequential classifier and evaluate
the extraction performance on a set of hand-labeled
listings. Using maximum entropy and SVM as the
base classifier (for classifying the individual word
tokens), a hidden Markov model (HMM) is trained
on the the probabilistic output of the base classifier,
and a sequential label prediction is obtained using a
Viterbi decoding. We show a performance compar-
ison of supervised HMM, MaxEnt, SVM, and CRF
for this task.
In the second part of our work, to grow our seed
list of attributes, we present a bootstrapped algo-
rithm for attribute value discovery and normaliza-
tion, honing in on one particular attribute (brand).
The goal is given an initial list of unambiguous
brands, we grow the seed dictionary by discover-
ing context patterns that are often associated with
such attribute type. First, we automatically parti-
tion data into a training/test set by labeling word to-
kens in each listing using exact matching to entries
in the dictionary. Brand phrases that can be confused
with other attributes, e.g. the word camel ? both a
brand and a color ? will not be a part of this ini-
tial seed list to create the training set. A classifier
is then trained to learn context patterns surrounding
the known brands from the training set, and is used
to discover new brands from the test set.
Finally, for known attribute values, we normalize
the results to match to words in our dictionary. Nor-
malizing the variants of a known brand to a single
normalized output value is an important aspect of
a successful information extraction system. To this
end, we investigate several string similarity/distance
measures for this task and found that n-gram sub-
string similarity (Kondrak 2005) yields accurate nor-
malization results.
The main contribution of this work is a product
attribute extraction system that addresses the unique
problems of information extraction from short list-
ing titles. We combine supervised NER with boot-
strapping to expand the seed list, and investigate sev-
eral methods to normalize the extracted results. Our
system has been tested on large-scale eBay listing
datasets to demonstrate its effectiveness.
2 Related Work
Recent work on product attribute extraction by
(Brody and Elhadad 2010) applies a Latent Dirich-
let Allocation (LDA) model to identify different as-
pects of products from user reviews. Similar work
is presented in (Liu et al 2005). Topic models such
as LDA groups similar words together by identify-
ing topics (product aspects) from patterns of word-
occurrences. Such grouping can discover new as-
pects of a product such as ?portability? (for net-
book computers), but it may generate aspects that
are vague and not easily interpretable. Indeed, how
to refine discovered aspects and clean up words in
each aspect remains an open question. The LDA
approach also treats documents as bags of words,
where important information in word sequences is
not taken into account in learning the model.
Our work is most closely related to (Ghani 2006),
where a set of product attributes of interests are pre-
defined and a supervised learning method is applied
to extract the correct attribute values for each class.
Starting out from a small set of training examples,
a bootstrapping technique is used to generate more
training data from unlabeled data. The main dif-
ference to our method lies in how bootstrapping is
used. (Ghani 2006) used EM to add more train-
ing data from unlabeled data, while in our approach
bootstrapping is used to expand the seed list. First,
we automatically generate labeled data by matching
seed list to unlabeled data. Then, these auto-labeled
training set is used to train a classifier to identify new
attribute values from a separate set of unlabeled data.
Thirdly, newly discovered product attribute values
are added back to our seed list. Thus our original
classifier for product attribute extraction can be im-
proved through an expanded seed list.
In (Ghani and Jones 2002; Jones 2005), several
bootstrapping methods are compared. These meth-
ods include self-training, co-EM and EM. All of
these approaches are different from ours, as de-
scribed in detail earlier. In (Probst et al 2006),
a Naive Bayes learner is combined with Co-EM to
generate more training data from unlabeled data, and
attribute-value pairs are extracted on adjacent words.
The automatic bootstrapping in this paper was in-
spired by (Pakhomov 2002)?an acronym expansion
algorithm for medical text documents. The underly-
ing assumption is that abbreviated forms and their1558
corresponding expansions occur in similar contexts;
consequently, the surrounding context patterns can
be used in associating the correct expansion to its
acronym.
Our seed list expansion algorithm indeed bears
some similarity to the work of (Nadeau el al 2006)
and (Nadeau 2007). In (Nadeau el al 2006), automo-
bile brands are learned automatically from web page
context. First, a small set of 196 seed brands are ex-
tracted together with their associated web page con-
texts from popular news feed. The web context is
subsequently used to extract additional automobile
brands, which result in a total of 5701 brands. How-
ever, the reported results in (Nadeau el al 2006) have
low precision, in some case less than 50%. Eventu-
ally their approach needs to rely on rule-based ambi-
guity resolver to increase the precision. Our system
does not rely on manually created rules.
A more NLP-oriented approach is proposed in
(Popescu and Etzioni 2005), where noun phrases are
extracted from online user reviews. Their system
tries to identify product features and user opinions
from such noun phrases. A PMI (pointwise mutual
information) score is evaluated between each noun
phrase and discriminators associated with the prod-
uct class. The noun-phrase approach does not work
well in informal texts. In our case, user-generated
short product listings may have many nouns con-
catenated together without forming a phrase or
obeying correct grammatical rules.
Finally, another similar bootstrapping method is
presented in (Mintz et al 2009), where instances
of known entity relations (or seed list in our paper)
are matched to sentences in a set of Wikipedia arti-
cles, and a learning algorithm is trained from the sur-
rounding features of the entities. The trained model
is then applied to a test set of Wikipedia articles,
and has been reported to be able to discover new in-
stances. In our case, we apply our learned model to
a new test set, and discover new brand names from
the listings.
The nature of non-grammatical text we face
makes our work similar to the NER work on infor-
mal texts. (Minkov et al 2005) proposes an NER
system that extracts personal names from emails.
The work in (Gruhl et al2009) identifies song titles
from online forums on popular music, where song
titles can be very ambiguous. By using real-world
constraints such as known song titles, (Gruhl et al
2009) restricts the set of possible entities and are
able to obtain reasonable recognition performance.
3 Corpus
The data used in all analysis in this paper is obtained
from eBay?s clothing and shoes category. Clothing
and shoes have been important revenue-generating
categories on the eBay site, and a successful at-
tribute extraction system will serve as an invaluable
tool for gathering important business and market-
ing intelligence. For these categories, the attributes
that we are interested in are brand (B), garment
type/style (G), size (S), and color (C). We gather
105, 335 listings from men?s clothing category and
72, 628 listings from women?s clothing category,
constituting a dataset of 1, 380, 337 word tokens. On
average, each listing title contains 7.76 words.
A few examples of listings from eBay?s clothing
and shoes categories are shown in Fig 1. When de-
signing an attribute extraction system to distinguish
between the 4 attribute types, we must take into ac-
count the fact that individual words alone ? with-
out considering context ? are ambiguous, as each
word can belong to multiple attribute types. To give
concrete examples, inc is a brand name of women?s
apparel but many sellers use it as an acronym for
inch (brand vs. size). The word blazer can be a
brand entity or it can be a garment type (brand vs.
garment type). In addition, like other real-world
user-generated texts, eBay listings are littered with
site-specific acronyms, e.g. BNWT (brand new with
tag), NIB (new in box), and abbreviations introduced
by individual sellers, e.g. immac (immaculate), trs
(trousers). In designing an information extraction
system for our dataset, we need to account for the
general as well as specific properties of our dataset.
4 Supervised Named Entity Recognition
In the first part of this work, we adopt a supervised
named entity recognition (NER) framework for the
attribute extraction problem from eBay listing titles.
The goal is to correctly extract attribute values cor-
responding to the 4 attribute types, from each list-
ing. One key assumption of the supervised learn-
ing paradigm is the availability of a labeled training
data for training a classifier to distinguish between
different classes. We generate our training data in1559
Figure 1: Example listings and their corresponding labels from the clothing and shoes category.
the following manner. For each listing, we remove
extraneous punctuation symbols (*,(,),!,:,;) and tok-
enize each listing into a sequence of tokens. Given 4
dictionaries of seed values for the 4 attribute types,
we match n-gram tokens to the seed values in the
dictionaries, and create an initial round of labeled
training set, which must then be manually inspected
for correctness. In this work, we tagged and manu-
ally verified 1, 000 listings randomly sampled from
the 105, 335 listings from the men?s clothing cate-
gory, resulting in a total of 7, 921 labeled tokens with
1, 521-word vocabulary. Fig. 1 shows examples of
labeled listings, with tags B corresponding to brand,
C for color, S for size, G for garment type/style, and
NA for none of the above.
4.1 Classifiers
One of the most popular generative model based
classifiers for named entity recognition tasks is Hid-
den Markov Model (HMM), which explicitly cap-
tures temporal statistics in the data by modeling
state (label/tag) transitions over time. Discrimina-
tive classifiers, which directly model the posterior
distribution of class label given features, i.e. SVM
(Isozaki and Kazawa 2002) and Maximum Entropy
model for NER (Chieu and Ng 2003), have been
shown to outperform generative model based clas-
sifiers. More recently, Conditional Random Fields
(CRF) (Feng and McCallum 2004; McCallum 2003)
has been proposed for a sequence labeling problem
and has been established by many as the state-of-
the-art model for supervised named entity recogni-
tion task. In this section, we briefly summarize the
pros and cons of each approach.
4.1.1 Hidden Markov Models
A hidden Markov model (HMM) is a probabilistic
generative model for sequential data. HMM is char-
acterized by 2 sets of model parameters ? emission
probabilities which produce the observation variable
given the hidden state, and the state transition prob-
ability matrix which captures the temporal correla-
tion in the hidden state sequences. Given a set of la-
beled training sequences as shown in Figure 1, one
can train an HMM to model temporal statistics in
the observation sequences. In our task, a sequence
of word tokens from listing titles are our observa-
tions. One simple approach to use HMM is to set a
hidden state to correspond to a tag class. In the train-
ing phase, since all the tags are given, the hidden
states indeed become visible and inference in this
model becomes much more simplified. The multino-
mial parameter for the emission probabilities p(w|s)
can be learned with a closed-form update (maximum
likelihood estimate). During testing, however, an ef-
ficient forward-backward algorithm must be used to
infer the most likely tag sequence that accounts for
the observation.
One main drawback of HMM is the type of fea-
tures that it can handle. Like other probabilistic gen-
erative models, in order to account for rich, over-
lapping feature sets, e.g. text formatting features,
the correlation structures in the overlapping features
must be explicitly modeled. Indeed, in the clas-
sic HMM based NER, the simple feature used is
the word identity itself, which might not be suffi-
ciently discriminative in distinguishing between dif-
ferent classes. In addition, because of data sparsity
(out-of-vocabulary) problem due to the long-tailed
distribution of words in natural language, sophisti-
cated unknown word models are generally needed
for good performance (Klein et al 2003).
4.1.2 Maximum Entropy models
The principle of maximum entropy states that
among all the distributions that satisfy feature con-
straints, we should pick the distribution with the
highest entropy, since it makes the least assumption
about the data and will have better generalization
capability to unseen data. Maximum entropy clas-
sifier, therefore, is the highest entropy conditional
distribution of the class label given features, which
has been shown to conveniently take an exponential
form. Maximum entropy classifier is thus closely
related to logistic regression model.1560
Position Features:
- Position from the beginning of listing
- Position to the end of listing
Orthographic Features:
- Identity of the current word
- Current word contains a digit
- Current word contains only digits
- Current word is capitalized
- Current word begins with a capitalized letter followed by
all non-cap letters.
- Current word is &
- Current word is ?
- N -gram substring features of current word (N = 4, 5, 6)
Context Features:
- Identity of 2 words before the current word
- Identity of 2 words after the current word
- Previous word is from
- Previous word is by
- Previous word is and
- N -gram substring features of neighboring words (N = 4, 5, 6)
Dictionary Features:
- Membership to the 4 dictionaries of attributes
- Exclusive membership to dictionary of brand names
- Exclusive membership to dictionary of garment types
- Exclusive membership to dictionary of sizes
- Exclusive membership to dictionary of colors
Table 1: Feature set used in discriminative classifiers.
MaxEnt classifiers (Ratnaparkhi 1996; Ratna-
parkhi 1998) have been applied to various NLP ap-
plications. The attraction of the framework lies in
the ease with which different information sources
used in the modeling process are combined and the
good results that are reported with the use of these
models. The set of redundant features used for the
MaxEnt classifier is the same as those used for the
SVM classifier, which we outline in the next section.
4.1.3 Support Vector Machines
Support Vector Machine (SVM) is yet another
popular classifier for a supervised NER task. In a
binary classification case, SVM finds parameters of
a linearly separating hyperplane that best separates
data from the 2 classes, in a sense that the margin
of separation is maximized. Since only the samples
closest to the decision boundary (the so-called sup-
port vectors) determine the location of the separating
hyperplane, SVM can be trained on very few train-
ing examples even for data in a high-dimensional
space. For our supervised NER system, we use the
following features, as described in detail in Table 1,
as input to the discriminative classifiers.
The use of char N -gram (N -gram substring) fea-
tures was inspired by the work of (Klein et al 2003),
where the introduction of such features has been
shown to improve the overall F1 score by over 20%.
In (Kanaris et al 2006), char N -gram features con-
sistently outperform word features in learning effec-
tive spam classifiers. Indeed the use of character N -
gram features as an input to the classifier subsumes
the use of prefix, suffix, and the entire word features.
Generally speaking, char N -gram features provide a
more robust representation against misspelling since
string s1 and its spelling variant s2 may share many
char N -gram substrings in common.
POS and punctuation features are not used in our
NER system. This is mainly due to the fact that eBay
listing titles are not complete sentences and the out-
put from running a POS tagger through such data
can indeed be unreliable. For punctuation features,
eBay sellers are known to abuse punctuation marks
excessively to draw attention of the potential buyers
to click on their listings. In addition, we find that
morphological features are less predictive of entity
names in eBay listing titles than they are in formal
documents. To give a concrete example, capitaliza-
tion is a good predictor of entity names in traditional
NER systems, but on the eBay site, many sellers
use all-cap or all-lowercase letters for every word
in their titles, bringing into question the discrimi-
native power of widely used features in traditional
NER systems.
4.1.4 Viterbi Smoothing
The Viterbi algorithm can be used to smooth the
prediction output from SVM or MaxEnt. More
specifically, the Viterbi decoder enforces the tempo-
ral consistency on the individual label prediction as
inferred by the base classifier ? MaxEnt or SVM,
independently based on the feature representation
of each word token. The probabilistic ouput of the
base classifier is the observation or evidence, while
the temporal consistency is encoded in the empirical
state transition probability matrix inferred from the
training data. This scenario is analogous to compar-
ing MAP (maximum a priori) estimate with that of
ML (maximum likelihood) in that the former incor-
porates a prior belief when making a final estimate
of the parameter values (most likely label sequence
predicted by the Viterbi algorithm), while the latter
uses only the observation to infer the most likely pa-
rameter estimate (independently inferred predicted
labels of each word token from the base classifier).1561
We adopt the approach from the work of (Chieu
and Ng 2003), which uses Viterbi to improve the
classification results from MaxEnt classifier for
NER tasks. Instead of computing the transition
probability matrix by recording the frequency of
how many times state i at time T transitions to state
j at time T + 1, we simply record that this state i
to j transition is admissible. This approach, indeed,
divides a set of all label sequences into ones that are
admissible and inadmissible, and assign equal prob-
abilities to all the admissible sequences. Such an ap-
proach therefore eliminates all the inadmissible se-
quences of labels (i.e. prohibit the scenario where
-in sub-tag is followed by -begin sub-tag), while al-
lowing the Viterbi algorithm to give more weight to
the classification outputs from SVM or MaxEnt.
4.1.5 Conditional Random Field (CRF)
Conditional Random Field, since its conception in
the seminal work of (Lafferty et al 2002), is a dis-
criminative classifier for sequential data that com-
bines the best of both worlds. Like SVM and Max-
Ent, CRF is a discriminative classifier that directly
models the conditional distribution of the target vari-
able given the observed variable, i.e. no modeling
resource is wasted in modeling complex correlation
structures in the observation sequences. Like HMM,
CRF makes prediction on the label sequence by in-
corporating the temporal smoothness. Indeed CRF
has been established by many as the state-of-the-
art supervised named entity recognition system for
traditional NER tasks (Feng and McCallum 2004;
McCallum 2003), for NER in biomedical texts (Set-
tles 2004), and in various languages besides English,
such as Bengali (Ekbal et al 2008) and Chinese
(Mao et al2008). Various modifications to CRF
have recently been introduced to take into account
of non-local dependencies (Krishnan and Manning
2006) or broader context beyond training data (Du
et al 2010).
4.2 Experimental Results
In this section, we compare the generative model
based and discriminative model classifiers for super-
vised NER tasks. Given 1, 000 manually tagged list-
ings from the clothing and shoes category in eBay,
we adopt a 90-10 split and use 90% of the data for
training and 10% for testing. Each listing title is to-
kenized into a sequence of word tokens, each manu-
SVM MaxEnt HMM CRF
w/o Viterbi 89.05% 87.64% - -
w/ Viterbi 89.47% 88.13% 83.82 93.35%
Table 2: Classification accuracy (%) on 9-class NER on
men?s clothing dataset, comparing SVM, MaxEnt, super-
vised HMM, and CRF.
ally assigned to one of the 5 tags: brand (B), size
(S), color (C), garment type (G), and none of the
above (NA). In order to more accurately capture the
boundary of multi-token attribute values, we further
sub-divide each tag into 2 classes using -beg and -in
sub-tags. This step increases the number of classes
that our classifier needs to handle from 5 to 9 classes
given as follows: {B-beg, B-in, C-beg, C-in, S-beg,
S-in, G-beg, G-in, and NA}.
Table 2 shows a comparison of classification ac-
curacy from 4 classifiers ? SVM, MaxEnt, HMM,
and CRF. Supervised HMM, with the most simplis-
tic feature, yields the baseline result at 83.82% accu-
racy. All the discriminative classifiers ? CRF, Max-
Ent, and SVM ? outperform the baseline by HMM,
with CRF improving on the baseline performance by
the largest margin, concurring to other reports of its
state-of-the-art results. Indeed, when using exactly
the same set of features as SVM and MaxEnt, the
performance of CRF indeed drops to 89.11%, which
is on par with that of SVM and MaxEnt. However,
when restricting to using dictionary and word iden-
tity features, the performance of CRF improves, in-
dicating the importance of feature selection to such
model. SVM and MaxEnt yield similar performance
with SVM slightly outperforming MaxEnt classifier
by 1.6%. The incorporation of temporal smoothness
constraint enforced by the Viterbi algorithm slightly
improves the label sequence prediction (comparing
row 1 and row 2 in Table 2).
The HMM implementation used in our experi-
ments is the Hunpos tagger in (Halacsy et al 2007),
which captures the state transitional probabilities us-
ing second-order Markov model. For SVM, we use
the popular libSVM package (Chang and Lin 2001)
which produces probabilistic output from fitting a
sigmoid function to the distances between samples
and the separating hyperplane. We use linear kernel
in our experiments, although RBF kernel with grid
search for optimal parameters yield slightly superior1562
performance, with a significantly higher computa-
tional cost. The MaxEnt implementation used in our
experiment is the version available from the NLTK
toolkit, with BFGS optimizer. For CRF, we use the
linear-chain CRF model available from the Mallet
package1.
5 Bootstrapping for Dictionary Expansion
The supervised learning approach assumes the ex-
istence of an annotated set of training data. Often
times, training data must be painstakingly marked
up and collecting large-scale labeled training exam-
ples can be very costly. In recent years, more and
more research effort has been focused on how to
leverage a vast amount of unlabeled data in a semi-
supervised or entirely unsupervised fashion for NER
as well as for other similar NLP tasks, e.g. POS
tagging, sentence boundary detection, and word
sense disambiguation (Riloff 1999; Ghani and Jones
2002; Probst et al 2006; Brody and Elhadad 2010;
Haghighi 2010).
One way to incorporate a vast amount of unla-
beled data is to learn a clustering of words that as-
signs syntactically similar words to the same clus-
ters. Popular clustering algorithms used prevalently
in many NER systems are, for example, the combi-
nation of distributional and morphological similar-
ity work of (Clark 2003) or the classic N -gram lan-
guage model based clustering algorithm of (Brown
et al 1992). In such a system, when training an
NER classifier, we introduce a word cluster id as an
additional feature in the input, with the hope that the
model will pick out clusters that are highly indica-
tive of each class. When encountering words that
are out-of-vocabulary (OOV) in the test set, if those
words are assigned the same cluster membership as
some other words in the training set, the cluster fea-
ture will fire, allowing for correct classification re-
sults to be obtained (Lin and Wu 2009; Faruqui and
Pado 2010).
5.1 Growing Seed Dictionary
In this work, we focus on the problem of how to
grow the seed dictionary and discovering new brand
names from eBay listing data. While the perfor-
mances of supervised NER classifiers as described
in sections 4.1.1-4.1.5 are satisfactory, in practice,
1http://mallet.cs.umass.edu/
however, especially with a small training set size,
we often find that the trained model puts too much
weight on the dictionary membership feature and
new attribute values are not properly detected. In
this section, instead of using the seed list of known
attribute values as a feature into a classifier, we
use the seed values to automatically generate la-
beled training data. For the specific case of brand
discovery, this initial list used to generate training
data must contain only names that are unambigu-
ously brands. We hence remove ambiguous names
or phrases that belong to multiple attribute types
from the list, such as jumpers(both a brand name
and a garment type), or (ii) camel is a short name
of brand Camel active as well as a color, or (iii) lrg
is an acronym for a brand as well as an acronym for
large which specifies size.
The training/test data is generated by matching
N -gram tokens in listing titles to all the entries in
the initial brand seed dictionary. Following the con-
vention in (Minkov et al 2005), we use the follow-
ing set of 5 tags, (1) one-token entity (B1 tag) (2)
first token of a multi-token entity (Bo tag for Brand-
open) (3) last token of a multi-token entity (Bc tag
for Brand-close) (4) middle token of a multi-token
entity (Bi tag for Brand-inside) (5) token that is not
part of a brand entity (NA tag). The listings with
at least one non-NA tags are put in the training set,
and listings that contain only NA tags are in the test
set. Similar to the acronym expansion algorithm of
(Pakhomov 2002) which learns contexts that asso-
ciate acronyms to their correct expansions, the in-
tuition behind our work in this section is that the
classifier, trained on a labeled training set of known
brands, learns context patterns that can discriminate
the current word as being a brand (more precisely as
part of a brand) from the other attribute types, which
are now lumped together as NA.
5.2 Experiments
In the first experiment, a set of 72, 628 listings from
the women?s clothing category is partitioned into a
training set of 39, 448 listings and test set of 33, 180
listings based on an initial seed list of known 6, 312
women?s apparel brands manually prepared by our
fashion experts. The partitioning is done, as de-
scribed in great detail above, in such a way that
known brands in the seed list do not exist in the1563
Women?s Clothing Men?s Clothing Garment Type
?monsoon? henley?s nightshirt
riverislandtop abercrombie&fitch cargoshorts
dorothyperkins lacost trenchcoat
river islanfd versace sweatpants
marks&spencers sonnetti cardigans
river islands supremebeing boardshorts
river islan brookhaven tracksuite
monsoomn guiness swimshorts
dorothry perkins ?next? trouses
principle suprerdry microfleece
?river island henbury boilersuit
bnwtmonsoon paul smiths snopants
marella ricci pjs
soulcal craghopper jkt
Table 3: Discovered attribute values, ranked order by
their confidence scores. (Left) Discovered brands from
Women?s clothing category. We use 6,312 brands as seed
values. (Middle) Discovered brands from Men?s clothing
category, with 3,499 seed values used. (Right) Discov-
ered garment types (styles) from Men?s clothing category,
learned from 203 seed values.
test data (using exact string matching criterion). We
train a 5-class MaxEnt classifier and adopt the same
feature sets as described in Section 4.1.3. During
the test phase, the classifier predicts the most likely
brand attribute from each listing, where we are only
interested in the predictions with confidence scores
exceeding a set threshold. We ranked order the pre-
dicted brands by their confidence scores (probabil-
ities) and the top 300 unique brands are selected.
We manually verify the 300 predicted brands and
found that 90.33% of the predicted brands are indeed
names of designers or women?s apparel stores (true
positive), resulting a precision score of 90.33%.
Indeed, the precision score presented above is ob-
tained using an exact matching criterion where par-
tial extraction of a brand is regarded as a miss, i.e.
our extractor extracts only Calvin when Calvin Klein
is present in the listing (false positive). The left col-
umn of Table 3 shows examples of newly discovered
brands from Women?s clothing category. Many of
these newly discovered brands are indeed misspelled
versions of the known brands in the seed dictionary.
The middle column of Table 3 shows a set of
Men?s clothing brands learned automatically from a
similar experiment conducted on a set of 105, 335
listings from Men?s clothing category. Using an ini-
Seed list Test set 1 Test set 2
Orig. seeds 83.56% 90.02%
Orig. seed + 200 new brands 92.75% 93.66%
Table 4: NER Accuracy on 2 test sets as the seed dictio-
nary for brands grows. Results shown here are obtained
the same Men?s clothing category dataset, as used to show
the supervised NER results in Table 2.
tial set of 3, 499 known brand seeds, we partition
the dataset into a training set of 67, 307 listings and
a test set of 38, 028 listings (for later reference we
refer to this test set as set A). Based on the top 200
predicted brands, 179 of which are verified as being
true positive samples, resulting in 89.5% precision.
We carry out a similar experiment to grow the seed
dictionary for garment type, and are able to iden-
tify the top 60 new garment types. 54 out of 60 are
true positive samples, resulting in precision score =
90%. Examples of the newly discovered garment
types are shown in Table 3 (right column), where ab-
breviated forms of garment types such as jkt (short
for jacket) and pjs (short for pajamas) are also dis-
covered through our algorithm.
By adding these newly discovered attributes back
to the dictionary, we can now re-evaluate our super-
vised NER system from section 4 with the grown
seed list. To this end, we construct 2 test sets from
the same 105, 335 listings of Men?s clothing cate-
gory as used in Section 4. Test set 1 is a set of
500 listings randomly sampled from the 38, 028-
listing subset known not to contain any brands in
the original brand seed dictionary (set A). As seen
in Table 4, an improvement of 9% in accuracy re-
sults from the use of the grown seed list. Since
this dataset is known to not contain any brands from
the original brand seed dictionary, the addition of
200 new brands solely accounts for all the accuracy
boost. Test set 2 is constructed slightly differently
by randomly sampling 500 listings from the entire
105, 335 listings of Men?s clothing category. As
seen in Table 4, a smaller improvement of 3.7% is
observed.
6 Normalization
With the above described brand discovery algorithm,
the newly discovered brands from the test set can be
grouped into 2 categories ? (i) misspelling, spelling1564
invariants, abbreviated forms of known brands in the
seed list or (ii) novel brands or clothing/shoes de-
signers, which are not members of the original seed
list. Normalizing the variants of a known brand to
a single normalized output value is an important as-
pect of our attribute extraction algorithm, as these
variants account for over 20% of listings in the eBay
clothing and shoes category. When gathering busi-
ness/marketing intelligence, missing out on 20% of
the data could skew the calculation of supply, de-
mand, and pricing metrics, and eventually lead to
the wrong policy decision made.
The problem of alternate spellings of names has
been addressed in the database community success-
fully using fuzzy string matching algorithms e.g.
Soundex or string edit distance. In this work, since
the attribute values are often partially extracted, i.e.
a word in a multi-word phrase is extracted, in or-
der to match to the correct normalized value, we
must investigate robust substring matching algo-
rithms suitable for partial matching. To this end,
we explore 2 string similarity/distance measures for
normalizing the extracted attributes. First, we in-
vestigate n-gram similarity measures defined as the
number of shared character n-grams, i.e. substrings
of length n (Kondrak 2005). More specifically, a
string similarity measure between s1 and s2 is de-
fined as the percentage of common substrings of
length n (out of all substrings of length n). This
similarity measure is quite robust to partial match-
ing, as a two-word phrase can appear out of order
while most of the character n-grams, where n = 3,
remain virtually unchanged. Certainly, finding the
right value of n will greatly impact the matching per-
formance of the algorithm. In our experiment, we
find the optimal n for brands to be 3 and 4. Table 5
shows a few examples of normalized outputs as a re-
sult of finding the best match for the extracted brand
names from among a set of predefined normalized
values. When the best matching score falls below a
threshold, we declare no match is found and classify
the extracted brand as a new brand.
Another distance measure that we explore is the
Jaro-Winkler distance. Designed to be more suitable
for matching short strings such as people?s names,
Jaro-Winkler distance is defined based on the num-
ber of character transpositions and the number of
matching characters. In addition, a prefix scale p
Extracted brands Normalized values
river islands river island
fruit of loom fruit of the loom
fruit loom fruit of the loom
?ralph lauren ralph lauren
mark & spencer marks & spencer
yvessaintlaurent yves saint laurent
yves st laurent yves saint laurent
combats combat
?kickers? kickers
kickers kickers
armarni armani
abrecrombie abercrombie
life & limb NEW BRAND
oliver baker NEW BRAND
haines & bonner NEW BRAND
dehavilland NEW BRAND
nigel cabourn NEW BRAND
Table 5: Extracted brands and their corresponding nor-
malized values.
parameter is used and can be tuned to weigh more
favorably on strings that match from the beginning
for a set prefix length. In our experiments with brand
normalization, over 50% of the matches from the
Jaro-Winkler distance are, however, identified as be-
ing incorrect.
7 Conclusion
In this work, we have described an information ex-
traction system for applications in the domain of in-
ventory/business Intelligence. The goal is given an
eBay listing title, our system correctly extracts the
defining attributes in order to associate each item to
a specific product. We investigate and compare sev-
eral supervised NER systems ? supervised HMM,
SVM, MaxEnt, and CRF ? and found SVM and
MaxEnt with Viterbi decoding to yield the best per-
formance. Focusing on the clothing and shoes cat-
egories on eBay?s site, we presented a bootstrapped
algorithm that can identify new brand names corre-
sponding to (1) spelling invariants or typographical
errors of the known brands in the seed list and (2)
novel brands or designers. Our attribute extractor
correctly discovers new brands with over 90% pre-
cision on multiple corpora of listings. To output nor-
malized attribute values, we explore several fuzzy
string comparison algorithms and found n-gram sub-
string matching to work well in practice.1565
8 Acknowledgment
The authors would like to thank Nalini Johnas and
Padmanaban Ramasamy for their help in gathering
listing data used in all of our experiments.
References
A. Berger, S. Pietra, V. Pietra, A Maximum Entropy Ap-
proach to Natural Language Processing, ACL 1996.
S. Brody, N. Elhadad, An Unsupervised Aspect-Sentiment
Model for Online Reviews, HLT-NAACL 2010.
P. Brown, P. deSouza, R. Mercer, V. Della Pietra, J.
Lai, Class-based n-gram Models of Natural Language,
ACL 1992.
C.-C Chang, C.-J. Lin, LibSVM: A Library for Support
Vector Machines (2001).
H. L. Chieu, H. T. Ng, Named Entity Recognition with a
Maximum Entropy Approach, ACL 2003.
A. Clark, Combining Distributional and Morphological
Information for Part of Speech Induction, EACL 2003
G. Demartini, C. S. Firan, M. Georgescu, T. Iofciu, R.
Krestel, and W. Nejdl, An Architecture for Finding En-
tities on the web, Latin American Web Congress 2009.
J. Du, Z. Zhang, J. Yan, Y. Cui, and Z. Chen. Using
search session context for named entity recognition in
query. In SIGIR10, Geneva, Switzerland, July 19-23
2010.
Asif Ekbal, Rejwanul Haque, and Sivaji Bandyopadhyay.
2008. Named entity recognition in Bengali: A condi-
tional random field approach. In Proceedings of IJC-
NLP, pages 589594.
M. Faruqui, S. Pado, Training and Evaluating a German
Named Entity Recognizer with Semantic Generaliza-
tion, Proceedings of Konvens 2010, Saarbrucken, Ger-
many.
F. Feng, A. McCallum, Chinese segmentation and new
word detection using conditional random fields, in
COLING 2004.
J. R. Finkel, T. Grenager, and C. Manning, Incorporat-
ing Non-local Information into Information Extraction
Systems by Gibbs Sampling, ACL 2005.
J. R. Finkel, C. Manning, Nested Named Entity Recogni-
tion, EMNLP 2009.
R. Ghani, K. Probst, Y. Liu, M. Krema, A. Fano, Text
Mining for Product Attribute Extraction, SIGKDD,
2006.
R. Ghani, R. Jones, A comparison of efficacy and as-
sumptions of bootstrapping algorithms for training
information extraction systems, Workshop on Lin-
guistic Knowledge Acquisition and Representation at
the Third International Conference on Language Re-
sources and Evaluation (LREC), 2002.
T. Grenager, D. Klein, and C. D. Manning, Unsupervised
Learning of Field Segmentation Models for Informa-
tion Extraction, ACL 2005.
D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and A.
Sheth. Context and Domain Knowledge Enhanced En-
tity Spotting In Informal Text. In Proceedings of the 8th
International Semantic Web Conference (ISWC 2009).
Springer, 2009.
A. D. Haghighi, Unsupervised Models of Entity Refer-
ence Resolution, Ph. D. Thesis, University of Calfor-
nia, Berkeley, 2010.
P. Halacsy, A. Kornai, C. Oravecz, HunPos: an open
source trigram tagger, ACL 2007.
H. Isozaki and H. Kazawa, Efficient Support Vector Clas-
sifiers for Named Entity Recognition, ACL 2002.
R. Jones, Learning to Extract Entities from Labeled and
Unlabeled Text, PhD Thesis, 2005.
I. Kanaris, K. Kanaris, I. Houvardas, E. Stamatatos,
Words vs. Character N-grams for Anti-spam Filtering,
International Journal on Artificial Intelligence Tools,
2006.
D. Klein, J. Smarr, H. Nguyen, C. Manning, Named En-
tity Recognition with Character-level Models, CoNLL
2003.
R. Koeling, Chunking with Maximum Entropy Models,
Proc. of CoNLL-2000.
G. Kondrak, N-Gram Similarity and Distance, SPIRE
2005.
V. Krishnan and C. D. Manning, An effective two-stage
model for exploiting non-local dependencies in named
entity recognition, in ACL-COLING, 2006.
T. Kudo, Y. Matsumoto, Chunking with Support Vector
Machines, ACL 2001.
J. Lafferty, A. McCallum, F. Pereira, Conditional Ran-
dom Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data, ICML 2002.
V. I. Levenshtein, Binary code capable of correcting dele-
tions, insertions, and reversals. Phs. Dokl., 6:707-710.
D. Lin, X. Wu, Phrase Clustering for Discriminative
Learning, ACL 2009.
B. Liu, M. Hu, and J. Cheng, Opinion Observer: Ana-
lyzing and Comparing Opinions on the Web, WWW
2005.
Xinnian Mao, Saike He, Sencheng Bao, Yuan Dong,
and Haila Wang, Chinese Word Segmentation and
Named Entity Recognition Based on Conditional Ran-
dom Fields, Sixth SIGHAN Workshop on Chinese
Language Processing, 2008
A. McCallum, Efficiently Inducing Features of Condi-
tional Random Fields, UAI 2003.
A. McCallum, D. Jensen, A Note on Unification
of Information Extraction and Data Mining using
Conditional-Probability, Relational Models, Proceed-
ings of IJCAI-2003 on Learning Statistical Models
from Relational Data, 2003.1566
J. F. McCarthy, A Trainable Approach to Coreference
Resolution for Information Extraction, Ph. D. Thesis,
University of Massachusetts at Amherst, 1996.
E. Minkov, R. C. Wang, and W. W. Cohen, Extracting
Personal Names from Email: Applying Named Entity
Recognition to Informal Text, ACL 2005.
Mike Mintz, Steven Bills, Rion Snow, Daniel Juraf-
sky. 2009. Distant Supervision for Relation Extraction
without Labeled Data, In Proceedings of ACL/AFNLP
2009.
S. Moghaddam, M. Ester, Opinion Digger: An Unsuper-
vised Opinion Miner from Unstructured Product Re-
views, CIKM 2010
David Nadeau, P. Turney, S.Matwin, Unsupervised
Named Entity Recognition: Generating Gazetteers
and Resolving Ambiguity. In Proc. Canadian Confer-
ence on Artificial Intelligence, 2006.
David Nadeau and Satoshi Sekine. A survey of named en-
tity recognition and classification. Linguisticae Inves-
tigationes, 30(1):326, 2007.
Nadeau, D., Semi-Supervised Named Entity Recognition:
Learning to Recognize 100 Entity Types with Little Su-
pervision, PhD thesis, University of Ottawa, 2007.
S. Pakhomov, Semi-supervised Maximum Entropy Based
Approach to Acronym and Abbreviation Normalization
in Medical Texts, ACL 2002.
A.-M. Popescu, O. Etzioni, Extracting Product Features
and Opinions from Reviews, EMNLP 2005.
K. Probst, R. Ghani, M. Krema, A. Fano, Semi-
Supervised Learning to Extract Attribute-Value Pairs
from Product Descriptions on the Web, ECML 2006.
V. Punyakanok, D. Roth, The use of classifiers in sequen-
tial inference, NIPS 2001.
H. Raghavan, J. Allan, Matching Inconsistently Spelled
Names in Automatic Speech Recognizer Output for In-
formation Retrieval, HLT-EMNLP 2005.
A. Ratnaparkhi, A Maximum Entropy Part of Speech Tag-
ger. In EMNLP 1996.
A. Ratnaparkhi, Maximum Entropy Models forNatural
Language Ambiguity Resolution, Ph. D. Thesis, Uni-
versity of Pennsylvania.
E. Riloff, R. Jones, Learning Dictionaries for Informa-
tion Extraction by Multi-Level Bootstrapping, AAAI
1999.
Settles, B. (2004), Biomedical named entity recognition
using conditional random fields and rich feature sets,
in Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (NLPBA), 2004, Geneva, Switzerland.
W. M. Soon, H. T. Ng, D. Chung, Y. Lim, A machine
learning approach to coreference resolution of noun
phrases, Computational Linguistics, 27(4): 521-544,
2001.
H. Wallach, Efficient Training of Conditional Random
Fields, M. Sc. Thesis, Division of Informatics, Uni-
versity of Edinburgh, 2002.
D. Wu, W. S. Lee, N. Ye, and H. L. Chieu, Domain
adaptive bootstrapping for named entity recognition,
EMNLP 2009.
Y. Zhao, B. Qin, S. Hu, T. Liu, Generalizing Syntactic
Structures for Product Attribute Candidate Extraction,
ACL 2010
1567
