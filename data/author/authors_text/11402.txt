Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 16?23,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Experiments Using OSTIA for a Language Production Task
Dana Angluin and Leonor Becerra-Bonache
Department of Computer Science, Yale University
P.O.Box 208285, New Haven, CT, USA
{dana.angluin, leonor.becerra-bonache}@yale.edu
Abstract
The phenomenon of meaning-preserving
corrections given by an adult to a child
involves several aspects: (1) the child
produces an incorrect utterance, which
the adult nevertheless understands, (2) the
adult produces a correct utterance with the
same meaning and (3) the child recognizes
the adult utterance as having the same
meaning as its previous utterance, and
takes that as a signal that its previous ut-
terance is not correct according to the adult
grammar. An adequate model of this phe-
nomenon must incorporate utterances and
meanings, account for how the child and
adult can understand each other?s mean-
ings, and model how meaning-preserving
corrections interact with the child?s in-
creasing mastery of language production.
In this paper we are concerned with how
a learner who has learned to comprehend
utterances might go about learning to pro-
duce them.
We consider a model of language com-
prehension and production based on finite
sequential and subsequential transducers.
Utterances are modeled as finite sequences
of words and meanings as finite sequences
of predicates. Comprehension is inter-
preted as a mapping of utterances to mean-
ings and production as a mapping of mean-
ings to utterances. Previous work (Castel-
lanos et al, 1993; Pieraccini et al, 1993)
has applied subsequential transducers and
the OSTIA algorithm to the problem of
learning to comprehend language; here we
apply them to the problem of learning to
produce language. For ten natural lan-
guages and a limited domain of geomet-
ric shapes and their properties and rela-
tions we define sequential transducers to
produce pairs consisting of an utterance
in that language and its meaning. Using
this data we empirically explore the prop-
erties of the OSTIA and DD-OSTIA al-
gorithms for the tasks of learning compre-
hension and production in this domain, to
assess whether they may provide a basis
for a model of meaning-preserving correc-
tions.
1 Introduction
The role of corrections in language learning has
recently received substantial attention in Gram-
matical Inference. The kinds of corrections con-
sidered are mainly syntactic corrections based on
proximity between strings. For example, a cor-
rection of a string may be given by using edit
distance (Becerra-Bonache et al, 2007; Kinber,
2008) or based on the shortest extension of the
queried string (Becerra-Bonache et al, 2006),
among others. In these approaches semantic in-
formation is not used.
However, in natural situations, a child?s er-
roneous utterances are corrected by her parents
based on the meaning that the child intends to ex-
press; typically, the adult?s corrections preserve
the intended meaning of the child. Adults use cor-
rections in part as a way of making sure they have
understood the child?s intentions, in order to keep
the conversation ?on track?. Thus the child?s ut-
terance and the adult?s correction have the same
meaning, but the form is different. As Chouinard
and Clark point out (2003), because children at-
tend to contrasts in form, any change in form that
does not mark a different meaning will signal to
children that they may have produced something
that is not acceptable in the target language. Re-
sults in (Chouinard and Clark, 2003) show that
adults reformulate erroneous child utterances of-
ten enough to help learning. Moreover, these re-
16
sults show that children can not only detect differ-
ences between their own utterance and the adult
reformulation, but that they do make use of that
information.
Thus in some natural situations, corrections
have a semantic component that has not been taken
into account in previous Grammatical Inference
studies. Some interesting questions arise: What
are the effects of corrections on learning syntax?
Can corrections facilitate the language learning
process? One of our long-term goals is to find a
formal model that gives an account of this kind
of correction and in which we can address these
questions. Moreover, such a model might allow us
to show that semantic information can simplify the
problem of learning formal languages.
A simple computational model of semantics and
context for language learning incorporating se-
mantics was proposed in (Angluin and Becerra-
Bonache, 2008). This model accommodates two
different tasks: comprehension and production.
That paper focused only on the comprehension
task and formulated the learning problem as fol-
lows. The teacher provides to the learner several
example pairs consisting of a situation and an ut-
terance denoting something in the situation; the
goal of the learner is to learn the meaning func-
tion, allowing the learner to comprehend novel ut-
terances. The results in that paper show that under
certain assumptions, a simple algorithm can learn
to comprehend an adult?s utterance in the sense of
producing the same sequence of predicates, even
without mastering the adult?s grammar. For exam-
ple, receiving the utterance the blue square above
the circle, the learner would be able to produce the
sequence of predicates (bl, sq, ab, ci).
In this paper we focus on the production task,
using sequential and subsequential transducers to
model both comprehension and production. Adult
production can be modeled as converting a se-
quence of predicates into an utterance, which can
be done with access to the meaning transducer for
the adult?s language.
However, we do not assume that the child ini-
tially has access to the meaning transducer for
the adult?s language; instead we assume that the
child?s production progresses through different
stages. Initially, child production is modeled as
consisting of two different tasks: finding a correct
sequence of predicates, and inverting the meaning
function to produce a kind of ?telegraphic speech?.
For example, from (gr, tr, le, sq) the child may
produce green triangle left square. Our goal is to
model how the learner might move from this tele-
graphic speech to speech that is grammatical in the
adult?s sense. Moreover, we would like to find a
formal framework in which corrections (in form of
expansions, for example, the green triangle to the
left of the square) can be given to the child dur-
ing the intermediate stages (before the learner is
able to produce grammatically correct utterances)
to study their effect on language learning.
We thus propose to model the problem of
child language production as a machine trans-
lation problem, that is, as the task of translat-
ing a sequence of predicate symbols (representing
the meaning of an utterance) into a correspond-
ing utterance in a natural language. In this pa-
per we explore the possibility of applying existing
automata-theoretic approaches to machine transla-
tion to model language production. In Section 2,
we describe the use of subsequential transducers
for machine translation tasks and review the OS-
TIA algorithm to learn them (Oncina, 1991). In
Section 3, we present our model of how the learner
can move from telegraphic to adult speech. In Sec-
tion 4, we present the results of experiments in the
model made using OSTIA. Discussion of these re-
sults is presented in Section 5 and ideas for future
work are in Section 6.
2 Learning Subsequential Transducers
Subsequential transducers (SSTs) are a formal
model of translation widely studied in the liter-
ature. SSTs are deterministic finite state mod-
els that allow input-output mappings between lan-
guages. Each edge of an SST has an associated
input symbol and output string. When an in-
put string is accepted, an SST produces an out-
put string that consists of concatenating the out-
put substrings associated with sequence of edges
traversed, together with the substring associated
with the last state reached by the input string. Sev-
eral phenomena in natural languages can be eas-
ily represented by means of SSTs, for example,
the different orders of noun and adjective in Span-
ish and English (e.g., un cuadrado rojo - a red
square). Formal and detailed definitions can be
found in (Berstel, 1979).
For any SST it is always possible to find an
equivalent SST that has the output strings assigned
to the edges and states so that they are as close to
17
the initial state as they can be. This is called an
Onward Subsequential Transducer (OST).
It has been proved that SSTs are learnable in
the limit from a positive presentation of sentence
pairs by an efficient algorithm called OSTIA (On-
ward Subsequential Transducer Inference Algo-
rithm) (Oncina, 1991). OSTIA takes as input a fi-
nite training set of input-output pairs of sentences,
and produces as output an OST that generalizes
the training pairs. The algorithm proceeds as fol-
lows (this description is based on (Oncina, 1998)):
? A prefix tree representation of all the input
sentences of the training set is built. Empty
strings are assigned as output strings to both
the internal nodes and the edges of this tree,
and every output sentence of the training set
is assigned to the corresponding leaf of the
tree. The result is called a tree subsequential
transducer.
? An onward tree subsequential transducer
equivalent to the tree subsequential trans-
ducer is constructed by moving the longest
common prefixes of the output strings, level
by level, from the leaves of the tree towards
the root.
? Starting from the root, all pairs of states of
the onward tree subsequential transducer are
considered in order, level by level, and are
merged if possible (i.e., if the resulting trans-
ducer is subsequential and does not contra-
dict any pair in the training set).
SSTs and OSTIA have been successfully ap-
plied to different translation tasks: Roman numer-
als to their decimal representations, numbers writ-
ten in English to their Spanish spelling (Oncina,
1991) and Spanish sentences describing simple
visual scenes to corresponding English and Ger-
man sentences (Castellanos et al, 1994). They
have also been applied to language understanding
tasks (Castellanos et al, 1993; Pieraccini et al,
1993).
Moreover, several extensions of OSTIA have
been introduced. For example, OSTIA-DR incor-
porates domain (input) and range (output) mod-
els in the learning process, allowing the algorithm
to learn SSTs that accept only sentences compat-
ible with the input model and produce only sen-
tences compatible with the output model (Oncina
and Varo, 1996). Experiments with a language un-
derstanding task gave better results with OSTIA-
DR than with OSTIA (Castellanos et al, 1993).
Another extension is DD-OSTIA (Oncina, 1998),
which instead of considering a lexicographic order
to merge states, uses a heuristic order based on a
measure of the equivalence of the states. Experi-
ments in (Oncina, 1998) show that better results
can be obtained by using DD-OSTIA in certain
translation tasks from Spanish to English.
3 From telegraphic to adult speech
To model how the learner can move from tele-
graphic speech to adult speech, we reduce this
problem to a translation problem, in which the
learner has to learn a mapping from sequences of
predicates to utterances. As we have seen in the
previous section, SSTs are an interesting approach
to machine translation. Therefore, we explore the
possibility of modeling language production using
SSTs and OSTIA, to see whether they may pro-
vide a good framework to model corrections.
As described in (Angluin and Becerra-Bonache,
2008), after learning the meaning function the
learner is able to assign correct meanings to ut-
terances, and therefore, given a situation and an
utterance that denotes something in the situation,
the learner is able to point correctly to the object
denoted by the utterance. To simplify the task
we consider, we make two assumptions about the
learner at the start of the production phase: (1)
the learner?s lexicon represents a correct meaning
function and (2) the learner can generate correct
sequences of predicates.
Therefore, in the initial stage of the production
phase, the learner is able to produce a kind of
?telegraphic speech? by inverting the lexicon con-
structed during the comprehension stage. For ex-
ample, if the sequence of predicates is (bl, sq, ler,
ci), and in the lexicon blue is mapped to bl, square
to sq, right to ler and circle to ci, then by invert-
ing this mapping, the learner would produce blue
square right circle.
In order to explore the capability of SSTs and
OSTIA to model the next stage of language pro-
duction (from telegraphic to adult speech), we take
the training set to be input-output pairs each of
which contains as input a sequence of predicates
(e.g., (bl, sq, ler, ci)) and as output the correspond-
ing utterance in a natural language (e.g., the blue
square to the right of the circle). In this example,
18
the learner must learn to include appropriate func-
tion words. In other languages, the learner may
have to learn a correct choice of words determined
by gender, case or other factors. (Note that we are
not yet in a position to consider corrections.)
4 Experiments
Our experiments were made for a limited domain
of geometric shapes and their properties and re-
lations. This domain is a simplification of the
Miniature Language Acquisition task proposed by
Feldman et al (Feldman et al, 1990). Previous
applications of OSTIA to language understanding
and machine translation have also used adapta-
tions and extensions of the Feldman task.
In our experiments, we have predicates for three
different shapes (circle (ci), square (sq) and tri-
angle (tr)), three different colors (blue (bl), green
(gr) and red (re)) and three different relations (to
the left of (le), to the right of (ler), and above (ab)).
We consider ten different natural languages: Ara-
bic, English, Greek, Hebrew, Hindi, Hungarian,
Mandarin, Russian, Spanish and Turkish.
We created a data sequence of input-output
pairs, each consisting of a predicate sequence and
a natural language utterance. For example, one
pair for Spanish is ((ci, re, ler, tr), el circulo rojo
a la derecha del triangulo). We ran OSTIA on ini-
tial segments of the sequence of pairs, of lengths
10, 20, 30, . . ., to produce a sequence of subse-
quential transducers. The whole data sequence
was used to test the correctness of the transducers
generated during the process. An error is counted
whenever given a data pair (x, y), the subsequen-
tial transducer translates x to y?, and y? 6= y. We
say that OSTIA has converged to a correct trans-
ducer if all the transducers produced afterwards
have the same number of states and edges, and 0
errors on the whole data sequence.
To generate the sequences of input-output pairs,
for each language we constructed a meaning trans-
ducer capable of producing the 444 different pos-
sible meanings involving one or two objects. We
randomly generated 400 unique (non-repeated)
input-output pairs for each language. This process
was repeated 10 times. In addition, to investigate
the effect of the order of presentation of the input-
output pairs, we repeated the data generation pro-
cess for each language, sorting the pairs according
to a length-lex ordering of the utterances.
We give some examples to illustrate the trans-
ducers produced. Figure 1 shows an example of
a transducer produced by OSTIA after just ten
pairs of input-output examples for Spanish. This
transducer correctly translates the ten predicate se-
quences used to construct it, but the data is not
sufficient for OSTIA to generalize correctly in all
cases, and many other correct meanings are still
incorrectly translated. For example, the sequence
(ci, bl) is translated as el circulo a la izquierda del
circulo verde azul instead of el circulo azul.
The transducers produced after convergence by
OSTIA and DD-OSTIA correctly translate all 444
possible correct meanings. Examples for Spanish
are shown in Figure 2 (OSTIA) and Figure 3 (DD-
OSTIA). Note that although they correctly trans-
late all 444 correct meanings, the behavior of these
two transducers on other (incorrect) predicate se-
quences is different, for example on (tr, tr).
1
bl/ azul
 sq/ el cuadrado
 ci/el circulo a la
 izquierda del
  circulo verde
2
tr/ el triangulo
 le/ 
 ler/ 
re/ rojo a la derecha
 del cuadrado
sq/ 
 ci/ 
 bl/ azul
 gr/ 
 ler/ a la derecha
 del cuadrado
3ab/ 
tr/ encima del triangulo
 ci/ verde encima del
 circulo azul
 bl/ 
 re/ rojo
Figure 1: Production task, OSTIA. A transducer
produced using 10 random unique input-output
pairs (predicate sequence, utterance) for Spanish.
1
bl/ azul
 sq/ el cuadrado
2
le/ a la izquierda del
 ler/ a la derecha del
 ab/ encima del
 re/ rojo
 gr/ verde
 ci/ el circulo
 tr/ el triangulo
le/ a la izquierda del
 ler/ a la derecha del
 ab/ encima del
 bl/ azul
 re/ rojo
 gr/ verde
 sq/ cuadrado
 ci/ circulo
 tr/ triangulo
Figure 2: Production task, OSTIA. A transducer
produced (after convergence) by using random
unique input-output pairs (predicate sequence, ut-
terance) for Spanish.
Different languages required very different
numbers of data pairs to converge. Statistics on
the number of pairs needed until convergence for
OSTIA for all ten languages for both random
unique and random unique sorted data sequences
are shown in Table 1. Because better results were
reported using DD-OSTIA in machine translation
19
1bl/ azul
 re/ rojo
 gr/ verde
 sq/ el cuadrado
 ci/ el circulo
 tr/ el triangulo
2
le/ a la izquierda del
 ler/ a la derecha del
 ab/ encima del
sq/ cuadrado
 ci/ circulo
 tr/ triangulo
Figure 3: Production task, DD-OSTIA. A trans-
ducer produced (after convergence) using random
unique input-output pairs (predicate-sequence, ut-
terance) for Spanish.
Language # Pairs # Sorted Pairs
Arabic 150 200
English 200 235
Greek 375 400
Hebrew 195 30
Hindi 380 350
Hungarian 365 395
Mandarin 45 150
Russian 270 210
Spanish 190 150
Turkish 185 80
Table 1: Production task, OSTIA. The entries give
the median number of input-output pairs until con-
vergence in 10 runs. For Greek, Hindi and Hun-
garian, the median for the unsorted case is calcu-
lated using all 444 random unique pairs, instead of
400.
tasks (Oncina, 1998), we also tried using DD-
OSTIA for learning to translate a sequence of
predicates to an utterance. We used the same se-
quences of input-output pairs as in the previous
experiment. The results obtained are shown in Ta-
ble 2.
We also report the sizes of the transducers
learned by OSTIA and DD-OSTIA. Table 3 and
Table 4 show the numbers of states and edges
of the transducers after convergence for each lan-
guage. In case of disagreements, the number re-
ported is the mode.
To answer the question of whether production
is harder than comprehension in this setting, we
also considered the comprehension task, that is,
to translate an utterance in a natural language
into the corresponding sequence of predicates.
Language # Pairs # Sorted Pairs
Arabic 80 140
English 85 180
Greek 350 400
Hebrew 65 80
Hindi 175 120
Hungarian 245 140
Mandarin 40 150
Russian 185 210
Spanish 80 150
Turkish 50 40
Table 2: Production task, DD-OSTIA. The entries
give the median number of input-output pairs un-
til convergence in 10 runs. For Greek, Hindi and
Hungarian, the median for the unsorted case is cal-
culated using all 444 random unique pairs, instead
of 400.
Languages #states #edges
Arabic 2 20
English 2 20
Greek 9 65
Hebrew 2 20
Hindi 7 58
Hungarian 3 20
Mandarin 1 10
Russian 3 30
Spanish 2 20
Turkish 4 31
Table 3: Production task, OSTIA. Sizes of trans-
ducers at convergence.
The comprehension task was studied by Oncina
et al (Castellanos et al, 1993). They used En-
glish sentences, with a more complex version of
the Feldman task domain and more complex se-
mantic representations than we use. Our results
are presented in Table 5. The number of states
and edges of the transducers after convergence is
shown in Table 6.
5 Discussion
It should be noted that because the transducers
output by OSTIA and DD-OSTIA correctly repro-
duce all the pairs used to construct them, once ei-
ther algorithm has seen all 444 possible data pairs
in either the production or the comprehension task,
the resulting transducers will correctly translate all
correct inputs. However, state-merging in the al-
20
Languages #states #edges
Arabic 2 17
English 2 16
Greek 9 45
Hebrew 2 13
Hindi 7 40
Hungarian 3 20
Mandarin 1 10
Russian 3 23
Spanish 2 13
Turkish 3 18
Table 4: Production task, DD-OSTIA. Sizes of
transducers at convergence.
Languages OSTIA DD-OSTIA
Arabic 65 65
English 60 20
Greek 325 60
Hebrew 90 45
Hindi 60 35
Hungarian 40 45
Mandarin 60 40
Russian 280 55
Spanish 45 30
Turkish 60 35
Table 5: Comprehension task, OSTIA and DD-
OSTIA. Median number (in 10 runs) of input-
output pairs until convergence using a sequence of
400 random unique pairs of (utterance, predicate
sequence).
gorithms induces compression and generalization,
and the interesting questions are how much data
is required to achieve correct generalization, and
how that quantity scales with the complexity of
the task. This are very difficult questions to ap-
proach analytically, but empirical results can offer
valuable insights.
Considering the comprehension task (Tables 5
and 6), we see that OSTIA generalizes correctly
from at most 15% of all 444 possible pairs except
in the cases of Greek, Hebrew and Russian. DD-
OSTIA improves the OSTIA results, in some cases
dramatically, for all languages except Hungarian.
DD-OSTIA achieves correct generalization from
at most 15% of all possible pairs for all ten lan-
guages. Because the meaning function for all ten
language transducers is independent of the state,
in each case there is a 1-state sequential trans-
Languages #states #edges
Arabic 1 15
English 1 13
Greek 2 25
Hebrew 1 13
Hindi 1 13
Hungarian 1 14
Mandarin 1 17
Russian 1 24
Spanish 1 14
Turkish 1 13
Table 6: Comprehension task, OSTIA and DD-
OSTIA. Sizes of transducers at convergence using
400 random unique input-output pairs (utterance,
predicate sequence). In cases of disagreement, the
number reported is the mode.
ducer that achieves correct translation of correct
utterances into predicate sequences. OSTIA and
DD-OSTIA converged to 1-state transducers for
all languages except Greek, for which they con-
verged to 2-state transducers. Examining one such
transducer for Greek, we found that the require-
ment that the transducer be ?onward? necessitated
two states. These results are broadly compatible
with the results obtained by Oncina et al (Castel-
lanos et al, 1993) on language understanding; the
more complex tasks they consider also give evi-
dence that this approach may scale well for the
comprehension task.
Turning to the production task (Tables 1, 2, 3
and 4), we see that providing the random samples
with a length-lex ordering of utterances has incon-
sistent effects for both OSTIA and DD-OSTIA,
sometimes dramatically increasing or decreasing
the number of samples required. We do not fur-
ther consider the sorted samples.
Comparing the production task with the com-
prehension task for OSTIA, the production task
generally requires substantially more random
unique samples than the comprehension task for
the same language. The exceptions are Mandarin
(production: 45 and comprehension: 60) and Rus-
sian (production: 270 and comprehension: 280).
For DD-OSTIA the results are similar, with the
sole exception of Mandarin (production: 40 and
comprehension: 40). For the production task DD-
OSTIA requires fewer (sometimes dramatically
fewer) samples to converge than OSTIA. How-
ever, even with DD-OSTIA the number of sam-
21
ples is in several cases (Greek, Hindi, Hungarian
and Russian) a rather large fraction (40% or more)
of all 444 possible pairs. Further experimentation
and analysis is required to determine how these re-
sults will scale.
Looking at the sizes of the transducers learned
by OSTIA and DD-OSTIA in the production task,
we see that the numbers of states agree for all lan-
guages except Turkish. (Recall from our discus-
sion in Section 4 that there may be differences in
the behavior of the transducers learned by OSTIA
and DD-OSTIA at convergence.) For the produc-
tion task, Mandarin gives the smallest transducer;
for this fragment of the language, the translation
of correct predicate sequences into utterances can
be achieved with a 1-state transducer. In contrast,
English and Spanish both require 2 states to handle
articles correctly. For example, in the transducer
in Figure 3, the predicate for a circle (ci) is trans-
lated as el circulo if it occurs as the first object (in
state 1) and as circulo if it occurs as second ob-
ject (in state 2) because del has been supplied by
the translation of the intervening binary relation
(le, ler, or ab.) Greek gives the largest transducer
for the production task, with 9 states, and requires
the largest number of samples for DD-OSTIA to
achieve convergence, and one of the largest num-
bers of samples for OSTIA. Despite the evidence
of the extremes of Mandarin and Greek, the rela-
tion between the size of the transducer for a lan-
guage and the number of samples required to con-
verge to it is not monotonic.
In our model, one reason that learning the pro-
duction task may in general be more difficult than
learning the comprehension task is that while the
mapping of a word to a predicate does not depend
on context, the mapping of a predicate to a word
or words does (except in the case of our Mandarin
transducer.) As an example, in the comprehension
task the Russian words triugolnik, triugolnika and
triugonikom are each mapped to the predicate tr,
but the reverse mapping must be sensitive to the
context of the occurrence of tr.
These results suggest that OSTIA or DD-
OSTIA may be an effective method to learn to
translate sequences of predicates into natural lan-
guage utterances in our domain. However, some of
our objectives seem incompatible with the proper-
ties of OSTIA. In particular, it is not clear how
to incorporate the learner?s initial knowledge of
the lexicon and ability to produce ?telegraphic
speech? by inverting the lexicon. Also, the in-
termediate results of the learning process do not
seem to have the properties we expect of a learner
who is progressing towards mastery of produc-
tion. That is, the intermediate transducers per-
fectly translate the predicate sequences used to
construct them, but typically produce other trans-
lations that the learner (using the lexicon) would
know to be incorrect. For example, the intermedi-
ate transducer from Figure 1 translates the predi-
cate sequence (ci) as el circulo a la izquierda del
circulo verde, which the learner?s lexicon indicates
should be translated as (ci, le, ci, gr).
6 Future work
Further experiments and analysis are required to
understand how these results will scale with larger
domains and languages. In this connection, it may
be interesting to try the experiments of (Castel-
lanos et al, 1993) in the reverse (production) di-
rection. Finding a way to incorporate the learner?s
initial lexicon seems important. Perhaps by incor-
porating the learner?s knowledge of the input do-
main (the legal sequences of predicates) and using
the domain-aware version, OSTIA-D, the interme-
diate results in the learning process would be more
compatible with our modeling objectives. Coping
with errors will be necessary; perhaps an explic-
itly statistical framework for machine translation
should be considered.
If we can find an appropriate model of how
the learner?s language production process might
evolve, then we will be in a position to model
meaning-preserving corrections. That is, the
learner chooses a sequence of predicates and maps
it to a (flawed) utterance. Despite its flaws, the
learner?s utterance is understood by the teacher
(i.e., the teacher is able to map it to the sequence
of predicates chosen by the learner) and responds
with a correction, that is, a correct utterance for
that meaning. The learner, recognizing that the
teacher?s utterance has the same meaning but a
different form, then uses the correct utterance (as
well as the meaning and the incorrect utterance) to
improve the mapping of sequences of predicates to
utterances.
It is clear that in this model, corrections are not
necessary to the process of learning comprehen-
sion and production; once the learner has a correct
lexicon, the utterances of the teacher can be trans-
lated into sequences of predicates, and the pairs
22
of (predicate sequence, utterance) can be used to
learn (via an appropriate variant of OSTIA) a per-
fect production mapping. However, it seems very
likely that corrections can make the process of
learning a production mapping easier or faster, and
finding a model in which such phenomena can be
studied remains an important goal of this work.
7 Acknowledgments
The authors sincerely thank Prof. Jose Oncina
for the use of his programs for OSTIA and DD-
OSTIA, as well as his helpful and generous ad-
vice. The research of Leonor Becerra-Bonache
was supported by a Marie Curie International
Fellowship within the 6th European Community
Framework Programme.
References
Dana Angluin and Leonor Becerra-Bonache. 2008.
Learning Meaning Before Syntax. ICGI, 281?292.
Leonor Becerra-Bonache, Colin de la Higuera, J.C.
Janodet, and Frederic Tantini. 2007. Learning Balls
of Strings with Correction Queries. ECML, 18?29.
Leonor Becerra-Bonache, Adrian H. Dediu, and
Cristina Tirnauca. 2006. Learning DFA from Cor-
rection and Equivalence Queries. ICGI, 281?292.
Jean Berstel. 1979. Transductions and Context-Free
Languages. PhD Thesis, Teubner, Stuttgart, 1979.
Antonio Castellanos, Enrique Vidal, and Jose Oncina.
1993. Language Understanding and Subsequential
Transducers. ICGI, 11/1?11/10.
Antonio Castellanos, Ismael Galiano, and Enrique Vi-
dal. 1994. Applications of OSTIA to machine trans-
lation tasks. ICGI, 93?105.
Michelle M. Chouinard and Eve V. Clark. 2003. Adult
Reformulations of Child Errors as Negative Evi-
dence. Journal of Child Language, 30:637?669.
Jerome A. Feldman, George Lakoff, Andreas Stolcke,
and Susan Hollback Weber. 1990. Miniature Lan-
guage Acquisition: A touchstone for cognitive sci-
ence. Technical Report, TR-90-009. International
Computer Science Institute, Berkeley, California.
April, 1990.
Efim Kinber. 2008. On Learning Regular Expres-
sions and Patterns Via Membership and Correction
Queries. ICGI, 125?138.
Jose Oncina. 1991. Aprendizaje de lenguajes regu-
lares y transducciones subsecuenciales. PhD Thesis,
Universitat Politecnica de Valencia, Valencia, Spain,
1998.
Jose Oncina. 1998. The data driven approach applied
to the OSTIA algorithm. ICGI, 50?56.
Jose Oncina and Miguel Angel Varo. 1996. Using do-
main information during the learing of a subsequen-
tial transducer. ICGI, 301?312.
Roberto Pieraccini, Esther Levin, and Enrique Vidal.
1993. Learning how to understand language. Eu-
roSpeech?93, 448?458.
23
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 97?105,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Effects of Meaning-Preserving Corrections on Language Learning
Dana Angluin ?
Department of Computer Science
Yale University, USA
dana.angluin@yale.edu
Leonor Becerra-Bonache
Laboratoire Hubert Curien
Universite? de Saint-Etienne, France
leonor.becerra@univ-st-etienne.fr
Abstract
We present a computational model of lan-
guage learning via a sequence of interactions
between a teacher and a learner. Experiments
learning limited sublanguages of 10 natural
languages show that the learner achieves a
high level of performance after a reasonable
number of interactions, the teacher can pro-
duce meaning-preserving corrections of the
learner?s utterances, and the learner can de-
tect them. The learner does not treat correc-
tions specially; nonetheless in several cases,
significantly fewer interactions are needed by
a learner interacting with a correcting teacher
than with a non-correcting teacher.
1 Introduction
A child learning his or her native language typically
does so while interacting with other people who are
using the language to communicate in shared situ-
ations. The correspondence between situations and
utterances seems likely to be a very important source
of information for the language learner. Once a
child begins to produce his or her own utterances,
other people?s responses to them (or lack thereof)
are another source of information about the lan-
guage. When the child?s utterances fall short of
adult-level competence, sometimes the other person
in the conversation will repeat the child?s utterance
in a more correct form. A number of studies have
focused on the phenomenon of such corrections and
questions of their frequency in child-directed speech
?Research supported by the National Science Foundation,
Grant CCF-0916389.
and whether children can and do make use of them;
some of these studies are discussed in the next sec-
tion.
In this paper we construct a computational model
with a learner and a teacher who interact in a se-
quence of shared situations. In each situation the
teacher and learner interact as follows. First the
learner uses what it has learned about the language
to (attempt to) generate an utterance appropriate to
the situation. The teacher then analyzes the correct-
ness of the learner?s utterance and either generates
an utterance intended as a correction of the learner?s
utterance, or generates another utterance of its own
appropriate to the situation. Finally, the learner uses
information given by its own utterance, the teacher?s
utterance and the situation to update its knowledge
of the language. At the conclusion of this interac-
tion, a new interaction is begun with the next situa-
tion in the sequence.
Both the learner and the teacher engage in com-
prehension and production of utterances which are
intended to be appropriate to their shared situation.
This setting allows us to study several questions:
whether the teacher can offer meaningful correc-
tions to the learner, whether the learner can detect
intended corrections by the teacher, and whether the
presence of corrections by the teacher has an ef-
fect on language acquisition by the learner. For our
model, the answer to each of these questions is yes,
and while the model is in many respects artificial and
simplified, we believe it sheds new light on these is-
sues. Additional details are available (Angluin and
Becerra-Bonache, 2010).
97
2 Meaning-preserving corrections
Formal models of language acquisition have mainly
focused on learning from positive data, that is, utter-
ances that are grammatically correct. But a question
that remains open is: Do children receive negative
data and can they make use of it?
Chomsky?s poverty of stimulus argument has
been used to support the idea of human innate lin-
guistic capacity. It is claimed that there are princi-
ples of grammar that cannot be learned from positive
data only, and negative evidence is not available to
children. Hence, since children do not have enough
evidence to induce the grammar of their native lan-
guage, the additional knowledge language learners
need is provided by some form of innate linguistic
capacity.
E. M. Gold?s negative results in the framework
of formal language learning have also been used to
support the innateness of language. Gold proved
that superfinite classes of languages are not learn-
able from positive data only, which implies than
none of the language classes defined by Chomsky
to model natural language is learnable from positive
data only (Gold, 1967).
Brown and Hanlon (Brown and Hanlon, 1970)
studied negative evidence understood as explicit
approvals or disapprovals of a child?s utterance
(e.g.,?That?s right? or ?That?s wrong.?) They
showed that there is no dependence between these
kinds of answers and the grammaticality of chil-
dren?s utterances. These results were taken as show-
ing that children do not receive negative data. But
do these results really show this? It seems evident
that parents rarely address their children in that way.
During the first stages of language acquisition chil-
dren make a lot of errors, and parents are not con-
stantly telling them that their sentences are wrong;
rather the important thing is that they can communi-
cate with each other. However, it is worth studying
whether other sources of negative evidence are pro-
vided to children. Is this the only form of negative
data? Do adults correct children in a different way?
Some researchers have studied other kinds of
negative data based on reply-types (e.g., Hirsh-
Pasek et al (Hirsh-Pasek et al, 1984), Demetras
et al (Demetras et al, 1986) and Morgan and
Travis (Morgan and Travis, 1989).) These studies
argue that parents provide negative evidence to their
children by using different types of reply to gram-
matical versus ungrammatical sentences. Marcus
analyzed such studies and concluded that there is
no evidence that this kind of feedback (he called it
noisy feedback) is required for language learning,
or even that it exists (Marcus, 1993). He argued
for the weakness, inconsistency and inherently ar-
tificial nature of this kind of feedback. Moreover, he
suggested that even if such feedback exists, a child
would learn which forms are erroneous only after
complex statistical comparisons. Therefore, he con-
cluded that internal mechanisms are necessary to ex-
plain how children recover from errors in language
acquisition.
Since the publication of the work of Marcus, the
consensus seemed to be that children do not have ac-
cess to negative data. However, a study carried out
by Chouinard and Clark shows that this conclusion
may be wrong (Chouinard and Clark, 2003). First,
they point out that the reply-type approach does not
consider whether the reply itself also contains cor-
rective information, and consequently, replies that
are corrective are erroneously grouped with those
that are not. Moreover, if we consider only reply-
types, they may not help to identify the error made.
Hence, Chouinard and Clark propose another view
of negative evidence that builds on Clark?s principle
of contrast (Clark, 1987; Clark, 1993). Parents often
check up on a child?s erroneous utterances, to make
sure they have understood them. They do this by
reformulating what they think the child intended to
express. Hence, the child?s utterance and the adult?s
reformulation have the same meaning, but different
forms. Because children attend to contrasts in form,
any change in form that does not mark a different
meaning will signal to children that they may have
produced an utterance that is not acceptable in the
target language. In this way, reformulations iden-
tify the locus of any error, and hence the existence
of an error. Chouinard and Clark analyze longitu-
dinal data from five children between two and four
years old, and show that adults reformulate erro-
neous child utterances often enough to help learn-
ing. Moreover, these results show that children not
only detect differences between their own utterance
and the adult reformulation, but that they make use
of that information.
98
In this paper we explore this new view of nega-
tive data proposed by Chouinard and Clark. Cor-
rections (in form of reformulations) have a semantic
component that has not been taken into account in
previous studies. Hence, we propose a new com-
putational model of language learning that gives an
account of meaning-preserving corrections, and in
which we can address questions such as: What are
the effects of corrections on learning syntax? Can
corrections facilitate the language learning process?
3 The Model
We describe the components of our model, and give
examples drawn from the primary domain we have
used to guide the development of the model.
3.1 Situation, meanings and utterances.
A situation is composed of some objects and some
of their properties and relations, which pick out
some aspects of the world of joint interest to the
teacher and learner. A situation is represented as a
set of ground atoms over some constants (denoting
objects) and predicates (giving properties of the ob-
jects and relations between them.) For example, a
situation s1 consisting of a big purple circle to the
left of a big red star is represented by the follow-
ing set of ground atoms: s1 = {bi1 (t1), pu1 (t1),
ci1 (t1), le2 (t1, t2), bi1 (t2), re1 (t2), st1 (t2)}.
Formally, we have a finite set P of predicate
symbols, each of a specific arity. We also have a
set of constant symbols t1, t2, . . ., which are used
to represent distinct objects. A ground atom is an
expression formed by applying a predicate symbol
to the correct number of constant symbols as argu-
ments.
We also have a set of of variables x1, x2, . . .. A
variable atom is an expression formed by applying
a predicate symbol to the correct number of vari-
ables as arguments. A meaning is a finite sequence
of variable atoms. Note that the atoms do not con-
tain constants, and the order in which they appear is
significant. A meaning is supported in a situation if
there exists a support witness, that is, a mapping of
its variables to distinct objects in the situation such
that the image under the mapping of each atom in
the meaning appears in the situation. If a meaning is
supported in a situation by a unique support witness
then it is denoting in the situation. We assume that
both the teacher and learner can determine whether
a meaning is denoting in a situation.
We also have a finite alphabet W of words. An
utterance is a finite sequence of words. The tar-
get language is the set of utterances the teacher may
produce in some situation; in our examples, this in-
cludes utterances like the star or the star to the right
of the purple circle but not star of circle small the
green. We assume each utterance in the target lan-
guage is assigned a unique meaning. An utterance
is denoting in a situation if the meaning assigned
to utterance is denoting in the situation. Intuitively,
an utterance is denoting if it uniquely picks out the
objects it refers to in a situation.
In our model the goal of the learner is to be able
to produce every denoting utterance in any given sit-
uation. Our model is probabilistic, and what we re-
quire is that the probability of learner errors be re-
duced to very low levels.
3.2 The target language and meaning
transducers.
We represent the linguistic competence of the
teacher by a finite state transducer that both recog-
nizes the utterances in the target language and trans-
lates each correct utterance to its meaning. Let A
denote the set of all variable atoms over P . We de-
fine a meaning transducer M with input symbols
W and output symbols A as follows. M has a fi-
nite set Q of states, an initial state q0 ? Q, a finite
set F ? Q of final states, a deterministic transition
function ? mappingQ?W toQ, and an output func-
tion ? mapping Q?W to A? {?}, where ? denotes
the empty sequence.
The transition function ? is extended in the usual
way to ?(q, u). The language of M , denoted L(M)
is the set of all utterances u ? W ? such that
?(q0, u) ? F . For each utterance u, we defineM(u)
to be the meaning of u, that is, the finite sequence of
non-empty outputs produced by M in processing u.
Fig. 1 shows a meaning transducer M1 for a limited
sublanguage of Spanish. M1 assigns the utterance el
triangulo rojo the meaning (tr1 (x1), re1 (x1)).
3.3 The learning task.
Initially the teacher and learner know the predicates
P and are able to determine whether a meaning is
99
 a / ? 
0 1 2 
6 
3 4 el / ? circulo / ci1(x1) cuadrado / sq1(x1) triangulo / tr1(x1) encima / ab2(x1,x2)   
a / ? la / ? 5 izquierda / le2(x1,x2) derecha / le2(x2,x1)  
rojo / re1(x1) verde / gr1(x1) azul / bl1(x1) encima / ab2(x1,x2)   7 del / ? 8 9 circulo / ci1(x2) cuadrado / sq1(x2) triangulo / tr1(x2) rojo / re1(x2) verde / gr1(x2) azul / bl1(x2) 
Figure 1: Meaning transducer M1.
denoting in a situation. The learner and teacher both
also know a shared set of categories that classify a
subset of the predicates into similarity groups. The
categories facilitate generalization by the learner
and analysis of incorrect learner utterances by the
teacher. In our geometric shape domain the cate-
gories are shape, size, and color; there is no category
for the positional relations. Initially the teacher also
has the meaning transducer for the target language,
but the learner has no language-specific knowledge.
4 The Interaction of Learner and Teacher
In one interaction of the learner and teacher, a new
situation is generated and presented to both of them.
The learner attempts to produce a denoting utter-
ance for the situation, and the teacher analyzes the
learner?s utterance and decides whether to produce
a correction of the learner?s utterance or a new de-
noting utterance of its own. Finally, the learner uses
the situation and the teacher?s utterance to update its
current grammar for the language.
In this section we describe the algorithms used by
the learner and teacher to carry out the steps of this
process.
4.1 Comprehension and the co-occurrence
graph.
To process the teacher?s utterance, the learner
records the words in the utterance and the predi-
cates in the situation in an undirected co-occurrence
graph. Each node is a word or predicate symbol and
there is an edge for each pair of nodes. Each node u
has an occurrence count, c(u), recording the number
of utterances or situations it has occurred in. Each
edge (u, v) also has an occurrence count, c(u, v),
recording the number of utterance/situation pairs in
which the endpoints of the edge have occurred to-
gether. From the co-occurrence graph the learner de-
rives a directed graph with the same nodes, the im-
plication graph, parameterized by a noise threshold
? (set at 0.95 in the experiments.) For each ordered
pair of nodes u and v, the directed edge (u, v) is in-
cluded in the implication graph if c(u, v)/c(u) ? ?.
The learner then deletes edges from predicates to
words and computes the transitively reduced im-
plication graph.
The learner uses the transitively reduced implica-
tion graph to try to find the meaning of the teacher?s
utterance by translating the words of the utterance
into a set of sequences of predicates, and determin-
ing if there is a unique denoting meaning corre-
sponding to one of the predicate sequences. If so, the
unique meaning is generalized into a general form
by replacing each predicate by its category gener-
alization. For example, if the learner detects the
unique meaning (tr1 (x1), re1 (x1)), it is general-
ized to the general form (shape1 (x1), color1 (x1)).
The learner?s set of general forms is the basis for its
production.
4.2 Production by the learner.
Each general form denotes the set of possible mean-
ings obtained by substituting appropriate symbols
from P for the category symbols. To produce a de-
noting utterance for a situation, the learner finds all
the meanings generated by its general forms using
predicates from the situation and tests each meaning
to see if it is denoting, producing a set of possible de-
noting meanings. If the set is empty, the learner pro-
duces no utterance. Otherwise, it attempts to trans-
late each denoting meaning into an utterance.
The learner selects one of these utterances with
a probability depending on a number stored with
the corresponding general form recording the last
time a teacher utterance matched it. This ensures
that repeatedly matched general forms are selected
with asymptotically uniform probability, while gen-
eral forms that are only matched a few times are se-
lected with probability tending to zero.
100
4.3 From meaning to utterance.
The process the learner uses to produce an utter-
ance from a denoting meaning is as follows. For
a meaning that is a sequence of k atoms, there are
two related sequences of positions: the atom posi-
tions 1, 2, . . . , k and the gap positions 0, 1, . . . , k.
The atom positions refer to the corresponding atoms,
and gap position i refers to the position to the right
of atom i, (where gap position 0 is to the left of atom
a1.) The learner generates a sequence of zero or
more words for each position in left to right order:
gap position 0, atom position 1, gap position 1, atom
position 2, and so on, until gap position k. The re-
sulting sequences of words are concatenated to form
the final utterance.
The choice of what sequence of words to pro-
duce for each position is represented by a decision
tree. For each variable atom the learner has en-
countered, there is a decision tree that determines
what sequence of words to produce for that atom
in the context of the whole meaning. For exam-
ple, in a sublanguage of Spanish in which there are
both masculine and feminine nouns for shapes, the
atom re1 (x1) has a decision tree that branches on
the value of the shape predicate applied to x1 to se-
lect either rojo or roja as appropriate. For the gap
positions, there are decision trees indexed by the
generalizations of all the variable atoms that have
occurred; the variable atom at position i is general-
ized, and the corresponding decision tree is used to
generate a sequence of words for gap position i. Gap
position 0 does not follow any atom position and has
a separate decision tree.
If there is no decision tree associated with a given
atom or gap position in a meaning, the learner falls
back on a ?telegraphic speech? strategy. For a gap
position with no decision tree, no words are pro-
duced. For an atom position whose atom has no as-
sociated decision tree, the learner searches the tran-
sitively reduced implication graph for words that
approximately imply the predicate of the atom and
chooses one of maximum observed frequency.
4.4 The teacher?s response.
If the learner produces an utterance, the teacher an-
alyzes it and then chooses its own utterance for the
situation. The teacher may find the learner?s utter-
ance correct, incorrect but correctable, or incorrect
and uncorrectable. If the learner?s utterance is in-
correct but correctable, the teacher chooses a pos-
sible correction for it. The teacher randomly de-
cides whether or not to use the correction as its ut-
terance according to the correction probability. If
the teacher does not use the correction, then its own
utterance is chosen uniformly at random from the
denoting utterances for the situation.
If the learner?s utterance is one of the correct de-
noting utterances for the situation, the teacher clas-
sifies it as correct. If the learner?s utterance is
not correct, the teacher ?translates? the learner?s ut-
terance into a sequence of predicates by using the
meaning transducer for the language. If the result-
ing sequence of predicates corresponds to a denot-
ing meaning, the learner?s utterance is classified as
having an error in form. The correction is cho-
sen by considering the denoting utterances with the
same sequence of predicates as the learner?s utter-
ance, and choosing one that is ?most similar? to the
learner?s utterance. For example, if the learner?s ut-
terance was el elipse pequeno and (el1 , sm1 ) cor-
responds to a denoting utterance for the situation,
the teacher chooses la elipse pequena as the cor-
rection. If the learner?s utterance is neither correct
nor an error in form, the teacher uses a measure of
similarity between the learner?s sequence of predi-
cates and those of denoting utterances to determine
whether there is a ?close enough? match. If so, the
teacher classifies the learner?s utterance as having
an error in meaning and chooses as the possible
correction a denoting utterance whose predicate se-
quence is ?most similar? to the learner?s predicate
sequence. If the learner produces an utterance and
none of these cases apply, then the teacher classifies
the learner?s utterance as uninterpretable and does
not offer a correction.
When the teacher has produced an utterance, the
learner analyzes it and updates its grammar of the
language as reflected in the co-occurrence graph,
the general forms, and the decision trees for word
choice. The decision trees are updated by comput-
ing an alignment between the teacher?s utterance and
the learner?s understanding of the teacher?s meaning,
which assigns a subsequence of words from the ut-
terance to each atom or gap position in the meaning.
Each subsequence of words is then added to the data
101
for the decision tree corresponding to the position of
that subsequence.
If the learner has produced an utterance and finds
that the teacher?s utterance has the same meaning,
but is expressed differently, then the learner classi-
fies the teacher?s utterance as a correction. In the
current model, the learner reports this classification,
but does not use it in any way.
5 Empirical Results
We have implemented and tested our learning and
teaching procedures in order to explore questions
about the roles of corrections in language learning.
We have used a simplified version of the Miniature
Language Acquisition task proposed by Feldman et
al. (Feldman et al, 1990). Although this task is not
as complex as those faced by children, it involves
enough complexity to be compared to many real-
word tasks.
The questions that we address in this section are
the following. (1) Can the learner accomplish the
learning task to a high level of correctness and cov-
erage from a ?reasonable? number of interactions
(that is, well short of the number needed to memo-
rize every legal situation/sentence pair)? (2) What
are the effects of correction or non-correction by
the teacher on the learner?s accomplishment of the
learning tasks?
5.1 The specific learning tasks.
Each situation has two objects, each with three at-
tributes (shape, color and size), and one binary rela-
tion between the two objects (above or to the left of.)
The attribute of shape has six possible values (cir-
cle, square, triangle, star, ellipse, and hexagon), that
of color has six possible values (red, orange, yellow,
green, blue, and purple), and that of size three possi-
ble values (big, medium, and small.) There are 108
distinct objects and 23,328 distinct situations. Situ-
ations are generated uniformly at random.
For several natural languages we construct a lim-
ited sublanguage of utterances related to these situ-
ations. A typical utterance in English is the medium
purple star below the small hexagon. There are 168
meanings referring to a single object and 112,896
meanings referring to two objects, for a total of
113,064 possible meanings. The 113,064 possible
meanings are instances of 68 general forms: 4 refer-
ring to a single object and 64 referring to two ob-
jects. These languages are the 68-form languages.
We consulted at least one speaker of each lan-
guage to help us construct a meaning transducer to
translate appropriate phrases in the language to all
113,064 possible meanings. Each transducer was
constructed to have exactly one accepted phrase for
each possible meaning. We also constructed trans-
ducers for reduced sublanguages, consisting of the
subset of utterances that refer to a single object (168
utterances) and those that refer to two objects, but in-
clude all three attributes of both (46,656 utterances.)
Each meaning in the reduced sublanguage is an in-
stance of one of 8 general forms, while most of the
lexical and syntactic complexity of the 68-form lan-
guage is preserved. We refer to these reduced sub-
languages as the 8-form languages.
5.2 How many interactions are needed to
learn?
The level of performance of a learner is measured
using two quantities: the correctness and complete-
ness of the learner?s utterances in a given situation.
The learning procedure has a test mode in which the
learner receives a situation and responds with the
set of U utterances it could produce in that situa-
tion, with their corresponding production probabili-
ties. The correctness of the learner is the sum of the
production probabilities of the elements of U that
are in the correct denoting set. The completeness
of the learner is the fraction of all correct denoting
utterances that are in U . The averages of correct-
ness and completeness of the learner for 200 ran-
domly generated situations are used to estimate the
overall correctness and completeness of the learner.
A learner reaches a level p of performance if both
correctness and completeness are at least p.
In the first set of trials the target level of per-
formance is 0.99 and the learner and teacher en-
gage in a sequence of interactions until the learner
first reaches this level of performance. The perfor-
mance of the learner is tested at intervals of 100 in-
teractions. Fig. 2 shows the number of interactions
needed to reach the 0.99 level of performance for
each 68-form language with correction probabilities
of 0.0 (i.e., the teacher never corrects the learner)
and 1.0 (i.e., the teacher offers a correction to the
102
learner every time it classifies the learner?s utterance
as an error in form or an error in meaning.) For
correction probability 1.0, it also shows the number
of incorrect utterances by the learner, the number of
corrections offered by the teacher, and the percent-
age of teacher utterances that were corrections. Each
entry is the median value of 10 trials except those in
the last column. It is worth noting that the learner
does not treat corrections specially.
0.0 1.0 incorrect corrections c/u%
English 700 750 25.0 11.5 1.5%
German 800 750 71.5 52.5 7.0%
Greek 3400 2600 344.0 319.0 12.3%
Hebrew 900 900 89.5 62.5 6.9%
Hungarian 750 800 76.5 58.5 7.3%
Mandarin 700 800 50.0 31.5 3.9%
Russian 3700 2900 380.0 357.0 12.3%
Spanish 1000 850 86.0 68.0 8.0%
Swedish 1000 900 54.0 43.5 4.8%
Turkish 800 900 59.0 37.0 4.1%
Figure 2: Interactions, incorrect learner utterances and
corrections by the teacher to reach the 0.99 level of per-
formance for 68-form languages.
In the column for correction probability 0.0 there
are two clear groups: Greek and Russian, each with
at least 3400 interactions and the rest of the lan-
guages, each with at most 1000 interactions. The
first observation is that the learner achieves correct-
ness and completeness of 0.99 for each of these lan-
guages after being exposed to a small fraction of all
possible situations and utterances. Even 3700 inter-
actions involve at most 16.5% of all possible situa-
tions and at most 3.5% of all possible utterances by
the teacher, while 1000 interactions involve fewer
than 4.3% of all situations and fewer than 1% of all
possible utterances.
5.3 How do corrections affect learning?
In the column for correction probability 1.0 we see
the same two groups of languages. For Greek, the
number of interactions falls from 3400 to 2600, a
decrease of about 24%. For Russian, the number of
interactions falls from 3700 to 2900, a decrease of
about 21%. Corrections have a clear positive effect
in these trials for Greek and Russian, but not for the
rest of the languages.
Comparing the numbers of incorrect learner utter-
ances and the number of corrections offered by the
teacher, we see that the teacher finds corrections for
a substantial fraction of incorrect learner utterances.
The last column of Fig. 2 shows the percentage of
the total number of teacher utterances that were cor-
rections, from a low of 1.5% to a high of 12.3%.
There are several processes at work in the im-
provement of the learner?s performance. Compre-
hension improves as more information accumulates
about words and predicates. New correct general
forms are acquired, and unmatched incorrect gen-
eral forms decrease in probability. More data im-
proves the decision tree rules for choosing phrases.
Attainment of the 0.99 level of performance may be
limited by the need to acquire all the correct general
forms or by the need to improve the correctness of
the phrase choices.
In the case of Greek and Russian, most of the tri-
als had acquired their last general form by the time
the 0.90 level of performance was reached, but for
the other languages correct general forms were still
being acquired between the 0.95 and the 0.99 lev-
els of performance. Thus the acquisition of gen-
eral forms was not a bottleneck for Greek and Rus-
sian, but was for the other languages. Because the
teacher?s corrections generally do not help with the
acquisition of new general forms (the general form
in a correction is often the same one the learner
just used), but do tend to improve the correctness
of phrase choice, we do not expect correction to re-
duce the number of interactions to attain the 0.99
level of performance when the bottleneck is the ac-
quisition of general forms. This observation led us
to construct reduced sublanguages with just 8 gen-
eral forms to see if correction would have more of
an effect when the bottleneck of acquiring general
forms was removed.
The reduced sublanguages have just 8 general
forms, which are acquired relatively early. Fig. 3
gives the numbers of interactions to reach the 0.99
level of performance (except for Turkish, where the
level is 0.95) for the 8-form sublanguages with cor-
rection probability 0.0 and 1.0. These numbers are
the means of 100 trials (except for Greek and Rus-
sian, which each had 20 trials); the performance of
the learner was tested every 50 interactions.
Comparing the results for 8-form sublanguages
with corresponding 68-form languages, we see that
some require notably fewer interactions for 8-form
103
0.0 1.0 % reduction
English 247.0 202.0 18.2 %
German 920.0 683.5 25.7 %
Greek 6630.0 4102.5 38.1 %
Hebrew 1052.0 771.5 26.7 %
Hungarian 1632.5 1060.5 35.0 %
Mandarin 340.5 297.5 12.6 %
Russian 6962.5 4640.0 33.4 %
Spanish 908.0 630.5 30.6 %
Swedish 214.0 189.0 11.7 %
Turkish 1112.0* 772.0* 30.6 %
Figure 3: Interactions to reach the 0.99 level of perfor-
mance for 8-form languages. (For Turkish: the 0.95
level.)
sublanguages (English, Mandarin, and Swedish)
while others require notably more (Greek, Hungar-
ian and Russian.) In the case of Turkish, the learner
cannot attain the 0.99 level of performance for the
8-form sublanguage at all, though it does so for the
68-form language; this is caused by limitations in
learner comprehension as well as the differing fre-
quencies of forms. Thus, the 8-form languages are
neither uniformly easier nor uniformly harder than
their 68-form counterparts. Arguably, the restric-
tions that produce the 8-form languages make them
?more artificial? than the 68-form languages; how-
ever, the artificiality helps us understand more about
the possible roles of correction in language learning.
Even though in the case of the 8-form languages
there are only 8 correct general forms to acquire, the
distribution on utterances with one object versus ut-
terances with two objects is quite different from the
case of the 68-form languages. For a situation with
two objects of different shapes, there are 40 denot-
ing utterances in the case of 68-form languages, of
which 8 refer to one object and 32 refer to two ob-
jects. In the case of the 8-form languagues, there are
10 denoting utterances, of which 8 refer to one ob-
ject and 2 refer to two objects. Thus, in situations
of this kind (which are 5/6 of the total), utterances
referring to two objects are 4 times more likely in
the case of 68-form languages than in the case of 8-
form languages. This means that if the learner needs
to see utterances involving two objects in order to
master certain aspects of syntax (for example, cases
of articles, adjectives and nouns), the waiting time is
noticeably longer in the case of 8-form languages.
This longer waiting time emphasizes the effects
of correction, because the initial phase of learning
is a smaller fraction of the whole. In the third col-
umn of Fig. 3 we show the percentage reduction in
the number of interactions to reach the 0.99 level
of performance (except: 0.95 for Turkish) from cor-
rection probability 0.0 to correction probability 1.0
for the 8-form languages. For each language, cor-
rections produce a reduction, ranging from a low of
11.7% for Swedish to a high of 38.1% for Greek.
This confirms our hypothesis that corrections can
substantially help the learner when the problem of
acquiring all the general forms is not the bottleneck.
6 Discussion and Future Work
We show that a simple model of a teacher can offer
meaning-preserving corrections to the learner and
such corrections can significantly reduce the num-
ber of interactions for the learner to reach a high
level of performance. This improvement does not
depend on the learner?s ability to detect corrections:
the effect depends on the change in the distribution
of teacher utterances in the correcting versus non-
correcting conditions. This suggests re-visiting dis-
cussions in linguistics that assume that the learner
must identify teacher corrections in order for them
to have an influence on the learning process.
Our model of language is very simplified, and
would have to be modified to deal with issues such
as multi-word phrases bearing meaning, morpho-
logical relations between words, phonological rules
for word choice, words with more than one mean-
ing and meanings that can be expressed in more
than one way, languages with freer word-orders and
meaning components expressed by non-contiguous
sequences of words. Other desirable directions
to explore include more sophisticated use of co-
occurrence information, more powerful methods of
learning the grammars of meanings, feedback to al-
low the learning of production to improve compre-
hension, better methods of alignment between utter-
ances and meanings, methods to allow the learner?s
semantic categories to evolve in response to lan-
guage learning, and methods allowing the learner to
make use of its ability to detect corrections.
104
References
D. Angluin and L. Becerra-Bonache. 2010. A Model
of Semantics and Corrections in Language Learning.
Technical Report, Yale University Department of
Computer Science, YALE/DCS/TR-1425.
R. Brown and C. Hanlon. 1970. Derivational complexity
and the order of acquisition in child speech. In J.R.
Hayes (ed.): Cognition and the Development of Lan-
guage. Wiley, New York, NY.
M.M. Chouinard and E.V. Clark. 2003. Adult Reformu-
lations of Child Errors as Negative Evidence. Journal
of Child Language, 30:637?669.
E.V. Clark 1987. The principle of contrast: a constraint
on language acquisition. In B. MacWhinney (ed.):
Mechanisms of language acquisition. Erlbaum, Hills-
dale, NJ.
E.V. Clark 1993. The Lexicon in Acquisition. Cambridge
University Press, Cambridge, UK.
M. J. Demetras, K. N. Post and C.E. Snow. 1986. Brown
and Hanlon revisited: mothers? sensitivity to ungram-
matical forms. Journal of Child Language, 2:81?88.
J.A. Feldman, G. Lakoff, A. Stolcke and S. Weber 1990.
Miniature Language Acquisition: A Touchstone for
Cognitive Science. Annual Conference of the Cogni-
tive Science Society, 686?693.
E.M. Gold. 1967. Language identification in the limit.
Information and Control, 10:447?474.
K. Hirsh-Pasek, R.A. Treiman M. and Schneiderman.
1984. Brown and Hanlon revisited: mothers? sensi-
tivity to ungrammatical forms. Journal of Child Lan-
guage, 2:81?88.
G.F. Marcus 1993. Negative evidence in language acqui-
sition. Cognition, 46:53?95.
J.L. Morgan and L.L. Travis. 1989. Limits on negative
information in language input. Journal of Child Lan-
guage, 16:531?552.
105
