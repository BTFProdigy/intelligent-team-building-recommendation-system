Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 65?73,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Modular resource development and diagnostic evaluation framework for
fast NLP system improvement
Gae?l de Chalendar, Damien Nouvel
CEA, LIST, Multilingual Multimedia Knowledge Engineering Laboratory,
F-92265 Fontenay-aux-Roses, France.
{Gael.de-Chalendar,Damien.Nouvel}@cea.fr
Abstract
Natural Language Processing systems are
large-scale softwares, whose development in-
volves many man-years of work, in terms of
both coding and resource development. Given
a dictionary of 110k lemmas, a few hundred
syntactic analysis rules, 20k ngrams matrices
and other resources, what will be the impact
on a syntactic analyzer of adding a new pos-
sible category to a given verb? What will be
the consequences of a new syntactic rules ad-
dition? Any modification may imply, besides
what was expected, unforeseeable side-effects
and the complexity of the system makes it dif-
ficult to guess the overall impact of even small
changes. We present here a framework de-
signed to effectively and iteratively improve
the accuracy of our linguistic analyzer LIMA
by iterative refinements of its linguistic re-
sources. These improvements are continu-
ously assessed by evaluating the analyzer per-
formance against a reference corpus. Our first
results show that this framework is really help-
ful towards this goal.
1 Introduction
1.1 The evaluation framework
In Natural Language Processing (NLP), robustness
and reliability of linguistic analyzers becomes an
everyday more addressed issue, given the increas-
ing size of resources and the amount of code im-
plied by the implementation of such systems. Be-
yond choosing a sound technology, one must now
have efficients and user-friendly tools around the
system itself, for evaluating its accuracy. As shown
by (Chatzichrisafis et al, 2008), where developers
receive daily reports of system?s performance for
improving their system, systematic evaluation with
regression testing has shown to be gainful to accel-
erate grammar engineering.
Evaluation campaigns, where several participants
evaluate their system?s performance on a specific
task against other systems, are a good mean to
search for directions in which a system may be able
to improve its performance. Often, these evaluation
campaigns also give possibility for participants to
run their analyzer on test data and retrieve evalua-
tion results. In this context, parsers authors may rely
on evaluation campaigns to provide performance re-
sults, but they should also be able to continuously
evaluate and improve their analyzers between evalu-
ation campaigns. We aim at providing such a generic
evaluation tool, using evaluation data to assess sys-
tems accuracy, this software will be referenced as
the ?Benchmarking Tool?.
Approaches concerning Natural Language Pro-
cessing involve everyday more and more resource
data for analyzing texts. These resources have
grown enough (in terms of volume and diversity),
that it now becomes a challenge to manipulate them,
even for experienced users. Moreover, it is needed
to have non-developers being able to work on these
resources: it is necessary to develop accessible tools
through intuitive graphical user interfaces. Such a
resource editing GUI tool represent the second part
of our contribution, called the ?Resource Tool?.
The overall picture is to build a diagnostic frame-
work enabling a language specialist, such as a lin-
guist, to status, almost in real-time, how modifica-
65
tions impact our analyzer on as much test data as
possible. For analyzers, each resource may have an
effect on the final accuracy of the analysis. It is of-
ten needed to iterate over tests before understanding
what resource, what part of the code needs to be im-
proved. This is especially the case with grammar
engineering, where it is difficult to predict the con-
sequences of modifying a single rule. Ideally, our
framework would allow the manipulator to slightly
alter a resource, trigger an evaluation and, almost in-
stantaneously, view results and interpret them. With
this framework, we expect a large acceleration in the
process of improving our analyzer.
In the remaining of this introduction, we will
describe our analyzer and Passage, a collabora-
tive project including an evaluation campaign and
the production of a reference treebank for French
through a voting procedure. Section 2 will describe
our evaluation framework; its architecture, its two
main modules and our first results using it. Section
3 describes some related works. We conclude in sec-
tion 4 by describing the next steps of our work.
1.2 The LIMA linguistic analyzer
Our linguistic analyzer LIMA (LIc2m Multilingual
Analyzer, (Besancon and de Chalendar, 2005)), is
implemented as a pipeline of independent modules
applied successively on a text. It implements a de-
pendency grammar (Kahane, 2000) in the sense that
produced analysis are exclusively represented as bi-
nary dependency relations between tokens.
The analyzer includes, among other modules, a
tokenizer segmenting the text based on punctuation
marks, a part of speech tagger, short and long dis-
tance dependencies extractors based on finite-state
automata defined by contextualized rules. The latter
rules express successions of categories, augmented
with constraints (on words inflexion, existence of
other dependencies, etc.). The analyzer also in-
cludes modules to find idiomatic expressions and
named entities that, once recognized, are merged
into a single token, thus allowing grammar rules to
apply on those. Furthermore, modules may be spe-
cialized in processing language-specific phenomena,
e.g. Chinese tokenization, German compounds, etc.
Currently, the analyzer is able to process more or
less deeply ten languages, including English, Span-
ish, Chinese, Arab, French and German.
1.3 The Passage Project
Our work is part of the Passage project (Clergerie
et al, 2008b). The objectives of this project are
twofold. Firstly, it organizes two evaluation cam-
paigns of syntactic analyzers (around 15 participat-
ing systems) for the French language. Secondly, it
aims at producing a large scale reference treebank
for French by merging the output of all the partic-
ipating parsers, using a Rover (Recognizer Output
Voting Error Reduction) (Fiscus, 1997) approach.
Within this project, syntactic annotations are pro-
duced in a common format, rich enough to represent
all necessary linguistic features and simple enough
to allow participating parsers (using very different
parsing approaches) to represent their analysis in
this format. It is an evolution of the EASy cam-
paign format, mixing simple non recursive chunks
and dependency relations between chunks or tokens.
It respects two proposed ISO specifications: MAF
(ISO 24611) and SynAF (ISO 24615). The chunks
and dependencies types are issued from the ISO data
category registry, DCR1, currently using the French
language section names. The syntactic analysis of
a corpus in the Passage format provides information
about:
? Segmentation of the corpus into sentences
? Segmentation of sentences into forms
? Non-recursive typed (listed in Table 1) chunks
embedding forms
? Labeled typed (listed in Table 2) dependencies
that are anchored by either forms or chunks
Type Explanation
GN Nominal Chunk
NV Verbal Kernel
GA Adjectival Chunk
GR Adverbial Chunk
GP Prepositional Chunk
PV Prepositional non-tensed Verbal Kernel
Table 1: Chunks types
Within the EASy project, parsers have been eval-
uated against a reference, which itself was a small
subset of the available corpora. The reference was
1http://www.isocat.org
66
Type Explanation
SUJ-V Subject-verb
AUX-V Aux-verb
COD-V Direct objects
CPL-V Other verb arguments/complements
MOD-V Verb modifiers (e.g. adverbs)
COMP Subordinate sentences
ATB-SO Verb attribute
MOD-N Noun modifier
MOD-A Adjective modifier
MOD-R Adverb modifier
MOD-P Preposition modifier
COORD Coordination
APPOS Apposition
JUXT Juxtaposition
Table 2: Dependencies types
created by human annotation of random sentences
within the corpora. Thus, once this evaluation cam-
paign had been finished, the annotated corpora ref-
erence was released for participants to test and im-
prove their parser. Currently, we use this reference
for benchmarking our analyzer.
1.4 Metrics for parsing evaluation
We are constantly recalled that evaluation metrics
and methodologies evolve and are subject to intense
research and innovation (Carroll et al, 2002). Dis-
cussing these metrics is not in the scope of this pa-
per, we only need to be able to work out as many
metrics as possible on the entire corpus or on any
part of it. The evaluation is supposed, for each doc-
ument d and for each type (of chunk or of depen-
dency) t within all types set T , to return following
counts:
? Number of items found and correct - fc(d, t)
? Number of items found - f(d, t)
? Number of items correct - c(d, t)
With this approach, we are able to compute com-
mon Information Retrieval (IR) metrics (Rijsbergen,
1979): precision, recall, f-measure. We also intro-
duce a new metric that gives us indications about
what types are the most lowering overall perfor-
mance, called ?Type error ratio?:
f(d, t) + c(d, t)? 2.fc(d, t)? t?T f(d, t) + c(d, t)? 2.fc(d, t) (1)
This metric counts the number of errors and
misses for a given type reported to the total number
of errors and misses. It allows us to quantify how
much an improvement on a given type will improve
the overall score. In our case, scores are computed
for chunks on the one hand, and for dependencies
on the other hand. For instance, we have notices
that GN errors represent 34.6% of the chunks errors,
whereas PV only represent 2.2%: we are thus much
more interested in improving detection of GN than
PV regarding current evaluation campaign.
2 The evaluation framework
2.1 Architecture
We need our framework to be portable and to be im-
plemented using an agile approach: each new ver-
sion should be fully functional while adding some
more features. It also must be user-friendly, allow-
ing to easily add eye-candy features. Consequently,
we have chosen to implement these tools in C++,
using the Qt 4.5 library2. This library satisfies our
requirements and will allow to rely on stable and
open source (LGPL) tools, making it feasible for us
to possibly deliver our framework as a free software.
This approach allows us to quickly deliver work-
ing software while continuously testing and devel-
oping it. Iterations of this process are still occurring
but the current version, with its core functions, al-
ready succeeded in running benchmarks and in be-
ginning the improvement of our linguistic resources
while regularly delivering upgraded versions of our
framework. First results of this work will be pre-
sented below in this paper.
The open architecture we have chosen implies to
use externals tools, for analysis and evaluation on
the one hand, for compiling and installing resources
on the other hand. These tools may then be con-
sidered as black boxes, being externals commands
called with convenient parameters. In particular, the
Benchmarking Tool relies on two commands: the
analyzer command, receiving input file as a param-
eter and producing the analyzed file, the evaluation
command, receiving the analyzed file and the ref-
erence file as parameters and outputting counts of
found, correct, found and correct items for each di-
mension. This allows, for example, to replace our
2http://www.qtsoftware.com/
67
analyzer with another one, by just wrapping the lat-
ter in a thin conversion layer to convert its inputs and
its outputs.
2.2 Benchmarking Tool
The Benchmarking Tool, which architecture is de-
picted in Figure 1, is responsible of executing anal-
ysis and evaluation on pairs of data and reference
files, using commands stored in benchmarking con-
figuration. For each pair of files, the registered anal-
ysis command is executed followed by the evalua-
tion one. In our case, those commands apply to the
task of annotating files for syntactic chunks and de-
pendencies.
Figure 1: Benchmarking Tool data flow
We may consider the type of chunks and depen-
dencies as dimensions of an evaluation. To a certain
extent, these may be associated to linguistics phe-
nomena which are tested, as proposed within the
TSNLP project (Balkan et al, 1994) or, more re-
cently, for Q/A systems by (Paiva et al, 2008). But
in these projects, focus is also made on the evalua-
tion tool, where we do not implement the evaluation
tool but rely on an external program to provide ac-
curacy of analysis.
The pairs of data and reference files are inserted
inside a structure implemented as a pipeline, which
may be modified (adding, removing, reordering
units) with common GUI interfaces. After creation
of the pipeline, the user may trigger a benchmark-
ing (progress is shown by coloring pipeline units),
which may be suspended, resumed or restarted at
any moment. For note, the current version of the
framework uses the local machine?s processors to
analyze pipeline units in parallel, but we intend to
distribute the analyzes on the available nodes of a
cluster soon. As soon as results are received, tables
and graphics are updated on screen within a view
showing previous and current results for each eval-
uated dimension. To refine diagnosis, the user may
choose what dimensions are displayed, what met-
rics should be computed, and what pipeline units are
used. Finally, any evaluation may be deleted if the
corresponding modification did not increase perfor-
mance and should be reverted.
Upon demand, the tool saves current benchmark-
ing configuration and results as an XML file. Con-
versely, it loads a pipeline and results from file, so
as to resume or switch between evaluations. The
parsed output of the evaluator tool is recorded for
each pipeline unit and for each dimension, so that
metrics based on those quantities are computed for
each pipeline unity or for the overall corpus. Be-
sides, the date and a user comment for each evalua-
tion are also saved for these records. Writing com-
ments has proved to be very helpful to keep track
of what changes have been made on code, linguistic
resources, configuration, parameters, etc.
As an example within the Passage project, run-
ning evaluation with the Benchmarking Tool al-
lowed us to notice that we had difficulties in rec-
ognizing verb-auxiliary dependencies. Considering
previous results, we detected that this issue appeared
after having introduced a set of idioms concerning
pronominal verbs. Unit testing showed that the anal-
ysis of past perfect progressive for pronominal verbs
was buggy. Patching the code gave us a 10 points f-
measure gain for AUX-V dimension and 0.3 for all
dependencies dimensions (AUX-V having a 2.6%
global error rate within dependencies). Thus, bench-
marking results have been saved with appropriate
comment and other improvements or deficiencies
could be examined.
With these features, the tool offers the possibility
to have an overall view on evaluation results and on
their evolution across time, given multiple data, di-
mensions of analysis and computed metrics. There-
fore, it helps us, without any complex manipulation,
to get a visual report on what implication on evalu-
ation results has a modification to the analysis pro-
cess. Furthermore, those tests allow to search for
errors in resources as well as in code, so as to find
how to enrich our linguistic resources or to identify
deficiencies in our code.
Figure 2 shows a benchmarking using a set of 24
evaluation files (left part) to improve the analyzer?s
68
Figure 2: Chunks (CONSTS), dependencies (RELS), nominal chunks (GN) and direct objects dependencies (COD V)
f-measure results evolution through 4 evaluations on a 24 files corpus
results. The central table shows the measures corre-
sponding to 4 successive evaluations, displaying re-
sults for the dimensions selected on the top most part
(check-boxes). The right-hand side shows graph-
ically the same data, successive evaluations being
displayed as its abscissa and measures as its ordi-
nate.
2.3 Resource Tool
The Resource Tool, which modular design is de-
picted in Figure 3, aims at making resources edit-
ing accessible for people who have neither a deep
knowledge of the system internals nor computer pro-
gramming skills. Enriching our resources implies
having people, either specialized in linguistics or in
testing to interact with the resources, even if not ac-
customed to our specific storage format for each re-
source.
In its current version, the Resource Tool allow to
edit the following resources:
? Dictionary: items and their categories
? Syntactic rules: syntactic dependency detection
Figure 3: Resource Tool modular design
? Part-of-speech tagger learning corpus: tagged
examples of ngrams disambiguation matrices
? Idioms: language dependent fixed expressions
Those resources are presented in a tabbed view,
each having a dedicated interface and access func-
tions. Within each resource, a search feature is im-
plemented, which has shown to be really useful, es-
pecially for dictionary. The tool also provides sim-
ple means to save, compile and install resources,
once they have been modified. This has to be very
transparent for the user and we just provide a ?Save?
button and another ?Compile and install? button.
The current version of Resource Tool is quite ba-
69
Figure 4: Viewing and editing disambiguation matrices: probabilities and examples for articles followed by nouns
sic in terms of edition capacities. Dictionary has a
dedicated interface for editing words and their cat-
egories, but ngrams, syntactic rules and idioms re-
sources may yet only be changed through a basic
text editor.
Figure 4 shows the resource tool interface for the
annotated corpus that allows to build part-of-speech
disambiguation matrices. The top most tabs allow
to switch between resources among editable ones.
The data table shows the computed 3-grams (from
our own tag set). The left part text field shows a
list of sentences, where occurrences of the ngrams
selected in the above table appear. The right part
text field shows correspondences between two tag
sets. Eventually, the ?Edit corpus file? button opens
an editor for the user to add sentences or to modify
sentences in the tagged corpus.
The Resource Tool and the Benchmarking Tool
communicate together through two signals: on the
one hand when resources are installed, the Resource
Tool may trigger an evaluation in the Benchmarking
Tool, on the other hand when the evaluation has fin-
ished, the Resource Tool is notified and warns the
user. Being aware of their respective status, we also
warn the user for dangerous operations, like when
trying to install resources while a benchmarking is
still running, or when quitting the application before
last benchmark is finished.
While these two applications are connected to be
aware of benchmarking and resource installation sta-
tus, no more interaction has been implemented for
the moment to link evaluation and resource edition
together. We have considered implementing a fea-
ture making possible to automatically do unit testing
resource modifications, but, from our point of view,
this has to be implemented with following restric-
tions: the Benchmarking Tool should remain generic
(modifying configuration and resources should not
be part of the tool) ; amount of required disk space
should remain minimal (only differences between
evaluations should be stored).
2.4 Preliminary results
We recently finished the first implementation itera-
tion. The evaluator itself is provided by a partner
laboratory. Its measurement methodology is deeply
presented in (Paroubekr, 2006). From our point of
view, we are only concerned in the fact that these
70
Chunks Dependencies Modifications
F P R F P R
72.6 72.0 73.2 45.9 54.2 39.8 Initial evaluation
76.3 76.2 76.3 47.5 56.1 41.1 Code reengineering / debugging
76.7 76.7 76.7 47.6 56.2 41.3 New set of syntactic rules
76.9 76.9 76.9 47.8 56.7 41.4 Specified preposition detection rules
Table 3: Benchmarking results, f-measure (F), precision (P), recall (R)
measures are relevant for improving the quality of
analysis produced by our parser.
We applied our resource improvement methodol-
ogy on a small annotated corpus of approximately
80.000 words, delivered after the EASy campaign,
among 27 thematic files. For information, the whole
process (analysis and evaluation for each file) is 5
minutes long on a bi-processor: this allows the soft-
ware to be used intensively on a personal computer.
Results in Table 3 show that the use of our frame-
work already allowed us to introduce modifications
of the linguistic resources with the Resource Tool;
these changes lead to a slight improvement of the
overall score of the system.
First, we obtained confirmation that some code
reengineering and some debugging was required.
These tasks, associated with iterative evaluation,
have allowed us to detect parts of the code which
did not give entire satisfaction, especially in the step
transforming output from our analyzer to the ex-
pected Passage format. We also found a bug within
the evaluation scripts, which, once corrected, forced
to restart evaluation measures from the beginning:
this shows the importance of having a stable en-
vironment apart analyzer (evaluation process, valid
data and reference file). These results show that iter-
ating over time and saving history may help to reveal
potential weaknesses of the code and to detect what
goes wrong.
Secondly, these tools where well-suited for eval-
uating the impact of a new set of syntactic rules,
for which we did not have opportunities to do pre-
cise evaluation before. For this set of 20 rules,
we systematically tried each rule separately, then
kept the combination of the rules increasing scores.
This improvement may appear as minimal, but these
rules where written in the context of an ongoing
work on our grammar. It gave an intuitive idea that
this approach is not a dead-end and may be further
explored. Besides, methodologies have been sug-
gested to test the impact of each rule in the entire
set of rules by systematically testing combinations
of rules. But, currently, this is beyond our goal.
Finally, we also introduced some ?syntactic
sugar?, by grouping some expressions within rules,
and successfully obtained insurance that these mod-
ification did not lower scores. This is an important
result for us in the sense that we ensure that the same
set of rules expressed differently (with rules more
concise thus more readable) do not introduce regres-
sions.
3 Related works
We have previously described the test suite ap-
proach, along with the TSNLP project. This ap-
proach was concerned with identifying and system-
atically testing linguistic phenomena. As a conclu-
sion of TSNLP, (Oepen et al, 1998) points out the
necessity ?to assess the impact of individual contri-
butions, regularly evaluate the quality of the overall
grammar, and compare it to previous versions?. This
project thus showed how positive it is to identify de-
ficiencies and improve grammars by iterating tests
over time. This is the goal we intend to reach with
our framework.
More recently, in biomedical domain, (Baum-
gartner et al, 2008) describes implementation of
a framework and, although it is applied to a text
mining task, the approach remains quite close in
its foundations (evaluation oriented, iterative testing,
modular framework, open source, corpora based,
etc.) to ours and encourages these kind of initiative
by showing the importance of continuous evaluation
while coding parser and engineering grammar. This
work present the interest to rely on the UIMA frame-
work, thus allowing a good modularity. In the future,
we should study the interest to give the ability to our
framework to integrate UIMA-ready modules.
71
Close to our Benchmarking Tool, some projects
aim at building frameworks for text analysis, an-
notation and evaluation, which projects encourage
people to use a common architecture, as openNLP
or GATE. Those may also be used for benchmark-
ing and evaluation tasks (Cunningham et al, 2002)
as part of their process. But, while these frame-
work often provide evaluation and regression test-
ing tools, they are rarely well-suited for only imple-
menting specific diagnostic tasks. We would appre-
ciate that such frameworks focusing on evaluating,
benchmarking and diagnosing, as generic as possi-
ble across IR tasks, become more widely available.
If our Benchmarking Tool appears to be appropri-
ate for other systems evaluations, we will consider
making it available for the IR community.
4 Conclusions and future work
From our first use of the framework, we are con-
vinced of the importance of diagnostic for acceler-
ating the improvement of our analyzer, by making
linguistic resources accessible and by iterating tests
and comparing results obtained over time. We also
concluded that this generic framework would be use-
ful in other tasks, such as Information Retrieval. Es-
pecially, image retrieval is a very active and growing
field of research, and we currently consider apply-
ing the Benchmarking Tool for accelerating the im-
provement of the image retrieval system developed
in our laboratory (Joint et al, 2004).
This work also emphasizes the great distinc-
tion between performance evaluation and diagnos-
tic evaluation. In our case, the association of the
Benchmarking Tool and the Resource Tool used in
conjunction with unit and regression testings helps
to identify what part of the analysis process is con-
cerned and, for grammar engineering, what rule or
set of rules have to be questioned in order to improve
the overall system performance.
Future directions of our work include the paral-
lelization of the analysis on a cluster, so as to re-
trieve evaluation results as quickly as possible. This
should allow us to use evaluation results from a
larger annotated corpus. We also intend to focus on
visualization of results for better identification and
interpretation of errors, in order to access directly er-
roneous analysis and involved resources. A second
development iteration will include the development
of more user friendly resources editors.
We also plan to work on automatic syntactic rules
inference, based on previous work in our laboratory
(Embarek and Ferret, 2008). For this goal, contin-
uous benchmarking will be even more important as
the system will rely on experts tuning parameters for
learning rules, the syntactic rules themselves being
not necessarily edited nor viewable for the expert.
Acknowledgments
This work was partly funded by the French National
Research Agency (ANR), MDCA program 2006.
References
Lorna Balkan, Klaus Netterz, Doug Arnold, Siety Meijer,
1994. Test Suites for Natural Language Processing.
Proceedings of the Language Engineering Convention
(LEC?94), 17?22.
William A Baumgartner, Kevin Bretonnel Cohen,
Lawrence Hunter, 2008. An open-source framework
for large-scale, flexible evaluation of biomedical text
mining systems. Journal of Biomedical Discovery and
Collaboration 2008, Vol. 3, pp 1.
Romaric Besanc?on, Gae?l de Chalendar, 2005.
L?analyseur syntaxique de LIMA dans la campagne
d?valuation EASY. Actes des Ateliers de la 12e Con-
frence annuelle sur le Traitement Automatique des
Langues Naturelles (TALN 2005), Vol. 2, pp 21.
John Carroll, Anette Frank, Dekang Lin, Detlef Prescher,
Hans Uszkoreit, 2002. Proceedings of the workshop
beyond parseval - toward improved evaluation mea-
sures for parsing systems. Proceedings of the 3rd
International Conference on Language Resources and
Evaluation (LREC?02).
Nikos Chatzichrisafis, Dick Crouch, Tracy Holloway
King, Rowan Nairn, Manny Rayner, Marianne Santa-
holma, 2007. Regression Testing For Grammar-Based
Systems. Proceedings of the GEAF07 Workshop, pp
128?143.
Eric V. de la Clergerie, Olivier Hamon, Djamel Mostefa,
Christelle Ayache, Patrick Paroubek, Anne Vilnat,
2008. PASSAGE: from French Parser Evaluation
to Large Sized Treebank. Proceedings of the Sixth
International Language Resources and Evaluation
(LREC?08).
Eric V. de la Clergerie, Christelle Ayache, Gae?l de
Chalendar, Gil Francopoulo, Claire Gardent, Patrick
Paroubek, 2008. Large scale production of syntactic
72
annotations for French. In Proceedings of the interna-
tional workshop on Automated Syntactic Annotations
for Interoperable Language Resources, Hong-Kong.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, 2002. GATE: A
framework and graphical development environment
for robust NLP tools and applications. Proceedings of
the 40th Anniversary Meeting of the ACL, 2002.
Mehdi Embarek, Olivier Ferret, 2008. Learning patterns
for building resources about semantic relations in the
medical domain. 6th Conference on Language Re-
sources and Evaluation (LREC?08), Marrakech, Mo-
rocco.
Jonathan G. Fiscus, 1997. A Post-Processing System to
Yield Reduced Word Error Rates: Recognizer Output
Voting Error Reduction (ROVER). Proceedings IEEE
Workshop on Automatic Speech Recognition and Un-
derstanding (ASRU97), pp 347?352.
Magali Joint, Pierre-Alain Moellic, Patrick Hede, Pas-
cal Adam, 2004. PIRIA: a general tool for indexing,
search, and retrieval of multimedia content. Proceed-
ings of SPIE, Vol. 5298, 116 (2004), San Jose, CA,
USA.
Sylvain Kahane, 2000. Les grammaires de dpendance.
Traitement Automatique des Langues, Vol. 41.
Stephan Oepen, Daniel P. Flickinger, 1998. Towards sys-
tematic grammar profiling. Test suite technology ten
years after. Special Issue on Evaluation 12, 411?436.
Valeria de Paiva, Tracy Holloway King, 2008. Design-
ing Testsuites for Grammar-based Systems in Appli-
cations. Proceedings of the GEAF08 Workshop, pp
49?56.
Patrick Paroubek, Isabelle Robba, Anne Vilnat, Christelle
Ayache, 2006. Data, Annotations and Measures in
EASY, the Evaluation Campaign for Parsers of French.
5th Conference on Language Resources and Evalua-
tion (LREC?06), Genoa, Italy.
C. J. van Rijsbergen, 1979. Information Retrieval, 2nd
edition.
73
Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 69?77,
Avignon, France, April 23 2012. c?2012 Association for Computational Linguistics
Coupling Knowledge-Based and Data-Driven Systems
for Named Entity Recognition
Damien Nouvel Jean-Yves Antoine Nathalie Friburger Arnaud Soulet
Universite? Franc?ois Rabelais Tours, Laboratoire d?Informatique
3, place Jean Jaures, 41000 Blois, France
{damien.nouvel, jean-yves.antoine, nathalie.friburger, arnaud.soulet}@univ-tours.fr
Abstract
Within Information Extraction tasks,
Named Entity Recognition has received
much attention over latest decades. From
symbolic / knowledge-based to data-driven
/ machine-learning systems, many ap-
proaches have been experimented. Our
work may be viewed as an attempt to
bridge the gap from the data-driven per-
spective back to the knowledge-based one.
We use a knowledge-based system, based
on manually implemented transducers,
that reaches satisfactory performances. It
has the undisputable advantage of being
modular. However, such a hand-crafted
system requires substantial efforts to
cope with dedicated tasks. In this con-
text, we implemented a pattern extractor
that extracts symbolic knowledge, using
hierarchical sequential pattern mining
over annotated corpora. To assess the
accuracy of mined patterns, we designed a
module that recognizes Named Entities in
texts by determining their most probable
boundaries. Instead of considering Named
Entity Recognition as a labeling task, it
relies on complex context-aware features
provided by lower-level systems and
considers the tagging task as a markovian
process. Using thos systems, coupling
knowledge-based system with extracted
patterns is straightforward and leads to a
competitive hybrid NE-tagger. We report
experiments using this system and compare
it to other hybridization strategies along
with a baseline CRF model.
1 Introduction
Named Entity Recognition (NER) is an informa-
tion extraction (IE) task that aims at extracting
and categorizing specific entities (proper names
or dedicated linguistic units as time expressions,
amounts, etc.) in texts. These texts can be pro-
duced in diverse conditions. In particular, they
may correspond to either electronic written doc-
uments (Marsh & Perzanowski, 1998) or more
recently speech transcripts provided by a human
expert or an automatic speech recognition (ASR)
system (Galliano et al, 2009). The recognized en-
tities may later be used by higher-level tasks for
different purposes such as Information Retrieval
or Open-Domain Question-Answering (Voorhees
& Harman, 2000).
While NER is often considered as quite a sim-
ple task, there is still room for improvement when
it is confronted to difficult contexts. For instance,
NER systems may have to cope with noisy data
such as word sequences containing speech recog-
nition errors in ASR. In addition, NER is no more
circumscribed to proper names, but may also in-
volve common nouns (e.g., ?the judge?) or com-
plex multi-word expressions (e.g. ?the Com-
puter Science department of the New York Uni-
versity?). These complementary needs for robust
and detailed processing explain that knowledge-
based and data-driven approaches remain equally
competitive on NER tasks as shown by numerous
evaluation campaigns. For instance, the French-
speaking Ester2 evaluation campaign on radio
broadcasts (Galliano et al, 2009) has shown that
knowledge-based approaches outperformed data-
driven ones on manual transcriptions while a sys-
tem based on Conditional Random Fields (CRFs,
participant LIA) is ranked first on noisy ASR tran-
scripts. This is why the development of hybrid
systems has been investigated by the NER com-
munity.
69
In this paper, we present a strategy of hy-
bridization benefiting from features produced by
a knowledge-based system (CasEN) and a data-
driven pattern extractor (mineXtract). CasEN
has been manually implemented based on finite-
state transducers. Such a hand-crafted system
requires substantial efforts to be adapted to ded-
icated tasks. We developed mineXtract, a text-
mining system that automatically extracts infor-
mative rules, based on hierarchical sequential pat-
tern mining. Both implement processings that are
context-aware and use lexicons. Finally, to rec-
ognize NEs, we propose mStruct, a light multi-
purpose automatic annotator, parameterized using
logistic regression over available features. It takes
into account features provided by lower-level sys-
tems and annotation scheme constraints to output
a valid annotation maximizing likelihood. Our ex-
periments show that the resulting hybrid system
outperforms standalone systems and reaches per-
formances comparable to a baseline hybrid CRF
system. We consider this as a step forward to-
wards a tighter integration of knowledge-based
and data-driven approaches for NER.
The paper is organized as follows. Section 2
describes the context of this work and reviews
related work. Section 3 describes CasEN, the
knowledge-based NE-tagger. Section 4 details the
process of extracting patterns from annotated data
as informative rules. We then introduce the au-
tomatic annotator mStruct in Section 5. Section 6
describes how to gather features from systems and
present diverse hybridization strategies. Corpora,
metrics used and evaluation results are reported in
Section 7. We conclude in Section 8.
2 Context and Related Work
2.1 Ester2 Evaluation Campaign
This paper focuses on NER in the context of
the Ester2 evaluation campaign (Galliano et al,
2009). This campaign assesses system?s perfor-
mance for IE tasks over ASR outputs and manual
transcriptions of radio broadcast news (see details
in Section 7). The annotation guidelines speci-
fied 7 kinds of entities to be detected and cate-
gorized: persons (?pers?), organizations (?org?),
locations (?loc?), amounts (?amount?), time ex-
pressions (?time?), functions (?func?), products
(?prod?). Technically, the annotation scheme is
quite simple: only one annotation per entity, al-
D
Sent. Tokens and NEs
s1 <pers> Isaac Newton </pers> was admitted in
<time> June 1661 </time> to <org> Cambridge
</org>.
s2 <time> In 1696 </time>, he moved to <loc> Lon-
don </loc> as <func> warden of the Royal Mint
</func>.
s3 He was buried in <loc> Westminster Abbey </loc>.
Table 1: Sentences from an annotated corpus
most no nesting (except for persons collocated
with their function: both should be embedded in
an encompassing ?pers? NE).
We illustrate the annotation scheme using a
running example. Table 1 presents the expected
annotation in the context of Ester2 from ?Isaac
Newton was admitted in June 1661 to Cam-
bridge. In 1696, he moved to London as warden
of the Royal Mint. He was buried in Westmin-
ster Abbey.?. This example illustrates frequent
problems for NER task. Determining the extent
of a NE may be difficult. For instance, NER
should consider here either ?Westminster? (city)
or ?Westminster Abbey? (church, building). Cat-
egorizing NEs is confronted to words ambiguities,
for instance ?Cambridge? may be considered as a
city (?loc?) or a university (?org?). In addition, oral
transcripts may contain disfluencies, repetitions,
hesitations, speech recognition errors: overall dif-
ficulty is significantly increased. For these rea-
sons, NER over such noisy data is a challenging
task.
2.2 State of the Art
Knowledge-based approaches Most of the
symbolic systems rely on shallow parsing tech-
niques, applying regular expressions or linguistic
patterns over Part-Of-Speech (POS), in addition
to proper name lists checking. Some of them han-
dle a deep syntactic analysis which has proven
its ability to reach outstanding levels of perfor-
mances (Brun & Hage`ge, 2004; Brun & Hage`ge,
2009; van Shooten et al, 2009).
Data-driven approaches A large diversity of
data-driven approaches have been proposed dur-
ing the last decade for NER. Generative models
such as Hidden Markov Models or stochastic fi-
nite state transducers (Miller et al, 1998; Favre et
al., 2005) benefit from their ability to take into
account the sequential nature of language. On
the other hand, discriminative classifiers such as
70
Support Vector Machines (SVMs) are very effec-
tive when a large variety of features (Isozaki &
Kazawa, 2002) is used, but lack the ability to
take a global decision over an entire sentence.
Context Random Fields (CRFs) (Lafferty et al,
2001) have enabled NER to benefit from the ad-
vantages of both generative and discriminative ap-
proaches (McCallum & Li, 2003; Zidouni et al,
2010; Be?chet & Charton, 2010). Besides, the
robustness of data-driven / machine-learning ap-
proaches explains that the latter are more appro-
priate on noisy data such as ASR transcripts.
Hybrid systems Considering the complemen-
tary behaviors of knowledge-based and data-
driven systems for NER, projects have been con-
ducted to investigate how to conciliate both ap-
proaches. Work has been done to automatically
induce symbolic knowledge (Hingston, 2002;
Kushmerick et al, 1997) that may be used as
NE taggers. But in most cases, hybridization for
NER relies a much simpler principle: outputs of
knowledge-based systems are considered as fea-
tures by a machine learning algorithm. For in-
stance, maximum entropy may be used when a
high diversity of knowledge sources are to be
taken into account (Borthwick et al, 1998). CRFs
also have demonstrated their ability to merge
symbolic and statistic processes in a machine
learning framework (Zidouni et al, 2010).
We propose an approach to combine
knowledge-based and data-driven approaches in
a modular way. Our first concern is to implement
a module that automatically extracts knowledge
that should be interoperable with the existing
system?s transducers. This is done by focusing, in
annotated corpora, more on ?markers? (tags) that
are to be inserted between tokens (e.g. <pers>,
</pers>, <org>, </org>, etc.), than on
?labels? assigned to each token, as transducer
do. By doing so, we expect to establish a better
grounding for hybriding manually implemented
and automatically extracted patterns. Afterwards,
another module is responsible of annotating
NEs by using those context-aware patterns and
standard machine-learning techniques.
3 CasEN: a knowledge-based system
The knowledge-based system is based on CasSys
(Friburger & Maurel, 2004), a finite-state cascade
system that implements processings on texts at di-
verse levels (morphology, lexicon, chunking). It
may be used for various IE tasks, or simply to
transform or prepare a text for further processings.
The principle of this finite-state processor is to
first consider islands of certainty (Abney, 2011),
so as to give priority to most confident rules. Each
transducer describes local patterns corresponding
to NEs or interesting linguistic units available to
subsequent transducers within the cascade.
Casen is the set of NE recognition transduc-
ers. It was initially designed to process written
texts, taking into account diverse linguistic clues,
proper noun lists (covering a broad range of first
names, countries, cities, etc.) and lexical evi-
dences (expressions that may trigger recognition
of a named entity).
Figure 1: A transducer recognizing person names
Figure 2: Transducer ?patternFirstName?
As an illustration, Figure 1 presents a very sim-
ple transducer tagging person names made of an
optional title, a first name and a surname. The
boxes contain the transitions of the transducer as
items to be matched for recognizing a person?s
name. Grayed boxes contain inclusions of other
transducers (e.g. box ?patternFirstName? in Fig-
ure 1 is to be replaced by the transducer depicted
in Figure 2). Other boxes can contain lists of
words or diverse tags (e.g. <N+firstname>
for a word tagged as first name by lexicon). The
outputs of transducers are displayed below boxes
(e.g. ?{? and ?,.entity+pers+hum}? in Figure 1).
For instance, that transducer matches the
word sequence ?Isaac Newton? and outputs:
?{{Isaac ,.firstname} {Newton ,.surname} ,.en-
tity+pers+hum}?. By applying multiple transduc-
71
ers on a text sequence, CasEN can provide sev-
eral (possibly nested) annotations on a NE and
its components. This has the advantage of pro-
viding detailed information about CasEN internal
processings for NER.
Finally, the processing of examples in Table 1
leads to annotations such as:
? { { June ,.month} { 1661 ,.year} ,en-
tity+time+date+rel}
? { Westminster ,.entity+loc+city}
{ Abbey ,buildingName} ,.en-
tity+loc+buildingCityName }
In standalone mode, post-processing steps con-
vert outputs into Ester2 annotation scheme (e.g.
<pers> Isaac Newton </pers>).
Experiments conducted on newspaper docu-
ments for recognizing persons, organizations and
locations on an extract of the Le Monde corpus
have shown that CasEN reaches 93.2% of recall
and 91.1% of f-score (Friburger, 2002). Dur-
ing the Ester2 evaluation campaign, CasEN (?LI
Tours? participant in (Galliano et al, 2009)) ob-
tained 33.7% SER (Slot Error Rate, see section
about metrics description) and a f-score of 75%.
This may be considered as satisfying when one
knows the lack of adaptation of Casen to speci-
ficities of oral transcribed texts.
4 mineXtract: Pattern Mining Method
4.1 Enriching an Annotated Corpus
We investigate the use of data mining techniques
in order to supplement our knowledge-based sys-
tem. To this end, we use an annotated corpus to
mine patterns related to NEs. Sentences are con-
sidered as sequences of items (this precludes ex-
traction of patterns accross sentences). An item is
either a word from natural language (e.g. ?admit-
ted?, ?Newton?) or a tag delimiting NE categories
(e.g., <pers>, </pers> or <loc>). The an-
notated corpus D is a multiset of sequences.
Preprocessing steps enrich the corpus by (1) us-
ing lexical resources (lists of toponyms, anthro-
ponyms and so on) and (2) lemmatizing and ap-
plying a POS tagger. This results in a multi-
dimensional corpus where a token may gradually
be generalized to its lemma, POS or lexical cate-
gory. Figure 3 illustrates this process on the words
sequence ?moved to <loc> London </loc>?.
move
VER
moved
PRP
to
<loc> PN
CITY
</loc>
Figure 3: Multi-dimensional representation of the
phrase ?moved to <loc> London </loc>?
The first preprocessing step consists in consid-
ering lexical resources to assign tokens to lexi-
cal categories (e.g., CITY for ?London?) when-
ever possible. Note that those resources contain
multi-word expressions. Figure 4 provides a short
extract limited to tokens of Table 1) of lexical
ressources (totalizing 201,057 entries). This as-
signment should be ambiguous. For instance, pro-
cessing ?Westminster Abbey? would lead to cat-
egorizing ?Westminster? as CITY and the whole
as INST.
Afterwards, a POS tagger based on TreeTag-
ger (Schmid, 1994) distinguishes common nouns
(NN) from proper names (PN). Besides, token is
deleted (only PN category is kept) to avoid extrac-
tion of patterns that would be specific to a given
proper name (on Figure 3, ?London? is removed).
Figure 5 shows how POS, tokens and lemmas are
organized as a hierarchy.
Category Tokens
ANTHRO Newton, Royal . . .
CITY Cambridge, London, Westminster . . .
INST Cambridge, Royal Mint, Westminster Abbey . . .
METRIC Newton . . .
. . . . . .
Figure 4: Lexical Ressources
in of to
PRP
admit
admitted
be
was
bury
buried
VER
Figure 5: Items Hierarchy
4.2 Discovering Informative Rules
We mine this large enriched annotated corpus to
find generalized patterns correlated to NE mark-
ers. It consists in exhaustively enumerating all the
contiguous patterns mixing words, POS and cat-
72
egories. This provides a very broad spectrum of
patterns, diversely accurate to recognize NEs. As
an illustration, if you consider the words sequence
?moved to <loc> London </loc>? in Figure 3
leads to examining patterns as:
? ? VER PRP <loc> PN </loc>?
? ? VER to <loc> PN </loc>?
? ? moved PRP <loc> CITY </loc>?
The most relevant patterns will be filtered by
considering two thresholds which are usual in
data mining: support and confidence (Agrawal
& Srikant, 1994). The support of a pattern P
is its number of occurrences in D, denoted by
supp(P,D). The greater the support of P , the
more general the pattern P . As we are only inter-
ested in patterns sufficiently correlated to mark-
ers, a transduction rule R is defined as a pattern
containing at least one marker. To estimate em-
pirically how much R is accurate to detect mark-
ers, we calculate its confidence. A dedicated func-
tion suppNoMark(R,D) returns the support of
R when markers are omitted both in the rule and
in the data. The confidence of R is:
conf(R,D) =
supp(R,D)
suppNoMark(R,D)
For instance, consider the rule R = ? VER PRP
<loc>? in Table 1. Its support is 2 (sentences
s2 and s3). But its support without considering
markers is 3, since sentence s1 matches the rule
when markers are not taken in consideration. The
confidence of R is 2/3.
In practice, the whole collection of transduc-
tion rules exceeding minimal support and con-
fidence thresholds remains too large, especially
when searching for less frequent patterns. Conse-
quently, we filter-out ?redundant rules?: those for
which a more specific rule exists with same sup-
port (both cover same examples in corpus). For
instance, the rules R1 = ? VER VER in <loc>?
and R2 = ? VER in <loc>? are more general
and have same support than R3 = ? was VER
in <loc>?: we only retain the latter.
The system mineXtract implements those pro-
cessing using a level-wise algorithm (Mannila &
Toivonen, 1997).
5 mStruct: Stochastic Model for NER
We have established a common ground for the
systems to interact with a higher level model.
Our assumption is that lower level systems ex-
amine the input (sentences) and provide valu-
able clues playing a key role in the recognition
of NEs. In that context, the annotator is im-
plemented as an abstracted view of sentences.
Decisions will only have to be taken whenever
one of the lower-level systems provides infor-
mation. Formally, beginning or ending a NE
at a given position i may be viewed as the af-
fectation of a random variable P (Mi = mji)
where the value of mji is one of the markers
({?,<pers>,</pers>,<loc>,<org>, . . . }).
For a given sentence, we use binary features
triggered by lower-level systems at a given posi-
tion (see section 6.1) for predicting what marker
would be the most probable at that very position.
This may be viewed as an instance of a classifi-
cation problem (more precisely multilabel clas-
sification since several markers may appear at a
single position, but we won?t enter into that level
of detail due to lack of space). Empirical exper-
iments with diverse machine learning algorithms
using Scikit-learn (Pedregosa et al, 2011) lead us
to consider logistic regression as the most effec-
tive on the considered task.
Considering those probabilities, it is now pos-
sible to estimate the likelihood of a given annota-
tion over a sentence. Here, markers are assumed
to be independent. With this approximation, the
likehood of an annotation is computed by a sim-
ple product:
P (M1 = mj1 ,M2 = mj2 , . . . ,Mn = mjn)
?
?
i=1...n
P (Mi = mji)
As an illustration, Figure 6 details the compu-
tation of an annotation given the probability of ev-
ery markers, using the Ester2 annotation scheme.
For clarity purposes, only sufficiently probable
markers (including ?) are displayed at each po-
sition. A possible <func> is discarded (crossed
out), being less probable than a previous one. An
annotation solution <org> . . .</org> is evalu-
ated, but is less likely (0.3 ? 0.4 ? 0.9 ? 0.4 ? 0.4 ?
0.1 = 0.0017) than warden of the Royal Mint as a
function (0.6?0.4?0.9?0.3?0.5?0.4 = 0.0129)
73
which will be retained (and is the expected anno-
tation).
as
PRP
? 0.3
<func> 0.6
warden
NN
JOB
? 0.4
</func> 0.5
of
PRP
? 0.9
the
DET
? 0.3
<org> 0.2
<pers> 0.4
Royal
NP
INST
? 0.5
</pers> 0.4
Mint
NP
INST
? 0.1
</func> 0.4
<org> 0.4
Figure 6: Stochastic Annotation of a Sequence
Estimating markers probabilities allows the
model to combine evidences from separate
knowledge sources when recognizing starting or
ending boundaries. For instance, CasEN may re-
congize intermediary structures but not the whole
entity (e.g. when unexpected words appear inside
it) while extracted rules may propose markers that
are not necessarily paired. The separate detection
of markers enables the system to recognize named
entities without modeling all their tokens. This
may be useful when NER has to face noisy data
or speech disfluences.
Finally, it is not necessary to compute likeli-
hoods over all possible combination of markers,
since the annotation scheme is much constrained.
As the sentence is processed, some annotation so-
lutions are to be discarded. It is straightforward
to see that this problem may be resolved using
dynamic programming, as did Borthwick et al
(1998). Depending on the annotation scheme,
constraints are provided to the annotator which
outputs an annotation for a given sentence that
is valid and that maximizes likelihood. Our sys-
tem mStruct (micro-Structure) implements this
(potentially multi-purpose) automatic annotation
process as a separate module.
6 Hybriding systems
6.1 Gathering Clues from Systems
Figure 7 describes the diverse resources and algo-
rithms that are plugged together. The knowledge-
based system uses lists that recognize lexical pat-
terns useful for NER (e.g. proper names, but also
automata to detect time expressions, functions,
etc.). Those resources are exported and available
to the data mining software as lexical resources
(see section 4) and (as binary features) to the base-
line CRF model.
Lists
Mining
Corpus mineXtract
Transducers CasEN
Learning
Corpus
Hybridation
Gather
Features
mStruct
Figure 7: Systems Modules (Hybrid data flow)
Each system processes input text and provides
features used by the Stochastic Model mStruct. It
is quite simple to take in consideration mined in-
formative rules: each time a rule i proposes its
jth marker, a Boolean feature Mij is activated.
What is provided by CasEN is more sophisticated,
since each transducer is able to indicate more de-
tailed information (see section 3), as multiple fea-
tures separated by ?+? (e.g. ?entity+pers+hum?).
We want to benefit as much as possible from this
richness: whenever a CasEN tag begins or ends,
we activate a boolean feature for each mentioned
feature plus one for each prefixes of features (e.g.
?entity?, ?pers?, ?hum? but also ?entity.pers? and
?entity.pers.hum?).
6.2 Coupling Strategies
We report results for the following hybridizations
and CRF-based system using Wapiti (Lavergne et
al., 2010).
? CasEN: knowledge-based system standalone
? mXS: mineXtract extracts, mStruct annotates
? Hybrid: gather features from CasEN and mineX-
tract, mStruct annotates
? Hybrid-sel: as Hybrid, but features are selected
? CasEN-mXS-mine: as mXS, but text is pre-
processed by CasEN (adding a higher general-
ization level above lexical lists)
? mXS-CasEN-vote: as mXS, plus a post-
processing step as a majority vote based on mXS
and CasEN outputs
? CRF: baseline CRF, using BIO and common fea-
tures (unigrams: lemma and lexical lists, bi-
grams: previous, current and next POS)
74
Corpus Tokens Sentences NEs
Ester2-Train 1 269 138 44 211 80 227
Ester2-Dev 73 375 2 491 5 326
Ester2-Test-corr 39 704 1 300 2 798
Ester2-Test-held 47 446 1 683 3 067
Table 2: Characteristics of Corpora
? CasEN-CRF: same as CRF, but the output of
CasEN is added as a single feature (concatena-
tion of CasEN features)
7 Experimentations
7.1 Corpora and Metrics
For experimentations, we use the corpus that has
been made available after the Ester2 evaluation
campaign. Table 2 gives statistics on diverse sub-
parts of this corpus. Unfortunately, many incon-
sistencies where noted for manual annotation, es-
pecially for ?Ester2-Train? part that won?t be used
for training.
There were fewer irregularities in other parts of
the corpus. Although, manual corrections were
done on half of the Test corpus (Nouvel et al,
2010) (Ester2-Test-corr in Table 2), to obtain a
gold standard that we will use to evaluate our ap-
proach. The remaining part of the Test corpus
(Ester2-Test-held in Table 2) merged with the Dev
part constitute our training set (Ester2-Dev in Ta-
ble 2), used as well to extract rules with mineX-
tract, to estimate stochastic model probabilities of
mStruct and to learn CRF models.
We evaluate systems using following metrics:
? detect: rate of detection of the presence of
any marker (binary decision) at any position
? desamb: f-score of markers when comparing
N actual markers to N most probable mark-
ers, computed over positions where k mark-
ers are expected (N=k) or the most probable
marker is not ? (N=1)
? precision, recall, f-score: evaluation of NER
by categories by examining labels assigned
to tokens (similarly to Ester2 results)
? SER (Slot Error Rate): weighted error rate of
NER (official Ester2 performance metric, to
be lowered), where errors are discounted per
entity as Galliano et al (2009) (deletion and
insertion errors are weighted 1 whereas type
and boundary errors, 0.5)
System support confidence detect disamb f-score SER
CasEN ? ? ? ? 78 30.8
mXS 5 0.1 97 73 76 28.4
5 0.5 96 71 74 31.2
15 0.1 96 72 73 30.1
Hybrid 5 0.1 97 78 79 26.3
5 0.5 97 77 77 28.3
15 0.1 97 78 76 28.2
inf inf 96 71 70 42.0
Table 3: Performance of Systems
7.2 Comparing Hybridation with Systems
First, we separately evaluate systems. While
CasEN is not to be parameterized, mineXtract
has to be given minimum frequency and support
thresholds. Table 3 shows results for each sys-
tem separately and for the combination of sys-
tems. Results obtained by mXS show that even
less confident rules are improving performances.
Generally speaking, the detect score is very high,
but this mainly due to the fact that the ? case is
very frequent. The disamb score is much corre-
lated to the SER. This reflects the fact that the
challenge is for mStruct to determine the correct
markers to insert.
Comparing systems shows that the hybridiza-
tion strategy is competitive. The knowledge-
based system yields to satisfying results. mXS
obtains slightly better SER and the hybrid sys-
tem outperforms both in most cases. Considering
SER, the only exception to this is the ?inf? line
(mStruct uses only CasEN features) where perfor-
mances are degraded. We note that mStruct ob-
tains better results as more rules are extracted.
7.3 Assessing Hybridation Strategies
amount func loc org pers time all
10
20
30
40
50
CasEN
mXS
Hybrid
Hybrid-sel
Figure 8: SER of Systems by NE types
75
System precision recall f-score SER
Hybrid-sel 83.1 74.8 79 25.2
CasEN-mXS-mine 76.8 75.5 76 29.4
mXS-CasEN-vote 78.7 79.0 79 26.9
CRF 83.8 77.3 80 26.1
CasEN-CRF 84.1 77.5 81 26.0
Table 4: Comparing performances of systems
In a second step, we look in detail what NE
types are the most accurately recognized. Those
results are reported in Figure 8, where is depicted
the error rates (to be lowered) for main types
(?prod?, being rare, is not reported). This revealed
that features provided by CasEN for ?loc? type ap-
peared to be unreliable for mStruct. Therefore, we
filtered-out related features, so as to couple sys-
tems in a more efficient fashion. This leads to a
1.1 SER gain (from 26.3 to 25.2) when running
the so-called ?Hybrid-sel? system, and demon-
strates that the hybridation is very sensitive to
what is provided by CasEN.
With this constrained hybridization, we com-
pare previous results to other hybridization strate-
gies and a baseline CRF system as described in
section 6. Those experiments are reported in Ta-
ble 4. We see that, when considering SER, the hy-
bridization strategy using CasEN features within
mStruct stochastic model slightly outperforms
?simpler? hybridizations schemes (pre-processing
or post-processing with CasEN) and the CRF
model (even when it uses CasEN preprocessing
as a single unigram feature).
However the f-score metric gives advantage
to CasEN-CRF, especially when considering re-
call. By looking indepth into errors and when re-
minded that SER is a weighted metric based on
slots (entities) while f-score is based on tokens
(see section 7.1), we noted that on longest NEs
(mainly ?func?), Hybrid-sel does type errors (dis-
counted as 0.5 in SER) while CasEN-CRF does
deletion errors (1 in SER). This is pointed out by
Table 5. The influence of error?s type is clear
when considering the SER for ?func? type for
which Hybrid-sel is better while f-score doesn?t
measure such a difference.
7.4 Discussion and Perspectives
Assessment of performances using a baseline
CRF pre-processed by CasEN and the hybrided
strategy system shows that our approach is com-
petitive, but do not allow to draw definitive con-
System NE type insert delet type SER f-score
Hybrid-sel func 8 21 7 40.3 65
all 103 205 210 25.2 79
CasEN-CRF func 9 37 0 53.5 64
all 77 251 196 26.0 81
Table 5: Impact of ?func? over SER and f-score
clusions. We keep in mind that the evaluated CRF
could be further improved. Other methods have
been successfully experimented to couple more
efficiently that kind of data-driven approach with
a knowledge-based one (for instance Zidouni et
al. (2010) reports 20.3% SER on Ester2 test cor-
pus, but they leverage training corpus).
Nevertheless, the CRFs models do not allow
to directly extract symbolic knowledge from data.
We aim at organizing our NER system in a mod-
ular way, so as to be able to adapt it to dedicated
tasks, even if no training data is available. Results
show that this proposed hybridization reaches a
satisfactory level of performances.
This kind of hybridization, focusing on ?mark-
ers?, is especially relevant for annotation tasks.
As a next step, experiments are to be conducted
on other tasks, especially those involving nested
annotations that our current system is able to pro-
cess. We will also consider how to better organize
and integrate automatically extracted informative
rules into our existing knowledge-based system.
8 Conclusion
In this paper, we consider Named Entity Recog-
nition task as the ability to detect boundaries of
Named Entities. We use CasEN, a knowledge-
based system based on transducers, and mineX-
tract, a text-mining approach, to extract informa-
tive rules from annotated texts. To test these rules,
we propose mStruct, a light multi-purpose annota-
tor that has the originality to focus on boundaries
of Named Entities (?markers?), without consider-
ing the labels associated to tokens. The extraction
module and the stochastic model are plugged to-
gether, resulting in mXS, a NE-tagger that gives
satisfactory results. Those systems altogether
may be hybridized in an efficient fashion. We as-
sess performances of our approach by reporting
results of our system compared to other baseline
hybridization strategies and CRF systems.
76
References
Steven P. Abney. 1991. Parsing by Chunks. Principle-
Based Parsing, 257?278.
Rakesh Agrawal and Ramakrishnan Srikant. 1994.
Fast algorithms for mining association rules in large
databases. Very Large Data Bases, 487?499.
Fre?de?ric Bechet and Eric Charton. 2010. Unsuper-
vised knowledge acquisition for Extracting Named
Entities from speech. Acoustics, Speech, and Signal
Processing (ICASSP?10), Dallas, USA.
Andrew Borthwick, John Sterling, Eugene Agichtein
and Ralph Grishman. 1998. Exploiting Di-
verse Knowledge Sources via Maximum Entropy
in Named Entity Recognition. Very Large Corpora
(VLC?98), Montreal, Canada.
Caroline Brun and Caroline Hage`ge. 2004. Intertwin-
ing Deep Syntactic Processing and Named Entity
Detection. Advances in Natural Language Process-
ing, 3230:195-206.
Caroline Brun and Maud Ehrmann. 2009. Adapta-
tion of a named entity recognition system for the es-
ter 2 evaluation campaign. Natural Language Pro-
cessing and Knowledge Engineering (NLPK?09),
Dalian, China.
Beno??t Favre, Fre?de?ric Be?chet, and Pascal Nocera.
2005. Robust Named Entity Extraction from Large
Spoken Archives. Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP?05), Vancouver, Canada.
Nathalie Friburger. 2002. Reconnaissance automa-
tique des noms propres: Application a` la classifica-
tion automatique de textes journalistiques. PhD.
Nathalie Friburger and Denis Maurel. 2004. Finite-
state transducer cascades to extract named entities.
Theoretical Computer Sciences (TCS), 313:93?104.
Sylvain Galliano, Guillaume Gravier and Laura
Chaubard. 2009. The ESTER 2 evaluation cam-
paign for the rich transcription of French radio
broadcasts. International Speech Communication
Association (INTERSPEECH?09), Brighton, UK.
Philip Hingston. 2002. Using Finite State Automata
for Sequence Mining. Australasian Computer Sci-
ence Conference (ACSC?02), Melbourne, Australia.
Hideki Isozaki and Hideto Kazawa. 2002. Efficient
support vector classifiers for named entity recog-
nition. Conference on Computational linguistics
(COLING?02), Taipei, Taiwan.
Nicholas Kushmerick and Daniel S. Weld and Robert
Doorenbos. 1997. Wrapper Induction for Informa-
tion Extraction. International Joint Conference on
Artificial Intelligence (IJCAI?97), Nagoya, Japan.
John D. Lafferty, Andrew McCallum and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and Labeling
Sequence Data. International Conference on Ma-
chine Learning (ICML?01), Massachusetts, USA.
Thomas Lavergne and Olivier Cappe? and Franc?ois
Yvon 2010. Practical Very Large Scale CRFs. As-
sociation for Computational Linguistics (ACL?10),
Uppsala, Sweden.
Heikki Mannila and Hannu Toivonen. 1997. Level-
wise search and borders of theories in knowledge
discovery. Data Mining and Knowledge Discovery,
1(3):241?258.
Elaine Marsh and Dennis Perzanowski. 1998. MUC-7
Evaluation of IE Technology: Overview of Results.
Message Understanding Conference (MUC-7).
Andrew McCallum and Wei Li. 2003. Early re-
sults for named entity recognition with conditional
random fields, feature induction and web-enhanced
lexicons. Computational Natural Language Learn-
ing (CONLL?03), Edmonton, Canada.
Scott Miller, Michael Crystal, Heidi Fox, Lance
Ramshaw, Richard Schwartz, Rebecca Stone and
Ralph Weischedel. 1998. Algorithms That Learn
To Extract Information BBN: Description Of The
Sift System As Used For MUC-7. Message Under-
standing Conference (MUC-7).
Damien Nouvel, Jean-Yves Antoine, Nathalie
Friburger and Denis Maurel. 2010. An Analysis
of the Performances of the CasEN Named Entities
Recognition System in the Ester2 Evaluation
Campaign. Language Resources and Evaluation
(LREC?10), Valetta, Malta.
Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot and E?douard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825?2830.
Helmut Schmid. 1994. Probabilistic POS Tagging
Using Decision Trees. New Methods in Language
Processing (NEMLP?94, Manchester, UK.
Boris W. van Schooten, Sophie Rosset, Olivier Galib-
ert, Aure?lien Max, Rieks op den Akker, and Gabriel
Illouz. 2009. Handling speech in the ritel QA
dialogue system. International Speech Communi-
cation Association (INTERSPEECH?09), Brighton,
UK.
Ellen M. Voorhees and Donna Harman. 2000.
Overview of the Ninth Text REtrieval Conference
(TREC-9). International Speech Communication
Association (INTERSPEECH?09), Brighton, UK.
Azeddine Zidouni and Sophie Rosset and Herve? Glotin
2010. Efficient combined approach for named
entity recognition in spoken language. Interna-
tional Speech Communication Association (INTER-
SPEECH?10), Makuhari, Japan.
77
