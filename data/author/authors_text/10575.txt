Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1041?1050,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Unsupervised Multilingual Learning for POS Tagging
Benjamin Snyder and Tahira Naseem and Jacob Eisenstein and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge MA 02139
{bsnyder, tahira, jacobe, regina}@csail.mit.edu
Abstract
We demonstrate the effectiveness of multilin-
gual learning for unsupervised part-of-speech
tagging. The key hypothesis of multilin-
gual learning is that by combining cues from
multiple languages, the structure of each be-
comes more apparent. We formulate a hier-
archical Bayesian model for jointly predicting
bilingual streams of part-of-speech tags. The
model learns language-specific features while
capturing cross-lingual patterns in tag distri-
bution for aligned words. Once the parame-
ters of our model have been learned on bilin-
gual parallel data, we evaluate its performance
on a held-out monolingual test set. Our evalu-
ation on six pairs of languages shows consis-
tent and significant performance gains over a
state-of-the-art monolingual baseline. For one
language pair, we observe a relative reduction
in error of 53%.
1 Introduction
In this paper, we explore the application of multilin-
gual learning to part-of-speech tagging when no an-
notation is available. This core task has been studied
in an unsupervised monolingual framework for over
a decade and is still an active area of research. In this
paper, we demonstrate the effectiveness of multilin-
gual learning when applied to both closely related
and distantly related language pairs. We further ana-
lyze the language features which lead to robust bilin-
gual performance.
The fundamental idea upon which our work is
based is that the patterns of ambiguity inherent in
part-of-speech tag assignments differ across lan-
guages. At the lexical level, a word with part-of-
speech tag ambiguity in one language may corre-
spond to an unambiguous word in the other lan-
guage. For example, the word ?can? in English may
function as an auxiliary verb, a noun, or a regular
verb. However, each of the corresponding functions
in Serbian is expressed with a distinct lexical item.
Languages also differ in their patterns of structural
ambiguity. For example, the presence of an article
in English greatly reduces the ambiguity of the suc-
ceeding tag. In Serbian, a language without articles,
this constraint is obviously absent. The key idea of
multilingual learning is that by combining cues from
multiple languages, the structure of each becomes
more apparent.
While multilingual learning can address ambigu-
ities in each language, it must be flexible enough
to accommodate cross-lingual variations such as tag
inventory and syntactic structure. As a result of
such variations, two languages often select and order
their tags differently even when expressing the same
meaning. A key challenge of multilingual learning
is to model language-specific structure while allow-
ing information to flow between languages.
We jointly model bilingual part-of-speech tag se-
quences in a hierarchical Bayesian framework. For
each word, we posit a hidden tag state which gen-
erates the word as well as the succeeding tag. In
addition, the tags of words with common seman-
tic or syntactic function in parallel sentences are
combined into bilingual nodes representing the tag
pair. These joined nodes serve as anchors that cre-
ate probabilistic dependencies between the tag se-
1041
quences in each language. We use standard tools
from machine translation to discover aligned word-
pairs, and thereafter our model treats the alignments
as observed data.
Our model structure allows language-specific tag
inventories. Additionally, it assumes only that the
tags at joined nodes are correlated; they need not be
identical. We factor the conditional probabilities of
joined nodes into two individual transition probabil-
ities as well as a coupling probability. We define
priors over the transition, emission, and coupling
parameters and perform Bayesian inference using
Gibbs sampling and the Metropolis-Hastings algo-
rithm.
We evaluate our model on a parallel corpus of
four languages: English, Bulgarian, Serbian, and
Slovene. For each of the six language pairs, we
train a bilingual model on this corpus, and evaluate it
on held-out monolingual test sets. Our results show
consistent improvement over a monolingual baseline
for all languages and all pairings. In fact, for one
language pair ? Serbian and Slovene ? the error is
reduced by over 53%. Moreover, the multilingual
model significantly reduces the gap between unsu-
pervised and supervised performance. For instance,
in the case of Slovene this gap is reduced by 71%.
We also observe significant variation in the level of
improvement across language pairs. We show that a
cross-lingual entropy measure corresponds with the
observed differentials in performance.
2 Related Work
Multilingual Learning A number of approaches
for multilingual learning have focused on induc-
ing cross-lingual structures, with applications to
machine translation. Examples of such efforts
include work on the induction of synchronous
grammars (Wu and Wong, 1998; Chiang, 2005)
and learning multilingual lexical resources (Genzel,
2005).
Another thread of work using cross-lingual links
has been in word-sense disambiguation, where
senses of words can be defined based on their trans-
lations (Brown et al, 1991; Dagan et al, 1991;
Resnik and Yarowsky, 1997; Ng et al, 2003).
When annotations for a task of interest are avail-
able in a source language but are missing in the
target language, the annotations can be projected
across a parallel corpus (Yarowsky et al, 2000;
Diab and Resnik, 2002; Pado? and Lapata, 2006; Xi
and Hwa, 2005). In fact, projection methods have
been used to train highly accurate part-of-speech
taggers (Yarowsky and Ngai, 2001; Feldman et al,
2006). In contrast, our own work assumes that an-
notations exist for neither language.
Finally, there has been recent work on applying
unsupervised multilingual learning to morphologi-
cal segmentation (Snyder and Barzilay, 2008). In
this paper, we demonstrate that unsupervised mul-
tilingual learning can be successfully applied to the
sentence-level task of part-of-speech tagging.
Unsupervised Part-of-Speech Tagging Since
the work of Merialdo (1994), the HMM has been the
model of choice for unsupervised tagging (Banko
and Moore, 2004). Recent advances in these
approaches include the use of a fully Bayesian
HMM (Johnson, 2007; Goldwater and Griffiths,
2007). In very recent work, Toutanova and John-
son (2008) depart from this framework and propose
an LDA-based generative model that groups words
through a latent layer of ambiguity classes thereby
leveraging morphological features. In addition, a
number of approaches have focused on develop-
ing discriminative approaches for unsupervised and
semi-supervised tagging (Smith and Eisner, 2005;
Haghighi and Klein, 2006).
Our focus is on developing a simple model that
effectively incorporates multilingual evidence. We
view this direction as orthogonal to refining mono-
lingual tagging models for any particular language.
3 Model
We propose a bilingual model for unsupervised part-
of-speech tagging that jointly tags parallel streams
of text in two languages. Once the parameters have
been learned using an untagged bilingual parallel
text, the model is applied to a held-out monolingual
test set.
Our key hypothesis is that the patterns of ambigu-
ity found in each language at the part-of-speech level
will differ in systematic ways; by considering multi-
ple language simultaneously, the total inherent am-
biguity can be reduced in each language. The model
is designed to permit information to flow across the
1042
I love fish
J' adore les poissons
x1
y1
x2
y2 y3 y4
x3
I love fish
J' adore les poissons
x1/y1 x1/y1 x1/y1
y3
(a) (b)
Figure 1: (a) Graphical structure of two standard monolingual HMM?s. (b) Graphical structure of our bilingual model
based on word alignments.
language barrier, while respecting language-specific
idiosyncrasies such as tag inventory, selection, and
order. We assume that for pairs of words that share
similar semantic or syntactic function, the associ-
ated tags will be statistically correlated, though not
necessarily identical. We use such word pairs as
the bilingual anchors of our model, allowing cross-
lingual information to be shared via joint tagging de-
cisions. We use standard tools from machine trans-
lation to identify these aligned words, and thereafter
our model treats them as fixed and observed data.
To avoid cycles, we remove crossing edges from the
alignments.
For unaligned parts of the sentence, the tag and
word selections are identical to standard monolin-
gual HMM?s. Figure 1 shows an example of the
bilingual graphical structure we use, in comparison
to two independent monolingual HMM?s.
We formulate a hierarchical Bayesian model that
exploits both language-specific and cross-lingual
patterns to explain the observed bilingual sentences.
We present a generative story in which the observed
words are produced by the hidden tags and model
parameters. In Section 4, we describe how to in-
fer the posterior distribution over these hidden vari-
ables, given the observations.
3.1 Generative Model
Our generative model assumes the existence of two
tagsets, T and T ?, and two vocabularies W and W ?,
one of each for each language. For ease of exposi-
tion, we formulate our model with bigram tag de-
pendencies. However, in our experiments we used
a trigram model, which is a trivial extension of the
model discussed here and in the next section.
1. For each tag t ? T , draw a transition distri-
bution ?t over tags T , and an emission distri-
bution ?t over words W , both from symmetric
Dirichlet priors.1
2. For each tag t ? T ?, draw a transition distri-
bution ??t over tags T ?, and an emission distri-
bution ??t over words W ?, both from symmetric
Dirichlet priors.
3. Draw a bilingual coupling distribution ? over
tag pairs T ? T ? from a symmetric Dirichlet
prior.
4. For each bilingual parallel sentence:
(a) Draw an alignment a from an alignment
distribution A (see the following para-
graph for formal definitions of a and A),
(b) Draw a bilingual sequence of part-of-
speech tags (x1, ..., xm), (y1, ..., yn) ac-
cording to:
P (x1, ..., xm, y1, ..., yn|a, ?, ??, ?). 2
This joint distribution is given in equa-
tion 1.
1The Dirichlet is a probability distribution over the simplex,
and is conjugate to the multinomial (Gelman et al, 2004).
2Note that we use a special end state rather than explicitly
modeling sentence length. Thus the values of m and n depend
on the draw.
1043
(c) For each part-of-speech tag xi in the first
language, emit a word from W : ei ? ?xi ,
(d) For each part-of-speech tag yj in the sec-
ond language, emit a word from W ?: fj ?
??yj .
We define an alignment a to be a set of one-to-
one integer pairs with no crossing edges. Intuitively,
each pair (i, j) ? a indicates that the words ei and
fj share some common role in the bilingual paral-
lel sentences. In our experiments, we assume that
alignments are directly observed and we hold them
fixed. From the perspective of our generative model,
we treat alignments as drawn from a distribution A,
about which we remain largely agnostic. We only
require that A assign zero probability to alignments
which either: (i) align a single index in one language
to multiple indices in the other language or (ii) con-
tain crossing edges. The resulting alignments are
thus one-to-one, contain no crossing edges, and may
be sparse or even possibly empty. Our technique for
obtaining alignments that display these properties is
described in Section 5.
Given an alignment a and sets of transition param-
eters ? and ??, we factor the conditional probability
of a bilingual tag sequence (x1, ...xm), (y1, ..., yn)
into transition probabilities for unaligned tags, and
joint probabilities over aligned tag pairs:
P (x1, ..., xm, y1, ..., yn|a, ?, ??, ?) =
?
unaligned i
?xi?1(xi) ?
?
unaligned j
??yj?1(yj) ?
?
(i,j)?a
P (xi, yj |xi?1, yj?1, ?, ??, ?)
(1)
Because the alignment contains no crossing
edges, we can model the tags as generated sequen-
tially by a stochastic process. We define the dis-
tribution over aligned tag pairs to be a product of
each language?s transition probability and the cou-
pling probability:
P (xi, yj |xi?1, yj?1, ?, ??, ?) =
?xi?1(xi) ??yj?1(yj) ?(xi, yj)
Z (2)
The normalization constant here is defined as:
Z =
?
x,y
?xi?1(x) ??yj?1(y) ?(x, y)
This factorization allows the language-specific tran-
sition probabilities to be shared across aligned and
unaligned tags. In the latter case, the addition of
the coupling parameter ? gives the tag pair an addi-
tional role: that of multilingual anchor. In essence,
the probability of the aligned tag pair is a product
of three experts: the two transition parameters and
the coupling parameter. Thus, the combination of
a high probability transition in one language and a
high probability coupling can resolve cases of inher-
ent transition uncertainty in the other language. In
addition, any one of the three parameters can ?veto?
a tag pair to which it assigns low probability.
To perform inference in this model, we predict
the bilingual tag sequences with maximal probabil-
ity given the observed words and alignments, while
integrating over the transition, emission, and cou-
pling parameters. To do so, we use a combination of
sampling-based techniques.
4 Inference
The core element of our inference procedure is
Gibbs sampling (Geman and Geman, 1984). Gibbs
sampling begins by randomly initializing all unob-
served random variables; at each iteration, each ran-
dom variable zi is sampled from the conditional dis-
tribution P (zi|z?i), where z?i refers to all variables
other than zi. Eventually, the distribution over sam-
ples drawn from this process will converge to the
unconditional joint distribution P (z) of the unob-
served variables. When possible, we avoid explic-
itly sampling variables which are not of direct inter-
est, but rather integrate over them?this technique
is known as ?collapsed sampling,? and can reduce
variance (Liu, 1994).
We sample: (i) the bilingual tag sequences (x,y),
(ii) the two sets of transition parameters ? and ??,
and (iii) the coupling parameter ?. We integrate over
the emission parameters ? and ??, whose priors are
Dirichlet distributions with hyperparameters ?0 and
??0. The resulting emission distribution over words
ei, given the other words e?i, the tag sequences x
1044
and the emission prior ?0, can easily be derived as:
P (ei|x, e?i, ?0) =
?
?xi
?xi(ei)P (?xi |?0) d?xi
= n(xi, ei) + ?0n(xi) + Wxi?0
(3)
Here, n(xi) is the number of occurrences of the
tag xi in x?i, n(xi, ei) is the number of occurrences
of the tag-word pair (xi, ei) in (x?i, e?i), and Wxi
is the number of word types in the vocabulary W
that can take tag xi. The integral is tractable due
to Dirichlet-multinomial conjugacy (Gelman et al,
2004).
We will now discuss, in turn, each of the variables
that we sample. Note that in all cases we condi-
tion on the other sampled variables as well as the
observed words and alignments, e, f and a, which
are kept fixed throughout.
4.1 Sampling Part-of-speech Tags
This section presents the conditional distributions
that we sample from to obtain the part-of-speech
tags. Depending on the alignment, there are several
scenarios. In the simplest case, both the tag to be
sampled and its succeeding tag are not aligned to
any tag in the other language. If so, the sampling
distribution is identical to the monolingual case, in-
cluding only terms for the emission (defined in equa-
tion 3), and the preceding and succeeding transi-
tions:
P (xi|x?i, y, e, f, a, ?, ??, ?, ?0, ??0) ?
P (ei|x, e?i, ?0) ?xi?1(xi) ?xi(xi+1).
For an aligned tag pair (xi, yj), we sample the
identity of the tags jointly. By applying the chain
rule we obtain terms for the emissions in both lan-
guages and a joint term for the transition probabili-
ties:
P (xi, yj |x?i, y?j , e, f, a, ?, ??, ?, ?0, ??0) ?
P (ei|x, e?i, ?0)P (fj |y, f?j , ??0)
P (xi, yj |x?i, y?j , a, ?, ??, ?)
The expansion of the joint term depends on the
alignment of the succeeding tags. In the case that
the successors are not aligned, we have a product of
the bilingual coupling probability and four transition
probabilities (preceding and succeeding transitions
in each language):
P (xi, yj |x?i, y?j , a, ?, ??, ?) ?
?(xi, yj)?xi?1(xi) ??yj?1(yj) ?xi(xi+1) ?
?
yj (yj+1)
Whenever one or more of the succeeding tags is
aligned, the sampling formulas must account for the
effect of the sampled tag on the joint probability
of the succeeding tags, which is no longer a sim-
ple multinomial transition probability. We give the
formula for one such case?when we are sampling
an aligned tag pair (xi, yj), whose succeeding tags
(xi+1, yj+1) are also aligned to one another:
P (xi, yj |x?i, y?j , a, ?, ??, ?) ? ?(xi, yj)
? ?xi?1(xi)??yj?1(yj)
[
?xi(xi+1)??yj (yj+1)
?
x,y ?xi(x)??yj (y)?(x, y)
]
Similar equations can be derived for cases where
the succeeding tags are not aligned to each other, but
to other tags.
4.2 Sampling Transition Parameters and the
Coupling Parameter
When computing the joint probability of an aligned
tag pair (Equation 2), we employ the transition pa-
rameters ?, ?? and the coupling parameter ? in a nor-
malized product. Because of this, we can no longer
regard these parameters as simple multinomials, and
thus can no longer sample them using the standard
closed formulas.
Instead, to resample these parameters, we re-
sort to the Metropolis-Hastings algorithm as a sub-
routine within Gibbs sampling (Hastings, 1970).
Metropolis-Hastings is a Markov chain sampling
technique that can be used when it is impossible to
directly sample from the posterior. Instead, sam-
ples are drawn from a proposal distribution and then
stochastically accepted or rejected on the basis of:
their likelihood, their probability under the proposal
distribution, and the likelihood and proposal proba-
bility of the previous sample.
We use a form of Metropolis-Hastings known as
an independent sampler. In this setup, the proposal
distribution does not depend on the value of the
previous sample, although the accept/reject decision
1045
does depend on the previous model likelihood. More
formally, if we denote the proposal distribution as
Q(z), the target distribution as P (z), and the previ-
ous sample as z, then the probability of accepting a
new sample z? ? Q is set at:
min
{
1, P (z
?) Q(z)
P (z) Q(z?)
}
Theoretically any non-degenerate proposal distri-
bution may be used. However, a higher acceptance
rate and faster convergence is achieved when the
proposal Q is a close approximation of P . For a par-
ticular transition parameter ?x, we define our pro-
posal distribution Q to be Dirichlet with parameters
set to the bigram counts of the tags following x in
the sampled tag data. Thus, the proposal distribu-
tion for ?x has a mean proportional to these counts,
and is thus likely to be a good approximation to the
target distribution.
Likewise for the coupling parameter ?, we de-
fine a Dirichlet proposal distribution. This Dirichlet
is parameterized by the counts of aligned tag pairs
(x, y) in the current set of tag samples. Since this
sets the mean of the proposal to be proportional to
these counts, this too is likely to be a good approxi-
mation to the target distribution.
4.3 Hyperparameter Re-estimation
After every iteration of Gibbs sampling the hyper-
parameters ?0 and ??0 are re-estimated using a single
Metropolis-Hastings move. The proposal distribu-
tion is set to a Gaussian with mean at the current
value and variance equal to one tenth of the mean.
5 Experimental Set-Up
Our evaluation framework follows the standard pro-
cedures established for unsupervised part-of-speech
tagging. Given a tag dictionary (i.e., a set of possi-
ble tags for each word type), the model has to select
the appropriate tag for each token occurring in a text.
We also evaluate tagger performance when only in-
complete dictionaries are available (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007). In both
scenarios, the model is trained only using untagged
text.
In this section, we first describe the parallel data
and part-of-speech annotations used for system eval-
uation. Next we describe a monolingual base-
line and our procedures for initialization and hyper-
parameter setting.
Data As a source of parallel data, we use Orwell?s
novel ?Nineteen Eighty Four? in the original English
as well as translations to three Slavic languages ?
Bulgarian, Serbian and Slovene. This data is dis-
tributed as part of the Multext-East corpus which
is publicly available. The corpus provides detailed
morphological annotation at the world level, includ-
ing part-of-speech tags. In addition a lexicon for
each language is provided.
We obtain six parallel corpora by considering
all pairings of the four languages. We compute
word level alignments for each language pair using
Giza++. To generate one-to-one alignments at the
word level, we intersect the one-to-many alignments
going in each direction and automatically remove
crossing edges in the order in which they appear left
to right. This process results in alignment of about
half the tokens in each bilingual parallel corpus. We
treat the alignments as fixed and observed variables
throughout the training procedure.
The corpus consists of 94,725 English words (see
Table 2). For every language, a random three quar-
ters of the data are used for learning the model while
the remaining quarter is used for testing. In the test
set, only monolingual information is made available
to the model, in order to simulate future performance
on non-parallel data.
Tokens Tags/Token
SR 89,051 1.41
SL 91,724 1.40
BG 80,757 1.34
EN 94,725 2.58
Table 2: Corpus statistics: SR=Serbian, SL=Slovene,
EN=English, BG=Bulgarian
Tagset The Multext-East corpus is manually an-
notated with detailed morphosyntactic information.
In our experiments, we focus on the main syntac-
tic category encoded as a first letter of the labels.
The annotation distinguishes between 13 parts-of-
speech, of which 11 are common for all languages
1046
Random Monolingual Unsupervised Monolingual Supervised Trigram Entropy
EN 56.24 90.71 96.97 1.558
BG 82.68 88.88 96.96 1.708
SL 84.70 87.41 97.31 1.703
SR 83.41 85.05 96.72 1.789
Table 1: Monolingual tagging accuracy for English, Bulgarian, Slovene, and Serbian for two unsupervised baselines
(random tag selection and a Bayesian HMM (Goldwater and Griffiths, 2007)) as well as a supervised HMM. In
addition, the trigram part-of-speech tag entropy is given for each language.
in our experiments.3
In the Multext-East corpus, punctuation marks are
not annotated. We expand the tag repository by
defining a separate tag for all punctuation marks.
This allows the model to make use of any transition
or coupling patterns involving punctuation marks.
We do not consider punctuation tokens when com-
puting model accuracy.
Table 2 shows the tag/token ratio for these lan-
guages. For Slavic languages, we use the tag dic-
tionaries provided with the corpus. For English,
we use a different process for dictionary construc-
tion. Using the original dictionary would result in
the tag/token ratio of 1.5, in comparison to the ra-
tio of 2.3 observed in the Wall Street Journal (WSJ)
corpus. To make our results on English tagging more
comparable to previous benchmarks, we expand the
original dictionary of English tags by merging it
with the tags from the WSJ dictionary. This process
results in a tag/token ratio of 2.58, yielding a slightly
more ambiguous dictionary than the one used in pre-
vious tagging work. 4
Monolingual Baseline As our monolingual base-
line we use the unsupervised Bayesian HMM model
of Goldwater and Griffiths (2007) (BHMM1). This
model modifies the standard HMM by adding pri-
ors and by performing Bayesian inference. Its is in
line with state-of-the-art unsupervised models. This
model is a particulary informative baseline, since
our model reduces to this baseline model when there
are no alignments in the data. This implies that any
performance gain over the baseline can only be at-
3The remaining two tags are Particle and Determiner; The
English tagset does not include Particle while the other three
languages Serbian, Slovene and Bulgarian do not have Deter-
miner in their tagset.
4We couldn?t perform the same dictionary expansion for the
Slavic languages due to a lack of additional annotated resources.
tributed to the multilingual aspect of our model. We
used our own implementation after verifying that its
performance on WSJ was identical to that reported
in (Goldwater and Griffiths, 2007).
Supervised Performance In order to provide a
point of comparison, we also provide supervised re-
sults when an annotated corpus is provided. We use
the standard supervised HMM with Viterbi decod-
ing.
Training and Testing Framework Initially, all
words are assigned tags randomly from their tag
dictionaries. During each iteration of the sam-
pler, aligned tag pairs and unaligned tags are sam-
pled from their respective distributions given in Sec-
tion 4.1 above. The hyperparameters ?0 and ??0 are
initialized with the values learned during monolin-
gual training. They are re-estimated after every iter-
ation of the sampler using the Metropolis Hastings
algorithm. The parameters ? and ?? are initially
set to trigram counts and the ? parameter is set to
tag pair counts of aligned pairs. After every 40 it-
erations of the sampler, a Metropolis Hastings sub-
routine is invoked that re-estimates these parameters
based on the current counts. Overall, the algorithm
is run for 1000 iterations of tag sampling, by which
time the resulting log-likelihood converges to stable
values. Each Metropolis Hastings subroutine sam-
ples 20 values, with an acceptance ratio of around
1/6, in line with the standard recommended values.
After training, trigram and word emission prob-
abilities are computed based on the counts of tags
assigned in the final iteration. For smoothing, the
final sampled values of the hyperparameters are
used. The highest probability tag sequences for each
monolingual test set are then predicted using trigram
Viterbi decoding. We report results averaged over
five complete runs of all experiments.
1047
6 Results
Complete Tag Dictionary In our first experiment,
we assume that a complete dictionary listing the pos-
sible tags for every word is provided in each lan-
guage. Table 1 shows the monolingual results of a
random baseline, an unsupervised Bayesian HMM
and a supervised HMM. Table 3 show the results
of our bilingual models for different language pair-
ings while repeating the monolingual unsupervised
results from Table 1 for easy comparison. The final
column indicates the absolute gain in performance
over this monolingual baseline.
Across all language pairs, the bilingual model
consistently outperforms the monolingual baseline.
All the improvements are statistically significant by
a Fisher sign test at p < 0.05. For some lan-
guage pairs, the gains are quite high. For instance,
the pairing of Serbian and Slovene (two closely re-
lated languages) yields absolute improvements of
6.7 and 7.7 percentage points, corresponding to rel-
ative reductions in error of 51.4% and 53.2%. Pair-
ing Bulgarian and English (two distantly related lan-
guages) also yields large gains: 5.6 and 1.3 percent-
age points, corresponding to relative reductions in
error of 50% and 14%, respectively.5
When we compare the best bilingual result for
each language (Table 3, in bold) to the monolin-
gual supervised results (Table 1), we find that for
all languages the gap between supervised and un-
supervised learning is reduced significantly. For En-
glish, this gap is reduced by 21%. For the Slavic lan-
guages, the supervised-unsupervised gap is reduced
by even larger amounts: 57%, 69%, and 78% for
Serbian, Bulgarian, and Slovene respectively.
While all the languages benefit from the bilin-
gual learning framework, some language combina-
tions are more effective than others. Slovene, for in-
stance, achieves a large improvement when paired
with Serbian (+7.7), a closely related Slavic lan-
guage, but only a minor improvement when coupled
5The accuracy of the monolingual English tagger is rela-
tively high compared to the 87% reported by (Goldwater and
Griffiths, 2007) on the WSJ corpus. We attribute this discrep-
ancy to the slight differences in tag inventory used in our data-
set. For example, when Particles and Prepositions are merged
in the WSJ corpus (as they happen to be in our tag inventory
and corpus), the performance of Goldwater?s model on WSJ is
similar to what we report here.
Entropy Mono- Bilingual Absolute
lingual Gain
EN 0.566 90.71 91.01 +0.30
SR 0.554 85.05 90.06 +5.03
EN 0.578 90.71 92.00 +1.29
BG 0.543 88.88 94.48 +5.61
EN 0.571 90.71 92.01 +1.30
SL 0.568 87.41 88.54 +1.13
SL 0.494 87.41 95.10 +7.69
SR 0.478 85.05 91.75 +6.70
BG 0.568 88.88 91.95 +3.08
SR 0.588 85.05 86.58 +1.53
BG 0.579 88.88 90.91 +2.04
SL 0.609 87.41 88.20 +0.79
Table 3: The tagging accuracy of our bilingual models
on different language pairs, when a full tag dictionary is
provided. The Monolingual Unsupervised results from
Table 1 are repeated for easy comparison. The first col-
umn shows the cross-lingual entropy of a tag when the
tag of the aligned word in the other language is known.
The final column shows the absolute improvement over
the monolingual Bayesian HMM. The best result for each
language is shown in boldface.
with English (+1.3). On the other hand, for Bulgar-
ian, the best performance is achieved when coupling
with English (+5.6) rather than with closely related
Slavic languages (+3.1 and +2.4). As these results
show, an optimal pairing cannot be predicted based
solely on the family connection of paired languages.
To gain a better understanding of this variation
in performance, we measured the internal tag en-
tropy of each language as well as the cross-lingual
tag entropy of language pairs. For the first measure,
we computed the conditional entropy of a tag de-
cision given the previous two tags. Intuitively, this
should correspond to the inherent structural uncer-
tainty of part-of-speech decisions in a language. In
fact, as Table 1 shows, the trigram entropy is a good
indicator of the relative performance of the mono-
lingual baseline. To measure the cross-lingual tag
entropies of language pairs, we considered all bilin-
gual aligned tag pairs, and computed the conditional
entropy of the tags in one language given the tags
in the other language. This measure should indi-
cate the amount of information that one language in
a pair can provide the other. The results of this anal-
1048
Mono- Bilingual Absolute
lingual Gain
EN 63.57 68.22 +4.66
SR 41.14 54.73 +13.59
EN 63.57 71.34 +7.78
BG 53.19 62.55 +9.37
EN 63.57 66.48 +2.91
SL 49.90 53.77 +3.88
SL 49.90 59.68 +9.78
SR 41.14 54.08 +12.94
BG 53.19 54.22 +1.04
SR 41.14 56.91 +15.77
BG 53.19 55.88 +2.70
SL 49.90 58.50 +8.60
Table 4: Tagging accuracy for Bilingual models with re-
duced dictionary: Lexicon entries are available for only
the 100 most frequent words, while all other words be-
come fully ambiguous. The improvement over the mono-
lingual Bayesian HMM trained under similar circum-
stances is shown. The best result for each language is
shown in boldface.
ysis are given in the first column of Table 3. We ob-
serve that the cross-lingual entropy is lowest for the
Serbian and Slovene pair, corresponding with their
large gain in performance. Bulgarian, on the other
hand, has lowest cross-lingual entropy when paired
with English. This corresponds with the fact that
English provides Bulgarian with its largest perfor-
mance gain. In general, we find that the largest per-
formance gain for any language is achieved when
minimizing its cross-lingual entropy.
Reduced Tag Dictionary We also conducted ex-
periments to investigate the impact of the dictio-
nary size on the performance of the bilingual model.
Here, we provide results for the realistic scenario
where only a very small dictionary is present. Ta-
ble 4 shows the performance when a tag dictionary
for the 100 most frequent words is present in each
language. The bilingual model?s results are consis-
tently and significantly better than the monolingual
baseline for all language pairs.
7 Conclusion
We have demonstrated the effectiveness of multilin-
gual learning for unsupervised part-of-speech tag-
ging. The key hypothesis of multilingual learn-
ing is that by combining cues from multiple lan-
guages, the structure of each becomes more appar-
ent. We formulated a hierarchical Bayesian model
for jointly predicting bilingual streams of tags. The
model learns language-specific features while cap-
turing cross-lingual patterns in tag distribution. Our
evaluation shows significant performance gains over
a state-of-the-art monolingual baseline.
Acknowledgments
The authors acknowledge the support of the National
Science Foundation (CAREER grant IIS-0448168 and
grant IIS-0835445) and the Microsoft Research Faculty
Fellowship. Thanks to Michael Collins, Amir Glober-
son, Lillian Lee, Yoong Keok Lee, Maria Polinsky and
the anonymous reviewers for helpful comments and sug-
gestions. Any opinions, findings, and conclusions or rec-
ommendations expressed above are those of the authors
and do not necessarily reflect the views of the NSF.
References
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of the COL-
ING, pages 556?561.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1991. Word-sense dis-
ambiguation using statistical methods. In Proceedings
of the ACL, pages 264?270.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the ACL, pages 263?270.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Proceed-
ings of the ACL, pages 130?137.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of the ACL, pages 255?262.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of new
morpho-syntactically annotated resources. In Pro-
ceedings of LREC, pages 549?554.
Andrew Gelman, John B. Carlin, Hal .S. Stern, and Don-
ald .B. Rubin. 2004. Bayesian data analysis. Chap-
man and Hall/CRC.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions, and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
1049
Dmitriy Genzel. 2005. Inducing a multilingual dictio-
nary from a parallel multitext in related languages. In
Proceedings of HLT/EMNLP, pages 875?882.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. Proceedings of HLT-
NAACL, pages 320?327.
W. K. Hastings. 1970. Monte carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57:97?109.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP/CoNLL,
pages 296?305.
Jun S. Liu. 1994. The collapsed Gibbs sampler in
Bayesian computations with applications to a gene
regulation problem. Journal of the American Statis-
tical Association, 89(427):958?966.
Bernard Merialdo. 1994. Tagging english text with
a probabilistic model. Computational Linguistics,
20(2):155?171.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
an empirical study. In Proceedings of the ACL, pages
455?462.
Sebastian Pado? and Mirella Lapata. 2006. Optimal con-
stituent alignment with edge covers for semantic pro-
jection. In Proceedings of ACL, pages 1161 ? 1168.
Philip Resnik and David Yarowsky. 1997. A perspective
on word sense disambiguation methods and their eval-
uation. In Proceedings of the ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 79?86.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the ACL, pages 354?362.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In Proceedings of the ACL/HLT, pages 737?
745.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian lda-based model for semi-supervised part-
of-speech tagging. In Advances in Neural Information
Processing Systems 20, pages 1521?1528. MIT Press.
Dekai Wu and Hongsing Wong. 1998. Machine trans-
lation with a stochastic grammatical channel. In Pro-
ceedings of the ACL/COLING, pages 1408?1415.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851 ? 858.
David Yarowsky and Grace Ngai. 2001. Inducing mul-
tilingual pos taggers and np bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1?8.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2000. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT, pages 161?168.
1050
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 83?91,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Adding More Languages Improves Unsupervised Multilingual
Part-of-Speech Tagging: A Bayesian Non-Parametric Approach
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{bsnyder, tahira, jacobe, regina}@csail.mit.edu
Abstract
We investigate the problem of unsupervised
part-of-speech tagging when raw parallel data
is available in a large number of languages.
Patterns of ambiguity vary greatly across lan-
guages and therefore even unannotated multi-
lingual data can serve as a learning signal. We
propose a non-parametric Bayesian model that
connects related tagging decisions across lan-
guages through the use of multilingual latent
variables. Our experiments show that perfor-
mance improves steadily as the number of lan-
guages increases.
1 Introduction
In this paper we investigate the problem of unsu-
pervised part-of-speech tagging when unannotated
parallel data is available in a large number of lan-
guages. Our goal is to develop a fully joint multilin-
gual model that scales well and shows improved per-
formance for individual languages as the total num-
ber of languages increases.
Languages exhibit ambiguity at multiple levels,
making unsupervised induction of their underlying
structure a difficult task. However, sources of lin-
guistic ambiguity vary across languages. For exam-
ple, the word fish in English can be used as either a
verb or a noun. In French, however, the noun pois-
son (fish) is entirely distinct from the verbal form
pe?cher (to fish). Previous work has leveraged this
idea by building models for unsupervised learning
from aligned bilingual data (Snyder et al, 2008).
However, aligned data is often available for many
languages. The benefits of bilingual learning vary
markedly depending on which pair of languages is
selected, and without labeled data it is unclear how
to determine which supplementary language is most
helpful. In this paper, we show that it is possi-
ble to leverage all aligned languages simultaneously,
achieving accuracy that in most cases outperforms
even optimally chosen bilingual pairings.
Even in expressing the same meaning, languages
take different syntactic routes, leading to variation
in part-of-speech sequences. Therefore, an effec-
tive multilingual model must accurately model com-
mon linguistic structure, yet remain flexible to the
idiosyncrasies of each language. This tension only
becomes stronger as additional languages are added
to the mix. From a computational standpoint, the
main challenge is to ensure that the model scales
well as the number of languages increases. Care
must be taken to avoid an exponential increase in
the parameter space as well as the time complexity
of inference procedure.
We propose a non-parametric Bayesian model for
joint multilingual tagging. The topology of our
model connects tagging decisions within a language
as well as across languages. The model scales lin-
early with the number of languages, allowing us to
incorporate as many as are available. For each lan-
guage, the model contains an HMM-like substruc-
ture and connects these substructures to one another
by means of cross-lingual latent variables. These
variables, which we refer to as superlingual tags,
capture repeated multilingual patterns and thus re-
duce the overall uncertainty in tagging decisions.
We evaluate our model on a parallel corpus of
eight languages. The model is trained once using all
83
languages, and its performance is tested separately
for each on a held-out monolingual test set. When a
complete tag lexicon is provided, our unsupervised
model achieves an average accuracy of 95%, in com-
parison to 91% for an unsupervised monolingual
Bayesian HMM and 97.4% for its supervised coun-
terpart. Thus, on average, the gap between unsu-
pervised and supervised monolingual performance
is cut by nearly two thirds. We also examined sce-
narios where the tag lexicon is reduced in size. In
all cases, the multilingual model yielded substantial
performance gains. Finally, we examined the per-
formance of our model when trained on all possible
subsets of the eight languages. We found that perfor-
mance improves steadily as the number of available
languages increases.
2 Related Work
Bilingual Part-of-Speech Tagging Early work on
multilingual tagging focused on projecting annota-
tions from an annotated source language to a target
language (Yarowsky and Ngai, 2001; Feldman et al,
2006). In contrast, we assume no labeled data at
all; our unsupervised model instead symmetrically
improves performance for all languages by learning
cross-lingual patterns in raw parallel data. An addi-
tional distinction is that projection-based work uti-
lizes pairs of languages, while our approach allows
for continuous improvement as languages are added
to the mix.
In recent work, Snyder et al (2008) presented
a model for unsupervised part-of-speech tagging
trained from a bilingual parallel corpus. This bilin-
gual model and the model presented here share a
number of similarities: both are Bayesian graphi-
cal models building upon hidden Markov models.
However, the bilingual model explicitly joins each
aligned word-pair into a single coupled state. Thus,
the state-space of these joined nodes grows exponen-
tially in the number of languages. In addition, cross-
ing alignments must be removed so that the result-
ing graph structure remains acyclic. In contrast, our
multilingual model posits latent cross-lingual tags
without explicitly joining or directly connecting the
part-of-speech tags across languages. Besides per-
mitting crossing alignments, this structure allows the
model to scale gracefully with the number of lan-
guages.
Beyond Bilingual Learning While most work on
multilingual learning focuses on bilingual analysis,
some models operate on more than one pair of lan-
guages. For instance, Genzel (2005) describes a
method for inducing a multilingual lexicon from
a group of related languages. His model first in-
duces bilingual models for each pair of languages
and then combines them. Our work takes a different
approach by simultaneously learning from all lan-
guages, rather than combining bilingual results.
A related thread of research is multi-source ma-
chine translation (Och and Ney, 2001; Utiyama and
Isahara, 2006; Cohn and Lapata, 2007) where the
goal is to translate from multiple source languages to
a single target language. Rather than jointly training
all the languages together, these models train bilin-
gual models separately, and then use their output to
select a final translation. The selection criterion can
be learned at training time since these models have
access to the correct translation. In unsupervised set-
tings, however, we do not have a principled means
for selecting among outputs of different bilingual
models. By developing a joint multilingual model
we can automatically achieve performance that ri-
vals that of the best bilingual pairings.
3 Model
We propose a non-parametric directed Bayesian
graphical model for multilingual part-of-speech tag-
ging using a parallel corpus. We perform a joint
training pass over the corpus, and then apply the
parameters learned for each language to a held-out
monolingual test set.
The core idea of our model is that patterns of
ambiguity vary across languages and therefore even
unannotated multilingual data can serve as a learn-
ing signal. Our model is able to simultaneously har-
ness this signal from all languages present in the
corpus. This goal is achieved by designing a sin-
gle graphical model that connects tagging decisions
within a language as well as across languages.
The model contains language-specific HMM sub-
structures connected to one another by cross-lingual
latent variables spanning two or more languages.
These variables, which we refer to as superlingual
tags, capture repeated cross-lingual patterns and
84
I l o v e f i s h J ? a d o r e l e s p o i s s o n
a n i o h e v d a g i m M u j h e m a c h c h l i p a s a n d h a i
Figure 1: Model structure for parallel sentences in English, French, Hebrew, and Urdu. In this example, there are three
superlingual tags, each connected to the part-of-speech tag of a word in each of the four languages.
thus reduce the overall uncertainty in tagging deci-
sions. To encourage the discovery of a compact set
of such cross-lingual patterns, we place a Dirichlet
process prior on the superlingual tag values.
3.1 Model Structure
For each language, our model includes an HMM-
like substructure with observed word nodes, hid-
den part-of-speech nodes, and directed transition
and emission edges. For each set of aligned words
in parallel sentences, we add a latent superlingual
variable to capture the cross-lingual context. A set
of directed edges connect this variable to the part-
of-speech nodes of the aligned words. Our model
assumes that the superlingual tags for parallel sen-
tences are unordered and are drawn independently
of one another.
Edges radiate outward from superlingual tags to
language-specific part-of-speech nodes. Thus, our
model implicitly assumes that superlingual tags are
drawn prior to the part-of-speech tags of all lan-
guages and probabilistically influence their selec-
tion. See Figure 1 for an example structure.
The particular model structure for each set of par-
allel sentences (i.e. the configuration of superlingual
tags and their edges) is determined by bilingual lexi-
cal alignments and ? like the text itself ? is consid-
ered an observed variable. In practice, these lexical
alignments are obtained using standard techniques
from machine translation.
Our model design has several benefits. Crossing
and many-to-many alignments may be used with-
out creating cycles in the graph, as all cross-lingual
information emanates from the hidden superlingual
tags. Furthermore, the model scales gracefully with
the number of languages, as the number of new
edges and nodes will be proportional to the number
of words for each additional language.
3.2 Superlingual Tags
Each superlingual tag value specifies a set of dis-
tributions ? one for each language?s part-of-speech
tagset. In order to learn repeated cross-lingual pat-
terns, we need to constrain the number of superlin-
gual tag values and thus the number of distributions
they provide. For example, we might allow the su-
perlingual tags to take on integer values from 1 to
K, with each integer value indexing a separate set
of distributions. Each set of distributions should cor-
respond to a discovered cross-lingual pattern in the
data. For example, one set of distributions might fa-
vor nouns in each language and another might favor
verbs.
Rather than fixing the number of superlingual
tag values to an arbitrary and predetermined size
1, . . . ,K, we allow them to range over the entire set
of integers. In order to encourage the desired multi-
lingual clustering behavior, we use a Dirichlet pro-
cess prior for the superlingual tags. This prior allows
high posterior probability only when a small number
85
of values are used repeatedly. The actual number of
sampled values will be dictated by the data and the
number of languages.
More formally, suppose we have n lan-
guages, ?1, . . . , ?n. According to our genera-
tive model, a countably infinite sequence of sets
???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ?, . . . is drawn from
some base distribution. Each ??i is a distribution
over the parts-of-speech in language ?.
In parallel, an infinite sequence of mixing compo-
nents ?1, ?2, . . . is drawn from a stick-breaking pro-
cess (Sethuraman, 1994). These components define
a distribution over the integers with most probabil-
ity mass placed on some initial set of values. The
two sequences ???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ? . . .
and ?1, ?2 . . . now define the distribution over su-
perlingual tags and their associated distributions on
parts-of-speech. That is, each superlingual tag z ?
N is drawn with probability ?z , and indexes the set
of distributions ???1z , . . . , ??nz ?.
3.3 Part-of-Speech Tags
Finally, we need to define the generative probabili-
ties of the part-of-speech nodes. For each such node
there may be multiple incoming edges. There will
always be an incoming transition edge from the pre-
vious tag (in the same language). In addition, there
may be incoming edges from zero or more superlin-
gual tags. Each edge carries with it a distribution
over parts-of-speech and these distributions must be
combined into the single distribution from which the
tag is ultimately drawn.
We choose to combine these distributions as a
product of experts. More formally: for language ?
and tag position i, the part-of-speech tag yi is drawn
according to
yi ?
?yi?1(yi)
?
z ??z(yi)
Z (1)
Where ?yi?1 indicates the transition distribution,
and the z?s range over the values of the incoming
superlingual tags. The normalization term Z is ob-
tained by summing the numerator over all part-of-
speech tags yi in the tagset.
This parameterization allows for a relatively sim-
ple and small parameter space. It also leads to a
desirable property: for a tag to have high probabil-
ity each of the incoming distributions must allow it.
That is, any expert can ?veto? a potential tag by as-
signing it low probability, generally leading to con-
sensus decisions.
We now formalize this description by giving the
stochastic process by which the observed data (raw
parallel text) is generated, according to our model.
3.4 Generative Process
For n languages, we assume the existence of n
tagsets T 1, . . . , Tn and vocabularies, W 1, . . . ,Wn,
one for each language. For clarity, the generative
process is described using only bigram transition
dependencies, but our experiments use a trigram
model.
1. Transition and Emission Parameters: For
each language ? and for each tag t ? T ?, draw
a transition distribution ??t over tags T? and
an emission distribution ??t over words W ?, all
from symmetric Dirichlet priors of appropriate
dimension.
2. Superlingual Tag Parameters:
Draw an infinite sequence of sets
???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ?, . . . from
base distribution G0. Each ??i is a distribution
over the tagset T ?. The base distribution G0 is
a product of n symmetric Dirichlets, where the
dimension of the ith such Dirichlet is the size
of the corresponding tagset T ?i .
At the same time, draw an infinite sequence
of mixture weights ? ? GEM(?), where
GEM(?) indicates the stick-breaking distribu-
tion (Sethuraman, 1994), and ? = 1. These
parameters together define a prior distribution
over superlingual tags,
p(z) =
??
k
?k?k=z, (2)
or equivalently over the part-of-speech distri-
butions ???1 , . . . , ??n? that they index:
??
k
?k????1k ,...,??nk ?=???1 ,...,??n ?. (3)
In both cases, ?v=v? is defined as one when
v = v? and zero otherwise. Distribution 3 is
said to be drawn from a Dirichlet process, con-
ventionally written as DP (?,G0).
86
3. Data: For each multilingual parallel sentence,
(a) Draw an alignment a specifying sets of
aligned indices across languages. Each
such set may consist of indices in any sub-
set of the languages. We leave the distri-
bution over alignments undefined, as we
consider alignments observed variables.
(b) For each set of indices in a, draw a super-
lingual tag value z according to Distribu-
tion 2.
(c) For each language ?, for i = 1, . . . (until
end-tag reached):
i. Draw a part-of-speech tag yi ? T ? ac-
cording to Distribution 1
ii. Draw a word wi ? W ? according to
the emission distribution ?yi .
To perform Bayesian inference under this model
we use a combination of sampling techniques, which
we describe in detail in the next section.
3.5 Inference
Ideally we would like to predict the part-of-speech
tags which have highest marginal probability given
the observed words x and alignments a. More
specifically, since we are evaluating our accuracy per
tag-position, we would like to predict, for language
index ? and word index i, the single part-of-speech
tag:
argmax
t?T ?
P
(
y?i = t
??x,a)
which we can rewrite as the argmaxt?T ? of the inte-
gral,
? [
P
(
y?i = t
???y?(?,i),?,?, z,?,x,a
)
?
P
(
y?(?,i),?,?, z,pi,?,
???x,a
)]
dy?(?,i) d? d? dz dpi d?,
in which we marginalize over the settings of all
tags other than y?i (written as y?(?,i)), the tran-
sition distributions ? = {??t?
}
, emission distri-
butions ? = {??t?
}
, superlingual tags z, and su-
perlingual tag parameters pi = {?1, ?2, . . .} and
? = {???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ? . . .} (where t?
ranges over all part-of-speech tags).
As these integrals are intractable to compute ex-
actly, we resort to the standard Monte Carlo approx-
imation. We collect N samples of the variables over
which we wish to marginalize but for which we can-
not compute closed-form integrals, where each sam-
ple samplek is drawn from P (samplek|x,a). We
then approximate the tag marginals as:
P
(
y?i = t
??x,a) ?
?
k P
(
y?i = t
??samplek,x,a
)
N (4)
We employ closed forms for integrating out the
emission parameters ?, transition parameters ?, and
superlingual tag parameters pi and ?. We explic-
itly sample only part-of-speech tags y, superlingual
tags z, and the hyperparameters of the transition and
emission Dirichlet priors. To do so, we apply stan-
dard Markov chain sampling techniques: a Gibbs
sampler for the tags and a within-Gibbs Metropolis-
Hastings subroutine for the hyperparameters (Hast-
ings, 1970).
Our Gibbs sampler samples each part-of-speech
and superlingual tag separately, conditioned on the
current value of all other tags. In each case, we use
standard closed forms to integrate over all parameter
values, using currently sampled counts and hyperpa-
rameter pseudo-counts. We note that conjugacy is
technically broken by our use of a product form in
Distribution 1. Nevertheless, we consider the sam-
pled tags to have been generated separately by each
of the factors involved in the numerator. Thus our
method of using count-based closed forms should be
viewed as an approximation.
3.6 Sampling Part-of-Speech Tags
To sample the part-of-speech tag for language ? at
position i we draw from
P (y?i |y?(?,i),x,a, z) ?
P (y?i+1|y?i ,y?(?,i),a, z) P (y?i |y?(?,i),a, z)?
P (x?i |x??i,y?) ,
where the first two terms are the generative proba-
bilities of (i) the current tag given the previous tag
and superlingual tags, and (ii) the next tag given the
current tag and superlingual tags. These two quan-
tities are similar to Distribution 1, except here we
integrate over the transition parameter ?yi?1 and the
superlingual tag parameters ??z . We end up with a
product of integrals. Each integral can be computed
in closed form using multinomial-Dirichlet conju-
gacy (and by making the above-mentioned simpli-
fying assumption that all other tags were gener-
ated separately by their transition and superlingual
87
parameters), just as in the monolingual Bayesian
HMM of (Goldwater and Griffiths, 2007).
For example, the closed form for integrating over
the parameter of a superlingual tag with value z is
given by:
?
??z(yi)P (??z|?0)d??z = count(z, yi, ?) + ?0count(z, ?) + T ??0
where count(z, yi, ?) is the number of times that tag
yi is observed together with superlingual tag z in
language ?, count(z, ?) is the total number of times
that superlingual tag z appears with an edge into lan-
guage ?, and ?0 is a hyperparameter.
The third term in the sampling formula is the
emission probability of the current word x?i given
the current tag and all other words and sampled tags,
as well as a hyperparameter which is suppressed for
the sake of clarity. This quantity can be computed
exactly in closed form in a similar way.
3.7 Sampling Superlingual Tags
For each set of aligned words in the observed align-
ment a we need to sample a superlingual tag z.
Recall that z is an index into an infinite sequence
???11 , . . . , ??n1 ?, ???12 , . . . , ??n2 ? . . ., where each ??z is
a distribution over the tagset T ?. The generative dis-
tribution over z is given by equation 2. In our sam-
pling scheme, however, we integrate over all possi-
ble settings of the mixing components pi using the
standard Chinese Restaurant Process (CRP) closed
form (Antoniak, 1974):
P
(
zi
??z?i,y
)
?
?
?
P
(
y?i
??z,y?(?,i)
)
?
{
1
k+?count(zi) if zi ? z?i
?
k+? otherwise
The first term is the product of closed form tag prob-
abilities of the aligned words, given z. The final term
is the standard CRP closed form for posterior sam-
pling from a Dirichlet process prior. In this term,
k is the total number of sampled superlingual tags,
count(zi) is the total number of times the value zi
occurs in the sampled tags, and ? is the Dirichlet
process concentration parameter (see Step 2 in Sec-
tion 3.4).
Finally, we perform standard hyperparameter re-
estimation for the parameters of the Dirichlet distri-
bution priors on ? and ? (the transition and emis-
sion distributions) using Metropolis-Hastings. We
assume an improper uniform prior and use a Gaus-
sian proposal distribution with mean set to the pre-
vious value, and variance to one-tenth of the mean.
4 Experimental Setup
We test our model in an unsupervised framework
where only raw parallel text is available for each
of the languages. In addition, we assume that for
each language a tag dictionary is available that cov-
ers some subset of words in the text. The task is to
learn an independent tagger for each language that
can annotate non-parallel raw text using the learned
parameters. All reported results are on non-parallel
monolingual test data.
Data For our experiments we use the Multext-
East parallel corpus (Erjavec, 2004) which has been
used before for multilingual learning (Feldman et
al., 2006; Snyder et al, 2008). The tagged portion of
the corpus includes a 100,000 word English text, Or-
well?s novel ?Nineteen Eighty Four?, and its trans-
lation into seven languages: Bulgarian, Czech, Es-
tonian, Hungarian, Romanian, Slovene and Serbian.
The corpus also includes a tag lexicon for each of
these languages. We use the first 3/4 of the text for
learning and the last 1/4 as held-out non-parallel test
data.
The corpus provides sentence level alignments.
To obtain word level alignments, we run GIZA++
(Och and Ney, 2003) on all 28 pairings of the 8 lan-
guages. Since we want each latent superlingual vari-
able to span as many languages as possible, we ag-
gregate the pairwise lexical alignments into larger
sets of aligned words. These sets of aligned words
are generated as a preprocessing step. During sam-
pling they remain fixed and are treated as observed
data.
We use the set of 14 basic part-of-speech tags pro-
vided by the corpus. In our first experiment, we
assume that a complete tag lexicon is available, so
that for each word, its set of possible parts-of-speech
is known ahead of time. In this setting, the aver-
age number of possible tags per token is 1.39. We
also experimented with incomplete tag dictionaries,
where entries are only available for words appearing
more than five or ten times in the corpus. For other
words, the entire tagset of 14 tags is considered. In
these two scenarios, the average per-token tag ambi-
88
Lexicon: Full Lexicon: Frequency > 5 Lexicon: Frequency > 10
MONO
BI
MULTI MONO
BI
MULTI MONO
BI
MULTI
AVG BEST AVG BEST AVG BEST
BG 88.8 91.3 94.7 92.6 73.5 80.2 82.7 81.3 71.9 77.8 80.2 78.8
CS 93.7 97.0 97.7 98.2 72.2 79.0 79.7 83.0 66.7 75.3 76.7 79.4
EN 95.8 95.9 96.1 95.0 87.3 90.4 90.7 88.1 84.4 88.8 89.4 86.1
ET 92.5 93.4 94.3 94.6 72.5 76.5 77.5 80.6 68.3 72.9 74.9 77.9
HU 95.3 96.8 96.9 96.7 73.5 77.3 78.0 80.8 69.0 73.8 75.2 76.4
RO 90.1 91.8 94.0 95.1 77.1 82.7 84.4 86.1 73.0 80.5 82.1 83.1
SL 87.4 89.3 94.8 95.8 75.7 78.7 80.9 83.6 70.4 76.1 77.6 80.0
SR 84.5 90.2 94.5 92.3 66.3 75.9 79.4 78.8 63.7 72.4 76.1 75.9
Avg. 91.0 93.2 95.4 95.0 74.7 80.1 81.7 82.8 70.9 77.2 79.0 79.7
Table 1: Tagging accuracy for Bulgarian, Czech, English, Estonian, Hungarian, Romanian, Slovene, and Serbian. In
the first scenario, a complete tag lexicon is available for all the words. In the other two scenarios the tag lexicon
only includes words that appear more than five or ten times. Results are given for a monolingual Bayesian HMM
(Goldwater and Griffiths, 2007), a bilingual model (Snyder et al, 2008), and the multilingual model presented here.
In the case of the bilingual model, we present both the average accuracy over all pairings as well as the result from the
best performing pairing for each language. The best results for each language in each scenario are given in boldface.
guity is 4.65 and 5.58, respectively.
Training and testing In the full lexicon ex-
periment, each word is initialized with a random
part-of-speech tag from its dictionary entry. In the
two reduced lexicon experiments, we initialize the
tags with the result of our monolingual baseline (see
below) to reduce sampling time. In both cases,
we begin with 14 superlingual tag values ? corre-
sponding to the parts-of-speech ? and initially as-
sign them based on the most common initial part-of-
speech of words in each alignment.
We run our Gibbs sampler for 1,000 iterations,
and store the conditional tag probabilities for the last
100 iterations. We then approximate marginal tag
probabilities on the training data using Equation 4
and predict the highest probability tags. Finally, we
compute maximum likelihood transition and emis-
sion probabilities using these tag counts, and then
apply smoothed viterbi decoding to each held-out
monolingual test set. All reported results are aver-
aged over five runs of the sampler.
Monolingual and bilingual baselines We
reimplemented the Bayesian HMM model of Gold-
water and Griffiths (2007) (BHMM1) as our mono-
lingual baseline. It has a standard HMM structure
with conjugate Bayesian priors over transitions and
emissions. We note that our model, in the absence
of any superlingual tags, reduces to this Bayesian
HMM. As an additional baseline we use a bilingual
model (Snyder et al, 2008). It is a directed graphical
model that jointly tags two parallel streams of text
aligned at the word level. The structure of the model
consists of two parallel HMMs, one for each lan-
guage. The aligned words form joint nodes that are
shared by both HMMs. These joint nodes are sam-
pled from a probability distribution that is a prod-
uct of the transition and emission distributions in the
two languages and a coupling distribution.
We note that the numbers reported here for
the bilingual model differ slightly from those re-
ported by Snyder et al (2008) for two reasons: we
use a slightly larger set of sentences, and an im-
proved sampling scheme. The new sampling scheme
marginalizes over the transition and coupling param-
eters by using the same count-based approximation
that we utilize for our multilingual model. This leads
to higher performance, and thus a stronger baseline.1
5 Results
Table 1 shows the tagging accuracy of our multilin-
gual model on the test data, when training is per-
formed on all eight languages together. Results from
both baselines are also reported. In the case of the
bilingual baseline, seven pairings are possible for
each language, and the results vary by pair. There-
1Another difference is that we use the English lexicon pro-
vided with the Multext-East corpus, whereas (Snyder et al,
2008) augment this lexicon with tags found in WSJ.
89
fore, for each language, we present the average accu-
racy over all seven pairings, as well as the accuracy
of its highest performing pairing.
We provide results for three scenarios. In the first
case, a tag dictionary is provided for all words, lim-
iting them to a restricted set of possible tags. In the
other two scenarios, dictionary entries are limited to
words that appear more than five or ten times in the
corpus. All other words can be assigned any tag,
increasing the overall difficulty of the task. In the
full lexicon scenario, our model achieves an average
tagging accuracy of 95%, compared to 91% for the
monolingual baseline and 93.2% for the bilingual
baseline when averaged over all pairings. This ac-
curacy (95%) comes close to the performance of the
bilingual model when the best pairing for each lan-
guage is chosen by an oracle (95.4%). This demon-
strates that our multilingual model is able to effec-
tively learn from all languages. In the two reduced
lexicon scenarios, the gains are even more striking.
In both cases the average multilingual performance
outpaces even the best performing pairs.
Looking at individual languages, we see that in
all three scenarios, Czech, Estonian, Romanian, and
Slovene show their best performance with the mul-
tilingual model. Bulgarian and Serbian, on the
other hand, give somewhat better performance with
their optimal pairings under the bilingual model, but
their multilingual performance remains higher than
their average bilingual results. The performance of
English under the multilingual model is somewhat
lower, especially in the full lexicon scenario, where
it drops below monolingual performance. One pos-
sible explanation for this decrease lies in the fact that
English, by far, has the lowest trigram tag entropy of
all eight languages (Snyder et al, 2008). It is pos-
sible, therefore, that the signal it should be getting
from its own transitions is being drowned out by less
reliable information from other languages.
In order to test the performance of our model as
the number of languages increases, we ran the full
lexicon experiment with all possible subsets of the
eight languages. Figure 2 plots the average accuracy
as the number of languages varies. For comparison,
the monolingual and average bilingual baseline re-
sults are given, along with supervised monolingual
performance. Our multilingual model steadily gains
in accuracy as the number of available languages in-
Figure 2: Performance of the multilingual model as the
number of languages varies. Performance of the mono-
lingual and average bilingual baselines as well as a su-
pervised monolingual performance are given for compar-
ison.
creases. Interestingly, it even outperforms the bilin-
gual baseline (by a small margin) when only two lan-
guages are available, which may be attributable to
the more flexible non-parametric dependencies em-
ployed here. Finally, notice that the gap between
monolingual supervised and unsupervised perfor-
mance is cut by nearly two thirds under the unsu-
pervised multilingual model.
6 Conclusion
In this paper we?ve demonstrated that the benefits of
unsupervised multilingual learning increase steadily
with the number of available languages. Our model
scales gracefully as languages are added and effec-
tively incorporates information from them all, lead-
ing to substantial performance gains. In one experi-
ment, we cut the gap between unsupervised and su-
pervised performance by nearly two thirds. A fu-
ture challenge lies in incorporating constraints from
additional languages even when parallel text is un-
available.
Acknowledgments
The authors acknowledge the support of the National Sci-
ence Foundation (CAREER grant IIS-0448168 and grant IIS-
0835445). Thanks to Tommi Jaakkola and members of the
MIT NLP group for helpful discussions. Any opinions, find-
ings, or recommendations expressed above are those of the au-
thors and do not necessarily reflect the views of the NSF.
90
References
C. E. Antoniak. 1974. Mixtures of Dirichlet processes
with applications to Bayesian nonparametric prob-
lems. The Annals of Statistics, 2:1152?1174, Novem-
ber.
Trevor Cohn and Mirella Lapata. 2007. Machine trans-
lation by triangulation: Making effective use of multi-
parallel corpora. In Proceedings of ACL.
T. Erjavec. 2004. MULTEXT-East version 3: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Fourth International Conference on Lan-
guage Resources and Evaluation, LREC, volume 4,
pages 1535?1538.
Anna Feldman, Jirka Hana, and Chris Brew. 2006.
A cross-language approach to rapid creation of new
morpho-syntactically annotated resources. In Pro-
ceedings of LREC, pages 549?554.
Dmitriy Genzel. 2005. Inducing a multilingual dictio-
nary from a parallel multitext in related languages. In
Proceedings of the HLT/EMNLP, pages 875?882.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully Bayesian approach to unsupervised part-of-
speech tagging. In Proceedings of the ACL, pages
744?751.
W. K. Hastings. 1970. Monte Carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57:97?109.
Franz Josef Och and Hermann Ney. 2001. Statistical
multi-source translation. In MT Summit 2001, pages
253?258.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
J. Sethuraman. 1994. A constructive definition of Dirich-
let priors. Statistica Sinica, 4:639?650.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein, and
Regina Barzilay. 2008. Unsupervised multilingual
learning for POS tagging. In Proceedings of the
EMNLP, pages 1041?1050.
Masao Utiyama and Hitoshi Isahara. 2006. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Proceedings of NAACL/HLT,
pages 484?491.
David Yarowsky and Grace Ngai. 2001. Inducing multi-
lingual POS taggers and NP bracketers via robust pro-
jection across aligned corpora. In Proceedings of the
NAACL, pages 1?8.
91
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 73?81,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Unsupervised Multilingual Grammar Induction
Benjamin Snyder, Tahira Naseem, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{bsnyder, tahira, regina}@csail.mit.edu
Abstract
We investigate the task of unsupervised
constituency parsing from bilingual par-
allel corpora. Our goal is to use bilin-
gual cues to learn improved parsing mod-
els for each language and to evaluate these
models on held-out monolingual test data.
We formulate a generative Bayesian model
which seeks to explain the observed par-
allel data through a combination of bilin-
gual and monolingual parameters. To this
end, we adapt a formalism known as un-
ordered tree alignment to our probabilistic
setting. Using this formalism, our model
loosely binds parallel trees while allow-
ing language-specific syntactic structure.
We perform inference under this model us-
ing Markov Chain Monte Carlo and dy-
namic programming. Applying this model
to three parallel corpora (Korean-English,
Urdu-English, and Chinese-English) we
find substantial performance gains over
the CCM model, a strong monolingual
baseline. On average, across a variety of
testing scenarios, our model achieves an
8.8 absolute gain in F-measure. 1
1 Introduction
In this paper we investigate the task of unsuper-
vised constituency parsing when bilingual paral-
lel text is available. Our goal is to improve pars-
ing performance on monolingual test data for each
language by using unsupervised bilingual cues at
training time. Multilingual learning has been suc-
cessful for other linguistic induction tasks such as
lexicon acquisition, morphological segmentation,
and part-of-speech tagging (Genzel, 2005; Snyder
and Barzilay, 2008; Snyder et al, 2008; Snyder
1Code and the outputs of our experiments are available at
http://groups.csail.mit.edu/rbg/code/multiling induction.
et al, 2009). We focus here on the unsupervised
induction of unlabeled constituency brackets. This
task has been extensively studied in a monolingual
setting and has proven to be difficult (Charniak
and Carroll, 1992; Klein and Manning, 2002).
The key premise of our approach is that am-
biguous syntactic structures in one language may
correspond to less uncertain structures in the other
language. For instance, the English sentence I
saw [the student [from MIT]] exhibits the classic
problem of PP-attachment ambiguity. However,
its Urdu translation, literally glossed as I [[MIT of ]
student] saw, uses a genitive phrase that may only
be attached to the adjacent noun phrase. Know-
ing the correspondence between these sentences
should help us resolve the English ambiguity.
One of the main challenges of unsupervised
multilingual learning is to exploit cross-lingual
patterns discovered in data, while still allowing
a wide range of language-specific idiosyncrasies.
To this end, we adapt a formalism known as un-
ordered tree alignment (Jiang et al, 1995) to
a probabilistic setting. Under this formalism,
any two trees can be embedded in an alignment
tree. This alignment tree allows arbitrary parts
of the two trees to diverge in structure, permitting
language-specific grammatical structure to be pre-
served. Additionally, a computational advantage
of this formalism is that the marginalized probabil-
ity over all possible alignments for any two trees
can be efficiently computed with a dynamic pro-
gram in linear time.
We formulate a generative Bayesian model
which seeks to explain the observed parallel data
through a combination of bilingual and mono-
lingual parameters. Our model views each pair
of sentences as having been generated as fol-
lows: First an alignment tree is drawn. Each
node in this alignment tree contains either a soli-
tary monolingual constituent or a pair of coupled
bilingual constituents. For each solitary mono-
73
lingual constituent, a sequence of part-of-speech
tags is drawn from a language-specific distribu-
tion. For each pair of coupled bilingual con-
stituents, a pair of part-of-speech sequences are
drawn jointly from a cross-lingual distribution.
Word-level alignments are then drawn based on
the tree alignment. Finally, parallel sentences are
assembled from these generated part-of-speech se-
quences and word-level alignments.
To perform inference under this model, we use
a Metropolis-Hastings within-Gibbs sampler. We
sample pairs of trees and then compute marginal-
ized probabilities over all possible alignments us-
ing dynamic programming.
We test the effectiveness of our bilingual gram-
mar induction model on three corpora of parallel
text: English-Korean, English-Urdu and English-
Chinese. The model is trained using bilingual
data with automatically induced word-level align-
ments, but is tested on purely monolingual data
for each language. In all cases, our model out-
performs a state-of-the-art baseline: the Con-
stituent Context Model (CCM) (Klein and Man-
ning, 2002), sometimes by substantial margins.
On average, over all the testing scenarios that we
studied, our model achieves an absolute increase
in F-measure of 8.8 points, and a 19% reduction
in error relative to a theoretical upper bound.
2 Related Work
The unsupervised grammar induction task has
been studied extensively, mostly in a monolin-
gual setting (Charniak and Carroll, 1992; Stolcke
and Omohundro, 1994; Klein and Manning, 2002;
Seginer, 2007). While PCFGs perform poorly on
this task, the CCM model (Klein and Manning,
2002) has achieved large gains in performance and
is among the state-of-the-art probabilistic models
for unsupervised constituency parsing. We there-
fore use CCM as our basic model of monolingual
syntax.
While there has been some previous work on
bilingual CFG parsing, it has mainly focused on
improving MT systems rather than monolingual
parsing accuracy. Research in this direction was
pioneered by (Wu, 1997), who developed Inver-
sion Transduction Grammars to capture cross-
lingual grammar variations such as phrase re-
orderings. More general formalisms for the same
purpose were later developed (Wu and Wong,
1998; Chiang, 2005; Melamed, 2003; Eisner,
2003; Zhang and Gildea, 2005; Blunsom et al,
2008). We know of only one study which eval-
uates these bilingual grammar formalisms on the
task of grammar induction itself (Smith and Smith,
2004). Both our model and even the monolingual
CCM baseline yield far higher performance on the
same Korean-English corpus.
Our approach is closer to the unsupervised
bilingual parsing model developed by Kuhn
(2004), which aims to improve monolingual per-
formance. Assuming that trees induced over paral-
lel sentences have to exhibit certain structural reg-
ularities, Kuhn manually specifies a set of rules
for determining when parsing decisions in the two
languages are inconsistent with GIZA++ word-
level alignments. By incorporating these con-
straints into the EM algorithm he was able to im-
prove performance over a monolingual unsuper-
vised PCFG. Still, the performance falls short of
state-of-the-art monolingual models such as the
CCM.
More recently, there has been a body of work
attempting to improve parsing performance by ex-
ploiting syntactically annotated parallel data. In
one strand of this work, annotations are assumed
only in a resource-rich language and are projected
onto a resource-poor language using the parallel
data (Hwa et al, 2005; Xi and Hwa, 2005). In
another strand of work, syntactic annotations are
assumed on both sides of the parallel data, and a
model is trained to exploit the parallel data at test
time as well (Smith and Smith, 2004; Burkett and
Klein, 2008). In contrast to this work, our goal
is to explore the benefits of multilingual grammar
induction in a fully unsupervised setting.
We finally note a recent paper which uses pa-
rameter tying to improve unsupervised depen-
dency parse induction (Cohen and Smith, 2009).
While the primary performance gains occur when
tying related parameters within a language, some
additional benefit is observed through bilingual ty-
ing, even in the absence of a parallel corpus.
3 Model
We propose an unsupervised Bayesian model for
learning bilingual syntactic structure using paral-
lel corpora. Our key premise is that difficult-to-
learn syntactic structures of one language may cor-
respond to simpler or less uncertain structures in
the other language. We treat the part-of-speech
tag sequences of parallel sentences, as well as their
74
(i) (ii) (iii)
Figure 1: A pair of trees (i) and two possible alignment trees. In (ii), no empty spaces are inserted, but
the order of one of the original tree?s siblings has been reversed. In (iii), only two pairs of nodes have
been aligned (indicated by arrows) and many empty spaces inserted.
word-level alignments, as observed data. We ob-
tain these word-level alignments using GIZA++
(Och and Ney, 2003).
Our model seeks to explain this observed data
through a generative process whereby two aligned
parse trees are produced jointly. Though they
are aligned, arbitrary parts of the two trees are
permitted to diverge, accommodating language-
specific grammatical structure. In effect, our
model loosely binds the two trees: node-to-node
alignments need only be used where repeated
bilingual patterns can be discovered in the data.
3.1 Tree Alignments
We achieve this loose binding of trees by adapting
unordered tree alignment (Jiang et al, 1995) to a
probabilistic setting. Under this formalism, any
two trees can be aligned using an alignment tree.
The alignment tree embeds the original two trees
within it: each node is labeled by a pair (x, y),
(?, y), or (x, ?) where x is a node from the first
tree, y is a node from the second tree, and ? is an
empty space. The individual structure of each tree
must be preserved under the embedding with the
exception of sibling order (to allow variations in
phrase and word order).
The flexibility of this formalism can be demon-
strated by two extreme cases: (1) an alignment be-
tween two trees may actually align none of their
individual nodes, instead inserting an empty space
? for each of the original two trees? nodes. (2)
if the original trees are isomorphic to one an-
other, the alignment may match their nodes ex-
actly, without inserting any empty spaces. See
Figure 1 for an example.
3.2 Model overview
As our basic model of syntactic structure, we
adopt the Constituent-Context Model (CCM) of
Klein and Manning (2002). Under this model,
the part-of-speech sequence of each span in a sen-
tence is generated either as a constituent yield
? if it is dominated by a node in the tree ?
or otherwise as a distituent yield. For example,
in the bracketed sentence [John/NNP [climbed/VB
[the/DT tree/NN]]], the sequence VB DT NN is gen-
erated as a constituent yield, since it constitutes a
complete bracket in the tree. On the other hand,
the sequence VB DT is generated as a distituent,
since it does not. Besides these yields, the con-
texts (two surrounding POS tags) of constituents
and distituents are generated as well. In this exam-
ple, the context of the constituent VB DT NN would
be (NNP, #), while the context of the distituent VB
DT would be (NNP, NN). The CCM model em-
ploys separate multinomial distributions over con-
stituents, distituents, constituent contexts, and dis-
tituent contexts. While this model is deficient ?
each observed subsequence of part-of-speech tags
is generated many times over ? its performance
is far higher than that of unsupervised PCFGs.
Under our bilingual model, each pair of sen-
tences is assumed to have been generated jointly in
the following way: First, an unlabeled alignment
tree is drawn uniformly from the set of all such
trees. This alignment tree specifies the structure
of each of the two individual trees, as well as the
pairs of nodes which are aligned and those which
are not aligned (i.e. paired with a ?).
For each pair of aligned nodes, a correspond-
ing pair of constituents and contexts are jointly
drawn from a bilingual distribution. For unaligned
nodes (i.e. nodes paired with a ? in the alignment
75
tree), a single constituent and context are drawn,
from language-specific distributions. Distituents
and their contexts are also drawn from language-
specific distributions. Finally, word-level align-
ments are drawn based on the structure of the
alignment tree.
In the next two sections, we describe our model
in more formal detail by specifying the parame-
ters and generative process by which sentences are
formed.
3.3 Parameters
Our model employs a number of multinomial dis-
tributions:
? piCi : over constituent yields of language i,
? piDi : over distituent yields of language i,
? ?Ci : over constituent contexts of language i,
? ?Di : over distituent contexts of language i,
? ? : over pairs of constituent yields, one from
the first language and the other from the sec-
ond language,
? Gzpair : over a finite set of integer val-
ues {?m, . . . ,?2,?1, 0, 1, 2, . . . ,m}, mea-
suring the Giza-score of aligned tree node
pairs (see below),
? Gznode : over a finite set of integer values
{?m, . . . ,?2,?1, 0}, measuring the Giza-
score of unaligned tree nodes (see below).
The first four distributions correspond exactly to
the parameters of the CCM model. Parameter ? is
a ?coupling parameter? which measures the com-
patibility of tree-aligned constituent yield pairs.
The final two parameters measure the compatibil-
ity of syntactic alignments with the observed lexi-
cal GIZA++ alignments. Intuitively, aligned nodes
should have a high density of word-level align-
ments between them, and unaligned nodes should
have few lexical alignments.
More formally, consider a tree-aligned node
pair (n1, n2) with corresponding yields (y1, y2).
We call a word-level alignment good if it aligns
a word in y1 with a word in y2. We call a word-
level alignment bad if it aligns a word in y1 with
a word outside y2, or vice versa. The Giza-
score for (n1, n2) is the number of good word
alignments minus the number of bad word align-
ments. For example, suppose the constituent my
long name is node-aligned to its Urdu translation
mera lamba naam. If only the word-pairs my/mera
and name/naam are aligned, then the Giza-score
for this node-alignment would be 2. If however,
the English word long were (incorrectly) aligned
under GIZA++ to some Urdu word outside the cor-
responding constituent, then the score would drop
to 1. This score could even be negative if the num-
ber of bad alignments exceeds those that are good.
DistributionGzpair provides a probability for these
scores (up to some fixed absolute value).
For an unaligned node n with corresponding
yield y, only bad GIZA++ alignments are possible,
thus the Giza-score for these nodes will always be
zero or negative. Distribution Gznode provides a
probability for these scores (down to some fixed
value). We want our model to find tree alignments
such that both aligned node pairs and unaligned
nodes have high Giza-score.
3.4 Generative Process
Now we describe the stochastic process whereby
the observed parallel sentences and their word-
level alignments are generated, according to our
model.
As the first step in the Bayesian generative pro-
cess, all the multinomial parameters listed in the
previous section are drawn from their conjugate
priors ? Dirichlet distributions of appropriate di-
mension. Then, each pair of word-aligned parallel
sentences is generated through the following pro-
cess:
1. A pair of binary trees T1 and T2 along with
an alignment tree A are drawn according to
P (T1, T2, A). A is an alignment tree for T1
and T2 if it can be obtained by the follow-
ing steps: First insert blank nodes (labeled by
?) into T1 and T2. Then permute the order
of sibling nodes such that the two resulting
trees T ?1 and T
?
2 are identical in structure. Fi-
nally, overlay T ?1 and T
?
2 to obtain A. We ad-
ditionally require that A contain no extrane-
ous nodes ? that is no nodes with two blank
labels (?, ?). See Figure 1 for an example.
We define the distribution P (T1, T2, A) to be
uniform over all pairs of binary trees and their
alignments.
2. For each node in A of the form (n1, ?) (i.e.
nodes in T1 left unaligned by A), draw
(i) a constituent yield according to piC1 ,
76
(ii) a constituent context according to ?C1 ,
(iii) a Giza-score according to Gznode.
3. For each node in A of the form (?, n2) (i.e.
nodes in T2 left unaligned by A), draw
(i) a constituent yield according to piC2 ,
(ii) a constituent context according to ?C2 ,
(iii) a Giza-score according to Gznode.
4. For each node in A of the form (n1, n2) (i.e.
tree-aligned node pairs), draw
(i) a pair of constituent yields (y1, y2) ac-
cording to:
?C1 (y1) ? ?
C
2 (y2) ? ?(y1, y2)
Z
(1)
which is a product of experts combining
the language specific context-yield dis-
tributions as well as the coupling distri-
bution ? with normalization constant Z,
(ii) a pair of contexts according to the ap-
propriate language-specific parameters,
(iii) a Giza-score according to Gzpair.
5. For each span in Ti not dominated by a node
(for each language i ? {1, 2}), draw a dis-
tituent yield according to piDi and a distituent
context according to ?Di .
6. Draw actual word-level alignments consis-
tent with the Giza-scores, according to a uni-
form distribution.
In the next section we turn to the problem of
inference under this model when only the part-
of-speech tag sequences of parallel sentences and
their word-level alignments are observed.
3.5 Inference
Given a corpus of paired part-of-speech tag se-
quences (s1, s2) and their GIZA++ alignments
g, we would ideally like to predict the set of
tree pairs (T1,T2) which have highest proba-
bility when conditioned on the observed data:
P
(
T1,T2
?
?s1, s2,g
)
. We could rewrite this by
explicitly integrating over the yield, context, cou-
pling, Giza-score parameters as well as the align-
ment trees. However, since maximizing this in-
tegral directly would be intractable, we resort to
standard Markov chain sampling techniques. We
use Gibbs sampling (Hastings, 1970) to draw trees
for each sentence conditioned on those drawn for
all other sentences. The samples form a Markov
chain which is guaranteed to converge to the true
joint distribution over all sentences.
In the monolingual setting, there is a well-
known tree sampling algorithm (Johnson et al,
2007). This algorithm proceeds in top-down fash-
ion by sampling individual split points using the
marginal probabilities of all possible subtrees.
These marginals can be efficiently pre-computed
and form the ?inside? table of the famous Inside-
Outside algorithm. However, in our setting, trees
come in pairs, and their joint probability crucially
depends on their alignment.
For the ith parallel sentence, we wish to jointly
sample the pair of trees (T1, T2)i together with
their alignment Ai. To do so directly would in-
volve simultaneously marginalizing over all pos-
sible subtrees as well as all possible alignments
between such subtrees when sampling upper-level
split points. We know of no obvious algorithm
for computing this marginal. We instead first sam-
ple the pair of trees (T1, T2)i from a simpler pro-
posal distributionQ. Our proposal distribution as-
sumes that no nodes of the two trees are aligned
and therefore allows us to use the recursive top-
down sampling algorithm mentioned above. After
a new tree pair T ? = (T ?1 , T
?
2 )i is drawn from Q,
we accept the pair with the following probability:
min
{
1,
P (T ?|T?i,A?i) Q(T |T?i,A?i)
P (T |T?i,A?i) Q(T ?|T?i,A?i)
}
where T is the previously sampled tree-pair for
sentence i, P is the true model probability, and
Q is the probability under the proposal distribu-
tion. This use of a tractable proposal distribution
and acceptance ratio is known as the Metropolis-
Hastings algorithm and it preserves the conver-
gence guarantee of the Gibbs sampler (Hastings,
1970). To compute the terms P (T ?|T?i,A?i)
and P (T |T?i,A?i) in the acceptance ratio above,
we need to marginalize over all possible align-
ments between tree pairs.
Fortunately, for any given pair of trees T1 and
T2 this marginalization can be computed using
a dynamic program in time O(|T1||T2|). Here
we provide a very brief sketch. For every pair
of nodes n1 ? T1, n2 ? T2, a table stores the
marginal probability of the subtrees rooted at n1
and n2, respectively. A dynamic program builds
this table from the bottom up: For each node pair
n1, n2, we sum the probabilities of all local align-
ment configurations, each multiplied by the appro-
77
priate marginals already computed in the table for
lower-level node pairs. This algorithm is an adap-
tation of the dynamic program presented in (Jiang
et al, 1995) for finding minimum cost alignment
trees (Fig. 5 of that publication).
Once a pair of trees (T1, T2) has been sam-
pled, we can proceed to sample an alignment tree
A|T1, T2.2 We sample individual alignment deci-
sions from the top down, at each step using the
alignment marginals for the remaining subtrees
(already computed using the afore-mentioned dy-
namic program). Once the triple (T1, T2, A) has
been sampled, we move on to the next parallel sen-
tence.
We avoid directly sampling parameter val-
ues, instead using the marginalized closed forms
for multinomials with Dirichlet conjugate-priors
using counts and hyperparameter pseudo-counts
(Gelman et al, 2004). Note that in the case of
yield pairs produced according to Distribution 1
(in step 4 of the generative process) conjugacy is
technically broken, since the yield pairs are no
longer produced by a single multinomial distribu-
tion. Nevertheless, we count the produced yields
as if they had been generated separately by each
of the distributions involved in the numerator of
Distribution 1.
4 Experimental setup
We test our model on three corpora of bilin-
gual parallel sentences: English-Korean, English-
Urdu, and English-Chinese. Though the model is
trained using parallel data, during testing it has ac-
cess only to monolingual data. This set-up ensures
that we are testing our model?s ability to learn bet-
ter parameters at training time, rather than its abil-
ity to exploit parallel data at test time. Following
(Klein and Manning, 2002), we restrict our model
to binary trees, though we note that the alignment
trees do not follow this restriction.
Data The Penn Korean Treebank (Han et al,
2002) consists of 5,083 Korean sentences trans-
lated into English for the purposes of language
training in a military setting. Both the Korean
and English sentences are annotated with syntactic
trees. We use the first 4,000 sentences for training
and the last 1,083 sentences for testing. We note
that in the Korean data, a separate tag is given for
2Sampling the alignment tree is important, as it provides
us with counts of aligned constituents for the coupling pa-
rameter.
each morpheme. We simply concatenate all the
morpheme tags given for each word and treat the
concatenation as a single tag. This procedure re-
sults in 199 different tags. The English-Urdu par-
allel corpus3 consists of 4,325 sentences from the
first three sections of the Penn Treebank and their
Urdu translations annotated at the part-of-speech
level. The Urdu side of this corpus does not pro-
vide tree annotations so here we can test parse ac-
curacy only on English. We use the remaining
sections of the Penn Treebank for English test-
ing. The English-Chinese treebank (Bies et al,
2007) consists of 3,850 Chinese newswire sen-
tences translated into English. Both the English
and Chinese sentences are annotated with parse
trees. We use the first 4/5 for training and the final
1/5 for testing.
During preprocessing of the corpora we remove
all punctuation marks and special symbols, fol-
lowing the setup in previous grammar induction
work (Klein and Manning, 2002). To obtain lex-
ical alignments between the parallel sentences we
employ GIZA++ (Och and Ney, 2003). We use in-
tersection alignments, which are one-to-one align-
ments produced by taking the intersection of one-
to-many alignments in each direction. These one-
to-one intersection alignments tend to have higher
precision.
We initialize the trees by making uniform split
decisions recursively from the top down for sen-
tences in both languages. Then for each pair of
parallel sentences we randomly sample an initial
alignment tree for the two sampled trees.
Baseline We implement a Bayesian version of
the CCM as a baseline. This model uses the same
inference procedure as our bilingual model (Gibbs
sampling). In fact, our model reduces to this
Bayesian CCM when it is assumed that no nodes
between the two parallel trees are ever aligned
and when word-level alignments are ignored. We
also reimplemented the original EM version of
CCM and found virtually no difference in perfor-
mance when using EM or Gibbs sampling. In both
cases our implementation achieves F-measure in
the range of 69-70% on WSJ10, broadly in line
with the performance reported by Klein and Man-
ning (2002).
Hyperparameters Klein (2005) reports using
smoothing pseudo-counts of 2 for constituent
3http://www.crulp.org
78
Figure 2: The F-measure of the CCM baseline (dotted line) and bilingual model (solid line) plotted on
the y-axis, as the maximum sentence length in the test set is increased (x-axis). Results are averaged over
all training scenarios given in Table 1.
yields and contexts and 8 for distituent yields and
contexts. In our Bayesian model, these similar
smoothing counts occur as the parameters of the
Dirichlet priors. For Korean we found that the
baseline performed well using these values. How-
ever, on our English and Chinese data, we found
that somewhat higher smoothing values worked
best, so we utilized values of 20 and 80 for con-
stituent and distituent smoothing counts, respec-
tively.
Our model additionally requires hyperparam-
eter values for ? (the coupling distribution for
aligned yields), Gzpair and Gznode (the distribu-
tions over Giza-scores for aligned nodes and un-
aligned nodes, respectively). For ? we used a
symmetric Dirichlet prior with parameter 1. For
Gzpair and Gznode, in order to create a strong bias
towards high Giza-scores, we used non-symmetric
Dirichlet priors. In both cases, we capped the ab-
solute value of the scores at 3, to prevent count
sparsity. In the case of Gzpair we gave pseudo-
counts of 1,000 for negative values and zero, and
pseudo-counts of 1,000,000 for positive scores.
For Gznode we gave a pseudo-count of 1,000,000
for a score of zero, and 1,000 for all nega-
tive scores. This very strong prior bias encodes
our intuition that syntactic alignments which re-
spect lexical alignments should be preferred. Our
method is not sensitive to these exact values and
any reasonably strong bias gave similar results.
In all our experiments, we consider the hyper-
parameters fixed and observed values.
Testing and evaluation As mentioned above,
we test our model only on monolingual data,
where the parallel sentences are not provided to
the model. To predict the bracketings of these
monolingual test sentences, we take the smoothed
counts accumulated in the final round of sampling
over the training data and perform a maximum
likelihood estimate of the monolingual CCM pa-
rameters. These parameters are then used to pro-
duce the highest probability bracketing of the test
set.
To evaluate both our model as well as the base-
line, we use (unlabeled) bracket precision, re-
call, and F-measure (Klein and Manning, 2002).
Following previous work, we include the whole-
sentence brackets but ignore single-word brack-
ets. We perform experiments on different subsets
of training and testing data based on the sentence-
length. In particular we experimented with sen-
tence length limits of 10, 20, and 30 for both the
training and testing sets. We also report the upper
bound on F-measure for binary trees. We average
the results over 10 separate sampling runs.
5 Results
Table 1 reports the full results of our experiments.
In all testing scenarios the bilingual model out-
performs its monolingual counterpart in terms of
both precision and recall. On average, the bilin-
gual model gains 10.2 percentage points in preci-
sion, 7.7 in recall, and 8.8 in F-measure. The gap
between monolingual performance and the binary
tree upper bound is reduced by over 19%.
The extent of the gain varies across pairings.
For instance, the smallest improvement is ob-
served for English when trained with Urdu. The
Korean-English pairing results in substantial im-
provements for Korean and quite large improve-
ments for English, for which the absolute gain
reaches 28 points in F-measure. In the case of Chi-
nese and English, the gains for English are fairly
minimal whereas those for Chinese are quite sub-
79
Max Sent. Length Monolingual Bilingual Upper Bound
Test Train Precision Recall F1 Precision Recall F1 F1
E
N
w
ith
K
R 10
10 52.74 39.53 45.19 57.76 43.30 49.50 85.6
20 41.87 31.38 35.87 61.66 46.22 52.83 85.6
30 33.43 25.06 28.65 64.41 48.28 55.19 85.6
20
20 35.12 25.12 29.29 56.96 40.74 47.50 83.3
30 26.26 18.78 21.90 60.07 42.96 50.09 83.3
30 30 23.95 16.81 19.76 58.01 40.73 47.86 82.4
K
R
w
ith
E
N 10
10 71.07 62.55 66.54 75.63 66.56 70.81 93.6
20 71.35 62.79 66.80 77.61 68.30 72.66 93.6
30 71.37 62.81 66.82 77.87 68.53 72.91 93.6
20
20 64.28 54.73 59.12 70.44 59.98 64.79 91.9
30 64.29 54.75 59.14 70.81 60.30 65.13 91.9
30 30 63.63 54.17 58.52 70.11 59.70 64.49 91.9
E
N
w
ith
C
H 10
10 50.09 34.18 40.63 37.46 25.56 30.39 81.0
20 58.86 40.17 47.75 50.24 34.29 40.76 81.0
30 64.81 44.22 52.57 68.24 46.57 55.36 81.0
20
20 41.90 30.52 35.31 38.64 28.15 32.57 84.3
30 52.83 38.49 44.53 58.50 42.62 49.31 84.3
30 30 46.35 33.67 39.00 51.40 37.33 43.25 84.1
C
H
w
ith
E
N 10
10 39.87 27.71 32.69 40.62 28.23 33.31 81.9
20 43.44 30.19 35.62 47.54 33.03 38.98 81.9
30 43.63 30.32 35.77 54.09 37.59 44.36 81.9
20
20 29.80 23.46 26.25 36.93 29.07 32.53 88.0
30 30.05 23.65 26.47 43.99 34.63 38.75 88.0
30 30 24.46 19.41 21.64 39.61 31.43 35.05 88.4
E
N
w
ith
U
R 10
10 57.98 45.68 51.10 73.43 57.85 64.71 88.1
20 70.57 55.60 62.20 80.24 63.22 70.72 88.1
30 75.39 59.40 66.45 79.04 62.28 69.67 88.1
20
20 57.78 43.86 49.87 67.26 51.06 58.05 86.3
30 63.12 47.91 54.47 64.45 48.92 55.62 86.3
30 30 57.36 43.02 49.17 57.97 43.48 49.69 85.7
Table 1: Unlabeled precision, recall and F-measure for the monolingual baseline and the bilingual model
on several test sets. We report results for different combinations of maximum sentence length in both the
training and test sets. The right most column, in all cases, contains the maximum F-measure achievable
using binary trees. The best performance for each test-length is highlighted in bold.
stantial. This asymmetry should not be surprising,
as Chinese on its own seems to be quite a bit more
difficult to parse than English.
We also investigated the impact of sentence
length for both the training and testing sets. For
our model, adding sentences of greater length to
the training set leads to increases in parse accu-
racy for short sentences. For the baseline, how-
ever, adding this additional training data degrades
performance in the case of English paired with Ko-
rean. Figure 2 summarizes the performance of
our model for different sentence lengths on sev-
eral of the test-sets. As shown in the figure, the
largest improvements tend to occur at longer sen-
tence lengths.
6 Conclusion
We have presented a probabilistic model for bilin-
gual grammar induction which uses raw parallel
text to learn tree pairs and their alignments. Our
formalism loosely binds the two trees, using bilin-
gual patterns when possible, but allowing substan-
tial language-specific variation. We tested our
model on three test sets and showed substantial
improvement over a state-of-the-art monolingual
baseline.4
4The authors acknowledge the support of the NSF (CA-
REER grant IIS-0448168, grant IIS-0835445, and grant IIS-
0835652). Thanks to Amir Globerson and members of the
MIT NLP group for their helpful suggestions. Any opinions,
findings, or conclusions are those of the authors, and do not
necessarily reflect the views of the funding organizations
80
References
Ann Bies, Martha Palmer, Justin Mott, and Colin
Warner. 2007. English Chinese translation treebank
v 1.0. LDC2007T02.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
Bayesian synchronous grammar induction. In Pro-
ceedings of NIPS.
David Burkett and Dan Klein. 2008. Two languages
are better than one (for syntactic parsing). In Pro-
ceedings of EMNLP, pages 877?886.
Eugene Charniak and Glen Carroll. 1992. Two exper-
iments on learning probabilistic dependency gram-
mars from corpora. In Proceedings of the AAAI
Workshop on Statistically-Based NLP Techniques,
pages 1?13.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the ACL, pages 263?270.
Shay B. Cohen and Noah A. Smith. 2009. Shared lo-
gistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Proceedings
of the NAACL/HLT.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In The Compan-
ion Volume to the Proceedings of the ACL, pages
205?208.
Andrew Gelman, John B. Carlin, Hal S. Stern, and
Donald B. Rubin. 2004. Bayesian data analysis.
Chapman and Hall/CRC.
Dmitriy Genzel. 2005. Inducing a multilingual dictio-
nary from a parallel multitext in related languages.
In Proceedings of EMNLP/HLT, pages 875?882.
C. Han, N.R. Han, E.S. Ko, H. Yi, and M. Palmer.
2002. Penn Korean Treebank: Development and
evaluation. In Proc. Pacific Asian Conf. Language
and Comp.
W. K. Hastings. 1970. Monte carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57:97?109.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and
O. Kolak. 2005. Bootstrapping parsers via syntactic
projection across parallel texts. Journal of Natural
Language Engineering, 11(3):311?325.
T. Jiang, L. Wang, and K. Zhang. 1995. Alignment of
trees ? an alternative to tree edit. Theoretical Com-
puter Science, 143(1):137?148.
M. Johnson, T. Griffiths, and S. Goldwater. 2007.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proceedings of the NAACL/HLT,
pages 139?146.
Dan Klein and Christopher D. Manning. 2002. A
generative constituent-context model for improved
grammar induction. In Proceedings of the ACL,
pages 128?135.
D. Klein. 2005. The Unsupervised Learning of Natu-
ral Language Structure. Ph.D. thesis, Stanford Uni-
versity.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the ACL,
pages 470?477.
I. Dan Melamed. 2003. Multitext grammars
and synchronous parsers. In Proceedings of the
NAACL/HLT, pages 79?86.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Yoav Seginer. 2007. Fast unsupervised incremental
parsing. In Proceedings of the ACL, pages 384?391.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceeding of EMNLP, pages 49?
56.
Benjamin Snyder and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proceedings of the ACL/HLT,
pages 737?745.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2008. Unsupervised multi-
lingual learning for POS tagging. In Proceedings of
EMNLP, pages 1041?1050.
Benjamin Snyder, Tahira Naseem, Jacob Eisenstein,
and Regina Barzilay. 2009. Adding more languages
improves unsupervised multilingual part-of-speech
tagging: A Bayesian non-parametric approach. In
Proceedings of the NAACL/HLT.
Andreas Stolcke and Stephen M. Omohundro. 1994.
Inducing probabilistic grammars by Bayesian model
merging. In Proceedings of ICGI, pages 106?118.
Dekai Wu and Hongsing Wong. 1998. Machine
translation with a stochastic grammatical channel.
In Proceedings of the ACL/COLING, pages 1408?
1415.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Chenhai Xi and Rebecca Hwa. 2005. A backoff
model for bootstrapping resources for non-english
languages. In Proceedings of EMNLP, pages 851 ?
858.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the ACL, pages 475?482.
81
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1234?1244,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Using Universal Linguistic Knowledge to Guide Grammar Induction
Tahira Naseem, Harr Chen, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{tahira, harr, regina} @csail.mit.edu
Mark Johnson
Department of Computing
Macquarie University
mark.johnson@mq.edu.au
Abstract
We present an approach to grammar induc-
tion that utilizes syntactic universals to im-
prove dependency parsing across a range of
languages. Our method uses a single set
of manually-specified language-independent
rules that identify syntactic dependencies be-
tween pairs of syntactic categories that com-
monly occur across languages. During infer-
ence of the probabilistic model, we use pos-
terior expectation constraints to require that a
minimum proportion of the dependencies we
infer be instances of these rules. We also auto-
matically refine the syntactic categories given
in our coarsely tagged input. Across six lan-
guages our approach outperforms state-of-the-
art unsupervised methods by a significant mar-
gin.1
1 Introduction
Despite surface differences, human languages ex-
hibit striking similarities in many fundamental as-
pects of syntactic structure. These structural corre-
spondences, referred to as syntactic universals, have
been extensively studied in linguistics (Baker, 2001;
Carnie, 2002; White, 2003; Newmeyer, 2005) and
underlie many approaches in multilingual parsing.
In fact, much recent work has demonstrated that
learning cross-lingual correspondences from cor-
pus data greatly reduces the ambiguity inherent in
syntactic analysis (Kuhn, 2004; Burkett and Klein,
2008; Cohen and Smith, 2009a; Snyder et al, 2009;
Berg-Kirkpatrick and Klein, 2010).
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/dependency/
Root ? Auxiliary Noun ? Adjective
Root ? Verb Noun ? Article
Verb ? Noun Noun ? Noun
Verb ? Pronoun Noun ? Numeral
Verb ? Adverb Preposition ? Noun
Verb ? Verb Adjective ? Adverb
Auxiliary ? Verb
Table 1: The manually-specified universal dependency
rules used in our experiments. These rules specify head-
dependent relationships between coarse (i.e., unsplit)
syntactic categories. An explanation of the ruleset is pro-
vided in Section 5.
In this paper, we present an alternative gram-
mar induction approach that exploits these struc-
tural correspondences by declaratively encoding a
small set of universal dependency rules. As input
to the model, we assume a corpus annotated with
coarse syntactic categories (i.e., high-level part-of-
speech tags) and a set of universal rules defined over
these categories, such as those in Table 1. These
rules incorporate the definitional properties of syn-
tactic categories in terms of their interdependencies
and thus are universal across languages. They can
potentially help disambiguate structural ambiguities
that are difficult to learn from data alone ? for
example, our rules prefer analyses in which verbs
are dependents of auxiliaries, even though analyz-
ing auxiliaries as dependents of verbs is also consis-
tent with the data. Leveraging these universal rules
has the potential to improve parsing performance
for a large number of human languages; this is par-
ticularly relevant to the processing of low-resource
1234
languages. Furthermore, these universal rules are
compact and well-understood, making them easy to
manually construct.
In addition to these universal dependencies, each
specific language typically possesses its own id-
iosyncratic set of dependencies. We address this
challenge by requiring the universal constraints to
only hold in expectation rather than absolutely, i.e.,
we permit a certain number of violations of the con-
straints.
We formulate a generative Bayesian model that
explains the observed data while accounting for
declarative linguistic rules during inference. These
rules are used as expectation constraints on the
posterior distribution over dependency structures.
This approach is based on the posterior regular-
ization technique (Grac?a et al, 2009), which we
apply to a variational inference algorithm for our
parsing model. Our model can also optionally re-
fine common high-level syntactic categories into
per-language categories by inducing a clustering of
words using Dirichlet Processes (Ferguson, 1973).
Since the universals guide induction toward linguis-
tically plausible structures, automatic refinement be-
comes feasible even in the absence of manually an-
notated syntactic trees.
We test the effectiveness of our grammar induc-
tion model on six Indo-European languages from
three language groups: English, Danish, Portuguese,
Slovene, Spanish, and Swedish. Though these lan-
guages share a high-level Indo-European ancestry,
they cover a diverse range of syntactic phenomenon.
Our results demonstrate that universal rules greatly
improve the accuracy of dependency parsing across
all of these languages, outperforming current state-
of-the-art unsupervised grammar induction meth-
ods (Headden III et al, 2009; Berg-Kirkpatrick and
Klein, 2010).
2 Related Work
Learning with Linguistic Constraints Our work
is situated within a broader class of unsupervised ap-
proaches that employ declarative knowledge to im-
prove learning of linguistic structure (Haghighi and
Klein, 2006; Chang et al, 2007; Grac?a et al, 2007;
Cohen and Smith, 2009b; Druck et al, 2009; Liang
et al, 2009a). The way we apply constraints is clos-
est to the latter two approaches of posterior regular-
ization and generalized expectation criteria.
In the posterior regularization framework, con-
straints are expressed in the form of expectations on
posteriors (Grac?a et al, 2007; Ganchev et al, 2009;
Grac?a et al, 2009; Ganchev et al, 2010). This de-
sign enables the model to reflect constraints that are
difficult to encode via the model structure or as pri-
ors on its parameters. In their approach, parame-
ters are estimated using a modified EM algorithm,
where the E-step minimizes the KL-divergence be-
tween the model posterior and the set of distributions
that satisfies the constraints. Our approach also ex-
presses constraints as expectations on the posterior;
we utilize the machinery of their framework within
a variational inference algorithm with a mean field
approximation.
Generalized expectation criteria, another tech-
nique for declaratively specifying expectation con-
straints, has previously been successfully applied to
the task of dependency parsing (Druck et al, 2009).
This objective expresses constraints in the form of
preferences over model expectations. The objective
is penalized by the square distance between model
expectations and the prespecified values of the ex-
pectation. This approach yields significant gains
compared to a fully unsupervised counterpart. The
constraints they studied are corpus- and language-
specific. Our work demonstrates that a small set of
language-independent universals can also serve as
effective constraints. Furthermore, we find that our
method outperforms the generalized expectation ap-
proach using corpus-specific constraints.
Learning to Refine Syntactic Categories Recent
research has demonstrated the usefulness of auto-
matically refining the granularity of syntactic cat-
egories. While most of the existing approaches
are implemented in the supervised setting (Finkel
et al, 2007; Petrov and Klein, 2007), Liang et al
(2007) propose a non-parametric Bayesian model
that learns the granularity of PCFG categories in
an unsupervised fashion. For each non-terminal
grammar symbol, the model posits a Hierarchical
Dirichlet Process over its refinements (subsymbols)
to automatically learn the granularity of syntactic
categories. As with their work, we also use non-
parametric priors for category refinement and em-
1235
ploy variational methods for inference. However,
our goal is to apply category refinement to depen-
dency parsing, rather than to PCFGs, requiring a
substantially different model formulation. While
Liang et al (2007) demonstrated empirical gains on
a synthetic corpus, our experiments focus on unsu-
pervised category refinement on real language data.
Universal Rules in NLP Despite the recent surge
of interest in multilingual learning (Kuhn, 2004; Co-
hen and Smith, 2009a; Snyder et al, 2009; Berg-
Kirkpatrick and Klein, 2010), there is surprisingly
little computational work on linguistic universals.
On the acquisition side, Daume? III and Campbell
(2007) proposed a computational technique for dis-
covering universal implications in typological fea-
tures. More closely related to our work is the posi-
tion paper by Bender (2009), which advocates the
use of manually-encoded cross-lingual generaliza-
tions for the development of NLP systems. She ar-
gues that a system employing such knowledge could
be easily adapted to a particular language by spe-
cializing this high level knowledge based on the ty-
pological features of the language. We also argue
that cross-language universals are beneficial for au-
tomatic language processing; however, our focus is
on learning language-specific adaptations of these
rules from data.
3 Model
The central hypothesis of this work is that unsu-
pervised dependency grammar induction can be im-
proved using universal linguistic knowledge. To-
ward this end our approach is comprised of two
components: a probabilistic model that explains
how sentences are generated from latent dependency
structures and a technique for incorporating declar-
ative rules into the inference process.
We first describe the generative story in this sec-
tion before turning to how constraints are applied
during inference in Section 4. Our model takes as
input (i.e., as observed) a set of sentences where
each word is annotated with a coarse part-of-speech
tag. Table 2 provides a detailed technical descrip-
tion of our model?s generative process, and Figure 1
presents a model diagram.
For each observed coarse symbol s:
1. Draw top-level infinite multinomial over
subsymbols ?s ? GEM(?).
2. For each subsymbol z of symbol s:
(a) Draw word emission multinomial
?sz ? Dir(?0).
(b) For each context value c:
i. Draw child symbol generation
multinomial ?szc ? Dir(?0).
ii. For each child symbol s?:
A. Draw second-level infinite
multinomial over subsymbols
pis?szc ? DP(?, ?s?).
For each tree node i generated in context c by
parent symbol s? and parent subsymbol z?:
1. Draw coarse symbol si ? Mult(?s?z?).
2. Draw subsymbol zi ? Mult(pisis?z?c).
3. Draw word xi ? Mult(?sizi).
Table 2: The generative process for model parameters
and parses. In the above GEM, DP, Dir, and Mult refer
respectively to the stick breaking distribution, Dirichlet
process, Dirichlet distribution, and multinomial distribu-
tion.
Generating Symbols and Words We describe
how a single node of the tree is generated before
discussing how the entire tree structure is formed.
Each node of the dependency tree is comprised of
three random variables: an observed coarse symbol
s, a hidden refined subsymbol z, and an observed
word x. In the following let the parent of the cur-
rent node have symbol s? and subsymbol z?; the root
node is generated from separate root-specific distri-
butions. Subsymbol refinement is an optional com-
ponent of the full model and can be omitted by de-
terministically equating s and z. As we explain at
the end of this section, without this aspect the gener-
ative story closely resembles the classic dependency
model with valence (DMV) of Klein and Manning
(2004).
First we draw symbol s from a finite multinomial
1236
s - coarse symbol (observed)
z - refined subsymbol
x - word (observed)
?szc - distr over child coarse symbols for
each parent s and z and context c
?s - top-level distr over subsymbols for s
piss?z?c - distr over subsymbols for each s,
parent s? and z?, and context c
?sz - distr over words for s and z
Figure 1: Graphical representation of the model and a summary of the notation. There is a copy of the outer plate for
each distinct symbol in the observed coarse tags. Here, node 3 is shown to be the parent of nodes 1 and 2. Shaded
variables are observed, square variables are hyperparameters. The elongated oval around s and z represents the two
variables jointly. For clarity the diagram omits some arrows from ? to each s, pi to each z, and ? to each x.
distribution with parameters ?s?z?c. As the indices
indicate, we have one such set of multinomial pa-
rameters for every combination of parent symbol
s? and subsymbol z? along with a context c. Here
the context of the current node can take one of six
values corresponding to every combination of di-
rection (left or right) and valence (first, second, or
third or higher child) with respect to its parent. The
prior (base distribution) for each ?s?z?c is a symmet-
ric Dirichlet with hyperparameter ?0.
Next we draw the refined syntactic category sub-
symbol z from an infinite multinomial with parame-
ters piss?z?c. Here the selection of pi is indexed by the
current node?s coarse symbol s, the symbol s? and
subsymbol z? of the parent node, and the context c
of the current node.
For each unique coarse symbol s we tie together
the distributions piss?z?c for all possible parent and
context combinations (i.e., s?, z?, and c) using a Hi-
erarchical Dirichlet Process (HDP). Specifically, for
a single s, each distribution piss?z?c over subsymbols
is drawn from a DP with concentration parameter
? and base distribution ?s over subsymbols. This
base distribution ?s is itself drawn from a GEM prior
with concentration parameter ?. By formulating the
generation of z as an HDP, we can share parame-
ters for a single coarse symbol?s subsymbol distribu-
tion while allowing for individual variability based
on node parent and context. Note that parameters
are not shared across different coarse symbols, pre-
serving the distinctions expressed via the coarse tag
annotations.
Finally, we generate the word x from a finite
multinomial with parameters ?sz , where s and z are
the symbol and subsymbol of the current node. The
? distributions are drawn from a symmetric Dirich-
let prior.
Generating the Tree Structure We now consider
how the structure of the tree arises. We follow
an approach similar to the widely-referenced DMV
model (Klein and Manning, 2004), which forms
the basis of the current state-of-the-art unsuper-
vised grammar induction model (Headden III et al,
2009). After a node is drawn we generate children
on each side until we produce a designated STOP
symbol. We encode more detailed valence informa-
tion than Klein and Manning (2004) and condition
child generation on parent valence. Specifically, af-
ter drawing a node we first decide whether to pro-
ceed to generate a child or to stop conditioned on the
parent symbol and subsymbol and the current con-
text (direction and valence). If we decide to gener-
ate a child we follow the previously described pro-
cess for constructing a node. We can combine the
stopping decision with the generation of the child
symbol by including a distinguished STOP symbol
as a possible outcome in distribution ?.
No-Split Model Variant In the absence of sub-
symbol refinement (i.e., when subsymbol z is set to
be identical to coarse symbol s), our model simpli-
fies in some respects. In particular, the HDP gener-
1237
ation of z is obviated and word x is drawn from a
word distribution ?s indexed solely by coarse sym-
bol s. The resulting simplified model closely resem-
bles DMV (Klein and Manning, 2004), except that it
1) explicitly generate words x rather than only part-
of-speech tags s, 2) encodes richer context and va-
lence information, and 3) imposes a Dirichlet prior
on the symbol distribution ?.
4 Inference with Constraints
We now describe how to augment our generative
model of dependency structure with constraints de-
rived from linguistic knowledge. Incorporating arbi-
trary linguistic rules directly in the generative story
is challenging as it requires careful tuning of either
the model structure or priors for each constraint. In-
stead, following the approach of Grac?a et al (2007),
we constrain the posterior to satisfy the rules in ex-
pectation during inference. This effectively biases
the inference toward linguistically plausible settings.
In standard variational inference, an intractable
true posterior is approximated by a distribution from
a tractable set (Bishop, 2006). This tractable set typ-
ically makes stronger independence assumptions be-
tween model parameters than the model itself. To in-
corporate the constraints, we further restrict the set
to only include distributions that satisfy the specified
expectation constraints over hidden variables.
In general, for some given model, let ? denote
the entire set of model parameters and z and x de-
note the hidden structure and observations respec-
tively. We are interested in estimating the posterior
p(?, z | x). Variational inference transforms this
problem into an optimization problem where we try
to find a distribution q(?, z) from a restricted set Q
that minimizes the KL-divergence between q(?, z)
and p(?, z | x):
KL(q(?, z) ? p(?, z | x))
=
?
q(?, z) log
q(?, z)
p(?, z, x)
d?dz + log p(x).
Rearranging the above yields:
log p(x) = KL(q(?, z) ? p(?, z | x)) + F ,
where F is defined as
F ?
?
q(?, z) log
p(?, z, x)
q(?, z)
d?dz. (1)
Thus F is a lower bound on likelihood. Maximizing
this lower bound is equivalent to minimizing the KL-
divergence between p(?, z | x) and q(?, z). To make
this maximization tractable we make a mean field
assumption that q belongs to a set Q of distributions
that factorize as follows:
q(?, z) = q(?)q(z).
We further constrain q to be from the subset of Q
that satisfies the expectation constraintEq[f(z)] ? b
where f is a deterministically computable function
of the hidden structures. In our model, for exam-
ple, f counts the dependency edges that are an in-
stance of one of the declaratively specified depen-
dency rules, while b is the proportion of the total
dependencies that we expect should fulfill this con-
straint.2
With the mean field factorization and the expec-
tation constraints in place, solving the maximization
of F in (1) separately for each factor yields the fol-
lowing updates:
q(?) = argmin
q(?)
KL
(
q(?) ? q?(?)
)
, (2)
q(z) = argmin
q(z)
KL
(
q(z) ? q?(z)
)
s.t. Eq(z)[f(z)] ? b, (3)
where
q?(?) ? expEq(z)[log p(?, z, x)], (4)
q?(z) ? expEq(?)[log p(?, z, x)]. (5)
We can solve (2) by setting q(?) to q?(?) ? since
q(z) is held fixed while updating q(?), the expecta-
tion function of the constraint remains constant dur-
ing this update. As shown by Grac?a et al (2007), the
update in (3) is a constrained optimization problem
and can be solved by performing gradient search on
its dual:
argmin
?
?>b + log
?
z
q?(z) exp(??>f(z)) (6)
For a fixed value of ? the optimal q(z) ?
q?(z) exp(??>f(z)). By updating q(?) and q(z)
as in (2) and (3) we are effectively maximizing the
lower bound F .
2Constraints of the form Eq[f(z)] ? b are easily imposed
by negating f(z) and b.
1238
4.1 Variational Updates
We now derive the specific variational updates for
our dependency induction model. First we assume
the following mean-field factorization of our varia-
tional distribution:
q(?, ?, pi, ?, z)
= q(z) ?
?
s?
q(?s?) ?
T?
z?=1
q(?s?z?)?
?
c
q(?s?z?c) ?
?
s
q(piss?z?c), (7)
where s? varies over the set of unique symbols in the
observed tags, z? denotes subsymbols for each sym-
bol, c varies over context values comprising a pair
of direction (left or right) and valence (first, second,
or third or higher) values, and s corresponds to child
symbols.
We restrict q(?s?z?c) and q(?s?z?) to be Dirichlet
distributions and q(z) to be multinomial. As with
prior work (Liang et al, 2009b), we assume a de-
generate q(?) ? ???(?) for tractability reasons, i.e.,
all mass is concentrated on some single ??. We also
assume that the top level stick-breaking distribution
is truncated at T , i.e., q(?) assigns zero probability
to integers greater than T . Because of the truncation
of ?, we can approximate q(piss?z?c) with an asym-
metric finite dimensional Dirichlet.
The factors are updated one at a time holding all
other factors fixed. The variational update for q(pi)
is given by:
q(piss?z?c) = Dir
(
piss?z?c;?? + Eq(z)[Css?z?c(z)]
)
,
where term Eq(z)[Css?z?c(z)] is the expected count
w.r.t. q(z) of child symbol s and subsymbol z in
context c when generated by parent symbol s? and
subsymbol z?.
Similarly, the updates for q(?) and q(?) are given
by:
q(?s?z?c) = Dir
(
?s?z?c; ?0 + Eq(z)[Cs?z?c(s)]
)
,
q(?s?z?) = Dir
(
?s?z? ;?0 + Eq(z)[Cs?z?(x)]
)
,
where Cs?z?c(s) is the count of child symbol s being
generated by the parent symbol s? and subsymbol z?
in context c and Cs?z?x is the count of word x being
generated by symbol s? and subsymbol z?.
The only factor affected by the expectation con-
straints is q(z). Recall from the previous section that
the update for q(z) is performed via gradient search
on the dual of a constrained minimization problem
of the form:
q(z) = argmin
q(z)
KL(q(z) ? q?(z)).
Thus we first compute the update for q?(z):
q?(z) ?
N?
n=1
len(n)?
j=1
(expEq(?)[log ?snjznj (xnj)]
? expEq(?)[log ?sh(nj)zh(nj)cnj (snj)]
? expEq(pi)[log pisnjsh(nj)zh(nj)cnj (znj)]),
where N is the total number of sentences, len(n)
is the length of sentence n, and index h(nj) refers
to the head of the jth node of sentence n. Given
this q?(z) a gradient search is performed using (6) to
find the optimal ? and thus the primal solution for
updating q(z).
Finally, we update the degenerate factor q(?s)
with the projected gradient search algorithm used
by Liang et al (2009b).
5 Linguistic Constraints
Universal Dependency Rules We compile a set of
13 universal dependency rules consistent with vari-
ous linguistic accounts (Carnie, 2002; Newmeyer,
2005), shown in Table 1. These rules are defined
over coarse part-of-speech tags: Noun, Verb, Adjec-
tive, Adverb, Pronoun, Article, Auxiliary, Preposi-
tion, Numeral and Conjunction. Each rule specifies
a part-of-speech for the head and argument but does
not provide ordering information.
We require that a minimum proportion of the pos-
terior dependencies be instances of these rules in ex-
pectation. In contrast to prior work on rule-driven
dependency induction (Druck et al, 2009), where
each rule has a separately specified expectation, we
only set a single minimum expectation for the pro-
portion of all dependencies that must match one of
the rules. This setup is more relevant for learn-
ing with universals since individual rule frequencies
vary greatly between languages.
1239
1. Identify non-recursive NPs:
? All nouns, pronouns and possessive
marker are part of an NP.
? All adjectives, conjunctions and deter-
miners immediately preceding an NP
are part of the NP.
2. The first verb or modal in the sentence is the
headword.
3. All words in an NP are headed by the last
word in the NP.
4. The last word in an NP is headed by the
word immediately before the NP if it is a
preposition, otherwise it is headed by the
headword of the sentence if the NP is be-
fore the headword, else it is headed by the
word preceding the NP.
5. For the first word set its head to be the head-
word of the sentence. For each other word
set its headword to be the previous word.
Table 3: English-specific dependency rules.
English-specific Dependency Rules For English,
we also consider a small set of hand-crafted depen-
dency rules designed by Michael Collins3 for deter-
ministic parsing, shown in Table 3. Unlike the uni-
versals from Table 1, these rules alone are enough to
construct a full dependency tree. Thus they allow us
to judge whether the model is able to improve upon
a human-engineered deterministic parser. Moreover,
with this dataset we can assess the additional benefit
of using rules tailored to an individual language as
opposed to universal rules.
6 Experimental Setup
Datasets and Evaluation We test the effective-
ness of our grammar induction approach on English,
Danish, Portuguese, Slovene, Spanish, and Swedish.
For English we use the Penn Treebank (Marcus et
al., 1993), transformed from CFG parses into depen-
3Personal communication.
dencies with the Collins head finding rules (Collins,
1999); for the other languages we use data from the
2006 CoNLL-X Shared Task (Buchholz and Marsi,
2006). Each dataset provides manually annotated
part-of-speech tags that are used for both training
and testing. For comparison purposes with previ-
ous work, we limit the cross-lingual experiments to
sentences of length 10 or less (not counting punc-
tuation). For English, we also explore sentences of
length up to 20.
The final output metric is directed dependency ac-
curacy. This is computed based on the Viterbi parses
produced using the final unnormalized variational
distribution q(z) over dependency structures.
Hyperparameters and Training Regimes Un-
less otherwise stated, in experiments with rule-based
constraints the expected proportion of dependencies
that must satisfy those constraints is set to 0.8. This
threshold value was chosen based on minimal tun-
ing on a single language and ruleset (English with
universal rules) and carried over to each other ex-
perimental condition. A more detailed discussion of
the threshold?s empirical impact is presented in Sec-
tion 7.1.
Variational approximations to the HDP are trun-
cated at 10. All hyperparameter values are fixed to 1
except ? which is fixed to 10.
We also conduct a set of No-Split experiments to
evaluate the importance of syntactic refinement; in
these experiments each coarse symbol corresponds
to only one refined symbol. This is easily effected
during inference by setting the HDP variational ap-
proximation truncation level to one.
For each experiment we run 50 iterations of vari-
ational updates; for each iteration we perform five
steps of gradient search to compute the update for
the variational distribution q(z) over dependency
structures.
7 Results
In the following section we present our primary
cross-lingual results using universal rules (Sec-
tion 7.1) before performing a more in-depth analysis
of model properties such as sensitivity to ruleset se-
lection and inference stability (Section 7.2).
1240
DMV PGI No-Split HDP-DEP
English 47.1 62.3 71.5 71.9 (0.3)
Danish 33.5 41.6 48.8 51.9 (1.6)
Portuguese 38.5 63.0 54.0 71.5 (0.5)
Slovene 38.5 48.4 50.6 50.9 (5.5)
Spanish 28.0 58.4 64.8 67.2 (0.4)
Swedish 45.3 58.3 63.3 62.1 (0.5)
Table 4: Directed dependency accuracy using our model
with universal dependency rules (No-Split and HDP-
DEP), compared to DMV (Klein andManning, 2004) and
PGI (Berg-Kirkpatrick and Klein, 2010). The DMV re-
sults are taken from Berg-Kirkpatrick and Klein (2010).
Bold numbers indicate the best result for each language.
For the full model, the standard deviation in performance
over five runs is indicated in parentheses.
7.1 Main Cross-Lingual Results
Table 4 shows the performance of both our full
model (HDP-DEP) and its No-Split version using
universal dependency rules across six languages.
We also provide the performance of two baselines?
the dependency model with valence (DMV) (Klein
and Manning, 2004) and the phylogenetic grammar
induction (PGI) model (Berg-Kirkpatrick and Klein,
2010).
HDP-DEP outperforms both DMV and PGI
across all six languages. Against DMV we achieve
an average absolute improvement of 24.1%. This
improvement is expected given that DMV does not
have access to the additional information provided
through the universal rules. PGI is more relevant
as a point of comparison, since it is able to lever-
age multilingual data to learn information similar to
what we have declaratively specified using universal
rules. Specifically, PGI reduces induction ambigu-
ity by connecting language-specific parameters via
phylogenetic priors. We find, however, that we out-
perform PGI by an average margin of 7.2%, demon-
strating the benefits of explicit rule specification.
An additional point of comparison is the lexi-
calized unsupervised parser of Headden III et al
(2009), which yields the current state-of-the-art un-
supervised accuracy on English at 68.8%. Our
method also outperforms this approach, without em-
ploying lexicalization and sophisticated smoothing
as they do. This result suggests that combining the
complementary strengths of their approach and ours
English
Rule Excluded Acc Loss Gold Freq
Preposition ? Noun 61.0 10.9 5.1
Verb ? Noun 61.4 10.5 14.8
Noun ? Noun 64.4 7.5 10.7
Noun ? Article 64.7 7.2 8.5
Spanish
Rule Excluded Acc Loss Gold Freq
Preposition ? Noun 53.4 13.8 8.2
Verb ? Noun 61.9 5.4 12.9
Noun ? Noun 62.6 4.7 2.0
Root ? Verb 65.4 1.8 12.3
Table 5: Ablation experiment results for universal depen-
dency rules on English and Spanish. For each rule, we
evaluate the model using the ruleset excluding that rule,
and list the most significant rules for each language. The
second last column is the absolute loss in performance
compared to the setting where all rules are available. The
last column shows the percentage of the gold dependen-
cies that satisfy the rule.
can yield further performance improvements.
Table 4 also shows the No-Split results where syn-
tactic categories are not refined. We find that such
refinement usually proves to be beneficial, yielding
an average performance gain of 3.7%. However, we
note that the impact of incorporating splitting varies
significantly across languages. Further understand-
ing of this connection is an area of future research.
Finally, we note that our model exhibits low vari-
ance for most languages. This result attests to how
the expectation constraints consistently guide infer-
ence toward high-accuracy areas of the search space.
Ablation Analysis Our next experiment seeks to
understand the relative importance of the various
universal rules from Table 1. We study how accu-
racy is affected when each of the rules is removed
one at a time for English and Spanish. Table 5 lists
the rules with the greatest impact on performance
when removed. We note the high overlap between
the most significant rules for English and Spanish.
We also observe that the relationship between
a rule?s frequency and its importance for high ac-
curacy is not straightforward. For example, the
?Preposition ? Noun? rule, whose removal de-
grades accuracy the most for both English and Span-
1241
50?
55?
60?
65?
70?
75?
Gold? 70? 75? 80? 85? 90?
Ac
cu
rac
y?
Constraints?Threshold?
Average? English?
Figure 2: Accuracy of our model with different threshold
settings, on English only and averaged over all languages.
?Gold? refers to the setting where each language?s thresh-
old is set independently to the proportion of gold depen-
dencies satisfying the rules ? for English this proportion
is 70%, while the average proportion across languages is
63%.
ish, is not the most frequent rule in either language.
This result suggests that some rules are harder to
learn than others regardless of their frequency, so
their presence in the specified ruleset yields stronger
performance gains.
Varying the Constraint Threshold In our main
experiments we require that at least 80% of the ex-
pected dependencies satisfy the rule constraints. We
arrived at this threshold by tuning on the basis of En-
glish only. As shown in Figure 2, for English a broad
band of threshold values from 75% to 90% yields re-
sults within 2.5% of each other, with a slight peak at
80%.
To further study the sensitivity of our method to
how the threshold is set, we perform post hoc ex-
periments with other threshold values on each of the
other languages. As Figure 2 also shows, on average
a value of 80% is optimal across languages, though
again accuracy is stable within 2.5% between thresh-
olds of 75% to 90%. These results demonstrate that
a single threshold is broadly applicable across lan-
guages.
Interestingly, setting the threshold value indepen-
dently for each language to its ?true? proportion
based on the gold dependencies (denoted as the
?Gold? case in Figure 2) does not achieve optimal
Length
? 10 ? 20
Universal Dependency Rules
1 HDP-DEP 71.9 50.4
No Rules (Random Init)
2 HDP-DEP 24.9 24.4
3 Headden III et al (2009) 68.8 -
English-Specific Parsing Rules
4 Deterministic (rules only) 70.0 62.6
5 HDP-DEP 73.8 66.1
Druck et al (2009) Rules
6 Druck et al (2009) 61.3 -
7 HDP-DEP 64.9 42.2
Table 6: Directed accuracy of our model (HDP-DEP) on
sentences of length 10 or less and 20 or less from WSJ
with different rulesets and with no rules, along with vari-
ous baselines from the literature. Entries in this table are
numbered for ease of reference in the text.
performance. Thus, knowledge of the true language-
specific rule proportions is not necessary for high
accuracy.
7.2 Analysis of Model Properties
We perform a set of additional experiments on En-
glish to gain further insight into HDP-DEP?s behav-
ior. Our choice of language is motivated by the
fact that a wide range of prior parsing algorithms
were developed for and tested exclusively on En-
glish. The experiments below demonstrate that 1)
universal rules alone are powerful, but language-
and dataset-tailored rules can further improve per-
formance; 2) our model learns jointly from the
rules and data, outperforming a rules-only deter-
ministic parser; 3) the way we incorporate posterior
constraints outperforms the generalized expectation
constraint framework; and 4) our model exhibits low
variance when seeded with different initializations.
These results are summarized in Table 6 and dis-
cussed in detail below; line numbers refer to entries
in Table 6. Each run of HDP-DEP below is with
syntactic refinement enabled.
Impact of Rules Selection We compare the per-
formance of HDP-DEP using the universal rules ver-
sus a set of rules designed for deterministically pars-
ing the Penn Treebank (see Section 5 for details).
1242
As lines 1 and 5 of Table 6 show, language-specific
rules yield better performance. For sentences of
length 10 or less, the difference between the two
rulesets is a relatively small 1.9%; for longer sen-
tences, however, the difference is a substantially
larger 15.7%. This is likely because longer sen-
tences tend to be more complex and thus exhibit
more language-idiosyncratic dependencies. Such
dependencies can be better captured by the refined
language-specific rules.
We also test model performance when no linguis-
tic rules are available, i.e., performing unconstrained
variational inference. The model performs substan-
tially worse (line 2), confirming that syntactic cat-
egory refinement in a fully unsupervised setup is
challenging.
Learning Beyond Provided Rules Since HDP-
DEP is provided with linguistic rules, a legitimate
question is whether it improves upon what the rules
encode, especially when the rules are complete and
language-specific. We can answer this question by
comparing the performance of our model seeded
with the English-specific rules against a determin-
istic parser that implements the same rules. Lines
4 and 5 of Table 6 demonstrate that the model out-
performs a rules-only deterministic parser by 3.8%
for sentences of length 10 or less and by 3.5% for
sentences of length 20 or less.
Comparison with Alternative Semi-supervised
Parser The dependency parser based on the gen-
eralized expectation criteria (Druck et al, 2009) is
the closest to our reported work in terms of tech-
nique. To compare the two, we run HDP-DEP using
the 20 rules given by Druck et al (2009). Our model
achieves an accuracy of 64.9% (line 7) compared to
61.3% (line 6) reported in their work. Note that we
do not rely on rule-specific expectation information
as they do, instead requiring only a single expecta-
tion constraint parameter.4
Model Stability It is commonly acknowledged
in the literature that unsupervised grammar induc-
tion methods exhibit sensitivity to initialization.
As in the previous section, we find that the pres-
4As explained in Section 5, having a single expectation pa-
rameter is motivated by our focus on parsing with universal
rules.
ence of linguistic rules greatly reduces this sensitiv-
ity: for HDP-DEP, the standard deviation over five
randomly initialized runs with the English-specific
rules is 1.5%, compared to 4.5% for the parser de-
veloped by Headden III et al (2009) and 8.0% for
DMV (Klein and Manning, 2004).
8 Conclusions
In this paper we demonstrated that syntactic uni-
versals encoded as declarative constraints improve
grammar induction. We formulated a generative
model for dependency structure that models syntac-
tic category refinement and biases inference to co-
here with the provided constraints. Our experiments
showed that encoding a compact, well-accepted set
of language-independent constraints significantly
improves accuracy on multiple languages compared
to the current state-of-the-art in unsupervised pars-
ing.
While our present work has yielded substantial
gains over previous unsupervised methods, a large
gap still remains between our method and fully su-
pervised techniques. In future work we intend to
study ways to bridge this gap by 1) incorporat-
ing more sophisticated linguistically-driven gram-
mar rulesets to guide induction, 2) lexicalizing the
model, and 3) combining our constraint-based ap-
proach with richer unsupervised models (e.g., Head-
den III et al (2009)) to benefit from their comple-
mentary strengths.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168, grant IIS-0904684,
and a Graduate Research Fellowship). We are es-
pecially grateful to Michael Collins for inspiring us
toward this line of inquiry and providing determin-
istic rules for English parsing. Thanks to Taylor
Berg-Kirkpatrick, Sabine Iatridou, Ramesh Sridha-
ran, and members of the MIT NLP group for their
suggestions and comments. Any opinions, findings,
conclusions, or recommendations expressed in this
paper are those of the authors, and do not necessar-
ily reflect the views of the funding organizations.
1243
References
Mark C. Baker. 2001. The Atoms of Language: The
Mind?s Hidden Rules of Grammar. Basic Books.
Emily M. Bender. 2009. Linguistically na??ve != lan-
guage independent: Why NLP needs linguistic typol-
ogy. In Proceedings of the EACL 2009 Workshop
on the Interaction between Linguistics and Compu-
tational Linguistics: Virtuous, Vicious or Vacuous?,
pages 26?32.
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phylo-
genetic grammar induction. In Proceedings of ACL,
pages 1288?1297.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Information Science and Statis-
tics. Springer.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Andrew Carnie. 2002. Syntax: A Generative Introduc-
tion (Introducing Linguistics). Blackwell Publishing.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of ACL, pages 280?
287.
Shay B. Cohen and Noah A. Smith. 2009a. Shared lo-
gistic normal distributions for soft parameter tying in
unsupervised grammar induction. In Proceedings of
NAACL/HLT, pages 74?82.
Shay B. Cohen and Noah A. Smith. 2009b. Variational
inference for grammar induction with prior knowl-
edge. In Proceedings of ACL/IJCNLP 2009 Confer-
ence Short Papers, pages 1?4.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, University
of Pennsylvania.
Hal Daume? III and Lyle Campbell. 2007. A bayesian
model for discovering typological implications. In
Proceedings of ACL, pages 65?72.
Gregory Druck, Gideon Mann, and Andrew McCal-
lum. 2009. Semi-supervised learning of dependency
parsers using generalized expectation criteria. In Pro-
ceedings of ACL/IJCNLP, pages 360?368.
Thomas S. Ferguson. 1973. A bayesian analysis of
some nonparametric problems. Annals of Statistics,
1(2):209?230.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In Proceedings of
ACL, pages 272?279.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of ACL/IJCNLP,
pages 369?377.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. Journal of Machine
Learning Research, 11:2001?2049.
Joa?o Grac?a, Kuzman Ganchev, Ben Taskar, and Fernando
Pereira. 2009. Posterior vs. parameter sparsity in la-
tent variable models. In Advances in NIPS, pages 664?
672.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS, pages 569?576.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
grammar induction. In Proceedings of ACL, pages
881?888.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of NAACL/HLT, pages 101?109.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 478?485.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of ACL, pages
470?477.
Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.
2007. The infinite PCFG using hierarchical Dirichlet
processes. In Proceedings of EMNLP/CoNLL, pages
688?697.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009a.
Learning from measurements in exponential families.
In Proceedings of ICML, pages 641?648.
Percy Liang, Michael I. Jordan, and Dan Klein. 2009b.
Probabilistic grammars and hierarchical Dirichlet pro-
cesses. The Handbook of Applied Bayesian Analysis.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of english: The penn treebank. Computational
Linguistics, 19(2):313?330.
Frederick J. Newmeyer. 2005. Possible and Probable
Languages: A Generative Perspective on Linguistic
Typology. Oxford University Press.
Slav Petrov and Dan Klein. 2007. Learning and infer-
ence for hierarchically split PCFGs. In Proceeding of
AAAI, pages 1663?1666.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of ACL/IJCNLP, pages 73?81.
Lydia White. 2003. Second Language Acquisition and
Universal Grammar. Cambridge University Press.
1244
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 530?540,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
In-domain Relation Discovery with Meta-constraints
via Posterior Regularization
Harr Chen, Edward Benson, Tahira Naseem, and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{harr, eob, tahira, regina} @csail.mit.edu
Abstract
We present a novel approach to discovering re-
lations and their instantiations from a collec-
tion of documents in a single domain. Our
approach learns relation types by exploiting
meta-constraints that characterize the general
qualities of a good relation in any domain.
These constraints state that instances of a
single relation should exhibit regularities at
multiple levels of linguistic structure, includ-
ing lexicography, syntax, and document-level
context. We capture these regularities via the
structure of our probabilistic model as well
as a set of declaratively-specified constraints
enforced during posterior inference. Across
two domains our approach successfully recov-
ers hidden relation structure, comparable to
or outperforming previous state-of-the-art ap-
proaches. Furthermore, we find that a small
set of constraints is applicable across the do-
mains, and that using domain-specific con-
straints can further improve performance. 1
1 Introduction
In this paper, we introduce a novel approach for the
unsupervised learning of relations and their instan-
tiations from a set of in-domain documents. Given
a collection of news articles about earthquakes, for
example, our method discovers relations such as the
earthquake?s location and resulting damage, and ex-
tracts phrases representing the relations? instantia-
tions. Clusters of similar in-domain documents are
1The source code for this work is available at:
http://groups.csail.mit.edu/rbg/code/relation extraction/
A strong earthquake rocked the Philippine island of Min-
doro early Tuesday, [destroying]ind [some homes]arg ...
A strong earthquake hit the China-Burma border early
Wednesday ... The official Xinhua News Agency said
[some houses]arg were [damaged]ind ...
A strong earthquake with a preliminary magnitude of 6.6
shook northwestern Greece on Saturday, ... [destroying]ind
[hundreds of old houses]arg ...
Figure 1: Excerpts from newswire articles about earth-
quakes. The indicator and argument words for the dam-
age relation are highlighted.
increasingly available in forms such as Wikipedia ar-
ticle categories, financial reports, and biographies.
In contrast to previous work, our approach learns
from domain-independent meta-constraints on rela-
tion expression, rather than supervision specific to
particular relations and their instances. In particular,
we leverage the linguistic intuition that documents
in a single domain exhibit regularities in how they
express their relations. These regularities occur both
in the relations? lexical and syntactic realizations as
well as at the level of document structure. For in-
stance, consider the damage relation excerpted from
earthquake articles in Figure 1. Lexically, we ob-
serve similar words in the instances and their con-
texts, such as ?destroying? and ?houses.? Syntacti-
cally, in two instances the relation instantiation is the
dependency child of the word ?destroying.? On the
discourse level, these instances appear toward the
beginning of their respective documents. In general,
valid relations in many domains are characterized by
these coherence properties.
We capture these regularities using a Bayesian
model where the underlying relations are repre-
530
sented as latent variables. The model takes as in-
put a constituent-parsed corpus and explains how the
constituents arise from the latent variables. Each re-
lation instantiation is encoded by the variables as
a relation-evoking indicator word (e.g., ?destroy-
ing?) and corresponding argument constituent (e.g.,
?some homes?).2 Our approach capitalizes on rela-
tion regularity in two ways. First, the model?s gen-
erative process encourages coherence in the local
features and placement of relation instances. Sec-
ond, we apply posterior regularization (Grac?a et
al., 2007) during inference to enforce higher-level
declarative constraints, such as requiring indicators
and arguments to be syntactically linked.
We evaluate our approach on two domains pre-
viously studied for high-level document structure
analysis, news articles about earthquakes and finan-
cial markets. Our results demonstrate that we can
successfully identify domain-relevant relations. We
also study the importance and effectiveness of the
declaratively-specified constraints. In particular, we
find that a small set of declarative constraints are
effective across domains, while additional domain-
specific constraints yield further benefits.
2 Related Work
Extraction with Reduced Supervision Recent
research in information extraction has taken large
steps toward reducing the need for labeled data. Ex-
amples include using bootstrapping to amplify small
seed sets of example outputs (Agichtein and Gra-
vano, 2000; Yangarber et al, 2000; Bunescu and
Mooney, 2007; Zhu et al, 2009), leveraging ex-
isting databases that overlap with the text (Mintz
et al, 2009; Yao et al, 2010), and learning gen-
eral domain-independent knowledge bases by ex-
ploiting redundancies in large web and news cor-
pora (Hasegawa et al, 2004; Shinyama and Sekine,
2006; Banko et al, 2007; Yates and Etzioni, 2009).
Our approach is distinct in both the supervision
and data we operate over. First, in contrast to boot-
strapping and database matching approaches, we
learn from meta-qualities, such as low variability in
syntactic patterns, that characterize a good relation.
2We do not use the word ?argument? in the syntactic sense?
a relation?s argument may or may not be the syntactic depen-
dency argument of its indicator.
We hypothesize that these properties hold across re-
lations in different domains. Second, in contrast to
work that builds general relation databases from het-
erogeneous corpora, our focus is on learning the re-
lations salient in a single domain. Our setup is more
germane to specialized domains expressing informa-
tion not broadly available on the web.
Earlier work in unsupervised information extrac-
tion has also leveraged meta-knowledge indepen-
dent of specific relation types, such as declaratively-
specified syntactic patterns (Riloff, 1996), frequent
dependency subtree patterns (Sudo et al, 2003), and
automatic clusterings of syntactic patterns (Lin and
Pantel, 2001; Zhang et al, 2005) and contexts (Chen
et al, 2005; Rosenfeld and Feldman, 2007). Our ap-
proach incorporates a broader range of constraints
and balances constraints with underlying patterns
learned from the data, thereby requiring more so-
phisticated machinery for modeling and inference.
Extraction with Constraints Previous work has
recognized the appeal of applying declarative con-
straints to extraction. In a supervised setting, Roth
and Yih (2004) induce relations by using linear pro-
gramming to impose global declarative constraints
on the output from a set of classifiers trained on lo-
cal features. Chang et al (2007) propose an objec-
tive function for semi-supervised extraction that bal-
ances likelihood of labeled instances and constraint
violation on unlabeled instances. Recent work has
also explored how certain kinds of supervision can
be formulated as constraints on model posteriors.
Such constraints are not declarative, but instead
based on annotations of words? majority relation la-
bels (Mann and McCallum, 2008) and pre-existing
databases with the desired output schema (Bellare
and McCallum, 2009). In contrast to previous work,
our approach explores a different class of constraints
that does not rely on supervision that is specific to
particular relation types and their instances.
3 Model
Our work performs in-domain relation discovery by
leveraging regularities in relation expression at the
lexical, syntactic, and discourse levels. These regu-
larities are captured via two components: a proba-
bilistic model that explains how documents are gen-
erated from latent relation variables and a technique
531
? ? ? ?
is_verb 0 1 0
earthquake 1 0 0
hit 0 1 0
? ? ? ?
has_proper 0 0 1
has_number 0 0 0
depth 1 3 2
Figure 2: Words w and constituents x of syntactic parses
are represented with indicator features ?i and argument
features ?a respectively. A single relation instantiation is
a pair of indicator w and argument x; we filter w to be
nouns and verbs and x to be noun phrases and adjectives.
for biasing inference to adhere to declaratively-
specified constraints on relation expression. This
section describes the generative process, while Sec-
tions 4 and 5 discuss declarative constraints.
3.1 Problem Formulation
Our input is a corpus of constituent-parsed docu-
ments and a number K of relation types. The output
is K clusters of semantically related relation instan-
tiations. We represent these instantiations as a pair
of indicator word and argument sequence from the
same sentence. The indicator?s role is to anchor a
relation and identify its type. We only allow nouns
or verbs to be indicators. For instance, in the earth-
quake domain a likely indicator for damage would
be ?destroyed.? The argument is the actual rela-
tion value, e.g., ?some homes,? and corresponds to
a noun phrase or adjective.3
Along with the document parse trees, we utilize
a set of features ?i(w) and ?a(x) describing each
potential indicator word w and argument constituent
x, respectively. An example feature representation
is shown in Figure 2. These features can encode
words, part-of-speech tags, context, and so on. Indi-
cator and argument feature definitions need not be
the same (e.g., has number is important for argu-
3In this paper we focus on unary relations; binary relations
can be modeled with extensions of the hidden variables and con-
straints.
ments but irrelevant for indicators).4
3.2 Generative Process
Our model associates each relation type k with a set
of feature distributions ?k and a location distribution
?k. Each instantiation?s indicator and argument, and
its position within a document, are drawn from these
distributions. By sharing distributions within each
relation, the model places high probability mass on
clusters of instantiations that are coherent in features
and position. Furthermore, we allow at most one in-
stantiation per document and relation, so as to target
relations that are relevant to the entire document.
There are three steps to the generative process.
First, we draw feature and location distributions for
each relation. Second, an instantiation is selected
for every pair of document d and relation k. Third,
the indicator features of each word and argument
features of each constituent are generated based on
the relation parameters and instantiations. Figure 3
presents a reference for the generative process.
Generating Relation Parameters Each relation k
is associated with four feature distribution param-
eter vectors: ?ik for indicator words, ?
bi
k for non-
indicator words, ?ak for argument constituents, and
?bak for non-argument constituents. Each of these is
a set of multinomial parameters per feature drawn
from a symmetric Dirichlet prior. A likely indica-
tor word should have features that are highly proba-
ble according to ?ik, and likewise for arguments and
?ak. Parameters ?
bi
k and ?
ba
k represent background dis-
tributions for non-relation words and constituents,
similar in spirit to other uses of background distri-
butions that filter out irrelevant words (Che, 2006).5
By drawing each instance from these distributions,
we encourage the relation to be coherent in local lex-
ical and syntactic properties.
Each relation type k is also associated with a pa-
rameter vector ?k over document segments drawn
from a symmetric Dirichlet prior. Documents are
divided into L equal-length segments; ?k states how
likely relation k is for each segment, with one null
outcome for the relation not occurring in the doc-
ument. Because ?k is shared within a relation, its
4We consider only categorical features here, though the ex-
tension to continuous or ordinal features is straightforward.
5We use separate background distributions for each relation
to make inference more tractable.
532
For each relation type k:
? For each indicator feature ?i draw feature distri-
butions ?ik,?i , ?
bi
k,?i ? Dir(?0)
? For each argument feature ?a draw feature dis-
tributions ?ak,?a , ?
ba
k,?a ? Dir(?0)
? Draw location distribution ?k ? Dir(?0)
For each relation type k and document d:
? Select document segment sd,k ? Mult(?k)
? Select sentence zd,k uniformly from segment
sd,k, and indicator id,k and argument ad,k uni-
formly from sentence zd,k
For each word w in every document d:
? Draw each indicator feature ?i(w) ?
Mult
(
1
Z
?K
k=1 ?k,?i
)
, where ?k,?i is ?
i
k,?i
if id,k = w and ?bik,?i otherwise
For each constituent x in every document d:
? Draw each argument feature ?a(x) ?
Mult
(
1
Z
?K
k=1 ?k,?a
)
, where ?k,?a is ?ak,?a
if ad,k = x and ?bak,?a otherwise
Figure 3: The generative process for model parameters
and features. In the above Dir and Mult refer respectively
to the Dirichlet distribution and multinomial distribution.
Fixed hyperparameters are subscripted with zero.
instances will tend to occur in the same relative po-
sitions across documents. The model can learn, for
example, that a particular relation typically occurs in
the first quarter of a document (if L = 4).
Generating Relation Instantiations For every rela-
tion type k and document d, we first choose which
portion of the document (if any) contains the instan-
tiation by drawing a document segment sd,k from
?k. Our model only draws one instantiation per pair
of k and d, so each discovered instantiation within a
document is a separate relation. We then choose the
specific sentence zd,k uniformly from within the seg-
ment, and the indicator word id,k and argument con-
stituent ad,k uniformly from within that sentence.
Generating Text Finally, we draw the feature val-
ues. We make a Na??ve Bayes assumption between
features, drawing each independently conditioned
on relation structure. For a word w, we want all re-
lations to be able to influence its generation. Toward
this end, we compute the element-wise product of
feature parameters across relations k = 1, . . . ,K,
using indicator parameters ?ik if relation k selected
w as an indicator word (if id,k = w) and background
parameters ?bik otherwise. The result is then normal-
ized to form a valid multinomial that produces word
w?s features. Constituents are drawn similarly from
every relations? argument distributions.
4 Inference with Constraints
The model presented above leverages relation reg-
ularities in local features and document placement.
However, it is unable to specify global syntactic
preferences about relation expression, such as indi-
cators and arguments being in the same clause. An-
other issue with this model is that different relations
could overlap in their indicators and arguments.6
To overcome these obstacles, we apply declara-
tive constraints by imposing inequality constraints
on expectations of the posterior during inference
using posterior regularization (Grac?a et al, 2007).
In this section we present the technical details
of the approach; Section 5 explains the specific
linguistically-motivated constraints we consider.
4.1 Inference with Posterior Regularization
We first review how posterior regularization impacts
the variational inference procedure in general. Let
?, z, and x denote the parameters, hidden struc-
ture, and observations of an arbitrary model. We
are interested in estimating the posterior distribution
p(?, z | x) by finding a distribution q(?, z) ? Q that
is minimal in KL-divergence to the true posterior:
KL(q(?, z) ? p(?, z | x))
=
?
q(?, z) log
q(?, z)
p(?, z, x)
d?dz + log p(x). (1)
For tractability, variational inference typically
makes a mean-field assumption that restricts the set
Q to distributions where ? and z are independent,
i.e., q(?, z) = q(?)q(z). We then optimize equa-
tion 1 by coordinate-wise descent on q(?) and q(z).
To incorporate constraints into inference, we fur-
ther restrict Q to distributions that satisfy a given
6In fact, a true maximum a posteriori estimate of the model
parameters would find the same most salient relation over and
over again for every k, rather than finding K different relations.
533
set of inequality constraints, each of the form
Eq[f(z)] ? b. Here, f(z) is a deterministic func-
tion of z and b is a user-specified threshold. Inequal-
ities in the opposite direction simply require negat-
ing f(z) and b. For example, we could apply a syn-
tactic constraint of the form Eq[f(z)] ? b, where
f(z) counts the number of indicator/argument pairs
that are syntactically connected in a pre-specified
manner (e.g., the indicator and argument modify the
same verb), and b is a fixed threshold.
Given a set C of constraints with functions fc(z)
and thresholds bc, the updates for q(?) and q(z) from
equation 1 are as follows:
q(?) = argmin
q(?)
KL
(
q(?) ? q?(?)
)
, (2)
where q?(?) ? expEq(z)[log p(?, z, x)], and
q(z) = argmin
q(z)
KL
(
q(z) ? q?(z)
)
s.t. Eq(z)[fc(z)] ? bc, ?c ? C, (3)
where q?(z) ? expEq(?)[log p(?, z, x)]. Equation 2
is not affected by the posterior constraints and is up-
dated by setting q(?) to q?(?). We solve equation 3
in its dual form (Grac?a et al, 2007):
argmin
?
?
c?C
?cbc + log
?
z
q?(z)e?
P
c?C ?cfc(z)
s.t. ?c ? 0, ?c ? C. (4)
With the box constraints of equation 4, a numerical
optimization procedure such as L-BFGS-B (Byrd
et al, 1995) can be used to find optimal dual pa-
rameters ??. The original q(z) is then updated to
q?(z) exp
(
?
?
c?C ?
?
cfc(z)
)
and renormalized.
4.2 Updates for our Model
Our model uses this mean-field factorization:
q(?, ?, z, a, i)
=
K?
k=1
q(?k; ??k)q(?
i
k; ??
i
k)q(?
bi
k ; ??
bi
k )q(?
a
k; ??
a
k)q(?
ba
k ; ??
ba
k )
?
?
d
q(zd,k, ad,k, id,k; c?d,k) (5)
In the above, ?? and ?? are Dirichlet distribution pa-
rameters, and c? are multinomial parameters. Note
that we do not factorize the distribution of z, i, and
a for a single document and relation, instead repre-
senting their joint distribution with a single set of
variational parameters c?. This is tractable because a
single relation occurs only once per document, re-
ducing the joint search space of z, i, and a. The
factors in equation 5 are updated one at a time while
holding the other factors fixed.
Updating ?? Due to the Na??ve Bayes assumption
between features, each feature?s q(?) distributions
can be updated separately. However, the product
between feature parameters of different relations in-
troduces a nonconjugacy in the model, precluding
a closed form update. Instead we numerically opti-
mize equation 1 with respect to each ??, similarly to
previous work (Boyd-Graber and Blei, 2008). For
instance, ??ik,? of relation k and feature ? is updated
by finding the gradient of equation 1 with respect to
??ik,? and applying L-BFGS. Parameters ??
bi, ??a, and
??ba are updated analogously.
Updating ?? This update follows the standard
closed form for Dirichlet parameters:
??k,` = ?0 + Eq(z,a,i)[C`(z, a, i)], (6)
whereC` counts the number of times z falls into seg-
ment ` of a document.
Updating c? Parameters c? are updated by first com-
puting an unconstrained update q?(z, a, i; c??):
c??d,k,(z,a,i) ? exp
?
?Eq(?k)[log p(z, a, i | ?k)]
+ Eq(?ik)[log p(i | ?
i
k)] +
?
w 6=i
Eq(?bik )[log p(w | ?
bi
k )]
+ Eq(?ak)[log p(a | ?
a
k)] +
?
x 6=a
Eq(?bak )[log p(x | ?
ba
k )]
?
?
We then perform the minimization on the dual in
equation 4 under the provided constraints to derive a
final update to the constrained c?.
Simplifying Approximation The update for ?? re-
quires numerical optimization due to the nonconju-
gacy introduced by the point-wise product in fea-
ture generation. If instead we have every relation
type separately generate a copy of the corpus, the ??
534
Quantity f(z, a, i) ? or ? b
Syntax ?k Counts i, a of relation k that match a pattern (see text) ? 0.8D
Prevalence ?k Counts instantiations of relation k ? 0.8D
Separation (ind) ?w Counts times w selected as i ? 2
Separation (arg) ?w Counts times w selected as part of a ? 1
Table 1: Each constraint takes the form Eq[f(z, a, i)] ? b or Eq[f(z, a, i)] ? b; D denotes the number of corpus
documents, ?k means one constraint per relation type, and ?w means one constraint per token in the corpus.
updates becomes closed-form expressions similar to
equation 6. This approximation yields similar pa-
rameter estimates as the true updates while vastly
improving speed, so we use it in our experiments.
5 Declarative Constraints
We now have the machinery to incorporate a va-
riety of declarative constraints during inference.
The classes of domain-independent constraints we
study are summarized in Table 1. For the propor-
tion constraints we arbitrarily select a threshold of
80% without any tuning, in the spirit of building a
domain-independent approach.
Syntax As previous work has observed, most rela-
tions are expressed using a limited number of com-
mon syntactic patterns (Riloff, 1996; Banko and Et-
zioni, 2008). Our syntactic constraint captures this
insight by requiring that a certain proportion of the
induced instantiations for each relation match one of
these syntactic patterns:
? The indicator is a verb and the argument?s
headword is either the child or grandchild of
the indicator word in the dependency tree.
? The indicator is a noun and the argument is a
modifier or complement.
? The indicator is a noun in a verb?s subject and
the argument is in the corresponding object.
Prevalence For a relation to be domain-relevant, it
should occur in numerous documents across the cor-
pus, so we institute a constraint on the number of
times a relation is instantiated. Note that the effect
of this constraint could also be achieved by tuning
the prior probability of a relation not occurring in a
document. However, this prior would need to be ad-
justed every time the number of documents or fea-
ture selection changes; using a constraint is an ap-
pealing alternative that is portable across domains.
Separation The separation constraint encourages
diversity in the discovered relation types by restrict-
ing the number of times a single word can serve as
either an indicator or part of the argument of a re-
lation instance. Specifically, we require that every
token of the corpus occurs at most once as a word
in a relation?s argument in expectation. On the other
hand, a single word can sometimes be evocative of
multiple relations (e.g., ?occurred? signals both date
and time in ?occurred on Friday at 3pm?). Thus, we
allow each word to serve as an indicator more than
once, arbitrarily fixing the limit at two.
6 Experimental Setup
Datasets and Metrics We evaluate on two datasets,
financial market reports and newswire articles about
earthquakes, previously used in work on high-level
content analysis (Barzilay and Lee, 2004; Lap-
ata, 2006). Finance articles chronicle daily mar-
ket movements of currencies and stock indexes, and
earthquake articles document specific earthquakes.
Constituent parses are obtained automatically us-
ing the Stanford parser (Klein and Manning, 2003)
and then converted to dependency parses using the
PennConvertor tool (Johansson and Nugues, 2007).
We manually annotated relations for both corpora,
selecting relation types that occurred frequently in
each domain. We found 15 types for finance and
9 for earthquake. Corpus statistics are summarized
below, and example relation types are shown in Ta-
ble 2.
Docs Sent/Doc Tok/Doc Vocab
Finance 100 12.1 262.9 2918
Earthquake 200 9.3 210.3 3155
In our task, annotation conventions for desired
output relations can greatly impact token-level per-
formance, and the model cannot learn to fit a par-
ticular convention by looking at example data. For
example, earthquakes times are frequently reported
in both local and GMT, and either may be arbitrar-
ily chosen as correct. Moreover, the baseline we
535
F
in
an
ce Bond 104.58 yen, 98.37 yen
Dollar Change up 0.52 yen, down 0.01 yen
Tokyo Index Change down 5.38 points or 0.41 percent, up 0.16 points, insignificant in percentage terms
E
ar
th
qu
ak
e Damage about 10000 homes, some buildings, no information
Epicenter
Patuca about 185 miles (300 kilometers) south of Quito, 110 kilometers (65 miles)
from shore under the surface of the Flores sea in the Indonesian archipelago
Magnitude 5.7, 6, magnitude-4
Table 2: Example relation types identified in the finance and earthquake datasets with example instance arguments.
compare against produces lambda calculus formulas
rather than spans of text as output, so a token-level
comparison requires transforming its output.
For these reasons, we evaluate on both sentence-
level and token-level precision, recall, and F-score.
Precision is measured by mapping every induced re-
lation cluster to its closest gold relation and comput-
ing the proportion of predicted sentences or words
that are correct. Conversely, for recall we map ev-
ery gold relation to its closest predicted relation and
find the proportion of gold sentences or words that
are predicted. This mapping technique is based on
the many-to-one scheme used for evaluating unsu-
pervised part-of-speech induction (Johnson, 2007).
Note that sentence-level scores are always at least as
high as token-level scores, since it is possible to se-
lect a sentence correctly but none of its true relation
tokens while the opposite is not possible.
Domain-specific Constraints On top of the cross-
domain constraints from Section 5, we study
whether imposing basic domain-specific constraints
can be beneficial. The finance dataset is heav-
ily quantitative, so we consider applying a single
domain-specific constraint stating that most rela-
tion arguments should include a number. Likewise,
earthquake articles are typically written with a ma-
jority of the relevant information toward the begin-
ning of the document, so its domain-specific con-
straint is that most relations should occur in the
first two sentences of a document. Note that these
domain-specific constraints are not specific to in-
dividual relations or instances, but rather encode a
preference across all relation types. In both cases,
we again use an 80% threshold without tuning.
Features For indicators, we use the word, part of
speech, and word stem. For arguments, we use the
word, syntactic constituent label, the head word of
the parent constituent, and the dependency label of
the argument to its parent.
Baselines We compare against three alternative un-
supervised approaches. Note that the first two only
identify relation-bearing sentences, not the specific
words that participate in the relation.
Clustering (CLUTO): A straightforward way of
identifying sentences bearing the same relation is
to simply cluster them. We implement a cluster-
ing baseline using the CLUTO toolkit with word and
part-of-speech features. As with our model, we set
the number of clusters K to the true number of rela-
tion types.
Mallows Topic Model (MTM): Another technique
for grouping similar sentences is the Mallows-based
topic model of Chen et al (2009). The datasets we
consider here exhibit high-level regularities in con-
tent organization, so we expect that a topic model
with global constraints could identify plausible clus-
ters of relation-bearing sentences. Again, K is set to
the true number of relation types.
Unsupervised Semantic Parsing (USP): Our fi-
nal unsupervised comparison is to USP, an unsuper-
vised deep semantic parser introduced by Poon and
Domingos (2009). USP induces a lambda calculus
representation of an entire corpus and was shown to
be competitive with open information extraction ap-
proaches (Lin and Pantel, 2001; Banko et al, 2007).
We give USP the required Stanford dependency for-
mat as input (de Marneffe and Manning, 2008). We
find that the results are sensitive to the cluster granu-
larity prior, so we tune this parameter and report the
best-performing runs.
We recognize that USP targets a different out-
put representation than ours: a hierarchical semantic
structure over the entirety of a dependency-parsed
text. In contrast, we focus on discovering a limited
numberK of domain-relevant relations expressed as
constituent phrases. Despite these differences, both
536
methods ultimately aim to capture domain-specific
relations expressed with varying verbalizations, and
both operate over in-domain input corpora supple-
mented with syntactic information. For these rea-
sons, USP provides a clear and valuable point of
comparison. For this comparison, we transform
USP?s lambda calculus formulas to relation spans as
follows. First, we group lambda forms by a combi-
nation of core form, argument form, and the parent?s
core form.7 We then filter to the K relations that
appear in the most documents. For token-level eval-
uation we take the dependency tree fragment corre-
sponding to the lambda form. For example, in the
sentence ?a strong earthquake rocked the Philippines
island of Mindoro early Tuesday,? USP learns that
the word ?Tuesday? has a core form corresponding
to words {Tuesday, Wednesday, Saturday}, a parent
form corresponding to words {shook, rock, hit, jolt},
and an argument form of TMOD; all phrases with
this same combination are grouped as a relation.
Training Regimes and Hyperparameters For each
run of our model we perform three random restarts
to convergence and select the posterior with lowest
final free energy. We fix K to the true number of
annotated relation types for both our model and USP
and L (the number of document segments) to five.
Dirichlet hyperparameters are set to 0.1.
7 Results
Table 3?s first two sections present the results of our
main evaluation. For earthquake, the far more diffi-
cult domain, our base model with only the domain-
independent constraints strongly outperforms all
three baselines across both metrics. For finance,
the CLUTO and USP baselines achieve performance
comparable to or slightly better than our base model.
Our approach, however, has the advantage of provid-
ing a formalism for seamlessly incorporating addi-
tional arbitrary domain-specific constraints. When
we add such constraints (denoted as model+DSC),
we achieve consistently higher performance than all
baselines across both datasets and metrics, demon-
strating that this approach provides a simple and ef-
fective framework for injecting domain knowledge
into relation discovery.
7This grouping mechanism yields better results than only
grouping by core form.
The first two baselines correspond to a setup
where the number of sentence clusters K is set to
the true number of relation types. This has the effect
of lowering precision because each sentence must be
assigned a cluster. To mitigate this impact, we exper-
imented with using K+N clusters, with N ranging
from 1 to 30. In each case, we then keep only the K
largest clusters. For the earthquake dataset, increas-
ing N improves performance until some point, after
which performance degrades. However, the best F-
Score corresponding to the optimal number of clus-
ters is 42.2, still far below our model?s 66.0 F-score.
For the finance domain, increasing the number of
clusters hurts performance.
Our results show a large gap in F-score between
the sentence and token-level evaluations for both the
USP baseline and our model. A qualitative analysis
of the results indicates that our model often picks up
on regularities that are difficult to distinguish with-
out relation-specific supervision. For earthquake, a
location may be annotated as ?the Philippine island
of Mindoro? while we predict just the word ?Min-
doro.? For finance, an index change can be anno-
tated as ?30 points, or 0.8 percent,? while our model
identifies ?30 points? and ?0.8 percent? as separate
relations. In practice, these outputs are all plausi-
ble discoveries, and a practitioner desiring specific
outputs could impose additional constraints to guide
relation discovery toward them.
The Impact of Constraints To understand the im-
pact of the declarative constraints, we perform an
ablation analysis on the constraint sets. We con-
sider removing the constraints on syntactic patterns
(no-syn) and the constraints disallowing relations to
overlap (no-sep) from the full domain-independent
model.8 We also try a version with hard syntac-
tic constraints (hard-syn), which requires that every
extraction match one of the three syntactic patterns
specified by the syntactic constraint.
Table 3?s bottom section presents the results of
this evaluation. The model?s performance degrades
when either of the two constraint sets are removed,
demonstrating that the constraints are in fact benefi-
cial for relation discovery. Additionally, in the hard-
syn case, performance drops dramatically for finance
8Prevalence constraints are always enforced, as otherwise
the prior on not instantiating a relation would need to be tuned.
537
Finance Earthquake
Sentence-level Token-level Sentence-level Token-level
Prec Rec F1 Prec Rec F1 Prec Rec F1 Prec Rec F1
Model 82.1 59.7 69.2 42.2 23.9 30.5 54.2 68.1 60.4 20.2 16.8 18.3
Model+DSC 87.3 81.6 84.4 51.8 30.0 38.0 66.4 65.6 66.0 22.6 23.1 22.8
CLUTO 56.3 92.7 70.0 ? ? ? 19.8 58.0 29.5 ? ? ?
MTM 40.4 99.3 57.5 ? ? ? 18.6 74.6 29.7 ? ? ?
USP 91.3 66.1 76.7 28.5 32.6 30.4 61.2 43.5 50.8 9.9 32.3 15.1
No-sep 97.8 35.4 52.0 86.1 8.7 15.9 42.2 21.9 28.8 16.1 4.6 7.1
No-syn 83.3 46.1 59.3 20.8 9.9 13.4 53.8 60.9 57.1 14.0 13.8 13.9
Hard-syn 47.7 39.0 42.9 11.6 7.0 8.7 55.0 66.2 60.1 20.1 17.3 18.6
Table 3: Top section: our model, with and without domain-specific constraints (DSC). Middle section: The three
baselines. Bottom section: ablation analysis of constraint sets for our model. For all scores, higher is better.
while remaining almost unchanged for earthquake.
This suggests that formulating constraints as soft in-
equalities on posterior expectations gives our model
the flexibility to accommodate both the underlying
signal in the data and the declarative constraints.
Comparison against Supervised CRF Our final
set of experiments compares a semi-supervised ver-
sion of our model against a conditional random field
(CRF) model. The CRF model was trained using
the same features as our model?s argument features.
To incorporate training examples in our model, we
simply treat annotated relation instances as observed
variables. For both the baselines and our model,
we experiment with using up to 10 annotated docu-
ments. At each of those levels of supervision, we av-
erage results over 10 randomly drawn training sets.
At the sentence level, our model compares very
favorably to the supervised CRF. For finance, it takes
at least 10 annotated documents (corresponding to
roughly 130 annotated relation instances) for the
CRF to match the semi-supervised model?s perfor-
mance. For earthquake, using even 10 annotated
documents (about 71 relation instances) is not suf-
ficient to match our model?s performance.
At the token level, the supervised CRF base-
line is far more competitive. Using a single la-
beled document (13 relation instances) yields su-
perior performance to either of our model variants
for finance, while four labeled documents (29 re-
lation instances) do the same for earthquake. This
result is not surprising?our model makes strong
domain-independent assumptions about how under-
lying patterns of regularities in the text connect to
relation expression. Without domain-specific super-
vision such assumptions are necessary, but they can
prevent the model from fully utilizing available la-
beled instances. Moreover, being able to annotate
even a single document requires a broad understand-
ing of every relation type germane to the domain,
which can be infeasible when there are many unfa-
miliar, complex domains to process.
In light of our strong sentence-level performance,
this suggests a possible human-assisted application:
use our model to identify promising relation-bearing
sentences in a new domain, then have a human an-
notate those sentences for use by a supervised ap-
proach to achieve optimal token-level extraction.
8 Conclusions
This paper has presented a constraint-based ap-
proach to in-domain relation discovery. We have
shown that a generative model augmented with
declarative constraints on the model posterior can
successfully identify domain-relevant relations and
their instantiations. Furthermore, we found that a
single set of constraints can be used across divergent
domains, and that tailoring constraints specific to a
domain can yield further performance benefits.
Acknowledgements
The authors gratefully acknowledge the support
of Defense Advanced Research Projects Agency
(DARPA) Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract
no. FA8750-09-C-0172. Any opinions, findings,
and conclusion or recommendations expressed in
this material are those of the authors and do not nec-
essarily reflect the view of the DARPA, AFRL, or
the US government. Thanks also to Hoifung Poon
and the members of the MIT NLP group for their
suggestions and comments.
538
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
In Proceedings of DL.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
HLT/NAACL.
Kedar Bellare and Andrew McCallum. 2009. Gen-
eralized expectation criteria for bootstrapping extrac-
tors using record-text alignment. In Proceedings of
EMNLP.
Jordan Boyd-Graber and David M. Blei. 2008. Syntactic
topic models. In Advances in NIPS.
Razvan C. Bunescu and Raymond J. Mooney. 2007.
Learning to extract relations from the web using mini-
mal supervision. In Proceedings of ACL.
Richard H. Byrd, Peihuang Lu, Jorge Nocedal, and Ciyou
Zhu. 1995. A limited memory algorithm for bound
constrained optimization. SIAM Journal on Scientific
Computing, 16(5):1190?1208.
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of ACL.
2006. Modeling general and specific aspects of docu-
ments with a probabilistic topic model. In Advances
in NIPS.
Jinxiu Chen, Dong-Hong Ji, Chew Lim Tan, and Zheng-
Yu Niu. 2005. Automatic relation extraction with
model order selection and discriminative label identi-
fication. In Proceedings of IJCNLP.
Harr Chen, S.R.K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using la-
tent permutations. Journal of Artificial Intelligence
Research, 36:129?163.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-framework and Cross-domain Parser Evalu-
ation.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS.
Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.
2004. Discovering relations among named entities
from large corpora. In Proceedings of ACL.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proceedings of NODALIDA.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proceedings of EMNLP.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering: Kendall?s tau. Computational Linguis-
tics, 32(4):471?484.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
SIGKDD.
Gideon S. Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learning
of conditional random fields. In Proceedings of ACL.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of ACL/IJCNLP.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In Proceedings of EMNLP.
Ellen Riloff. 1996. Automatically generating extraction
patterns from untagged texts. In Proceedings of AAAI.
Benjamin Rosenfeld and Ronen Feldman. 2007. Clus-
tering for unsupervised relation identification. In Pro-
ceedings of CIKM.
Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted relation
discovery. In Proceedings of HLT/NAACL.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic IE pattern acquisition. In Pro-
ceedings of ACL.
Roman Yangarber, Ralph Grishman, Pasi Tapanainen,
and Silja Huttunen. 2000. Automatic acquisition of
domain knowledge for information extraction. In Pro-
ceedings of COLING.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Cross-document relation extraction without la-
belled data. In Proceedings of EMNLP.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, and
Chew Lim Tan. 2005. Discovering relations between
named entities from a large raw corpus using tree
similarity-based clustering. In Proceedings of IJC-
NLP.
539
Jun Zhu, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and Ji-
Rong Wen. 2009. StatSnowball: a statistical approach
to extracting entity relationships. In Proceedings of
WWW.
540
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 629?637,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Selective Sharing for Multilingual Dependency Parsing
Tahira Naseem
CSAIL, MIT
tahira@csail.mit.edu
Regina Barzilay
CSAIL, MIT
regina@csail.mit.edu
Amir Globerson
Hebrew University
gamir@cs.huji.ac.il
Abstract
We present a novel algorithm for multilin-
gual dependency parsing that uses annotations
from a diverse set of source languages to parse
a new unannotated language. Our motiva-
tion is to broaden the advantages of multilin-
gual learning to languages that exhibit signif-
icant differences from existing resource-rich
languages. The algorithm learns which as-
pects of the source languages are relevant for
the target language and ties model parame-
ters accordingly. The model factorizes the
process of generating a dependency tree into
two steps: selection of syntactic dependents
and their ordering. Being largely language-
universal, the selection component is learned
in a supervised fashion from all the training
languages. In contrast, the ordering decisions
are only influenced by languages with simi-
lar properties. We systematically model this
cross-lingual sharing using typological fea-
tures. In our experiments, the model con-
sistently outperforms a state-of-the-art multi-
lingual parser. The largest improvement is
achieved on the non Indo-European languages
yielding a gain of 14.4%.1
1 Introduction
Current top performing parsing algorithms rely on
the availability of annotated data for learning the
syntactic structure of a language. Standard ap-
proaches for extending these techniques to resource-
lean languages either use parallel corpora or rely on
1The source code for the work presented in this paper is
available at http://groups.csail.mit.edu/rbg/code/unidep/
annotated trees from other source languages. These
techniques have been shown to work well for lan-
guage families with many annotated resources (such
as Indo-European languages). Unfortunately, for
many languages there are no available parallel cor-
pora or annotated resources in related languages.
For such languages the only remaining option is to
resort to unsupervised approaches, which are known
to produce highly inaccurate results.
In this paper, we present a new multilingual al-
gorithm for dependency parsing. In contrast to pre-
vious approaches, this algorithm can learn depen-
dency structures using annotations from a diverse
set of source languages, even if this set is not re-
lated to the target language. In our selective shar-
ing approach, the algorithm learns which aspects of
the source languages are relevant for the target lan-
guage and ties model parameters accordingly. This
approach is rooted in linguistic theory that charac-
terizes the connection between languages at various
levels of sharing. Some syntactic properties are uni-
versal across languages. For instance, nouns take ad-
jectives and determiners as dependents, but not ad-
verbs. However, the order of these dependents with
respect to the parent is influenced by the typological
features of each language.
To implement this intuition, we factorize genera-
tion of a dependency tree into two processes: selec-
tion of syntactic dependents and their ordering. The
first component models the distribution of depen-
dents for each part-of-speech tag, abstracting over
their order. Being largely language-universal, this
distribution can be learned in a supervised fashion
from all the training languages. On the other hand,
629
ordering of dependents varies greatly across lan-
guages and therefore should only be influenced by
languages with similar properties. Furthermore, this
similarity has to be expressed at the level of depen-
dency types ? i.e., two languages may share noun-
adposition ordering, but differ in noun-determiner
ordering. To systematically model this cross-lingual
sharing, we rely on typological features that reflect
ordering preferences of a given language. In addi-
tion to the known typological features, our parsing
model embeds latent features that can capture cross-
lingual structural similarities.
While the approach described so far supports a
seamless transfer of shared information, it does not
account for syntactic properties of the target lan-
guage unseen in the training languages. For in-
stance, in the CoNLL data, Arabic is the only lan-
guage with the VSO ordering. To handle such cases,
our approach augments cross-lingual sharing with
unsupervised learning on the target languages.
We evaluated our selective sharing model on 17
languages from 10 language families. On this di-
verse set, our model consistently outperforms state-
of-the-art multilingual dependency parsers. Per-
formance gain, averaged over all the languages, is
5.9% when compared to the highest baseline. Our
model achieves the most significant gains on non-
Indo-European languages, where we see a 14.4%
improvement. We also demonstrate that in the ab-
sence of observed typological information, a set of
automatically induced latent features can effectively
work as a proxy for typology.
2 Related Work
Traditionally, parallel corpora have been a main-
stay of multilingual parsing (Wu, 1997; Kuhn, 2004;
Smith and Smith, 2004; Hwa et al, 2005; Xi and
Hwa, 2005; Burkett and Klein, 2008; Snyder et al,
2009). However, recent work in multilingual pars-
ing has demonstrated the feasibility of transfer in the
absence of parallel data. As a main source of guid-
ance, these methods rely on the commonalities in de-
pendency structure across languages. For instance,
Naseem et al (2010) explicitly encode these similar-
ities in the form of universal rules which guide gram-
mar induction in the target language. An alterna-
tive approach is to directly employ a non-lexicalized
parser trained on one language to process a target
language (Zeman and Resnik, 2008; McDonald et
al., 2011; S?gaard, 2011). Since many unlexicalized
dependencies are preserved across languages, these
approaches are shown to be effective for related
languages. For instance, when applied to the lan-
guage pairs within the Indo-European family, such
parsers outperform unsupervised monolingual tech-
niques by a significant margin.
The challenge, however, is to enable dependency
transfer for target languages that exhibit structural
differences from source languages. In such cases,
the extent of multilingual transfer is determined by
the relation between source and target languages.
Berg-Kirkpatrick and Klein (2010) define such a re-
lation in terms of phylogenetic trees, and use this
distance to selectively tie the parameters of mono-
lingual syntactic models. Cohen et al (2011) do not
use a predefined linguistic hierarchy of language re-
lations, but instead learn the contribution of source
languages to the training mixture based on the like-
lihood of the target language. S?gaard (2011)
proposes a different measure of language related-
ness based on perplexity between POS sequences
of source and target languages. Using this measure,
he selects a subset of training source sentences that
are closer to the target language. While all of the
above techniques demonstrate gains from modeling
language relatedness, they still underperform when
the source and target languages are unrelated.
Our model differs from the above approaches in
its emphasis on the selective information sharing
driven by language relatedness. This is further com-
bined with monolingual unsupervised learning. As
our evaluation demonstrates, this layered approach
broadens the advantages of multilingual learning to
languages that exhibit significant differences from
the languages in the training mix.
3 Linguistic Motivation
Language-Independent Dependency Properties
Despite significant syntactic differences, human lan-
guages exhibit striking similarity in dependency pat-
terns. For a given part-of-speech tag, the set of tags
that can occur as its dependents is largely consistent
across languages. For instance, adverbs and nouns
are likely to be dependents of verbs, while adjectives
630
are not. Thus, these patterns can be freely trans-
ferred across languages.
Shared Dependency Properties Unlike dependent
selection, the ordering of dependents in a sentence
differs greatly across languages. In fact, cross-
lingual syntactic variations are primarily expressed
in different ordering of dependents (Harris, 1968;
Greenberg, 1963). Fortunately, the dimensions of
these variations have been extensively studied in lin-
guistics and are documented in the form of typo-
logical features (Comrie, 1989; Haspelmath et al,
2005). For instance, most languages are either dom-
inantly prepositional like English or post-positional
like Urdu. Moreover, a language may be close to dif-
ferent languages for different dependency types. For
instance, Portuguese is a prepositional language like
English, but the order of its noun-adjective depen-
dency is different from English and matches that of
Arabic. Therefore, we seek a model that can express
parameter sharing at the level of dependency types
and can benefit from known language relations.
Language-specific Dependency Variations Not
every aspect of syntactic structure is shared across
languages. This is particularly true given a limited
number of supervised source languages; it is quite
likely that a target language will have previously un-
seen syntactic phenomena. In such a scenario, the
raw text in the target language might be the only
source of information about its unique aspects.
4 Model
We propose a probabilistic model for generating
dependency trees that facilitates parameter sharing
across languages. We assume a setup where de-
pendency tree annotations are available for a set of
source languages and we want to use these annota-
tions to infer a parser for a target language. Syn-
tactic trees for the target language are not available
during training. We also assume that both source
and target languages are annotated with a coarse
parts-of-speech tagset which is shared across lan-
guages. Such tagsets are commonly used in multilin-
gual parsing (Zeman and Resnik, 2008; McDonald
et al, 2011; S?gaard, 2011; Naseem et al, 2010).
The key feature of our model is a two-tier ap-
proach that separates the selection of dependents
from their ordering:
1. Selection Component: Determines the depen-
dent tags given the parent tag.
2. Ordering Component: Determines the position
of each dependent tag with respect to its parent
(right or left) and the order within the right and
left dependents.
This factorization constitutes a departure from
traditional parsing models where these decisions are
tightly coupled. By separating the two, the model
is able to support different degrees of cross-lingual
sharing on each level.
For the selection component, a reasonable ap-
proximation is to assume that it is the same for all
languages. This is the approach we take here.
As mentioned in Section 3, the ordering of depen-
dents is largely determined by the typological fea-
tures of the language. We assume that we have a
set of such features for every language l, and denote
this feature vector by vl. We also experiment with a
variant of our model where typological features are
not observed. Instead, the model captures structural
variations across languages by means of a small set
of binary latent features. The values of these fea-
tures are language dependent. We denote the set of
latent features for language l by bl.
Finally, based on the well known fact that long
distance dependencies are less likely (Eisner and
Smith, 2010), we bias our model towards short de-
pendencies. This is done by imposing a corpus-level
soft constraint on dependency lengths using the pos-
terior regularization framework (Grac?a et al, 2007).
4.1 Generative Process
Our model generates dependency trees one fragment
at a time. A fragment is defined as a subtree com-
prising the immediate dependents of any node in the
tree. The process recursively generates fragments
in a head outwards manner, where the distribution
over fragments depends on the head tag. If the gen-
erated fragment is not empty then the process con-
tinues for each child tag in the fragment, drawing
new fragments from the distribution associated with
the tag. The process stops when there are no more
non-empty fragments.
A fragment with head node h is generated in lan-
guage l via the following stages:
631
h{N,A,N, V,D}
h
{N,N, V } {A,D}
h
N NV D A
(a) (b) (c)
Figure 1: The steps of the generative process for a fragment with head h. In step (a), the unordered set of dependents
is chosen. In step (b) they are partitioned into left and right unordered sets. Finally, each set is ordered in step (c).
? Generate the set of dependents of h via a distri-
bution Psel(S|h). Here S is an unordered set of
POS tags. Note that this part is universal (i.e.,
it does not depend on the language l).
? For each element in S decide whether it should
go to the right or left of h as follows: for every
a ? S, draw its direction from the distribution
Pord(d|a, h, l), where d ? {R,L}. This results
in two unordered sets SR, SL, the right and left
dependents of h. This part does depend on the
language l, since the relative ordering of depen-
dents is not likely to be universal.
? Order the sets SR, SL. For simplicity, we as-
sume that the order is drawn uniformly from
all the possible unique permutations over SR
and SL. We denote the number of such unique
permutations of SR by n(SR).2 Thus the prob-
ability of each permutation of SR is 1n(SR)
3.
Figure 1 illustrates the generative process. The first
step constitutes the selection component and the last
two steps constitute the ordering component. Given
this generation scheme, the probability P (D) of
generating a given fragment D with head h will be:
Psel({D}|h)
?
a?D
Pord(dD(a)|a, h, l)
1
n(DR)n(DL)
(1)
Where we use the following notations:
? DR, DL denote the parts of the fragment that
are to the left and right of h.
2This number depends on the count of each distinct tag in
SR. For example if SR = {N,N,N} then n(SR) = 1. If
SR = {N,D, V } then n(SR) = 3!.
3We acknowledge that assuming a uniform distribution over
the permutations of the right and left dependents is linguistically
counterintuitive. However, it simplifies the model by greatly
reducing the number of parameters to learn.
? {D} is the unordered set of tags in D.
? dD(a) is the position (either R or L) of the de-
pendent a w.r.t. the head of D.
In what follows we discuss the parameterizations
of the different distributions.
4.1.1 Selection Component
The selection component draws an unordered set
of tags S given the head tag h. We assume that the
process is carried out in two steps. First the number
of dependents n is drawn from a distribution:
Psize(n|h) = ?size(n|h) (2)
where ?size(n|h) is a parameter for each value of
n and h. We restrict the maximum value of n to
four, since this is a reasonable bound on the total
number of dependents for a single parent node in
a tree. These parameters are non-negative and sat-
isfy
?
n ?size(n|h) = 1. In other words, the size
is drawn from a categorical distribution that is fully
parameterized.
Next, given the size n, a set S with |S| = n is
drawn according to the following log-linear model:
Pset(S|h, n) =
1
Zset(h, n)
e
?
Si?S
?sel(Si|h)
Zset(h, n) =
?
S:|S|=n
e
?
Si?S
?sel(Si|h)
In the above, Si is the ith POS tag in the unordered
set S, and ?sel(Si|h) are parameters. Thus, large val-
ues of ?sel(Si|h) indicate that POS Si is more likely
to appear in the subset with parent POS h.
Combining the above two steps we have the fol-
lowing distribution for selecting a set S of size n:
Psel(S|h) = Psize(n|h)Pset(S|h, n) . (3)
632
ID Feature Description Values
81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV
85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions
86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive
87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective
88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative
89A Order of Numeral and Noun Numeral-Noun, Noun-Numeral
Table 1: The set of typological features that we use in our model. For each feature, the first column gives the ID of
the feature as used in WALS, the second column describes the feature and the last column enumerates the allowable
values for the feature. Besides these values, each feature can also have a value of ?No dominant order?.
4.1.2 Ordering Component
The ordering component consists of distributions
Pord(d|a, h, l) that determine whether tag a will be
mapped to the left or right of the head tag h. We
model it using the following log-linear model:
Pord(d|a, h, l) =
1
Zord(a, h, l)
eword?g(d,a,h,vl)
Zord(a, h, l) =
?
d?{R,L}
eword?g(d,a,h,vl)
Note that in the above equations the ordering
component depends on the known typological fea-
tures vl. In the setup when typological features are
not known, vl is replaced with the latent ordering
feature set bl.
The feature vector g contains indicator features
for combinations of a, h, d and individual features
vli (i.e., the ith typological features for language l).
4.2 Typological Features
The typological features we use are a subset of
order-related typological features from ?The World
Atlas of Language Structure? (Haspelmath et al,
2005). We include only those features whose val-
ues are available for all the languages in our dataset.
Table 1 summarizes the set of features that we use.
Note that we do not explicitly specify the correspon-
dence between these features and the model param-
eters. Instead, we leave it for the model to learn this
correspondence automatically.
4.3 Dependency Length Constraint
To incorporate the intuition that long distance de-
pendencies are less likely, we impose a posterior
constraint on dependency length. In particular, we
use the Posterior Regularization (PR) framework of
Grac?a et al (2007). The PR framework incorporates
constraints by adding a penalty term to the standard
likelihood objective. This term penalizes the dis-
tance of the model posterior from a set Q, where
Q contains all the posterior distributions that satisfy
the constraints. In our case the constraint is that the
expected dependency length is less than or equal to
a pre-specified threshold value b. If we denote the
latent dependency trees by z and the observed sen-
tences by x then
Q = {q(z|x) : Eq[f(x, z)] ? b} (4)
where f(x, z) computes the sum of the lengths of all
dependencies in z with respect to the linear order of
x. We measure the length of a dependency relation
by counting the number of tokens between the head
and its modifier. The PR objective penalizes the KL-
divergence of the model posterior from the set Q:
L?(x)?KL (Q ? p?(z|x))
where ? denotes the model parameters and the first
term is the log-likelihood of the data. This objective
can be optimized using a modified version of the EM
algorithm (Grac?a et al, 2007).
5 Parameter Learning
Our model is parameterized by the parameters ?sel,
?size and word. We learn these by maximizing the
likelihood of the training data. As is standard, we
add `2 regularization on the parameters and tune it
on source languages. The likelihood is marginalized
over all latent variables. These are:
? For sentences in the target language: all pos-
sible derivations that result in the observed
POS tag sequences. The derivations include
the choice of unordered sets size n, the un-
ordered sets themselves S, their left/right al-
633
locations and the orderings within the left and
right branches.
? For all languages: all possible values of the la-
tent features bl.4
Since we are learning with latent variables, we use
the EM algorithm to monotonically improve the
likelihood. At each E step, the posterior over latent
variables is calculated using the current model. At
the M step this posterior is used to maximize the
likelihood over the fully observed data. To com-
pensate for the differences in the amount of training
data, the counts from each language are normalized
before computing the likelihood.
The M step involves finding maximum likelihood
parameters for log-linear models in Equations 3 and
4. This is done via standard gradient based search;
in particular, we use the method of BFGS.
We now briefly discuss how to calculate the pos-
terior probabilities. For estimating the word param-
eters we require marginals of the type P (bli|Dl;wt)
where Dl are the sentences in language l, bli is the
ith latent feature for the language l and wt are the
parameter values at iteration t. Consider doing this
for a source language l. Since the parses are known,
we only need to marginalize over the other latent
features. This can be done in a straightforward man-
ner by using our probabilistic model. The complex-
ity is exponential in the number of latent features,
since we need to marginalize over all features other
than bli. This is feasible in our case, since we use a
relatively small number of such features.
When performing unsupervised learning for the
target language, we need to marginalize over possi-
ble derivations. Specifically, for the M step, we need
probabilities of the form P (a modifies h|Dl;wt).
These can be calculated using a variant of the inside
outside algorithm. The exact version of this algo-
rithm would be exponential in the number of depen-
dents due to the 1n(Sr) term in the permutation factor.
Although it is possible to run this exact algorithm in
our case, where the number of dependents is limited
to 4, we use an approximation that works well in
practice: instead of 1n(Sr) we use
1
|Sr|!
. In this case
the runtime is no longer exponential in the number
of children, so inference is much faster.
4This corresponds to the case when typological features are
not known.
Finally, given the trained parameters we generate
parses in the target language by calculating the max-
imum a posteriori derivation. This is done using a
variant of the CKY algorithm.
6 Experimental Setup
Datasets and Evaluation We test the effectiveness
of our approach on 17 languages: Arabic, Basque,
Bulgarian, Catalan, Chinese, Czech, Dutch, English,
German, Greek, Hungarian, Italian, Japanese, Por-
tuguese, Spanish, Swedish and Turkish. We used
datasets distributed for the 2006 and 2007 CoNLL
Shared Tasks (Buchholz and Marsi, 2006; Nivre
et al, 2007). Each dataset provides manually an-
notated dependency trees and POS tags. To en-
able crosslingual sharing, we map the gold part-
of-speech tags in each corpus to a common coarse
tagset (Zeman and Resnik, 2008; S?gaard, 2011;
McDonald et al, 2011; Naseem et al, 2010). The
coarse tagset consists of 11 tags: noun, verb, ad-
jective, adverb, pronoun, determiner, adposition, nu-
meral, conjunction, particle, punctuation mark, and
X (a catch-all tag). Among several available fine-
to-coarse mapping schemes, we employ the one of
Naseem et al (2010) that yields consistently better
performance for our method and the baselines than
the mapping proposed by Petrov et al (2011).
As the evaluation metric, we use directed depen-
dency accuracy. Following standard evaluation prac-
tices, we do not evaluate on punctuation. For both
the baselines and our model we evaluate on all sen-
tences of length 50 or less ignoring punctuation.
Training Regime Our model typically converges
quickly and does not require more than 50 iterations
of EM. When the model involves latent typological
variables, the initialization of these variables can im-
pact the final performance. As a selection criterion
for initialization, we consider the performance of the
final model averaged over the supervised source lan-
guages. We perform ten random restarts and select
the best according to this criterion. Likewise, the
threshold value b for the PR constraint on the depen-
dency length is tuned on the source languages, using
average test set accuracy as the selection criterion.
Baselines We compare against the state-of-the-art
multilingual dependency parsers that do not use par-
allel corpora for training. All the systems were eval-
634
uated using the same fine-to-coarse tagset mapping.
The first baseline, Transfer, uses direct transfer of a
discriminative parser trained on all the source lan-
guages (McDonald et al, 2011). This simple base-
line achieves surprisingly good results, within less
than 3% difference from a parser trained using par-
allel data. In the second baseline (Mixture), pa-
rameters of the target language are estimated as a
weighted mixture of the parameters learned from an-
notated source languages (Cohen et al, 2011). The
underlying parsing model is the dependency model
with valance (DMV) (Klein and Manning, 2004).
Originally, the baseline methods were evaluated on
different sets of languages using a different tag map-
ping. Therefore, we obtained new results for these
methods in our setup. For the Transfer baseline,
for each target language we trained the model on
all other languages in our dataset. For the Mixture
baseline, we trained the model on the same four lan-
guages used in the original paper ? English, Ger-
man, Czech and Italian. When measuring the per-
formance on these languages, we selected another
set of four languages with a similar level of diver-
sity.5
7 Results
Table 2 summarizes the performance for different
configurations of our model and the baselines.
Comparison against Baselines On average, the
selective sharing model outperforms both base-
lines, yielding 8.9% gain over the weighted mixture
model (Cohen et al, 2011) and 5.9% gain over the
direct transfer method (McDonald et al, 2011). Our
model outperforms the weighted mixture model on
15 of the 17 languages and the transfer method on
12 of the 17 languages. Most of the gains are ob-
tained on non-Indo-European languages, that have
little similarity with the source languages. For this
set, the average gain over the transfer baseline is
14.4%. With some languages, such as Japanese,
achieving gains of as much as 30%.
On Indo-European languages, the model perfor-
mance is almost equivalent to that of the best per-
forming baseline. To explain this result we con-
5We also experimented with a version of the Cohen et al
(2011) model trained on all the source languages. This setup
resulted in decreased performance. For this reason, we chose to
train the model on the four languages.
sider the performance of the supervised version of
our model which constitutes an upper bound on the
performance. The average accuracy of our super-
vised model on these languages is 66.8%, compared
to the 76.3% of the unlexicalized MST parser. Since
Indo-European languages are overrepresented in our
dataset, a target language from this family is likely
to exhibit more similarity to the training data. When
such similarity is substantial, the transfer baseline
will benefit from the power of a context-rich dis-
criminative parser.
A similar trait can be seen by comparing the per-
formance of our model to an oracle version of our
model which selects the optimal source language
for a given target language (column 7). Overall,
our method performs similarly to this oracle variant.
However, the gain for non Indo-European languages
is 1.9% vs -1.3% for Indo-European languages.
Analysis of Model Properties We first test our
hypothesis about the universal nature of the depen-
dent selection. We compare the performance of
our model (column 6) against a variant (column 8)
where this component is trained from annotations on
the target language. The performance of the two is
very close ? 1.8%, supporting the above hypothesis.
To assess the contribution of other layers of selec-
tive sharing, we first explore the role of typological
features in learning the ordering component. When
the model does not have access to observed typo-
logical features, and does not use latent ones (col-
umn 4), the accuracy drops by 2.6%6. For some
languages (e.g., Turkish) the decrease is very pro-
nounced. Latent typological features (column 5) do
not yield the same gain as observed ones, but they do
improve the performance of the typology-free model
by 1.4%.
Next, we show the importance of using raw tar-
get language data in training the model. When
the model has to make all the ordering decisions
based on meta-linguistic features without account
for unique properties of the target languages, the
performance decreases by 0.9% (see column 3).
To assess the relative difficulty of learning the
ordering and selection components, we consider
model variants where each of these components is
6In this setup, the ordering component is trained in an unsu-
pervised fashion on the target language.
635
Baselines Selective Sharing Model
Mixture Transfer (D-,To) (D+) (D+,Tl) (D+,To) Best Pair Sup. Sel. Sup. Ord. MLE
Catalan 64.9 69.5 71.9 66.1 66.7 71.8 74.8 70.2 73.2 72.1
Italian 61.9 68.3 68.0 65.5 64.2 65.6 68.3 65.1 70.7 72.3
Portuguese 72.9 75.8 76.2 72.3 76.0 73.5 76.4 77.4 77.6 79.6
Spanish 57.2 65.9 62.3 58.5 59.4 62.1 63.4 61.5 62.6 65.3
Dutch 50.1 53.9 56.2 56.1 55.8 55.9 57.8 56.3 58.6 58.0
English 45.9 47.0 47.6 48.5 48.1 48.6 44.4 46.3 60.0 62.7
German 54.5 56.4 54.0 53.5 54.3 53.7 54.8 52.4 56.2 58.0
Swedish 56.4 63.6 52.0 61.4 60.6 61.5 63.5 67.9 67.1 73.0
Bulgarian 67.7 64.0 67.6 63.5 63.9 66.8 66.1 66.2 69.5 71.0
Czech 39.6 40.3 43.9 44.7 45.4 44.6 47.5 53.2 51.2 58.9
Arabic 44.8 40.7 57.2 58.8 60.3 58.9 57.6 62.9 61.9 64.2
Basque 32.8 32.4 39.7 40.1 39.8 47.6 42.0 46.2 47.9 51.6
Chinese 46.7 49.3 59.9 52.2 52.0 51.2 65.4 62.3 65.5 73.5
Greek 56.8 60.4 61.9 67.5 67.3 67.4 60.6 67.2 69.0 70.5
Hungarian 46.8 54.3 56.9 58.4 58.8 58.5 57.0 57.4 62.0 61.6
Japanese 33.5 34.7 62.3 56.8 61.4 64.0 54.8 63.4 69.7 75.6
Turkish 28.3 34.3 59.1 43.6 57.8 59.2 56.9 66.6 59.5 67.6
Average 50.6 53.6 58.6 56.9 58.3 59.5 59.5 61.3 63.7 66.8
Table 2: Directed dependency accuracy of different variants of our selective sharing model and the baselines. The
first section of the table (column 1 and 2) shows the accuracy of the weighted mixture baseline (Cohen et al, 2011)
(Mixture) and the multi-source transfer baseline (McDonald et al, 2011) (Transfer). The middle section shows the
performance of our model in different settings. D? indicates the presence/absence of raw target language data during
training. To indicates the use of observed typological features for all languages and Tl indicates the use of latent
typological features for all languages. The last section shows results of our model with different levels of oracle
supervision: a. (Best Pair) Model parameters are borrowed from the best source language based on the accuracy on
the target language b. (Sup. Sel.) Selection component is trained using MLE estimates from target language c. (Sup.
Ord.) Ordering component is trained using MLE estimates from the target language d. (MLE) All model parameters
are trained on the target language in a supervised fashion. The horizontal partitions separate language families. The
first three families are sub-divisions of the Indo-European language family.
trained using annotations in the target language. As
shown in columns 8 and 9, these two variants out-
perform the original model, achieving 61.3% for su-
pervised selection and 63.7% for supervised order-
ing. Comparing these numbers to the accuracy of
the original model (column 6) demonstrates the dif-
ficulty inherent in learning the ordering information.
This finding is expected given that ordering involves
selective sharing from multiple languages.
Overall, the performance gap between the selec-
tive sharing model and its monolingual supervised
counterpart is 7.3%. In contrast, the unsupervised
monolingual variant of our model achieves a mea-
ger 26%.7 This demonstrates that our model can ef-
fectively learn relevant aspects of syntactic structure
from a diverse set of languages.
7This performance is comparable to other generative models
such as DMV (Klein and Manning, 2004).
8 Conclusions
We present a novel algorithm for multilingual de-
pendency parsing that uses annotations from a di-
verse set of source languages to parse a new unan-
notated language. Overall, our model consistently
outperforms the multi-source transfer based depen-
dency parser of McDonald et al (2011). Our ex-
periments demonstrate that the model is particularly
effective in processing languages that exhibit signif-
icant differences from the training languages.
Acknowledgments
The authors acknowledge the support of the NSF
(IIS-0835445), the MURI program (W911NF-10-
1-0533), the DARPA BOLT program, and the ISF
(1789/11). We thank Tommi Jaakkola, Ryan Mc-
Donald and the members of the MIT NLP group for
their comments.
636
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In ACL, pages 1288?1297.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164.
David Burkett and Dan Klein. 2008. Two languages are
better than one (for syntactic parsing). In Proceedings
of EMNLP, pages 877?886.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In EMNLP, pages 50?61.
Bernard Comrie. 1989. Language Universals and Lin-
guistic Typology: Syntax and Morphology. Oxford:
Blackwell.
Jason Eisner and Noah A. Smith. 2010. Favor short de-
pendencies: Parsing with soft and hard constraints on
dependency length. In Trends in Parsing Technology:
Dependency Parsing, Domain Adaptation, and Deep
Parsing, pages 121?150.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
In Advances in NIPS, pages 569?576.
Joseph H Greenberg. 1963. Some universals of language
with special reference to the order of meaningful ele-
ments. In Joseph H Greenberg, editor, Universals of
Language, pages 73?113. MIT Press.
Z.S. Harris. 1968. Mathematical structures of language.
Wiley.
Martin Haspelmath, Matthew S. Dryer, David Gil, and
Bernard Comrie, editors. 2005. The World Atlas of
Language Structures. Oxford University Press.
R. Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-
lak. 2005. Bootstrapping parsers via syntactic projec-
tion across parallel texts. Journal of Natural Language
Engineering, 11(3):311?325.
Dan Klein and Christopher Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of ACL,
pages 478?485.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the ACL, pages
470?477.
Ryan T. McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In EMNLP, pages 62?72.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowledge
to guide grammar induction. In EMNLP, pages 1234?
1244.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
David A. Smith and Noah A. Smith. 2004. Bilingual
parsing with factored estimation: Using English to
parse Korean. In Proceeding of EMNLP, pages 49?
56.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of ACL/AFNLP, pages 73?81.
Anders S?gaard. 2011. Data point selection for cross-
language adaptation of dependency parsers. In ACL
(Short Papers), pages 682?686.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Chenhai Xi and Rebecca Hwa. 2005. A backoff model
for bootstrapping resources for non-english languages.
In Proceedings of EMNLP, pages 851 ? 858.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. In Pro-
ceedings of the IJCNLP-08 Workshop on NLP for Less
Privileged Languages, pages 35?42, January.
637
