Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1092?1100,
Beijing, August 2010
Dependency Forest for Statistical Machine Translation
Zhaopeng Tu ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
We propose a structure called dependency
forest for statistical machine translation.
A dependency forest compactly represents
multiple dependency trees. We develop
new algorithms for extracting string-to-
dependency rules and training depen-
dency language models. Our forest-based
string-to-dependency system obtains sig-
nificant improvements ranging from 1.36
to 1.46 BLEU points over the tree-based
baseline on the NIST 2004/2005/2006
Chinese-English test sets.
1 Introduction
Dependency grammars have become increasingly
popular in syntax-based statistical machine trans-
lation (SMT). One important advantage of depen-
dency grammars is that they directly capture the
dependencies between words, which are key to re-
solving most parsing ambiguities. As a result, in-
corporating dependency trees proves to be effec-
tive in improving statistical machine translation
(Quirk et al, 2005; Ding and Palmer, 2005; Shen
et al, 2008).
However, most dependency-based translation
systems suffer from a major drawback: they only
use 1-best dependency trees for rule extraction,
dependency language model training, and decod-
ing, which potentially introduces translation mis-
takes due to the propagation of parsing errors
(Quirk and Corston-Oliver, 2006). While the
treelet system (Quirk et al, 2005) takes a de-
pendency tree as input, the string-to-dependency
system (Shen et al, 2008) decodes on a source-
language string. However, as we will show, the
string-to-dependency system still commits to us-
ing degenerate rules and dependency language
models learned from noisy 1-best trees.
To alleviate this problem, an obvious solu-
tion is to offer more alternatives. Recent studies
have shown that SMT systems can benefit from
widening the annotation pipeline: using packed
forests instead of 1-best trees (Mi and Huang,
2008), word lattices instead of 1-best segmenta-
tions (Dyer et al, 2008), and weighted alignment
matrices instead of 1-best alignments (Liu et al,
2009).
Along the same direction, we propose a struc-
ture called dependency forest, which encodes ex-
ponentially many dependency trees compactly, for
dependency-based translation systems. In this pa-
per, we develop two new algorithms for extracting
string-to-dependency rules and for training depen-
dency language models, respectively. We show
that using the rules and dependency language
models learned from dependency forests leads to
consistent and significant improvements over that
of using 1-best trees on the NIST 2004/2005/2006
Chinese-English test sets.
2 Background
Figure 1 shows a dependency tree of an English
sentence he saw a boy with a telescope. Arrows
point from the child to the parent, which is often
referred to as the head of the child. For example,
in Figure 1, saw is the head of he. A dependency
tree is more compact than its constituent counter-
part because there is no need to build a large su-
perstructure over a sentence.
Shen et al (2008) propose a novel string-to-
dependency translation model that features two
important advantages. First, they define that
a string-to-dependency rule must have a well-
formed dependency structure on the target side,
which makes efficient dynamic programming pos-
sible and manages to retain most useful non-
constituent rules. A well-formed structure can be
either fixed or floating . A fixed structure is a
1092
saw
he boy with
a telescope
a
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 1: A training example for tree-based rule
extraction.
dependency tree with all the children complete.
Floating structures consist of sibling nodes of a
common head, but the head itself is unspecified
or floating. For example, Figure 2(a) and Figure
2(b) are two fixed structures while Figure 2(c) is a
floating one.
Formally, for a given sentence w1:l = w1 . . . wl,
d1 . . . dl represent the parent word IDs for each
word. If wi is a root, we define di = 0.
Definition 1. A dependency structure di..j is fixed
on head h, where h /? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
Definition 2. A dependency structure di..j is
floating with children C, for a non-empty set C
? {i, ..., j}, or floating for short, if and only if it
meets the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
A dependency structure is well-formed if and
only if it is either fixed or floating.
2.1 Tree-based Rule Extraction
Figure 1 shows a training example consisting of an
English dependency tree, its Chinese translation,
boy
a
(a)
with
telescope
a
(b)
boy with
a telescope
a
(c)
Figure 2: Well-formed dependency structures cor-
responding to Figure 1. (a) and (b) are fixed and
(c) is floating.
and the word alignments between them. To facil-
itate identifying the correspondence between the
English and Chinese words, we also gives the En-
glish sentence. Extracting string-to-dependency
rules from aligned string-dependency pairs is sim-
ilar to extracting SCFG (Chiang, 2007) except that
the target side of a rule is a well-formed struc-
ture. For example, we can first extract a string-to-
dependency rule that is consistent with the word
alignment (Och and Ney, 2004):
with ((a) telescope) ? dai wangyuanjing de
Then a smaller rule
(a) telescope ? wangyuanjing
can be subtracted to obtain a rule with one non-
terminal:
with (X1) ? dai X1 de
where X is a non-terminal and the subscript indi-
cates the correspondence between non-terminals
on the source and target sides.
2.2 Tree-based Dependency Language Model
As dependency relations directly model the se-
mantics structure of a sentence, Shen et al (2008)
introduce dependency language model to better
account for the generation of target sentences.
Compared with the conventional n-gram language
models, dependency language model excels at
capturing non-local dependencies between words
(e.g., saw ... with in Figure 1). Given a depen-
dency tree, its dependency language model prob-
ability is a product of three sub-models defined
between headwords and their dependants. For ex-
ample, the probability of the tree in Figure 1 can
1093
saw0,7
he0,1 boy2,4 with4,7
a2,3 telescope5,7
a5,6
(a)
saw0,7
he0,1 boy2,7
a2,3 with4,7
telescope5,7
a5,6
(b)
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
(c)
Figure 3: (a) the dependency tree in Figure 1, (b) another dependency tree for the same sentence, and
(c) a dependency forest compactly represents the two trees.
be calculated as:
Prob = PT (saw)
?PL(he|saw-as-head)
?PR(boy|saw-as-head)
?PR(with|boy, saw-as-head)
?PL(a|boy-as-head)
?PR(telescope|with-as-head)
?PL(a|telescope-as-head)
where PT (x) is the probability of word x being
the root of a dependency tree. PL and PR are the
generative probabilities of left and right sides re-
spectively.
As the string-to-tree system relies on 1-best
trees for parameter estimation, the quality of rule
table and dependency language model might be
affected by parsing errors and therefore ultimately
results in translation mistakes.
3 Dependency Forest
We propose to encode multiple dependency trees
in a compact representation called dependency
forest, which offers an elegant solution to the
problem of parsing error propagation.
Figures 3(a) and 3(b) show two dependency
trees for the example English sentence in Figure
1. The prepositional phrase with a telescope could
either depend on saw or boy. Figure 3(c) is a
dependency forest compactly represents the two
trees by sharing common nodes and edges.
Each node in a dependency forest is a word.
To distinguish among nodes, we attach a span to
each node. For example, in Figure 1, the span of
the first a is (2, 3) because it is the third word in
the sentence. As the fourth word boy dominates
the node a2,3, it can be referred to as boy2,4. Note
that the position of boy itself is taken into consid-
eration. Similarly, the word boy in Figure 3(b) can
be represented as boy2,7.
The nodes in a dependency forest are connected
by hyperedges. While an edge in a dependency
tree only points from a dependent to its head, a
hyperedge groups all the dependants that have a
common head. For example, in Figure 3(c), the
hyperedge
e1: ?(he0,1, boy2,4,with4,7), saw0,7?
denotes that he0,1, boy2,4, and with4,7 are depen-
dants (from left to right) of saw0,7.
More formally, a dependency forest is a pair
?V,E?, where V is a set of nodes, and E
is a set of hyperedges. For a given sentence
w1:l = w1 . . . wl, each node v ? V is in the
form of wi,j , which denotes that w dominates
the substring from positions i through j (i.e.,
wi+1 . . . wj). Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the
head and tails(e) ? V are its dependants.
A dependency forest has a structure of a hy-
pergraph such as packed forest (Klein and Man-
ning, 2001; Huang and Chiang, 2005). However,
while each hyperedge in a packed forest naturally
treats the corresponding PCFG rule probability as
its weight, it is challenging to make dependency
forest to be a weighted hypergraph because depen-
dency parsers usually only output a score, which
can be either positive or negative, for each edge
in a dependency tree rather than a hyperedge in a
1094
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 4: A training example for forest-based rule
extraction.
dependency forest. For example, in Figure 3(a),
the scores for the edges he ? saw, boy ? saw,
and with ? saw could be 13, 22, and -12, respec-
tively.
To assign a probability to each hyperedge, we
can first obtain a positive number for a hyperedge
using the scores of the corresponding edges:1
c(e) = exp
(?
v?tails(e) s
(
v, head(e)
)
|tails(e)|
)
(1)
where c(e) is the count of a hyperedge e, head(e)
is a head, tails(e) is a set of dependants of the
head, v is one dependant, and s(v, head(e)) is the
score of an edge from v to head(e). For example,
the count of the hyperedge e1 in Figure 3(c) is
c(e1) = exp
(
13 + 22 ? 12
3
)
(2)
Then, the probability of a hyperedge can be ob-
tained by normalizing the count among all hyper-
edges with the same head collected from a training
corpus:
p(e) = c(e)?
e?:head(e?)=head(e) c(e?)
(3)
Therefore, we obtain a weighted dependency
forest in which each hyperedge has a probability.
1It is difficult to assign a probability to each hyperedge.
The current method is arbitrary, and we will improve it in the
future.
Algorithm 1 Forest-based Initial Phrase Extrac-
tion
Input: a source sentence ?, a forest F , an alignment a,
and k
Output: minimal initial phrase setR
1: for each node v ? V in a bottom-up order do
2: for each hyperedge e ? E and head(e) = v do
3: W ? ?
4: fixs? EnumFixed(v,modifiers(e))
5: floatings? EnumFloating(modifiers(e))
6: add structures fixs, floatings to W
7: for each ? ?W do
8: if ? is consistent with a then
9: generate a rule r
10: R.append(r)
11: keep k-best dependency structures for v
4 Forest-based Rule Extraction
In tree-based rule extraction, one just needs to first
enumerate all bilingual phrases that are consis-
tent with word alignment and then check whether
the dependency structures over the target phrases
are well-formed. However, this algorithm fails to
work in the forest scenario because there are usu-
ally exponentially many well-formed structures
over a target phrase.
The GHKM algorithm (Galley et al, 2004),
which is originally developed for extracting tree-
to-string rules from 1-best trees, has been suc-
cessfully extended to packed forests recently (Mi
and Huang, 2008). The algorithm distinguishes
between minimal and composed rules. Although
there are exponentially many composed rules, the
number of minimal rules extracted from each node
is rather limited (e.g., one or zero). Therefore, one
can obtain promising composed rules by combin-
ing minimal rules.
Unfortunately, the GHKM algorithm cannot be
applied to extracting string-to-dependency rules
from dependency forests. This is because the
GHKM algorithm requires a complete subtree to
exist in a rule while neither fixed nor floating de-
pendency structures ensure that all dependants of
a head are included. For example, the floating
structure shown in Figure 2(c) actually contains
two trees.
Alternatively, our algorithm searches for well-
formed structures for each node in a bottom-up
style. Algorithm 1 shows the algorithm for ex-
tracting initial phrases, that is, rules without non-
1095
terminals from dependency forests. The algorithm
maintains k-best well-formed structures for each
node (line 11). The well-formed structures of a
head can be constructed from those of its depen-
dants. For example, in Figure 4, as the fixed struc-
ture rooted at telescope5,7 is
(a) telescope
we can obtain a fixed structure rooted for the node
with4,7 by attaching the fixed structure of its de-
pendant to the node (EnumFixed in line 4). Figure
2(b) shows the resulting fixed structure.
Similarly, the floating structure for the node
saw0,7 can be obtained by concatenating the fixed
structures of its dependants boy2,4 and with4,7
(EnumFloating in line 5). Figure 2(c) shows the
resulting fixed structure. The algorithm is similar
to Wang et al (2007), which binarize each con-
stituent node to create some intermediate nodes
that correspond to the floating structures.
Therefore, we can find k-best fixed and float-
ing structures for a node in a dependency forest
by manipulating the fixed structures of its depen-
dants. Then we can extract string-to-dependency
rules if the dependency structures are consistent
with the word alignment.
How to judge a well-formed structure extracted
from a node is better than others? We follow Mi
and Huang (2008) to assign a fractional count to
each well-formed structure. Given a tree fragment
t, we use the inside-outside algorithm to compute
its posterior probability:
??(t) = ?(root(t)) ?
?
e?t
p(e)
?
?
v?leaves(t)
?(v) (4)
where root(t) is the root of the tree, e is an edge,
leaves(t) is a set of leaves of the tree, ?(?) is out-
side probability, and ?(?) is inside probability.
For example, the subtree rooted at boy2,7 in Fig-
ure 4 has the following posterior probability:
?(boy2,7) ? p(e4) ? p(e5)
?p(e6) ? ?(a2,3) ? ?(a5,6) (5)
Now the fractional count of the subtree t is
c(t) = ??(t)??(TOP ) (6)
where TOP denotes the root node of the forest.
As a well-formed structure might be non-
constituent, we approximate the fractional count
by taking that of the minimal constituent tree frag-
ment that contains the well-formed structure. Fi-
nally, the fractional counts of well-formed struc-
tures can be used to compute the relative frequen-
cies of the rules having them on the target side (Mi
and Huang, 2008):
?(r|lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(7)
?(r|rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(8)
Often, our approach extracts a large amount of
rules from training corpus as we usually retain ex-
ponentially many well-formed structures over a
target phrase. To maintain a reasonable rule ta-
ble size, we discard any rule that has a fractional
count lower that a threshold t.
5 Forest-based Dependency Language
Model Training
Dependency language model plays an important
role in string-to-dependency system. Shen et
al. (2008) show that string-to-dependency system
achieves 1.48 point improvement in BLEU along
with dependency language model, while no im-
provement without it. However, the string-to-
dependency system still commits to using depen-
dency language model from noisy 1-best trees.
We now turn to dependency forest for it encodes
multiple dependency trees.
To train a dependency language model from a
dependency forest, we need to collect all heads
and their dependants. This can be easily done by
enumerating all hyperedges. Similarly, we use the
inside-outside algorithm to compute the posterior
probability of each hyperedge e,
??(e) = ?(head(e)) ? p(e)
?
?
v?tailes(e)
?(v) (9)
For example, the posterior probability of the hy-
peredge e2 in Figure 4 is calculated as
??(e2) = ?(saw0,7) ? p(e2)
??(he0,1) ? ?(boy2,7) (10)
1096
Rule DepLM NIST 2004 NIST 2005 NIST 2006 time
tree tree 33.97 30.21 30.73 19.6
tree forest 34.42? 31.06? 31.37? 24.1
forest tree 34.60? 31.16? 31.45? 21.7
forest forest 35.33?? 31.57?? 32.19?? 28.5
Table 1: BLEU scores and average decoding time (second/sentence) on the Chinese-English test sets.
The baseline system (row 2) used the rule table and dependency language model learned both from
1-best dependency trees. We use ? *? and ?**? to denote a result is better than baseline significantly at
p < 0.05 and p < 0.01, respectively.
Then, we can obtain the fractional count of a
hyperedge e,
c(e) = ??(e)??(TOP ) (11)
Each n-gram (e.g., ?boy-as-head a?) is assigned
the same fractional count of the hyperedge it be-
longs to.
We also tried training dependency language
model as in (Shen et al, 2008), which means
all hyperedges were on equal footing without re-
garding probabilities. However, the performance
is about 0.8 point lower in BLEU. One possbile
reason is that hyperedges with probabilities could
distinguish high quality structures better.
6 Experiments
6.1 Results on the Chinese-English Task
We used the FBIS corpus (6.9M Chinese words
+ 8.9M English words) as our bilingual train-
ing corpus. We ran GIZA++ (Och and Ney,
2000) to obtain word alignments. We trained a
4-gram language model on the Xinhua portion
of GIGAWORD corpus using the SRI Language
Modeling Toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Kneser and Ney,
1995). We optimized feature weights using the
minimum error rate training algorithm (Och and
Ney, 2002) on the NIST 2002 test set. We evalu-
ated the translation quality using case-insensitive
BLEU metric (Papineni et al, 2002) on the NIST
2004/2005/2006 test sets.
To obtain dependency trees and forests, we
parsed the English sentences of the FBIS corpus
using a shift-reduce dependency parser that en-
ables beam search (Huang et al, 2009). We only
Rules Size New Rules
tree 7.2M -
forest 7.6M 16.86%
Table 2: Statistics of rules. The last column shows
the ratio of rules extracted from non 1-best parses
being used in 1-best derivations.
retained the best well-formed structure for each
node when extracting string-to-tree rules from de-
pendency forests (i.e., k = 1). We trained two
3-gram depLMs (one from trees and another from
forests) on English side of FBIS corpus plus 2M
sentence pairs from other LDC corpus.
After extracting rules and training depLMs, we
ran our replication of string-to-dependency sys-
tem (Shen et al, 2008) to translate the develop-
ment and test sets.
Table 1 shows the BLEU scores on the test
sets. The first column ?Rule? indicates where
the string-to-dependency rules are learned from:
1-best dependency trees or dependency forests.
Similarly, the second column ?DepLM? also dis-
tinguish between the two sources for training de-
pendency language models. The baseline sys-
tem used the rule table and dependency lan-
guage model both learned from 1-best depen-
dency trees. We find that adding the rule table and
dependency language models obtained from de-
pendency forests improves string-to-dependency
translation consistently and significantly, ranging
from +1.3 to +1.4 BLEU points. In addition, us-
ing the rule table and dependency language model
trained from forest only increases decoding time
insignificantly.
How many rules extracted from non 1-best
1097
Rule DepLM BLEU
tree tree 22.31
tree forest 22.73?
forest tree 22.80?
forest forest 23.12??
Table 3: BLEU scores on the Korean-Chinese test
set.
parses are used by the decoder? Table 2 shows the
number of rules filtered on the test set. We observe
that the rule table size hardly increases. One pos-
sible reason is that we only keep the best depen-
dency structure for each node. The last row shows
that 16.86% of the rules used in 1-best deriva-
tions are extracted from non 1-best parses in the
forests, indicating that some useful rules cannot
be extracted from 1-best parses.
6.2 Results on the Korean-Chinese Task
To examine the efficacy of our approach on differ-
ent language pairs, we carried out an experiment
on Korean-Chinese translation. The training cor-
pus contains about 8.2M Korean words and 7.3M
Chinese words. The Chinese sentences were used
to train a 5-gram language model as well as a 3-
gram dependency language model. Both the de-
velopment and test sets consist of 1,006 sentences
with single reference. Table 3 shows the BLEU
scores on the test set. Again, our forest-based ap-
proach achieves significant improvement over the
baseline (p < 0.01).
6.3 Effect of K-best
We investigated the effect of different k-best
structures for each node on translation quality
(BLEU scores on the NIST 2005 set) and the rule
table size (filtered for the tuning and test sets), as
shown in Figure 5. To save time, we extracted
rules just from the first 30K sentence pairs of the
FBIS corpus. We trained a language model and
depLMs on the English sentences. We used 10
different k: 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10. Ob-
viously, the higher the k is, the more rules are
extracted. When k=10, the number of rules used
on the tuning and test sets was 1,299,290 and the
BLEU score was 20.88. Generally, both the num-
ber of rules and the BLEU score went up with
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35
BL
EU
 s
co
re
rule table size(M)
k=1,2,...,10
Figure 5: Effect of k-best on rule table size and
translation quality.
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.98 1.00 1.02 1.04 1.06 1.08 1.10
BL
EU
 s
co
re
rule table size(M)
t=1.0,0.9,...,0.1
Figure 6: Effect of pruning threshold on rule table
size and translation quality.
the increase of k. However, this trend did not
hold within the range [4,10]. We conjecture that
when retaining more dependency structures for
each node, low quality structures would be intro-
duced, resulting in much rules of low quality.
An interesting finding is that the rule table grew
rapidly when k is in range [1,4], while gradually
within the range [4,10]. One possible reason is
that there are limited different dependency struc-
tures in the spans with a maximal length of 10,
which the target side of rules cover.
6.4 Effect of Pruning Threshold
Figure 6 shows the effect of pruning threshold on
translation quality and the rule table size. We
retained 10-best dependency structures for each
node in dependency forests. We used 10 different
1098
pruning thresholds: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
0.8, 0.9 and 1.0. Intuitively, the higher the prun-
ing threshold is, the less rules are extracted. When
t=0.1, the number of rules used on the tuning and
test sets was 1,081,841 and the BLEU score was
20.68.
Lots of rules are pruned when the pruning
threshold increases from 0.0 to 0.3 (around 20%).
After pruning away these rules, we achieved 0.6
point improvement in BLEU. However, when we
filtered more rules, the BLEU score went down.
Figures 5 and 6 show that using two parame-
ters that have to be hand-tuned achieves a small
improvement at the expense of an additional com-
plexity. To simplify the approach, we only keep
the best dependency structure for each node with-
out pruning any rule.
7 Related Works
While Mi and Huang (2008) and we both use
forests for rule extraction, there remain two ma-
jor differences. Firstly, Mi and Huang (2008) use
a packed forest, while we use a dependency forest.
Packed forest is a natural weighted hypergraph
(Klein and Manning, 2001; Huang and Chiang,
2005), for each hyperedge treats the correspond-
ing PCFG rule probability as its weight. However,
it is challenging to make dependency forest to be a
weighted hypergraph because dependency parsers
usually only output a score for each edge in a de-
pendency tree rather than a hyperedge in a depen-
dency forest. Secondly, The GHKM algorithm
(Galley et al, 2004), which is originally devel-
oped for extracting tree-to-string rules from 1-best
trees, has been successfully extended to packed
forests recently (Mi and Huang, 2008). Unfor-
tunately, the GHKM algorithm cannot be applied
to extracting string-to-dependency rules from de-
pendency forests, because the GHKM algorithm
requires a complete subtree to exist in a rule while
neither fixed nor floating dependency structures
ensure that all dependants of a head are included.
8 Conclusion and Future Work
In this paper, we have proposed to use dependency
forests instead of 1-best parses to extract string-to-
dependency tree rules and train dependency lan-
guage models. Our experiments show that our ap-
proach improves translation quality significantly
over a state-of-the-art string-to-dependency sys-
tem on various language pairs and test sets. We
believe that dependency forest can also be used to
improve the dependency treelet system (Quirk et
al., 2005) that takes 1-best trees as input.
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang for his invaluable help in dependency
forest.
References
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages 201?
228.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of IWPT.
Kneser, R. and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
Acoustics, Speech, and Signal.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP.
Mi, Haitao and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
1099
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proceedings of
EMNLP.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In Proceedings of ACL.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Wang, Wei, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proceedings of
EMNLP.
1100
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 33?37,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Head-Driven Hierarchical Phrase-based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
This paper presents an extension of Chi-
ang?s hierarchical phrase-based (HPB) model,
called Head-Driven HPB (HD-HPB), which
incorporates head information in translation
rules to better capture syntax-driven infor-
mation, as well as improved reordering be-
tween any two neighboring non-terminals at
any stage of a derivation to explore a larger
reordering search space. Experiments on
Chinese-English translation on four NIST MT
test sets show that the HD-HPB model signifi-
cantly outperforms Chiang?s model with aver-
age gains of 1.91 points absolute in BLEU.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. Due to lack of linguistic knowledge,
Chiang?s HPB model contains only one type of non-
terminal symbol X , often making it difficult to se-
lect the most appropriate translation rules.1 What
is more, Chiang?s HPB model suffers from limited
phrase reordering combining translated phrases in a
monotonic way with glue rules. In addition, once a
1Another non-terminal symbol S is used in glue rules.
glue rule is adopted, it requires all rules above it to
be glue rules.
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble. Inspired by previous work in parsing (Char-
niak, 2000; Collins, 2003), our Head-Driven HPB
(HD-HPB) model is based on the intuition that lin-
guistic heads provide important information about a
constituent or distributionally defined fragment, as
in HPB. We identify heads using linguistically mo-
tivated dependency parsing, and use their POS to
refine X. In addition HD-HPB provides flexible re-
ordering rules freely mixing translation and reorder-
ing (including swap) at any stage in a derivation.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
33
 ??/NR 
Ouzhou 
??/NN 
baguo 
??/AD 
lianming 
??/VV 
zhichi 
??/NR 
meiguo 
??/NN 
lichang 
root 
Eight European countries jointly support America?s stand 
Figure 1: An example word alignment for a Chinese-
English sentence pair with the dependency parse tree for
the Chinese sentence. Here, each Chinese word is at-
tached with its POS tag and Pinyin.
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simulta-
neously using a context-free grammar. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), given a
word sequence f ij from position i to position j, we
first find heads and then concatenate the POS tags
of these heads as f ij?s non-terminal symbol. Specif-
ically, we adopt unlabeled dependency structure to
derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 1
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
According to the occurrence of terminals in
translation rules, we group rules in the HD-HPB
model into two categories: head-driven hierarchical
rules (HD-HRs) and non-terminal reordering rules
(NRRs), where the former have at least one terminal
on both source and target sides and the later have no
terminals. For rule extraction, we first identify ini-
tial phrase pairs on word-aligned sentence pairs by
using the same criterion as most phrase-based trans-
lation models (Och and Ney, 2004) and Chiang?s
HPB model (Chiang, 2005; Chiang, 2007). We
extract HD-HRs and NRRs based on initial phrase
pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that contain
other phrases and then replace sub-phrases with POS
tags corresponding to their heads. Given the word
alignment in Figure 1, Table 1 demonstrates the dif-
ference between hierarchical rules in Chiang (2007)
and HD-HRs defined here.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair on the source side, there are
four possible positional relationships for their target
side translations (we use Y as a variable for non-
terminals on the source side while all non-terminals
on the target side are labeled as X):
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
34
phrase pairs hierarchical rule head-driven hierarchical rule
lichang, stand X?lichang, stand
NN?lichang,
X?stand
meiguo lichang1, America?s stand1 X?meiguo X1, America?s X1
NN?meiguo NN1,
X?America?s X1
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 lichang,
support America?s1 stand
X?X1 lichang,
X1 stand
VV?VV-NR1 lichang,
X?X1 stand
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
2.3 Features and Decoding
Given e for the translation output in the target lan-
guage, s and t for strings of terminals and non-
terminals on the source and target side, respectively,
we use a feature set analogous to the default feature
set of Chiang (2007), including:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
Our decoder is based on CKY-style chart parsing
with beam search and searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang?s HPB model, it is pos-
sible for a non-terminal generated by a NRR to be
included afterwards by a HD-HR or another NRR.
3 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for our
HD-HPB and HPB systems, including initial phrase
length (as 10) in training, the maximum number of
non-terminals (as 2) in translation rules, maximum
number of non-terminals plus terminals (as 5) on
the source, beam threshold ? (as 10?5) (to discard
derivations with a score worse than ? times the best
score in the same chart cell), beam size b (as 200)
(i.e. each chart cell contains at most b derivations).
For Moses HPB, we use ?grow-diag-final-and? to
obtain symmetric word alignments, 10 for the max-
imum phrase length, and the recommended default
values for all other parameters.
We train our model on a dataset with ?1.5M sen-
tence pairs from the LDC dataset.2 We use the
2002 NIST MT evaluation test data (878 sentence
pairs) as the development data, and the 2003, 2004,
2005, 2006-news NIST MT evaluation test data
(919, 1788, 1082, and 616 sentence pairs, respec-
tively) as the test data. To find heads, we parse the
source sentences with the Berkeley Parser3 (Petrov
and Klein, 2007) trained on Chinese TreeBank 6.0
and use the Penn2Malt toolkit4 to obtain (unlabeled)
dependency structures.
We obtain the word alignments by running
2This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
35
GIZA++ (Och and Ney, 2000) on the corpus in both
directions and applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
BLEU scores. To test whether a performance differ-
ence is statistically significant, we conduct signifi-
cance tests following the paired bootstrap approach
(Koehn, 2004). In this paper,?**? and?*? de-
note p-values less than 0.01 and in-between [0.01,
0.05), respectively.
Table 2 lists the rule table sizes. The full rule ta-
ble size (including HD-HRs and NRRs) of our HD-
HPB model is ?1.5 times that of Chiang?s, largely
due to refining the non-terminal symbol X in Chi-
ang?s model into head-informed ones in our model.
It is also unsurprising, that the test set-filtered rule
table size of our model is only ?0.7 times that of Chi-
ang?s: this is due to the fact that some of the refined
translation rule patterns required by the test set are
unattested in the training data. Furthermore, the rule
table size of NRRs is much smaller than that of HD-
HRs since a NRR contains only two non-terminals.
Table 3 lists the translation performance with
BLEU scores. Note that our re-implementation of
Chiang?s original HPB model performs on a par with
Moses HPB. Table 3 shows that our HD-HPB model
significantly outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU (and sim-
ilar improvements over Moses HPB).
Table 3 shows that the head-driven scheme out-
performs a SAMT-style approach (for each test set
p < 0.01), indicating that head information is more
effective than (partial) CFG categories. Taking lian-
ming zhichi in Figure 1 as an example, HD-HPB
labels the span VV, as lianming is dominated by
zhichi, effecively ignoring lianming in the transla-
tion rule, while the SAMT label is ADVP:AD+VV5
which is more susceptible to data sparsity. In addi-
tion, SAMT resorts to X if a text span fails to satisify
pre-defined categories. Examining initial phrases
5the constituency structure for lianming zhichi is (VP (ADVP
(AD lianming)) (VP (VV zhichi) ...)).
System Total MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6 2.8 4.7 3.3 3.0 3.4
HD-HPB 59.5/0.6 1.9/0.1 3.4/0.2 2.3/0.2 2.0/0.1 2.4/0.2
Table 2: Rule table sizes (in million) of different mod-
els. Note: 1) For HD-HPB, the rule sizes separated by /
indicate HD-HRs and NRRs, respectively; 2) Except for
?Total?, the figures correspond to rules filtered on the cor-
responding test set.
System MT 03 MT 04 MT 05 MT 06 Avg.
Moses HPB 32.94* 35.16 32.18 29.88* 32.54
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50** 37.61** 34.56** 31.78** 34.86
SAMT-HPB 34.07 36.52** 32.90* 30.66 33.54
HD-HR+Glue 34.58** 36.55** 33.84** 31.06 34.01
Table 3: BLEU (%) scores of different models. Note:
1) SAMT-HPB indicates our HD-HPB model with non-
terminal scheme of Zollmann and Venugopal (2006);
2) HD-HR+Glue indicates our HD-HPB model replac-
ing NRRs with glue rules; 3) Significance tests for
Moses HPB, HD-HPB, SAMT-HPB, and HD-HR+Glue
are done against HPB.
extracted from the SAMT training data shows that
28% of them are labeled as X.
In order to separate out the individual contribu-
tions of the novel HD-HRs and NRRs, we carry out
an additional experiment (HD-HR+Glue) using HD-
HRs with monotonic glue rules only (adjusted to re-
fined rule labels, but effectively switching off the ex-
tra reordering power of full NRRs). Table 3 shows
that on average more than half of the improvement
over HPB (Chiang and Moses) comes from the re-
fined HD-HRs, the rest from NRRs.
Examining translation rules extracted from the
training data shows that there are 72,366 types of
non-terminals with respect to 33 types of POS tags.
On average each sentence employs 16.6/5.2 HD-
HRs/NRRs in our HD-HPB model, compared to
15.9/3.6 hierarchical rules/glue rules in Chiang?s
model, providing further indication of the impor-
tance of NRRs in translation.
4 Conclusion
We present a head-driven hierarchical phrase-based
(HD-HPB) translation model, which adopts head in-
formation (derived through unlabeled dependency
analysis) in the definition of non-terminals to bet-
ter differentiate among translation rules. In ad-
36
dition, improved and better integrated reordering
rules allow better reordering between consecutive
non-terminals through exploration of a larger search
space in the derivation. Experimental results on
Chinese-English translation across four test sets
demonstrate significant improvements of the HD-
HPB model over both Chiang?s HPB and a source-
side SAMT-style refined version of HPB.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
37
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338?343,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Identifying High-Impact Sub-Structures for Convolution Kernels in
Document-level Sentiment Classification
Zhaopeng Tu? Yifan He?? Jennifer Foster? Josef van Genabith? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Computer Science Department ?School of Computing
Institute of Computing Technology, CAS New York University Dublin City University
?{tuzhaopeng,liuqun,sxlin}@ict.ac.cn,
?yhe@cs.nyu.edu, ?{jfoster,josef}@computing.dcu.ie
Abstract
Convolution kernels support the modeling of
complex syntactic information in machine-
learning tasks. However, such models are
highly sensitive to the type and size of syntac-
tic structure used. It is therefore an importan-
t challenge to automatically identify high im-
pact sub-structures relevant to a given task. In
this paper we present a systematic study inves-
tigating (combinations of) sequence and con-
volution kernels using different types of sub-
structures in document-level sentiment classi-
fication. We show that minimal sub-structures
extracted from constituency and dependency
trees guided by a polarity lexicon show 1.45
point absolute improvement in accuracy over a
bag-of-words classifier on a widely used sen-
timent corpus.
1 Introduction
An important subtask in sentiment analysis is sen-
timent classification. Sentiment classification in-
volves the identification of positive and negative
opinions from a text segment at various levels of
granularity including document-level, paragraph-
level, sentence-level and phrase-level. This paper
focuses on document-level sentiment classification.
There has been a substantial amount of work
on document-level sentiment classification. In ear-
ly pioneering work, Pang and Lee (2004) use a
flat feature vector (e.g., a bag-of-words) to rep-
resent the documents. A bag-of-words approach,
however, cannot capture important information ob-
tained from structural linguistic analysis of the doc-
uments. More recently, there have been several ap-
proaches which employ features based on deep lin-
guistic analysis with encouraging results including
Joshi and Penstein-Rose (2009) and Liu and Senef-
f (2009). However, as they select features manually,
these methods would require additional labor when
ported to other languages and domains.
In this paper, we study and evaluate diverse lin-
guistic structures encoded as convolution kernels for
the document-level sentiment classification prob-
lem, in order to utilize syntactic structures without
defining explicit linguistic rules. While the applica-
tion of kernel methods could seem intuitive for many
tasks, it is non-trivial to apply convolution kernels
to document-level sentiment classification: previous
work has already shown that categorically using the
entire syntactic structure of a single sentence would
produce too many features for a convolution ker-
nel (Zhang et al, 2006; Moschitti et al, 2008). We
expect the situation to be worse for our task as we
work with documents that tend to comprise dozens
of sentences.
It is therefore necessary to choose appropriate
substructures of a sentence as opposed to using the
whole structure in order to effectively use convolu-
tion kernels in our task. It has been observed that
not every part of a document is equally informa-
tive for identifying the polarity of the whole doc-
ument (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Koppel and Schler, 2005; Ferguson et
al., 2009): a film review often uses lengthy objective
paragraphs to simply describe the plot. Such objec-
tive portions do not contain the author?s opinion and
are irrelevant with respect to the sentiment classifi-
338
cation task. Indeed, separating objective sentences
from subjective sentences in a document produces
encouraging results (Yu and Hatzivassiloglou, 2003;
Pang and Lee, 2004; Koppel and Schler, 2005; Fer-
guson et al, 2009). Our research is inspired by these
observations. Unlike in the previous work, however,
we focus on syntactic substructures (rather than en-
tire paragraphs or sentences) that contain subjective
words.
More specifically, we use the terms in the lexi-
con constructed from (Wilson et al, 2005) as the
indicators to identify the substructures for the con-
volution kernels, and extract different sub-structures
according to these indicators for various types of
parse trees (Section 3). An empirical evaluation on
a widely used sentiment corpus shows an improve-
ment of 1.45 point in accuracy over the baseline
resulting from a combination of bag-of-words and
high-impact parse features (Section 4).
2 Related Work
Our research builds on previous work in the field
of sentiment classification and convolution kernel-
s. For sentiment classification, the design of lexi-
cal and syntactic features is an important first step.
Several approaches propose feature-based learning
algorithms for this problem. Pang and Lee (2004)
and Dave et al (2003) represent a document as a
bag-of-words; Matsumoto et al, (2005) extract fre-
quently occurring connected subtrees from depen-
dency parsing; Joshi and Penstein-Rose (2009) use
a transformation of dependency relation triples; Liu
and Seneff (2009) extract adverb-adjective-noun re-
lations from dependency parser output.
Previous research has convincingly demonstrat-
ed a kernel?s ability to generate large feature set-
s, which is useful to quickly model new and not
well understood linguistic phenomena in machine
learning, and has led to improvements in various
NLP tasks, including relation extraction (Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2006; Nguyen et al, 2009), question
answering (Moschitti and Quarteroni, 2008), seman-
tic role labeling (Moschitti et al, 2008).
Convolution kernels have been used before in sen-
timent analysis: Wiegand and Klakow (2010) use
convolution kernels for opinion holder extraction,
Johansson and Moschitti (2010) for opinion expres-
sion detection and Agarwal et al (2011) for sen-
timent analysis of Twitter data. Wiegand and K-
lakow (2010) use e.g. noun phrases as possible can-
didate opinion holders, in our work we extract any
minimal syntactic context containing a subjective
word. Johansson and Moschitti (2010) and Agarwal
et al (2011) process sentences and tweets respec-
tively. However, as these are considerably shorter
than documents, their feature space is less complex,
and pruning is not as pertinent.
3 Kernels for Sentiment Classification
3.1 Linguistic Representations
We explore both sequence and convolution kernels
to exploit information on surface and syntactic lev-
els. For sequence kernels, we make use of lexical
words with some syntactic information in the form
of part-of-speech (POS) tags. More specifically, we
define three types of sequences:
? SW, a sequence of lexical words, e.g.: A tragic
waste of talent and incredible visual effects.
? SP, a sequence of POS tags, e.g.: DT JJ NN IN
NN CC JJ JJ NNS.
? SWP, a sequence of words and POS tags,
e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN
and/CC incredible/JJ visual/JJ effects/NNS.
In addition, we experiment with constituency tree
kernels (CON), and dependency tree kernels (D),
which capture hierarchical constituency structure
and labeled dependency relations between words,
respectively. For dependency kernels, we test with
word (DW), POS (DP), and combined word-and-
POS settings (DWP), and similarly for simple se-
quence kernels (SW, SP and SWP). We also use a
vector kernel (VK) in a bag-of-words baseline. Fig-
ure 1 shows the constituent and dependency struc-
ture for the above sentence.
3.2 Settings
As kernel-based algorithms inherently explore the
whole feature space to weight the features, it is im-
portant to choose appropriate substructures to re-
move unnecessary features as much as possible.
339
NP
PP
NP
DT JJ NN
A tragic waste
NP
IN
of
NP NP
NN
talent
CC
and
JJ JJ NNS
incredible visual effect
(a)
waste
det amod prep of
A tragic talent
conj and
effects
amod amod
incredible visual
(b)
waste
det amod prep of
DT JJ NN
conj and
NNS
amod amod
JJ JJ
(c)
waste
det amod prep of
DT
A
JJ
tragic
NN
talent
conj and
NNS
effects
amod amod
JJ
incredible
visual
visual
(d)
Figure 1: Illustration of the different tree structures employed for convolution kernels. (a) Constituent parse tree
(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)
with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).
NP
DT JJ NN
A tragic waste
(a)
waste
amod
JJ
tragic
(b)
Figure 2: Illustration of the different settings on con-
stituency (CON) and dependency (DWP) parse trees with
tragic as the indicator word.
Unfortunately, in our task there exist several cues
indicating the polarity of the document, which are
distributed in different sentences. To solve this prob-
lem, we define the indicators in this task as subjec-
tive words in a polarity lexicon (Wilson et al, 2005).
For each polarity indicator, we define the ?scope?
(the minimal syntactic structure containing at least
one subjective word) of each indicator for different
representations as follows:
For a constituent tree, a node and its children
correspond to a grammatical production. There-
fore, considering the terminal node tragic in the con-
stituent structure tree in Figure 1(a), we extract the
subtree rooted at the grandparent of the terminal, see
Figure 2(a). We also use the corresponding sequence
Scopes Trees Size
Document 32 24
Subjective Sentences 22 27
Constituent Substructures 30 10
Dependency Substructures 40 3
Table 1: The detail of the corpus. Here Trees denotes the
average number of trees, and Size denotes the averaged
number of words in each tree.
of words in the subtree for the sequential kernel.
For a dependency tree, we only consider the sub-
tree containing the lexical items that are directly
connected to the subjective word. For instance, giv-
en the node tragic in Figure 1(d), we will extract its
direct parent waste integrated with dependency rela-
tions and (possibly) POS, as in Figure 2(b).
We further add two background scopes, one be-
ing subjective sentences (the sentences that contain
subjective words), and the entire document.
4 Experiments
4.1 Setup
We carried out experiments on the movie review
dataset (Pang and Lee, 2004), which consists of
340
1000 positive reviews and 1000 negative reviews.
To obtain constituency trees, we parsed the docu-
ment using the Stanford Parser (Klein and Man-
ning, 2003). To obtain dependency trees, we passed
the Stanford constituency trees through the Stanford
constituency-to-dependency converter (de Marneffe
and Manning, 2008).
We exploited Subset Tree (SST) (Collins and
Duffy, 2001) and Partial Tree (PT) kernels (Mos-
chitti, 2006) for constituent and dependency parse
trees1, respectively. A sequential kernel is applied
for lexical sequences. Kernels were combined using
plain (unweighted) summation. Corpus statistics are
provided in Table 1.
We use a manually constructed polarity lexicon
(Wilson et al, 2005), in which each entry is annotat-
ed with its degree of subjectivity (strong, weak), as
well as its sentiment polarity (positive, negative and
neutral). We only take into account the subjective
terms with the degree of strong subjectivity.
We consider two baselines:
? VK: bag-of-words features using a vector ker-
nel (Pang and Lee, 2004; Ng et al, 2006)
? Rand: a number of randomly selected sub-
structures similar to the number of extracted
substructures defined in Section 3.2
All experiments were carried out using the SVM-
Light-TK toolkit2 with default parameter settings.
All results reported are based on 10-fold cross vali-
dation.
4.2 Results and Discussions
Table 2 lists the results of the different kernel type
combinations. The best performance is obtained by
combining VK and DW kernels, gaining a signifi-
cant improvement of 1.45 point in accuracy. As far
as PT kernels are concerned, we find dependency
trees with simple words (DW) outperform both de-
pendency trees with POS (DP) and those with both
words and POS (DWP). We conjecture that in this
case, as syntactic information is already captured by
1A SubSet Tree is a structure that satisfies the constraint that
grammatical rules cannot be broken, while a Partial Tree is a
more general form of substructures obtained by the application
of partial production rules of the grammar.
2available at http://disi.unitn.it/moschitti/
Kernels Doc Sent Rand Sub
VK 87.05
VK + SW 87.25 86.95 87.25 87.40
VK + SP 87.35 86.95 87.45 87.35
VK + SWP 87.30 87.45 87.30 88.15*
VK + CON 87.45 87.65 87.45 88.30**
VK + DW 87.35 87.50 87.30 88.50**
VK + DP 87.75* 87.20 87.35 87.75
VK + DWP 87.70* 87.30 87.65 87.80*
Table 2: Results of kernels. Here Doc denotes the whole
document of the text, Sent denotes the sentences that con-
tains subjective terms in the lexicon, Rand denotes ran-
domly selected substructures, and Sub denotes the sub-
structures defined in Section 3.2. We use ?*? and ?**? to
denote a result is better than baseline VK significantly at
p < 0.05 and p < 0.01 (sign test), respectively.
the dependency representation, POS tags can intro-
duce little new information, and will add unneces-
sary complexity. For example, given the substruc-
ture (waste (amod (JJ (tragic)))), the PT kernel will
use both (waste (amod (JJ))) and (waste (amod (JJ
(tragic)))). We can see that the former is adding no
value to the model, as the JJ tag could indicate ei-
ther positive words (e.g. good) or negative words
(e.g. tragic). In contrast, words are good indicators
for sentiment polarity.
The results in Table 2 confirm two of our hy-
potheses. Firstly, it clearly demonstrates the val-
ue of incorporating syntactic information into the
document-level sentiment classifier, as the tree k-
ernels (CON and D*) generally outperforms vector
and sequence kernels (VK and S*). More impor-
tantly, it also shows the necessity of extracting ap-
propriate substructures when using convolution ker-
nels in our task: when using the dependency kernel
(VK+DW), the result on lexicon guided substruc-
tures (Sub) outperforms the results on document,
sentence, or randomly selected substructures, with
statistical significance (p<0.05).
5 Conclusion and Future Work
We studied the impact of syntactic information on
document-level sentiment classification using con-
volution kernels, and reduced the complexity of the
kernels by extracting minimal high-impact substruc-
tures, guided by a polarity lexicon. Experiments
341
show that our method outperformed a bag-of-words
baseline with a statistically significant gain of 1.45
absolute point in accuracy.
Our research focuses on identifying and using
high-impact substructures for convolution kernels in
document-level sentiment classification. We expect
our method to be complementary with sophisticated
methods used in state-of-the-art sentiment classifica-
tion systems, which is to be explored in future work.
Acknowledgement
The authors were supported by 863 State Key
Project No. 2006AA010108, the EuroMatrixPlus F-
P7 EU project (grant No 231720) and Science Foun-
dation Ireland (Grant No. 07/CE/I1142). Part of the
research was done while Zhaopeng Tu was visiting,
and Yifan He was at the Centre for Next Generation
Localisation (www.cngl.ie), School of Computing,
Dublin City University. We thank the anonymous
reviewers for their insightful comments. We are al-
so grateful to Junhui Li for his helpful feedback.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, pages 30?38. Association
for Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005a. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Methods
in Natural Language Processing, pages 724?731, Van-
couver, British Columbia, Canada, oct. Association for
Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005b. Sub-
sequence Kernels for Relation Extraction. In Y Weis-
s, B Sch o lkopf, and J Platt, editors, Proceedings of
the 19th Conference on Neural Information Processing
Systems, pages 171?178, Cambridge, MA. MIT Press.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation, Manchester, August.
Paul Ferguson, Neil O?Hare, Michael Davy, Adam
Bermingham, Paraic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Exploring the use of
paragraph-level annotations for sentiment analysis of
financial blogs. In Proceedings of the Workshop on
Opinion Mining and Sentiment Analysis.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing Dependency Features for Opinion Mining.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 313?316, Suntec, Singapore, jul.
Suntec, Singapore.
Dan Klein and Christopher D Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, jul. As-
sociation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2005. Using neutral
examples for learning polarity. In Proceedings of In-
ternational Joint Conferences on Artificial Intelligence
(IJCAI) 2005, pages 1616?1616.
Steve Lawrence Kushal Dave and David Pennock. 2003.
Mining the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proceed-
ings of the 12th International Conference on World
Wide Web, pages 519?528, ACM. ACM.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Paradigm.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 161?
169, Singapore, aug. Singapore.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. Proceed-
ings of PAKDD?05, the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
3518/2005:21?32.
Alessandro Moschitti and Silvia Quarteroni. 2008. K-
ernels on Linguistic Structures for Answer Extraction.
In Proceedings of ACL-08: HLT, Short Papers, pages
113?116, Columbus, Ohio, jun. Association for Com-
putational Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning, pages 318?329, Berlin, Germany,
342
sep. Machine Learning: ECML 2006, 17th European
Conference on Machine Learning, Proceedings.
Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006.
Examining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 611?618,
Sydney, Australia, jul. Sydney, Australia.
Truc-Vien T Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1378?1387.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 271?278, Barcelona, S-
pain, jun. Barcelona, Spain.
Michael Wiegand and Dietrich Klakow. 2010. Convolu-
tion Kernels for Opinion Holder Extraction. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 795?803, Los An-
geles, California, jun. Los Angeles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, British Columbia, Cana-
da, oct. Association for Computational Linguistics.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Toward-
s answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Association for Computational Lin-
guistics. Association for Computational Linguistics.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Features.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 825?832, Sydney, Australia, jul. Association for
Computational Linguistics.
343
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 358?363,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Novel Graph-based Compact Representation of Word Alignment
Qun Liu?? Zhaopeng Tu? Shouxun Lin?
?Centre for Next Generation Locolisation ?Key Lab. of Intelligent Info. Processing
Dublin City University Institute of Computing Technology, CAS
qliu@computing.dcu.ie {tuzhaopeng,sxlin}@ict.ac.cn
Abstract
In this paper, we propose a novel compact
representation called weighted bipartite
hypergraph to exploit the fertility model,
which plays a critical role in word align-
ment. However, estimating the probabili-
ties of rules extracted from hypergraphs is
an NP-complete problem, which is com-
putationally infeasible. Therefore, we pro-
pose a divide-and-conquer strategy by de-
composing a hypergraph into a set of inde-
pendent subhypergraphs. The experiments
show that our approach outperforms both
1-best and n-best alignments.
1 Introduction
Word alignment is the task of identifying trans-
lational relations between words in parallel cor-
pora, in which a word at one language is usually
translated into several words at the other language
(fertility model) (Brown et al, 1993). Given that
many-to-many links are common in natural lan-
guages (Moore, 2005), it is necessary to pay atten-
tion to the relations among alignment links.
In this paper, we have proposed a novel graph-
based compact representation of word alignment,
which takes into account the joint distribution of
alignment links. We first transform each align-
ment to a bigraph that can be decomposed into a
set of subgraphs, where all interrelated links are
in the same subgraph (? 2.1). Then we employ
a weighted partite hypergraph to encode multiple
bigraphs (? 2.2).
The main challenge of this research is to effi-
ciently calculate the fractional counts for rules ex-
tracted from hypergraphs. This is equivalent to the
decision version of set covering problem, which is
NP-complete. Observing that most alignments are
not connected, we propose a divide-and-conquer
strategy by decomposing a hypergraph into a set
Figure 1: A bigraph constructed from an align-
ment (a), and its disjoint MCSs (b).
of independent subhypergraphs, which is compu-
tationally feasible in practice (? 3.2). Experimen-
tal results show that our approach significantly im-
proves translation performance by up to 1.3 BLEU
points over 1-best alignments (? 4.3).
2 Graph-based Compact Representation
2.1 Word Alignment as a Bigraph
Each alignment of a sentence pair can be trans-
formed to a bigraph, in which the two disjoint ver-
tex sets S and T are the source and target words re-
spectively, and the edges are word-by-word links.
For example, Figure 1(a) shows the corresponding
bigraph of an alignment.
The bigraph usually is not connected. A graph
is called connected if there is a path between every
pair of distinct vertices. In an alignment, words in
a specific portion at the source side (i.e. a verb
phrase) usually align to those in the corresponding
portion (i.e. the verb phrase at the target side), and
would never align to other words; and vice versa.
Therefore, there is no edge that connects the words
in the portion to those outside the portion.
Therefore, a bigraph can be decomposed into
a unique set of minimum connected subgraphs
(MCSs), where each subgraph is connected and
does not contain any other MCSs. For example,
the bigraph in Figure 1(a) can be decomposed into
358
the
book
is
on
the
desk
?
?
??
?
D
the
book
is
on
the
desk
?
?
??
?
e1
F
the
book
is
on
the
desk
?
?
??
?
E
e2 e3
e4
e5
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c) the
resulting hypergraph that takes the two alignments as samples.
the MCSs in Figure 1(b). We can see that all in-
terrelated links are in the same MCS. These MCSs
work as fundamental units in our approach to take
advantage of the relations among the links. Here-
inafter, we use bigraph to denote the alignment of
a sentence pair.
2.2 Weighted Bipartite Hypergraph
We believe that offering more alternatives to ex-
tracting translation rules could help improve trans-
lation quality. We propose a new structure called
weighted bipartite hypergraph that compactly en-
codes multiple alignments.
We use an example to illustrate our idea. Fig-
ures 2(a) and 2(b) show two bigraphs of the same
sentence pair. Intuitively, we can encode the
union set of subgraphs in a bipartite hypergraph,
in which each MCS serves as a hyperedge, as in
Figure 2(c). Accordingly, we can calculate how
well a hyperedge is by calculating its relative fre-
quency, which is the probability sum of bigraphs
in which the corresponding MCS occurs divided
by the probability sum of all possible bigraphs.
Suppose that the probabilities of the two bigraphs
in Figures 2(a) and 2(b) are 0.7 and 0.3, respec-
tively. Then the weight of e1 is 1.0 and e2 is
0.7. Therefore, each hyperedge is associated with
a weight to indicate how well it is.
Formally, a weighted bipartite hypergraph H is
a triple ?S, T,E? where S and T are two sets of
vertices on the source and target sides, and E are
hyperedges associated with weights. Currently,
we estimate the weights of hyperedges from an n-
best list by calculating relative frequencies:
w(ei) =
?
BG?N p(BG) ? ?(BG, gi)?
BG?N p(BG)
Here N is an n-best bigraph (i.e., alignment) list,
p(BG) is the probability of a bigraph BG in the n-
best list, gi is the MCS that corresponds to ei, and
?(BG, gi) is an indicator function which equals 1
when gi occurs in BG, and 0 otherwise.
It is worthy mentioning that a hypergraph en-
codes much more alignments than the input n-best
list. For example, we can construct a new align-
ment by using hyperedges from different bigraphs
that cover all vertices.
3 Graph-based Rule Extraction
In this section we describe how to extract transla-
tion rules from a hypergraph (? 3.1) and how to
estimate their probabilities (? 3.2).
3.1 Extraction Algorithm
We extract translation rules from a hypergraph
for the hierarchical phrase-based system (Chiang,
2007). Chiang (2007) describes a rule extrac-
tion algorithm that involves two steps: (1) extract
phrases from 1-best alignments; (2) obtain vari-
able rules by replacing sub-phrase pairs with non-
terminals. Our extraction algorithm differs at the
first step, in which we extract phrases from hyper-
graphs instead of 1-best alignments. Rather than
restricting ourselves by the alignment consistency
in the traditional algorithm, we extract all possible
candidate target phrases for each source phrase.
To maintain a reasonable rule table size, we fil-
ter out less promising candidates that have a frac-
tional count lower than a threshold.
3.2 Calculating Fractional Counts
The fractional count of a phrase pair is the proba-
bility sum of the alignments with which the phrase
pair is consistent (?3.2.2), divided by the probabil-
ity sum of all alignments encoded in a hypergraph
(?3.2.1) (Liu et al, 2009).
359
Intuitively, our approach faces two challenges:
1. How to calculate the probability sum of all
alignments encoded in a hypergraph (?3.2.1)?
2. How to efficiently calculate the probability
sum of all consistent alignments for each
phrase pair (?3.2.2)?
3.2.1 Enumerating All Alignments
In theory, a hypergraph can encode all possible
alignments if there are enough hyperedges. How-
ever, since a hypergraph is constructed from an n-
best list, it can only represent partial space of all
alignments (p(A|H) < 1) because of the limiting
size of hyperedges learned from the list. There-
fore, we need to enumerate all possible align-
ments in a hypergraph to obtain the probability
sum p(A|H).
Specifically, generating an alignment from a hy-
pergraph can be modelled as finding a complete
hyperedge matching, which is a set of hyperedges
without common vertices that matches all vertices.
The probability of the alignment is the product of
hyperedge weights. Thus, enumerating all possi-
ble alignments in a hypergraph is reformulated as
finding all complete hypergraph matchings, which
is an NP-complete problem (Valiant, 1979).
Similar to the bigraph, a hypergraph is also usu-
ally not connected. To make the enumeration prac-
tically tractable, we propose a divide-and-conquer
strategy by decomposing a hypergraph H into a set
of independent subhypergraphs {h1, h2, . . . , hn}.
Intuitively, the probability of an alignment is the
product of hyperedge weights. According to the
divide-and-conquer strategy, the probability sum
of all alignments A encoded in a hypergraph H is:
p(A|H) =
?
hi?H
p(Ai|hi)
Here p(Ai|hi) is the probability sum of all sub-
alignments Ai encoded in the subhypergraph hi.
3.2.2 Enumerating Consistent Alignments
Since a hypergraph encodes many alignments, it is
unrealistic to enumerate all consistent alignments
explicitly for each phrase pair.
Recall that a hypergraph can be decomposed
to a list of independent subhypergraphs, and an
alignment is a combination of the sub-alignments
from the decompositions. We observe that a
phrase pair is absolutely consistent with the sub-
alignments from some subhypergraphs, while pos-
sibly consistent with the others. As an example,
E
the
book
is
on
the
desk
?
?
??
?
e1
D
e2 e3
e4
e5
the
book
is
on
the
desk
?
?
??
?
e1
e2 e3
e4
e5
h1
h3
h2
Figure 3: A hypergraph with a candidate phrase
in the grey shadow (a), and its independent subhy-
pergraphs {h1, h2, h3}.
consider the phrase pair in the grey shadow in Fig-
ure 3(a), it is consistent with all sub-alignments
from both h1 and h2 because they are outside and
inside the phrase pair respectively, while not con-
sistent with the sub-alignment that contains hyper-
edge e2 from h3 because it contains an alignment
link that crosses the phrase pair.
Therefore, to calculate the probability sum of all
consistent alignments, we only need to consider
the overlap subhypergraphs, which have at least
one hyperedge that crosses the phrase pair. Given
a overlap subhypergraph, the probability sum of
consistent sub-alignments is calculated by sub-
tracting the probability sum of the sub-alignments
that contain crossed hyperedges, from the proba-
bility sum of all sub-alignments encoded in a hy-
pergraph.
Given a phrase pair P , let OS and NS de-
notes the sets of overlap and non-overlap subhy-
pergraphs respectively (NS = H ?OS). Then
p(A|H,P ) =
?
hi?OS
p(Ai|hi, P )
?
hj?NS
p(Aj|hj)
Here the phrase pair is absolutely consistent with
the sub-alignments from non-overlap subhyper-
graphs (NS), and we have p(A|h, P ) = p(A|h).
Then the fractional count of a phrase pair is:
c(P |H) = p(A|H,P )p(A|H) =
?
hi?OS p(A|hi, P )?
hi?OS p(A|hi)
After we get the fractional counts of transla-
tion rules, we can estimate their relative frequen-
cies (Och and Ney, 2004). We follow (Liu et al,
2009; Tu et al, 2011) to learn lexical tables from
n-best lists and then calculate the lexical weights.
360
Rules from. . . Rules MT03 MT04 MT05 Avg.
1-best 257M 33.45 35.25 33.63 34.11
10-best 427M 34.10 35.71 34.04 34.62
Hypergraph 426M 34.71 36.24 34.41 35.12
Table 1: Evaluation of translation quality.
4 Experiments
4.1 Setup
We carry out our experiments on Chinese-English
translation tasks using a reimplementation of the
hierarchical phrase-based system (Chiang, 2007).
Our training data contains 1.5 million sentence
pairs from LDC dataset.1 We train a 4-gram
language model on the Xinhua portion of the
GIGAWORD corpus using the SRI Language
Toolkit (Stolcke, 2002) with modified Kneser-Ney
Smoothing (Kneser and Ney, 1995). We use min-
imum error rate training (Och, 2003) to optimize
the feature weights on the MT02 testset, and test
on the MT03/04/05 testsets. For evaluation, case-
insensitive NIST BLEU (Papineni et al, 2002) is
used to measure translation performance.
We first follow Venugopal et al (2008) to pro-
duce n-best lists via GIZA++. We produce 10-best
lists in two translation directions, and use ?grow-
diag-final-and? strategy (Koehn et al, 2003) to
generate the final n-best lists by selecting the
top n alignments. We re-estimated the probabil-
ity of each alignment in the n-best list using re-
normalization (Venugopal et al, 2008). Finally we
construct weighted alignment hypergraphs from
these n-best lists.2 When extracting rules from hy-
pergraphs, we set the pruning threshold t = 0.5.
4.2 Tractability of Divide-and-Conquer
Strategy
Figure 4 shows the distribution of vertices (hy-
peredges) number of the subhypergraphs. We can
see that most of the subhypergraphs have just less
than two vertices and hyperedges.3 Specifically,
each subhypergraph has 2.0 vertices and 1.4 hy-
1The corpus includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.
2Here we only use 10-best lists, because the alignments
beyond top 10 have very small probabilities, thus have negli-
gible influence on the hypergraphs.
3It?s interesting that there are few subhypergraphs that
have exactly 2 hyperedges. In this case, the only two hy-
peredges fully cover the vertices and they differ at the word-
by-word links, which is uncommon in n-best lists.
 0
 0.2
 0.4
 0.6
 0.8
 1
1 2 3 4 5 6 7 8 9 10
pe
rc
en
ta
ge
number of vertices (hyperedges)
vertices
hyperedges
Figure 4: The distribution of vertices (hyperedges)
number of the subhypergraphs.
peredges on average. This suggests that the divide-
and-conquer strategy makes the extraction compu-
tationally tractable, because it greatly reduces the
number of vertices and hyperedges. For computa-
tional tractability, we only allow a subhypergraph
has at most 5 hyperedges. 4
4.3 Translation Performance
Table 1 shows the rule table size and transla-
tion quality. Using n-best lists slightly improves
the BLEU score over 1-best alignments, but at
the cost of a larger rule table. This is in ac-
cord with intuition, because all possible transla-
tion rules would be extracted from different align-
ments in n-best lists without pruning. This larger
rule table indeed leads to a high rule coverage, but
in the meanwhile, introduces translation errors be-
cause of the low-quality rules (i.e., rules extracted
only from low-quality alignments in n-best lists).
By contrast, our approach not only significantly
improves the translation performance over 1-best
alignments, but also outperforms n-best lists with
a similar-scale rule table. The absolute improve-
ments of 1.0 BLEU points on average over 1-best
alignments are statistically significant at p < 0.01
using sign-test (Collins et al, 2005).
4If a subhypergraph has more than 5 hyperedges, we
forcibly partition it into small subhypergraphs by iteratively
removing lowest-probability hyperedges.
361
Rules from. . . Shared Non-shared AllRules BLEU Rules BLEU Rules BLEU
10-best 1.83M 32.75 2.81M 30.71 4.64M 34.62
Hypergraph 1.83M 33.24 2.89M 31.12 4.72M 35.12
Table 2: Comparison of rule tables learned from n-best lists and hypergraphs. ?All? denotes the full rule
table, ?Shared? denotes the intersection of two tables, and ?Non-shared? denotes the complement. Note
that the probabilities of ?Shared? rules are different for the two approaches.
Why our approach outperforms n-best lists? In
theory, the rule table extracted from n-best lists
is a subset of that from hypergraphs. In prac-
tice, however, this is not true because we pruned
the rules that have fractional counts lower than a
threshold. Therefore, the question arises as to how
many rules are shared by n-best and hypergraph-
based extractions. We try to answer this ques-
tion by comparing the different rule tables (filtered
on the test sets) learned from n-best lists and hy-
pergraphs. Table 2 gives some statistics. ?All?
denotes the full rule table, ?Shared? denotes the
intersection of two tables, and ?Non-shared? de-
notes the complement. Note that the probabil-
ities of ?Shared? rules are different for the two
approaches. We can see that both the ?Shared?
and ?Non-shared? rules learned from hypergraphs
outperform n-best lists, indicating: (1) our ap-
proach has a better estimation of rule probabili-
ties because we estimate the probabilities from a
much larger alignment space that can not be rep-
resented by n-best lists, (2) our approach can ex-
tract good rules that cannot be extracted from any
single alignments in the n-best lists.
5 Related Work
Our research builds on previous work in the field
of graph models and compact representations.
Graph models have been used before in word
alignment: the search space of word alignment can
be structured as a graph and the search problem
can be reformulated as finding the optimal path
though this graph (e.g., (Och and Ney, 2004; Liu et
al., 2010)). In addition, Kumar and Byrne (2002)
define a graph distance as a loss function for
minimum Bayes-risk word alignment, Riesa and
Marcu (2010) open up the word alignment task to
advances in hypergraph algorithms currently used
in parsing. As opposed to the search problem, we
propose a graph-based compact representation that
encodes multiple alignments for machine transla-
tion.
Previous research has demonstrated that com-
pact representations can produce improved re-
sults by offering more alternatives, e.g., using
forests over 1-best trees (Mi and Huang, 2008;
Tu et al, 2010; Tu et al, 2012a), word lattices
over 1-best segmentations (Dyer et al, 2008),
and weighted alignment matrices over 1-best word
alignments (Liu et al, 2009; Tu et al, 2011; Tu et
al., 2012b). Liu et al, (2009) estimate the link
probabilities from n-best lists, while Gispert et
al., (2010) learn the alignment posterior probabil-
ities directly from IBM models. However, both of
them ignore the relations among alignment links.
By contrast, our approach takes into account the
joint distribution of alignment links and explores
the fertility model past the link level.
6 Conclusion
We have presented a novel compact representa-
tion of word alignment, named weighted bipar-
tite hypergraph, to exploit the relations among
alignment links. Since estimating the probabil-
ities of rules extracted from hypergraphs is an
NP-complete problem, we propose a computation-
ally tractable divide-and-conquer strategy by de-
composing a hypergraph into a set of independent
subhypergraphs. Experimental results show that
our approach outperforms both 1-best and n-best
alignments.
Acknowledgement
The authors are supported by 863 State Key
Project No. 2011AA01A207, National Key Tech-
nology R&D Program No. 2012BAH39B03 and
National Natural Science Foundation of China
(Contracts 61202216). Qun Liu?s work is partially
supported by Science Foundation Ireland (Grant
No.07/CE/I1142) as part of the CNGL at Dublin
City University. We thank Junhui Li, Yifan He
and the anonymous reviewers for their insightful
comments.
362
References
Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 531?540.
Adria` de Gispert, Juan Pino, and William Byrne. 2010.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
545?554.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012?1020.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
181?184.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 48?54.
Shankar Kumar and William Byrne. 2002. Mini-
mum Bayes-risk word alignments of bilingual texts.
In Proceedings of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing, pages
140?147.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1017?1026.
Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discrim-
inative word alignment by linear modeling. Compu-
tational Linguistics, 36(3):303?339.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206?214.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, October.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 157?166.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of Seventh Inter-
national Conference on Spoken Language Process-
ing, volume 3, pages 901?904. Citeseer.
Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency forest
for statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 1092?1100.
Zhaopeng Tu, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Extracting hierarchical rules from a weighted
alignment matrix. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 1294?1303.
Zhaopeng Tu, Wenbin Jiang, Qun Liu, and Shouxun
Lin. 2012a. Dependency forest for sentiment anal-
ysis. In Springer-Verlag Berlin Heidelberg, pages
69?77.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012b. Combining mul-
tiple alignments to improve machine translation. In
Proceedings of the 24th International Conference on
Computational Linguistics, pages 1249?1260.
Leslie G Valiant. 1979. The complexity of comput-
ing the permanent. Theoretical Computer Science,
8(2):189?201.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-best
alignments and parses in mt training. In Proceed-
ings of AMTA, pages 192?201.
363
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 232?242,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Using Syntactic Head Information in Hierarchical Phrase-Based Translation
Junhui Li Zhaopeng Tu? Guodong Zhou? Josef van Genabith
Centre for Next Generation Localisation
School of Computing, Dublin City University
? Key Lab. of Intelligent Info. Processing
Institute of Computing Technology, Chinese Academy of Sciences
?School of Computer Science and Technology
Soochow University, China
{jli,josef}@computing.dcu.ie
tuzhaopeng@ict.ac.cn gdzhou@suda.edu.cn
Abstract
Chiang?s hierarchical phrase-based (HPB)
translation model advances the state-of-the-art
in statistical machine translation by expanding
conventional phrases to hierarchical phrases
? phrases that contain sub-phrases. How-
ever, the original HPB model is prone to over-
generation due to lack of linguistic knowl-
edge: the grammar may suggest more deriva-
tions than appropriate, many of which may
lead to ungrammatical translations. On the
other hand, limitations of glue grammar rules
in the original HPB model may actually pre-
vent systems from considering some reason-
able derivations. This paper presents a sim-
ple but effective translation model, called the
Head-Driven HPB (HD-HPB) model, which
incorporates head information in translation
rules to better capture syntax-driven informa-
tion in a derivation. In addition, unlike the
original glue rules, the HD-HPB model allows
improved reordering between any two neigh-
boring non-terminals to explore a larger re-
ordering search space. An extensive set of ex-
periments on Chinese-English translation on
four NIST MT test sets, using both a small
and a large training set, show that our HD-
HPB model consistently and statistically sig-
nificantly outperforms Chiang?s model as well
as a source side SAMT-style model.
1 Introduction
Chiang?s hierarchical phrase-based (HPB) transla-
tion model utilizes synchronous context free gram-
mar (SCFG) for translation derivation (Chiang,
2005; Chiang, 2007) and has been widely adopted
in statistical machine translation (SMT). Typically,
such models define two types of translation rules:
hierarchical (translation) rules which consist of both
terminals and non-terminals, and glue (grammar)
rules which combine translated phrases in a mono-
tone fashion. However, due to lack of linguistic
knowledge, Chiang?s HPB model contains only one
type of non-terminal symbol X , often making it
difficult to select the most appropriate translation
rules.1
One important research question is therefore how
to refine the non-terminal category X using linguis-
tically motivated information: Zollmann and Venu-
gopal (2006) (SAMT) e.g. use (partial) syntactic
categories derived from CFG trees while Zollmann
and Vogel (2011) use word tags, generated by ei-
ther POS analysis or unsupervised word class in-
duction. Almaghout et al (2011) employ CCG-
based supertags. Mylonakis and Sima?an (2011) use
linguistic information of various granularities such
as Phrase-Pair, Constituent, Concatenation of Con-
stituents, and Partial Constituents, where applica-
ble.
By contrast, and inspired by previous work in
parsing (Charniak, 2000; Collins, 2003), our Head-
Driven HPB (HD-HPB) model is based on the in-
tuition that linguistic heads provide important in-
formation about a constituent or distributionally de-
fined fragment, as in HPB. We identify heads using
linguistically motivated dependency parsing, and
use head information to refine X.
Furthermore, Chiang?s HPB model suffers from
limited phrase reordering by combining translated
1Another non-terminal symbol S is used in glue rules.
232
 (a) (b) 
zuotian chuxi huiyi 
attended a meeting yesterday 
X2 X1 
X1 X2 
S2 
S1 
S2 
S1 
zuotian chuxi huiyi 
attended a meeting yesterday 
X4 X3 
X3 X4 
X2 
X1 
X2 
S2 
X1 
S1 
S1 
X1 
Figure 1: Example of derivations disallowed in Chiang?s
HPB model. The rules with dotted lines are not covered
in Chiang?s model.
phrases in a monotonic way with glue rules. In
addition, once a glue rule is adopted, it requires
all rules above it to be glue rules. For exam-
ple, given a Chinese-English sentence pair (?
?/zuotian1 ??/chuxi2 ??/huiyi3, Attended2 a3
meeting3 yesterday1), a correct translation is impos-
sible via HPB derivations in Figure 1. For the deriva-
tion in Figure 1(a), swap reordering in the glue rule
(i.e., S1 ? ?S2X2, X2S2?) is disallowed and, even
if such a swap reordering is available, it lacks useful
information for rule selection. For the derivation in
Figure 1(b), the combination of two non-terminals
(i.e., X2 ? ?X3X4, X3X4?) is disallowed to form
a new non-terminal which in turn is a sub-phrase of
a hierarchical rule. These limitations prevent tra-
ditional HPB systems from even considering some
reasonable derivations.
To tackle the problem of glue rules, He (2010) ex-
tended the HPB model by using bracketing transduc-
tion grammar (Wu, 1996) instead of the monotone
glue rules, and trained an extra classifier for glue
rules to predict reorderings of neighboring phrases.
By contrast, our HD-HPB model refines the non-
terminal symbol X with syntactic head informa-
tion and provides flexible reordering rules, including
swap, which can mix freely with hierarchical trans-
lation rules for better interleaving of translation and
reordering in translation derivations.
Different from the soft constraint modeling
adopted in (Chan et al, 2007; Marton and Resnik,
2008; Shen et al, 2009; He et al, 2010; Huang et
al., 2010; Gao et al, 2011), our approach encodes
syntactic information in translation rules. However,
the two approaches are not mutually exclusive, as
we could also include a set of syntax-driven features
into our translation model. Our approach maintains
the advantages of Chiang?s HPB model while at the
same time incorporating head information and flex-
ible reordering in a derivation in a natural way. Ex-
periments on Chinese-English translation using four
NIST MT test sets show that our HD-HPB model
significantly outperforms Chiang?s HPB as well as a
SAMT-style refined version of HPB.
The paper is structured as follows: Section 2
describes the synchronous context-free grammar
(SCFG) in our HD-HPB translation model. Sec-
tion 3 presents our model and features, followed by
the decoding algorithm in Section 4. We report ex-
perimental results in Section 5. Finally we conclude
in Section 6.
2 Head-Driven HPB Translation Model
Like Chiang (2005) and Chiang (2007), our HD-
HPB translation model adopts a synchronous con-
text free grammar, a rewriting system which gen-
erates source and target side string pairs simultane-
ously using a context-free grammar. In particular,
each synchronous rule rewrites a non-terminal into
a pair of strings, s and t, where s (or t) contains ter-
minals and non-terminals from the source (or target)
language and there is a one-to-one correspondence
between the non-terminal symbols on both sides.
A good and informative inventory of non-terminal
symbols is always important, especially for a suc-
cessful SCFG-based translation model. Instead of
collapsing all non-terminals in the source language
into a single symbol X as in Chiang (2007), ideally
non-terminals should capture important information
of the word sequences they cover to be able to prop-
erly discriminate between similar and different word
sequences during translation. This motivates our
approach to provide syntax-enriched non-terminal
symbols. Given a word sequence f ij from position i
to position j, we refine the non-terminal symbol X
to reflect some of the internal syntactic structure of
233
?
?
/N
R
 
O
uz
ho
u 
?
?
/N
N 
ba
gu
o 
?
?
/A
D
li
an
mi
ng
?
?
/V
V
zh
ic
hi
 
?
?
/N
R 
me
ig
uo
 
?
/P du
i
?
?
/N
N
ce
li
e 
?
/N
R
yi
 
ro
ot
E
ig
ht
 
E
ur
op
ea
n 
co
un
tr
ie
s
jo
in
tly
su
pp
or
t
A
m
er
ic
a?
s
st
an
d
ag
ai
ns
t
Ir
aq
Figure 2: An example word alignment for a Chinese-English sentence pair with the dependency parse tree for the
Chinese sentence. Here, each Chinese word is attached with its POS tag and Pinyin.
the word sequence covered by X . A correct transla-
tion rule selection therefore not only maps terminals
into terminals, but is both constrained and guided
by syntactic information in the non-terminals. At
the same time, it is not clear whether an ?ideal? ap-
proach that captures a full syntactic analysis of the
string fragment covered by a non-terminal is feasi-
ble: the diversity of syntactic structures could make
training impossible and lead to serious data sparse-
ness issues. As a compromise, given a word se-
quence f ij , we first find heads and then concatenate
the POS tags of these heads as f ij?s non-terminal
symbol.2 Our approach is guided by the intuition
that linguistic heads provide important information
about a constituent or distributionally defined frag-
ment, as in HPB. Specifically, we adopt dependency
structure to derive heads, which are defined as:
Definition 1. For word sequence f ij , word
fk (i ? k ? j) is regarded as a head if it is domi-
nated by a word outside of this sequence.
Note that this definition (i) allows for a word se-
quence to have one or more heads (largely due to
the fact that a word sequence is not necessarily lin-
guistically constrained) and (ii) ensures that heads
are always the highest heads in the sequence from a
dependency structure perspective. For example, the
word sequence ouzhou baguo lianming in Figure 2
has two heads (i.e., baguo and lianming, ouzhou is
not a head of this sequence since its headword baguo
falls within this sequence) and the non-terminal cor-
responding to the sequence is thus labeled as NN-
AD. It is worth noting that in this paper we only
refine non-terminal X on the source side to head-
informed ones, while still usingX on the target side.
2Note that instead of POS tags, it is also possible to use other
types of syntactic information associated with heads to refine
non-terminal symbols (Section 5.5.2).
In our HD-HPB model, the SCFG is defined as
a tuple ??, N,?,?,<?, where ? is a set of source
language terminals,N is a set of non-terminals cate-
gorizing terminals in ?, ? is a set of target language
terminals, ? is a set of non-terminals categorizing
terminals in ?, and < is a set of translation rules.
A rule ? in < is in the form of ?Ps ? s, Pt ? t, ??,
where:
? Ps ? N and Pt ? ?;
? s ? (? ?N)+ and t ? (? ? ?)+
? ? is a bijection between non-terminals in s and t.
According to the occurrence of terminals in s and
t, we group the rules in the HD-HPB model into two
categories: head-driven hierarchical rules (HD-HRs)
and non-terminal reordering rules (NRRs), where
the former have at least one terminal on both source
and target sides and the later have no terminals. For
rule extraction, we first identify initial phrase pairs
on word-aligned sentence pairs by using the same
criterion as most phrase-based translation models
(Och and Ney, 2004) and Chiang?s HPB model (Chi-
ang, 2005; Chiang, 2007). We extract HD-HRs and
NRRs based on initial phrase pairs, respectively.
2.1 HD-HRs: Head-Driven Hierarchical Rules
As mentioned, a HD-HR has at least one terminal
on both source and target sides. This is the same
as the hierarchical rules defined in Chiang?s HPB
model (Chiang, 2007), except that we use head POS-
informed non-terminal symbols in the source lan-
guage. We look for initial phrase pairs that con-
tain other phrases and then replace sub-phrases with
their corresponding non-terminal symbols. Given
the word alignment as shown in Figure 2, Table 1
demonstrates the difference between hierarchical
rules in Chiang (2007) and HD-HRs defined here.
234
phrase pairs hierarchical rule head-driven hierarchical rule
celie, stand X?celie, stand
NN?celie,
X?stand
dui yi celie1, stand1 against Iraq X?dui yi X1, X1 against Iraq
NN?dui yi NN1,
X?X1 against Iraq
zhichi meiguo, support America?s X?zhichi meiguo, support America?s
VV-NR?zhichi meiguo,
X?support America?s
zhichi meiguo1 dui yi celie2,
support America?s1 stand2 against Iraq
X?X1 dui yi X2,
X1 X2 against Iraq
VV?VV-NR1 dui yi NN2,
X?X1 X2 against Iraq
Table 1: Comparison of hierarchical rules in Chiang (2007) and HD-HRs. Indexed underlines indicate sub-phrases
and corresponding non-terminal symbols. The non-terminals in HD-HRs (e.g., NN, VV, VV-NR) capture the head(s)
POS tags of the corresponding word sequence in the source language.
Similar to Chiang?s HPB model, our HD-HPB
model will result in a large number of rules causing
problems in decoding. To alleviate these problems,
we filter our HD-HRs according to the same con-
straints as described in Chiang (2007). Moreover,
we discard rules that have non-terminals with more
than four heads.
2.2 NRRs: Non-terminal Reordering Rules
NRRs are translation rules without terminals. Given
an initial phrase pair
?
f ij , e
i?
j?
?
, we check all other
initial phrase pairs
?
fkl , e
k?
l?
?
which satisfy k = j+1
(i.e., phrase fkl is located immediately to the right
of f ij in the source language). For their target
side translations, there are four possible positional
relationships: monotone, discontinuous monotone,
swap, and discontinuous swap. In order to differen-
tiate non-terminals from those in the target language
(i.e., X), we use Y as a variable for non-terminals in
the source language, and obtain four types of NRRs:
? Monotone ?Y ? Y1Y2, X ? X1X2?;
? Discontinuous monotone
?Y ? Y1Y2, X ? X1 . . . X2?;
? Swap ?Y ? Y1Y2, X ? X2X1?;
? Discontinuous swap
?Y ? Y1Y2, X ? X2 . . . X1?.
For example in Figure 2, the NRR for initial
phrase pairs ?zhichi meiguo, support America?s?
and ?dui yi celie, stand against Iraq? would be
?V V ? V V -NR1NN2, X ? X1X2?.
Merging two neighboring non-terminals into a
single non-terminal, NRRs enable the translation
model to explore a wider search space. During train-
ing, we extract four types of NRRs and calculate
probabilities for each type. To speed up decoding,
we currently (i) only use monotone and swap NRRs
and (ii) limit the number of non-terminals in a NRR
to 2.
3 Log-linear Model and Features
Following Och and Ney (2002), we depart from the
traditional noisy-channel approach and use a general
log-linear model. Let d be a derivation from sen-
tence f in the source language to sentence e in the
target language. The probability of d is defined as:
P (d) ?
?
i
?i (d)
?i (1)
where ?i are features defined on derivations and
?i are feature weights. In particular, we use a fea-
ture set analogous to the default feature set of Chi-
ang (2007), which includes:
? Phd-hr (t|s) and Phd-hr (s|t), translation probabili-
ties for HD-HRs;
? Plex (t|s) and Plex (s|t), lexical translation proba-
bilities for HD-HRs;
? Ptyhd-hr = exp (?1), rule penalty for HD-HRs;
? Pnrr (t|s), translation probability for NRRs;
? Ptynrr = exp (?1), rule penalty for NRRs;
? Plm (e), language model;
? Ptyword (e) = exp (?|e|), word penalty.
235
Algorithm 1: Decoding Algorithm
Input: Sentence f1n in the source language
Dependency structure of f1n
HD-HR rule set HDHR
NRR rule set NRR
Initial phrase length K
Output: Best derivation d?
1. set chart[i, j]=NIL (1 ? i ? j ? n);
2. for l from 1 to n do
3. for all i, j such that j ? i = l do
4. if l ? K do
5. for all derivations d derived from
HDHR spanning from i to j do
6. add d into chart[i, j]
7. for all derivations d derived from
NRR spanning from i to j do
8. add d into chart[i, j]
9. set d? as the top derivation of chart[1, n]
10.return d?
It is worth pointing out that we define translation
probabilities for NRRs only for the direction from
source language to target language, although trans-
lation probabilities for HD-HRs are defined for both
directions. This is mostly due to the fact that a NRR
excludes terminals and has only two options on the
target side (i.e., either X ? X1X2 or X ? X2X1).
4 Decoding
Our decoder is based on CKY-style chart parsing
with beam search. Given an input sentence f , it finds
a sentence e in the target language derived from the
best derivation d? among all possible derivations D:
d? = arg max
d?D
P (D) (2)
Algorithm 1 presents the decoding process. Given
a source sentence, it searches for the best deriva-
tion bottom-up. For a source span [i, j], it applies
both types of HD-HRs and NRRs. However, HD-
HRs are only applied to generate derivations span-
ning no more than K words ? the initial phrase
length limit used in training to extract HD-HRs ?
while NRRs are applied to derivations spanning any
length. Unlike in Chiang (2007), it is possible for
a non-terminal generated by a NRR to be included
afterwards by a HD-HR or another NRR. Similar to
Chiang (2007) in generating k-best derivations from
i to j, we make use of cube pruning (Huang and Chi-
ang, 2005) with an integrated language model for
each derivation.
5 Experiments
We evaluate the performance of our HD-HPB model
and compare it with our implementation of Chiang?s
HPB model (Chiang, 2007), a source-side SAMT-
style refined version of HPB (SAMT-HPB), and the
Moses implementation of HPB. For fair compari-
son, we adopt the same parameter settings for HD-
HPB, HPB and SAMT-HPB systems, including ini-
tial phrase length (as 10) in training, the maximum
number of non-terminals (as 2) in translation rules,
maximum number of non-terminals plus terminals
(as 5) on the source, prohibition of non-terminals
to be adjacent on the source, beam threshold ? (as
10?5) (to discard derivations with a score worse than
? times the best score in the same chart cell), beam
size b (as 200) (i.e. each chart cell contains at most
b derivations). For Moses HPB, we use ?grow-diag-
final-and? to obtain symmetric word alignments, 10
for the maximum phrase length, and the recom-
mended default values for all other parameters.
5.1 Experimental Settings
To examine the efficacy of our approach on training
datasets of different scales, we first train translation
models on a small-sized corpus, and then scale to a
larger one. We use the 2002 NIST MT evaluation
test data (878 sentence pairs) as the development
data, and the 2003, 2004, 2005, 2006-news NIST
MT evaluation test data (919, 1788, 1082, and 616
sentence pairs, respectively) as the test data. To find
heads, we parse the source sentences with the Berke-
ley Parser3 (Petrov and Klein, 2007) trained on Chi-
nese TreeBank 6.0 and use the Penn2Malt toolkit4
to obtain dependency structures.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2000) on the corpus in
both directions, applying ?grow-diag-final-and? re-
finement (Koehn et al, 2003). We use the SRI lan-
guage modeling toolkit to train a 5-gram language
model on the Xinhua portion of the Gigaword corpus
and standard MERT (Och, 2003) to tune the feature
3http://code.google.com/p/berkeleyparser/
4http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html/
236
weights on the development data.
For evaluation, the NIST BLEU script (version
12) with the default settings is used to calculate the
NIST and the BLEU scores, which measures case-
insensitive matching of n-grams with n up to 4. To
test whether a performance difference is statistically
significant, we conduct significance tests following
the paired bootstrap approach (Koehn, 2004). In this
paper, ?**? and ?*? denote p-values less than
0.01 and in-between [0.01, 0.05), respectively.
5.2 Results on Small Data
To test the HD-HPB models, we firstly carried out
experiments using the FBIS corpus as training data,
which contains ?240K sentence pairs. Table 2 lists
the rule table sizes. The full rule table size (includ-
ing HD-HRs and NRRs) of our HD-HPB model is
about 1.5 times that of Chiang?s, largely due to re-
fining the non-terminal symbolX in Chiang?s model
into head-informed ones in our model. It is also
unsurprising, that the test set-filtered rule table size
of our model is only about 0.8 times that of Chi-
ang?s: this is due to the fact that some of the re-
fined translation rule patterns required by the test
set are unattested in the training data. Furthermore,
the rule table size of NRRs is much smaller than
that of HD-HRs since a NRR contains only two
non-terminals. Table 3 lists the translation perfor-
mance with NIST and BLEU scores. Note that our
re-implementation of Chiang?s original HPB model
performs on a par with Moses HPB. Table 3 shows
that our HD-HPB model significantly outperforms
Chiang?s HPB model with an average improvement
of 1.32 in BLEU and 0.16 in NIST (and similar im-
provements over Moses HPB).
Although HD-HPB has small size of phrase ta-
bles compared to HPB, it still consumes more time
in decoding (e.g., 15.1 vs. 11.0), mostly due to the
flexible reordering of NRRs.
5.3 Results on Large Data
We also conduct experiments on larger training
data with ?1.5M sentence pairs from the LDC
dataset.5 Table 4 lists the rule table sizes and Ta-
ble 5 presents translation performance with NIST
5This dataset includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06
and BLEU scores. It shows that our HD-HPB model
consistently outperforms Chiang?s HPB model with
an average improvement of 1.91 in BLEU and 0.35
in NIST (similar for Moses HPB). Compared to the
improvement achieved on the small data, it is en-
couraging to see that our HD-HPB model benefits
more from larger training data with little adverse ef-
fect on decoding time which increases only slightly
from 15.1 to 16.6 seconds per sentence.
5.4 Comparison with SAMT-HPB
Comparing the performance of SAMT-HPB with
regular HPB in Table 3 and Table 5, it is interest-
ing to see that in general the SAMT-style approach
leads to a deterioration of translation performance
for the small training set (e.g., 30.09 for SAMT-HPB
vs. 30.64 for HPB) while it comes into its own for
the large training set (e.g., 33.54 for SAMT-HPB vs.
32.95 for HPB), indicating that the SAMT-style ap-
proach is more prone to data sparseness than HPB
(or, indeed, HD-HPB).
Comparing the performance of SAMT-HPB with
HD-HPB, shows that our head-driven non-terminal
refining approach consistently outperforms the
SAMT-style approach on an extensive set of ex-
periments (for each test set p < 0.01), indicating
that head information is more effective than (par-
tial) CFG categories. To make the comparison fair,
it is important to note that our implementation of
source-side SAMT-HPB includes the same sophis-
ticated non-terminal re-ordering NRR rules as HD-
HPB (Section 2.2 ). Thus the performance differ-
ences reported here are not due to different reorder-
ing capabilities, but to the discriminative impact of
the head information in HD-HPB over SAMT-style
annotation. Taking lianming zhichi in Figure 2 as an
example, HD-HPB labels the span VV, as lianming
is dominated by zhichi, effecively ignoring lianming
in the translation rule, while the SAMT label is
ADVP:AD+VV6 which is more susceptible to data
sparsity (Table 2 and Table 4). In addition, SAMT
resorts to X if a text span fails to satisify pre-defined
categories. Examining initial phrases extracted from
the SAMT training data shows that 28% of them are
labeled as X. Finally, for Chinese syntactic analy-
6The constituency structure for lianming zhichi is (VP
(ADVP (AD lianming)) (VP (VV zhichi) ...)).
237
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 39.6M 2.8M 4.7M 3.3M 3.0M 3.4M
HD-HPB 59.5/0.6M 1.9/0.1M 3.4/0.2M 2.3/0.2M 2.0/0.1M 2.4/0.2M
SAMT-HPB 70.1/0.4M 2.2/0.2M 4.0/0.2M 2.7/0.2M 2.3/0.2M 2.8/0.2M
Table 2: Rule table sizes of different models trained on small data. Note: 1) SAMT-HPB indicates our HD-HPB model
with the non-terminal scheme of Zollmann and Venugopal (2006); 2) For HD-HPB and SAMT-HPB, the rule sizes
separated by / indicate HD-HRs and NRRs, respectively; 2) Except for ?Total Rules?, the figures correspond to rules
filtered on the corresponding test set.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.377 29.67 8.209 33.60 7.571 29.49 6.773 28.90 7.483 30.42 NA
HPB 8.137 29.75 9.050 34.06 8.264 30.09 7.788 28.64 8.310 30.64 11.0
HD-HPB 8.308 31.01** 9.211 35.11** 8.426 31.57** 7.930 30.15** 8.469 31.96 15.1
SAMT-HPB 7.886 29.14* 8.703 33.32** 7.961 29.49* 7.307 28.41 7.964 30.09 17.3
HD-HR+Glue 7.966 29.51 8.826 33.68 8.116 29.84 7.474 28.51 8.095 30.39 5.4
Table 3: NIST and BLEU (%) scores of different models trained on small data. Note: 1) HD-HR+Glue indicates our
HD-HPB model replacing NRRs with glue rules; 2) Significance tests for Moses HPB, HD-HPB, SAMT-HPB and
HD-HR+Glue are done against HPB.
System Total Rules MT 03 MT 04 MT 05 MT 06 Avg.
HPB 206.8M 11.3M 17.6M 12.9M 10.4M 13.0M
HD-HPB 318.6/2.3M 7.3/0.3M 12.2/0.4M 8.5/0.3M 6.7/0.2M 8.7/0.3M
SAMT-HPB 371.0/1.1M 8.6/0.3M 14.3/0.4M 10.1/0.3M 7.9/0.3M 10.2/0.3M
Table 4: Rule table sizes of different models trained on large data.
System
MT 03 MT 04 MT 05 MT 06 Avg.
Time
NIST BLEU NIST BLEU NIST BLEU NIST BLEU NIST BLEU
Moses HPB 7.914 32.94* 8.429 35.16 7.962 32.18 6.483 29.88* 7.697 32.54 NA
HPB 8.583 33.59 9.114 35.39 8.465 32.20 7.532 30.60 8.423 32.95 13.7
HD-HPB 8.885 35.50** 9.494 37.61** 8.871 34.56** 7.839 31.78** 8.772 34.86 16.6
SAMT-HPB 8.644 34.07 9.245 36.52** 8.618 32.90* 7.543 30.66 8.493 33.54 19.1
HD-HR+Glue 8.831 34.58** 9.435 36.55** 8.821 33.84** 7.863 31.06 8.737 34.01 6.7
Table 5: NIST and BLEU (%) scores of different models trained on large data. Note: System labels and significance
testing as in Table 3.
238
sis, dependency structure is more reliable than con-
stituency structure. Moreover, SAMT-HPB takes
more time in decoding than HD-HPB due to larger
phrase tables.
5.5 Discussion
5.5.1 Individual Contribution of HD-HRs and
NRRs
Examining translation output shows that on aver-
age each sentence employs 16.6/5.2 HD-HRs/NRRs
in our HD-HPB model, compared to 15.9/3.6 hier-
archical rules/glue rules in Chiang?s model, provid-
ing further indication of the importance of NRRs in
translation. In order to separate out the individual
contributions of the novel HD-HRs and NRRs, we
carry out an additional experiment (HD-HR+Glue)
using HD-HRs with monotonic glue rules only (ad-
justed to refined rule labels, but effectively switching
off the extra reordering power of full NRRs) both
on the small and the large datasets, with interest-
ing results: Table 3 (HD-HR+Glue) shows that for
the small training set most of the improvement of
our full HD-HPB model comes from the NRRs, as
RR+Glue performs on the same level as Chiang?s
original and Moses HPB (the differences are not
statistically significant), perhaps indicating sparse-
ness for the refined HD-HRs given the small train-
ing set. Table 5 shows that for the large training
set, HD-HRs come into their own: on average more
than half of the improvement over HPB (Chiang and
Moses) comes from the refined HD-HRs, the rest
from NRRs.
It is not surprising that compared to the others
HD-HR+Glue takes much less time in decoding.
This is due to the fact that 1) compared to HPB, the
refined translation rule patterns on the source side
have fewer entries in phrase table; 2) compared to
HD-HPB, HD-HR+Glue switches off the extra re-
ordering of NRRs. The decoding time for HD-HPB
and HD-HR+Glue suggests that NRRs are more than
doubling the time required to decode.
5.5.2 Different Head Label Sets
Examining initial phrases extracted from the large
size training data shows that there are 63K types
of refined non-terminals with respect to 33 types of
POS tags. Considering the sparseness in translation
rules caused by this comparatively detained POS tag
set, we carry out an experiment with a reduced set
of non-terminal types by using a less granular POS
tag set (C-HPB). Moreover, due to the fact that con-
catenation of POS tags of heads mostly captures in-
ternal structure of a text span, it is interesting to ex-
amine the effect of other syntactic labels, in partic-
ular dependency labels, to try to better capture the
impact of the external context on the text span. To
this end, we replace the POS tag of head with its
incoming dependency label (DL-HPB), or the com-
bination of (the original fine-grained) POS tag and
its dependency label (POS-DL-HPB). For C-HPB
we use the coarse POS tag set obtained by group-
ing the 33 types of Chinese POS tags into 11 types
following Xia (2000). For example, we generalize
all verbal tags (e.g., VA, VC, VE, and VV ) and all
nominal tags (e.g., NR, NT, and NN) into Verb and
Noun, respectively. We use the dependency labels
in Penn2Malt which defines 9 types of dependency
labels for Chinese, including AMOD, DEP, NMOD,
P, PMOD, ROOT, SBAR, VC, and VMOD.7
Table 6 shows the results trained on large data.
Although the number of non-terminal types de-
creased sharply from 63K to 3K, using the coarse
POS tag set in C-HPB surprisingly lowers the per-
formance with 1.1 BLEU scores on average (e.g.,
33.75 vs. 34.86), indicating that grouping POS
tags using simple linguistic rules is inappropriate for
HD-HPB. We still believe that this initial negative
finding should be supplemented by future work on
groupping POS tags using machine learning tech-
niques considering contextual information.
Table 6 also shows that replacing POS tags
of heads with their dependency labels (DL-HPB)
substantially lowers the average performance from
34.86 on BLEU score to 32.54, probably due to
the very coarse granularity of the dependency la-
bels used. In addition, replacing non-terminal label
with more refined tags (e.g., combination of original
POS tag and dependency label) also lowers trans-
lation performance (POS-DL-HPB). Further experi-
ments with more fine-grained dependency labels are
required.
7Some other types of dependency labels (e.g., SUB, OBJ)
are generated from function tags which are not available in our
automatic parse trees.
239
V
V-
N
R
1 d
u
i 
yi
 
N
N
2 
V
V
?
 
,
 
X
?
 
X
1 
X
2 
ag
ai
n
st
 
Ir
aq
 
(b)
 
zh
ic
hi
 
m
ei
gu
o
1 
du
i y
i c
el
ie
2,
 
 
su
pp
o
rt
 
A
m
er
ic
a?
s 1
 
st
an
d 2
 
ag
ai
n
st
 
Ir
aq
 
 
V
V-
N
R
?
 
zh
ic
hi
 
m
ei
gu
o
 ,
 
X
?
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
(a)
 
zh
ic
hi
 
m
ei
gu
o
,
 
su
pp
o
rt
 
A
m
er
ic
a?
s 
Figure 3: Examples of pharse pairs and their head-driven
translation rules with dependency relation, regarding Fig-
ure 2
System MT 03 MT 04 MT 05 MT 06 Avg.
HPB 33.59 35.39 32.20 30.60 32.95
HD-HPB 35.50 37.61 34.56 31.78 34.86
C-HPB 34.10 36.43 33.46 31.00 33.75
DL-HPB 32.81 35.19 32.27 29.89 32.54
POS-DL-HPB 34.08 36.78 33.14 30.43 33.61
HD-DEP-HPB 35.48 38.17 34.81 32.38 35.21
Table 6: BLEU (%) scores of models trained on large
data.
5.5.3 Encoding Full Dependency Relations in
Translation Rule
Xie et al (2011) present a dependency-to-string
translation model with a complete dependency struc-
ture on the source side and a moderate average im-
provement of 0.46 BLEU over the HPB baseline. By
contrast, in our HD-HPB approach, dependency in-
formation is used to identify heads in the strings cov-
ered by non-terminals in HD-HR rules, and to refine
non-terminal labels accordingly, with an average im-
provement of 1.91 in BLEU over the HPB baseline
(when trained on the large data). This raises the
question whether and to what extent complete (un-
labeled) dependency information between the string
and the heads in head-labeled non-terminal parts of
the source side of SCFGs in HD-HPB can further
improve results.
Given the source side of a translation rule (ei-
ther HD-HR or NRR), say Ps ? s1 . . . sm (where
each si is either a terminal or a head POS in a re-
fined non-terminal), in a further set of experiments
we keep the full unlabeled dependency relations be-
tween s1 . . . sm so as to capture contextual syntactic
information in translation rules. For example, on the
source side of Figure 3 (b) where VV-NR maps into
words zhichi and meiguo while NN maps into word
celie, we keep the full unlabeled dependency rela-
tions among words {zhichi, meiguo, dui, yi, celie}.
HD-DEP-HPB (Table 6) augments translation rules
in HD-HPB with full dependency relations on the
source side. This further boosts the performance
by 0.35 BLEU scores on average over HD-HPB and
outperforms the HPB baseline by 2.26 BLEU scores
on average.
5.5.4 Error Analysis
We carried out a manual error analysis compar-
ing the outputs of our HD-HPB system with those
of Chiang?s (both trained on the large data). We ob-
serve that improved BLEU score often correspond to
better topological ordering of phrases in the hierar-
chical structure of the source side, with a direct im-
pact on which words in a source sentence should be
translated first, and which later. As ungrammatical
translations are often due to inappropriate topologi-
cal orderings of phrases in the hierarchical structure,
guiding the translation through appropriate topolog-
ical ordering should improve translation quality. To
give an example, consider the following input sen-
tence from the 04 NIST MT test data and its two
translation results:
? Input: ??0 ??1 ?2 ?3 ??4 ????5 ?
?6 ?7 ??8 ??9
? HPB: chinese delegation to us dollar purchase of
more high technology equipment
? HD-HPB: chinese delegation went to the united
states to buy more us high - tech equipment
Figure 4 demonstrates the topological orderings
in the two hierarchical structures. In addition to dis-
fluency and some grammar errors (e.g., a main verb
is missing), the basic HPB system also makes mis-
takes in reordering (e.g., ??4 ????5 ??6
translated as dollar purchase of more). The poor
translation quality, unsurprisingly, is caused by in-
appropriate topological ordering (Figure 4(a)). By
comparison, the topological ordering reflected in the
hierarchical structure of our HD-HPB model bet-
ter respects syntactic structure (Figure 4(b)). Let
240
??
?
0?
??
?
1?
?? 2?
?? 3?
??
?
4?
??
??
?
5?
??
?
6?
?? 7?
??
?
8?
??
?
9?
X [4
?4]
?
X [6
?6]
?
X [4
?6]
?
X [3
?7]
?
X [3
?8]
?
X [2
?9]
?
X [1
?9]
?
X [0
?9]
?
S [0
?9]
?
(a)
.?T
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?C
hia
ng
?s?
HP
B.?
(b
).?I
mp
ro
ve
d?t
op
olo
gic
al?
or
de
rin
gs
?of
?ph
ra
se
s?i
n?H
D?
HP
B.
1.?
S [0
?9]
??
?X [
0?9
],??
????
????
????
????
??X
[0?
9]
?
2.?
X [0
?9]
??
???
[0?
0]
?X [
1?9
],??
????
????
????
????
???c
hin
es
e?X
[1?
9]?
3.?
X [1
?9]
??
???
[1?
1]
?X [
2?9
],??
????
????
????
????
??d
ele
ga
tio
n?X
[2
?9
]?
4.?
X [2
?9]
??
?? [
2?2
]?X
[3
?8]
???
[9?
9],?
?
????
????
????
????
??to
?X [
3?8
]?e
qu
ipm
en
t?
5.?
X [3
?8]
??
?X [
3?7
]??
?,?
?
X [3
?7]
?te
ch
no
log
y?
6.?
X [3
?7]
??
?? [
3?3
]?X
[4
?6]
?? [
7?
7],?
?
us
?X [
4?6
]?h
igh
?
7.?
X [4
?6]
??
?X [
4?4
]??
??
? [5
?5]
?X [
6?6
],?
X [6
?6]
?X [
4?
4]?o
f?m
or
e?
8.?
X [4
?4]
??
???
[4?
4]
,??
pu
rch
as
e?
9.?
X [6
?6]
??
???
[6?
6]
,??
do
lla
r
1. ?
VV
[0
?9]
??
?N
N [
0?1
]?V
V [2
?9
],??
????
????
????
?X?
?
?X [
0?
1]?X
[2
?9]
?
2.?
NN
[0?
1]
??
???
[0?
0]?N
N [
1?
1]
,??
????
????
????
??X
??
?ch
ine
se
?X [
1?
1]??
3.?
NN
[1?
1]
??
???
[1
?1]
,??
????
????
????
??X
??
?de
leg
ati
on
?
4.?
VV
[2
?9]
??
?? [
2?2
]??
[3?
3]?
VV
[4?
9]
,???
????
?X?
?
?w
en
t?t
o?t
he
?un
ite
d?s
tat
es
?to
?X [
4?
9]?
5.?
VV
[4
?9]
??
?VV
?M
[4
?6
]??
[7?
7]
???
[8
?8]
?N
N [
9?9
],?
????
????
????
X??
?X [
4?6
]?h
igh
??t
ec
h?X
[9?
9]??
6.?
VV
?M
[4
?6]
??
???
[4
?4]
?M
[5?
6]
,?
????
????
????
????
??X
??
?bu
y?X
[5?
6]
??
7.?
M
[5
?6]
??
?CD
[5?
5]?M
[6
?6]
,??
????
????
????
X??
?X [
5?5
]?X
[6
?6]
?
8.?
CD
[5
?5]
??
???
??
[5
?5]
,??
????
????
????
?X?
?
?m
or
e?
9.?
M
[6
?6]
??
???
[6?
6]
,??
????
????
????
X??
?us
?
10
.?N
N [
9?9
]??
???
[9
?9]
,??
????
????
????
?X?
?
?eq
uip
me
nt
?
ro
ot
?? NR 0 
?? NN 1 
? VV 2
? NR 3
??
 
VV 4 
??
?? CD 5 
?? M 6 
? JJ 7
?? NN 8 
?? NN 9
CD
[5-5
]
M
[6-6
]
M
[5-6
]
VV-M
[4-6
]
VV
[4-9
]
VV
[2-9
] 
NN
 
VV
[0-9
]
[0-1
]
NN
NN
[1-1
]
[9-9
]
Figure 4: An example Chinese sentence and its two hierarchical structures. Note: subscript [i-j] represents spanning
from word i to word j on the source side.
us refer to the HD-HPB hierarchical structure on
the source side as translation parse tree and to the
treebank-based parser derived tree as syntactic parse
tree from which we obtain unlabeled dependency
structure. Examining the translation parse trees of
our HD-HPB model shows that phrases with 1/2/3/4
heads account for 64.9%/23.1%/8.8%/3.2%, respec-
tively. Compared to 37.9% of the phrases in the
translation parse trees of the HPB model, 43.2% of
the phrases of our HD-HPB model correspond to a
linguistically motivated constituent in the syntactic
parse tree with exactly the same text span. In sum,
therefore, instead of simply enforcing hard linguistic
constraints imposed by a full syntactic parse struc-
ture, our model opts for a successful mix of linguis-
tically motivated and combinatorial (matching sub-
phrases in HPB) constraints.
6 Conclusion
In this paper, we present a head-driven hierarchi-
cal phrase-based translation model, which adopts
head information (derived through unlabeled depen-
dency analysis) in the definition of non-terminals
to better differentiate among translation rules. In
addition, improved and better integrated reorder-
ing rules allow better reordering between consecu-
tive non-terminals through exploration of a larger
search space in the derivation. Our model main-
tains the strengths of Chiang?s HPB model while at
the same time it addresses the over-generation prob-
lem caused by using a uniform non-terminal symbol.
Experimental results on Chinese-English translation
across a wide range of training and test sets demon-
strate significant and consistent improvements of our
HD-HPB model over Chiang?s HPB model as well
as over a source side version of the SAMT-style
model.
Currently, we only consider head information in a
word sequence. In the future work, we will exploit
more syntactic and semantic information to system-
atically and automatically define the inventory of
non-terminals (in source and target). For example,
for a non-terminal symbol VV, we believe it will
benefit translation if we use fine-grained dependency
labels (subject, object etc.) used to link it to its gov-
erning head elsewhere in the translation rule.
Acknowledgments
This work was supported by Science Foundation Ire-
land (Grant No. 07/CE/I1142) as part of the Cen-
tre for Next Generation Localisation (www.cngl.ie)
at Dublin City University. It was also partially
supported by Project 90920004 under the National
Natural Science Foundation of China and Project
2012AA011102 under the ?863? National High-
Tech Research and Development of China. We
thank the reviewers for their insightful comments.
References
Hala Almaghout, Jie Jiang, and Andy Way. 2011. CCG
contextual labels in hierarchical phrase-based SMT. In
Proceedings of EAMT 2011, pages 281?288.
241
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of ACL 2007, pages
33?40.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL 2000, pages 132?
139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguis-
tics, 29(4):589?637.
Yang Gao, Philipp Koehn, and Alexandra Birch. 2011.
Soft dependency constraints for reordering in hierar-
chical phrase-based translation. In Proceedings of
EMNLP 2011, pages 857?868.
Zhongjun He, Yao Meng, and Hao Yu. 2010. Maxi-
mum entropy based phrase reordering for hierarchical
phrase-based translation. In Proceedings of EMNLP
2010, pages 555?563.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT 2005, pages 53?64.
Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.
2010. Soft syntactic constraints for hierarchical
phrase-based translation using latent syntactic distri-
butions. In Proceedings of EMNLP 2010, pages 138?
147.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL 2003, pages 48?54.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388?395.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-HLT 2008, pages 1003?1011.
Markos Mylonakis and Khalil Sima?an. 2011. Learning
hierarchical translation structure with linguistic anno-
tations. In Proceedings of ACL-HLT 2011, pages 642?
652.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of ACL
2000, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of NAACL
2007, pages 404?411.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proceedings of EMNLP 2009, pages
72?80.
Dekai Wu. 1996. A polynomial-time algorithm for sta-
tistical machine translation. In Proceedings of ACL
1996, pages 152?158.
Fei Xia. 2000. The part-of-speech tagging guidelines for
the Penn Chinese Treebank (3.0). Technical Report
IRCS-00-07, University of Pennsylvania Institute for
Research in Cognitive Science Technical.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of EMNLP 2011, pages
216?226.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of NAACL 2006 - Workshop on Statistical
Machine Translation, pages 138?141.
Andreas Zollmann and Stephan Vogel. 2011. A word-
class approach to labeling PSCFG rules for machine
translation. In Proceedings of ACL-HLT 2011, pages
1?11.
242
