Robust Segmentation of Japanese Text into a Lattice for Parsing 
Gary Kaemarcik, Chris Brockett, Hisami Suzuki 
M icrosofl Research 
One Microsoft Way 
Redmond WA, 98052 USA 
{ garykac,chrisbkt, hisamis }@in i croso ft.com 
Abstract 
We describe a segmentation component that 
utilizes minimal syntactic knowledge to produce a
lattice of word candidates for a broad coverage 
Japanese NL parser. The segmenter is a finite 
state morphological nalyzer and text normalizer 
designed to handle the orthographic variations 
characteristic of written Japanese, including 
alternate spellings, script variation, vowel 
extensions and word-internal parenthetical 
material. This architecture differs from con- 
ventional Japanese wordbreakers in that it does 
not attempt to simultaneously attack the problems 
of identifying segmentation candidates and 
choosing the most probable analysis. To minimize 
duplication of effort between components and to 
give the segmenter greater fi'eedom to address 
orthography issues, the task of choosing the best 
analysis is handled by the parser, which has access 
to a much richer set of linguistic information. By 
maximizing recall in the segmenter and allowing a 
precision of 34.7%, our parser currently achieves a
breaking accuracy of ~97% over a wide variety of 
corpora. 
Introduction 
The task of segmenting Japanese text into word 
units (or other units such as bunsetsu (phrases)) 
has been discussed at great length in Japanese NL 
literature (\[Kurohashi98\], \[Fuchi98\], \[Nagata94\], 
et al). Japanese does not typically have spaces 
between words, which means that a parser must 
first have the input string broken into usable units 
before it can analyze a sentence. Moreover, a 
variety of issues complicate this operation, most 
notably that potential word candidate records may 
overlap (causing ambiguities for the parser) or 
there may be gaps where no suitable record is 
found (causing a broken span). 
These difficulties are commonly addressed using 
either heuristics or statistical methods to create a 
model for identifying the best (or n-best) sequence 
of records for a given input string. This is 
typically done using a connective-cost model 
(\[Hisamitsu90\]), which is either maintained 
laboriously by hand, or trained on large corpora. 
Both of these approaches uffer fiom problems. 
Handcrafted heuristics may become a maintenance 
quagmire, and as \[Kurohashi98\] suggests in his 
discussion of the JUMAN scgmenter, statistical 
models may become increasingly fi'agile as the 
system grows and eventually reach a point where 
side effects rule out fiwther improvements. The 
sparse data problem commonly encountered in 
statistical methods is exacerbated in Japanese by 
widespread orthographic variation (see ?3). 
Our system addresses these pitfalls by assigning 
completely separate roles to the segmeuter and the 
parser to allow each to delve deeper into the 
complexities inherent in its tasks. 
Other NL systems (\[Kitani93\], \[Ktu'ohashi98\]) 
have separated the segmentation and parsing 
components. However, these dual-level systems 
are prone to duplication of effort since mauy 
segmentation ambiguities cannot be resolved 
without invoking higher-level syntactic or 
semantic knowledge. Our system avoids this 
duplication by relaxing the requirement that the 
segmenter identify the best path (or even n-best 
paths) through the lattice of possible records. The 
segmenter is responsible only for ensuring that a 
correct set of records is present in its output. It is 
the filnction of the parsing component to select he 
best analysis from this lattice. With tiffs model, 
our system achieves roughly 97% recall/precision 
(see \[Suzuki00\] for more details). 
1 System Overview 
Figure shows a simple block diagram of our 
Natural Language Understanding system for 
Japanese, the goal of which is to robustly produce 
syntactic and logical forms that allow automatic 
390 
Word Segmentation 
l{ 
\[ Dcrivational .,\sscmhl~ I 
Syntactic Analysis \] 
\[ \[,ogical Form \] 
,0 ( )rthograph.v 
l.exicon 
Syntax 
I.cxicon 
% 
Figure 1: Block diagram of Japanese NL system 
extraction of semantic relationships (see 
\[Richardson98\]) and support other lirlguistic 
projects like information retrieval, NL interfaces 
and dialog systems, auto-.summarization and 
machine translation. 
The segmenter is the frst level of' processing. This 
is a finite-state morphological nalyzer esponsible 
for generating all possible word candidates into a 
word lattice. It has a custom lexicon (auto: 
matically derived from the main lexicon to ensure 
consistency) that is designed to facilitate the 
identification of orfllographic variants. 
Records representing words and morphemes are 
handed off by the segmenter to the derivational 
assembly component, which uses syntax-like rules 
to generate additional derived forms that are then 
used by the parser to create syntax trees and logical 
forms. Many of the techniques here are similar to 
what we use in our Chinese NI., system (see 
\[Wu98\] for more details). 
The parser (described exterisively in \[Jensen93\]) 
generates syntactic representatioris arm logical 
forms. This is a bottomoup chart parser with 
binary rnles within the Augnmnted Phrase 
Structure Grammar formalism. The grammar rules 
are language--specific while the core engine is 
shared among 7 languages (Chinese, Japanese, 
Korean, English, French, German, Spanish). The 
Japanese parser is described in \[Suzuki00\]. 
2 Recall vs? Precision 
In this architecture, data is fed forward from one 
COlnponent to the next; hence, it is crucial that the 
base components (like the segmenter) generate a
minimal number of omission errors. 
Since segmentation errors may affect subsequent 
components, it is convenient to divide these errors 
into two types: recoverable and non-recoverable. 
A ram-recoverable error is one that prevents the 
syntax (or any downstream) component from 
arriving at a correct analysis (e.g., a missing 
record). A recoverable rror is one that does not 
interfere with the operation of following 
components. An example of the latter is the 
inchision of an extra record. This extra record 
does not (theoretically) prevent the parser from 
doing its lob (although in practice it may since it 
eonsun les  resot l rces) .  
Using standard definitions of recall (R) and 
precision (P): 
*~ Jr R - Seg~,,,.,.,.,., p = Seg~,,,.,.~.~., 
7bg,,,,,/ &g,,,,,,i 
where Segcor~ec t and .<,egmxal are the number q/" "'cotwect" 
and total number o/'segments returned by the segmentet; 
and "\['agto~a I is the total Jlttmber of "correct" segments 
fi'om a tagged corpus, 
we can see that recall measures non-recoverable 
errors and precision measures recoverable rrors. 
Since our goal is to create a robust NL system, it 
behooves us to maximize recall (i.e., make very 
few non-recoverable errors) in open text while 
keeping precision high enough that the extra 
records (recoverable errors) do not interfere with 
the parsing component. 
Achieving near-100% recall might initially seem to 
be a relatively straightforward task given a 
sufficiently large lexicon - simply return every 
possible record that is found in the input string, in 
practice, tile mixture of scripts and flexible 
orthography rules of Japanese (in addition to the 
inevitable non-lexicalized words) make the task of 
identifying potential lexical boundaries an 
interesting problem in its own right. 
3 Japanese Orthographic Variation 
Over tile centuries, Japanese has evolved a 
complex writing system that gives tile writer a 
great deal of flexibility when composing text. 
Four scripts are in common use (kanji, hiragana, 
katakana and roman), and can co-occur within 
lexical entries (as shown ill Table 1). 
Some mixed-script entries could be handled as 
syntactic ompounds, for example, ID ~a---1-" /at 
dii kaado="ID card'7 could be derived fl'om 
1DNotJN + 79-- I ~ NOUN. tlowever, many such items 
are preferably treated as lexical entries because 
391 
i!i \[~ ~ ' \[atarashii ,: "'new "\] 
Kanji-I l iragana I~LII~J \[ho~(vtnn'ui = "'mammal"\] 
~ 7/" "7 :./\[haburashi -~ "'toothbrush "\] 
. . . . . . . . . . . . . . . . . . .  . . . . . . . . .  
K'}n.!i-<~!P!\]!! ...................... E ( !S  ' ! t ( !Z . /~( :GS tm! ' i  t? ::c:(';S's:vstet!* :'1 ........ 
12 ) J \[lmtmL, atsu - "December"/ 
Kallji-Synlbol v ,{'~ \ [gcmma sen = "'~amma rays "\] 
.i-3 1" 4" t/ \[otoim - "'toilet "'\] 
Mixed kana 
............................. \[ -/")3 hl~?!,,!! ? .;;(9 c?f~,,,~/5' ;7/ ........... 
II3 )-~ - -  b" \[aidtt kaado = "lP card"\] 
Kana-Alpha ..t .y-t~ .>"-5;-V - -RNA \[messe~gaa RA'A = 
................................................................. T ie~t~:s~'~lxe~ ~U.:I. '7 .................................. 
7, J- U 2/-)'- ~) 2, 90 \[suloronchiumu 90 - 
"'Strontiunt 90 "\] 
Kana-Symbol I~ 2_ 7~ 40 ? \[hoeru yonjuu do = "'roaring 
............................................................................ fo t : ! !~, , ; .7  ......................................................... 
~i~ b ~" ~, \[keshigomu - "'eraser "1 
a >,I- 21" ~ e/ ~) ~: \[artiSt kentauri set : 
Other mixed 
"'Alpha Centauri "\] 
\[. ~: ~ \[togaki = "'stage directions"\] 
Table 1: Mixed-script lexical entries 
they have non-compositional syntactic or semantic 
attributes. 
In addition, many Japanese verbs and adjectives 
(and words derived from them) have a variety of 
accepted spellings associated with okurigana, 
optional characters representing inflectional 
endings. For example, the present ense of e)j b ~;?; 
~- (kiriotosu = "to prune ") can be written as any 
of: ~)Ji:~~J --, ~;)J b .."?,:t, ?:JJTf; & ,~, ~JJb.~s&~-, ~ ~:; ~s ~ I 
or even ~ ~) ~'g ~ 4-, -~ 9 7~;-~-. 
Matters become even more complex when one 
script is substituted for another at the word or sub- 
word level. This can occur for a var iety o f  
reasons: to replace a rare or d i f f i cu l t  ka@ (.~?~ 
\[rachi= "kMnap"\] instead of f,):~); to highlight a 
word in a sentence ( ~ >" t2 -h, o ~_ *) \[henna 
kakkou = "strange appearance '7); or to indicate a 
particular, often technical, sense (7 Y o -c \[watatte 
="crossing over"\] instead of iA~o-c, to emphasize 
the domain-specific sense of "connecting 2 
groups" in Go literature). 
More colloquial writing allows for a variety of 
contracted tbrms like ~ t~j~.\-~  ~ t~ + !=t \[ore~ 
tacha = ore-tachi + wa = "we'" + TOPIC\] and 
phonological inutations as in ~d.~:--~- = -d'4 \ [dee- 
su ~ desu = "is "\]. 
This is only a sampling of the orthographic issues 
present in Japanese. Many of these variations pose 
serious sparse-data problems, and lexicalization of 
all variants is clearly out of the questioi1. 
II.\]'~., ~lJ < . .~  lt,l~L~l~;'~q ?gJ \[H/ikc,kkoku "'every 
repeat moment "'\] 
characters Ill ~ ~., e L I~, .~ HI :!:~ til-!~ U ~ ' \[kaigaishu 
"'diligent "'\] 
distribution of  t: " '>" v~- ~ t:":#v\]- \[huleo "vMeo"\] voicina nl,qrks 
halt\vidth & 
lhllwidth 
composite 
symbols 
F M b2J~" ~ FM )/ZJ~ \[I"M housml ~ "FM 
broadcast "'\] 
;~ (i'?" ~U, "-~ 5" 4, "V <e" :>, 2, \[daivaguramu : 
"'diagram 'J 
;~; "-~ . . . . .  L" 2/ 1- \[paasento =: "percent :;\] 
r tz  , .  . . /~ ,  . 
"'incorporated 'i\] 
N\] ~ 2 8 FI \[n!jmthactH niciu = "28 'j' day of the 
month "\] 
Table 2: Character type normalizations 
4 Segmenter Design 
Given the broad long-term goals for' the overall 
system, we address the issues of recall/precision 
and orthographic variation by narrowly defining 
the responsibilities of the segmenter as: 
(i) Maximize recall 
(2) Normalize word variants 
4.1 Maxinf fze Recall  
Maximal recall is imperative, Any recall mistake 
lnade in the segmenter prevents the parser from 
reaching a successful analysis. Since the parser in 
our NL system is designed to handle ambiguous 
input in the fbrm of a word lattice of potentially 
overlapping records, we can accept lower precision 
if that is what is necessary to achieve high recall? 
Conversely, high precision is specifically not a 
goal for the segmenter. While desirable, high 
precision may be at odds with the primary goal of 
maximizing recall. Note that the lower bound for 
precision is constrained by the lexicon. 
4?2 Normal ize word variants 
Given tile extensive amount of orthographic 
variability present in Japanese, some form of 
normalization into a canonical form is a pre- 
requisite for any higher-.order linguistic processing. 
The segmenter performs two basic kinds of 
nomlalization: \[,emmatization f inflected forms 
and Orthographic Norlnalization. 
392 
,~kur ieana . . . .  n),: g.). z~ -+ i,J,: ~- ~),~ :~ \[lhkmuk~ :: "drafty"/  ;5'./~ <% J)-tJ- ; '3_ \[11i1:i:;5,.,7~-\]\[ ~", ,. ,,,,.. ~ ' >\]-~J-'"o kammwaseru = 7o  
. . . . . . . . . . . . . . . . . . .  ~ !o{ ~ c!i ~s /./!,a!!{,6! : ::e ,!:e!l?!el<, 71 . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . .  engage (gear.w i' 
a ml l .~ ' l lmor i  - "'~111 non-s tandard  tA'ct) ::~ "~ ~-0~ \]'- \[onnanoko :: "gir l"/  .;.'/. <) ~, l) \[ ~u:<;'~-\]\[ ~ [:~-). {,. ~') \] estimate "' 
script +~:t , - t  - "+ 0 :4  7, :~ \[d,s'uko : "d i sco" /  I I )  ) :2  I" \ [ l i ' , ;4 -  l l l \ ]) :<fi4 - - i :DZ i D: a/ih, kaado2:" lDc iml  '" 
? D, I\] "+ " "~ Jl \[tkka,4etsu :: "one month" /  . . . . . . . . . . . . . . . .  
9:0  var iants  kagi tahako "'smf/'f --I, I!:~ " \['~\] \[ktl.~'uml~aseki = "Kasunli~aseki "\] 
numera ls  fi~r 5 i\[i~ .+ ti\]!i~ \[gorm : "'Olympics "\] 
kalt j i  1 ),, ~ ")v \[hitori : "one person"/  
:t.; i Z - -  ~ ,t~, k.  ~ -~J { 2 b ' ~5 /~ \[oniisan = "'older 
vovcel 
extens ions  brother "'\] 
-7 -,- 4' \]" 4 "+ 7 -," 4" I" \[/'aito 'ftght "'\] 
'Y - /  4- ~- 9 "../-i~ i<.( ~- 9 "-./\[batorm - "'vloti~F7 
ka lakana  
-)" 9 :E- - -  Iv ..+ 9 ~ . O ? :E- --" F' \ [aramoode "'h la v ari all Is too& "'\] 
IPL~(!rt)2_ ~ + ll~),t 2. ".'~ \[haeru = "'to shine "7 
in l ine yomi  ~ ~_lt(f:l: ~o ~I> ") )\]~(i "+l/'~ ~t JL?\[ \[hachuurui 
D,3 '/, < :~' II~';L D ', ~ II:t:~'i 0:: 'J "< -~ I toDac~'o" 
/g;'~; V, 1-{{:/~ D;\]\[!i,~:l '  '1 na~ai "'a long visit" 
Table 4: Orthography lattices 
lexical entry. Examples are given in "fable 3. Two 
cases of special interest are okurigana and inline 
yomi/kanji normalizations. The okurigana 
normalization expands shortened forms into fully 
specified forms (i.e., fornls with all optional 
................................. 7"~'!~t!?C'\] ..................................................................................... characters present). Tile yomi/kanji handling takes 
in l ine kanj i  l Yo (N)~: i i l2d  -'~ tgs ~ft~ \[~essh~rul =: "'rodent"/ 
Table 3: Script normalizations 
4.Z 1 Lemmatization 
LEMMATIZATION iri Japanese is the same as that 
for any language with inflected forms - a lemma, 
or dictionary form, is returned along with the 
inflection attributes. So, a form like ~:~-7~ \[tabeta 
= "ate "J would retum a lemma of f,~ ~<~; \[taberu = 
"'eat"\] along with a PAST attribute. 
Contracted forms are expanded and lemmatized 
individually, so that f,~ ~<-<o ~:~ .> o ?= \[tabe~ 
tecchatta = "has ectten and gone'7 is returned as: 
f~  ~-Z. 7 0 G\[-RUND -F (, x < GERUND -F L. +E ") PASr  
\ [taberu: "eat" + iku--++go" F s \ ]T imaz l=. . iSpE(7\ [ . ' \ ] .  
4..2.2 Orthographic Normalizatio,, 
ORTIIOGRAPttlC NORMALIZATION smoothes out 
orthographic variations o that words are returned 
in a standardized form. This facilitates lexical 
lookup and allows tile system to map the variant 
representations to a single lexicon entry. 
We distinguish two classes of orqthographic 
normalization: character type normalization and 
script normalization. 
CI IARAC' IER  TYPE  NORMAI . IZAT ION takes tile 
various representations allowed by the Unicode 
specification and converts them into a single 
consistent form. Table 2 summarizes this class of 
normalization. 
SCR. I I ' T  NORMAI,IZAI'ION rewrites the word so that 
it conforms to tile script and :~pelling used in the 
infixed parenthetical material and normalizes it out 
(after using the parenthetical infommtion to verify 
segmentation accuracy). 
5 Lexicon Structures 
Several special lexicon structures were developed 
to support hese features. Tile most significant is 
an orthography lattice* that concisely encapsulates 
all orthographic variants for each lexicon entry and 
implicitly specifies the normalized form. This has 
the advantage of compactness and facilitates 
lexicon maintenance since lexicographic inform- 
ation is stored in one location. 
The orthography lattice stores kana inforrnation 
about each kanji or group of kanji in a word. For 
example, the lattice far the verb Y~:-<~D \[taberu = 
"eat'7 is \[~:#_\]-<~, because the first character 
(ta) can be written as either kanji 5~ or kana 1=. A 
richer lattice is needed for entries with okurigana 
variants~ like LJJ 0 i'~:~ 4 \[kiriotosu = "'to prune "\] 
cited earlier: commas separate each okurigana 
grouping. The lattice for kiriotosu is \[OJ:~, 0 \]\[i"#: 
~,  E \]~j-. Table 4 contains more lattice examples. 
Enabling all possible variants can proliferate 
records and confiise the analyzer (see \[Kurohashi 
94\]). We therefore suppress pathological variants 
that cause confusion with more common words 
and constructions. For example, f:L-t,q- \[n~gai = "a 
long visit'7 never occurs as I.~ ~' since this is 
ambiguous with the highly fi'equent adjective ~-~v, 
/nasal - "l<mg'7. Likewise, a word like !t 
' Not to be confiised with the word lattice, which is the 
set of records passed fi'om the segmenter tothe parser. 
393 
\[nihon = ",Aq)an "7 is constrained to inhibit invalid 
variants like 124< which cause confusion with: {c 
I'OSl' + # NOUN \ [ t I i : : I ' . - tRT IC I . I : "  + /1on  = "book  " \ ] .  
We default to enabling all possible orthographies 
for each ennT and disable only those that are 
required. This saves US from having to update the 
lexicon whenever we encounter a novel 
orthographic variant since the lattice anticipates all 
possible variants. 
6 Unknown Words 
Unknown words pose a significant recall problem 
in languages that don't place spaces between 
words. The inability to identify a word in the input 
stream of characters can cause neighboring words 
to be misidentified. 
We have divided this problem space into six 
categories: variants of lexical entries (e.g., 
okurigana variations, vowel extensions, et al); 
non-lexiealized proper nouns; derived forms; 
foreign Ioanwords; mimetics; and typographical 
errors. This allows us to devise focused heuristics 
to attack each class of unfound words. 
The first category, variants of lexical entries, has 
been addressed through the script normalizations 
discussed earlier. 
Non-lexicalized proper nouns and derived words, 
which account for the vast majority of unfound 
words, are handled in the derivational assembly 
component. This is where compounds like -: ~ >i 
x ~':, ffuransugo = "French (language)"\] are 
assembled from their base components ;1 5~ J x 
\[furansu : "France "\] and at~ \[go = "language "J. 
Unknown foreign Ioanwords are identified by a 
simple maximal-katakana heuristic that returns the 
longest run of katakana characters. Despite its 
simplicity, this algorithm appears to work quite 
reliably when used in conjunction with the other 
mechanisms in our system. 
Mimetic words in Japanese tend to follow simple 
ABAB or ABCABC patterns in hiragana or 
katakana, so we look for these patterns and 
propose them as adverb records. 
The last category, typographical errors, remains 
mostly the subject for future work. Currently, we 
only address basic : (kanji) ~-~ -: (katakana) and 
i-, (hiragana) +~ : ' -  (katakana) substitutions. 
50% 
40% 
30% 
20% 
"10% 
0% 
15 25 35 45 55 65 75 85 95 105 115 
- - -~ Japanese  =-~t-=Chinese \]
? . . . . .  72 .27_~_z z zs?  27 ~ 7 ~Lz77 ~z ~25z ~ 2 7~ . . . . . . . .  
Figure 2: Worst-case segmenter precision (y-axis) versus 
sentence length (x-axis - in characters) 
7 Eva|uation 
Our goal is to improve the parser coverage by 
improving the recall in the segmenter. Evaluation 
of this component is appropriately conducted in the 
context of its impact on the entire system, 
Z 1 Parser Evaluation 
Running on top of our segmenter, our current 
parsing system reports ~71% coverage + (i.e, input 
strings for which a complete and acceptable 
sentential parse is obtained), and -,97% accuracy 
for POS labeled breaking accuracy? A full 
description of these results is given in \[Suzuki00\]. 
Z 2 Segmenter Evaluatkm 
Three criteria are relevant to segmenter per- 
formance: recall precision and speed. 
Z Z 1 Recall 
Analysis of a randonlly chosen set of tagged 
sentences gives a recall of 99.91%. This result is 
not surprising since maxindzing recall was a 
prinlary focus of our efforts. 
The breakdown of the recall errors is as follows: 
missing proper nouns = 47%, missing nouns = 
15%.. missing verbs/adjs = 15%, orthographic 
idiosyncrasies = 15%, archaic inflections = 8%. 
It is worth noting that for derived forms (those that 
Tested on a 15,000 sentence blind, balanced corpus. 
See \[SuzuldO0\] fordetails. 
394 
3000 \ [ i  
2000 I 
1 
<':> ,# ,~, ?> e <# ~,~, e e @,, ,+>,e,e 
Figure 3: Characters/second (y~axis) vs. sentence 
length (x-axis) for se~<ginenter alone (upper curve) 
and our NL system as a whole (lower curve) 
are tiandled in the derivational assembly corn-. 
ponent), tim segmenter is considered correct as 
long as it produces the necessary base records 
needed to build the derived fom-t. 
ZZ2 Precision 
Since we focused our effbrts on maximizing recall,, 
a valid concern is the impact of the extra records 
on the parser, that is, the effect of lower segmenter 
precision oll the system as a whole. 
Figure 2 shows the baselirie segrnenter precision 
plotted against sentence length using the 3888 
tagged sentences ~: For compaiison~ data for 
Chinese ~is included. These are baseline vahles in 
the sense they represent the riumber of records 
looked up in the lexicon without application of ariy 
heuristics to suppress invalid records. Thus, these 
mnnbers represent worst--case segmenter p ecision. 
The baseline precisior, for the Japariese segmenter 
averages 24.8%, whicl-i means that a parser would 
need to discard 3 records for each record it used in 
the final parse. TMs value stays fairly constant as 
the sentence length increases. The baseline 
precision for Chir, ese averages 37.1%. The 
disparity between the Japanese and Chinese worst- 
case scenario is believed to reflect the greater 
ambiguity inherent in the Japanese v<'riting system, 
owing to orthographic w~riation and the use of a 
syllabic script. 
++ The " <,<," o .~ t<%~,% was obtained by usin,,the results of the 
parser on untagged sentences. 
39112 sentences tagged in a sirnilar fashion using our 
Chinese NI,P system. 
100% 
70% "-:-: 5 ~ ::.::~::,~::-5.. ,'i ,': ~ -"r.'-~,'~7,:'s~'-,.: ~ :  .~ ~ ::,~ x;K< ~ 
50% 
40% ~ ~ - . ---..-~ 
30% ::::::::::::::::::::::::::::::::::::::::::::::::::::::: 
20% :::i:!)?i:~:~)}ii!:i\]::{i)~:,x::i!illii.:i!:'-.~!!~\]:!21{7-i\[.g{:!:'7:7:~::?. . . . . . . . . . . . . . . . . . . . . .  ,~< ............ 
10% !::::::ii'::::ii!i'ii{f!}{ii'.".'::iii::::ii 
0% ~'~S 2 
15 25 35 45 55 65 75 85 95 105 115 125 135 
\ [BSegmenter  \ [ \ ]Lex ica l  E IDer iv  BOther  El Parser  
Figure 4: Percentage oftime spent in each component (y- 
axis) vs. sentence l ngth x-axis) 
Using conservative pruning heuristics, we are able 
to bring the precision tip to 34.7% without 
affecting parser recall. Primarily, these heuristics 
work by suppressing the hiragana form of shork 
ambiguous words (like ~ \[ki="tree, air, .slJirit, 
season, record, yellow,... '7, which is normally 
written using kanji to identify the intended sense). 
Z2..3 Speed 
Another concern with lower precision values has to 
do with performance measured in terms of speed. 
Figure 3 summarizes characters-per.-second per- 
formance of the segmentation component and our 
NL system as a whole (irmluding the segmentation 
component). As expected, the system takes more 
time for longer senterlces. Crucially, however, the 
system slowdowri s shown to be roughly linear, 
Figure 4 shows how nluch time is spent in each 
component during sentence analysis. As the sen- 
tence length increases, lexical lookup+ derivational 
morphology and '+other" stay approximately con- 
starit while the percentage of time spent in the 
parsing component increases. 
Table 5 compares parse time performance for 
tagged and untagged sentences. This table 
qnantifies the potential speed improvement that the 
parser could realize if the segmenter precision was 
improved. Cohunn A provides baseline lexical 
lookup and parsing times based on untagged input. 
Note that segmenter time is not given this table 
because it would not be comparable to tile hypothetical 
segmenters devised for columns P, and C. 
395 
A 
Lexical processing 7.66 s 
Parsing 3.480 s
Other 4. 95 s 
Total 25.336 s 
Overall 
Percent Lexical 
Improvement I'arsing 
Other 
B c 
2.5 0 s 2.324 s
8.865 s 7. 79 s 
3.620 s 3.5 9 s 
4.995 s 3.022 s
40.82% 48.60% 
67.24% 69.66% 
34.24% 46.74% 
3.7 % 6. % 
Table 5: Summary of performance (speed) 
experiment where untagged input (A) is compared 
with space-broken i put (B) and space-broken i put 
with POS tags (C). 
Columns B and C give timings based on a 
(hypothetical) segmenter that correctly identifies 
all word botmdaries (B) and one that identifies all 
word boundaries and POS (C) 1'I". C represents the 
best-case parser performance since it assumes 
perfect precision and recall in the segmenter. The 
bottom portion of Table ,5 restates these 
improvements a percentages. 
This table suggests that adding conservative 
pruning to enhance segmenter precision may 
improve overall system performance. It also 
provides a metric for evaluating the impact of 
heuristic rule candidates. The parse-time 
improvemeuts from a rule candidate can be 
weighed against the cost of implementing this 
additional code to determine the overall benefit o 
the entire system. 
8 Future 
Planued near-term enhancements include adding 
context-sensitive h uristic rules to the segmenter as 
appropriate. In addition to the speed gains 
quantified in Table 5, these heuristics can also be 
expected to improve parser coverage by reducing 
resource requiremeuts. 
Other areas for improvement are unfotmd word 
models, particularly typographical error detection, 
and addressing the issue of probabilities as they 
apply to orthographic variants. Additionally, we 
are experimenting with various lexicon formats to 
more efficiently support Japanese. 
tt For the hypothetical segmenters, our segmenter was 
modified to return only the records consistent with a 
tagged input set. 
9 Conclusion 
The complexities involved in segmenting Japanese 
text make it beneficial to treat this task 
independently from parsing. These separate tasks 
are each simplified, thcilitating the processing of a 
wider range of phenomenon specific to their 
respective domains. The gains in robustness 
greatly outweigh the impact on parser performance 
caused by the additional records. Our parsing 
results demonstrate that this compartmentalized 
approach works well, with overall parse times 
increasing linearly with sentence length. 
10 References 
\[Fuchi98\] Fuchi,T., Takagi,S., "Japanese 
Morphological Analyzer using Word Co-occurrence", 
ACL/COLING 98, pp409-4 3, 998. 
\[Hisamitsu90\] Hisamitsu,T., Nitta, Y., 
Morphological Analyis by Minimum Connective-Cost 
Method", SIGNLC 90-8, IEICE pp 7-24, 990 (in 
Japanese). 
\[Jensen93\] Jensen,K., Heidorn,G., Richardson,S, 
(eds.) "Natural Language Processing: The PLNLP 
Approach", Kluwer, Boston, 993. 
\[Kitani93\] Kitani,T., Mitamura,T., "A Japanese 
Preprocessor for Syntactic and Semantic Parsing", 9 th 
Conference on AI in Applications, pp86-92, 993. 
\[Kurohashi94\] Kurohashi,S., Nakamura,Y., 
Matsumoto,Y., Nagao,M., "hnprovements of Japanese 
Morphological Analyzer JUMAN", SNLR, pp22-28, 
994. 
\[Kurohashi98\] Kurohashi,S., Nagao,M., "Building a 
Japanese Parsed Corpus while hnproving the Parsing 
System", First LREC Proceedings, pp7 9-724, 998. 
\[Nagata94\] Nagata,M., "A Stochastic Japanese 
Morphological Analyzer Using a Forward-DP 
Backward-A* N-Best Search Algorithm", COL1NG, 
pp20-207, 994. 
\[Richardson98\] Richardson,S.D., Dolan,W.B., 
Vanderwende,L., "MindNet: Acquiring and Structuring 
Semantic Information from Text", COLING/ACL 98, 
pp 098- 02, 998. 
\[Suzuki00\] Suzuki,H., Brockett,C., Kacmarcik,G., 
"Using a broad-coverage parser for word-breaking in 
Japanese", COLING 2000. 
\[Wu98\] Wu,A., Zixin,J., "Word Segmentation in 
Sentence Analysis", Microsoft Technical Report MSR- 
TR-99- 0, 999. 
396 
Using a Broad-Coverage Parser for Word-Breaking in Japanese 
Hisami Suzuki, Chris Brockett and Gary Kacmarcik 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
{ hisamis, chrisbkt, garykac }@ microsoft.corn 
Abstract 
We describe a method of word segmentation i
Japanese in which a broad-coverage parser selects 
the best word sequence while producing a syntactic 
analysis. This technique is substantially different 
from traditional statistics- or heuristics-based 
models which attempt o select the best word 
sequence before handing it to the syntactic 
component. By breaking up the task of finding the 
best word sequence into the identification of words 
(in the word-breaking component) and the selection 
of the best sequence (a by-product of parsing), we 
have been able to simplify the task of each 
component and achieve high accuracy over a wide 
varicty of data. Word-breaking accuracy of our 
system is currently around 97-98%. 
1. Introduction 
Word-breaking is an unavoidable and crucial first 
step toward sentence analysis in Japanese. In a 
sequential model of word-breaking and syntactic 
analysis without a feedback loop, the syntactic 
analyzer assumes that the results of word-breaking 
are correct, so for the parse to be successful, the 
input from the word-breaking component must 
include all words needed for a desired syntactic 
analysis. Previous approaches to Japanese word 
segmentation have relied on heuristics- or 
statistics-based models to find the single most 
likely sequence of words for a given string, which 
can then be passed to the syntactic omponent for 
further processing. The most common 
heuristics-based approach utilizes a connectivity 
matrix between parts-of-speech and word 
probabilities. The most likely analysis can be 
obtained by searching for the path with the 
minimum connective cost (Hisamitsu and Nitta 
1990), often supplemented by additional heuristic 
devices such as the longest-string-match or the 
least-number-of-bunsetsu (phrase). Despite its 
popularity, the connective cost method has a major 
disadvantage in that hand-tuning is not only 
labor-intensive but also unsafe, since adjusting the 
cost for one string may cause another to break. 
Various heuristic (e.g. Kurohashi and Nagao 1998) 
and statistical (e.g. Takeuchi and Matsumoto 1997) 
augmentations of the minimum connective cost 
method have been proposed, bringing 
segmentation accuracy up to around 98-99% (e.g. 
Kurohashi and Nagao 1998, Fuchi and Takagi 
1998). 
Fully stochastic language models (e.g. Nagata 
1994), on the other hand, do not allow such manual 
cost manipulation and precisely for that reason, 
improvements in segmentation accuracy are harder 
to achieve. Attaining a high accuracy using fully 
stochastic methods is particularly difficult for 
Japanese due to the prevalence of orthographic 
variants (a word can be spelled in many different 
ways by combining different character sets), which 
exacerbates the sparse data problem. As a result, 
the performance of stochastic models is usually not 
as good as the heuristics-based language models. 
The best accuracy reported for statistical methods 
to date is around 95% (e.g. Nagata 1994). 
Our approach contrasts with the previous 
approaches in that the word-breaking component 
itself does not perform the selection of the best 
segmentation analysis at all. Instead, the 
word-breaker returns all possible words that span 
the given string in a word lattice, and the best word 
sequence is determined by applying the syntactic 
rules for building parse trees. In other words, there 
is no task of selecting the best segmentation per se; 
the best word-breaking analysis is merely a 
concomitant of the best syntactic parse. We 
demonstrate hat a robust, broad-coverage parser 
can be implemented irectly on a word lattice 
input and can be used to resolve word-breaking 
ambiguities effectively without adverse 
performance effects. A similar model of 
word-breaking is reported for the problem of 
Chinese word segmentation (Wu and Jiang 1998), 
but the amount of ambiguity that exists in the word 
822 
lattice is nmch larger in Japanese, which requires a
different treatment. In the l'ollowing, we first 
describe the word-breaker and the parser in more 
detail (Section 2); we then report the results of 
segmentation accuracy (Section 3) and the results 
of related experinaents a sessing the effects of the 
segmentation ambiguities in the word lattice to 
parsing (Section 4). In Conclusion, we discuss 
implications for future research. 
2. Using a broad-coverage parser for 
word-breaking 
The word-breaking and syntactic components 
discussed in the current study are implelnented 
within a broad-coverage, multi-purpose natural 
hmguage understanding system being developed at 
Microsoft Research, whose ultimate goal is to 
achieve deep Selnantic understanding of natural 
language I. A detailed escription of the system is 
found in Heidom (in press). Though we focus on 
the word-breaking and syntactic components in 
this paper, the syntactic analysis is by no means 
the final goal of the system; rather, a parse tree is 
considered to be an approxilnate first step toward a 
more useful meaning representation. We also aim 
at being truly broad-coverage, i.e., returning useful 
analyses irrespective of the genre or the subject 
matter of the input text, be it a newspaper articlc or 
a piece of e-mail. For the proposed model of 
word-breaking to work well, the following 
properties of the parser are particularly important. 
? The bottom-up chart parser creates syntactic 
analyses by building incrementally arger phrases 
fl'om individual words and phrases (Jensen et al 
1993). The analyses that span the entire input 
string are the complete analyses, and the words 
used in that analysis constitutes the word-breaking 
analysis for the string. Incorrect words returned by 
the word-breaker are filtered out by the syntactic 
rules, and will not make it into the final complete 
parse. 
? All the grammar rules, written in the 
formalism of Augmented Phrase Structure 
Grammar (lteidorn 1975), are binary, a feature 
crucial for dealing with free word-order and 
i Japanese is one of the seven languages under 
development in our lab, along with Chinese, English, 
French, German, Korean and Spanish. 
missing constituents (Jensen 1987). Not only has 
the rule formalism proven to be indispensable for 
parsing a wide range of English texts, it is all tile 
more critical 1'o1 parsing Japanese, as the free 
word-order and missing constituents are the norm 
for Japanese sentences. 
? There is very little semantic dependency in the 
grammar rules, which is essential if the grammar is
to be domain-independent. However, the grammar 
rules are elaborately conditioned on morphological 
and syntactic l'eatums, enabling much finer-grained 
parsing analyses than just relying on a small 
number of basic parts-of speech (POS). This gives 
the grammar the power to disambiguate multiple 
word analyses in the input lattice. 
13ecause we do not utilize semantic information, 
we perforln no selnantically motivated attachlnent 
of phrases during parsing. Instead, we parse them 
into a default analysis, which can then be expanded 
and disambiguatcd at later stages of processing 
using a large semantic knowledge base 
(Richardson 1997, Richardson et al 1998). One of 
the goals o1' Ihis paper is to show that the syntactic 
information alone can resolve the ambiguities in 
the word lattice sufficiently well to select the best 
breaking analysis in the absence of elaborate 
semantic information. Figure 1 (see Appendix) 
shows the default attachment of the relative clause 
to the closest NP. Though this structure may be 
semantically implausible, the word-breaking 
analysis is correct. 
The word-breaking colnponent of out" system is 
described in detail in Kacmarcik et al (2000). For 
the lmrpose of robust parsing, the component is 
expected to solve the following two problems: 
? Lemmatization: Find possible words in the 
input text using a dictionary and its inflectional 
morphology, and return the dictionary entry forms 
(lemmas). Note that multiple lemmas are often 
possible for a given inflected form (e.g. surface 
form /o,~z -(- (kalte) could be an inflected form of 
the verbs /9~3 (kau "buy"), /0~o (katu "win")or 
/o,~ (karu "trim"), in which case all these forms 
must be returned. The dictionary the word-breaker 
uses has about 70,000 unique entries. 
? Orthography norlnalization: Identify and 
norlnalize orthographic variants. This is a 
non-trivial task in Japanese, as words can be 
spelled using any colnbination of the tout" chanmter 
823 
4.3 Parser precision 
An initial concern in implementing the present 
model was that parsing ambiguous input might 
proliferate syntactic analyses. In theory, the 
number of analyses might grow exponentially as 
the input sentence length increased, making the 
reliable ranking of parse results unmanageable. In 
practice, however, pathological proliferation of 
syntactic analyses is not a problem s. Figure 4 
tallies the average number of parses obtained in 
relation to sentence l ngth for all successful parses 
in the 5,000-sentence t st corpus (corpus A in 
Table 1). There were 4,121 successful parses in the 
corpus, corresponding to 82.42% coverage. From 
Figure 4, we can see that the number of parses 
does increase as the sentence grows longer, but the 
increment is linear and the slope is very moderate. 
Even in the highest-scoring range, the mean 
number of parses is only 2.17. Averaged over all 
sentence lengths, about 68% of the successfully 
parsed sentences receive only one parse, and 22% 
receive two parses. Only about 10% of sentences 
receive more than 2 analyses. From these results 
we conclude that the overgeneration f parse trees 
is not a practical concern within our approach. 
3 
"6 2 
E 
==t 
1-10 11- 21- 31- 41- 51- 61- 71- 81- 91- >101 
20 30 40 50 60 70 80 90 100 
sentence length (in char) 
Figure 4. Average number ol'parses for corpus A 
(5,000 sentences) 
4.4 Performance 
A second potential concern was performance: 
would the increased number of records in the chart 
cause unacceptable degradation of system speed? 
5 A similar observation is made by Charniak et al 
(forthcoming), who find that the number ot' final parses 
caused by additional POS tags is far less than the 
theoretical worst case in reality. 
This concern also proved unfounded in practice. In 
another experiment, we evaluated the processing 
speed of the system by measuring the time it takes 
per character in the input sentence (in 
milliseconds) relative to the sentence length. The 
results are given in Figure 5. This figure shows 
that the processing time per-character grows 
moderately as the sentence grows longer, due to 
the increased number of intermediate analyses 
created during the parsing. But the increase is 
linear, and we interpret these results as indicating 
that our approach is fully viable and realistic in 
terms of processing speed, and robust against input 
sentence length. The current average parsing time 
for our 15,000-sentence corpus (with average 
sentence length of 49.02 characters) is 23.09 
sentences per second on a Dell 550MHz Pentium 
III machine with 512MB of RAM. 
1.5 
1.4 
1.3 
1.2 
1.1 
1 
0.9 
0.8 
0.7 
15 25 35 45 55 65 75 85 95 105 115 125 135 
sentence length (in char) 
Figure 5. Processing speed on a 15,000-sentence corpus 
5. Conclusion 
We have shown that a practical, broad-coverage 
parser can be implemented without requiring the 
word-breaking component to return a single 
segmentation a alysis, and that it can at the same 
time achieve high accuracy in POS-labeled 
word-breaking. Separating the tasks of word 
ident~/'l'cation a d best sequence selection offers 
flexibility in enhancing both recall and precision 
without sacrificing either at the cost of the other. 
Our results show that morphological nd syntactic 
information alone can resolve most word-breaking 
ambiguities. Nonetheless, some ambiguities 
require semantic and contextual information. For 
example, the following sentence allows two parses 
corresponding to two word-breaking analyses, of 
which the first is semantically preferred: 
826 
(1) ocha-ni haitte-irtt arukaroido 
tea-in contain-ASP alkaloid 
"the alkaloid contained in lea" 
(2) ocha-ni-ha itte-iru arukatwido 
tea-in-TOP go-ASP alkaloid 
? ? the alkaloid that has gone to the tea" 
Likewise, the sentence below allows two different 
interpretations of the morpheme de, either as a 
locative marker (1) or as a copula (2). Both 
interpretations are syntactically and semantically 
wflid; only contextual information can resolve the 
ambiguity. 
(1) minen-ha isuraeru-de aru 
next year-TOP Israel-LOC be-held 
"It will be held in Israel next year". 
(2) rainen-ha isuraeru de-artt 
next year-TOP Israel be-PP, ES 
"It will be Israel next year". 
In both these sentences, we create syntactic trees 
for all syntactically valid interpretations, leaving 
the ambiguity intact. Such ambiguities can only be 
resolved with semantic and contextual information 
eventually made available by higher processing 
components. This will be Ihe focus of our ongoing 
rese.arclt. 
Acknowledgements 
We: would like to thank Mari Bmnson and Kazuko 
Robertshaw for annotating corpora for target 
word-breaking and POS tagging. We also thank 
the anonymous reviewers and the members of the 
MSR NLP group for their comments on the earlier 
version of the paper. 
References  
Charniak, Eugene, Glenn Carroll, John Adcock, Antony 
Cassandra, Yoshihiko Gotoh, Jeremy Katz, Michael 
Littmaa, and John McCann. Forthcoming. Taggers l'or 
parsers. To appear in Artificial Intelligence. 
Fuchi, Takeshi and Shinichiro Takagi. 1998. Japanese 
Morphological Analyzer Using Word Co-Occurrence 
-JTAG-. Proceeding of ACL-COLING: 409-413. 
Gamon, Michael, Carmen Lozano, Jessie Pinkham and 
Tom Reutter. 1997. Practical Experience with 
Grammar Sharing in Multilingual NLP. Jill Burstcin 
and Chmdia Leacock (eds.), From Research to 
Commercial Applications: Making NLP Work in 
Practice (Proceedings of a Workshop Sponsored by 
the Association for Computational Linguistics). 
pp.49-56. 
Heidorn, George. 1975. Attgmented Phrase Structure 
Grammars. In B.L. Webber and R.C. Schank (eds.), 
Theoretical Issues in Natural Language Processing. 
ACL 1975: 1-5. 
Heidorn, George. In press. Intelligent Writing 
Assistance. To appear in Robert Dale, Hermann Moisl 
and Harold Seiners (eds.), lfandbook of Natural 
Language Processing. Chapter 8. 
Hisamitsu, T. and Y. Nitta. 1990. Morphological 
Analysis by Minimum Connective-Cost Method. 
Technical Report, SIGNLC 90-8. IEICE pp.17-24 (in 
Japanese). 
Jensen, Karen. 1987. Binary Rules and Non-Binary 
Trees: Breaking Down the Concept o1' Phrase 
Structure. In Alexis Manasler-Ramer (ed.), 
Mathematics of Language. Amsterdam: John 
Benjamins Publishing. pp.65-86. 
Jensen, Karen, George E. Hektorn and Stephen D. 
Richardson (eds.). 1993. Natural Language 
Processing: The PLNLP approach. Kluwer: Boston. 
Kacmareik, Gary, Chris Brockett and Hisami St, zuki. 
2000. Robust Segmentation of Japanese Text into a 
l,attice for Parsing. Proceedings of COLING 2000. 
Kurohashi, Sadao and Makoto Nagao. 1998. Building a 
Japanese Parsed Corpus While hnproving the Parsing 
System. First LREC Proceedings: 719-724. 
Murakami, J. and S. Sagayama. 1992. Hidden Markov 
Model Applied to Morphological Analysis. lPSJ 3: 
161 - 162 (in Japanese). 
Nagata, Masaaki. 1994. A Stochastic Japanese 
Morphological Analyzer Using a Forward-DP 
Backward-A* N-Best Search Algorithm. Proceedings 
o1' COLING '94:201-207. 
Richardson, Stephen D. 1997. Determining Similarity 
and Inferring Relatkms in a Lexical Knowledge Base. 
Ph.D. dissertation. The City University of New York. 
Richardson, Stephen D., William B. Dolan and Lucy 
Vanderwende. 1998. MindNet: acquiring and 
structuring semantic information f l 'om text. 
Proceedings of COLING-ACL OS: 1098-1102. 
Taket,chi, Koichi and Yuji Matsumoto. 1997. HMM 
Parameter Learning for Japanese Morphological 
Analyzer. IPSJ: 38-3 (in Japanese). 
Wu, Andi and Zixin Jiang. 1998. Word Segmentation in 
Sentence Analysis. Technical Report, MSR-TR-99-10. 
Microsoft Reseamh. 
827 
Appendix 
"(It) has been annoyed by the successive interventions by the neighboring country". 
~b~O<" VERB1 
~t~ NOUN1 
~t~ VERB2 
:::::::: O< ~ VERB3 
::: : : : : : : : : : : : : :  ~ NOUN2 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ NOUN3 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ PRONI 
: : : : : : : : : : : : : : : : : : : : : : : :  ~ POSPI 
: : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ NOUN4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[C~ NOUN5 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[: VERB4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  \[C POSP2 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB5 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~NOUN6 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB~ 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ IJl 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ POSP3 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB7 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ Ia2 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ POSP4 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~NOUN7 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ VERB8 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  ~ CONJI 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  b%~NOUN8 
: : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :  L%~ VERB9 
DECL I  NP I  NP2  RELCL I  VERB1*  "~%O?"  
NOUN2*  "~"  
PP I  POSP I*  "~"  
NOUN4*  "=~\]~" 
PP2 POSP2*  "\[C" 
VERBS* "~&~"  
AUXPI  VERB9*  " ~ "  
CHAR1 "o" 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
(successive) 
(neighboring country) 
(GEN) 
(intervention) 
(by) 
(annoyed) 
(be) 
Figure 1. Example of ambiguous attachment. RELCL I  can syntactically modify either NOUN2 or NOUNa. 
NOUN4 (non-local attachment) is the semantically correct choice. Shown above the parse tree is the 
input word lattice returned from the word-breaker. 
94~-~l~:{~,,~,W~\[7_-tk~bvEl,~Teo "Classical Thai literature is based on tradition and history". 
F ITTED1 NP I  
NP2  
NOUN1*  "9-1" i~  ~ "  (classical Thai literature) 
PPI POSP i* " \ [~"  (TOP IC)  
UP3 NOUn2 * "{~"  (tradition) 
posp2 * ,, k" ,, (and) 
NP4 NOUN3 * "~"  (history) 
PP2 POSP3 * "17-" (on) 
VPI  VERB1*  ":6~'-5t VC"  (based) 
AUXPI  VERB2 * ,, I, xT~ ,, (be) 
CHAR1 " o " 
Figure 2. Example of an incomplete parse with correct word-breaking results. 
828 
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1066?1075,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Discriminative Substring Decoding for Transliteration
Colin Cherry and Hisami Suzuki
Microsoft Research
One Microsoft Way
Redmond, WA, 98052
{colinc,hisamis}@microsoft.com
Abstract
We present a discriminative substring de-
coder for transliteration. This decoder
extends recent approaches for discrimi-
native character transduction by allow-
ing for a list of known target-language
words, an important resource for translit-
eration. Our approach improves upon
Sherif and Kondrak?s (2007b) state-of-the-
art decoder, creating a 28.5% relative im-
provement in transliteration accuracy on
a Japanese katakana-to-English task. We
also conduct a controlled comparison of
two feature paradigms for discriminative
training: indicators and hybrid generative
features. Surprisingly, the generative hy-
brid outperforms its purely discriminative
counterpart, despite losing access to rich
source-context features. Finally, we show
that machine transliterations have a posi-
tive impact on machine translation quality,
improving human judgments by 0.5 on a
4-point scale.
1 Introduction
Transliteration occurs when a word is borrowed
into a language with a different character set.
The word is transcribed into the new character
set in such a way as to maintain rough phonetic
correspondence; for example, the English word
hip-hop becomes 2#7;#7 [hippuhoppu],
when transliterated into Japanese. A task fre-
quently of interest to the NLP community is back-
transliteration, where one seeks the original word,
given the borrowed form.
We investigate machine transliteration as a
method to handle out-of-vocabulary items in a
Japanese-to-English translation system. More
often than not, this will correspond to back-
transliteration. Our goal is to prevent the copy-
ing or deletion of Japanese words when they are
missing from our statistical machine translation
(SMT) system?s translation tables. This can have
a substantial impact on the quality of SMT output,
transforming translations of questionable useful-
ness, such as:
Avoid using a5JAK account.1
into the far more informative:
Avoid using a Freemail account.
Though the techniques we present here are
language-independent, we focus this study on
the task of Japanese katakana-to-English back-
transliteration. Katakana is one of the four char-
acter types used in the Japanese writing system
(along with hiragana, kanji and Roman alpha-
bet), consisting of about 50 syllabic characters.
It is used primarily to spell foreign loanwords
(e.g., !GL( [chokoreeto] ? chocolate),
and names (e.g., JS(S [kurinton] ? Clin-
ton). Therefore, katakana is a strong indicator
that a Japanese word can be back-transliterated.
However, katakana can also be used to spell sci-
entific names of animals and plants (e.g., B
[kamo] ? duck), onomatopoeic expressions (e.g.,
0C0C [bashabasha] ? splash) and for-
eign origin words that are not transliterations (e.g.,
;! [hochikisu] ? stapler). These un-
transliterable cases constitute about 10% of the
katakana words in our data.
We employ a discriminative substring decoder
for machine transliteration. Following Sherif and
Kondrak (2007b), the decoder operates on short
source substrings, with each operation producing
one or more target characters, as shown in Fig-
ure 1. However, where previous approaches em-
ploy generative modeling, we use structured per-
ceptron training to discriminatively tune parame-
ters according to 0-1 transliteration accuracy. This
15JAK is romanized as [furiimeeru]
1066
? ? ??
tho m son
Figure 1: Example substring derivation
allows us to test novel methods for the use of tar-
get lexicons in discriminative character transduc-
tion, allowing our decoder to benefit from a list of
known target words. Perhaps more significantly,
our framework allows us to test two competing
styles of features:
? sparse indicators, designed to capture the
same channel and language modeling data
collected by previous generative models, and
? components of existing generative models,
used as real-valued features in a discrimina-
tively weighted, generative hybrid.
Note that generative hybrids are the norm in
SMT, where translation scores are provided by
a discriminative combination of generative mod-
els (Och, 2003). Substring-based transliteration
with a generative hybrid model is very similar to
existing solutions for phrasal SMT (Koehn et al,
2003), operating on characters rather than words.
Unlike out-of-the-box phrasal SMT solutions, our
generative hybrid benefits from a target a lexicon.
As we will show, this is the difference between a
weak baseline and a strong competitor.
We demonstrate that despite recent successes in
discriminative character transduction using indi-
cator features (Jiampojamarn et al, 2008; Dreyer
et al, 2008), our generative hybrid performs sur-
prisingly well, producing our highest translitera-
tion accuracies. Researchers frequently compare
against a phrasal SMT baseline when evaluating a
new transduction technique (Freitag and Khadivi,
2007; Dreyer et al, 2008); however, we are careful
to vary only the features in our comparison. Con-
founding variables, such as alignment, decoder
and training method, are held constant.
We also include a human evaluation of
transliteration-augmented SMT output. Though
human evaluations are too expensive to allow a
comparison between transliteration systems, we
are able to show that adding our transliterations
to a production-level SMT engine results in a sub-
stantial improvement in translation quality.
2 Background
This work draws inspiration from previous work
in transliteration, which we divide into similarity
and transduction-based approaches. We also dis-
cuss recent successes in discriminative character
transduction that have influenced this work.
2.1 Similarity-based transliteration
In similarity-based transliteration, a character-
based, cross-lingual similarity metric is calculated
(or bootstrapped) from known transliteration pairs.
Given a source word s, its transliteration is the tar-
get word t most similar to s, where t is drawn from
some pool of candidates. This approach may also
be referred to as transliteration discovery.
Brill et al (2001) describe a katakana-to-
English approach with an EM-learned edit dis-
tance, which bootstraps from a small number of
examples to learn transliteration pairs from query
logs. Bilac and Tanaka (2005) harvest translitera-
tion candidates from comparable bilingual corpora
(conference abstracts in English and Japanese),
and use distributional as well as phonetic simi-
larity to choose among them. Sherif and Kon-
drak (2007a) also bootstrap a learned edit dis-
tance for Arabic named entities, with candidate
pairs drawn from sentence or document-aligned
parallel text. Klementiev and Roth (2006) boot-
strap an SVM classifier trained to detect true
transliteration-pairs. They draw candidates from
comparable news text, using date information to
provide further clues as to aligned named entities.
Bergsma and Kondrak (2007) extend the classifi-
cation approach with features derived from a char-
acter alignment. They train from bilingual dic-
tionaries and word-aligned parallel text, selecting
negative examples to target false-friends.
The work of Hermjakob et al (2008) is par-
ticularly relevant to this paper, as they incorpo-
rate a similarity-based transliteration system into
an Arabic-to-English SMT engine. They employ
a hand-crafted cross-lingual similarity metric, and
use capitalized n-grams from the Google n-gram
corpus as candidates. With such a huge candidate
list, a cross-lingual indexing scheme is designed
for fast candidate look-up. Their work also ad-
dresses the question of when to transliterate (as
opposed to translate), a realistic concern when de-
ploying a transliteration component in SMT. This,
however, is not of so much concern for katakana,
as it is used primarily for loanwords.
1067
2.2 Transduction-based transliteration
The approach presented in this paper is an instance
of transduction-based transliteration, where the
source word is transformed into a target word us-
ing a sequence of character-level operations. The
parameters of the transduction process are learned
from a collection of transliteration pairs. These
systems do not require a list of candidates, but
many incorporate a target lexicon, favoring target
words that occur in the lexicon. This approach is
also known as transliteration generation.
The majority of transliteration generation ap-
proaches are based on the noisy channel model,
where a target t is generated according to
P (t|s) ? P (s|t)P (t). This approach is typi-
fied by finite-state transliteration, where the var-
ious stages of the channel model are represented
by finite state transducers and automata. Early
systems employed a complex channel, passing
through multiple phonetic representations (Knight
and Graehl, 1998; Bilac and Tanaka, 2004), but
later versions replaced characters directly (Al-
Onaizan and Knight, 2002). Sherif and Kondrak
(2007b) extend this approach with substring oper-
ations in the style of phrasal SMT, and show that
doing so improves both accuracy as well as space
and time efficiency. Note that it is possible to in-
corporate a target lexicon by making P (t) a word
unigram model with a character-based back-off.
Li et al (2004) present an alternative to the
noisy channel with their joint n-gram model,
which calculates P (s, t). This formulation allows
operations to be conditioned on both source and
target context. However, the inclusion of a candi-
date list is more difficult in this setting, as P (t) is
not given its own model.
Zelenko and Aone (2006) investigate a purely
discriminative, alignment-free approach to
transliteration generation. The target word is
constructed one character at a time, with each
new character triggering a suite of features,
including indicators for near-by source and target
characters, as well a generative target language
model. Freitag and Khadivi (2007) propose a dis-
criminative, latent edit distance for transliteration.
In this case, training data need not be aligned in
advance, but a latent alignment is produced during
decoding. Again, the target word is constructed
one character at a time, using edit operations
that are scored according to source and target
context features. Both approaches train using a
structured perceptron, as we do here. However,
these models represent a dramatic departure from
the existing literature, while ours has clear analogs
to the well-known noisy-channel paradigm, which
allows for useful comparisons and insights into
the advantages of discriminative training.
2.3 Discriminative character transduction
While our chosen application is transliteration,
our decoder is influenced by recent successes in
general-purpose discriminative transduction. Ji-
ampojamarn et al (2008) describe a discrimina-
tive letter-to-phoneme substring transducer, while
Dreyer et al (2008) describe a discriminative char-
acter transducer with a latent derivation structure
for morphological transformations. Both models
are extremely effective, but both rely exclusively
on indicator features; they do not explore the use
of knowledge-rich generative models. Our indica-
tor system uses an extended version of the Jiampo-
jamarn et al (2008) feature set.
3 Methods
We adopt a discriminative substring decoder for
our transliteration task. A structured percep-
tron (Collins, 2002) learns weights for our translit-
eration features, which are drawn from two broad
classes: indicator and hybrid generative features.
3.1 Structured perceptron
The decoder?s discriminative parameters are
learned with structured perceptron training. Let
a derivation d describe a substring operation se-
quence that transliterates a source word into a tar-
get word. Given an input training corpus of such
derivations D = d
1
. . . d
n
, a vector feature func-
tion on derivations
~
F (d), and an initial weight vec-
tor ~w, the perceptron performs two steps for each
training example d
i
? D:
? Decode:
?
d = argmax
d?D(src(d
i
))
(
~w ?
~
F (d)
)
? Update: ~w = ~w +
~
F (d
i
)?
~
F (
?
d)
where D(src(d)) enumerates all possible deriva-
tions with the same source side as d. To improve
generalization, the final feature vector is the aver-
age of all vectors found during learning (Collins,
2002). Accuracy on the development set is used
to select the number of times we pass through all
d
i
? D.
Given the above framework, we require training
derivations D, feature vectors
~
F , and a decoder to
1068
carry out the argmax over all d reachable from a
particular source word. We describe each of these
components in turn below.
3.2 Training derivations
Note that the above framework describes a max-
derivation decoder trained on a corpus of gold-
standard derivations, as opposed to a max-
transliteration decoder trained directly on source-
target pairs. By building the entire system on the
derivation level, we side-step issues that can oc-
cur when perceptron training with hidden deriva-
tions (Liang et al, 2006), but we also introduce the
need to transform our training source-target pairs
into training derivations.
Training derivations can be learned unsu-
pervised from source-target pairs using char-
acter alignment techniques. Previously, this
has been done using an EM-learned edit dis-
tance (Ristad and Yianilos, 1998), or generaliza-
tions thereof (Brill and Moore, 2000; Jiampoja-
marn et al, 2007). We opt for an alternative align-
ment technique, similar to the word-aligner de-
scribed by Zhang et al (2008). This approach
employs variational EM with sparse priors, along
with hard length limits, to reduce the length of
substrings operated upon. By doing so, we hope to
learn only non-compositional transliteration units.
Our aligner produces only monotonic align-
ments, and does not allow either the source or tar-
get side of an operation to be empty. The same
restrictions are imposed during decoding. In this
way, each alignment found by variational EM is
also an unambiguous derivation. We align our
training corpus with a maximum substring length
of three characters. The same derivations are used
to train all of the transliteration systems tested in
this paper.
3.3 Features
We employ two main types of features: indicators
and hybrid generative models. Indicators detect
binary events in a derivation, such as the presence
of a particular operation. Hybrid generative fea-
tures assign a real-valued probability to a deriva-
tion, based on statistics collected from training
derivations. There are few generative features and
each carries a substantial amount of information,
while indicators are sparse and knowledge-poor.
We treat these two classes of features as distinct.
We do so because researchers often use either one
approach or the other.
2
Furthermore, it is not
clear how to optimally employ training derivations
when combining generative models and sparse in-
dicators: generative models need large amounts of
data to collect statistics and relatively little for per-
ceptron training,
3
while sparse indicators require
only a large perceptron training set.
We can further divide feature space according
to the information required to calculate each fea-
ture. Both feature sets can be partitioned into the
following subtypes:
? Emission: How accurate are the operations
used by this derivation?
? Transition: Does the target string produced
by this derivation look like a well-formed tar-
get character sequence?
? Lexicon: Does the target string contain
known words from a target lexicon?
Indicator Features
Previous approaches to discriminative character
transduction tend to employ only sparse indica-
tors (Jiampojamarn et al, 2008; Dreyer et al,
2008). This is because sparsity is not a major con-
cern in character-based domains, and sparse indi-
cators are extremely flexible.
Our emission and transition indicator features
follow Jiampojamarn et al (2008). Emission indi-
cators are centered around an operation, such as
[( ? tho]. Minimally, an indicator exists for
each operation. Many more source context fea-
tures can be generated by conjoining an operation
with source n-grams found within a fixed win-
dow of C characters to either side of the operation.
These source context features have minimal com-
putational cost, and they allow each operator to ac-
count for large, overlapping portions of the source,
even when the substrings being operated upon are
small. Meanwhile, transition indicators stand in
for a character-based target language model. Indi-
cators are built for each possible target n-gram, for
n = 1 . . .K, allowing the perceptron to construct
a discriminative back-off model. Development ex-
periments lead us to select C = 3 and K = 5.
2
Generative hybrids are often accompanied by a small
number of unsparse indicators, such as operation count.
3
Perceptron training on the same data used for model
construction can lead to overconfidence in model quality.
One can address this problem by using a large number of
modeling-training folds (Collins et al, 2005), but we do not
do so here.
1069
Indicator lexicon features are novel to this work.
Given access to a target lexicon with type fre-
quencies, we opt to create features that indicate
the frequencies of generated target words accord-
ing to coarse bins. Experiments on our develop-
ment set lead to the selection of 5 frequency bins:
[< 2,000], [< 200], [< 20], [< 2], [< 1]. To keep
the model linear, these features are cumulative;
thus, generating a word with frequency 126 will
result in both the [< 2, 000] and [< 200] features
firing. Note that a single transliteration can po-
tentially generate multiple target words, and doing
so can have a major impact on how often the lex-
icon features fire. Thus, we employ another fea-
ture that indicates the introduction of a new word.
We expect these frequency indicators to be supe-
rior to a word-level unigram model, as they allow
the designer to select notable frequencies. In par-
ticular, the bins we have selected do not give any
advantage to extremely common words, as these
are generally less likely to be transliterated.
Hybrid Generative Features
We begin with the three components of the gener-
ative noisy channel employed by Sherif and Kon-
drak (2007b). Their transliteration probability is:
P (t|s) ? P
E
(s|t) ?max [P
T
(t), P
L
(t)] (1)
Inspired by the linear models used in SMT (Och,
2003), we can discriminatively weight the compo-
nents of this generative model, producing:
w
E
logP
E
(s|t) + w
T
logP
T
(t) + w
L
logP
L
(t)
with weights w learned by perceptron training.
These three models conveniently align with our
three feature subtypes. Emission information is
provided by P
E
(s|t), which is estimated by maxi-
mum likelihood on the operations observed in our
training derivations. Including source context is
difficult in such a model. To compensate for this,
all systems using P
E
(s|t) also use composed op-
erations, which are constructed from operation se-
quences observed in the training set. This removes
the length limit on substring operations.
4
P
T
(t)
provides transition information through a charac-
ter language model, estimated on the target side
4
Derivations built by our character aligner use opera-
tions on substrings of maximum length 3. To enable per-
ceptron training with composed operations, once P
E
(s|t)
has been estimated by counting composed operations in the
initial alignments, we re-align our training examples with
those composed operations to maximize P
E
(s|t), creating
new training derivations.
of the training derivations. In our implementation,
we employ a KN-smoothed 7-gram model (Kneser
and Ney, 1995). Finally, P
L
(t) is a unigram tar-
get word model, estimated from the same type fre-
quencies used to build our lexicon indicators.
Since we have adopted a linear model, we are
no longer constrained by the original generative
story. Therefore, we are free to incorporate other
SMT-inspired features: P
E
?
(t|s), target character
count, and operation count.
5
Feature summary
The indicator and hybrid-generative feature sets
each provide a discriminative version of the noisy
channel model. In the case of transition and lexi-
con features, both systems have access to the ex-
act same information, but encode that information
differently. The lexicon encoding is the most dra-
matic difference, with the indicators using a small
number of frequency bins, and the generative uni-
gram model providing a single, real-valued feature
that is proportional to frequency.
In the case of their emission features, the
two systems actually encode different information.
Both have access to the same training derivations,
but the indicator system provides source context
through n-gram indicators, while the generative
system does so using composed operations.
3.4 Decoder
Our decoder builds upon machine translation?s
monotone phrasal decoding (Zens and Ney, 2004),
or equivalently, the sequence tagging algorithm
used in semi-Markov CRFs (Sarawagi and Co-
hen, 2004). This dynamic programming (DP) de-
coder extends the Viterbi algorithm for HMMs
by operating on one or more source characters (a
substring) at each step. A DP block stores the
best scoring solution for a particular prefix. Each
block is subdivided into cells, which maintain the
context necessary to calculate target-side features.
We employ a beam, keeping only the 40 highest-
scoring cells for each block, which speeds up in-
ference at the expense of optimality. We found
that the beam had no major effect on perceptron
training, nor on the system?s final accuracy.
Previously, target lexicons have been used
primarily in finite-state transliteration, as they
are easily encoded as finite-state-acceptors (Al-
Onaizan and Knight, 2002; Sherif and Kondrak,
5
Character and operation counts also fit in the indicator
system, but did not improve performance in development.
1070
2007b). It is possible to extend the DP decoder to
also use a target lexicon. By encoding the lexicon
as a trie, and adding the trie index to the context
tracked by the DP cells, we can provide access to
frequency estimates for words and word prefixes.
This has the side-effect of creating a new cell for
each target prefix; however, in the character do-
main, this remains computationally tractable.
4 Data
4.1 Wikipedia training and test data
Our katakana-to-English training data is de-
rived from bilingually-linked Wikipedia titles.
Any Japanese Wikipedia article with an entirely
katakana title and a linked English article results
in training pair. This results in 60K transliteration
pairs; we removed 2K pairs for development, and
2K for held-out testing.
The remaining 56K training pairs are quite
noisy. As mentioned earlier, roughly 10% of our
examples are simply not transliterable, but ap-
proximate Wikipedia title translations are an even
more substantial source of noise. For example,
S4E@ [konpyuutageemu] ? com-
puter game is aligned with the English article
Computer and video games. We found it ben-
eficial, in terms of both speed and accuracy, to
do some coarse alignment-based pruning. After
alignment, the operations used by all derivations
are counted. Any operation that is used fewer than
three times is eliminated, along with any deriva-
tion using that operation. The goal is to eliminate
loose transliteration pairs from our data, where a
word or initial is included in one language but
not the other. This results in 40K training pairs.
Despite the noise in the Wikipedia data, there are
clear advantages in using it for training transliter-
ation models: it is available for any language pair,
it reflects recent trends and events, and the amount
of data increases daily. As we will see below, the
model trained on this data performs well on a test
set from a very different domain.
All systems use development set accuracy to
select their meta-parameters, such as the number
of perceptron iterations, the size of the source-
context window, and the n-gram length used in
character language modeling. The hybrid gener-
ative system further splits the training set, using
38K derivations for the calculation of its emission
and transition models, and 2K derivations for per-
ceptron training its model weights.
4.2 Machine translation test data
In order to see how effective our transliterator
is on out-of-domain test data, we also created
test data from a log of translation requests to
a web-based, Japanese-to-English translation ser-
vice.
6
Out of 5,000 randomly selected transla-
tion requests, there are 312 cases where katakana
source words are out-of-vocabulary for the MT
system, and therefore remain untranslated. We
created a reference translation (not necessarily a
transliteration) for these katakana words by man-
ually selecting the corresponding English word(s)
in the sentence-level reference translation, which
was produced independently from this experiment.
This test set is quite divergent from the Wikipedia
titles: only 17 (5.5%) of its katakana words are
found in the Wikipedia training data, and six of
these did not agree on the English translation.
4.3 English lexicon
Our English lexicon is derived from two over-
lapping data sources: the English gigaword cor-
pus (LDC2003T05; GW) and the language model
training data for our SMT system, which contains
selections from Europarl, gigaword, and web-
harvested text. Both are lowercased. We com-
bine the unigram frequency counts from the two
sources by taking the max when they overlap. The
resulting lexicon has 5M types, 2.5M of which
have frequency 1.
5 Experiments
In this section, we summarize development exper-
iments, and then conduct a comparison on our two
transliteration test sets. We report 0-1 accuracy: a
transliteration is only correct if it exactly matches
the reference. For the comparison experiments,
we also report 10-best accuracy, where a system
is correct if it includes the correct transliteration
somewhere in its 10-best list.
5.1 Baselines
We compare our systems against a re-
implementation of Sherif and Kondrak?s (2007b)
noisy-channel substring decoder. This uses the
same P
E
, P
T
and P
L
models as our hybrid gen-
erative system, but employs a two-pass decoding
scheme to find the max transliteration according
to Equation 1. It represents a purely generative
solution using otherwise identical architecture.
6
http://www.microsofttranslator.com
1071
Since our hybrid generative system implements
a model that is very similar to those used in phrasal
SMT, we also compare against a state-of-the-art
phrasal SMT system (Moore and Quirk, 2007).
This system is trained by applying the standard
SMT pipeline to our Wikipedia title pairs, treat-
ing characters as words, using a 7-gram character-
level language model, and disabling re-ordering.
Unfortunately, the decoder?s architecture does not
allow the use of a word-level unigram model, re-
ducing the usefulness of this baseline. Instead, we
include the target lexicon as a second character-
level language model. This baseline indicates the
level of performance one can expect by applying
phrasal SMT straight out of the box.
Comparing the two baselines qualitatively, both
use a combination of generative models inspired
by the noisy channel. Sherif and Kondrak em-
ploy a word-level unigram model without discrim-
inatively weighting the models, while the Phrasal
SMT approach uses weights derived from max-
BLEU training without word-level unigrams. The
obvious question of what happens when one does
both will be answered by our hybrid generative
system.
5.2 Development experiments
Table 1 shows development set accuracy for a
number of systems and feature types, along with
the model size of the corresponding systems,
where size is measured in terms of the number of
non-zero discriminatively-trained parameters. The
accuracy of the Sherif and Kondrak baseline is
shown as SK07. Despite its lack of discrimina-
tive training, word-level unigrams allow the SK07
baseline to outperform Phrasal SMT . In future ex-
periments, we compare only against SK07.
The indicator system was tested using only op-
eration indicators, with source context, transition
and lexicon indicators added incrementally. All
feature types have a substantial impact, with the
lexicon providing the boost needed to surpass the
baseline. Note that the inclusion of the five fre-
quency bins is sufficient to decrease the overall
feature count of the system by 600K, as much
fewer mistakes are made during training.
Development of the hybrid generative system
used the SK07 baseline as a starting point. The re-
sult of combining its three components into a flat
linear model, with all weights set to 1, is shown
in Table 1 as Linear SK07. This violation of
Table 1: Development accuracy and model size
System Acc. Size
Baseline Phrasal SMT 30.7 8
SK07 33.5 ?
Indicator Operations only 3.6 6.8K
+ source context 23.9 2.8M
+ transition 28.6 3.1M
+ lexicon 44.2 2.5M
+ gen. lexicon 44.1 3.0M
Generative Linear SK07 31.7 ?
+ perceptron 42.4 3
+ SMT features 44.1 6
+ ind. lexicon 44.3 12
conditional independence assumptions results in a
drop in accuracy. However, the + perceptron line
shows that setting the three weights with percep-
tron training results in a huge boost in accuracy,
nearly matching our indicator system. Adding fea-
tures inspired by SMT, such as P
E
?
(t|s), elimi-
nates the gap between the two.
5.3 Development discussion
Considering their differences, the two systems?
proximity in score is quite surprising. Given the
character domain?s lack of sparsity, and the large
amount of available training data, we had expected
the hybrid generative system to behave only as
a strong baseline; instead, it matched the perfor-
mance of the indicator system. However, this
is not unprecedented: discriminatively weighted
generative models have been shown to outperform
purely discriminative competitors in various NLP
classification tasks (Raina et al, 2004; Toutanova,
2006), and remain the standard approach in statis-
tical translation modeling (Och, 2003).
Examining the development results on an
example-by-example basis, we see that the two
systems make mostly the same mistakes: for 87%
of examples, either both systems are right, or both
are wrong. The remainder represents a (relatively
small) opportunity to improve through system or
feature combination: an oracle that perfectly se-
lects between the two scores 50.6.
One opportunity for straight-forward combina-
tion is the target lexicon. Because lexicon frequen-
cies are drawn from an independent word list, and
not the transliteration training derivations, there is
no reason why both systems cannot use both lex-
icon representations. Unfortunately, doing so has
1072
Table 2: Test set comparisons
Wikipedia MT
System Acc. Top 10 Acc. Top 10
SK07 33.5 57.9 38.8 57.0
Generative 43.0 65.6 42.9 58.3
Indicator 42.5 63.5 43.6 57.7
little impact, as is shown in each system?s final row
in Table 1. Adding the word unigram model to
the indicator system results in slightly lower per-
formance, and a much larger model. Adding the
frequency bins to the generative system does im-
prove performance slightly, but attempts to com-
pletely replace the generative system?s word uni-
gram model with frequency bins resulted in a sub-
stantial drop in accuracy.
7
5.4 Test set comparisons
Table 2 shows the accuracies of the systems se-
lected during development on our testing data. On
the held-out Wikipedia examples, the trends ob-
served during development remain the same, with
the generative system expanding its lead. Mov-
ing to 10-best accuracies changes little, except for
slightly narrowing the gap between SK07 and the
discriminative systems.
The second column of Table 2 compares the
systems on our MT test set. As discussed ear-
lier, this data is quite different from the Wikipedia
training set, and as a result, the systems? differ-
ences are less pronounced. 1-best accuracy still
shows the discriminative systems having a definite
advantage, but at the 10-best level, those distinc-
tions are muted.
Compared with the previous work on katakana-
to-English transliteration, these accuracies do not
look particularly high: both Knight and Graehl
(1998) and Bilac and Tanaka (2004) report accu-
racies above 60% for 1-best transliteration. We
should emphasize that this is due to the difficulty
of our test data, and that we have tested against a
baseline that has been shown to outperform Knight
and Graehl (1998). The test data was not filtered
for noise, leaving untransliterable cases and loose
translations intact. The accuracies reported above
are under-estimates of real performance: many
transliterations not matching the reference may
still be useful to a human reader, such as differ-
7
Lexicon replacement experiment is not shown in Table 1.
ences in inflection (e.g.,L!.) [rechinoido]
? retinoids, transliterated as retinoid), and spac-
ing (e.g. IL
- [shierareone]? Sierra
Leone, transliterated as sierraleone).
6 Integration with machine translation
We used the transliterations from our indicator
system to augment a Japanese-to-English MT sys-
tem.
8
This treelet-based SMT system (Quirk et
al., 2005) is trained on about 4.6M parallel sen-
tence pairs from diverse sources including bilin-
gual books, dictionaries and web publications.
Our goal is to measure the impact of machine
transliterations on end-to-end translation quality.
6.1 Evaluation method
We use the MT-log translation pairs described
in Section 4.2 as a sentence-level translation test
set. For each katakana word left untranslated by
the baseline SMT engine, we generated 10-best
transliteration candidates and added the katakana-
English pairs to the SMT system?s translation ta-
ble. Perceptron scores were exponentiated, then
normalized, to create probabilities, which were
given to the SMT system as P (source|target);
9
all other translation features were set to log 1.
We translated the test set with and without the
augmented translation table. 120 sentences were
randomly selected from the cases where the trans-
lations output by the two SMT systems differed,
and were submitted for two types of human evalu-
ation. In the absolute evaluation, each SMT out-
put was assigned a score between 1 and 4 (1 =
completely useless; 4 = perfect translation); in the
relative evaluation, the evaluators were presented
with a pair of SMT outputs, with and without the
transliteration table, and were asked to judge if
they preferred one translation over the other. In
both evaluation settings, the machine-translated
sentences were evaluated by two native speakers
of English who have no knowledge of Japanese,
with access to a reference translation.
6.2 Results
The evaluation results show that our translitera-
tor does improve the quality of SMT. The BLEU
8
The human evaluation was carried out before we discov-
ered the effectiveness of the hybrid generative system, but
recall that the performance of the two is similar.
9
The perceptron scores are more naturally interpreted as
P (target |source), but the opposite direction is generally the
highest-weighted feature in the SMT system?s linear model.
1073
Table 3: Relative translation evaluation
evaluator 1 preference
eval2pref +translit equal baseline sum
+translit 95 0 2 97
equal 19 1 2 22
baseline 1 0 0 1
sum 115 1 4 120
score on the entire test set improved only slightly,
from 21.8 to 22.0. However, in the absolute hu-
man evaluation, the transliteration table increased
the average human judgement from 1.5 to 2 out of
a maximum score of 4. Table 3 shows the results
of the relative evaluation along with the judges?
sentence-level agreement. In 95 out of 120 cases,
both annotators agreed that the augmented table
produced a better translation than the baseline.
One might expect that any replacement of
katakana would improve the perception of MT
quality. This is not necessarily the case: it
can be more confusing to have a drastically
incorrect transliteration, such as transliterating
#7M [appurooda] ? uploader incor-
rectly as applaud. Fortunately, Table 3 shows that
we make very few of these sorts of mistakes: the
baseline is preferred only rarely. Also note that,
according the MT 10-best accuracies in Table 2,
we would have expected to improve at most 60%
of cases, however, the human judgements indicate
that our actual rate of improvement is closer to
80%, which demonstrates that even an imperfect
transliteration is often useful.
7 Conclusion
We have presented a discriminative substring de-
coder for transliteration. Our decoder is based
on recent approaches for discriminative charac-
ter transduction, extended to provide access to a
target lexicon. We have presented a comparison
of indicator and hybrid generative features in a
controlled setting, demonstrating that generative
models perform surprisingly well when discrim-
inatively weighted. We have also shown our dis-
criminative models to be superior to a state-of-the-
art generative system. Finally, we have demon-
strated that machine transliteration is immediately
useful to end-to-end SMT.
As mentioned earlier, by focusing on katakana,
we bypass the problem of deciding when to
transliterate rather than translate; next, we plan to
combine our models with a classifier that makes
such a decision, allowing us to integrate transliter-
ation into SMT for other language pairs.
References
Yaser Al-Onaizan and Kevin Knight. 2002. Machine
transliteration of names in Arabic text. In ACL
Workshop on Comp. Approaches to Semitic Lan-
guages.
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
ACL, pages 656?663, Prague, Czech Republic, June.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In COL-
ING, pages 597?603, Geneva, Switzerland.
Slaven Bilac and Hozumi Tanaka. 2005. Extracting
transliteration pairs from comparable corpora. In
Proceedings of the Annual Meeting of the Natural
Language Processing Society, Japan.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL, pages 286?293, Morristown, NJ.
Eric Brill, Gary Kacmarcik, and Chris Brockett.
2001. Automatically harvesting katakana-english
term pairs from search engine query logs. In Asia
Federation of Natural Language Processing.
Michael Collins, Brian Roark, and Murat Sarac?lar.
2005. Discriminative syntactic language modeling
for speech recognition. In ACL, pages 507?514,
Ann Arbor, USA, June.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Markus Dreyer, Jason Smith, and Jason Eisner. 2008.
Latent-variable modeling of string transductions
with finite-state methods. In EMNLP, pages 1080?
1089, Honolulu, Hawaii, October.
Dayne Freitag and Shahram Khadivi. 2007. A se-
quence alignment model based on the averaged per-
ceptron. In EMNLP, pages 238?247, Prague, Czech
Republic, June.
Ulf Hermjakob, Kevin Knight, and Hal Daum?e III.
2008. Name translation in statistical machine trans-
lation - learning when to transliterate. In ACL, pages
389?397, Columbus, Ohio, June.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many align-
ments and hidden markov models to letter-to-
phoneme conversion. In HLT-NAACL, pages 372?
379, Rochester, New York, April.
1074
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL,
pages 905?913, Columbus, Ohio, June.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL, pages
82?88, New York City, USA, June.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In In-
ternational Conference on Acoustics, Speech, and
Signal Processing (ICASSP-95), pages 181?184.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In HLT-NAACL.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In ACL, pages 159?166, Barcelona, Spain, July.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006. An end-to-end discrimina-
tive approach to machine translation. In COLING-
ACL, pages 761?768, Sydney, Australia, July.
Robert Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In MT Summit XI.
Franz J. Och. 2003. Minimum error rate training for
statistical machine translation. In ACL, pages 160?
167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In ACL, pages 271?279, Ann
Arbor, USA, June.
Rajat Raina, Yirong Shen, Andrew Y. Ng, and Andrew
McCallum. 2004. Classification with hybrid gener-
ative/discriminative models. In Advances in Neural
Information Processing Systems 16.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, 20(5):522?
532.
Sunita Sarawagi and William Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In ICML.
Tarek Sherif and Grzegorz Kondrak. 2007a. Boot-
strapping a stochastic transducer for Arabic-English
transliteration extraction. In ACL, pages 864?871,
Prague, Czech Republic, June.
Tarek Sherif and Grzegorz Kondrak. 2007b.
Substring-based transliteration. In ACL, pages 944?
951, Prague, Czech Republic, June.
Kristina Toutanova. 2006. Competitive generative
models with structure learning for nlp classification
tasks. In EMNLP, pages 576?584, Sydney, Aus-
tralia, July.
Dmitry Zelenko and Chinatsu Aone. 2006. Discrimi-
native methods for transliteration. In EMNLP, pages
612?617, Sydney, Australia, July.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT-NAACL, pages 257?264, Boston, USA, May.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL, pages 97?105, Columbus, Ohio, June.
1075
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484?1492,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Discovery of Term Variation in Japanese Web Search Queries 
 
 Hisami Suzuki, Xiao Li, and Jianfeng Gao 
Microsoft Research, Redmond 
One Microsoft Way, Redmond, WA 98052 USA 
{hisamis,xiaol,jfgao}@microsoft.com 
 
 
 
 
 
  
 
Abstract 
In this paper we address the problem of identi-
fying a broad range of term variations in Japa-
nese web search queries, where these varia-
tions pose a particularly thorny problem due to 
the multiple character types employed in its 
writing system. Our method extends the tech-
niques proposed for English spelling correc-
tion of web queries to handle a wider range of 
term variants including spelling mistakes, va-
lid alternative spellings using multiple charac-
ter types, transliterations and abbreviations. 
The core of our method is a statistical model 
built on the MART algorithm (Friedman, 
2001). We show that both string and semantic 
similarity features contribute to identifying 
term variation in web search queries; specifi-
cally, the semantic similarity features used in 
our system are learned by mining user session 
and click-through logs, and are useful not only 
as model features but also in generating term 
variation candidates efficiently. The proposed 
method achieves 70% precision on the term 
variation identification task with the recall 
slightly higher than 60%, reducing the error 
rate of a na?ve baseline by 38%.  
1 Introduction 
Identification of term variations is fundamental 
to many NLP applications: words (or more gen-
erally, terms) are the building blocks of NLP ap-
plications, and any robust application must be 
able to handle variations in the surface represen-
tation of terms, be it a spelling mistake, valid 
spelling variation, or abbreviation. In search ap-
plications, term variations can be used for query 
expansion, which generates additional query 
terms for better matching with the terms in the 
document set. Identifying term variations is also 
useful in other scenarios where semantic equiva-
lence of terms is sought, as it represents a very 
special case of paraphrase.  
This paper addresses the problem of identify-
ing term variations in Japanese, specifically for 
the purpose of query expansion in web search, 
which appends additional terms to the original 
query string for better retrieval quality. Query 
expansion has been shown to be effective in im-
proving web search results in English, where dif-
ferent methods of generating the expansion terms 
have been attempted, including relevance feed-
back (e.g., Salton and Buckley, 1990), correction 
of spelling errors (e.g., Cucerzan and Brill, 2004), 
stemming or lemmatization (e.g., Frakes, 1992), 
use of manually- (e.g., Aitchison and Gilchrist, 
1987) or automatically- (e.g., Rasmussen 1992) 
constructed thesauri, and Latent Semantic Index-
ing (e.g., Deerwester et al 1990). Though many 
of these methods can be applied to Japanese 
query expansion, there are unique problems 
posed by Japanese search queries, the most chal-
lenging of which is that valid alternative spel-
lings of a word are extremely common due to the 
multiple script types employed in the language. 
For example, the word for 'protein' can be spelled 
as ??????, ?????, ???, ???? 
and so on, all pronounced tanpakushitsu but us-
ing combinations of different script types. We 
give a detailed description of the problem posed 
by the Japanese writing system in Section 2. 
Though there has been previous work on ad-
dressing specific subsets of spelling alterations 
within and across character types in Japanese, 
there has not been any comprehensive solution 
for the purpose of query expansion.  
Our approach to Japanese query expansion is 
unique in that we address the problem compre-
hensively: our method works independently of 
the character types used, and targets a wide range 
of term variations that are both orthographically 
and semantically similar, including spelling er-
rors, valid alternative spellings, transliterations 
and abbreviations. As described in Section 4, we 
define the problem of term variation identifica-
1484
tion as a binary classification task, and build two 
types of classifiers according to the maximum 
entropy model (Berger et al, 1996) and the 
MART algorithm (Friedman, 2001), where all 
term similarity metrics are incorporated as fea-
tures and are jointly optimized. Another impor-
tant contribution of our approach is that we de-
rive our semantic similarity models by mining 
user query logs, which has been explored for the 
purposes of collecting related words (e.g., Jones 
et al, 2006a), improving search results ranking 
(e.g., Craswell and Szummer, 2007) and learning 
query intention (e.g., Li et al, 2008), but not for 
the task of collecting term variations. We show 
that our semantic similarity models are not only 
effective in the term variation identification task, 
but also for generating candidates of term varia-
tions much more efficiently than the standard 
method whose candidate generation is based on 
edit distance metrics.  
2 Term Variations in Japanese 
In this section we give a summary of the Japa-
nese writing system and the problem it poses for 
identifying term variations, and define the prob-
lem we want to solve in this paper.  
2.1 The Japanese Writing System 
There are four different character types that are 
used in Japanese text: hiragana, katakana, kanji 
and Roman alphabet. Hiragana and katakana are 
the two subtypes of kana characters, which are 
syllabic character sets, each with about 50 basic 
characters. There is a one-to-one correspondence 
between hiragana and katakana characters, and, 
as they are phonetic, they can be unambiguously 
converted into a sequence of Roman characters. 
For example, the word for 'mackerel' is spelled in 
hiragana as ?? or in katakana as ??, both of 
which can be transcribed in Roman characters as 
saba, which is how the word is pronounced. 
Kanji characters, on the other hand, are ideo-
graphic and therefore numerous ? more than 
5,000 are in common usage. One difficulty in 
handling Japanese kanji is that each character has 
multiple pronunciations, and the correct pronun-
ciation is determined by the context in which the 
character is used. For instance, the character ? is 
read as kou in the word ?? ginkou 'bank', gyou 
in ?  'column', and i or okona in ???  itta 
'went' or okonatta 'done' depending on the con-
text in which the word is used.1  Proper name 
readings are particularly difficult to disambiguate, 
as their pronunciation cannot be inferred from 
the context (they tend to have the same grammat-
ical function) or from the dictionary (they tend to 
be out-of-vocabulary). Therefore, in Japanese, 
computing a pronunciation-based edit distance 
metric is not straightforward, as it requires esti-
mating the readings of kanji characters.  
2.2 Term Variation by Character Type 
Spelling variations are commonly observed both 
within and across character types in Japanese. 
Within a character type, the most prevalent is the 
variation observed in katakana words. Katakana 
is used to transliterate words from English and 
other foreign languages, and therefore reflects 
the variations in the sound adaptation from the 
source language. For example, the word 
'spaghetti' is transliterated into six different 
forms (?????? supagetti, ???????
supagettii, ?????? supagettei, ?????
supageti, ?????? supagetii, ?????
supagetei) within a newspaper corpus (Masuya-
ma et al, 2004).  
Spelling variants are also prevalent across 
character types: in theory, a word can be spelled 
using any of the character types, as we have seen 
in the example for the word 'protein' in Section 1. 
Though there are certainly preferred character 
types for spelling each word, variations are still 
very common in Japanese text and search queries. 
Alterations are particularly common among hira-
gana, katakana and kanji (e.g. ??~??~ ? sa-
ba 'mackerel'), and between katakana and Roman 
alphabet (e.g. ??????  fedekkusu fedex). 
This latter case constitutes the problem of transli-
teration, which has been extensively studied in 
the context of machine translation (e.g. Knight 
and Graehl, 1998; Bilac and Tanaka, 2004; Brill 
et al, 2001).  
2.3 Term Variation by Re-write Categories 
Table 1 shows the re-write categories of related 
terms observed in web query logs, drawing on 
our own data analysis as well as on previous 
work such as Jones et al (2006a) and Okazaki et 
al. (2008b). Categories 1 though 9 represent 
strictly synonymous relations; in addition, terms 
in Categories 1 through 5 are also similar ortho-
graphically or in pronunciation. Categories 10 
                                                 
1 In a dictionary of 200K entries, we find that on average 
each kanji character has 2.5 readings, with three characters 
(?,?,?) with as many as 11 readings. 
1485
through 12, on the other hand, specify non-
synonymous relations.  
Different sets out of these categories can be 
useful for different purposes. For example, Jones 
et al(2006a; 2006b) target al of these categories, 
as their goal is to collect related terms as broadly 
as possible for the application of sponsored 
search, i.e., mapping search queries to a small 
corpus of advertiser listings. Okazaki et al 
(2008b) define their task narrowly, to focusing 
on spelling variants and inflection, as they aim at 
building lexical resources for the specific domain 
of medical text.  
For web search, a conservative definition of 
the task as dealing only with spelling errors has 
been successful for English; a more general defi-
nition using related words for query expansion 
has been a mixed blessing as it compromises re-
trieval precision. A comprehensive review on 
this topic is provided by Baeza-Yates and Ribei-
ro-Neto (1999). In this paper, therefore, we adopt 
a working definition of the term variation identi-
fication task as including Categories 1 through 5, 
i.e., those that are synonymous and also similar 
in spelling or in pronunciation.2 This definition is 
reasonably narrow so as to make automatic dis-
covery of term variation pairs realistic, while 
covering all common cases of term variation in 
Japanese, including spelling variants and transli-
terations. It is also appropriate for the purpose of 
query expansion: because term variation defined 
in this manner is based on spelling or pronuncia-
tion similarity, their meaning and function tend 
                                                 
2 In reality, Category 3 (Inflection) is extremely rare in Jap-
anese web queries, because nouns do not inflect in Japanese, 
and most queries are nominals.  
to be completely equivalent, as opposed to Cate-
gories 6 through 9, where synonymy is more 
context- or user-dependent. This will ensure that 
the search results by query expansion will avoid 
the problem of compromised precision.  
3 Related Work 
In information retrieval, the problem of vocabu-
lary mismatch between the query and the terms 
in the document has been addressed in many 
ways, as mentioned in Section 1, achieving vary-
ing degrees of success in the retrieval task. In 
particular, our work is closely related to research 
in spelling correction for English web queries 
(e.g., Cucerzan and Brill, 2004; Ahmad and 
Kondrak, 2005; Li et al, 2006; Chen et al, 2007). 
Among these, Li et al (2006) and Chen et al 
(2007) incorporate both string and semantic simi-
larity in their discriminative models of spelling 
correction, similarly to our approach. In Li et al 
(2006), semantic similarity was computed as dis-
tributional similarity of the terms using query 
strings in the log as context. Chen et al (2007) 
point out that this method suffers from the data 
sparseness problem in that the statistics for rarer 
terms are unreliable, and propose using web 
search results as extended contextual information. 
Their method, however, is expensive as it re-
quires web search results for each query-
candidate pair, and also because their candidate 
set, generated using an edit distance function and 
phonetic similarity from query log data, is im-
practically large and must be pruned by using a 
language model. Our approach differs from these 
methods in that we exploit user query logs to 
derive semantic knowledge of terms, which is 
Categories Example in English Example in Japanese 
1. Spelling mistake aple ~ apple ???? guuguru ~ ???? gu-guru 'google' 
2. Spelling variant color ~ colour ??~??~?; ?????~??????? (Cf. Sec.2.2) 
3. Inflection matrix ~ matrices ?? tsukuru 'make' ~ ??? tsukutta 'made' 
4. Transliteration  ?????? ~ fedex 'Fedex' 
5. Abbreviation/ 
Acronym 
macintosh ~ mac ???? sekaiginkou ~ ?? segin 'World Bank'; ???
??? makudonarudo ~ ??? makku 'McDonald's' 
6. Alias republican party ~ gop ???? furansu ~ ? futsu 'France' 
7. Translation ???????? pakisutantaishikan ~ Pakistan embassy 
8. Synonym carcinoma ~ cancer ? koyomi ~ ????? karendaa 'calendar' 
9. Abbreviation 
    (user specific) 
mini ~ mini cooper ??????? kuronekoyamato ~ ???? kuroneko 
(name of a delivery service company)  
10. Generalization nike shoes ~ shoes ???? ?? shibikku buhin 'Civic parts' ~ ? ?? ku-
ruma buhin 'car parts' 
11. Specification ipod ~ ipod nano ??? toukyoueki 'Tokyo station' ~ ?????? tou-
kyouekijikokuhyou 'Tokyo station timetable' 
12. Related windows ~ microsoft ??? toyota 'Toyota' ~ ??? honda 'Honda' 
Table 1: Categories of Related Words Found in Web Search Logs 
1486
used both for the purpose of generating a candi-
date set efficiently and as features in the term 
variation identification model.  
Acquiring semantic knowledge from a large 
quantity of web query logs has become popular 
in recent years. Some use only query strings and 
their counts for learning word similarity (e.g., 
Sekine and Suzuki, 2007; Komachi and Suzuki, 
2008), while others use additional information, 
such as the user session information (i.e., a set of 
queries issued by the same user within a time 
frame, e.g., Jones et al, 2006a) or the URLs 
clicked as a result of the query (e.g., Craswell 
and Szummer, 2007; Li et al, 2008). This addi-
tional data serves as an approximation to the 
meaning of the query; we use both user session 
and click-through data for discovering term vari-
ations.  
Our work also draws on some previous work 
on string transformation, including spelling nor-
malization and transliteration. In addition to the 
simple Levenshtein distance, we also use genera-
lized string-to-string edit distance (Brill and 
Moore, 2000), which we trained on aligned kata-
kana-English word pairs in the same manner as 
Brill et al (2001). As mentioned in Section 2.2, 
our work also tries to address the individual 
problems targeted by such component technolo-
gies as Japanese katakana variation, English-to-
katakana transliteration and katakana-to-English 
back-transliteration in a unified framework.  
4 Discriminative Model of Identifying 
Term Variation 
Recent work in spelling correction (Ahmed and 
Kondrak, 2005; Li et al, 2006; Chen et al, 2007) 
and normalization (Okazaki et al, 2008b) formu-
lates the task in a discriminative framework:  
??  = argmax??gen  ? ?(?|?) 
This model consists of two components: gen(q) 
generates a list of candidates C(q) for an input 
query q, which are then ranked by the ranking 
function P(c|q). In previous work, gen(q) is typi-
cally generated by using an edit distance function 
or using a discriminative model trained for its 
own purpose (Okazaki et al, 2008b), often in 
combination with a pre-complied lexicon. In the 
current work, we generate the list of candidates 
by learning pairs of queries and their re-write 
candidates automatically from query session and 
click logs, which is far more robust and efficient 
than using edit distance functions. We describe 
our candidate generation method in detail in Sec-
tion 5.1.  
Unlike the spelling correction and normaliza-
tion tasks, our goal is to identify term variations, 
i.e., to determine whether each query-candidate 
pair (q,c) constitutes a term variation or not. We 
formulate this problem as a binary classification 
task. There are various choices of classifiers for 
such a task: we chose to build two types of clas-
sifiers that make a binary decision based on the 
probability distribution P(c|q) over a set of fea-
ture functions fi(q,c). In maximum entropy 
framework, this is defined as:  
? ? ? =
exp ???? ?, ? 
?
?=1
 exp ???? ?, ? 
?
?=1?
 
where ?1,?, ?k are the feature weights. The op-
timal set of feature weights ?* is computed by 
maximizing the log-likelihood of the training 
data. We used stochastic gradient descent for 
training the model with a Gaussian prior.   
The second classifier is built on MART 
(Friedman, 2001), which is a boosting algorithm. 
At each boosting iteration, MART builds a re-
gression tree to model the functional gradient of 
the cost function (which is cross entropy in our 
case), evaluated on all training samples.  MART 
has three main parameters: M, the total number 
of boosting iterations, L, the number of leaf 
nodes for each regression tree, and v, the learning 
rate. The optimal values of these parameters can 
be chosen based on performance on a validation 
set.  In our experiments, we found that the per-
formance of the algorithm is relatively insensi-
tive to these parameters as long as they are in a 
reasonable range: given the training set of a few 
thousand samples or more, as in our experiments, 
M=100, L=15, and v=0.1 usually give good per-
formance. Smaller trees and shrinkage may be 
used if the training data set is smaller. 
The classifiers output a binary decision ac-
cording to P(c|q): positive when P(c|q) > 0.5 and 
negative otherwise.  
5 Experiments 
5.1 Candidate Generation 
We used a set of Japanese query logs collected 
over one year period in 2007 and 2008. More 
specifically, we used two different extracts of log 
data for generating term variation candidate 
pairs:  
Query session data. From raw query logs, we 
extracted pairs of queries q1 and q2 such that they 
are (i) issued by the same user; (ii) q2 follows 
within 3 minutes of issuing q1; and (iii) q2 gener-
ated at least one click of a URL on the result 
1487
page while q1 did not result in any click. We then 
scored each query pair (q1,q2) in this subset using 
the log-likelihood ratio (LLR, Dunning, 1993) 
between q1 and q2, which measures the mutual 
dependence within the context of web search 
queries (Jones et al, 2006a). After applying an 
LLR threshold (LLR > 15) and a count cutoff 
(we used only the top 15 candidate q2 according 
to the LLR value for each q1), we obtained a list 
of 47,139,976 pairs for the 14,929,497 distinct q1, 
on average generating 3.2 candidates per q1
3. We 
took this set as comprising query-candidate pairs 
for our model, along with the set extracted by 
click-through data mining explained below.  
Click-through data. This data extract is based 
on the idea that if two queries led to the same 
URLs being repeatedly clicked, we can reasona-
bly infer that the two queries are semantically 
related. This is similar to computing the distribu-
tional similarity of terms given the context in 
which they appear, where context is most often 
defined as the words co-occurring with the terms. 
Here, the clicked URLs serve as their context.  
One challenge in using the URLs as contex-
tual information is that the contextual representa-
tion in this format is very sparse, as user clicks 
are rare events. To learn query similarities from 
incomplete click-through data, we used the ran-
dom walk algorithm similar to the one described 
in Craswell and Szummer (2007). Figure 1 illu-
strates the basic idea: initially, document ?3 has 
a click-through link consisting of query ?2 only; 
the random walk algorithm adds the link from ?3 
to ?1 , which has a similar click pattern as ?2 . 
Formally, we construct a click graph which is a 
bipartite-graph representation of click-through 
data. We use  ?? ?=1
?  to represent a set of query 
nodes and  ??  ?=1
?
 a set of document nodes. We 
further define an  ? ? ? matrix ? in which ele-
ment ???  represents the click count associated 
with  ?? ,??  . This matrix can be normalized to 
be a query-to-document transition matrix, de-
                                                 
3 We consider each query as an unbreakable term in this 
paper, so term variation is equivalent to query variation. 
noted by ?, where ??? = ?
(1)(?? |??) is the prob-
ability that ??  transits to ??  in one step. Similarly, 
we can normalize the transpose of ?  to be a 
document-to-query transition matrix, denoted by 
?, where ?? ,? = ?
(1)(??|?? ). It is easy to see that 
using ? and ? we can compute the probability of 
transiting from any node to any other node in ? 
steps. In this work, we use a simple measure 
which is the probability that one query transits to 
another in two steps, and the corresponding 
probability matrix is given by ??.  
We used this probability and ranked all pairs 
of queries in the same raw query logs as in the 
query session data described above to generate 
additional candidates for term variation pairs. 
20,308,693 pairs were extracted after applying 
the count cutoff of 5, generating on average 6.8 
candidates for 2,973,036 unique queries. 
It is interesting to note that these two data ex-
tracts are quite complementary: of all the data 
generated, only 4.2% of the pairs were found in 
both the session and click-through data. We be-
lieve that this diversity is attributable to the na-
ture of the extracts: the session data tends to col-
lect the term pairs that are issued by the same 
user as a result of conscious re-writing effort, 
such as typing error corrections and query speci-
fications (Categories 1 and 11 in Table 1), while 
the click-though data collects the terms issued by 
different users, possibly with different intentions, 
and tends to include many spelling variants, syn-
onyms and queries with different specificity 
(Categories 2, 8, 10 and 11).  
5.2 Features 
We used the same set of features for the maxi-
mum entropy and MART models, which are giv-
en in Table 2. They are divided into three main 
types: string similarity features (1-16), semantic 
similarity features (17, 18), and character type 
features (19-39). Among the string similarity 
features, half of them are based on Levenshtein 
distance applied to surface forms (1-8), while the 
other half is based on Levenshtein and string-to-
string edit distance metrics computed over the 
Romanized form of the query, reflecting its pro-
nunciation. The conversion into Roman charac-
ters was done deterministically for kana charac-
ters using a simple mapping table. For Romaniz-
ing kanji characters, we used the function availa-
ble from Windows IFELanguage API (version 
 
Figure 1. Random Walk Algorithm 
1488
2).4 The character equivalence table mentioned in 
the features 3,4,7,8 is a table of 643 pairs of cha-
racters that are known to be equivalent, including 
kanji allography (same kanji in different graphi-
cal styles). The alpha-beta edit distance (11, 12, 
15, 16) is the string-to-string edit distance pro-
posed in Brill and Moore (2001), which we 
trained over about 60K parallel English-to-
katakana Wikipedia title pairs, specifically to 
capture the edit operations between English and 
katakana words, which are different from the edit 
operations between two Japanese words. Seman-
tic similarity features (17, 18) use the LLR score 
from the session data, and the click-though pair 
probability described in the subsection above. 
Finally, features 19-39 capture the script types of 
the query-candidate pair. We first defined six 
basic character types for each query or candidate: 
Hira (hiragana only), Kata (katakana only), Kanji 
(kanji only), Roman (Roman alphabet only), 
MixedNoKanji (includes more than one charac-
ter sets but not kanji) and Mixed (includes more 
than one character sets with kanji). We then de-
rived 21 binary features by concatenating these 
basic character type features for the combination 
                                                 
4 http://msdn.microsoft.com/en-us/library/ms970129.aspx. 
We took the one-best conversion result from the API. The 
conversion accuracy on a randomly sampled 100 kanji que-
ries was 89.6%.  
of query and candidate strings. For example, if 
both the query and candidate are in hiragana, 
BothHira will be on; if the query is Mixed and 
the candidate is Roman, then RomanMixed will 
be on. Punctuation characters and Arabic numer-
als were treated as being transparent to character 
type assignment. The addition of these features is 
motivated by the assumption that appropriate 
types of edit distance operations might depend 
on different character types for the query-
candidate pair.  
Since the dynamic ranges of different features 
can be drastically different, we normalized each 
feature dimension to a normal variable with zero-
mean and unit-variance. We then used the same 
normalized features for both the maximum en-
tropy and the MART classifiers. 
5.3 Training and Evaluation Data 
In order to generate the training data for the bi-
nary classification task, we randomly sampled 
the query session (5,712 samples) and click-
through data (6,228 samples), and manually la-
beled each pair as positive or negative: the posi-
tive label was assigned when the term pair fell 
into Categories 1 through 5 in Table 1; otherwise 
it was assigned a negative label. Only 364 (6.4%) 
and 244 (3.9%) of the samples were positive ex-
amples for the query session and click-through 
data respectively, which makes the baseline per-
String similarity features (16 real-valued features) 
1. Lev distance on surface form 
2. Lev distance on surface form normalized by q1 length 
3. Lev distance on surface form using character equivalence table 
4. Lev distance on surface form normalized by  q1 length using character equivalence table 
5. Lev distance on surface form w/o space 
6. Lev distance on surface form normalized q1 length w/o space 
7. Lev distance on surface form using  character equivalence table w/o space 
8. Lev distance on surface form normalized by q1 using character equivalence table  w/o space 
9. Lev distance on Roman 
10. Lev distance on Roman normalized by q1 length 
11. Alpha-beta edit distance on Roman 
12. Alpha-beta edit distance on Roman normalized by q1 length 
13. Lev distance  on Roman w/o space 
14. Lev distance  on Roman normalized by q1 length w/o space 
15. Alpha-beta edit distance on Roman w/o space 
16. Alpha-beta edit distance on Roman normalized by q1 length w/o space 
Features for semantic similarity (2 real-valued features) 
17. LLR score 
18. Click-though data probability 
Character type features (21 binary features) 
19. BothHira, 20. BothKata, 21. BothRoman, 22. BothKanji, 23. BothMixedNoKanji, 24. BothMixed,  
25. HiraKata, 26. HiraKanji, 27. HiraRoman, 28. HiraMixedNoKanji, 29. HiraMixed, 30. KataKanji, 
31.KataRoman, 32. KataMixedNoKanji, 33. KataMixed, 34. KanjiRoman, 35. KanjiMixedNoKanji,  
36. KanjiMixed, 37. RomanMixedNoKanji, 38. RomanMixed, 39. MixedNoKanjiMixed 
Table 2: Classifier Features 
1489
formance of the classifier quite high (always 
predict the negative label ? the accuracy will be 
95%). Note, however, that these data sets include 
term variation candidates much more efficiently 
than a candidate set generated by the standard 
method that uses an edit distance function with a 
threshold. For example, there is a query-
candidate pair q=???? kafuujouhou 'house-
style information' c= ? ? ? ?  kafunjouhou 
'pollen information') in the session data extract, 
the first one of which is likely to be a mis-
spelling of the second.5 If we try to find candi-
dates for the query ???? using an edit dis-
tance function naively with a threshold of 2 from 
the queries in the log, we end up collecting a 
large amount of completely irrelevant set of can-
didates such as ???? taifuujouhou 'typhoon 
information', ??? kabu jouhou 'stock informa-
tion', ???? kouu jouhou 'rainfall information' 
and so on ? as many as 372 candidates were 
found in the top one million most frequent que-
ries in the query log from the same period; for 
rarer queries these numbers will only be worse. 
Computing the edit distance based on the pro-
nunciation will not help here: the examples 
above are within the edit distance of 2 even in 
terms of Romanized strings.  
Another advantage of generating the annotated 
data using the result of query log data mining is 
that the annotation process is less prone to sub-
jectivity than creating the annotation from 
scratch. As Cucerzan and Brill (2004) point out, 
the process of manually creating a spelling cor-
rection candidate is seriously flawed as the inten-
tion of the original query is completely lost: for 
the query gogle, it is not clear out of context if 
the user meant goggle, google, or gogle. Using 
data mined from query logs solves this problem: 
an annotator can safely assume that if gogle-
goggle appears in the candidate set, it is very 
likely to be a valid term variation intended by the 
user. This makes the annotation more robust and 
efficient: the inter-annotator agreement rate for 
2,000 query pairs by two annotators was 95.7% 
on our data set, each annotator spending only 
about two hours to annotate 2,000 pairs.  
5.4 Results and Discussion 
In order to compare the performance of two clas-
sifiers, we first built maximum entropy and 
MART classifiers as described in Section 4 using 
                                                 
5 ???? does not make any sense in Japanese; on the 
other hand, information about cedar pollen is commonly 
sought after in spring due to widespread pollen allergy.  
all the features in Section 5.2. We run five expe-
riments using different random split of training 
and test data: in each run,  we used 10,000 sam-
ples for training and the remaining 1,940 samples 
for testing, and measured the performance of the 
two classifiers on the task of term variation iden-
tification in terms of the error rate i.e., 1?
accuracy. The results, average over five runs, 
were 4.18 for the maximum entropy model, and 
3.07 for the MART model. In all five runs, the 
MART model outperformed the maximum en-
tropy classifier. This is not surprising given the 
superior performance of tree-boosting algorithms 
previously reported on similar classification 
tasks (e.g., Hastie et al, 2001). In our task where 
different types of features are likely to perform 
better when they are combined (such as semantic 
features and character types features), MART 
would be a better fit than linear classifiers  be-
cause the decision trees generated by MART op-
timally combines features in the local sense. In 
what follows, we only discuss the results pro-
duced by MART for further experiments. Note 
that the baseline classifier, which always predicts 
the label to be negative, achieves 95.04% in ac-
curacy (or 4.96% error rate), which sounds ex-
tremely high, but in fact this baseline classifier is 
useless for the purpose of collecting term varia-
tions, as it learns none of them by classifying all 
samples as negative.  
For evaluating the contribution of different 
types of features in Section 5.2, we performed 
feature ablation experiments using MART. Table 
3 shows the results in error rate by various 
MART classifiers using different combination of 
features. The results in this table are also aver-
aged over five run with random training/test data 
split. From Table 3, we can see that the best per-
formance was achieved by the model using all 
features (line A of the table), which reduces the 
baseline error rate (4.96%) by 38%. The im-
provement is statistically significant according to 
the McNemar test (P < 0.05). Models that use 
string edit distance features only (lines B and C) 
did not perform well: in particular, the model 
that uses surface edit distance features only 
Features Error rate (%) 
A. All features (1-39 in Table 2) 3.07 
B. String features only (1-16) 3.49 
C. Surface string features only (1-8) 4.9 
D. No semantic feats (1-16,19-39) 3.28 
E. No character type feats (1-18) 3.5 
Table 3: Results of Features Ablation Experiments 
Using MART Model 
1490
without considering the term pronunciation per-
formed horribly (line C), which confirms the re-
sults reported by Jones et al (2006b). However, 
unlike Jones et al (2006b), we see a positive 
contribution of semantic features: the use of se-
mantic features reduced the error rate from 3.28 
(line D) to 3.07 (line A), which is statistically 
significant. This may be attributable to the nature 
of semantic information used in our experiments: 
we used the user session and click-though data to 
extract semantic knowledge, which may be se-
mantically more specific than the probability of 
word substitution in a query collection as a 
whole, which is used by Jones et al (2006b). 
Finally, the character type features also contri-
buted to reducing the error rate (lines A and E). 
In particular, the observation that the addition of 
semantic features without the character type fea-
tures (comparing lines B and E) did not improve 
the error rate indicates that the character type 
features are also important in bringing about the 
contribution of semantic features.   
Figure 2 displays the test data precision/recall 
curve of one of the runs of MART that uses all 
features. The x-axis of the graph is the confi-
dence score of classification P(c|q), which was 
set to 0.5 for the results in Table 3. At this confi-
dence, the model achieves 70% precision with 
the recall slightly higher than 60%. In the graph, 
we observe a familiar trade-off between preci-
sion and recall, which is useful for practical ap-
plications that may favor one over the other.  
In order to find out where the weaknesses of 
our classifiers lie, we performed a manual error 
analysis on the same MART run whose results 
are shown in Figure 2. Most of the classification 
errors are false negatives, i.e., the model failed to 
predict a case of term variation as such. The most 
conspicuous error is the failure to capture ab-
breviations, such as failing to capture the altera-
tion between ?????  juujoochuugakkou 
'Juujoo middle school' and ??? juujoochuu, 
which our edit distance-based features fail as the 
length difference between a term and its abbrevi-
ation is significant. Addition of more targeted 
features for this subclass of term variation (e.g., 
Okazaki et al, 2008a) is called for, and will be 
considered in future work. Mistakes in the Ro-
manization of kanji characters were not always 
punished as the query and the candidate string 
may contain the same mistake, but when they 
occurred in either in the query or the candidate 
string (but not in both), the result was destruc-
tive: for example, we assigned a wrong Romani-
zation on ??? as suiginnakari ?mercury lamp?, 
as opposed to the correct suiginntou, which caus-
es the failure to capture the alteration with ??
? suiginntou, (a misspelling of ???). Using 
N-best (N>1) candidate pronunciations for kanji 
terms or using all possible pronunciations for 
kanji characters might reduce this type of error. 
Finally, the features of our models are the edit 
distance functions themselves, rather than the 
individual edit rules or operations. Using these 
individual operations as features in the classifica-
tion task directly has been shown to perform well 
on spelling correction and normalization tasks 
(e.g., Brill and Moore, 2000; Okazaki et al, 
2008b). Okazaki et al?s (2008b) method of gene-
rating edit operations may not be viable for our 
purposes, as they assume that the original and 
candidate strings are very similar in their surface 
representation ? they target only spelling variants 
and inflection in English. One interesting future 
avenue to consider is to use the edit distance 
functions in our current model to select a subset 
of query-candidate pairs that are similar in terms 
of these functions, separately for the surface and 
Romanized forms, and use this subset to align 
the character strings in these query-candidate 
pairs as described in Brill and Moore (2000), and 
add the edit operations derived in this manner to 
the term variation identification classifier as fea-
tures.  
6 Conclusion 
In this paper we have addressed the problem of 
acquiring term variations in Japanese query logs 
for the purpose of query expansion. We generate 
term variation candidates efficiently by mining 
query log data, and our best classifier, based on 
the MART algorithm, can make use of both edit-
distance-based and semantic features, and can 
identify term variation with the precision of 70% 
at the recall slightly higher than 60%. Our next 
 
Figure 2: Precision/Recall Curve of MART 
0
10
20
30
40
50
60
70
80
90
100
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
confidence
p
r
e
c
is
io
n
/
r
e
c
a
ll
 
(
%
)
precision
recall
1491
goal is to use and evaluate the term variation col-
lected by the proposed method in an actual 
search scenario, as well as improving the per-
formance of our classifier by using individual, 
character-dependent edit operations as features in 
classification.  
 
References  
Ahmad, Farooq, and Grzegorz Kondrak. 2005. Learn-
ing a spelling error model from search query logs. 
In Proceedings of EMNLP, pp.955-962.  
Aitchison, J. and A. Gilchrist. 1987. Thesaurus Con-
struction: A Practical Manual. Second edition. 
ASLIB, London. 
Aramaki, Eiji, Takeshi Imai, Kengo Miyo, and Kazu-
hiko Ohe. 2008. Orthographic disambiguation in-
corporating transliterated probability. In Proceed-
ings of IJCNLP, pp.48-55. 
Baeza-Yates, Ricardo, and Berthier Ribeiro-Neto. 
1999. Modern Information Retrieval. Addison 
Wesley. 
Berger, A.L., S. A. D. Pietra, and V. J. D. Pietra. 1996. 
A maximum entropy approach to natural language 
processing. Computational Linguistics, 22(1): 39-
72. 
Bilac, Slaven, and Hozumi Tanaka. 2004. A hybrid 
back-transliteration system for Japanese. In Pro-
ceedings of COLING, pp.597-603. 
Brill, Eric, Gary Kacmarcik and Chris Brockett. 2001. 
Automatically harvesting katakana-English term 
pairs from search engine query logs. In Proceed-
ings of the Sixth Natural Language Processing Pa-
cific Rim Symposium (NLPRS-2001), pp.393-399. 
Brill, Eric, and Robert C. Moore. 2000. An improved 
error model for noisy channel spelling. In Proceed-
ings of ACL, pp.286-293. 
Chen, Qing, Mu Li and Ming Zhou. 2007. Improving 
query spelling correction using web search results. 
In Proceedings of EMNLP-CoNLL, pp.181-189. 
Craswell, Nick, and Martin Szummer. 2007. Random 
walk on the click graph. In Proceedings of SIGIR. 
Cucerzan, Silviu, and Eric Brill. 2004. Spelling cor-
rection as an iterative process that exploits the col-
lective knowledge of web users. In Proceedings of 
EMNLP, pp.293-300. 
Deerwester, S., S.T. Dumais, T. Landauer and 
Harshman. 1990. Indexing by latent semantic anal-
ysis. In Journal of the American Society for Infor-
mation Science, 41(6): 391-407. 
Dunning, Ted. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational 
Linguistics, 19(1): 61-74. 
Frakes, W.B. 1992. Stemming algorithm. In 
W.B.Frakes and R.Baeza-Yates (eds.), Information 
Retrieval: Data Structure and Algorithms, Chapter 
8. Prentice Hall. 
Friedman, J. 2001. Greedy function approximation: a 
gradient boosting machine. Annals of Statistics, 
29(5). 
Jones, Rosie, Benjamin Rey, Omid Madani and Wiley 
Greiner. 2006a. Generating query substitutions. In 
Proceedings of WWW, pp.387?396. 
Jones, Rosie, Kevin Bartz, Pero Subasic and Benja-
min Rey. 2006b. Automatically generating related 
aueries in Japanese. Language Resources and 
Evaluation 40: 219-232.  
Hastie, Trevor, Robert Tibshirani and Jerome Fried-
man. 2001. The Elements of Statistical Learning. 
Springer. 
Knight, Kevin, and Jonathan Graehl. 1998. Machine 
transliteration. Computational Linguistics, 24(4): 
599-612. 
Komachi, Mamoru and Hisami Suzuki. 2008. Mini-
mally supervised learning of semantic knowledge 
from query logs. In Proceedings of IJCNLP, 
pp.358?365. 
Li, Mu, Muhua Zhu, Yang Zhang and Ming Zhou. 
2006. Exploring distributional similarity based 
models for query spelling correction. In Proceed-
ings of COLING/ACL, pp.1025-1032. 
Li, Xiao, Ye-Yi Wang and Alex Acero. 2008. Learn-
ing query intent from regularized click graphs. In 
Proceedings of SIGIR.  
Masuyama, Takeshi, Satoshi Sekine, and Hiroshi Na-
kagawa. 2004. Automatic construction of Japanese 
katakana variant list from large corpus. In Proceed-
ings COLING, pp.1214-1219. 
Okazaki, Naoaki, Mitsuru Ishizuka and Jun?ichi Tsujii. 
2008a. A discriminative approach to Japanese ab-
breviation extraction. In Proceedings of IJCNLP.  
Okazaki, Naoaki, Yoshimasa Tsuruoka, Sophia Ana-
niadou and Jun?ichi Tsujii. 2008b. A discriminative 
candidate generator for string transformations. In 
Proceedings of EMNLP.  
Rasmussen, E. 1992. Clustering algorithm. In 
W.B.Frakes and R.Baeza-Yates (eds.), Information 
Retrieval: Data Structure and Algorithms, Chapter 
16. Prentice Hall. 
Salton, G., and C. Buckley. 1990. Improving retrieval 
performance by relevance feedback. Journal of the 
American Society for Information Science, 41(4): 
288-297. 
Sekine, Satoshi, and Hisami Suzuki. 2007. Acquiring 
ontological knowledge from query logs. In Pro-
ceedings of WWW, pp.1223-1224 
1492
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 265?272, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Comparative Study on Language Model Adaptation Techniques      
Using New Evaluation Metrics 
 
Hisami Suzuki Jianfeng Gao 
Microsoft Research Microsoft Research Asia 
One Microsoft Way  49 Zhichun Road, Haidian District 
Redmond WA 98052 USA Beijing 100080 China 
hisamis@microsoft.com jfgao@microsoft.com 
  
Abstract 
This paper presents comparative experimen-
tal results on four techniques of language 
model adaptation, including a maximum a 
posteriori (MAP) method and three dis-
criminative training methods, the boosting 
algorithm, the average perceptron and the 
minimum sample risk method, on the task of 
Japanese Kana-Kanji conversion. We evalu-
ate these techniques beyond simply using 
the character error rate (CER): the CER re-
sults are interpreted using a metric of do-
main similarity between background and 
adaptation domains, and are further evalu-
ated by correlating them with a novel metric 
for measuring the side effects of adapted 
models. Using these metrics, we show that 
the discriminative methods are superior to a 
MAP-based method not only in terms of 
achieving larger CER reduction, but also of 
being more robust against the similarity of 
background and adaptation domains, and 
achieve larger CER reduction with fewer 
side effects.  
1 Introduction 
Language model (LM) adaptation attempts to ad-
just the parameters of a LM so that it performs well 
on a particular (sub-)domain of data. Currently, 
most LMs are based on the Markov assumption 
that the prediction of a word depends only on the 
preceding n?1 words, but such n-gram statistics are 
known to be extremely susceptible to the charac-
teristics of training samples. This is true even when 
the data sources are supposedly similar: for exam-
ple, Rosenfeld (1996) showed that perplexity dou-
bled when a LM trained on the Wall Street Journal 
(1987-1989) was tested on the AP newswire stories 
of the same period. This observation, coupled with 
the fact that training data is available in large quan-
tities only in selected domains, facilitates the need 
for LM adaptation.  
There have been two formulations of the LM 
adaptation problem. One is the within-domain ad-
aptation, in which adapted LMs are created for 
different topics in a single domain (e.g., Seymore 
and Rosenfeld, 1997; Clarkson and Robinson, 
1997; Chen et al, 1998). In these studies, a domain 
is defined as a body of text originating from a sin-
gle source, and the main goal of LM adaptation is 
to fine-tune the model parameters so as to improve 
the LM performance on a specific sub-domain (or 
topic) using the training data at hand.  
The other formulation, which is the focus of the 
current study, is to adapt a LM to a novel domain, 
for which only a very small amount of training 
data is available. This is referred to as cross-
domain adaptation. Following Bellegarda (2001), 
we call the domain used to train the original model 
the background domain, and the novel domain 
with a small amount of training data as the adapta-
tion domain. Two major approaches to cross-
domain adaptation have been investigated: maxi-
mum a posteriori (MAP) estimation and discrimi-
native training methods. In MAP estimation 
methods, adaptation data is used to adjust the pa-
rameters of the background model so as to maxi-
mize the likelihood of the adaptation data. Count 
merging and linear interpolation of models are the 
two MAP estimation methods investigated in 
speech recognition experiments (Iyer et al, 1997; 
Bacchiani and Roark, 2003), with count merging 
reported to slightly outperform linear interpolation. 
Discriminative approaches to LM adaptation, on 
the other hand, aim at using the adaptation data to 
directly minimize the errors on the adaptation data 
made by the background model. These techniques 
have been applied successfully to the task of lan-
guage modeling in non-adaptation (Roark et al, 
265
2004) as well as adaptation (Bacchiani et al, 2004) 
scenarios.  
In this paper, we present comparative experi-
mental results on four language model adaptation 
techniques and evaluate them from various angles, 
attempting to elucidate the characteristics of these 
models. The four models we compare are a maxi-
mum a posteriori (MAP) method and three dis-
criminative training methods, namely the boosting 
algorithm (Collins, 2000), the average perceptron 
(Collins, 2002) and the minimum sample risk 
method (Gao et al, 2005). Our evaluation of these 
techniques is unique in that we go beyond simply 
comparing them in terms of character error rate 
(CER): we use a metric of distributional similarity 
to measure the distance between background and 
adaptation domains, and attempt to correlate it with 
the CER of each adaptation method. We also pro-
pose a novel metric for measuring the side effects 
of adapted models using the notion of backward 
compatibility, which is very important from a soft-
ware deployment perspective.  
Our experiments are conducted in the setting of 
Japanese Kana-Kanji conversion, as we believe 
this task is excellently suited for evaluating LMs. 
We begin with the description of this task in the 
following section.  
2 Language Modeling in the Task of IME 
This paper studies language modeling in the con-
text of Asian language (e.g., Chinese or Japanese) 
text input. The standard method for doing this is 
that the users first input the phonetic strings, which 
are then converted into the appropriate word string 
by software. The task of automatic conversion has 
been the subject of language modeling research in 
the context of Pinyin-to-Character conversion in 
Chinese (Gao et al, 2002a) and Kana-Kanji con-
version in Japanese (Gao et al, 2002b). In this pa-
per, we call the task IME (Input Method Editor), 
based on the name of the commonly used Win-
dows-based application.  
The performance of IME is typically measured 
by the character error rate (CER), which is the 
number of characters wrongly converted from the 
phonetic string divided by the number of charac-
ters in the correct transcript. Current IME systems 
exhibit about 5-15% CER on real-world data in a 
wide variety of domains.  
In many ways, IME is a similar task to speech 
recognition. The most obvious similarity is that 
IME can also be viewed as a Bayesian decision 
problem: let A be the input phonetic string (which 
corresponds to the acoustic signal in speech); the 
task of IME is to choose the most likely word 
string W* among those candidates that could have 
been converted from A: 
)|()(maxarg)|(maxarg*
)()(
WAPWPAWPW
AWAW GENGEN ??
==
 
(1) 
where GEN(A) denotes the candidate set given A.  
Unlike speech recognition, however, there is no 
acoustic ambiguity in IME, because the phonetic 
string is provided directly by users. Moreover, we 
can assume a unique mapping from W to A in IME, 
i.e., P(A|W) = 1. So the decision of Equation (1) 
depends solely on P(W), which makes IME ideal 
for testing language modeling techniques. Another 
advantage of using IME for language modeling 
research is that it is relatively easy to convert W to 
A, which facilitates the creation of training data for 
discriminative learning, as described later.  
From the perspective of LM adaptation, IME 
faces the same problem speech recognition faces: 
the quality of the model depends heavily on the 
similarity of the training and test data. This poses a 
serious challenge to IME, as it is currently the most 
widely used method of inputting Chinese or Japa-
nese characters, used by millions of users for in-
putting text of any domain. LM adaptation in IME 
is therefore an imminent requirement for improv-
ing user experience, not only as we build static 
domain-specific LMs, but also in making online 
user adaptation possible in the future.  
3 Discriminative Algorithms for LM Ad-
aptation 
This section describes three discriminative training 
methods we used in this study. For a detailed de-
scription of each algorithm, readers are referred to 
Collins (2000) for the boosting algorithm, Collins 
(2002) for perceptron learning, and Gao et al 
(2005) for the minimum sample risk method. 
3.1 Definition 
The following set-up, adapted from Collins (2002), 
was used for all three discriminative training meth-
ods:  
266
?  Training data is a set of input-output pairs. In the 
task of IME, we have training samples {Ai, WiR}, 
for i = 1?M, where each Ai is an input phonetic 
string and each WiR is the reference transcript of Ai. 
?  We assume a set of D + 1 features fd(W), for d = 
0?D. The features could be arbitrary functions 
that map W to real values. Using vector notation, 
we have f(W)??D+1, where f(W) = {f0(W), f1(W), 
?, fD(W)}. The feature f0(W) is called the base 
model feature, and is defined as the log probability 
that the word trigram model assigns to W. The fea-
tures fd(W) for d = 1?D are defined as the word n-
gram counts (n = 1 and 2 in our experiments) in W. 
?  The parameters of the model form a vector of D 
+ 1 dimensions, one for each feature function, ?= 
{?0, ?1, ?, ?D}. The likelihood score of a word 
string W can then be written as 
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2) 
Given a model ? and an input A, the decision rule 
of Equation (1) can then be rewritten as 
).,(maxarg),(* ??
GEN
WScoreAW
(A)W ?
=
 (3) 
We can obtain the number of conversion errors in 
W by comparing it with the reference transcript WR 
using an error function Er(WR,W), which is an edit 
distance in our case. We call the sum of error 
counts over the training set the sample risk (SR). 
Discriminative training methods strive to optimize 
the parameters of a model by minimizing SR, as in 
Equation (4). 
?
=
==
Mi
ii
R
i AWWSR
...1
* )),(,Er(minarg)(minarg ???
??
 
(4) 
However, (4) cannot be optimized directly by regu-
lar gradient-based procedures as it is a piecewise 
constant function of ? and its gradient is undefined. 
The discriminative training methods described be-
low differ in how they achieve the optimization: 
the boosting and perceptron algorithms approxi-
mate SR by loss functions that are suitable for op-
timization; the minimum sample risk method, on 
the other hand, uses a simple heuristic training pro-
cedure to minimize SR directly without resorting 
to an approximated loss function. 
3.2 The boosting algorithm  
The boosting algorithm we used is based on 
Collins (2000). Instead of measuring the number of 
conversion errors directly, it uses a loss function 
that measures the number of ranking errors, i.e., 
cases where an incorrect candidate W receives a 
higher score than the correct conversion WR. The 
margin of the pair (WR, W) with respect to the 
model ? is given by 
),(),(),( ?? WScoreWScoreWWM RR ?=
 
(5) 
The loss function is then defined as 
 ? ?
= ?
=
Mi iAiW
i
R
i WWMI
...1 )(
)],([)RLoss(
GEN
?
 (6) 
where I[?] = 1 if ? ? 0, and 0 otherwise. Note that 
RLoss takes into account all candidates in GEN(A).  
Since optimizing (6) is NP-complete, the boost-
ing algorithm optimizes its upper bound:  
? ?
= ?
?=
Mi AW
i
R
i
ii
WWM
...1 )(
)),(exp()ExpLoss(
GEN
?
 
(7) 
Figure 1 summarizes the boosting algorithm we 
used. After initialization, Step 2 and 3 are repeated 
N times; at each iteration, a feature is chosen and 
its weight is updated. We used the following up-
date for the dth feature fd:  
ZC
ZC
d
d
d ?
??
+
+
=
+
_log2
1
 (8) 
where Cd+ is a value increasing exponentially with 
the sum of margins of (WR, W) pairs over the set 
where fd is seen in WR but not in W; Cd-  is the value 
related to the sum of margins over the set where fd 
is seen in W but not in WR. ? is a smoothing factor 
(whose value is optimized on held-out data) and Z 
is a normalization constant. 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Select a feature fd which has largest estimated im-
pact on reducing ExpLoss of Equation (7) 
3 Update ?d by Equation (8), and return to Step 2 
Figure 1: The boosting algorithm 
3.3 The perceptron algorithm 
The perceptron algorithm can be viewed as a form 
of incremental training procedure that optimizes a 
minimum square error (MSE) loss function, which 
is an approximation of SR (Mitchell, 1997). As 
shown in Figure 2, it starts with an initial parame-
ter setting and updates it for each training sample. 
We used the average perceptron algorithm of 
Collins (2002) in our experiments, a variation that 
has been proven to be more effective than the stan-
dard algorithm shown in Figure 2. Let ?dt,i be the 
267
value for the dth parameter after the ith training 
sample has been processed in pass t over the train-
ing data. The average parameters are defined as  
)./()()(
1 1
, MT
T
t
M
i
it
davgd ?= ??
= =
??  (9) 
3.4 The minimum sample risk method 
The minimum sample risk (MSR, Gao et al, 2005) 
training algorithm is motivated by analogy with the 
feature selection procedure for the boosting algo-
rithm (Freund et al, 1998). It is a greedy procedure 
for selecting a small subset of the features that 
have the largest contribution in reducing SR in a 
sequential manner. Conceptually, MSR operates 
like any multidimensional function optimization 
approach: a direction (i.e., feature) is selected and 
SR is minimized along that direction using a line 
search, i.e., adjusting the parameter of the selected 
feature while keeping all other parameters fixed. 
This is repeated until SR stops decreasing.  
Regular numerical line search algorithms cannot 
be applied directly because, as described above, 
the value of a feature parameter versus SR is not 
smooth and there are many local minima. MSR 
thus adopts the method proposed by Och (2003). 
Let GEN(A) be the set of n-best candidate word 
strings that could be converted from A. By adjust-
ing ?d for a selected feature fd, we can find a set of 
intervals for ?d within which a particular candidate 
word string is selected. We can compute Er(.) for 
the candidate and use it as the Er(.) value for the 
corresponding interval. As a result, we obtain an 
ordered sequence of Er(.) values and a correspond-
ing sequence of ? intervals for each training sample. 
By summing Er(.) values over all training samples, 
we obtain a global sequence of SR and the corre-
sponding global sequence of ?d intervals. We can 
then find the optimal ?d as well as its correspond-
ing SR by traversing the sequence. 
Figure 3 summarizes the MSR algorithm. See 
Gao et al (2005) for a complete description of the 
MSR implementation and the empirical justifica-
tion for its performance.   
4 Experimental Results 
4.1 Data  
The data used in our experiments come from five 
distinct sources of text. A 36-million-word Nikkei 
newspaper corpus was used as the background 
domain. We used four adaptation domains: Yomi-
uri (newspaper corpus), TuneUp (balanced corpus 
containing newspaper and other sources of text), 
Encarta (encyclopedia) and Shincho (collection of 
novels). The characteristics of these domains are 
measured using the information theoretic notion of 
cross entropy, which is described in the next sub-
section.  
 For the experiment of LM adaptation, we used 
the training data consisting of 8,000 sentences and 
test data of 5,000 sentences from each adaptation 
domain. Another 5,000-sentence subset was used 
as held-out data for each domain, which was used 
to determine the values of tunable parameters. All 
the corpora used in our experiments are pre-
segmented into words using a baseline lexicon 
consisting of 167,107 entries.  
4.2 Computation of domain characteristics 
Yuan et al (2005) introduces two notions of do-
main characteristics: a within-domain notion of 
diversity, and a cross-domain concept of similarity. 
Diversity is measured by the entropy of the corpus 
and indicates the inherent variability within the 
domain. Similarity, on the other hand, is intended 
to capture the difficulty of a given adaptation task, 
and is measured by the cross entropy.  
For the computation of these metrics, we ex-
tracted 1 million words from the training data of 
each domain respectively, and created a lexicon 
consisting of the words in our baseline lexicon plus 
all words in the corpora used for this experiment 
(resulting in 216,565 entries) to avoid the effect of 
out-of-vocabulary items. Given two domains A and 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 For t = 1?T (T = the total number of iterations) 
3 
   For each training sample (Ai, WiR), i = 1?M 
4 
      Choose the best candidate Wi from GEN(Ai)   
      according to Equation (3) 
5 
      For each ?d (? = size of learning step) 
6 
          ?d = ?d + ?(fd(WiR) ? fd(Wi))          
Figure 2: The perceptron algorithm 
1 Set ?0 = 1 and ?d = 0 for d=1?D 
2 Rank all features by its expected impact on reduc-
ing SR and select the top N features 
3 For each n = 1?N  
4    Update the parameter of f using line search  
Figure 3: The MSR algorithm 
268
B, we then trained a word trigram model for each 
domain B, and used the resulting model in comput-
ing the cross entropy of domain A. For simplicity, 
we denote this as H(A,B).  
Table 1 summarizes our corpora along this di-
mension. Note that the cross entropy is not sym-
metric, i.e., H(A,B) is not necessarily the same as 
H(B,A), so we only present the average cross en-
tropy in Table 1. We can observe that Yomiuri and 
TuneUp are much more similar to the background 
Nikkei corpus than Encarta and Shincho.  
H(A,A) along the diagonal of Table 1 (in bold-
face) is the entropy of the corpus, indicating the 
corpus diversity. This quantity indeed reflects the 
in-domain variability of text: newspaper and ency-
clopedia articles are highly edited text, following 
style guidelines and often with repetitious content. 
In contrast, Shincho is a collection of novels, on 
which no style or content restriction is imposed. 
We use these metrics in the interpretation of CER 
results in Section 5. 
4.3 Results of LM adaptation 
The discriminative training procedure was carried 
out as follows: for each input phonetic string A in 
the adaptation training set, we produced a word 
lattice using the baseline trigram models described 
in Gao et al (2002b). We kept the top 20 hypothe-
ses from this lattice as the candidate conversion set 
GEN(A). The lowest CER hypothesis in the lattice 
rather than the reference transcript was used as WR. 
We used unigram and bigram features that oc-
curred more than once in the training set.  
We compared the performance of discriminative 
methods against a MAP estimation method as the 
baseline, in this case the linear interpolation 
method. Specifically, we created a word trigram 
model using the adaptation data for each domain, 
which was then linearly interpolated at the word 
level with the baseline model. The probability ac-
cording to the combined model is given by 
)|()1()|()|( hwPhwPhwp iAiBi ?? ?+= ,  
where PB is the probability of the background 
model, PA the probability of the adaptation model, 
and the history h corresponds to two preceding 
words. ? was tuned using the held-out data.  
In evaluating both MAP estimation and dis-
criminative models, we used an N-best rescoring 
approach. That is, we created N best hypotheses 
using the baseline trigram model (N=100 in our 
experiments) for each sentence in the test data, and 
used adapted models to rescore the N-best list. The 
oracle CERs (i.e., the minimal possible CER given 
the available hypotheses) ranged from 1.45% to 
5.09% depending on the adaptation domain.  
The results of the experiments are shown in Ta-
ble 2. We can make some observations from the 
table. First, all discriminative methods signifi-
cantly outperform the linear interpolation (statisti-
cally significant according to the t-test at p < 0.01). 
In contrast, the differences among three discrimi-
native methods are very subtle and most of them 
are not statistically significant. Secondly, the CER 
results correlate well with the metric of domain 
similarity in Table 1 (r=0.94 using the Pearson 
product moment correlation coefficient). This is 
consistent with our intuition that the closer the ad-
aptation domain is to the background domain, the 
easier the adaptation task.  
Regarding the similarity of the adaptation do-
main to the background, we also observe that the 
CER reduction of the linear interpolation model is 
particularly limited when the adaptation domain is 
similar to the background domain: the CER reduc-
tion of the linear interpolation model for Yomiuri 
and TuneUp over the baseline is 0% and 1.89% 
respectively, in contrast to ~22% and ~5.8% im-
provements achieved by the discriminative models. 
The discriminative methods are therefore more 
robust against the similarity of the adaptation and 
background data than the linear interpolation.  
Our results differ from Bacchiani et al (2004) in 
that in our system, the perceptron algorithm alone 
achieved better results than MAP estimation. 
However, the difference may only be apparent, 
given different experimental settings for the two 
 N Y T E S 
Nikkei 3.94 7.46 7.65 9.81 10.10 
Yomiuri  4.09 7.82 8.96 9.29 
TuneUp   4.41 8.82 8.56 
Encarta    4.40 9.20 
Shincho     4.61 
Table 1: Cross entropy 
Domain Base LI MSR Boost Percep 
Yomiuri 3.70 3.69 2.89 2.88 2.85 
TuneUp 5.81 5.70 5.48 5.47 5.47 
Encarta 10.24 8.64 8.39 8.54 8.34 
Shincho 12.18 11.47 11.05 11.09 11.20 
Table 2: CER results (%) (Base=baseline model; 
LI=linear interpolation) 
269
studies. We used the N-best reranking approach 
with the same N-best list for both MAP estimation 
and discriminative training, while in Bacchiani et 
al. (2004), two different lattices were used: the per-
ceptron model was applied to rerank the lattice 
created by the background model, while the MAP 
adaptation model was used to produce the lattice 
itself. The fact that the combination of these mod-
els (i.e., first use the MAP estimation to create hy-
potheses and then use the perceptron algorithm to 
rerank them) produced the best results indicates 
that given a candidate lattice, the perceptron algo-
rithm is effective in candidate reranking, thus mak-
ing our results compatible with theirs. 
5 Discussion 
The results in Section 4 demonstrate that discrimi-
native training methods for adaptation are overall 
superior to MAP adaptation methods. In this sec-
tion, we show additional advantages of discrimina-
tive methods beyond simple CER improvements.   
5.1 Using metrics for side effects  
In the actual deployment of LM adaptation, one 
issue that bears particular importance is the num-
ber of side effects that are introduced by an 
adapted model. For example, consider an adapted 
model which achieves 10% CER improvements 
over the baseline. Such a model can be obtained by 
improving 10%, or by improving 20% and by in-
troducing 10% of new errors. Clearly, the former 
model is preferred, particularly if the models be-
fore and after adaptation are both to be exposed to 
users. This concept is more widely acknowledged 
within the software industry as backward compati-
bility ? a requirement that an updated version of 
software supports all features of its earlier versions. 
In IME, it means that all phonetic strings that can 
be converted correctly by the earlier versions of the 
system should also be converted correctly by the 
new system as much as possible. Users are typi-
cally more intolerant to seeing errors on the strings 
that used to be converted correctly than seeing er-
rors that also existed in the previous version. 
Therefore, it is crucial that when we adapt to a new 
domain, we do so by introducing the smallest 
number of side effects, particularly in the case of 
an incremental adaptation to the domain of a par-
ticular user, i.e., to building a model with incre-
mental learning capabilities.   
5.2 Error ratio 
In order to measure side effects, we introduce the 
notion of error ratio (ER), which is defined as  
||
||
B
A
E
EER = , 
 
where |EA| is the number of errors found only in the 
new (adaptation) model, and |EB| the number of 
errors corrected by the new model. Intuitively, this 
quantity captures the cost of improvement in the 
adaptation model, corresponding to the number of 
newly introduced errors per each improvement. 
The smaller the ratio is, the better the model is at 
the same CER: ER=0 if the adapted model intro-
duces no new errors, ER<1 if the adapted model 
makes CER improvements, ER=1 if the CER im-
provement is zero (i.e., the adapted model makes 
as many new mistakes as it corrects old mistakes), 
and ER>1 when the adapted model has worse CER 
performance than the baseline model.  
Given the notion of CER and ER, a model can 
be plotted on a graph as in Figure 4: the relative 
error reduction (RER, i.e., the CER difference be-
tween the background and adapted models) is plot-
ted along the x-axis, and ER along the y-axis. 
Figure 4 plots the models obtained after various 
numbers of iterations for MSR training and at vari-
ous interpolation weights for linear interpolation 
for the TuneUp domain. The points in the upper-
left quadrant, ER>1 and RER<0, are the models 
that performed worse than the baseline model 
(some of the interpolated models fall into this cate-
gory); the shaded areas (upper-right and lower-left 
quadrants) are by definition empty. The lower-
right quadrant is the area of interest to us, as they 








     
	


 
Figure 4: RER/ER plot for MSR and LI models for 
TuneUp domain 
270
represent the models that led to CER improve-
ments; we will focus only on this area now in 
Figure 5. 
In this figure, a model is considered to have 
fewer side effects when the ER is smaller at the 
same RER (i.e., smaller value of y for a fixed value 
of x), or when the RER is larger at the same ER 
(i.e., larger value of x at the fixed y). That is, the 
closer a model is plotted to the corner B of the 
graph, the better the model is; the closer it is to the 
corner A, the worse the model is.  
5.3 Model comparison using RER/ER 
From Figure 5, we can clearly see that MSR mod-
els have better RER/ER-performance than linear 
interpolation models, as they are plotted closer to 
the corner B. Figure 6 displays the same plot for all 
four domains: the same trend is clear from all 
graphs. We can therefore conclude that a discrimi-
native method (in this case MSR) is superior to 
linear interpolation not only in terms of CER re-
duction, but also of having fewer side effects. This 
desirable result is attributed to the nature of dis-
criminative training, which works specifically to 
adjust feature weights so as to minimize error.  
 
Figure 7: RER/ER plot for MSR, boosting and percep-
tron models (X-axis is normalized to represent relative 
error rate reduction) 
Figure 7 compares the three discriminative 
models with respect to RER/ER by plotting the 
best models (i.e., models used to produce the re-
sults in Table 1) for each algorithm. We can see 
that even though the boosting and perceptron algo-
rithms have the same CER for Yomiuri and 
TuneUp from Table 2, the perceptron is better in 
terms of ER; this may be due to the use of expo-
nential loss function in the boosting algorithm 
which is less robust against noisy data (Hastie et al, 
2001). We also observe that Yomiuri and Encarta 
do better in terms of side effects than TuneUp and 
Shincho for all algorithms, which can be explained 
by corpus diversity, as the former set is less stylis-
tically diverse and thus more consistent within the 
domain.  
5.4 Overfitting and side effects 
The RER/ER graph also casts the problem of over-
fitting in an interesting perspective. Figure 8 is de-
rived from running MSR on the TuneUp test 
corpus, which depicts a typical case of overfitting: 
the CER drops in the beginning, but after a certain 
number of iterations, it goes up again. The models 
indicated by ? and ? in the graph are of the same 
CER, and as such, these models are equivalent. 
When plotted on the RER/ER graph in Figure 5, 











    
	




 
Figure 5: RER/ER plot for the models with ER<1 and 
RER>0 for TuneUp domain. See Figure 8 for the de-
scription of ? and ?  











          











       











      











         

Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 8?9,
Vancouver, October 2005.
MindNet: an automatically-created lexical resource 
 
Lucy Vanderwende, Gary Kacmarcik, Hisami Suzuki, Arul Menezes 
Microsoft Research 
Redmond, WA 98052, USA 
{lucyv, garykac, hisamis, arulm}@microsoft.com 
 
Abstract 
We will demonstrate MindNet, a lexical resource 
built automatically by processing text.  We will 
present two forms of MindNet: as a static lexical 
resource, and, as a toolkit which allows MindNets 
to be built from arbitrary text.  We will also intro-
duce a web-based interface to MindNet lexicons 
(MNEX) that is intended to make the data con-
tained within MindNets more accessible for explo-
ration.  Both English and Japanese MindNets will 
be shown and will be made available, through 
MNEX, for research purposes. 
1 MindNet 
A MindNet is a collection of semantic relations 
that is automatically extracted from text data using 
a broad coverage parser. Previous publications on 
MindNet (Suzuki et al, 2005, Richardson et al, 
1998, Vanderwende 1995) have focused on the 
effort required to build a MindNet from the data 
contained in Japanese and English lexicons. 
Semantic Relations 
The semantic relations that are stored in MindNet 
are directed, labeled relationships between two 
words; see Table 1:  
Attributive Manner Source 
Cause Means Synonym 
Goal Part Time 
Hypernym Possessor TypicalObject 
Location Result TypicalSubject 
Table 1: A sampling of the semantic relations stored in 
MindNet 
 
These semantic relations are obtained from the 
Logical Form analysis of our broad coverage 
parser NLPwin (Heidorn, 2000).  The Logical 
Form is a labeled dependency analysis with func-
tion words removed.  We have not completed an 
evaluation of the quality of the extracted semantic 
relations.  Anecdotally, however, the quality varies 
according to the relation type, with Hypernym and 
grammatical relations TypicalSubject and Typi-
calObj being reliable, while relations such as Part 
and Purpose are less reliable. By making MindNet 
available, we solicit feedback on the utility of these 
labeled relationships, especially in contrast to sim-
ple co-occurrence statistics and to the heavily used 
hypernymy and synonymy links. Furthermore, we 
solicit feedback on the level of accuracy which is 
tolerable for specific applications. 
Semantic Relation Structures 
We refer to the hierarchical collection of semantic 
relations (semrels) that are automatically extracted 
from a source sentence as a semrel structure. Each 
semrel structure contains all of the semrels ex-
tracted from a single source sentence.  A semrel 
structure can be viewed from the perspective of 
each unique word that occurs in the structure; we 
call these inverted structures.  They contain the 
same information as the original, but with a differ-
ent word placed at the root of the structure. An ex-
ample semrel structure for the definition of 
swallow is given in Figure 1a, and its inversion, 
from the perspective of wing is given in Figure 1b: 
 
swallow           wing 
 Hyp bird           PartOf bird 
       Part wing             Attrib small 
       Attrib small          HypOf swallow 
 
Figure 1a and b: Figure 1a is the semrel structure for the 
definition of swallow1, Figure 1b the inversion on wing. 
2 MNEX 
MNEX (MindNet Explorer) is the web-based inter-
face to MindNet that is designed to facilitate 
browsing MindNet structure and relations. MNEX 
displays paths based on the word or words that the 
                                                          
1
 Swallow: a small bird with wings (LDOCE).  Definition 
abbreviated for purposes of exposition.   
8
user enters. A path is a set of links that connect one 
word to another within either a single semrel struc-
ture or by combining fragments from multiple 
semrel structures.  Paths are weighted for compari-
son (Richardson, 1997). Currently, either one or 
two words can be specified and we allow some 
restrictions to refine the path search.  A user can 
restrict the intended part of speech of the words 
entered, and/or the user can restrict the paths to 
include only the specified relation. When two 
words are provided, the UI returns a list of the 
highest ranked paths between those two words. 
When only one word is given, then all paths from 
that word are ranked and displayed.  Figure 2 
shows the MNEX interface, and a query requesting 
all paths from the word bird, restricted to Noun 
part of speech, through the Part relation:  
 
 
Figure 2: MNEX output for ?bird (Noun) Part? query 
3 Relation to other work 
For English, WordNet is the most widely used 
knowledgebase. Aside from being English-only, 
this database was hand-coded and significant effort 
is required to create similar databases for different 
domains and languages. Projects like EuroWord-
Net address the monolingual aspect of WordNet, 
but these databases are still labor intensive to cre-
ate.  On the other hand, the quality of the informa-
tion contained in a WordNet (Fellbaum et al, 
1998) is very reliable, exactly because it was 
manually created.  FrameNet (Baker et al, 1998) 
and OpenCyc are other valuable resources for Eng-
lish, also hand-created, that contain a rich set of 
relations between words and concepts. Their use is 
still being explored as they have been made avail-
able only recently. For Japanese, there are also 
concept dictionaries providing semantic relations, 
similarly hand-created, e.g., EDR and Nihongo 
Goi-taikei (NTT). 
The demonstration of MindNet will highlight 
that this resource is automatically created, allowing 
domain lexical resources to be built quickly, albeit 
with lesser accuracy.  We are confident that this is 
a trade-off worth making in many cases, and en-
courage experimentation in this area.  MNEX al-
lows the exploration of the rich set of relations 
through which paths connecting words are linked. 
4 References 
Baker, Collin F., Fillmore, Charles J., and Lowe, John 
B. (1998): The Berkeley FrameNet project. in Pro-
ceedings of the COLING-ACL, Montreal, Canada. 
Fellbaum, C. (ed). 1998. WordNet: An Electronic Lexi-
cal Database. MIT Press. 
Heidorn, G. 2000. Intelligent writing assistance. in 
R.Dale, H.Moisl and H.Somers (eds.), A Handbook 
of Natural Langauge Processing: Techniques and 
Applications for the Processing of Language as Text. 
New York: Marcel Dekker. 
National Institute of Information and Communications 
Technology. 2001. EDR Electronic Dictionary Ver-
sion 2.0 Technical Guide. 
NTT Communications Science Laboratories. 1999. Goi-
Taikei - A Japanese Lexicon. Iwanami Shoten. 
OpenCyc. Available at: http://www.cyc.com/opencyc. 
Richardson, S.D. 1997, Determining Similarity and In-
ferring Relations in a Lexical Knowledge Base. PhD. 
dissertation, City University of New York. 
Richardson, S.D., W. B. Dolan, and L. Vanderwende. 
1998. MindNet: Acquiring and Structuring Semantic 
Information from Text, In Proceedings of ACL-
COLING. Montreal, pp. 1098-1102. 
Suzuki, H., G. Kacmarcik, L. Vanderwende and A. 
Menezes. 2005. Mindnet and mnex. In Proceedings 
of the 11th Annual meeting of the Society of Natural 
Language Processing (in Japanese).  
Vanderwende, L. 1995. Ambiguity in the acquisition of 
lexical information. In Proceedings of the AAAI 
1995 Spring Symposium Series, symposium on rep-
resentation and acquisition of lexical knowledge, 
174-179. 
9
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 957 ? 968, 2005. 
? Springer-Verlag Berlin Heidelberg 2005 
An Empirical Study on Language Model Adaptation 
Using a Metric of Domain Similarity 
Wei Yuan1, Jianfeng Gao2, and Hisami Suzuki3 
1
 Shanghai Jiao Tong University, 1954 Huashan Road, Shanghai 200030 
sunnyuanovo@sjtu.edu.cn 
2
 Microsoft Research, Asia, 49 Zhichun Road, Haidian District, Beijing 100080 
jfgao@microsoft.com 
3
 Microsoft Research, One Microsoft Way, Redmond WA 98052 
hisamis@microsoft.com 
Abstract. This paper presents an empirical study on four techniques of lan-
guage model adaptation, including a maximum a posteriori (MAP) method and 
three discriminative training models, in the application of Japanese Kana-Kanji 
conversion. We compare the performance of these methods from various angles 
by adapting the baseline model to four adaptation domains. In particular, we at-
tempt to interpret the results given in terms of the character error rate (CER) by 
correlating them with the characteristics of the adaptation domain measured us-
ing the information-theoretic notion of cross entropy. We show that such a met-
ric correlates well with the CER performance of the adaptation methods, and 
also show that the discriminative methods are not only superior to a MAP-based 
method in terms of achieving larger CER reduction, but are also more robust 
against the similarity of background and adaptation domains.  
1   Introduction 
Language model (LM) adaptation attempts to adjust the parameters of a LM so that it 
performs well on a particular domain of data. This paper presents an empirical study 
of several LM adaptation methods on the task of Japanese text input. In particular, we 
focus on the so-called cross-domain LM adaptation paradigm, i.e. to adapt a LM 
trained on one domain to a different domain, for which only a small amount of train-
ing data is available. 
The LM adaptation methods investigated in this paper can be grouped into two 
categories: maximum a posterior (MAP) and discriminative training. Linear interpola-
tion is representative of the MAP methods [1]. The other three methods, including the 
boosting [2] and perceptron [3] algorithms and minimum sample risk (MSR) method 
[4], are discriminative methods, each of which uses a different training algorithm. 
We carried out experiments over many training data sizes on four distinct adapta-
tion corpora, the characteristics of which were measured using the information-
theoretic notion of cross entropy. We found that discriminative training methods  
                                                          
1
 This research was conducted while the author was visiting Microsoft Research Asia.  
958 W. Yuan, J. Gao, and H. Suzuki 
outperformed the LI method in all cases, and were more robust across different train-
ing sets of different domains and sizes. However, none of the discriminative training 
methods was found to outperform the others in our experiments.  
The paper is organized as follow. Section 2 introduces the task of IME and the role 
of LM. In Section 3, we review related work. After a description of the LM adaptation 
methods in our experiments in Section 4, Sections 5 and 6 present experimental re-
sults and their discussions. We conclude our paper in Section 7. 
2   Language Model and the Task of IME 
Our study falls into the context of Asian language (Japanese in this study) text input. 
The standard method for doing this is that the users first input the phonetic strings, 
which are then converted into the appropriate word string by software. The task of 
automatic conversion is called IME in this paper, which stands for Input Method Edi-
tor, based on the name of the commonly used Windows-based application. 
The performance of IME is typically measured in terms of the character error rate 
(CER), which is the number of characters wrongly converted from the phonetic string 
divided by the number of characters in the correct transcript. Current Japanese IME 
systems exhibit about 5-15% CER in conversion of real-world data in a wide variety 
of domains. In the following, we argue that the IME is a similar problem to speech 
recognition but is a better choice for evaluating language modeling techniques. 
Similar to speech recognition, IME can also be viewed as a Bayesian decision 
problem. Let A be the input phonetic string (which corresponds to the acoustic signal 
in speech). The task of IME is to choose the most likely word string W* among those 
candidates that could have been converted from A: 
)|()(maxarg)(
),(
maxarg)|(maxarg*
)()()(
WAPWP
AP
AWPAWPW
AGENWAGENWAGENW ???
===
 (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, there is almost no acoustic ambiguity in IME, because 
the phonetic string is provided directly by users. Moreover, we can assume a many-to-
one mapping from W to A in IME, i.e. P(A|W) = 1. So the decision of Equation (1) 
depends solely upon P(W), making IME a more direct evaluation test-bed for LM 
than speech recognition. Another advantage is that it is relatively easy to convert W to 
A, making it possible to obtain a large amount of training data for discriminative 
learning, as described later.  
3   Related Work 
Our goal is to quantify the characteristics of different domains of text, and to correlate 
them with the performance of various techniques for LM adaptation to compare their 
effectiveness and robustness. This relates our work to the study of domain similarity 
calculation and to different techniques for LM adaptation. 
 An Empirical Study on Language Model Adaptation 959 
3.1 Measuring Domain Similarity 
Statistical language modeling (SLM) assumes that language is generated from under-
lying distributions. When we discuss different domains of text, we assume that the 
text from each of these domains is generated from a different underlying distribution. 
We therefore consider the problem of distributional similarity in this paper. 
Cross entropy is a widely used measure in evaluating LM. Given a language L 
with its true underlying probability distribution p and another distribution q (e.g. a 
SLM) which attempts to model L, the cross entropy of L with respect to q is 
?
??
?=
nww
nn
n
wwqwwp
n
qLH
...
11
1
)...(log)...(1lim),(  (2) 
where w1?wn is a word string in L. However, in reality, the underlying p is never 
known and the corpus size is never infinite. We therefore make the assumption that L 
is an ergodic and stationary process [5], and approximate the cross entropy by calcu-
lating it for a sufficiently large n instead of calculating it for the limit. 
)...(log1),( 1 nwwq
n
qLH ??  (3) 
This measures how well a model approximates the language L.  
The KL divergence, or relative entropy, is another measure of distributional simi-
larity that has been widely used in NLP and IR [6]. Given the two distributions p and 
q above, the KL divergence is defined as 
?=
nww n
n
nnn
wwq
wwp
wwpwwqwwpD
...1 1
1
111 )...(
)...(log)...())...(||)...((  (4) 
The cross entropy and the KL divergence are related notions. Given the notations 
of L, p and q above, [5] shows that   
)||()(),( qpDLHqLH +=
 (5) 
In other words, the cross entropy takes into account both the similarity between two 
distributions (given by KL divergence) and the entropy of the corpus in question, both 
of which contribute to the complexity of a LM task. In this paper we are interested in 
measuring the complexity of the LM adaptation task. We therefore define the similar-
ity between two domains using the cross entropy. We will also use the metric that 
approximates the entropy of the corpus to capture the in-domain diversity of a corpus 
in Section 5.2.2  
3.2 LM Adaptation Methods 
In this paper, two major approaches to cross-domain adaptation have been investi-
gated: maximum a posteriori (MAP) estimation and discriminative training methods. 
                                                          
2
  There are other well-known metrics of similarity within NLP literature, such as the mutual 
information or cosine similarity [7], which we do not discuss in this paper.  
960 W. Yuan, J. Gao, and H. Suzuki 
In MAP estimation methods, adaptation data is used to adjust the parameters of the 
background model so as to maximize the likelihood of the adaptation data [1]. Dis-
criminative training methods to LM adaptation, on the other hand, aim at using the 
adaptation data to directly minimize the errors on the adaptation data made by the 
background model. These techniques have been applied successfully to the task of 
language modeling in non-adaptation [8] as well as adaptation scenarios [9] for speech 
recognition. But most of them focused on the investigation of performance of certain 
methods for LM adaptation, without analyzing in detail the underlying reasons of 
different performance achieved by different methods. In this paper we attempt to 
investigate the effectiveness of different discriminative methods in an IME adaptation 
scenario, with a particular emphasis on correlating their performance with the charac-
teristics of adaptation domain. 
4   LM Adaptation Methods 
We implement four methods in our experiments. The Linear Interpolation (LI) falls 
into the framework of MAP while the boosting, the perceptron and the MSR methods 
fall into that of discriminative training.  
4.1   Linear Interpolation (MAP) 
In MAP estimation methods, adaptation data is used to adjust the parameters of the 
background model so as to maximize the likelihood of the adaptation data. 
The linear interpolation is a special case of MAP according to [10]. At first, we 
generate trigram models on background data and adaptation data respectively. The 
two models are then combined into one as: 
)|()1()|()|( hwPhwPhwP iAiBi ?? ?+=  (6) 
where PB is the probability of the background model, PA is the probability of the adap-
tation model and the history h corresponds to two preceding words. For simplicity, we 
chose a single ? for all histories and tuned it on held-out data. 
4.2   Discriminative Training 
Discriminative training follows the general framework of linear models [2][3]. We use 
the following notation in the rest of the paper. 
? Training data is a set of example input/output pairs {Ai, WiR} for i = 1?M, where 
each Ai is an input phonetic string and each WiR is the reference transcript of Ai. 
? We assume a set of D+1 features, fd(W), for d=0?D, where each feature is a func-
tion that maps W to a real value. Using vector notation, we have f(W)={ f0(W), 
f1(W)?fD(W)} and f(W) .1+? DR  Without loss of generality, f0(W) is called the base 
model feature, and is defined in this paper as the log probability that the back-
ground trigram model assigns to W. fd(W), for d=1?D, are defined as the counts of 
the word n-gram in W, where n = 1 and 2 in our case. 
 An Empirical Study on Language Model Adaptation 961 
? Finally, the parameters of the model form a vector of D + 1 dimensions, each for 
one feature function, ?= {?0, ?1? ?D}. The likelihood score of a word string W is 
?
=
==
D
d
dd WfWWScore
0
)()(),( ??f?  (7) 
Then the decision rule of Equation (1) can be re-written as  
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
=
 (8) 
Assume that we can measure the number of conversion errors in W by comparing it 
with a reference transcript WR using an error function Er(WR, W), which is an edit 
distance in our case. We call the sum of conversion errors over the training data as 
sample risk (SR). Discriminative training methods strive to minimize the SR by opti-
mizing the model parameters, as defined in Equation (9). 
?
=
==
Mi
ii
R
i
* AWWErSR
...1
)),(,(minarg)(minarg ???
??
 (9) 
However, SR(.) cannot be optimized easily since Er(.) is a piecewise constant (or 
step) function of ? and its gradient is undefined. Therefore, discriminative methods 
apply different approaches that optimize it approximately. As we shall describe be-
low, the boosting and perceptron algorithms approximate SR(.) by loss functions that 
are suitable for optimization, while MSR uses a simple heuristic training procedure to 
minimize SR(.) directly without applying any approximated loss function. We now 
describe each of the discriminative methods in turn. 
The boosting algorithm [2] uses an exponential function to approximate SR(.). We 
define a ranking error in a case where an incorrect candidate conversion W gets a 
higher score than the correct conversion WR. The margin of the pair (WR, W) with 
respect to the model ? is estimated as 
),(),(),( ?? WScoreWScoreWWM RR ?=  (10) 
Then we define an upper bound to the number of ranking errors as the loss function, 
? ?
= ?
?=
Mi AW
i
R
i
ii
WWM
...1 )(
)),(exp()ExpLoss(
GEN
?  (11) 
Now, ExpLoss(.) is convex with respect to ?, so there are no problems with local 
minima when optimizing it. The boosting algorithm can be viewed as an iterative 
feature selection method: at each iteration, the algorithm selects from all possible 
features the one that is estimated to have the largest impact on reducing the ExpLoss 
function with respect to the current model, and then optimizes the current model by 
adjusting only the parameter of the selected feature while keeping the parameters of 
other features fixed. 
The perceptron algorithm [3] can be viewed as a form of incremental training pro-
cedure that optimizes a minimum square error (MSE) loss function, which is an ap-
proximation of SR(.). As shown in Figure 1, it starts with an initial parameter setting 
and adapts it each time a training sample is wrongly converted.  
962 W. Yuan, J. Gao, and H. Suzuki 
1 Initialize all parameters in the model, i.e. ?0 = 1 and ?d = 0 for d=1?D 
2 For t = 1?T, where T is the total number of iterations 
For each training sample (Ai, WiR), i = 1?M 
Use current model ? to choose some Wi from GEN(Ai) by Equation (8) 
    For d = 1 ? D 
        ?d  = ?d + ?(fd (WiR)- fd (Wi)), where ? is the size of the learning step  
Fig. 1. The standard perceptron algorithm with delta rule 
In our experiments, we used the average perceptron algorithm in [3], a simple refine-
ment to the algorithm in Figure 1, which has been proved to be more robust. Let ?dt,i 
be the value for the dth parameter after the ith training sample has been processed in 
pass t over the training data. Then the ?average parameters? are defined as in Equa-
tion (12). 
)/()()(
1 1
, MT
T
t
M
i
it
davgd ?= ??
= =
??  (12) 
The minimum sample risk (MSR) method [4] can be viewed as a greedy stage-
wise learning algorithm that minimizes the sample risk SR(.) directly as it appears in 
Equation (9). Similar to the boosting method, it is an iterative procedure. In each 
iteration, MSR selects a feature that is estimated to be most effective in terms of re-
ducing SR(.), and then optimizes the current model by adjusting the parameters of the 
selected feature. MSR, however, differs from the boosting method in that MSR tries 
to optimize the sample risk directly while the boosting optimizes the loss function that 
is an upper bound of the sample risk.  
As mentioned earlier, SR(.) can be optimized using regular gradient-based optimi-
zation algorithms. MSR therefore uses a particular implementation of line search, 
originally proposed in [11], to optimize the current model by adjusting the parameter 
of a selected feature while keeping other parameters fixed.  
Assuming fd is the selected feature, its parameter ?d is optimized by line search as 
follows. Recall that Er(WR,W) is the function that measures the number of conversion 
errors in W versus its reference transcript WR. The value of SR(.) is the sum of Er(.) 
over all training samples. For each A in training set, let GEN(A) be the set of n-best 
candidate word strings that could be converted from A. By adjusting ?d, we obtain for 
each training sample an ordered sequence of ?d intervals. For ?d in each interval, a 
particular candidate would be selected according to Equation (8). Then the corre-
sponding Er(.) is associated with the interval. As a result, for each training sample, we 
obtain a sequence of ?d intervals and their corresponding Er(.) values. By combining 
the sequences over all training samples, we obtain a global sequence of ?d intervals, 
each of which is associated with a SR(.) value. Therefore we can find the optimal 
interval of ?d as well as its corresponding sample risk by traversing the sequence and 
taking the center of the interval as the optimal value of ?d. 
 An Empirical Study on Language Model Adaptation 963 
5   Experimental Results 
5.1   Data 
The data used in our experiments stem from five distinct sources of text. A 36-
million-word Nikkei newspaper corpus was used as the background domain. We used 
four adaptation domains: Yomiuri (newspaper corpus), TuneUp (balanced corpus 
containing newspaper and other sources of text), Encarta (encyclopedia) and Shincho 
(collection of novels).  
For the computation of domain characteristics (Section 5.2), we extracted 1 million 
words from the training data of each domain respectively (corresponding to 13K to 
78K sentences depending on the domain). For this experiment, we also used a lexicon 
consisting of the words in our baseline lexicon (167,107 words) plus all words in the 
corpora used for this experiment (that is, 1M words times 5 domains), which included 
216,565 entries. The use of such a lexicon was motivated by the need to eliminate the 
effect of out-of-vocabulary (OOV) items.  
For the experiment of LM adaptation (Section 5.3), we created training data con-
sisting of 72K sentences (0.9M~1.7M words) and test data of 5K sentences 
(65K~120K words) from each adaptation domain. The first 800 and 8,000 sentences 
of each adaptation training data were also used to show how different sizes of adapta-
tion training data affected the performances of various adaptation methods. Another 
5K-sentence subset was used as held-out data for each domain. For domain adaptation 
experiments, we used our baseline lexicon consisting of 167,107 entries.   
5.2   Computation of Domain Characteristics 
The first domain characteristic we computed was the similarity between two domains 
for the task of LM. As discussed in Section 3, we used the cross entropy as the metric: 
we first trained a word trigram model using the system described in [12] on the 1-
million-word corpus of domain B, and used it in the computations of the cross entropy 
H(LA, qB) following equation (3). For simplicity, we denote H(LA, qB) as H(A,B).  
Table 1 displays the cross entropy between two domains of text. Note that the cross 
entropy is not symmetric, i.e., H(A,B) is not necessarily the same as H(B,A). In order 
to have a representative metric of similarity between two domains, we computed the 
average cross entropy between two domains, shown in Table 2, and used this quantity 
as the metric for domain similarity.   
Along the main diagonal in the tables below, we also have the cross entropy com-
puted for H(A,A), i.e., when two domains we compare are the same (in boldface). This 
value, which we call self entropy for convenience, is an approximation of the entropy 
of the corpus A, and measures the amount of information per word, i.e., the diversity 
of the corpus. Note that the self entropy increases in the order of Nikkei ? Yomiuri 
? Encarta ? TuneUp ? Shincho. This indeed reflects the in-domain variability of 
text: Nikkei, Yomiuri and Encarta are highly edited text, following style guidelines; 
they also tend to have repetitious content. In contrast, Shincho is a collection of nov-
els, on which no style or content restriction is imposed. We expect that the LM task to 
964 W. Yuan, J. Gao, and H. Suzuki 
be more difficult as the corpus is more diverse; we will further discuss the effect of 
diversity in Section 6.3 
Table 1. Cross entropy (rows: corpora; column: models) 
 Nikkei Yomiuri TuneUp Encarta Shincho 
Nikkei 3.94 7.46 7.65 9.81 10.10 
Yomiuri 7.93 4.09 7.62 9.26 9.97 
TuneUp 8.25 8.03 4.41 9.04 9.06 
Encarta 8.79 8.66 8.60 4.40 9.30 
Shincho 8.70 8.61 8.07 9.10 4.61 
Table 2. Average cross entropy 
 Nikkei Yomiuri TuneUp Encarta Shincho 
Nikkei 3.94 7.69 7.95 9.30 9.40 
Yomiuri  4.09 7.82 8.96 9.29 
TuneUp   4.41 8.82 8.56 
Encarta    4.40 9.20 
Shincho     4.61 
5.3   Results of LM Adaptation 
We trained our baseline trigram model on our background (Nikkei) corpus using the 
system described in [12]. The CER (%) of this model on each adaptation domain is in 
the second column of Table 3. For the LI adaptation method (the third column of 
Table 3), we trained a word trigram model on the adaptation data, and linearly com-
bined it with the background model, as described in Equation (6).  
For the discriminative methods (the last three columns in Table 3), we produced  a 
candidate word lattice for each input phonetic string in the adaptation training set 
using the background trigram model mentioned above. For efficiency purposes, we 
kept only the best 20 hypotheses from the lattice as the candidate conversion set for 
discriminative training. The lowest CER hypothesis in the lattice, rather than the ref-
erence transcript, was used as the gold standard.  
To compare the performances of different discriminative methods, we fixed the 
following parameter settings: we set the number of iterations N to be 2,000 for the 
boosting and MSR methods (i.e., at most 2,000 features in the final models); for the 
perceptron algorithm, we set T = 40 (in Figure 1). These settings might lead to an 
                                                          
3
  Another derivative notion from Table 1 is the notion of balanced corpus. In Table 1, the 
smallest cross entropy for each text domain (rows) is the self entropy (in boldface), as ex-
pected. Note, however, that the second smallest cross entropy (underlined) is always obtained 
from the TuneUp model (except for Nikkei, for which Yomiuri provides the second smallest 
cross entropy). This reflects the fact that the TuneUp corpus was created by collecting sen-
tences from various sources of text, in order to create a representative test corpus. Using the 
notion of cross entropy, such a characteristic of a test corpus can also be quantified.  
 An Empirical Study on Language Model Adaptation 965 
unfair comparison, as the perceptron algorithm will select far more features than the 
boosting and MSR algorithm. However, we used these settings as they all converged 
under these settings.  All other parameters were tuned empirically. 
In evaluating both MAP and discriminative methods, we used an N-best rescoring 
approach. That is, we created N best hypotheses using the background trigram model 
(N=100 in our experiments) for each sentence in test data, and used domain-adapted 
models to rescore the lattice. The oracle CERs (i.e., the minimal possible CER given 
the hypotheses in the lattice) ranged from 1.45% to 5.09% depending on the adapta-
tion domain. Table 3 below summarizes the results of various adaptation methods in 
terms of CER (%) and CER reduction (in parentheses) over the baseline model. In the 
first column, the numbers in parentheses next to the domain name indicates the num-
ber of training sentences used for adaptation. 
Table 3. CER (%) and CER reduction over Baseline 
Domain Baseline LI Boosting Perceptron MSR 
Yomiuri (800) 3.70 3.70 (0.00%) 3.13 (15.41%) 3.18 (14.05%) 3.17 (14.32%) 
Yomiuri (8K) 3.70 3.69 (0.27%) 2.88 (22.16%) 2.85 (22.97%) 2.88 (22.16%) 
Yomiuri (72K) 3.70 3.69 (0.27%) 2.78 (24.86%) 2.78 (24.86%) 2.73 (26.22%) 
TuneUp (800) 5.81 5.81 (0.00%) 5.69 (2.07%) 5.69 (2.07%) 5.70 (1.89%) 
TuneUp (8K) 5.81 5.70 (1.89%) 5.47 (5.85%) 5.47 (5.85%) 5.47 (5.85%) 
TuneUp (72K) 5.81 5.47 (5.85%) 5.33 (8.26%) 5.20 (10.50%) 5.15 (11.36%) 
Encarta (800) 10.24 9.60 (6.25%) 9.82 (4.10%) 9.43 (7.91%) 9.44 (7.81%) 
Encarta (8K) 10.24 8.64 (15.63%) 8.54 (16.60%) 8.34 (18.55%) 8.42 (17.77%) 
Encarta (72K) 10.24 7.98 (22.07%) 7.53 (26.46%) 7.44 (27.34%) 7.40 (27.73%) 
Shincho (800) 12.18 11.86 (2.63%) 11.91 (2.22%) 11.90 (2.30%) 11.89 (2.38%) 
Shincho (8K) 12.18 11.15 (8.46%) 11.09 (8.95%) 11.20 (8.05%) 11.04 (9.36%) 
Shincho (72K) 12.18 10.76 (11.66%) 10.25 (15.85%) 10.18 (16.42%) 10.16 (16.58%) 
6   Discussion 
6.1   Domain Similarity and CER 
The first row of Table 2 shows that the average cross entropy with respect to the 
background domain (Nikkei) increases in the following order: Yomiuri ? TuneUp ? 
Encarta ? Shincho. This indicates that among the adaptation domains, Yomiuri is the 
most similar to Nikkei, closely followed by TuneUp; Shincho and Encarta are the 
least similar to Nikkei. This is consistent with our intuition, since Nikkei and Yomiuri 
are both newspaper corpora, and TuneUp, which is a manually constructed corpus 
from various representative domains of text, contains newspaper articles.  
966 W. Yuan, J. Gao, and H. Suzuki 
This metric of similarity correlates perfectly with the CER. In Table 3, we see that 
for all sizes of training data for all adaptation methods, the following order of CER 
performance is observed, from better to worse: Yomiuri ? TuneUp ? Encarta ? 
Shincho. In other words, the more similar the adaptation domain is to the background 
domain, the better the CER results are.  
6.2   Domain Similarity and the Effectiveness of Adaptation Methods  
The effectiveness of a LM adaptation method is measured by the relative CER reduc-
tion over the baseline model. Figure 3 shows the CER reduction of various methods 
for each domain when the training data size was 8K.4 
0.00%
5.00%
10.00%
15.00%
20.00%
25.00%
Yomiuri TuneUp Encarta Shincho
LI
Boosting
Perceptron
MSR
  
Fig. 2. CER reduction by different adaptation methods 
In Figure 2 the X-axis is arranged in the order of domain similarity with the back-
ground domain, i.e., Yomiuri ? TuneUp ? Encarta ? Shincho. The first thing we 
note is that the discriminative methods outperform LI in all cases: in fact, for all rows 
in Table 3, MSR outperforms LI in a statistically significant manner (p < 0.01 using t-
test);5 the differences among the three discriminative methods, on the other hand, are 
not statistically significant in most cases. 
We also note that the performance of LI is greatly influenced by domain similarity. 
More specifically, when the adaptation domain is similar to the background domain 
(i.e., for Yomiuri and TuneUp corpora), the contribution of the LI model is extremely 
limited. This can be explained as follows: if the adaptation data is too similar to the 
background, the difference between the two underlying distributions is so slight that 
adding adaptation data leads to no or very small improvements.  
Such a limitation is not observed with the discriminative methods. For example, all 
discriminative methods are quite effective on Yomiuri, achieving more than 20% 
CER reduction. We therefore conclude that discriminative methods, unlike LI, are 
robust against the similarity between background and adaptations domains.  
                                                          
4
 Essentially the same trend is observed with other training data sizes.  
5
 The only exception to this was Shincho (800).  
 An Empirical Study on Language Model Adaptation 967 
6.3   Adaptation Data Size and CER Reduction 
We have seen in Table 3 that in all cases, discriminative methods outperform LI. 
Among the discriminative methods, an interesting characteristic regarding the CER 
reduction and the data size is observed. Figure 3 displays the self entropy of four 
adaptation corpora along the X-axis, and the improvement in CER reduction when 
72K-sentence adaptation data is used over when 800 sentences are used along the Y-
axis. In other words, for each adaptation method, each point in the figure corresponds 
to the CER reduction ratio on a domain (corresponding to Yomiuri, Encarta, TuneUp, 
Shincho from left to right) when 90 times more adaptation data was available.  
0
1
2
3
4
5
6
7
4 4.2 4.4 4.6 4.8
self entropy
C
E
R
 
r
e
d
u
c
t
i
o
n
 
r
a
t
i
o
 
(
%
)
Boosting
Perceptron
MSR
  
Fig. 3. Improvement in CER reduction for discriminative methods by increasing the adaptation 
data size from 800 to 72K sentences 
From this figure, we can see that there is a positive correlation between the diversity 
of the adaptation corpus and the benefit of having more training data available. This 
has an intuitive explanation: the less diverse the adaptation data is, the less distinct 
training examples it will include for discriminative training. This result is useful in 
guiding the process of adaptation data collection.  
7   Conclusion and Future Work 
In this paper, we have examined the performance of various LM adaptation methods 
in terms of domain similarity and diversity. We have found that (1) the notion of 
cross-domain similarity, measured by the cross entropy, correlates with the CER of all 
models (Section 6.1), and (2) the notion of in-domain diversity, measured by the self 
entropy, correlates with the utility of more adaptation training data for discriminative 
training methods (Section 6.3). In comparing discriminative methods with a MAP-
based method, we have also found that (1) the former uniformly achieve better CER 
performance than the latter, and (2) are more robust against the similarity of back-
ground and adaptation data (Section 6.2).  
968 W. Yuan, J. Gao, and H. Suzuki 
Though we believe these results are useful in designing the future experiments in 
domain adaptation, some results and correlations indicated in the paper are still incon-
clusive. We hope to run additional experiments to confirm these findings. We also did 
not fully investigate into characterizing the differences among the three discriminative 
methods; such an investigation is also left for future research.  
References 
1. Bellagarda, J. An Overview of Statistical Language Model Adaptation. In ITRW on Adap-
tation Methods for Speech Recognition (2001): 165-174. 
2. Collins, M. Ranking Algorithms for Name-Entity Extraction: Boosting and the Voted Per-
ceptron. ACL (2002).  
3. Collins, M. Discriminative Training Methods for Hidden Markov Models: Theory and Ex-
periments with Perceptron Algorithms. EMNLP (2002). 
4. Gao. J., H. Yu, P. Xu, and W. Yuan. Minimum Sample Risk Methods for Language Mod-
eling. To Appear (2005). 
5. Manning, C.D., and H. Sch?tze. Foundations of Statistical Natural Language Processing. 
The MIT Press (1999).  
6. Dagan, I., L. Lee, and F. Pereira. Similarity-based models of cooccurrence probabilities. 
Machine Learning, 34(1-3): 43-69 (1999).  
7. Lee, L. Measures of distributional similarity. ACL (1999): 25-32. 
8. Roark, B, M. Saraclar and M. Collins. Corrective Language Modeling for Large Vocabu-
lary ASR with the Perceptron Algorithm. ICASSP (2004): 749-752. 
9. Bacchiani, M., B. Roark and M. Saraclar. Language Model Adaptation with MAP Estima-
tion and the Perceptron Algorithm. HLT-NAACL (2004): 21-24. 
10. Bacchiani, M. and B. Roark. Unsupervised language model adaptation. ICASSP (2003): 
224-227 
11. Och, F.J. Minimum error rate training in statistical machine translation. ACL (2003): 160-
167. 
12. Gao, J., J. Goodman, M. Li, and K.F. Lee. Toward a unified approach to statistical lan-
guage modeling for Chinese. ACM Transactions on Asian Language Information Process-
ing l-1 (2002): 3-33.  
Minimally Supervised Learning of Semantic Knowledge                 
from Query Logs 
                    Mamoru Komachi                                        Hisami Suzuki 
         Nara Institute of Science and Technology                      Microsoft Research 
   8916-5 Takayama                                      One Microsoft Way 
Ikoma, Nara 630-0192, Japan                          Redmond, WA 98052 USA   
 mamoru-k@is.naist.jp            hisamis@microsoft.com 
 
 
Abstract 
We propose a method for learning semantic 
categories of words with minimal supervi-
sion from web search query logs. Our me-
thod is based on the Espresso algorithm 
(Pantel and Pennacchiotti, 2006) for ex-
tracting binary lexical relations, but makes 
important modifications to handle query 
log data for the task of acquiring semantic 
categories. We present experimental results 
comparing our method with two state-of-
the-art minimally supervised lexical know-
ledge extraction systems using Japanese 
query log data, and show that our method 
achieves higher precision than the pre-
viously proposed methods. We also show 
that the proposed method offers an addi-
tional advantage for knowledge acquisition 
in an Asian language for which word seg-
mentation is an issue, as the method utiliz-
es no prior knowledge of word segmenta-
tion, and is able to harvest new terms with 
correct word segmentation.  
1 Introduction 
Extraction of lexical knowledge from a large col-
lection of text data with minimal supervision has 
become an active area of research in recent years. 
Automatic extraction of relations by exploiting 
recurring patterns in text was pioneered by Hearst 
(1992), who describes a bootstrapping procedure 
for extracting words in the hyponym (is-a) relation, 
starting with three manually given lexico-syntactic 
patterns. This idea of learning with a minimally 
supervised bootstrapping method using surface text 
patterns was subsequently adopted for many tasks, 
including relation extraction (e.g., Brin, 1998; Ri-
loff and Jones, 1999; Pantel and Pennacchiotti, 
2006) and named entity recognition (e.g., Collins 
and Singer, 1999; Etzioni et al, 2005).  
In this paper, we describe a method of learning 
semantic categories of words using a large collec-
tion of Japanese search query logs. Our method is 
based on the Espresso algorithm (Pantel and Pen-
nacchiotti, 2006) for extracting binary lexical rela-
tions, adapting it to work well on learning unary 
relations from query logs. The use of query data as 
a source of knowledge extraction offers some 
unique advantages over using regular text. 
? Web search queries capture the interest of search 
users directly, while the distribution of the Web 
documents do not necessarily reflect the distri-
bution of  what people search (Silverstein et al,  
1998). The word categories acquired from query 
logs are thus expected to be more useful for the 
tasks related to search.  
? Though user-generated queries are often very 
short, the words that appear in queries are gen-
erally highly relevant for the purpose of word 
classification.  
? Many search queries consist of keywords, which 
means that the queries include word segmenta-
tion specified by users. This is a great source of 
knowledge for learning word boundaries for 
those languages whose regularly written text 
does not indicate word boundaries, such as Chi-
nese and Japanese. 
Although our work naturally fits into the larger 
goal of building knowledge bases automatically 
from text, to our knowledge we are the first to ex-
plore the use of Japanese query logs for the pur-
pose of minimally supervised semantic category 
acquisition. Our work is similar to Sekine and Su-
zuki (2007), whose goal is to augment a manually 
created dictionary of named entities by finding 
358
contextual patterns from English query logs. Our 
work is different in that it does not require a full-
scale list of categorized named entities but a small 
number of seed words, and iterates over the data to 
extract more patterns and instances. Recent work 
by Pa?ca (2007) and Pa?ca and Van Durme (2007) 
also uses English query logs to extract lexical 
knowledge, but their focus is on learning attributes 
for named entities, a different focus from ours.  
2 Related Work 
In this section, we describe three state-of-the-art 
algorithms of relation extraction, which serve as 
the baseline for our work. They are briefly summa-
rized in Table 1. The goal of these algorithms is to 
learn target instances, which are the words belong-
ing to certain categories (e.g., cat for the Animal 
class), or in the case of relation extraction, the 
pairs of words standing in a particular relationship 
(e.g., pasta::food for is-a relationship), given the 
context patterns for the categories or relation types 
found in source data.  
2.1 Pattern Induction 
The first step toward the acquisition of instances is 
to extract context patterns. In previous work, these 
are surface text patterns, e.g., X such as Y, for ex-
tracting words in an is-a relation, with some heu-
ristics for finding the pattern boundaries in text. As 
we use query logs as the source of knowledge, we 
simply used everything but the instance string in a 
query as the pattern for the instance, in a manner 
similar to Pa?ca et al (2006). For example, the 
seed word JAL in the query ?JAL+flight_schedule? 
yields the pattern "#+flight_schedule".1 Note that 
we perform no word segmentation or boundary 
detection heuristics in identifying these patterns, 
which makes our approach fast and robust, as the 
                                                 
1 # indicates where the instance occurs in the query 
string, and + indicates a white space in the original Jap-
anese query. The underscore symbol (_) means there 
was originally no white space; it is used merely to make 
the translation in English more readable.  
2 The manual classification assigns only one category 
segmentation errors introduce noise in extracted 
patterns, especially when the source data contains 
many out of vocabulary items. 
The extracted context patterns must then be as-
signed a score reflecting their usefulness in extract-
ing the instances of a desired type. Frequency is a 
poor metric here, because frequent patterns may be 
extremely generic, appearing across multiple cate-
gories. Previously proposed methods differ in how 
to assign the desirability scores to the patterns they 
find and in using the score to extract instances, as 
well as in the treatment of generic patterns, whose 
precision is low but whose recall is high.   
2.2 Sekine and Suzuki (2007)?s Algorithm 
For the purpose of choosing the set of context pat-
terns that best characterizes the categories, Sekine 
and Suzuki (2007) report that none of the conven-
tional co-occurrence metrics such as tf.idf, mutual 
information and chi-squared tests achieved good 
results on their task, and propose a new measure, 
which is based on the number of different instances 
of the category a context c co-occurs with, 
lized by its token frequency for all categories: 
CcgfcScore type )(log)( ?
 
)1000()1000(
)()()(
ctopFctopfC
cFcfcg
insttype
insttype
?
?  
where ftype is the type frequency of instance terms 
that c co-occurs with in the category, Finst is the 
token frequency of context c in the entire data and 
ctop1000 is the 1000 most frequent contexts. Since 
they start with a large and reliable named entity 
dictionary, and can therefore use several hundred 
seed terms, they simply used the top-k highest-
scoring contexts and extracted new named entities 
once and for all, without iteration. Generic patterns 
receive low scores, and are therefore ignored by 
this algorithm.  
2.3 The Basilisk Algorithm 
Thelen and Riloff (2002) present a framework 
called Basilisk, which extracts semantic lexicons 
 # of seed Target # of iteration Corpus Language 
Sekine & Suzuki ~600 Categorized NEs 1 Query log English 
Basilisk 10 Semantic lexicon ? MUC-4 English 
Espresso ~10 Semantic relations ? TREC English 
Tchai 5 Categorized words ? Query log Japanese 
Table 1: Summary of algorithms 
359
for multiple categories. It starts with a small set of 
seed words and finds all patterns that match these 
seed words in the corpus. The bootstrapping 
process begins by selecting a subset of the patterns 
by the RlogF metric (Riloff, 1996): 
)log()(log i
i
ii FN
FpatternFR ??
 
where Fi is the number of category members ex-
tracted by patterni and Ni is the total number of 
instances extracted by patterni. It then identifies 
instances by these patterns and scores each in-
stance by the following formula: 
i
P
j
j
i P
F
wordAvgLog
i?
?
?
? 1
)1log(
)(  
where Pi is the number of patterns that extract 
wordi. They use the average logarithm to select 
instances to balance the recall and precision of ge-
neric patterns. They add five best instances to the 
lexicon according to this formula, and the boot-
strapping process starts again. Instances are cumu-
latively collected across iterations, while patterns 
are discarded at the end of each iteration.  
2.4 The Espresso Algorithm 
We will discuss the Espresso framework (Pantel 
and Pennacchiotti, 2006) in some detail because 
our method is based on it. It is a general-purpose, 
minimally supervised bootstrapping algorithm that 
takes as input a few seed instances and iteratively 
learns surface patterns to extract more instances. 
The key to Espresso lies in its use of generic pat-
terns: Pantel and Pennacchiotti (2006) assume that 
correct instances captured by a generic pattern will 
also be instantiated by some reliable patterns, 
which denote high precision and low recall pat-
terns.  
Espresso starts from a small set of seed in-
stances of a binary relation, finds a set of surface 
patterns P, selects the top-k patterns, extracts the 
highest scoring m instances, and repeats the 
process. Espresso ranks all patterns in P according 
to reliability r?, and retains the top-k patterns for 
instance extraction. The value of k is incremented 
by one after each iteration. 
 The reliability of a pattern p is based on the in-
tuition that a reliable pattern co-occurs with many 
reliable instances. They use pointwise mutual in-
formation (PMI) and define the reliability of a pat-
tern p as its average strength of association across 
each input instance i in the set of instances I, 
weighted by the reliability of each instance i: 
I
irpipmi
pr Ii pm i
?
? ?
?
?
?
???
?
?
?
)(max
),(
)(
?
?
 
where r?(i) is the reliability of the instance i  and 
maxpmi is the maximum PMI between all patterns 
and all instances. The PMI between instance i = 
{x,y} and pattern p  is estimated by: 
,**,,*,
,,log),( pyx
ypxpipmi ?
 
where ypx ,, is the frequency of pattern p instan-
tiated with terms x and y (recall that Espresso is 
targeted at extracting binary relations) and where 
the asterisk represents a wildcard. They multiplied 
pmi(i,p) with the discounting factor suggested in 
Pantel and Ravichandran (2004) to alleviate a bias 
towards infrequent events. 
The reliability of an instance is defined similar-
ly: a reliable instance is one that associates with as 
many reliable patterns as possible. 
 
P
prpipmi
ir Pp pm i
?
? ?
?
?
?
???
?
?
?
)(max
),(
)(
?
?
 
where r?(p) is the reliability of pattern p, and P is 
the set of surface patterns. Note that r?(i) and r?(p) 
are recursively defined: the computation of the pat-
tern and instance reliability alternates between per-
forming pattern reranking and instance extraction. 
Similarly to Basilisk, instances are cumulatively 
learned, but patterns are discarded at the end of 
each iteration.  
3 The Tchai Algorithm 
In this section, we describe the modifications we 
made to Espresso to derive our algorithm called 
Tchai.  
3.1 Filtering Ambiguous Instances and Pat-
terns 
As mentioned above, the treatment of high-recall, 
low-precision generic patterns (e.g., #+map, 
#+animation) present a challenge to minimally 
supervised learning algorithms due to their am-
guity. In the case of semantic category acquisition, 
the problem of ambiguity is exacerbated, because 
not only the acquired patterns, but also the in-
stances can be highly ambiguous. For example, 
360
once we learn an ambiguous instance such as Po-
kemon, it will start collecting patterns for multiple 
categories (e.g., Game, Animation and Movie), 
which is not desirable.  
In order to control the negative effect of the ge-
neric patterns, Espresso introduces a confidence 
metric, which is similar but separate from the re-
liability measure, and uses it to filter out the gener-
ic patterns falling below a confidence threshold. In 
our experiments, however, this metric did not pro-
duce a score that was substantially different from 
the reliability score. Therefore, we did not use a 
confidence metric, and instead opted for not 
ing ambiguous instances and patterns, where we 
define ambiguous instance as one that induces 
more than 1.5 times the number of patterns of 
viously accepted reliable instances, and ambiguous 
(or generic) pattern as one that extracts more than 
twice the number of instances of previously ac-
cepted reliable patterns. As we will see in Section 
4, this modification improves the precision of the 
extracted instances, especially in the early stages of 
iteration.   
3.2 Scaling Factor in Reliability Scores 
Another modification to the Espresso algorithm to 
reduce the power of generic patterns is to use local 
maxpmi instead of global maxpmi. Since PMI ranges 
[??, +?], the point of dividing pmi(i,p) by maxpmi 
in Espresso is to normalize the reliability to [0, 1]. 
However, using PMI directly to estimate the relia-
bility of a pattern when calculating the reliability 
of an instance may lead to unexpected results be-
cause the absolute value of PMI is highly variable 
across instances and patterns. We define the local 
maxpmi of the reliability of an instance to be the 
absolute value of the maximum PMI for a given 
instance, as opposed to taking the maximum for all 
instances in a given iteration. Local maxpmi of the 
reliability of a pattern is defined in the same way. 
As we show in the next section, this modification 
has a large impact on the effectiveness of our algo-
rithm. 
3.3 Performance Improvements 
Tchai, unlike Espresso, does not perform the 
pattern induction step between iterations; rather, it 
simply recomputes the reliability of the patterns 
induced at the beginning. Our assumption is that 
fairly reliable patterns will occur with at least one 
of the seed instances if they occur frequently 
enough in query logs. Since pattern induction is 
computationally expensive, this modification 
reduces the computation time by a factor of 400. 
4 Experiment 
In this section, we present an empirical comparison 
of Tchai with the systems described in Section 2. 
4.1 Experimental Setup 
Query logs: The data source for instance extrac-
tion is an anonymized collection of query logs 
submitted to Live Search from January to February 
2007, taking the top 1 million unique queries. Que-
ries with garbage characters are removed. Almost 
all queries are in Japanese, and are accompanied 
by their frequency within the logs. 
Target categories: Our task is to learn word cate-
gories that closely reflect the interest of web search 
users. We believe that a useful categorization of 
words is task-specific, therefore we did not start 
with any externally available ontology, but chose 
to start with a small number of seed words. For our 
task, we were given a list of 23 categories relevant 
for web search, with a manual classification of the 
10,000 most frequent search words in the log of 
December 2006 (which we henceforth refer to as 
the 10K list) into one of these categories. 2  For 
evaluation, we chose two of the categories, Travel 
and Financial Services: Travel is the largest cate-
gory containing 712 words of the 10K list (as all 
the location names are classified into this category), 
while Financial Services was the smallest, contain-
ing 240 words.   
Systems: We compared three different systems 
described in Section 2 that implement an iterative 
algorithm for lexical learning:  
                                                 
2 The manual classification assigns only one category 
per word, which is not optimal given how ambiguous 
the category memberships are. However, it is also very 
difficult to reliably perform a multi-class categorization 
by hand.  
Category Seeds (with English translation) 
Travel jal, ana, jr, ????(jalan), his 
Finance ?????(Mizuho Bank), ?????
? (SMBC), jcb, ? ? ? ? (Shinsei 
Bank), ????(Nomura Securities) 
 
Table 2: Seed instances for Travel and Financial Ser-
vices categories 
361
? Basilisk: The algorithm by (Thelen and Riloff, 
2002) described in Section 2.  
? Espresso: The algorithm by (Pantel and Pennac-
chiotti, 2006) described in Sections 2 and 3. 
? Tchai: The Tchai algorithm described in this 
paper. 
For each system, we gave the same seed instances. 
The seed instances are the 5 most frequent words 
belonging to these categories in the 10K list; they 
are given in Table 2. For the Travel category, ?jal? 
and ?ana? are airline companies, ?jr? stand for Ja-
pan Railways, ?jalan? is an online travel informa-
tion site, and ?his? is a travel agency. In the 
Finance category, three of them are banks, and the 
other two are a securities company and a credit 
card firm. Basilisk starts by extracting 20 patterns, 
and adds 100 instances per iteration. Espresso and 
Tchai start by extracting 5 patterns and add 200 
instances per iteration. Basilisk and Tchai iterated 
20 times, while Espresso iterated only 5 times due 
to computation time. 
4.2 Results 
4.2.1 Results of the Tchai algorithm 
Tables 3 and 4 are the results of the Tchai algo-
rithm compared to the manual classification. Table 
3 shows the results for the Travel category. The 
precision of Tchai is very high: out of the 297 
words classified into the Travel domain that were 
also in the 10K list, 280 (92.1%) were learned 
rectly. 3  It turned out that the 17 instances that 
                                                 
3 As the 10K list contained 712 words in the Travel cat-
egory, the recall against that list is fairly low (~40%). 
The primary reason for this is that all location names are 
classified as Travel in the 10K list, and 20 iterations are 
represent the precision error were due to the ambi-
guity of hand labeling, as in?????????? 
?Tokyo Disneyland?, which is a popular travel des-
tination, but is classified as Entertainment in the 
manual annotation. We were also able to correctly 
learn 251 words that were not in the 10K list ac-
cording to manual verification; we also harvested 
125 new words ?incorrectly? into the Travel do-
main, but these words include common nouns re-
lated to Travel, such as ?? ?fishing? and ????
?  ?rental car?. Results for the Finance domain 
show a similar trend, but fewer instances are ex-
tracted.  
Sample instances harvested by our algorithm 
are given in Table 5. It includes subclasses of tra-
vel-related terms, for some of which no seed words 
were given (such as Hotels and Attractions). We 
also note that segmentation errors are entirely ab-
sent from the collected terms, demonstrating that 
query logs are in fact excellently suited for acquir-
ing new words for languages with no explicit word 
segmentation in text.  
4.2.2 Comparison with Basilisk and Espresso 
Figures 1 and 2 show the precision results compar-
ing Tchai with Basilisk and Espresso for the Travel 
and Finance categories. Tchai outperforms Basilisk 
and Espresso for both categories: its precision is 
constantly higher for the Travel category, and it 
achieves excellent precision for the Finance cate-
gory, especially in early iterations. The differences 
in behavior between these two categories are due 
to the inherent size of these domains. For the 
                                                                             
not enough to enumerate all frequent location names. 
Another reason is that the 10K list consists of queries 
but our algorithm extracts instances ? this sometimes 
causes a mismatch, e.g.,Tchai extracts??? ?Ritz? but 
the 10K list contains ??????  ?Ritz Hotel?.  
 
 
 
10K list Not in 
10K list Travel Not Travel 
Travel 280 17 251 
Not Travel 0 7 125 
Table 3: Comparison with manual annotation: 
Travel category 
 10K list  Not in 
10K list Finance Not Finance 
Finance 41 30 30 
Not Finance 0 5 99 
Table 4: Comparison with manual annotation: 
Financial Services category 
 
Type Examples (with translation) 
Place ??? (Turkey), ????? (Las 
Vegas), ??? (Bali Island) 
Travel agency Jtb, ???  (www.tocoo.jp), ya-
hoo (Yahoo ! Travel), net cruiser 
Attraction ????????  (Disneyland), 
usj (Universal Studio Japan) 
Hotel ?????(Imperial Hotel), ??
?(Ritz Hotel) 
Transportation ????(Keihin Express), ???
?(Nara Kotsu Bus Lines) 
 
Table 5: Extracted Instances 
362
smaller Finance category, Basilisk and Espresso 
both suffered from the effect of generic patterns 
such as #?????? ?homepage? and #??? 
?card? in early iterations, whereas Tchai did not 
select these patterns.  
 
Figure 1: Basilisk, Espresso vs. Tchai: Travel 
 
Figure 2: Basilisk, Espresso vs. Tchai: Finance 
Comparing these algorithms in terms of recall 
is more difficult, as the complete set of words for 
each category is not known. However, we can es-
timate the relative recall given the recall of another 
system. Pantel and Ravichandran (2004) defined 
relative recall as: 
||
||
| BP
AP
C
C
CC
CC
R
RR
B
A
B
A
B
A
B
A
BA ?
?????
 
where RA|B is the relative recall of system A given 
system B, CA and CB are the number of correct in-
stances of each system, and C is the number of true 
correct instances. CA and CB can be calculated by 
using the precision, PA and PB, and the number of 
instances from each system. Using this formula, 
we estimated the relative recall of each system rel-
ative to Espresso. Tables 6 and 7 show that Tchai 
achieved the best results in both precision and rela-
tive recall in the Travel domain. In the Finance 
domain, Espresso received the highest relative 
call but the lowest precision. This is because Tchai 
uses a filtering method so as not to select generic 
patterns and instances. 
Table 8 shows the context patterns acquired by 
different systems after 4 iterations for the Travel 
domain.4 The patterns extracted by Basilisk are not 
entirely characteristic of the Travel category. For 
example, ?p#sonic? and ?google+#lytics? only 
match the seed word ?ana?, and are clearly irrele-
vant to the domain. Basilisk uses token count to 
estimate the score of a pattern, which may explain 
the extraction of these patterns. Both Basilisk and 
Espresso identify location names as context pat-
terns (e.g., #?? ?Tokyo?, #?? ?Kyushu?), which 
may be too generic to be characteristic of the do-
main. In contrast, Tchai finds context patterns that 
are highly characteristic, including terms related to 
transportation (#+????? ?discount plane tick-
et?, #?????  ?mileage?) and accommodation 
(#+??? ?hotel?).  
4.2.3 Contributions of Tchai components 
In this subsection, we examine the contribution of 
each modification to the Espresso algorithm we 
made in Tchai.  
Figure 3 illustrates the effect of each 
modification proposed for the Tchai algorithm in 
Section 3 on the Travel category. Each line in the 
graph corresponds to the Tchai algorithm with and 
without the modification described in Sections 3.1 
and 3.2. It shows that the modification to the 
maxpmi function (purple) contributes most signifi-
cantly to the improved accuracy of our system. The 
filtering of generic patterns (green) does not show 
                                                 
4 Note that Basilisk and Espresso use context patterns 
only for the sake of collecting instances, and are not 
interested in the patterns per se. However, they can be 
quite useful in characterizing the semantic categories 
they are acquired for, so we chose to compare them here.  
 # of inst. Precision Rel.recall 
Basilisk 651 63.4 1.26 
Espresso 500 65.6 1.00 
Tchai 680 80.6 1.67 
Table 6: Precision (%) and relative recall: Tra-
vel domain 
 # of inst. Precision Rel.recall 
Basilisk 278 27.3 0.70 
Espresso 704 15.2 1.00 
Tchai 223 35.0 0.73 
Table 7: Precision (%) and relative recall: Finan-
cial Services domain 
 
363
a large effect in the precision of the acquired in-
stances for this category, but produces steadily bet-
ter results than the system without it. 
Figure 4 compares the original Espresso algo-
rithm and the modified Espresso algorithm which 
performs the pattern induction step only at the be-
ginning of the bootstrapping process, as described 
in Section 3.3. Although there is no significant dif-
ference in precision between the two systems, this 
modification greatly improves the computation 
time and enables efficient extraction of instances. 
We believe that our choice of the seed instances to 
be the most frequent words in the category produc-
es sufficient patterns for extracting new instances. 
 
Figure 3: System precision w/o each modification 
 
Figure 4: Modification to the pattern induction step 
 
5 Conclusion 
We proposed a minimally supervised bootstrap-
ping algorithm called Tchai. The main contribution 
of the paper is to adapt the general-purpose Es-
presso algorithm to work well on the task of learn-
ing semantic categories of words from query logs. 
The proposed method not only has a superior per-
formance in the precision of the acquired words 
into semantic categories, but is faster and collects 
more meaningful context patterns for characteriz-
ing the categories than the unmodified Espresso 
algorithm. We have also shown that the proposed 
method requires no pre-segmentation of the source 
text for the purpose of knowledge acquisition.  
Acknowledgements 
This research was conducted during the first au-
thor?s internship at Microsoft Research. We would 
like to thank the colleagues at Microsoft Research, 
especially Dmitriy Belenko and Christian K?nig, 
for their help in conducting this research.  
References 
Sergey Brin. 1998. Extracting Patterns and Relations 
from the World Wide Web. WebDB Workshop at 6th 
International Conference on Extending Database 
Technology, EDBT '98. pp. 172-183. 
Michael Collins and Yoram Singer. 1999. Unsupervised 
Models for Named Entity Classification. Proceedings 
of the Joint SIGDAT Conference on Empirical Me-
thods in Natural Language Processing and Very 
Large Corpora. pp. 100-110. 
Oren Etzioni, Michael Cafarella, Dong Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, Da-
niel S. Weld, and Alexander Yates. 2005. Unsuper-
vised Named-Entity Extraction from the Web: An 
Experimental Study. Artificial Intelligence. 165(1). 
pp. 91-134. 
Marti Hearst. 1992. Automatic Acquisition of Hypo-
nyms from Large Text Corpora. Proceedings of the 
System Sample Patterns (with English translation) 
Basilisk #???(east_japan), #???(west_japan), p#sonic, #???(timetable), #??(Kyushu),  #+???
??(mileage), #??(bus),  google+#lytics, #+??(fare),  #+??(domestic), #???(hotel) 
Espresso #??(bus), ??#(Japan), #???(hotel), #??(road), #??(inn), ??#(Fuji), #??(Tokyo), #?
?(fare), #??(Kyushu), #???(timetable), #+??(travel), #+???(Nagoya) 
Tchai #+???(hotel), #+???(tour), #+??(travel), #??(reserve), #+???(flight_ticket), #+???
??(discount_flight_titcket), #?????(mileage), ????+#(Haneda Airport) 
 
Table 8: Sample patterns acquired by three algorithms 
364
Fourteenth International Conference on Computa-
tional Linguistics. pp 539-545. 
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: 
Leveraging Generic Patterns for Automatically Har-
vesting Semantic Relations. Proceedings of the 21st 
International Conference on Computational Linguis-
tics and the 44th annual meeting of the ACL. pp. 113-
120.  
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically Labeling Semantic Classes. Proceedings of 
Human Language Technology Conference of the 
North American Chapter of the Association for Com-
putational Linguistics (HLT/NAACL-04). pp. 321-
328. 
Marius Pa?ca. 2004. Acquisition of Categorized Named 
Entities for Web Search. Proceedings of the 13th 
ACM Conference on Information and Knowledge 
Management (CIKM-04). pp. 137-145. 
Marius Pa?ca. 2007. Organizing and Searching the 
World Wide Web of Fact ? Step Two: Harnessing the 
Wisdom of the Crowds. Proceedings of the 16th In-
ternational World Wide Web Conference (WWW-07). 
pp. 101-110. 
Marius Pa?ca and Benjamin Van Durme. 2007. What 
You Seek is What You Get: Extraction of Class 
Attributes from Query Logs. Proceedings of the 20th 
International Joint Conference on Artificial Intelli-
gence (IJCAI-07). pp. 2832-2837. 
Marius Pa?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits and Alpa Jain. 2006. Organizing and Searching 
the World Wide Web of Facts ? Step One: the One-
Million Fact Extraction Challenge. Proceedings of 
the 21st National Conference on Artificial Intelli-
gence (AAAI-06). pp. 1400-1405. 
Ellen Riloff. 1996. Automatically Generating Extraction 
Patterns from Untagged Text. Proceedings of the 
Thirteenth National Conference on Artificial Intelli-
gence. pp. 1044-1049. 
Ellen Riloff and Rosie Jones. 1999. Learning Dictiona-
ries for Information Extraction by Multi-Level Boot-
strapping. Proceedings of the Sixteenth National 
Conference on Artificial Intellligence (AAAI-99). pp. 
474-479. 
Satoshi Sekine and Hisami Suzuki. 2007. Acquiring 
Ontological Knowledge from Query Logs. Proceed-
ings of the 16th international conference on World 
Wide Web. pp. 1223-1224. 
Craig Silverstein, Monika Henzinger, Hannes Marais, 
and Michael Moricz. 1998. Analysis of a Very Large 
AltaVista Query Log. Digital SRC Technical Note 
#1998-014. 
Michael Thelen and Ellen Riloff. 2002. A Bootstrapping 
Method for Learning Semantic Lexicons using Ex-
traction Pattern Contexts. Proceedings of Conference 
on Empirical Methods in Natural Language 
Processing. pp. 214-221. 
 
365
Proceedings of NAACL HLT 2007, pages 49?56,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Generating Case Markers in Machine Translation 
Kristina Toutanova  Hisami Suzuki 
Microsoft Research 
One Microsoft Way, Redmond WA 98052 USA 
{hisamis,kristout}@microsoft.com 
 
Abstract 
We study the use of rich syntax-based 
statistical models for generating gram-
matical case for the purpose of machine 
translation from a language which does 
not indicate case explicitly (English) to a 
language with a rich system of surface 
case markers (Japanese). We propose an 
extension of n-best re-ranking as a 
method of integrating such models into a 
statistical MT system and show that this 
method substantially outperforms stan-
dard n-best re-ranking. Our best perform-
ing model achieves a statistically signifi-
cant improvement over the baseline MT 
system according to the BLEU metric. 
Human evaluation also confirms the re-
sults. 
1 Introduction 
Generation of grammatical elements such as in-
flectional endings and case markers is an impor-
tant component technology for machine transla-
tion (MT). Statistical machine translation (SMT) 
systems, however, have not yet successfully in-
corporated components that generate grammati-
cal elements in the target language. Most state-
of-the-art SMT systems treat grammatical ele-
ments in exactly the same way as content words, 
and rely on general-purpose phrasal translations 
and target language models to generate these ele-
ments (e.g., Och and Ney, 2002; Koehn et al, 
2003; Quirk et al, 2005; Chiang, 2005; Galley et 
al., 2006). However, since these grammatical 
elements in the target language often correspond 
to long-range dependencies and/or do not have 
any words corresponding in the source, they may 
be difficult to model, and the output of an SMT 
system is often ungrammatical.  
 For example, Figure 1 shows an output from 
our baseline English-to-Japanese SMT system on 
a sentence from a computer domain. The SMT 
system, trained on this domain, produces a natu-
ral lexical translation for the English word patch 
as correction program, and translates replace 
into passive voice, which is more appropriate in 
Japanese. 1  However, there is a problem in the 
case marker assignment: the accusative marker 
wo, which was output by the SMT system, is 
completely inappropriate when the main verb is 
passive. This type of mistake in case marker as-
signment is by no means isolated in our SMT 
system: a manual analysis showed that 16 out of 
100 translations had mistakes solely in the as-
signment of case markers. A better model of case 
assignment could therefore improve the quality 
of an SMT system significantly.  
S: The patch replaces the .dll file.  
O: ????????.dll?????????????? 
    shuusei puroguramu-wo    .dll fairu-ga   okikae-raremasu 
    correction program-ACC dll file-NOM replace-PASS 
C: ????????.dll?????????????? 
    shuusei puroguramu-de    .dll fairu-ga   okikae-raremasu 
    correction program-with dll file-NOM replace-PASS  
Figure 1: Example of SMT (S: source; O: output of 
MT; C: correct translation) 
 In this paper, we explore the use of a statisti-
cal model for case marker generation in  English-
to-Japanese SMT. Though we focus on the gen-
eration of case markers in this paper, there are 
many other surface grammatical phenomena that 
can be modeled in a similar way, so any SMT 
system dealing with morpho-syntactically diver-
gent language pairs may benefit from a similar 
approach to modeling grammatical elements. Our 
model uses a rich set of syntactic features of both 
the source (English) and the target (Japanese) 
sentences, using context which is broader than 
that utilized by existing SMT systems. We show 
that the use of such features results in very high 
case assignment quality and also leads to a nota-
ble improvement in MT quality.  
Previous work has discussed the building of 
special-purpose classifiers which generate gram-
matical elements such as prepositions (Haji? et al 
2002), determiners (Knight and Chander, 1994) 
and case markers (Suzuki and Toutanova, 2006) 
with an eye toward improving MT output. How-
                                               
1
 There is a strong tendency to avoid transitive sentences 
with an inanimate subject in Japanese.  
49
ever, these components have not actually been 
integrated in an MT system. To our knowledge, 
this is the first work to integrate a grammatical 
element production model in an SMT system and 
to evaluate its impact in the context of end-to-
end MT.  
 A common approach of integrating new mod-
els with a statistical MT system is to add them as 
new feature functions which are used in decod-
ing or in models which re-rank n-best lists from 
the MT system (Och et al, 2004). In this paper 
we propose an extension of the n-best re-ranking 
approach, where we expand n-best candidate lists 
with multiple case assignment variations, and 
define new feature functions on this expanded 
candidate set. We show that expanding the n-best 
lists significantly outperforms standard n-best re-
ranking. We also show that integrating our case 
prediction model improves the quality of transla-
tion according to BLEU (Papineni et al, 2002) 
and human evaluation. 
2 Background 
In this section, we provide necessary background 
of the current work. 
2.1 Task of case marker prediction 
Our definition of the case marker prediction task 
follows Suzuki and Toutanova (2006). That is, 
we assume that we are given a source English 
sentence, and its translation in Japanese which 
does not include case markers. Our task is to pre-
dict all case markers in the Japanese sentence.   
We determine the location of case marker in-
sertion using the notion of bunsetsu. A bunsetsu 
consists of one content (head) word followed by 
any number of function words. We can therefore 
segment any sentence into a sequence of bun-
setsu by using a part-of-speech (POS) tagger. 
Once a sentence is segmented into bunsetsu, it 
is trivial to determine the location of case mark-
ers in a sentence: each bunsetsu can have at most 
one case marker, and the position of the case 
maker within a phrase is predictable, i.e., the 
rightmost position before any punctuation marks. 
The sentence in Figure 1 thus has the following 
bunsetsu analysis (denoted by square brackets), 
with the locations of potential case marker inser-
tion indicated by ?:  
[??'correction'?][?????'program'?][.dll?][?
???'file'?][????????'replace-PASS'??] 
For each of these positions, our task is to predict 
the case marker or to predict NONE, which means 
that the phrase does not have a case marker. 
The case markers we used for the prediction 
task are the same as those defined in Suzuki and 
Toutatnova (2006), and are summarized in Table 
1: in addition to the case markers in a strict sense, 
the topic marker wa is also included as well as 
the combination of a case marker plus the topic 
marker for the case markers with the column 
+wa checked in the table. In total, there are 18 
case markers to predict: ten simple case markers, 
the topic marker wa, and seven case+wa combi-
nations. The case prediction task is therefore a 
19-fold classification task: for each phrase, we 
assign one of the 18 case markers or NONE. 
2.2 Treelet translation system 
We constructed and evaluated our case predic-
tion model in the context of a treelet-based trans-
lation system, described in Quirk et al (2005).2 
In this approach, translation is guided by treelet 
translation pairs, where a treelet is a connected 
subgraph of a dependency tree.  
A sentence is translated in the treelet system 
as follows. The input sentence is first parsed into 
a dependency structure, which is then partitioned 
into treelets, assuming a uniform probability dis-
tribution over all partitions. Each source treelet is 
then matched to a treelet translation pair, the col-
lection of which will form the target translation. 
The target language treelets are then joined to 
form a single tree, and the ordering of all the 
nodes is determined, using the method described 
in Quirk et al (2005).  
Translations are scored according to a linear 
combination of feature functions:  
( ) ( )j j
j
score t f t?= ?  (1) 
                                               
2
 Though this paper reports results in the context of a treelet 
system, the model is also applicable to other syntax-based 
or phrase-based SMT systems.  
case markers grammatical functions +wa 
? 
ga subject; object  
? wo object; path  
? 
no genitive; subject  
? 
ni dative object, location 
? 
?? 
kara source 
? 
? 
to quotative, reciprocal, as 
? 
? 
de location,instrument, cause 
? 
? e goal, direction ? 
?? 
made goal (up to, until) 
? 
?? 
yori source, comparison target 
? 
? 
wa Topic  
Table 1. Case markers to be predicted 
50
where 
 
j are the model parameters and fj(t) is the 
value of the feature function j on the candidate t. 
There are ten feature functions in the treelet sys-
tem, including log-probabilities according to in-
verted and direct channel models estimated by 
relative frequency, lexical weighting channel 
models following Vogel et al (2003), a trigram 
target language model, an order model, word 
count, phrase count, average phrase size func-
tions, and whole-sentence IBM Model 1 log-
probabilities in both directions (Och et al 2004). 
The weights of these models are determined us-
ing the max-BLEU method described in Och 
(2003). As we describe in Section 4, the case 
prediction model is integrated into the system as 
an additional feature function.  
The treelet translation model is estimated us-
ing a parallel corpus. First, the corpus is word-
aligned using GIZA++ (Och and Ney, 2000); 
then the source sentences are parsed into a de-
pendency structure, and the dependency is pro-
jected onto the target side following the heuris-
tics described in Quirk et al (2005). Figure 2 
shows an example of an aligned sentence pair: on 
the source (English) side, POS tags and word 
dependency structure are assigned (solid arcs); 
the word alignments between English and Japa-
nese words are indicated by the dotted lines. On 
the target (Japanese) side, projected word de-
pendencies (solid arcs) are available. Additional 
annotations in Figure 2, namely the POS tags and 
the bunsetsu dependency structure (bold arcs) on 
the target side, are derived from the treelet sys-
tem to be used for building a case prediction 
model, which we describe in Section 3.  
2.3 Data 
All experiments reported in this paper are run 
using parallel data from a technical (computer) 
domain. We used two main data sets: train-500K, 
consisting of 500K sentence pairs which we used 
for training the baseline treelet system as well as 
the case prediction model, and a disjoint set of 
three data sets, lambda-1K, dev-1K and test-2K, 
which are used to integrate and evaluate the case 
prediction model in an end-to-end MT scenario. 
Some characteristics of these data sets are given 
in Table 2. We will refer to this table as we de-
scribe our experiments in later sections.  
# sent 
pairs 
# of words  
(average sent length in words) 
data set 
 English Japanese 
train-500K 500K 7,909,198 
(15.81) 
9,379,240 
(18.75) 
lambda-1K 1,000 15,219(15.2) 20,660 (20.7) 
dev-1K 1,000 15,397(15.4) 21,280 (21.3) 
test-2K 2,000 30,198(15.1) 41,269 (20.6) 
Table 2: Data set characteristics 
3 Statistical Models for Case Prediction 
in MT 
3.1 Case prediction model  
Our model of case marker prediction closely fol-
lows our previous work of case prediction in a 
non-MT context (Suzuki and Toutanova, 2006). 
The model is a multi-class log-linear (maximum 
entropy) classifier using 19 classes (18 case 
markers and NONE). It assigns a probability dis-
tribution over case marker assignments given a 
source English sentence, all non-case marker 
words of a candidate Japanese translation, and 
additional annotation information. Let t denote a 
Japanese translation, s a corresponding source 
sentence, and A additional annotation informa-
tion such as alignment, dependency structure, 
and POS tags (such as shown in Figure 2). Let 
rest(t) denote the sequence of words in t exclud-
ing all case markers, and case(t) a case marking 
assignment for all phrases in t. Our case marking 
model estimates the probability of a case as-
signment given all other information:  
),),(|)(( AstresttcasePcase  
The probability of a complete case assignment is 
a product over all phrases of the probability of 
the case marker of the phrase given all context 
features used by the model. Our model assumes 
that the case markers in a sentence are independ-
ent of each other given the input features. This 
independence assumption may seem strong, but 
the results presented in our previous work (Su-
zuki and Toutanova, 2006) showed that a joint 
model did not result in large improvements over 
a local one in predicting case markers in a non-
MT context. 
 
Figure 2. Aligned English-Japanese sentence pair 
51
3.2 Model features and feature selection  
The features of our model are similar to the ones 
described in Suzuki and Toutanova (2006). The 
main difference is that in the current model we 
applied a feature selection and induction algo-
rithm to determine the most useful features and 
feature combinations. This is important for un-
derstanding what sources of information are im-
portant for predicting grammatical elements, but 
are currently absent from SMT systems. We 
used 490K sentence pairs for training the case 
prediction model, which is a subset of the train-
500K set of Table 2. We divided the remaining 
10K sentences for feature selection (5K-feat) and 
for evaluating the case prediction models on ref-
erence translations (5K-test, discussed in Section 
3.3). The paired data is annotated using the 
treelet translation system: as shown in Figure 2, 
we have source and target word dependency 
structure, source language POS and word align-
ment directly from the aligned treelet structure. 
Additionally, we used a POS tagger of Japanese 
to assign POS to the target sentence as well as to 
parse the sentence into bunsetsu (indicated by 
brackets in Figure 2), using the method described 
in Section 2.1. We then compute bunsetsu de-
pendency structure on the target side (indicated 
by bold arcs in Figure 2) based on the word de-
pendency structure projected from English. We 
apply this procedure to annotate a paired corpus 
(in which case the Japanese sentence is a refer-
ence translation) as well as translations generated 
by the SMT system (which may potentially be 
ill-formed).  
We derived a large set of possible features 
from these annotations. The features are repre-
sented as feature templates, such as "Headword 
POS=X", which generate a set of binary features 
corresponding to different instantiations of the 
template, such as "Headword POS=NOUN". We 
applied an automatic feature selection and induc-
tion algorithm to the base set of templates. 
The feature selection algorithm considers the 
original templates as well as arbitrary (bigram 
and trigram) conjunctions of these templates. 
The algorithm performs forward stepwise feature 
selection, choosing templates which result in the 
highest increase in model accuracy on the 5K-
feat set mentioned above. The algorithm is simi-
lar to the one described in McCallum (2003).  
The application of this feature selection pro-
cedure gave us 17 templates, some of which are 
shown in Table 3, along with example instantia-
tions for the phrase headed by saabisu ?service? 
from Figure 2. Conjunctions are indicated by &. 
Note that many features that refer to POS and 
syntactic (parent) information are selected, on 
both the target and source sides. We also note 
that the context required by these features is 
more extensive than what is usually available 
during decoding in an SMT system due to a limit 
imposed on the treelet or phrase size. For exam-
ple, our model uses word lemma and POS tags of 
up to six words (previous word, next word, word 
in position +2, head word, previous head word 
and parent word), which covers more context 
than the treelet system we used (the system im-
poses the treelet size limit of four words). This 
means that the case model can make use of much 
richer information from both the source and tar-
get than the baseline MT system. Furthermore, 
our model makes better use of the context by 
combining the contributions of multiple sources 
of knowledge using a maximum entropy model, 
rather than using the relative frequency estimates 
with a very limited amount of smoothing, which 
are used by most state-of-the art SMT systems. 
3.3 Performance on reference translations 
Before discussing the integration of the case pre-
diction model with the MT system, we present an 
evaluation of the model on the task of predicting 
the case assignment of reference translations. 
This performance constitutes an upper bound on 
the model?s performance in MT, because in ref-
erence translations, the word choice and the word 
order are perfect. 
 Table 4 summarizes the results of the refer-
ence experiments on the 5K-test set using two 
metrics: accuracy, which denotes the percentage 
of phrases for which the respective model 
guessed the case marker correctly, and BLEU 
score against the reference translation. For com-
Features Example 
Words in position  ?1 and +2 kono,moodo 
Headword & previous headword saabisu&kono 
Parent word kaishi 
Aligned word  service 
Parent of word aligned to headword started 
Next word POS NOUN 
Next word & next word POS seefu&NN 
Headword POS NOUN 
Parent headword POS VN 
Aligned to parent word POS & next word 
POS & prev word POS 
VERB&NN&an
d 
Parent POS of word aligned to headword VERB 
Aligned word POS & headword POS & 
prev word POS 
NN&NN&ADN 
POS of word aligned to headword NOUN 
Table 3: Features for the case prediction model 
52
parison, we also include results from two base-
lines: a frequency-based baseline, which always 
assigns the most likely class (NONE), and a lan-
guage model (LM) baseline, which is one of the 
standard methods of generating grammatical 
elements in MT. We trained a word-trigram LM 
using the CMU toolkit (Clarkson and Rosenfeld, 
1997) on the same 490K sentences which we 
used for training the case prediction model. 
Table 4 shows that our model performs sub-
stantially better than both baselines: the accuracy 
of the frequency-based baseline is 59%, and an 
LM-based model improves it to 87.2%. In con-
trast, our model achieves an accuracy of 95%, 
which is a 60% error reduction over the LM 
baseline. It is also interesting to note that as the 
accuracy goes up, so does the BLEU score.   
 These results show that our best model can 
very effectively predict case markers when the 
input to the model is clean, i.e., when the input 
has correct words in correct order. Next, we see 
the impact of applying this model to improve MT 
output.  
4 Integrating Case Prediction Models in 
MT 
In the end-to-end MT scenario, we integrate our 
case assignment model with the SMT system and 
evaluate its contribution to the final MT output.  
 As a method of integration with the MT sys-
tem, we chose an n-best re-ranking approach, 
where the baseline MT system is left unchanged 
and additional models are integrated in the form 
of feature functions via re-ranking of n-best lists 
from the system. Such an approach has been 
taken by Och et al (2004) for integrating sophis-
ticated syntax-informed models in a phrase-
based SMT system. We also chose this approach 
for ease of implementation: as discussed in Sec-
tion 3.2, the features we use in our case model 
extend over long distance, and are not readily 
available during decoding. Though a tighter inte-
gration with the decoding process is certainly 
worth exploring in the future, we have taken an 
approach here that allows fast experimentation.  
 Within the space of n-best re-ranking, we 
have considered two variations: the standard n-
best re-ranking method, and our significantly 
better performing extension. These are now dis-
cussed in turn.  
4.1 Method 1: Standard n-best re-ranking 
This method is a straightforward application of 
the n-best re-ranking approach described in Och 
et al (2004). As described in Section 2.2, our 
baseline SMT system is a linear model which 
weighs the values of ten feature functions. To 
integrate a case prediction model, we simply add 
it to the linear model as an 11th feature function, 
whose value is the log-probability of the case 
assignment of the candidate hypothesis t accord-
ing to our model. The weights of all feature func-
tions are then re-estimated using max-BLEU 
training on the n-best list of the lambda-1K set in 
Table 2. As we show in Section 5, this re-ranking 
method did not result in good performance.  
4.2 Method 2: Re-ranking of expanded 
candidate lists 
A drawback of the previous method is that in an 
n-best list, there may not be sufficiently many 
case assignment variations of existing hypothe-
ses. If this is the case, the model cannot be effec-
tive in choosing a hypothesis with a good case 
assignment. We performed a simple experiment 
to test this. We took the first (best) hypothesis t 
from the MT system and generated the top 40 
case variations t? of t, according to the case as-
signment model. These variations differ from t 
only in their case markers. We wanted to see 
what fraction of these new hypotheses t? oc-
curred in a 1000-best list of the MT system. In 
the dev-1K set of Table 2, the fraction of new 
case variations of the first hypothesis occurring 
in the 1000-best list of hypotheses was 0.023. 
This means that only less than one (2.3% of 40 = 
0.92) case variant of the first hypothesis is ex-
pected to be found in the 1000-best list, indicat-
ing that even an n-best list for a reasonably large 
n (such as 1000) does not contain enough candi-
dates varying in case marker assignment. 
 In order to allow more case marking candi-
dates to be considered, we propose the following 
method to expand the candidate translation list: 
for each translation t in the n-best list of the base-
line SMT system, we also consider case assign-
ment variations of t. For simplicity, we chose to 
consider the top k case assignment variations of 
each hypothesis according to our case model,3 
for 1 ? k ? 40.4  
                                               
3
 From a computational standpoint, it is non-trivial to con-
Model ACC BLEU 
Baseline (frequency) 58.9 40.0 
Baseline (490K LM) 87.2 83.6 
Log-linear model 94.9 93.0 
Table 4: Accuracy (%) and BLEU score for case 
prediction when given correct context (reference 
translations) on the 5K-test set 
53
  After we expand the translation candidate set, 
we compute feature functions for all candidates 
and train a linear model which chooses from this 
larger set. While some features (e.g., word count 
feature) are easy to recompute for a new candi-
date, other features (e.g., treelet phrase transla-
tion probability) are difficult to recompute. We 
have chosen to recompute only four features of 
the baseline model:  the language model feature, 
the word count feature, and the direct and reverse 
whole-sentence IBM Model 1 features,  assum-
ing that the values of the other baseline model 
features for a casing variation t? of t are the same 
as their values for t. In addition, we added the 
following four feature functions, specifically 
meant to capture the extent to which the newly 
generated case marking variations differ from the 
original baseline system hypotheses they are de-
rived from: 
 Generated: a binary feature with a value of 0 
for original baseline system candidates, and a 
value of 1 for newly generated candidates. 
 Number NONE?non-NONE: the count of case 
markers changed from NONE to non-NONE 
with respect to an original translation candi-
date. 
 Number non-NONE?NONE: the count of case 
markers changed from non-NONE to NONE. 
 Number non-NONE?non-NONE: the count of 
case markers changed from non-NONE to an-
other non-NONE case marker. 
Note that these newly defined features all have a 
value of 0 for original baseline system candidates 
(i.e., when k=0) and therefore would have no 
effect in Method 1. Therefore, the only differ-
ence between our two methods of integration is 
the presence or absence of case-expanded candi-
date translations. 
5 Experiments and Results  
5.1 Data and settings 
For our end-to-end MT experiments, we used 
three datasets in Table 2 that are disjoint from 
the train-500K data set. They consist of source 
English sentences and their top 1000 candidate 
translations produced by the baseline SMT sys-
                                                                       
sider all possible case assignment variations of a hypothesis: 
even though the case assignment score for a sentence is 
locally decomposable, there are still global dependencies in 
the linear model from Equation (1) due to the reverse 
whole-sentence IBM model 1 score used as a feature func-
tion.  
4
 Our results indicate that additional case variations would 
not be helpful. 
tem. These datasets are the lambda-1K set for 
training the weights   of the linear model from 
Equation (1), the dev-1K set for model selection, 
and the test-2K set for final testing including 
human evaluation. 
5.2 Results  
The results for the end-to-end experiments on the 
dev-1K set are summarized in Table 5. The table 
is divided into four sections. The first section 
(row) shows the BLEU score of the baseline 
SMT system, which is equivalent to the 1-best 
re-ranking scenario with no case expansion. The 
BLEU score for the baseline was 37.99. In the 
table, we also show the oracle BLEU scores for 
each model, which are computed by greedily se-
lecting the translation in the candidate list with 
the highest BLEU score.5 
The second section of Table 5 corresponds to 
the results obtained by Method 1, i.e., the stan-
dard n-best re-ranking, for n = 20, 100, and 1000. 
Even though the oracle scores improve as n is 
increased, the actual performance improves only 
slightly. These results show that the strategy of 
only including the new information as features in 
a standard n-best re-ranking scenario does not 
lead to an improvement over the baseline. 
 In contrast, Method 2 obtains notable im-
provements over the baseline. Recall that we ex-
pand the n-best SMT candidates with their k-best 
case marking variations in this method, and re-
                                               
5
 A modified version of BLEU was used to compute sen-
tence-level BLEU in order to select the best hypothesis per 
sentence. The table shows corpus-level BLEU on the result-
ing set of translations. 
Models #MT 
hypothe
ses 
#case  
expan-
sions 
BLEU Oracle 
BLEU 
Baseline 1 0 37.99 37.99 
 20 0 37.83 41.79 
Method 1 100 0 38.02 42.79 
 1000 0 38.08 43.14 
 1 1 38.18 38.75 
Method 2 1 10 38.42 40.51 
 1 20 38.54 41.15 
 1 40 38.41 41.74 
 20 10 38.91 45.32 
 20 20 38.72 45.94 
Method 2 20 40 38.78 46.56 
 100 10 38.73 46.87 
 100 20 38.64 47.47 
 100 40 38.74 47.96 
Table 5. Results of end-to-end experiments on the 
dev-1K set 
54
train the model parameters on the resulting can-
didate lists. For the values n=1 and k=1 (which 
we refer to as 1best-1case), we observe a small 
BLEU gain of .19 over the baseline. Even though 
this is not a big improvement, it is still better 
than the improvement of standard n-best re-
ranking with a 1000-best list. By considering 
more case marker variations (k = 10, 20 and 40), 
we are able to gain about a half BLEU point over 
the baseline. The fact that using more case varia-
tions performs better than using only the best 
case assignment candidate proposed by the case 
model suggests that the proposed approach, 
which integrates the case prediction model as a 
feature function and retrains the weights of the 
linear model, works better than using the case 
prediction model as a post-processor of the MT 
output.  
The last section of the table explores combi-
nations of the values for n and k. Considering 20 
best SMT candidates and their top 10 case varia-
tions gave the highest BLEU score on the dev-
1K set of 38.91, which is an 0.92 BLEU points 
improvement over the baseline. Considering 
more case variations (20 or 40), and more SMT 
candidates (100) resulted in a similar but slightly 
lower performance in BLEU. This is presumably 
because the case model does affect the choice of 
content words as well, but this influence is lim-
ited and can be best captured when using a small 
number (n=20) of baseline system candidates.  
Based on these results on the dev-1K set, we 
chose the best model (i.e., 20-best-10case) and 
evaluated it on the test-2K set against the base-
line. Using the pair-wise statistical test design 
described in Collins et al (2005), the BLEU im-
provement (35.53 vs. 36.29) was statistically 
significant (p < .01) according to the Wilcoxon 
signed-rank test. 
5.3 Human evaluation 
These results demonstrate that the proposed 
model is effective at improving the translation 
quality according to the BLEU score. In this sec-
tion, we report the results of human evaluation to 
ensure that the improvements in BLEU lead to 
better translations according to human evaluators. 
 We performed human evaluation on the 
20best-10case (n=20, k=10) and 1best-40case 
(n=1, k=40) models against the baseline using 
our final test set, the test-2K data. The perform-
ance in BLEU of these models on the full test-2K 
data was 35.53 for the baseline, 36.09 for the 
1best-40case model, and 36.29 for the 20best-
10case model, respectively. 
In our human evaluation, two annotators were 
asked to evaluate a random set of 100 sentences 
for which the models being compared produced 
different translations. The judges were asked to 
compare two translations, the baseline output 
from the original SMT system and the output 
chosen by the system augmented with the case 
marker generation component. Each judge was 
asked to run two separate evaluations along dif-
ferent evaluation criteria. In the evaluation of 
fluency, the judges were asked to decide which 
translation is more readable/grammatical, ignor-
ing the reference translation. In the evaluation of 
adequacy, they were asked to judge which trans-
lation more correctly reflects the meaning of the 
reference translation. In either setting, they were 
not given the source sentence.  
 Table 6 summarizes the results of the evalua-
tion of the 20best-10case model. The table shows 
the results along two evaluation criteria sepa-
rately, fluency on the left and adequacy on the 
right. The evaluation results of Annotator #1 are 
shown in the columns, while those of Annotator 
#2 are in the rows. Each grid in the table shows 
the number of sentences the annotators classified 
as the proposed system output better (S), the 
baseline system better (B) or the translations are 
of equal quality (E). Along the diagonal (in bold-
face) are the judgments that were agreed on by 
the two annotators: both annotators judged the 
output of the proposed system to be more fluent 
in 27 translations, less fluent in 9 translations; 
they judged that our system output was more 
adequate in 17 translations and less adequate in 9 
translations. Our system output was thus judged 
better under both criteria, though according to a 
sign test, the improvement is statistically signifi-
cant (p < .01) in fluency, but not in adequacy.  
One of the reasons for this inconclusive result 
is that human evaluation may be very difficult 
and can be unreliable when evaluating very dif-
ferent translation candidates, which happens of-
ten when comparing the results of models that 
consider n-best candidates where n>1, as is the 
case with the 20best-10case model. In Table 6, 
Fluency Adequacy 
Annotator #1 Annotator #1 
 
S B E S B E 
S 27 1 8 17 0 9 
B 1 9 16 0 9 12 
Anno- 
tator 
#2 E 7 4 27 9 8 36 
Table 6. Results of human evaluation comparing 
20best-10case vs. baseline. S: proposed system is bet-
ter; B: baseline is better; E: of equal quality  
55
we can see that the raw agreement rate between 
the two annotators (i.e., number of agreed judg-
ments over all judgments) is only 63% (27+9+27 
/100) in fluency and 62% (17+9+36/100) in ade-
quacy. We therefore performed an additional 
human evaluation where translations being com-
pared differ only in case markers: the baseline vs. 
the 1best-40case model output. The results are 
shown in Table 7.  
This evaluation has a higher rate of agreement, 
74% for fluency and 71% for adequacy, indicat-
ing that comparing two translations that differ 
only minimally (i.e., in case markers) is more 
reliable. The improvements achieved by our 
model are statistically significant in both fluency 
and adequacy according to a sign test; in particu-
lar, it is remarkable that on 42 sentences, the 
judges agreed that our system was better in flu-
ency, and there were no sentences on which the 
judges agreed that our system caused degradation. 
This means that the proposed system, when 
choosing among candidates differing only in case 
markers, can improve the quality of MT output 
in an extremely precise manner, i.e. making im-
provements without causing degradations. 
6 Conclusion 
We have described a method of using a case 
marker generation model to improve the quality 
of English-to-Japanese MT output. We have 
shown that the use of such a model contributes to 
improving MT output, both in BLEU and human 
evaluation. We have also proposed an extension 
of n-best re-ranking which significantly outper-
formed standard n-best re-ranking. This method 
should be generally applicable to integrating 
models which target specific phenomena in 
translation, and for which an extremely large n-
best list would be needed to cover enough vari-
ants of the phenomena in question. 
Our model improves the quality of generated 
case markers in an extremely precise manner. 
We believe this result is significant, as there are 
many phenomena in the target language of MT 
that may be improved by using special-purpose 
models, including the generation of articles, aux-
iliaries, inflection and agreement. We plan to 
extend and generalize the current approach to 
cover these phenomena in morphologically com-
plex languages in general in the future. 
References 
Clarkson, P.R. and R. Rosenfeld. 1997. Statistical 
Language Modeling Using the CMU-Cambridge 
Toolkit. In ESCA Eurospeech, pp. 2007-2010. 
Collins, M., P. Koehn and I. Ku?erov?. 2005. Clause 
Restructuring for Statistical Machine Translation. 
In ACL, pp.531-540.  
Chiang, D. 2005. A Hierarchical Phrase-based Model 
for Statistical Machine Translation. In ACL. 
Galley, M., J. Graehl, K. Knight, D. Marcu, S. 
DeNeefe, W. Wang and I. Thayer. 2006. Scalable 
Inference and Training of Context-Rich Syntactic 
Translation Models. In ACL. 
Koehn, P., F. J. Och and D. Marcu. 2003. Statistical 
Phrase-based Translation. In HLT-NAACL. 
Haji?, J., M. ?mejrek, B. Dorr, Y. Ding, J. Eisner, D. 
Gildea, T. Koo, K. Parton, G. Penn, D. Radev and 
O. Rambow. 2002. Natural Language Generation 
in the Context of Machine Translation. Technical 
report, Center for Language and Speech Process-
ing, Johns Hopkins University 2002 Summer Work-
shop Final Report.  
Knight, K. and I. Chander. 1994. Automatic Postedit-
ing of Documents. In AAAI.  
McCallum, A. 2003. Efficiently inducing features of 
conditional random fields. In UAI. 
Och, F. J. 2003. Minimum Error-rate Training for 
Statistical Machine Translation. In ACL. 
Och, F. J. and H. Ney. 2000. Improved Statistical 
Alignment Models. In ACL.  
Och, F. J. and H. Ney. 2002. Discriminative Training 
and Maximum Entropy Models for Statistical Ma-
chine Translation. In ACL 2002. 
Och, F. J., D. Gildea, S. Khudanpur, A. Sarkar, K. 
Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, 
K. Eng, V. Jain, Z. Jin and D. Radev. 2004. A 
Smorgasbord of Features for Statistical Machine 
Translation. In NAACL. 
Papineni, K., S. Roukos, T. Ward and W.J. Zhu. 2002. 
BLEU: A Method for Automatic Evaluation of 
Machine Translation. In ACL.  
Quirk, C., A. Menezes and C. Cherry. 2005. Depend-
ency Tree Translation: Syntactically Informed 
Phrasal SMT. In ACL. 
Suzuki, H. and K. Toutanova. 2006. Learning to Pre-
dict Case Markers in Japanese. In ACL-COLING. 
Vogel, S., Y. Zhang, F. Huang, A. Tribble, A. 
Venugopal, B. Zhao and A. Waibel. 2003. The 
CMU Statistical Machine Translation System. In 
Proceedings of the MT Summit.  
Fluency Adequacy 
Annotator #1 Annotator #1 
 
S B E S B E 
S 42 0 9 30 1 9 
B 1 0 7 0 9 7 
Anno- 
tator 
#2 E 7 2 32 9 3 32 
Table 7. Results of human evaluation comparing 
1best-40case vs. baseline  
56
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 191?199,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Japanese Query Alteration Based on Semantic Similarity
Masato Hagiwara
Nagoya University
Furo-cho, Chikusa-ku
Nagoya 464-8603, Japan
hagiwara@kl.i.is.nagoya-u.ac.jp
Hisami Suzuki
Microsoft Research
One Microsoft Way
Redmond, WA 98052, USA
hisamis@microsoft.com
Abstract
We propose a unified approach to web search
query alterations in Japanese that is not lim-
ited to particular character types or ortho-
graphic similarity between a query and its al-
teration candidate. Our model is based on pre-
vious work on English query correction, but
makes some crucial improvements: (1) we
augment the query-candidate list to include
orthographically dissimilar but semantically
similar pairs; and (2) we use kernel-based
lexical semantic similarity to avoid the prob-
lem of data sparseness in computing query-
candidate similarity. We also propose an ef-
ficient method for generating query-candidate
pairs for model training and testing. We show
that the proposed method achieves about 80%
accuracy on the query alteration task, improv-
ing over previously proposed methods that use
semantic similarity.
1 Introduction
Web search query correction is an important prob-
lem to solve for robust information retrieval given
how pervasive errors are in search queries: it is said
that more than 10% of web search queries contain
errors (Cucerzan and Brill, 2004). English query
correction has been an area of active research in re-
cent years, building on previous work on general-
purpose spelling correction. However, there has
been little investigation of query correction in lan-
guages other than English.
In this paper, we address the issue of query cor-
rection, and more generally, query alteration in
Japanese. Japanese poses particular challenges to
the query correction task due to its complex writ-
ing system, summarized in Fig. 11. There are four
1The figure is somewhat over-simplified as it does not in-
clude any word consisting of multiple character types. It also
does not include examples of spelling mistakes and variants in
word segmentation.
Kanji
Sp: ?????
Abbr: ???????
Abbr: ?????????
Hiragana
Sp: ?????????
Roman
Alphabet
Sp: Ohno~Oono
Sp: center~centre
Katakana
Sp: ????????????
Abbr: ??????????
Abbr: ?????????????
Sp: Fedex???????
Abbr: MS????????
Sp: ?????
Syn: ?????????
Sp: ???????
Sp: ??????
Sp:???????
Syn: ??????
Syn: ????ANA
Figure 1: Japanese character types and spelling variants
main character types, including two types of kana
(phonetic alphabet - hiragana and katakana), kanji
(ideographic - characters represent meaning) and
Roman alphabet; a word can be legitimately spelled
in multiple ways, combining any of these character
sets. For example, the word for ?protein? can be
spelled as ?????? (all in hiragana), ????
? (katakana+kanji), ??? (all in kanji) or ???
? (hiragana+kanji), all pronounced in the same way
(tanpakushitsu). Some examples of these spelling
variants are shown in Fig. 1 with the prefix Sp: as is
observed from the figure, spelling variation occurs
within and across different character types. Resolv-
ing these variants will be essential not only for in-
formation retrieval but practically for all NLP tasks.
A particularly prolific source of spelling varia-
tions in Japanese is katakana. Katakana charac-
ters are used to transliterate words from English and
other foreign languages, and as such, the variations
in the source language pronunciation as well as the
ambiguity in sound adaptation are reflected in the
katakana spelling. For example, Masuyama et al
(2004) report that at least six distinct translitera-
tions of the word ?spaghetti? (??????, ???
???, etc.) are attested in the newspaper corpus
they studied. Normalizing katakana spelling varia-
tions has been the subject of research by itself (Ara-
maki et al, 2008; Masuyama et al, 2004). Similarly,
English-to-katakana transliteration (e.g., ?fedex? as
?????? fedekkusu in Fig. 1) and katakana-to-
191
English back-transliteration (e.g.,?????? back
into ?fedex?) have also been studied extensively (Bi-
lac and Tanaka, 2004; Brill et al, 2001; Knight and
Graehl, 1998), as it is an essential component for
machine translation. To our knowledge, however,
there has been no work that addresses spelling vari-
ation in Japanese generally.
In this paper, we propose a general approach to
query correction/alteration in Japanese. Our goal is
to find precise re-write candidates for a query, be
it a correction of a spelling error, normalization of
a spelling variant, or finding a strict synonym in-
cluding abbreviations (e.g., MS ???????
?Microsoft?, prefixed by Abbr in Fig. 1) and true
synonyms (e.g., ?? (translation of ?seat?) ???
(transliteration of ?seat?, indicated by Syn in Fig. 1)2.
Our method is based on previous work on English
query correction in that we use both spelling and se-
mantic similarity between a query and its alteration
candidate, but is more general in that we include al-
teration candidates that are not similar to the original
query in spelling. In computing semantic similar-
ity, we adopt a kernel-based method (Kandola et al,
2002), which improves the accuracy of the query al-
teration results over previously proposed methods.
We also introduce a novel approach to creating a
dataset of query and alteration candidate pairs effi-
ciently and reliably from query session logs.
2 Related Work
The key difference between traditional general-
purpose spelling correction and search query cor-
rection lies in the fact that the latter cannot rely on
a lexicon: web queries are replete with valid out-
of-dictionary words which are not mis-spellings of
in-vocabulary words. Cucerzan and Brill (2004) pi-
oneered the research of query spelling correction,
with an excellent description of how a traditional
dictionary-based speller had to be adapted to solve
the realistic query correction problem. The model
they proposed is a source-channel model, where the
source model is a word bigram model trained on
query logs, and the channel model is based on a
weighted Damerau-Levenshtein edit distance. Brill
2Our goal is to harvest alternation candidates; therefore, ex-
actly how they are used in the search task (whether it is used to
substitute the original query, to expand it, or simply to suggest
an alternative) is not a concern to us here.
and Moore (2000) proposed a general, improved
source model for general spelling correction, while
Ahmad and Kondrak (2005) learned a spelling error
model from search query logs using the Expectation
Maximization algorithm, without relying on a train-
ing set of misspelled words and their corrections.
Extending the work of Cucerzan and Brill (2004),
Li et al (2006) proposed to include semantic sim-
ilarity between the query and its correction candi-
date. They point out that adventura is a common
misspelling of aventura, not adventure, and this can-
not be captured by a simple string edit distance, but
requires some knowledge of distributional similar-
ity. Distributional similarity is measured by the sim-
ilarity of the context shared by two terms, and has
been successfully applied to many natural language
processing tasks, including semantic knowledge ac-
quisition (Lin, 1998).
Though the use of distributional similarity im-
proved the query correction results in Li et al?s
work, one problem is that it is sparse and is not avail-
able for many rarer query strings. Chen et al (2007)
addressed this problem by using external informa-
tion (i.e., web search results); we take a different ap-
proach to solve the sparseness problem, namely by
using semantic kernels.
Jones et al (2006a) generated Japanese query al-
teration pairs from by mining query logs and built a
regression model which predicts the quality of query
rewriting pairs. Their model includes a wide variety
of orthographical features, but not semantic similar-
ity features.
3 Query Alteration Model
3.1 Problem Formulation
We employ a formulation of query alteration model
that is similar to conventional query correction mod-
els. Given a query string q as input, a query correc-
tion model finds a correct alteration c? within the
confusion set of q, so that it maximizes the posterior
probability:
c? = arg max
c?CF(q)?C
P (c|q) (1)
whereC is the set of all white-space separated words
and their bigrams in query logs in our case3, and
3In regular text, Japanese uses no white spaces to separate
words; however, white spaces are often (but not consistently)
192
CF(q) ? C is the confusion set of q, consisting of
the candidates within a certain edit distance from q,
i.e., CF(q) = {c ? C|ED(q, c) < ?}. We set ? =
24 using an unnormalized edit distance. The detail
of the edit distance ED(q, c) is described in Section
3.2. The query string q itself is contained in CF(q),
and if the model output is different from q, it means
the model suggests a query alteration. Formulated
in this way, both query error detection and alteration
are performed in a unified way.
After computing the posterior probability of each
candidate in CF(q) by the source channel model
(Section 3.2), an N-best list is obtained as the ini-
tial candidate set C0, which is then augmented by
the bootstrapping method Tchai (Section 3.4) to cre-
ate the final candidate list C(q). The candidates in
C(q) are re-ranked by a maximum entropy model
(Section 3.5) and the candidate with the highest pos-
terior probability is selected as the output.
3.2 Source Channel Model
Source channel models are widely used for spelling
and query correction (Brill and Moore, 2000;
Cucerzan and Brill, 2004). Instead of directly com-
puting Eq. (1), we can decompose the posterior
probability using Bayes? rule as:
c? = arg max
c?CF(q)?C
P (c)P (q|c), (2)
where the source model P (c) measures how proba-
ble the candidate c is, while the error model P (q|c)
measures how similar q and c are.
For the source model, an n-gram based statisti-
cal language model is the standard in previous work
(Ahmad and Kondrak, 2005; Li et al, 2006). Word
n-gram models are simple to create for English,
which is easy to tokenize and to obtain word-based
statistics, but this is not the case with Japanese.
Therefore, we simply considered the whole input
string as a candidate to be altered, and used the rel-
ative frequency of candidates in the query logs to
build the language model:
P (c) = Freq(c)?
c??C Freq(c?) . (3)
For the error model, we used an improved chan-
nel model described in (Brill and Moore, 2000),
used to separate words in Japanese search queries, due to their
keyword-based nature.
which we call the alpha-beta model in this paper.
The model is a weighted extension of the normal
Damerau-Levenshtein edit distance which equally
penalizes single character insertion, substitution, or
deletion operations (Damerau, 1964; Levenshtein,
1966), and considers generic edit operations of the
form ? ? ?, where ? and ? are any (possibly
null) strings. From misspelled/correct word pairs,
alpha-beta trains the probability P (? ? ?|PSN),
conditioned by the position PSN of ? in a word,
where PSN ? {start of word, middle of word, end of
word}. Under this model, the probability of rewrit-
ing a string w to a string s is calculated as:
P??(s|w) = max
R?Part(w),T?Part(s)
|R|?
i=1
P (Ri ? Ti|PSN(Ri)),
which corresponds to finding best partitionsR and T
in all possible partitions Part(w) and Part(s). Brill
and Moore (2000) reported that this model gave a
significant improvement over conventional edit dis-
tance methods.
Brill et al (2001) applied this model for ex-
tracting katakana-English transliteration pairs from
query logs. They trained the edit distance between
character chunks of katakana and Roman alphabets,
after converting katakana strings to Roman script.
We also trained this model using 59,754 katakana-
English pairs extracted from aligned Japanese and
English Wikipedia article titles. In this paper we al-
lowed |?|, |?| ? 3. The resulting edit distance is
obtained as the negative logarithm of the alpha-beta
probability, i.e., ED??(q|c) = ? logP??(q|c).
Since the edit operations are directional and c and
q can be any string consisting of katakana and En-
glish, distance in both directions were considered.
We also included a modified edit distance EDhd for
simple kana-kana variations after converting them
into Roman script. The distance EDhd is essen-
tially the same as the normal Damerau-Levenshtein
edit distance, with the modification that it does not
penalize character halving (aa ? a) and doubling
(a ? aa), because a large part of katakana vari-
ants only differ in halving/doubling (e.g. ?????
(supageti) vs?????? (supagetii)4.
The final error probability is obtained from the
minimum of these three distances:
4However, character length can be distinctive in katakana,
as in?? biru ?building? vs.??? biiru ?beer?.
193
ED(q, c) = min[ED??(q|c),ED??(c|q),EDhd(q, c)],(4)
P (q|c) = exp[?ED(q, c)] (5)
where every edit distance is normalized to [0, 1] by
multiplying by a factor of 2/(|q||c|) so that it does
not depend on the length of the input strings5.
3.3 Kernel-based Lexical Semantic Similarity
3.3.1 Distributional Similarity
The source channel model described in Sec-
tion 3.2 only considers language and error models
and cannot capture semantic similarity between the
query and its correction candidate. To address this
issue, we use distributional similarity (Lin, 1998) es-
timated from query logs as additional evidence for
query alteration, following Li et al (2006).
For English, it is relatively easy to define the con-
text of a word based on the bag-of-words model. As
this is not expected to work on Japanese, we de-
fine context as everything but the query string in a
query log, as Pas?ca et al (2006) and Komachi and
Suzuki (2008) did for their information extraction
tasks. This formulation does not involve any seg-
mentation or boundary detection, which makes this
method fast and robust. On the other hand, this may
cause additional sparseness in the vector representa-
tion; we address this issue in the next two sections.
Once the context of a candidate ci is de-
fined as the patterns that the candidate co-occurs
with, it can be represented as a vector ci =
[pmi(ci, p1), . . . ,pmi(ci, pM )]?, where M denotes
the number of context patterns and x? is the trans-
position of a vector (or possibly a matrix) x. The el-
ements of the vector are given by pointwise mutual
information between the candidate ci and the pattern
pj , computed as:
pmi(ci, pj) = log |ci, pj ||ci, ?||?, pj | , (6)
where |ci, pj | is the frequency of the pattern pj in-
stantiated with the candidate ci, and ?*? denotes a
5We did not include kanji variants here, because disam-
biguating kanji readings is a very difficult task, and the ma-
jority of the variations in queries are in katakana and Roman
alphabet. The framework proposed in this paper, however, can
incorporate kanji variants straightforwardly into ED(q, c) once
we have reasonable edit distance functions for kanji variations.
wildcard, i.e., |ci, ?| = ?p |ci, p| and |?, pj | =?
c |c, pj |. With these defined, the distributional
similarity can be calculated as cosine similarity. Let
c?i be the L2-normalized pattern vector of the candi-
date ci, and X = {c?i} be the candidate-pattern co-
occurrence matrix. The candidate similarity matrix
K can then be obtained asK = X ?X . In the follow-
ing, the (i, j)-element of the matrix K is denoted as
Kij , which corresponds to the cosine similarity be-
tween candidates ci and cj .
3.3.2 Semantic Kernels
Although distributional similarity serves as strong
evidence for semantically relevant candidates, di-
rectly applying the technique to query logs faces the
sparseness problem. Because context patterns are
drawn from query logs and can also contain spelling
errors, alterations, and word permutations as much
as queries do, context differs so greatly in represen-
tations that even related candidates might not have
sufficient contextual overlap between them. For
example, a candidate ?YouTube? matched against
the patterns ?YouTube+movie?, ?movie+YouTube?
and ?You-Tube+movii? (with a minor spelling er-
ror) will yield three distinct patterns ?#+movie?,
?movie+#? and ?#+movii?6, which will be treated as
three separate dimensions in the vector space model.
This sparseness problem can be partially ad-
dressed by considering the correlation between pat-
terns. Kandola et al (2002) proposed new kernel-
based similarity methods which incorporate indirect
similarity between terms for a text retrieval task. Al-
though their kernels are built on a document-term
co-occurrence model, they can also be applied to our
candidate-pattern co-occurrence model. The pro-
posed kernel is recursively defined as:
K? = ?X ?G?X + K, G? = ?XK?X ? + G, (7)
where G = XX ? is the correlation matrix between
patterns and ? is the factor to ensure that longer
range effects decay exponentially. This can be in-
terpreted as augmenting the similarity matrix K
through indirect similarities of patterns G? and vice
versa. Semantically related pairs of patterns are ex-
pected to be given high correlation in the matrix G?
and this will alleviate the sparseness problem. By
6?+? denotes a white space, and ?#? indicates where the word
of interest is found in a context pattern.
194
?YouTube?
?#+movie?
?stage6?
?You+Tube?
?movie+#? ?#+anime?
c
1
c
2
c
3
p
1
p
2
p
3
(a)
?YouTube?
?#+movie?
?stage6?
?You+Tube?
?movie+#? ?#+anime?
c
1
c
2
c
3
p
1
p
2
p
3
(b)
Figure 2: Orthographically Augmented Graph
solving the above recursive definition, one obtains
the von Neumann kernel:
K?(?) = K(I ? ?K)?1 =
??
t=1
?t?1Kt. (8)
This can also be interpreted in terms of a random
walk in a graph where the nodes correspond to all the
candidates and the weight of an edge (i, j) is given
by Kij . A simple calculation shows that Kij equals
the sum of the products of the edge weights over all
possible paths between the nodes corresponding ci
and cj in the graph. Also, Ktij corresponds to the
probability that a random walk beginning at node ci
ends up at node cj after t steps, assuming that the en-
tries are all positive and the sum of the connections
is 1 at each node. Following this notion, Kandola
et al (2002) proposed another kernel called expo-
nential kernel, with alternative faster decay factors:
K?(?) = K
??
t=1
?tKt
t! = K exp(?K). (9)
They showed that this alternative kernel achieved a
better performance for their text retrieval task. We
employed these two kernels to compute distribu-
tional similarity for our query correction task.
3.3.3 Orthographically Augmented Kernels
Although semantic relatedness can be partially
captured by the semantic kernels introduced in the
previous section, they may still have difficulties
computing correlations between candidates and pat-
terns especially for only sparsely connected graphs.
Take the graph (a) in Fig. 2 for example, which is
a simplified yet representative graph topology for
candidate-pattern co-occurrence we often encounter.
In this case K = X ?X equals I , meaning that the
connections between candidates and patterns are too
sparse to obtain sufficient correlation even when se-
mantic kernels are used.
Input
query q
0
C
Pattern
induction
Source channel
model
0
P
1
C
1
P
Instance
induction
Pattern
induction
10
)( CCqC ?=
1
P
Distributional
similarity
Figure 3: Bootstrapping Additional Candidates
In order to address this issue, we propose to aug-
ment the graph by weakly connecting the candidate
and pattern nodes as shown in the graph (b) of Fig. 2
based on prior knowledge of orthographic similarity
about candidates and patterns. This can be achieved
using the following candidate similarity matrix K+
instead of K:
K+ = ?SC + (1? ?)X ? [?SP + (1? ?)I]X (10)
where SC = {sc(i, j)} is the orthographical similar-
ity matrix of candidates in which the (i, j)-element
is given by the edit distance based similarity, i.e.,
sc(i, j) = exp [?ED(ci, cj)]. The orthographical
similarity matrix of patterns SP = {sP (i, j)} is de-
fined similarly, i.e., sP (i, j) = exp[?ED(pi, pj)].
Note that using this similarity matrix K+ can be
interpreted as a random walk process on a bipar-
tite graph as follows. Let C and P as the sets of
candidates and patterns. K+ corresponds to a sin-
gle walking step from C to C, by either remaining
within C with a probability of ? or moving to ?the
other side? P of the graph with a probability of 1??.
When the walking remains in C, it is allowed to
move to another candidate node following the candi-
date orthographic similarity SC . Otherwise it moves
to P by the matrix X , chooses either to move within
P with a probability ?SP or to stay with a probabil-
ity 1? ?, and finally comes back to C by the matrix
X ?. Multiplication (K+)t corresponds to repeating
this process t times. Using this similarity, we can de-
fine two orthographically augmented semantic ker-
nels which differ in the decaying factors, augmented
von Neumann kernel and exponential kernel:
K?+(?) = K+(I ? ?K+)?1 (11)
K?+(?) = K+ exp(?K+). (12)
3.4 Bootstrapping Additional Candidates
Now that we have a semantic model, our query
correction model can cover query-candidate pairs
195
which are only semantically related. However, pre-
vious work on query correction all used a string dis-
tance function and a threshold to restrict the space of
potential candidates, allowing only the orthographi-
cally similar candidates.
To collect additional candidates, the use of
context-based semantic extraction methods would
be effective because semantically related candidates
are likely to share context with the initial query
q, or at least with the initial candidate set C0.
Here we used the Tchai algorithm (Komachi and
Suzuki, 2008), a modified version of Espresso (Pan-
tel and Pennacchiotti, 2006) to collect such candi-
dates. This algorithm starts with initial seed in-
stances, then induces reliable context patterns co-
occurring with the seeds, induces instances from
the patterns, and iterates this process to obtain cat-
egories of semantically related words. Using the
candidates in C0 as the seed instances, one boot-
strapping iteration of the Tchai algorithm is executed
to obtain the semantically related set of instances
C1. The seed instance reliabilities are given by the
source channel probabilities P (c)P (q|c). Finally we
take the union C0 ? C1 to obtain the candidate set
C(q). This process is outlined in Fig. 3.
3.5 Maximum Entropy Model
In order to build a unified probabilistic query al-
teration model, we used the maximum entropy ap-
proach of (Beger et al, 1996), which Li et al (2006)
also employed for their query correction task and
showed its effectiveness. It defines a conditional
probabilistic distribution P (c|q) based on a set of
feature functions f1, . . . , fK :
P (c|q) = exp
?K
i=1 ?ifi(c, q)?
c exp?Ki=1 ?ifi(c, q)
, (13)
where ?1, . . . , ?K are the feature weights. The op-
timal set of feature weights ?? can be computed by
maximizing the log-likelihood of the training set.
We used the Generalized Iterative Scaling (GIS)
algorithm (Darroch and Ratcliff, 1972) to optimize
the feature weights. GIS trains conditional proba-
bility in Eq. (13), which requires the normalization
over all possible candidates. However, the number
of all possible candidates C obtained from a query
log can be very large, so we only calculated the sum
over the candidates in C(q). This is the same ap-
proach that Och and Ney (2002) took for statistical
machine translation, and Li et al (2006) for query
spelling correction.
We used the following four categories of func-
tions as the features:
1. Language model feature, given by the logarithm
of the source model probability: logP (c).
2. Error model features, which are composed of
three edit distance functions: ?ED??(q|c),
?ED??(c|q), and ?EDhd(q, c).
3. Similarity based feature, computed as the loga-
rithm of distributional similarity between q and c:
log sim(q, c), which is calcualted using one of the
following kernels (Section 3.3): K, K?, K?, K?+,
and K?+. The similarity values were normalized
to [0, 1] after adding a small discounting factor
? = 1.0? 10?5.
4. Similarity based correction candidate features,
which are binary features with a value of 1 if and
only if the frequency of c is higher than that of
q, and distributional similarity between them is
higher than a certain threshold. Li et al (2006)
used this set of features, and suggested that these
features give the evidence that q may be a com-
mon misspelling of c. The thresholds on the nor-
malized distributional similarity are enumerated
from 0.5 to 0.9 with the interval 0.1.
4 Experiment
4.1 Dataset Creation
For all the experiments conducted in this paper, we
used a subset of the Japanese search query logs sub-
mitted to Live Search (www.live.com) in November
and December of 2007. Queries submitted less than
eight times were deleted. The query log we used
contained 83,080,257 tokens and 1,038,499 unique
queries.
Models of query correction in previous work were
trained and evaluated using manually created query-
candidate pairs. That is, human annotators were
given a set of queries and were asked to provide a
correction for each query when it needed to be re-
written. As Cucerzan and Brill (2004) point out,
however, this method is seriously flawed in that the
intention of the original query is completely lost to
the annotator, without which the correction is often
impossible: it is not clear if gogle should be cor-
rected to google or goggle, or neither ? gogle may
be a brand new product name. Cucerzan and Brill
196
therefore performed a second evaluation, where the
test data was drawn by sampling the query logs for
successive queries (q1, q2) by the same user where
the edit distance between q1 and q2 are within a cer-
tain threshold, which are then submitted to annota-
tors for generating the correction. While this method
makes the annotation more reliable by relying on
user (rather than annotator) reformulation, the task
is still overly difficult: going back to the example
in Section 1, it is unclear which spelling of ?protein?
produces the best search results? it can only be em-
pirically determined. Their method also eliminates
all pairs of candidates that are not orthographically
similar. We have therefore improved their method
in the following manner, making the process more
automated and thus more reliable.
We first collected a subset of the query log that
contains only those pairs (q1, q2) that are issued suc-
cessively by the same user, q2 is issued within 3 min-
utes of q1, and q2 resulted in a click of the resulting
page while q1 did not. The last condition adds the
evidence that q2 was a better formulation than q1.
We then ranked the collected query pairs using log-
likelihood ratio (LLR) (Dunning, 1993), which mea-
sures the dependence between q1 and q2 within the
context of web queries (Jones et al, 2006b). We ran-
domly sampled 10,000 query pairs with LLR? 200,
and submitted them to annotators, who only confirm
or reject a query pair as being synonymous. For ex-
ample, q1 = nikon and q2 = canon are related but
not synonymous, while we are reasonably sure q1 =
ipot and q2 = ipod are synonymous, given that this
pair has a high LLR value. This verification process
is extremely fast and consistent across annotators:
it takes less than 1 hour to go through 1,000 query
pairs, and the inter-annotator agreement rate of two
annotators on 2,000 query pairs was 95.7%. We
annotated 10,000 query pairs consisting of alpha-
numerical and kana characters in this manner. After
rejecting non-synonymous pairs and those which do
not co-occur with any context patterns, 6,489 pairs
remained, and we used 1,243 pairs for testing, 628
as a development set, and 4,618 for training the max-
imum entropy model.
4.2 Experimental Settings
The performance of query alteration was evaluated
based on the following measures (Li et al, 2006).
Table 1: Performance results (%)
Model Accuracy Recall Precision
SC 71.12 39.29 45.09
ME-NoSim 74.58 44.58 52.52
ME-Cos 74.18 45.84 50.70
ME-vN 74.34 45.59 52.16
ME-Exp 73.61 44.84 50.57
ME-vN+ 75.06 44.33 53.01
ME-Exp+ 75.14 44.08 53.52
The input queries, correct suggestions, and outputs
were matched in a case-insensitive manner.
? Accuracy: The number of correct outputs gener-
ated by the system divided by the total number of
queries in the test set;
? Recall: The number of correct suggestions for al-
tered queries divided by the total number of al-
tered queries in the test set;
? Precision: The number of correct suggestions for
altered queries divided by the total number of al-
terations made by the system.
The parameters for the kernels, namely, ?, ?, and
?, are tuned using the development set. The finally
employed values are: ? = 0.3 for K?, K?, and K?+,
? = 0.2 for K?+, ? = 0.2 and ? = 0.4 for K?+, and
? = 0.35 and ? = 0.7 for K?+. In the source channel
model, we manually scaled the language probability
by a factor of 0.1 to alleviate the bias toward highly
frequent candidates.
As the initial candidate set C0, top-50 instances
were selected by the source channel model, and 100
patterns were extracted as P0 by the Tchai iteration
after removing generic patterns, which we detected
simply by rejecting those which induced more than
200 unique instances. Finally top-30 instances were
induced using P0 to create C1. Generic instances
were not removed in this process because they may
still be alterations of input query q. The maximum
size of P1 was set to 2,000, after removing unreliable
patterns with reliability smaller than 0.0001.
4.3 Results
Table 1 shows the evaluation results. SC is the
source channel model, while the others are maxi-
mum entropy (ME) models with different features.
ME-NoSim uses the same features as SC, but con-
siderably outperforms SC in all three measures, con-
firming the superiority of the ME approach. Decom-
posing the three edit distance functions into three
197
separate features in the ME model may also explain
the better result. All the ME approaches outper-
formed SC in accuracy with a statistically significant
difference (p < 0.0001 on McNemar?s test).
The model with the cosine similarity (ME-Cos)
in addition to the basic set of features yielded higher
recall compared to ME-NoSim, but decreased accu-
racy and precision, which are more important than
recall for our purposes because a false alteration
does more damage than no alteration. This is also
the case when the kernel-based methods, ME-vN
(the von Neumann kernel) and ME-Exp (the expo-
nential kernel), are used in place of the cosine sim-
ilarity. This shows that using semantic similarity
does not always help, which we believe is due to
the sparseness of the contextual information used in
computing semantic similarity.
On the other hand, ME-vN+ (with augmented von
Neumann kernel) and ME-Exp+ (with augmented
exponential kernel) increased both accuracy and pre-
cision with a slight decrease of recall, compared to
the distributional similarity baseline and the non-
augmented kernel-based methods. ME-Exp+ was
significantly better than ME-Exp (p < 0.01).
Note that the accuracy values appear lower than
some of the previous results on English (e.g., more
than 80% in (Li et al, 2006)), but this is because
the dataset creation method we employed tends to
over-represent the pairs that lead to alteration: the
simplest baseline (= always propose no alteration)
performs 67.3% accuracy on our data, in contrast to
83.4% on the data used in (Li et al, 2006).
Manually examining the suggestions made by the
system also confirms the effectiveness of our model.
For example, the similarity-based models altered the
query ipot to ipod, while the simple ME-NoSim
model failed, because it depends too much on the
edit distance-based features. We also observed that
many of the suggestions made by the system were
actually reasonable, even though they were differ-
ent from the annotated gold standard. For example,
ME-vN+ suggests a re-write of the query 2tyann as
2????? (?2-channel?), while the gold standard
was an abbreviated form 2??? (?2-chan?).
To incorporate such possibly correct candidates
into account, we conducted a follow-up experiment
where we considered multiple reference alterations,
created automatically from our data set in the fol-
Table 2: Performance with the multiple reference model
Model Accuracy Recall Precision
SC 75.30 48.61 55.78
ME-NoSim 79.49 56.17 66.17
ME-Cos 79.32 58.19 64.35
ME-vN 79.24 57.18 65.42
ME-Exp 78.52 56.42 63.64
ME-vN+ 79.89 55.67 66.57
ME-Exp+ 79.81 54.91 66.67
lowing manner. Suppose that a query q1 is corrected
as q2, and q2 is corrected as q3 in our annotated data.
If this is the case, we considered q1 ? q3 as a valid
alteration as well. By applying this chaining oper-
ation up to 5 degrees of separation, we re-created a
set of valid alterations for each input query. Note
that directionality is important ? in the above ex-
ample, q1 ? q3 is valid, while q3 ? q1 is not. Table
2 shows the results of evaluation with multiple refer-
ences. The numbers substantially improved over the
single reference cases, as expected, but did not af-
fect the relative performance of each model. Again,
the differences in accuracy between the SC and ME
models, and ME-Exp and ME-Exp+ were statisti-
cally significant (p < 0.01).
5 Conclusion and future work
In this paper we have presented a unified approach
to Japanese query alteration. Our approach draws
on previous research in English spelling and query
correction, Japanese katakana variation and translit-
eration, and semantic similarity, and builds a model
that makes improvements over previously proposed
query correction methods. In particular, the use of
orthographically augmented semantic kernels pro-
posed in this paper is general and applicable to other
languages, including English, for query alteration,
especially when the data sparseness is an issue. In
the future, we also plan to investigate other meth-
ods, such as PLSI (Hofmann, 1999), to deal with
data sparseness in computing semantic similarity.
Acknowledgments
This research was conducted during the first au-
thor?s internship at Micorosoft Research. We thank
the colleagues, especially Dmitriy Belenko, Chris
Brockett, Jianfeng Gao, Christian Ko?nig, and Chris
Quirk for their help in conducting this research.
198
References
Farooq Ahmad and Grzegorz Kondrak. 2005. Learning
a spelling error model from search query logs. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-2005), pages
955?962.
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko
Ohe. 2008. Orthographic disambiguation incorporat-
ing transliterated probability. In Proceedings in the
third International Joint Conference on Natural Lan-
guage Processing (IJCNLP-2008), pages 48?55.
Adam L. Beger, Stephen A. Della Pietra, and Vincent
J. Della Pietra. 1996. A maximum entropy approach
to natural language processing. Computational Lin-
guistics, 22(1):39?72.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid back-
transliteration system for japanese. In Proceedings of
the 20th international conference on Computational
Linguistics (COLING-2004), pages 597?603.
Eric Brill and Robert C. Moore. 2000. An improved er-
ror model for noisy channel spelling. In Proceedings
of the 38th Annual Meeting on Association for Com-
putational Linguistics (ACL-2000), pages 286?293.
Eric Brill, Gary Kacmarcik, and Chris Brockett. 2001.
Automatically harvesting katakana-english term pairs
from search engine query logs. In Proceedings of the
Sixth Natural Language Processing Pacific Rim Sym-
posium (NLPRS-2001), pages 393?399.
Qing Chen, Mu Li, , and Ming Zhou. 2007. Improv-
ing query spelling correction using web search results.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 181?189.
Silviu Cucerzan and Eric Brill. 2004. Spelling correc-
tion as an iterative process that exploits the collective
knowledge of web users. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2004), pages 293?300.
Fred Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communication
of the ACM, 7(3):659?664.
J.N. Darroch and D. Ratcliff. 1972. Generalized iterative
scaling for log-linear models. Annuals of Mathemati-
cal Statistics, 43:1470?1480.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19(1):61?74.
Thomas Hofmann. 1999. Probabilistic latent semantic
indexing. In Research and Development in Informa-
tion Retrieval, pages 50?57.
Rosie Jones, Kevin Bartz, Pero Subasic, and Benjamin
Rey. 2006a. Automatically generating related queries
in japanese. Language Resources and Evaluation
(LRE), 40(3-4):219?232.
Rosie Jones, Benjamin Rey, Omid Madani, and Wiley
Greiner. 2006b. Generating query substitutions. In
Proceedings of the 15th international World Wide Web
conference (WWW-06), pages 387?396.
Jaz Kandola, John Shawe-Taylor, and Nello Cristianini.
2002. Learning semantic similarity. In Neural Infor-
mation Processing Systems (NIPS 15), pages 657?664.
Kevin Knight and Jonathan Graehl. 1998. Machine
transliteration. Computational Linguistics, 24(4):599?
612.
Mamoru Komachi and Hisami Suzuki. 2008. Mini-
mally supervised learning of semantic knowledge from
query logs. In Proceedings of the 3rd International
Joint Conference on Natural Language Processing
(IJCNLP-2008), pages 358?365.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions and reversals. Soviet
Physice - Doklady, 10:707?710.
Mu Li, Muhua Zhu, Yang Zhang, and Ming Zhou.
2006. Exploring distributional similarity based mod-
els for query spelling correction. In Proceedings of
COLING/ACL-2006, pages 1025?1032.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL-1998,
pages 786?774.
Takeshi Masuyama, Satoshi Sekine, and Hiroshi Nak-
agawa. 2004. Automatic construction of japanese
katakana variant list from large corpus. In Proceed-
ings of Proceedings of the 20th international confer-
ence on Computational Linguistics (COLING-2004),
pages 1214?1219.
Franz Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of the 40th an-
nual meeting of ACL, pages 295?302.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Organizing and searching
the world wide web of facts - step one: the one-million
fact extraction challenge. In Proceedings of the 21st
National Conference on Artificial Intelligence (AAAI-
06), pages 1400?1405.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically har-
vesting semantic relations. In Proceedings of
COLING/ACL-2006, pages 113?120.
199
Unsupervised Learning of Dependency Structure for Language Modeling  
Jianfeng Gao  
Microsoft Research, Asia  
49 Zhichun Road, Haidian District 
Beijing 100080 China 
jfgao@microsoft.com  
Hisami Suzuki  
Microsoft Research 
One Microsoft Way 
 Redmond WA 98052 USA  
hisamis@microsoft.com 
 
Abstract 
This paper presents a dependency language 
model (DLM) that captures linguistic con-
straints via a dependency structure, i.e., a set 
of probabilistic dependencies that express 
the relations between headwords of each 
phrase in a sentence by an acyclic, planar, 
undirected graph. Our contributions are 
three-fold. First, we incorporate the de-
pendency structure into an n-gram language 
model to capture long distance word de-
pendency. Second, we present an unsuper-
vised learning method that discovers the 
dependency structure of a sentence using a 
bootstrapping procedure. Finally, we 
evaluate the proposed models on a realistic 
application (Japanese Kana-Kanji conver-
sion). Experiments show that the best DLM 
achieves an 11.3% error rate reduction over 
the word trigram model. 
1 Introduction 
In recent years, many efforts have been made to 
utilize linguistic structure in language modeling, 
which for practical reasons is still dominated by 
trigram-based language models. There are two 
major obstacles to successfully incorporating lin-
guistic structure into a language model: (1) captur-
ing longer distance word dependencies leads to 
higher-order n-gram models, where the number of 
parameters is usually too large to estimate; (2) 
capturing deeper linguistic relations in a language 
model requires a large annotated training corpus 
and a decoder that assigns linguistic structure, 
which are not always available. 
This paper presents a new dependency language 
model (DLM) that captures long distance linguistic 
constraints between words via a dependency 
structure, i.e., a set of probabilistic dependencies 
that capture linguistic relations between headwords 
of each phrase in a sentence. To deal with the first 
obstacle mentioned above, we approximate 
long-distance linguistic dependency by a model that 
is similar to a skipping bigram model in which the 
prediction of a word is conditioned on exactly one 
other linguistically related word that lies arbitrarily 
far in the past. This dependency model is then in-
terpolated with a headword bigram model and a 
word trigram model, keeping the number of pa-
rameters of the combined model manageable. To 
overcome the second obstacle, we used an unsu-
pervised learning method that discovers the de-
pendency structure of a given sentence using an 
Expectation-Maximization (EM)-like procedure. In 
this method, no manual syntactic annotation is 
required, thereby opening up the possibility for 
building a language model that performs well on a 
wide variety of data and languages. The proposed 
model is evaluated using Japanese Kana-Kanji 
conversion, achieving significant error rate reduc-
tion over the word trigram model.   
2 Motivation 
A trigram language model predicts the next word 
based only on two preceding words, blindly dis-
carding any other relevant word that may lie three 
or more positions to the left. Such a model is likely 
to be linguistically implausible: consider the Eng-
lish sentence in Figure 1(a), where a trigram model 
would predict cried from next seat, which does not 
agree with our intuition. In this paper, we define a 
dependency structure of a sentence as a set of 
probabilistic dependencies that express linguistic 
relations between words in a sentence by an acyclic, 
planar graph, where two related words are con-
nected by an undirected graph edge (i.e., we do not 
differentiate the modifier and the head in a de-
pendency). The dependency structure for the sen-
tence in Figure 1(a) is as shown; a model that uses 
this dependency structure would predict cried from 
baby, in agreement with our intuition. 
 
 
(a) [A baby] [in the next seat] cried [throughout the flight] 
 
 
 
(b) [/] [/	] [
/	] [/] [] [/]
 
Figure 1. Examples of dependency structure. (a) A 
dependency structure of an English sentence. Square 
brackets indicate base NPs; underlined words are the 
headwords. (b) A Japanese equivalent of (a). Slashes 
demarcate morpheme boundaries; square brackets 
indicate phrases (bunsetsu).  
A Japanese sentence is typically divided into 
non-overlapping phrases called bunsetsu. As shown 
in Figure 1(b), each bunsetsu consists of one con-
tent word, referred to here as the headword H, and 
several function words F. Words (more precisely, 
morphemes) within a bunsetsu are tightly bound 
with each other, which can be adequately captured 
by a word trigram model. However, headwords 
across bunsetsu boundaries also have dependency 
relations with each other, as the diagrams in Figure 
1 show. Such long distance dependency relations 
are expected to provide useful and complementary 
information with the word trigram model in the task 
of next word prediction.  
In constructing language models for realistic 
applications such as speech recognition and Asian 
language input, we are faced with two constraints 
that we would like to satisfy: First, the model must 
operate in a left-to-right manner, because (1) the 
search procedures for predicting words that corre-
spond to the input acoustic signal or phonetic string 
work left to right, and (2) it can be easily combined 
with a word trigram model in decoding. Second, the 
model should be computationally feasible both in 
training and decoding. In the next section, we offer 
a DLM that satisfies both of these constraints.  
3 Dependency Language Model 
The DLM attempts to generate the dependency 
structure incrementally while traversing the sen-
tence left to right. It will assign a probability to 
every word sequence W and its dependency struc-
ture D. The probability assignment is based on an 
encoding of the (W, D) pair described below. 
Let W be a sentence of length n words to which 
we have prepended <s> and appended </s> so that 
w0 = <s>, and wn+1 = </s>. In principle, a language 
model recovers the probability of a sentence P(W) 
over all possible D given W by estimating the joint 
probability P(W, D): P(W) = ?D P(W, D). In prac-
tice, we used the so-called maximum approximation 
where the sum is approximated by a single term 
P(W, D*): 
? ??=
D
DWPDWPWP ),(),()( . (1) 
Here, D* is the most probable dependency structure 
of the sentence, which is generally discovered by 
maximizing P(W, D): 
D
DWPD ),(maxarg=? . (2) 
Below we restrict the discussion to the most prob-
able dependency structure of a given sentence, and 
simply use D to represent D*. In the remainder of 
this section, we first present a statistical dependency 
parser, which estimates the parsing probability at 
the word level, and generates D incrementally while 
traversing W left to right. Next, we describe the 
elements of the DLM that assign probability to each 
possible W and its most probable D, P(W, D). Fi-
nally, we present an EM-like iterative method for 
unsupervised learning of dependency structure.  
3.1 Dependency parsing 
The aim of dependency parsing is to find the most 
probable D of a given W by maximizing the prob-
ability P(D|W). Let D be a set of probabilistic de-
pendencies d, i.e. d ? D. Assuming that the de-
pendencies are independent of each other, we have 
?
?
=
Dd
WdPWDP )|()|(
 
(3) 
where P(d|W) is the dependency probability condi-
tioned by a particular sentence.1 It is impossible to 
estimate P(d|W) directly because the same sentence 
is very unlikely to appear in both training and test 
data. We thus approximated P(d|W) by P(d), and 
estimated the dependency probability from the 
training corpus. Let dij = (wi, wj) be the dependency 
                                                     
1
 The model in Equation (3) is not strictly probabilistic 
because it drops the probabilities of illegal dependencies 
(e.g., crossing dependencies).  
between wi and wj. The maximum likelihood esti-
mation (MLE) of P(dij) is given by  
),(
),,()(
ji
ji
ij
wwC
RwwC
dP =  
(4) 
where C(wi, wj, R) is the number of times wi and wj 
have a dependency relation in a sentence in training 
data, and C(wi, wj) is the number of times wi and wj 
are seen in the same sentence. To deal with the data 
sparseness problem of MLE, we used the backoff 
estimation strategy similar to the one proposed in 
Collins (1996), which backs off to estimates that 
use less conditioning context. More specifically, we 
used the following three estimates: 
4
4
4
32
32
23
1
1
1 ?
?
??
??
?
?
=
+
+
== EEE , 
(5) 
Where 
 
),,(1 RwwC ji=? , ),(1 ji wwC=? ,  
),*,(2 RwC i=? , ,*)(2 iwC=? ,  
),(*,3 RwC j=? , )(*,3 jwC=? ,  
)(*,*,4 RC=? , (*,*)4 C=? .  
in which * indicates a wild-card matching any 
word. The final estimate E is given by linearly 
interpolating these estimates: 
))1()(1( 42232111 EEEE ???? ?+?+=  (6) 
where ?1 and ?2 are smoothing parameters.  
Given the above parsing model, we used an ap-
proximation parsing algorithm that is O(n2). Tradi-
tional techniques use an optimal Viterbi-style algo-
rithm (e.g., bottom-up chart parser) that is O(n5).2 
Although the approximation algorithm is not 
guaranteed to find the most probable D, we opted 
for it because it works in a left-to-right manner, and 
is very efficient and simple to implement. In our 
experiments, we found that the algorithm performs 
reasonably well on average, and its speed and sim-
plicity make it a better choice in DLM training 
where we need to parse a large amount of training 
data iteratively, as described in Section 3.3.  
The parsing algorithm is a slightly modified 
version of that proposed in Yuret (1998). It reads a 
sentence left to right; after reading each new word 
                                                     
2
 For parsers that use bigram lexical dependencies, Eis-
ner and Satta (1999) presents parsing algorithms that are 
O(n4) or O(n3). We thank Joshua Goodman for pointing 
this out.  
wj, it tries to link wj to each of its previous words wi, 
and push the generated dependency dij into a stack. 
When a dependency crossing or a cycle is detected 
in the stack, the dependency with the lowest de-
pendency probability in conflict is eliminated. The 
algorithm is outlined in Figures 2 and 3. 
DEPENDENCY-PARSING(W) 
1 for j ? 1 to LENGTH(W) 
2 for i ? j-1 downto 1 
3 PUSH dij = (wi, wj) into the stack Dj  
4 if a dependency cycle (CY) is detected in Dj 
(see Figure 3(a)) 
5 REMOVE d, where )(minarg dPd
CYd?
=  
6 while a dependency crossing (CR) is detected 
in Dj (see Figure 3(b)) do 
7 REMOVE d, where )(minarg dPd
CRd?
=  
8 OUTPUT(D) 
Figure 2. Approximation algorithm of dependency 
parsing 
 
 
 
 
 
 
(a) (b) 
Figure 3. (a) An example of a dependency cycle: given 
that P(d23) is smaller than P(d12) and P(d13), d23 is 
removed (represented as dotted line). (b) An example of 
a dependency crossing: given that P(d13) is smaller than 
P(d24), d13 is removed. 
Let the dependency probability be the measure of 
the strength of a dependency, i.e., higher probabili-
ties mean stronger dependencies. Note that when a 
strong new dependency crosses multiple weak 
dependencies, the weak dependencies are removed 
even if the new dependency is weaker than the sum 
of the old dependencies. 3  Although this action 
results in lower total probability, it was imple-
mented because multiple weak dependencies con-
nected to the beginning of the sentence often pre-
                                                     
3
 This operation leaves some headwords disconnected; in 
such a case, we assumed that each disconnected head-
word has a dependency relation with its preceding 
headword.  
w1 w2 w3 w1 
w2 w3 w4 
vented a strong meaningful dependency from being 
created. In this manner, the directional bias of the 
approximation algorithm was partially compen-
sated for.4 
3.2 Language modeling 
The DLM together with the dependency parser 
provides an encoding of the (W, D) pair into a se-
quence of elementary model actions. Each action 
conceptually consists of two stages. The first stage 
assigns a probability to the next word given the left 
context. The second stage updates the dependency 
structure given the new word using the parsing 
algorithm in Figure 2. The probability P(W, D) is 
calculated as:  
=),( DWP  (7) 
?
=
?????
??
n
j
jjj
j
jjjj wDWDPDWwP
1
11111 )),,(|()),(|(  
=?
???
)),,(|( 111 jjjjj wDWDP  (8) 
?
=
???
j
i
j
i
j
jj
j
i ppDWpP
1
1111 ),...,,,|( .  
Here (Wj-1, Dj-1) is the word-parse (j-1)-prefix that 
Dj-1 is a dependency structure containing only those 
dependencies whose two related words are included 
in the word (j-1)-prefix, Wj-1. wj is the word to be 
predicted. Dj-1j is the incremental dependency 
structure that generates Dj = Dj-1 || Dj-1j (|| stands for 
concatenation) when attached to Dj-1; it is the de-
pendency structure built on top of Dj-1 and the 
newly predicted word wj (see the for-loop of line 2 
in Figure 2). pij denotes the ith action of the parser at 
position j in the word string: to generate a new 
dependency dij, and eliminate dependencies with 
the lowest dependency probability in conflict (see 
lines 4 ? 7 in Figure 2). ? is a function that maps the 
history (Wj-1, Dj-1) onto equivalence classes. 
The model in Equation (8) is unfortunately in-
feasible because it is extremely difficult to estimate 
the probability of pij due to the large number of 
parameters in the conditional part. According to the 
parsing algorithm in Figure 2, the probability of 
                                                     
4
 Theoretically, we should arrive at the same dependency 
structure no matter whether we parse the sentence left to 
right or right to left. However, this is not the case with the 
approximation algorithm. This problem is called direc-
tional bias. 
each action pij  depends on the entire history (e.g. 
for detecting a dependency crossing or cycle), so 
any mapping ? that limits the equivalence classi-
fication to less context suitable for model estima-
tion would be very likely to drop critical conditional 
information for predicting pij. In practice, we ap-
proximated P(Dj-1j| ?(Wj-1, Dj-1), wj) by P(Dj|Wj) of 
Equation (3), yielding P(Wj, Dj) ? P(Wj| ?(Wj-1, 
Dj-1)) P(Dj|Wj). This approximation is probabilisti-
cally deficient, but our goal is to apply the DLM to a 
decoder in a realistic application, and the perform-
ance gain achieved by this approximation justifies 
the modeling decision.  
Now, we describe the way P(wj|?(Wj-1,Dj-1)) is 
estimated. As described in Section 2, headwords 
and function words play different syntactic and 
semantic roles capturing different types of de-
pendency relations, so the prediction of them can 
better be done separately. Assuming that each word 
token can be uniquely classified as a headword or a 
function word in Japanese, the DLM can be con-
ceived of as a cluster-based language model with 
two clusters, headword H and function word F. We 
can then define the conditional probability of wj 
based on its history as the product of two factors: 
the probability of the category given its history, and 
the probability of wj given its category. Let hj or fj be 
the actual headword or function word in a sentence, 
and let Hj or Fj be the category of the word wj. 
P(wj|?(Wj-1,Dj-1)) can then be formulated as: 
=?
??
)),(|( 11 jjj DWwP   (9) 
)),,(|()),(|( 1111 jjjjjjj HDWwPDWHP ???? ???  
)),,(|()),(|( 1111 jjjjjjj FDWwPDWFP ???? ???+ . 
We first describe the estimation of headword 
probability P(wj | ?(Wj-1, Dj-1), Hj). Let HWj-1 be the 
headwords in (j-1)-prefix, i.e., containing only 
those headwords that are included in Wj-1. Because 
HWj-1 is determined by Wj-1, the headword prob-
ability can be rewritten as P(wj | ?(Wj-1, HWj-1, Dj-1), 
Hj). The problem is to determine the mapping ? so 
as to identify the related words in the left context 
that we would like to condition on. Based on the 
discussion in Section 2, we chose a mapping func-
tion that retains (1) two preceding words wj-1 and 
wj-2 in Wj-1, (2) one preceding headword hj-1 in 
HWj-1, and (3) one linguistically related word wi 
according to Dj-1. wi is determined in two stages: 
First, the parser updates the dependency structure 
Dj-1 incrementally to Dj assuming that the next word 
is wj. Second, when there are multiple words that 
have dependency relations with wj in Dj, wi is se-
lected using the following decision rule: 
),|(maxarg
),(:
RwwPw ij
Dwww
i
jjii ?
= , (10) 
where the probability P(wj | wi, R) of the word wj 
given its linguistic related word wi is computed 
using MLE by Equation (11): 
?=
jw
ji
ji
ij RwwC
RwwC
RwwP ),,(
),,(),|( . (11) 
We thus have the mapping function ?(Wj-1, HWj-1, 
Dj-1) = (wj-2, wj-1, hj-1, wi). The estimate of headword 
probability is an interpolation of three probabilities:  
=?
??
)),,(|( 11 jjjj HDWwP   (12) 
),|(( 121 jjj HhwP ???   
)),|()1( 2 RwwP ij??+   
),,|()1( 121 jjjj HwwwP ???+ ? .  
Here P(wj|wj-2, wj-1, Hj) is the word trigram prob-
ability given that wj is a headword, P(wj|hj-1, Hj) is 
the headword bigram probability, and ?1, ?2 ? [0,1] 
are  the interpolation weights optimized on held-out 
data. 
We now come back to the estimate of the other 
three probabilities in Equation (9). Following the 
work in Gao et al (2002b), we used the unigram 
estimate for word category probabilities, (i.e., 
P(Hj|?(Wj-1, Dj-1)) ? P(Hj) and P(Fj | ?(Wj-1, Dj-1)) ? 
P(Fj)), and the standard trigram estimate for func-
tion word probability (i.e., P(wj |?(Wj-1,Dj-1),Fj) ? 
P(wj | wj-2, wj-1, Fj)). Let Cj be the category of wj; we 
approximated P(Cj)? P(wj|wj-2, wj-1, Cj) by P(wj | wj-2, 
wj-1). By separating the estimates for the probabili-
ties of headwords and function words, the final 
estimate is given below: 
P(wj | ?(Wj-1, Dj-1))= (13) 
)|()((( 121 ?jjj hwPHP ??
)),|()1( 2 RwwP ij??+
),|()1( 121 ???+ jjj wwwP?  
 
wj: headword  
),|( 12 ?? jjj wwwP   ?
??
?
??
?
?
?
 
wj: function word  
All conditional probabilities in Equation (13) are 
obtained using MLE on training data. In order to 
deal with the data sparseness problem, we used a 
backoff scheme (Katz, 1987) for parameter estima-
tion. This backoff scheme recursively estimates the 
probability of an unseen n-gram by utilizing 
(n?1)-gram estimates. In particular, the probability 
of Equation (11) backs off to the estimate of 
P(wj|R), which is computed as: 
N
RwC
RwP jj
),()|( = , (14) 
where N is the total number of dependencies in 
training data, and C(wj, R) is the number of de-
pendencies that contains wj. To keep the model size 
manageable, we removed all n-grams of count less 
than 2 from the headword bigram model and the 
word trigram model, but kept all long-distance 
dependency bigrams that occurred in the training 
data. 
3.3 Training data creation 
This section describes two methods that were used 
to tag raw text corpus for DLM training: (1) a 
method for headword detection, and (2) an unsu-
pervised learning method for dependency structure 
acquisition. 
In order to classify a word uniquely as H or F, 
we used a mapping table created in the following 
way. We first assumed that the mapping from 
part-of-speech (POS) to word category is unique 
and fixed;5 we then used a POS-tagger to generate a 
POS-tagged corpus, which are then turned into a 
category-tagged corpus.6 Based on this corpus, we 
created a mapping table which maps each word to a 
unique category: when a word can be mapped to 
either H or F, we chose the more frequent category 
in the corpus. This method achieved a 98.5% ac-
curacy of headword detection on the test data we 
used. 
Given a headword-tagged corpus, we then used 
an EM-like iterative method for joint optimization 
of the parsing model and the dependency structure 
of training data. This method uses the maximum 
likelihood principle, which is consistent with lan-
                                                     
5
 The tag set we used included 1,187 POS tags, of which 
102 counted as headwords in our experiments. 
6
 Since the POS-tagger does not identify phrases (bun-
setsu), our implementation identifies multiple headwords 
in phrases headed by compounds.  
guage model training. There are three steps in the 
algorithm: (1) initialize, (2) (re-)parse the training 
corpus, and (3) re-estimate the parameters of the 
parsing model. Steps (2) and (3) are iterated until 
the improvement in the probability of training data 
is less than a threshold. 
Initialize: We set a window of size N and assumed 
that each headword pair within a headword N-gram 
constitutes an initial dependency. The optimal value 
of N is 3 in our experiments. That is, given a 
headword trigram (h1, h2, h3), there are 3 initial 
dependencies: d12, d13, and d23. From the initial 
dependencies, we computed an initial dependency 
parsing model by Equation (4).  
(Re-)parse the corpus: Given the parsing model, 
we used the parsing algorithm in Figure 2 to select 
the most probable dependency structure for each 
sentence in the training data. This provides an up-
dated set of dependencies. 
Re-estimate the parameters of parsing model: 
We then re-estimated the parsing model parameters 
based on the updated dependency set. 
4 Evaluation Methodology 
In this study, we evaluated language models on the 
application of Japanese Kana-Kanji conversion, 
which is the standard method of inputting Japanese 
text by converting the text of a syllabary-based 
Kana string into the appropriate combination of 
Kanji and Kana. This is a similar problem to speech 
recognition, except that it does not include acoustic 
ambiguity. Performance on this task is measured in 
terms of the character error rate (CER), given by the 
number of characters wrongly converted from the 
phonetic string divided by the number of characters 
in the correct transcript.  
For our experiments, we used two newspaper 
corpora, Nikkei and Yomiuri Newspapers, both of 
which have been pre-word-segmented. We built 
language models from a 36-million-word subset of 
the Nikkei Newspaper corpus, performed parameter 
optimization on a 100,000-word subset of the Yo-
miuri Newspaper (held-out data), and tested our 
models on another 100,000-word subset of the 
Yomiuri Newspaper corpus. The lexicon we used 
contains 167,107 entries.  
Our evaluation was done within a framework of 
so-called ?N-best rescoring? method, in which a list 
of hypotheses is generated by the baseline language 
model (a word trigram model in this study), which 
is then rescored using a more sophisticated lan-
guage model. We use the N-best list of N=100,
whose ?oracle? CER (i.e., the CER of the hy-
potheses with the minimum number of errors) is 
presented in Table 1, indicating the upper bound on 
performance. We also note in Table 1 that the per-
formance of the conversion using the baseline tri-
gram model is much better than the state-of-the-art 
performance currently available in the marketplace, 
presumably due to the large amount of training data 
we used, and to the similarity between the training 
and the test data.  
Baseline Trigram Oracle of 100-best 
3.73% 1.51% 
Table 1. CER results of baseline and 100-best list 
5 Results 
The results of applying our models to the task of 
Japanese Kana-Kanji conversion are shown in 
Table 2. The baseline result was obtained by using a 
conventional word trigram model (WTM).7 HBM 
stands for headword bigram model, which does not 
use any dependency structure (i.e. ?2 = 1 in Equation 
(13)). DLM_1 is the DLM that does not use head-
word bigram (i.e. ?
 2 = 0 in Equation (13)). DLM_2 
is the model where the headword probability is 
estimated by interpolating the word trigram prob-
ability, the headword bigram probability, and the 
probability given one previous linguistically related 
word in the dependency structure. 
Although Equation (7) suggests that the word 
probability P(wj|?(Wj-1,Dj-1)) and the parsing model 
probability can be combined through simple multi-
plication, some weighting is desirable in practice, 
especially when our parsing model is estimated 
using an approximation by the parsing score 
P(D|W). We therefore introduced a parsing model 
weight PW: both DLM_1 and DLM_2 models were 
built with and without PW. In Table 2, the PW- 
prefix refers to the DLMs with PW = 0.5, and the 
DLMs without PW- prefix refers to DLMs with PW 
= 0. For both DLM_1 and DLM_2, models with the 
parsing weight achieve better performance; we 
                                                     
7
 For a detailed description of the baseline trigram model, 
see Gao et al (2002a).  
therefore discuss only DLMs with the parsing 
weight for the rest of this section. 
Model ?1 ?2 CER CER reduction 
WTM ---- ---- 3.73% ---- 
HBM 0.2 1 3.40% 8.8% 
DLM_1  0.1 0 3.48% 6.7% 
PW-DLM_1 0.1 0 3.44% 7.8% 
DLM_2 0.3 0.7 3.33% 10.7% 
PW-DLM_2 0.3 0.7 3.31% 11.3% 
Table 2. Comparison of CER results 
By comparing both HBM and PW-LDM_1 models 
with the baseline model, we can see that the use of 
headword dependency contributes greatly to the 
CER reduction: HBM outperformed the baseline 
model by 8.8% in CER reduction, and PW-LDM_1 
by 7.8%. By combining headword bigram and 
dependency structure, we obtained the best model 
PW-DLM_2 that achieves 11.3% CER reduction 
over the baseline. The improvement achieved by 
PW-DLM_2 over the HBM is statistically signifi-
cant according to the t test (P<0.01). These results 
demonstrate the effectiveness of our parsing tech-
nique and the use of dependency structure for lan-
guage modeling. 
6 Discussion 
In this section, we relate our model to previous 
research and discuss several factors that we believe 
to have the most significant impact on the per-
formance of DLM. The discussion includes: (1) the 
use of DLM as a parser, (2) the definition of the 
mapping function ?, and (3) the method of unsu-
pervised dependency structure acquisition. 
One basic approach to using linguistic structure 
for language modeling is to extend the conventional 
language model P(W) to P(W, T), where T is a parse 
tree of W. The extended model can then be used as a 
parser to select the most likely parse by T* = arg-
maxT P(W, T). Many recent studies (e.g., Chelba 
and Jelinek, 2000; Charniak, 2001; Roark, 2001) 
adopt this approach. Similarly, dependency-based 
models (e.g., Collins, 1996; Chelba et al, 1997) use 
a dependency structure D of W instead of a parse 
tree T, where D is extracted from syntactic trees. 
Both of these models can be called grammar-based 
models, in that they capture the syntactic structure 
of a sentence, and the model parameters are esti-
mated from syntactically annotated corpora such as 
the Penn Treebank. DLM, on the other hand, is a 
non-grammar-based model, because it is not based 
on any syntactic annotation: the dependency struc-
ture used in language modeling was learned directly 
from data in an unsupervised manner, subject to two 
weak syntactic constraints (i.e., dependency struc-
ture is acyclic and planar).8 This resulted in cap-
turing the dependency relations that are not pre-
cisely syntactic in nature within our model. For 
example, in the conversion of the string below, the 
word  ban 'evening' was correctly predicted in 
DLM by using the long-distance bigram ~ 
asa~ban 'morning~evening', even though these two 
words are not in any direct syntactic dependency 
relationship:  
	

 
'asks for instructions in the morning and submits 
daily reports in the evening'  
Though there is no doubt that syntactic dependency 
relations provide useful information for language 
modeling, the most linguistically related word in the 
previous context may come in various linguistic 
relations with the word being predicted, not limited 
to syntactic dependency. This opens up new possi-
bilities for exploring the combination of different 
knowledge sources in language modeling.  
Regarding the function ? that maps the left 
context onto equivalence classes, we used a simple 
approximation that takes into account only one 
linguistically related word in left context. An al-
ternative is to use the maximum entropy (ME) 
approach (Rosenfeld, 1994; Chelba et al, 1997). 
Although ME models provide a nice framework for 
incorporating arbitrary knowledge sources that can 
be encoded as a large set of constraints, training and 
using ME models is extremely computationally 
expensive. Our working hypothesis is that the in-
formation for predicting the new word is dominated 
by a very limited set of words which can be selected 
heuristically: in this paper, ? is defined as a heu-
ristic function that maps D to one word in D that has 
the strongest linguistic relation with the word being 
predicted, as in (8). This hypothesis is borne out by 
                                                     
8
 In this sense, our model is an extension of a depend-
ency-based model proposed in Yuret (1998). However, 
this work has not been evaluated as a language model 
with error rate reduction.  
an additional experiment we conducted, where we 
used two words from D that had the strongest rela-
tion with the word being predicted; this resulted in a 
very limited gain in CER reduction of 0.62%, which 
is not statistically significant (P>0.05 according to 
the t test).  
The EM-like method for learning dependency 
relations described in Section 3.3 has also been 
applied to other tasks such as hidden Markov model 
training (Rabiner, 1989), syntactic relation learning 
(Yuret, 1998), and Chinese word segmentation 
(Gao et al, 2002a). In applying this method, two 
factors need to be considered: (1) how to initialize 
the model (i.e. the value of the window size N), and 
(2) the number of iterations. We investigated the 
impact of these two factors empirically on the CER 
of Japanese Kana-Kanji conversion. We built a 
series of DLMs using different window size N and 
different number of iterations. Some sample results 
are shown in Table 3: the improvement in CER 
begins to saturate at the second iteration. We also 
find that a larger N results in a better initial model 
but makes the following iterations less effective. 
The possible reason is that a larger N generates 
more initial dependencies and would lead to a better 
initial model, but it also introduces noise that pre-
vents the initial model from being improved. All 
DLMs in Table 2 are initialized with N = 3 and are 
run for two iterations.  
Iteration N = 2 N = 3 N = 5 N = 7 N = 10 
Init. 3.552% 3.523% 3.540% 3.514 % 3.511% 
1 3.531% 3.503% 3.493% 3.509% 3.489% 
2 3.527% 3.481% 3.483% 3.492% 3.488% 
3 3.526% 3.481% 3.485% 3.490% 3.488% 
Table 3. CER of DLM_1 models initialized with dif-
ferent window size N, for 0-3 iterations 
7 Conclusion 
We have presented a dependency language model 
that captures linguistic constraints via a dependency 
structure ? a set of probabilistic dependencies that 
express the relations between headwords of each 
phrase in a sentence by an acyclic, planar, undi-
rected graph. Promising results of our experiments 
suggest that long-distance dependency relations can 
indeed be successfully exploited for the purpose of 
language modeling.   
There are many possibilities for future im-
provements. In particular, as discussed in Section 6, 
syntactic dependency structure is believed to cap-
ture useful information for informed language 
modeling, yet further improvements may be possi-
ble by incorporating non-syntax-based dependen-
cies. Correlating the accuracy of the dependency 
parser as a parser vs. its utility in CER reduction 
may suggest a useful direction for further research.  
Reference 
Charniak, Eugine. 2001. Immediate-head parsing for 
language models. In ACL/EACL 2001, pp.124-131.  
Chelba, Ciprian and Frederick Jelinek. 2000. Structured 
Language Modeling. Computer Speech and Language, 
Vol. 14, No. 4. pp 283-332.  
Chelba, C, D. Engle, F. Jelinek, V. Jimenez, S. Khu-
danpur, L. Mangu, H. Printz, E. S. Ristad, R. 
Rosenfeld, A. Stolcke and D. Wu. 1997. Structure and 
performance of a dependency language model. In 
Processing of Eurospeech, Vol. 5, pp 2775-2778.  
Collins, Michael John. 1996. A new statistical parser 
based on bigram lexical dependencies. In ACL 
34:184-191. 
Eisner, Jason and Giorgio Satta. 1999. Efficient parsing 
for bilexical context-free grammars and head 
automaton grammars. In ACL 37: 457-464. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and 
Kai-Fu Lee. 2002a. Toward a unified approach to sta-
tistical language modeling for Chinese. ACM Trans-
actions on Asian Language Information Processing, 
1-1: 3-33.  
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002b. 
Exploiting headword dependency and predictive 
clustering for language modeling. In EMNLP 2002: 
248-256. 
Katz, S. M. 1987. Estimation of probabilities from sparse 
data for other language component of a speech recog-
nizer. IEEE transactions on Acoustics, Speech and 
Signal Processing, 35(3): 400-401. 
Rabiner, Lawrence R. 1989. A tutorial on hidden Markov 
models and selected applications in speech recognition. 
Proceedings of IEEE 77:257-286. 
Roark, Brian. 2001. Probabilistic top-down parsing and 
language modeling. Computational Linguistics, 17-2: 
1-28. 
Rosenfeld, Ronald. 1994. Adaptive statistical language 
modeling: a maximum entropy approach. Ph.D. thesis, 
Carnegie Mellon University.  
Yuret, Deniz. 1998. Discovery of linguistic relations 
using lexical attraction. Ph.D. thesis, MIT. 
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 225?232,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Approximation Lasso Methods for Language Modeling 
Jianfeng Gao 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
jfgao@microsoft.com 
Hisami Suzuki 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
hisamis@microsoft.com 
Bin Yu 
Department of Statistics 
University of California 
Berkeley., CA 94720 U.S.A. 
binyu@stat.berkeley.edu 
 
Abstract 
Lasso is a regularization method for pa-
rameter estimation in linear models. It op-
timizes the model parameters with respect 
to a loss function subject to model com-
plexities. This paper explores the use of 
lasso for statistical language modeling for 
text input. Owing to the very large number 
of parameters, directly optimizing the pe-
nalized lasso loss function is impossible. 
Therefore, we investigate two approxima-
tion methods, the boosted lasso (BLasso) 
and the forward stagewise linear regres-
sion (FSLR). Both methods, when used 
with the exponential loss function, bear 
strong resemblance to the boosting algo-
rithm which has been used as a discrimi-
native training method for language mod-
eling. Evaluations on the task of Japanese 
text input show that BLasso is able to 
produce the best approximation to the 
lasso solution, and leads to a significant 
improvement, in terms of character error 
rate, over boosting and the traditional 
maximum likelihood estimation. 
1 Introduction 
Language modeling (LM) is fundamental to a 
wide range of applications. Recently, it has been 
shown that a linear model estimated using dis-
criminative training methods, such as the boost-
ing and perceptron algorithms, outperforms 
significantly a traditional word trigram model 
trained using maximum likelihood estimation 
(MLE) on several tasks such as speech recogni-
tion and Asian language text input (Bacchiani et 
al. 2004; Roark et al 2004; Gao et al 2005; Suzuki 
and Gao 2005). 
The success of discriminative training meth-
ods is largely due to fact that unlike the tradi-
tional approach (e.g., MLE) that maximizes the 
function (e.g., likelihood of training data) that is 
loosely associated with error rate, discriminative 
training methods aim to directly minimize the 
error rate on training data even if they reduce the 
likelihood. However, given a finite set of training 
samples, discriminative training methods could 
lead to an arbitrary complex model for the pur-
pose of achieving zero training error. It is 
well-known that complex models exhibit high 
variance and perform poorly on unseen data. 
Therefore some regularization methods have to 
be used to control the complexity of the model. 
Lasso is a regularization method for parame-
ter estimation in linear models. It optimizes the 
model parameters with respect to a loss function 
subject to model complexities. The basic idea of 
lasso is originally proposed by Tibshirani (1996). 
Recently, there have been several implementa-
tions and experiments of lasso on multi-class 
classification tasks where only a small number of 
features need to be handled and the lasso solu-
tion can be directly computed via numerical 
methods. To our knowledge, this paper presents 
the first empirical study of lasso for a realistic, 
large scale task: LM for Asian language text in-
put. Because the task utilizes millions of features 
and training samples, directly optimizing the 
penalized lasso loss function is impossible. 
Therefore, two approximation methods, the 
boosted lasso (BLasso, Zhao and Yu 2004) and 
the forward stagewise linear regression (FSLR, 
Hastie et al 2001), are investigated. Both meth-
ods, when used with the exponential loss func-
tion, bear strong resemblance to the boosting 
algorithm which has been used as a discrimina-
tive training method for LM. Evaluations on the 
task of Japanese text input show that BLasso is 
able to produce the best approximation to the 
lasso solution, and leads to a significant im-
provement, in terms of character error rate, over 
the boosting algorithm and the traditional MLE. 
2 LM Task and Problem Definition 
This paper studies LM on the application of 
Asian language (e.g. Chinese or Japanese) text 
input, a standard method of inputting Chinese or 
Japanese text by converting the input phonetic 
symbols into the appropriate word string. In this 
paper we call the task IME, which stands for 
225
input method editor, based on the name of the 
commonly used Windows-based application. 
Performance on IME is measured in terms of 
the character error rate (CER), which is the 
number of characters wrongly converted from 
the phonetic string divided by the number of 
characters in the correct transcript.  
Similar to speech recognition, IME is viewed 
as a Bayes decision problem. Let A be the input 
phonetic string. An IME system?s task is to 
choose the most likely word string W* among 
those candidates that could be converted from A: 
)|()(maxarg)|(maxarg
(A))(
* WAPWPAWPW
WAW GENGEN ??
==  (1) 
where GEN(A) denotes the candidate set given A. 
Unlike speech recognition, however, there is no 
acoustic ambiguity as the phonetic string is in-
putted by users. Moreover, we can assume a 
unique mapping from W and A in IME as words 
have unique readings, i.e. P(A|W) = 1. So the 
decision of Equation (1) depends solely upon 
P(W), making IME an ideal evaluation test bed 
for LM.  
In this study, the LM task for IME is formu-
lated under the framework of linear models (e.g., 
Duda et al 2001). We use the following notation, 
adapted from Collins and Koo (2005):  
? Training data is a set of example in-
put/output pairs. In LM for IME, training sam-
ples are represented as {Ai, WiR}, for i = 1?M, 
where each Ai is an input phonetic string and WiR 
is the reference transcript of Ai. 
? We assume some way of generating a set of 
candidate word strings given A, denoted by 
GEN(A).  In our experiments, GEN(A) consists of 
top n word strings converted from A using a 
baseline IME system that uses only a word tri-
gram model. 
? We assume a set of D+1 features fd(W), for d 
= 0?D. The features could be arbitrary functions 
that map W to real values. Using vector notation, 
we have f(W)??D+1, where f(W) = [f0(W), f1(W), 
?, fD(W)]T. f0(W) is called the base feature, and is 
defined in our case as the log probability that the 
word trigram model assigns to W. Other features 
(fd(W), for d = 1?D) are defined as the counts of 
word n-grams (n = 1 and 2 in our experiments) in 
W. 
? Finally, the parameters of the model form a 
vector of D+1 dimensions, each for one feature 
function, ? = [?0, ?1, ?, ?D]. The score of a word 
string W can be written as 
)(),( WWScore ?f? = ?
=
=
D
d
dd Wf?
0
)( . (2)
The decision rule of Equation (1) is rewritten as 
),(maxarg),(
(A)
* ??
GEN
WScoreAW
W?
= . (3)
Equation (3) views IME as a ranking problem, 
where the model gives the ranking score, not 
probabilities. We therefore do not evaluate the 
model via perplexity. 
Now, assume that we can measure the num-
ber of conversion errors in W by comparing it 
with a reference transcript WR using an error 
function Er(WR,W), which is the string edit dis-
tance function in our case. We call the sum of 
error counts over the training samples sample risk. 
Our goal then is to search for the best parameter 
set ? which minimizes the sample risk, as in 
Equation (4):  
?
=
=
Mi
ii
R
i
def
MSR AWW
...1
* )),(,Er(minarg ??
?
. (4)
However, (4) cannot be optimized easily since 
Er(.) is a piecewise constant (or step) function of ? 
and its gradient is undefined. Therefore, dis-
criminative methods apply different approaches 
that optimize it approximately. The boosting 
algorithm described below is one of such ap-
proaches.  
3 Boosting 
This section gives a brief review of the boosting 
algorithm, following the description of some 
recent work (e.g., Schapire and Singer 1999; 
Collins and Koo 2005).  
The boosting algorithm uses an exponential 
loss function (ExpLoss) to approximate the sam-
ple risk in Equation (4). We define the margin of 
the pair (WR, W) with respect to the model ? as 
),(),(),( ?? WScoreWScoreWWM RR ?=  (5)
Then, ExpLoss is defined as 
? ?
= ?
?=
Mi AW
i
R
i
ii
WWM
...1 )(
)),(exp()ExpLoss(
GEN
?  (6)
Notice that ExpLoss is convex so there is no 
problem with local minima when optimizing it. It 
is shown in Freund et al (1998) and Collins and 
Koo (2005) that there exist gradient search pro-
cedures that converge to the right solution.  
Figure 1 summarizes the boosting algorithm 
we used. After initialization, Steps 2 and 3 are 
1 Set ?0 = argmin?0ExpLoss(?); and ?d = 0 for d=1?D 
2 Select a feature fk* which has largest estimated 
impact on reducing ExpLoss of Eq. (6) 
3 Update ?k* ?  ?k* + ?*, and return to Step 2 
Figure 1: The boosting algorithm 
226
repeated N times; at each iteration, a feature is 
chosen and its weight is updated as follows.  
First, we define Upd(?, k, ?) as an updated 
model, with the same parameter values as ? with 
the exception of ?k, which is incremented by ? 
},...,,...,,{),,Upd( 10 Dkk ?????? +=?  
Then, Steps 2 and 3 in Figure 1 can be rewritten 
as Equations (7) and (8), respectively. 
)),,d(ExpLoss(Upminarg*)*,(
,
??
?
kk
k
?=  (7)
*)*,,Upd( 1 ?ktt ?= ??  (8)
The boosting algorithm can be too greedy: 
Each iteration usually reduces the ExpLoss(.) on 
training data, so for the number of iterations 
large enough this loss can be made arbitrarily 
small. However, fitting training data too well 
eventually leads to overfiting, which degrades 
the performance on unseen test data (even 
though in boosting overfitting can happen very 
slowly).  
Shrinkage is a simple approach to dealing 
with the overfitting problem. It scales the incre-
mental step ? by a small constant ?, ? ? (0, 1). 
Thus, the update of Equation (8) with shrinkage 
is 
*)*,,Upd( 1 ??ktt ?= ??  (9)
Empirically, it has been found that smaller values 
of ? lead to smaller numbers of test errors. 
4 Lasso 
Lasso is a regularization method for estimation in 
linear models (Tibshirani 1996). It regularizes or 
shrinks a fitted model through an L1 penalty or 
constraint.  
Let T(?) denote the L1 penalty of the model, 
i.e., T(?) = ?d = 0?D|?d|. We then optimize the 
model ? so as to minimize a regularized loss 
function on training data, called lasso loss defined 
as 
)()ExpLoss(),LassoLoss( ??? T?? +=  (10)
where T(?) generally penalizes larger models (or 
complex models), and the parameter ? controls 
the amount of regularization applied to the esti-
mate. Setting ? = 0 reverses the LassoLoss to the 
unregularized ExpLoss; as ? increases, the model 
coefficients all shrink, each ultimately becoming 
zero. In practice, ? should be adaptively chosen 
to minimize an estimate of expected loss, e.g., ? 
decreases with the increase of the number of 
iterations.  
Computation of the solution to the lasso prob-
lem has been studied for special loss functions. 
For least square regression, there is a fast algo-
rithm LARS to find the whole lasso path for dif-
ferent ?? s (Obsborn et al 2000a; 2000b; Efron et 
al. 2004); for 1-norm SVM, it can be transformed 
into a linear programming problem with a fast 
algorithm similar to LARS (Zhu et al 2003). 
However, the solution to the lasso problem for a 
general convex loss function and an adaptive ? 
remains open. More importantly for our pur-
poses, directly minimizing lasso function of 
Equation (10) with respect to ? is not possible 
when a very large number of model parameters 
are employed, as in our task of LM for IME. 
Therefore we investigate below two methods that 
closely approximate the effect of the lasso, and 
are very similar to the boosting algorithm. 
It is also worth noting the difference between 
L1 and L2 penalty. The classical Ridge Regression 
setting uses an L2 penalty in Equation (10) i.e., 
T(?) = ?d = 0?D(?d)2, which is much easier to 
minimize (for least square loss but not for Ex-
pLoss). However, recent research (Donoho et al 
1995) shows that the L1 penalty is better suited for 
sparse situations, where there are only a small 
number of features with nonzero weights among 
all candidate features. We find that our task is 
indeed a sparse situation: among 860,000 features, 
in the resulting linear model only around 5,000 
features have nonzero weights. We then focus on 
the L1 penalty. We leave the empirical compari-
son of the L1 and L2 penalty on the LM task to 
future work. 
4.1 Forward Stagewise Linear 
Regression (FSLR) 
The first approximation method we used is FSLR, 
described in (Algorithm 10.4, Hastie et al 2001), 
where Steps 2 and 3 in Figure 1 are performed 
according to Equations (7) and (11), respectively. 
)),,d(ExpLoss(Upminarg*)*,(
,
??
?
kk
k
?=  (7) 
*))sign(*,,Upd( 1 ?? ?= ? ktt ??  (11)
Notice that FSLR is very similar to the boosting 
algorithm with shrinkage in that at each step, the 
feature fk* that has largest estimated impact on 
reducing ExpLoss is selected. The only difference 
is that FSLR updates the weight of fk* by a small 
fixed step size ?. By taking such small steps, FSLR 
imposes some implicit regularization, and can 
closely approximate the effect of the lasso in a 
local sense (Hastie et al 2001). Empirically, we 
find that the performance of the boosting algo-
rithm with shrinkage closely resembles that of 
FSLR, with the learning rate parameter ? corre-
sponding to ?. 
227
4.2 Boosted Lasso (BLasso) 
The second method we used is a modified ver-
sion of the BLasso algorithm described in Zhao 
and Yu (2004). There are two major differences 
between BLasso and FSLR. At each iteration, 
BLasso can take either a forward step or a backward 
step. Similar to the boosting algorithm and FSLR, 
at each forward step, a feature is selected and its 
weight is updated according to Equations (12) 
and (13). 
)),,d(ExpLoss(Upminarg*)*,(
,
??
??
kk
k
?
?=
=  (12)
*))sign(*,,Upd( 1 ?? ?= ? ktt ??  (13)
However, there is an important difference be-
tween Equations (12) and (7). In the boosting 
algorithm with shrinkage and FSLR, as shown in 
Equation (7), a feature is selected by its impact on 
reducing the loss with its optimal update ?*. In 
contract, in BLasso, as shown in Equation (12), 
the optimization over ? is removed, and for each 
feature, its loss is calculated with an update of 
either +? or -?, i.e., the grid search is used for 
feature selection. We will show later that this 
seemingly trivial difference brings a significant 
improvement. 
The backward step is unique to BLasso. In 
each iteration, a feature is selected and its weight 
is updated backward if and only if it leads to a 
decrease of the lasso loss, as shown in Equations 
(14) and (15): 
))sign(,,d(ExpLoss(Upminarg*
0,
??
?
??=
?
k
k
kk
k
? (14)
))sign(*,,Upd( *
1 ?? ??= ? ktt k??  
??? >??? ),LassoLoss(),LassoLoss( if 11 tttt ??
(15)
where ?  is a tolerance parameter. 
Figure 2 summarizes the BLasso algorithm we 
used. After initialization, Steps 4 and 5 are re-
peated N times; at each iteration, a feature is 
chosen and its weight is updated either backward 
or forward by a fixed amount ?. Notice that the 
value of ? is adaptively chosen according to the 
reduction of ExpLoss during training. The algo-
rithm starts with a large initial ?, and then at each 
forward step the value of ? decreases until the 
ExpLoss stops decreasing. This is intuitively 
desirable: It is expected that most highly effective 
features are selected in early stages of training, so 
the reduction of ExpLoss at each step in early 
stages are more substantial than in later stages. 
These early steps coincide with the boosting steps 
most of the time. In other words, the effect of 
backward steps is more visible at later stages. 
Our implementation of BLasso differs slightly 
from the original algorithm described in Zhao 
and Yu (2004). Firstly, because the value of the 
base feature f0 is the log probability (assigned by 
a word trigram model) and has a different range 
from that of other features as in Equation (2), ?0 is 
set to optimize ExpLoss in the initialization step 
(Step 1 in Figure 2) and remains fixed during 
training. As suggested by Collins and Koo (2005), 
this ensures that the contribution of the 
log-likelihood feature f0 is well-calibrated with 
respect to ExpLoss. Secondly, when updating a 
feature weight, if the size of the optimal update 
step (computed via Equation (7)) is smaller than 
?, we use the optimal step to update the feature. 
Therefore, in our implementation BLasso does 
not always take a fixed step; it may take steps 
whose size is smaller than ?. In our initial ex-
periments we found that both changes (also used 
in our implementations of boosting and FSLR) 
were crucial to the performance of the methods.  
1 Initialize ?0: set ?0 = argmin?0ExpLoss(?), and ?d = 0 
for d=1?D. 
2 Take a forward step according to Eq. (12) and (13), 
and the updated model is denoted by ?1 
3 Initialize ? = (ExpLoss(?0)-ExpLoss(?1))/? 
4 Take a backward step if and only if it leads to a 
decrease of LassoLoss according to Eq. (14) and 
(15), where ?  = 0; otherwise 
5 Take a forward step according to Eq. (12) and (13); 
update ? = min(?, (ExpLoss(?t-1)-ExpLoss(?t))/? ); 
and return to Step 4. 
Figure 2: The BLasso algorithm 
(Zhao and Yu 2004) provides theoretical justi-
fications for BLasso. It has been proved that (1) it 
guarantees that it is safe for BLasso to start with 
an initial ? which is the largest ? that would 
allow an ? step away from 0 (i.e., larger ??s cor-
respond to T(?)=0); (2) for each value of ?, BLasso 
performs coordinate descent (i.e., reduces Ex-
pLoss by updating the weight of a feature) until 
there is no descent step; and (3) for each step 
where the value of ? decreases, it guarantees that 
the lasso loss is reduced.  As a result, it can be 
proved that for a finite number of features and ? 
= 0, the BLasso algorithm shown in Figure 2 
converges to the lasso solution when ? ? 0. 
5 Evaluation 
5.1 Settings 
We evaluated the training methods described 
above in the so-called cross-domain language 
model  adaptation paradigm, where we adapt a 
model trained on one domain (which we call the 
228
background domain) to a different domain (adap-
tation domain), for which only a small amount of 
training data is available. 
The data sets we used in our experiments 
came from five distinct sources of text. A 
36-million-word Nikkei Newspaper corpus was 
used as the background domain, on which the 
word trigram model was trained. We used four 
adaptation domains: Yomiuri (newspaper cor-
pus), TuneUp (balanced corpus containing 
newspapers and other sources of text), Encarta 
(encyclopedia) and Shincho (collection of novels). 
All corpora have been pre-word-segmented us-
ing a lexicon containing 167,107 entries. For each 
of the four domains, we created training data 
consisting of 72K sentences (0.9M~1.7M words) 
and test data of 5K sentences (65K~120K words) 
from each adaptation domain. The first 800 and 
8,000 sentences of each adaptation training data 
were also used to show how different sizes of 
training data affected the performances of vari-
ous adaptation methods. Another 5K-sentence 
subset was used as held-out data for each do-
main.  
We created the training samples for discrimi-
native learning as follows. For each phonetic 
string A in adaptation training data, we pro-
duced a lattice of candidate word strings W using 
the baseline system described in (Gao et al 2002), 
which uses a word trigram model trained via 
MLE on the Nikkei Newspaper corpus. For effi-
ciency, we kept only the best 20 hypotheses in its 
candidate conversion set  GEN(A) for each 
training sample for discriminative training. The 
oracle best hypothesis, which gives the minimum 
number of errors, was used as the reference tran-
script of A.  
We used unigrams and bigrams that occurred 
more than once in the training set as features in 
the linear model of Equation (2). The total num-
ber of candidate features we used was around 
860,000.  
5.2 Main Results 
Table 1 summarizes the results of various model 
training (adaptation) methods in terms of CER 
(%) and CER reduction (in parentheses) over 
comparing models. In the first column, the 
numbers in parentheses next to the domain name 
indicates the number of training sentences used 
for adaptation. 
Baseline, with results shown in Column 3, is 
the word trigram model. As expected, the CER 
correlates very well the similarity between the 
background domain and the adaptation domain, 
where domain similarity is measured in terms of 
cross entropy (Yuan et al 2005) as shown in Col-
umn 2.  
MAP (maximum a posteriori), with results 
shown in Column 4, is a traditional LM adapta-
tion method where the parameters of the back-
ground model are adjusted in such a way that 
maximizes the likelihood of the adaptation data. 
Our implementation takes the form of linear 
interpolation as described in Bacchiani et al 
(2004): P(wi|h) = ?Pb(wi|h) + (1-?)Pa(wi|h), where 
Pb is the probability of the background model, Pa 
is the probability trained on adaptation data 
using MLE and the history h corresponds to two 
preceding words (i.e. Pb and Pa are trigram 
probabilities). ? is the interpolation weight opti-
mized on held-out data.  
Boosting, with results shown in Column 5, is 
the algorithm described in Figure 1. In our im-
plementation, we use the shrinkage method 
suggested by Schapire and Singer (1999) and 
Collins and Koo (2005). At each iteration, we 
used the following update for the kth feature 
ZC
ZC
k
k
k ?
?? +
+=
+
_log2
1  (16)
where Ck+ is a value increasing exponentially 
with the sum of margins of (WR, W) pairs over the 
set where fk is seen in WR but not in W; Ck-  is the 
value related to the sum of margins over the set 
where fk is seen in W but not in WR. ? is a 
smoothing factor (whose value is optimized on 
held-out data) and Z is a normalization constant 
(whose value is the ExpLoss(.) of training data 
according to the current model). We see that ?Z in 
Equation (16) plays the same role as ? in Equation 
(9).  
BLasso, with results shown in Column 6, is 
the algorithm described in Figure 2. We find that 
the performance of BLasso is not very sensitive to 
the selection of the step size ? across training sets 
of different domains and sizes. Although small ? 
is preferred in theory as discussed earlier, it 
would lead to a very slow convergence. There-
fore, in our experiments, we always use a large 
step (? = 0.5) and use the so-called early stopping 
strategy, i.e., the number of iterations before 
stopping is optimized on held-out data. 
In the task of LM for IME, there are millions of 
features and training samples, forming an ex-
tremely large and sparse matrix. We therefore 
applied the techniques described in Collins and 
Koo (2005) to speed up the training procedure. 
The resulting algorithms run in around 15 and 30 
minutes respectively for Boosting and BLasso to 
converge on an XEON? MP 1.90GHz machine 
when training on an 8K-sentnece training set. 
229
The results in Table 1 give rise to several ob-
servations. First of all, both discriminative train-
ing methods (i.e., Boosting and BLasso) outper-
form MAP substantially. The improvement mar-
gins are larger when the background and adap-
tation domains are more similar. The phenome-
non is attributed to the underlying difference 
between the two adaptation methods: MAP aims 
to improve the likelihood of a distribution, so if 
the adaptation domain is very similar to the 
background domain, the difference between the 
two underlying distributions is so small that 
MAP cannot adjust the model effectively. Dis-
criminative methods, on the other hand, do not 
have this limitation for they aim to reduce errors 
directly. Secondly, BLasso outperforms Boosting 
significantly (p-value < 0.01) on all test sets. The 
improvement margins vary with the training sets 
of different domains and sizes. In general, in 
cases where the adaptation domain is less similar 
to the background domain and larger training set 
is used, the improvement of BLasso is more visi-
ble.    
Note that the CER results of FSLR are not in-
cluded in Table 1 because it achieves very similar 
results to the boosting algorithm with shrinkage 
if the controlling parameters of both algorithms 
are optimized via cross-validation. We shall dis-
cuss their difference in the next section. 
5.3 Dicussion 
This section investigates what components of 
BLasso bring the improvement over Boosting. 
Comparing the algorithms in Figures 1 and 2, we 
notice three differences between BLasso and 
Boosting: (i) the use of backward steps in BLasso; 
(ii) BLasso uses the grid search (fixed step size) 
for feature selection in Equation (12) while 
Boosting uses the continuous search (optimal 
step size) in Equation (7); and (iii) BLasso uses a 
fixed step size for feature update in Equation (13) 
while Boosting uses an optimal step size in 
Equation (8). We then investigate these differ-
ences in turn. 
To study the impact of backward steps, we 
compared BLasso with the boosting algorithm 
with a fixed step search and a fixed step update, 
henceforth referred to as F-Boosting. F-Boosting 
was implemented as Figure 2, by setting a large 
value to ? in Equation (15), i.e., ? = 103, to prohibit 
backward steps. We find that although the 
training error curves of BLasso and F-Boosting 
are almost identical, the T(?) curves grow apart 
with iterations, as shown in Figure 3. The results 
show that with backward steps, BLasso achieves 
a better approximation to the true lasso solution: 
It leads to a model with similar training errors 
but less complex (in terms of L1 penalty). In our 
experiments we find that the benefit of using 
backward steps is only visible in later iterations 
when BLasso?s backward steps kick in. A typical 
example is shown in Figure 4. The early steps fit 
to highly effective features and in these steps 
BLasso and F-Boosting agree. For later steps, 
fine-tuning of features is required. BLasso with 
backward steps provides a better mechanism 
than F-Boosting to revise the previously chosen 
features to accommodate this fine level of tuning. 
Consequently we observe the superior perform-
ance of BLasso at later stages as shown in our 
experiments.  
As well-known in linear regression models, 
when there are many strongly correlated fea-
tures, model parameters can be poorly estimated 
and exhibit high variance. By imposing a model 
size constraint, as in lasso, this phenomenon is 
alleviated. Therefore, we speculate that a better 
approximation to lasso, as BLasso with backward 
steps, would be superior in eliminating the nega-
tive effect of strongly correlated features in 
model estimation. To verify our speculation, we 
performed the following experiments. For each 
training set, in addition to word unigram and 
bigram features, we introduced a new type of 
features called headword bigram.  
As described in Gao et al (2002), headwords 
are defined as the content words of the sentence. 
Therefore, headword bigrams constitute a special 
type of skipping bigrams which can capture 
dependency between two words that may not be 
adjacent. In reality, a large portion of headword 
bigrams are identical to word bigrams, as two 
headwords can occur next to each other in text. In 
the adaptation test data we used, we find that 
headword bigram features are for the most part 
either completely overlapping with the word bi-
gram features (i.e., all instances of headword 
bigrams also count as word bigrams) or not over-
lapping at all (i.e., a headword bigram feature is 
not observed as a word bigram feature) ? less 
than 20% of headword bigram features displayed 
a variable degree of overlap with word bigram 
features. In our data, the rate of completely 
overlapping features is 25% to 47% depending on 
the adaptation domain. From this, we can say 
that the headword bigram features show moder-
ate to high degree of correlation with the word 
bigram features.  
We then used BLasso and F-Boosting to train 
the linear language models including both word 
bigram and headword bigram features. We find 
that although the CER reduction by adding 
230
headword features is overall very small, the dif-
ference between the two versions of BLasso is 
more visible in all four test sets. Comparing Fig-
ures 5 ? 8 with Figure 4, it can be seen that BLasso 
with backward steps outperforms the one with-
out backward steps in much earlier stages of 
training with a larger margin. For example, on 
Encarta data sets, BLasso outperforms F-Boosting 
after around 18,000 iterations with headword 
features (Figure 7), as opposed to 25,000 itera-
tions without headword features (Figure 4). The 
results seem to corroborate our speculation that 
BLasso is more robust in the presence of highly 
correlated features. 
To investigate the impact of using the grid 
search (fixed step size) versus the continuous 
search (optimal step size) for feature selection, 
we compared F-Boosting with FSLR since they 
differs only in their search methods for feature 
selection. As shown in Figures 5 to 8, although 
FSLR is robust in that its test errors do not in-
crease after many iterations, F-Boosting can reach 
a much lower error rate on three out of four test 
sets. Therefore, in the task of LM for IME where 
CER is the most important metric, the grid search 
for feature selection is more desirable.  
To investigate the impact of using a fixed ver-
sus an optimal step size for feature update, we 
compared FSLR with Boosting. Although both 
algorithms achieve very similar CER results, the 
performance of FSLR is much less sensitive to the 
selected fixed step size. For example, we can 
select any value from 0.2 to 0.8, and in most set-
tings FSLR achieves the very similar lowest CER 
after 20,000 iterations, and will stay there for 
many iterations. In contrast, in Boosting, the 
optimal value of ? in Equation (16) varies with the 
sizes and domains of training data, and has to be 
tuned carefully. We thus conclude that in our 
task FSLR is more robust against different train-
ing settings and a fixed step size for feature up-
date is more preferred. 
6 Conclusion 
This paper investigates two approximation lasso 
methods for LM applied to a realistic task with a 
very large number of features with sparse feature 
space. Our results on Japanese text input are 
promising. BLasso outperforms the boosting 
algorithm significantly in terms of CER reduction 
on all experimental settings. 
We have shown that this superior perform-
ance is a consequence of BLasso?s backward step 
and its fixed step size in both feature selection 
and feature weight update.  Our experimental 
results in Section 5 show that the use of backward 
step is vital for model fine-tuning after major 
features are selected and for coping with strongly 
correlated features; the fixed step size of BLasso 
is responsible for the improvement of CER and 
the robustness of the results. Experiments on 
other data sets and theoretical analysis are 
needed to further support our findings in this 
paper. 
References 
Bacchiani, M., Roark, B., and Saraclar, M. 2004. Lan-
guage model adaptation with MAP estimation and 
the perceptron algorithm. In HLT-NAACL 2004. 21-24. 
Collins, Michael and Terry Koo 2005. Discriminative 
reranking for natural language parsing. Computational 
Linguistics 31(1): 25-69. 
Duda, Richard O, Hart, Peter E. and Stork, David G. 
2001. Pattern classification. John Wiley & Sons, Inc. 
Donoho, D., I. Johnstone, G. Kerkyachairan, and D. 
Picard. 1995. Wavelet shrinkage; asymptopia? (with 
discussion), J. Royal. Statist. Soc. 57: 201-337. 
Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani. 
2004. Least angle regression. Ann. Statist. 32, 407-499. 
Freund, Y, R. Iyer, R. E. Schapire, and Y. Singer. 1998. 
An efficient boosting algorithm for combining pref-
erences. In ICML?98.  
Hastie, T., R. Tibshirani and J. Friedman. 2001. The 
elements of statistical learning. Springer-Verlag, New 
York. 
Gao, Jianfeng, Hisami Suzuki and Yang Wen. 2002. 
Exploiting headword dependency and predictive 
clustering for language modeling. In EMNLP 2002. 
Gao. J., Yu, H., Yuan, W., and Xu, P. 2005. Minimum 
sample risk methods for language modeling. In 
HLT/EMNLP 2005. 
Osborne, M.R. and Presnell, B. and Turlach B.A. 2000a. 
A new approach to variable selection in least squares 
problems. Journal of Numerical Analysis, 20(3). 
Osborne, M.R. and Presnell, B. and Turlach B.A. 2000b. 
On the lasso and its dual. Journal of Computational and 
Graphical Statistics, 9(2): 319-337. 
Roark, Brian, Murat Saraclar and Michael Collins. 
2004. Corrective language modeling for large vo-
cabulary ASR with the perceptron algorithm. In 
ICASSP 2004. 
Schapire, Robert E. and Yoram Singer. 1999. Improved 
boosting algorithms using confidence-rated predic-
tions. Machine Learning, 37(3): 297-336. 
Suzuki, Hisami and Jianfeng Gao. 2005. A comparative 
study on language model adaptation using new 
evaluation metrics. In HLT/EMNLP 2005. 
Tibshirani, R. 1996. Regression shrinkage and selection 
via the lasso. J. R. Statist. Soc. B, 58(1): 267-288. 
Yuan, W., J. Gao and H. Suzuki. 2005. An Empirical 
Study on Language Model Adaptation Using a Met-
ric of Domain Similarity. In IJCNLP 05.  
Zhao, P. and B. Yu. 2004. Boosted lasso. Tech Report, 
Statistics Department, U. C. Berkeley. 
Zhu, J. S. Rosset, T. Hastie, and R. Tibshirani. 2003. 
1-norm support vector machines. NIPS 16. MIT Press. 
231
 Table 1. CER (%) and CER reduction (%) (Y=Yomiuri; T=TuneUp; E=Encarta; S=-Shincho) 
Domain Entropy vs.Nikkei Baseline MAP (over Baseline) Boosting (over MAP) BLasso (over MAP/Boosting) 
Y (800) 7.69 3.70 3.70 (+0.00) 3.13 (+15.41) 3.01 (+18.65/+3.83) 
Y (8K) 7.69 3.70 3.69 (+0.27) 2.88 (+21.95) 2.85 (+22.76/+1.04) 
Y (72K) 7.69 3.70 3.69 (+0.27) 2.78 (+24.66) 2.73 (+26.02/+1.80) 
T (800) 7.95 5.81 5.81 (+0.00) 5.69 (+2.07) 5.63 (+3.10/+1.05) 
T (8K) 7.95 5.81 5.70 (+1.89) 5.48 (+5.48) 5.33 (+6.49/+2.74) 
T (72K) 7.95 5.81 5.47 (+5.85) 5.33 (+2.56) 5.05 (+7.68/+5.25) 
E (800) 9.30 10.24 9.60 (+6.25) 9.82 (-2.29) 9.18 (+4.38/+6.52) 
E (8K) 9.30 10.24 8.64 (+15.63) 8.54 (+1.16) 8.04 (+6.94/+5.85) 
E (72K) 9.30 10.24 7.98 (+22.07) 7.53 (+5.64) 7.20 (+9.77/+4.38) 
S (800) 9.40 12.18 11.86 (+2.63) 11.91 (-0.42) 11.79 (+0.59/+1.01) 
S (8K) 9.40 12.18 11.15 (+8.46) 11.09 (+0.54) 10.73 (+3.77/+3.25) 
S (72K) 9.40 12.18 10.76 (+11.66) 10.25 (+4.74) 9.64 (+10.41/+5.95) 
 
  
 
Figure 3. L1 curves: models are trained 
on the E(8K) dataset. 
Figure 4. Test error curves: models are 
trained on the E(8K) dataset. 
Figure 5. Test error curves: models are 
trained on the Y(8K) dataset, including 
headword bigram features. 
   
Figure 6. Test error curves: models are 
trained on the T(8K) dataset, including 
headword bigram features. 
Figure 7. Test error curves: models are 
trained on the E(8K) dataset, including 
headword bigram features. 
Figure 8. Test error curves: models are 
trained on the S(8K) dataset, including 
headword bigram features. 
232
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1049?1056,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning to Predict Case Markers in Japanese                
 
     Hisami Suzuki    Kristina Toutanova1 
Microsoft Research 
One Microsoft Way, Redmond WA 98052 USA 
{hisamis,kristout}@microsoft.com 
 
Abstract 
Japanese case markers, which indicate the gram-
matical relation of the complement NP to the 
predicate, often pose challenges to the generation 
of Japanese text, be it done by a foreign language 
learner, or by a machine translation (MT) system. 
In this paper, we describe the task of predicting 
Japanese case markers and propose machine 
learning methods for solving it in two settings: (i) 
monolingual, when given information only from 
the Japanese sentence; and (ii) bilingual, when 
also given information from a corresponding Eng-
lish source sentence in an MT context. We formu-
late the task after the well-studied task of English 
semantic role labelling, and explore features from 
a syntactic dependency structure of the sentence. 
For the monolingual task, we evaluated our models 
on the Kyoto Corpus and achieved over 84% ac-
curacy in assigning correct case markers for each 
phrase. For the bilingual task, we achieved an ac-
curacy of 92% per phrase using a bilingual dataset 
from a technical domain. We show that in both 
settings, features that exploit dependency informa-
tion, whether derived from gold-standard annota-
tions or automatically assigned, contribute signifi-
cantly to the prediction of case markers.1  
1 Introduction: why predict case? 
Generation of grammatical elements such as inflec-
tional endings and case markers has become an impor-
tant component technology, particularly in the context 
of machine translation (MT). In an English-to-Japanese 
MT system, for example, Japanese case markers, 
which indicate grammatical relations (e.g., subject, 
object, location) of the complement noun phrase to the 
predicate, are among the most difficult to generate 
appropriately. This is because the case markers often 
do not correspond to any word in the source language 
as many grammatical relations are expressed via word 
order in English. It is also difficult because the map-
ping between the case markers and the grammatical 
                                                        
1
 Author names arranged alphabetically 
relations they express is very complex. For the same 
reasons, generation of case markers is challenging to 
foreign language learners. This difficulty in generation, 
however, does not mean the choice of case markers is 
insignificant: when a generated sentence contains mis-
takes in grammatical elements, they often lead to se-
vere unintelligibility, sometimes resulting in a different 
semantic interpretation from the intended one. There-
fore, having a model that makes reasonable predictions 
about which case marker to generate given the content 
words of a sentence, is expected to help MT and gen-
eration in general, particularly when the source (or 
native) and the target languages are morphologically 
divergent.  
But how reliably can we predict case markers in 
Japanese using the information that exists only in the 
sentence? Consider the example in Figure 1. This sen-
tence contains two case markers, kara 'from' and ni, the 
latter not corresponding to any word in English. If we 
were to predict the case markers in this sentence, there 
are multiple valid answers for each decision, many of 
which correspond to different semantic relations. For 
example, for the first case marker slot in Figure 1 filled 
by kara, wa (topic marker), ni 'in' or no case marker at 
all are all reasonable choices, while other markers such 
as wo (object marker), de 'at', made 'until', etc. are not 
considered reasonable. For the second slot filled by ni, 
ga (subject marker) is also a grammatically reasonable 
choice, making Einstein the subject of idolize, thus 
changing the meaning of the sentence. As is obvious in 
this example, the choice among the correct answers is 
determined by the speaker's intent in uttering the sen-
tence, and is therefore impossible to recover from the 
content words or the sentence structure alone. At the 
same time, many impossible or unlikely case marking 
decisions can be eliminated by a case prediction model. 
Combined with an external component (for example an 
MT component) that can resolve semantic and inten-
tional ambiguity, a case prediction model can be quite 
useful in sentence generation. 
This paper discusses the task of case marker as-
signment in two distinct but related settings. After 
defining the task in Section 2 and describing our mod-
els in Section 3, we first discuss the monolingual task 
in Sections 4, whose goal is to predict the case markers 
1049
 using Japanese sentences and their dependency struc-
ture alone. We formulated this task after the 
well-studied task of semantic role labeling in English 
(e.g., Gildea and Jurafsky, 2002; Carreras and M?rques, 
2005), whose goal is to assign one of 20 semantic role 
labels to each phrase in a sentence with respect to a 
given predicate, based on the annotations provided by 
PropBank (Palmer et al, 2005). Though the task of 
case marker prediction is more ambiguous and subject 
to uncertainty than the semantic role labeling task, we 
obtained some encouraging results which we present in 
Section 4. Next, in Section 5, we describe the bilingual 
task, in which information about case assignment can 
be extracted from a corresponding source language 
sentence. Though the process of MT introduces uncer-
tainties in generating the features we use, we show that 
the benefit of using dependency structure in our mod-
els is far greater than not using it even when the as-
signed structure is not perfect.  
2 The task of case prediction 
In this section, we define the task of case prediction. 
We start with the description of the case markers we 
used in this study. 
2.1 Nominal particles in Japanese 
Traditionally, Japanese nominal postpositions are clas-
sified into the following three categories (e.g., Tera-
mura, 1991; Masuoka and Takubo, 1992):   
Case particles (or case markers). They indicate 
grammatical relations of the complement NP to the 
predicate. As they are jointly determined by the NP 
and the predicate, case markers often do not allow a 
simple mapping to a word in another language, which 
makes their generation more difficult. The relationship 
between the case marker and the grammatical relation 
it indicates is not straightforward either: a case marker 
can (and often does) indicate multiple grammatical 
relations as in Ainshutain-ni akogareru "idolize Ein-
stein" where ni marks the Object relation, and in To-
kyo-ni sumu "live in Tokyo" where ni indicates Loca-
tion. Conversely, the same grammatical relation may 
be indicated by different case markers: both ni and de 
in Tokyo-ni sumu "live in Tokyo" and Tokyo-de au 
"meet in Tokyo" indicate the Location relation. We 
included 10 case markers as the primary target of pre-
diction, as shown in the first 10 lines of Table 1. 
Conjunctive particles. These particles are used to 
conjoin words and phrases, corresponding to English 
"and" and "or". As their occurrence is not predictable 
from the sentence structure alone, we did not include 
them in the current prediction task.  
Focus particles. These particles add focus to a phrase 
against a given background or contextual knowledge, 
for example shika and mo in pasuta-shika tabenakatta 
"ate only pasta" and pasuta-mo tabeta "also ate pasta", 
corresponding to only and also respectively. Note that 
they often replace case markers: in the above examples, 
the object marker wo is no longer present when shika 
or mo is used. As they add information to the predi-
cate-argument structure and are in principle not pre-
dictable given the sentence structure alone, we did not 
consider them as the target of our task. One exception 
is the topic marker wa, which we included as a target 
of prediction for the following reasons:  
 Some linguists recognize wa as a topic marker, 
separately from other focus particles (e.g. Masuoka 
and Takubo, 1992). The main function of wa is to 
introduce a topic in the sentence, which is to a some 
extent predictable from the structure of the sentence.  
 wa is extremely frequent in Japanese text. For ex-
ample, it accounts for 13.2% of all postpositions in 
Kyoto University Text Corpus (henceforth Kyoto 
Corpus, Kurohashi and Nagao, 1997), making it the 
third most frequent postposition after no (20.57%) 
and wo (13.5%). Generating wa appropriately thus 
greatly enhances the readability of the text.   
 Unlike other focus particles such as shika and mo, 
wa does not translate into any word in English, 
which makes it difficult to generate by using the in-
formation from the source language.  
Therefore, in addition to the 10 true case markers, we 
also included wa as a case marker in our study.2 Fur-
thermore, we also included the combination of case 
particles plus wa as a secondary target of prediction. 
The case markers that can appear followed by wa are 
indicated by a check mark in the column "+wa" in 
Table 1. Thus there are seven secondary targets: niwa, 
karawa, towa, dewa, ewa, madewa, yoriwa. Therefore, 
we have in total 18 case particles to assign to phrases.  
2.2 Task definition 
The case prediction task we are solving is as follows. 
We are given a sentence as a list of bunsetsu together 
                                                        
2
 This set comprises the majority (92.5%) of the nominal parti-
cles, while conjunctive and focus particles account for only 
7.5% of the nominal particles in Kyoto Corpus. 
 
Figure 1. Example of case markers in Japanese (taken 
from the Kyoto Corpus). Square brackets indicate bun-
setsu (phrase) boundaries, to be discussed below. Ar-
rows between phrases indicate dependency relations.  
1050
 with a dependency structure. For our monolingual 
experiments, we used the dependency structure annota-
tion in the Kyoto Corpus; for our bilingual experiments, 
we used automatically derived dependency structure 
(Quirk et al, 2005). Each bunsetsu (or simply phrase 
in this paper) is defined as consisting of one content 
word (or n-content words in the case of compounds 
with n-components) plus any number of function 
words (including particles, auxiliaries and affixes). 
Case markers are classified as function words, and 
there is at most one case marker per phrase.3 In testing, 
the case marker for each phrase is hidden; the task is to 
assign to each phrase one of the 18 case markers de-
fined above or NONE; NONE indicates that the phrase 
does not have a case marker.  
2.3 Related work 
Though the task of case marker prediction as formu-
lated in this paper is novel, similar tasks have been 
defined in the past. The semantic role labeling task 
mentioned in Section 1 is one example; the task of 
function tag assignment in English (e.g., Blaheta and 
Charniak, 2000) is another. These tasks are similar to 
the case prediction task in that they try to assign se-
mantic or function tags to a parsed structure. However, 
there is one major difference between these tasks and 
the current task: semantic role labels and function tags 
can for the most part be uniquely determined given the 
sentence and its parse structure; decisions about case 
markers, on the other hand, are highly ambiguous 
given the sentence structure alone, as mentioned in 
Section 1. This makes our task more ambiguous than 
the related tasks. As a concrete comparison, the two 
most frequent semantic role labels (ARG0 and ARG1) 
account for 60% of the labeled arguments in PropBank 
                                                        
3
 One exception is that no can appear after certain case markers; 
in such cases, we considered no to be the case for the phrase.  
4
 no is typically not considered as a case marker but rather as a 
conjunctive particle indicating adnominal relation; however, as
no can also be used to indicate the subject in a relative clause, 
we included it in our study.  
(Carreras and M?rquez, 2005), whereas our 2 most 
frequent case markers (no and wo) account for only 
43% of the case-marked phrases. We should also note 
that semantic role labels and function tags have been 
artificially defined in accordance with theoretical deci-
sions about what annotations should be useful for 
natural language understanding tasks; in contrast, the 
case markers are part of the surface sentence string and 
do not reflect any theoretical decisions. 
The task of case prediction in Japanese has previ-
ously focused on recovering implicit case relations, 
which result when noun phrases are relativized or 
topicalized (e.g., Baldwin, 2000; Kawahara et al, 
2004; Murata and Isahara, 2005). Their goal is differ-
ent form ours, as we aim to generate surface forms of 
case markers rather than recover deeper case relations 
for which surface case marker are often used as a 
proxy.  
In the context of sentence generation, Gamon et al 
(2002) used a decision tree to classify nouns into one 
of the four cases in German, as part of their sentence 
realization from a semantic representation, achieving 
high accuracy (87% to 93.5%). Again, this is a sub-
stantially easier task than ours, because there are only 
four classes and one of them (nominative) accounts for 
70% of all cases. Uchimoto et al (2002), which is the 
work most related to ours, propose a model of generat-
ing function words (not limited to case markers) from 
"keywords" or headwords of phrases in Japanese. The 
components of their model are based on n-gram lan-
guage models using the surface word strings and bun-
setsu dependency information, and the results they 
report are not comparable to ours, as they limit their 
test sentences to the ones consisting only of two or 
three content words. We will see in the next section 
that our models are also quite different from theirs as 
we employ a much richer set of features.  
3 Classifiers for case prediction  
We implemented two types of models for the task of 
case prediction: local models, which choose the case 
marker of each phrase independently of the case mark-
ers of other phrases, and joint models, which incorpo-
rate dependencies among the case markers of depend-
ents of the same head phrase. We describe the two 
types of models in turn.  
3.1 Local classifiers 
Following the standard practice in semantic role label-
ing, we divided the case prediction task into the tasks 
of identification and classification (Gildea and Juraf-
sky, 2002; Pradhan et al, 2004). In the identification 
task, we assign to each phrase one of two labels: HAS-
CASE, meaning that the phrase has a case marker, or 
NONE, meaning that it does not have a case. In the 
case markers grammatical functions (e.g.) +wa 
 
 ga subject; object  

 wo object; path  
4
 
no genitive; subject  

 ni dative object, location  

 kara source  

 to quotative, reciprocal, as  

 de location, instrument, cause  
	
 e goal, direction  



 made goal (up to, until)  

 yori source, object of comparison  

 wa topic  
Table 1. Case markers included in this study 
1051
 classification task, we assign one of the 18 case mark-
ers to each phrase that has been labeled with HASCASE 
by the identification model. 
We train a binary classifier for identification and a 
multi-class classifier (with 18 classes) for classification. 
We obtain a classifier for the complete task by chain-
ing the two classifiers. Let PID(c|b) and  PCLS(c|b) 
denote the probability of class c for bunsetsu b accord-
ing to the identification and classification models, re-
spectively. We define the probability distribution over 
classes of the complete model for case assignment as 
follows: 
 PCaseAssign(NONE |b) = PID(NONE |b)  
 PCaseAssign(l|b) = PID(HASCASE |b)* PCLS(l|b) 
Here, l denotes one of the 18 case markers. 
We employ this decomposition mainly for effi-
ciency in training: that is, the decomposition allows us 
to train the classification models on a subset of training 
examples consisting only of those phrases that have a 
case marker, following Toutanova et al (2005). 
Among various machine learning methods that can be 
used to train the classifiers, we chose log-linear models 
for both identification and classification tasks, as they 
produce probability distributions which allows chain-
ing of  the two component models and easy integra-
tion into an MT system. 
3.2 Joint classifiers 
Toutanova et al (2005) report a substantial improve-
ment in performance on the semantic role labeling task 
by building a joint classifier, which takes the labels of 
other phrases into account when classifying a given 
phrase. This is motivated by the fact that the argument 
structure is a joint structure, with strong dependencies 
among arguments. Since the case markers also reflect 
the argument structure to some extent, we implemented 
a joint classifier for the case prediction task as well.  
We applied the joint classifiers in the framework of 
N-best reranking (Collins, 2000), following Toutanova 
et al (2005). That is, we produced N-best (N=5 in our 
experiments) case assignment sequence candidates for 
a set of sister phrases using the local models, and 
trained a joint classifier that learns to choose the best 
candidate from the set of sisters. The oracle accuracy 
of the 5-best candidate list was 95.9% per phrase.   
4 Monolingual case prediction task 
In this section we describe our models trained and 
evaluated using the gold-standard dependency annota-
tions provided by the Kyoto Corpus. These annotations 
allow us to define a rich set of features exploring the 
syntactic structure. 
4.1 Features 
The basic local model features we used for the identi-
fication and classification models are listed in Table 2. 
They consist of features for a phrase, for its parent 
phrase and for their relations. Only one feature 
(GrandparentNounSubPos) currently refers to the 
grandparent of the phrase; all other features are be-
tween the phrase, its parent and its sibling nodes, and 
are a superset of the dependency-based features used 
by Hacioglu (2004) for the semantic labeling task. In 
addition to these basic features, we added 20 combined 
features, some of which are shown at the bottom of 
Table 2. 
For the joint model, we implemented only two 
types of features: sequence of non-NONE case markers 
for a set of sister phrases, and repetition of non-NONE 
case markers. These features are intended to capture 
regularities in the sequence of case markers of phrases 
that modify the same head phrase.  
All of these features are represented as binary fea-
tures: that is, when the value of a feature is not binary, 
we have treated the combination of the feature name 
plus the value as a unique feature. With a count cut-off 
of 2 (i.e., features must occur at least twice to be in the 
model), we have 724,264 features in the identification 
Basic features for phrases (self, parent) 
HeadPOS, PrevHeadPOS, NextHeadPOS  
PrevPOS,Prev2POS,NextPOS,Next2POS 
HeadNounSubPos: time, formal nouns, adverbial 
HeadLemma 
HeadWord, PrevHeadWord, NextHeadWord 
PrevWord, Prev2Word, NextWord, Next2Word 
LastWordLemma (excluding case markers) 
LastWordInfl (excluding case markers) 
IsFiniteClause 
IsDateExpression 
IsNumberExpression 
HasPredicateNominal 
HasNominalizer 
HasPunctuation: comma, period 
HasFiniteClausalModifier 
RelativePosition: sole, first, mid, last 
NSiblings (number of siblings) 
Position (absolute position among siblings) 
Voice: pass, caus, passcaus 
Negation 
Basic features for phrase relations (parent-child pair) 
DependencyType: D,P,A,I 
Distance: linear distance in bunsetsu, 1, 2-5, >6 
Subcat: POS tag of parent + POS tag of all children + 
indication for current 
Combined features (selected) 
HeadPOS + HeadLemma 
ParentLemma + HeadLemma 
Position + NSiblings 
IsFiniteClause + GrandparentNounSubPos 
Table 2: Basic and combined features for local classifiers 
1052
 model, and 3,963,096 features in the classification 
model. The number of joint features in the joint model 
is 3,808. All models are trained using a Gaussian prior.  
4.2 Data and baselines 
We divided the Kyoto Corpus (version 3.0) into the 
following three sections:   
 Training: contains news articles of January 1, 3-11 
and editorial articles of January-August; 24,263 
sentences, 234,474 phrases.  
 Devtest: contains news articles of January 12-13 and 
editorial article of September. 4,833 sentences, 
47,580 phrases.  
 Test: contains news articles of January 14-17 and 
editorial articles of October-December. 9,287 sen-
tences, 89,982 phrases.  
The devtest set was used only for tuning model pa-
rameters and for performing error analysis.  
As no previous work exists on the task of predicting 
case markers on the Kyoto Corpus, it is important to 
establish a good baseline. The simplest baseline of 
always selecting the most frequent label (NONE) gives 
us an accuracy of 47.5% on the test set. Out of the 
non-NONE case markers, the most frequent is no, 
which occurs in 26.6% of all case-marked phrases.   
A more reasonable baseline is to use a language 
model to predict case. We trained and tested two lan-
guage models: the first model, called KCLM, is trained 
on the same data as our log-linear models (24,263 sen-
tences); the second model, called BigCLM, is trained 
on much more data from the same domain (826,373 
sentences), taking advantage of the fact that language 
models do not require dependency annotation for 
training. The language models were trained using the 
CMU language modeling toolkit with default parame-
ter settings (Clarkson and Rosenfeld, 1997). 
We tested the language model baselines using the 
same task set-up as for our classifier: for each phrase, 
each of the 18 possible case markers and NONE is 
evaluated. The position for insertion of a case marker 
in each phrase is given according to our task set-up, i.e., 
at the end of a phrase preceding any punctuation. We 
choose the case assignment of the sequence of phrases 
in the sentence that maximizes the language model 
probability of the resulting sentence. We computed the 
most likely case assignment sequence using a dynamic 
programming algorithm.  
4.3 Results and discussion 
The results of running our models on case marker pre-
diction are shown in Table 3. The first three rows cor-
respond to the components of the local model: the 
identification task (Id, for all phrases), the classifica-
tion task (Cls, only for case-marked phrases) and the 
complete task (Both, for all phrases). The accuracy on 
the complete task using the local model is 83.9%; the 
joint model improves it to 84.3%.   
The improvement due to the joint model is small in 
absolute percentage points (0.4%), but is statistically 
significant according to a test for the difference of 
proportions (p< 0.05). The use of a joint classifier did 
not lead to as large an improvement over the local 
classifier as for the semantic role labeling task.  There 
are several reasons for that we can think of. First, we 
have only used a limited set of features for the joint 
model, i.e., case sequence and repetition features. A 
more extensive use of global features might lead to a 
larger improvement. Secondly, unlike the task of se-
mantic role labeling, where there are about 20 phrases 
that need to be labeled with respect to a predicate, 
about 50% of all phrases in the Kyoto Corpus do not 
have sister nodes. This means that these phrases cannot 
take advantage of the joint classifier using the current 
model formulation. Finally, case markers are much 
shallower than semantic role labels in the level of lin-
guistic analysis, and so are inherently subject to more 
variations, including missing arguments (so called zero 
pronouns) and repeated case markers corresponding to 
different semantic roles.  
From Table 3, it is clear that our models outperform 
the baseline model significantly. The language model 
trained on the same data has much lower performance 
(67.0% vs. 84.3%), which shows that our system is 
exploiting the training data much more efficiently by 
looking at the dependency and other syntactic features. 
An inspection of the 500 most highly weighted features 
also indicates that phrase dependency-based features 
are very useful for both identification and classification. 
Given much more data, though, the language model 
improves significantly to 78%, but our classifier still 
achieves a 29% error reduction over it. The differences 
between the language models and the log-linear models 
are statistically significant at level p < 0.01 according 
to a test for the difference of proportions. 
 Figure 2 plots the recall and precision for the fre-
quently occurring (>500) cases. We achieve good re-
sults on NONE and no, which are the least ambiguous 
decisions. Cases such as ni, wa, ga, and de are highly 
confusable with other markers as they indicate multiple 
grammatical relations, and the performance of our 
Models Task Training  Test  
log-linear Id 99.8 96.9 
log-linear Cls 96.6 74.3 
log-linear (local) Both 98.0 83.9 
log-linear( joint) Both 97.8 84.3 
baseline (frequency) Both 48.2 47.5 
baseline (KCLM) Both 93.9 67.0 
baseline (BigCLM) Both ? 78.0 
Table 3: Accuracy of case prediction models (%) 
1053
 models on them is therefore limited. As expected, per-
formance (especially recall) on secondary targets 
(dewa, niwa) suffers greatly due to the ambiguity with 
their primary targets.  
5 Bilingual case prediction task: simulating 
case prediction in MT 
Incorporating a case prediction model into MT requires 
taking additional factors into consideration, compared 
to the monolingual task described above. On the one 
hand, we need to extend our model to handle the addi-
tional knowledge source, i.e., the source sentence. This 
can potentially provide very useful features to our 
model, which are not available in the monolingual task. 
On the other hand, since gold-standard dependency 
annotation is not available in the MT context, we must 
deal with the imperfections in structural annotations.  
In this section, we describe our case prediction 
models in the context of English-to-Japanese MT. In 
this setting, dependency information for the target 
language (Japanese) is available only through projec-
tion of a dependency structure from the source lan-
guage (English) in a tree-to-string-based statistical MT 
system (Quirk et al, 2005). We conducted experiments 
using the English source sentences and the reference 
translations in Japanese: that is, our task is to predict 
the case markers of the Japanese reference translations 
correctly using all other words in the reference sen-
tence, information from the source sentence through 
word alignment, and the Japanese dependency struc-
ture projected via an MT component. Ultimately, our 
goal is to improve the case marker assignment of a 
candidate translation using a case prediction model; the 
experiments described in this section on reference 
translations serve as an important preliminary step 
toward achieving that final goal. We will show in this 
section that even the automatically derived syntactic 
information is very useful in assigning case markers in 
the target language, and that utilizing the information 
from the source language also greatly contributes to 
reducing case marking errors.  
5.1 Data and task set-up 
The dataset we used is a collection of parallel Eng-
lish-Japanese sentences from a technical (computer) 
domain. We used 15,000 sentence pairs for training, 
5,000 for development, and 4,241 for testing.  
The parallel sentences were word-aligned using 
GIZA++ (Och and Ney, 2000), and submitted to a 
tree-to-string-based MT system (Quirk et al, 2005) 
which utilizes the dependency structure of the source 
language and projects dependency structure to the 
target language. Figure 3 shows an example of an 
aligned sentence pair: on the source (English) side, 
part-of-speech (POS) tags and word dependency 
structure are assigned (solid arcs). The alignments 
between English and Japanese words are indicated by 
the dotted lines. In order to create phrase-level de-
pendency structures like the ones utilized in the Kyoto 
Corpus monolingual task, we derived some additional 
information for the Japanese sentence in the following 
manner.  
 
Figure 3. Aligned English-Japanese sentence pair 
First, we tagged the sentence using an automatic 
tagger with a set of 19 POS tags. We used these POS 
tags to parse the words into phrases (bunsetsu): each 
bunsetsu consists of one content word plus any number 
of function words, where content and function words 
are defined via POS. We then constructed a 
phrase-level dependency structure using a breadth-first 
traversal of the word dependency structure projected 
from English. These phrase dependencies are indicated 
by bold arcs in Figure 3. The case markers to be pre-
dicted (wa and de in this case) are underlined.   
 The task of case marker prediction is the same as 
described in Section 2: to assign one of the 18 case 
markers described in Section 2 or NONE to each phrase. 
5.2 Baseline models 
We implemented the baseline models discussed in 
Section 4.2 for this domain as well. The most frequent 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
niwa (523)
dewa (548)
kara (868)
de (2582)
to (3664)
ga (5797)
wa (5937)
ni (6457)
wo (7782)
no (12570)
NONE (42756)
precision
recall
 
Figure 2: Precision and recall per case marker (frequency 
in parentheses) 
1054
 case assignment is again NONE, which accounts for 
62.0% of the test set. The frequency of NONE is higher 
in this task than in the Kyoto Corpus, because our 
bunsetsu-parsing algorithm prefers to err on the side of 
making too many rather than too few phrases. This is 
because our final goal is to generate all case markers, 
and if we mistakenly joined two bunsetsu into one, our 
case assigner would be able to propose only one case 
marker for the resulting bunsetsu, which would be 
necessarily wrong if both bunsetsu had case markers. 
The most frequent case marker is again no, which oc-
curs in 29.4% of all case-marked phrases. As in the 
monolingual task, we trained two trigram language 
models: one was trained on the training set of our case 
prediction models (15,000 sentences); another was 
trained on a much larger set of 450,000 sentences from 
the same domain. The results of these baselines are 
discussed in Section 5.4. 
5.3 Log-linear models 
The models we built for this task are log-linear models 
as described in Section 3. In order to isolate the impact 
of information from the source language available for 
the case prediction task, we built two kinds of models: 
monolingual models, which do not use any information 
from the source English sentences, and bilingual mod-
els, which use information from the source. Both mod-
els are local models in the sense discussed in Section 3.  
Table 4 shows the features used in the monolingual 
and bilingual models, along with the examples (the 
value of the feature for the phrase [saabisu wa] in Fig-
ure 3); in addition to these, we also provided some 
feature combinations for both monolingual and bilin-
gual models. Many of the monolingual features (i.e., 
first 11 lines in Table 4) are also present in Table 2. 
Note that lexically based features are of greater impor-
tance for this task, as the dependency information 
available in this context is of much poorer quality than 
that provided by the Kyoto Corpus. In addition to the 
features in Table 2, we added a Direction feature (with 
values left and right), and an Alternative Parent feature. 
Alternative parents are all words which are the parents 
of any word in the phrase, according to the word-based 
dependency tree, with the constraint that case markers 
cannot be alternative parents. This feature captures the 
information that is potentially lost in the process of 
building a phrase dependency structure from word 
dependency information in the target language.  
The bottom half of Table 4 shows bilingual features. 
The features of the source sentence are obtained 
through word alignments. We create features from the 
source words aligned to the head of the phrase, to the 
head of the parent phrase, or to any alterative parents. 
If any word in the phrase is aligned to a preposition in 
the source language, our model can use the information 
as well. In addition to word- and POS-features for 
aligned source words, we also refer to the correspond-
ing dependency between the phrase and its parent 
phrase in the English source. If the head of the Japa-
nese phrase is aligned to a single source word s1, and 
the head of its parent phrase is aligned to a single 
source word s2, we extract the relationship between s1 
and s2, and define subcategorization, direction, distance, 
and number of siblings features, in order to capture the 
grammatical relation in the source, which is more reli-
able than in the projected target dependency structure. 
 
5.4 Results and discussion 
Table 5 summarizes the results on the complete case 
assignment task in the MT context. Compared to the 
language model trained on the same data (15kLM), our 
Monolingual features  
Feature Example 
HeadWord /HeadPOS saabisu/NN 
PrevWord/PrevPOS kono/AND 
Prev2Word/Prev2WordPOS none/none 
NextWord/NextPOS seefu/NN 
Next2Word/Net2POS moodo/NN 
PrevHeadWord/PrevHeadPOS kono/AND 
NextHeadWord/NextHeadPOS seefu/NN 
ParentHeadWord/ParentHeadPOS kaishi/VN 
Subcat: POS tags of all sisters and parent NN-c,NN,VN-h 
NSiblings (including self) 2 
Distance 1 
Direction left 
Alternative Parent Word /POS saabisu/NN 
Bilingual features 
Feature Example 
Word/POS of source words aligned to the 
head of the phrase 
service/NN 
Word/POS of all source words aligned to 
any word in the phrase 
service/NN 
Word/POS of all source words aligned to 
the head word of the parent phrase 
started/VERB 
Word/POS of all source words aligned to 
alternative parent words of the phrase 
service/NN, 
started/VERB 
All source preposition words in 
Word/POS of parent of source word aligned 
to any word in the phrase 
started/VERB 
Aligned Subcat          NN-c,VERB,VERB,VERB-h,PREP 
Aligned NSiblings 4 
Aligned Distance 2 
Aligned Direction left 
Table 4: Monolingual and bilingual features 
Model Test data 
baseline (frequency) 62.0 
baseline (15kLM) 79.0 
baseline (450kLM) 83.6 
log-linear monolingual 85.3 
log-linear bilingual 92.3 
Table 5: Accuracy of bilingual case prediction (%) 
1055
 monolingual model performs significantly better, 
achieving a 30% error reduction (85.3% vs. 79.0%). 
Our monolingual model outperforms even the language 
model trained on 30 times more data (85.3% vs. 
83.6%), with an error reduction of 10%. The difference 
is statistically significant at level p < 0.01 according to 
a test for the difference of proportions. This means that 
even though the projected dependency information is 
not perfect, it is still useful for the case prediction task.  
When we add the bilingual features, the error rate of 
our model is cut almost in half: the bilingual model 
achieves an error reduction of 48% over the monolin-
gual model (92.3% vs. 85.3%, statistically significant 
at level p < 0.01). This result is very encouraging: it 
indicates that information from the source sentence can 
be exploited very effectively to improve the accuracy 
of case assignment. The usefulness of the source lan-
guage information is also obvious when we inspect 
which case markers had the largest gains in accuracy 
due to this information: the top three cases were kara 
(0.28 to 0.65, a 57% gain), dewa (0.44 to 0.65, a 32% 
gain) and to (0.64 to 0.85, a 24% gain), all of which 
have translations as English prepositions. Markers such 
as ga (subject marker, 0.68 to 0.74, a 8% gain) and wo 
(object marker, 0.83 to 0.86, a 3.5% gain), on the other 
hand, showed only a limited gain.  
6 Conclusion and future directions 
This paper described the task of predicting case mark-
ers in Japanese, and reported results in a monolingual 
and a bilingual settings. The results show that the mod-
els we proposed, which explore syntax-based features 
and features from the source language in the bilingual 
task, can effectively predict case markers.  
There are a number of extensions and next steps we 
can think of at this point, the most immediate and im-
portant one of which is to incorporate the proposed 
model in an end-to-end MT system to make improve-
ments in the output of MT. We would also like to per-
form a more extensive analysis of features and feature 
ablation experiments. Finally, we would also like to 
extend the proposed model to include languages with 
inflectional morphology and the prediction of gram-
matical elements in general.  
Acknowledgements 
We would like to thank the anonymous reviewers for 
their comments, and Bob Moore, Arul Menezes, Chris 
Quirk, and Lucy Vanderwende for helpful discussions. 
References 
Baldwin, T. 2004. Making Sense of Japanese Relative 
Clause Constructions, In Proceedings of the 2nd 
Workshop on Text Meaning and Interpretation.  
Blaheta, D. and E. Charniak. 2000. Assigning function 
tags to parsed text. In Proceedings of NAACL, 
pp.234-240. 
Carreras, X. and L. M?rquez. 2005. Introduction to the 
CoNLL-2005 Shared Task: Semantic Role Labeling. In 
Proceedings of CoNLL-2005.  
Clarkson, P.R. and R. Rosenfeld. 1997. Statistical Lan-
guage Modeling Using the CMU-Cambridge Toolkit. 
In Proceedings of ESCA Eurospeech, pp. 2007-2010. 
Collins, M. 2000. Discriminative reranking for natural 
language parsing. In Proceedings of ICML.  
Gamon, M., E. Ringger, S. Corston-Oliver and R. Moore. 
2002. Machine-learned Context for Linguistic Opera-
tions in German Sentence Realization. In Proceeding 
of ACL.  
Gildea, D. and D. Jurafsky. 2002. Automatic Labeling of 
Semantic Roles. In Computational Linguistics 28(3): 
245-288.  
Hacioglu, K. 2004.  Semantic Role Labeling using De-
pendency Trees. In Proceedings of COLING 2004. 
Kawahara, D., N. Kaji and S. Kurohashi. 2000. Japanese 
Case Structure Analysis by Unsupervised Construction 
of a Case Frame Dictionary. In Proceedings of COL-
ING, pp. 432-438.  
Kurohashi, S. and M.Nagao. 1997. Kyoto University Text 
Corpus Project. In Proceedings of ANLP, pp.115-118.   
Masuoka, T. and Y. Takubo. 1992. Kiso Nihongo Bunpou 
(Fundamental Japanese grammar), revised version. 
Kuroshio Shuppan, Tokyo.   
Murata, M., and H. Isahara. 2005. Japanese Case Analysis 
Based on Machine Learning Method that Uses Bor-
rowed Supervised Data. In Proceedings of IEEE 
NLP-KE-2005, pp.774-779.  
Och, F.J. and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL: pp.440-447.  
Palmer, M., D. Gildea and P. Kingsbury. 2005. The 
Proposition Bank: An Annotated Corpus of Semantic 
Roles. In Computational Linguistics 31(1).  
Pradhan, S., W. Ward, K. Hacioglu, L. Martin, D. Juraf-
sky. 2004. Shallow Semantic Parsing Using Support 
Vector Machines. In Proceedings of HLT/NAACL. 
Quirk, C., A. Menezes and C. Cherry. 2005. Dependency 
Tree Translation: Syntactically Informed Phrasal SMT. In 
Proceedings of ACL. 
Teramura, H. 1991. Nihongo-no shintakusu-to imi (Japa-
nese syntax and meaning). Volume III. Kuroshio 
Shuppan, Tokyo.   
Toutanova, K., A. Haghighi and C. D. Manning. 2005. 
Joint Learning Improves Semantic Role Labeling. In 
Proceeding of ACL, pp.589-596.  
Uchimoto, K., S. Sekine and H. Isahara. 2002. Text Gen-
eration from Keywords. In Proceedings of COLING 
2002, pp.1037-1043.  
1056
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128?135,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generating Complex Morphology for Machine Translation
Einat Minkov?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
einatm@cs.cmu.edu
Kristina Toutanova
Microsoft Research
Redmond, WA, USA
kristout@microsoft.com
Hisami Suzuki
Microsoft Research
Redmond, WA, USA
hisamis@microsoft.com
Abstract
We present a novel method for predicting in-
flected word forms for generating morpho-
logically rich languages in machine trans-
lation. We utilize a rich set of syntactic
and morphological knowledge sources from
both source and target sentences in a prob-
abilistic model, and evaluate their contribu-
tion in generating Russian and Arabic sen-
tences. Our results show that the proposed
model substantially outperforms the com-
monly used baseline of a trigram target lan-
guage model; in particular, the use of mor-
phological and syntactic features leads to
large gains in prediction accuracy. We also
show that the proposed method is effective
with a relatively small amount of data.
1 Introduction
Machine Translation (MT) quality has improved
substantially in recent years due to applying data
intensive statistical techniques. However, state-of-
the-art approaches are essentially lexical, consider-
ing every surface word or phrase in both the source
sentence and the corresponding translation as an in-
dependent entity. A shortcoming of this word-based
approach is that it is sensitive to data sparsity. This is
an issue of importance as aligned corpora are an ex-
pensive resource, which is not abundantly available
for many language pairs. This is particularly prob-
lematic for morphologically rich languages, where
word stems are realized in many different surface
forms, which exacerbates the sparsity problem.
? This research was conducted during the author?s intern-
ship at Microsoft Research.
In this paper, we explore an approach in which
words are represented as a collection of morpholog-
ical entities, and use this information to aid in MT
for morphologically rich languages. Our goal is two-
fold: first, to allow generalization over morphology
to alleviate the data sparsity problem in morphology
generation. Second, to model syntactic coherence in
the form of morphological agreement in the target
language to improve the generation of morphologi-
cally rich languages. So far, this problem has been
addressed in a very limited manner in MT, most typ-
ically by using a target language model.
In the framework suggested in this paper, we train
a model that predicts the inflected forms of a se-
quence of word stems in a target sentence, given
the corresponding source sentence. We use word
and word alignment information, as well as lexi-
cal resources that provide morphological informa-
tion about the words on both the source and target
sides. Given a sentence pair, we also obtain syntactic
analysis information for both the source and trans-
lated sentences. We generate the inflected forms of
words in the target sentence using all of the available
information, using a log-linear model that learns the
relevant mapping functions.
As a case study, we focus on the English-Russian
and English-Arabic language pairs. Unlike English,
Russian and Arabic have very rich systems of mor-
phology, each with distinct characteristics. Trans-
lating from a morphology-poor to a morphology-
rich language is especially challenging since de-
tailed morphological information needs to be de-
coded from a language that does not encode this in-
formation or does so only implicitly (Koehn, 2005).
We believe that these language pairs are represen-
128
tative in this respect and therefore demonstrate the
generality of our approach.
There are several contributions of this work. First,
we propose a general approach that shows promise
in addressing the challenges of MT into morpholog-
ically rich languages. We show that the use of both
syntactic and morphological information improves
translation quality. We also show the utility of
source language information in predicting the word
forms of the target language. Finally, we achieve
these results with limited morphological resources
and training data, suggesting that the approach is
generally useful for resource-scarce language pairs.
2 Russian and Arabic Morphology
Table 1 describes the morphological features rele-
vant to Russian and Arabic, along with their possible
values. The rightmost column in the table refers to
the morphological features that are shared by Rus-
sian and Arabic, including person, number, gender
and tense. While these features are fairly generic
(they are also present in English), note that Rus-
sian includes an additional gender (neuter) and Ara-
bic has a distinct number notion for two (dual). A
central dimension of Russian morphology is case
marking, realized as suffixation on nouns and nom-
inal modifiers1. The Russian case feature includes
six possible values, representing the notions of sub-
ject, direct object, location, etc. In Arabic, like other
Semitic languages, word surface forms may include
proclitics and enclitics (or prefixes and suffixes as
we refer to them in this paper), concatenated to in-
flected stems. For nouns, prefixes include conjunc-
tions (wa: ?and?, fa: ?and, so?), prepositions (bi:
?by, with?, ka: ?like, such as?, li: ?for, to?) and a de-
terminer, and suffixes include possessive pronouns.
Verbal prefixes include conjunction and negation,
and suffixes include object pronouns. Both object
and possessive pronouns are captured by an indica-
tor function for its presence or absence, as well as
by the features that indicate their person, number
and gender. As can be observed from the table, a
large number of surface inflected forms can be gen-
erated by the combination of these features, making
1Case marking also exists in Arabic. However, in many in-
stances, it is realized by diacritics which are ignored in standard
orthography. In our experiments, we include case marking in
Arabic only when it is reflected in the orthography.
the morphological generation of these languages a
non-trivial task.
Morphologically complex languages also tend to
display a rich system of agreements. In Russian, for
example, adjectives agree with head nouns in num-
ber, gender and case, and verbs agree with the sub-
ject noun in person and number (past tense verbs
agree in gender and number). Arabic has a similarly
rich system of agreement, with unique characteris-
tics. For example, in addition to agreement involv-
ing person, number and gender, it also requires a de-
terminer for each word in a definite noun phrase with
adjectival modifiers; in a noun compound, a deter-
miner is attached to the last noun in the chain. Also,
non-human subject plural nouns require the verb to
be inflected in a singular feminine form. Generating
these morphologically complex languages is there-
fore more difficult than generating English in terms
of capturing the agreement phenomena.
3 Related Work
The use of morphological features in language mod-
elling has been explored in the past for morphology-
rich languages. For example, (Duh and Kirchhoff,
2004) showed that factored language models, which
consider morphological features and use an opti-
mized backoff policy, yield lower perplexity.
In the area of MT, there has been a large body
of work attempting to modify the input to a transla-
tion system in order to improve the generated align-
ments for particular language pairs. For example,
it has been shown (Lee, 2004) that determiner seg-
mentation and deletion in Arabic sentences in an
Arabic-to-English translation system improves sen-
tence alignment, thus leading to improved over-
all translation quality. Another work (Koehn and
Knight, 2003) showed improvements by splitting
compounds in German. (Nie?en and Ney, 2004)
demonstrated that a similar level of alignment qual-
ity can be achieved with smaller corpora applying
morpho-syntactic source restructuring, using hierar-
chical lexicon models, in translating from German
into English. (Popovic? and Ney, 2004) experimented
successfully with translating from inflectional lan-
guages into English making use of POS tags, word
stems and suffixes in the source language. More re-
cently, (Goldwater and McClosky, 2005) achieved
improvements in Czech-English MT, optimizing a
129
Features Russian Arabic Both
POS (11 categories) (18 categories)
Person 1,2,3
Number dual sing(ular), pl(ural)
Gender neut(er) masc(uline), fem(inine)
Tense gerund present, past, future, imperative
Mood subjunctive, jussive
Case dat(ive), prep(ositional), nom(inative), acc(usative), gen(itive)
instr(umental)
Negation yes, no
Determiner yes, no
Conjunction wa, fa, none
Preposition bi, ka, li, none
ObjectPronoun yes, no
Pers/Numb/Gend of pronoun, none
PossessivePronoun Same as ObjectPronoun
Table 1: Morphological features used for Russian and Arabic
set of possible source transformations, incorporat-
ing morphology. In general, this line of work fo-
cused on translating from morphologically rich lan-
guages into English; there has been limited research
in MT in the opposite direction. Koehn (2005) in-
cludes a survey of statistical MT systems in both di-
rections for the Europarl corpus, and points out the
challenges of this task. A recent work (El-Kahlout
and Oflazer, 2006) experimented with English-to-
Turkish translation with limited success, suggesting
that inflection generation given morphological fea-
tures may give positive results.
In the current work, we suggest a probabilistic
framework for morphology generation performed as
post-processing. It can therefore be considered as
complementary to the techniques described above.
Our approach is general in that it is not specific to
a particular language pair, and is novel in that it al-
lows modelling of agreement on the target side. The
framework suggested here is most closely related to
(Suzuki and Toutanova, 2006), which uses a proba-
bilistic model to generate Japanese case markers for
English-to-Japanese MT. This work can be viewed
as a generalization of (Suzuki and Toutanova, 2006)
in that our model generates inflected forms of words,
and is not limited to generating a small, closed set of
case markers. In addition, the morphology genera-
tion problem is more challenging in that it requires
handling of complex agreement phenomena along
multiple morphological dimensions.
4 Inflection Prediction Framework
In this section, we define the task of of morphologi-
cal generation as inflection prediction, as well as the
lexical operations relevant for the task.
4.1 Morphology Analysis and Generation
Morphological analysis can be performed by ap-
plying language specific rules. These may include
a full-scale morphological analysis with contextual
disambiguation, or, when such resources are not
available, simple heuristic rules, such as regarding
the last few characters of a word as its morphogical
suffix. In this work, we assume that lexicons LS and
LT are available for the source and translation lan-
guages, respectively. Such lexicons can be created
manually, or automatically from data. Given a lexi-
con L and a surface word w, we define the following
operations:
? Stemming - let Sw = {s1, ..., sl} be the set of
possible morphological stems (lemmas) of w
according to L.2
? Inflection - let Iw = {i1, ..., im} be the set of
surface form words that have the same stem as
w. That is, i ? Iw iff Si?Sw 6= ?.
? Morphological analysis - let Aw = {a1, ..., av}
be the set of possible morphological analyses
for w. A morphological analysis a is a vector of
categorical values, where the dimensions and
possible values for each dimension in the vector
representation space are defined by L.
4.2 The Task
We assume that we are given aligned sentence pairs,
where a sentence pair includes a source and a tar-
2Multiple stems are possible due to ambiguity in morpho-
logical analysis.
130
NN+sg+nom+neut
the
DET
allocation of resources has completed
NN+sg PREP NN+pl AUXV+sg VERB+pastpart
?????????????
NN+sg+gen+pl+masc
????????
VERB+perf+pass+part+neut+sg
?????????
raspredelenie resursov zaversheno
Figure 1: Aligned English-Russian sentence pair
with syntactic and morphological annotation
get sentence, and lexicons LS and LT that support
the operations described in the section above. Let
a sentence w1, ...wt, ...wn be the output of a MT
system in the target language. This sentence can
be converted into the corresponding stem set se-
quence S1, ...St, ...Sn, applying the stemming op-
eration. Then the task is, for every stem set St in
the output sentence, to predict an inflection yt from
its inflection set It. The predicted inflections should
both reflect the meaning conveyed by the source sen-
tence, and comply with the agreement rules of the
target language. 3
Figure 1 shows an example of an aligned English-
Russian sentence pair: on the source (English) side,
POS tags and word dependency structure are indi-
cated by solid arcs. The alignments between En-
glish and Russian words are indicated by the dot-
ted lines. The dependency structure on the Russian
side, indicated by solid arcs, is given by a treelet MT
system in our case (see Section 6.1), projected from
the word dependency structure of English and word
alignment information. Note that the Russian sen-
tence displays agreement in number and gender be-
tween the subject noun (raspredelenie) and the pred-
icate (zaversheno); note also that resursov is in gen-
itive case, as it modifies the noun on its left.
5 Models for Inflection Prediction
5.1 A Probabilistic Model
Our learning framework uses a Maximum Entropy
Markov model (McCallum et al, 2000). The model
decomposes the overall probability of a predicted
inflection sequence into a product of local proba-
bilities for individual word predictions. The local
3That is, assuming that the stem sequence that is output by
the MT system is correct.
probabilities are conditioned on the previous k pre-
dictions. The model implemented here is of second
order: at any decision point t we condition the prob-
ability distribution over labels on the previous two
predictions yt?1 and yt?2 in addition to the given
(static) word context from both the source and tar-
get sentences. That is, the probability of a predicted
inflection sequence is defined as follows:
p(y | x) =
n
?
t=1
p(yt | yt?1, yt?2, xt), yt ? It
where xt denotes the given context at position t
and It is the set of inflections corresponding to St,
from which the model should choose yt.
The features we constructed pair up predicates on
the context ( x?, yt?1, yt?2) and the target label (yt).
In the suggested framework, it is straightforward to
encode the morphological properties of a word, in
addition to its surface inflected form. For example,
for a particular inflected word form yt and its con-
text, the derived paired features may include:
?k =
{
1 if surface word yt is y? and s? ? St+1
0 otherwise
?k+1 =
{ 1 if Gender(yt) =?Fem? and Gender(yt?1) =?Fem?
0 otherwise
In the first example, a given neighboring stem set
St+1 is used as a context feature for predicting the
target word yt. The second feature captures the gen-
der agreement with the previous word. This is possi-
ble because our model is of second order. Thus, we
can derive context features describing the morpho-
logical properties of the two previous predictions.4
Note that our model is not a simple multi-class clas-
sifier, because our features are shared across mul-
tiple target labels. For example, the gender fea-
ture above applies to many different inflected forms.
Therefore, it is a structured prediction model, where
the structure is defined by the morphological proper-
ties of the target predictions, in addition to the word
sequence decomposition.
5.2 Feature Categories
The information available for estimating the distri-
bution over yt can be split into several categories,
4Note that while we decompose the prediction task left-to-
right, an appealing alternative is to define a top-down decompo-
sition, traversing the dependency tree of the sentence. However,
this requires syntactic analysis of sufficient quality.
131
corresponding to feature source. The first ma-
jor distinction is monolingual versus bilingual fea-
tures: monolingual features refer only to the context
(and predicted label) in the target language, while
bilingual features have access to information in the
source sentences, obtained by traversing the word
alignment links from target words to a (set of) source
words, as shown in Figure 1.
Both monolingual and bilingual features can be
further split into three classes: lexical, morpholog-
ical and syntactic. Lexical features refer to surface
word forms, as well as their stems. Since our model
is of second order, our monolingual lexical fea-
tures include the features of a standard word trigram
language model. Furthermore, since our model is
discriminative (predicting word forms given their
stems), the monolingual lexical model can use stems
in addition to predicted words for the left and cur-
rent position, as well as stems from the right con-
text. Morphological features are those that refer to
the features given in Table 1. Morphological infor-
mation is used in describing the target label as well
as its context, and is intended to capture morpho-
logical generalizations. Finally, syntactic features
can make use of syntactic analyses of the source
and target sentences. Such analyses may be derived
for the target language, using the pre-stemmed sen-
tence. Without loss of generality, we will use here
a dependency parsing paradigm. Given a syntactic
analysis, one can construct syntactic features; for ex-
ample, the stem of the parent word of yt. Syntactic
features are expected to be useful in capturing agree-
ment phenomena.
5.3 Features
Table 2 gives the full set of suggested features for
Russian and Arabic, detailed by type. For monolin-
gual lexical features, we consider the stems of the
predicted word and its immediately adjacent words,
in addition to traditional word bigram and trigram
features. For monolingual morphological features,
we consider the morphological attributes of the two
previously predicted words and the current predic-
tion; for monolingual syntactic features, we use the
stem of the parent node.
The bilingual features include the set of words
aligned to the focus word at position t, where they
are treated as bag-of-words, i.e., each aligned word
Feature categories Instantiations
Monolingual lexical
Word stem st?1,st?2,st,st+1
Predicted word yt, yt?1, yt?2
Monolingual morphological
f : POS, Person, Number, Gender, Tense f(yt?2),f(yt?1),f(yt)
Neg, Det, Prep, Conj, ObjPron, PossPron
Monolingual syntactic
Parent stem sHEAD(t)
Bilingual lexical
Aligned word set Al Alt, Alt?1, Alt+1
Bilingual morph & syntactic
f : POS, Person, Number, Gender, Tense f(Alt), f(Alt?1),
Neg, Det, Prep, Conj, ObjPron, PossPron, f(Alt+1), f(AlHEAD(t))
Comp
Table 2: The feature set suggested for English-
Russian and English-Arabic pairs
is assigned a separate feature. Bilingual lexical fea-
tures can refer to words aligned to yt as all as words
aligned to its immediate neighbors yt?1 and yt+1.
Bilingual morphological and syntactic features re-
fer to the features of the source language, which
are expected to be useful for predicting morphol-
ogy in the target language. For example, the bilin-
gual Det (determiner) feature is computed accord-
ing to the source dependency tree: if a child of a
word aligned to wt is a determiner, then the fea-
ture value is assigned its surface word form (such
as a or the). The bilingual Prep feature is com-
puted similarly, by checking the parent chain of the
word aligned to wt for the existence of a preposi-
tion. This feature is hoped to be useful for predict-
ing Arabic inflected forms with a prepositional pre-
fix, as well as for predicting case marking in Rus-
sian. The bilingual ObjPron and PossPron features
represent any object pronoun of the word aligned to
wt and a preceding possessive pronoun, respectively.
These features are expected to map to the object and
possessive pronoun features in Arabic. Finally, the
bilingual Compound feature checks whether a word
appears as part of a noun compound in the English
source. f this is the case, the feature is assigned the
value of ?head? or ?dependent?. This feature is rel-
evant for predicting a genitive case in Russian and
definiteness in Arabic.
6 Experimental Settings
In order to evaluate the effectiveness of the sug-
gested approach, we performed reference experi-
ments, that is, using the aligned sentence pairs of
132
Data Eng-Rus Eng-Ara
Avg. sentlen Eng Rus Eng Ara
Training 1M 470K
14.06 12.90 12.85 11.90
Development 1,000 1,000
13.73 12.91 13.48 12.90
Test 1,000 1,000
13.61 12.84 8.49 7.50
Table 3: Data set statistics: corpus size and average
sentence length (in words)
reference translations rather than the output of an
MT system as input.5 This allows us to evaluate
our method with a reduced noise level, as the words
and word order are perfect in reference translations.
These experiments thus constitute a preliminary step
for tackling the real task of inflecting words in MT.
6.1 Data
We used a corpus of approximately 1 million aligned
sentence pairs for English-Russian, and 0.5 million
pairs for English-Arabic. Both corpora are from a
technical (software manual) domain, which we be-
lieve is somewhat restricted along some morpho-
logical dimensions, such as tense and person. We
used 1,000 sentence pairs each for development and
testing for both language pairs. The details of the
datasets used are given in Table 3.
The sentence pairs were word-aligned using
GIZA++ (Och and Ney, 2000) and submitted to a
treelet-based MT system (Quirk et al, 2005), which
uses the word dependency structure of the source
language and projects word dependency structure to
the target language, creating the structure shown in
Figure 1 above.
6.2 Lexicon
Table 4 gives some relevant statistics of the lexicons
we used. For Russian, a general-domain lexicon was
available to us, consisting of about 80,000 lemmas
(stems) and 9.4 inflected forms per stem.6 Limiting
the lexicon to word types that are seen in the train-
ing set reduces its size substantially to about 14,000
stems, and an average of 3.8 inflections per stem.
We will use this latter ?domain-adapted? lexicon in
our experiments.
5In this case, yt should equal wt, according to the task defi-
nition.
6The averages reported in Table 4 are by type and do not
consider word frequencies in the data.
Source Stems Avg(| I |) Avg(| S |)
Rus. Lexicon 79,309 9.4
Lexicon ? Train 13,929 3.8 1.6
Ara. Lexicon ? Train 12,670 7.0 1.7
Table 4: Lexicon statistics
For Arabic, as a full-size Arabic lexicon was not
available to us, we used the Buckwalter morpholog-
ical analyzer (Buckwalter, 2004) to derive a lexicon.
To acquire the stemming and inflection operators, we
submit all words in our training data to the Buckwal-
ter analyzer. Note that Arabic displays a high level
of ambiguity, each word corresponding to many pos-
sible segmentations and morphological analyses; we
considered all of the different stems returned by the
Buckwalter analyzer in creating a word?s stem set.
The lexicon created in this manner contains 12,670
distinct stems and 89,360 inflected forms.
For the generation of word features, we only con-
sider one dominant analysis for any surface word
for simplicity. In case of ambiguity, we considered
only the first (arbitrary) analysis for Russian. For
Arabic, we apply the following heuristic: use the
most frequent analysis estimated from the gold stan-
dard labels in the Arabic Treebank (Maamouri et al,
2005); if a word does not appear in the treebank, we
choose the first analysis returned by the Buckwal-
ter analyzer. Ideally, the best word analysis should
be provided as a result of contextual disambiguation
(e.g., (Habash and Rambow, 2005)); we leave this
for future work.
6.3 Baseline
As a baseline, we pick a morphological inflection yt
at random from It. This random baseline serves as
an indicator for the difficulty of the problem. An-
other more competitive baseline we implemented
is a word trigram language model (LM). The LMs
were trained using the CMU language modelling
toolkit (Clarkson and Rosenfeld, 1997) with default
settings on the training data described in Table 3.
6.4 Experiments
In the experiments, our primary goal is to evaluate
the effectiveness of the proposed model using all
features available to us. Additionally, we are inter-
ested in knowing the contribution of each informa-
tion source, namely of morpho-syntactic and bilin-
gual features. Therefore, we study the performance
133
of models including the full feature schemata as well
as models that are restricted to feature subsets ac-
cording to the feature types as described in Section
5.2. The models are as follows: Monolingual-Word,
including LM-like and stem n-gram features only;
Bilingual-Word, which also includes bilingual lex-
ical features;7 Monolingual-All, which has access
to all the information available in the target lan-
guage, including morphological and syntactic fea-
tures; and finally, Bilingual-All, which includes all
feature types from Table 2.
For each model and language, we perform feature
selection in the following manner. The features are
represented as feature templates, such as ?POS=X?,
which generate a set of binary features correspond-
ing to different instantiations of the template, as in
?POS=NOUN?. In addition to individual features, con-
junctions of up to three features are also considered
for selection (e.g., ?POS=NOUN & Number=plural?).
Every conjunction of feature templates considered
contains at least one predicate on the prediction yt,
and up to two predicates on the context. The feature
selection algorithm performs a greedy forward step-
wise feature selection on the feature templates so as
to maximize development set accuracy. The algo-
rithm is similar to the one described in (Toutanova,
2006). After this process, we performed some man-
ual inspection of the selected templates, and finally
obtained 11 and 36 templates for the Monolingual-
All and Bilingual-All settings for Russian, respec-
tively. These templates generated 7.9 million and
9.3 million binary feature instantiations in the fi-
nal model, respectively. The corresponding num-
bers for Arabic were 27 feature templates (0.7 mil-
lion binary instantiations) and 39 feature templates
(2.3 million binary instantiations) for Monolingual-
All and Bilingual-All, respectively.
7 Results and Discussion
Table 5 shows the accuracy of predicting word forms
for the baseline and proposed models. We report ac-
curacy only on words that appear in our lexicons.
Thus, punctuation, English words occurring in the
target sentence, and words with unknown lemmas
are excluded from the evaluation. The reported ac-
curacy measure therefore abstracts away from the is-
7Overall, this feature set approximates the information that
is available to a state-of-the-art statistical MT system.
Model Eng-Rus Eng-Ara
Random 31.7 16.3
LM 77.6 31.7
Monolingual Word 85.1 69.6
Bilingual Word 87.1 71.9
Monolingual All 87.1 71.6
Bilingual All 91.5 73.3
Table 5: Accuracy (%) results by model
sue of incomplete coverage of the lexicon. When
we encounter these words in the true MT scenario,
we will make no predictions about them, and simply
leave them unmodified. In our current experiments,
in Russian, 68.2% of all word tokens were in Cyril-
lic, of which 93.8% were included in our lexicon.
In Arabic, 85.5% of all word tokens were in Arabic
characters, of which 99.1% were in our lexicon.8
The results in Table 5 show that the suggested
models outperform the language model substantially
for both languages. In particular, the contribution of
both bilingual and non-lexical features is notewor-
thy: adding non-lexical features consistently leads
to 1.5% to 2% absolute gain in both monolingual
and bilingual settings in both language pairs. We
obtain a particularly large gain in the Russian bilin-
gual case, in which the absolute gain is more than
4%, translating to 34% error rate reduction. Adding
bilingual features has a similar effect of gaining
about 2% (and 4% for Russian non-lexical) in ac-
curacy over monolingual models. The overall accu-
racy is lower in Arabic than in Russian, reflecting
the inherent difficulty of the task, as indicated by the
random baseline (31.7 in Russian vs. 16.3 in Ara-
bic).
In order to evaluate the effectiveness of the model
in alleviating the data sparsity problem in morpho-
logical generation, we trained inflection prediction
models on various subsets of the training data de-
scribed in Table 3, and tested their accuracy. The
results are given in Figure 2. We can see that with as
few as 5,000 training sentences pairs, the model ob-
tains much better accuracy than the language model,
which is trained on data that is larger by a few orders
of magnitude. We also note that the learning curve
8For Arabic, the inflection ambiguity was extremely high:
there were on average 39 inflected forms per stem set in our
development corpus (per token), as opposed to 7 in Russian.
We therefore limited the evaluation of Arabic to those stems that
have up to 30 inflected forms, resulting in 17 inflected forms per
stem set on average in the development data.
134
50
55
60
65
70
75
80
85
90
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 10
0
Training data size (x1,000)
Ac
cu
ra
cy
 
(%
)
RUS-bi-word
RUS-bi-all
ARA-bi-word
ARA-bi-all
Figure 2: Accuracy, varying training data size
becomes less steep as we use more training data,
suggesting that the models are successfully learning
generalizations.
We have also manually examined some repre-
sentative cases where the proposed model failed to
make a correct prediction. In both Russian and Ara-
bic, a very common pattern was a mistake in pre-
dicting the gender (as well as number and person in
Arabic) of pronouns. This may be attributed to the
fact that the correct choice of the pronoun requires
coreference resolution, which is not available in our
model. A more thorough analysis of the results will
be helpful to bring further improvements.
8 Conclusions and Future Work
We presented a probabilistic framework for mor-
phological generation given aligned sentence pairs,
incorporating morpho-syntactic information from
both the source and target sentences. The re-
sults, using reference translations, show that the pro-
posed models achieve substantially better accuracy
than language models, even with a relatively small
amount of training data. Our models using morpho-
syntactic information also outperformed models us-
ing only lexical information by a wide margin. This
result is very promising for achieving our ultimate
goal of improving MT output by using a special-
ized model for target language morphological gener-
ation. Though this goal is clearly outside the scope
of this paper, we conducted a preliminary experi-
ment where an English-to-Russian MT system was
trained on a stemmed version of the aligned data and
used to generate stemmed word sequences, which
were then inflected using the suggested framework.
This simple integration of the proposed model with
the MT system improved the BLEU score by 1.7.
The most obvious next step of our research, there-
fore, is to further pursue the integration of the pro-
posed model to the end-to-end MT scenario.
There are multiple paths for obtaining further im-
provements over the results presented here. These
include refinement in feature design, word analysis
disambiguation, morphological and syntactic anal-
ysis on the source English side (e.g., assigning se-
mantic role tags), to name a few. Another area of
investigation is capturing longer-distance agreement
phenomena, which can be done by implementing a
global statistical model, or by using features from
dependency trees more effectively.
References
Tim Buckwalter. 2004. Buckwalter arabic morphological ana-
lyzer version 2.0.
Philip Clarkson and Roni Rosenfeld. 1997. Statistical language
modelling using the CMU cambridge toolkit. In Eurospeech.
Kevin Duh and Kathrin Kirchhoff. 2004. Automatic learning of
language model structure. In COLING.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Initial ex-
plorations in English to Turkish statistical machine transla-
tion. In NAACL workshop on statistical machine translation.
Sharon Goldwater and David McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In EMNLP.
Nizar Habash and Owen Rambow. 2005. Arabic tokenization,
part-of-speech tagging and morphological disambiguation in
one fell swoop. In ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for
compound splitting. In EACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In MT Summit.
Young-Suk Lee. 2004. Morphological analysis for statistical
machine translation. In HLT-NAACL.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Hubert
Jin. 2005. Arabic Treebank: Part 1 v 3.0. Linguistic Data
Consortium.
Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira.
2000. Maximum entropy markov models for information
extraction and segmentation. In ICML.
Sonja Nie?en and Hermann Ney. 2004. Statistical machine
translation with scarce resources using morpho-syntactic in-
formation. Computational Linguistics, 30(2):181?204.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In ACL.
Maja Popovic? and Hermann Ney. 2004. Towards the use of
word stems and suffixes for statistical machine translation.
In LREC.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Depen-
dency tree translation: Syntactically informed phrasal SMT.
In ACL.
Hisami Suzuki and Kristina Toutanova. 2006. Learning to pre-
dict case markers in Japanese. In COLING-ACL.
Kristina Toutanova. 2006. Competitive generative models with
structure learning for NLP classification tasks. In EMNLP.
135
Proceedings of ACL-08: HLT, pages 514?522,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Applying Morphology Generation Models to Machine Translation
Kristina Toutanova
Microsoft Research
Redmond, WA, USA
kristout@microsoft.com
Hisami Suzuki
Microsoft Research
Redmond, WA, USA
hisamis@microsoft.com
Achim Ruopp
Butler Hill Group
Redmond, WA, USA
v-acruop@microsoft.com
Abstract
We improve the quality of statistical machine
translation (SMT) by applying models that
predict word forms from their stems using
extensive morphological and syntactic infor-
mation from both the source and target lan-
guages. Our inflection generation models are
trained independently of the SMT system. We
investigate different ways of combining the in-
flection prediction component with the SMT
system by training the base MT system on
fully inflected forms or on word stems. We
applied our inflection generation models in
translating English into two morphologically
complex languages, Russian and Arabic, and
show that our model improves the quality of
SMT over both phrasal and syntax-based SMT
systems according to BLEU and human judge-
ments.
1 Introduction
One of the outstanding problems for further improv-
ing machine translation (MT) systems is the diffi-
culty of dividing the MT problem into sub-problems
and tackling each sub-problem in isolation to im-
prove the overall quality of MT. Evidence for this
difficulty is the fact that there has been very little
work investigating the use of such independent sub-
components, though we started to see some success-
ful cases in the literature, for example in word align-
ment (Fraser and Marcu, 2007), target language cap-
italization (Wang et al, 2006) and case marker gen-
eration (Toutanova and Suzuki, 2007).
This paper describes a successful attempt to in-
tegrate a subcomponent for generating word inflec-
tions into a statistical machine translation (SMT)
system. Our research is built on previous work in
the area of using morpho-syntactic information for
improving SMT. Work in this area is motivated by
two advantages offered by morphological analysis:
(1) it provides linguistically motivated clustering of
words and makes the data less sparse; (2) it cap-
tures morphological constraints applicable on the
target side, such as agreement phenomena. This sec-
ond problem is very difficult to address with word-
based translation systems, when the relevant mor-
phological information in the target language is ei-
ther non-existent or implicitly encoded in the source
language. These two aspects of morphological pro-
cessing have often been addressed separately: for
example, morphological pre-processing of the input
data is a common method of addressing the first as-
pect, e.g. (Goldwater and McClosky, 2005), while
the application of a target language model has al-
most solely been responsible for addressing the sec-
ond aspect. Minkov et al (2007) introduced a way
to address these problems by using a rich feature-
based model, but did not apply the model to MT.
In this paper, we integrate a model that predicts
target word inflection in the translations of English
into two morphologically complex languages (Rus-
sian and Arabic) and show improvements in the MT
output. We study several alternative methods for in-
tegration and show that it is best to propagate un-
certainty among the different components as shown
by other research, e.g. (Finkel et al, 2006), and in
some cases, to factor the translation problem so that
the baseline MT system can take advantage of the
reduction in sparsity by being able to work on word
stems. We also demonstrate that our independently
trained models are portable, showing that they can
improve both syntactic and phrasal SMT systems.
514
2 Related work
There has been active research on incorporating
morphological knowledge in SMT. Several ap-
proaches use pre-processing schemes, including
segmentation of clitics (Lee, 2004; Habash and Sa-
dat, 2006), compound splitting (Nie?en and Ney,
2004) and stemming (Goldwater and McClosky,
2005). Of these, the segmentation approach is dif-
ficult to apply when the target language is morpho-
logically rich as the segmented morphemes must be
put together in the output (El-Kahlout and Oflazer,
2006); and in fact, most work using pre-processing
focused on translation into English. In recent
work, Koehn and Hoang (2007) proposed a general
framework for including morphological features in
a phrase-based SMT system by factoring the repre-
sentation of words into a vector of morphological
features and allowing a phrase-based MT system to
work on any of the factored representations, which
is implemented in the Moses system. Though our
motivation is similar to that of Koehn and Hoang
(2007), we chose to build an independent compo-
nent for inflection prediction in isolation rather than
folding morphological information into the main
translation model. While this may lead to search er-
rors due to the fact that the models are not integrated
as tightly as possible, it offers some important ad-
vantages, due to the very decoupling of the compo-
nents. First, our approach is not affected by restric-
tions on the allowable context size or a phrasal seg-
mentation that are imposed by current MT decoders.
This also makes the model portable and applicable
to different types of MT systems. Second, we avoid
the problem of the combinatorial expansion in the
search space which currently arises in the factored
approach of Moses.
Our inflection prediction model is based on
(Minkov et al, 2007), who build models to predict
the inflected forms of words in Russian and Arabic,
but do not apply their work to MT. In contrast, we
focus on methods of integration of an inflection pre-
diction model with an MT system, and on evaluation
of the model?s impact on translation. Other work
closely related to ours is (Toutanova and Suzuki,
2007), which uses an independently trained case
marker prediction model in an English-Japanese
translation system, but it focuses on the problem of
generating a small set of closed class words rather
than generating inflected forms for each word in
translation, and proposes different methods of inte-
gration of the components.
3 Inflection prediction models
This section describes the task and our model for in-
flection prediction, following (Minkov et al, 2007).
We define the task of inflection prediction as the
task of choosing the correct inflections of given tar-
get language stems, given a corresponding source
sentence. The stemming and inflection operations
we use are defined by lexicons.
3.1 Lexicon operations
For each target language we use a lexicon L which
determines the following necessary operations:
Stemming: returns the set of possible morpholog-
ical stems Sw = {s1, ..., sl} for the word w accord-
ing to L. 1
Inflection: returns the set of surface word forms
Iw = {i1, ..., im} for the stems Sw according to L.
Morphological analysis: returns the set of possible
morphological analyses Aw = {a1, ..., av} for w. A
morphological analysis a is a vector of categorical
values, where each dimension and its possible values
are defined by L.
For the morphological analysis operation, we
used the same set of morphological features de-
scribed in (Minkov et al, 2007), that is, seven fea-
tures for Russian (POS, Person, Number, Gender,
Tense, Mood and Case) and 12 for Arabic (POS,
Person, Number, Gender, Tense, Mood, Negation,
Determiner, Conjunction, Preposition, Object and
Possessive pronouns). Each word is factored into
a stem (uninflected form) and a subset of these fea-
tures, where features can have either binary (as in
Determiner in Arabic) or multiple values. Some fea-
tures are relevant only for a particular (set of) part-
of-speech (POS) (e.g., Gender is relevant only in
nouns, pronouns, verbs, and adjectives in Russian),
while others combine with practically all categories
(e.g., Conjunction in Arabic). The number of possi-
ble inflected forms per stem is therefore quite large:
as we see in Table 1 of Section 3, there are on av-
erage 14 word forms per stem in Russian and 24 in
1Alternatively, stemming can return a disambiguated stem
analysis; in which case the set Sw consists of one item. The
same is true with the operation of morphological analysis.
515
Arabic for our dataset. This makes the generation of
correct forms a challenging problem in MT.
The Russian lexicon was obtained by intersecting
a general domain lexicon with our training data (Ta-
ble 2), and the Arabic lexicon was obtained by run-
ning the Buckwalter morphological analyser (Buck-
walter, 2004) on the training data. Contextual dis-
ambiguation of morphology was not performed in
either of these languages. In addition to the forms
supposed by our lexicon, we also treated capitaliza-
tion as an inflectional feature in Russian, and defined
all true-case word variants as possible inflections of
its stem(s). Arabic does not use capitalization.
3.2 Task
More formally, our task is as follows: given a source
sentence e, a sequence of stems in the target lan-
guage S1, . . . St, . . . Sn forming a translation of e,
and additional morpho-syntactic annotations A de-
rived from the input, select an inflection yt from its
inflection set It for every stem set St in the target
sentence.
3.3 Models
We built a Maximum Entropy Markov model for in-
flection prediction following (Minkov et al, 2007).
The model decomposes the probability of an inflec-
tion sequence into a product of local probabilities for
the prediction for each word. The local probabilities
are conditioned on the previous k predictions (k is
set to four in Russian and two in Arabic in our ex-
periments). The probability of a predicted inflection
sequence, therefore, is given by:
p(y | x) =
n?
t=1
p(yt | yt?1...yt?k, xt), yt ? It,
where It is the set of inflections corresponding to St,
and xt refers to the context at position t. The con-
text available to the task includes extensive morpho-
logical and syntactic information obtained from the
aligned source and target sentences. Figure 1 shows
an example of an aligned English-Russian sentence
pair: on the source (English) side, POS tags and
word dependency structure are indicated by solid
arcs. The alignments between English and Russian
words are indicated by the dotted lines. The de-
pendency structure on the Russian side, indicated by
solid arcs, is given by a treelet MT system (see Sec-
tion 4.1), projected from the word dependency struc-
NN+sg+nom+neut
the
DET
allocation of resources has completed
NN+sg PREP NN+pl AUXV+sg VERB+pastpart
?????????????
NN+pl+gen+masc
????????
VERB+perf+pass+neut+sg
?????????
raspredelenie resursov zaversheno
Figure 1: Aligned English-Russian sentence pair with
syntactic and morphological annotation.
ture of English and word alignment information.
The features for our inflection prediction model
are binary and pair up predicates on the context
(x?, yt?1...yt?k) and the target label (yt). The fea-
tures at a certain position t can refer to any word
in the source sentence, any word stem in the tar-
get language, or any morpho-syntactic information
in A. This is the source of the power of a model
used as an independent component ? because it does
not need to be integrated in the main search of an
MT decoder, it is not subject to the decoder?s local-
ity constraints, and can thus make use of more global
information.
3.4 Performance on reference translations
Table 1 summarizes the results of applying the in-
flection prediction model on reference translations,
simulating the ideal case where the translations in-
put to our model contain correct stems in correct
order. We stemmed the reference translations, pre-
dicted the inflection for each stem, and measured the
accuracy of prediction, using a set of sentences that
were not part of the training data (1K sentences were
used for Arabic and 5K for Russian).2 Our model
performs significantly better than both the random
and trigram language model baselines, and achieves
an accuracy of over 91%, which suggests that the
model is effective when its input is clean in its stem
choice and order. Next, we apply our model in the
more noisy but realistic scenario of predicting inflec-
tions of MT output sentences.
2The accuracy is based on the words in our lexicon. We
define the stem of an out-of-vocabulary (OOV) word to be it-
self, so in the MT scenario described below, we will not predict
the word forms for an OOV item, and will simply leave it un-
changed.
516
Russian Arabic
Random 16.4 8.7
LM 81.0 69.4
Model 91.6 91.0
Avg | I | 13.9 24.1
Table 1: Results on reference translations (accuracy, %).
4 Machine translation systems and data
We integrated the inflection prediction model with
two types of machine translation systems: systems
that make use of syntax and surface phrase-based
systems.
4.1 Treelet translation system
This is a syntactically-informed MT system, de-
signed following (Quirk et al, 2005). In this ap-
proach, translation is guided by treelet translation
pairs, where a treelet is a connected subgraph of a
syntactic dependency tree. Translations are scored
according to a linear combination of feature func-
tions. The features are similar to the ones used in
phrasal systems, and their weights are trained us-
ing max-BLEU training (Och, 2003). There are
nine feature functions in the treelet system, includ-
ing log-probabilities according to inverted and direct
channel models estimated by relative frequency, lex-
ical weighting channel models following Vogel et
al. (2003), a trigram target language model, two or-
der models, word count, phrase count, and average
phrase size functions.
The treelet translation model is estimated using
a parallel corpus. First, the corpus is word-aligned
using an implementation of lexicalized-HMMs (He,
2007); then the source sentences are parsed into a
dependency structure, and the dependency is pro-
jected onto the target side following the heuristics
described in (Quirk et al, 2005). These aligned sen-
tence pairs form the training data of the inflection
models as well. An example was given in Figure 1.
4.2 Phrasal translation system
This is a re-implementation of the Pharaoh trans-
lation system (Koehn, 2004). It uses the same
lexicalized-HMM model for word alignment as the
treelet system, and uses the standard extraction
heuristics to extract phrase pairs using forward and
backward alignments. In decoding, the system uses
a linear combination of feature functions whose
weights are trained using max-BLEU training. The
features include log-probabilities according to in-
verted and direct channel models estimated by rel-
ative frequency, lexical weighting channel models,
a trigram target language model, distortion, word
count and phrase count.
4.3 Data sets
For our English-Russian and English-Arabic experi-
ments, we used data from a technical (computer) do-
main. For each language pair, we used a set of paral-
lel sentences (train) for training the MT system sub-
models (e.g., phrase tables, language model), a set
of parallel sentences (lambda) for training the com-
bination weights with max-BLEU training, a set of
parallel sentences (dev) for training a small number
of combination parameters for our integration meth-
ods (see Section 5), and a set of parallel sentences
(test) for final evaluation. The details of these sets
are shown in Table 2. The training data for the in-
flection models is always a subset of the training set
(train). All MT systems for a given language pair
used the same datasets.
Dataset sent pairs word tokens (avg/sent)
English-Russian
English Russian
train 1,642K 24,351K (14.8) 22,002K (13.4)
lambda 2K 30K (15.1) 27K (13.7)
dev 1K 14K (13.9) 13K (13.5)
test 4K 61K (15.3) 60K(14.9)
English-Arabic
English Arabic
train 463K 5,223K (11.3) 4,761K (10.3)
lambda 2K 22K (11.1) 20K (10.0)
dev 1K 11K (11.1) 10K (10.0)
test 4K 44K (11.0) 40K (10.1)
Table 2: Data set sizes, rounded up to the nearest 1000.
5 Integration of inflection models with MT
systems
We describe three main methods of integration we
have considered. The methods differ in the extent to
which the factoring of the problem into two subprob-
lems ? predicting stems and predicting inflections
? is reflected in the base MT systems. In the first
method, the MT system is trained to produce fully
inflected target words and the inflection model can
change the inflections. In the other two methods, the
517
MT system is trained to produce sequences of tar-
get language stems S, which are then inflected by
the inflection component. Before we motivate these
methods, we first describe the general framework for
integrating our inflection model into the MT system.
For each of these methods, we assume that the
output of the base MT system can be viewed as a
ranked list of translation hypotheses for each source
sentence e. More specifically, we assume an out-
put {S1,S2,. . . ,Sm} of m-best translations which
are sequences of target language stems. The transla-
tions further have scores {w1,w2,. . . ,wm} assigned
by the base MT system. We also assume that each
translation hypothesis Si together with source sen-
tence e can be annotated with the annotation A, as
illustrated in Figure 1. We discuss how we convert
the output of the base MT systems to this form in the
subsections below.
Given such a list of candidate stem sequences, the
base MT model together with the inflection model
and a language model choose a translation Y? as
follows:
(1) Yi = argmaxY ?i ?Infl(Si)?1logPIM (Y ?i |Si)+
?2logPLM (Y ?i ), i = 1 . . . n
(2) Y ? = argmaxi=1...n ?1logPIM (Yi|Si) +
?2logPLM (Yi) + ?3wi
In these formulas, the dependency on e and A
is omitted for brevity in the expression for the
probability according to the inflection model PIM .
PLM (Y ?i ) is the joint probability of the sequence
of inflected words according to a trigram language
model (LM). The LM used for the integration is the
same LM used in the base MT system that is trained
on fully inflected word forms (the base MT system
trained on stems uses an LM trained on a stem se-
quence). Equation (1) shows that the model first se-
lects the best sequence of inflected forms for each
MT hypothesis Si according to the LM and the in-
flection model. Equation (2) shows that from these
n fully inflected hypotheses, the model then selects
the one which has the best score, combined with
the base MT score wi for Si. We should note that
this method does not represent standard n-best re-
ranking because the input from the base MT system
contains sequences of stems, and the model is gen-
erating fully inflected translations from them. Thus
the chosen translation may not be in the provided n-
best list. This method is more similar to the one used
in (Wang et al, 2006), with the difference that they
use only 1-best input from a base MT system.
The interpolation weights ? in Equations (1) and
(2) as well as the optimal number of translations n
from the base MT system to consider, given a maxi-
mum of m=100 hypotheses, are trained using a sep-
arate dataset. We performed a grid search on the
values of ? and n, to maximize the BLEU score of
the final system on a development set (dev) of 1000
sentences (Table 2).
The three methods of integration differ in the way
the base MT engine is applied. Since we always dis-
card the choices of specific inflected forms for the
target stems by converting candidate translations to
sequences of stems, it is interesting to know whether
we need a base MT system that produces fully in-
flected translations or whether we can do as well
or better by training the base MT systems to pro-
duce sequences of stems. Stemming the target sen-
tences is expected to be helpful for word alignment,
especially when the stemming operation is defined
so that the word alignment becomes more one-to-
one (Goldwater and McClosky, 2005). In addition,
stemming the target sentences reduces the sparsity
in the translation tables and language model, and is
likely to impact positively the performance of an MT
system in terms of its ability to recover correct se-
quences of stems in the target. Also, machine learn-
ing tells us that solving a more complex problem
than we are evaluated on (in our case for the base
MT, predicting stems together with their inflections
instead of just predicting stems) is theoretically un-
justified (Vapnik, 1995).
However, for some language pairs, stemming one
language can make word alignment worse, if it
leads to more violations in the assumptions of cur-
rent word alignment models, rather than making the
source look more like the target. In addition, using a
trigram LM on stems may lead to larger violations of
the Markov independence assumptions, than using a
trigram LM on fully inflected words. Thus, if we ap-
ply the exact same base MT system to use stemmed
forms in alignment and/or translation, it is not a pri-
ori clear whether we would get a better result than if
we apply the system to use fully inflected forms.
518
5.1 Method 1
In this method, the base MT system is trained in
the usual way, from aligned pairs of source sen-
tences and fully inflected target sentences. The in-
flection model is then applied to re-inflect the 1-best
or m-best translations and to select an output trans-
lation. The hypotheses in the m-best output from the
base MT system are stemmed and the scores of the
stemmed hypotheses are assumed to be equal to the
scores of the original ones.3 Thus we obtain input of
the needed form, consisting of m sequences of target
language stems along with scores.
For this and other methods, if we are working
with an m-best list from the treelet system, every
translation hypothesis contains the annotations A
that our model needs, because the system maintains
the alignment, parse trees, etc., as part of its search
space. Thus we do not need to do anything further
to obtain input of the form necessary for application
of the inflection model.
For the phrase-based system, we generated the
annotations needed by first parsing the source sen-
tence e, aligning the source and candidate transla-
tions with the word-alignment model used in train-
ing, and projected the dependency tree to the target
using the algorithm of (Quirk et al, 2005). Note that
it may be better to use the word alignment main-
tained as part of the translation hypotheses during
search, but our solution is more suitable to situations
where these can not be easily obtained.
For all methods, we study two settings for integra-
tion. In the first, we only consider (n=1) hypotheses
from the base MT system. In the second setting, we
allow the model to use up to 100 translations, and
to automatically select the best number to use. As
seen in Table 3, (n=16) translations were chosen for
Russian and as seen in Table 5, (n=2) were chosen
for Arabic for this method.
5.2 Method 2
In this method, the base MT system is trained to pro-
duce sequences of stems in the target language. The
most straightforward way to achieve this is to stem
the training parallel data and to train the MT sys-
tem using this input. This is our Method 3 described
3It may be better to take the max of the scores for a stem
sequence occurring more than once in the list, or take the log-
sum-exp of the scores.
below. We formulated Method 2 as an intermedi-
ate step, to decouple the impact of stemming at the
alignment and translation stages.
In Method 2, word alignment is performed us-
ing fully inflected target language sentences. After
alignment, the target language is stemmed and the
base MT systems? sub-models are trained using this
stemmed input and alignment. In addition to this
word-aligned corpus the MT systems use another
product of word alignment: the IBM model 1 trans-
lation tables. Because the trained translation tables
of IBM model 1 use fully inflected target words, we
generated stemmed versions of the translation tables
by applying the rules of probability.
5.3 Method 3
In this method the base MT system produces se-
quences of target stems. It is trained in the same way
as the baseline MT system, except its input parallel
training data are preprocessed to stem the target sen-
tences. In this method, stemming can impact word
alignment in addition to the translation models.
6 MT performance results
Before delving into the results for each method, we
discuss our evaluation measures. For automatically
measuring performance, we used 4-gram BLEU
against a single reference translation. We also report
oracle BLEU scores which incorporate two kinds of
oracle knowledge. For the methods using n=1 trans-
lation from a base MT system, the oracle BLEU
score is the BLEU score of the stemmed translation
compared to the stemmed reference, which repre-
sents the upper bound achievable by changing only
the inflected forms (but not stems) of the words in a
translation. For models using n > 1 input hypothe-
ses, the oracle also measures the gain from choos-
ing the best possible stem sequence in the provided
(m=100-best) hypothesis list, in addition to choos-
ing the best possible inflected forms for these stems.
For the models in the tables, even if, say, n=16 was
chosen in parameter fitting, the oracle is measured
on the initially provided list of 100-best.
6.1 English-Russian treelet system
Table 3 shows the results of the baseline and the
model using the different methods for the treelet
MT system on English-Russian. The baseline is the
519
Model BLEU Oracle BLEU
Base MT (n=1) 29.24 -
Method 1 (n=1) 30.44 36.59
Method 1 (n=16) 30.61 45.33
Method 2 (n=1) 30.79 37.38
Method 2 (n=16) 31.24 48.48
Method 3 (n=1) 31.42 38.06
Method 3 (n=32) 31.80 49.19
Table 3: Test set performance for English-to-Russian MT
(BLEU) results by model using a treelet MT system.
treelet system described in Section 4.1 and trained
on the data in Table 2.
We can see that Method 1 results in a good im-
provement of 1.2 BLEU points, even when using
only the best (n = 1) translation from the baseline.
The oracle improvement achievable by predicting
inflections is quite substantial: more than 7 BLEU
points. Propagating the uncertainty of the baseline
system by using more input hypotheses consistently
improves performance across the different methods,
with an additional improvement of between .2 and
.4 BLEU points.
From the results of Method 2 we can see that re-
ducing sparsity at translation modeling is advanta-
geous. Both the oracle BLEU of the first hypothe-
sis and the achieved performance of the model im-
proved; the best performance achieved by Method 2
is .63 points higher than the performance of Method
1. We should note that the oracle performance for
Method 2, n > 1 is measured using 100-best lists of
target stem sequences, whereas the one for Method
1 is measured using 100-best lists of inflected target
words. This can be a disadvantage for Method 1,
because a 100-best list of inflected translations actu-
ally contains about 50 different sequences of stems
(the rest are distinctions in inflections). Neverthe-
less, even if we measure the oracle for Method 2
using 40-best, it is higher than the 100-best oracle
of Method 1. In addition, it appears that using a hy-
pothesis list larger than n > 1=100 is not be helpful
for our method, as the model chose to use only up to
32 hypotheses.
Finally, we can see that using stemming at the
word alignment stage further improved both the or-
acle and the achieved results. The performance of
the best model is 2.56 BLEU points better than the
baseline. Since stemming in Russian for the most
part removes properties of words which are not ex-
pressed in English at the word level, these results
are consistent with previous results using stemming
to improve word alignment. From these results, we
also see that about half of the gain from using stem-
ming in the base MT system came from improving
word alignment, and half came from using transla-
tion models operating at the less sparse stem level.
Overall, the improvement achieved by predicting
morphological properties of Russian words with a
feature-rich component model is substantial, given
the relatively large size of the training data (1.6 mil-
lion sentences), and indicates that these kinds of
methods are effective in addressing the problems
in translating morphology-poor to morphology-rich
languages.
6.2 English-Russian phrasal system
For the phrasal system, we performed integration
only with Method 1, using the top 1 or 100-
best translations. This is the most straightforward
method for combining with any system, and we ap-
plied it as a proof-of-concept experiment.
Model BLEU Oracle BLEU
Base MT (n=1) 36.00 -
Method 1 (n=1) 36.43 42.33
Method 1 (n=100) 36.72 55.00
Table 4: Test set performance for English-to-Russian MT
(BLEU) results by model using a phrasal MT system.
The phrasal MT system is trained on the same
data as the treelet system. The phrase size and dis-
tortion limit were optimized (we used phrase size of
7 and distortion limit of 3). This system achieves a
substantially better BLEU score (by 6.76) than the
treelet system. The oracle BLEU score achievable
by Method 1 using n=1 translation, though, is still
6.3 BLEU point higher than the achieved BLEU.
Our model achieved smaller improvements for the
phrasal system (0.43 improvement for n=1 transla-
tions and 0.72 for the selected n=100 translations).
However, this improvement is encouraging given the
large size of the training data. One direction for
potentially improving these results is to use word
alignments from the MT system, rather than using
an alignment model to predict them.
520
Model BLEU Oracle BLEU
Base MT (n=1) 35.54 -
Method 1 (n=1) 37.24 42.29
Method 1 (n=2) 37.41 52.21
Method 2 (n=1) 36.53 42.46
Method 2 (n=4) 36.72 54.74
Method 3 (n=1) 36.87 42.96
Method 3 (n=2) 36.92 54.90
Table 5: Test set performance for English-to-Arabic MT
(BLEU) results by model using a treelet MT system.
6.3 English-Arabic treelet system
The Arabic system also improves with the use of our
mode: the best system (Method 1, n=2) achieves
the BLEU score of 37.41, a 1.87 point improve-
ment over the baseline. Unlike the case of Rus-
sian, Method 2 and 3 do not achieve better results
than Method 1, though the oracle BLEU score im-
proves in these models (54.74 and 54.90 as opposed
to 52.21 of Method 1). We do notice, however, that
the oracle improvement for the 1-best analysis is
much smaller than what we obtained in Russian.
We have been unable to closely diagnose why per-
formance did not improve using Methods 2 and 3
so far due to the absence of expertise in Arabic, but
one factor we suspect is affecting performance the
most in Arabic is the definition of stemming: the
effect of stemming is most beneficial when it is ap-
plied specifically to normalize the distinctions not
explicitly encoded in the other language; it may hurt
performance otherwise. We believe that in the case
of Arabic, this latter situation is actually happen-
ing: grammatical properties explicitly encoded in
English (e.g., definiteness, conjunction, pronominal
clitics) are lost when the Arabic words are stemmed.
This may be having a detrimental effect on the MT
systems that are based on stemmed input. Further
investigation is necessary to confirm this hypothesis.
6.4 Human evaluation
In this section we briefly report the results of human
evaluation on the output of our inflection prediction
system, as the correlation between BLEU scores and
human evaluation results is not always obvious. We
compared the output of our component against the
best output of the treelet system without our com-
ponent. We evaluated the following three scenarios:
(1) Arabic Method 1 with n=1, which corresponds
to the best performing system in BLEU according to
Table 5; (2) Russian, Method 1 with n=1; (3) Rus-
sian, Method 3 with n=32, which corresponds to the
best performing system in BLEU in Table 3. Note
that in (1) and (2), the only differences in the com-
pared outputs are the changes in word inflections,
while in (3) the outputs may differ in the selection
of the stems.
In all scenarios, two human judges (native speak-
ers of these languages) evaluated 100 sentences that
had different translations by the baseline system and
our model. The judges were given the reference
translations but not the source sentences, and were
asked to classify each sentence pair into three cate-
gories: (1) the baseline system is better (score=-1),
(2) the output of our model is better (score=1), or (3)
they are of the same quality (score=0).
human eval score BLEU diff
Arabic Method 1 0.1 1.9
Russian Method 1 0.255 1.2
Russian Method 3 0.26 2.6
Table 6: Human evaluation results
Table 6 shows the results of the averaged, aggre-
gated score across two judges per evaluation sce-
nario, along with the BLEU score improvements
achieved by applying our model. We see that in all
cases, the human evaluation scores are positive, indi-
cating that our models produce translations that are
better than those produced by the baseline system. 4
We also note that in Russian, the human evaluation
scores are similar for Method 1 and 3 (0.255 and
0.26), though the BLEU score gains are quite differ-
ent (1.2 vs 2.6). This may be attributed to the fact
that human evaluation typically favors the scenario
where only word inflections are different (Toutanova
and Suzuki, 2007).
7 Conclusion and future work
We have shown that an independent model of mor-
phology generation can be successfully integrated
with an SMT system, making improvements in both
phrasal and syntax-based MT. In the future, we
would like to include more sophistication in the de-
sign of a lexicon for a particular language pair based
on error analysis, and extend our pre-processing to
include other operations such as word segmentation.
4However, the improvement in Arabic is not statistically sig-
nificant on this 100 sentence set.
521
References
Tim Buckwalter. 2004. Buckwalter arabic morphological
analyzer version 2.0.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Ini-
tial explorations in English to Turkish statistical ma-
chine translation. In NAACL workshop on statistical
machine translation.
Jenny Finkel, Christopher Manning, and Andrew Ng.
2006. Solving the problem of cascading errors: ap-
proximate Bayesian inference for linguistic annotation
pipelines. In EMNLP.
Alexander Fraser and Daniel Marcu. 2007. Measuring
word alignment quality for statistical machine transla-
tion. Computational Linguistics, 33(3):293?303.
Sharon Goldwater and David McClosky. 2005. Improv-
ing statistical MT through morphological analysis. In
EMNLP.
Nizar Habash and Fatiha Sadat. 2006. Arabic prepro-
cessing schemes for statistical machine translation. In
HLT-NAACL.
Xiaodong He. 2007. Using word-dependent transition
models in HMM based word alignment for statistical
machine translation. In ACL Workshop on Statistical
Machine Translation.
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In EMNLP-CoNNL.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
AMTA.
Young-Suk Lee. 2004. Morphological analysis for statis-
tical machine translation. In HLT-NAACL.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007. Generating complex morphology for machine
translation. In ACL.
Sonja Nie?en and Hermann Ney. 2004. Statistical ma-
chine translation with scarce resources using morpho-
syntactic information. Computational Linguistics,
30(2):181?204.
Franz Och. 2003. Minimum error rate training for statis-
tical machine translation. In ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency tree translation: Syntactically informed
phrasal SMT. In ACL.
Kristina Toutanova and Hisami Suzuki. 2007. Generating
case markers in machine translation. In NAACL-HLT.
Vladimir Vapnik. 1995. The nature of Statistical Learn-
ing Theory. Springer-Verlag.
Wei Wang, Kevin Knight, and Daniel Marcu. 2006. Cap-
italizing machine translation. In HLT-NAACL.
522
Exploiting Headword Dependency and Predictive Clustering for 
Language Modeling  
 
Jianfeng Gao  
Microsoft Research, Asia  
Beijing, 100080, China  
jfgao@microsoft.com 
Hisami Suzuki  
Microsoft Research 
 Redmond WA 98052, USA  
hisamis@microsoft.com 
Yang Wen* 
Department of Computer & 
Information Sciences of 
Tsinghua University, China 
  
                                                     
* This work was done while the author was visiting Microsoft Research Asia.  
Abstract 
This paper presents several practical ways 
of incorporating linguistic structure into 
language models. A headword detector is 
first applied to detect the headword of each 
phrase in a sentence. A permuted headword 
trigram model (PHTM) is then generated 
from the annotated corpus. Finally, PHTM 
is extended to a cluster PHTM (C-PHTM) 
by defining clusters for similar words in the 
corpus. We evaluated the proposed models 
on the realistic application of Japanese 
Kana-Kanji conversion. Experiments show 
that C-PHTM achieves 15% error rate 
reduction over the word trigram model. This 
demonstrates that the use of simple methods 
such as the headword trigram and predictive 
clustering can effectively capture long 
distance word dependency, and 
substantially outperform a word trigram 
model. 
1 Introduction 
In spite of its deficiencies, trigram-based language 
modeling still dominates the statistical language 
modeling community, and is widely applied to tasks 
such as speech recognition and Asian language text 
input (Jelinek, 1990; Gao et al, 2002).  
Word trigram models are deficient because they 
can only capture local dependency relations, taking 
no advantage of richer linguistic structure. Many 
proposals have been made that try to incorporate 
linguistic structure into language models (LMs), but 
little improvement has been achieved so far in 
realistic applications because (1) capturing longer 
distance word dependency leads to higher-order 
n-gram models, where the number of parameters is 
usually too large to estimate; (2) capturing deeper 
linguistic relations in a LM requires a large amount 
of annotated training corpus and a decoder that 
assigns linguistic structure, which are not always 
available. 
This paper presents several practical ways of 
incorporating long distance word dependency and 
linguistic structure into LMs. A headword detector 
is first applied to detect the headwords in each 
phrase in a sentence. A permuted headword trigram 
model (PHTM) is then generated from the 
annotated corpus. Finally, PHTM is extended to a 
cluster model (C-PHTM), which clusters similar 
words in the corpus.  
Our models are motivated by three assumptions 
about language: (1) Headwords depend on previous 
headwords, as well as immediately preceding 
words; (2) The order of headwords in a sentence can 
freely change in some cases; and (3) Word clusters 
help us make a more accurate estimate of the 
probability of word strings. We evaluated the 
proposed models on the realistic application of 
Japanese Kana-Kanji conversion, which converts 
phonetic Kana strings into proper Japanese 
orthography. Results show that C-PHTM achieves a 
15% error rate reduction over the word trigram 
model. This demonstrates that the use of simple 
methods can effectively capture long distance word 
dependency, and substantially outperform the word 
trigram model. Although the techniques in this 
paper are described in the context of Japanese 
Kana-Kanji conversion, we believe that they can be 
extended to other languages and applications. 
This paper is organized as follows. Sections 2 
and 3 describe the techniques of using headword 
                                            Association for Computational Linguistics.
                    Language Processing (EMNLP), Philadelphia, July 2002, pp. 248-256.
                         Proceedings of the Conference on Empirical Methods in Natural
dependency and clustering for language modeling. 
Section 4 reviews related work. Section 5 
introduces the evaluation methodology, and Section 
6 presents the results of our main experiments. 
Section 7 concludes our discussion.  
2 Using Headwords 
2.1 Motivation 
Japanese linguists have traditionally distinguished 
two types of words1, content words (jiritsugo) and 
function words (fuzokugo), along with the notion of 
the bunsetsu (phrase). Each bunsetsu typically 
consists of one content word, called a headword in 
this paper, and several function words. Figure 1 
shows a Japanese example sentence and its English 
translation2.  
[??+?][??+??][??+??][??+?][??+?][??+?]
[chiryou+ni]   [sennen+shite]       [zenkai+made] 
[treatment+to][concentration+do][full-recovery+until] 
[juubun+na]      [ryouyou+ni]  [tsutome+ru] 
[enough+ADN] [rest+for]        [try+PRES] 
'(One) concentrates on the treatment and tries to rest 
enough until full recovery' 
Figure 1. A Japanese example sentence with 
bunsetsu and headword tags 
In Figure 1, we find that some headwords in the 
sentence are expected to have a stronger 
dependency relation with their preceding 
headwords than with their immediately preceding 
function words. For example, the three headwords 
??~??~??  (chiryou 'treatment' ~ sennen 
'concentrate' ~ zenkai 'full recovery') form a trigram 
with very strong semantic dependency. Therefore, 
we can hypothesize (in the trigram context) that 
headwords may be conditioned not only by the two 
immediately preceding words, but also by two 
previous headwords. This is our first assumption.  
We also note that the order of headwords in a 
sentence is flexible in some sense. From the 
                                                     
1 Or more correctly, morphemes. Strictly speaking, the 
LMs discussed in this paper are morpheme-based models 
rather than word-based, but we will not make this 
distinction in this paper.  
2 Square brackets demarcate the bunsetsu boundary, and 
+ the morpheme boundary; the underlined words are the 
headwords. ADN indicates an adnominal marker, and 
PRES indicates a present tense marker.  
example in Figure 1, we find that if ??~??~?? 
(chiryou 'treatment' ~ sennen 'concentrate' ~ zenkai 
'full recovery') is a meaningful trigram, then its 
permutations (such as ??~??~?? (zenkai 'full 
recovery' ~ chiryou 'treatment' ~ sennen 
'concentrate')) should also be meaningful, because 
headword trigrams tend to capture an order-neutral 
semantic dependency. This reflects a characteristic 
of Japanese, in which arguments and modifiers of a 
predicate can freely change their word order, a 
phenomenon known as "scrambling" in linguistic 
literature. We can then introduce our second 
assumption: headwords in a trigram are permutable. 
Note that the permutation of headwords should be 
useful more generally beyond Japanese: for 
example, in English, the book Mary bought and 
Mary bought a book can be captured by the same 
headword trigram (Mary ~ bought ~ book) if we 
allow such permutations. 
In this subsection, we have stated two 
assumptions about the structure of Japanese that can 
be exploited for language modeling. We now turn to 
discuss how to incorporate these assumptions in 
language modeling.  
2.2 Permuted headword trigram model 
(PHTM) 
A trigram model predicts the next word wi by 
estimating the conditional probability P(wi|wi-2wi-1), 
assuming that the next word depends only on two 
preceding words, wi-2 and wi-1. The PHTM is a 
simple extension of the trigram model that 
incorporates the dependencies between headwords. 
If we assume that each word token can uniquely be 
classified as a headword or a function word, the 
PHTM can be considered as a cluster-based 
language model with two clusters, headword H and 
function word F. We can then define the conditional 
probability of wi based on its history as the product 
of the two factors: the probability of the category (H 
or F), and the probability of wi given its category. 
Let hi or fi be the actual headword or function word 
in a sentence, and let Hi or Fi be the category of the 
word wi. The PHTM can then be formulated as: 
=? ? ))...(|( 11 ii wwwP   (1) 
))...(|())...(|( 1111 iiiii HwwwPwwHP ?? ???   
))...(|())...(|( 1111 iiiii FwwwPwwFP ?? ???+   
where ? is a function that maps the word history 
(w1?wi-1) onto equivalence classes. 
P(Hi|?(w1?wi-1)) and P(Fi|?(w1?wi-1)) are 
category probabilities, and P(wi|?(w1?wi-1)Fi) is 
the word probability given that the category of wi is 
function word. For these three probabilities, we 
used the standard trigram estimate (i.e., ?(w1?wi-1) 
= (wi-2wi-1)). The estimation of headword 
probability is slightly more elaborate, reflecting the 
two assumptions described in Section 2.1:  
)|(())...(|( 122111 iiiiiii HhhwPHwwwP ??? =? ??   (2) 
))|()1( 212 iiii HhhwP ???+ ?   
)|()1( 121 iiii HwwwP ???+ ? .  
This estimate is an interpolated probability of three 
probabilities: P(wi|hi-2hi-1Hi) and P(wi|hi-1hi-2Hi), 
which are the headword trigram probability with or 
without permutation, and P(wi|wi-2wi-1Hi), which is 
the probability of wi given that it is a headword, 
where hi-1 and hi-2 denote the two preceding 
headwords, and ?1, ?2 ? [0,1] are the interpolation 
weights optimized on held-out data. 
The use of ?1 in Equation (2) is motivated by the 
first assumption described in Section 2.1: 
headwords are conditioned not only on two 
immediately preceding words, but also on two 
previous headwords. In practice, we estimated the 
headword probability by interpolating the 
conditional probability based on two previous 
headwords P(wi|hi-2hi-1Hi) (and P(wi|hi-1hi-2Hi) with 
permutation), and the conditional probability based 
on two preceding words P(wi|wi-2wi-1Hi). If ?1 is 
around zero, it indicates that this assumption does 
not hold in real data. Note that we did not estimate 
the conditional probability P(wi|wi-2wi-1hi-2hi-1Hi) 
directly, because this is in the form of a 5-gram, 
where the number of parameters are too large to 
estimate. 
The use of ?2 in Equation (2) comes from the 
second assumption in Section 2.1: headword 
trigrams are permutable. This assumption can be 
formulated as a co-occurrence model for headword 
prediction: that is, the probability of a headword is 
determined by the occurrence of other headwords 
within a window. However, in our experiments, we 
instead used an interpolated probability 
?2?P(wi|hi-2hi-1Hi) + (1??2)?P(wi|hi-1hi-2Hi) for two 
reasons. First, co-occurrence models do not predict 
words from left to right, and are thus very difficult 
to interpolate with trigram models for decoding. 
Second, if we see n-gram models as one extreme 
that predicts the next word based on a strictly 
ordered word sequence, co-occurrence models go to 
the other extreme of predicting the next word based 
on a bag of previous words without taking word 
order into account at all. We prefer models that lie 
somewhere between the two extremes, and consider 
word order in a more flexible way. In PHTM of 
Equation (2), ?2 represents the impact of word order 
on headword prediction. When ?2 = 1 (i.e., the 
resulting model is a non-permuted headword 
trigram model, referred to as HTM), it indicates that 
the second assumption does not hold in real data. 
When ?2 is around 0.5, it indicates that a headword 
bag model is sufficient. 
2.3 Model parameter estimation  
Assume that all conditional probabilities in 
Equation (1) are estimated using maximum 
likelihood estimation (MLE). Then 
)|( 12 ?? iii wwwP =  
)|()|( 1212 iiiiiii HwwwPwwHP ???? , wi: headword  
??
?
?
??
?
?
?
 
)|()|( 1212 iiiiiii FwwwPwwFP ???? , wi: function word 
is a strict equality when each word token is uniquely 
classified as a headword or a function word. This 
can be trivially proven as follows. Let Ci represent 
the category of wi (Hi or Fi in our case). We have 
)|()|( 1212 iiiiiii CwwwPwwCP ???? ?   
)(
)(
)(
)(
12
12
12
2
iii
iiii
ii
iiii
CwwP
wCwwP
wwP
CwwP
??
??
??
?? ?=   
)(
)(
12
12
??
??=
ii
iiii
wwP
wCwwP  (3) 
Since each word is uniquely assigned to a category, 
P(Ci|wi)=1, and thus it follows that 
)|()()( 121212 iiiiiiiiiii wwwCPwwwPwCwwP ?????? ?=  
)|()( 12 iiiii wCPwwwP ?= ??  
)( 12 iii wwwP ??= . (4) 
Substituting Equation (4) into Equation (3), we get 
)|()|( 1212 iiiiiii CwwwPwwCP ???? ?  
)|()(
)(
12
12
12
??
??
?? == iii
ii
iii wwwPwwP
wwwP . (5) 
Now, by separating the estimates of probabilities of 
headwords and function words, Equation (1) can be 
rewritten as: 
P(wi|?(w1?wi-1))= (6) 
)|()(|(( 122121 ???? iiiiii hhwPwwHP ??
))|()1( 212 ???+ iii hhwP?
)|()1( 121 ???+ iii wwwP?  
 
wi: headword  
)|( 12 ?? iii wwwP   ??
?
?
??
?
?
?
 
wi: function word  
There are three probabilities to be estimated in 
Equation (6): word trigram probability 
P(wi|wi-2wi-1), headword trigram probability 
P(wi|hi-2hi-1) and P(wi|hi-1hi-2) (where wi is a 
headword), and category probability P(Hi|wi-2wi-1). 
In order to deal with the data sparseness problem 
of MLE, we used a backoff scheme (Katz, 1987) for 
the parameter estimation. The backoff scheme 
recursively estimates the probability of an unseen 
n-gram by utilizing (n?1)-gram estimates. To keep 
the model size manageable, we also removed all 
n-grams with frequency less than 2.  
In order to classify a word uniquely as H or F, 
we needed a mapping table where each word in the 
lexicon corresponds to a category. The table was 
generated in the following manner. We first 
assumed that the mapping from part-of-speech 
(POS) to word category is fixed. The tag set we 
used included 1,187 POS tags, of which 102 count 
as headwords in our experiments. We then used a 
POS-tagger to generate a POS-tagged corpus, from 
which we generated the mapping table3. If a word 
could be mapped to both H and F, we chose the 
more frequent category in the corpus. Using this 
mapping table, we achieved a 98.5% accuracy of 
headword detection on the test data we used. 
Through our experiments, we found that 
P(Hi|wi-2wi-1) is a poor estimator of category 
probability; in fact, the unigram estimate P(Hi) 
achieved better results in our experiments as shown 
in Section 6.1. Therefore, we also used the unigram 
estimate for word category probability in our 
                                                     
3 Since the POS-tagger does not identify phrases, our 
implementation does not identify precisely one 
headword for a phrase, but identify multiple headwords 
in the case of compounds.  
experiments. The alternative model that uses the 
unigram estimate is given below:  
 
P(wi|?(w1?wi-1))= (7) 
)|()((( 1221 ?? iiii hhwPHP ??
))|()1( 212 ???+ iii hhwP?
)|()1( 121 ???+ iii wwwP?  
 
wi: headword  
)|( 12 ?? iii wwwP   ??
?
?
??
?
?
?
wi: function word  
We will denote the models using trigram for 
category probability estimation of Equation (6) as 
T-PHTM, and the models using unigram for 
category probability estimation of Equation (7) as 
U-PHTM. 
3 Using Clusters 
3.1 Principle 
Clustering techniques attempt to make use of 
similarities between words to produce a better 
estimate of the probability of word strings 
(Goodman, 2001).  
We have mentioned in Section 2.2 that the 
headword trigram model can be thought of as a 
cluster-based model with two clusters, the 
headword and the function word. In this section, we 
describe a method of clustering automatically 
similar words and headwords. We followed the 
techniques described in Goodman (2001) and Gao 
et al (2001), and performed experiments using 
predictive clustering along with headword trigram 
models.  
3.2 Predictive clustering model 
Consider a trigram probability P(w3|w1w2), where 
w3 is the word to be predicted, called the predicted 
word, and w1 and w2 are context words used to 
predict w3, called the conditional words. Gao et al 
(2001) presents a thorough comparative study on 
various clustering models for Asian languages, 
concluding that a model that uses clusters for 
predicted words, called the predictive clustering 
model, performed the best in most cases.  
Let iw  be the cluster which word wi belongs to. 
In this study, we performed word clustering for 
words and headwords separately. As a result, we 
have the following two predictive clustering models, 
(8) for words and (9) for headwords:  
)|()|()|( 121212 iiiiiiiiii wwwwPwwwPwwwP ?????? ?=  (8) 
)|()|()|( 121212 iiiiiiiiii whhwPhhwPhhwP ?????? ?=   
wi: headword 
(9) 
Substituting Equations (8) and (9) into Equation (7), 
we get the cluster-based PHTM of Equation (10), 
referred to as C-PHTM. 
 
P(wi|?(w1?wi-1))= (10) 
)|()|()((( 121221 iiiiiiii whhwPhhwPHP ???? ???
))|()|()1( 21212 iiiiiii whhwPhhwP ???? ??+ ?  
)|()|()1( 12121 iiiiiii wwwwPwwwP ???? ??+ ?  
 
wi: headword  
)|()|( 1212 iiiiiii wwwwPwwwP ???? ?  
 
??
?
?
??
?
?
?
 
wi: function word  
3.3 Finding clusters: model estimation 
In constructing clustering models, two factors were 
considered: how to find optimal clusters, and the 
optimal number of clusters.  
The clusters were found automatically by 
attempting to minimize perplexity (Brown et al, 
1992). In particular, for predictive clustering 
models, we tried to minimize the perplexity of the 
training data of )|()|( 1 iiii wwPwwP ?? . Letting N be 
the size of the training data, we have 
?
=
? ?
N
i
iiii wwPwwP
1
1 )|()|(  
?
= ?
? ?=
N
i i
ii
i
ii
WP
wwP
wP
wwP
1 1
1
)(
)(
)(
)(  
?
=
?
?
?=
N
i i
ii
i
ii
wP
wwP
wP
wwP
1
1
1 )(
)(
)(
)(  
?
=
?
?
?=
N
i
ii
i
i wwPwP
wP
1
1
1
)|()(
)(  
Now, 
)(
)(
1?i
i
wP
wP is independent of the clustering used. 
Therefore, in order to select the best clusters, it is 
sufficient to try to maximize ?= ?Ni ii wwP1 1 )|( . 
The clustering technique we used creates a 
binary branching tree with words at the leaves. By 
cutting the tree at a certain level, it is possible to 
achieve a wide variety of different numbers of 
clusters. For instance, if the tree is cut after the sixth 
level, there will be roughly 26=64 clusters. In our 
experiments, we always tried the numbers of 
clusters that are the powers of 2. This seems to 
produce numbers of clusters that are close enough 
to optimal. In Equation (10), the optimal number of 
clusters we used was 27. 
4 Relation to Previous Work 
Our LMs are similar to a number of existing ones. 
One such model was proposed by ATR (Isotani and 
Matsunaga, 1994), which we will refer to as ATR 
model below. In ATR model, the probability of 
each word in a sentence is determined by the 
preceding content and function word pair. Isotani 
and Matsunaga (1994) reported slightly better 
results over word bigram models for Japanese 
speech recognition. Geutner (1996) interpolated the 
ATR model with word-based trigram models, and 
reported very limited improvements over word 
trigram models for German speech recognition.  
One significant difference between the ATR 
model and our own lies in the use of predictive 
clustering. Another difference is that our models 
use separate probability estimates for headwords 
and function words, as shown in Equations (6) and 
(7). In contrast, ATR models are conceptually more 
similar to skipping models (Rosenfeld, 1994; Ney et 
al., 1994; Siu and Ostendorf, 2000), where only one 
probability estimate is applied for both content and 
function words, and the word categories are used 
only for the sake of finding the content and function 
word pairs in the context. 
Another model similar to ours is Jelinek (1990), 
where the headwords of the two phrases 
immediately preceding the word as well as the last 
two words were used to compute a word 
probability. The resulting model is similar to a 
5-gram model. A sophisticated interpolation 
formula had to be used since the number of 
parameters is too large for direct estimation. Our 
models are easier to learn because they use 
trigrams. They also differ from Jelinek's model in 
that they separately estimate the probability for 
headwords and function words.  
A significant number of sophisticated techniques 
for language modeling have recently been proposed 
in order to capture more linguistic structure from a 
larger context. Unfortunately, most of them suffer 
from either high computational cost or difficulty in 
obtaining enough manually parsed corpora for 
parameter estimation, which make it difficult to 
apply them successfully to realistic applications. 
For example, maximum entropy (ME) models 
(Rosenfeld, 1994) provide a nice framework for 
incorporating arbitrary knowledge sources, but 
training and using ME models is computationally 
extremely expensive.  
Another interesting idea that exploits the use of 
linguistic structure is structured language modeling 
(SLM, Chelba and Jelinek, 2000). SLM uses a 
statistical parser trained on an annotated corpus in 
order to identify the headword of each constituent, 
which are then used as conditioning words in the 
trigram context. Though SLMs have been shown to 
significantly improve the performance of the LM 
measured in perplexity, they also pose practical 
problems. First, the performance of SLM is 
contingent on the amount and quality of 
syntactically annotated training data, but such data 
may not always be available. Second, SLMs are 
very time-intensive, both in their training and use.  
Charniak (2001) and Roark (2001) also present 
language models based on syntactic dependency 
structure, which use lexicalized PCFGs that sum 
over the derivation probabilities. They both report 
improvements in perplexity over Chelba and 
Jelinek (2000) on the Wall Street Journal section of 
the Penn Treebank data, suggesting that syntactic 
structure can be further exploited for language 
modeling. The kind of linguistic structure used in 
our models is significantly more modest than that 
provided by parser-based models, yet offers 
practical benefits for realistic applications, as 
shown in the next section.  
5 Evaluation Methodology 
The most common metric for evaluating a language 
model is perplexity. Perplexity can be roughly 
interpreted as the expected branching factor of the 
test document when presented to a language model. 
Perplexity is widely used due to its simplicity and 
efficiency. However, the ultimate quality of a 
language model must be measured by its effect on 
the specific task to which it is applied, such as 
speech recognition. Lower perplexities usually 
result in lower error rates, but there are numerous 
counterexamples to this in the literature. 
In this study, we evaluated our language models 
on the application of Japanese Kana-Kanji 
conversion, which is the standard method of 
inputting Japanese text by converting the text of 
syllabary-based Kana string into the appropriate 
combination of ideographic Kanji and Kana. This is 
a similar problem to speech recognition, except that 
it does not include acoustic ambiguity. Performance 
on this task is generally measured in terms of the 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. The role of the language model is 
to select the word string (in a combination of Kanji 
and Kana) with the highest probability among the 
candidate strings that match the typed phonetic 
(Kana) string. Current products make about 5-10% 
errors in conversion of real data in a wide variety of 
domains.  
For our experiments, we used two newspaper 
corpora: Nikkei and Yomiuri Newspapers. Both 
corpora have been word-segmented. We built 
language models from a 36-million-word subset of 
the Nikkei Newspaper corpus. We performed 
parameter optimization on a 100,000-word subset 
of the Yomiuri Newspaper (held-out data). We 
tested our models on another 100,000-word subset 
of the Yomiuri Newspaper corpus. The lexicon we 
used contains 167,107 entries.  
In our experiments, we used the so-called 
?N-best rescoring? method. In this method, a list of 
hypotheses is generated by the baseline language 
model (a word trigram model in this study4), which 
is then rescored using a more sophisticated LM. 
Due to the limited number of hypotheses in the 
N-best list, the second pass may be constrained by 
the first pass. In this study, we used the 100-best 
list. The ?oracle? CER (i.e., the CER among the 
hypotheses with the minimum number of errors) is 
presented in Table 1. This is the upper bound on 
performance in our experiments. The performance 
of the conversion using the baseline trigram model 
is much better than the state-of-the-art performance 
currently available in the marketplace. This may be 
due to the large amount of training data we used, 
and to the similarity between the training and the 
test data. We also notice that the ?oracle? CER is 
                                                     
4  For the detailed description of the baseline trigram 
model, see Gao et al (2002).  
relatively high due to the high out-of-vocabulary 
rate, which is 1.14%. Because we have only limited 
room for improvement, the reported results of our 
experiments in this study may be underestimated. 
Baseline Trigram Oracle of 100-best 
3.73% 1.51% 
Table 1. CER results of baseline and 100-best list 
6 Results and Discussion 
6.1 Impact of headword dependency and 
predictive clustering 
We applied a series of language models proposed in 
this paper to the Japanese Kana-Kanji conversion 
task in order to test the effectiveness of our 
techniques. The results are shown in Table 2. The 
baseline result was obtained by using a 
conventional word trigram model. HTM stands for 
the headword trigram model of Equation (6) and (7) 
without permutation (i.e., ?2=1), while PHTM is the 
model with permutation. The T- and U-prefixes 
refer to the models using trigram (Equation (6)) or 
unigram (Equation (7)) estimate for word category 
probability. The C-prefix, as in C-PHTM, refers to 
PHTM with predictive clustering (Equation (10)). 
For comparison, we also include in Table 2 the 
results of using the predictive clustering model 
without taking word category into account, referred 
to as predictive clustering trigram model (PCTM). 
In PCTM, the probability for all words is estimated 
by )|()|( 1212 iiiiiii wwwwPwwwP ???? ? . 
Model ?1 ? 2 CER CER reduction 
Baseline ---- ---- 3.73% ----
T-HTM 0.2 1 3.54% 5.1% 
U-HTM  0.2 1 3.41% 8.6% 
T-PTHM 0.2 0.7 3.53% 5.4% 
U-PHTM  0.2 0.7 3.34% 10.5% 
PCTM ---- ---- 3.44% 7.8% 
C-HTM  0.3 1 3.23% 13.4% 
C-PHTM  0.3 0.7 3.17% 15.0% 
Table 2. Comparison of CER results 
In Table 2, we find that for both PHTM and HTM, 
models U-HTM and U-PHTM achieve better 
performance than models T-HTM and T-PHTM. 
Therefore, only models using unigram for category 
probability estimation are used for further 
experiments, including the models with predictive 
clustering. 
By comparing U-HTM with the baseline model, 
we can see that the headword trigram contributes 
greatly to the CER reduction: U-HTM 
outperformed the baseline model by about 8.6% in 
error rate reduction. HTM with headword 
permutation (U-PHTM) achieves further 
improvements of 10.5% CER reduction against the 
baseline. The contribution of predictive clustering is 
also very encouraging. Using predictive clustering 
alone (PCTM), we reduced the error rate by 7.8%.  
What is particularly noteworthy is that the 
combination of both techniques leads to even larger 
improvements: for both HTM and PHTM, 
predictive clustering (C-HTM and C-PHTM) brings 
consistent improvements over the models without 
clustering, achieving the CER reduction of 13.4% 
and 15.0% respectively against the baseline model, 
or 4.8% and 4.5% against the models without 
clustering.  
In sum, considering the good performance of our 
baseline system and the upper bound on 
performance improvement due to the 100-best list 
as shown in Table 1, the improvements we obtained 
are very promising. These results demonstrate that 
the simple method of using headword trigrams and 
predictive clustering can be used to effectively 
improve the performance of word trigram models. 
6.2 Comparsion with other models 
In this subsection, we present a comparison of our 
models with some of the previously proposed 
models, including the higher-order n-gram models, 
skipping models, and the ATR models.  
Higher-order n-gram models refer to those 
n-gram models in which n>3. Although most of the 
previous research showed little improvement, 
Goodman (2001) showed recently that, with a large 
amount of training data and sophisticated 
smoothing techniques, higher-order n-gram models 
could be superior to trigram models.  
The headword trigram model proposed in this 
paper can be thought of as a variation of a higher 
order n-gram model, in that the headword trigrams 
capture longer distance dependencies than trigram 
models. In order to see how far the dependency goes 
within our headword trigram models, we plotted the 
distribution of headword trigrams (y-axis) against 
the n of the word n-gram were it to be captured by 
the word n-gram (x-axis) in Figure 2. For example, 
given a word sequence w1w2w3w4w5w6, and if w1, w3 
and w6 are headwords, then the headword trigram 
P(w6|w3w1) spans the same distance as the word 
6-gram model.  
0.0E+00
5.0E+06
1.0E+07
1.5E+07
2.0E+07
2.5E+07
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
N of n-gram
Nu
mb
er 
of 
wo
rds
 Figure 2. Distribution of headword trigrams 
against the n of word n-gram 
From Figure 2, we can see that approximately 
95% of the headword trigrams can be captured by 
the higher-order n-gram model with the value of n 
smaller than 7. Based on this observation, we built 
word n-gram models with the values of n=4, 5 and 
6. For all n-gram models, we used the interpolated 
modified absolute discount smoothing method (Gao 
et al, 2001), which, in our experiments, achieved 
the best performance among the state-of-the-art 
smoothing techniques. Results showed that the 
performance of the higher-order word n-gram 
models becomes saturated quickly as n grows: the 
best performance was achieved by the word 5-gram 
model, with the CER of 3.71%. Following 
Goodman (2001), we suspect that the poor 
performance of these models is due to the data 
sparseness problem. 
Skipping models are an extension of an n-gram 
model in that they predict words based on n 
conditioning words, except that these conditioning 
words may not be adjacent to the predicted word. 
For instance, instead of computing P(wi|wi-2wi-1), a 
skipping model might compute P(wi|wi-3wi-1) or 
P(wi|wi-4wi-2). Goodman (2001) performed 
experiments of interpolating various kinds of 
higher-order n-gram skipping models, and obtained 
a very limited gain.  Our results confirm his results 
and suggest that simply extending the context 
window by brute-force can achieve little 
improvement, while the use of even the most 
modest form of structural information such as the 
identification of headwords and automatic 
clustering can help improve the performance.  
We also compared our models with the trigram 
version of the ATR models discussed in Section 4, 
in which the probability of a word is conditioned by 
the preceding content and function word pair. We 
performed experiments using the ATR models as 
described in Isotani and Matsunaga (1994). The 
results show that the CER of the ATR model alone 
is much higher than that of the baseline model, but 
when interpolated with a word trigram model, the 
CER is slightly reduced by 1.6% from 3.73% to 
3.67%. These results are consistent with those 
reported in previous work. The difference between 
the ATR model and our models indicates that the 
predictions of headwords and function words can 
better be done separately, as they play different 
semantic and syntactic roles capturing different 
dependency structure.  
6.3 Discussion 
In order to better understand the effect of the 
headword trigram, we have manually inspected the 
actual improvements given by PHTM. As expected, 
many of the improvements seem to be due to the use 
of larger context: for example, the headword 
trigram?? ~?? ~??  (shouhi 'consume' ~ 
shishutsu 'expense' ~ genshou 'decrease') 
contributed to the correct conversion of the 
phonetic string ?????  genshou into ?? 
genshou 'decrease' rather than ? ?  genshou 
'phenomenon' in the context of ?????????
? shouhi shishutsu hajimete no genshou  'consumer 
spending decreases for the first time'.  
On the other hand, the use of headword trigrams 
and predictive clustering is not without side effects. 
The overall gain in CER was 15% as we have seen 
above, but a closer inspection of the conversion 
results reveals that while C-PHTM corrected the 
conversion errors of the baseline model in 389 
sentences (8%), it also introduced new conversion 
errors in 201 sentences (4.1%). Among the newly 
introduced errors, one type of error is particularly 
worth noting: these are the errors where the 
candidate conversion preferred by the HTM is 
grammatically impossible or unlikely. For example, 
???????? (beikoku-ni shinkou-dekiru, 
USA-to invade-can 'can invade USA') was 
misconverted as ???????? (beikoku-ni 
shinkou-dekiru, USA-to new-can), even though ?
? shinkou 'invade' is far more likely to be preceded 
by the morpheme ? ni 'to', and ?? shinkou 'new' 
practically does not precede ??? dekiru 'can'. 
The HTM does not take these function words into 
account, leading to a grammatically impossible or 
implausible conversion. Finding the types of errors 
introduced by particular modeling assumptions in 
this manner and addressing them individually will 
be the next step for further improvements in the 
conversion task.   
7 Conclusion 
We proposed and evaluated a new language model, 
the permuted headword trigram model with 
clustering (C-PHTM). We have shown that the 
simple model that combines the predictive 
clustering with a headword detector can effectively 
capture structure in language. Experiments show 
that the proposed model achieves an encouraging 
15% CER reduction over a conventional word 
trigram model in a Japanese Kana-Kanji conversion 
system. We also compared C-PTHM to several 
similar models, showing that our model has many 
practical advantages, and achieves substantially 
better performance.  
One issue we did not address in this paper was 
the language model size: the models that use HTM 
are larger than the baseline model we compared the 
performance with. Though we did not pursue the 
issue of size reduction in this paper, there are many 
known techniques that effectively reduce the model 
size while minimizing the loss in performance. One 
area of future work is therefore to reduce the model 
size. Other areas include the application of the 
proposed model to a wider variety of test corpora 
and to related tasks.  
Acknowledgements 
We would like to thank Ciprian Chelba, Bill Dolan, 
Joshua Goodman, Changning Huang, Hang Li and 
Yoshiharu Sato for their comments on early 
thoughts and drafts of the paper. We would also like 
to thank Hiroaki Kanokogi, Noriko Ishibashi and 
Miyuki Seki for their help in our experiments. 
 
References 
Brown, Peter F., Vincent J. Della Pietra, Peter V. 
deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. 
Class-Based N-gram Models of Natural Language. 
Computational Linguistics, 18-4: 467-479. 
Charniak, Eugene. 2001. Immediate-head parsing for 
language models. In ACL/EACL 2001, pp.124-131.  
Chelba, Ciprian and Frederick Jelinek. 2000. Structured 
Language Modeling. Computer Speech and Language, 
Vol. 14, No. 4. pp 283-332.  
Gao, Jianfeng, Joshua T. Goodman and Jiangbo Miao. 
2001. The use of clustering techniques for language 
model ? application to Asian language. Computational 
Linguistics and Chinese Language Processing. Vol. 6, 
No. 1, pp 27-60. 
Gao, Jianfeng, Joshua Goodman, Mingjing Li and 
Kai-Fu Lee. 2002. Toward a unified approach to 
statistical language modeling for Chinese. ACM 
Transactions on Asian Language Information 
Processing, Vol. 1, No. 1, pp 3-33.  
Geutner, Petra. 1996. Introducing linguistic constraints 
into statistical language modeling. In International 
Conference on Spoken Language Processing, 
Philadelphia, USA. pp.402-405.  
Goodman, Joshua T. 2001. A bit of progress in language 
modeling. Computer Speech and Language. October, 
2001, pp 403-434. 
Goodman, Joshua T., and Jianfeng Gao. 2000. Language 
model size reduction by pruning and clustering. 
ICSLP-2000, Beijing.  
Isotani, Ryosuke and Shoichi Matsunaga. 1994. A 
stochastic language model for speech recognition 
integrating local and global constraints. ICASSP-94, 
pp. 5-8. 
Jelinek, Frederick. 1990. Self-organized language 
modeling for speech recognition. In A. Waibel and K. 
F. Lee (eds.), Readings in Speech Recognition, 
Morgan-Kaufmann, San Mateo, CA. pp. 450-506. 
Katz, S. M. 1987. Estimation of probabilities from sparse 
data for other language component of a speech 
recognizer. IEEE transactions on Acoustics, Speech 
and Signal Processing, 35(3): 400-401. 
Ney, Hermann, Ute Essen and Reinhard Kneser. 1994. 
On structuring probabilistic dependences in stochastic 
language modeling. Computer Speech and Language, 
8: 1-38. 
Roark, Brian. 2001. Probabilistic top-down parsing and 
language modeling. Computational Linguistics, 17-2: 
1-28. 
Rosenfeld, Ronald. 1994. Adaptive statistical language 
modeling: a maximum entropy approach. Ph.D. thesis, 
Carnegie Mellon University.  
Siu, Manhung and Mari Ostendorf. 2000. Variable 
n-grams and extensions for conversational speech 
language modeling. IEEE Transactions on Speech and 
Audio Processing, 8: 63-75. 
A Development Environment for Large-scale Multi-lingual Parsing Systems  
Hisami Suzuki 
Microsoft Research 
One Microsoft Way 
Redmond WA 98052 USA 
hisamis@microsoft.com 
 
Abstract 
We describe the development environment 
available to linguistic developers in our lab 
in writing large-scale grammars for 
multiple languages. The environment 
consists of the tools that assist writing 
linguistic rules and running regression 
testing against large corpora, both of which 
are indispensable for realistic development 
of large-scale parsing systems. We also 
emphasize the importance of parser 
efficiency as an integral part of efficient 
parser development. The tools and methods 
described in this paper are actively used in 
the daily development of broad-coverage 
natural language understanding systems in 
seven languages (Chinese, English, French, 
German, Japanese, Korean and Spanish).  
1 Introduction 
The goal of the grammar development at Microsoft 
Research is to build robust, broad-coverage 
analysis and generation systems for multiple 
languages. The runtime system is referred to as 
NLPWin, which provides the grammar 
development environment described in this paper. 
The graphical user interface of NLPWin is shown 
in Figure A in the Appendix. The system is modular 
in that the linguistic code is separate from 
non-linguistic code. All languages share the same 
parsing engine, which is a bottom-up chart parser 
and is fully Unicode-enabled. Linguistic code itself 
is also modular, in that it can be specific to a 
particular language (e.g., syntax rules) or can be 
largely shared across languages (e.g., semantic 
mapping rules). Linguistic rules are written in a 
proprietary language called G; a sample syntax rule 
written in G is given in Figure B in the Appendix. 
G-rules are translated into C, yet they are more 
convenient for a linguist to use than C, as it gives 
special notational support to attribute-value data 
structures used within the system. The rule and data 
structure formalisms are shared by all languages; 
for details, see Jensen et al (1993) and Heidorn 
(2000).  
In this paper, we describe the tools and 
methods for the cross-linguistic development of 
analysis components of our system, which consists 
of three major modules: (i) the tokenization 
component, which performs word segmentation (in 
the case of Chinese and Japanese) and 
morphological analysis; (ii) the parsing component, 
which performs phrase-structure analysis and 
creates parse tree(s)1; (iii) the Logical Form (LF) 
component, which computes the basic 
predicate-argument structure from parse tree(s)2. In 
this paper, we focus on the tools and the methods 
for the development of parsing and LF components, 
which are essentially the same3.  
For an efficient development of a computational 
grammar of these modules, we find it necessary to 
have a development environment that can provide 
immediate feedback to the grammar-writer of the 
changes he or she has made. We have three types of 
tools in our system to meet these requirements:  
y Tools for linguistic rule writing: these include 
the tools that let linguists navigate through the 
final and intermediate parse trees, and trace 
rule application (Section 2).  
y Tools for grammar testing: these tools allow 
linguists to compare results of two versions of 
                                                     
1
 This component is further divided into the Sketch 
component, which produces trees with default 
attachment of constituents, and the Portrait component, 
which finds the best attachment sites (Heidorn, 2000).  
2
 LF is computed from a surface syntax tree via a level of 
representation called Language-Neutral Syntax (LNS), 
which serves as an interface to various semantic 
representations including predicate-argument structure. 
For a more detailed description of LNS, see Campbell 
and Suzuki (2002).  
3
 Similar tools and methods are also available for the 
development of sentence realization component.  
 grammar, and update the database of desired 
output structures (called regression suites, 
Section 3). 
y A very fast processing environment (Section 
4).  
These tools are described in the following three 
sections of this paper. Section 5 gives a summary 
and suggests directions for future research.  
2 Tools for linguistic rule writing 
In this section we present the tools available for the 
development of the parsing component. The output 
structure of parsing is graphically represented as a 
phrase-structure tree, as in Figure 1 above. Various 
functionalities are available to navigate through this 
tree as well as intermediate (or failed) 
representations, by simple operations such as 
double-clicking the node in the user interface, or by 
typing in commands in the Command window, 
which can be invoked by the Command menu in the 
user interface (see Figure A). Below is a selected 
set of examples of tree navigation functionalities 
which are essential to the fast development of 
linguistic rules: 
 
Figure 1: A parse tree 
 
Figure 2: Lexical record for VERB1 ?discusses?                Figure 3: A derivational tree 
 
 
 Accessible records 
At any point, a linguist can access the records 
underlying the parse tree by double-clicking the 
node. The record for a node is comprised of lexical 
and morphological information, syntactic and 
functional features and attributes, as well as 
pointers to the sub-constituents and parent of the 
node. For example, double-clicking on the VERB1 
node in Figure 1 will display the record structure in 
Figure 2.  
Derivational tree 
We can also display the history of rule application 
in graphical form, as in Figure 3. Any node in the 
history tree (called the derivational tree) can also 
be double-clicked in order to access the record 
underlying it.  
Apply Rule and Rule Explain  
Rule Explain shows the application of the rule 
underlying the formation of a node in the tree. The 
rule application is displayed using a color-coded 
display to highlight successful conditions (green), 
failed conditions (red) and the actions performed on 
the resulting record (purple) on the rules such as the 
one displayed in Figure B. The display is available 
for both successful and failed rule applications: we 
can access the Rule Explain display by 
double-clicking the resulting node, or we can 
manually apply any rule to any constituent to bring 
up Rule Explain.  
Compare 
Parsed trees can be quite large and it may be 
difficult to determine exactly where two trees differ 
from each other. In such a case, trees and nodes can 
be easily compared to detect subtle differences in 
composition or rule history by the Compare 
function.  
Display trees 
This command is particularly useful in checking the 
edges of possible, intermediate constituents. It 
displays all the partial trees with a certain label that 
includes a particular node or spans over specified 
nodes. The following are some examples of 
possible variations in the query:  
(a)  display trees VP 1 5 
(b)  display trees NP NOUN4 
(c)  display trees AJP 
(a) displays all VPs that span from position 1 to 5; 
(b) displays all NPs that include the node NOUN4;
and (c) displays all possible subtrees whose 
nodetype (label) is AJP.  
Tree filters 
This functionality does not directly assist the 
grammarian in writing rules, yet is extremely useful 
in collecting and examining particular linguistic 
constructions of interest that are output by the 
parser. The linguistic developer can write 
custom-made tree filters in G, which traverses the 
parse trees or LF structures and exports only the 
information needed for a particular purpose, or only 
those sentences with particular linguistic 
configurations. Tree filters are also convenient in 
creating a linguistic annotation for external 
applications.   
The tools described in this section enable linguists 
to inspect the effect of grammar changes in detail, 
with the information of how exactly a particular 
rule applied or failed. These tools are used in the 
context of daily grammar development, which we 
describe in the next section.  
3 Process of grammar development 
3.1 Incremental grammar testing and 
creation of regression suites 
The standard practice of parser development within 
our group is schematically shown in Figure 4. The 
grammarian for each language processes a text file 
with input sentences and adds only the sentences 
with desired parses to what we call a master file, 
which contains the sentences and their target 
structure. A collection of master files is called a 
regression suite. A regression suite thus contains 
the target structures given a particular version of 
the grammar. When new grammar changes are 
made in order to accommodate a new sentence or 
construction, the linguist runs the new grammar 
against the regression suite (called regression 
testing) to examine the consequences of the 
changes to the grammar. When differences are 
found, they are kept in *.dff files and are displayed 
in two colors, highlighting the differences. Figure C 
in the Appendix is an example display of a 
difference (unfortunately in black and white): the 
highlights in green (here the first three lines) 
correspond to the analysis in the master file, while 
those in red (the next three lines) indicate the new 
 analysis introduced by the new grammar rules. The 
lines that did not change are grayed out. If the 
change is an improvement, the developer can 
choose to update the master file by double-clicking 
on the sentence number (in purple), adding the 
sentence or construction that is newly 
accommodated by the parser to the regression suite. 
If the change is evaluated as negative, the linguistic 
developer reworks the rules that caused the 
regression.  
3.2 Testing against relative standards 
As is described above, we run regression tests 
against the machine-created master files rather than 
against an independent set of hand-annotated target 
corpora. The test is therefore incremental and 
relative, in that new sentences and their target 
structures are constantly added as the grammar 
develops, and what it measures is not the coverage 
against an absolute standard, but the coverage 
improvements relative to the output of an old 
version of the grammar.  
The incremental and relative testing method has 
proven to facilitate the development of a 
broad-coverage parsing system in some important 
respects. First, it ensures that the desired structures 
in the master files are always current. Because the 
master files are constantly incremented and updated 
using the most recent version of the grammar, they 
will never become obsolete should the target 
structures change. The ease of maintenance of the 
regression suite is one of the key features 
contributing to the usefulness of the regression 
suite in our daily development work. 
Secondly, because the master files are created 
automatically rather than by hand, the resultant 
annotation is guaranteed to be consistent. Creating 
a test corpus for parser evaluation by hand is known 
to be an extremely laborious, inconsistency-prone 
task, especially when the tagging is performed on 
real-life data 4 . In addition, a broad-coverage 
grammar must also work with input strings that are 
not necessarily well-formed, including sentence 
fragments, ungrammatical sentences and extreme 
colloquialisms. Hand-annotating these structures 
may either be impossible or extremely error-prone. 
In contrast, by annotating them automatically 
using the output of the parser, these structures can 
be added to our regression suite easily and 
consistently. Effects of later grammar changes can 
easily be detected by running the regression testing 
as part of the regular development process.  
Finally, incremental and relative testing makes 
the parser development data-driven rather than 
being dictated by a theory. This is an important 
feature for a large-scale system. Though it is 
eventually up to the grammarian to accept or reject 
a particular analysis, the system always provides a 
candidate analysis for any input string, which 
facilitates the rapid creation of the master files. It 
also allows linguistic developers to experiment on 
the grammar code in the following sense: assume 
that there is a sentence or a construction that allows 
multiple linguistically valid analyses, and that there 
is no obvious reason to prefer one to the other, a 
situation that arises often in the development of a 
broad-coverage grammar. In this case, the 
grammarian can temporarily choose one of the 
structures as a target, and add it to the regression 
suite. If the target structure the grammarian has 
selected is inconsistent with the rest of the grammar, 
it will constantly come back as a regression 
(difference) when further changes to the other parts 
of the grammar are made, because the assumption 
implicit in the tentative target structure is not 
consistent with the rest of the grammar. Once the 
change is made to the target structure that is 
consistent with the remainder of the grammar, it 
typically stops appearing as a difference in 
regression tests. The data-driven nature of 
development therefore helps the grammarians to 
proceed with grammar development even when 
there is indeterminacy in the target structure. 
Regular regression testing over large corpora 
                                                     
4
 One piece of evidence for this statement is that the 
bracketing guidelines for Penn Treebank project (Bies et 
al. 1995) consist of over 300 pages of documentation for 
annotating relatively homogeneous text.  
 
Figure 4. Flow diagram of daily grammar 
development 
 ensures that any outlandish analyses have only a 
short life span in our regression suites.  
Possible disadvantages of testing against a 
relative standard include: (i) it is difficult get a feel 
for how mature the grammar is in general; (ii) it 
makes the comparison across different systems 
difficult. The first problem is addressed partially by 
running evaluation testing against blind benchmark 
corpora, which consists of sentences never used in 
the grammar development. The parser coverage is 
automatically measured in terms of the number of 
sentences that received at least one spanning parse, 
versus those that failed to receive any spanning 
analysis.  
Testing and comparing parser performance 
across different systems is an extremely difficult 
task, given different aims and grammatical 
foundations. One possibility, which is currently 
pursued in our group, is to develop a metric that 
enables comparison with manually created golden 
standards, as they have become more widely 
available for various languages, such as the Penn 
Treebank for Chinese and English, NEGRA corpus 
for German, and Kyoto Corpus for Japanese.  
Ultimately, the parser output must be compared 
and evaluated at the level of an application that uses 
the result of linguistic analysis. Campbell et al 
(2002) is an attempt to use machine translation as a 
test bed for a multi-lingual parsing system.  
4 Parser efficiency as part of efficient 
parser development 
For a development of a truly broad-coverage parser, 
it is critical that grammar changes are constantly 
verified against a very large set of sentences, and 
that the time for feedback is minimal. The 
efficiency of the parsing engine is thus inseparable 
from efficient grammar development.  
Our parsing engine is already quite fast: for 
example, our English system currently parses 
Section 21 of Penn Wall Street Journal (WSJ) 
Treebank (1,671 sentences) in 110 seconds (or 
about 15 sentences/sec) on a standard machine 
(993MHz Pentium III with 512MB RAM); this 
performance is comparable across languages.  
Speed improvements are usually performed by 
non-linguistic developers following standard 
optimization techniques. We use internal profiling 
tools to identify performance bottlenecks, and 
make a special effort to ensure that the G-to-C 
translator generates efficient C-code. Because the 
linguistic code is independent of the non-linguistic 
code of the system, the parsers for any language can 
immediately benefit from performance 
improvements made at the system level.  
For regression testing, we also have a means to 
distribute the processing onto multiple CPUs: the 
processing cluster currently consists of 19 
machines with 2 CPUs each (500MHz, 
128~512MB RAM), which parses the entire WSJ 
section of Penn Treebank (49,208 sentences) in 3 
minutes and 10 seconds (or 259 sentences/sec), and 
a one million-sentence Nikkei newspaper corpus of 
Japanese in about 30 minutes (550+ sentences/sec). 
In daily grammar development, each grammarian 
typically works with a regression suite consisting of 
10,000 to 30,000 sentences at various levels of 
analyses; the time required for processing a 
regression suite is 2 to 6 minutes. In addition, 
automatic regression testing is run nightly against 
relevant regression suites using the most recent 
builds of the system, ensuring that no negative 
impact is made by any changes introduced during 
the day5.  
In this section, we have discussed the issue of 
parser efficiency from the perspective of grammar 
development. Our processing environment enables 
immediate feedback to grammar changes over very 
large corpora, and is thus an essential part of the 
development environment for a broad-coverage 
parser. 
5 Conclusion 
In this paper we have described the tools and 
methods for a development of large-scale parsing 
systems. We have argued that constant testing of 
the grammar against a large regression suite is the 
central part of the daily grammar development, and 
that the tools and methods described in this paper 
are indispensable for maximizing the productivity 
of linguistic developers. Though the tools are 
specific to NLPWin, we believe that the general 
practice of grammar development presented in this 
paper is of interest to anyone engaged in grammar 
development under any grammar formalism.  
 As a cross-linguistic development environment 
for analysis and generation components, some of 
                                                     
5
 We use standard version control software to manage 
both linguistic and non-linguistic source code.  
 the properties of NLPWin discussed in this paper 
are shared with such projects as ParGram (Butt et 
al., 1999). One of the main differences between 
ParGram and NLPWin is that the latter has so far 
been developed and used at one site. As there are 
more parsers available in many languages, it would 
be interesting to see if externally developed 
components can be plugged into NLPWin at the 
level of LNS. Such research is left for the future as 
a possible extension to the modularity and 
cross-linguistic aspect of NLPWin.  
Acknowledgements 
This paper presents the work that has been designed 
and implemented by many people in the NLP 
Group at Microsoft Research, particularly George 
Heidorn and Karen Jensen.  
References 
Bies, Ann, Mark Ferguson, Karen Katz, and Robert 
MacIntyre, 1995. Bracketing Guidelines for 
Treebank II Style. Penn Treebank Project, 
University of Pennsylvania. 
Butt, Miriam, Tracy Holloway King, 
Maria-Eugenia Ni?o and Fr?d?rique Segond. 
1999. A Grammar Writer's Cookbook. CSLI 
Publications, Stanford.  
Campbell, Richard, and Hisami Suzuki. 2002. 
Language-Neutral Representation of Syntactic 
Structure.  In Proceedings of SCANALU 2002. 
Campbell, Richard, Carmen Lozano, Jessie 
Pinkham and Martine Smets. 2002. Machine 
Translation as a Test Bed for Multilingual 
Analysis. In Proceedings of the Workshop on 
Grammar Engineering and Evaluation, COLING 
2002, Taipei.  
Heidorn, George. 2000. Intelligent Writing 
Assistance. In Robert Dale, Hermann Moisl and 
Harold Somers (eds.), A Handbook of Natural 
Language Processing: Techniques and 
Applications for the Processing of Language as 
Text. Marcel Dekker, New York. Chapter 8.  
Jensen, Karen, George E. Heidorn and Stephen D. 
Richardson (eds.). 1993. Natural Language 
Processing: The PLNLP Approach. Kluwer 
Academic Publishers, Dordorecht. 
Appendix 
Figure A: Graphical user interface of NLPWin 
 
 Figure B: Example of a phrase-structure rule6 
AVPwAVPl: 
    AVP#1 (^Comma & ^Conjt & ^NoAdv & ^Top & Nodetype(Head)^="IJ" & 
          (Nodetype ^in? set{AVPNP AVPVP} | Compr | Intens | Tme | Ntimes) & 
          (Advrz -> (^Adv(Lex) & Intens)) & 
          (ModalAdvs -> Intens) & 
          (Nconj -> (^Conj(Lex) & Lemma^in? set{})) ) 
    AVP#2 (^AVPcoord & ^Conjt & ^Kakari & ^Nconj & ^NoAdv & ^Wh & 
           Nodetype^in? set{AVPNP AVPVP} & Nodetype(Head)^="IJ" &             
          ^tokntest("ADV", Ft(AVP#1), Lt, []) & 
          ^tokntest(-1, Ft(AVP#1), Lt(first(Factrecs)), []) & 
          (Compr(AVP#1) -> (Advrz | Quant)) & 
          (Demo(AVP#1) -> ^J_state_zyoo) & 
          (Intens -> Intens(AVP#1)) & 
          (Tme -> (Quant | P_every_mai)) & 
          (Tme(AVP#1) -> (Intens(AVP#1) | Compr(AVP#1))) & 
           Lem^="hoka" &    
           Lemma^in? set{} )  
--> AVP { %%AVP#2; Temp=segrec{%%AVP#1; -Quant; 
            if (Quant) Nodetype="QUANP";  
            if (Comp & ^Wa5(AVP#2)) -Mim; }; 
           Prmods=Temp++Prmods; Degree=Degree(AVP); -Temp;   
           if (Compr(AVP#1) | (Lem(AVP)=="mou" & Quant)) +Comp; 
       } 
 
 
Figure C: Example of the difference display with master file 
 
 
                                                     
6
 This rule, taken from the NLPWin Japanese grammar, is read as "AVP with AVP to the left", which takes two 
adjacent nodes, whose categories are both AVP (adverbial phrase), and creates a new node that spans both of the input 
nodes, also labeled as AVP, whose head is the second AVP of the left-hand side of the rule (indicated by %%AVP#2 
in the right-hand side of the rule). A rule can be as small as this one, or can be very large (up to hundreds of lines of 
code). Each language in NLPWin has about 100 to 150 phrase-structure rules, in 10 to 20 files that are 
language-specific. LF rules are also written in G and have a similar format, but the files are shared by all languages, as 
are most rules, to ensure the output of the LF component is consistent across languages.   
English-Japanese Example-Based Machine Translation Using Abstract 
Linguistic Representations 
 
Chris Brockett, Takako Aikawa, Anthony Aue, Arul Menezes, Chris Quirk  
and Hisami Suzuki 
Natural Language Processing Group, Microsoft Research 
One Microsoft Way 
Redmond, WA 98052, USA 
{chrisbkt,takakoa,anthaue,arulm,chrisq,hisamis}@microsoft.com 
 
 
Abstract 
This presentation describes an example- 
based English-Japanese machine trans- 
lation system in which an abstract 
linguistic representation layer is used to 
extract and store bilingual translation 
knowledge, transfer patterns between 
languages, and generate output strings. 
Abstraction permits structural neutral-
izations that facilitate learning of trans-
lation examples across languages with 
radically different surface structure charac-
teristics, and allows MT development to 
proceed within a largely language- 
independent NLP architecture. Com-
parative evaluation indicates that after 
training in a domain the English-Japanese 
system is statistically indistinguishable 
from a non-customized commercially 
available MT system in the same domain. 
Introduction 
In the wake of the pioneering work of Nagao 
(1984), Brown et al (1990) and Sato and 
Nagao (1990), Machine Translation (MT) 
research has increasingly focused on the issue 
of how to acquire translation knowledge from 
aligned parallel texts. While much of this 
research effort has focused on acquisition of 
correspondences between individual lexical 
items or between unstructured strings of words, 
closer attention has begun to be paid to the 
learning of structured phrasal units: Yamamoto 
and Matsumoto (2000), for example, describe a 
method for automatically extracting correspon-
dences between dependency relations in 
Japanese and English. Similarly, Imamura 
(2001a, 2001b) seeks to match corresponding 
Japanese and English phrases containing 
information about hierarchical structures, 
including partially completed parses. 
 
Yamamoto and Matsumoto (2000) explicitly 
assume that dependency relations between 
words will generally be preserved across 
languages. However, when languages are as 
different as Japanese and English with respect 
to their syntactic and informational structures, 
grammatical or dependency relations may not 
always be preserved: the English sentence ?the 
network failed? has quite a different 
grammatical structure from its Japanese 
translation equivalent ??????????
???? ?a defect arose in the network.? One 
issue for example-based MT, then, is to capture 
systematic divergences through generic 
learning applicable to multiple language pairs. 
 
In this presentation we describe the MSR-MT 
English-Japanese system, an example-based 
MT system that learns structured phrase-sized 
translation units. Unlike the systems discussed 
in Yamamoto and Matsumoto (2000) and 
Imamura (2001a, 2001b), MSR-MT places the 
locus of translation knowledge acquisition at a 
greater level of abstraction than surface 
relations, pushing it into a semantically- 
motivated layer called LOGICAL FORM (LF) 
(Heidorn 2000; Campbell & Suzuki 2002a, 
2002b). Abstraction has the effect of 
neutralizing (or at least minimizing) differences 
in word order and syntactic structure, so that 
mappings between structural relations 
associated with lexical items can readily be 
acquired within a general MT architecture. 
 
In Section 1 below, we present an overview of 
the characteristics of the system, with special 
reference to English-Japanese MT. Section 2 
discusses a class of structures learned through 
phrase alignment, Section 3 presents the results 
of comparative evaluation, and Section 4 some 
factors that contributed to the evaluation results. 
Section 5 addresses directions for future work. 
 
  
1 The MSR-MT System 
The MSR-MT English-Japanese system is a 
hybrid example-based machine translation 
system that employs handcrafted broad- 
coverage augmented phrase structure grammars 
for parsing, and statistical and heuristic 
techniques to capture translation knowlege and 
for transfer between languages. The parsers are 
general purpose: the English parser, for 
example, forms the core of the grammar 
checkers used in Microsoft Word (Heidorn 
2000). The Japanese grammar utilizes much of 
the same codebase, but contains language- 
specific grammar rules and additional features 
owing to the need for word-breaking in 
Japanese (Suzuki et al 2000; Kacmarcik et al 
2000). These parsers are robust in that if the 
analysis grammar fails to find an appropriate 
parse, it outputs a best-guess ?fitted? parse. 
 
System development is not confined to 
English-Japanese: MSR-MT is part of a 
broader natural language processing project 
involving three Asian languages (Japanese, 
Chinese, and Korean) and four European 
languages (English, French, German, and 
Spanish). Development of the MSR-MT 
systems proceeds more or less simultaneously 
across these languages and in multiple 
directions, including Japanese-English. The 
Spanish-English version of MSR-MT has been 
described in Richardson et al 2001a, Richardson 
et al2001b, and the reader is referred to these 
papers for more information concerning 
algorithms employed during phrase alignment. 
A description of the French-Spanish MT 
system is found in Pinkham & Smets. 2002. 
 
1.1 Training Data 
MSR-MT requires that a large corpus of 
aligned sentences be available as examples for 
training. For English-Japanese MT, the system 
currently trains on a corpus of approximately 
596,000 pre-aligned sentence pairs. About 
274,000 of these are sentence pairs extracted 
from Microsoft technical documentation that 
had been professionally translated from 
English into Japanese. The remaining 322,000 
are sentence examples or sentence fragments 
extracted from electronic versions of student 
dictionaries.1  
1.2  Logical Form 
MSR-MT employs a post-parsing layer of 
semantic representation called LOGICAL FORM 
(LF) to handle core components of the 
translation process, namely acquisition and 
storage of translation knowledge, transfer 
between languages, and generation of target 
output. LF can be viewed as a representation of 
the various roles played by the content words 
after neutralizing word order and local 
morphosyntactic variation (Heidorn 2000; 
Campbell & Suzuki 2002a; 2002b). These can 
be seen in the Tsub (Typical Subject) and Tobj 
(Typical Object) relations in Fig. 1 in the 
sentence ?Mary eats pizza? and its Japanese 
counterpart. The graphs are simplified for 
expository purposes. 
Although our hypothesis is that equivalent 
sentences in two languages will tend to 
resemble each other at LF more than they do in 
the surface parse, we do not adopt a na?ve 
reductionism that would attempt to make LFs 
completely identical. In Fig. 2, for example, the 
LFs of the quantified nouns differ in that the 
Japanese LF preserves the classifier, yet are 
similar enough that learning the mapping 
between the two structures is straightforward. 
It will be noted that since the LF for each 
language stores words or morphemes of that 
language, this level of representation is not in 
any sense an interlingua. 
 
                                                   
1 Kodansha?s Basic English-Japanese Dictionary, 
1999; Kenkyusha?s New College Japanese-English 
Dictionary, 4th Edition, 1995 ; and Kenkyusha?s 
New College English-Japanese Dictionary, 6th 
Edition, 1994. 
 
 
Fig. 1  Canonical English and Japanese 
Logical Forms 
 
 
1.3  Mapping Logical Forms 
In the training phase, MSR-MT learns transfer 
mappings from the sentence-aligned bilingual 
corpus. First, the system deploys the 
general-purpose parsers to analyze the English 
and Japanese sentence pairs and generate LFs 
for each sentence. In the next step, an LF 
alignment algorithm is used to match source 
language and target language LFs at the 
sub-sentence level. 
 
The LF alignment algorithm first establishes 
tentative lexical correspondences between 
nodes in the source and target LFs on the basis 
of lexical matching over dictionary information 
and approximately 31,000 ?word associations,? 
that is, lexical mappings extracted from the 
training corpora using statistical techniques 
based on mutual information (Moore 2001). 
From these possible lexical correspondences, 
the algorithm uses a small grammar of 
(language-pair-independent) rules to align LF 
nodes on lexical and structural principles. The 
aligned LF pairs are then partitioned into 
smaller aligned LF segments, with individual 
node mappings captured in a relationship we 
call ?sublinking.? Finally, the aligned LF 
segments are filtered on the basis of frequency, 
and compiled into a database known as a 
Mindnet. (See Menezes & Richardson 2001 for a 
detailed description of this process.) 
 
The Mindnet is a general-purpose database of 
semantic information (Richardson et al 1998) 
that has been repurposed as the primary 
repository of translation information for MT 
applications. The process of building the 
Mindnet is entirely automated; there is no 
human vetting of candidate entries. At the end 
of a typical training session, 1,816,520 transfer 
patterns identified in the training corpus may 
yield 98,248 final entries in the Mindnet. Only 
the output of successful parses is considered 
for inclusion, and each mapping of LF 
segments must have been encountered twice in 
the corpus before it is incorporated into the 
Mindnet. 
 
In the Mindnet, LF segments from the source 
language are represented as linked to the 
corresponding LF segment from the target 
languages. These can be seen in Figs. 3 and 4, 
discussed below in Section 2. 
1.4  Transfer and Generation 
At translation time, the broad-coverage source 
language parser processes the English input 
sentence, and creates a source-language LF. 
This LF is then checked against the Mindnet 
entries. 2  The best matching structures are 
extracted and stitched together determinist-
ically into a new target-language ?transferred 
LF? that is then submitted to the Japanese 
system for generation of the output string. 
 
The generation module is language-specific 
and used for both monolingual generation and 
MT. In the context of MT, generation takes as 
input the transferred LF and converts it into a 
basic syntactic tree. A small set of heuristic 
rules preprocesses the transferred LF to 
?nativize? some structural differences, such as 
pro-drop phenomena in Japanese. A series of 
core generation rules then applies to the LF tree, 
transforming it into a Japanese sentence string. 
Generation rules operate on a single tree only, 
are application-independent and are developed 
in a monolingual environment (see Aikawa et 
al. 2001a, 2001b for further details.) 
Generation of inflectional morphology is also 
handled in this component. The generation 
component has no explicit knowledge of the 
source language. 
 
2 Acquisition of Complex Structural 
Mappings 
The generalization provided by LF makes it 
possible for MSR-MT to handle complex 
structural relations in cases where English and 
Japanese are systematically divergent. This is 
                                                   
2 MSR-MT resorts to lexical lookup only when a 
term is not found in the Mindnet. The handcrafted 
dictionary is slated for replacement by purely 
statistically generated data.  
 
 
Fig. 2  Cross-Linguistic Variation in Logical 
Form 
 
illustrated by the sample training pair in the 
lefthand column of Table 1. In Japanese, 
inanimate nouns tend to be avoided as subjects 
of transitive verbs; the word ?URL?, which is 
subject in the English sentence, thus 
corresponds to an oblique relation in the 
Japanese. (The Japanese sentence, although a 
natural and idiomatic translation of the English,  
is literally equivalent to ?one can access public 
folders with this URL.?)   
 
Nonetheless, mappings turn out to be learnable 
even where the information is structured so 
radically differently. Fig. 3 shows the Mindnet 
entry for ?provide,? which is result of training 
on sentence pairs like those in the lefthand 
column of Table 1. The system learns not only 
the mapping between the phrase ?provide 
access? and the potential form of ???? 
?access?, but also the crucial sublinking of the 
Tsub node of the English sentence and the node 
headed by ?  (underspecified for semantic 
role) in the Japanese. At translation time the 
system is able to generalize on the basis of the 
functional roles stored in the Mindnet; it can 
substitute lexical items to achieve a relatively 
natural translation of similar sentences such as 
shown in the right-hand side of Table 1.  
Differences of the kind seen in Fig 3 are 
endemic in our Japanese and English corpora. 
Fig. 4 shows part of the example Mindnet entry 
for the English word ?fail? referred to in the 
Introduction, which exhibits another mismatch 
in grammatical roles somewhat similar to that 
in observed in Fig. 3. Here again, the lexical 
matching and generic alignment heuristics have 
allowed the match to be captured into the 
Mindnet. Although the techniques employed 
may have been informed by analysis of 
language-specific data, they are in principle of 
general application. 
 
 
3 Evaluation 
In May 2002, we compared output of the 
MSR-MT English-Japanese system with a 
commercially available desktop MT system.3 
                                                   
3 Toshiba?s The Honyaku Office v2.0 desktop MT 
system was selected for this purpose. The Honyaku 
is a trademark of the Toshiba Corporation. Another 
desktop system was also considered for evaluation; 
however, comparative evaluation with that system 
indicated that the Toshiba system performed 
marginally, though not significantly, better on our 
technical documentation.  
 
Training Data Translation Output  
This URL provides access to public folders. 
 
This computer provides access to the internet. 
 
?? URL ?????? ????? 
????????? 
????????????????? 
????????? 
 
Table 1.  Sample Input and Output 
Fig. 3.  Part of the Mindnet Entry for ?provide? 
 
 
Fig. 4.  Part of the Mindnet Entry for ?fail? 
 
A total of 238 English-Japanese sentence pairs 
were randomly extracted from held-out 
software manual data of the same kinds used 
for training the system. 4  The Japanese 
sentences, which had been translated by human 
translators, were taken as reference sentences 
(and were assumed to be correct translations). 
The English sentences were then translated by 
the two MT systems into Japanese for blind 
evaluation performed by seven outside vendors 
unfamiliar with either system?s characteristics. 
 
No attempt was made to constrain or modify 
the English input sentences on the basis of 
length or other characteristics. Both systems 
provided a translation for each sentence.5  
 
For each of the Japanese reference sentences, 
evaluators were asked to select which 
translation was closer to the reference sentence. 
A value of +1 was assigned if the evaluator 
considered MSR-MT output sentence better 
and ?1 if they considered the comparison 
system better. If two translated sentences were 
considered equally good or bad in comparison 
                                                   
4  250 sentences were originally selected for 
evaluation; 12 were later discarded when it was 
discovered by evaluators that the Japanese reference 
sentences (not the input sentences) were defective 
owing to the presence of junk characters (mojibake) 
and other deficiencies.  
5 In MSR-MT, Mindnet coverage is sufficiently 
complete with respect to the domain that an 
untranslated sentence normally represents a 
complete failure to parse the input, typically owing 
to excessive length. 
to the reference, a value of 0 was assigned. On 
this metric, MSR-MT scored slightly worse 
than the comparison system rating of ?0.015. 
At a two-way confidence measure of +/?0.16, 
the difference between the systems is 
statistically insignificant. By contrast, an 
earlier evaluation conducted in October 2001 
yielded a score of ?0.34 vis-?-vis the 
comparison system. 
 
In addition, the evaluators were asked to rate 
the translation quality on an absolute scale of 1 
through 4, according to the following criteria: 
 
1. Unacceptable: Absolutely not comprehen- 
sible and/or little or no information trans- 
ferred accurately. 
2. Possibly Acceptable: Possibly compre- 
hensible (given enough context and/or 
time to work it out); some information 
transferred accurately. 
3. Acceptable: Not perfect, but definitely 
comprehensible, and with accurate transfer 
of all important information. 
4. Ideal: Not necessarily a perfect translation, 
but grammatically correct, and with all 
information accurately transferred. 
 
On this absolute scale, neither system 
performed exceptionally well: MSR-MT scored 
an average 2.25 as opposed to 2.32 for the 
comparison system. Again, the difference 
between the two is statistically insignificant. It 
should be added that the comparison presented 
here is not ideal, since MSR-MT was trained 
principally on technical manual sentences, 
 Evaluation 
Date 
Transfers 
per Sentence 
Nodes  
Per Transfer
 
 Oct. 2001 5.8 1.6  
 May 2002 6.7 2.0  
Table 2. Number of Transfers and Nodes Transferred per Sentence 
 
 Evaluation Date Word Class Total From 
Mindnet 
From 
Dictionary
Untranslated  
 Prepositions 410 17.1% 77.1% 5.9%  
 
Oct. 2001  
(250 sentences) Content Lemmas 2124 88.4% 7.8% 3.9%  
 Prepositions 842 61.9% 37.5% 0.6%  
 
May 2002 
(520 sentences) Content Lemmas 4429 95.9% 1.5% 2.6%  
Table 3.  Sources of Different Word Classes at Transfer 
 
while the comparison system was not 
specifically tuned to this corpus. Accordingly 
the results of the evaluation need to be 
interpreted narrowly, as demonstrating that:  
l  A viable example-based English-Japanese 
MT system can be developed that applies 
general-purpose alignment rules to semantic 
representations; and  
l  Given general-purpose grammars, a 
representation of what the sentence means, 
and suitable learning techniques, it is 
possible to achieve in a domain, results 
analogous with those of a mature 
commercial product, and within a relatively 
short time frame. 
4 Discussion 
It is illustrative to consider some of the factors 
that contributed to these results. Table 2 shows 
the number of transfers per sentence and the 
number of LF nodes per transfer in versions of 
the system evaluated in October 2001 and May 
2002. Not only is the MSR-MT finding more 
LF segments in the Mindnet, crucially the 
number of nodes transferred has also grown. 
An average of two connected nodes are now 
transferred with each LF segment, indicating 
that the system is increasingly learning its 
translation knowledge in terms of complex 
structures rather than simple lexical 
correspondences. 
 
It has been our experience that the greater 
MSR-MT?s reliance on the Mindnet, the better 
the quality of its output. Table 2 shows the 
sources of selected word classes in the two 
systems. Over time, reliance on the Mindnet 
has increased overall, while reliance on 
dictionary lookup has now diminished to the 
point where, in the case of content words, it 
should be possible to discard the handcrafted 
dictionary altogether and draw exclusively on 
the contextualized resources of the Mindnet 
and statistically-generated lexical data. Also 
striking in Table 2 is the gain shown in 
preposition handling: a majority of English 
prepositions are now being transferred only in 
the context of LF structures found in the 
Mindnet. 
 
The important observation underlying the gains 
shown in these tables is that they have 
primarily been obtained either as the result of 
LF improvements in English or Japanese (i.e., 
from better sentence analysis or LF 
construction), or as a result of generic 
improvements to the algorithms that map 
between LF segments (notably better 
coindexation and improved learning of 
mappings involving lexical attributes). In the 
latter case, although certain modifications may 
have been driven by phenomena observed 
between Japanese and English, the heuristics 
apply across all seven languages on which our 
group is currently working. Adaptation to the 
case of Japanese-English MT usually takes the 
form of loosening rather than tightening of 
constraints.  
 
 
5 Future Work 
Ultimately it is probably desirable that the 
system?s mean absolute score should approach 
3 (Acceptable) within the training domain: this 
is a high quality bar that is not attained by 
off-the-shelf systems. Much of the work will be 
of a general nature: improving the parses and 
LF structures of source and target languages 
will bring automatic benefits to both alignment 
of structured phrases and runtime translation. 
For example, efforts are currently underway to 
redesign LF to better represent scopal 
properties of quantifiers and negation 
(Campbell & Suzuki 2002a, 2002b). 
 
Work to improve the quality of alignment and 
transfer is ongoing within our group. In 
addition to improvement of alignment itself, 
we are also exploring techniques to ensure that 
the transferred LF is consistent with known 
LFs in the target language, with the eventual 
goal of obviating the need for heuristic rules 
used in preprocessing generation. Again, these 
improvements are likely to be system-wide and 
generic, and not specific to the 
English-Japanese case. 
 
 
Conclusions 
Use of abstract semantically-motivated 
linguistic representations (Logical Form) 
permits MSR-MT to align, store, and translate 
sentence patterns reflecting widely varying 
syntactic and information structures in 
Japanese and English, and to do so within the 
framework of a general-purpose NLP 
architecture applicable to both European 
languages and Asian languages. 
 
Our experience with English-Japanese example 
based MT suggests that the problem of MT 
among Asian languages may be recast as a 
problem of implementing a general represen- 
tation of structured meaning across languages 
that neutralizes differences where possible, and 
where this is not possible, readily permits 
researchers to identify general-purpose 
techniques of bridging the disparities that are 
viable across multiple languages. 
 
Acknowledgements 
We would like to thank Bill Dolan and Rich 
Campbell for their comments on a draft of this 
paper. Our appreciation also goes to the 
members of the Butler Hill Group for their 
assistance with conducting evaluations. 
References  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001a. Multilingual sentence generation. In 
Proceedings of 8th European Workshop on 
Natural Language Generation, Toulouse, France.  
Aikawa, T., M. Melero, L. Schwartz, and A. Wu. 
2001b. Sentence generation for multilingual 
machine translation. In Proceedings of the MT 
Summit VIII, Santiago de Compostela, Spain.  
Brown, P. F.,  J. Cocke, S. A. D. Pietra, V. J. D. 
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, 
and P. S. Roossin. 1990. A statistical approach to 
machine translation. Computational Linguistics, 
16(2): 79-85. 
Campbell, R. and H. Suzuki. 2002a. Language- 
neutral representation of syntactic structure. In 
Proceedings of the First International Workshop 
on Scalable Natural Language Understanding 
(SCANALU 2002), Heidelberg, Germany. 
Campbell, R. and H. Suzuki. 2002b. Language- 
Neutral Syntax: An Overview. Microsoft Research 
Techreport: MSR-TR-2002-76. 
Heidorn, G. 2000. Intelligent writing assistance. In 
R. Dale, H. Moisl and H. Somers (eds.), A 
Handbook of Natural Language Processing: 
Techniques and Applications for the Processing 
of Language as Text. Marcel Dekker, New York. 
pp. 181-207.  
Imamura, K. 2001a. Application of translation 
knowledge acquired by ierarchical phrase 
alignment. In Proceedings of TMI.  
Imamura, K. 2001b. Hierarchical phrase alignment 
harmonized with parsing. In Proceedings of 
NLPRS, Tokyo, Japan, pp 377-384.  
Kacmarcik, G., C. Brockett, and H. Suzuki. 2000. 
Robust segmentation of Japanese text into a 
lattice for parsing. In Proceedings of COLING 
2000, Saarbrueken, Germany, pp. 390-396. 
Menezes, A. and S. D. Richardson. 2001. A 
best-first alignment algorithm for automatic 
extraction of transfer mappings from bilingual 
corpora. In Proceedings of the Workshop on 
Data-driven Machine Translation at 39th Annual 
Meeting of the Association for Computational 
Linguistics, Toulouse, France, pp. 39-46. 
Moore, R. C. 2001. Towards a simple and accurate 
statistical approach to learning translation 
relationships among words," in Proceedings, 
Workshop on Data-driven Machine Translation, 
39th Annual Meeting and 10th Conference of the 
European Chapter, Association for 
Computational Linguistics, Toulouse, France, pp. 
79-86. 
Nagao, M. 1984. A framework of a mechanical 
translation between Japanese and English by 
analogy principle. In A. Elithorn. and R. Bannerji 
(eds.) Artificial and Human Intelligence.  Nato 
Publications. pp. 181-207.   
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro. 2001. Rapid Assembly of a Large-scale 
French-English MT system. In Proceedings of the 
MT Summit VIII, Santiago de Compostela, Spain. 
Pinkham, J., and M. Smets. 2002. Machine 
translation without a bilingual dictionary. In 
Proceedings of the 9th International Conference 
on Theoretical and Methodological Issues in 
Machine Translation. Kyoto, Japan, pp. 146-156. 
Richardson, S. D., W. B. Dolan, A. Menezes, and M. 
Corston-Oliver. 2001. Overcoming the 
customization bottleneck using example-based 
MT. In Proceedings, Workshop on Data-driven 
Machine Translation, 39th Annual Meeting and 
10th Conference of the European Chapter, 
Association for Computational Linguistics. 
Toulouse, France, pp. 9-16. 
Richardson, S. D., W. B. Dolan, A. Menezes, and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods. In 
Proceedings of MT Summit VIII, Santiago De 
Compostela, Spain, pp. 293-298.  
Richardson, S. D., W. B. Dolan, and L. 
Vanderwende. 1998 MindNet: Acquiring and 
structuring semantic information from text, 
ACL-98. pp. 1098-1102. 
Sato, S. and Nagao M. 1990. Toward 
memory-based translation. In Proceedings of 
COLING 1990, Helsinki, Finland, pp. 247-252. 
Suzuki, H., C. Brockett, and G. Kacmarcik. 2000. 
Using a broad-coverage parser for word-breaking 
in Japanese. In Proceedings of COLING 2000, 
Saarbrueken, Germany, pp. 822-827.  
Yamamoto K., and Y Matsumoto. 2000. 
Acquisition of phrase-level bilingual 
correspondence using dependency structure. In 
Proceedings of COLING 2000, Saarbrueken, 
Germany, pp. 933-939.  
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 69?71,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
NEWS 2009 Machine Transliteration Shared Task System Description:
Transliteration with Letter-to-Phoneme Technology
Colin Cherry and Hisami Suzuki
Microsoft Research
One Microsoft Way
Redmond, WA, 98052
{colinc,hisamis}@microsoft.com
Abstract
We interpret the problem of transliterat-
ing English named entities into Hindi or
Japanese Katakana as a variant of the
letter-to-phoneme (L2P) subtask of text-
to-speech processing. Therefore, we apply
a re-implementation of a state-of-the-art,
discriminative L2P system (Jiampojamarn
et al, 2008) to the problem, without fur-
ther modification. In doing so, we hope
to provide a baseline for the NEWS 2009
Machine Transliteration Shared Task (Li
et al, 2009), indicating how much can be
achieved without transliteration-specific
technology. This paper briefly sum-
marizes the original work and our re-
implementation. We also describe a bug
in our submitted implementation, and pro-
vide updated results on the development
and test sets.
1 Introduction
Transliteration occurs when a word is borrowed
into a language with a different character set from
its language of origin. The word is transcribed into
the new character set in a manner that maintains
phonetic correspondence.
When attempting to automate machine translit-
eration, modeling the channel that transforms
source language characters into transliterated tar-
get language characters is a key component to
good performance. Since the primary signal fol-
lowed by human transliterators is phonetic corre-
spondence, it makes sense that a letter-to-phoneme
(L2P) transcription engine would perform well at
this task. Of course, transliteration is often framed
within the larger problems of translation and bilin-
gual named entity co-reference, making available
a number of other interesting features, such as tar-
get lexicons (Knight and Graehl, 1998), distribu-
tional similarity (Bilac and Tanaka, 2005), or the
dates of an entity?s mentions in the news (Kle-
mentiev and Roth, 2006). However, this task?s fo-
cus on generation has isolated the character-level
component, which makes L2P technology a near-
ideal match. For our submission, we re-implement
the L2P approach described by Jiampojamarn et
al. (2008) as faithfully as possible, and apply it
unmodified to the transliteration shared task for
the English-to-Hindi (Kumaran and Kellner, 2007)
and English-to-Japanese Katakana1 tests.
2 Approach
2.1 Summary of L2P approach
The core of the L2P transduction engine is the
dynamic programming algorithm for monotone
phrasal decoding (Zens and Ney, 2004). The main
feature of this algorithm is its capability to trans-
duce many consecutive characters with a single
operation. This algorithm is used to conduct a
search for a max-weight derivation according to
a linear model with indicator features. A sample
derivation is shown in Figure 1.
There are two main categories of features: con-
text and transition features, which follow the first
two feature templates described by Jiampojamarn
et al (2008). Context features are centered around
a transduction operation. These features include
an indicator for the operation itself, which is then
conjoined with indicators for all n-grams of source
context within a fixed window of the operation.
Transition features are Markov or n-gram features.
They ensure that the produced target string makes
sense as a character sequence, and are represented
as indicators on the presence of target n-grams.
The feature templates have two main parameters,
the size S of the character window from which
source context features are drawn, and the max-
imum length T of target n-gram indicators. We
fit these parameters using grid search over 1-best
1Provided by http://www.cjk.org
69
ame ?A , ri ?J , can ?S
Figure 1: Example derivation transforming
?American? into ?AJS?.
accuracy on the provided development sets.
The engine?s features are trained using the
structured perceptron (Collins, 2002). Jiampo-
jamarn et al (2008) show strong improvements
in the L2P domain using MIRA in place of the
perceptron update; unfortunately, we did not im-
plement a k-best MIRA update due to time con-
straints. In our implementation, no special con-
sideration was given to the availability of multi-
ple correct answers in the training data; we always
pick the first reference transliteration and treat it
as the only correct answer. Investigating the use
of all correct answers would be an obvious next
step to improve the system.
2.2 Major differences in implementation
Our system made two alternate design decisions
(we do not claim improvements) over those made
by (Jiampojamarn et al, 2008), mostly based on
the availability of software. First, we employed a
beam of 40 candidates in our decoder, to enable ef-
ficient use of large language model contexts. This
is put to good use in the Hindi task, where we
found n-gram indicators of length up to n = 6
provided optimal development performance.
Second, we employed an alternate character
aligner to create our training derivations. This
aligner is similar to recent non-compositional
phrasal word-alignment models (Zhang et al,
2008), limited so it can only produce monotone
character alignments. The aligner creates sub-
string alignments, without insertion or deletion
operators. As such, an aligned transliteration pair
also serves as a transliteration derivation. We em-
ployed a maximum substring length of 3.
The training data was heuristically cleaned af-
ter alignment. Any derivation found by the aligner
that uses an operation occurring fewer than 3 times
throughout the entire training set was eliminated.
This reduced training set sizes to 8,511 pairs
for English-Hindi and 20,306 pairs for English-
Katakana.
Table 1: Development and test 1-best accuracies,
as reported by the official evaluation tool
System / Test set With Bug Fixed
Hindi Dev 36.7 39.6
Hindi Test 41.8 46.6
Katakana Dev 46.0 47.1
Katakana Test 46.6 46.9
3 The Bug
The submitted version of our system had a bug
in its transition features: instead of generating an
indicator for every possible n-gram in the gener-
ated target sequence, it generated n-grams over
target substrings, defined by the operations used
during transduction. Consider, for example, the
derivation shown in Figure 1, which generates
?AJS?. With buggy trigram transition
features, the final operation would produce the
single indicator [AJ|S], instead of the two
character-level trigrams [AJ|] and [J|S].
This leads to problems with data sparsity, which
we had not noticed on unrelated experiments with
larger training data. We report results both with
the bug and with fixed transition features. We do
so to emphasize the importance of a fine-grained
language discriminative language model, as op-
posed to one which operates on a substring level.
4 Development
Development consisted of performing a parameter
grid search over S and T for each language pair?s
development set. All combinations of S = 0 . . . 4
and T = 0 . . . 7 were tested for each language
pair. Based on these experiments, we selected (for
the fixed version), values of S = 2, T = 6 for
English-Hindi, and S = 4, T = 3 for English-
Katakana.
5 Results
The results of our internal experiments with the
official evaluation tool are shown in Table 1. We
report 1-best accuracy on both development and
test sets, with both the buggy and fixed versions of
our system. As one can see, the bug makes less of
an impact in the English-Katakana setting, where
more training data is available.
70
6 Conclusion
We have demonstrated that an automatic letter-
to-phoneme transducer performs fairly well on
this transliteration shared task, with no language-
specific or transliteration-specific modifications.
Instead, we simply considered Hindi or Katakana
to be an alternate encoding for English phonemes.
In the future, we would like to investigate proper
use of multiple reference answers during percep-
tron training.
Acknowledgments
We would like to thank the NEWS 2009 Machine
Transliteration Shared Task organizers for creating
this venue for comparing transliteration methods.
We would also like to thank Chris Quirk for pro-
viding us with his alignment software.
References
Slaven Bilac and Hozumi Tanaka. 2005. Extracting
transliteration pairs from comparable corpora. In
Proceedings of the Annual Meeting of the Natural
Language Processing Society, Japan.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL,
pages 905?913, Columbus, Ohio, June.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL, pages
82?88, New York City, USA, June.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599?612.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. of
the 30th SIGIR.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report on NEWS 2009 machine
transliteration shared task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT-NAACL, pages 257?264, Boston, USA, May.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL, pages 97?105, Columbus, Ohio, June.
71
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 609?618, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
A Unified Approach to Transliteration-based Text Input                     
with Online Spelling Correction 
 
Hisami Suzuki              Jianfeng Gao 
Microsoft Research 
One Microsoft Way, Redmond WA 98052 USA 
{hisamis,jfgao}@microsoft.com 
 
Abstract 
This paper presents an integrated, end-to-end 
approach to online spelling correction for text 
input. Online spelling correction refers to the 
spelling correction as you type, as opposed to 
post-editing. The online scenario is 
particularly important for languages that 
routinely use transliteration-based text input 
methods, such as Chinese and Japanese, 
because the desired target characters cannot 
be input at all unless they are in the list of 
candidates provided by an input method, and 
spelling errors prevent them from appearing 
in the list. For example, a user might type 
suesheng by mistake to mean xuesheng ?? 
'student' in Chinese; existing input methods 
fail to convert this misspelled input to the 
desired target Chinese characters. In this 
paper, we propose a unified approach to the 
problem of spelling correction and 
transliteration-based character conversion 
using an approach inspired by the phrase-
based statistical machine translation 
framework. At the phrase (substring) level, k 
most probable pinyin (Romanized Chinese) 
corrections are generated using a monotone 
decoder; at the sentence level, input pinyin 
strings are directly transliterated into target 
Chinese characters by a decoder using a log-
linear model that refer to the features of both 
levels. A new method of automatically 
deriving parallel training data from user 
keystroke logs is also presented. Experiments 
on Chinese pinyin conversion show that our 
integrated method reduces the character error 
rate by 20% (from 8.9% to 7.12%) over the 
previous state-of-the art based on a noisy 
channel model.  
1 Introduction 
This paper addresses the problem of online 
spelling correction, which tries to correct users' 
misspellings as they type, rather than post-editing 
them after they have already been input. This 
online scenario is particularly important for 
languages that routinely use transliteration-based 
text input methods, including Chinese and 
Japanese: in these languages, characters (called 
hanzi in Chinese and kanji/kana in Japanese) are 
typically input by typing how they are pronounced 
in Roman alphabet (called pinyin in Chinese, 
romaji in Japanese), and selecting a conversion 
candidate among those that are offered by an input 
method system, often referred to as IMEs or input 
method editors. One big challenge posed by 
spelling mistakes is that they prevent the desired 
candidates from appearing as conversion 
candidates, as in Figure 1: suesheng is likely to be 
a spelling error of xuesheng?? 'student', but it is 
not included as one of the candidates.  
                  
Figure 1: Spelling mistake prevents the desired output 
(??) from appearing in the list of candidates 
This severely limits the utility of an IME, as 
spelling errors are extremely common. Speakers of 
a non-standard dialect and non-native speakers 
have a particularly hard time, because they may 
not know the standard pronunciation of the word to 
begin with, preventing them from inputting the 
word altogether. Error-tolerant word completion 
and next word prediction are also highly desirable 
features for text input on software (onscreen) 
keyboards for any language, making the current 
work relevant beyond Chinese and Japanese.  
In this paper, we propose a novel, unified 
system of text input with spelling correction, using 
609
Chinese pinyin-to-hanzi conversion as an example. 
We first formulate the task of pinyin spelling 
correction as a substring-based monotone 
translation problem, inspired by phrase-based 
statistical machine translation (SMT) systems 
(Koehn et al2003; Och and Ney, 2004): we 
consider the pinyin input (potentially with errors) 
as the source language and the error-corrected 
pinyin as the target, and build a log-linear model 
for spelling correction. In doing so, we also 
propose a novel, unsupervised method of 
collecting parallel training data from user input 
logs. We then build an integrated end-to-end text 
input system that directly converts a potentially 
erroneous input pinyin sequence into a desired 
hanzi sequence, also formulated as a monotone 
phrase-based SMT problem, in which the feature 
functions of the substring-based error correction 
component are integrated and jointly optimized 
with the sentence-level feature functions for 
character conversion  
Our method generalizes and improves over the 
previous state-of-the-art methods for the task of 
error correction and text input in several crucial 
respects. First, our error correction model is 
designed and implemented as a substring-based, 
fully trainable system based on a log-linear model, 
which has been shown effective for related tasks 
such as transliteration and letter-to-phone 
conversion, but has not been attempted for the task 
of spelling correction. Second, we build an end-to-
end pinyin-to-hanzi conversion system by 
combining all the feature functions used in the 
error correction and character conversion 
components in an SMT-style log-linear model, 
where the feature weights are trained 
discriminatively for the end-to-end task. This 
integration method generalizes the previous 
approach based on a noisy channel model (Chen 
and Lee, 2000; Zheng et al011b), in which only 
the error model and the conversion model 
probabilities are used and combined with equal 
weights. Finally, like other statistical systems, the 
amount and quality of training data control the 
quality of the outcome; we thus propose a new, 
language-independent method of deriving parallel 
data for spelling correction from user keystroke 
logs.  
We performed experiments on various methods 
of integrating the error correction and character 
conversion sub-components. Our best system, a 
fully integrated SMT-based approach, reduces the 
character error rate by 35% on test data that is 
completely independent of the creation of error 
correction and character conversion models.  
In what follows, we first give the background of 
this research in Section 2. We then describe our 
approach to the spelling correction task (Section 3) 
and the end-to-end conversion task (Section 4). We 
summarize our contribution and conclude with 
remarks for future directions in Section 5.  
2 Related Work  
The current work builds on many previous works 
on the task of monotone substring-based 
transduction, including spelling correction, letter-
to-phone conversion and transliteration between 
different scripts. In particular, our substring-based 
approach to spelling correction is motivated by the 
success on transliteration (e.g., Sherif and Kondrak, 
2007; Cherry and Suzuki, 2009) and letter-to-
phoneme conversion (e.g., Jiampojamarn et al
2007; Rama et al2009). One big challenge of the 
spelling correction research is the general lack of 
naturally occurring paired data of contextual 
spelling errors and their correction. Previous work 
has therefore either focused on the task of 
correcting out-of-vocabulary words out of context 
(e.g., Brill and Moore, 2000; Toutanova and 
Moore, 2002), or has resorted to innovative 
methods of data collection. For example, Banko 
and Brill (2001) generate data artificially by 
substituting words from a confusion word set in 
text for building a contextual speller; Whitelaw et 
al. (2009) use word frequency and edit distance 
information to harvest error pairs from a web 
corpus in an unsupervised manner; Bertoldi et al
(2010) intentionally corrupt clean text by adding 
noise to the data. Another approach to spelling 
error data collection uses web search query logs, 
available in large quantity (albeit to limited 
institutions), and limit its focus on the task of 
correcting misspelled queries (e.g., Cucerzan and 
Brill, 2004; Gao et al2010; Sun et al2010; 
Duan and Hsu, 2011). The problem of data 
collection is particularly difficult for pinyin error 
correction, as pinyin is not a final form of text in 
Chinese, so it is not recorded in final text. Zheng et 
al. (2011a) study a log of pinyin input method and 
use the backspace key to learn the user mistyping 
behavior, but they do so only for the purpose of 
610
data analysis, and do not build a statistical model 
from this data.  
Text input methods have been commercially 
available for decades for inputting Chinese and 
Japanese, but have also recently become available 
for other non-Roman script languages including 
Arabic and the languages of India.1 Early research 
work on text input methods includes e.g., Mori et 
al. (1998), Chen and Lee (2000) and Gao et al
(2002), all of which approach the problem using a 
noisy channel model. Discriminative approaches 
have also been proposed, e.g., Suzuki and Gao 
(2005); Tokunaga et al2011). There is only a 
very limited amount of work that deals with 
spelling correction in the context of text input: 
Zheng et al2011b) represents a recent work 
based on a noisy channel model, which defines our 
baseline. Their work is strictly word-based and 
only handles the correction of out-of-vocabulary 
pinyin words into in-vocabulary pinyin words, 
while our substring-based model is not limited by 
these constraints.  
The current work also has an affinity to the task 
of speech translation in that the parallel data 
between the input (speech signal) and the output 
(text in foreign language) is not directly available, 
but is mediated by a corrected (transcribed) form 
of input. Zhang et al2011) is thus relevant to our 
study, though their approach differs from ours in 
that we build an integrated system that include the 
feature functions of both error correction and 
character conversion sub-systems.  
3 Substring-based Spelling Correction 
using a Log-linear Model 
In this section, we describe our approach to pinyin 
error correction within a log-linear framework. 
Though our current target is pinyin error correction, 
the method described in this section is applicable 
to any language of interest.  
The spelling correction problem has been 
standardly formulated within the framework of 
noisy channel model (e.g., Kernighan et al1990). 
Let A be the input phonetic string in pinyin. The 
task of spelling correction is to search for the best 
                                                          
1 A few examples include Google Transliterate 
(http://www.google.com/transliterate/) and Microsoft Maren 
(http://www.microsoft.com/middleeast/egypt/cmic/maren/) / 
ILIT (http://specials.msn.co.in/ilit/Hindi.aspx). Quillpad 
(http://quillpad.in/) is also popularly used in India. 
correction candidate in pinyin C* among all 
possible corrections for each potentially misspelled 
pinyin A: 
         
        
   |                                               
Applying Bayes' Rule and dropping the constant 
denominator, we have 
         
        
   |                                         
where the error model    |  models the 
translation probability from C to A, and the 
language model      models how likely the 
output C is a correctly spelled pinyin sequence. 
Many variations on the error model have been 
proposed, including substring-based (Brill and 
Moore, 2000) and pronunciation-based (Toutanova 
and Moore, 2002) models.  
Our model is inspired by the SMT framework, 
in which the error correction probability    |   of 
Equation (1) is directly modeled using a log-linear 
model of the following form:   
   |   
 
    
   ?                               
 
 
where Z(A) is the normalization factor, hi is a 
feature function and ?i is the feature weight. 
Similarly to phrase-based SMT, many feature 
functions are derived from the translation and 
language models, where the translation model-
derived features are trained using a parallel corpus 
of original pinyin and correction pairs. The argmax 
of Equation (1) defines the search operation: we 
use a left-to-right beam search decoder to seek for 
each input pinyin the best correction according to 
Equation (3).   
We first describe how the paired data for 
deriving the error model probabilities is generated 
from user logs in Section 3.1, and then how the 
models are trained and the model weights are 
learned in Section 3.2. We discuss the results of 
pinyin error correction as an independent task in 
Section 3.3.  
3.1 Generating error correction pairs from 
keystroke logs 
Unlike English text, which includes instances of 
misspelled words explicitly, pinyin spelling errors 
are not found in a corpus, because pinyin is used as 
a means of inputting text, and is not part of the 
611
final written form of the language. Therefore, 
pinyin error correction pairs must be created 
intentionally. We chose the method of 
implementing a version of an input method which 
records the keystrokes of users while they are 
asked to type a particular Chinese text in hanzi; in 
doing so, we captured each keystroke issued by the 
user behind the scene. Such keystroke logs include 
the use of the backspace key, from which we 
compute the pinyin strings after the usage of the 
backspace keys as well as the putative pinyin string 
had the user not corrected it using the backspace 
key.2 Table 1 shows a few examples of the entries 
in the keystroke log, along with the computed 
pinyin strings before and after correction. Each 
entry (or phrase) in the log represents the unit that 
corresponds to the sequence the user input at once, 
at the end of which the user committed to a 
conversion candidate, which typically consists of 
one or more words. While the post-correction 
string can be straightforwardly derived by deleting 
the same number of characters preceding the 
backspaces, the computation of the pre-correction 
string is trickier and ambiguous, because the 
backspace key is used for the purpose of both 
deletion and substitution (delete and replace) 
operations. In Table 1, a backspace usage is 
indicated by _ in the original keystroke sequence 
that is logged. In the second example, a deletion 
interpretation will generate zhonguo as a pre-
correction string, while substitution interpretation 
will generate zhonguoo. In order to recover the 
desired pre-correcting string, we compared the 
prefix of the backspace usage (zhonguo) with the 
substrings after error correction (zhong, zhongg, 
zhonggu?). We considered that the prefix was 
spell-corrected into the substring which is the 
longest and with the smallest edit distance: in this 
case, zhonguo is considered an error for 
zhongguo, therefore recovering the pre-correction 
string of the whole sequence as zhonguo. Note 
that this method of error data extraction is general 
                                                          
2 Zheng et al2011a) also uses the backspace key in the IME 
log to generate error-correction pairs, but they focus on the 
usage of a backspace after the desired hanzi characters have 
been input, i.e., the backspace key is used to delete one or 
more hanzi characters. In contrast, our method focuses on the 
use of backspace to delete one or more pinyin characters 
before conversion. This simulates the scenario of online error 
correction more truthfully, and can collect paired data in large 
quantity faster.  
and is language-independent. Since paired error 
correction data do not exist naturally and is 
expensive to collect for any language, we believe 
that the proposed method is useful beyond the case 
of Chinese text input and applicable to the data 
collection of the spelling correction task in general. 
In a related work (Baba and Suzuki, 2012), we 
collected such keystroke data using Amazon's 
Mechanical Turk for English and Japanese, and 
released the error-correction pairs for research 
purposes.3  
The extracted pairs are still quite noisy, because 
one error correction behavior might not completely 
eliminate the errors in typing a word. For example, 
in trying to type women ?? 'we', a user might 
first type wmen, hit the backspaces key four times, 
retype womeen, and commit to a conversion 
candidate by mistake. We extract the pair (wmen, 
womeen) from this log incorrectly, which is one of 
the causes of the noise in the data. Despite these 
remaining errors, we use the data without further 
cleaning, as we expect our approach to be robust 
against a certain amount of noise.  
Keystroke data was collected for three text 
domains (chat, blog and online forum) from 60 
users, resulting in 86,783 pairs after removing 
duplicates. The data includes the pairs with the 
same source and target, with about 41% 
representing the case of correction. We used 5,000 
pairs for testing, 1,000 pairs for tuning the log-
linear model weights (see the next subsection), and 
the remaining portion for training the error 
correction component.  
3.2 Training the log-linear model 
The translation model captures substring-based 
spelling error patterns and their transformation 
probabilities. The model is learned from large 
amounts of pinyin-correction pairs mined from 
user keystroke logs discussed above. Take the 
                                                          
3 Available at http://research.microsoft.com/en-
us/downloads/4eb8d4a0-9c4e-4891-8846-
7437d9dbd869/default.aspx.  
keystroke pre-
correction 
post-
correction 
n a n s _ r e n nansen nanren 
z h o n g u o _ _ g u o zhonguo 
(*zhonguoo) 
zhongguo 
Table 1: Computation of pre- and post-correction 
strings from keystroke log 
612
following pinyin-correction pair as an example, 
where the input pinyin and its correction are 
aligned at the character level: given a pair (A,C), 
we align the letters in A with those in C so as to 
minimize the edit distance between A and C based 
on single character insertions, deletions and 
substitutions. 
 
From this pair, we learn a set of error patterns that 
are consistent with the character alignment,4 each 
of which is a pair of substrings indicating how the 
spelling is transformed from one to another. Some 
examples of extracted phrases are (wanmian, 
waimian) and (andshi, andeshi). In our 
implementation, we extract all patterns with a 
substring length of up to 9 characters. We then 
learn the translation probabilities for each pair 
using maximum likelihood estimation (MLE). Let 
(a,c) denote a pair. For each pair, we learn the 
translation probabilities P(c|a) and P(a|c), 
estimated using MLE, as well as lexical weights in 
two directions following Koehn et al2003).  Our 
error correction model is completely substring-
based and does not use a word-based lexicon, 
which gives us the flexibility of generating unseen 
correction targets as well as supporting pinyin 
input consisting of multiple words at a time. For 
the language model, we use a character 9-gram 
model to capture the knowledge of correctly 
spelled pinyin words and phrases. We trained the 
language model using the target portion of the 
parallel data described in Section 3.1, though it is 
possible to train it with an arbitrary text in pinyin 
when such data is available.  
In addition to the feature functions derived from 
the error and language models, we also use word 
and phrase penalties as feature functions, which are 
commonly used in SMT. These features also make 
sense in the current context, as using fewer phrase 
means encouraging longer ones with more context, 
and the target character length can capture 
tendencies to delete or insert words in errors. 
                                                          
4 Consistency here implies two things. First, there must be at 
least one aligned character pair in the aligned phrase. Second, 
there must not be any alignments from characters inside the 
aligned phrase to characters outside the phrase. That is, we do 
not extract a phrase pair if there is an alignment from within 
the phrase pair to outside the phrase pair. 
Overall, the log-linear model uses 7 feature 
functions: 4 derived from the translation models, 
word and phrase penalties, and the language model. 
The model weights were trained using the 
minimum error rate training algorithm (MERT, 
Och, 2003). We tried MERT with two objective 
functions: one that uses the 4-gram BLEU score as 
straightforwardly adapted from SMT, and the other 
that minimizes the character error rate (CER). CER 
is based on the edit distance between the reference 
and system output, which is used for evaluating the 
IME accuracy (Section 4.3). It is more directly 
related with the word/phrase-level accuracy, which 
we used to evaluate the error correction module in 
isolation, than the BLEU metric. As we will show 
below, however, using different objective 
functions turned out to have only a minimal impact 
on the spelling correction accuracy.  
3.3 Experiments and results 
The performance of pinyin error correction was 
evaluated on two data sets: (1) log-test: the test set 
of the data in Section 3.1, which is derived in the 
same way as the training data but is noisy, 
consisting of 5,000 phrases of which 2,020 are 
misspelled; (2) CHIME: the gold standard from the 
CHIME data set made available by Zheng et al
(2011b), 5  which is also used in the end-to-end 
evaluation in Section 4. This data set consists of 
2,000 sentence pairs of pinyin input with errors 
and the target hanzi characters, constructed by 
collecting actual user typing logs of the Lancaster 
corpus (McEnery and Xiao, 2004), which includes 
text from newspaper, fiction, and essays. 6  The 
CHIME data set does not include the corrected 
pinyin string; we therefore generated this by 
running a text-to-pinyin utility, 7  and created the 
pairs before and after error correction for 
evaluating our pinyin spelling correction module. 
The set contains 11,968 words of which 908 are 
misspelled. 
The results of the evaluation are given in Table 
2. They are for phrase/word-level accuracy, as the 
log-derived data set is for each phrase (a user-
                                                          
5 Available from http://chime.ics.uci.edu/ 
6 Details on the Lancaster corpus are found at 
http://www.lancs.ac.uk/fass/projects/corpus/LCMC/.  
7 We used an in-house tool, but many tools are available 
online. Unlike pinyin-to-hanzi, hanzi-to-pinyin is relatively 
straightforward as most characters have a unique 
pronunciation. 
613
defined unit of conversion, consisting of one to a 
few words), while the CHIME data set is word-
segmented. The baseline accuracy is the accuracy 
of not correcting any error, which is very strong in 
this task: 59.6% and 92.41% for the two data sets, 
respectively. The accuracy on the log-test data is 
generally much lower than the CHIME data, 
presumably because the latter is cleaner, contains 
less errors to begin with, and the unit of evaluation 
is smaller (word) than the log-test (phrase). 
Though CHIME is an out-of-domain data set, the 
proposed model works very well on this set, 
achieving more than 93% accuracy with the best 
output, significantly (at p<0.001 using McNemar's 
test) improving on the strong baseline of not 
correcting any error. The proposed log-linear 
approach is also compared against the noisy 
channel model baseline, which is simulated by 
only using one error model-derived feature 
function    |   and the language model, weighted 
equally, using the same beam search decoder. 
Somewhat surprisingly, the noisy channel model 
results fall below the baseline in both data sets, 
while the log-linear model improves over the 
baseline, especially on the 1-best accuracy: all 
differences between the noisy channel model and 
the log-linear model outputs are significant. Finally, 
regarding the effect of using the CER as the 
objective function of MERT, we only observe 
minimal impact: none of the differences in 
accuracy between the BLEU and CER objectives is 
statistically significant on either data set. For a 
monotone decoding task such as spelling 
correction, using either objective function therefore 
seems to suffice, even though BLEU is more 
indirect and redundant in capturing the phrase-
level accuracy.   
4 A Unified Model of Character 
Conversion with Spelling Correction  
In this section we describe our unified model of 
spelling correction and transliteration-based 
character conversion. Analogous to the spelling 
correction task, the character conversion problem 
can also be considered as a substring-based 
translation problem. The novelty of our approach 
lies in the fact that we take advantage of the 
parallelism between these tasks, and build an 
integrated model that performs spelling correction 
and character conversion at the same time, within 
the log-linear framework. This allows us to 
optimize the feature weights directly for the end 
goal, from which from we can expect a better 
overall conversion accuracy.  
4.1 Noisy channel model approach to 
incorporating error correction in 
character conversion 
The task of pinyin-to-hanzi conversion consists of 
converting the input phonetic strings provided by 
the user into the appropriate word string using 
ideographic characters. This has been formulated 
within the noisy channel model (Chen and Lee, 
2000), in exactly the same manner as the spelling 
correction, as describe in Equations (1) and (2) in 
Section 3. Given the pinyin input A, the task is to 
find the best output hanzi sequence W*: 
  
       
        
   |                                                     
       
        
       |   
In traditional conversion systems which do not 
consider spelling errors, P(A|W) is usually set to 1 
if the word is found in a dictionary of word-
pronunciation pairs, which also defines GEN(A). 
Therefore, the ranking of the candidates relies 
exclusively on the language model probability 
P(W).  
An extension of this formulation to handle 
spelling errors can be achieved by incorporating an 
actual error model P(A|W). Assuming a conditional 
independence of A and W given the error-corrected 
pinyin sequence C, Equation (4) can be re-written 
as: 
 1-best 3-best 20-best 
log-test: No correction  59.6   
log-test: Noisy Channel 49.5 67.86 84.8 
log-test: Proposed (BLEU) 62.46 74.58 86.66 
log-test: Proposed (CER) 62.82 75.06 86.8 
CHIME: No correction 92.41   
CHIME: Noisy Channel 91.29 95.75 98.82 
CHIME: Proposed (BLEU) 93.51 97.38 99.06 
CHIME: Proposed (CER) 93.49 97.29 99.08 
Table 2: Pinyin error correction accuracy (in %) 
614
         
 
   |  
       
 
?   |     |                                      
 
       
 
?   |         |  
 
 
Here, P(C|W) corresponds to the channel model of 
traditional input methods, P(W) the language 
model, and P(C|A) the pinyin error correction 
model. There have been attempts to use this 
formulation in text input: for example, Chen and 
Lee (2000) trained a syllable-based model for 
P(C|A) with user keystroke data,8 and Zheng et al
(2011b) used a model based on a weighted 
character edit distance whose weights are manually 
assigned. This noisy channel integration of error 
correction and character conversion is the state-of-
the-art in the task of error-correcting text input, 
and will serve as our baseline.  
4.2 Log-linear model for error-correcting 
character conversion 
 Similar to the formulation of our error correction 
model in Section 3, we adopt the log-linear model 
for modeling the character conversion probability 
in (4):  
   |   
 
    
   ?         
 
 
where A = a1,?,an is a sequence of phrases in 
pinyin, and W = w1,?,wn is the corresponding 
sequence in hanzi. A unique challenge of the 
current task is that the parallel data for A and W do 
not exist directly. Therefore, we generated the 
translation phrase table offline by merging the 
                                                          
8 No detail of this data is available in Chen and Lee (2000).  
substring-based phrase table generated for the 
pinyin error correction task in Section 3 with the 
results of character conversion. This process is 
described in detail in Figure 2: k-best candidates 
for each input pinyin phrase a are generated by the 
error model in Section 3, which are then submitted 
offline to an IME system to obtain n-best 
conversion candidates with probabilities. For the 
IME system, we used an in-house conversion 
system, which only uses a word trigram language 
model for ranking. In the resulting translation table, 
defined for each (a, w) pair, the feature functions 
and their values are inherited from the pinyin error 
correction translation table mediated by the 
correction candidates c1?k for a, plus the function 
that defines the IME conversion probability for (cj, 
w). Note that in this final phrase table, the 
correction candidates for a are latent, only 
affecting the values of the feature functions.9 The 
final end-to-end system uses the following 11 
features:  
- 7 error correction model features at the phrase 
level  
- IME conversion probability at the phrase level 
- language model probability at the sentence level 
- word/phrase penalty features at the sentence 
level 
The language model at the sentence level is trained 
on a large monolingual corpus of Chinese in hanzi, 
consisting of about 13 million sentences (176 
million words). The IME conversion probability 
                                                          
9 The final phrase table needs to be unique for each phrase pair 
(a, w), though the process described here results in multiple 
entries with the same pair having different feature values, 
because the generation of (a, w) is mediated by multiple 
correction candidates c1?k. These entries need to be added up 
to remove duplicates; we used a heuristic approximation of 
taking the pair where a equals cj (i.e., no spelling correction) 
when multiple entries are found.  
c1  xuesheng f1 ... f7
c2  xueshereng   f1 ... f7
c3  xueshusheng  f1 ... f7
...
+
c1  xuesheng    w1 ?? 1
c2  xueshereng w1 ??? 0.103
        w2 ??? 0.101
        w3 ??? 0.101
             ...
c3  xueshusheng  w1 ??? 0.102 
            w2 ??? 0.101 
            w3 ??? 0.101
           ...
...
?
w11 xueshseng ?? f1 ... f7 1
w21 xueshseng ??? f1 ... f7 0.103
w22 xueshseng ??? f1 ... f7 0.101
w23 xueshseng ??? f1 ... f7 0.101
...
w31 xueshseng ??? f1 ... f7 0.102
w32 xueshseng ??? f1 ... f7 0.101
w33 xueshseng ??? f1 ... f7 0.101
...
k-best error correction candidates c1...k 
n-best IME conversion 
candidates w1...n for c1...k
combined translation table w11...kn
 
Figure 2: Generation of integrated translation table for the pinyin input a = xueshseng 
615
also uses a word trigram model, but it is trained on 
a different data set which we did not have access 
to; we therefore used both of these models. The 
values for k and n can be determined empirically; 
we used 20 for both of them. 10  This generates 
maximally 400 conversion candidates for each 
input pinyin.  
The feature weights of the log-linear model are 
tuned using MERT. As running MERT on a CER-
based target criterion on the similar, monotone 
translation task of spelling correction did not lead 
to a significant improvement (Section 3.3), we 
simply report the results of using the 4-gram 
BLEU as the training criterion in this task.  
4.3 Experiments and results 
For the evaluation of the end-to-end conversion 
task, we used the CHIME corpus mentioned above. 
In order to use the word trigram language model 
that is built in-house, we re-segmented the CHIME 
corpus using our word-breaker, resulting in 12,102 
words in 2,000 sentences. We then divided the 
sentences in the corpus randomly into two halves, 
and performed a two-fold cross validation 
evaluation. The development portion of the data is 
used to tune the weights of the feature functions in 
MERT-style training. We measured our results 
using character error rate (CER), which is based on 
the longest common subsequence match in 
characters between the reference and the best 
system output. This is a standard metric used in 
evaluating IME systems (e.g., Mori et al1998; 
Gao et al2002). Let NREF be the number of 
characters in a reference sentence, NSYS be the 
character length of a system output, and NLCS be 
the length of the longest common subsequence 
between them. Then the character-level recall is 
defined as NLCS/NREF, and the precision as NLCS/NSYS. 
The CER based on recall and on precision are then 
defined as 1 ? recall and 1 ? precision, respectively. 
We report the harmonic mean of these values, 
similarly to the widely used F1-measure. 
As our goal is to show the effectiveness of the 
unified approach, we used simpler methods of 
integrating pinyin error correction with character 
conversion to create baselines. The simplest 
                                                          
10 From Table 2, we observe that the accuracy of the 20-best 
output of the spelling correction component is over 99%. An 
offline run with the IME system on an independent data set 
also showed that the accuracy of the 20-best IME output is 
over 99%.  
baseline is a pre-processing approach: we use the 
pinyin error correction model to convert A into a 
single best candidate C, and run an IME system on 
C. Another more realistic baseline is the noisy 
channel integration discussed in Section 4.1. We 
approximated this integration method by re-
ranking all the candidates generated by the 
proposed log-linear model with only the channel 
and language model probabilities, equally 
weighted.  
The results are shown Table 3. 5-best results as 
well as the 1-best results are shown, because in an 
IME application, providing the correct candidate in 
the candidate list is particularly important even if it 
is not the best candidate. Let us first discuss the 1-
best results. The CER of this test corpus using the 
in-house IME system without correcting any errors 
is 10.91. The oracle CER, which is the result of 
applying the IME on the gold standard pinyin input 
derived from the reference text using a hanzi-to-
pinyin converter (as mentioned in Section 3.3), is 
4.08, which is the upper-bound imposed by the 
IME conversion accuracy. The simple pipeline 
approach of concatenating the pinyin correction 
component with the character conversion 
component improves the CER by 1% to 9.93. 
Assuming that there are on average 20 words in a 
sentence, and each word consists of 2 characters, 
1% CER reduction means one improvement every 
2.5 sentences. Noisy channel integration improves 
over this quite substantially, achieving a CER of 
7.92, demonstrating the power of the word 
language model in character conversion. 
Incidentally, the CER of the output by Zheng et al
(2011b)'s model is 8.90.11 Their results are not as 
good as our noisy channel integration, as their 
system uses a manually defined error model and a 
word bigram language model. With the use of 
additional feature functions weighted 
discriminatively for the final conversion task, the 
                                                          
11 Available at http://chime.ics.uci.edu/. 
 CER on 
1-best 
CER on 
5-best 
Baseline: No correction 10.91 7.76 
Baseline: Pre-processing 9.93 6.75 
Baseline: Zheng et al2011b) 8.90  
Baseline: Noisy channel 7.92 3.93 
Proposed: SMT model 7.12 3.63 
Oracle 4.08 1.51 
Table 3: CER results for the conversion task (%) 
616
proposed method outperforms all these baselines to 
reduce the CER to 7.12, a 35% relative error rate 
reduction compared with the no correction baseline, 
a 20% reduction against Zheng et al011b) and a 
10% reduction from our noisy channel baseline. 
The 5-best results follow the same trend of steady 
improvement as we use a more integrated system.  
In order to understand the characteristics of the 
errors and remaining issues, we ran an error 
analysis on the 1-best results of the proposed 
system. For each word in the test data (all 2,000 
sentences) for which the system output had an 
error, we classified the reasons of failure into one 
of the four categories: (1) character conversion 
error: correct pinyin was input to the IME but the 
conversion failed; (2) over-correction of pinyin 
input: the system corrected the pinyin input when 
it should not have; (3) under-correction of pinyin 
input: the system did not correct an error in the 
input pinyin when it should have; (4) wrong 
correction: input pinyin string had a spelling error 
but it was corrected incorrectly.   
Table 4 shows the results of the error analysis. 
We find that somewhat contrary to our expectation, 
over-correction of the spelling mistakes was not a 
conspicuous problem, even though the pinyin 
correction rate of the training data is much higher 
than that of the test data. We therefore conclude 
that the error correction model adapts very well to 
the characteristics of the test data in our integrated 
SMT-based approach, which trains the unified 
feature weights to optimize the end goal.  
5 Conclusion and Future Work 
In this paper we have presented a unified approach 
to error-tolerant text input, inspired by the phrase-
based SMT framework, and demonstrated its 
effectiveness over the traditional method based on 
the noisy channel model. We have also presented a 
new method of automatically collecting parallel 
data for spelling correction from user keystroke 
logs, and showed that the log-linear model works 
well on the task of spelling correction in isolation 
as well.  
In this study, we isolated the problem of spelling 
errors and studied the effectiveness of error 
correction over a basic IME system that does not 
include advanced features such as abbreviated 
input (e.g., typing only "py" for ?? pengyou 
'friend' or ?? pinyin in Chinese) and auto-
completion (e.g., typing only "ari" for ????? 
arigatou 'thank you' in Japanese). Integrating data-
driven error correction feature with these advanced 
features for the benefit of users is the challenge we 
face in the next step.  
Acknowledgements 
We are indebted to many colleagues at Microsoft 
and MSR for their help in conducting this research, 
particularly to Xi Chen, Pallavi Choudhury, Chris 
Quirk, Mei-Yuh Hwang and Kristina Toutanova. 
We are also grateful for the comments we received 
from the reviewers of this paper.  
References  
Baba, Y. and H. Suzuki. 2012. How are spelling errors 
generated and corrected? A study of corrected and 
uncorrected spelling errors using keystroke logs. In 
Proceedings of ACL. 
Banko, M. and E. Brill. 2001. Scaling to very very large 
corpora for natural language disambiguation. In 
Proceedings of ACL.  
Bertoldi, N., M. Cettolo, and M. Federico. 2010. 
Statistical machine translation of texts with 
misspelled words. In Proceedings of HLT-NAACL.  
Brill, E., and R. C. Moore. 2000. An improved error 
model for noisy channel spelling correction. In 
Proceedings of ACL.  
Chen, Z., and K. F. Lee. 2000. A new statistical 
approach to Chinese Pinyin input. In Proceedings of 
ACL.  
Cherry, C., and H. Suzuki. 2009. Discriminative 
substring decoding for transliteration. In Proceedings 
of EMNLP. 
Cucerzan, S., and E. Brill. 2004. Spelling correction as 
an iterative process that exploits the collective 
knowledge of web users. In Proceedings of EMNLP.  
Duan, H., and P. Hsu. 2011. Online spelling correction 
for query completion. In Proceedings of WWW.  
Gao, J., J. Goodman, M. Li and K.-F. Lee. 2002. 
Toward a unified approach to statistical language 
modeling for Chinese. In ACM Transactions on 
Overall errors (words) 1,074 / 12,102 
  Conversion 646 (60.14%) 
  Over-corrections 155 (14.43%) 
  Under-correction 161 (14.99%) 
  Wrong correction 112 (10.42%) 
Table 4: Classification of errors  
617
Asian Language Information Processing, Vol. 1, No. 
1, pp 3-33. 
Gao, J., X. Li, D. Micol, C. Quirk and X. Sun. 2010. A 
large scale ranker-based system for search query 
spelling correction. In Proceedings of COLING. 
Jiampojamarn, S., G. Kondrak and T. Sherif, 2007. 
Applying many-to-many alignments and hidden 
markov models to letter-to-phoneme conversion. In 
Proceedings of HLT/NAACL. 
Kernighan, M., K. Church, and W. Gale. 1990. A 
spelling correction program based on a noisy channel 
model. In Proceedings of COLING. 
Koehn, P., F. Och and D. Marcu. 2003. Statistical 
phrase-based translation. In Proceedings of HLT-
NAACL.  
McEnery, A. and Xiao, Z. 2004. The Lancaster Corpus 
of Mandarin Chinese: A Corpus for Monolingual and 
Contrastive Language Study. In Proceedings of 
LREC.  
Mori, S., M. Tsuchiya, O. Yamaji and M. Nagao. 1998. 
Kana-kanji conversion by a stochastic model. In 
Proceedings of Information Processing Society of 
Japan, SIG-NL-125-10 (in Japanese). 
Och, F. J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
ACL.  
Och, F., and Ney, H. 2004. The alignment template 
approach to statistical machine translation. 
Computational Linguistics, 30(4): 417-449. 
Rama, T., A. K. Singh and S. Kolachina. 2009. 
Modeling letter-to-phoneme conversion as a phrase 
based statistical machine translation problem with 
minimum error rate training. In Proceedings of the 
NAACL HLT Student Research Workshop and 
Doctoral Consortium. 
Sherif, T. and G. Kondrak. 2007. Substring-based 
transliteration. In Proceedings of ACL. 
Sun, X., J. Gao, D. Micol and C. Quirk. 2010. Learning 
phrase-based spelling error models from clickthrough 
data. In Proceedings of ACL.  
Suzuki, H. and J. Gao. 2005. A comparative study on 
language model adaptation using new evaluation 
metrics. In Proceedings of EMNLP.  
Toutanova, K., and R. C. Moore. 2002. Pronunciation 
modeling for improved spelling correction. In 
Proceedings of ACL.  
Tokunaga, H., D. Okanohara and S. Mori. 2011. 
Discriminative method for Japanese kana-kanji input 
method. In Proceedings of the Workshop on 
Advances in Text Input Methods (WTIM 2011).  
Whitelaw, C., B. Hutchinson, G. Y. Chung, and G. 
Ellis. 2009. Using the web for language independent 
spellchecking and autocorrection. In Proceedings of 
ACL. 
Zhang, Y., L. Deng, X. He and A. Acero. 2011. A novel 
decision function and the associated decision-
feedback learning for speech translation. In 
Proceedings of ICASSP.  
Zheng, Y., L. Xie, Z. Liu, M. Sun. Y. Zhang and L. Ru. 
2011a. Why press backspace? Understanding user 
input behaviors in Chinese pinyin input method. In 
Proceedings of ACL.  
Zheng, Y., C. Li and M. Sun. 2011b. CHIME: An 
efficient error-tolerant Chinese pinyin input method. 
In Proceedings of IJCAI.  
 
618
Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21?24,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
MSR SPLAT, a language analysis toolkit 
 
Chris Quirk, Pallavi Choudhury, Jianfeng 
Gao, Hisami Suzuki, Kristina Toutanova, 
Michael Gamon, Wen-tau Yih, Lucy 
Vanderwende 
Colin Cherry 
Microsoft Research National Research Council Canada 
Redmond, WA 98052 USA 1200 Montreal Road 
 Ottawa, Ontario K1A 0R6 
{chrisq, pallavic, jfgao, 
hisamis, kristout, 
mgamon,scottyih, 
lucyv@microsoft.com} 
colin.cherry@nrccnrc.gc.ca 
 
Abstract 
We describe MSR SPLAT, a toolkit for lan-
guage analysis that allows easy access to the 
linguistic analysis tools produced by the NLP 
group at Microsoft Research. The tools in-
clude both traditional linguistic analysis tools 
such as part-of-speech taggers, constituency 
and dependency parsers, and more recent de-
velopments such as sentiment detection and 
linguistically valid morphology. As we ex-
pand the tools we develop for our own re-
search, the set of tools available in MSR 
SPLAT will be extended. The toolkit is acces-
sible as a web service, which can be used 
from a broad set of programming languages. 
1 Introduction 
The availability of annotated data sets that have 
become community standards, such as the Penn 
TreeBank (Marcus et al, 1993) and PropBank 
(Palmer et al, 2005), has enabled many research 
institutions to build core natural language pro-
cessing components, including part-of-speech tag-
gers, chunkers, and parsers. There remain many 
differences in how these components are built, re-
sulting in slight but noticeable variation in the 
component output. In experimental settings, it has 
proved sometimes difficult to distinguish between 
improvements contributed by a specific component 
feature from improvements due to using a differ-
ently-trained linguistic component, such as tokeni-
zation. The community recognizes this difficulty, 
and shared task organizers are now providing ac-
companying parses and other analyses of the 
shared task data. For instance, the BioNLP shared 
task organizers have provided output from a num-
ber of parsers1, alleviating the need for participat-
ing systems to download and run unfamiliar tools. 
On the other hand, many community members 
provide downloads of NLP tools2 to increase ac-
cessibility and replicability of core components.  
Our toolkit is offered in this same spirit. We 
have created well-tested, efficient linguistic tools 
in the course of our research, using commonly 
available resources such as the PTB and PropBank. 
We also have created some tools that are less 
commonly available in the community, for exam-
ple linguistically valid base forms and semantic 
role analyzers. These components are on par with 
other state of the art systems. 
We hope that sharing these tools will enable 
some researchers to carry out their projects without 
having to re-create or download commonly used 
NLP components, or potentially allow researchers 
to compare our results with those of their own 
tools. The further advantage of designing MSR 
SPLAT as a web service is that we can share new 
components on an on-going basis. 
2 Parsing Functionality 
2.1 Constituency Parsing 
                                                          
1 See www-tsujii.is.s.u-tokyo.ac.jp/GENIA/SharedTask  for 
the description of other resources made available in addition to 
the shared task data. 
2 See, for example, http://nlp.stanford.edu/software; 
http://www.informatics.sussex.ac.uk/research/groups/nlp/rasp; 
http://incubator.apache.org/opennlp 
21
The syntactic parser in MSR SPLAT attempts to 
reconstruct a parse tree according the Penn Tree-
Bank specification (Marcus et al, 1993). This rep-
resentation captures the notion of labeled syntactic 
constituents using a parenthesized representation. 
For instance, the sentence ?Colorless green ideas 
sleep furiously.? could be assigned the following 
parse tree, written in the form of an S expression: 
(TOP (S 
   (NP (JJ Colorless) (JJ green) (NNS ideas)) 
   (VP (VB sleep) (ADVP (RB furiously))) 
   (. .))) 
For instance, this parse tree indicates that ?Color-
less green ideas? is a noun phrase (NP), and ?sleep 
furiously? is a verb phrase (VP). 
Using the Wall Street Journal portion of the 
Penn TreeBank, we estimate a coarse grammar 
over the given grammar symbols. Next, we per-
form a series of refinements to automatically learn 
fine-grained categories that better capture the im-
plicit correlations in the tree using the split-merge 
method of Petrov et al (2006). Each input symbol 
is split into two new symbols, both with a new 
unique symbol label, and the grammar is updated 
to include a copy of each original rule for each 
such refinement, with a small amount of random 
noise added to the probability of each production 
to break ties. We estimate new grammar parame-
ters using an accelerated form of the EM algorithm 
(Salakhutdinov and Roweis, 2003). Then the low-
est 50% of the split symbols (according to their 
estimated contribution to the likelihood of the data) 
are merged back into their original form and the 
parameters are again re-estimated using AEM. We 
found six split-merge iterations produced optimal 
accuracy on the standard development set. 
The best tree for a given input is selected ac-
cording to the max-rule approach (cf. Petrov et al 
2006). Coarse-to-fine parsing with pruning at each 
level helps increase speed; pruning thresholds are 
picked for each level to have minimal impact on 
development set accuracy. However, the initial 
coarse pass still has runtime cubic in the length of 
the sentence. Thus, we limit the search space of the 
coarse parse by closing selected chart cells before 
the parse begins (Roark and Hollingshead, 2008). 
We train a classifier to determine if constituents 
may start or end at each position in the sentence. 
For instance, constituents seldom end at the word 
?the? or begin at a comma. Closing a number of 
chart cells can substantially improve runtime with 
minimal impact on accuracy. 
2.2 Dependency Parsing 
The dependency parses produced by MSR SPLAT 
are unlabeled, directed arcs indicating the syntactic 
governor of each word. 
These dependency trees are computed from the 
output of the constituency parser. First, the head of 
each non-terminal is computed according to a set 
of rules (Collins, 1999). Then, the tree is flattened 
into maximal projections of heads. Finally, we in-
troduce an arc from a parent word p to a child 
word c if the non-terminal headed by p is a parent 
of the non-terminal headed by c. 
2.3 Semantic Role Labeling 
The Semantic Role Labeling component of MSR 
SPLAT labels the semantic roles of verbs accord-
ing to the PropBank specification (Palmer et al, 
2005). The semantic roles represent a level of 
broad-coverage shallow semantic analysis which 
goes beyond syntax, but does not handle phenome-
na like co-reference and quantification.  
For example, in the two sentences ?John broke 
the window? and ?The window broke?, the phrase 
the window will be marked with a THEME label. 
Note that the syntactic role of the phrase in the two 
sentences is different but the semantic role is the 
same. The actual labeling scheme makes use of 
numbered argument labels, like ARG0, ARG1, ?, 
ARG5 for core arguments, and labels like ARGM-
TMP,ARGM-LOC, etc. for adjunct-like argu-
ments. The meaning of the numbered arguments is 
verb-specific, with ARG0 typically representing an 
agent-like role, and ARG1 a patient-like role. 
This implementation of an SRL system follows 
the approach described in (Xue and Palmer, 04), 
and includes two log-linear models for argument 
identification and classification. A single syntax 
tree generated by the MSR SPLAT split-merge 
parser is used as input. Non-overlapping arguments 
are derived using the dynamic programming algo-
rithm by Toutanova et al (2008).  
3 Other Language Analysis Functionality 
3.1 Sentence Boundary / Tokenization 
22
This analyzer identifies sentence boundaries and 
breaks the input into tokens. Both are represented 
as offsets of character ranges. Each token has both 
a raw form from the string and a normalized form 
in the PTB specification, e.g., open and close pa-
rentheses are replaced by -LRB- and -RRB-, re-
spectively, to remove ambiguity with parentheses 
indicating syntactic structure. A finite state ma-
chine using simple rules and abbreviations detects 
sentence boundaries with high accuracy, and a set 
of regular expressions tokenize the input. 
3.2 Stemming / Lemmatization 
We provide three types of stemming: Porter stem-
ming, inflectional morphology and derivational 
morphology. 
3.2.1 Stems  
The stemmer analyzer indicates a stem form for 
each input token, using the standard Porter stem-
ming algorithm (Porter, 1980). These forms are 
known to be useful in applications such as cluster-
ing, as the algorithm assigns the same form ?dai? 
to ?daily? and ?day?, but as these forms are not 
citation forms of these words, presentation to end 
users is known to be problematic. 
3.2.2 Lemmas 
The lemma analyzer uses inflectional morphology 
to indicate the dictionary lookup form of the word. 
For example, the lemma of ?daily? will be ?daily?, 
while the lemma of ?children? will be ?child?. We 
have mined the lemma form of input tokens using 
a broad-coverage grammar NLPwin (Heidorn, 
2000) over very large corpora. 
3.2.3 Bases  
The base analyzer uses derivational morphology to 
indicate the dictionary lookup form of the word; as 
there can be more than one derivation for a given 
word, the base type returns a list of forms. For ex-
ample, the base form of ?daily? will be ?day?, 
while the base form of ?additional? will be ?addi-
tion? and ?add?. We have generated a static list of 
base forms of tokens using a broad-coverage 
grammar NLPwin (Heidorn, 2000) over very large 
corpora. If the token form has not been observed in 
those corpora, we will not return a base form. 
3.3 POS tagging 
We train a maximum entropy Markov Model on 
part-of-speech tags from the Penn TreeBank. This 
optimized implementation has very high accuracy 
(over 96% on the test set) and yet can tag tens of 
thousands of words per second. 
3.4 Chunking 
The chunker (Gao et al, 2001) is based on a Cas-
caded Markov Model, and is trained on the Penn 
TreeBank. With state-of-the-art chunking accuracy 
as evaluated on the benchmark dataset, the chunker 
is also robust and efficient, and has been used to 
process very large corpora of web documents. 
4 The Flexibility of a Web Service 
By making the MSR SPLAT toolkit available as a 
web service, we can provide access to new tools, 
e.g. sentiment analysis. We are in the process of 
building out the tools to provide language analysis 
for languages other than English. One step in this 
direction is a tool for transliterating between Eng-
lish and Katakana words. Following Cherry and 
Suzuki (2009), the toolkit currently outputs the 10-
best transliteration candidates with probabilities for 
both directions.  
Another included service is the Triples analyz-
er, which returns the head of the subject, the verb, 
and the head of the object, whenever such a triple 
is encountered. We found this functionality to be 
useful as we were exploring features for our sys-
tem submitted to the BioNLP shared task. 
5 Programmatic Access 
5.1 Web service reference 
We have designed a web service that accepts a 
batch of text and applies a series of analysis tools 
to that text, returning a bag of analyses. This main 
web service call, named ?Analyze?, requires four 
parameters: the language of the text (such as ?en? 
for English), the raw text to be analyzed, the set of 
analyzers to apply, and an access key to monitor 
and, if necessary, constrain usage. It returns a list 
of analyses, one from each requested analyzer, in a 
23
simple JSON (JavaScript Object Notation) format 
easy to parse in many programming languages. 
In addition, there is a web service call ?Lan-
guages? that enumerates the list of available lan-
guages, and ?Analyzers? to discover the set of 
analyzers available in a given language.  
5.2 Data Formats 
We use a relatively standard set of data representa-
tions for each component. Parse trees are returned 
as S expressions, part-of-speech tags are returned 
as lists, dependency trees are returned as lists of 
parent indices, and so on. The website contains an 
authoritative description of each analysis format. 
5.3 Speed 
Speed of analysis is heavily dependent on the 
component involved. Analyzers for sentence sepa-
ration, tokenization, and part-of-speech tagging 
process thousands of sentences per second; our 
fastest constituency parser handles tens of sentenc-
es per second. Where possible, the user is encour-
aged to send moderate sized requests (perhaps a 
paragraph at a time) to minimize the impact of 
network latency. 
6 Conclusion 
We hope that others will find the tools that we 
have made available as useful as we have. We en-
courage people to send us their feedback so that we 
can improve our tools and increase collaboration in 
the community. 
7 Script Outline 
The interactive UI (Figure 1) allows an arbitrary 
sentence to be entered and the desired levels of 
analysis to be selected as output. As there exist 
other such toolkits, the demonstration is primarily 
aimed at allowing participants to assess the quality, 
utility and speed of the MSR SPLAT tools. 
http://research.microsoft.com/en-us/projects/msrsplat/ 
References  
Colin Cherry and Hisami Suzuki. 2009. Discriminative sub-
string decoding for transliteration. In Proceedings of 
EMNLP. 
Michael Collins. 1999. Head-driven statistical models for 
natural language parsing. PhD Dissertation, University of 
Pennsylvania. 
Jianfeng Gao, Jian-Yun Nie, Jian Zhang, Endong Xun, Ming 
Zhou and Chang-Ning Huang. 2001. Improving query 
translation for CLIR using statistical Models. In Proceed-
ings of SIGIR. 
George Heidorn. 2000. Intelligent writing assistance. In R. 
Dale, H. Moisl and H. Somers (eds.), A Handbook of Natu-
ral Language Processing: Techniques and Applications for 
the Processing of Text. New York: Marcel Dekker. 
Mitchell Marcus, Beatrice Santorini, and Mary Ann 
Marcinkiewicz. 1993. Building a Large Annotated Corpus 
of English: The Penn Treebank. Computational Linguistics 
19(2): 313-330. 
Martha Palmer, Dan Gildea, Paul Kingsbury. 2005. The Prop-
osition Bank: An Annotated Corpus of Semantic Roles. 
Computational Linguistics, 31(1): 71-105 
Martin Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3): 130-137. 
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 
2006. Learning Accurate, Compact, and Interpretable Tree 
Annotation. In Proceedings of ACL. 
Brian Roark and Kristy Hollingshead. 2008. Classifying chart 
cells for quadratic complexity context-free inference. In 
Proceedings of COLING. 
Ruslan Salakhutdinov and Sam Roweis. 2003. Adaptive Over-
relaxed Bound Optimization Methods. In Proceedings of 
ICML. 
Kristina Toutanova, Aria Haghighi, and Christopher D. Man-
ning. 2008. A global joint model for semantic role labeling, 
Computational Linguistics, 34(2): 161-191. 
Nianwen Xue and Martha Palmer. 2004. Calibrating Features 
for Semantic Role Labeling. In Proceedings of EMNLP. 
Munmun de Choudhury, Scott Counts, Michael Gamon. Not 
All Moods are Created Equal! Exploring Human Emotional 
States in Social Media. Accepted for presentation in 
ICWSM 2012 
Munmun de Choudhury, Scott Counts, Michael Gamon. Hap-
py, Nervous, Surprised? Classification of Human Affective 
States in Social Media. Accepted for presentation (short 
paper) in ICWSM 2012 
 
 
Figure 1. Screenshot of the MSR SPLAT interactive UI 
showing selected functionalities which can be toggled 
on and off. This is the interface that we propose to 
demo at NAACL. 
 
 
24
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 373?377,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
How Are Spelling Errors Generated and Corrected? A Study of Corrected
and Uncorrected Spelling Errors Using Keystroke Logs
Yukino Baba
The University of Tokyo
yukino.baba@gmail.com
Hisami Suzuki
Microsoft Research
hisamis@microsoft.com
Abstract
This paper presents a comparative study of
spelling errors that are corrected as you type,
vs. those that remain uncorrected. First,
we generate naturally occurring online error
correction data by logging users? keystrokes,
and by automatically deriving pre- and post-
correction strings from them. We then per-
form an analysis of this data against the errors
that remain in the final text as well as across
languages. Our analysis shows a clear distinc-
tion between the types of errors that are gen-
erated and those that remain uncorrected, as
well as across languages.
1 Introduction
When we type text using a keyboard, we generate
many spelling errors, both typographical (caused by
the keyboard layout and hand/finger movement) and
cognitive (caused by phonetic or orthographic sim-
ilarity) (Kukich, 1992). When the errors are caught
during typing, they are corrected on the fly, but un-
noticed errors will persist in the final text. Previ-
ous research on spelling correction has focused on
the latter type (which we call uncorrected errors),
presumably because the errors that are corrected on
the spot (referred to here as corrected errors) are
not recoded in the form of a text. However, study-
ing corrected errors is important for at least three
reasons. First, such data encapsulates the spelling
mistake and correction by the author, in contrast
to the case of uncorrected errors in which the in-
tended correction is typically assigned by a third
person (an annotator), or by an automatic method
(Whitelaw et al, 2009; Aramaki et al, 2010)1. Sec-
ondly, data on corrected errors will enable us to build
a spelling correction application that targets correc-
tion on the fly, which directly reduces the number of
keystrokes in typing. This is crucial for languages
that use transliteration-based text input methods,
such as Chinese and Japanese, where a spelling error
in the input Roman keystroke sequence will prevent
1Using web search query logs is one notable exception,
which only targets spelling errors in search queries (Gao et al,
2010)
Keystroke
missspell misspellPre-correction strings Post-correction strings
m - i - s - s - s - p -  BACKSPACE  -  BACKSPACE  - p - e - l - l
Figure 1: Example of keystroke
the correct candidate words from appearing in the
list of candidates in their native scripts, thereby pre-
venting them from being entered altogether. Finally,
we can collect a large amount of spelling errors and
their corrections by logging keystrokes and extract-
ing the pre- and post-correction strings from them.
By learning the characteristics of corrected and un-
corrected errors, we can expect to use the data for
improving the correction of the errors that persisted
in the final text as well.
In this paper, we collect naturally occurring
spelling error data that are corrected by the users
online from keystroke logs, through the crowd-
sourcing infrastructure of Amazon?s Mechanical
Turk (MTurk). As detailed in Section 3, we dis-
play images to the worker of MTurk, and collect
the descriptions of these images, while logging their
keystrokes including the usage of backspace keys,
via a crowd-based text input service. We collected
logs for two typologically different languages, En-
glish and Japanese. An example of a log along
with the extracted pre- and post-correction strings
is shown in Figure 1. We then performed two com-
parative analyses: corrected vs. uncorrected errors
in English (Section 4.3), and English vs. Japanese
corrected errors (Section 4.4). Finally, we remark
on an additional cause of spelling errors observed in
all the data we analyzed (Section 4.5).
2 Related Work
Studies on spelling error generation mechanisms are
found in earlier work such as Cooper (1983). In
particular, Grudin (1983) offers a detailed study of
the errors generated in the transcription typing sce-
nario, where the subjects are asked to transcribe a
text without correcting the errors they make. In a
more recent work, Aramaki et al (2010) automati-
cally extracted error-correction candidate pairs from
Twitter data based on the assumption that these pairs
373
fall within a small edit distance, and that the errors
are not in the dictionary and substantially less fre-
quent than the correctly spelled counterpart. They
then studied the effect of five factors that cause er-
rors by building a classifier that uses the features as-
sociated with these classes and running ablation ex-
periments. They claim that finger movements cause
the spelling errors to be generated, but the uncor-
rected errors are characterized by visual factors such
as the visual similarity of confused letters. Their ex-
periments however target only the persisted errors,
and their claim is not based on the comparison of
generated and persisted errors.
Outside of English, Zheng et al (2011) analyzed
the keystroke log of a commercial text input system
for Simplified Chinese, and compared the error pat-
terns in Chinese with those in English. Their use of
the keystroke log is different from ours in that they
did not directly log the input in pinyin (Romanized
Chinese by which native characters are input), but
the input pinyin sequences are recovered from the
Chinese words in the native script (hanzi) after the
character conversion has already applied.
3 Keystroke Data Collection
Amazon?s Mechanical Turk (MTurk) is a web ser-
vice that enables crowdsourcing of tasks that are dif-
ficult for computers to solve, and has become an im-
portant infrastructure for gathering data and annota-
tion for NLP research in recent years (Snow et al
2008). To the extent of our knowledge, our work
is the first to use this infrastructure to gather user
keystroke data.
3.1 Task design
In order to collect naturally occurring keystrokes,
we have designed two types of tasks, both of which
consist of writing something about images. In one
task type, we asked the workers to write a short
description of images (image description task); in
the other, the workers were presented with im-
ages of a person or an animal, and were asked to
guess and type what she/he was saying (let-them-
talk task). Using images as triggers for typing keeps
the underlying motivation of keystroke collection
hidden from the workers, simultaneously allowing
language-independent data collection. For the im-
age triggers, we used photos from the Flickr?s Your
Best Shot 2009/2010 groups . Examples of the tasks
and collected text are given in Figure 2.
?????????????????????? ?????????????
Image Description Task Let-them-talk Task
?oh mummy. please dont take a clip. i am naked and i feel shy. at least give me a towel.?
En ?A flock of penguins waddle towards two trees over snow covered ground.?
Ja
En
Ja
Figure 2: Examples of tasks and collected text (Translated text:
?A flock of penguines are marching in the snow.? and ?Mummy,
my feet can?t touch the bottom.?)
3.2 Task interface
For logging the keystrokes including the use of
backspaces, we designed an original interface for the
text boxes in the MTurk task. In order to simplify
the interpretation of the log, we disabled the cursor
movements and text highlighting via a mouse or the
arrow keys in the text box; the workers are therefore
forced to use the backspace key to make corrections.
In Japanese, many commercially available text in-
put methods (IMEs) have an auto-complete feature
which prevents us from collecting all keystrokes for
inputting a word. We therefore used an in-house
IME that has disabled this feature to collect logs.
This IME is hosted as a web service, and keystroke
logs are also collected through the service. For En-
glish, we used the service for log collection only.
4 Keystroke Log Analysis
4.1 Data
We used both keystroke-derived and previously
available error data for our analysis.
Keystroke-derived error pairs for English and
Japanese (en keystroke, ja keystroke): from the
raw keystroke logs collected using the method de-
scribed in Section 3, we extracted only those words
that included a use of the backspace key. We then
recovered the strings before and after correction by
the following steps (Cf. Figure 1):
? To recover the post-correction string, we
deleted the same number of characters preced-
ing a sequence of backspace keys.
? To recover the pre-correction string, we com-
pared the prefix of the backspace usage
(misssp in Figure 1) with the substrings
after error correction (miss, missp, ? ? ? ,
misspell), and considered that the prefix
was spell-corrected into the substring which is
the longest and with the smallest edit distance
374
(in this case, misssp is an error for missp,
so the pre-correction string is missspell).
We then lower-cased the pairs and extracted only
those within the edit distance of 2. The resulting data
which we used for our analysis consists of 44,104
pairs in English and 4,808 pairs in Japanese2.
Common English errors (en common): follow-
ing previous work (Zheng et al, 2011), we ob-
tained word pairs from Wikipedia3 and SpellGood4.
We lower-cased the entries from these sources, re-
moved the duplicates and the pairs that included
non-Roman alphabet characters, and extracted only
those pairs within the edit distance of 2. This left us
with 10,608 pairs.
4.2 Factors that affect errors
Spelling errors have traditionally been classified into
four descriptive types: Deletion, Insertion, Substitu-
tion and Transposition (Damerau, 1964). For each
of these types, we investigated the potential causes
of error generation and correction, following previ-
ous work (Aramaki et al, 2010; Zheng et al, 2011).
Physical factors: (1) motor control of hands and fin-
gers; (2) distance between the keys; Visual factors:
(3) visual similarity of characters; (4) position in
a word; (5) same character repetition; Phonologi-
cal factors: (6) phonological similarity of charac-
ters/words.
In what follows, our discussion is based on the
frequency ratio of particular error types, where the
frequency ratio refers to the number of cases in
spelling errors divided by the total number of cases
in all data. For example, the frequency ratio of con-
sonant deletion is calculated by dividing the number
of missing consonants in errors by the total number
of consonants.
4.3 Corrected vs. uncorrected errors in English
In this subsection, we compare corrected and uncor-
rected errors of English, trying to uncover what fac-
tors facilitate the error correction.
Error types (Figure 3) Errors in en keystroke are
dominated by Substitution, while Deletion errors are
the most common in en common, indicating that
2The data is available for research purposes under http:
//research.microsoft.com/research/downloads/
details/4eb8d4a0-9c4e-4891-8846-7437d9dbd869/
details.aspx
3http://en.wikipedia.org/wiki/Wikipedia:
Lists of common misspellings/For machines
4http://www.spellgood.net/sitemap.html
ja_keystroke
en_keystroke
en_common
DeletionInsertion SubstitutionTransposition
Ratio (%)0 20 40 60 80 100
Figure 3: Ratios of error types
Substition
SimilarityF
req.
0.00
0
SimilarityF
req.
0.00
0
SimilarityF
req.
0.00
0
0.30 0.90
0.30 0.90
0.30 0.90en_keystroke ja_keystrokeen_common
Figure 4: Visual similarities
of characters in substitution
errors
0 20 40 60 80 100
Deletion
0?base position / (word length?1) (%)
De
ns
ity
0 20 40 60 80 100
Insertion
0?base position / (word length?1) (%)
De
ns
ity
0 20 40 60 80 100
Substitution
0?base position / (word length?1) (%)
De
ns
ity
0 20 40 60 80 100
Transposition
0?base position / (word length?1) (%)
De
ns
ity
en_keystroke ja_keystrokeen_common
Figure 5: Positions of errors within words
Substitution mistakes are easy to catch, while Dele-
tion mistakes tend to escape our attention. Zheng
et al (2011) reports that their pinyin correction er-
rors are dominated by Deletion, which suggests that
their log does in fact reflect the characteristics of cor-
rected errors.
Position of error within a word (Figure 5) In
en keystroke, Deletion errors at the word-initial po-
sition are the most common, while Insertion and
Substitution errors tend to occur both at the be-
ginning and the end of a word. In contrast, in
en common, all error types are more prone to oc-
cur word-medially. This means that errors at word
edges are corrected more often than the word-
internal errors, which can be attributed to cognitive
effect known as the bathtub effect (Aitchison, 1994),
which states that we memorize words at the periph-
ery most effectively in English.
Effect of character repetition (Figure 6) Dele-
tion errors where characters are repeated, as in
tomorow?tomorrow, is observed significantly
more frequently than in a non-repeating context in
en common, but no such difference is observed in
en keystroke, showing that visually conspicuous er-
rors tend to be corrected.
Visual similarity in Substitution errors (Figure
4) We computed the visual similarity of characters
by
2?(the area of overlap between character A andB)/
(area of character A+area of character B) follow-
375
Not Repeated / Repeated
Deletion
Ra
tio 
of F
req
.
0.0
0.4
0.8
en_keystroke ja_keystrokeen_common
Figure 6: Effect of character
repetition in Deletion
0.0
0.4
0.8
en_keystroke ja_keystrokeen_common
Diff=2 / Diff=1
Transposition
Ra
tio 
of F
req
.
Figure 7: Difference of posi-
tions within words in Trans-
position
Vowel / Consonant
Insertion
Inserted Character
0.0
0.4
0.8
C?>C C?>V V?>C V?>V
Substitution
Substituted Character ?> Correct Characteren_keystroke ja_keystrokeen_common
Freq
./ma
x(Fr
eq.)
0.0
0.4
0.8
Rat
io o
f Fre
q.
Figure 8: Consonants/vowels in Insertion and Substitution
ing Aramaki et al (2010)5. Figure 4 shows that in
en common, Substitution errors of visually similar
characters (e.g., yoqa?yoga) are in fact very
common, while in en keystroke, no such tendency
is observed.
Phonological similarity in Substitution errors
(Figure 8) In en keystroke, there is no notable
difference in consonant-to-consonant (C?C) and
vowel-to-vowel (V?V) errors, but in en common,
V?V errors are overwhelmingly more com-
mon, suggesting that C?C can easily be no-
ticed (e.g., eazy?easy) while V?V errors (e.g.,
visable?visible) are not. This tendency is
consistent with the previous work on the cognitive
distinction between consonants and vowels in En-
glish: consonants carry more lexical information
than vowels (Nespor et al, 2003), a claim also
supported by distributional evidence (Tanaka-Ishii,
2008). It may also be attributed to the fact that En-
glish vowel quality is not always reflected by the on-
thography in the straightforward maner.
Summarizing, we have observed both visual and
phonological factors affect the correction of errors.
Aramaki et al (2010)?s experiments did not show
that C/V distinction affect the errors, while our data
shows that it does in the correction of errors.
4.4 Errors in English vs. Japanese
From Figure 3, we can see that the general error
pattern is very similar between en keystroke and
ja keystroke. Looking into the details, we discov-
ered some characteristic errors in Japanese, which
are phonologically and orthographically motivated.
Syllable-based transposition errors (Figure 7)
When comparing the transposition errors by their
5We calculated the area using the Courier New font which
we used in our task interface.
Appeared Before To Appear
Substitution
Substituted Character
Fre
q. 
/ m
ax(
Fre
q.)
0.0
0.4
0.8
Not Appeared Before Not to Appear
en_keystroke ja_keystrokeen_common
Figure 9: Look-ahead and Look-behind in Substitution
distance, 1 being a transposition of adjacent char-
acters and 2 a transposition skipping a character, the
instances in en keystroke are mostly of distance of
1, while in ja keystroke, the distance of 2 also occurs
commonly (e.g., kotoro?tokoro). This is inter-
esting, because the Japanese writing system called
kana is a syllabary system, and our data suggests that
users may be typing a kana character (typically CV)
as a unit. Furthermore, 73% of these errors share
the vowel of the transposed syllables, which may be
serving as a strong condition for this type of error.
Errors in consonants/vowels (Figure 8) Errors
in ja keystroke are characterized by a smaller ra-
tio of insertion errors of vowels relative to conso-
nants, and by a relatively smaller ratio of V?V sub-
stitution errors. Both point to the relative robust-
ness of inputting vowels as opposed to consonants
in Japanese. Unlike English, Japanese only has five
vowels whose pronunciations are transparently car-
ried by the orthography; they are therefore expected
to be less prone to cognitive errors.
4.5 Look-ahead and look-behind errors
In Substitution errors for all data we analyzed, sub-
stituting for the character that appeared before, or
are to appear in the word was common (Figure
9). In particular, in en keystroke and ja keystroke,
look-ahead errors are much more common than non-
look-ahead errors. Grudin (1983) reports cases
of permutation (e.g., gib?big) but our data in-
cludes non-permutation look-ahead errors such as
puclic?public and otigaga?otibaga.
5 Conclusion
We have presented our collection methodology and
analysis of error correction logs across error types
(corrected vs. uncorrected) and languages (English
and Japanese). Our next step is to utilize the col-
lected data and analysis results to build online and
offline spelling correction models.
Acknowledgments
This work was conducted during the internship of
the first author at Microsoft Research. We are grate-
ful to the colleagues for their help and feedback in
conducting this research.
376
References
Aitchison, J. 1994. Words in the Mind. Blackwell.
Aramaki, E., R. Uno and M. Oka. 2010. TYPO Writer:
????????????????? (TYPO
Writer: how do humans make typos?). In Proceedings
of the 16th Annual Meeting of the Natural Language
Society (in Japanese).
Cooper, W. E. (ed.) 1983. Cognitive Aspects of Skilled
Typewriting. Springer-Verlag.
Damerau, F. 1964. A technique for computer detection
and correction of spelling errors. Communications of
the ACM 7(3): 659-664.
Gao, J., X. Li, D. Micol, C. Quirk and X. Sun. 2010.
A large scale ranker-based system for search query
spelling correction. In Proceedings of COLING.
Grudin, J. T. 1983. Error patterns in novice and skilled
transcription typing. In Cooper, W.E. (ed.), Cognitive
Aspects of Skilled Typewriting. Springer-Verlag.
Kukich, K. 1992. Techniques for automatically correct-
ing words in text. In ACM Computing Surveys, 24(4).
Nespor, M., M. Pen?a, and J. Mehler. 2003. On the differ-
ent roles of vowels and consonants in speech process-
ing and language acquisition. Lingue e Linguaggio,
pp. 221?247.
Snow, R., B. O?Connor, D. Jurafsky, and A. Ng. 2008.
Cheap and fast ? but is it good?: evaluating non-expert
annotations for natural language tasks. In Proceedings
of EMNLP.
Tanaka-Ishii, K. 2008. ?????????????
(On the uneven distribution of information in words).
In Proceedings of the 14th Annual Meeting of the Nat-
ural Language Society (in Japanese).
Whitelaw, Casey, Ben Hutchinson, Grace Y. Chung, and
Gerard Ellis. 2009. Using the web for language in-
dependent spellchecking and autocorrection. In Pro-
ceedings of ACL.
Zheng, Y., L. Xie, Z. Liu, M. Sun. Y. Zhang and L. Ru
2011. Why press backspace? Understanding user in-
put behaviors in Chinese pinyin input method. In Pro-
ceedings of ACL
377
