Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 9?16, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
                                
Automatic Short Answer Marking 
 
 
Stephen G. Pulman                  Jana Z. Sukkarieh  
Computational Linguistics Group, Computational Linguistics Group, 
University of Oxford. University of Oxford. 
Centre for Linguistics and Philology, Centre for Linguistics and Philology, 
Walton St., Oxford, OX1 2HG, UK Walton St., Oxford,  OX1 2HG, UK 
sgp@clg.ox.ac.uk  Jana.Sukkarieh@clg.ox.ac.uk  
 
 
 
 
Abstract 
Our aim is to investigate computational lin-
guistics (CL) techniques in marking short free 
text responses automatically. Successful auto-
matic marking of free text answers would seem 
to presuppose an advanced level of perform-
ance in automated natural language under-
standing.  However, recent advances in CL 
techniques have opened up the possibility of 
being able to automate the marking of free text 
responses typed into a computer without hav-
ing to create systems that fully understand the 
answers. This paper describes  some of the 
techniques we have tried so far vis-?-vis this 
problem with results, discussion and descrip-
tion of the main issues encountered.1 
 
1. Introduction 
 
Our aim is to investigate computational linguistics 
techniques in marking short free text responses 
automatically. The free text responses we are deal-
ing with are answers ranging from a few words up 
to 5 lines. These answers are for factual science 
questions that typically ask candidates to state, de-
scribe, suggest, explain, etc. and where there is an 
objective criterion for right and wrong. These 
questions are from an exam known as GCSE (Gen-
eral Certificate of Secondary Education): most 16 
                                                          
1
 This is a 3-year project funded by the University of Cam-
bridge Local Examinations Syndicate. 
 
 
year old students take up to 10 of these in different 
subjects in the UK school system. 
 
 
   2. The Data 
 
Consider the following GCSE biology question: 
 
Statement of the 
question 
The blood vessels 
help to maintain 
normal body tem-
perature. Explain 
how the blood ves-
sels reduce heat 
loss if the body 
temperature falls 
below normal. 
Marking Scheme (full mark 3)2  
any three: 
vasoconstriction; explanation (of 
vasoconstriction); less blood 
flows to / through the skin / close 
to the surface; less heat loss to 
air/surrounding/from the blood / 
less radiation / conduction / con-
vection; 
 
Here is a sample of real answers: 
 
1. all the blood move faster and dose not go near the 
top of your skin they stay close to the moses 
2. The blood vessels stops a large ammount of blood 
going to the blood capillary and sweat gland.  
This prents the presonne from sweating and loos-
ing heat.  
3. When the body falls below normal the blood ves-
sels 'vasoconstrict' where the blood supply to the 
skin is cut off, increasing the metabolism of the 
                                                          
2
 X;Y/D/K;V is equivalent to saying that each of X, [L]={Y, 
D,K}, and V deserves 1 mark. The student has to write only 2 
of these to get the full mark. [L] denotes an equivalence class 
i.e. Y, D, K are equivalent. If the student writes Y and D s/he 
will get only 1 mark.    
9
body.  This prevents heat loss through the skin, 
and causes the body to shake to increase metabo-
lism. 
 
It will be obvious that many answers are ungram-
matical with many spelling mistakes, even if they 
contain more or less the right content. Thus using 
standard syntactic and semantic analysis methods 
will be difficult. Furthermore, even if we had fully 
accurate syntactic and semantic processing, many 
cases require a degree of inference that is beyond 
the state of the art, in at least the following re-
spects: 
? The need for reasoning and making infer-
ences:  a student may answer with we do not 
have to wait until Spring,which only implies 
the marking key it can be done at any time.  
Similarly, an answer such as don?t have sperm 
or egg will get a 0 incorrectly if there is no 
mechanism to infer no fertilisation. 
? Students tend to use a negation of a negation 
(for an affirmative):  An answer like won?t be 
done only at a specific time is the equivalent to 
will be done at any time.  An answer like it is 
not formed from more than one egg and sperm 
is the same as saying formed from one egg and 
sperm.  This category is merely an instance of 
the need for more general reasoning and infer-
ence outlined above.  We have given this case 
a separate category because here, the wording 
of the answer is not very different, while in the 
general case, the wording can be completely 
different. 
? Contradictory or inconsistent information:  
Other than logical contradiction like needs fer-
tilisation and does not need fertilisation, an an-
swer such as identical twins have the same 
chromosomes but different DNA holds incon-
sistent scientific information that needs to be 
detected. 
Since we were sceptical that existing deep process-
ing NL systems would succeed with our data,  
we chose to adopt a shallow processing approach, 
trading robustness for complete accuracy. After 
looking carefully at the data we also discovered 
other issues which will affect assessment of  the 
accuracy of any automated system, namely: 
      
? Unconventional expression for scientific 
knowledge: Examiners sometimes accept un-
conventional or informal ways of expressing 
scientific knowledge, for example, ?sperm and 
egg get together? for ?fertilisation?.  
? Inconsistency across answers: In some cases, 
there is inconsistency in marking across an-
swers. Examiners sometimes make mistakes 
under pressure. Some biological information is 
considered relevant in some answers and ir-
relevant in others.  
 
In the following, we describe various implemented 
systems and report on their accuracy. 
We  conclude with some current work and suggest 
a road map.    
 
3. Information Extraction for Short An-
swers 
In our initial experiments, we adopted an Informa-
tion Extraction approach (see also Mitchell et al 
2003). We used an existing Hidden Markov Model 
part-of-speech (HMM POS) tagger trained on the 
Penn Treebank corpus, and a Noun Phrase (NP) 
and Verb Group (VG) finite state machine (FSM) 
chunker.  The NP network was induced from the 
Penn Treebank, and then tuned by hand.  The Verb 
Group FSM (i.e. the Hallidayean constituent con-
sisting of the verbal cluster without its comple-
ments) was written by hand. Relevant missing 
vocabulary was added to the tagger from the 
tagged British National Corpus (after mapping 
from their tag set to ours), and from examples en-
countered in our training data. The tagger also in-
cludes some suffix-based heuristics for guessing 
tags for unknown words.    
In real information extraction, template merging 
and reference resolution are important components. 
Our answers display little redundancy, and are 
typically less than 5 lines long, and so template 
merging is not necessary. Anaphors do not occur 
very frequently, and when they do, they often refer 
back to entities introduced in the text of  the ques-
tion (to which the system does not have access). So 
at the cost of missing some correct answers, the 
information extraction components really consists 
of little more than a set of patterns applied to the 
tagged and chunked text. 
We wrote our initial patterns by hand, although we 
are currently working on the development of a tool 
to take most of the tedious effort out of this task. 
We base the patterns on recurring head words or 
phrases, with syntactic annotation where neces-
10
sary,  in the training data. Consider the following 
example training answers:  
the egg after fertilisation 
splits in two 
the fertilised egg has di-
vided into two 
The egg was fertilised it 
split in two 
One fertilised egg splits 
into two 
one egg fertilised which 
split into two 
1 sperm has fertilized an 
egg.. that split into two 
These are all paraphrases of It is the same fertilised 
egg/embryo, and variants of what is written above 
could be captured by a pattern like: 
singular_det + <fertilised egg> +{<split>; <divide>; 
<break>} + {in, into} + <two_halves>, where 
<fertilised egg>  = NP with the content of ?fertilised 
egg? 
singular_det       = {the, one, 1, a, an} 
<split>               = {split, splits, splitting, has split, etc.} 
<divide>            = {divides, which divide, has gone, 
being broken...} 
<two_halves>    = {two, 2, half, halves} 
etc. 
The pattern basically is all the paraphrases col-
lapsed into one. It is essential that the patterns use 
the linguistic knowledge we have at the moment, 
namely, the part-of-speech tags, the noun phrases 
and verb groups. In our previous example, the re-
quirement that <fertilised egg>  is an NP will ex-
clude something like ?one sperm has fertilized an 
egg? while accept something like ?an egg which is 
fertilized ...?. 
 
System Architecture: 
 ?When the caterpillars are feeding on the tomato plants, a chemical is 
released from the plants?. 
 
 
 
 
 
 
When/WRB [the/DT caterpillars/NNS]/NP[are/VBP feed-
ing/VBG]/VG on/IN [the/DT tomato/JJ plants/NNS] /NP,/,  [a/DT 
chemical/NN]/NP  
[is/VBZ released/VBN]/VG from/IN [the/DT plants/NNS]/NP./. 
 
  
 
 
 
 
 
 
 
 
 
 
Table 1 gives results for the current version of the 
system. For each of 9 questions, the patterns were 
developed using a training set of about 200 
marked answers, and tested on 60 which were 
not released to us until the patterns had been writ-
ten. Note that the full mark for each question 
ranges between 1-4. 
 
Question Full Mark %  Examiner 
Agreement 
%  Mark Scheme 
 Agreement  
1 2 89.4   93.8  
2 2 91.8 96.5 
3 2 84 94.2 
4 1 91.3 94.2 
5 2 76.4 93.4 
6 3 75 87.8 
7 1 95.6 97.5 
8 4 75.3 86.1 
9 2 86.6 92 
Average ---- 84 93 
 
Table 1. Results for the manually-written IE approach. 
 
Column 3 records the percentage agreement be-
tween our system and the marks assigned by a hu-
man examiner. As noted earlier, we detected a 
certain amount of inconsistency with the marking 
scheme in the grades actually awarded. Column 4 
reflects the degree of agreement between the 
grades awarded by our system and those which 
would have been awarded by following the mark-
ing scheme consistently. Notice that agreement is 
correlated with the mark scale: the system appears 
less accurate on multi-part questions. We adopted 
an extremely strict measure, requiring an exact 
match. Moving to a pass-fail criterion produces 
much higher agreement for questions 6 and 8. 
 
4. Machine Learning 
 
Of course, writing patterns by hand  requires ex-
pertise both in the domain of the examination, and 
in computational linguistics. This requirement 
makes the commercial deployment of a system like 
this problematic, unless specialist staff are taken 
on. We have therefore been experimenting with 
ways in which a short answer marking system 
might be developed rapidly using machine learning 
methods on a training set of marked answers. 
 
Previously (Sukkarieh et al 2003) we reported the 
results we obtained using  a simple  Nearest 
HMM Pos Tagger 
NP & VG Chunker Specialized  
lexicon 
  Pattern Matcher 
 
     Score and Justification 
Marker  
 
General  
lexicon 
Patterns 
Grammar 
11
Neighbour Classification techniques. In the follow-
ing, we report our results using three different ma-
chine learning methods: Inductive Logic 
progamming (ILP), decision tree learning(DTL) 
and Naive Bayesian learning (Nbayes). ILP 
(Progol, Muggleton 1995)  was chosen as a repre-
sentative symbolic learning method. DTL and 
NBayes were chosen following the Weka (Witten 
and Frank, 2000) injunction to `try the simple 
things first?. With ILP, only 4 out of the 9 ques-
tions shown in the previous section were tested, 
due to resource limitations. With DTL and Nbayes, 
we conducted two experiments on all 9 questions. 
The first experiments show the results with non-
annotated data; we then repeat the experiments 
with annotated data. Annotation in this context is a 
lightweight activity, simply consisting of a domain 
expert highlighting the part of the answer that de-
serves a mark. Our idea was to make this as simple 
a process as possible, requiring minimal software, 
and being exactly analogous to what some markers 
do with pencil and paper. As it transpired, this was 
not always straightforward, and does not mean that 
the training data is noiseless since sometimes an-
notating the data accurately requires non-adjacent 
components to be linked: we could not take ac-
count of this. 
 
4.1 Inductive Logic Programming 
 
For our problem, for every question, the set of 
training data consists of students? answers, to that 
question, in a Prologised version of their textual 
form, with no syntactic analysis at all initially. We 
supplied some `background knowledge? predicates 
based on the work of  (Junker et al 1999). Instead 
of using their 3 Prolog basic predicates, however, 
we only defined 2, namely, word-
pos(Text,Word,Pos) which represents words and 
their position in the text and window(Pos2-
Pos1,Word1,Word2) which represents two words 
occurring within a Pos2-Pos1 window distance. 
 
After some initial experiments, we believed that a 
stemmed and tagged training data should give bet-
ter results and that window should be made inde-
pendent to occur in the logic rules learned by 
Progol. We used our POS tagger mentioned above 
and the Porter stemmer (Porter 1980). We set the 
Progol noise parameter to 10%, i.e. the rules do not 
have to fit the training data perfectly. They can be 
more general. The percentages of agreement are 
shown in table 23. The results reported are on a 5-
fold cross validation testing and the agreement is 
on whether an answer is marked 0 or a mark >0, 
i.e. pass-fail, against the human examiner scores. 
The baseline is the number of answers with the 
most common mark multiplied by 100 over the 
total number of answers. 
 
Question Baseline % of agreement 
6 51,53 74,87 
7 73,63 90,50 
8 57,73 74,30 
9 70,97 65,77 
Average 71,15 77,73 
                         
             Table 2. Results using ILP. 
 
The results of the experiment are not very promis-
ing. It seems very hard to learn the rules with ILP.  
Most rules state that an answer is correct if it con-
tains a certain word, or two certain words within a 
predefined distance. A question such as 7, though, 
scores reasonably well. This is because Progol 
learns a rule such as mark(Answer) only if word-
pos(Answer,?shiver?, Pos) which is, according to 
its marking scheme, all it takes to get its full mark, 
1. ILP has in effect found the single keyword that 
the examiners were looking for.  
Recall that we only have ~200 answers for train-
ing. By training on a larger set, the learning algo-
rithm may be able to find more structure in the 
answers and may come up with better results. 
However, the rules learned may still be basic since, 
with the background knowledge we have supplied 
the ILP learner always tries to find simple and 
small predicates over (stems of) keywords. 
4.2 Decision Tree Learning and Bayesian 
Learning 
In our marking problem, seen as a machine learn-
ing problem, the outcome or target attribute is 
well-defined. It is the mark for each question and 
its values are {0,1, ?, full_mark}. The input at-
tributes could vary from considering each word to 
be an attribute or considering deeper linguistic fea-
tures like a head of a noun phrase or a verb group 
to be an attribute, etc. In the following experi-
ments, each word in the answer was considered to 
be an attribute. Furthermore, Rennie et al (2003) 
                                                          
3
 Our thanks to our internship student, Leonie IJzereef for the 
results in table 2. 
12
propose simple heuristic solutions to some prob-
lems with na?ve classifiers. In Weka, Complement 
of Na?ve Bayes (CNBayes) is a refinement to the 
selection process that Na?ve Bayes makes when 
faced with instances where one outcome value has 
more training data than another. This is true in our 
case. Hence, we ran our experiments using this 
algorithm also to see if there were any differences. 
The results reported are on a 10-fold cross valida-
tion testing. 
 
4.2.1 Results on Non-Annotated data 
We first considered the non-annotated data, that is, 
the answers given by students in their raw form. 
The first experiment considered the values of the 
marks to be {0,1, ?, full_mark} for each question. 
The results of decision tree learning and Bayesian 
learning are reported in the columns titled DTL1 
and NBayes/CNBayes1. The second experiment 
considered the values of the marks to be either 0 or 
>0, i.e. we considered two values only, pass and 
fail. The results are reported in columns DTL2 and 
NBayes2/CNBayes2. The baseline is calculated the 
same way as in the ILP case. Obviously, the result 
of the baseline differs in each experiment only 
when the sum of the answers with marks greater 
than 0 exceeds that of those with mark 0. This af-
fected questions 8 and 9 in Table 3 below. Hence, 
we took the average of both results. It was no sur-
prise that the results of the second experiment were 
better than the first on questions with the full mark  
>1, since the number of target features is smaller. 
In both experiments, the complement of Na?ve 
Bayes did slightly better or equally well on ques-
tions with a full mark of 1, like questions 4 and 7 
in the table, while it resulted in a worse perform-
ance on questions with full marks >1. 
 
Ques. Base-
line 
DTL1 N/CNBayes1 N/CNBayes2 DTL2 
1 69 73.52 73.52 / 66.47 81.17 / 73.52 76.47 
2 54 62.01 65.92  /61.45 73.18/  68.15 62.56 
3 46 68.68 72.52 / 61.53 93.95 / 92.85 93.4 
4 58 69.71 75.42 /  76 75.42 / 76 69.71 
5 54 60.81 66.66 / 53.21 73.09 / 73.09 67.25 
6 51 47.95 59.18 / 52.04 81.63  /77.55 67.34 
7 73 88.05 88.05 / 88.05 88.05 / 88.05 88.05 
8 42   41.75 43.29 / 37.62 70.10/ 69.07 72.68 
9 60  61.82 67.20 / 62.36 79.03 / 76.88 76.34 
Ave. 60.05 63.81 67.97/62.1 79.51/77.3 74.86 
 
Table 3.  Results for Bayesian learning and decision tree learning  
on non-annotated data. 
Since we were using the words as attributes, we 
expected that in some cases stemming the words in 
the answers would improve the results. Hence, we 
experimented with the answers of 6, 7, 8 and 9 
from the list above but there was only a tiny im-
provement (in question 8). Stemming does not 
necessarily make a difference if the attrib-
utes/words that make a difference appear in a root 
form already. The lack of any difference or worse 
performance may also be due to the error rate in 
the stemmer.   
4.2.2 Results on Annotated data 
We repeated the second experiments with the an-
notated answers. The baseline for the new data dif-
fers and the results are shown in Table 4.  
 
Question Baseline DTL NBayes/CNBayes 
1 58 74.87 86.69  /  81.28 
2 56 75.89 77.43   /  73.33 
3 86 90.68 95.69   /  96.77 
4 62 79.08 79.59   /  82.65 
5 59 81.54 86.26   /  81.97 
6 69 85.88 92.19   /  93.99 
7 79 88.51 91.06   /  89.78 
8 78 94.47 96.31   /   93.94 
9 79 85.6 87.12   /   87.87 
Average 69.56 84.05  88.03  /  86.85 
 
Table 4. Results for Bayesian learning and decision tree learning 
on annotated data. 
 
As we said earlier, annotation in this context sim-
ply  means highlighting the part of the answer that 
deserves 1 mark (if the answer has >=1 mark), so 
for e.g. if an answer was given a 2 mark then at 
least two pieces of information should be high-
lighted and answers with 0 mark stay the same. 
Obviously, the first experiments could not be con-
ducted since with the annotated answers the mark 
is either 0 or 1. Bayesian learning is doing better 
than DTL and 88% is a promising result. Further-
more, given the results of CNBayes in Table 3, we 
expected that CNBayes would do better on ques-
tions 4 and 7. However, it actually did better on 
questions 3, 4, 6 and 9. Unfortunately, we cannot 
see a pattern or a reason for this. 
5. Comparison of Results 
IE did best on all the questions before annotating 
the data as it can be seen in Fig. 1. Though, the 
training data for the machine learning algorithms is 
13
tiny relative to what usually such algorithms con-
sider, after annotating the data, the performance of 
NBayes on questions 3, 6 and 8 were better than 
IE. This is seen in Fig. 2. However, as we said ear-
lier in section 2, the percentages shown for IE 
method are on the whole mark while the results of 
DTL and Nbayes, after annotation,  are  calculated 
on pass-fail. 
 
F ig. 1. IE vs D T L & N bayes pre-anno tat io n
0
20
40
60
80
100
120
1 2 3 4 5 6 7 8 9
Quest ion
IE
DTL1
NBayes1
DTL2
NBayes2
 
 
In addition, in the pre-annotation experiments re-
ported in Fig. 1, the NBayes algorithm did better 
than that of DTL.  Post-annotation, results in Fig. 2 
show, again, that NBayes is doing better than the 
DTL algorithm. It is worth noting that, in the anno-
tated data, the number of answers whose marks are 
0 is less than in the answers whose mark is 1, ex-
cept for questions 1 and 2. This may have an effect 
on the results. 
 
Fig.2. IE vs DTL & NBayes post-annotation
0
20
40
60
80
100
120
1 2 3 4 5 6 7 8 9
Question
%
 
Pe
rf
o
rm
an
ce
IE
DTL
NBayes
 
 
Moreover, after getting the worse performance in 
NBayes2 before annotation, question 8 jumps to 
best performance. The rest of the questions main-
tained the same position more or less, with ques-
tion 3 always coming nearest to the top (see Fig. 
3). We noted that Count(Q,1)-Count(Q,0) is high-
est for questions 8 and 3, where Count(Q,N) is, for 
question Q, the number of answers whose mark is 
N. Also, the improvement of performance for 
question 8 in relation to Count(8,1) was not sur-
prising, since question 8 has a full-mark of 4 and 
the annotation?s role was an attempt at a one-to-
one correspondence between an answer and 1 
mark.  
 
 
Fig. 3. NBayes before and after annotation
0
20
40
60
80
100
120
1 2 3 4 5 6 7 8 9
Question
%
 
Pe
rf
o
rm
an
ce
Nbayes1_before
Nbayes2_before
Nbayes_after
 
 
On the other hand, question 1 that was in seventh 
place in DTL2 before annotation, jumps down to 
the worst place after annotation. In both cases, 
namely, NBayes2 and DTL2 after annotation, it 
seems reasonable to hypothesize that P(Q1) is bet-
ter than P(Q2) if Count(Q1,1)-Count(Q1,0) >> 
Count(Q2,1)-Count(Q2,0), where P(Q) is the per-
centage of agreement for question Q. 
 
As they stand, the results of agreement with given 
marks are encouraging. However, the models that 
the algorithms are learning are very na?ve in the 
sense that they depend on words only. Unlike the 
IE approach, it would not be possible to provide a  
reasoned justification for a student as to why they 
have got the mark they have. One of the advan-
tages to the pattern-matching approach is that it is 
very easy, knowing which patterns have matched, 
to provide some simple automatic feed-back to the 
student as to which components of the answer were 
responsible for the mark awarded. 
 
We began experimenting with machine learning 
methods in order to try to overcome the IE cus-
tomisation bottleneck. However, our experience so 
far has been that in short answer marking (as op-
posed to essay marking) these methods are, while 
promising, not accurate enough at present to be a 
real alternative to the hand-crafted, pattern-
14
matching approach. We should instead think of 
them either as aids to the pattern writing process ? 
for example, frequently the decision trees that are 
learned are quite intuitive, and suggestive of useful 
patterns ? or perhaps as complementary supporting 
assessment techniques to give extra confirmation. 
 
6. Other work  
 
Several other groups are working on this problem, 
and we have learned from all of them. Systems 
which share properties with ours are C-Rater, de-
veloped by Leacock et al (2003) at the Educa-
tional Testing Service(ETS),  the IE-based system 
of Mitchell et al (2003) at Intelligent Assessment 
Technologies, and Ros? et al (2003) at Carnegie 
Mellon University. The four systems are being de-
veloped independently, yet it seems they share 
similar characteristics. Commercial and resource 
pressures currently make it impossible to try these 
different systems on the same data, and so per-
formance comparisons are meaningless: this is a 
real hindrance to progress in this area. The field of 
automatic marking really needs a MUC-style com-
petition to be able to develop and assess these tech-
niques and systems in a controlled and objective  
way.   
 
7. Current and Future Work 
 
The manually-engineered IE approach requires 
skill, much labour, and familiarity with both do-
main and tools. To save time and labour, various 
researchers have investigated machine-learning 
approaches to learn IE patterns (Collins et al 1999, 
Riloff 1993). We are currently investigating ma-
chine learning algorithms to learn the patterns used 
in IE (an initial skeleton-like algorithm can be 
found in Sukkarieh et al 2004).  
 
We are also in the process of evaluating our system 
along two dimensions: firstly, how long it takes, 
and how difficult it is, to customise to new ques-
tions; and secondly, how easy it is for students to 
use this kind of system for formative assessment. 
In the first trial, a domain expert (someone other 
than us) is annotating some new training data for 
us. Then we will measure how long it takes us (as 
computational linguists familiar with the system) 
to write IE patterns for this data, compared to the 
time taken by a computer scientist who is familiar 
with the domain and with general concepts of pat-
tern matching but with no computational linguis-
tics expertise. We will also assess the performance 
accuracy of the resulting patterns. 
 
For the second evaluation, we have collaborated 
with UCLES to build a web-based demo which 
will be trialled during May and June 2005 in a 
group of schools in the Cambridge (UK) area. Stu-
dents will be given access to the system as a 
method of self-assessment. Inputs and other as-
pects of the transactions will be logged and used to 
improve the IE pattern accuracy. Students? reac-
tions to the usefulness of the tool will also be re-
corded. Ideally, we would go on to compare the 
future examination performance of students with 
and without access to the demo, but that is some 
way off at present. 
 
References 
Collins, M. and Singer, Y. 1999. Unsupervised models 
for named entity classification.  Proceedings Joint 
SIGDAT Conference on Empirical Methods in Natural 
Language Processing  and Very Large Corpora, pp. 189-
196. 
   
Junker, M, M. Sintek & M. Rinck 1999. Learning for 
Text Categorization and Information Extraction with 
ILP. In: Proceedings of the 1st Workshop on Learning 
Language in Logic, Bled, Slovenia, 84-93. 
 
Leacock, C. and Chodorow, M. 2003. C-rater: Auto-
mated Scoring of Short-Answer Questions. Computers 
and Humanities 37:4. 
 
Mitchell, T. Russell, T. Broomhead, P. and Aldridge, N. 
2003. Computerized marking of short-answer free-text 
responses. Paper presented at the 29th annual confer-
ence of the International Association for Educational 
Assessment (IAEA), Manchester, UK.   
 
Muggleton, S. 1995. Inverting Entailment and Progol. 
In: New Generation Computing, 13:245-286. 
 
Porter, M.F. 1980. An algorithm for suffix stripping, 
Program, 14(3):130-137. 
 
Rennie, J.D.M., Shih, L., Teevan, J. and Karger, D. 
2003 Tackling the Poor Assumptions of Na?ve Bayes 
TextClassifiers. 
http://haystack.lcs.mit.edu/papers/rennie.icml03.pdf. 
15
 Riloff, E. 1993. Automatically constructing a dictionary 
for information extraction tasks. Proceedings 11th Na-
tional Conference on Artificial Intelligence, pp. 811-
816. 
 
Ros?, C. P. Roque, A., Bhembe, D. and VanLehn, K. 
2003. A hybrid text classification approach for analysis 
of student essays. In Building Educational Applications 
Using Natural Language Processing, pp. 68-75. 
 
Sukkarieh, J. Z., Pulman, S. G. and Raikes N. 2003. 
Auto-marking: using computational linguistics to score 
short, free text responses.  Paper presented at the 29th 
annual conference of the International Association for 
Educational Assessment (IAEA), Manchester, UK.   
 
Sukkarieh, J. Z., Pulman, S. G. and Raikes N. 2004. 
Auto-marking2: An update on the UCLES-OXFORD 
University research into using computational linguistics 
to score short, free text responses.  Paper presented at 
the 30th annual conference of the International Associa-
tion for Educational Assessment (IAEA), Philadelphia, 
USA. 
   
Witten, I. H. Eibe, F. 2000. Data Mining. Academic 
Press.  
 
16
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 42?44,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Towards Agile and Test-Driven Development in NLP Applications  
 
 Jana Z. Sukkarieh Jyoti Kamal  
 Educational Testing Service Educational Testing Service 
Rosedale Road  Rosedale Road 
Princeton, NJ 08541, USA Princeton, NJ 08541, USA 
Jsukkarieh@ets.org Jkamal@ets.org 
 
 
 
 
 
Abstract 
c-rater? is the Educational Testing Service technol-
ogy for automatic content scoring for short free-text 
responses. In this paper, we contend that an Agile 
and test-driven development environment optimizes 
the development of an NLP-based technology. 
1 Introduction 
c-rater (Leacock and Chodorow, 2003) is the Edu-
cational Testing Service technology for the auto-
matic content scoring of short free-text responses 
for items whose rubrics are concept-based. This 
means that a set of concepts or main points are pre-
specified in the rubric (see the example in Table 1). 
We view c-rater?s task as a textual entailment 
problem that involves the detection of whether a 
student?s answer entails a particular concept (with 
the additional challenge that the students? data con-
tains misspellings and grammatical errors). Our 
solution depends on a combination of rule-based 
and statistically-based NLP modules (Sukkarieh 
and Blackmore, 2009). In addition to databases, a 
JBOSS server (www.jboss.org), and two user inter-
faces, c-rater consists of 10 modules?eight of 
which are Natural Language Processing (NLP) 
modules. Figure 1 depicts the system?s architec-
ture. The c-rater engine is where all the linguistic 
processing and concept detection takes place. Sec-
tion 2 lists some of the major problems we face 
while developing such a complex NLP-based ap-
plication and how our adoption of Agile and test-
driven development is helping us.  
Example Item (Full Credit 2) 
Figures are given 
 
Prompt:  
 
The figures show three poly-
gons. Is the polygon in Figure 1 
an octagon, hexagon, or paral-
lelogram? Explain your answer. 
Concepts or main/key points: 
C1: The polygon/it is a quadri-
lateral with two sets of par-
allel sides OR the opposite 
sides are of equal length OR 
opposite angles are equal  
C2: The polygon/it has four/4 
sides 
Scoring rules:  
2 points for C1 (only if C2 is not present) 
1 point for C1 and C2  
Otherwise 0 
Table 1. Example item for c-rater scoring 
 
Figure 1. c-rater?s System Architecture 
2 Major Concerns and Solutions  
2.1 Communication 
In the past, the implementation of each module 
was done in isolation and communication among 
team members was lacking.  When a team member 
42
encountered a problem, it was only then that s/he 
would be aware of some logic or data structure 
changes by another member. This is not necessar-
ily an NLP-specific problem, however due to the 
particularly frequent modifications in NLP-based 
applications (see Section 2.2), communication is 
more challenging and updates are even more cru-
cial. The adoption of Scrum within Agile 
(Augustine, 2005) has improved communication 
tremendously. Although both the task backlog and 
the choice of tasks within each sprint is done by 
the product owner, throughout the sprint the plan-
ning, requirement analysis, design, coding, and 
testing is performed by all of the team members. 
This has been effecting in decreasing the number 
of logic design errors. 
2.2 Planning and Frequent Modification 
Very frequent modifications and re-prioritizing are, 
to a great extent, due to the nature of NL input and 
constant re-specification, extension, and customi-
zation of NLP modules. This could also be due to 
changes in business requirements, e.g. to tailor the 
needs of the application to a particular client?s 
needs. Further, this could be a response to emerg-
ing research, following a sudden intuition or per-
forming a heuristic approach. Agile takes care of 
all these issues. It allows the development to adapt 
to changes more quickly and retract/replace the last 
feature-based enhancement(s) when the need 
arises. It allows for incorporating research time and 
experimental studies into the task backlog; hence 
the various sprints. The nature of the Agile envi-
ronment allows us also to add tasks driven by the 
business needs and consider them highest in value.  
2.3 Metrics for Functionality and Progress 
Metrics for functionality includes measuring pro-
gress, comparing one version to another and moni-
toring the effect of frequent modifications. This 
particularly proves challenging due to the nature of 
c-rater?s tasks and the NLP modules. In most soft-
ware, the business value is a working product. In c-
rater, it is not only about producing a score but 
producing one for the ?right? reasons and not due 
to errors in the linguistic features obtained.  
Until recently, comparing versions meant compar-
ing holistic scores without a sense of the effect of 
particular changes. Evaluating the effect of a 
change often meant hand-checking hundreds and 
hundreds of cases. To improve monitoring, we 
have designed an engine test suite (each is a pair 
<model-sentence, answer> where model-sentence 
is a variant of a concept) and introduced automated 
testing. The suite is categorized according to the 
linguistic phenomenon of interest (e.g., passive, 
ergative, negation, appositive, parser output, co-
reference output). Some categories follow the phe-
nomena in Vanderwende and Dolan (2006). Some 
RTE data was transformed for engine tests. This 
produced a finer-grained view of the NLP modules 
performance, decreased the amount of hand-
checking, and increased our confidence about the 
?correctness? of our scores. 
2.4 Maintenance and Debugging 
Until very recently maintaining and debugging 
the system was very challenging. We faced many 
issues including the unsystematic scattering of 
common data structures, making it hard to manage 
dependencies; long functions making it difficult to 
track bugs; and late integration or lack of regular 
updates causing, at times, the system to crash or 
not compile. Although this may not be deemed 
NLP-specific, the need to modify NLP modules 
more frequently than anticipated has made this par-
ticularly challenging. To face this challenge, we 
introduced unit tests (UT) and continuous integra-
tion. We usually select some representative or 
?typical? NL input for certain phenomena, create 
an expected output, create a failed UT, and make it 
pass.  An additional challenge is that since stu-
dents? responses are noisy, sometimes choosing 
?typical? text is hard. Ideally, unit tests are sup-
posed to be written before or at the same time as 
the code; we were able to do that for approxi-
mately 40% of the code. The rest of the unit testing 
was being written after the code was written. For 
legacy code, we have covered around 10-20% of 
the code.  
 
In conclusion, we strongly believe like Degerstedt 
and J?nsson (2006), Agile and Test-Driven Devel-
opment form a most-suitable environment for 
building NLP-based applications.   
Acknowledgments 
Special thanks to Kenneth Willian, and Rene Law-
less. 
43
References  
Augustine, S. Managing Agile Projects. 2005. Published 
by Prentice Hall Professional Technical Reference. 
ISBN 0131240714, 9780131240711. 229 pages. 
Degerstedt, L. and J?nsson, A. 2006. LINTest, A devel-
opment tool for testing dialogue systems. In: Pro-
ceedings of the 9th International Conference on 
Spoken Language Processing (Interspeech/ICSLP), 
Pittsburgh, USA, pp. 489-492.   
Leacock, C. and Chodorow, M. 2003. C-rater: Auto-
mated Scoring of Short-Answer Question. Journal of 
Computers and Humanities. pp. 389-405. 
Sukkarieh, J. Z., & Blackmore, J. To appear. c-rater: 
Automatic Content Scoring for Short Constructed 
Responses. To appear in the Proceedings of the 22nd 
International Conference for the Florida Artificial In-
telligence Research Society, Florida, USA, May 
2009.   
Vanderwende, L. and Dolan, W. B. 2006. What Syntax 
Can Contribute in the Entailment Task. J. Quinonero-
Candela et al (eds.). Machine Learning Challenges, 
Lecture notes in computer science, pp. 205-216. 
Springer Berlin/Heidelberg.  
 
 
 
44
Proceedings of the 2009 Workshop on Applied Textual Inference, ACL-IJCNLP 2009, pages 61?69,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Automating Model Building in c-rater 
 
 
Jana Z. Sukkarieh 
Educational Testing Service  
Rosedale Road, Princeton, NJ 08541 
jsukkarieh@ets.org 
Svetlana Stoyanchev 
Stony Brook University  
Stony Brook, NY, 11794 
svetastenchikova@gmail.com 
 
  
 
 
Abstract 
c-rater is Educational Testing Service?s 
technology for the content scoring of short 
student responses.  A major step in the scor-
ing process is Model Building where vari-
ants of model answers are generated that 
correspond to the rubric for each item or test 
question. Until recently, Model Building 
was knowledge-engineered (KE) and hence 
labor and time intensive. In this paper, we 
describe our approach to automating Model 
Building in c-rater. We show that c-rater 
achieves comparable accuracy on automati-
cally built and KE models. 
1 Introduction 
c-rater (Leacock and Chodorow, 2003) is Edu-
cational Testing Service?s (ETS) technology 
for the automatic content scoring of short free-
text student answers, ranging in length from a 
few words to approximately 100 words. While 
other content scoring systems [e.g., Intelligent. 
Essay Assessor (Foltz, Laham and Landauer, 
2003), SEAR (Christie, 1999), IntelliMetric 
(Vantage Learning Tech, 2000)] take a holis-
tic 1  approach, c-rater takes an analytical ap-
proach to scoring content. The item rubrics 
specify content in terms of main points or con-
cepts required to appear in a student?s correct 
answer. An example of a test question or item 
follows: 
                                                 
1 Holistic means an overall score is given for a student?s 
answer as opposed to scores for individual components of 
a student?s answer. 
 
Item 1 (Full credit: 2 points) 
Stimulus: A Reading passage 
 
Prompt:  
In the space below, write the 
question that Alice was most 
likely trying to answer when 
she performed Step B. 
Concepts or main/key points: 
C :1  How does rain forma-
tion occur in winter? 
C : 2 How is rain formed? 
C : 3 How do temperature 
and altitude contribute 
to the formation of 
rain? 
 
Scoring rules:  
2 points for C1  
1 for C2 (only if C1 is not present) 
1 for C3 (only if C1 and C2 are not present)  
Otherwise 0 
 
We view c-rater's task as a textual entailment 
(TE) problem. We use TE here to mean either 
a paraphrase or an inference (up to the context 
of the item or test question). c-rater's task is 
reduced to a TE problem in the following way:  
 
Given a concept, C, (e.g., ?body increases 
its temperature?) and a student answer, A, 
(e.g., either ?the body raises temperature,? 
?the body responded. His temperature was 
37? and now it is 38?,? or ?Max has a fe-
ver?) and the context of the item, the goal 
is to check whether C is an inference or 
paraphrase of A (in other words, A implies 
C and A is true). 
 
There are four main steps in c-rater. The first 
one is Model Building (MB), where a set of 
model answers are generated (either manually 
or automatically). Second, c-rater automati-
cally processes model answers and students? 
answers using a set of natural language proc-
essing (NLP) tools and extracts the linguistic 
features. Third, the matching algorithm  
Goldmap uses the linguistic features culmi-
nated from both MB and NLP to automatically 
determine whether a student?s response entails 
the expected concepts. Finally, c-rater applies 
61
the scoring rules to produce a score and feed-
back that justifies the score to the student.  
 
Until recently, MB was knowledge-engineered 
(KE). The KE approach for one item required, 
on average, 12 hours of time and labor. This 
paper describes our approach to automatic MB. 
We show that c-rater achieves comparable ac-
curacy on automatically- and manually-built 
models. Section 2 outlines others? work in this 
domain and emphasizes the contribution of this 
paper. Section 3 outlines c-rater. In Section 4, 
we describe how MB works. Section 5 ex-
plains how we automate the process. Prior to 
the conclusion, we report the evaluation of this 
work.    
 
2 Automatic Content Scoring:  
Others? Work  
A few systems that deal with both short an-
swers and analytic-based content exist. The 
task, in general, is reduced to comparing a stu-
dent?s answer to a model answer. Recent work 
by Mohler and Mihalcea (2009) at the Univer-
sity of North Texas uses unsupervised methods 
in text-to-text semantic similarity comparing 
unseen students? answers to one correct an-
swer. Previous work, including c-rater, used 
supervised techniques to compare unseen stu-
dents? answers to the space of potentially ?all 
possible correct answers? specified in the ru-
bric of the item at hand. The techniques varied 
from information extraction with knowledge-
engineered patterns representing the model 
answers [Automark at Intelligent Assessment 
Technologies (Mitchell, 2002), the Oxford-
UCLES system (Sukkarieh, et. al., 2003) at the 
University of Oxford] to data mining tech-
niques using very shallow linguistic features 
[e.g., Sukkarieh and Pulman (2005) and Car-
melTC at Carnegie Mellon University (Rose, 
et al 2003)]. Data mining techniques proved 
not to be very transparent when digging up 
justifications for scores. 
 
c-rater?s model building process is similar to 
generating patterns but the patterns in c-rater 
are written in English instead of a formal lan-
guage. The aim of the process is to produce a 
non-trivial space of possible correct answers 
guided by a subset of the students? answers. 
The motivation is that the best place to look for 
variations and refinements for the rubric is the 
students? answers. This is what test developers 
do before piloting a large-scale exam. From an 
NLP point of view, the idea is that generating 
this space will make scoring an unseen answer 
easier than just having one correct answer. 
However, similar to what other systems re-
ported, generating manually-engineered pat-
terns is very costly. In Sukkarieh et al (2004) 
there was an attempt to generate patterns 
automatically but the results reported were not 
comparable to those using manually-generated 
patterns. This paper presents improvements on 
previous supervised approaches by automating 
the process of model-answer building using 
well-known NLP methods and resources while 
yielding comparable results to knowledge-
engineered methods.  
3 c-rater, in Brief 
In c-rater, manual MB has its own graphical 
interface, Alchemist. MB uses the NLP tools 
and Goldmap (which reside in the c-rater 
Engine). On the other hand, Goldmap depends 
on the model generated. The c-rater Engine 
performs NLP on input text and concept rec-
ognition or TE between the input text and each 
concept (see Figure 1). First, a student answer 
is processed for spelling corrections in an at-
tempt to decrease the noise for subsequent 
NLP tools. In the next stage, parts-of-speech 
tagging and parsing are performed (the 
OpenNLP parser is used 
http://opennlp.sourceforge.net). In the third 
stage, a parse tree is passed through a feature 
extractor. Manually-generated rules extract 
features from the parse tree. The result is a flat 
structure representing phrases, predicates, and 
relationships between predicates and entities. 
Each phrase is annotated with a label indicat-
ing whether it is independent or dependent. 
Each entity is annotated with a syntactic and 
semantic role. In the pronoun resolution 
stage, pronouns are resolved to either an entity 
in the student?s answer or the question. Finally, 
a morphology analyzer reduces words to their 
lemmas.2 The culmination of the above tools 
results in a set of linguistic features used by the 
matching algorithm, Goldmap. In addition to 
the item-independent linguistic features col-
lected by the NLP tools, Goldmap uses item-
dependent features specified in MB to decide 
whether a student?s answer, A, and a model 
                                                 
2 We do not go into detail, assuming that the reader is 
familiar with the described NLP techniques. 
62
answer match, i.e. that concept C represented 
in the model answer, is entailed by A.   
 
 
 
Figure 1. c-rater Engine   
 
4 KE Model Building 
A dataset of student answers for an item is split 
into development (DEV), cross-validation 
(XVAL), and blind (BLIND) datasets. DEV is 
used to build the model, XVAL is used to vali-
date it and BLIND is used to evaluate it. All 
datasets are double-scored holistically by hu-
man raters and the scoring process takes an 
average 3 hours per item for a dataset of 
roughly 200 answers. 
 
For each concept Ci in item X, a model builder 
uses DEV to create a set of Model Sentences 
(MSij) that s/he believes entails concept Ci in 
the context of the item. S/he is required to 
write MSij in complete sentences. For each 
model sentence MSij,, the model builder selects 
the Required Lexicon (RLijk), a set of the most 
essential lexical entities required to appear in a 
student?s answer. Then, for each RLijk, the 
model builder selects a set of Similar Lexicon 
(SLijkt), guided by the list of words automati-
cally extracted from a dependency-based the-
saurus (cs.ualberta.ca/~lindek/downloads.htm).  
 
The process is exemplified in Figure 2. Pre-
sented with the concept, ?What causes rain to 
form in winter time?,? a model builder writes 
model sentences like ?Why does rain fall in 
the winter?,? highlights or selects lexical items 
that s/he believes are the required tokens  
(e.g., ?why,? ?rain,? ?fall,? ?in,? ?winter?) 
and writes a list of similar lexical entities for 
each required token if needed (e.g., {descend, 
go~down, ?} are similar to words like?fall?).3
 
 
 
Figure 2. KE Model Building 
 
The model for each item X is comprised of the 
scoring rules, the collections of model sen-
tences MSij, associated lexical entities RLijk, 
and corresponding similar lexicon SLijkt. Each 
model answer is written in terms of MSij 
where:  
 
MSij entails Ci for i=1,?, N, and N is the 
number of concepts specified for item X. 
For each concept Ci, Goldmap checks 
whether answer A entails Ci, by check-
ing whether A entails one of the model 
sentences MSij, given the additional fea-
tures RLijk and corresponding SLijkt. 
 
In practice, model building works as follows. 
The model builder, guided by the DEV dataset 
and holistic scores, starts with writing a few 
model sentences and selects corresponding 
required (RLijk) and similar (SLijkt) lexicon. 
S/he then uses the c-rater engine to automati-
cally evaluate the model using the DEV data-
set, i.e., using the model produced up to that 
point. Goldmap is used to detect if any answers 
in the DEV dataset contain any of the model 
sentences and scores are assigned for each an-
swer. If the scoring agreement between c-rater 
and each of the two human raters (in terms of a 
kappa statistic) is much lower than that be-
tween the two human raters, then the model is 
judged unsuitable and the process continues 
iteratively until kappa statistics on the DEV 
dataset are satisfactory, i.e., c-rater?s agree-
ment with human raters is as high as the kappa 
between human raters. Once kappa statistics on 
DEV are satisfactory, the model builder uses  
                                                 
3 We use lexicon, lexical entities, words, terms and to-
kens interchangeably meaning either uni- or bi-grams. 
63
c-rater to evaluate the model on the XVAL 
dataset automatically. Again, until the scoring 
agreement between c-rater and human raters 
on XVAL dataset is satisfactory, the model 
builder iteratively changes the model. Unlike 
the DEV dataset, the XVAL dataset is never 
seen by a model builder. The logic here is that 
over-fitting DEV is a concern, making it hard 
or impossible to generalize beyond this set. 
Hence, the results on XVAL can help prevent 
over-fitting and ideally would predict results 
over unseen data. 
    
Note that a model builder can introduce what 
we call a negative concept Ci-1 for a concept Ci 
and adjust the scoring rules accordingly. When 
this happens, a model builder writes model 
sentences MSi-1j  entailing Ci-1 , and selects re-
quired words RLi-1jk and corresponding similar 
words SLi-1jkt  in the same way for any other 
(positive) concept. 
 
On average, MB takes 12 hours of manual 
work per item (plus 2 hours, on average, for an 
optional model review by someone other than 
the model builder). This process is time con-
suming and error-prone despite utilizing a 
user-friendly interface like Alchemist. In addi-
tion, the satisfaction criterion while building a 
model is subjective to the model builder.  
5 Automated Model Building 
The process of writing model sentences de-
scribed above involves: 1) finding the parts of 
students? answers containing the concept for 
each expected concept, 2) abstracting over 
?similar? parts, and 3) representing the abstrac-
tion in one (or more) model sentence(s). The 
process, as mentioned earlier, is similar to 
writing rules for information extraction, but 
here one writes them in English sentences and 
not in a formal language. In practice, there is 
no mechanism in Alchemist to cluster ?simi-
lar? parts and MB, in this aspect, is not per-
formed in any systematic manner. Hence, we 
introduce what we call concept-based scoring 
? used instead of the holistic human scoring. In 
concept-based scoring, human raters annotate 
students? responses for each concept C, and 
highlight the part of the answer that entails C.  
In Sukkarieh and Blackmore (2009), we de-
scribe concept-based scoring in detail and how 
this helps in the KE-MB approach. In this pa-
per, we extend the approach by showing how 
concept-based scores used in the automated 
approach reduce the time needed for MB sub-
stantially while yielding comparable results. 
Concept-based scoring is done manually. On 
average, it takes around 3.5 hours per item for 
a dataset of roughly 200 answers.  
 
The MB process is reduced to: 
  
1. Concept-based scoring 
2. Automatically selecting required lexicon 
3. Automatically selecting similar lexicon 
 
While holistic scoring takes on average 3 hours 
for a dataset of 200 answers, concept-based 
scoring takes 3.5 hours for the same set. How-
ever, automated MB takes 0 hours of human 
intervention?a substantial reduction over the 
12 hours required for manual MB.    
5.1   Concept-based Scoring 
We have developed a concept-based scoring 
interface (CBS) that can be customized for 
each item [due to lack of space we do not in-
clude an illustration].  The CBS interface dis-
plays a student?s answer to an item and all of 
the concepts corresponding to that item. The 
terms {Absent, Present, Negated} are what we 
call analytic or concept-based scores. Using 
CBS, the human scorer clicks Present when a 
concept is present and Negated when a concept 
is negated or refuted (the default is Absent). 
This is done for each concept. The human 
scorer also highlights the part of a student?s 
answer that entails the concept in the context 
of the item. We call a quote corresponding to 
concept C ?Positive Evidence? or ?Negative 
Evidence? for Present and Negated, respec-
tively. For example, assume a student answer 
for Item 1 is ?Her research tells us a lot about 
rain and hail; in particular, the impact that 
temperature variations have on altitude con-
tribute to the formation of rain.? For  
Concept C3, the human rater highlights the 
Positive Evidence, ?the impact that tempera-
ture variations have on altitude contribute to 
the formation of rain.? Parts of answers corre-
sponding to one piece of Evidence (positive or 
negative) do not need to be in the same sen-
tence and could be scattered over a few lines.  
 
Similar to the KE approach, we split the  
double-concept-based scored dataset into DEV 
and XVAL sets. However, the splitting is done 
64
according to the presence (or absence) of a 
concept. We use stratified sampling (Tucker, 
1998) trying to uniformly split data such that 
each concept is represented in the DEV as well 
as the XVAL datasets. As mentioned earlier, 
the KE approach can include negative con-
cepts; currently we do not use Negative Evi-
dence automatically. In the remainder of this 
paper, Evidence is taken to mean the collection 
of Positive Evidence.    
5.2 Automatically Selecting Model  
Sentences 
Motivation
During manual MB with Alchemist, a model 
builder is guided by the complete set of stu-
dents? answers in the DEV dataset, including 
holistic scores. Concept-based scoring allows a 
model builder, if we were to continue the man-
ual MB, to be guided by concept-based scores 
and students? answers highlighted with the 
Evidence that corresponds to each concept 
when writing model sentences as shown, 
where MSij entails Ci and Eir entails Ci. 
 
Concept Ci Evidence Eir MSij
C1 E11 MS11
 E1s1 MS1t1
C2 E21 MS21
 E2s2 MS2t2
Cn ? ? 
 
Further, students may misspell, write ungram-
matically, or use incomplete sentences. Hence, 
Evidence may contain spelling and grammati-
cal errors. Evidence may also be in the form of 
incomplete sentences. Although human model 
builders generating sentences with Alchemist 
are asked to write complete MSij,, there is no 
reason why MSij, needs to be in the form of 
complete sentences. The NLP tools in the  
c-rater engine can cope with a reasonable 
amount of misspelled words as well as un-
grammatical and/or incomplete sentences.  
 
We observe the following: 
 
1. Concepts are seen as a set of model sen-
tences that are subsumed by the list of 
model sentences built by humans 
2. Evidence is seen as a list of model 
?sentences? that nearly subsume the set gener-
ated by humans (i.e., the intersection is not 
empty)   
Approach 
In the automatic approach, we select the Evi-
dence highlighted in the DEV dataset as MSijs. 
We either choose the intersection of Evidence 
(i.e., where both human raters agree) or the 
union (i.e., highlighted by either human) as 
entailing a concept.  
5.3 Automatically Selecting Required 
Lexicon 
Motivation 
Required lexicon for an item includes the most 
essential lexicon for this item. In the KE ap-
proach, the required lexicon is selected by the 
model builder, who makes a judgment about it. 
In Alchemist, a model builder is presented 
with a tokenized model sentence and s/he 
clicks on a token to select it as a required lexi-
cal entity. 
  
We have observed that selecting required lexi-
con RLijk involves ignoring or removing noise, 
such as stop-words (e.g., ?a,? ?the,? ?to,? etc.), 
from the presented model sentence. For exam-
ple, a model builder may select the words, 
?how,? ?rain,? ?formation,? and ?winter? in 
the model sentence ?How does rain formation 
occur in the winter?? and ignore the rest. In 
addition, there might be words other than stop-
words that can be ignored. For example, if a 
model builder writes, ?It may help Alice and 
scientists to know how rain formation occurs 
in the winter? ? the tokens ?scientists? and 
?Alice? are not stop-words and can be ignored.  
Approach 
We evaluate five methods of automatically 
selecting the required lexicon: 
 
1. Consider all tokens in MSij  
2. Consider all tokens in MSij without stop-
words 
3. Consider all heads of NPs and VPs (nouns 
and verbs) 
4. Consider all heads of all various syntactic 
roles including adjectives and adverbs 
5. Consider the lexicon with the highest mu-
tual information measures, with all lexical 
tokens in model sentences corresponding 
to the same concept   
 
65
The first method does not need any elabora-
tion. In the following, we briefly elaborate on 
each of the other methods. 
 
5.3.1 All Words Without Stop Lexicon 
In addition to the list of stop-words provided in 
Van Rijsbergen?s book (Rijsbergen, 2004) and 
the ones we extracted from WordNet 2.0 
http://wordnet.princeton.edu/
(except for ?zero,? ?minus,? ?plus,? and ?op-
posite?), we have developed a list of approxi-
mately 2,000 stop-words based on students? 
data. This includes various interjections and 
common short message service (SMS) abbre-
viations that are found in students? data (see 
Table 1 for examples).  
 
1. Umm 2. Aka 3. Coz 
4. Viz. 5. e.g. 6. Hmm 
7. Phew 8. Aha 9. Wow 
10. Ta 11.Yippee 12. NTHING 
13. Dont know 14. Nada 15. Guess 
16. Yoink 17. RUOK 18. SPK 
Table 1. Student-driven stop-words 
 
5.3.2 Head Words of Noun and Verb 
Phrases  
The feature extractor in c-rater, mentioned in 
Section 2, labels the various noun and verb 
phrases with a corresponding syntactic or se-
mantic role using in-house developed rules. 
We extract the heads of these by simply con-
sidering the rightmost lexical entity with an 
expected POS tag, i.e., for noun phrases we 
look for the rightmost nominal lexical entity, 
for verb phrases we look for the rightmost 
verbs.   
 
5.3.3 Head Words of all Phrases 
We consider all phrases or syntactic roles, i.e., 
not only noun and verb phrases but also adjec-
tive and adverb phrases. 
 
5.3.4 Words with Highest Mutual  
Information  
The mutual information (MI) method measures 
the mutual dependence of two variables. MI in 
natural language tasks has been used for in-
formation retrieval (Manning et. al., 2008) and 
for feature selection in classification tasks 
(Stoyanchev and Stent, 2009).  
 
Here, MI selects words that are indicative of 
the correct answer while filtering out the words 
that are also frequent in incorrect answers. Our 
algorithm selects a lexical term if it has high 
mutual dependence with a correct concept or 
Evidence in students? answers. For each term 
mentioned in a students? answer we compute 
mutual information measure (I): 
 
where N11 is the number of student answers 
with the term co-occurring with a correct con-
cept or Evidence, N01 is the number of student 
answers with a correct concept but without the 
term, N10 is the number of student answers 
with the term but without a correct concept, 
N00 is the number of student answers with nei-
ther the term nor a correct concept, N1. is the 
total number of student answers with the term, 
N.1 is the total number of utterances with a cor-
rect concept, and N is the total number of ut-
terances. The MI method selects the terms or 
words predictive of both presence and absence 
of a concept.  In this task we are interested in 
finding the terms that indicate presence of a 
correct concept. We ignore the words that are 
more likely to occur without the concept (the 
words for which N11< N10). In this study, after 
looking at the list of words produced, we sim-
ply selected the top 40 words with the highest 
mutual information measure.  
5.4 Automatically Selecting Similar  
Lexicon 
Motivation 
In the KE approach, once a model builder se-
lects a required word, a screen on Alchemist 
lists similar words extracted automatically 
from Dekang Lin?s dependency-based thesau-
rus. The model builder can also use other re-
sources like Roget?s thesaurus 
(http://gutenberg.org/etext/22) and WordNet 
3.0 (http://wordnet.princeton.edu/). The model 
builder can also write her/his own words that 
s/he believes are similar to the required word.  
 
Approach 
Other than choosing no similar lexicon to a 
required word W, automatically selecting simi-
66
lar lexicon consists of the following experi-
ments: 
 
1. All words similar to W in Dekang Lin?s 
generated list 
2. Direct synonyms for W or its lemma from 
WordNet 3.0 (excluding compounds). 
Compounds are excluded because we no-
ticed many irrelevant compounds that 
could not replace uni-grams in our data. 
3. All similar words for W or its lemma from 
WordNet 3.0, i.e., direct synonyms, related 
words and hypernyms (excluding com-
pounds). Hypernyms of W are restricted to 
a maximum of 2 levels up from W 
 
To summarize, for each concept in the KE ap-
proach, a model builder writes a set of Model 
Sentences, manually selects Required Lexicon 
and Similar Lexicon for each required word. In 
the automated approach, all of the above is 
selected automatically. Table 2 summarizes the 
methods or experiments. We refer to a method 
or experiment in the order of selection of RLijk 
and SLijkt; e.g., we denote the method where all 
words were required and similar lexicon cho-
sen from WordNet Direct synonyms by AWD. 
HSVocWA denotes the method where heads of 
NPs and VPs with similar words from Word-
Net All, i.e., direct, related, and hypernyms are 
selected.  A method name preceded by I or U 
refers to Evidence Intersection or Union, re-
spectively. For each item, there are 40 experi-
ments/methods performed with Evidence as 
model sentences. 
 
Model 
Sentences Required Lexicon Similar Lexicon 
Concepts  
(C) 
 
All words (A) None chosen (N) 
Evidence 
Intersection 
(I) 
 
All words with no stop-
words (S) 
Lin all (L) 
Evidence 
Union (U) 
Heads of NPs and VPs 
(HSvoc) 
WordNet direct 
synonyms (WD) 
 Heads of all phrases (HA) WordNet al 
similar words 
(WA) 
 Highest Mutual informa-
tion measure (M) 
 
Table 2. Parameters and ?Values? of Model  
Building 
Before presenting the evaluation results, we 
make a note about spelling correction. c-rater 
has its own automatic spelling corrector. Here, 
we only outline how spelling correction relates 
to a model. In the KE approach, model sen-
tences are assumed to not having spelling er-
rors. We use the model sentences, the stimulus 
(if it exists), and the prompt of the item for 
additional guidance to select the correctly-
spelled word from a list of potential correctly-
spelled words designated by the spelling cor-
rector. On the other hand, the Evidence can be 
misspelled. Consequently, when the Evidence 
is considered for model sentences, the spelling 
corrector first performs spelling correction on 
the Evidence, using stimulus, concepts, and 
prompts as guides. The students? answers are 
then corrected, as in the KE approach. 
6 Evaluation 
The study involves 12 test items developed at 
ETS for grades 7 and 8. There are seven Read-
ing Comprehension items, denoted R1-R7 and 
five Mathematics items, denoted M1-M5. 
Score points for the items range from 0 to 3 
and the number of concepts ranges from 2 to 7. 
The answers for these items were collected in 
schools in Maine, USA. The number of an-
swers collected for each item ranges from 190-
264. Answers were concept-based scored by 
two human raters (H1, H2). We split the dou-
ble-scored students? answers available into 
DEV (90-100 answers), XVAL (40-50) and 
BLIND (60-114). Training data refer to DEV 
together with XVAL datasets.  Results are re-
ported in terms of un-weighted kappa, repre-
senting scoring agreement with humans on the 
BLIND dataset.  H1/2 refers to the agreement 
between the two humans, c-H1/2 denotes the 
average of kappa values between c-rater and 
each human (c-H1 and c-H2). Table 3 reports 
the best kappa over the 40 experiments on 
BLIND (Auto I or U). The baseline (Auto C) 
uses concepts as model sentences.  
 
Item 
#Training 
(Blind) H1/2 Manual 
Auto 
C 
Auto 
I or U 
   c-H1/2 c-H1/2 c-H1/2 
R1 150  (114) 1.0    0.94   0.51 0.97 
R2 150  (113) 0.76    0.69   0.28 0.76 
R3 150  (107) 0.96    0.87   0.18 0.88 
R4 150    (66) 0.77    0.71   0.46 0.75 
R5 130    (60) 0.71    0.58   0.22 0.61 
R6 130    (61) 0.71    0.73   0.23 0.77 
R7 130    (61) 0.87    0.55   0.42 0.42 
M1 130    (67) 0.71      0.6   0.0 0.66 
M2 130    (67) 0.8     0.71   0.54 0.67 
M3 130    (67) 0.86    0.76   0.0 0.79 
M4 130    (67) 0.87    0.82   0.13 0.82 
M5 130    (67) 0.77    0.63   0.29 0.65 
Table 3. Best on BLIND over all experiments 
67
The accuracy using the automated approach 
with Evidence as model sentences is compara-
ble to that of the KE approach (noted in the 
column labeled, ?Manual?) with a 0.1 maxi-
mum difference in un-weighted kappa statis-
tics. The first methods (in terms of running 
order) yielding the best results for the items (in 
order of appearance in Table 3) are ISWD, 
ISW, ISN, IMN, IHSVocN, UHALA, ISN, 
UHSVocN, SLA, ISN, IHAN and IHS-
VocWA. The methods yielding the best results 
(regardless of running order) for all items us-
ing the Evidence were: 
IHAN U/IHAWD IHAWA 
U/IHALA U/IHSvocN IHSvocWA 
UHSvocLA UHSvocWA UHSvocWD 
U/ISLA U/ISN U/ISWA 
U/ISWD U/IAWA IMN 
IMWD   
This approach was only evaluated on a small 
number of items. We expect that some meth-
ods will outperform others through additional 
evaluation.  
In an operational setting (i.e., not a research 
environment), we must choose a model before 
we score the BLIND data. Hence, a voting 
strategy over all the experiments has to be de-
vised based on the results on DEV and XVAL. 
Following our original logic, i.e., using XVAL 
to avoid over-fitting and predicting the results 
of BLIND, we implemented a simple voting 
strategy. We considered c-H1/2 on XVAL for 
each experiment. We found the maximum over 
all the c-H1/2 for all experiments. The model 
corresponding to the maximum was considered 
the model for the item and used to score the 
BLIND data.  When there was a tie, the first 
method to yield the maximum W chosen.  
Table 4 shows the results on BLIND using the 
voting strategy. The results are comparable to 
those of the manual approach except for R7 
which has 7 concepts, the highest number of 
concepts among all items. The results also 
show that the voting strategy did not select the 
?best? model or experiment. We notice that 
some methods were better in detecting whether 
an answer entailed a concept C than detecting 
whether it entailed another  
concept D, specified for the same item. This 
implies that the voting strategy will have to be 
a function that not only considers the overall 
kappa agreement (i.e., holistic scores), but 
concept-based agreement (i.e., using concept-
based scores).  Next, we noticed that for R7, 
XVAL did not predict the results on BLIND. 
This was mainly due to the inability to apply 
stratified sampling with such a small sample 
size when there are 7 concepts involved. Fur-
ther, we may need to take advantage of the 
training data differently, e.g. an n-fold cross-
validation approach. Finally, when there is a 
tie, factors other than running order should be 
considered. 
 
Item 
#Training 
(Blind) H1/2 Manual 
Auto 
(C) 
Auto 
(I or U) 
   c-H1/2 c-H1/2 c-H1/2 
R1 150  (114) 1.0    0.94   0.51 0.88 
R2 150  (113) 0.76   0.69   0.18 0.61 
R3 150  (107) 0.96   0.87   0.18 0.86 
R4 150    (66) 0.77   0.71   0.38 0.67 
R5 130    (60) 0.71   0.58   0.17 0.51 
R6 130    (61) 0.71   0.73   0.13 0.73 
R7 130    (61) 0.87   0.55   0.39 0.16 
M1 130     67) 0.71    0.6    0.0 0.65 
M2 130     67) 0.8    0.71   0.54 0.58 
M3 130     67) 0.86   0.76   0.0 0.79 
M4 130     67) 0.87   0.82   0.13 0.68 
M5 130     67) 0.77   0.63   0.26 0.49 
Table 4. Voting Strategy results on BLIND 
In all of the above experiments, the Evidence 
was corrected using the c-rater?s automatic 
spelling corrector using the stimulus (in case of 
Reading), the concepts, and the prompts to 
guide the selection of the correctly-spelled 
words. 
7 Conclusion 
Analytic-based content scoring is an applica-
tion of textual entailment. The complexity of 
the problem increases due to the noise in stu-
dent data, the context of an item, and different 
subject areas. In this paper, we have shown 
that building a c-rater scoring model for an 
item can be reduced from 12 to 0 hours of hu-
man intervention with comparable scoring per-
formance. This is a significant improvement on 
research to date using supervised techniques.  
In addition, as far as we know, no one other 
than Calvo et al (2005) made any comparisons 
between a manually-built ?thesaurus? (e.g. 
WordNet) and an automatically-generated 
?thesaurus? (e.g. Dekang Lin?s database) in an 
NLP task or application prior to our work. Our 
next step is to evaluate (and refine) the ap-
proach on a larger set of items. Further im-
provements will include using Negative Evi-
dence, automating concept-based scoring, in-
vestigating a context-sensitive selection of 
similar words using the students? answers and 
experimenting with various voting strategies. 
Finally, we need to compare the results re-
ported using unsupervised techniques on the 
same items and datasets if possible.   
68
Acknowledgments 
Special thanks to Michael Flor, Rene Lawless, 
Sarah Ohls and Waverely VanWinkle. 
References 
Calvo H., Gelbukh A., and Kilgarriff A. (2005). 
Distributional thesaurus vs. WordNet: A com-
parison of backoff techniques for unsupervised 
PP attachment. In CICLing.  
Christie, J.R. (1999). Automated essay marking for 
both content and style. In Proceedings of the 3rd 
International Computer Assisted Assessment 
Conference. Loughborough University. 
Loughborough, Uk. 
Foltz, P.W. and Laham, D. and Landauer, T.K. 
(2003) Automated essay scoring. Applications to 
Educational technology. http://www-
psych.nmsu.edu/%7Epfoltz/reprints/Edmedia99.
html 
Leacock, C. and Chodorow, M. (2003) C-rater: 
Automated Scoring of Short-Answer Questions. 
Computers and Humanities. pp.  389-405 
Manning C. D., Raghavan P., and Sch?utze H. 
(2008). Introduction to Information Retrieval. 
Cambridge University Press. 
Mitchell, T. and Russel, T. and Broomhead, P. and 
Aldrige, N. (2002) Towards robust computerised 
marking of free-text responses. Proceedings of 
the 6th International Computer Assisted As-
sessment Conference. 
Mohler M. and Mihalcea R (2009). Text-to-text 
Semantic Similarity for Automatic Short Answer 
Grading. Proceedings of the European Chapter 
of the Association for Computational Linguis-
tics, Athens, Greece, March 2009. 
Ros?, C. P. and Roque, A. and Bhembe, D. and 
VanLehn, K.. (2003) A hybrid text classification 
approach for analysis of student essays. Proceed-
ings of the HLT-NAACL 03 Workshop on Edu-
cational Applications of NLP.  
Stoyanchev S. and Stent A. (2009). Predicting Con-
cept Types in User Corrections in Dialog. Pro-
ceedings of EACL Workshop on the Semantic 
Representation of Spoken Language. Athens, 
Greece. 
Sukkarieh, J. Z., and Blackmore, J. (2009). c-rater: 
Automatic Content Scoring for Short Con-
structed Responses. Proceedings of the 22nd In-
ternational Conference for the Florida Artificial 
Intelligence Research Society, Florida, USA. 
Sukkarieh, J.Z. and Stephen G. Pulman (2005). 
Information Extraction and Machine Learning: 
Auto-marking short free-text responses for Sci-
ence questions. Proceedings of the 12th Interna-
tional conference on Artificial Intelligence in 
Education, Amsterdam, The Netherlands. 
Sukkarieh, J.Z. Pulman S. G. and Raikes, N. 
(2004). Auto-marking 2: An update on the 
UCLES-Oxford University research into using 
computational linguistics to score short, free text 
responses. Proceedings of the AIEA, Philadel-
phia, USA. 
Sukkarieh, J. Z. and Pulman, S. G. and Raikes, N. 
(2003) Auto-marking: using computational lin-
guistics to score short, free text responses.  
Proceedings of international association of 
educational assessment. Manchester, UK. 
Tucker H. G. (1998) Mathematical Methods in 
Sample Surveys. Series on multivariate analysis 
Vol. 3. University of California, Irvine.  
Van Rijsbergen C. J. ( 2004) The Geometry of In-
formation Retrieval. Cambridge University 
Press.  The Edinburgh Building, Cambridge, 
CB2 2RU, UK. 
Vantage. (2000) A study of expert scoring and In-
telliMetric scoring accuracy for dimensional 
scoring of grade 11 student writing responses. 
Technical report RB-397, Vantage Learning 
Tech. 
   
 
 
 
69
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 122?126,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Scoring Spoken Responses Based on Content Accuracy
Fei Huang
CS Dept. Temple Univ.
Philadelphia, PA, 19122
tub58431@temple.edu
Lei Chen
Educational Testing Service (ETS)
Princeton, NJ, 08541
lchen@ets.org
Jana Sukkarieh
ETS
JSukkarieh@ets.org
Abstract
Accuracy of content have not been fully uti-
lized in the previous studies on automated
speaking assessment. Compared to writing
tests, responses in speaking tests are noisy
(due to recognition errors), full of incomplete
sentences, and short. To handle these chal-
lenges for doing content-scoring in speaking
tests, we propose two new methods based
on information extraction (IE) and machine
learning. Compared to using an ordinary
content-scoring method based on vector anal-
ysis, which is widely used for scoring written
essays, our proposed methods provided con-
tent features with higher correlations to human
holistic scores.
1 Introduction
In recent years, there is an increasing interest of
using speech processing and natural language pro-
cessing (NLP) technologies to automatically score
speaking tests (Eskenazi, 2009). A set of features
related to speech delivery, such as fluency, pronun-
ciation, and intonation, has been utilized in these
studies. However, accuracy of an answer?s content
to the question being asked, important factors to be
considered during the scoring process, have not been
fully utilized. In this paper, we will report our ini-
tial efforts exploring content scoring in an automated
speaking assessment task. To start, we will briefly
describe the speaking test questions in our research.
In the test we used for evaluation, there were
two types of questions. The first type, survey,
requires a test-taker to provide answers specific
to one or several key points in a survey ques-
tion without any background reading/listening re-
lated to the topic of the survey. Typical questions
could be ?how frequently do you go shopping?? or
?what kind of products did you purchase recently??
In contrast, the second type, opinion, requires a test-
taker to speak as long as 60 seconds to present his
or her opinions about some topic. An example of
such questions could be, ?Do you agree with the
statement that online shopping will be dominant in
future or not?? Compared to the essays in writing
tests, these spoken responses could just be incom-
plete sentences. For example, for the survey ques-
tions, test-takers could just say several words. For
the questions described above, some test-takers may
just use phrases like ?once a week? or ?books?. In
addition, given short responding durations, the num-
ber of words in test-takers? responses is limited. Fur-
thermore, since scoring speech responses requires
speech recognition, more noisy inputs are expected.
To tackle these challenges, we propose two novel
content scoring methods in this paper.
The remainder of the paper is organized as fol-
lows: Section 2 reviews the related previous re-
search efforts; Section 3 proposes the two content-
scoring methods we designed for two types of ques-
tions described above; Section 4 reports the experi-
mental results of applying the proposed methods; fi-
nally, Section 5 concludes our reported research and
describes our plans for future research.
2 Related Work
For writing tests, previous content scoring investiga-
tions can be divided into the following three groups.
The first group relies on obtaining and matching pat-
terns associated with the correct answers (Leacock
and Chodorow, 2003; Sukkarieh and Blackmore,
2009).
The second group of methods, also mostly used
122
for content-scoring, is to rely on a variety of text
similarity measurements to compare a response with
either pre-defined correct answers or a group of re-
sponses rated with a high score (Mohler and Mihal-
cea, 2009). Compared to the first group, such meth-
ods can bypass a labor intensive pattern-building
step. A widely used approach to measuring text
similarity between two text strings is to convert
each text string into a word vector and then use
the angle between these two vectors as a similar-
ity metric. For example, Content Vector Analy-
sis (CVA) has been successfully utilized to detect
off-topic essays (Higgins et al, 2006) and to pro-
vide content-related features for essay scoring (At-
tali and Burstein, 2004). For this group of meth-
ods, measuring the semantics similarity between two
terms is a key question. A number of metrics have
been proposed, including metrics (Courley and Mi-
halcea, 2005) derived from WordNet, a semantics
knowledge database (Fellbaum, 1998), and metrics
related to terms? co-occurrence in corpora or on the
Web (Turney, 2001).
The third group of methods treats content scor-
ing as a Text Categorization (TC) task, which treats
the responses being scored on different score levels
as different categories. Therefore, a large amount
of previous TC research, such as the many machine
learning approaches proposed for the TC task, can
be utilized. For example, Furnkranz et al (1998)
compared the performance of applying two machine
learning methods on a web-page categorization task
and found that the Repeated Incremental Pruning to
Produce Error Reduction algorithm (RIPPER) (Co-
hen, 1995) shows an advantage concerning the fea-
ture sparsity issue.
3 Methodology
As described in Section 1, for the two types of ques-
tions considered, the number of words appearing
in a response is quite limited given the short re-
sponse time. Therefore, compared to written es-
says, when applying the content-scoring methods
based on vector analysis, e.g., CVA, feature sparsity
becomes a major factor negatively influencing the
performance of these methods. Furthermore, there
are more challenges when applying vector analysis
on survey questions because test-takers could just
use words/phrases rather than completed sentences.
Also, some survey questions could have a very large
range of correct answers. For example, if a question
is about the name of a book, millions of book ti-
tles could be potential answers. Therefore, a simple
phrase-matching solution cannot work.
3.1 Semi-Automatic Information Extraction
For survey responses, the answers should be related
to the key points mentioned in the questions. For
example, for the question, ?What kind of TV pro-
grams do you like to watch??, possible correct an-
swers should be related to TV programs. Moreover,
it should be the instances of specific TV programs,
like news, comedy, talk shows, etc. Note that the ac-
ceptable answers may be infinite, so it is not realis-
tic to enumerate all possible answers. Therefore, we
proposed a method to extract the potential answer
candidates and then measure their semantic similar-
ities to the answer keys that could be determined
manually. In particular, the answer keys were deter-
mined by the first author based on her analysis of the
test prompts. For example, for the question ?What
kind of books do you like to read??, two answer keys,
?book? and ?reading? were selected. After a fur-
ther analysis of the questions, we found that most of
the survey questions are about ?when? ?where? and
?what?, and the answers in the responses were usu-
ally nouns or noun phrases. Therefore, we decided
to extract the noun phrases from each response and
use them as potential candidates.
We use two semantic similarity metrics (SSMs)
to evaluate how each candidate relates to an answer
key, including PMI-IR (Turney, 2001) and a word-
to-word similarity metric from WordNet (Courley
and Mihalcea, 2005). The PMI-IR is a measure
based on web query analysis using Pointwise Mutual
Information (PMI) and Information Retrieval (IR).
For an answer candidate (c) and an answer key (k),
their PMI-IR is computed as:
SSMPMI-IR(c, k) =
hits(cNEARk)
hits(c)
where the hits(x) function obtains the count of term
x returned by a web search engine and NEAR is a
query operator for proximity search, searching the
pages on which both k and c appear within a spec-
ified distance. Among many WordNet (WN) based
SSMs summarized in Courley and Mihalcea (2005),
123
we found that the Wu-Palmer metric proposed by
Wu and Palmer (1994) worked the best in our pilot
study. This metric is a score denoting how similar
two word senses are, based on the depth of the two
word senses in the taxonomy and their Least Com-
mon Subsumer 1 (LCS):
SSMWN(c, k) =
2 ? depth(LCS)
depth(c) + depth(k)
For each answer key, we calculated two sets of
SSMs (SSMPMI-IR and SSMWN , respectively)
from all candidates. Then, we selected the largest
SSMPMI-IR and SSMWN as the final SSMs for this
particular answer key. For each test question, using
the corresponding responses in the training set, we
built a linear regression model between these SSMs
for all answer keys and the human judged scores.
The learned regression model was applied to the re-
sponses to this particular testing question in the test-
ing set to convert a set of SSMs to predictions of
human scores. The predicted scores were then used
as a content feature. Since answer keys were deter-
mined manually, we refer to this method as semi-
automatic information extraction (Semi-IE).
3.2 Machine Learning Using Smoothed Inputs
For the opinion responses, inspired by Furnkranz
et al (1998), we decided to try sophisticated ma-
chine learning methods instead of the simple vector-
distance computation used in CVA. Due to short
response-time in the speaking test being considered,
the ordinary vector analysis may face a problem that
the obtained vectors are too short to be reliably used.
In addition, using other non-CVA machine learning
methods can enable us to try other types of linguis-
tic features. To address the feature sparsity issue, a
smoothing method, which converts word-based text
features into features based on other entities with
a much smaller vocabulary size, is used. We use
a Hidden Markov Model (HMM) based smooth-
ing method (Huang and Yates, 2009), which in-
duces classes, corresponding to hidden states in the
HMM model, from the observed word strings. This
smoothing method can use contextual information
of the word sequences due to the nature of HMM.
Then, we convert word-entity vectors to the vec-
tors based on the induced classes. TF-IDF (term
1Most specific ancestor node
frequency and inverse document frequency) weight-
ing is applied on the new class vectors. Finally,
the processed class vectors are used as input fea-
tures (smoothed) to a machine learning method. In
this research, after comparing several widely used
machine learning approaches, such as Naive Bayes,
CART, etc., we decided to use RIPPER proposed by
Cohen (1995), a rule induction method, similar to
Furnkranz et al (1998).
4 Experiments
Our experimental data was from a test for interna-
tional workplace English. Six testing papers were
used in our study and each individual test contains
three survey questions (1, 2, and 3) and two opin-
ion questions (4 and 5). Table 1 lists examples
for these question types. From the real test, we
collected spoken responses from a total of 1, 838
test-takers. 1, 470 test-takers were used for training
and 368 were used for testing. Following scoring
rubrics developed for this test by considering speak-
ers? various language skill aspects, such as fluency,
pronunciation, vocabulary, as well as content accu-
racy, the survey and opinion responses were scored
by a group of experienced human raters by using a
3-point scale and a 5-point scale respectively. For
the survey responses, the human judged scores were
centered on 2; for the opinion responses, the human
judged scores were centered on 3 and 4.
Qs. Example
1 How frequently do you go shopping?
2 What kinds of products do you buy often?
3 How should retailers improve their services?
4 Make a purchase decision based on the chart
provided and justify your decision.
5 Do you agree with the statement that online
shopping will be dominant in the future or
not? Please justify your point.
Table 1: Examples of the five kinds of questions investi-
gated in the study
All of these non-native speech responses were
manually transcribed. A state-of-the-art HMM Au-
tomatic Speech Recognition (ASR) system which
was trained from a large set of non-native speech
data was used. For each type of test question, acous-
tic and language model adaptations were applied
to further lower the recognition error rate. Finally,
124
a word error rate around 30% to 40% could be
achieved on the held-out speech data. In our exper-
iments, we used speech transcriptions in the model
training stage and used ASR outputs in the testing
stage. Note that we decided to use speech transcrip-
tions, instead of noisy ASR outputs that match to
the testing condition, to make sure that the learned
content-scoring model are based on correct word en-
tities related to content accuracy.
For the survey responses, we manually selected
the key points from the testing questions. Then,
using a Part-Of-Speech (POS) tagger and a sen-
tence chunker implemented by using the OpenNLP 2
toolkit, we found all possible nouns and noun-
phrases that could serve as answer candidates and
applied the Semi-IE method described in Sec-
tion 3.1. For opinion questions, based on Huang and
Yates (2009), we used 80 hidden states and applied
the method described in Section 3.2 for content scor-
ing. We used JRip, a Java implementation of the
RIPPER (Cohen, 1995) algorithm in the Weka (Hall
et al, 2009) machine learning toolkit, in our experi-
ments.
When measuring performance of content-related
features, following many automated assessment
studies (Attali and Burstein, 2004; Leacock and
Chodorow, 2003; Sukkarieh and Blackmore, 2009),
we used the Pearson correlation r between the con-
tent features and human scores as an evaluation met-
ric. We compared the proposed methods with a base-
line method, CVA. It works as follows: it first groups
all the training responses by scores, then it calculates
a TF vector from all the responses under a score
level. Also, an IDF matrix is generated from all
the training responses. After that, for each testing
response, CVA first converts it into a TF-IDF vec-
tor and then calculates the cosine similarity between
this vector with each score-level vector respectively
and uses the largest cosine similarity as the content
feature for that response. The experimental results,
including content-features? correlations r to human
scores from each proposed method and the correla-
tion increases measured on CVA results, are shown
in Table 2. First, we find that CVA, which is de-
signed for scoring lengthy written essays, does not
work well for the survey questions, especially on
2http://opennlp.sourceforge.net
Question rCV A rSemi?IE r ?
1 0.12 0.30 150%
2 0.15 0.27 80%
3 0.21 0.26 23.8%
Question rCV A rRipperHMM r ?
4 0.47 0.54 14.89%
5 0.33 0.39 18.18%
Table 2: Comparisons of the proposed content-scoring
methods with CVA on survey and opinion responses
first two questions, which are mostly phrases (not
completed sentences). By contrast, our proposed
Semi-IE method can provide more informative con-
tent measurements, indicated by substantially in-
creased r. Second, CVA works better on opinion
questions than on survey questions. This is because
that opinion questions can be treated as short spo-
ken essays and therefore are closer to the data on
which the CVA method was originally designed to
work. However, even on such a well-performing
CVA baseline, the HMM smoothing method allows
the Ripper algorithm to outperform the CVA method
in content-features? correlations to human scores.
For example, on question 4, on which either a table
or a chart has been provided to test-takers, the CVA
achieves a r of 0.47. The proposed method can still
improve the r by about 15%.
5 Conclusions and Future Works
In this paper, we proposed two content-scoring
methods for the two types of test questions in an
automated speaking assessment task. For particu-
lar properties of these two question types, we uti-
lized information extraction (IE) and machine learn-
ing technologies to better score them on content
accuracy. In our experiments, we compared these
two methods, Semi-IE and machine learning us-
ing smoothed inputs, with an ordinary word-based
vector analysis method, CVA. The content features
computed using the proposed methods show higher
correlations to human scores than what was obtained
by using the CVA method.
For the Semi-IE method, one direction of investi-
gation will be how to find the expected answer keys
automatically from testing questions. In addition,
we will investigate better ways to integrate many se-
125
mantic similarly measurements (SSMs) into a single
content feature. For the machine learning approach,
inspired by Furnkranz et al (1998), we will inves-
tigate how to use some linguistic features related to
response structures rather than just TF-IDF weights.
References
Y. Attali and J. Burstein. 2004. Automated essay scoring
with e-rater v.2.0. In Presented at the Annual Meet-
ing of the International Association for Educational
Assessment.
W. Cohen. 1995. Text categorization and relational
learning. In In Proceedings of the 12th International
Conference on Machine Learning.
C. Courley and R. Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In Proceedings of the ACL
Workshop on Empirical Modeling of Semantic Equiv-
alence and Entailment, pages 13?18.
M. Eskenazi. 2009. An overview of spoken language
technology for education. Speech Communication,
51(10):832?844.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. Bradford Books.
J. Furnkranz, T. Mitchell, and E. Riloff. 1998. A case
study in using linguistic phrases for text categorization
on the WWW. In Proceedings from the AAAI/ICML
Workshop on Learning for Text Categorization, page
512.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I. H Witten. 2009. The WEKA data min-
ing software: An update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
D. Higgins, J. Burstein, and Y. Attali. 2006. Identifying
off-topic student essays without topic-specific training
data. Natural Language Engineering, 12.
F. Huang and A. Yates. 2009. Distributional represen-
tations for handling sparsity in supervised sequence-
labeling. In Proceedings of ACL.
C. Leacock and M. Chodorow. 2003. C-rater: Auto-
mated scoring of short-answer questions. Computers
and the Humanities, 37(4):385?405.
M. Mohler and R. Mihalcea. 2009. Text-to-text seman-
tic similarity for automatic short answer grading. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 567?575.
J. Z. Sukkarieh and J. Blackmore. 2009. c-rater: Auto-
matic content scoring for short constructed responses.
In Paper presented at the Florida Artificial Intelli-
gence Research Society (FLAIRS) Conference, Sani-
bel, FL.
P. D. Turney. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Procs. of the
Twelfth European Conference on Machine Learning
(ECML), pages 491?502, Freiburg, Germany.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexi-
cal selection. In Proceeding ACL ?94 Proceedings of
the 32nd annual meeting on Association for Computa-
tional Linguistics.
126
