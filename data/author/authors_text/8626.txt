Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 112?121, Prague, June 2007. c?2007 Association for Computational Linguistics
A Comparative Evaluation of Deep and Shallow Approaches to the
Automatic Detection of Common Grammatical Errors
Joachim Wagner, Jennifer Foster, and Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University, Dublin 9, Ireland
{jwagner, jfoster, josef}@computing.dcu.ie
Abstract
This paper compares a deep and a shallow
processing approach to the problem of clas-
sifying a sentence as grammatically well-
formed or ill-formed. The deep processing
approach uses the XLE LFG parser and En-
glish grammar: two versions are presented,
one which uses the XLE directly to perform
the classification, and another one which
uses a decision tree trained on features con-
sisting of the XLE?s output statistics. The
shallow processing approach predicts gram-
maticality based on n-gram frequency statis-
tics: we present two versions, one which
uses frequency thresholds and one which
uses a decision tree trained on the frequen-
cies of the rarest n-grams in the input sen-
tence. We find that the use of a decision tree
improves on the basic approach only for the
deep parser-based approach. We also show
that combining both the shallow and deep
decision tree features is effective. Our eval-
uation is carried out using a large test set of
grammatical and ungrammatical sentences.
The ungrammatical test set is generated au-
tomatically by inserting grammatical errors
into well-formed BNC sentences.
1 Introduction
This paper is concerned with the task of predict-
ing whether a sentence contains a grammatical er-
ror. An accurate method for carrying out automatic
?Also affiliated to IBM CAS, Dublin.
grammaticality judgements has uses in the areas of
computer-assisted language learning and grammar
checking. Comparative evaluation of existing error
detection approaches has been hampered by a lack
of large and commonly used evaluation error cor-
pora. We attempt to overcome this by automatically
creating a large error corpus, containing four dif-
ferent types of frequently occurring grammatical er-
rors. We use this corpus to evaluate the performance
of two approaches to the task of automatic error de-
tection. One approach uses low-level detection tech-
niques based on POS n-grams. The other approach
is a novel parser-based method which employs deep
linguistic processing to discriminate grammatical in-
put from ungrammatical. For both approaches, we
implement a basic solution, and then attempt to im-
prove upon this solution using a decision tree clas-
sifier. We show that combining both methods im-
proves upon the individual methods.
N-gram-based approaches to the problem of error
detection have been proposed and implemented in
various forms by Atwell(1987), Bigert and Knutsson
(2002), and Chodorow and Leacock (2000) amongst
others. Existing approaches are hard to compare
since they are evaluated on different test sets which
vary in size and error density. Furthermore, most of
these approaches concentrate on one type of gram-
matical error only, namely, context-sensitive or real-
word spelling errors. We implement a vanilla n-
gram-based approach which is tested on a very large
test set containing four different types of error.
The idea behind the parser-based approach to er-
ror detection is to use a broad-coverage hand-crafted
precision grammar to detect ungrammatical sen-
112
tences. This approach exploits the fact that a pre-
cision grammar is designed, in the traditional gen-
erative grammar sense (Chomsky, 1957), to dis-
tinguish grammatical sentences from ungrammati-
cal sentences. This is in contrast to treebank-based
grammars which tend to massively overgenerate and
do not generally aim to discriminate between the
two. In order for our approach to work, the coverage
of the precision grammars must be broad enough to
parse a large corpus of grammatical sentences, and
for this reason, we choose the XLE (Maxwell and
Kaplan, 1996), an efficient and robust parsing sys-
tem for Lexical Functional Grammar (LFG) (Kaplan
and Bresnan, 1982) and the ParGram English gram-
mar (Butt et al, 2002) for our experiments. This sys-
tem employs robustness techniques, some borrowed
from Optimality Theory (OT) (Prince and Smolen-
sky, 1993), to parse extra-grammatical input (Frank
et al, 1998), but crucially still distinguishes between
optimal and suboptimal solutions.
The evaluation corpus is a subset of an un-
grammatical version of the British National Cor-
pus (BNC), a 100 million word balanced corpus of
British English (Burnard, 2000). This corpus is ob-
tained by automatically inserting grammatical errors
into the original BNC sentences based on an analysis
of a manually compiled ?real? error corpus.
This paper makes the following contributions to
the task of automatic error detection:
1. A novel deep processing XLE-based approach
2. An effective and novel application of decision
tree machine learning to both shallow and deep
approaches
3. A novel combination of deep and shallow pro-
cessing
4. An evaluation of an n-gram-based approach on
a wider variety of errors than has previously
been carried out
5. A large evaluation error corpus
The paper is organised as follows: in Section 2,
we describe previous approaches to the problem of
error detection; in Section 3, a description of the
error corpus used in our evaluation experiments is
presented, and in Section 4, the two approaches to
error detection are presented, evaluated, combined
and compared. Section 5 provides a summary and
suggestions for future work.
2 Background
2.1 Precision Grammars
A precision grammar is a formal grammar designed
to distinguish ungrammatical from grammatical sen-
tences. This is in contrast to large treebank-induced
grammars which often accept ungrammatical input
(Charniak, 1996). While high coverage is required,
it is difficult to increase coverage without also in-
creasing the amount of ungrammatical sentences
that are accepted as grammatical by the grammar.
Most publications in grammar-based automatic error
detection focus on locating and categorising errors
and giving feedback. Existing grammars are re-used
(Vandeventer Faltin, 2003), or grammars of limited
size are developed from scratch (Reuer, 2003).
The ParGram English LFG is a hand-crafted
broad-coverage grammar developed over several
years with the XLE platform (Butt et al, 2002). The
XLE parser uses OT to resolve ambiguities (Prince
and Smolensky, 1993). Grammar constraints re-
sulting in rare constructions can be marked as ?dis-
preferred? and constraints resulting in common un-
grammatical constructions can be marked as ?un-
grammatical?. The use of constraint ordering and
marking increases the robustness of the grammar,
while maintaining the grammatical / ungrammati-
cal distinction (Frank et al, 1998). The English
Resource Grammar (ERG) is a precision Head-
Driven Phrase Structure Grammar (HPSG) of En-
glish (Copestake and Flickinger, 2000; Pollard and
Sag, 1994). Its coverage is not as broad as the XLE
English grammar. Baldwin et al (2004) propose a
method to identify gaps in the grammar. Blunsom
and Baldwin (2006) report ongoing development.
There has been previous work using the ERG and
the XLE grammars in the area of computer-assisted
language learning. Bender et al (2004) use a ver-
sion of the ERG containing mal-rules to parse ill-
formed sentences from the SST corpus of Japanese
learner English (Emi et al, 2004). They then use
the semantic representations of the ill-formed input
to generate well-formed corrections. Khader et al
(2004) study whether the ParGram English LFG can
be used for computer-assisted language learning by
113
adding additional OT marks for ungrammatical con-
structions observed in a learner corpus. However,
the evaluation is preliminary, on only 50 test items.
2.2 N-gram Methods
Most shallow approaches to grammar error detection
originate from the area of real-word spelling error
correction. A real-word spelling error is a spelling
or typing error which results in a token which is an-
other valid word of the language in question.
The (to our knowledge) oldest work in this area
is that of Atwell (1987) who uses a POS tagger to
flag POS bigrams that are unlikely according to a
reference corpus. While he speculates that the bi-
gram frequency should be compared to how often
the same POS bigram is involved in errors in an error
corpus, the proposed system uses the raw frequency
with an empirically established threshold to decide
whether a bigram indicates an error. In the same
paper, a completely different approach is presented
that uses the same POS tagger to consider spelling
variants that have a different POS. In the example
sentence I am very hit the POS of the spelling vari-
ant hot/JJ is added to the list NN-VB-VBD-VBN of
possible POS tags of hit. If the POS tagger chooses
hit/JJ, the word is flagged and the correction hot is
proposed to the user. Unlike most n-gram-based ap-
proaches, Atwell?s work aims to detect grammar er-
rors in general and not just real-word spelling errors.
However, a complete evaluation is missing.
The idea of disambiguating between the elements
of confusion sets is related to word sense disam-
biguation. Golding (1995) builds a classifier based
on a rich set of context features. Mays et al (1991)
apply the noisy channel model to the disambiguation
problem. For each candidate correction S? of the
input S the probability P (S?)P (S|S?) is calculated
and the most likely correction selected. This method
is re-evaluated by Wilcox-O?Hearn et al (2006) on
WSJ data with artificial real-word spelling errors.
Bigert and Knutsson (2002) extend upon a basic
n-gram approach by attempting to match n-grams of
low frequency with similar n-grams in order to re-
duce overflagging. Furthermore, n-grams crossing
clause boundaries are not flagged and the similarity
measure is adapted in the case of phrase boundaries
that usually result in low frequency n-grams.
Chodorow and Leacock (2000) use a mutual in-
formation measure in addition to raw frequency of n-
grams. Apart from this, their ALEK system employs
other extensions to the basic approach, for exam-
ple frequency counts from both generic and word-
specific corpora are used in the measures. It is not
reported how much each of these contribute to the
overall performance.
Rather than trying to implement all of the pre-
vious n-gram approaches, we implement the basic
approach which uses rare n-grams to predict gram-
maticality. This property is shared by all previous
shallow approaches. We also test our approach on a
wider class of grammatical errors.
3 Ungrammatical Data
In this section, we discuss the notion of an artifi-
cial error corpus (Section 3.1), define the type of
ungrammatical language we are dealing with (Sec-
tion 3.2), and describe our procedure for creating a
large artificial error corpus derived from the BNC
(Section 3.3).
3.1 An Artificial Error Corpus
In order to meaningfully evaluate a shallow ver-
sus deep approach to automatic error detection, a
large test set of ungrammatical sentences is needed.
A corpus of ungrammatical sentences can take the
form of a learner corpus (Granger, 1993; Emi et al,
2004), i. e. a corpus of sentences produced by lan-
guage learners, or it can take the form of a more gen-
eral error corpus comprising sentences which are not
necessarily produced in a language-learning context
and which contain competence and performance er-
rors produced by native and non-native speakers of
the language (Becker et al, 1999; Foster and Vogel,
2004; Foster, 2005). For both types of error corpus,
it is not enough to collect a large set of sentences
which are likely to contain an error - it is also neces-
sary to examine each sentence in order to determine
whether an error has actually occurred, and, if it has,
to note the nature of the error. Thus, like the cre-
ation of a treebank, the creation of a corpus of un-
grammatical sentences requires time and linguistic
knowledge, and is by no means a trivial task.
A corpus of ungrammatical sentences which is
large enough to be useful can be created auto-
matically by inserting, deleting or replacing words
114
in grammatical sentences. These transformations
should be linguistically realistic and should, there-
fore, be based on an analysis of naturally produced
grammatical errors. Automatically generated error
corpora have been used before in natural language
processing. Bigert (2004) and Wilcox-O?Hearn et
al. (2006), for example, automatically introduce
spelling errors into texts. Here, we generate a large
error corpus by automatically inserting four different
kinds of grammatical errors into BNC sentences.
3.2 Commonly Produced Grammatical Errors
Following Foster (2005), we define a sentence to be
ungrammatical if all the words in the sentence are
well-formed words of the language in question, but
the sentence contains one or more error. This er-
ror can take the form of a performance slip which
can occur due to carelessness or tiredness, or a com-
petence error which occurs due to a lack of knowl-
edge of a particular construction. This definition in-
cludes real-word spelling errors and excludes non-
word spelling errors. It also excludes the abbrevi-
ated informal language used in electronic communi-
cation. Using the above definition as a guideline, a
20,000 word corpus of ungrammatical English sen-
tences was collected from a variety of written texts
including newspapers, academic papers, emails and
website forums (Foster and Vogel, 2004; Foster,
2005). The errors in the corpus were carefully anal-
ysed and classified in terms of how they might be
corrected using the three word-level correction op-
erators: insert, delete and substitute. The following
frequency ordering of the three word-level correc-
tion operators was found:
substitute (48%) > insert (24%) > delete (17%) >
combination (11%)
Stemberger (1982) reports the same ordering of the
substitution, deletion and insertion correction oper-
ators in a study of native speaker spoken language
slips. Among the grammatical errors which can be
corrected by substituting one word for another, the
most common errors are real-word spelling errors
and agreement errors. In fact, 72% of all errors fall
into one of the following four classes:
1. missing word errors:
What are the subjects? > What the subjects?
2. extra word errors:
Was that in the summer? > Was that in the sum-
mer in?
3. real-word spelling errors:
She could not comprehend. > She could no
comprehend.
4. agreement errors:
She steered Melissa round a corner. > She
steered Melissa round a corners.
A similar classification was adopted by Nicholls
(1999), having analysed the errors in a learner cor-
pus. Our research is currently limited to the four er-
ror types given above, i. e. missing word errors, ex-
tra word errors, real-word spelling errors and agree-
ments errors. However, it is possible for it to be ex-
tended to handle a wider class of errors.
3.3 Automatic Error Creation
The error creation procedure takes as input a part-
of-speech-tagged corpus of sentences which are as-
sumed to be well-formed, and outputs a corpus of
ungrammatical sentences. The automatically intro-
duced errors take the form of the four most com-
mon error types found in the manually created cor-
pus, i. e. missing word errors, extra word errors, real-
word spelling errors and agreement errors. For each
sentence in the original tagged corpus, an attempt is
made to automatically produce four ungrammatical
sentences, one for each of the four error types. Thus,
the output of the error creation procedure is, in fact,
four error corpora.
3.3.1 Missing Word Errors
In the manually created error corpus of Foster
(2005), missing word errors are classified based on
the part-of-speech (POS) of the missing word. 98%
of the missing parts-of-speech come from the fol-
lowing list (the frequency distribution in the error
corpus is given in brackets):
det (28%) > verb (23%) > prep (21%) > pro (10%)
> noun (7%) > ?to? (7%) > conj (2%)
We use this information when introducing missing
word errors into the BNC sentences. For each sen-
tence, all words with the above POS tags are noted.
One of these is selected and deleted. The above
frequency ordering is respected so that, for exam-
ple, missing determiner errors are produced more of-
ten than missing pronoun errors. No ungrammatical
115
sentence is produced if the original sentence con-
tains just one word or if the sentence contains no
words with parts-of-speech in the above list.
3.3.2 Extra Word Errors
We introduce extra word errors in the following
three ways:
1. Random duplication of any token within a sen-
tence: That?s the way we we learn here.
2. Random duplication of any POS within a sen-
tence: There it he was.
3. Random insertion of an arbitrary token into the
sentence: Joanna drew as a long breadth.
Apart from the case of duplicate tokens, the extra
words are selected from a list of tagged words com-
piled from a random subset of the BNC. Again, our
procedure for inserting extra words is based on the
analysis of extra word errors in the 20,000 word er-
ror corpus of Foster (2005).
3.3.3 Real-Word Spelling Errors
We classify an error as a real-word spelling er-
ror if it can be corrected by replacing the erroneous
word with another word with a Levenshtein distance
of one from the erroneous word, e.g. the and they.
Based on the analysis of the manually created er-
ror corpus (Foster, 2005), we compile a list of com-
mon English real-word spelling error word pairs.
For each BNC sentence, the error creation proce-
dure records all tokens in the sentence which appear
as one half of one of these word pairs. One token
is selected at random and replaced by the other half
of the pair. The list of common real-word spelling
error pairs contains such frequently occurring words
as is and a, and the procedure therefore produces an
ill-formed sentence for most input sentences.
3.3.4 Agreement Errors
We introduce subject-verb and determiner-noun
number agreement errors into the BNC sentences.
We consider both types of agreement error equally
likely and introduce the error by replacing a singular
determiner, noun or verb with its plural counterpart,
or vice versa. For English, subject-verb agreement
errors can only be introduced for present tense verbs,
and determiner-noun agreement errors can only be
introduced for determiners which are marked for
number, e.g. demonstratives and the indefinite ar-
ticle. The procedure would be more productive if
applied to a morphologically richer language.
3.3.5 Covert Errors
James (1998) uses the term covert error to de-
scribe a genuine language error which results in a
sentence which is syntactically well-formed under
some interpretation different from the intended one.
The prominence of covert errors in our automati-
cally created error corpus is estimated by manually
inspecting 100 sentences of each error type. The per-
centage of grammatical structures that are inadver-
tently produced for each error type and an example
of each one are shown below:
? Agreement Errors, 7%
Mary?s staff include Jones,Smith and Murphy
> Mary?s staff includes Jones,Smith and Mur-
phy
? Real-Word Spelling Errors, 10%
And then? > And them?
? Extra Word Errors, 5%
in defiance of the free rider prediction > in de-
fiance of the free rider near prediction
? Missing Word Errors, 13%
She steered Melissa round a corner > She
steered round a corner
The occurrence of these covert errors can be re-
duced by fine-tuning the error creation procedure but
they can never be completely eliminated. Indeed,
they should not be eliminated from the test data,
because, ideally, an optimal error detection system
should be sophisticated enough to flag syntactically
well-formed sentences containing covert errors as
potentially ill-formed.1
4 Error Detection Evaluation
In this section we present the error detection eval-
uation experiments. The experimental setup is ex-
plained in Section 4.1, the results are presented in
Section 4.2 and they are analysed in Section 4.3.
1An example of this is given in the XLE User Documen-
tation (http://www2.parc.com/isl/groups/nltt/
xle/doc/). The authors remark that an ungrammatical read-
ing of the sentence Lets go to the store in which Lets is missing
an apostrophe, is preferable to the grammatical yet implausible
analysis in which Lets is a plural noun.
116
4.1 Experimental Setup
4.1.1 Test Data and Evaluation Procedure
The following steps are carried out to produce
training and test data for this experiment:
1. Speech material, poems, captions and list items
are removed from the BNC. 4.2 million sen-
tences remain. The order of sentences is ran-
domised.
2. For the purpose of cross-validation, the corpus
is split into 10 parts.
3. Each part is passed to the 4 automatic error in-
sertion modules described in Section 3.3, re-
sulting in 40 additional sets of varying size.
4. The first 60,000 sentences of each of the 50
sets, i. e. 3 million sentences, are parsed with
XLE.2
5. N-gram frequency information is extracted for
the first 60,000 sentences of each set. An addi-
tional 20,000 is extracted as held-out data.
6. 10 sets with mixed error types are produced by
joining a quarter of each respective error set.
7. For each error type (including mixed errors)
and cross-validation set, the 60,000 grammat-
ical and 60,000 ungrammatical sentences are
joined.
8. Each cross-validation run uses one set out of
the 10 as test data (120,000 sentences) and the
remaining 9 sets for training (1,080,000 sen-
tences).
The experiment is a standard binary classification
task. The methods classify the sentences of the test
sets as grammatical or ungrammatical. We use the
standard measures of precision, recall, f-score and
accuracy (Figure 1). True positives are understood
to be ungrammatical sentences that are identified as
such. The baseline precision and accuracy is 50%
as half of the test data is ungrammatical. If 100%
of the test data is classified as ungrammatical, re-
call will be 100% and f-score 2/3. Recall shows
the accuracy we would get if the grammatical half
of the test data was removed. Parametrised methods
2We use the XLE command parse-testfile with parse-
literally set to 1, max xle scratch storage set to 1,000 MB, time-
out to 60 seconds, and the XLE English LFG. Skimming is not
switched on and fragments are.
Measure Formula
precision tp/(tp + fp)
recall tp/(tp + fn)
f-score 2pr ? re/(pr + re)
accuracy (tp + tn)/(tp + tn + fp + fn)
Figure 1: Evaluation measures: tp = true positives,
fp = false positives, tn = true negatives, fn = false
negatives, pr = precision, re = recall
are first optimised for accuracy and then the other
measures are taken. Therefore, f-scores below the
artificial 2/3 baseline are meaningful.
4.1.2 Method 1: Precision Grammar
According to the XLE documentation, a sentence
is marked with a star (*) if its optimal solution uses
a constraint marked as ungrammatical. We use this
star feature, parser exceptions and zero number of
parses to classify a sentence as ungrammatical.
4.1.3 Method 2: POS N-grams
In each cross-validation run, the full data of the
remaining 9 sets of step 2 of the data generation
(see Section 4.1.1) is used as a reference corpus of
0.9?4, 200, 000 = 3, 800, 000 assumedly grammat-
ical sentences. The reference corpora and data sets
are POS tagged with the IMS TreeTagger (Schmidt,
1994). Frequencies of POS n-grams (n = 2, . . . , 7)
are counted in the reference corpora. A test sentence
is flagged as ungrammatical if it contains an n-gram
below a fixed frequency threshold. Method 2 has
two parameters: n and the frequency threshold.
4.1.4 Method 3: Decision Trees on XLE Output
The XLE parser outputs additional statistics for
each sentence that we encode in six features:
? An integer indicating starredness (0 or 1) and
various parser exceptions (-1 for time out, -2
for exceeded memory, etc.)
? The number of optimal parses3
? The number of unoptimal parses
? The duration of parsing
? The number of subtrees
? The number of words
3The use of preferred versus dispreferred constraints are
used to distinguish optimal parses from unoptimal ones.
117
Training data for the decision tree learner is com-
posed of 9?60, 000 = 540, 000 feature vectors from
grammatical sentences and 9 ? 15, 000 = 135, 000
feature vectors from ungrammatical sentences of
each error type, resulting in equal amounts of gram-
matical and ungrammatical training data.
We choose the weka implementation of machine
learning algorithms for the experiments (Witten and
Frank, 2000). We use a J48 decision tree learner
with the default model.
4.1.5 Method 4: Decision Trees on N-grams
Method 4 follows the setup of Method 3. How-
ever, the features are the frequencies of the rarest
n-grams (n = 2, . . . , 7) in the sentence. Therefore,
the feature vector of one sentence contains 6 num-
bers.
4.1.6 Method 5: Decision Trees on Combined
Feature Sets
This method combines the features of Methods 3
and 4 for training a decision tree.
4.2 Results
Table 1 shows the results for Method 1, which uses
XLE starredness, parser exceptions4 and zero parses
to classify grammaticality. Table 2 shows the re-
sults for Method 2, the basic n-gram approach. Ta-
ble 3 shows the results for Method 3, which classi-
fies based on a decision tree of XLE features. The
results for Method 4, the n-gram-based decision tree
approach, are shown in Table 4. Finally, Table 5
shows the results for Method 5 which combines n-
gram and XLE features in decision trees.
In the case of Method 2, we first have to find opti-
mal parameters. As only very limited integer values
for n and the threshold are reasonable, an exhaustive
search is feasible. We considered n = 2, . . . , 7 and
frequency thresholds below 20,000. Separate held-
out data (400,000 sentences) is used in order to avoid
overfitting. Best accuracy is achieved with 5-grams
and a threshold of 4. Table 2 reports results with
these parameters.
4XLE parsing (see footnote 2 for configuration) runs out
of time for 0.7 % and out of memory for 2.5 % of sentences,
measured on training data of the first cross-validation run, i. e.
540,000 grammatical sentence and 135,000 of each error type.
14 sentences of 3 million caused the parser to terminate abnor-
mally.
Error type Pr. Re. F-Sc. Acc.
Agreement 66.2 64.6 65.4 65.8
Real-word 63.5 57.3 60.3 62.2
Extra word 64.4 59.7 62.0 63.4
Missing word 59.2 47.8 52.9 57.4
Mixed errors 63.5 57.3 60.3 62.2
Table 1: Classification results with XLE starredness,
parser exceptions and zero parses (Method 1)
Error type Pr. Re. F-Sc. Acc.
Agreement 58.6 51.7 55.0 57.6
Real-word 64.0 64.9 64.5 64.2
Extra word 64.8 67.3 66.0 65.4
Missing word 57.2 48.8 52.7 56.1
Mixed errors 61.5 58.2 59.8 60.8
Table 2: Classification results with 5-gram and fre-
quency threshold 4 (Method 2)
The standard deviation of results across cross-
validation runs is below 0.006 on all measures, ex-
cept for Method 4. Therefore we only report average
percentages. The highest observed standard devia-
tion is 0.0257 for recall of Method 4 on agreement
errors.
For Methods 3, 4 and 5, the decision tree learner
optimises accuracy and, in doing so, chooses a trade-
off between precision and recall.
4.3 Analysis
Both Method 1 (Table 1) and Method 2 (Table 2)
achieve above baseline accuracy for all error types.
However, Method 1, which uses the XLE starred
feature, parser exceptions and zero parses to de-
termine whether or not a sentence is grammatical,
slightly outperforms Method 2, which uses the fre-
Error type Pr. Re. F-Sc. Acc.
Agreement 67.0 79.3 72.6 70.1
Real-word 63.4 67.6 65.4 64.3
Extra word 63.0 66.4 64.7 63.7
Missing word 59.7 57.8 58.7 59.4
Mixed errors 63.4 67.8 65.6 64.4
Table 3: Classification results with decision tree on
XLE output (Method 3)
118
Error type Pr. Re. F-Sc. Acc.
Agreement 61.2 53.8 57.3 59.9
Real-word 65.3 64.3 64.8 65.1
Extra word 66.4 67.4 66.9 66.7
Missing word 59.1 49.2 53.7 57.5
Mixed errors 63.3 58.7 60.9 62.3
Table 4: Classification results with decision tree on
vectors of frequency of rarest n-grams (Method 4)
Error type Pr. Re. F-Sc. Acc.
Agreement 67.1 75.2 70.9 69.2
Real-word 65.8 70.7 68.1 67.0
Extra word 65.9 71.2 68.5 67.2
Missing word 61.2 58.0 59.5 60.6
Mixed errors 65.2 68.8 66.9 66.0
Table 5: Classification results with decision tree on
joined feature set (Method 5)
quency of POS 5-grams to detect an error. The
XLE deep-processing approach is better than the n-
gram-based approach for agreement errors (f-score
+10.4). Examining the various types of agree-
ment errors, we can see that this is especially the
case for singular subjects followed by plural cop-
ula verbs (recall +37.7) and determiner-noun num-
ber mismatches (recall +23.6 for singular nouns and
+18.0 for plural nouns), but not for plural subjects
followed by singular verbs (recall -24.0). The rela-
tively poor performance of Method 2 on agreement
errors involving determiners could be due to the lack
of agreement marking on the Penn Treebank deter-
miner tag used by TreeTagger.
Method 1 is outperformed by Method 2 for real-
word spelling and extra word errors (f-score -4.2,
-4.0). Unsurprisingly, Method 2 has an advantage
on those real-word spelling errors that change the
POS (recall -8.8 for Method 1). Both methods per-
form poorly on missing word errors. For both meth-
ods there are only very small differences in perfor-
mance between the various missing word error sub-
types (identified by the POS of the deleted word).
Method 3, which uses machine learning to exploit
all the information returned by the XLE parser, im-
proves performance from Method 1, the basic XLE
method, for all error types.5 The general improve-
ment comes from an improvement in recall, mean-
ing that more ungrammatical sentences are actu-
ally flagged as such without compromising preci-
sion. The improvement is highest for agreement
errors (f-score +7.2). Singular subject with plural
copula errors (e. g. The man are) peak at a recall of
91.0. The Method 3 results indicate that information
on the number of solutions (optimal and unoptimal),
the number of subtrees, the time taken to parse the
sentence and the number of words can be used to
predict grammaticality. It would be interesting to
investigate this approach with other parsers.
Method 4, which uses a decision tree with n-
gram-based features, confirms the results of Method
2. The decision trees? root nodes are similar or even
identical (depending on cross-validation run) to the
decision rule of Method 2 (5-gram frequency below
4). However, the 10 decision trees have between
1,111 and 1,905 nodes and draw from all features,
even bigrams and 7-grams that perform poorly on
their own. The improvements are very small though
and they are not significant according the criterion of
non-overlapping cross-validation results. The main
reason for the evaluation of Method 4 is to provide
another reference point for comparison of the final
method.
The overall best results are those for Method 5,
the combined XLE, n-gram and machine-learning-
based method, which outperforms the next best
method, Method 3, on all error types apart from
agreement errors (f-score -1.7, +2.7, +3.8, +0.8).
For agreement errors, it seems that the relatively
poor results for n-grams have a negative effect on the
relatively good results for the XLE. Figure 2 shows
that the performance is almost constant on ungram-
matical data in the important sentence length range
from 5 to 40. However, there is a negative correla-
tion of accuracy and sentence length for grammati-
cal sentences. Very long sentences of any kind tend
to be classified as ungrammatical, except for missing
word errors which remain close to the 50% baseline
of coin-flipping.
For all methods, missing word errors are the
worst-performing, particularly in recall (i. e. the ac-
5The +0.3 increase in average accuracy for extra word errors
is not clearly significant as the results of cross-validation runs
overlap.
119
Figure 2: Accuracy by sentence length for Method 5
measured on separate grammatical and ungrammat-
ical data: Gr = Grammatical, AG = Agreement, RW
= Real-Word, EW = Extra Word, MW = Missing
Word
curacy on ungrammatical data alone). This means
that the omission of a word is less likely to result in
the sentence being flagged as erroneous. In contrast,
extra word errors perform consistently and relatively
well for all methods.
5 Conclusion and Future Work
We evaluated a deep processing approach and a POS
n-gram-based approach to the automatic detection of
common grammatical errors in a BNC-derived arti-
ficial error corpus. The results are broken down by
error type. Together with the deep approach, a deci-
sion tree machine learning algorithm can be used ef-
fectively. However, extending the shallow approach
with the same learning algorithm gives only small
improvements. Combining the deep and shallow ap-
proaches gives an additional improvement on all but
one error type.
Our plan is to investigate why all methods per-
form poorly on missing word errors, to extend the
error creation procedure so that it includes a wider
range of errors, to try the deep approach with other
parsers, to integrate additional features from state-
of-the-art shallow techniques and to repeat the ex-
periments for languages other than English.
Acknowledgements
This work is supported by the IRCSET Embark Ini-
tiative (basic research grant SC/02/298 and postdoc-
toral fellowship P/04/232). The training and test
data used in this reseach is based on the British Na-
tional Corpus (BNC), distributed by Oxford Univer-
sity Computing Services on behalf of the BNC Con-
sortium. We thank Djame? Seddah for helping us to
run the XLE parsing on the SFI/HEA Irish Centre
for High-End Computing (ICHEC) and the authors
wish to acknowledge ICHEC for the provision of
computational facilities and support.
References
Eric Atwell. 1987. How to detect grammatical errors in
a text without parsing it. In Proceedings of the 3rd
EACL, pages 38?45, Morristown, NJ.
Timothy Baldwin, John Beavers, Emily M. Bender, Dan
Flickinger, Ara Kim, and Stephan Oepen. 2004.
Beauty and the beast: What running a broad-coverage
precision grammar over the BNC taught us about the
grammar - and the corpus. In Pre-Proceedings of the
International Conference on Linguistic Evidence: Em-
pirical, Theoretical and Computational Perspectives,
pages 21?26.
Markus Becker, Andrew Bredenkamp, Berthold Crys-
mann, and Judith Klein. 1999. Annotation of error
types for German news corpus. In Proceedings of the
ATALA Workshop on Treebanks, Paris, France.
Emily M. Bender, Dan Flickinger, Stephan Oepen, and
Timothy Baldwin. 2004. Arboretum: Using a preci-
sion grammar for grammar checking in CALL. In Pro-
ceedings of the InSTIL/ICALL Symposium: NLP and
Speech Technologies in Advanced Language Learning
Systems, Venice, Italy.
Johnny Bigert and Ola Knutsson. 2002. Robust error
detection: a hybrid approach combining unsupervised
error detection and linguistic knowledge. In Proceed-
ings RO-MAND-02, Frascati, Italy.
Johnny Bigert. 2004. Probabilistic detection of context-
sensitive spelling errors. In Proceedings of LREC-04,
volume Five, pages 1633?1636, Lisbon, Portugal.
Phil Blunsom and Timothy Baldwin. 2006. Multilingual
deep lexical acquisition for HPSGs via supertagging.
In Proceedings of EMNLP-06, pages 164?171, Syd-
ney.
Lou Burnard. 2000. User reference guide for the British
national corpus. Technical report, Oxford University
Computing Services.
120
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The par-
allel grammar project. In Proceedings of COLING-
2002 Workshop on Grammar Engineering and Evalu-
ation, pages 1?7, Morristown, NJ, USA.
Eugene Charniak. 1996. Tree-bank grammars. Tech-
nical Report CS-96-02, Department of Computer Sci-
ence, Brown University.
Martin Chodorow and Claudia Leacock. 2000. An unsu-
pervised method for detecting grammatical errors. In
Proceedings of NAACL-00, pages 140?147, San Fran-
cisco, CA.
Noam Chomsky. 1957. Syntactic Structures. Mouton.
Ann Copestake and Dan Flickinger. 2000. An open-
source grammar development environment and broad-
coverage English grammar using HPSG. In Proceed-
ings of LREC-02, Athens, Greece.
Izumi Emi, Kiyotaka Uchimoto, and Hitoshi Isahara.
2004. The overview of the SST speech corpus of
Japanese learner English and evaluation through the
experiment on automatic detection of learners? er-
rors. In Proceedings of LREC-04, volume Four, pages
1435?1439, Lisbon, Portugal.
Jennifer Foster and Carl Vogel. 2004. Good reasons
for noting bad grammar: Constructing a corpus of un-
grammatical language. In Stephan Kepser and Marga
Reis, editors, Pre-Proceedings of the International
Conference on Linguistic Evidence: Empirical, The-
oretical and Computational Perspectives, pages 151?
152, Tu?bingen, Germany.
Jennifer Foster. 2005. Good Reasons for Noting Bad
Grammar: Empirical Investigations into the Parsing
of Ungrammatical Written English. Ph.D. thesis, Uni-
versity of Dublin, Trinity College, Dublin, Ireland.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John Maxwell. 1998. Optimality theory style con-
straint ranking in large-scale LFG grammars. In Pro-
ceedings of LFG-98, Brisbane, Australia.
Andrew R. Golding. 1995. A Bayesian hybrid method
for context-sensitive spelling correction. In Proceed-
ings of the Third Workshop on Very Large Corpora,
pages 39?53, Boston, MA.
Sylviane Granger. 1993. International corpus of learner
English. In J. Aarts, P. de Haan, and N.Oostdijk, ed-
itors, English Language Corpora: Design, Analysis
and Exploitation, pages 57?71. Rodopi, Amsterdam.
Carl James. 1998. Errors in Language Learning and
Use: Exploring Error Analysis. Addison Wesley
Longman.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar: a formal system for grammatical represen-
tation. In Joan Bresnan, editor, The Mental Represen-
tation of Grammatical Relations, pages 173?281. MIT
Press.
Rafiq Abdul Khader, Tracy Holloway King, and Miriam
Butt. 2004. Deep CALL grammars: The LFG-
OT experiment. http://ling.uni-konstanz.de/pages/
home/butt/dgfs04call.pdf.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
Eric Mays, Fred J. Damerau, and Robert L. Mercer.
1991. Context based spelling correction. Information
Processing and Management, 23(5):517?522.
D. Nicholls. 1999. The Cambridge learner corpus ? error
coding and analysis. In Summer Workshop on Learner
Corpora, Tokyo, Japan.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. University of Chicago Press and
CSLI Publications.
Alan Prince and Paul Smolensky. 1993. Optimality The-
ory. MIT Press, Cambridge, Massachusetts.
Veit Reuer. 2003. PromisD - Ein Analyseverfahren
zur antizipationsfreien Erkennung und Erkla?rung von
grammatischen Fehlern in Sprachlehrsystemen. Ph.D.
thesis, Humboldt-Universita?t zu Berlin, Berlin, Ger-
many.
Helmut Schmidt. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49, Manchester, England.
J.P. Stemberger. 1982. Syntactic errors in speech. Jour-
nal of Psycholinguistic Research, 11(4):313?45.
Anne Vandeventer Faltin. 2003. Syntactic Error Diag-
nosis in the context of Computer Assisted Language
Learning. Ph.D. thesis, Universite? de Gene`ve.
L. Amber Wilcox-O?Hearn, Graeme Hirst, and Alexan-
der Budanitsky. 2006. Real-word spelling correc-
tion with trigrams: A reconsideration of the Mays,
Damerau, and Mercer model. http://ftp.cs.toronto.edu/
pub/gh/WilcoxOHearn-etal-2006.pdf.
Ian H. Witten and Eibe Frank. 2000. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann Publishers.
121
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 221?224,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Adapting a WSJ-Trained Parser to Grammatically Noisy Text
Jennifer Foster, Joachim Wagner and Josef van Genabith
National Centre for Language Technology
Dublin City University
Ireland
jfoster, jwagner, josef@computing.dcu.ie
Abstract
We present a robust parser which is trained on
a treebank of ungrammatical sentences. The
treebank is created automatically by modify-
ing Penn treebank sentences so that they con-
tain one or more syntactic errors. We eval-
uate an existing Penn-treebank-trained parser
on the ungrammatical treebank to see how it
reacts to noise in the form of grammatical er-
rors. We re-train this parser on the training
section of the ungrammatical treebank, lead-
ing to an significantly improved performance
on the ungrammatical test sets. We show how
a classifier can be used to prevent performance
degradation on the original grammatical data.
1 Introduction
The focus in English parsing research in recent years
has moved from Wall Street Journal parsing to im-
proving performance on other domains. Our re-
search aim is to improve parsing performance on
text which is mildly ungrammatical, i.e. text which
is well-formed enough to be understood by people
yet which contains the kind of grammatical errors
that are routinely produced by both native and non-
native speakers of a language. The intention is not
to detect and correct the error, but rather to ignore
it. Our approach is to introduce grammatical noise
into WSJ sentences while retaining as much of the
structure of the original trees as possible. These
sentences and their associated trees are then used
as training material for a statistical parser. It is im-
portant that parsing on grammatical sentences is not
harmed and we introduce a parse-probability-based
classifier which allows both grammatical and un-
grammatical sentences to be accurately parsed.
2 Background
Various strategies exist to build robustness into the
parsing process: grammar constraints can be relaxed
(Fouvry, 2003), partial parses can be concatenated to
form a full parse (Penstein Rose? and Lavie, 1997),
the input sentence can itself be transformed until a
parse can be found (Lee et al, 1995), and mal-rules
describing particular error patterns can be included
in the grammar (Schneider and McCoy, 1998). For a
parser which tends to fail when faced with ungram-
matical input, such techniques are needed. The over-
generation associated with a statistical data-driven
parser means that it does not typically fail on un-
grammatical sentences. However, it is not enough
to return some analysis for an ungrammatical sen-
tence. If the syntactic analysis is to guide semantic
analysis, it must reflect as closely as possible what
the person who produced the sentence was trying to
express. Thus, while statistical, data-driven parsing
has solved the robustness problem, it is not clear that
it is has solved the accurate robustness problem.
The problem of adapting parsers to accurately
handle ungrammatical text is an instance of the do-
main adaptation problem where the target domain is
grammatically noisy data. A parser can be adapted
to a target domain by training it on data from the new
domain ? the problem is to quickly produce high-
quality training material. Our solution is to simply
modify the existing training material so that it re-
sembles material from the noisy target domain.
In order to tune a parser to syntactically ill-formed
text, a treebank is automatically transformed into an
ungrammatical treebank. This transformation pro-
cess has two parts: 1. the yield of each tree is trans-
formed into an ungrammatical sentence by introduc-
ing a syntax error; 2. each tree is minimally trans-
formed, but left intact as much as possible to reflect
the syntactic structure of the original ?intended? sen-
221
tence prior to error insertion. Artificial ungrammati-
calities have been used in various NLP tasks (Smith
and Eisner, 2005; Okanohara and Tsujii, 2007)
The idea of an automatically generated ungram-
matical treebank was proposed by Foster (2007).
Foster generates an ungrammatical version of the
WSJ treebank and uses this to train two statistical
parsers. The performance of both parsers signifi-
cantly improves on the artificially created ungram-
matical test data, but significantly degrades on the
original grammatical test data. We show that it
is possible to obtain significantly improved perfor-
mance on ungrammatical data without a concomi-
tant performance decline on grammatical data.
3 Generating Noisy Treebanks
Generating Noisy Sentences We apply the error
introduction procedure described in detail in Foster
(2007). Errors are introduced into sentences by ap-
plying the operations of word substitution, deletion
and insertion. These operations can be iteratively
applied to generate increasingly noisy sentences.
We restrict our attention to ungrammatical sentences
with a edit-distance of one or two words from the
original sentence, because it is reasonable to expect
a parser?s performance to degrade as the input be-
comes more ill-formed. The operations of substitu-
tion, deletion and insertion are not carried out en-
tirely at random, but are subject to some constraints
derived from an empirical study of ill-formed En-
glish sentences (Foster, 2005). Three types of word
substitution errors are produced: agreement errors,
real word spelling errors and verb form errors. Any
word that is not an adjective or adverb can be deleted
from any position within the input sentence, but
some part-of-speech tags are favoured over others,
e.g. it is more likely that a determiner will be deleted
than a noun. The error creation procedure can insert
an arbitrary word at any position within a sentence
but it has a bias towards inserting a word directly af-
ter the same word or directly after a word with the
same part of speech. The empirical study also in-
fluences the frequency at which particular errors are
introduced, with missing word errors being the most
frequent, followed by extra word errors, real word
spelling errors, agreement errors, and finally, verb
form errors. Table 1 shows examples of the kind of
ill-formed sentences that are produced when we ap-
ply the procedure to Wall Street Journal sentences.
Generating Trees for Noisy Sentences The tree
structures associated with the modified sentences are
also modified, but crucially, this modification is min-
imal, since a truly robust parser should return an
analysis for a mildly ungrammatical sentence that
remains as similar as possible to the analysis it re-
turns for the original grammatical sentence.
Assume that (1) is an original treebank tree for the
sentence A storm is brewing. Example (2) is then the
tree for the ungrammatical sentence containing an
is/it confusion. No part of the original tree structure
is changed apart from the yield.
(1) (S (NP A storm) (VP (VBZ is) (VP (VBG brewing))))
(2) (S (NP A storm) (VP (VBZ it) (VP (VBG brewing))))
An example of a missing word error is shown in
(3) and (4). A pre-terminal dominating an empty
node is introduced into the tree at the point where
the word has been omitted.
(3) (S (NP Annotators) (VP (VBP parse) (NP the sentences)))
(4) (S (NP Annotators) (VP (-NONE- 0) (NP the sentences)))
An example of an extra word error is shown in (5),
(6) and (7). For this example, two ungrammatical
trees, (6) and (7), are generated because there are
two possible positions in the original tree where the
extra word can be inserted which will result in a tree
with the yield He likes of the cake and which will not
result in the creation of any additional structure.
(5) (S (NP He) (VP (VBZ likes) (NP (DT the) (NN cake))))
(6) (S (NP He) (VP (VBZ likes) (IN of) (NP (DT the) (NN
cake))))
(7) (S (NP He) (VP (VBZ likes) (NP (IN of) (DT the) (NN
cake))))
4 Parser Adaptation Experiments
In order to obtain training data for our parsing ex-
periments, we introduce syntactic noise into the
usual WSJ training material, Sections 2-21, using
the procedures outlined in Section 3, i.e. for every
sentence-tree pair in WSJ2-21, we introduce an er-
ror into the sentence and then transform the tree so
that it covers the newly created ungrammatical sen-
tence. For 4 of the 20 sections in WSJ2-21, we apply
the noise introduction procedure to its own output to
222
Error Type WSJ00
Missing Word likely to bring new attention to the problem ? likely to new attention to the problem
Extra Word the $ 5.9 million it posted ? the $ 5.9 million I it posted
Real Word Spell Mr Vinken is chairman of Elsevier? Mr. Vinken if chairman of Elsevier
Agreement this event took place 35 years ago? these event took place 35 years ago
Verb Form But the Soviets might still face legal obstacles? But the Soviets might still faces legal obstacles
Table 1: Automatically Generated Ungrammatical Sentences
create even noisier data. Our first development set is
a noisy version of WSJ00, Noisy00, produced by ap-
plying the noise introduction procedure to the 1,921
sentences in WSJ00. Our second development set is
an even noisier version of WSJ00, Noisiest00, which
is created by applying our noise introduction proce-
dure to the output of Noisy00. We apply the same
process to WSJ23 to obtain our two test sets.
For all our parsing experiments, we use the June
2006 version of the two-stage parser reported in
Charniak and Johnson (2005). Evaluation is carried
out using Parseval labelled precision/recall. For ex-
tra word errors, there may be more than one gold
standard tree (see (6) and (7)). When this happens
the parser output tree is evaluated against all gold
standard trees and the maximum f-score is chosen.
We carry out five experiments. In the first ex-
periment, E0, we apply the parser, trained on well-
formed data, to noisy input. The purpose of E0 is to
ascertain how well a parser trained on grammatical
sentences, can ignore grammatical noise. E0 pro-
vides a baseline against which the subsequent ex-
perimental results can be judged. In the E1 experi-
ments, the parser is retrained using the ungrammati-
cal version of WSJ2-21. In experiment E1error, the
parser is trained on ungrammatical material only,
i.e. the noisy version of WSJ2-21. In experiment
E1mixed, the parser is trained on grammatical and
ungrammatical material, i.e. the original WSJ2-21 is
merged with the noisy WSJ2-21. In the E2 experi-
ments, a classifier is applied to the input sentence.
If the sentence is classified as ungrammatical, a ver-
sion of the parser that has been trained on ungram-
matical data is employed. In the E2ngram experi-
ment, we train a J48 decision tree classifier. Follow-
ing Wagner et al (2007), the decision tree features
are part-of-speech n-gram frequency counts, with n
ranging from 2 to 7 and with a subset of the BNC
as the frequency reference corpus. The decision tree
is trained on the original WSJ2-21 and the ungram-
matical WSJ2-21. In the E2prob experiment, the in-
put sentence is parsed with two parsers, the origi-
nal parser (the E0 parser) and the parser trained on
ungrammatical material (either the E1error or the
E1mixed parser). A very simple classifier is used
to decide which parser output to choose: if the E1
parser returns a higher parse probability for the most
likely tree than the E0 parser, the E1 parser output is
returned. Otherwise the E0 parser output is returned.
The baseline E0 results are in the first column of
Table 2. As expected, the performance of a parser
trained on well-formed input degrades when faced
with ungrammatical input. It is also not surprising
that its performance is worse on Noisiest00 (-8.8%
f-score) than it is on Noisy00 (-4.3%) since the Nois-
iest00 sentences contain two errors rather than one.
The E1 results occupy the second and third
columns of Table 2. An up arrow indicates a sta-
tistically significant improvement over the baseline
results, a down arrow a statistically significant de-
cline and a dash a change which is not statistically
significant (p < 0.01). Training the parser on un-
grammatical data has a positive effect on its perfor-
mance on Noisy00 and Noisiest00 but has a negative
effect on its performance on WSJ00. Training on a
combination of grammatical and ungrammatical ma-
terial gives the best results for all three development
sets. Therefore, for the E2 experiments we use the
E1mixed parser rather than the E1error parser.
The E2 results are shown in the last two columns
of Table 2 and the accuracy of the two classifiers in
Table 3. Over the three test sets, the E2prob classi-
fier outperforms the E2ngram classifier. Both classi-
fiers misclassify approximately 45% of the Noisy00
sentences. However, the sentences misclassified by
the E2prob classifier are those that are handled well
by the E0 parser, and this is reflected in the pars-
ing results for Noisy00. An important feature of the
223
Dev Set P R F P R F P R F P R F P R F
E0 E1-error E1-mixed E2prob E2ngram
WSJ00 91.5 90.3 90.9 91.0? 89.4 ? 90.2 91.3? 89.8 ? 90.5 91.5? 90.2? 90.9 91.3? 89.9? 90.6
Noisy00 87.5 85.6 86.6 89.4 ? 86.6 ? 88.0 89.4 ? 86.8 ? 88.1 89.1 ? 86.8 ? 87.9 88.7? 86.2? 87.5
Noisiest00 83.5 80.8 82.1 87.6 ? 83.6 ? 85.6 87.6 ? 83.8 ? 85.7 87.2 ? 83.7 ? 85.4 86.6? 83.0? 84.8
Table 2: Results of Parsing Experiments
Development Set E2prob E2ngram
WSJ00 76.7% 63.3%
Noisy00 55.1% 55.6%
Noisiest00 70.2% 66.0%
Table 3: E2 Classifier Accuracy
Test Set P R F P R F
E0 E2prob
WSJ23 91.7 90.8 91.3 91.7? 90.7? 91.2
Noisy23 87.4 85.6 86.5 89.2 ? 87.0 ? 88.1
Noisiest23 83.2 80.8 82.0 87.4 ? 84.1 ? 85.7
Table 4: Final Results for Section 23 Test Sets
E2prob classifier is that its use results in a constant
performance on the grammatical data - with no sig-
nificant degradation from the baseline.
Taking the E2prob results as our optimum, we
carry out the same experiment again on our WSJ23
test sets. The results are shown in Table 4. The same
effect can be seen for the test sets as for the devel-
opment sets - a significantly improved performance
on the ungrammatical data without an accompany-
ing performance decrease for the grammatical data.
The Noisy23 breakdown by error type is shown in
Table 5. The error type which the original parser is
most able to ignore is an agreement error. For this er-
ror type alone, the ungrammatical training material
seems to hinder the parser. The biggest improve-
ment occurs for real word spelling errors.
5 Conclusion
We have shown that it is possible to tune a WSJ-
trained statistical parser to ungrammatical text with-
Error Type P R F P R F
E0 E2-prob
Missing Word 88.5 83.7 86.0 88.9 84.3 86.5
Extra Word 87.2 89.4 88.3 89.2 89.7 89.4
Real Word Spell 84.3 83.0 83.7 89.5 88.2 88.9
Agreement 90.4 88.8 89.6 90.3 88.6 89.4
Verb Form 88.6 87.0 87.8 89.1 87.9 88.5
Table 5: Noisy23: Breakdown by Error Type
out affecting its performance on grammatical text.
This has been achieved using an automatically gen-
erated ungrammatical version of the WSJ treebank
and a simple binary classifier which compares parse
probabilities. The next step in this research is to see
how the method copes on ?real? errors - this will re-
quire manual parsing of a suitably large test set.
Acknowledgments We thank the IRCSET Em-
bark Initiative (postdoctoral fellowship P/04/232)
for supporting this research.
References
Eugene Charniak and Mark Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings of ACL-2005.
Jennifer Foster. 2005. Good Reasons for Noting Bad Gram-
mar: Empirical Investigations into the Parsing of Ungram-
matical Written English. Ph.D. thesis, University of Dublin,
Trinity College.
Jennifer Foster. 2007. Treebanks gone bad: Parser evaluation
and retraining using a treebank of ungrammatical sentences.
IJDAR, 10(3-4), December.
Frederik Fouvry. 2003. Robust Processing for Constraint-
based Grammar Formalisms. Ph.D. thesis, University of Es-
sex.
Kong Joo Lee, Cheol Jung Kweon, Jungyun Seo, and Gil Chang
Kim. 1995. A robust parser based on syntactic information.
In Proceedings of EACL-1995.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A discrimi-
native language model with pseudo-negative examples. In
Proceedings of ACL-2007.
Carolyn Penstein Rose? and Alon Lavie. 1997. An efficient dis-
tribution of labor in a two stage robust interpretation process.
In Proceedings of EMNLP-1997.
David Schneider and Kathleen McCoy. 1998. Recognizing
syntactic errors in the writing of second language learners.
In Proceedings of ACL/COLING-1998.
Noah A. Smith and Jason Eisner. 2005. Contrastive Estima-
tion: Training Log-Linear Models on Unlabeled Data. In
Proceedings of ACL-2005.
Joachim Wagner, Jennifer Foster, and Josef van Genabith.
2007. A comparative evaluation of deep and shallow ap-
proaches to the automatic detection of common grammatical
errors. In Proceedings of EMNLP-CoNLL-2007.
224
Proceedings of the 10th Conference on Parsing Technologies, pages 33?35,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Adapting WSJ-Trained Parsers to the British National Corpus Using
In-Domain Self-Training
Jennifer Foster, Joachim Wagner, Djame? Seddah and Josef van Genabith
National Centre for Language Technology
School of Computing, Dublin City University, Dublin 9, Ireland
{jfoster, jwagner, josef}@computing.dcu.ie, dseddah@paris4.sorbonne.fr?
Abstract
We introduce a set of 1,000 gold standard
parse trees for the British National Corpus
(BNC) and perform a series of self-training
experiments with Charniak and Johnson?s
reranking parser and BNC sentences. We
show that retraining this parser with a com-
bination of one million BNC parse trees
(produced by the same parser) and the orig-
inal WSJ training data yields improvements
of 0.4% on WSJ Section 23 and 1.7% on the
new BNC gold standard set.
1 Introduction
Given the success of statistical parsing models on
the Wall Street Journal (WSJ) section of the Penn
Treebank (PTB) (Charniak, 2000; Collins, 2003, for
example), there has been a change in focus in recent
years towards the problem of replicating this success
on genres other than American financial news sto-
ries. The main challenge in solving the parser adap-
tation problem are the resources required to con-
struct reliable annotated training examples.
A breakthrough has come in the form of research
by McClosky et al (2006a; 2006b) who show that
self-training can be used to improve parser perfor-
mance when combined with a two-stage reranking
parser model (Charniak and Johnson, 2005). Self-
training is the process of training a parser on its own
output, and earlier self-training experiments using
generative statistical parsers did not yield encour-
aging results (Steedman et al, 2003). McClosky et
al. (2006a; 2006b) proceed as follows: sentences
?Now affiliated to Lalic, Universite? Paris 4 La Sorbonne.
from the LA Times newspaper are parsed by a first-
stage generative statistical parser trained on some
seed training data (WSJ Sections 2-21) and the n-
best parse trees produced by this parser are reranked
by a discriminative reranker. The highest ranked
parse trees are added to the training set of the parser
and the parser is retrained. This self-training method
gives improved performance, not only on Section
23 of the WSJ (an absolute f-score improvement of
0.8%), but also on test sentences from the Brown
corpus (Francis and Kuc?era, 1979) (an absolute f-
score improvement of 2.6%).
In the experiments of McClosky et al (2006a;
2006b), the parse trees used for self-training come
from the same domain (American newspaper text)
as the parser?s original seed training material. Bac-
chiani et al (2006) find that self-training is ef-
fective when the parse trees used for self-training
(WSJ parse trees) come from a different domain to
the seed training data and from the same domain as
the test data (WSJ sentences). They report a per-
formance boost of 4.2% on WSJ Section 23 for a
generative statistical parser trained on Brown seed
data when it is self-trained using 200,000 WSJ parse
trees. However, McCloskey et al (2006b) report a
drop in performance for their reranking parser when
the experiment is repeated in the opposite direction,
i.e. with Brown data for self-training and testing,
and WSJ data for seed training. In contrast, we re-
port successful in-domain1 self-training experiments
with the BNC data as self-training and test material,
and with the WSJ-trained reranking parser used by
McCloskey et al (2006a; 2006b).
We parse the BNC (Burnard, 2000) in its entirety
1We refer to data as being in-domain if it comes from the
same domain as the test data and out-of-domain if it does not.
33
using the reranking parser of Charniak and Johnson
(2005). 1,000 BNC sentences are manually anno-
tated for constituent structure, resulting in the first
gold standard set for this corpus. The gold standard
set is split into a development set of 500 parse trees
and a test set of 500 parse trees and used in a series
of self-training experiments: Charniak and John-
son?s parser is retrained on combinations of WSJ
treebank data and its own parses of BNC sentences.
These combinations are tested on the BNC devel-
opment set and Section 00 of the WSJ. An optimal
combination is chosen which achieves a Parseval la-
belled bracketing f-score of 91.7% on Section 23
and 85.6% on the BNC gold standard test set. For
Section 23 this is an absolute improvement of 0.4%
on the baseline results of this parser, and for the
BNC data this is a statistically significant improve-
ment of 1.7%.
2 The BNC Data
The BNC is a 100-million-word balanced part-of-
speech-tagged corpus of written and transcribed
spoken English. Written text comprises 90% of the
BNC: 75% non-fictional and 25% fictional. To fa-
cilitate parsing with a WSJ-trained parser, some re-
versible transformations were applied to the BNC
data, e.g. British English spellings were converted
to American English and neutral quotes disam-
biguated. The reranking parser of Charniak and
Johnson (2005) was used to parse the BNC. 99.8%
of the 6 million BNC sentences obtained a parse,
with an average parsing speed of 1.4s per sentence.
A gold standard set of 1,000 BNC sentences was
constructed by one annotator by correcting the out-
put of the first stage of Charniak and Johnson?s
reranking parser. The sentences included in the gold
standard were chosen at random from the BNC, sub-
ject to the condition that they contain a verb which
does not occur in the training sections of the WSJ
section of the PTB (Marcus et al, 1993). A deci-
sion was made to select sentences for the gold stan-
dard set which differ from the sentences in the WSJ
training sections, and one way of finding different
sentences is to focus on verbs which are not attested
in the WSJ Sections 2-21. It is expected that these
gold standard parse trees can be used as training
data although they are used only as test and develop-
ment data in this work. Because they contain verbs
which do not occur in the parser?s training set, they
are likely to represent a hard test for WSJ-trained
parsers. The PTB bracketing guidelines (Bies et al,
1995) and the PTB itself were used as references by
the BNC annotator. Functional tags and traces were
not annotated. The annotator noticed that the PTB
parse trees sometimes violate the PTB bracketing
guidelines, and in these cases, the annotator chose
the analysis set out in the guidelines. It took approx-
imately 60 hours to build the gold standard set.
3 Self-Training Experiments
Charniak and Johnson?s reranking parser (June 2006
version) is evaluated against the BNC gold stan-
dard development set. Labelled precision (LP), re-
call (LR) and f-score measures2 for this parser are
shown in the first row of Table 1. The f-score of
83.7% is lower than the f-score of 85.2% reported
by McClosky et al (2006b) for the same parser on
Brown corpus data. This difference is reasonable
since there is greater domain variation between the
WSJ and the BNC than between the WSJ and the
Brown corpus, and all BNC gold standard sentences
contain verbs not attested in WSJ Sections 2-21.
We retrain the first-stage generative statistical
parser of Charniak and Johnson using combinations
of BNC trees (parsed using the reranking parser)
and WSJ treebank trees. We test the combinations
on the BNC gold standard development set and on
WSJ Section 00. Table 1 shows that parser accu-
racy increases with the size of the in-domain self-
training material.3 The figures confirm the claim of
McClosky et al (2006a) that self-training with a
reranking parsing model is effective for improving
parser accuracy in general, and the claim of Gildea
(2001) that training on in-domain data is effective
for parser adaption. They confirm that self-training
on in-domain data is effective for parser adaptation.
The WSJ Section 00 results suggest that, in order
to maintain performance on the seed training do-
main, it is necessary to combine BNC parse trees
2All scores are for the second stage of the parsing process,
i.e. the evaluation takes place after the reranking. All evalua-
tion is carried out using the Parseval labelled bracketing metrics,
with evalb and parameter file new.prm.
3The notation bnc500K+5wsj refers to a set of 500,000
parser output parse trees of sentences taken randomly from the
BNC concatenated with five copies of WSJ Sections 2-21.
34
BNC Development WSJ Section 00
Self-Training LP LR LF LP LR LF
- 83.6 83.7 83.7 91.6 90.5 91.0
bnc50k 83.7 83.7 83.7 90.0 88.0 89.0
bnc50k+1wsj 84.4 84.4 84.4 91.6 90.3 91.0
bnc250k 84.7 84.5 84.6 91.1 89.3 90.2
bnc250k+5wsj 85.0 84.9 85.0 91.8 90.5 91.2
bnc500k+5wsj 85.2 85.1 85.2 91.9 90.4 91.2
bnc500k+10wsj 85.1 85.1 85.1 91.9 90.6 91.2
bnc1000k+5wsj 86.5 86.2 86.3 91.7 90.3 91.0
bnc1000k+10wsj 86.1 85.9 86.0 92.0 90.5 91.3
bnc1000k+40wsj 85.5 85.5 85.5 91.9 90.6 91.3
BNC Test WSJ Section 23
- 84.0 83.7 83.9 91.8 90.9 91.3
bnc1000k+10wsj 85.7 85.4 85.6 92.3 91.1 91.7
Table 1: In-domain Self-Training Results
with the original seed training material during the
self-training phase.
Of the self-training combinations with above-
baseline improvements for both development sets,
the combination of 1,000K BNC parse trees and
Section 2-21 of the WSJ (multiplied by ten) yields
the highest improvement for the BNC data, and we
present final results with this combination for the
BNC gold standard test set and WSJ Section 23.
There is an absolute improvement on the original
reranking parser of 1.7% on the BNC gold standard
test set and 0.4% on WSJ Section 23. The improve-
ment on BNC data is statistically significant for both
precision and recall (p < 0.0002, p < 0.0002). The
improvement on WSJ Section 23 is statistically sig-
nificant for precision only (p < 0.003).
4 Conclusion and Future Work
We have introduced a set of 1,000 gold standard
parse trees for the BNC. We have performed self-
training experiments with Charniak and Johnson?s
reranking parser and sentences from the BNC. We
have shown that retraining this parser with a com-
bination of one million BNC parse trees (produced
by the same parser) and the original WSJ train-
ing data yields improvements of 0.4% on WSJ Sec-
tion 23 and 1.7% on the BNC gold standard sen-
tences. These results indicate that self-training on
in-domain data can be used for parser adaptation.
Our BNC gold standard set consists of sentences
containing verbs which are not in the WSJ train-
ing sections. We suspect that this makes the gold
standard set a hard test for WSJ-trained parsers, and
our results are likely to represent a lower bound for
WSJ-trained parsers on BNC data. When used as
training data, we predict that the novel verbs in the
BNC gold standard set add to the variety of train-
ing material, and will further help parser adaptation
from the WSJ domain ? a matter for further research.
Acknowledgments We thank the IRCSET Em-
bark Initiative (basic research grant SC/02/298
and postdoctoral fellowship P/04/232), Science
Foundation Ireland (Principal Investigator grant
04/IN.3/I527) and the Irish Centre for High End
Computing for supporting this research.
References
Michiel Bacchiani, Michael Riley, Brian Roark, and Richard
Sproat. 2006. Map adaptation of stochastic grammars.
Computer Speech and Language, 20(1):41?68.
Ann Bies, Mark Ferguson, Karen Katz, and Robert MacIntyre.
1995. Bracketing guidelines for treebank II style, Penn Tree-
bank project. Technical Report MS-CIS-95-06, University
of Pennsylvania.
Lou Burnard. 2000. User reference guide for the British Na-
tional Corpus. Technical report, Oxford University.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best-parsing and maxent discriminative reranking. In Pro-
ceedings of ACL-05, pages 173?180, Barcelona.
Eugene Charniak. 2000. A maximum-entropy-inspired parser.
In Proceedings of NAACL-00, pages 132?139, Seattle.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. Computational Linguistics,
29(4):499?637.
W. Nelson Francis and Henry Kuc?era. 1979. Brown Corpus
Manual. Technical report, Brown University, Providence.
Daniel Gildea. 2001. Corpus variation and parser performance.
In Proceedings of EMNLP-01, pages 167?202, Barcelona.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguistics,
19(2):313?330.
David McClosky, Eugene Charniak, and Mark Johnson. 2006a.
Effective self-training for parsing. In Proceedings of HLT-
NAACL-06, pages 152?159, New York.
David McClosky, Eugene Charniak, and Mark Johnson. 2006b.
Reranking and self-training for parser adaptation. In Pro-
ceedings of COLING-ACL-06, pages 337?344, Sydney.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-strapping
statistical parsers from small datasets. In Proceedings of
EACL-03, Budapest.
35
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 176?179,
Paris, October 2009. c?2009 Association for Computational Linguistics
The effect of correcting grammatical errors on parse probabilities
Joachim Wagner
CNGL
School of Computing
Dublin City University, Ireland
jwagner@computing.dcu.ie
Jennifer Foster
NCLT
School of Computing
Dublin City University, Ireland.
jfoster@computing.dcu.ie
Abstract
We parse the sentences in three parallel er-
ror corpora using a generative, probabilis-
tic parser and compare the parse probabil-
ities of the most likely analyses for each
grammatical sentence and its closely re-
lated ungrammatical counterpart.
1 Introduction
The syntactic analysis of a sentence provided by
a parser is used to guide the interpretation process
required, to varying extents, by applications such
as question-answering, sentiment analysis and ma-
chine translation. In theory, however, parsing also
provides a grammaticality judgement as shown in
Figure 1. Whether or not a sentence is grammati-
cal is determined by its parsability with a grammar
of the language in question.
The use of parsing to determine whether a sen-
tence is grammatical has faded into the back-
ground as hand-written grammars aiming to de-
scribe only the grammatical sequences in a lan-
guage have been largely supplanted by treebank-
derived grammars. Grammars read from treebanks
tend to overgenerate. This overgeneration is un-
problematic if a probabilistic model is used to rank
analyses and if the parser is not being used to pro-
vide a grammaticality judgement. The combina-
tion of grammar size, probabilistic parse selection
and smoothing techniques results in high robust-
ness to errors and broad language coverage, de-
sirable properties in applications requiring a syn-
tactic analysis of any input, regardless of noise.
However, for applications which rely on a parser?s
ability to distinguish grammatical sequences from
ungrammatical ones, e.g. grammar checkers, over-
generating grammars are perhaps less useful as
they fail to reject ungrammatical strings.
A naive solution might be to assume that the
probability assigned to a parse tree by its proba-
bilistic model could be leveraged in some way to
Figure 1: Grammaticality and formal languages
determine the sentence?s grammaticality. In this
paper, we explore one aspect of this question by
using three parallel error corpora to determine the
effect of common English grammatical errors on
the parse probability of the most likely parse tree
returned by a generative probabilistic parser.
2 Related Work
The probability of a parse tree has been used be-
fore in error detection systems. Sun et al (2007)
report only a very modest improvement when they
include a parse probability feature in their system
whose features mostly consist of linear sequential
patterns. Lee and Seneff (2006) detect ungram-
matical sentences by comparing the parse proba-
bility of a possibly ill-formed input sentence to the
parse probabilities of candidate corrections which
are generated by arbitrarily deleting, inserting and
substituting articles, prepositions and auxiliaries
and changing the inflection of verbs and nouns.
Foster et al (2008) compare the parse probabil-
ity returned by a parser trained on a regular tree-
bank to the probability returned by the same parser
trained on a ?noisy? treebank and use the differ-
ence to decide whether the sentence is ill-formed.
Research in the field of psycholinguistics has
explored the link between frequency and gram-
maticality, often focusing on borderline acceptable
sentences (see Crocker and Keller (2006) for a dis-
cussion of the literature). Koonst-Garboden and
Jaeger (2003) find a weak correlation between the
176
frequency ratios of competing surface realisations
and human acceptability judgements. Hale (2003)
calculates the information-theoretic load of words
in sentences assuming that they were generated ac-
cording to a probabilistic grammar and finds that
these values are good predictors for observed read-
ing time and other measures of cognitive load.
3 Experimental Setup
The aim of this experiment is to find out to
what extent ungrammatical sentences behave dif-
ferently from correct sentences as regards their
parse probabilities. There are two types of corpora
we study: two parallel error corpora that consist
of authentic ungrammatical sentences and manual
corrections, and a parallel error corpus that con-
sists of authentic grammatical sentences and auto-
matically induced errors. Using parallel corpora
allows us to compare pairs of sentences that have
the same or very similar lexical content and dif-
fer only with respect to their grammaticality. A
corpus with automatically induced errors is in-
cluded because such a corpus is much larger and
controlled error insertion allows us to examine di-
rectly the effect of a particular error type.
The first parallel error corpus contains 1,132
sentence pairs each comprising an ungrammatical
sentence and a correction (Foster, 2005). The sen-
tences are taken from written texts and contain ei-
ther one or two grammatical errors. The errors in-
clude those made by native English speakers. We
call this the Foster corpus. The second corpus
is a learner corpus. It contains transcribed spo-
ken utterances produced by learners of English of
varying L1s and levels of experience in a class-
room setting. Wagner et al (2009) manually cor-
rected 500 sentences of the transcribed utterances,
producing a parallel error corpus which we call
Gonzaga 500. The third parallel corpus contains
199,600 sentences taken from the British National
Corpus and ungrammatical sentences produced by
introducing errors of the following five types into
the original BNC sentences: errors involving an
extra word, errors involving a missing word, real-
word spelling errors, agreement errors and errors
involving an incorrect verbal inflection.
All sentence pairs in the three parallel cor-
pora are parsed using the June 2006 version
of the first-stage parser of Charniak and John-
son (2005), a lexicalised, generative, probabilistic
parser achieving competitive performance on Wall
Street Journal text. We compare the probability of
the highest ranked tree for the grammatical sen-
tence in the pair to the probability of the highest
ranked tree for the ungrammatical sentence.
4 Results
Figure 2 shows the results for the Foster corpus.
For ranges of 4 points on the logarithmic scale,
the bars depict how many sentence pairs have a
probability ratio within the respective range. For
example, there are 48 pairs (5th bar from left) for
which the correction has a parse probability which
is between 8 and 12 points lower than the parse
probability of its erroneous original, or, in other
words, for which the probability ratio is between
e?12 and e?8. 853 pairs show a higher probabil-
ity for the correction vs. 279 pairs which do not.
Since the probability of a tree is the product of
its rule probabilities, sentence length is a factor.
If we focus on corrections that do not change the
sentence length, the ratio sharpens to 414 vs. 90
pairs. Ungrammatical sentences do often receive
lower parse probabilities than their corrections.
Figure 3 shows the results for the Gonzaga 500.
Here we see a picture similar to the Foster cor-
pus although the peak for the range from e0 = 1
to e4 ? 54.6 is more pronounced. This time
there are more cases where the parse probability
drops despite a sentence being shortened and vice
versa. Overall, 348 sentence pairs show an in-
creased parse probability, 152 do not. For sen-
tences that stay the same length the ratio is 154
to 34, or 4.53:1, for this corpus which is almost
identical to the Foster corpus (4.60:1).
How do these observations translate to the artifi-
cial parallel error corpus created from BNC data?
Figure 4 shows the results for the BNC data. In
order to keep the orientation of the graph as be-
fore, we change the sign by looking at decrements
instead of increments. Also, we swap the keys
for shortened and lengthened sentences. Clearly,
the distribution is wider and moved to the right.
The peak is at the bar labelled 10. Accordingly,
the ratio of the number of sentence pairs above
and below the zero line is much higher than be-
fore (overall 32,111 to 167, 489 = 5.22, for same
length only 8,537 to 111,171 = 13.02), suggest-
ing that our artificial errors might have a stronger
effect on parse probability than authentic errors.
Another possible explanation is that the BNC data
only contains five error types, whereas the range of
177
Figure 2: Effect of correcting erroneous sentences (Foster corpus) on the probability of the best parse.
Each bar is broken down by whether and how the correction changed the sentence length in tokens. A
bar labelled x covers ratios from ex?2 to ex+2 (exclusive).
Figure 3: Effect of correcting erroneous sentences (Gonzaga 500 corpus) on the probability of the best
parse.
Figure 4: Effect of inserting errors into BNC sentences on the probability of the best parse.
178
errors in the Foster and Gonzaga corpus is wider.
Analysing the BNC data by error type and look-
ing firstly at those error types that do not involve a
change in sentence length, we see that:
? 96% of real-word spelling errors cause a re-
duction in parse probability.
? 91% of agreement errors cause a reduction in
parse probability. Agreement errors involving
articles most reliably decrease the probability.
? 92% of verb form errors cause a reduction.
Changing the form from present participle to
past participle1 is least likely to cause a reduc-
tion, whereas changing it from past participle
to third singular is most likely.
The effect of error types which change sentence
length is more difficult to interpret. Almost all of
the extra word errors cause a reduction in parse
probability and it is difficult to know whether this
is happening because the sentence length has in-
creased or because an error has been introduced.
The errors involving missing words do not system-
atically result in an increase in parse probability
? 41% of them cause a reduction in parse proba-
bility, and this is much more likely to occur if the
missing word is a function word (article, auxiliary,
preposition).
Since the Foster corpus is also error-annotated,
we can also examine its results by error type. This
analysis broadly agrees with that of the BNC data,
although the percentage of ill-formed sentences
for which there is a reduction in parse probability
is generally lower (see Fig. 2 vs. Fig. 4).
5 Conclusion
We have parsed the sentences in three parallel er-
ror corpora using a generative, probabilistic parser
and examined the parse probability of the most
likely analysis of each sentence. We find that
grammatical errors have some negative effect on
the probability assigned to the best parse, a find-
ing which corroborates previous evidence linking
sentence grammaticality to frequency. In our ex-
periment, we approximate sentence probability by
looking only at the most likely analysis ? it might
be useful to see if the same effect holds if we sum
1This raises the issue of covert errors, resulting in gram-
matical sentence structures. Lee and Seneff (2008) give the
example I am prepared for the exam which was produced by
a learner of English instead of I am preparing for the exam.
These occur in authentic error corpora and cannot be com-
pletely avoided when automatically introducing errors.
over parse trees. To fully exploit parse or sentence
probability in an error detection system, it is nec-
essary to fully account for the effect on probability
of 1) non-structural factors such as sentence length
and 2) particular error types. This study repre-
sents a contribution towards the latter.
Acknowledgements
We are grateful to James Hunter from Gonzaga
University for providing us with a learner corpus.
We thank Josef van Genabith and the reviewers for
their comments and acknowledge the Irish Cen-
tre for High-End Computing for the provision of
computational facilities. The BNC is distributed
by Oxford University Computing Services.
References
Eugene Charniak and Mark Johnson. 2005. Course-
to-fine n-best-parsing and maxent discriminative
reranking. In Proceedings of ACL.
Matthew W. Crocker and Frank Keller. 2006. Prob-
abilistic grammars as models of gradience in lan-
guage processing. In Gisbert Fanselow, C. Fe?ry,
R. Vogel, and M. Schlesewsky, editors, Gradience
in Grammar: Generative Perspectives, pages 227?
245. Oxford University Press.
Jennifer Foster, Joachim Wagner, and Josef van Gen-
abith. 2008. Adapting a WSJ-trained parser to
grammatically noisy text. In Proceedings of ACL.
Jennifer Foster. 2005. Good Reasons for Noting Bad
Grammar: Empirical Investigations into the Pars-
ing of Ungrammatical Written English. Ph.D. the-
sis, University of Dublin, Trinity College.
John Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101?123.
Andrew Koontz-Garboden and T. Florian Jaeger.
2003. An empirical investigation of the frequency-
grammaticality correlation hypothesis. Student es-
say received or downloaded on 2006-03-13.
John Lee and Stephanie Seneff. 2006. Automatic
grammar correction for second-language learners.
In Interspeech 2006 - 9th ICSLP, pages 1978?1981.
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. In Proceedings of ACL.
Guihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proc. of ACL.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experiments
in sentence classification. CALICO Journal, 26(3).
179
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 223?229,
Dublin, Ireland, August 23-24, 2014.
DCU: Aspect-based Polarity Classification for SemEval Task 4
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab Barman
Dasha Bogdanova, Jennifer Foster and Lamia Tounsi
CNGL Centre for Global Intelligent Content
National Centre for Language Technology
School of Computing
Dublin City University
Dublin, Ireland
{jwagner,parora,scortes,ubarman}@computing.dcu.ie
{dbogdanova,jfoster,ltounsi}@computing.dcu.ie
Abstract
We describe the work carried out by DCU
on the Aspect Based Sentiment Analysis
task at SemEval 2014. Our team submit-
ted one constrained run for the restaurant
domain and one for the laptop domain for
sub-task B (aspect term polarity predic-
tion), ranking highest out of 36 systems on
the restaurant test set and joint highest out
of 32 systems on the laptop test set.
1 Introduction
This paper describes DCU?s participation in the
Aspect Term Polarity sub-task of the Aspect Based
Sentiment Analysis task at SemEval 2014, which
focuses on predicting the sentiment polarity of as-
pect terms for a restaurant and a laptop dataset.
Given, for example, the sentence I have had so
many problems with the computer and the aspect
term the computer, the task is to predict whether
the sentiment expressed towards the aspect term is
positive, negative, neutral or conflict.
Our polarity classification system uses super-
vised machine learning with support vector ma-
chines (SVM) (Boser et al., 1992) to classify an
aspect term into one of the four classes. The fea-
tures we employ are word n-grams (with n rang-
ing from 1 to 5) in a window around the aspect
term, as well as features derived from scores as-
signed by a sentiment lexicon. Furthermore, to
reduce data sparsity, we experiment with replacing
sentiment-bearing words in our n-gram feature set
with their polarity scores according to the lexicon
and/or their part-of-speech tag.
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
The paper is organised as follows: in Section 2,
we describe the sentiment lexicons used in this
work and detail the process by which they are
combined, filtered and extended; in Section 3, we
describe our baseline method, a heuristic approach
which makes use of the sentiment lexicon, fol-
lowed by our machine learning method which in-
corporates the rule-based method as features in ad-
dition to word n-gram features; in Section 4, we
present the results of both methods on the training
and test data, and perform an error analysis on the
test set; in Section 5, we compare our approach to
previous research in sentiment classification; Sec-
tion 6 discusses efficiency of our system and on-
going work to improve its speed; finally, in Sec-
tion 7, we conclude and provide suggestions as to
how this research could be fruitfully extended.
2 Sentiment Lexicons
The following four lexicons are employed:
1. MPQA
1
(Wilson et al., 2005) classifies a
word or a stem and its part of speech tag
into positive, negative, both or neutral with
a strong or weak subjectivity.
2. SentiWordNet
2
(Baccianella et al., 2010)
specifies the positive, negative and objective
scores of a synset and its part of speech tag.
3. General Inquirer
3
indicates whether a word
expresses positive or negative sentiment.
4. Bing Liu?s Opinion Lexicon
4
(Hu and Liu,
1
http://mpqa.cs.pitt.edu/lexicons/
subj_lexicon/
2
http://sentiwordnet.isti.cnr.it/
3
http://www.wjh.harvard.edu/
?
inquirer/
inqtabs.txt
4
http://www.cs.uic.edu/
?
liub/FBS/
sentiment-analysis.html#lexicon
223
2004) indicates whether a word expresses
positive or negative sentiment.
2.1 Lexicon Combination
Since the four lexicons differ in their level of detail
and in how they present information, it is neces-
sary, when combining them, to consolidate the in-
formation and present it in a uniform manner. Our
combination strategy assigns a sentiment score to
a word as follows:
? MPQA: 1 for strong positive subjectivity, -1
for strong negative subjectivity, 0.5 for weak
positive subjectivity, -0.5 for weak negative
subjectivity, and 0 otherwise
? SentiWordNet: The positive score if the pos-
itive score is greater than the negative and ob-
jective scores, the negative score if the nega-
tive score is greater than the positive and the
objective scores, and 0 otherwise
? General Inquirer and Bing Liu?s Opinion
Lexicon: 1 for positive and -1 for negative
The above four scores are summed to arrive at a
final score between -4 and 4 for a word.
5
2.2 Lexicon Filtering
Initial experiments with our sentiment lexicon and
the training data led us to believe that there were
many irrelevant entries that, although capable of
conveying sentiment in some other context, were
not contributing to the sentiment of aspect terms
in the two domains of the task. Therefore, these
words are manually filtered from the lexicon. Ex-
amples of deleted words are just, clearly, indi-
rectly, really and back.
2.3 Adding Domain-Specific Words
A manual inspection of the training data revealed
words missing from the merged sentiment lexicon
but which do express sentiment in these domains.
Examples are mouthwatering, watery and better-
configured. We add these to the lexicon with a
score of either 1 or -1 (depending on their polarity
in the training data). We also add words (e.g. zesty,
acrid) from an online list of culinary terms.
6
5
We also tried to vote over the four lexicon scores but this
did not improve over summing.
6
http://world-food-and-wine.com/
describing-food
2.4 Handling Variation
In order to ensure that all inflected forms of a
word are covered, we lemmatise the words in the
training data using the IMS TreeTagger (Schmid,
1994) and we construct new possibilities using a
suffix list. To correct misspelled words, we con-
sider the corrected form of a misspelled word to be
the form with the highest frequency in a reference
corpus
7
among all the forms within an edit dis-
tance of 1 and 2 from the misspelled word (Norvig,
2012). Multi-word expressions of the form x-y
are added with the polarity of xy or x, as in laid-
back/laidback and well-shaped/well. Expressions
x y, are added with the polarity of x-y, as in so
so/so-so.
3 Methodology
We first build a rule-based system which classi-
fies the polarity of an aspect term based solely on
the scores assigned by the sentiment lexicon. We
then explore different ways of converting the rule-
based system into features which can then be com-
bined with bag-of-n-gram features in a supervised
machine learning set-up.
3.1 Rule-Based Approach
In order to predict the polarity of an aspect term,
we sum the polarity scores of all the words in the
surrounding sentence according to our sentiment
lexicon. Since not all the sentiment words occur-
ring in a sentence influence the polarity of the as-
pect term to the same extent, it is important to
weight the score of each sentiment word by its dis-
tance to the aspect term. Therefore, for each word
in the sentence which is found in our lexicon we
take the score from the lexicon and divide it by its
distance to the aspect term. The distance is calcu-
lated using the sum of the following three distance
functions:
? Token Distance: This function calculates the
difference in the position of the sentiment
word and the aspect term by counting the to-
kens between them.
7
The reference corpus consists of about a million
words retrieved from several public domain books from
Project Gutenberg (http://www.gutenberg.org/),
lists of most frequent words from Wiktionary (http:
//en.wiktionary.org/wiki/Wiktionary:
Frequency_lists) and the British National Corpus
(http://www.kilgarriff.co.uk/bnc-readme.
html) and two thousand laptop reviews crawled from CNET
(http://www.cnet.com/).
224
? Discourse Chunk Distance: This function
counts the discourse chunks that must be
crossed in order to get from the sentiment
word to the aspect term. If the sentiment
word and the aspect term are in the same
discourse chunk, then the distance is zero.
We use the discourse segmenter described in
(Tofiloski et al., 2009).
? Dependency Path Distance: This function
calculates the shortest path between the sen-
timent word and the aspect term in a syntac-
tic dependency graph for the sentence, pro-
duced by parsing the sentence with a PCFG-
LA parser (Attia et al., 2010) trained on con-
sumer review data (Le Roux et al., 2012)
8
,
and converting the resulting phrase-structure
tree into a dependency graph using the Stan-
ford converter (de Marneffe and Manning,
2008) (version 3.3.1).
Since our lexicon also contains multi-word ex-
pressions such as finger licking, we also look up
bigrams and trigrams from the input sentence in
our lexicon. Negation is handled by reversing the
polarity of sentiment words that appear within a
window of three words of the following negators:
not, n?t, no and never.
For each aspect term, we use the distance-
weighted sum of the polarity scores to predict one
of the three classes positive, negative and neutral.
9
After experimenting with various thresholds we
settled on the following simple strategy: if the po-
larity score for an aspect term is greater than zero
then it is classified as positive, if the score is less
than zero, then it is classified as negative, other-
wise it is classified as neutral.
3.2 Machine Learning Approach
We train a four-way SVM classifier for each do-
main (laptop and restaurant), using Weka?s SMO
implementation (Platt, 1998; Hall et al., 2009).
10
8
To facilitate parsing, the data was normalised using the
process described in (Le Roux et al., 2012) with minor mod-
ifications, e. g. treatment of non-breakable space characters,
abbreviations and emoticons. The normalised version of the
data was used for all experiments.
9
We also experimented with classifying aspect terms as
conflict when the individual scores for positive and negative
sentiment were both relatively high. However, this proved
unsuccessful.
10
We also experimented with logistic regression, random
forests, k-nearest neighbour, naive Bayes and multi-layer per-
ceptron in Weka, but did not match performance of an SVM
trained with default parameters.
Transf. n c n-gram Freq.
-L? 2 2 cord with 1
AL? 2 2 <aspect> with 56
ALS? 1 4 <negu080> 595
ALSR- 1 4 <negu080> 502
AL? 2 4 and skip 1
ALSR- 2 4 and <negu080> 25
ALSRP 1 4 <negu080>/vb 308
Table 1: 7 of the 2,640 bag-of-n-gram features
extracted for the aspect term cord from the lap-
top training sentence I charge it at night and skip
taking the cord with me because of the good bat-
tery life. The last column shows the frequency of
the feature in the training data. Transformations:
A=aspect, L=lowercase, S=score, R=restricted to
certain POS, P=POS annotation
Our system submission uses bag-of-n-gram fea-
tures and features derived from the rule-based ap-
proach. Decisions about parameters are made in 5-
fold cross-validation on the training data provided
for the task.
3.2.1 Bag-of-N-gram Features
We extract features encoding the presence of spe-
cific lower-cased n-grams (L) (n = 1, ..., 5) in
the context of the aspect term to be classified (c
words to the left and c words to the right with
c = 1, ..., 5, inf) for 10 combinations of trans-
formations: replacement of the aspect term with
<ASPECT> (A), replacement of sentiment words
with a discretised score (S), restriction (R) of the
sentiment word replacement to certain parts-of-
speech, and annotation of the discretised score
with the POS (P) of the sentiment word. An ex-
ample is shown in Table 1.
3.2.2 Adding Rule-Based Score Features
We explore two approaches for incorporating in-
formation from the rule-based approach (Sec-
tion 3.1) into our SVM classifier. The first ap-
proach is to encode polarity scores directly as the
following four features:
1. distance-weighted sum of scores of positive
words in the sentence
2. distance-weighted sum of scores of negative
words in the sentence
3. number of positive words in the sentence
225
4. number of negative words in the sentence
The second approach is less direct: for each do-
main, we train J48 decision trees with minimum
leaf size 60 using the four rule-based features de-
scribed above. We then use the decision rules
and the conjunctions leading from the root node
to each leaf node to binarise the above four basic
score features, producing 122 features. Further-
more, we add normalised absolute values, rank of
values and interval indicators, producing 48 fea-
tures.
3.2.3 Submitted Runs
We eliminate features that have redundant value
columns for the training data, and we apply fre-
quency thresholds (13, 18, 25 and 35) to further
reduce the number of features. We perform a grid-
search to optimise the parameters C and ? of the
SVM RBF kernel. We choose the system to sub-
mit based on average cross-validation accuracy.
We experiment with combinations of the three fea-
ture sets described above. We choose the bina-
rised features over the raw rule-based scores be-
cause cross-validation results are inferior for the
rule-based scores in initial experiments with fea-
ture frequency threshold 35: 70.26 vs. 71.36 for
laptop and 72.06 vs. 72.15 for restaurant. There-
fore, we decide to focus on systems with binarised
score features for lower feature frequency thresh-
olds, which are more CPU-intensive to train. For
both domains, the system we end up submitting
is a combination of the n-gram features and the
binarised features with parameters C = 3.981,
? = 0.003311 for the laptop data, C = 1.445,
? = 0.003311 for the restaurant data, and a fre-
quency threshold of 13.
4 Results and Analysis
Table 2 shows the training and test accuracy of
the task baseline system (Pontiki et al., 2014), a
majority baseline classifying everything as posi-
tive, our rule-based system and our submitted sys-
tem. The restaurant domain has a higher accuracy
than the laptop domain for all systems, the SVM
system outperforms the rule-based system on both
domains, and the test accuracy is higher than the
training accuracy for all systems in the restaurant
domain.
We observe that the majority of our systems? er-
rors fall into the following categories:
Dataset System Training Test
Laptop Baseline ? 51.1%
Laptop All positive 41.9% 52.1%
Laptop Rule-based 65.4% 67.7%
Laptop SVM 72.3% 70.5%
Restaurant Baseline ? 64.3%
Restaurant All positive 58.6% 64.2%
Restaurant Rule-based 69.5% 77.8%
Restaurant SVM 72.7% 81.0%
Table 2: Accuracy of the task baseline system, a
system classifying everything as positive, our rule-
based system and our submitted SVM-based sys-
tem on train (5-fold cross-validation) and test sets
? Sentiment not expressed explicitly: The
sentiment cannot be inferred from local lexi-
cal and syntactic information, e. g. The sushi
is cut in blocks bigger than my cell phone.
? Non-obvious expression of negation: For
example, The Management was less than ac-
comodating [sic]. The rule-based approach
does not capture such cases and there are
not enough similar training examples for the
SVM to learn to correctly classify them.
? Conflict cases: The training data contains
too few examples of conflict sentences for the
system to learn to detect them.
11
For the restaurant domain, there are more than
fifty cases where the rule-based approach fails to
detect sentiment, but the machine learning ap-
proach classifies it correctly. Most of these cases
contain no sentiment lexicon words, thus the rule-
based system marks them as being neutral. How-
ever, the machine learning system was able to fig-
ure out the correct polarity. Examples of such
cases include Try the rose roll (not on menu) and
The gnocchi literally melts in your mouth!. Fur-
thermore, in the laptop domain, a number of the
errors made by the rule-based system arise from
the ambiguous nature of some lexicon words. For
example, the sentence Only 2 usb ports ... seems
kind of ... limited is misclassified because the
word kind is considered to be positive.
There are a few cases where the rule-based sys-
tem outperforms the machine learning one. It hap-
pens when a sentence contains a rare word with
strong polarity, e. g. the word heavenly in The
11
We only classify one test instance as conflict.
226
chocolate raspberry cake is heavenly - not too
sweet, but full of flavor.
5 Related Work
The use of supervised machine learning with bag-
of-word or bag-of-n-gram feature sets has been
a standard approach to the problem of sentiment
polarity classification since the seminal work by
Pang et al. (2002) on movie review polarity pre-
diction. Heuristic methods which rely on a lexi-
con of sentiment words have also been widespread
and much of the research in this area has been
devoted to the unsupervised induction of good
quality sentiment indicators (see, for example,
Hatzivassiloglou and McKeown (1997) and Tur-
ney (2002), and Liu (2010) for an overview). The
integration of sentiment lexicon scores as fea-
tures in supervised machine learning to supple-
ment standard bag-of-n-gram features has also
been employed before (see, for example, Bak-
liwal et al. (2013)). The replacement of train-
ing/test words with scores/labels from sentiment
lexicons has also been used by Baccianella et
al. (2009), who supplement n-grams such as hor-
rible location with generalised expressions such
as NEGATIVE location. Linguistic features which
capture generalisations at the level of syntax (Mat-
sumoto et al., 2005), semantics (Johansson and
Moschitti, 2010) and discourse (Lazaridou et al.,
2013) have also been widely applied. In using bi-
narised features derived from the nodes of a deci-
sion tree, we are following our recent work which
uses the same technique in a different task: quality
estimation for machine translation (Rubino et al.,
2012; Rubino et al., 2013).
The main novelty in our system lies not in the
individual techniques but rather in they way they
are combined and integrated. For example, our
combination of token/chunk/dependency path dis-
tance used to weight the relationship between a
sentiment word and the aspect term has ? to the
best of our knowledge ? not been applied before.
6 Efficiency
Building a system for a shared task, we focus
solely on the accuracy of the system in all our deci-
sions. For example, we parse all training and test
data multiple times using different grammars to
increase sentence coverage from 99.87% to 100%.
To offer a more practical system, we work on
implementing a simplified, fully automated sys-
tem that is more efficient. So far, we replaced
time-consuming parsing with POS tagging. The
system accepts as input and generates as output
valid SemEval ABSA XML documents.
12
After
extracting the text and the aspect terms from the
input, the text is normalised using the process de-
scribed in Footnote 8. The feature extraction is
performed as described in Section 3 with the fol-
lowing modifications:
? The POS information used by the n-gram
feature extractor is obtained using the IMS
TreeTagger (Schmid, 1994) instead of using
the PCFG-LA parser (Attia et al., 2010).
? The distance used by the rule-based approach
is the token distance only, instead of a com-
bination of three distance functions.
The sentiment lexicon and the classification mod-
els used are described in Sections 2 and 3 respec-
tively.
The test sets containing 800 sentences are POS
tagged in less than half a second each. Surpris-
ingly, accuracy of aspect term polarity prediction
increases to 71.4% (from 70.5% for the submitted
system) on the laptop test set, using the same SVM
parameters as for the submitted system. However,
we see a degradation to 78.8% (from 81.0% for the
submitted system) for the restaurant test set. This
is an encouraging result as the SVM parameters
are not yet fully optimised for the slightly different
information and as the remaining modifications to
be implemented should not change accuracy any
further.
The next bottleneck that needs to be addressed
before the system can be used in applications re-
quiring quick responses is the current implementa-
tion of the n-gram feature extractor: It enumerates
all n-grams (for all context window sizes and n-
gram transformations) only to then intersect these
features with the list of selected features. For the
shared task, this made sense as we initially need
all features to make our selection of features, and
as we only need to run the feature extractor a few
times. For a practical system that has to process
new test sets frequently, however, it will be more
efficient to check for each selected feature whether
the respective event occurs in the input.
12
We validate documents using the XML schema defini-
tion provided on the shared task website.
227
7 Conclusion
We have described our aspect term polarity predic-
tion system, which employs supervised machine
learning using a combination of n-grams and sen-
timent lexicon features. Although our submitted
system performs very well, it is interesting to note
that our rule-based system is not that far behind.
This suggests that a state-of-the-art system can be
build without machine learning and that careful
design of the other system components is impor-
tant. However, the very good performance of our
machine-learning-based system also suggests that
word n-gram features do provide useful informa-
tion that is missed by a sentiment lexicon alone,
and that it is always worthwhile to perform careful
parameter tuning to eke out as much as possible
from such an approach.
Future work should investigate how much each
system component contributes to the overall per-
formance, e. g. lexicon combination, lemmatisa-
tion, spelling correction, other normalisations,
negation handling, distance function and n-gram
feature transformations. There is also room for
improvements in most of these components, e. g.
our handling of complex negations. Detection of
conflicts also needs more attention. Features in-
dicating the presence of trigger words for negation
and conflicts that are currently used only internally
in the rule-based component could be added to the
SVM feature set. It would also be interesting to
see how the compositional approach described by
Socher et al. (2013) handles these difficult cases.
The score features could be easily augmented by
breaking down scores by the four employed lexi-
cons. This way, the SVM can choose to combine
the information from these scores differently than
just summing them, allowing it to learn more com-
plex relations. Lexicon filtering and addition of
domain-specific entries could be automated to re-
duce the time needed to adjust to a new domain.
Finally, machine learning methods that can effi-
ciently handle large feature sets such as logistic
regression should be tried with the full feature set
(not applying frequency thresholds).
Acknowledgements
This research is supported by the Science Foun-
dation Ireland (Grant 12/CE/I2267) as part of
CNGL (www.cngl.ie) at Dublin City University.
The authors wish to acknowledge the DJEI/DES/
SFI/HEA Irish Centre for High-End Computing
(ICHEC) for the provision of computational facil-
ities and support. We are grateful to Qun Liu and
Josef van Genabith for their helpful comments.
References
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statis-
tical latent-variable parsing models for arabic, en-
glish and french. In Proceedings of the NAACL
HLT 2010 First Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 67?75.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2009. Multi-facet rating of product reviews.
In Proceedings of ECIR, pages 461?472.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh Conference
on International Language Resources and Evalua-
tion (LREC?10).
Akshat Bakliwal, Jennifer Foster, Jennifer van der Puil,
Ron O?Brien, Lamia Tounsi, and Mark Hughes.
2013. Sentiment analysis of political tweets: To-
wards an accurate classifier. In Proceedings of the
NAACL Workshop on Language Analysis in Social
Media, pages 49?58.
Bernhard E. Boser, Isabelle M. Guyon, and
Vladimir N. Vapnik. 1992. A training algo-
rithm for optimal margin classifiers. In Proceedings
of the Fifth Annual Workshop on Computational
Learning Theory, pages 144?152.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In COLING 2008 Workshop on Cross-
framework and Cross-domain Parser Evaluation.,
pages 1?8.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the 35th Annual Meeting
of the ACL and the 8th Conference of the European
Chapter of the ACL, pages 174?181.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177.
228
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 67?76.
Angeliki Lazaridou, Ivan Titov, and Caroline
Sporleder. 2013. A bayesian model for joint
unsupervised induction of sentiment, aspect and
discourse representations. In Proceedings of
the 51th Annual Meeting of the Association for
Computational Linguistics, pages 1630?1639.
Joseph Le Roux, Jennifer Foster, Joachim Wagner, Ra-
sul Samad Zadeh Kaljahi, and Anton Bryl. 2012.
DCU-Paris13 systems for the SANCL 2012 shared
task. Notes of the First Workshop on Syntactic
Analysis of Non-Canonical Language (SANCL).
Bing Liu. 2010. Sentiment analysis and subjectivity.
In Handbook of Natural Language Processing.
Shotaro Matsumoto, Hiroya Takamura, and Manubu
Okumura, 2005. Advances in Knowledge Discovery
and Data Mining, volume 3518 of Lecture Notes in
Computer Science, chapter Sentiment Classification
Using Word Sub-sequences and Dependency Sub-
trees, pages 301?311.
Peter Norvig. 2012. How to write a spelling corrector.
http://norvig.com/spell-correct.
html. [Online; accessed 2014-03-19].
Po Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86.
John C. Platt. 1998. Fast training of support vec-
tor machines using sequential minimal optimization.
In B. Schoelkopf, C. Burges, and A. Smola, edi-
tors, Advances in Kernel Methods - Support Vector
Learning, pages 185?208.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. Dcu-symantec submission for
the wmt 2012 quality estimation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 138?144.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, pages 44?49.
Richard Socher, Alex Perelygin, Jean Y. Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of EMNLP, pages 1631?
1642.
Milan Tofiloski, Julian Brooke, and Maite Taboada.
2009. A syntactic and lexical-based discourse seg-
menter. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, ACLShort ?09, pages 77?
80.
Peter Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classi-
cation of reviews. In Proceedings of the ACL, pages
417?424.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
HLT ?05, pages 347?354.
229
Parser-Based Retraining for Domain Adaptation of Probabilistic Generators
Deirdre Hogan, Jennifer Foster, Joachim Wagner and Josef van Genabith
National Centre for Language Technology
School of Computing
Dublin City University
Ireland
{dhogan, jfoster, jwagner, josef}@computing.dcu.ie
Abstract
While the effect of domain variation on Penn-
treebank-trained probabilistic parsers has been
investigated in previous work, we study its ef-
fect on a Penn-Treebank-trained probabilistic
generator. We show that applying the gener-
ator to data from the British National Corpus
results in a performance drop (from a BLEU
score of 0.66 on the standard WSJ test set to a
BLEU score of 0.54 on our BNC test set). We
develop a generator retraining method where
the domain-specific training data is automat-
ically produced using state-of-the-art parser
output. The retraining method recovers a sub-
stantial portion of the performance drop, re-
sulting in a generator which achieves a BLEU
score of 0.61 on our BNC test data.
1 Introduction
Grammars extracted from the Wall Street Journal
(WSJ) section of the Penn Treebank have been suc-
cessfully applied to natural language parsing, and
more recently, to natural language generation. It is
clear that high-quality grammars can be extracted
for the WSJ domain but it is not so clear how
these grammars scale to other text genres. Gildea
(2001), for example, has shown that WSJ-trained
parsers suffer a drop in performance when applied
to the more varied sentences of the Brown Cor-
pus. We investigate the effect of domain variation in
treebank-grammar-based generation by applying a
WSJ-trained generator to sentences from the British
National Corpus (BNC).
As with probabilistic parsing, probabilistic gener-
ation aims to produce the most likely output(s) given
the input. We can distinguish three types of prob-
abilistic generators, based on the type of probabil-
ity model used to select the most likely sentence.
The first type uses an n-gram language model, e.g.
(Langkilde, 2000), the second type uses a proba-
bility model defined over trees or feature-structure-
annotated trees, e.g. (Cahill and van Genabith,
2006), and the third type is a mixture of the first
and second type, employing n-gram and grammar-
based features, e.g. (Velldal and Oepen, 2005). The
generator used in our experiments is an instance of
the second type, using a probability model defined
over Lexical Functional Grammar c-structure and
f-structure annotations (Cahill and van Genabith,
2006; Hogan et al, 2007).
In an initial evaluation, we apply our probabilistic
WSJ-trained generator to BNC material, and show
that the generator suffers a substantial performance
degradation, with a drop in BLEU score from 0.66
to 0.54. We then turn our attention to the problem
of adapting the generator so that it can more accu-
rately generate the 1,000 sentences in our BNC test
set. The problem of adapting any NLP system to a
domain different from the domain upon which it has
been trained and for which no gold standard train-
ing material is available is a very real one, and one
which has been the focus of much recent research in
parsing. Some success has been achieved by training
a parser, not on gold standard hand-corrected trees,
but on parser output trees. These parser output trees
can by produced by a second parser in a co-training
scenario (Steedman et al, 2003), or by the same
parser with a reranking component in a type of self-
training scenario (McClosky et al, 2006). We tackle
165
the problem of domain adaptation in generation in
a similar way, by training the generator on domain
specific parser output trees instead of manually cor-
rected gold standard trees. This experiment achieves
promising results, with an increase in BLEU score
from 0.54 to 0.61. The method is generic and can be
applied to other probabilistic generators (for which
suitable training material can be automatically pro-
duced).
2 Background
The natural language generator used in our experi-
ments is the WSJ-trained system described in Cahill
and van Genabith (2006) and Hogan et al (2007).
Sentences are generated from Lexical Functional
Grammar (LFG) f-structures (Kaplan and Bresnan,
1982). The f-structures are created automatically
by annotating nodes in the gold standard WSJ trees
with LFG functional equations and then passing
these equations through a constraint solver (Cahill
et al, 2004). The generation algorithm is a chart-
based one which works by finding the most proba-
ble tree associated with the input f-structure. The
yield of the most probable tree is the output sen-
tence. An annotated PCFG, in which the non-
terminal symbols are decorated with functional in-
formation, is used to generate the most probable tree
from an f-structure. Cahill and van Genabith (2006)
attain 98.2% coverage and a BLEU score of 0.6652
on the standard WSJ test set (Section 23). Hogan
et al (2007) describe an extension to the system
which replaces the annotated PCFG selection model
with a more sophisticated history-based probabilis-
tic model. Instead of conditioning the righthand side
of a rule on the lefthand non-terminal and its asso-
ciated functional information alone, the new model
includes non-local conditioning information in the
form of functional information associated with an-
cestor nodes of the lefthand side category. This sys-
tem achieves a BLEU score of 0.6724 and 99.9%
coverage.
Other WSJ-trained generation systems include
Nakanishi et al (2005) and White et al (2007).
Nakanishi et al (2005) describe a generator trained
on a HPSG grammar derived from the WSJ Section
of the Penn Treebank. On sentences of ? 20 words
in length, their system attains coverage of 90.75%
and a BLEU score of 0.7733. White et al (2007)
describe a CCG-based realisation system which has
been trained on logical forms derived from CCG-
Bank (Hockenmaier and Steedman, 2005), achiev-
ing 94.3% coverage and a BLEU score of 0.5768 on
WSJ23 for all sentence lengths. The input structures
upon which these systems are trained vary in form
and specificity, but what the systems have in com-
mon is that their various input structures are derived
from Penn Treebank trees.
3 The BNC Test Data
The new English test set consists of 1,000 sentences
taken from the British National Corpus (Burnard,
2000). The BNC is a one hundred million word bal-
anced corpus of British English from the late twenti-
eth century. Ninety per cent of it is written text, and
the remaining 10% consists of transcribed sponta-
neous and scripted spoken language. The BNC sen-
tences in the test set are not chosen completely at
random. Each sentence in the test set has the prop-
erty of containing a word which appears as a verb
in the BNC but not in the usual training sections of
the Wall Street Journal section of the Penn Treebank
(WSJ02-21). Sentences were chosen in this way so
that the resulting test set would be a difficult one
for WSJ-trained systems. In order to produce in-
put f-structures for the generator, the test sentences
were manually parsed by one annotator, using as
references the Penn Treebank trees themselves and
the Penn Treebank bracketing guidelines (Bies et
al., 1995). When the two references did not agree,
the guidelines took precedence over the Penn Tree-
bank trees. Difficult parsing decisions were docu-
mented. Due to time constraints, the annotator did
not mark functional tags or traces. The context-free
gold standard parse trees were transformed into f-
structures using the automatic procedure of Cahill et
al. (2004).
4 Experiments
Experimental Setup In our first experiment, we
apply the original WSJ-trained generator to our
BNC test set. The gold standard trees for our BNC
test set differ from the gold standard Wall Street
Journal trees, in that they do not contain Penn-II
traces or functional tags. The process which pro-
166
duces f-structures from trees makes use of trace and
functional tag information, if available. Thus, to en-
sure that the training and test input f-structures are
created in the same way, we use a version of the
generator which is trained using gold standard WSJ
trees without functional tag or trace information.
When we test this system on the WSJ23 f-structures
(produced in the same way as the WSJ training ma-
terial), the BLEU score decreases slightly from 0.67
to 0.66. This is our baseline system.
In a further experiment, we attempt to adapt
the generator to BNC data by using BNC trees as
training material. Because we lack gold standard
BNC trees (apart from those in our test set), we
try instead to use parse trees produced by an accu-
rate parser. We choose the Charniak and Johnson
reranking parser because it is freely available and
achieves state-of-the-art accuracy (a Parseval f-score
of 91.3%) on the WSJ domain (Charniak and John-
son, 2005). It is, however, affected by domain vari-
ation ? Foster et al (2007) report that its f-score
drops by approximately 8 percentage points when
applied to the BNC domain. Our training size is
500,000 sentences. We conduct two experiments:
the first, in which 500,000 sentences are extracted
randomly from the BNC (minus the test set sen-
tences), and the second in which only shorter sen-
tences, of length ? 20 words, are chosen as training
material. The rationale behind the second experi-
ment is that shorter sentences are less likely to con-
tain parser errors.
We use the BLEU evaluation metric for our ex-
periments. We measure both coverage and full cov-
erage. Coverage measures the number of cases for
which the generator produced some kind of out-
put. Full coverage measures the number of cases for
which the generator produced a tree spanning all of
the words in the input.
Results The results of our experiments are shown
in Fig. 1. The first row shows the results we ob-
tain when the baseline system is applied to the f-
structures derived from the 1,000 BNC gold stan-
dard parse trees. The second row shows the results
on the same test set for a system trained on Charniak
and Johnson parser output trees for 500,000 BNC
sentences. The results in the final row are obtained
by training the generator on Charniak and Johnson
parser output trees for 500,000 BNC sentences of
length ? 20 words in length.
Discussion As expected, the performance of the
baseline system degrades when faced with out-of-
domain test data. The BLEU score drops from a
0.66 score for WSJ test data to a 0.54 score for
the BNC test data, and full coverage drops from
85.97% to 68.77%. There is a substantial improve-
ment, however, when the generator is trained on
BNC data. The BLEU score jumps from 0.5358
to 0.6135. There are at least two possible reasons
why a BLEU score of 0.66 is not obtained: The first
is that the quality of the f-structure-annotated trees
upon which the generator has been trained has de-
graded. For the baseline system, the generator is
trained on f-structure-annotated trees derived from
gold trees. The new system is trained on f-structure-
annotated parser output trees, and the performance
of Charniak and Johnson?s parser degrades when ap-
plied to BNC data (Foster et al, 2007). The second
reason has been suggested by Gildea (2001): WSJ
data is easier to learn than the more varied data in the
Brown Corpus or BNC. Perhaps even if gold stan-
dard BNC parse trees were available for training, the
system would not behave as well as it does for WSJ
material.
It is interesting to note that training on 500,000
shorter sentences does not appear to help. We hy-
pothesized that it would improve results because
shorter sentences are less likely to contain parser
errors. The drop in full coverage from 86.69% to
79.58% suggests that the number of short sentences
needs to be increased so that the size of the training
material stays constant.
5 Conclusion
We have investigated the effect of domain varia-
tion on a LFG-based WSJ-trained generation sys-
tem by testing the system?s performance on 1,000
sentences from the British National Corpus. Perfor-
mance drops from a BLEU score of 0.66 onWSJ test
data to 0.54 on the BNC test set. Encouragingly, we
have also shown that domain-specific training mate-
rial produced by a parser can be used to claw back
a significant portion of this performance degrada-
tion. Our method is general and could be applied
to other WSJ-trained generators (e.g. (Nakanishi et
167
Train BLEU Coverage Full Coverage
WSJ02-21 0.5358 99.1 68.77
BNC(500k) 0.6135 99.1 86.69
BNC(500k) ? 20 words 0.5834 99.1 79.58
Figure 1: Results for 1,000 BNC Sentences
al., 2005; White et al, 2007)). We intend to con-
tinue this research by training our generator on parse
trees produced by a BNC-self-trained version of the
Charniak and Johnson reranking parser (Foster et al,
2007). We also hope to extend the evaluation beyond
the BLEU metric by carrying out a human judge-
ment evaluation.
Acknowledgments
This research has been supported by the Enterprise
Ireland Commercialisation Fund (CFTD/2007/229),
Science Foundation Ireland (04/IN/I527) and the
IRCSET Embark Initative (P/04/232). We thank the
Irish Centre for High End Computing for providing
computing facilities.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank
II style, Penn Treebank project. Technical Report
Tech Report MS-CIS-95-06, University of Pennsylva-
nia, Philadelphia, PA.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
lfg approximations. In Proceedings of the 21st COL-
ING and the 44th Annual Meeting of the ACL, pages
1033?1040, Sydney.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Meeting of the ACL, pages
320?327, Barcelona.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor.
Jennifer Foster, Joachim Wagner, Djame? Seddah, and
Josef van Genabith. 2007. Adapting WSJ-trained
parsers to the British National Corpus using in-domain
self-training. In Proceedings of the Tenth IWPT, pages
33?35, Prague.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP, Pittsburgh.
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank:
Users? manual. Technical report, Computer and Infor-
mation Science, University of Pennsylvania.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units in
history-based probabilistic generation. In Proceedings
of the joint EMNLP/CoNLL, pages 267?276, Prague.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar: a Formal System for Grammatical Repre-
sentation. In Joan Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
MIT Press.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of NAACL, Seattle.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proceedings of the
Ninth IWPT, pages 93?102, Vancouver.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-
strapping statistical parsers from small datasets. In
Proceedings of EACL, pages 331?338, Budapest.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proceedings
of the MT-Summit, Phuket.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proceedings of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT), pages 267?276,
Copenhagen.
168
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 138?144,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DCU-Symantec Submission for the WMT 2012 Quality Estimation Task
Raphael Rubino??, Jennifer Foster?, Joachim Wagner?,
Johann Roturier?, Rasul Samad Zadeh Kaljahi??, Fred Hollowood?
?Dublin City University, ?Symantec, Ireland
?firstname.lastname@computing.dcu.ie
?firstname lastname@symantec.com
Abstract
This paper describes the features and the ma-
chine learning methods used by Dublin City
University (DCU) and SYMANTEC for the
WMT 2012 quality estimation task. Two sets
of features are proposed: one constrained, i.e.
respecting the data limitation suggested by the
workshop organisers, and one unconstrained,
i.e. using data or tools trained on data that was
not provided by the workshop organisers. In
total, more than 300 features were extracted
and used to train classifiers in order to predict
the translation quality of unseen data. In this
paper, we focus on a subset of our feature set
that we consider to be relatively novel: fea-
tures based on a topic model built using the
Latent Dirichlet Allocation approach, and fea-
tures based on source and target language syn-
tax extracted using part-of-speech (POS) tag-
gers and parsers. We evaluate nine feature
combinations using four classification-based
and four regression-based machine learning
techniques.
1 Introduction
For the first time, the WMT organisers this year pro-
pose a Quality Estimation (QE) shared task, which
is divided into two sub-tasks: scoring and ranking
automatic translations. The aim of this workshop is
to define useful sets of features and machine learn-
ing techniques in order to predict the quality of a
machine translation (MT) output T (Spanish) given
a source segment S (English). Quality is measured
using a 5-point likert scale which is based on post-
editing effort, following the scoring scheme:
1. The MT output is incomprehensible
2. About 50-70% of the MT output needs to be
edited
3. About 25-50% of the MT output needs to be
edited
4. About 10-25% of the MT output needs to be
edited
5. The MT output is perfectly clear and intelligi-
ble
The final score is a combination of the scores as-
signed by three evaluators. The use of a 5-point scale
makes the scoring task more difficult than a binary
classification task where a translation is considered
to be either good or bad. However, if the task is
successfully carried out, the score produced is more
useful.
Dublin City University and Symantec jointly ad-
dress the scoring task. For each pair (S, T ) of source
segment S and machine translation T , we train three
classifiers and one classifier combination using the
training data provided by the organisers to predict
5-point Likert scores. In this paper, we present the
classification results on the test set alng with addi-
tional results obtained using regression techniques.
We evaluate the usefulness of two new sets of fea-
tures:
1. topic-based features using Latent Dirichlet Al-
location (LDA (Blei et al, 2003)),
2. syntax-based features using POS taggers and
parsers (Wagner et al, 2009)
The remainder of this paper is organised as fol-
lows. In Section 2, we give an overview of all the
138
features employed in our QE system. Then, in Sec-
tion 3, we describe the topic and syntax-based fea-
tures in more detail. Section 4 presents the vari-
ous classification and regression techniques we ex-
plored. Our results are presented and discussed in
Section 5. Finally, we summarise and outline our
plans in Section 6.
2 Features Overview
In this section, we describe the features used in our
QE system. In the first subsection, the features in-
cluded in our constrained system are presented. In
the second subsection, we detail the features in-
cluded in our unconstrained system. Both of these
systems include the 17 baseline features provided
for the shared task.
2.1 Constrained System
The constrained system is based only on the data
provided by the organisers. We extracted 70 fea-
tures in total (including the baseline features) and
we present them here according to the type of infor-
mation they capture.
Word and Phrase-Level Features
? Ratio of source and target segment length:
the number of source words divided by the
number of target words
? Ratio of source and target number of punc-
tuation marks: the number of source punctua-
tion marks divided by the number of target ones
? Number of phrases comprising the MT out-
put: given a phrase-table, we assume that a
sentence composed of several phrases indicates
uncertainty on the part of the MT system.
? Average length of source and target phrases:
concatenating short phrases may result in lower
fluency compared to the use of longer ones.
? Ratio of source and target averaged phrase
length
? Number of source prepositions and conjunc-
tions word: our assumption here is that seg-
ments containing a relatively high number of
prepositions and conjunctions may be more
complex and difficult to translate.
? Number of source out-of-vocabulary words
Language Model Features
All the language models (LMs) used in our work
are n-gram LMs with Kneser-Ney smoothing built
with the SRI Toolkit (Stolcke, 2002).
? Backward 2-gram and 3-gram source and
target log probabilities: as proposed by
Duchateau et al (2002)
? Log probability of target segments on
5-gram MT-output-based LM: using
MOSES (Koehn et al, 2007) trained on the
provided parallel corpus, we translated the En-
glish side of this corpus into Spanish, assuming
that the MT output contains mistakes. This
MT output is used to build a LM that models
the behavior of the MT system. We assume
that for a given MT output, a high n-gram
probability (or a low perplexity) of the LM
indicates that the MT output contains mistakes.
MT-system Features
? 15 scores provided by Moses: phrase-table,
language model, reordering model and word
penalty (weighted and unweighted)
? Number of n-bests for each source segment
? MT output back-translation: from Spanish to
English using MOSES trained on the provided
parallel corpus, scored with TER (Snover et
al., 2006), BLEU (Papineni et al, 2002) and
the Levenshtein distance (Levenshtein, 1966),
based on the source segments as a translation
reference
Topic Model Features
? Probability distribution over topics: Source
and target segment probability distribution over
topics for a 10-dimension topic model
? Cosine distance between source and target
topic vectors
More details about these two features are provided
in Section 3.1.
2.2 Unconstrained System
In addition to the features used for the constrained
system, a further 238 unconstrained features were
included in our unconstrained system.
139
MT System Features
As for our constrained system, we use MT output
back-translation from Spanish to English, but this
time using Bing Translator1 in addition to Moses.
Each back-translated segment is scored with TER,
BLEU and the Levenshtein distance, based on the
source segments as a translation reference.
Source Syntax Features
Wagner et al (2007; 2009) propose a series of
features to measure sentence grammaticality. These
features rely on a part-of-speech tagger, a probabilis-
tic parser and a precision grammar/parser. We have
at our disposal these tools for English and so we ap-
ply them to the source data. The features themselves
are described in more detail in Section 3.2.
Target Syntax Features
We use a part-of-speech tagger trained on Spanish
to extract from the target data the subset of grammat-
icality features proposed by Wagner et al (2007;
2009) that are based on POS n-grams. In addition
we extract features which reflect the prevalence of
particular POS tags in each target segment. These
are explained in more detail in Section 3.2 below.
Grammar Checker Features
LANGUAGETOOL (based on (Naber, 2003)) is an
open-source grammar and style proofreading tool
that finds errors based on pre-defined, language-
specific rules. The latest version of the tool can
be run in server mode, so individual sentences can
be checked and assigned a total number of errors
(which may or may not be true positives).2 This
number is used as a feature for each source segment
and its corresponding MT output.
3 Topic and Syntax-based Features
In this section, we focus on the set of features
that aim to capture adequacy using topic modelling
and grammaticality using POS tagging and syntactic
parsing.
1http://www.microsofttranslator.com/
2The list of English and Spanish rules is available at:
http://languagetool.org/languages.
3.1 Topic-based Features
We extract source and target features based on a
topic model built using LDA. The main idea in topic
modelling is to produce a set of thematic word clus-
ters from a collection of documents. Using the par-
allel corpus provided for the task, a bilingual corpus
is built where each line is composed of a source seg-
ment and its translation separated by a space. Each
pair of segments is considered as a bilingual docu-
ment. This corpus is used to train a bilingual topic
model after stopwords removal. The resulting model
is one set of bilingual topics z containing words w
with a probability p(wn|zn, ?) (with n equal to the
vocabulary size in the whole parallel corpus). This
model can be used to infer the probability distri-
bution of unseen source and target segments over
bilingual topics. During the test step, each source
segment and its translation are considered individu-
ally, as two monolingual documents. This method
allows us to compare the source and target topic dis-
tributions. We assume that a source segment and its
translation share topic similarities.
We propose two ways of using topic-based fea-
tures for quality estimation: keeping source and tar-
get topic vectors as two sets of k features, or com-
puting a vector distance between these two vectors
and using one feature only. To measure the prox-
imity of two vectors, we decided to used the Co-
sine distance, as it leads to the best results in terms
of classification accuracy. However, we plan to
study different metrics in further experiments, like
the Manhattan or the Euclidean distances. Some
parameters related to LDA have to be studied more
carefully too, such as the number of topics (dimen-
sions in the topic space), the number of words per
topic, the Dirichlet hyperparameter ?, etc. In our
experiments, we built a topic model composed of 10
dimensions using Gibbs sampling with 1000 itera-
tions. We assume that a higher dimensionality can
lead to a better repartitioning of the vocabulary over
the topics.
Multilingual LDA has been used before in nat-
ural language processing, e.g. polylingual topic
models (Mimno et al, 2009) or multilingual topic
models for unaligned text (Boyd-Graber and Blei,
2009). In the field of machine translation, Tam et
al. (2007) propose to adapt a translation and a lan-
140
guage model to a specific topic using Latent Se-
mantic Analysis (LSA, or Latent Semantic Index-
ing, LSI (Deerwester et al, 1990)). More recently,
some studies were conducted on the use of LDA to
adapt SMT systems to specific domains (Gong et al,
2010; Gong et al, 2011) or to extract bilingual lexi-
con from comparable corpora (Rubino and Linare`s,
2011). Extracting features from a topic model is, to
the best of our knowledge, the first attempt in ma-
chine translation quality estimation.
3.2 Syntax-based Features
Syntactic features have previously been used in MT
for confidence estimation and for building automatic
evaluation measures. Corston-Oliver et al (2001)
build a classifier using 46 parse tree features to pre-
dict whether a sentence is a human translation or MT
output. Quirk (2004) uses a single parse tree feature
in the quality estimation task with a 4-point scale,
namely whether a spanning parse can be found, in
addition to LM perplexity and sentence length. Liu
and Gildea (2005) measure the syntactic similarity
between MT output and reference translation. Al-
brecht and Hwa (2007) measure the syntactic simi-
larity between MT output and reference translation
and between MT output and a large monolingual
corpus. Gimenez and Marquez (2007) explore lexi-
cal, syntactic and shallow semantic features and fo-
cus on measuring the similarity of MT output to ref-
erence translation. Owczarzak et al (2007) use la-
belled dependencies together with WordNet to avoid
penalising valid syntactic and lexical variations in
MT evaluation. In what follows, we describe how
we make use of syntactic information in the QE task,
i.e. evaluating MT output without a reference trans-
lation.
Wagner et al (2007; 2009) use three sources
of linguistic information in order to extract features
which they use to judge the grammaticality of En-
glish sentences:
1. For each POS n-gram (with n ranging from 2 to
7), a feature is extracted which represents the
frequency of the least frequent n-gram in the
sentence according to some reference corpus.
TreeTagger (Schmidt, 1994) is used to produce
POS tags.
2. Features provided by a hand-crafted, broad-
coverage precision grammar of English (Butt
et al, 2002) and a Lexical Functional Grammar
parser (Maxwell and Kaplan, 1996). These in-
clude whether or not a sentence could be parsed
without resorting to robustness measures, the
number of analyses found and the parsing time.
3. Features extracted from the output of three
probabilistic parsers of English (Charniak and
Johnson, 2005), one trained on Wall Street
Journal trees (Marcus et al, 1993), one trained
on a distorted version of the treebank obtained
by automatically creating grammatical error
and adjusting the parse trees, and the third
trained on the union of the original and dis-
torted versions.
These features were originally designed to distin-
guish grammatical sentences from ungrammatical
ones and were tested on sentences from learner cor-
pora by Wagner et al (2009) and Wagner (2012).
In this work we extract all three sets of features
from the source side of our data and the POS-based
subset from the target side.3 We use the publicly
available pre-trained TreeTagger models for English
and Spanish4. The reference corpus used to obtain
POS n-gram frequences is the MT translation model
training data.5
In addition to the POS-based features described in
Wagner et al (2007; 2009), we also extract the fol-
lowing features from the Spanish POS-tagged data:
for each POS tag P and target segment T , we ex-
tract a feature which is the proportion of words in
T that are tagged as P . Two additional features are
extracted to represent the proportion of words in T
that are assigned more than one tag by the tagger,
3Unfortunately, due to time constraints, we were unable to
source a suitable probabilistic phrase-structure parser and a pre-
cision grammar for Spanish and were thus unable to extract
parser-based features for Spanish. We expect that these features
would be more useful on the target side than the source side.
4http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/
5To aid machine learning methods that linearly combine fea-
ture values, we add binarised features derived from the raw XLE
and POS n-gram features described above, for example we add
a feature indicating whether the frequency of the least frequent
POS 5-gram is below 10. We base the choice of binary fea-
tures on (a) decision rules observed in decision trees trained for
a binary scoring task and (b) decision rules of simple classifiers
(decision trees with just one decision node and 2 leaf nodes)
that form a convex hull of optimal classifiers in ROC space.
141
and the proportion of words in T that are unknown
to the tagger.
4 Machine Learning
In this section, we describe the machine learning
methods that we experimented with. Our final sys-
tems submitted for the shared task are based on clas-
sification methods. However, we also performed
some experiments with regression methods.
We evaluate the systems on the test set using the
official evaluation script and the reference scores.
We report the evaluation results as Mean Aver-
age Error (MAE) and Root Mean Squared Error
(RMSE).
4.1 Classification
In order to apply classification algorithms to the
set of features associated with each source and tar-
get segment, we rounded the training data scores
to the closest integer. We tested several classifiers
and empirically chose three algorithms: Support
Vector Machine using sequential minimal optimiza-
tion and RBF kernel (parameters optimized by grid-
search) (Platt, 1999), Naive Bayes (John and Lang-
ley, 1995) and Random Forest (Breiman, 2001) (the
latter two techniques were applied with default pa-
rameters). We use the Weka toolkit (Hall et al,
2009) to train the classifiers and predict the scores
on the test set. Each method is evaluated individu-
ally and then combined by averaging the predicted
scores.
4.2 Regression
We applied three different regression techniques:
SVM epsilon-SVR with RBF kernel, Linear Regres-
sion and M5P (Quinlan, 1992; Wang and Witten,
1997). The two latter algorithms were used with
default parameters, whereas SVM parameters (?, c
and ) were optimized by grid-search. We also per-
formed a combination of the three algorithms by av-
eraging the predicted scores. We apply a linear func-
tion on the predicted scores S in order to keep them
in the correct range (from 1 to 5) as detailed in (1),
where S? is the rescaled sentence score, Smin is the
lowest predicted score and Smax is the highest pre-
dicted score.
S? = 1 + 4?
S ? Smin
Smax ? Smin
(1)
5 Evaluation
Table 1 shows the results obtained by our classifi-
cation approach on various feature subsets. Note
that the two submitted systems used the combined
classifier approach with the constrained and uncon-
strained feature sets. Table 2 shows the results for
the same feature combinations, this time using re-
gression rather than classification.
The results of quality estimation using classifica-
tion methods show that the baseline and the syntax-
based features with the classifier combination leads
to the best results with an MAE of 0.71 and an
RMSE of 0.87. However, these scores are substan-
tially lower than the ones obtained using regression,
where the unconstrained set of features with SVM
leads to an MAE of 0.62 and an RMSE of 0.78.
It seems that the classification methods are not
suitable for this task according to the different sets
of features studied. Furthermore, the topic-distance
feature is not correlated with the quality scores, ac-
cording to the regression results. On the other hand,
the syntax-based features appear to be the most in-
formative and lead to an MAE of 0.70.
6 Conclusion
We presented in this paper our submission for the
WMT12 Quality Estimation shared task. We also
presented further experiments using different ma-
chine learning techniques and we evaluated the im-
pact of two sets of features - one set which is based
on linguistic features extracted using POS tagging
and parsing, and a second set which is based on topic
modelling. The best results are obtained by our un-
constrained system containing all features and us-
ing an -SVR regression method with a Radial Basis
Function kernel. This setup leads to a Mean Aver-
age Error of 0.62 and a Root Mean Squared Error
of 0.78. Unfortunately, we did not submit our best
configuration for the shared task.
We plan to continue working on the task of ma-
chine translation quality estimation. Our immediate
next steps are to continue to investigate the contribu-
tion of individual features, to explore feature selec-
tion in a more detailed fashion and to apply our best
system to other types of data including sentences
taken from an online discussion forum.
142
SMO NAIVE BAYES RANDOM FOREST Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.74 0.89 0.85 1.10 0.84 1.06 0.71 0.88
topic distribution 0.84 1.02 1.09 1.38 0.91 1.15 0.78 0.98
topic distance 0.88 1.11 0.93 1.17 1.04 1.23 0.84 1.04
syntax 0.78 0.97 1.01 1.27 0.83 1.05 0.72 0.90
baseline + topic 0.82 1.01 1.00 1.31 0.84 1.05 0.75 0.95
baseline + syntax 0.76 0.94 1.01 1.25 0.79 0.98 0.71 0.87
baseline + topic + syntax 0.82 1.04 1.03 1.29 0.79 0.98 0.74 0.93
all constrained 0.99 1.26 1.12 1.46 0.71 0.88 0.86 ? 1.12 ?
all unconstrained 0.97 1.25 0.80 1.02 0.79 0.99 0.75 ? 0.97 ?
Table 1: MAE and RMSE results for different sets of features using three classification methods. The results with ?
and ? correspond to the DCU-SYMC constrained and the DCU-SYMC unconstrained systems respectively, submitted
for the shared task.
SVM LINEAR REG. M5P Combination
Features MAE RMSE MAE RMSE MAE RMSE MAE RMSE
baseline 0.78 0.93 0.80 0.99 0.73 0.91 0.72 0.88
topic distribution 0.78 0.95 0.79 0.96 0.80 0.96 0.79 0.95
topic distance 1.38 1.67 1.31 1.62 1.85 2.09 1.00 1.24
syntax 0.70 0.88 0.97 1.22 1.41 1.65 0.76 0.92
baseline + topic 0.78 0.96 1.06 1.31 1.16 1.42 0.88 1.10
baseline + syntax 0.67 0.82 0.90 1.12 2.17 2.38 0.98 1.22
baseline + topic + syntax 0.68 0.84 0.93 1.16 2.12 2.33 0.97 1.21
all constrained 0.83 1.02 0.94 1.18 0.78 0.99 0.71 0.88
all unconstrained 0.62 0.78 1.33 1.60 0.71 0.89 0.73 0.91
Table 2: MAE and RMSE results for different sets of features using three regression methods.
References
J. Albrecht and R. Hwa. 2007. A re-examination of ma-
chine learning approaches for sentence-level MT eval-
uation. In Proceedings of the 45th Annual Meeting of
the ACL, pages 880?887.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
Dirichlet Allocation. The Journal of Machine Learn-
ing Research, 3:993?1022.
Jordan Boyd-Graber and David M. Blei. 2009. Multilin-
gual topic models for unaligned text. In Proceedings
of the 25th Conference on Uncertainty in Artificial In-
telligence, pages 75?82.
L. Breiman. 2001. Random forests. Machine learning,
45(1):5?32.
M. Butt, H. Dyvik, T. Holloway King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project. In
Proceedings of the Coling Workshop on Grammar En-
gineering and Evaluation.
E. Charniak and M. Johnson. 2005. Course-to-fine n-
best-parsing and maxent discriminative reranking. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 173?180, Ann Arbor.
S. Corston-Oliver, M. Gamon, and C. Brockett. 2001.
A machine learning approach to the automatic evalu-
ation of machine translation. In Proceedings of 39th
Annual Meeting of the Association for Computational
Linguistics, pages 148?155, Toulouse, France, July.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science, 41(6):391?407.
J. Duchateau, K. Demuynck, and P. Wambacq. 2002.
Confidence scoring based on backward language mod-
els. In Proceedings IEEE international confer-
ence on acoustics, speech, and signal processing,
ICASSP?2002, volume 1, pages 221?224.
J. Gime?nez and L. Ma`rquez. 2007. Linguistic features
for automatic evaluation of heterogenous MT systems.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 256?264, Prague, Czech
Republic, June.
143
Z. Gong, Y. Zhang, and G. Zhou. 2010. Statistical ma-
chine translation based on lda. In Universal Commu-
nication Symposium (IUCS), 2010 4th International,
pages 286?290.
Z. Gong, G. Zhou, and L. Li. 2011. Improve smt with
source-side ?topic-document? distributions. In MT
Summit, pages 496?501.
M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-
mann, and I.H. Witten. 2009. The weka data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
G.H. John and P. Langley. 1995. Estimating continuous
distributions in bayesian classifiers. In Eleventh con-
ference on uncertainty in artificial intelligence, pages
338?345. Morgan Kaufmann Publishers Inc.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
V.I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions, and reversals. In Soviet
Physics Doklady, volume 10-8, pages 707?710.
D. Liu and D. Gildea. 2005. Syntactic features for eval-
uation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 25?32, Ann Arbor, Michigan.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of en-
glish: the penn treebank. Computational Linguistics,
19(2):313?330.
John Maxwell and Ron Kaplan. 1996. An Efficient
Parser for LFG. In Proceedings of LFG-96, Grenoble.
D. Mimno, H.M. Wallach, J. Naradowsky, D.A. Smith,
and A. McCallum. 2009. Polylingual topic models.
In Proceedings of EMNLP: Volume 2-Volume 2, pages
880?889. Association for Computational Linguistics.
D. Naber. 2003. A rule-based style and grammar
checker. Technical report, Bielefeld University Biele-
feld, Germany.
K. Owczarzak, J. van Genabith, and A. Way. 2007. La-
belled dependencies in machine translation evaluation.
In Proceedings of the Second Workshop on Statistical
Machine Translation, pages 104?111, Prague, Czech
Republic, June.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, pages 311?318.
J.C. Platt. 1999. Fast training of support vector machines
using sequential minimal optimization. In Advances in
kernel methods, pages 185?208. MIT Press.
R. J. Quinlan. 1992. Learning with continuous classes.
In 5th Australian Joint Conference on Artificial Intel-
ligence, pages 343?348, Singapore. World Scientific.
C. Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In Proceedings of LREC,
Lisbon, June.
R. Rubino and G. Linare`s. 2011. A multi-view approach
for term translation spotting. Computational Linguis-
tics and Intelligent Text Processing, 6609:29?40.
H. Schmidt. 1994. Probabilistic part-of-speech tagging
using decision trees. In Proceedings of the Interna-
tional Conference on New Methods in Natural Lan-
guage Processing.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proceedings of Associ-
ation for Machine Translation in the Americas, pages
223?231.
A. Stolcke. 2002. SRILM-an extensible language mod-
eling toolkit. In InterSpeech, volume 2, pages 901?
904.
Y.C. Tam, I. Lane, and T. Schultz. 2007. Bilingual
lsa-based adaptation for statistical machine translation.
Machine Translation, 21(4):187?207.
J. Wagner, J. Foster, and J. van Genabith. 2007. A com-
parative evaluation of deep and shallow approaches to
the automatic detection of common grammatical er-
rors. In Proceedings of EMNLP-CoNLL, pages 112?
121, Prague, Czech Republic, June.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal, 26(3):474?490.
J. Wagner. 2012. Detecting grammatical errors with
treebank-induced probabilistic parsers. Ph.D. thesis,
Dublin City University.
Y. Wang and I. H. Witten. 1997. Induction of model trees
for predicting continuous classes. In Poster papers of
the 9th European Conference on Machine Learning.
Springer.
144
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 392?397,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
DCU-Symantec at the WMT 2013 Quality Estimation Shared Task
Raphael Rubino??, Joachim Wagner??, Jennifer Foster?,
Johann Roturier? Rasoul Samad Zadeh Kaljahi?? and Fred Hollowood?
?NCLT, School of Computing, Dublin City University, Ireland
?Center for Next Generation Localisation, Dublin, Ireland
?Symantec Research Labs, Dublin, Ireland
?{rrubino, jwagner, jfoster}@computing.dcu.ie
?{johann roturier, fhollowood}@symantec.com
Abstract
We describe the two systems submit-
ted by the DCU-Symantec team to Task
1.1. of the WMT 2013 Shared Task on
Quality Estimation for Machine Transla-
tion. Task 1.1 involve estimating post-
editing effort for English-Spanish trans-
lation pairs in the news domain. The
two systems use a wide variety of fea-
tures, of which the most effective are the
word-alignment, n-gram frequency, lan-
guage model, POS-tag-based and pseudo-
references ones. Both systems perform at
a similarly high level in the two tasks of
scoring and ranking translations, although
there is some evidence that the systems are
over-fitting to the training data.
1 Introduction
The WMT 2013 Quality Estimation Shared Task
involve both sentence-level and word-level qual-
ity estimation (QE). The sentence-level task con-
sist of three subtasks: scoring and ranking transla-
tions with regard to post-editing effort (Task 1.1),
selecting among several translations produced by
multiple MT systems for the same source sentence
(Task 1.2), and predicting post-editing time (Task
1.3). The DCU-Symantec team enter two systems
to Task 1.1. Given a set of source English news
sentences and their Spanish translations, the goals
are to predict the HTER score of each translation
and to produce a ranking based on HTER for the
set of translations. A set of 2,254 sentence pairs
are provided for training.
On the ranking task, our system DCU-SYMC
alltypes is second placed out of thirteen sys-
tems and our system DCU-SYMC combine is
ranked fifth, according to the Delta Average met-
ric. According to the Spearman rank correlation,
our systems are the joint-highest systems. In the
scoring task, the DCU-SYMC alltypes system
is placed sixth out of seventeen systems accord-
ing to Mean Absolute Error (MAE) and third ac-
cording to Root Mean Squared Error (RMSE). The
DCU-SYMC combine system is placed fifth ac-
cording to MAE and second according to RMSE.
In this system description paper, we describe the
features, the learning methods used, the results for
the two submitted systems and some other systems
we experiment with.
2 Features
Our starting point for the WMT13 QE shared task
was the feature set used in the system we submit-
ted to the WMT12 QE task (Rubino et al, 2012).
This feature set, comprising 308 features in to-
tal, extended the 17 baseline features provided by
the task organisers to include 6 additional sur-
face features, 6 additional language model fea-
tures, 17 additional features derived from the
MT system components and the n-best lists, 138
features obtained by part-of-speech tagging and
parsing the source sentences and 95 obtained by
part-of-speech tagging the target sentences, 21
topic model features, 2 features produced by a
grammar checker1 and 6 pseudo-source (or back-
translation) features.
We made the following modifications to this
2012 feature set:
? The pseudo-source (or back-translation) fea-
tures were removed, as they did not con-
tribute useful information to our system last
year.
? The language model and n-gram frequency
feature sets were extended in order to cover
1 to 5 gram sequences, as well as source and
target ratios for these feature values.
? The word-alignment feature set was also
extended by considering several thresholds
1http://www.languagetool.org/
392
when counting the number of target words
aligned with source words.
? We extracted 8 additional features from the
decoder log file, including the number of dis-
carded hypotheses, the total number of trans-
lation options and the number of nodes in the
decoding graph.
? The set of topic model features was reduced
in order to keep only those that were shown
to be effective on three quality estimation
datasets (the details can be found in (Rubino
et al (to appear), 2013)). These features en-
code the difference between source and target
topic distributions according to several dis-
tance/divergence metrics.
? Following Soricut et al (2012), we employed
pseudo-reference features. The source sen-
tences were translated with three different
MT systems: an in-house phrase-based SMT
system built using Moses (Koehn et al,
2007) and trained on the parallel data pro-
vided by the organisers, the rule-based sys-
tem Systran2 and the online, publicly avail-
able, Bing Translator3. The obtained trans-
lations are compared to the target sentences
using sentence-level BLEU (Papineni et al,
2002), TER (Snover et al, 2006) and the Lev-
enshtein distance (Levenshtein, 1966).
? Also following Soricut et al (2012), one-
to-one word-alignments, with and without
Part-Of-Speech (POS) agreement, were in-
cluded as features. Using the alignment in-
formation provided by the decoder, we POS
tagged the source and target sentences with
TreeTagger (Schmidt, 1994) and the publicly
available pre-trained models for English and
Spanish. We mapped the tagsets of both lan-
guages by simplifying the initial tags and ob-
tain a reduced set of 8 tags. We applied that
simplification on the tagged sentences before
checking for POS agreement.
3 Machine Learning
In this section, we describe the learning algo-
rithms and feature selection used in our experi-
ments, leading to the two submitted systems for
the shared task.
2Systran Enterprise Server version 6
3http://www.bing.com/translator
3.1 Primary Learning Method
To estimate the post-editing effort of translated
sentences, we rely on regression models built us-
ing the Support Vector Machine (SVM) algorithm
for regression -SVR, implemented in the LIB-
SVM toolkit (Chang and Lin, 2011). To build
our final regression models, we optimise SVM
hyper-parameters (C, ? and ) using a grid-search
method with 5-fold cross-validation for each pa-
rameter triplet. The parameters leading to the best
MAE, RMSE and Pearson?s correlation coefficient
(r) are kept to build the model.
3.2 Feature Selection on Feature Types
In order to reduce the feature and obtain more
compact models, we apply feature selection on
each of our 15 feature types. Examples of feature
types are language model features or topic model
features. For each feature type, we apply a feature
subset evaluation method based on the wrapper
paradigm and using the best-first search algorithm
to explore the feature space. The M5P (Wang
and Witten, 1997) regression tree algorithm im-
plemented in the Weka toolkit (Hall et al, 2009)
is used with default parameters to train and eval-
uate a regression model for each feature subset
obtained with best-first search. A 10-fold cross-
validation is performed for each subset and we
keep the features leading to the best RMSE. We
use M5P regression trees instead of -SVR be-
cause grid-search with the latter is too computa-
tionally expensive to be applied so many times.
Using feature selection in this way, we obtain 15
reduced feature sets that we combine to form the
DCU-SYMC alltypes system, containing 102
features detailed in Table 1.
3.3 Feature Binarisation
In order to aid the SVM learner, we also experi-
ment with binarising our feature set, i.e. convert-
ing our features with various feature value ranges
into features whose values are either 1 or 0. Again,
we employ regression tree learning. We train
regression trees with M5P and M5P-R4 (imple-
mented in the Weka toolkit) and create a binary
feature for each regression rule found in the trees
(ignoring the leaf nodes). For example, a binary
feature indicating whether the Bing TER score is
less than or equal to 55.685 is derived from the
4We experiment with J48 decision trees as well, but this
method did not outperform regression tree methods.
393
Backward LM
Source 1-gram perplexity.
Source & target 1-grams perplexity ratio.
Source & target 3-grams and 4-gram perplexity ratio.
Target Syntax
Frequency of tags: ADV, FS, DM, VLinf, VMinf, semicolon, VLger, NC, PDEL, VEfin, CC, CCNEG, PPx, ART, SYM,
CODE, PREP, SE and number of ambiguous tags
Frequency of least frequent POS 3-gram observed in a corpus.
Frequency of least frequent POS 4-gram and 6-gram with sentence padding (start and end of sentence tags) observed in a
corpus.
Source Syntax
Features from three probabilistic parsers. (Rubino et al, 2012).
Frequency of least frequent POS 2-gram, 4-gram and 9-gram with sentence padding observed in a corpus.
Number of analyses found and number of words, using a Lexical Functional Grammar of English as described in Rubino
et al (2012).
LM
Source unigram perplexity.
Target 3-gram and 4-gram perplexity with sentence padding.
Source & target 1-gram and 5-gram perplexity ratio.
Source & target unigram log-probability.
Decoder
Component scores during decoding.
Number of phrases in the best translation.
Number of translation options.
N -gram Frequency
Target 2-gram in second and third frequency quartiles.
Target 3-gram and 5-gram in low frequency quartiles.
Number of target 1-gram seen in a corpus.
Source & target 1-grams in highest and second highest frequency quartile.
One-to-One Word-Alignment
Count of O2O word alignment, weighted by target sentence length.
Count of O2O word alignment with POS agreement, weighted by count of O2O, by source length, by target length.
Pseudo-Reference
Moses translation TER score.
Bing translation number of words and TER score.
Systran sBLEU, number of substitutions and TER score.
Surface
Source number of punctuation marks and average words occurrence in source sentence.
Target number of punctuation marks, uppercased letters and binary value if the last character of the sentence is a punctuation
mark.
Ratio of source and target sentence lengths, average word length and number of punctuation marks over sentence lengths.
Topic Model
Cosine distance between source and target topic distributions.
Jensen-Shannon divergence between source and target topic distributions.
Word Alignment
Averaged number of source words aligned per target words with p(s|t) thresholds: 1.0, 0.75, 0.5, 0.25, 0.01
Averaged number of source words aligned per target words with p(s|t) = 0.01 weighted by target words frequency
Averaged number of target words aligned per source word with p(t|s) = 0.01 weighted by source words frequency
Ratio of source and target averaged aligned words with thresholds: 1.0 and 0.1, and with threshold: 0.75, 0.5, 0.25 weighted
by words frequency
Table 1: Features selected with the wrapper approach using best-first search and M5P. These features are
included in the submitted system alltypes.
394
Feature to which threshold t is applied t (?)
Target 1-gram backward LM log-prob. ?35.973
Target 3-gram backward LM perplexity 7144.99
Probabilistic parsing feature 3.756
Probabilistic parsing feature 57.5
Frequency of least frequent POS 6-gram 0.5
Source 3-gram LM log-prob. 65.286
Source 4-gram LM perplexity with padding 306.362
Target 2-gram LM perplexity 176.431
Target 4-gram LM perplexity 426.023
Target 4-gram LM perplexity with padding 341.801
Target 5-gram LM perplexity 112.908
Ratio src&trg 5-gram LM log-prob. 1.186
MT system component score ?50
MT system component score ?0.801
Source 2-gram frequency in low quartile 0.146
Ratio src&trg 2-gram in high freq. quartile 0.818
Ratio src&trg 3-gram in high freq. quartile 0.482
O2O word alignment 15.5
Pseudo-ref. Moses Levenshtein 19
Pseudo-ref. Moses TER 21.286
Pseudo-ref. Bing TER 16.905
Pseudo-ref. Bing TER 23.431
Pseudo-ref. Bing TER 37.394
Pseudo-ref. Bing TER 55.685
Pseudo-ref. Systran sBLEU 0.334
Pseudo-ref. Systran TER 36.399
Source average word length 4.298
Target uppercased/lowercased letters ratio 0.011
Ratio src&trg average word length 1.051
Source word align., p(s|t) > 0.75 11.374
Source word align., p(s|t) > 0.1 485.062
Source word align., p(s|t) > 0.75 weighted 0.002
Target word align., p(t|s) > 0.01 weighted 0.019
Word align. ratio p > 0.25 weighted 1.32
Table 2: Features selected with the M5P-R M50
binarisation approach. For each feature, the cor-
responding rule indicates the binary feature value.
These features are included in the submitted sys-
tem combine in addition to the features presented
in Table 1.
regression rule Bing TER score ? 55.685.
The primary motivation for using regression
tree learning in this way was to provide a quick
and convenient method for binarising our feature
set. However, we can also perform feature selec-
tion using this method by experimenting with vari-
ous minimum leaf sizes (Weka parameter M ). We
plot the performance of the M5P and M5P-R (opti-
mising towards correlation) over the parameter M
and select the best three values of M . To experi-
ment with the effect of smaller and larger feature
sets, we further include parameters of M that (a)
lead to an approximately 50% bigger feature set
and (b) to an approximately 50% smaller feature
set.
Our DCU-SYMC combine system was built
by combining the DCU-SYMC alltypes fea-
ture set, reduced using the best-first M5P wrap-
per approach as described in subsection 3.2, with
a binarised set produced using an M5P regres-
sion tree with a minimum of 50 nodes per leaf.
This latter configuration, containing 34 features
detailed in Table 2, was selected according to the
evaluation scores obtained during cross-validation
on the training set using -SVR, as described in
the next section. Finally, we run a greedy back-
ward feature selection algorithm wrapping -SVR
on both DCU-SYMC alltypes and DCU-SYMC
combine in order to optimise our feature sets for
the SVR learning algorithm, removing 6 and 2 fea-
tures respectively.
4 System Evaluation and Results
In this section, we present the results obtained with
-SVR during 5-fold cross-validation on the train-
ing set and the final results obtained on the test
set. We selected two systems to submit amongst
the different configurations based on MAE, RMSE
and r. As several systems reach the same perfor-
mance according to these metrics, we use the num-
ber of support vectors (noted SV) as an indicator
of training data over-fitting. We report the results
obtained with some of our systems in Table 3.
The results show that the submitted sys-
tems DCU-SYMC alltypes and DCU-SYMC
combine lead to the best scores on cross-
validation, but they do not outperform the system
combining the 15 feature types without feature se-
lection (15 types). This system reaches the best
scores on the test set compared to all our systems
built on reduced feature sets. This indicates that
we over-fit and fail to generalise from the training
data.
Amongst the systems built using reduced fea-
ture sets, the M5P-R M80 system, based on the
tree binarisation approach using M5P-R, yields
the best results on the test set on 3 out of 4 offi-
cial metrics. These results indicate that this sys-
tem, trained on 16 features only, tends to estimate
HTER scores more accurately on the unseen test
data. The results of the two systems based on
the M5P-R binarisation method are the best com-
pared to all the other systems presented in this
Section. This feature binarisation and selection
method leads to robust systems with few features:
31 and 16 for M5P-R M50 and M5P-R M80 re-
spectively. Even though these systems do not lead
to the best results, they outperform the two sub-
mitted systems on one metric used to evaluate the
395
Cross-Validation Test
System nb feat MAE RMSE r SV MAE RMSE DeltaAvg Spearman
15 types 442 0.106 0.138 0.604 1194.6 0.126 0.156 0.108 0.625
M5P M50 34 0.106 0.138 0.600 1417.8 0.135 0.167 0.102 0.586
M5P M130 4 0.114 0.145 0.544 750.6 0.142 0.173 0.079 0.517
M5P-R M50 31 0.106 0.137 0.610 655.4 0.135 0.166 0.100 0.591
M5P-R M80 16 0.107 0.139 0.597 570.6 0.134 0.165 0.106 0.597
alltypes? 96 0.104 0.135 0.624 1130.6 0.135 0.171 0.101 0.589
combine? 134 0.104 0.134 0.629 689.8 0.134 0.166 0.098 0.588
Table 3: Results obtained with different regression models, during cross-validation on the training set
and on the test set, depending on the feature selection method. Systems marked with ? were submitted
for the shared task.
scoring task and two metrics to evaluate the rank-
ing task.
On the systems built using reduced feature sets,
we observe a difference of approximately 0.03pt
absolute between the MAE and RMSE scores ob-
tained during cross-validation and those on the test
set. Such a difference can be related to train-
ing data over-fitting, even though the feature sets
obtained with the tree binarisation methods are
small. For instance, the system M5P M130 is
trained on 4 features only, but the difference be-
tween cross-validation and test MAE scores is
similar to the other systems. We see on the fi-
nal results that our feature selection methods is an
over-fitting factor: by selecting the features which
explain well the training set, the final model tends
to generalise less. The selected features are suited
for the specificities of the training data, but are less
accurate at predicting values on the unseen test set.
5 Discussion
Training data over-fitting is clearly shown by the
results presented in Table 3, indicated by the per-
formance drop between results obtained during
cross-validation and the ones obtained on the test
set. While this drop may be related to data over-
fitting, it may also be related to the use of differ-
ent machine learning methods for feature selec-
tion (M5P and M5P-R) and for building the fi-
nal regression models (-SVR). In order to ver-
ify this aspect, we build two regression models
using M5P, based on the feature sets alltypes
and combine. Results are presented in Table 4
and show that, for the alltypes feature set, the
RMSE, DeltaAvg and Spearman scores are im-
proved using M5P compared to SVM. For the
combine feature set, the scoring results (MAE
and RMSE) are better using SVM, while the rank-
ing results are similar for both machine learning
methods.
The performance drop between the results on
the training data (or a development set) and the
test data was also observed by the highest ranked
participants in the WMT12 QE shared task. To
compare our system without feature selection to
the winner of the previous shared task, we eval-
uate the 15 types system in Table 3 using the
WMT12 QE dataset. The results are presented in
Table 5. We can see that similar MAEs are ob-
tained with our feature set and the WMT12 QE
winner, whereas our system gets a higher RMSE
(+0.01). For the ranking scores, our system is
worse using the DeltaAvg metric while it is bet-
ter on Spearman coefficient.
6 Conclusion
We presented in this paper our experiments for the
WMT13 Quality Estimation shared task. Our ap-
proach is based on the extraction of a large ini-
tial feature set, followed by two feature selection
methods. The first one is a wrapper approach us-
ing M5P and a best-first search algorithm, while
the second one is a feature binarisation approach
using M5P and M5P-R. The final regression mod-
els were built using -SVR and we selected two
systems to submit based on cross-validation re-
sults.
We observed that our system reaching the best
scores on the test set was not a system trained on
a reduced feature set and it did not yield the best
cross-validation results. This system was trained
using 442 features, which are the combination of
15 different feature types. Amongst the systems
built on reduced sets, the best results are obtained
396
System nb feat MAE RMSE DeltaAvg Spearman
alltypes 96 0.135 0.165 0.104 0.604
combine 134 0.139 0.169 0.098 0.587
Table 4: Results obtained with the two feature sets contained in our submitted systems using M5P to
build the regression models instead of -SVR.
System nb feat MAE RMSE DeltaAvg Spearman
WMT12 winner 15 0.61 0.75 0.63 0.64
15 types 442 0.61 0.76 0.60 0.65
Table 5: Results obtained on WMT12 QE dataset with our best system (15 types) compared to WMT12
QE highest ranked team, in the Likert score prediction task.
using the feature binarisation approach M5P-R
80, which contains 16 features selected from our
initial set of features. The tree-based feature bina-
risation is a fast and flexible method which allows
us to vary the number of features by optimising the
leaf size and leads to acceptable results with a few
selected features.
Future work involves a deeper analysis of the
over-fitting effect and an investigation of other
methods in order to outperform the non-reduced
feature set. We are also interested in finding a ro-
bust way to optimise the leaf size parameter for
our tree-based feature binarisation method, with-
out using cross-validation on the training set with
an SVM algorithm.
Acknowledgements
The research reported in this paper has been
supported by the Research Ireland Enterprise
Partnership Scheme (EPSPG/2011/102 and EP-
SPD/2011/135) and Science Foundation Ireland
(Grant 12/CE/I2267) as part of the Centre for
Next Generation Localisation (www.cngl.ie)
at Dublin City University.
References
Chih-Chung Chang and Chih-Jen Lin. 2011.
LIBSVM: A Library for Support Vector Ma-
chines. ACM Transactions on Intelligent Sys-
tems and Technology, 2:27:1?27:27. Soft-
ware available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The WEKA Data Mining Software: an
Update. ACM SIGKDD Explorations Newsletter,
11(1):10?18.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
pages 177?180.
Vladimir I Levenshtein. 1966. Binary codes capable
of correcting deletions, insertions and reversals. In
Soviet physics doklady, volume 10, page 707.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec Submission for
the WMT 2012 Quality Estimation Task. In Pro-
ceedings of the Seventh WMT, pages 138?144.
Raphael Rubino et al (to appear). 2013. Topic Models
for Translation Quality Estimation for Gisting Pur-
poses. In Proceeding of MT Summit XIV.
Helmut Schmidt. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Natu-
ral Language Processing.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In AMTA, pages 223?231.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceedings of
the Seventh WMT, pages 145?151.
Yong Wang and Ian H Witten. 1997. Inducing Model
Trees for Continuous Classes. In Proceedings of
ECML, pages 128?137. Prague, Czech Republic.
397
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 329?334,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Target-Centric Features for Translation Quality Estimation
Chris Hokamp and Iacer Calixto and Joachim Wagner and Jian Zhang
CNGL Centre for Global Intelligent Content
Dublin City University
School of Computing
Dublin, Ireland
{chokamp|icalixto|jwagner|zhangj}@computing.dcu.ie
Abstract
We describe the DCU-MIXED and DCU-
SVR submissions to the WMT-14 Quality
Estimation task 1.1, predicting sentence-
level perceived post-editing effort. Fea-
ture design focuses on target-side features
as we hypothesise that the source side has
little effect on the quality of human trans-
lations, which are included in task 1.1
of this year?s WMT Quality Estimation
shared task. We experiment with features
of the QuEst framework, features of our
past work, and three novel feature sets.
Despite these efforts, our two systems per-
form poorly in the competition. Follow up
experiments indicate that the poor perfor-
mance is due to improperly optimised pa-
rameters.
1 Introduction
Translation quality estimation tries to predict the
quality of a translation given the source and target
text but no reference translations. Different from
previous years (Callison-Burch et al., 2012; Bo-
jar et al., 2013), the WMT 2014 Quality Estima-
tion shared task is MT system-independent, i. e. no
glass-box features are available and translations in
the training and test sets are produced by different
MT systems and also by human translators.
This paper describes the CNGL@DCU team
submission to task 1.1 of the WMT 2014 Quality
Estimation shared task.
1
The task is to predict the
perceived post-editing effort given a source sen-
tence and its raw translation. Due to the inclusion
of human translation in the task, we focus our ef-
forts on target-side features as we expect that the
quality of a translation produced by a human trans-
lator is much less affected by features of the source
1
A CNGL system based on referential translation ma-
chines is submitted separately (Bic?ici and Way, 2014).
than by extrinsic factors such as time pressure and
familiarity with the domain.
To build our quality estimation system, we use
and extend the QuEst framework for translation
quality estimation
2
(Shah et al., 2013; Specia
et al., 2013). QuEst provides modules for fea-
ture extraction and machine learning. We modify
both the feature extraction framework and the ma-
chine learning components to add functionality to
QuEst.
The novel features we add to our systems are
(a) a language model on a combination of stop
words and POS tags, (b) inverse glass-box fea-
tures for translating the translation, and (c) ran-
dom indexing (Sahlgren, 2005) for measuring the
semantic similarity of source and target side across
languages. Furthermore, we integrated (d) source-
side pseudo-reference features (Soricut and Echi-
habi, 2010) and (e) error grammar features (Wag-
ner, 2012), which were used first in MT quality
estimation by (Rubino et al., 2012; Rubino et al.,
2013).
The remaining sections are organised as fol-
lows. Section 2 gives details on the features we
use. Section 3 describes how we set up our ex-
periments. Results are presented in Section 4 and
conclusions are drawn in Section 5 together with
pointers to future work.
2 Features
This section describes the features we extract from
source and target sentences in order to train predic-
tion models and to make predictions in addition to
the baseline features provided for the task.
We focus on the target side as we assume that
the quality of the source side has little predictive
power for human translations, which are included
in task 1.1.
2
http://www.quest.dcs.shef.ac.uk/
329
2.1 QuEst Black-Box Features and Baseline
Features
We use the QuEst framework to extract 47 ba-
sic black-box features from both source and tar-
get side, such as the ratio of the number of to-
kens, punctuation statistics, number if mismatched
brackets and quotes, language model perplexity,
n-gram frequency quartile statistics (n = 1, 2, 3),
and coarse-grained POS frequency ratios. 17 of
the 47 features are identical to the baseline fea-
tures from the shared task website, i. e. 30 fea-
tures are new. To train the language models and
to extract frequency information, we use the News
Commentary corpus (Bojar et al., 2013).
2.2 POS and Stop Word Language Model
Features
For all languages, we extract probability and per-
plexity features from language models trained on
POS tagged corpora. POS tagging is performed
using the IMS Tree Tagger (Schmid, 1994).
We also experiment with language models built
from a combination of stop words
3
and POS tags.
Starting with a tokenised corpus, and its POS-
tagged counterpart, we create a new representation
of the corpus by replacing POS tags for stop words
with the literal stop word that occurred in the orig-
inal corpus, leaving non-stop word tags intact.
4
The intuition behind the approach is that the com-
bined POS and stop word model should encode
the distributional tendencies of the most common
words in the language.
The log-probability and the perplexity of the
target side are used as features. The development
of these features was motivated by manual exam-
ination of the common error types in the train-
ing data. We noted that stop word errors (omis-
sion, mistranslation, mis-translation of idiom), are
prevalent in all language pairs, indicating that fea-
tures which focus on stop word usage could be
useful for predicting the quality of machine trans-
lation. We implement POS and stop word lan-
guage models inside the QuEst framework.
2.3 Source-Side Pseudo-Reference Features
We extract source-side pseudo-reference features
(Albrecht and Hwa, 2008; Soricut and Echihabi,
3
We use the stop word lists from Apache Lucene (McCan-
dless et al., 2010).
4
The News Commentary corpus from WMT13 was used
to build these models, same as for the black-box features
(Section 2.1).
2010; Rubino et al., 2012), for English to German
quality prediction using a highly-tuned German to
English translation system (Li et al., 2014) work-
ing in the reverse direction. The MT system trans-
lates the German target side, the quality of which
is to be predicted, back into English, and we ex-
tract pseudo-reference features on the source side:
? BLEU score (Papineni et al., 2002) be-
tween back-translation and original source
sentence, and
? TER score (Snover et al., 2006).
For the 5th English to German test set item, for
example, the translation
(1) Und belasse sie dort eine Woche.
is translated back to English as
(2) and leave it there for a week .
and compared to the original source sentence
(3) Leave for a week.
producing a BLEU score of 0.077 using the
Python interface to the cdec toolkit (Chahuneau et
al., 2012).
2.4 Inverse Glass-Box Features for
Translating the Translation
In the absence of direct glass-box features, we ob-
tain glass-box features from translating the raw
translation back to the source language using the
same MT system that we use for the source-side
pseudo-reference features. We extract features
from the following components of the Moses de-
coder: distortion model, language model, lexi-
cal reordering, lexical translation probability, op-
erational sequence model (Durrani et al., 2013),
phrase translation probability, and the decoder
score.
The intuition for this set of features is that back-
translating an incorrect translation will give low
system-internal scores, e. g. a low phrase transla-
tion score, and produce poor output with low lan-
guage model scores (garbage in, garbage out).
We are not aware of any previous work using
inverse glass-box features of translating the target
side to another language for quality estimation.
330
2.5 Semantic Similarity Using Random
Indexing
These features try to measure the semantic sim-
ilarity of source and target side of a translation
unit for quality estimation using random index-
ing (Sahlgren, 2005). We experiment with adding
the similarity score of the source and target ran-
dom vectors.
For each source and target pair in the English-
Spanish portion of the Europarl corpus (Koehn,
2005), we initialize a sparse random vector. We
then create token vectors for each source and tar-
get token by summing the vectors for all of the
segments where the token occurs. To extract the
similarity feature for new source and target pairs,
we map them into the vector space by taking the
centroid of the token vectors for the source side
and the target side, and computing their cosine
similarity.
2.6 Error Grammar Parsing
We obtain features from monolingual parsing with
three grammars:
1. the vanilla grammar shipped with the Blipp
parser (Charniak, 2000; Charniak and John-
son, 2005) induced from the Penn-Treebank
(Marcus et al., 1994),
2. an error grammar induced from Penn-Tree-
bank trees distorted according to an error
model (Foster, 2007), and
3. a grammar induced from the union of the
above two treebanks.
Features include the log-ratios between the prob-
ability of the best parse obtained with each gram-
mar and structural differences measured with Par-
seval (Black et al., 1991) and leaf-ancestor (Samp-
son and Babarczy, 2003) metrics. These features
have been shown to be useful for judging the
grammaticality of sentences (Wagner et al., 2009;
Wagner, 2012) and have been used in MT quality
estimation before (Rubino et al., 2012; Rubino et
al., 2013).
3 Experimental Setup
This section describes how we set up our experi-
ments.
3.1 Cross-Validation
Decisions about parameters are made in 10-fold
cross-validation on the training data provided for
the task. As the datasets for task 1.1 include
three to four translations for each source segment,
we group segments by their source side and split
the data for cross-validation between segments to
ensure that a source segment does not occur in
both training and test data for any of the cross-
validation runs.
We implement these modifications to cross-
validation and randomisation in the QuEst frame-
work.
3.2 Training
We use the QuEst framework to train our models.
Support vector regression (SVR) meta-parameters
are optimised using QuEst?s default settings, ex-
ploring RBF kernels with two possible values for
each of the three meta-parameters C, ? and .
5
The two final models are trained on the
full training set with the meta-parameters that
achieved the best average cross-validation score.
3.3 Classifier Combination
We experiment with combining logistic regression
(LR) and support vector regression (SVR) by first
choosing the instances where LR classification is
confident and using the LR class label (1, 2, or
3) as predicted perceived post-editing effort, and
falling back to SVR for all other instances.
We employ several heuristics to decide whether
to use the output of LR or SVR. As the LR classi-
fier learns a decision function for each of the three
classes, we can exploit the scores of the classes to
measure the confidence of the LR classifier about
its decision. If the LR classifier is confident, we
use its prediction directly, otherwise we use the
SVR prediction.
For the cases where one of the three decision
functions for the LR classifier is positive, we select
the prediction directly, falling back to SVR when
the classifier is not confident about any of the three
classes. We implement the LR+SVR classifier
combination inside the QuEst framework.
4 Results
Table 1 shows cross-validation results for the 17
baseline features, the combination of all features
and target-side features only. We do not show
combinations of individual feature sets and base-
line features that do not improve over the base-
5
We only discovered this limitation of the default config-
uration after the system submission, see Sections 4 and 5.
331
Features Classifier RMSE MAE
Basel.17 LR+SVR 0.75 0.62
ALL LR+SVR 0.74 0.59
ALL LR> 0.5+SVR 0.75 0.58
Target LR+SVR 0.75 0.59
ALL LR> 0.5+SVR-r 0.78 0.55
Table 1: Cross-validation results for English to
German. LR > 0.5 indicates that we require the
LR decision function to be > 0.5. SVR-r rounds
the output to the nearest natural number.
line. Several experiments, including those with the
semantic similarity feature sets, are thus omitted.
Furthermore, we only exemplify one language pair
(English to German), as the other language pairs
show similar patterns. The feature set target con-
tains the subset of the QuEst black-box features
(Section 2.1) which only examine the target side.
Our best results for English to German in the
cross-validation experiments are achieved by com-
bining a logistic regression (LR) classifier with
support vector regression (SVR). Furthermore,
performance on the cross-validation is slightly im-
proved for the mean absolute error (MAE) by
rounding SVR scores to the nearest integer. For
the root-mean-square error (RMSE), rounding has
the opposite effect.
Performing a more fine-grained grid search for
the meta-parameters C, ? and  after system sub-
mission, we were able to match the scores for
the baseline features published on the shared task
website.
4.1 Parameters for the Final Models
The final two models for system submission are
trained on the full data set. We submit our best sys-
tem according to MAE in cross-validation com-
bining LR, SVR and rounding with all features
(ALL) as DCU-MIXED. For our second submis-
sion, we choose SVR on its own (system DCU-
SVR). For English-Spanish, we only submit DCU-
SVR.
5 Conclusions and Future Work
We identified improperly optimised parameters of
the SVR component as the cause, or at least as a
contributing factor, for the placement of our sys-
tems below the official baseline system. Other po-
tential factors may be an error in our experimen-
tal setup or over-fitting. Therefore, we plan to re-
peat the experiments with a more fine-grained grid
search for optimal parameters and/or will try an-
other machine learning toolkit.
Unfortunately, due to the above problems with
our system so far, we cannot draw conclusions
about the effectiveness of our novel feature sets.
A substantial gain is achieved on the MAE met-
ric with the rounding method, indicating that the
majority of prediction errors are below 0.5.
6
Fu-
ture work should account for this effect. Two ideas
are: (a) round all predictions before evaluation
and (b) use more fine-grained gold values, e. g. the
(weighted) average over multiple annotations as in
the WMT 2012 quality estimation task (Callison-
Burch et al., 2012).
For the error grammar method, the next step
will be to adjust the error model to errors found in
translations. It may be possible to do this without a
time-consuming analysis of errors: Wagner (2012)
suggests to use parallel data of authentic errors and
corrections to build the error grammar, first pars-
ing the corrections and then guiding the error cre-
ation procedure with the edit operations inverse to
the corrections. Post-editing corpora can play this
role and have recently become available (Potet et
al., 2012).
Furthermore, future work should explore the
inverse glass-box feature idea with arbitrary tar-
get languages for the MT system. (There is no
requirement that the glass-box system translates
back to the original source language).
Finally, we would like to integrate referential
translation machines (Bic?ici, 2013; Bic?ici and
Way, 2014) into our system as they performed well
in the WMT quality estimation tasks this and last
year.
Acknowledgments
This research is supported by the European Com-
mission under the 7th Framework Programme,
specifically its Marie Curie Programme 317471,
and by the Science Foundation Ireland (Grant
12/CE/I2267) as part of CNGL (www.cngl.ie) at
Dublin City University. We thank the anonymous
reviewers and Jennifer Foster for their comments
on earlier versions of this paper.
6
The simultaneous increase on RMSE can be explained if
there is a sufficient number of errors above 0.5: After squar-
ing, these errors are still quite small, e. g. 0.36 for an error of
0.6, but after rounding, the square error becomes 1.0 or 4.0.
332
References
Joshua Albrecht and Rebecca Hwa. 2008. The role
of pseudo references in MT evaluation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 187?190, Columbus, Ohio, June.
Association for Computational Linguistics.
Ergun Bic?ici and Andy Way. 2014. Referential trans-
lation machines for predicting translation quality. In
Proceedings of the Nineth Workshop on Statistical
Machine Translation, Baltimore, USA, June. Asso-
ciation for Computational Linguistics.
Ergun Bic?ici. 2013. Referential translation machines
for quality estimation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation, pages
343?351, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Ezra Black, Steve Abney, Dan Flickinger, Claudia
Gdaniec, Robert Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Fred Jelinek, Judith Klavans,
Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991.
A procedure for quantitatively comparing the syn-
tactic coverage of English grammars. In E. Black,
editor, Proceedings of the HLT Workshop on Speech
and Natural Language, pages 306?311, Morristown,
NJ, USA. Association for Computational Linguis-
tics.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Victor Chahuneau, Noah A. Smith, and Chris Dyer.
2012. pycdec: A python interface to cdec. Prague
Bull. Math. Linguistics, 98:51?62.
Eugene Charniak and Mark Johnson. 2005. Course-
to-fine n-best-parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the ACL (ACL-05), pages 173?180, Ann Ar-
bor, Michigan, June. Association for Computational
Linguistics.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of the First Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (NAACL-00), pages
132?139, Seattle, WA.
Nadir Durrani, Barry Haddow, Kenneth Heafield, and
Philipp Koehn. 2013. Edinburgh?s machine trans-
lation systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 114?121, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Jennifer Foster. 2007. Treebanks gone bad: Parser
evaluation and retraining using a treebank of un-
grammatical sentences. International Journal on
Document Analysis and Recognition, 10(3-4):129?
145.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5, pages 79?86.
Liangyou Li, Xiaofeng Wu, Santiago Cort?es Va??llo, Jun
Xie, Jia Xu, Andy Way, and Qun Liu. 2014. The
DCU-ICTCAS-Tsinghua MT system at WMT 2014
on German-English translation task. In Proceed-
ings of the Nineth Workshop on Statistical Machine
Translation, Baltimore, USA, June. Association for
Computational Linguistics.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
the 1994 ARPA Speech and Natural Language
Workshop, pages 114?119.
Michael McCandless, Erik Hatcher, and Otis Gospod-
netic. 2010. Lucene in Action, Second Edition:
Covers Apache Lucene 3.0. Manning Publications
Co., Greenwich, CT, USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics (ACL02), pages 311?318, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
Marion Potet, Emmanuelle Esperanc?a-Rodier, Laurent
Besacier, and Herv?e Blanchon. 2012. Collection
of a large database of French-English SMT output
corrections. In Proceedings of the 8th International
Conference on Language Resources and Evaluation
(LREC).
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and
Fred Hollowood. 2012. Dcu-symantec submis-
sion for the wmt 2012 quality estimation task. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 138?144, Montr?eal,
Canada, June. Association for Computational Lin-
guistics.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
333
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria,
August. Association for Computational Linguistics.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering
(TKE), volume 5, Copenhagen, Denmark.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(4):365?380.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing, Manchester, United Kingdom.
Kashif Shah, Eleftherios Avramidis, Ergun Bic?ici, and
Lucia Specia. 2013. QuEst - design, implemen-
tation and extensions of a framework for machine
translation quality estimation. The Prague Bulletin
of Mathematical Linguistics, 100.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of association for machine transla-
tion in the Americas, pages 223?231.
Radu Soricut and Abdessamad Echihabi. 2010.
Trustrank: Inducing trust in automatic translations
via ranking. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics, pages 612?621, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. QuEst - a translation quality es-
timation framework. In Proceedings of the 51st An-
nual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 79?84,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Joachim Wagner, Jennifer Foster, and Josef van Gen-
abith. 2009. Judging grammaticality: Experiments
in sentence classification. CALICO Journal (Special
Issue of the 2008 CALICO Workshop on Automatic
Analysis of Learner Language), 26(3):474?490.
Joachim Wagner. 2012. Detecting grammatical errors
with treebank-induced, probabilistic parsers. Ph.D.
thesis, Dublin City University, Dublin, Ireland.
334
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 13?23,
October 25, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Code Mixing: A Challenge for Language Identification in the Language of
Social Media
Utsab Barman, Amitava Das
?
, Joachim Wagner and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Department of Computer Science and Engineering
University of North Texas, Denton, Texas, USA
{ubarman,jwagner,jfoster}@computing.dcu.ie
amitava.das@unt.edu
Abstract
In social media communication, multilin-
gual speakers often switch between lan-
guages, and, in such an environment, au-
tomatic language identification becomes
both a necessary and challenging task.
In this paper, we describe our work in
progress on the problem of automatic
language identification for the language
of social media. We describe a new
dataset that we are in the process of cre-
ating, which contains Facebook posts and
comments that exhibit code mixing be-
tween Bengali, English and Hindi. We
also present some preliminary word-level
language identification experiments using
this dataset. Different techniques are
employed, including a simple unsuper-
vised dictionary-based approach, super-
vised word-level classification with and
without contextual clues, and sequence la-
belling using Conditional Random Fields.
We find that the dictionary-based approach
is surpassed by supervised classification
and sequence labelling, and that it is im-
portant to take contextual clues into con-
sideration.
1 Introduction
Automatic processing and understanding of Social
Media Content (SMC) is currently attracting much
attention from the Natural Language Processing
research community. Although English is still by
far the most popular language in SMC, its domi-
nance is receding. Hong et al. (2011), for exam-
ple, applied an automatic language detection algo-
rithm to over 62 million tweets to identify the top
10 most popular languages on Twitter. They found
that only half of the tweets were in English. More-
over, mixing multiple languages together (code
mixing) is a popular trend in social media users
from language-dense areas (C?ardenas-Claros and
Isharyanti, 2009; Shafie and Nayan, 2013). In
a scenario where speakers switch between lan-
guages within a conversation, sentence or even
word, the task of automatic language identifica-
tion becomes increasingly important to facilitate
further processing.
Speakers whose first language uses a non-
Roman alphabet write using the Roman alphabet
for convenience (phonetic typing) which increases
the likelihood of code mixing with a Roman-
alphabet language. This can be especially ob-
served in South-East Asia and in the Indian sub-
continent. The following is a code mixing com-
ment taken from a Facebook group of Indian uni-
versity students:
Original: Yaar tu to, GOD hain. tui JU
te ki korchis? Hail u man!
Translation: Buddy you are GOD. What
are you doing in JU? Hail u man!
This comment is written in three languages: En-
glish, Hindi (italics), and Bengali (boldface). For
Bengali and Hindi, phonetic typing has been used.
We follow in the footsteps of recent work on
language identification for SMC (Hughes et al.,
2006; Baldwin and Lui, 2010; Bergsma et al.,
2012), focusing specifically on the problem of
word-level language identification for code mixing
SMC. Our corpus for this task is collected from
Facebook and contains instances of Bengali(BN)-
English(EN)-Hindi(HI) code mixing.
The paper is organized as follows: in Section 2,
we review related research in the area of code
mixing and language identification; in Section 3,
we describe our code mixing corpus, the data it-
13
self and the annotation process; in Section 4, we
list the tools and resources which we use in our
language identification experiments, described in
Section 5. Finally, in Section 6, we conclude
and provide suggestions for future research on this
topic.
2 Background and Related Work
The problem of language identification has been
investigated for half a century (Gold, 1967) and
that of computational analysis of code switching
for several decades (Joshi, 1982), but there has
been less work on automatic language identifi-
cation for multilingual code-mixed texts. Before
turning to that topic, we first briefly survey studies
on the general characteristics of code mixing.
Code mixing is a normal, natural product of
bilingual and multilingual language use. Signif-
icant studies of the phenomenon can be found
in the linguistics literature (Milroy and Muysken,
1995; Alex, 2008; Auer, 2013). These works
mainly discuss the sociological and conversational
necessities behind code mixing as well as its lin-
guistic nature. Scholars distinguish between inter-
sentence, intra-sentence and intra-word code mix-
ing.
Several researchers have investigated the rea-
sons for and the types of code mixing. Initial stud-
ies on Chinese-English code mixing in Hong Kong
(Li, 2000) and Macao (San, 2009) indicated that
mainly linguistic motivations were triggering the
code mixing in those highly bilingual societies.
Hidayat (2012) showed that Facebook users tend
to mainly use inter-sentential switching over intra-
sentential, and report that 45% of the switching
was instigated by real lexical needs, 40% was used
for talking about a particular topic, and 5% for
content clarification. The predominance of inter-
sentential code mixing in social media text was
also noted in the study by San (2009), which com-
pared the mixing in blog posts to that in the spoken
language in Macao. Dewaele (2010) claims that
?strong emotional arousal? increases the frequency
of code mixing. Dey and Fung (2014) present
a speech corpus of English-Hindi code mixing in
student interviews and analyse the motivations for
code mixing and in what grammatical contexts
code mixing occurs.
Turning to the work on automatic analysis of
code mixing, there have been some studies on de-
tecting code mixing in speech (Solorio and Liu,
2008a; Weiner et al., 2012). Solorio and Liu
(2008b) try to predict the points inside a set of spo-
ken Spanish-English sentences where the speak-
ers switch between the two languages. Other
studies have looked at code mixing in differ-
ent types of short texts, such as information re-
trieval queries (Gottron and Lipka, 2010) and SMS
messages (Farrugia, 2004; Rosner and Farrugia,
2007). Yamaguchi and Tanaka-Ishii (2012) per-
form language identification using artificial mul-
tilingual data, created by randomly sampling text
segments from monolingual documents. King
and Abney (2013) used weakly semi-supervised
methods to perform word-level language identifi-
cation. A dataset of 30 languages has been used
in their work. They explore several language
identification approaches, including a Naive Bayes
classifier for individual word-level classification
and sequence labelling with Conditional Random
Fields trained with Generalized Expectation crite-
ria (Mann and McCallum, 2008; Mann and Mc-
Callum, 2010), which achieved the highest scores.
Another very recent work on this topic is (Nguyen
and Do?gru?oz, 2013). They report on language
identification experiments performed on Turkish
and Dutch forum data. Experiments have been
carried out using language models, dictionaries,
logistic regression classification and Conditional
Random Fields. They find that language models
are more robust than dictionaries and that contex-
tual information is helpful for the task.
3 Corpus Acquisition
Taking into account the claim that code mixing is
frequent among speakers who are multilingual and
younger in age (C?ardenas-Claros and Isharyanti,
2009), we choose an Indian student community
between the 20-30 year age group as our data
source. India is a country with 30 spoken lan-
guages, among which 22 are official. code mix-
ing is very frequent in the Indian sub-continent
because languages change within very short geo-
distances and people generally have a basic knowl-
edge of their neighboring languages.
A Facebook group
1
and 11 Facebook users
(known to the authors) were selected to obtain
publicly available posts and comments. The Face-
book graph API explorer was used for data collec-
tion. Since these Facebook users are from West
Bengal, the most dominant language is Bengali
1
https://www.facebook.com/jumatrimonial
14
(Native Language), followed by English and then
Hindi (National Language of India). The posts
and comments in Bengali and Hindi script were
discarded during data collection, resulting in 2335
posts and 9813 comments.
3.1 Annotation
Four annotators took part in the annotation task.
Three were computer science students and the
other was one of the authors. The annotators are
proficient in all three languages of our corpus. A
simple annotation tool was developed which en-
abled these annotators to identify and distinguish
the different languages present in the content by
tagging them. Annotators were supplied with 4
basic tags (viz. sentence, fragment, inclusion and
wlcm (word-level code mixing)) to annotate differ-
ent levels of code mixing. Under each tag, six at-
tributes were provided, viz. English (en), Bengali
(bn), Hindi (hi), Mixed (mixd), Universal (univ)
and Undefined (undef). The attribute univ is as-
sociated with symbols, numbers, emoticons and
universal expressions (e.g. hahaha, lol). The at-
tribute undef is specified for a sentence or a word
for which no language tags can be attributed or
cannot be categorized as univ. In addition, anno-
tators were instructed to annotate named entities
separately. What follows are descriptions of each
of the annotation tags.
Sentence (sent): This tag refers to a sentence
and can be used to mark inter-sentential code mix-
ing. Annotators were instructed to identify a sen-
tence with its base language (e.g. en, bn, hi and
mixd) or with other types (e.g. univ, undef ) as the
first task of annotation. Only the attribute mixd is
used to refer to a sentence which contains multi-
ple languages in the same proportion. A sentence
may contain any number of inclusions, fragments
and word-level code mixing. A sentence can be at-
tributed as univ if and only if it contains symbols,
numbers, emoticons, chat acronyms and no other
words (Hindi, English or Bengali). A sentence can
be attributed as undef if it is not a sentence marked
as univ and has words/tokens that can not be cate-
gorized as Hindi, English or Bengali. Some exam-
ples of sentence-level annotations are the follow-
ing:
1. English-Sentence:
[sent-lang=?en?] what a.....6 hrs long...but re-
ally nice tennis.... [/sent]
2. Bengali-Sentence:
[sent-lang=?bn?] shubho nabo borsho.. :)
[/sent]
3. Hindi Sentence:
[sent-lang=?hi?] karwa sachh ..... :( [/sent]
4. Mixed-Sentence:
[sent-lang=?mixd?] [frag-lang=?hi?] oye
hoye ..... angreji me kahte hai ke [/frag]
[frag-lang=?en?] I love u.. !!! [/frag] [/sent]
5. Univ-Sentence:
[sent-lang=?univ?] hahahahahahah....!!!!!
[/sent]
6. Undef-Sentence:
[sent-lang=?undef?] Hablando de una triple
amenaza. [/sent]
Fragment (frag): This refers to a group of for-
eign words, grammatically related, in a sentence.
The presence of this tag in a sentence conveys that
intra-sentential code mixing has occurred within
the sentence boundary. Identification of fragments
(if present) in a sentence was the second task of
annotation. A sentence (sent) with attribute mixd
must contain multiple fragments (frag) with a spe-
cific language attribute. In the fourth example
above, the sentence contains a Hindi fragment oye
hoye ..... angreji me kahte hai ke and an English
fragment I love u.. !!!, hence it is considered as a
mixd sentence. A fragment can have any number
of inclusions and word-level code mixing. In the
first example below, Jio is a popular Bengali word
appearing in the English fragment Jio.. good joke,
hence tagged as a Bengali inclusion. One can ar-
gue that the word Jio could be a separate Bengali
inclusion (i.e. can be tagged as a Bengali inclu-
sion outside the English fragment). But looking
at the syntactic pattern and the sense expressed by
the comment, the annotator kept it as a single unit.
In the second example below, an instance of word-
level code mixing, typer, has been found in an En-
glish fragment (where the root English word type
has the Bengali suffix r).
1. Fragment with Inclusion:
[sent-lang=?mixd?] [frag-lang=?en?] [incl-
lang=?bn?] Jio.. [/incl] good joke [/frag] [frag
lang=?bn?] ?amar Babin? [/frag] [/sent]
2. Fragment with Word-Level code mixing:
[sent-lang=?mixd?] [frag-lang=?en?] ? I will
find u and marry you ? [/frag] [frag-
lang=?bn?] [wlcm-type=?en-and-bn-suffix?]
typer [/wlcm] hoe glo to! :D [/frag] [/sent]
15
Inclusion (incl): An inclusion is a foreign word
or phrase in a sentence or in a fragment which
is assimilated or used very frequently in native
language. Identification of inclusions can be per-
formed after annotating a sentence and fragment
(if present in that sentence). An inclusion within a
sentence or fragment also denotes intra-sentential
code mixing. In the example below, seriously is an
English inclusion which is assimilated in today?s
colloquial Bengali and Hindi. The only tag that an
inclusion may contain is word-level code mixing.
1. Sentence with Inclusion:
[sent-lang=?bn?] Na re [incl-lang=?en?] seri-
ously [/incl] ami khub kharap achi. [/sent]
Word-Level code mixing (wlcm): This is the
smallest unit of code mixing. This tag was in-
troduced to capture intra-word code mixing and
denotes cases where code mixing has occurred
within a single word. Identifying word-level code
mixing is the last task of annotation. Annotators
were told to mention the type of word-level code
mixing in the form of an attribute (Base Language
+ Second Language) format. Some examples are
provided below. In the first example below, the
root word class is English and e is an Bengali suf-
fix that has been added. In the third example be-
low, the opposite can be observed ? the root word
Kando is Bengali, and an English suffix z has been
added. In the second example below, a named en-
tity suman is present with a Bengali suffix er.
1. Word-Level code mixing (EN-BN):
[wlcm-type=?en-and-bn-suffix?] classe
[/wlcm]
2. Word-Level code mixing (NE-BN):
[wlcm-type=?NE-and-bn-suffix?] sumaner
[/wlcm]
3. Word-Level code mixing (BN-EN):
[wlcm-type=?bn-and-en-suffix?] kandoz
[/wlcm]
3.1.1 Inter Annotator Agreement
We calculate word-level inter annotator agreement
(Cohen?s Kappa) on a subset of 100 comments
(randomly selected) between two annotators. Two
annotators are in agreement about a word if they
both annotate the word with the same attribute
(en, bn, hi, univ, undef ), regardless of whether
the word is inside an inclusion, fragment or sen-
tence. Our observations that the word-level anno-
tation process is not a very ambiguous task and
that annotation instruction is also straightforward
are confirmed in a high inter-annotator agreement
(IAA) with a Kappa value of 0.884.
3.2 Data Characteristics
Tag-level and word-level statistics of annotated
data that reveal the characteristics of our data set
are described in Table 1 and in Table 2 respec-
tively. More than 56% of total sentences and al-
most 40% of total tokens are in Bengali, which is
the dominant language of this corpus. English is
the second most dominant language covering al-
most 33% of total tokens and 35% of total sen-
tences. The amount of Hindi data is substantially
lower ? nearly 1.75% of total tokens and 2% of to-
tal sentences. However, English inclusions (84%
of total inclusions) are more prominent than Hindi
or Bengali inclusions and there are a substantial
number of English fragments (almost 52% of total
fragments) present in our corpus. This means that
English is the main language involved in the code
mixing.
Statistics of Different Tags
Tags En Bn Hi Mixd Univ Undef
sent 5,370 8,523 354 204 746 15
frag 288 213 40 0 6 0
incl 7,377 262 94 0 1,032 1
wlcm 477
Name Entity 3,602
Acronym 691
Table 1: Tag-level statistics
Word-Level Tag Count
EN 66,298
BN 79,899
HI 3,440
WLCM 633
NE 5,233
ACRO 715
UNIV 39,291
UNDEF 61
Table 2: Word-level statistics
3.2.1 Code Mixing Types
In our corpus, inter- and intra-sentential code mix-
ing are more prominent than word-level code mix-
ing, which is similar to the findings of (Hidayat,
2012) . Our corpus contains every type of code
mixing in English, Hindi and Bengali viz. in-
ter/intra sentential and word-level as described in
the previous section. Some examples of different
types of code mixing in our corpus are presented
below.
16
1. Inter-Sentential:
[sent-lang=?hi?] Itna izzat diye aapne mujhe
!!! [/sent]
[sent-lang=?en?] Tears of joy. :?( :?( [/sent]
2. Intra-Sentential:
[sent-lang=?bn?] [incl-lang=?en?] by d way
[/incl] ei [frag-lang=?en?] my craving arms
shall forever remain empty .. never hold u
close .. [/frag] line ta baddo [incl-lang=?en?]
cheezy [/incl] :P ;) [/sent]
3. Word-Level:
[sent-lang=?bn?] [incl-lang=?en?] 1st yr
[/incl] eo to ei [wlcm-type=?en+bnSuffix?]
tymer [/wlcm] modhye sobar jute jay ..
[/sent]
3.2.2 Ambiguous Words
Annotators were instructed to tag an English word
as English irrespective of any influence of word
borrowing or foreign inclusion but an inspection of
the annotations revealed that English words were
sometimes annotated as Bengali or Hindi. To un-
derstand this phenomenon we processed the list
of language (EN,BN and HI) word types (total
26,475) and observed the percentage of types that
were not always annotated with the one language
throughout the corpus. The results are presented in
Table 3. Almost 7% of total types are ambiguous
(i.e. tagged in different languages during annota-
tion). Among them, a substantial amount (5.58%)
are English/Bengali.
Label(s) Count Percentage
EN 9,109 34.40
BN 14,345 54.18
HI 1,039 3.92
EN or BN 1,479 5.58
EN or HI 61 0.23
BN or HI 277 1.04
EN or BN or HI 165 0.62
Table 3: Statistics of ambiguous and monolingual
word types
There are two reasons why this is happening:
Same Words Across Languages Some words
are the same (e.g. baba, maa, na, khali) in Hindi
and Bengali because both of the languages orig-
inated from a single language Sanskrit and share
a good amount of common vocabulary. It also
occurred in English-Hindi and English-Bengali as
a result of word borrowing. Most of these are
commonly used inclusions like clg, dept, ques-
tion, cigarette, and topic. Sometimes the anno-
tators were careful enough to tag such words as
English and sometimes these words were tagged
in the annotators? native languages. During cross
checking of the annotated data the same error pat-
terns were observed for multiple annotators, i.e.
tagging commonly used foreign words into native
language. It only demonstrates that these English
words are highly assimilated in the conversational
vocabulary of Bengali and Hindi.
Phonetic Similarity of Spellings Due to pho-
netic typing some words share the same surface
form across two and sometimes across three lan-
guages. As an example, to is a word in the three
languages: it has occurred 1209 times as English,
715 times as Bengali and 55 times as Hindi in our
data. The meaning of these words (e.g. to, bolo,
die) are different in different languages. This phe-
nomenon is perhaps exacerbated by the trend to-
wards short and noisy spelling in SMC.
4 Tools and Resources
We have used the following resources and tools in
our experiment.
Dictionaries
1. British National Corpus (BNC): We com-
pile a word frequency list from the BNC (As-
ton and Burnard, 1998).
2. SEMEVAL 2013 Twitter Corpus (Se-
mevalTwitter): To cope with the language
of social media we use the SEMEVAL 2013
(Nakov et al., 2013) training data for the
Twitter sentiment analysis task. This data
comes from a popular social media site and
hence is likely to reflect the linguistic proper-
ties of SMC.
3. Lexical Normalization List (LexNorm-
List): Spelling variation is a well-known
phenomenon in SMC. We use a lexical nor-
malization dictionary created by Han et al.
(2012) to handle the different spelling vari-
ations in our data.
Machine Learning Toolkits
1. WEKA: We use the Weka toolkit (Hall et
al., 2009) for our experiments in decision tree
training.
2. MALLET: CRF learning is applied using the
MALLET toolkit (McCallum, 2002).
17
3. Liblinear: We apply Support Vector Ma-
chine (SVM) learning with a linear kernel us-
ing the Liblinear package (Fan et al., 2008).
NLP Tools For data tokenization we used the
CMU Tweet-Tokenizer (Owoputi et al., 2013).
5 Experiments
Since our training data is entirely labelled at the
word-level by human annotators, we address the
word-level language identification task in a fully
supervised way.
Out of the total data, 15% is set aside as a
blind test set, while the rest is employed in our ex-
periments through a 5-fold cross-validation setup.
There is a substantial amount of token overlap be-
tween the cross-validation data and the test set ?
88% of total EN tokens, 86% of total Bengali to-
kens and 57% of total Hindi tokens of the test set
are present in the cross-validation data.
2
We address the problem of word-level in three
different ways:
1. A simple heuristic-based approach which
uses a combination of our dictionaries to clas-
sify the language of a word
2. Word-level classification using supervised
machine learning with SVMs but no contex-
tual information
3. Word-level classification using supervised
machine learning with SVMs and sequence
labelling using CRFs, both employing con-
textual information
Named entities and instances of word-level
code mixing are excluded from evaluation. For
systems which do not take the context of a word
into account, i.e. the dictionary-based approach
(Section 5.1) and the SVM approach without con-
textual clues (Section 5.2), named entities and in-
stances of word-level code mixing can be safely
excluded from training. For systems which do
take context into account, the CRF system (Sec-
tion 5.3.1) and the SVM system with contextual
clues (Section 5.3.2), these are included in train-
ing, because to exclude them would result in un-
realistic contexts. This means that these systems
2
We found 25 comments and 17 posts common between
the cross-validation data and the test set. The reason for this
is that users of social media often express themselves in a
concise way. Almost all of these common data consisted of 1
to 3 token(s). In most of the cases these tokens were emoti-
cons, symbols or universal expressions such as wow and lol.
As the percentage of these comments is low, we keep these
comments as they are.
can classify a word to be a named entity or an in-
stance of word-level code mixing. To avoid this,
we implement a post-processor which backs off in
these cases to a system which hasn?t seen named
entities or word-level code mixing in training (see
Section 5.3).
5.1 Dictionary-Based Detection
We start with dictionary-based language detec-
tion. Generally a dictionary-based language de-
tector predicts the language of a word based on
its frequency in multiple language dictionaries. In
our data the Bengali and Hindi tokens are phoneti-
cally typed. As no such transliterated dictionary is,
to our knowledge, available for Bengali and Hindi,
we use the training set words as dictionaries. For
words that have multiple annotations in training
data (ambiguous words), we select the majority
tag based on frequency, e.g. the word to will al-
ways be tagged as English.
Our English dictionaries are those described
in Section 4 (BNC, LexNormList, SemEvalTwit-
ter) and the training set words. For LexNorm-
List, we have no frequency information, and so
we consider it as a simple word list. To pre-
dict the language of a word, dictionaries with nor-
malized frequency were considered first (BNC,
SemEvalTwitter, Training Data), if not found,
word list look-up was performed. The predicted
language is chosen based on the dominant lan-
guage(s) of the corpus if the word appears in mul-
tiple dictionaries with same frequency or if the
word does not appear in any dictionary or list.
A simple rule-based method is applied to pre-
dict universal expressions. A token is considered
as univ if any of the following conditions satisfies:
? All characters of the token are symbols or
numbers.
? The token contains certain repetitions identi-
fied by regular expressions.(e.g. hahaha).
? The token is a hash-tag or an URL or
mention-tags (e.g. @Sumit).
? Tokens (e.g. lol) identified by a word list
compiled from the relevant 4/5th of the train-
ing data.
Table 4 shows the results of dictionary-based
detection obtained from 5-fold cross-validation
averaging. We try different combinations and fre-
quency thresholds of the above dictionaries. We
find that using a normalized frequency is helpful
18
and that a combination of LexNormList and Train-
ing Data dictionaries is suited best for our data.
Hence, we consider this as our baseline language
identification system.
Dictionary Accuracy(%)
BNC 80.09
SemevalTwitter 77.61
LexNormList 79.86
Training Data 90.21
LexNormList+TrainingData (Baseline) 93.12
Table 4: Average cross-validation accuracy of
dictionary-based detection
5.2 Word-Level Classification without
Contextual Clues
The following feature types are employed:
1. Char-n-grams (G): We start with a character
n-gram-based approach (Cavnar and Tren-
kle, 1994), which is most common and fol-
lowed by many language identification re-
searchers. Following the work of King and
Abney (2013), we select character n-grams
(n=1 to 5) and the word as the features in our
experiments.
2. Presence in Dictionaries (D): We use pres-
ence in a dictionary as a features for all avail-
able dictionaries in previous experiments.
3. Length of words (L): Instead of using the
raw length value as a feature, we follow our
previous work (Rubino et al., 2013; Wagner
et al., 2014) and create multiple features for
length using a decision tree (J48). We use
length as the only feature to train a decision
tree for each fold and use the nodes obtained
from the tree to create boolean features.
4. Capitalization (C): We use 3 boolean fea-
tures to encode capitalization information:
whether any letter in the word is capitalized,
whether all letters in the word are capitalized
and whether the first letter is capitalized.
We perform experiments with an SVM classifier
(linear kernel) for different combination of these
features.
3
Parameter optimizations (C range 2
-15
to 2
10
) for SVM are performed for each feature
3
According to (Hsu et al., 2010) the SVM linear kernel
with parameter C optimization is good enough when dealing
with a large number of features. Though an RBF kernel can
be more effective than a linear one, it is possible only after
proper optimization of C and ? parameters, which is compu-
tational expensive for such a large feature set.
Features Accuracy Features Accuracy
G 94.62 GD 94.67
GL 94.62 GDL 94.73
GC 94.64 GDC 94.72
GLC 94.64 GDLC 94.75
Table 5: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
GDLC: 94.75%
GLC: 94.64% GDL: 94.73% GDC: 94.72%
GL: 94.62% GC: 94.64% GD: 94.67%
G: 94.62%
Figure 1: Average cross-validation accuracy for
SVM word-level classification (without context),
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features: cube visualization
set and best cross-validation accuracy is found for
the GDLC-based run (94.75%) at C=1 (see Table 5
and Fig. 1).
We also investigate the use of a dictionary-to-
char-n-gram back-off model ? the idea is to ap-
ply the char-n-gram model SVM-GDLC for those
words for which a majority-based decision is taken
during dictionary-based detection. However, it
does not outperform the SVM. Hence, we select
SVM-GDLC for the next steps of our experiments
as the best exemplar of our individual word-level
classifier (without contextual clues).
5.3 Language Identification with Contextual
Clues
Contextual clues can play a very important role in
word-level language identification. As an exam-
ple, a part of a comment is presented from cross-
validation fold 1 that contains the word die which
is wrongly classified by the SVM classifier. The
frequency of die in the training set of fold 1 is 6
for English, 31 for Bengali and 0 for Hindi.
Gold Data: ..../univ the/en movie/en
for/en which/en i/en can/en die/en for/en
19
Features Order-0 Order-1 Order-2
G 92.80 95.16 95.36
GD 93.42 95.59 95.98
GL 92.82 95.14 95.41
GDL 93.47 95.60 95.94
GC 92.07 94.60 95.05
GDC 93.47 95.62 95.98
GLC 92.36 94.53 95.02
GDLC 93.47 95.58 95.98
Table 6: Average cross-validation accuracy of
CRF-based predictions where G = char-n-gram, L
= length feature, D = single dictionary-based la-
bels (baseline system) and C = capitalization fea-
tures
...../univ
SVM Output: ..../univ the/en
movie/en for/en which/en i/en can/en
die/bn for/en ...../univ
We now investigate whether contextual informa-
tion can correct the mis-classified tags.
Although named entities and word-level code
mixing are excluded from evaluation, when deal-
ing with context it is important to consider named
entity and word-level code mixing during training
because these may contain some important infor-
mation. We include these tokens in the training
data for our context-based experiments, labelling
them as other. The presence of this new label may
affect the prediction for a language token during
classification and sequence labelling. To avoid this
situation, a 4-way (bn, hi, en, univ) backoff classi-
fier is trained separately on English, Hindi, Ben-
gali and universal tokens. During evaluation of
any context-based system we discard named en-
tity and word-level code mixing from the predic-
tion of that system. If any of the remaining tokens
is predicted as other we back off to the decision
of the 4-way classifier for that token. For the CRF
experiments (Section 5.3.1), the backoff classifier
is a CRF system, and, for the SVM experiments
(Section 5.3.2), the backoff classifier is an SVM
system.
5.3.1 Conditional Random Fields (CRF)
As our goal is to apply contextual clues, we first
employ Conditional Random Fields (CRF), an ap-
proach which takes history into account in pre-
dicting the optimal sequence of labels. We em-
ploy a linear chain CRF with an increasing or-
der (Order-0, Order-1 and Order-2) with 200 it-
erations for different feature combinations (used
GDLC: 95.98%
GLC: 95.02% GDL: 95.94% GDC: 95.98%
GL: 95.41% GC: 95.05% GD: 95.98%
G: 95.36%
Figure 2: CRF Order-2 results: cube visualisation
G = char-n-gram, L = binary length features, D
= presence in dictionaries and C = capitalization
features
Context Accuracy (%)
GDLC + P
1
94.66
GDLC + P
2
94.55
GDLC + N
1
94.53
GDLC + N
2
94.37
GDLC + P
1
N
1
95.14
GDLC + P
2
N
2
94.55
Table 7: Average cross-validation accuracy of
SVM (GDLC) context-based runs, where P-i =
previous i word(s) , N-i = next i word(s)
in SVM-based runs). However, we observe that
accuracy of CRF based runs decreases when bi-
narized length features (see Section 5.2 and dic-
tionary features (a feature for each dictionary) are
involved. Hence, we use the dictionary-based pre-
dictions of the baseline system to generate a single
dictionary feature for each token and only the raw
length value of a token instead of binarized length
features. The results are presented in Table 6 and
the second order results are visualized in Fig. 2.
As expected, the performance increases as the
order increases from zero to one and two. The use
of a single dictionary feature is also helpful. The
results for GDC, GDLC, and GD based runs are
almost similar (95.98%). However, we choose the
GDC system because it performed slightly better
(95.989%) than the GDLC (95.983%) and the GD
(95.983%) systems.
5.3.2 SVM with Context
We also add contextual clues to our SVM classi-
fier. To obtain contextual information we include
the previous and next two words as features in
the SVM-GDLC-based run.
4
All possible com-
4
We also experimented with extracting all GDLC features
for the context words but this did not help.
20
binations are considered during experiments (Ta-
ble 7). After C parameter optimization, the best
cross-validation accuracy is found for the P
1
N
1
(one word previous and one word next) run with
C=0.125 (95.14%).
5.4 Test Set Results
We apply our best dictionary-based system, our
best SVM system (with and without context) and
our best CRF system to the held-out test set. The
results are shown in Table 8. Our best result is
achieved using the CRF model (95.76%).
5.5 Error Analysis
Manual error analysis shows the limitations of
these systems. The word-level classifier without
contextual clues does not perform well with Hindi
data. The number of Hindi tokens is quite low.
Only 2.4% (4,658) of total tokens of the training
data are Hindi, out of which 55.36% are bilin-
gually ambiguous and 29.51% are tri-lingually
ambiguous tokens. Individual word-level systems
often fail to assign proper labels to ambiguous
words, but adding context information helps to
overcome this problem. Considering the previ-
ous example of die, both context-based SVM and
CRF systems classify it properly. Though the final
system CRF-GDC performs well, it also has some
limitations, failing to identify the language for the
tokens which appear very frequently in three lan-
guages (e.g. are, na, pic).
6 Conclusion
We have presented an initial study on automatic
language identification with Indian language code
mixing from social media communication. We
described our dataset of Bengali-Hindi-English
Facebook comments and we presented the results
of our word-level classification experiments on
this dataset. Our experimental results lead us to
conclude that character n-gram features are useful
for this task, contextual information is also impor-
tant and that information from dictionaries can be
effectively incorporated as features.
In the future we plan to apply the techniques
and feature sets that we used in these experiments
to other datasets. We have already started this by
applying variants of the systems presented here to
the Nepali-English and Spanish-English datasets
which were introduced as part of the 2014 code
mixing shared task (Solorio et al., 2014; Barman
et al., 2014).
We did not include word-level code mixing in
our experiments ? in our future experiments we
will explore ways to identify and segment this type
of code mixing. It will be also important to find the
best way to handle inclusions since there is a fine
line between word borrowing and code mixing.
Acknowledgements
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University. The
authors wish to acknowledge the DJEI/DES/SFI/
HEA for the provision of computational facili-
ties and support. Our special thanks to Soumik
Mandal from Jadavpur University, India for co-
ordinating the annotation task. We also thank the
administrator of JUMatrimonial and the 11 Face-
book users who agreed that we can use their posts
for their support and permission.
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Peter Auer. 2013. Code-Switching in Conversation:
Language, Interaction and Identity. Routledge.
Timothy Baldwin and Marco Lui. 2010. Language
identification: The long and the short of the mat-
ter. In Human Language Technologies: The 2010
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 229?237. Association for Computational Lin-
guistics.
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a,
and Jennifer Foster. 2014. DCU-UVT: Word-
level language classification with code-mixed data.
In Proceedings of the First Workshop on Com-
putational Approaches to Code-Switching. EMNLP
2014, Conference on Empirical Methods in Natural
Language Processing, Doha, Qatar. Association for
Computational Linguistics.
Shane Bergsma, Paul McNamee, Mossaab Bagdouri,
Clayton Fink, and Theresa Wilson. 2012. Language
identification for creating language-specific Twitter
collections. In Proceedings of the Second Workshop
on Language in Social Media, pages 65?74. Associ-
ation for Computational Linguistics.
21
System
Precision (%) Recall (%) Accuracy
(%)EN BN HI UNIV EN BN HI UNIV
Baseline (Dictionary) 92.67 90.73 80.64 99.67 92.28 94.63 43.47 94.99 93.64
SVM-GDLC 92.49 94.89 80.31 99.34 96.23 94.28 44.92 97.07 95.21
SVM-P
1
N
1
93.51 95.56 83.18 99.42 96.63 95.23 55.94 96.95 95.52
CRF-GDC 94.77 94.88 91.86 99.34 95.65 96.22 55.65 97.73 95.76
Table 8: Test set results for Baseline (Dictionary), SVM-GDLC, SVM-P1N1 and CRF-GDC
MS C?ardenas-Claros and N Isharyanti. 2009. Code-
switching and code-mixing in internet chatting:
Between?yes,?ya,?and?si?-a case study. The Jalt Call
Journal, 5(3):67?78.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Jean-Marc Dewaele. 2010. Emotions in Multiple Lan-
guages. Palgrave Macmillan.
Anik Dey and Pascale Fung. 2014. A Hindi-
English code-switching corpus. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), pages 2410?
2413, Reykjavik, Iceland. European Language Re-
sources Association (ELRA).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
E Mark Gold. 1967. Language identification in the
limit. Information and control, 10(5):447?474.
Thomas Gottron and Nedim Lipka. 2010. A compar-
ison of language identification approaches on short,
query-style texts. In Advances in Information Re-
trieval, pages 611?614. Springer.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explor. Newsl., 11(1):10?18.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Taofik Hidayat. 2012. An analysis of code switch-
ing used by facebookers: a case study in a
social network site. Student essay for the
study programme ?Pendidikan Bahasa Ing-
gris? (English Education) at STKIP Siliwangi
Bandung, Indonesia, http://publikasi.
stkipsiliwangi.ac.id/files/2012/
10/08220227-taofik-hidayat.pdf.
Lichan Hong, Gregorio Convertino, and Ed H. Chi.
2011. Language matters in twitter: A large scale
study. In Proceedings of the Fifth International
AAAI Conference on Weblogs and Social Media
(ICWSM-11), pages 518?521, Barcelona, Spain. As-
sociation for the Advancement of Artificial Intelli-
gence.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-
Jen Lin. 2010. A practical guide to sup-
port vector classification. Technical re-
port. Department of Computer Science, Na-
tional Taiwan University, Taiwan, https:
//www.cs.sfu.ca/people/Faculty/
teaching/726/spring11/svmguide.pdf.
Baden Hughes, Timothy Baldwin, Steven Bird, Jeremy
Nicholson, and Andrew MacKinlay. 2006. Recon-
sidering language identification for written language
resources. In Proc. of the 5th edition of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2006), pages 485?488, Genoa, Italy.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia. Association for Computa-
tional Linguistics.
David C. S. Li. 2000. Cantonese-English code-
switching research in Hong Kong: a Y2K review.
World Englishes, 19(3):305?322.
Gideon S. Mann and Andrew McCallum. 2008.
Generalized expectation criteria for semi-supervised
learning of conditional random fields. In Proceed-
ings of ACL-08: HLT, pages 870?878, Columbus,
Ohio. Association for Computational Linguistics.
22
Gideon S. Mann and Andrew McCallum. 2010.
Generalized expectation criteria for semi-supervised
learning with weakly labeled data. The Journal of
Machine Learning Research, 11:955?984.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Lesley Milroy and Pieter Muysken, editors. 1995. One
speaker, two languages: Cross-disciplinary per-
spectives on code-switching. Cambridge University
Press.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation (SemEval 2013), pages 312?
320, Atlanta, Georgia, USA. Association for Com-
putational Linguistics.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA. Association for Computational
Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT 2013), pages 380?390, Atlanta, Geor-
gia. Association for Computational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Hong Ka San. 2009. Chinese-English code-switching
in blogs by Macao young people. Master?s the-
sis, The University of Edinburgh, Edinburgh, UK.
http://hdl.handle.net/1842/3626.
Latisha Asmaak Shafie and Surina Nayan. 2013.
Languages, code-switching practice and primary
functions of facebook among university students.
Study in English Language Teaching, 1(1):187?
199. http://www.scholink.org/ojs/
index.php/selt.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar. Associ-
ation for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings
of the International Workshop on Semantic Evalu-
ation (SemEval-2014), pages 392?397, Dublin, Ire-
land. Association for Computational Linguistics.
Jochen Weiner, Ngoc Thang Vu, Dominic Telaar, Flo-
rian Metze, Tanja Schultz, Dau-Cheng Lyu, Eng-
Siong Chng, and Haizhou Li. 2012. Integration
of language identification into a recognition system
for spoken conversations containing code-switches.
In Proceedings of the 3rd Workshop on Spoken Lan-
guage Technologies for Under-resourced Languages
(SLTU?12), Cape Town, South Africa. International
Research Center MICA.
Hiroshi Yamaguchi and Kumiko Tanaka-Ishii. 2012.
Text segmentation by language using minimum de-
scription length. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1, pages 969?978.
Association for Computational Linguistics.
23
Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 127?132,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
DCU-UVT: Word-Level Language Classification with Code-Mixed Data
Utsab Barman, Joachim Wagner, Grzegorz Chrupa?a
?
and Jennifer Foster
CNGL Centre for Global Intelligent Content, National Centre for Language Technology
School of Computing, Dublin City University, Dublin, Ireland
?
Tilburg School of Humanities, Department of Communication and Information Sciences
Tilburg University, Tilburg, The Netherlands
{ubarman,jwagner,jfoster}@computing.dcu.ie
G.A.Chrupala@uvt.nl
Abstract
This paper describes the DCU-UVT
team?s participation in the Language Iden-
tification in Code-Switched Data shared
task in the Workshop on Computational
Approaches to Code Switching. Word-
level classification experiments were car-
ried out using a simple dictionary-based
method, linear kernel support vector ma-
chines (SVMs) with and without con-
textual clues, and a k-nearest neighbour
approach. Based on these experiments,
we select our SVM-based system with
contextual clues as our final system and
present results for the Nepali-English and
Spanish-English datasets.
1 Introduction
This paper describes DCU-UVT?s participation
in the shared task Language Identification in
Code-Switched Data (Solorio et al., 2014) at
the Workshop on Computational Approaches to
Code Switching, EMNLP, 2014. The task is to
make word-level predictions (six labels: lang1,
lang2, ne, mixed, ambiguous and other) for mixed-
language user generated content. We submit pre-
dictions for Nepali-English and Spanish-English
data and perform experiments using dictionaries, a
k-nearest neighbour (k-NN) classifier and a linear-
kernel SVM classifier.
In our dictionary-based approach, we investi-
gate the use of different English dictionaries as
well as the training data. In the k-NN based
approach, we use string edit distance, character-
n-gram overlap and context similarity to make
predictions. For the SVM approach, we experi-
ment with context-independent (word, character-
n-grams, length of a word and capitalisation in-
formation) and context-sensitive (adding the pre-
vious and next word as bigrams) features in differ-
ent combinations. We also experiment with adding
features from the k-NN approach and another set
of features from a neural network. Based on per-
formance in cross-validation, we select the SVM
classifier with basic features (word, character-n-
grams, length of a word, capitalisation information
and context) as our final system.
2 Background
While the problem of automatically identify-
ing and analysing code-mixing has been iden-
tified over 30 years ago (Joshi, 1982), it has
only recently drawn wider attention. Specific
problems addressed include language identifica-
tion in multilingual documents, identification of
code-switching points and POS tagging (Solorio
and Liu, 2008b) of code-mixing data. Ap-
proaches taken to the problem of identifying code-
mixing include the use of dictionaries (Nguyen
and Do?gru?oz, 2013; Barman et al., 2014; El-
fardy et al., 2013; Solorio and Liu, 2008b), lan-
guage models (Alex, 2008; Nguyen and Do?gru?oz,
2013; Elfardy et al., 2013), morphological and
phonological analysis (Elfardy et al., 2013; El-
fardy and Diab, 2012) and various machine learn-
ing algorithms such as sequence labelling with
Hidden Markov Models (Farrugia, 2004; Ros-
ner and Farrugia, 2007) and Conditional Random
Fields (Nguyen and Do?gru?oz, 2013; King and
Abney, 2013), as well as word-level classifica-
tion using Naive Bayes (Solorio and Liu, 2008a),
logistic regression (Nguyen and Do?gru?oz, 2013)
and SVMs (Barman et al., 2014), using features
such as word, POS, lemma and character-n-grams.
Language pairs that have been explored include
English-Maltese (Farrugia, 2004; Rosner and Far-
rugia, 2007), English-Spanish (Solorio and Liu,
2008b), Turkish-Dutch (Nguyen and Do?gru?oz,
127
2013), modern standard Arabic-Egyptian di-
alect (Elfardy et al., 2013), Mandarin-English (Li
et al., 2012; Lyu et al., 2010), and English-Hindi-
Bengali (Barman et al., 2014).
3 Data Statistics
The training data provided for this task consists of
tweets. Unfortunately, because of deleted tweets,
the full training set could not be downloaded. Out
of 9,993 Nepali-English training tweets, we were
able to download 9,668 and out of 11,400 Spanish-
English training tweets, we were able to download
11,353. Table 1 shows the token-level statistics of
the two datasets.
Label Nepali-English Spanish-English
lang1 (en) 43,185 76,204
lang2 (ne/es) 59,579 32,477
ne 3,821 2,814
ambiguous 125 341
mixed 112 51
other 34,566 21,813
Table 1: Number of tokens in the Nepali-English
and Spanish-English training data for each label
Nepali (lang2) is the dominant language in
the Nepali-English training data but for Spanish-
English, English (lang1) is dominant. The third
largest group contains tokens with the label other.
These are mentions (@username), punctuation
symbols, emoticons, numbers (except numbers
that represent words such as 2 for to), words in a
language other than lang1 and lang2 and unintel-
ligible words. Named entities (ne) are much less
frequent and mixed language words (e.g. ramri-
ness) and words for which there is not enough con-
text to disambiguate them are rare. Hash tags are
annotated as if the hash symbol was not there, e.g.
#truestory is labelled lang1.
4 Experiments
All experiments are carried out for Nepali-English
data. Later we apply the best approach to Spanish-
English. We train our systems in a five-fold cross-
validation and obtain best parameters based on
average cross-validation results. Cross-validation
splits are made based on users, i.e. we avoid the
occurrence of a user?s tweets both in training and
test splits for each cross-validation run. We ad-
dress the task with the following approaches:
1. a simple dictionary-based classifier,
Resource Accuracy
BNC 43.61
LexNorm 54.60
TrainingData 89.53
TrainingData+BNC+LexNorm 90.71
Table 2: Average cross-validation accuracy of
dictionary-based prediction for Nepali-English
2. classification using supervised machine
learning with k-nearest neighbour, and
3. classification using supervised machine
learning with SVMs.
4.1 Dictionary-Based Detection
We start with a simple dictionary-based approach
using as dictionaries (a) the British National Cor-
pus (BNC) (Aston and Burnard, 1998), (b) Han
et al.?s lexical normalisation dictionary (LexNorm)
(Han et al., 2012) and (c) the training data.
The BNC and LexNorm dictionaries are built by
recording all words occurring in the respective
corpus or word list as English. For the BNC, we
also collect word frequency information. For the
training data, we obtain dictionaries for each of the
six labels and each of the five cross-validation runs
(using the relevant 4/5 of training data).
To make a prediction, we consult all dictionar-
ies. If there are more than one candidate label,
we choose the label for which the frequency for
the query token is highest. To account for the fact
that the BNC is much larger than the training data,
we normalise all frequencies before comparison.
LexNorm has no frequency information, hence it
is added to our system as a simple word list (we
consider the language of a word to be English if it
appears in LexNorm). If a word appears in multi-
ple dictionaries with the same frequency or if the
word does not appear in any dictionary or list, the
predicted language is chosen based on the domi-
nant language(s)/label(s) of the corpus.
We experiment with the individual dictionar-
ies and the combination of all three dictionaries,
among which the combination achieves the high-
est cross-validation accuracy (90.71%). Table 2
shows the results of dictionary-based detection ob-
tained in five-fold cross-validation.
4.2 Classification with k-NN
For Nepali-English, we also experiment with a
simple k-nearest neighbour (k-NN) approach. For
each test item, we select a subset of the training
data using string edit distance and n-gram overlap
128
and choose the majority label of the subset as our
prediction. For efficiency, we first select k
1
items
that share an n-gram with the token to be classi-
fied.
1
The set of k
1
items is then re-ranked ac-
cording to string edit distance to the test item and
the best k
2
matches are used to make a prediction.
Apart from varying k
1
and k
2
, we experiment
with (a) lowercasing strings, (b) including context
by concatenating the previous, current and next
token, and (c) weighting context by first calcu-
lating edit distances for the previous, current and
next token separately and using a weighted aver-
age. The best configuration we found in cross-
validation uses lowercasing with k
1
= 800 and
k
2
= 16 but no context information. It achieves
an accuracy of 94.97%.
4.3 SVM Classification
We experiment with linear kernel SVM classifiers
using Liblinear (Fan et al., 2008). Parameter opti-
misation
2
is performed for each feature set combi-
nation to obtain best cross-validation accuracy.
4.3.1 Basic Features
Following Barman et al. (2014), our basic features
are:
Char-N-Grams (G): We start with a charac-
ter n-gram-based approach (Cavnar and Trenkle,
1994). Following King and Abney (2013), we se-
lect lowercased character n-grams (n=1 to 5) and
the word as the features in our experiments.
Dictionary-Based Labels (D): We use presence
in the dictionary of the 5,000 most frequent words
in the BNC and presence in the LexNorm dictio-
nary as binary features.
3
Length of words (L): We create multiple fea-
tures for token length using a decision tree (J48).
We use length as the only feature to train a deci-
sion tree for each fold and use the nodes obtained
from the tree to create boolean features (Rubino et
al., 2013; Wagner et al., 2014).
1
Starting with n = 5, we decrease n until there are at
least k
1
items and then we randomly remove items added in
the last augmentation step to arrive at exactly k
1
items. (For
n = 0, we randomly sample from the full training data.)
2
C = 2
i
with i = ?15,?14, ..., 10
3
We chose these parameters based on experiments with
each dictionary, combinations of dictionaries and various fre-
quency thresholds. We apply a frequency threshold to the
BNC to increase precision. We rank the words according to
frequency and used the rank as a threshold (e.g. top-5K, top-
10K etc.). With the top 5,000 ranked words and C = 0.25,
we obtained best accuracy (96.40%).
Features Accuracy Features Accuracy
G 96.02 GD 96.27
GL 96.11 GDL 96.32
GC 96.15 GDC 96.20
GLC 96.21 GDLC 96.40
Table 3: Average cross-validation accuracy of 6-
way SVMs on the Nepali-English data set; G =
char-n-gram, L = binary length features, D = dict.-
based labels and C = capitalisation features
Context Accuracy(%)
GDLC + P
1
96.41
GDLC + P
2
96.38
GDLC + N
1
96.41
GDLC + N
2
96.41
GDLC + P
1
+ N
1
96.42
GDLC + P
2
+ N
2
96.41
Table 4: Average cross-validation accuracy of 6-
way SVMs using contextual features for Nepali-
English
Capitalisation (C): We choose 3 boolean
features to encode capitalisation information:
whether any letter in the word is capitalised,
whether all letters in the word are capitalised and
whether the first letter is capitalised.
Context (P
i
and N
j
): We consider the previous
i and next j token to be combined with the current
token, forming an (i+1)-gram and a (j+1)-gram,
which we add as features. Six settings are tested.
Table 4 shows that using the bigrams formed with
the previous and next word are the best combina-
tion for the task (among those tested).
Among the eight combinations of the first four
feature sets that contain the first set (G), Table 3
shows that the 6-way SVM classifier
4
performs
best with all features sets (GDLC), achieving
96.40% accuracy. Adding contextual information
P
i
N
j
to GDLC, Table 4 shows best results for
i=j=1, achieving 96.42% accuracy, only slightly
ahead of the context-independent system.
4.3.2 Neural Network (Elman) and k-NN
Features
We experiment with two additional features sets
not covered by Barman et al. (2014):
Neural Network (Elman): We extract features
from the hidden layer of a recurrent neural net-
4
We also test 3-way SVM classification (lang1, lang2 and
other) and heuristic post-processing, but it does not outper-
form our 6-way classification runs.
129
Systems Accuracy
GDLC 96.40
k-NN 95.10
Elman 89.96
GDLC+k-NN 96.31
GDLC+Elman 96.46
GDLC+k-NN+Elman 96.40
GDLC+P
1
N
1
96.42
k-NN+P
1
N
1
95.11
Elman+P
1
N
1
91.53
GDLC+P
1
N
1
+k-NN 96.33
GDLC+P
1
N
1
+Elman 96.45
GDLC+P
1
N
1
+k-NN+Elman 96.40
Table 5: Average cross-validation accuracy of 6-
way SVMs of combinations of GDLC, k-NN, El-
man and P
1
N
1
features for Nepali-English
work that has been trained to predict the next char-
acter in a string (Chrupa?a, 2014). The 10 most ac-
tive units of the hidden layer for each of the initial
4 bytes and final 4 bytes of each token are bina-
rised by using a threshold of 0.5.
k-Nearest Neighbour (kNN): We obtain fea-
tures from our basic k-NN approach (Section 4.2),
encoding the prediction of the k-NN model with
six binary features (one for each label) and a nu-
meric feature for each label stating the relative
number of votes for the label, e.g. if k
2
= 16
and 12 votes are for lang1 the value of the fea-
ture votes4lang1 will be 0.75. Furthermore, we
add two features stating the minimum and maxi-
mum edit distance between the test token and the
k
2
selected training tokens.
Table 5 shows cross-validation results for these
new feature sets with and without the P
1
N
1
con-
text features. Excluding the GDLC features, we
can see that best accuracy is with k-NN and P
1
N
1
features (95.11%). For Elman features, the accu-
racy is lower (91.53% with context). In combina-
tion with the GDLC features, however, the Elman
features can achieve a small improvement over
the GDLC+P
1
N
1
combination (+0.04 percentage
points): 96.46% accuracy for the GDLC+Elman
setting (without P
1
N
1
features). Furthermore, the
k-NN features do not combine well.
5
4.3.3 Final System and Test Results
At the time of submission of predictions, we had
an error in our GDLC+Elman feature combiner re-
5
A possible explanation may be that the k-NN features
are based on only 3 of 5 folds for the training data (3 folds
are used to make predictions for the 4th set) but 4 of 5 folds
are used for test data predictions in each cross-validation run.
Tweets
Token-Level Tweet-Level
Nepali-English 96.3 95.8
Spanish-English 84.4 80.4
Surprise Genre
Token-Level Post-Level
Nepali-English 85.6 77.5
Spanish-English 94.4 80.0
Table 6: Test set results (overall accuracy) for
Nepali-English and Spanish-English tweet data
and surprise genre
sulting in slightly lower performance. Therefore,
we selected SVM-GDLC-P
1
N
1
as our final ap-
proach and trained the final two systems using the
full training data for Nepali-English and Spanish-
English respectively. While we knew that C =
0.125 is best for Nepali-English from our experi-
ments, we had to re-tune parameter C for Spanish-
English using cross-validation on the training data.
We found best accuracy of 94.16% for Spanish-
English with C = 128. Final predictions for the
test sets are made using these systems.
Table 6 shows the test set results. The test
set for this task is divided into tweets and a sur-
prise genre. For the tweets, we achieve 96.3%
and 84.4% accuracy (overall token-level accuracy)
in Nepali-English and in Spanish-English respec-
tively. For this surprise genre (a collection of posts
from Facebook and blogs), we achieve 85.6% for
Nepali-English and 94.4% for Spanish-English.
5 Conclusion
To summarise, we achieved reasonable accuracy
with a 6-way SVM classifier by employing basic
features only. We found that using dictionaries
is helpful, as are contextual features. The perfor-
mance of the k-NN classifier is also notable: it is
only 1.45 percentage points behind the final SVM-
based system (in terms of cross-validation accu-
racy). Adding neural network features can further
increase the accuracy of systems.
Briefly opening the test files to check for for-
matting issues, we notice that the surprise genre
data contains language-specific scripts that could
easily be addressed in an English vs. non-English
scenario.
Acknowledgments
This research is supported by the Science Founda-
tion Ireland (Grant 12/CE/I2267) as part of CNGL
(www.cngl.ie) at Dublin City University.
130
References
Beatrice Alex. 2008. Automatic detection of English
inclusions in mixed-lingual data with an application
to parsing. Ph.D. thesis, School of Informatics, The
University of Edinburgh, Edinburgh, UK.
Guy Aston and Lou Burnard. 1998. The BNC hand-
book: exploring the British National Corpus with
SARA. Capstone.
Utsab Barman, Amitava Das, Joachim Wagner, and
Jennifer Foster. 2014. Code-mixing: A challenge
for language identification in the language of so-
cial media. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
William B. Cavnar and John M. Trenkle. 1994. N-
gram-based text categorization. In Theo Pavlidis,
editor, Proceedings of SDAIR-94, Third Annual
Symposium on Document Analysis and Information
Retrieval, pages 161?175.
Grzegorz Chrupa?a. 2014. Normalizing tweets with
edit scripts and recurrent neural embeddings. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (Volume 2:
Short Papers), pages 680?686, Baltimore, Mary-
land, June. Association for Computational Linguis-
tics.
Heba Elfardy and Mona Diab. 2012. Token level
identification of linguistic code switching. In Pro-
ceedings of Proceedings of COLING 2012: Posters
(the 24th International Conference on Computa-
tional Linguistics), pages 287?296, Mumbai, India.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in Ara-
bic. In Natural Language Processing and Informa-
tion Systems, pages 412?416. Springer.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A library for large linear classification. Journal of
Machine Learning Research, 9:1871?1874.
Paulseph-John Farrugia. 2004. TTS pre-processing is-
sues for mixed language support. In Proceedings
of CSAW?04, the second Computer Science Annual
Workshop, pages 36?41. Department of Computer
Science & A.I., University of Malta.
Bo Han, Paul Cook, and Timothy Baldwin. 2012.
Automatically constructing a normalisation dictio-
nary for microblogs. In Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natu-
ral Language Learning, pages 421?432. Association
for Computational Linguistics.
Aravind K. Joshi. 1982. Processing of sentences with
intra-sentential code-switching. In J. Horeck?y, ed-
itor, Proceedings of the 9th conference on Compu-
tational linguistics - Volume 1 (COLING?82), pages
145?150. Academia Praha, North-Holland Publish-
ing Company.
Ben King and Steven Abney. 2013. Labeling the lan-
guages of words in mixed-language documents us-
ing weakly supervised methods. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1110?
1119, Atlanta, Georgia, June. Association for Com-
putational Linguistics.
Ying Li, Yue Yu, and Pascale Fung. 2012. A
mandarin-english code-switching corpus. In Nico-
letta Calzolari (Conference Chair), Khalid Choukri,
Thierry Declerck, Mehmet Uur Doan, Bente Mae-
gaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings
of the Eight International Conference on Language
Resources and Evaluation (LREC?12), Istanbul,
Turkey, may. European Language Resources Asso-
ciation (ELRA).
Dau-Cheng Lyu, Tien Ping Tan, Engsiong Chng, and
Haizhou Li. 2010. SEAME: A Mandarin-English
code-switching speech corpus in South-East Asia.
In INTERSPEECH 2010, 11th Annual Conference
of the International Speech Communication Asso-
ciation, volume 10, pages 1986?1989, Makuhari,
Chiba, Japan. ISCA Archive.
Dong Nguyen and A. Seza Do?gru?oz. 2013. Word
level language identification in online multilingual
communication. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013), pages 857?862, Seattle,
Washington, USA, October. Association for Com-
putational Linguistics.
Mike Rosner and Paulseph-John Farrugia. 2007. A
tagging algorithm for mixed language identifica-
tion in a noisy domain. In INTERSPEECH-2007,
8th Annual Conference of the International Speech
Communication Association, pages 190?193. ISCA
Archive.
Raphael Rubino, Joachim Wagner, Jennifer Foster, Jo-
hann Roturier, Rasoul Samad Zadeh Kaljahi, and
Fred Hollowood. 2013. DCU-Symantec at the
WMT 2013 quality estimation shared task. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 392?397, Sofia, Bulgaria.
Association for Computational Linguistics.
Thamar Solorio and Yang Liu. 2008a. Learning to pre-
dict code-switching points. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 973?981. Association for
Computational Linguistics.
Thamar Solorio and Yang Liu. 2008b. Part-of-speech
tagging for English-Spanish code-switched text. In
131
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 1051?
1060. Association for Computational Linguistics.
Thamar Solorio, Elizabeth Blair, Suraj Maharjan, Steve
Bethard, Mona Diab, Mahmoud Gonheim, Abdelati
Hawwari, Fahad AlGhamdi, Julia Hirshberg, Alison
Chang, and Pascale Fung. 2014. Overview for the
first shared task on language identification in code-
switched data. In Proceedings of the First Workshop
on Computational Approaches to Code-Switching.
EMNLP 2014, Conference on Empirical Methods in
Natural Language Processing, Doha, Qatar, Octo-
ber. Association for Computational Linguistics.
Joachim Wagner, Piyush Arora, Santiago Cortes, Utsab
Barman, Dasha Bogdanova, Jennifer Foster, and
Lamia Tounsi. 2014. DCU: Aspect-based polarity
classification for SemEval task 4. In Proceedings of
the International Workshop on Semantic Evaluation
(SemEval-2014), pages 392?397, Dublin, Ireland,
August. Association for Computational Linguistics.
132
