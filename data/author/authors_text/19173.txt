Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 15?19, Dublin, Ireland, August 23-29 2014.
Creating Custom Taggers by Integrating Web Page Annotation and
Machine Learning
Srikrishna Raamadhurai
?
Oskar Kohonen
?
Teemu Ruokolainen
??
?
Aalto University, Department of Information and Computer Science, Finland
??
Aalto University, Department of Signal Processing and Acoustics, Finland
firstname.lastname@aalto.fi
Abstract
We present an on-going work on a software package that integrates discriminative machine learn-
ing with the open source WebAnnotator system of Tannier (2012). The WebAnnotator system
allows users to annotate web pages within their browser with custom tag sets. Meanwhile, we
integrate the WebAnnotator system with a machine learning package which enables automatic
tagging of new web pages. We hope the software evolves into a useful information extraction
tool for motivated hobbyists who have domain expertise on their task of interest but lack machine
learning or programming knowledge. This paper presents the system architecture, including the
WebAnnotator-based front-end and the machine learning component. The system is available
under an open source license.
1 Introduction
A typical development cycle of a natural language processing (NLP) tool involves several different ex-
perts whose time is often limited as well as expensive. In particular, rule-based systems need experts to
construct the rules, while data-driven systems require domain experts to produce annotated training data
and machine learning experts to train the systems. Because of the required investment, tasks which lack
commercial or academic interest are often left completely without applicable tools. Nevertheless, we
believe that there exist many relatively simple tasks where necessary annotation for a machine learning
system could be produced by motivated hobbyists who possess domain expertise but lack machine learn-
ing or programming knowledge. For example, consider identifying fields in classified ads such as product
name, dimensions and price, or segmenting individual posts in a web forum. To this end, we present a
software package that integrates discriminative machine learning with the open source WebAnnotator
system (Tannier, 2012).
The combination of an annotation tool and machine learning is, of course, not a new idea, as it goes
back at least to the Alembic system (Day et al., 1997), which was developed to accelerate the process
of tailoring NLP tools to new domains, languages, and tasks, by attempting to reduce the work load of
human annotators by employing pre-taggers learned from previously annotated data. Despite these ideas
being around for a long time, they do not seem to have been integrated into a web-browser previously.
Since a large amount of information is consumed using the web-browser, it is desirable to be able to train
and apply automatic analysis tools directly within that context.
The paper is organized as follows. In Section 2, we review how the system is used, its general archi-
tecture and details related to how the machine learning is implemented. Section 4 provides discussion
and conclusions on the work.
2 System
In this section we present in some detail the system that integrates discriminative machine learning
with the open source WebAnnotator (Tannier, 2012). We review the usage of the system, its software
architecture, the central aspects related to how machine learning is applied: the employed Conditional
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
15
Random Fields-method, indexing and pre-processing web-pages, how training sets are constructed, and
the applied feature extraction. Finally, we review how the trained system is applied to new web-pages.
The latest version of the software can be found at https://github.com/okohonen/
semantify.
2.1 Overview of Usage
The system is installed as an add-on to the Firefox browser. Subsequently, it can be activated for any
web page. The user can train several different models by indicating which model a particular annotation
belongs to. The user can also define the tag set used for annotation. To annotate, the user highlights
parts of the page with the mouse and selects the desired tag from the shown menu. Assigned annotations
are denoted by colors. This process is presented in Figure 1 (a). When the user is done, she stores the
annotated page as shown in Figure 1 (b). The system then stores the annotated page in its page index.
Meanwhile, in the background, the system automatically produces a training set that contains all the
pages annotated so far and learns a tagger. The user can ask the system to tag a new page as shown in
Figure 1 (c), in which case the system pre-processes and tags the current page using the latest trained
model. The system then adds the annotations matching the output of the machine learning model to the
viewed web page. The automatically assigned tags are visually distinct from the manually annotated
ones (lighter color scheme). The automatic taggings can then be corrected manually and added to the
training set. An example of an automatically annotated page is shown in figure 1 (d).
2.2 Overview of Architecture
The system architecture consists of two main components, namely, 1) an add-on to the Firefox-browser
that allows annotation of web pages directly in the browser window, and 2) a machine learning compo-
nent for annotating new pages. The browser add-on extends the WebAnnotator system (Tannier, 2012)
by integrating it with the machine learning component and with functionality to show and edit the tags
produced by the trained taggers. The machine learning component indexes the annotated web pages, pre-
processes them to produce training sets of sentences, and trains models that can then be applied to new
data. The Firefox add-on is implemented in Javascript and XUL while the machine learning component
is implemented in Python. To bridge the language gap they communicate using XMLHTTPRequest.
The machine learning component implements the well-known conditional random field (CRF) method,
a discriminative modeling framework for sequence labeling (Lafferty et al., 2001).
2.3 Conditional Random Fields
Our system implements the linear-chain CRF model (Lafferty et al., 2001) which can inherently accom-
modate the arbitrary, overlapping features described below in Section 2.7. The CRF model is estimated
based on the available training set of exemplar input-output sequence pairs. Test instances are decoded
using the standard Viterbi search (Lafferty et al., 2001).
CRF parameter estimation is most commonly associated with the maximum likelihood approach em-
ployed by Lafferty et al.(2001). However, in our system, we rely on the averaged perceptron algorithm
following Collins (2002a). (Note that the CRFs correspond to discriminatively trained hidden Markov
models, and Collins (2002a) employs the latter terminology.) We apply the averaged perceptron learning
approach for its simplicity and competitive performance (Zhang and Clark, 2011).
2.4 Web Page Index
The web page index in the machine learning component stores the annotated pages using the internal
format of WebAnnotator, which is simply the original HTML-page augmented with <span>-tags to
encode and visualize the annotations. Apart from annotated pages, it is also possible to index unannotated
pages if one has a particular set of pages that are to be tagged by the model.
16
Figure 1: Sample screenshots of the tool depicting typical scenarios.
2.5 Pre-processing
We parse the HTML using the Beautiful Soup-library
1
, extracting the visible text parts. Subsequently, we
tokenize the text by splitting at white space and at non-alphanumeric characters. The token sequence is
grouped into sentences based on punctuation and HTML-tags. We consider that if an HTML-tag defines
a new block then it also starts a new sentence (e.g. <div> starts a block, while <span> does not). For
each token position we apply feature functions that are described in detail in Section 2.7.
The user?s annotations are stored as span-tags that are identified by their class attributes. The
span-tags are parsed so that the label information is extracted, but they are ignored when calculating
the feature functions which should be identical regardless of how the page is annotated. If the user has
not assigned any label we assume a default Outside class.
2.6 Building the Training Sets
The CRF parameter estimation requires training and development sets which must be drawn from an-
notated web pages in the page index. A special characteristic of the data set is that the label distribu-
tion is typically very skewed towards many Outside taggings and few non-Outside taggings. To
ensure that the development set gets sufficiently many non-Outside taggings, we use use a modulus-
based scheme that assigns 10% of the sentences to the development set while making sure that there are
enough sentences containing taggings from each class. The training set and development set are formed
by concatenating the preprocessed files for each individual page.
2.7 Feature Extraction
In addition to standard first-order state transition features, the CRF model includes feature functions
which operate on the token sequence of the sentence and the HTML-tree of the page. We use ortho-
graphic features and HTML-features which consider the token string and HTML-tree, respectively. For
orthographic features, we use the word in lower case, and generalized character-based following Collins
(2002b). We extract features based on the following properties of the current node in the HTML-tree:
parent nodes and the class-attribute. For the parent nodes we calculate both individual features for im-
1
http://www.crummy.com/software/BeautifulSoup/
17
mediate parents as well as very specific features that concatenates all parents up to the body-tag. For
the class-attribute we provide it both as it is and apply the same generalization functions as in the or-
thographic feature set. One could also extract features from other attributes than class. However, we
suspect that they are less informative for the tagging task compared to the class-attribute which is often
used to indicate structural properties of the page content. We also window the features to consider the
previous and next positions in the input.
2.8 Annotating New Pages Automatically
When the user asks the system to tag a new page, the current page is sent to the machine learning
component for preprocessing. The latest trained CRF is applied to the preprocessed page and each token
is assigned a tag. We produce <span>-tags similar to the internal format used by WebAnnotator, but
with distinct attributes so the browser extension can distinguish manual and automatic annotation. To
produce the modified HTML-page, we need to know the position in the HTML-string for each token in
the preproessed file. In order to achieve this, we create an index of the HTML-string during preprocessing
that maps every token to its original position in the string.
3 Discussion
For a software tool of the presented kind, the key performance measures are: accuracy on the task at
hand, as a function of the number of annotations; and training time. As both measures are specific to task
and implementation, addressing them both experimentally would require a large number of experiments
which is not feasible in the allowed space. However, for illustration purposes, we will present a simple
example application.
In general, for the proposed system to yield high accuracy, it needs to learn the desired categories from
a few annotated examples. This requires input features that predict the desired target category well. The
key benefit of the discriminative training employed is the ability to use a large set of rich and overlapping
features. This allows the construction of feature sets that yield good performance on several different
tasks, reducing the need for task-specific feature engineering which requires domain and programming
expertise. Future work includes identifying additional features that yield good performance in a number
of different tasks.
For training time, it would be ideal if the system could be trained in real time, that is, once the user
has submitted an annotated web page to the system, the training would be completed when the user
wants to apply the model to a new web page. This would require training times on the order of a few
seconds. Training time depends mostly on the employed classifier, training algorithm, and the number of
sequences and tokens in the training set. We had assumed that training time would not be an issue, even
for a naive implementation, because a typical user would only gather small data sets. However, it turned
out that while the annotation may be small in the number of annotated web pages, typical web pages
were larger in terms of token counts than we had anticipated and more advanced training techniques may
be needed to reach real-time performance.
To illustrate the properties related to tagging web pages using the current implementation, we present
a simple example application of the system to the task of extracting fields from the Internet Movie
Database (IMDB).
2
We annotated 50 web pages from IMDB, each describing a different movie. The
following fields were annotated: director, genre, title, rating, theme, writer, and release year. It should
be noted that this task is from the easier end of the spectrum, as the fields can be extracted with a high
accuracy using the markup structure alone. For experimental purposes, we performed cross-validation
with training, development, and test sets constructed from 36, 4, and 10 web pages respectively. The
system yielded the following token F-scores by category, director: 73%, genre: 99%, title: 86%, rating:
100%, theme: 100%, writer: 80%, and release year: 100%. This level of performance is promising,
as several fields were extracted with perfect, or near perfect accuracy. The training times for the 36-
page training sets varied between 1.5 and 3.5 minutes on a standard desktop computer (Intel i5-2500
with 16Gb RAM). The variance in training time is explained by the employed stopping criterion which
2
http://www.imdb.com
18
terminates training based on the performance on the development set, resulting in varying numbers of
passes over the training data. In the above example task, the training set sizes were on average: 22K
sequences, 133K tokens, and 30K features.
The empirical training times are longer than what could be considered real-time performance. The
goal of the current implementation has been accuracy rather than execution time, and for the latter, there
is certainly room for improvement. However, for real-time performance, the improvement needed is
large enough that a different training procedure may be necessary. A promising approach, that suits the
setting well, is using online training, such that one would only train on the latest submitted web-page.
Furthermore, it is usually the case, that the non-Outside annotation is concentrated on a fairly small
subpart of the web page. This structure could be utilized to reduce computational cost. These approaches
will be evaluated in future work.
4 Conclusions
We presented on-going work on a software package that integrates discriminative machine learning with
the open source WebAnnotator system of Tannier (2012). The system allows users to annotate web
pages directly within their web browser and train a machine learning tagger applicable for annotating
new web pages. We hope the software evolves into a useful information extraction tool for motivated
hobbyists who have domain expertise on their task of interest but lack machine learning or programming
knowledge.
In future work, we plan to investigate the following aspects of the system. The utility of the system
should be evaluated in real-life tasks and for the target user group. The machine learning component
could then be improved further based on user experience. Perhaps most importantly, the extracted fea-
tures can be improved using both generic and task-specific features. Also, while the system currently
applies only supervised learning, it would be a natural setting to apply semi-supervised learning.
Acknowledgements
The work was funded by an Exploratory Research Project grant from Aalto Science Institute, the
Academy of Finland research project on Multimodal Language Technology, Langnet (Finnish doctoral
programme in language studies), and the Academy of Finland under the Finnish Centre of Excellence
Program 2012?2017 (grant no. 251170).
References
Michael Collins. 2002a. Discriminative training methods for hidden Markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the ACL-02 conference on Empirical methods in natural lan-
guage processing, volume 10, pages 1?8.
Michael Collins. 2002b. Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.
In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 489?496.
Association for Computational Linguistics.
David Day, John Aberdeen, Lynette Hirschman, Robyn Kozierok, Patricia Robinson, and Marc Vilain. 1997.
Mixed-initiative development of language processing systems. In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing, ANLC ?97, pages 348?355, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
John Lafferty, Andrew McCallum, and Fernando C.N. Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference
on Machine Learning, pages 282?289.
Xavier Tannier. 2012. WebAnnotator, an Annotation Tool for Web Pages. In Proceedings of the 8th International
Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, May.
Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search.
Computational Linguistics, 37(1):105?151.
19
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 74?78,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Accelerated Estimation of Conditional Random Fields using a
Pseudo-Likelihood-inspired Perceptron Variant
Teemu Ruokolainen
a
Miikka Silfverberg
b
Mikko Kurimo
a
Krister Lind?n
b
a
Department of Signal Processing and Acoustics, Aalto University, firstname.lastname@aalto.fi
b
Department of Modern Languages, University of Helsinki, firstname.lastname@helsinki.fi
Abstract
We discuss a simple estimation approach
for conditional random fields (CRFs). The
approach is derived heuristically by defin-
ing a variant of the classic perceptron al-
gorithm in spirit of pseudo-likelihood for
maximum likelihood estimation. The re-
sulting approximative algorithm has a lin-
ear time complexity in the size of the la-
bel set and contains a minimal amount of
tunable hyper-parameters. Consequently,
the algorithm is suitable for learning CRF-
based part-of-speech (POS) taggers in
presence of large POS label sets. We
present experiments on five languages.
Despite its heuristic nature, the algorithm
provides surprisingly competetive accura-
cies and running times against reference
methods.
1 Introduction
The conditional random field (CRF) model (Laf-
ferty et al., 2001) has been successfully applied
to several sequence labeling tasks in natural lan-
guage processing, including part-of-speech (POS)
tagging. In this work, we discuss accelerating the
CRF model estimation in presence of a large num-
ber of labels, say, hundreds or thousands. Large la-
bel sets occur in POS tagging of morphologically
rich languages (Erjavec, 2010; Haverinen et al.,
2013).
CRF training is most commonly associated with
the (conditional) maximum likelihood (ML) crite-
rion employed in the original work of Lafferty et
al. (2001). In this work, we focus on an alternative
training approach using the averaged perceptron
algorithm of Collins (2002). While yielding com-
petitive accuracy (Collins, 2002; Zhang and Clark,
2011), the perceptron algorithm avoids extensive
tuning of hyper-parameters and regularization re-
quired by the stochastic gradient descent algo-
rithm employed in ML estimation (Vishwanathan
et al., 2006). Additionally, while ML and percep-
tron training share an identical time complexity,
the perceptron is in practice faster due to sparser
parameter updates.
Despite its simplicity, running the perceptron al-
gorithm can be tedious in case the data contains
a large number of labels. Previously, this prob-
lem has been addressed using, for example, k-best
beam search (Collins and Roark, 2004; Zhang and
Clark, 2011; Huang et al., 2012) and paralleliza-
tion (McDonald et al., 2010). In this work, we
explore an alternative strategy, in which we mod-
ify the perceptron algorithm in spirit of the classic
pseudo-likelihood approximation for ML estima-
tion (Besag, 1975). The resulting novel algorithm
has linear complexity w.r.t. the label set size and
contains only a single hyper-parameter, namely,
the number of passes taken over the training data
set.
We evaluate the algorithm, referred to as the
pseudo-perceptron, empirically in POS tagging
on five languages. The results suggest that the
approach can yield competitive accuracy com-
pared to perceptron training accelerated using a
violation-fixed 1-best beam search (Collins and
Roark, 2004; Huang et al., 2012) which also pro-
vides a linear time complexity in label set size.
The rest of the paper is as follows. In Section 2,
we describe the pseudo-perceptron algorithm and
discuss related work. In Sections 3 and 4, we
describe our experiment setup and the results, re-
spectively. Conclusions on the work are presented
in Section 5.
2 Methods
2.1 Pseudo-Perceptron Algorithm
The (unnormalized) CRF model for input and
output sequences x = (x
1
, x
2
, . . . , x
|x|
) and
74
y = (y
1
, y
2
, . . . , y
|x|
), respectively, is written as
p (y |x;w) ? exp
(
w ??(y, x)
)
=
|x|
?
i=n
exp
(
w ? ?(y
i?n
, . . . , y
i
, x, i)
)
,
(1)
where w denotes the model parameter vector, ?
the vector-valued global feature extracting func-
tion, ? the vector-valued local feature extracting
function, and n the model order. We denote the
tag set as Y . The model parameters w are esti-
mated based on training data, and test instances
are decoded using the Viterbi search (Lafferty et
al., 2001).
Given the model definition (1), the param-
eters w can be estimated in a straightforward
manner using the structured perceptron algo-
rithm (Collins, 2002). The algorithm iterates
over the training set a single instance (x, y) at
a time and updates the parameters according
to the rule w
(i)
= w
(i?1)
+ ??(x, y, z), where
??(x, y, z) for the ith iteration is written as
??(x, y, z) = ?(x, y)??(x, z). The predic-
tion z is obtained as
z = arg max
u?Y(x)
w ??(x, u) (2)
by performing the Viterbi search over
Y(x) = Y ? ? ? ? ? Y , a product of |x| copies
of Y . In case the perceptron algorithm yields
a small number of incorrect predictions on the
training data set, the parameters generalize well
to test instances with a high probability (Collins,
2002).
The time complexity of the Viterbi search is
O(|x| ? |Y|
n+1
). Consequently, running the per-
ceptron algorithm can become tedious if the la-
bel set cardinality |Y| and/or the model order n
is large. In order to speed up learning, we define
a variant of the algorithm in the spirit of pseudo-
likelihood (PL) learning (Besag, 1975). In anal-
ogy to PL, the key idea of the pseudo-perceptron
(PP) algorithm is to obtain the required predictions
over single variables y
i
while fixing the remaining
variables to their true values. In other words, in-
stead of using the Viterbi search to find the z as in
(2), we find a z
?
for each position i ? 1..|x| as
z
?
= arg max
u?Y
?
i
(x)
w ??(x, u) , (3)
with Y
?
i
(x) = {y
1
}?? ? ??{y
i?1
}?Y?{y
i+1
}?
? ? ? ? {y
|x|
}. Subsequent to training, test instances
are decoded in a standard manner using the Viterbi
search.
The appeal of PP is that the time complexity
of search is reduced to O(|x| ? |Y|), i.e., linear
in the number of labels in the label set. On the
other hand, we no longer expect the obtained pa-
rameters to necessarily generalize well to test in-
stances.
1
Consequently, we consider PP a heuris-
tic estimation approach motivated by the rather
well-established success of PL (Kor
?
c and F?rstner,
2008; Sutton and McCallum, 2009).
2
Next, we study yet another heuristic pseudo-
variant of the perceptron algorithm referred to as
the piecewise-pseudo-perceptron (PW-PP). This
algorithm is analogous to the piecewise-pseudo-
likelihood (PW-PL) approximation presented by
Sutton and McCallum (2009). In this variant, the
original graph is first split into smaller, possibly
overlapping subgraphs (pieces). Subsequently, we
apply the PP approximation to the pieces. We em-
ploy the approach coined factor-as-piece by Sut-
ton and McCallum (2009), in which each piece
contains n + 1 consecutive variables, where n is
the CRF model order.
The PW-PP approach is motivated by the results
of Sutton and McCallum (2009) who found PW-
PL to increase stability w.r.t. accuracy compared
to plain PL across tasks. Note that the piecewise
approximation in itself is not interesting in chain-
structured CRFs, as it results in same time com-
plexity as standard estimation. Meanwhile, the
PW-PP algorithm has same time complexity as PP.
2.2 Related work
Previously, impractical running times of percep-
tron learning have been addressed most notably
using the k-best beam search method (Collins and
Roark, 2004; Zhang and Clark, 2011; Huang et
al., 2012). Here, we consider the ?greedy? 1-best
beam search variant most relevant as it shares the
time complexity of the pseudo search. Therefore,
in the experimental section of this work, we com-
pare the PP and 1-best beam search.
We are aware of at least two other learning ap-
proaches inspired by PL, namely, the pseudo-max
and piecewise algorithms of Sontag et al. (2010)
and Alahari et al. (2010), respectively. Com-
pared to these approaches, the PP algorithm pro-
vides a simpler estimation tool as it avoids the
1
We leave formal treatment to future work.
2
Meanwhile, note that pseudo-likelihood is a consistent
estimator (Gidas, 1988; Hyv?rinen, 2006).
75
hyper-parameters involved in the stochastic gradi-
ent descent algorithms as well as the regularization
and margin functions inherent to the approaches of
Alahari et al. (2010) and Sontag et al. (2010). On
the other hand, Sontag et al. (2010) show that the
pseudo-max approach achieves consistency given
certain assumptions on the data generating func-
tion. Meanwhile, as discussed in previous section,
we consider PP a heuristic and do not provide any
generalization guarantees. To our understanding,
Alahari et al. (2010) do not provide generalization
guarantees for their algorithm.
3 Experimental Setup
3.1 Data
For a quick overview of the data sets, see Table 1.
Penn Treebank. The first data set we consider
is the classic Penn Treebank. The complete tree-
bank is divided into 25 sections of newswire text
extracted from the Wall Street Journal. We split
the data into training, development, and test sets
using the sections 0-18, 19-21, and 22-24, accord-
ing to the standardly applied division introduced
by Collins (2002).
Multext-East. The second data we consider is
the multilingual Multext-East (Erjavec, 2010) cor-
pus. The corpus contains the novel 1984 by
George Orwell. From the available seven lan-
guages, we utilize the Czech, Estonian and Ro-
manian sections. Since the data does not have a
standard division to training and test sets, we as-
sign the 9th and 10th from each 10 consecutive
sentences to the development and test sets, respec-
tively. The remaining sentences are assigned to the
training sets.
Turku Dependency Treebank. The third data
we consider is the Finnish Turku Dependency
Treebank (Haverinen et al., 2013). The treebank
contains text from 10 different domains. We use
the same data split strategy as for Multext East.
3.2 Reference Methods
We compare the PP and PW-PP algorithms with
perceptron learning accelerated using 1-best beam
search modified using the early update rule
(Huang et al., 2012). While Huang et al. (2012)
experimented with several violation-fixing meth-
ods (early, latest, maximum, hybrid), they ap-
peared to reach termination at the same rate in
lang. train. dev. test tags train. tags
eng 38,219 5,527 5,462 45 45
rom 5,216 652 652 405 391
est 5,183 648 647 413 408
cze 5,402 675 675 955 908
fin 5,043 630 630 2,355 2,141
Table 1: Overview on data. The training (train.),
development (dev.) and test set sizes are given in
sentences. The columns titled tags and train. tags
correspond to total number of tags in the data set
and number of tags in the training set, respectively.
POS tagging. Our preliminary experiments using
the latest violation updates supported this. Conse-
quently, we employ the early updates.
We also provide results using the CRFsuite
toolkit (Okazaki, 2007), which implements a 1st-
order CRF model. To best of our knowledge,
CRFsuite is currently the fastest freely available
CRF implementation.
3
In addition to the averaged
perceptron algorithm (Collins, 2002), the toolkit
implements several training procedures (Nocedal,
1980; Crammer et al., 2006; Andrew and Gao,
2007; Mejer and Crammer, 2010; Shalev-Shwartz
et al., 2011). We run CRFsuite using these algo-
rithms employing their default parameters and the
feature extraction scheme and stopping criterion
described in Section 3.3. We then report results
provided by the most accurate algorithm on each
language.
3.3 Details on CRF Training and Decoding
While the methods discussed in this work are ap-
plicable for nth-order CRFs, we employ 1st-order
CRFs in order to avoid overfitting the relatively
small training sets.
We employ a simple feature set including word
forms at position t? 2, . . . , t+ 2, suffixes of word
at position t up to four letters, and three ortho-
graphic features indicating if the word at position
t contains a hyphen, capital letter, or a digit.
All the perceptron variants (PP, PW-PP, 1-best
beam search) initialize the model parameters with
zero vectors and process the training instances in
the order they appear in the corpus. At the end
of each pass, we apply the CRFs using the latest
averaged parameters (Collins, 2002) to the devel-
opment set. We assume the algorithms have con-
verged when the model accuracy on development
3
See benchmark results at http://www.chokkan.
org/software/crfsuite/benchmark.html
76
has not increased during last three iterations. Af-
ter termination, we apply the averaged parameters
yielding highest performance on the development
set to test instances.
Test and development instances are decoded us-
ing a combination of Viterbi search and the tag
dictionary approach of Ratnaparkhi (1996). In this
approach, candidate tags for known word forms
are limited to those observed in the training data.
Meanwhile, word forms that were unseen during
training consider the full label set.
3.4 Software and Hardware
The experiments are run on a standard desktop
computer. We use our own C++-based implemen-
tation of the methods discussed in Section 2.
4 Results
The obtained training times and test set accuracies
(measured using accuracy and out-of-vocabulary
(OOV) accuracy) are presented in Table 2. The
training CPU times include the time (in minutes)
consumed by running the perceptron algorithm
variants as well as evaluation of the development
set accuracy. The column labeled it. corresponds
to the number of passes over training set made by
the algorithms before termination.
We summarize the results as follows. First, PW-
PP provided higher accuracies compared to PP on
Romanian, Czech, and Finnish. The differences
were statistically significant
4
on Czech. Second,
while yielding similar running times compared
to 1-best beam search, PW-PP provided higher
accuracies on all languages apart from Finnish.
The differences were significant on Estonian and
Czech. Third, while fastest on the Penn Treebank,
the CRFsuite toolkit became substantially slower
compared to PW-PP when the number of labels
were increased (see Czech and Finnish). The dif-
ferences in accuracies between the best perform-
ing CRFsuite algorithm and PP and PW-PP were
significant on Czech.
5 Conclusions
We presented a heuristic perceptron variant for
estimation of CRFs in the spirit of the classic
4
We establish significance (with confidence level 0.95)
using the standard 1-sided Wilcoxon signed-rank test per-
formed on 10 randomly divided, non-overlapping subsets of
the complete test sets.
method it. time (min) acc. OOV
English
PP 9 6 96.99 87.97
PW-PP 10 7 96.98 88.11
1-best beam 17 8 96.91 88.33
Pas.-Agg. 9 1 97.01 88.68
Romanian
PP 9 8 96.81 83.66
PW-PP 8 7 96.91 84.38
1-best beam 17 10 96.88 85.32
Pas.-Agg. 13 9 97.06 84.69
Estonian
PP 10 8 93.39 78.10
PW-PP 8 6 93.35 78.66
1-best beam 23 15 92.95 75.65
Pas.-Agg. 15 12 93.27 77.63
Czech
PP 11 26 89.37 70.67
PW-PP 16 41 89.84 72.52
1-best beam 14 19 88.95 70.90
Pegasos 15 341 90.42 72.59
Finnish
PP 11 58 87.09 58.58
PW-PP 11 56 87.16 58.50
1-best beam 21 94 87.38 59.29
Pas.-Agg. 16 693 87.17 57.58
Table 2: Results. We report CRFsuite results pro-
vided by most accurate algorithm on each lan-
guage: the Pas.-Agg. and Pegasos refer to the al-
gorithms of Crammer et al. (2006) and Shalev-
Shwartz et al. (2011), respectively.
pseudo-likelihood estimator. The resulting ap-
proximative algorithm has a linear time complex-
ity in the label set cardinality and contains only
a single hyper-parameter, namely, the number of
passes taken over the training data set. We eval-
uated the algorithm in POS tagging on five lan-
guages. Despite its heuristic nature, the algo-
rithm provided competetive accuracies and run-
ning times against reference methods.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language stud-
ies) and the Academy of Finland under the grant
no 251170 (Finnish Centre of Excellence Pro-
gram (2012-2017)). We would like to thank Dr.
Onur Dikmen for the helpful discussions during
the work.
77
References
Karteek Alahari, Chris Russell, and Philip H.S. Torr.
2010. Efficient piecewise learning for conditional
random fields. In Computer Vision and Pattern
Recognition (CVPR), 2010 IEEE Conference on,
pages 895?901.
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of L
1
-regularized log-linear models. In Proceed-
ings of the 24th international conference on Ma-
chine learning, pages 33?40.
Julian Besag. 1975. Statistical analysis of non-lattice
data. The statistician, pages 179?195.
Michael Collins and Brian Roark. 2004. Incremental
parsing with the perceptron algorithm. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 111.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing, volume 10, pages 1?8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. The Journal of Ma-
chine Learning Research, 7:551?585.
Toma? Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Basilis Gidas. 1988. Consistency of maximum like-
lihood and pseudo-likelihood estimators for Gibbs
distributions. In Stochastic differential systems,
stochastic control theory and applications, pages
129?145.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missil?,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the
Turku Dependency Treebank. Language Resources
and Evaluation.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151.
Aapo Hyv?rinen. 2006. Consistency of pseudolike-
lihood estimation of fully visible Boltzmann ma-
chines. Neural Computation, 18(10):2283?2292.
Filip Kor
?
c and Wolfgang F?rstner. 2008. Approximate
parameter learning in conditional random fields: An
empirical investigation. Pattern Recognition, pages
11?20.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed training strategies for the structured per-
ceptron. In Human Language Technologies: The
2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 456?464.
Avihai Mejer and Koby Crammer. 2010. Confidence
in structured-prediction using confidence-weighted
models. In Proceedings of the 2010 conference on
empirical methods in natural language processing,
pages 971?981.
Jorge Nocedal. 1980. Updating quasi-Newton matri-
ces with limited storage. Mathematics of computa-
tion, 35(151):773?782.
Naoaki Okazaki. 2007. CRFsuite: a fast implemen-
tation of conditional random fields (CRFs). URL
http://www.chokkan.org/software/crfsuite.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natural
language processing, volume 1, pages 133?142.
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro,
and Andrew Cotter. 2011. Pegasos: Primal esti-
mated sub-gradient solver for SVM. Mathematical
Programming, 127(1):3?30.
David Sontag, Ofer Meshi, Tommi Jaakkola, and Amir
Globerson. 2010. More data means less inference:
A pseudo-max approach to structured learning. In
Advances in Neural Information Processing Systems
23, pages 2181?2189.
Charles Sutton and Andrew McCallum. 2009. Piece-
wise training for structured prediction. Machine
learning, 77(2):165?194.
S.V.N. Vishwanathan, Nicol Schraudolph, Mark
Schmidt, and Kevin Murphy. 2006. Accelerated
training of conditional random fields with stochas-
tic gradient methods. In Proceedings of the 23rd in-
ternational conference on Machine learning, pages
969?976.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
78
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84?89,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Painless Semi-Supervised Morphological Segmentation using Conditional
Random Fields
Teemu Ruokolainen
a
Oskar Kohonen
b
Sami Virpioja
b
Mikko Kurimo
a
a
Department of Signal Processing and Acoustics, Aalto University
b
Department of Information and Computer Science, Aalto University
firstname.lastname@aalto.fi
Abstract
We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, that is the surface
forms of morphemes. We extend a re-
cent segmentation approach based on con-
ditional random fields from purely super-
vised to semi-supervised learning by ex-
ploiting available unsupervised segmenta-
tion techniques. We integrate the unsu-
pervised techniques into the conditional
random field model via feature set aug-
mentation. Experiments on three di-
verse languages show that this straight-
forward semi-supervised extension greatly
improves the segmentation accuracy of the
purely supervised CRFs in a computation-
ally efficient manner.
1 Introduction
We discuss data-driven morphological segmenta-
tion, in which word forms are segmented into
morphs, the surface forms of morphemes. This
type of morphological analysis can be useful for
alleviating language model sparsity inherent to
morphologically rich languages (Hirsim?ki et al.,
2006; Creutz et al., 2007; Turunen and Kurimo,
2011; Luong et al., 2013). Particularly, we focus
on a low-resource learning setting, in which only
a small amount of annotated word forms are avail-
able for model training, while unannotated word
forms are available in abundance.
We study morphological segmentation using
conditional random fields (CRFs), a discrimina-
tive model for sequential tagging and segmenta-
tion (Lafferty et al., 2001). Recently, Ruoko-
lainen et al. (2013) showed that the CRFs can
yield competitive segmentation accuracy com-
pared to more complex, previous state-of-the-
art techniques. While CRFs yielded generally
the highest accuracy compared to their reference
methods (Poon et al., 2009; Kohonen et al., 2010),
on the smallest considered annotated data sets of
100 word forms, they were outperformed by the
semi-supervised Morfessor algorithm (Kohonen et
al., 2010). However, Ruokolainen et al. (2013)
trained the CRFs solely on the annotated data,
without any use of the available unannotated data.
In this work, we extend the CRF-based ap-
proach to leverage unannotated data in a straight-
forward and computationally efficient manner via
feature set augmentation, utilizing predictions of
unsupervised segmentation algorithms. Experi-
ments on three diverse languages show that the
semi-supervised extension substantially improves
the segmentation accuracy of the CRFs. The ex-
tension also provides higher accuracies on all the
considered data set sizes and languages compared
to the semi-supervised Morfessor (Kohonen et al.,
2010).
In addition to feature set augmentation, there
exists numerous approaches for semi-supervised
CRF model estimation, exemplified by minimum
entropy regularization (Jiao et al., 2006), gen-
eralized expectations criteria (Mann and McCal-
lum, 2008), and posterior regularization (He et al.,
2013). In this work, we employ the feature-based
approach due to its simplicity and the availabil-
ity of useful unsupervised segmentation methods.
Varying feature set augmentation approaches have
been successfully applied in several related tasks,
such as Chinese word segmentation (Wang et al.,
2011; Sun and Xu, 2011) and chunking (Turian et
al., 2010).
The paper is organized as follows. In Section 2,
we describe the CRF-based morphological seg-
mentation approach following (Ruokolainen et al.,
2013), and then show how to extend this approach
to leverage unannotated data in an efficient man-
ner. Our experimental setup and results are dis-
cussed in Sections 3 and 4, respectively. Finally,
84
we present conclusions on the work in Section 5.
2 Methods
2.1 Supervised Morphological Segmentation
using CRFs
We present the morphological segmentation task
as a sequential labeling problem by assigning each
character to one of three classes, namely {be-
ginning of a multi-character morph (B), middle
of a multi-character morph (M), single character
morph (S)}. We then perform the sequential label-
ing using linear-chain CRFs (Lafferty et al., 2001).
Formally, the linear-chain CRF model distribu-
tion for label sequence y = (y
1
, y
2
, . . . , y
T
) and
a word form x = (x
1
, x
2
, . . . , x
T
) is written as a
conditional probability
p (y |x;w) ?
T
?
t=2
exp
(
w ? ?(y
t?1
, y
t
, x, t)
)
,
(1)
where t indexes the character positions,w denotes
the model parameter vector, and ? the vector-
valued feature extracting function. The model pa-
rameters w are estimated discrimatively based on
a training set of exemplar input-output pairs (x, y)
using, for example, the averaged perceptron algo-
rithm (Collins, 2002). Subsequent to estimation,
the CRF model segments test word forms using
the Viterbi algorithm (Lafferty et al., 2001).
We next describe the feature set
{?
i
(y
t?1
, y
t
, x, t)}
|?|
i=1
by defining emission
and transition features. Denoting the label set {B,
M, S} as Y , the emission feature set is defined as
{?
m
(x, t)1(y
t
= y
?
t
) |m ? 1..M ,?y
?
t
? Y} ,
(2)
where the indicator function 1(y
t
= y
?
t
) returns
one if and only if y
t
= y
?
t
and zero otherwise, that
is
1(y
t
= y
?
t
) =
{
1 if y
t
= y
?
t
0 otherwise
, (3)
and {?
m
(x, t)}
M
m=1
is the set of functions describ-
ing the character position t. Following Ruoko-
lainen et al. (2013), we employ binary functions
that describe the position t of word x using all left
and right substrings up to a maximum length ?.
The maximum substring length ?
max
is considered
a hyper-parameter to be adjusted using a develop-
ment set. While the emission features associate
the input to labels, the transition feature set
{1(y
t?1
= y
?
t?1
)1(y
t
= y
?
t
) | y
?
t
, y
?
t?1
? Y} (4)
captures the dependencies between adjacent labels
as irrespective of the input x.
2.2 Leveraging Unannotated Data
In order to utilize unannotated data, we explore a
straightforward approach based on feature set aug-
mentation. We exploit predictions of unsupervised
segmentation algorithms by defining variants of
the features described in Section 2.1. The idea is
to compensate the weaknesses of the CRF model
trained on the small annotated data set using the
strengths of the unsupervised methods that learn
from large amounts of unannotated data.
For example, consider utilizing predictions of
the unsupervised Morfessor algorithm (Creutz and
Lagus, 2007) in the CRF model. In order to ac-
complish this, we first learn the Morfessor model
from the unannotated training data, and then ap-
ply the learned model on the word forms in the
annotated training set. Assuming the annotated
training data includes the English word drivers,
the Morfessor algorithm might, for instance, re-
turn a (partially correct) segmentation driv + ers.
We present this segmentation by defining a func-
tion ?(t), which returns 0 or 1, if the position t is
in the middle of a segment or in the beginning of a
segment, respectively, as in
t 1 2 3 4 5 6 7
x
t
d r i v e r s
?(t) 1 0 0 0 1 0 0
Now, given a set of U functions {?
u
(t)}
U
u=1
, we
define variants of the emission features in (2) as
{?
u
(x, t)?
m
(x, t)1(y
t
= y
?
t
) |
?u ? 1..U ,?m ? 1..M ,?y
?
t
? Y} . (5)
By adding the expanded features of form (5), the
CRF model learns to associate the output of the
unsupervised algorithms in relation to the sur-
rounding substring context. Similarly, an ex-
panded transition feature is written as
{?
u
(x, t)1(y
t?1
= y
?
t?1
)1(y
t
= y
?
t
) |
?u ? 1..U ,?y
?
t
, y
?
t?1
? Y} . (6)
After defining the augmented feature set, the
CRF model parameters can be estimated in a stan-
dard manner on the small, annotated training data
set. Subsequent to CRF training, the Morfessor
model is applied on the test instances in order to
allow the feature set augmentation and standard
decoding with the estimated CRF model. We ex-
pect the Morfessor features to specifically improve
85
segmentation of compound words (for example,
brain+storm), which are modeled with high ac-
curacy by the unsupervised Morfessor algorithm
(Creutz and Lagus, 2007), but can not be learned
from the small number of annotated examples
available for the supervised CRF training.
As another example of a means to augment the
feature set, we make use of the fact that the output
of the unsupervised algorithms does not have to be
binary (zeros and ones). To this end, we employ
the classic letter successor variety (LSV) scores
presented originally by (Harris, 1955).
1
The LSV
scores utilize the insight that the predictability of
successive letters should be high within morph
segments, and low at the boundaries. Conse-
quently, a high variety of letters following a prefix
indicates a high probability of a boundary. We use
a variant of the LSV values presented by ??ltekin
(2010), in which we first normalize the scores by
the average score at each position t, and subse-
qently logarithmize the normalized value. While
LSV score tracks predictability given prefixes, the
same idea can be utilized for suffixes, providing
the letter predecessor variety (LPV). Subsequent
to augmenting the feature set using the functions
LSV (t) and LPV (t), the CRF model learns to
associate high successor and predecessor values
(low predictability) to high probability of a seg-
ment boundary. Appealingly, the Harris features
can be obtained in a computationally inexpensive
manner, as they merely require counting statistics
from the unannotated data.
The feature set augmentation approach de-
scribed above is computationally efficient, if the
computational overhead from the unsupervised
methods is small. This is because the CRF param-
eter estimation is still based on the small amount
of labeled examples as described in Section 2.1,
while the number of features incorporated in the
CRF model (equal to the number of parameters)
grows linearly in the number of exploited unsu-
pervised algorithms.
3 Experimental Setup
3.1 Data
We perform the experiments on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al., 2009; Ku-
1
We also experimented on modifying the output of the
Morfessor algorithm from binary to probabilistic, but these
soft cues provided no consistent advantage over the standard
binary output.
English Finnish Turkish
Train (unann.) 384,903 2,206,719 617,298
Train (ann.) 1,000 1,000 1,000
Devel. 694 835 763
Test 10,000 10,000 10,000
Table 1: Number of word types in the Morpho
Challenge data set.
rimo et al., 2010) consisting of manually prepared
morphological segmentations in English, Finnish
and Turkish. We follow the experiment setup, in-
cluding data partitions and evaluation metrics, de-
scribed by Ruokolainen et al. (2013). Table 1
shows the total number of instances available for
model estimation and testing.
3.2 CRF Feature Extraction and Training
The substring features included in the CRF model
are described in Section 2.1. We include all sub-
strings which occur in the training data. The Mor-
fessor and Harris (successor and predecessor va-
riety) features employed by the semi-supervised
extension are described in Section 2.2. We ex-
perimented on two variants of the Morfessor al-
gorithm, namely, the Morfessor Baseline (Creutz
and Lagus, 2002) and Morfessor Categories-MAP
(Creutz and Lagus, 2005), CatMAP for short. The
Baseline models were trained on word types and
the perplexity thresholds of the CatMAP models
were set equivalently to the reference runs in Mor-
pho Challenge 2010 (English: 450, Finnish: 250,
Turkish: 100); otherwise the default parameters
were used. The Harris features do not require any
hyper-parameters.
The CRF model (supervised and semi-
supervised) is trained using the averaged
perceptron algorithm (Collins, 2002). The num-
ber of passes over the training set made by the
perceptron algorithm, and the maximum length of
substring features are optimized on the held-out
development sets.
The experiments are run on a standard desktop
computer using a Python-based single-threaded
CRF implementation. For Morfessor Baseline, we
use the recently published implementation by Vir-
pioja et al. (2013). For Morfessor CatMAP, we
used the Perl implementation by Creutz and La-
gus (2005).
86
3.3 Reference Methods
We compare our method?s performance with
the fully supervised CRF model and the semi-
supervised Morfessor algorithm (Kohonen et al.,
2010). For semi-supervised Morfessor, we use the
Python implementation by Virpioja et al. (2013).
4 Results
Segmentation accuracies for all languages are pre-
sented in Table 2. The columns titled Train (ann.)
and Train (unann.) denote the number of anno-
tated and unannotated training instances utilized
by the method, respectively. To summarize, the
semi-supervised CRF extension greatly improved
the segmentation accuracy of the purely super-
vised CRFs, and also provided higher accuracies
compared to the semi-supervised Morfessor algo-
rithm
2
.
Appealingly, the semi-supervised CRF exten-
sion already provided consistent improvement
over the supervised CRFs, when utilizing the com-
putationally inexpensive Harris features. Addi-
tional gains were then obtained using the Morfes-
sor features. On all languages, highest accuracies
were obtained using a combination of Harris and
CatMAP features.
Running the CRF parameter estimation (includ-
ing hyper-parameters) consumed typically up to a
few minutes. Computing statistics for the Harris
features also took up roughly a few minutes on
all languages. Learning the unsupervised Mor-
fessor algorithm consumed 3, 47, and 20 min-
utes for English, Finnish, and Turkish, respec-
tively. Meanwhile, CatMAP model estimation
was considerably slower, consuming roughly 10,
50, and 7 hours for English, Finnish and Turkish,
respectively. Training and decoding with semi-
supervised Morfessor took 21, 111, and 47 hours
for English, Finnish and Turkish, respectively.
5 Conclusions
We extended a recent morphological segmenta-
tion approach based on CRFs from purely super-
vised to semi-supervised learning. We accom-
plished this in an efficient manner using feature set
augmentation and available unsupervised segmen-
tation techniques. Experiments on three diverse
2
The improvements over the supervised CRFs and semi-
supervised Morfessor were statistically significant (confi-
dence level 0.95) according to the standard 1-sided Wilcoxon
signed-rank test performed on 10 randomly divided, non-
overlapping subsets of the complete test sets.
Method Train (ann.) Train (unann.) F1
English
CRF 100 0 78.8
S-MORF. 100 384,903 83.7
CRF (Harris) 100 384,903 80.9
CRF (BL+Harris) 100 384,903 82.6
CRF (CM+Harris) 100 384,903 84.4
CRF 1,000 0 85.9
S-MORF. 1,000 384,903 84.3
CRF (Harris) 1,000 384,903 87.6
CRF (BL+Harris) 1,000 384,903 87.9
CRF (CM+Harris) 1,000 384,903 88.4
Finnish
CRF 100 0 65.5
S-MORF. 100 2,206,719 70.4
CRF (Harris) 100 2,206,719 78.9
CRF (BL+Harris) 100 2,206,719 79.3
CRF (CM+Harris) 100 2,206,719 82.0
CRF 1,000 0 83.8
S-MORF. 1,000 2,206,719 76.4
CRF (Harris) 1,000 2,206,719 88.3
CRF (BL+Harris) 1,000 2,206,719 88.9
CRF (CM+Harris) 1,000 2,206,719 89.4
Turkish
CRF 100 0 77.7
S-MORF. 100 617,298 78.2
CRF (Harris) 100 617,298 82.6
CRF (BL+Harris) 100 617,298 84.9
CRF (CM+Harris) 100 617,298 85.5
CRF 1,000 0 88.6
S-MORF. 1,000 617,298 87.0
CRF (Harris) 1,000 617,298 90.1
CRF (BL+Harris) 1,000 617,298 91.7
CRF (CM+Harris) 1,000 617,298 91.8
Table 2: Results on test data. CRF (BL+Harris)
denotes semi-supervised CRF extension using
Morfessor Baseline and Harris features, while
CRF (CM+Harris) denotes CRF extension em-
ploying Morfessor CatMAP and Harris features.
languages showed that this straightforward semi-
supervised extension greatly improves the seg-
mentation accuracy of the supervised CRFs, while
being computationally efficient. The extension
also outperformed the semi-supervised Morfessor
algorithm on all data set sizes and languages.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012?2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).
87
References
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1?8. Association for Computational
Linguistics.
?agr? ??ltekin. 2010. Improving successor variety
for morphological segmentation. In Proceedings of
the 20th Meeting of Computational Linguistics in the
Netherlands.
Mathias Creutz and Krista Lagus. 2002. Unsupervised
discovery of morphemes. In Mike Maxwell, editor,
Proceedings of the ACL-02 Workshop on Morpho-
logical and Phonological Learning, pages 21?30,
Philadelphia, PA, USA, July. Association for Com-
putational Linguistics.
Mathias Creutz and Krista Lagus. 2005. Inducing the
morphological lexicon of a natural language from
unannotated text. In Timo Honkela, Ville K?n?nen,
Matti P?ll?, and Olli Simula, editors, Proceedings of
AKRR?05, International and Interdisciplinary Con-
ference on Adaptive Knowledge Representation and
Reasoning, pages 106?113, Espoo, Finland, June.
Helsinki University of Technology, Laboratory of
Computer and Information Science.
Mathias Creutz and Krista Lagus. 2007. Unsuper-
vised models for morpheme segmentation and mor-
phology learning. ACM Transactions on Speech and
Language Processing, 4(1):3:1?3:34, January.
Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo,
Antti Puurula, Janne Pylkk?nen, Vesa Siivola, Matti
Varjokallio, Ebru Arisoy, Murat Sara?lar, and An-
dreas Stolcke. 2007. Morph-based speech recog-
nition and modeling of out-of-vocabulary words
across languages. ACM Transactions on Speech and
Language Processing, 5(1):3:1?3:29, December.
Zellig Harris. 1955. From phoneme to morpheme.
Language, 31(2):190?222.
Luheng He, Jennifer Gillenwater, and Ben Taskar.
2013. Graph-based posterior regularization for
semi-supervised structured prediction. In Proceed-
ings of the Seventeenth Conference on Computa-
tional Natural Language Learning, pages 38?46,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, Mikko
Kurimo, Sami Virpioja, and Janne Pylkk?nen.
2006. Unlimited vocabulary speech recognition
with morph language models applied to Finnish.
Computer Speech and Language, 20(4):515?541,
October.
Feng Jiao, Shaojun Wang, Chi-Hoon Lee, Russell
Greiner, and Dale Schuurmans. 2006. Semi-
supervised conditional random fields for improved
sequence segmentation and labeling. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 209?216. Association for Computational Lin-
guistics.
Oskar Kohonen, Sami Virpioja, and Krista Lagus.
2010. Semi-supervised learning of concatenative
morphology. In Proceedings of the 11th Meeting of
the ACL Special Interest Group on Computational
Morphology and Phonology, pages 78?86, Uppsala,
Sweden, July. Association for Computational Lin-
guistics.
Mikko Kurimo, Sami Virpioja, Ville Turunen,
Graeme W. Blackwood, and William Byrne. 2009.
Overview and results of Morpho Challenge 2009. In
Working Notes for the CLEF 2009 Workshop, Corfu,
Greece, September.
Mikko Kurimo, Sami Virpioja, and Ville Turunen.
2010. Overview and results of Morpho Chal-
lenge 2010. In Proceedings of the Morpho Chal-
lenge 2010 Workshop, pages 7?24, Espoo, Finland,
September. Aalto University School of Science and
Technology, Department of Information and Com-
puter Science. Technical Report TKK-ICS-R37.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Carla E. Brodley and Andrea Po-
horeckyj Danyluk, editors, Proceedings of the Eigh-
teenth International Conference on Machine Learn-
ing, pages 282?289, Williamstown, MA, USA. Mor-
gan Kaufmann.
Minh-Thang Luong, Richard Socher, and Christo-
pher D Manning. 2013. Better word representa-
tions with recursive neural networks for morphol-
ogy. In Proceedings of the Seventeenth Confer-
ence on Computational Natural Language Learning
(CoNLL), pages 29?37. Association for Computa-
tional Linguistics, August.
Gideon Mann and Andrew McCallum. 2008. General-
ized expectation criteria for semi-supervised learn-
ing of conditional random fields. In Proceedings
of ACL-08: HLT, pages 870?878. Association for
Computational Linguistics.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation
with log-linear models. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 209?217.
Association for Computational Linguistics.
Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,
and Mikko Kurimo. 2013. Supervised morpholog-
ical segmentation in a low-resource learning setting
using conditional random fields. In Proceedings of
88
the Seventeenth Conference on Computational Nat-
ural Language Learning (CoNLL), pages 29?37. As-
sociation for Computational Linguistics, August.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 970?979. As-
sociation for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 384?394. Association for
Computational Linguistics.
Ville Turunen and Mikko Kurimo. 2011. Speech re-
trieval from unsegmented Finnish audio using statis-
tical morpheme-like units for segmentation, recog-
nition, and retrieval. ACM Transactions on Speech
and Language Processing, 8(1):1:1?1:25, October.
Sami Virpioja, Peter Smit, Stig-Arne Gr?nroos, and
Mikko Kurimo. 2013. Morfessor 2.0: Python im-
plementation and extensions for Morfessor Baseline.
Report 25/2013 in Aalto University publication se-
ries SCIENCE + TECHNOLOGY, Department of
Signal Processing and Acoustics, Aalto University.
Yiou Wang, Yoshimasa Tsuruoka Jun?ichi Kazama,
Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang,
and Kentaro Torisawa. 2011. Improving Chinese
word segmentation and POS tagging with semi-
supervised methods using large auto-analyzed data.
In IJCNLP, pages 309?317.
89
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 259?264,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Part-of-Speech Tagging using Conditional Random Fields: Exploiting
Sub-Label Dependencies for Improved Accuracy
Miikka Silfverberg
a
Teemu Ruokolainen
b
Krister Lind?n
a
Mikko Kurimo
b
a
Department of Modern Languages, University of Helsinki,
firstname.lastname@helsinki.fi
b
Department of Signal Processing and Acoustics, Aalto University,
firstname.lastname@aalto.fi
Abstract
We discuss part-of-speech (POS) tagging
in presence of large, fine-grained la-
bel sets using conditional random fields
(CRFs). We propose improving tagging
accuracy by utilizing dependencies within
sub-components of the fine-grained labels.
These sub-label dependencies are incor-
porated into the CRF model via a (rela-
tively) straightforward feature extraction
scheme. Experiments on five languages
show that the approach can yield signifi-
cant improvement in tagging accuracy in
case the labels have sufficiently rich inner
structure.
1 Introduction
We discuss part-of-speech (POS) tagging using
the well-known conditional random field (CRF)
model introduced originally by Lafferty et al
(2001). Our focus is on scenarios, in which the
POS labels have a rich inner structure. For exam-
ple, consider
PRON+1SG V+NON3SG+PRES N+SG
I like ham
,
where the compound labels PRON+1SG,
V+NON3SG+PRES, and N+SG stand for pro-
noun first person singular, verb non-third singular
present tense, and noun singular, respectively.
Fine-grained labels occur frequently in mor-
phologically complex languages (Erjavec, 2010;
Haverinen et al, 2013).
We propose improving tagging accuracy by uti-
lizing dependencies within the sub-labels (PRON,
1SG, V, NON3SG, N, and SG in the above ex-
ample) of the compound labels. From a technical
perspective, we accomplish this by making use of
the fundamental ability of the CRFs to incorporate
arbitrarily defined feature functions. The newly-
defined features are expected to alleviate data spar-
sity problems caused by the fine-grained labels.
Despite the (relative) simplicity of the approach,
we are unaware of previous work exploiting the
sub-labels to the extent presented here.
We present experiments on five languages (En-
glish, Finnish, Czech, Estonian, and Romanian)
with varying POS annotation granularity. By uti-
lizing the sub-labels, we gain significant improve-
ment in model accuracy given a sufficiently fine-
grained label set. Moreover, our results indi-
cate that exploiting the sub-labels can yield larger
improvements in tagging compared to increasing
model order.
The rest of the paper is organized as follows.
Section 2 describes the methodology. Experimen-
tal setup and results are presented in Section 3.
Section 4 discusses related work. Lastly, we pro-
vide conclusions on the work in Section 5.
2 Methods
2.1 Conditional Random Fields
The (unnormalized) CRF model (Lafferty et al,
2001) for a sentence x = (x
1
, . . . , x
|x|
) and a POS
sequence y = (y
1
, . . . , y
|x|
) is defined as
p (y |x;w) ?
|x|
?
i=n
exp
(
w??(y
i?n
, . . . , y
i
, x, i)
)
,
(1)
where n denotes the model order,w the model pa-
rameter vector, and ? the feature extraction func-
tion. We denote the tag set as Y , that is, y
i
? Y
for i ? 1 . . . |x|.
2.2 Baseline Feature Set
We first describe our baseline feature set
{?
j
(y
i?1
, y
i
, x, i)}
|?|
j=1
by defining emission and
transition features. The emission feature set as-
sociates properties of the sentence position i with
259
the corresponding label as
{?
j
(x, i)1(y
i
= y
?
i
) | j ? 1 . . . |X | , ?y
?
i
? Y} ,
(2)
where the function 1(q) returns one if and only if
the proposition q is true and zero otherwise, that is
1(y
i
= y
?
i
) =
{
1 if y
i
= y
?
i
0 otherwise
, (3)
and X = {?
j
(x, i)}
|X |
j=1
is the set of functions
characterizing the word position i. Following the
classic work of Ratnaparkhi (1996), our X com-
prises simple binary functions:
1. Bias (always active irrespective of input).
2. Word forms x
i?2
, . . . , x
i+2
.
3. Prefixes and suffixes of the word form x
i
up
to length ?
suf
= 4.
4. If the word form x
i
contains (one or more)
capital letter, hyphen, dash, or digit.
Binary functions have a return value of either zero
(inactive) or one (active). Meanwhile, the transi-
tion features
{1(y
i?k
= y
?
i?k
) . . .1(y
i
= y
?
i
) |
y
?
i?k
, . . . , y
?
i
? Y ,?k ? 1 . . . n} (4)
capture dependencies between adjacent labels ir-
respective of the input x.
2.2.1 Expanded Feature Set Leveraging
Sub-Label Dependencies
The baseline feature set described above can yield
a high tagging accuracy given a conveniently sim-
ple label set, exemplified by the tagging results
of Collins (2002) on the Penn Treebank (Mar-
cus et al, 1993). (Note that conditional random
fields correspond to discriminatively trained hid-
den Markov models and Collins (2002) employs
the latter terminology.) However, it does to some
extent overlook some beneficial dependency infor-
mation in case the labels have a rich sub-structure.
In what follows, we describe expanded feature sets
which explicitly model the sub-label dependen-
cies.
We begin by defining a function P(y
i
) which
partitions any label y
i
into its sub-label compo-
nents and returns them in an unordered set. For
example, we could define P(PRON+1+SG) =
{PRON, 1, SG}. (Label partitions employed in
the experiments are described in Section 3.2.) We
denote the set of all sub-label components as S.
Subsequently, instead of defining only (2), we
additionally associate the feature functionsX with
all sub-labels s ? S by defining
{?
j
(x, i)1(s ? P(y
i
)) | ?j ? 1 . . . |X | ,?s ? S} ,
(5)
where 1(s ? P(y
i
)) returns one in case s is in
P(y
i
) and zero otherwise. Second, we exploit sub-
label transitions using features
{1(s
i?k
? P(y
i?k
)) . . .1(s
i
? P(y
i
)) |
?s
i?k
, . . . , s
i
? S ,?k ? 1 . . .m} . (6)
Note that we define the sub-label transitions up
to order m, 1 ? m ? n, that is, an nth-order
CRF model is not obliged to utilize sub-label tran-
sitions all the way up to order n. This is be-
cause employing high-order sub-label transitions
may potentially cause overfitting to training data
due to substantially increased number of features
(equivalent to the number of model parameters,
|w| = |?|). For example, in a second-order
(n = 2) model, it might be beneficial to em-
ploy the sub-label emission feature set (5) and
first-order sub-label transitions while discarding
second-order sub-label transitions. (See the exper-
imental results presented in Section 3.)
In the remainder of this paper, we use the fol-
lowing notations.
1. A standard CRF model incorporating (2) and
(4) is denoted as CRF(n,-).
2. A CRF model incorporating (2), (4), and (5)
is denoted as CRF(n,0).
3. A CRF model incorporating (2), (4), (5), and
(6) is denoted as CRF(n,m).
2.3 On Linguistic Intuition
This section aims to provide some intuition on the
types of linguistic phenomena that can be captured
by the expanded feature set. To this end, we con-
sider an example on the plural number in Finnish.
First, consider the plural nominative word form
kissat (cats) where the plural number is denoted
by the 1-suffix -t. Then, by employing the features
(2), the suffix -t is associated solely with the com-
pound label NOMINATIVE+PLURAL. However,
by incorporating the expanded feature set (5), -t
260
will also be associated to the sub-label PLURAL.
This can be useful because, in Finnish, also adjec-
tives and numerals are inflected according to num-
ber and denote the plural number with the suffix
-t (Hakulinen et al, 2004, ?79). Therefore, one
can exploit -t to predict the plural number also in
words such as mustat (plural of black) with a com-
pound analysis ADJECTIVE+PLURAL.
Second, consider the number agreement (con-
gruence). For example, in the sentence fragment
mustat kissat juoksevat (black cats are running),
the words mustat and kissat share the plural num-
ber. In other words, the analyses of both mustat
and kissat are required to contain the sub-label
PLURAL. This short-span dependency between
sub-labels will be captured by a first-order sub-
label transition feature included in (6).
Lastly, we note that the feature expansion sets
(5) and (6) will, naturally, capture any short-span
dependencies within the sub-labels irrespective if
the dependencies have a clear linguistic interpre-
tation or not.
3 Experiments
3.1 Data
For a quick overview of the data sets, see Table 1.
Penn Treebank. The English Penn Treebank
(Marcus et al, 1993) is divided into 25 sections
of newswire text extracted from the Wall Street
Journal. We split the data into training, develop-
ment, and test sets using the sections 0-18, 19-21,
and 22-24, according to the standardly applied di-
vision introduced by Collins (2002).
Turku Depedency Treebank. The Finnish
Turku Depedendency Treebank (Haverinen et al,
2013) contains text from 10 different domains.
The treebank does not have default partition to
training and test sets. Therefore, from each 10
consecutive sentences, we assign the 9th and 10th
to the development set and the test set, respec-
tively. The remaining sentences are assigned to
the training set.
Multext-East. The third data we consider is the
multilingual Multext-East (Erjavec, 2010) corpus,
from which we utilize the Czech, Estonian and Ro-
manian sections. The corpus corresponds to trans-
lations of the novel 1984 by George Orwell. We
apply the same data splits as for Turku Depen-
dency Treebank.
lang. train. dev. test tags train. tags
Eng 38,219 5,527 5,462 45 45
Rom 5,216 652 652 405 391
Est 5,183 648 647 413 408
Cze 5,402 675 675 955 908
Fin 5,043 630 630 2,355 2,141
Table 1: Overview on data. The training (train.),
development (dev.) and test set sizes are given in
sentences. The columns titled tags and train. tags
correspond to total number of tags in the data set
and number of tags in the training set, respectively.
3.2 Label Partitions
This section describes the employed compound la-
bel splits. The label splits for all data sets are sub-
mitted as data file attachments. All the splits are
performed a priori to model learning, that is, we
do not try to optimize them on the development
sets.
The POS labels in the Penn Treebank are split
in a way which captures relevant inflectional cat-
egories, such as tense and number. Consider, for
example, the split for the present tense third sin-
gular verb label P(VBZ) = {VB, Z}.
In the Turku Dependency Treebank, each
morphological tag consists of sub-labels mark-
ing word-class, relevant inflectional categories,
and their respective values. Each inflec-
tional category, such as case or tense, com-
bined with its value, such as nominative or
present, constitutes one sub-label. Consider,
for example, the split for the singular, adessive
noun P(N+CASE_ADE+NUM_SG) = {POS_N,
CASE_ADE, NUM_SG}.
The labeling scheme employed in the Multext-
East data set represents a considerably different
annotation approach compared to the Penn and
Turku Treebanks. Each morphological analysis is
a sequence of feature markers, for example Pw3?
r. The first feature marker (P) denotes word class
and the rest (w, 3, and r) encode values of inflec-
tional categories relevant for that word class. A
feature marker may correspond to several differ-
ent values depending on word class and its posi-
tion in the analysis. Therefore it becomes rather
difficult to split the labels into similar pairs of in-
flectional category and value as we are able to do
for the Turku Dependency Treebank. Since the in-
terpretation of a feature marker depends on its po-
sition in the analysis and the word class, the mark-
ers have to be numbered and appended with the
261
word class marker. For example, consider the split
P(Pw3?r) = {0 : P, 1 : Pw, 2 : P3, 5 : Pr}.
3.3 CRF Model Specification
We perform experiments using first-order and
second-order CRFs with zeroth-order and first-
order sub-label features. Using the notation
introduced in Section 2, the employed mod-
els are CRF(1,-), CRF(1,1), CRF(2,-), CRF(2,0),
and CRF(2,1). We do not report results us-
ing CRF(2,2) since, based on preliminary exper-
iments, this model overfits on all languages.
The CRF model parameters are estimated using
the averaged perceptron algorithm (Collins, 2002).
The model parameters are initialized with a zero
vector. We evaluate the latest averaged parameters
on the held-out development set after each pass
over the training data and terminate training if no
improvement in accuracy is obtained during three
last passes. The best-performing parameters are
then applied on the test instances.
We accelerate the perceptron learning using
beam search (Zhang and Clark, 2011). The beam
width, b, is optimized separately for each lan-
guage on the development sets by considering b =
1, 2, 4, 8, 16, 32, 64, 128 until the model accuracy
does not improve by at least 0.01 (absolute).
Development and test instances are decoded us-
ing Viterbi search in combination with the tag dic-
tionary approach of Ratnaparkhi (1996). In this
approach, candidate tags for known word forms
are limited to those observed in the training data.
Meanwhile, word forms that were unseen during
training consider the full label set.
3.4 Software and Hardware
The experiments are run on a standard desktop
computer (Intel Xeon E5450 with 3.00 GHz and
64 GB of memory). The methods discussed in
Section 2 are implemented in C++.
3.5 Results
The obtained tagging accuracies and training
times are presented in Table 2. The times in-
clude running the averaged perceptron algorithm
and evaluation of the development sets. The col-
umn labeled it. corresponds to the number of
passes over the training data made by the percep-
tron algorithm before termination. We summarize
the results as follows.
First, compared to standard feature extraction
approach, employing the sub-label transition fea-
tures resulted in improved accuracy on all lan-
guages apart from English. The differences were
statistically significant on Czech, Estonian, and
Finnish. (We establish statistical significance
(with confidence level 0.95) using the standard 1-
sided Wilcoxon signed-rank test performed on 10
randomly divided, non-overlapping subsets of the
complete test sets.) This results supports the in-
tuition that the sub-label features should be most
useful in presence of large, fine-grained label sets,
in which case the learning is most affected by data
sparsity.
Second, on all languages apart from English,
employing a first-order model with sub-label fea-
tures yielded higher accuracy compared to a
second-order model with standard features. The
differences were again statistically significant on
Czech, Estonian, and Finnish. This result suggests
that, compared to increasing model order, exploit-
ing the sub-label dependencies can be a preferable
approach to improve the tagging accuracy.
Third, applying the expanded feature set in-
evitably causes some increase in the computa-
tional cost of model estimation. However, as
shown by the running times, this increase is not
prohibitive.
4 Related Work
In this section, we compare the approach pre-
sented in Section 2 to two prior systems which at-
tempt to utilize sub-label dependencies in a similar
manner.
Smith et al (2005) use a CRF-based system
for tagging Czech, in which they utilize expanded
emission features similar to our (5). However, they
do not utilize the full expanded transition features
(6). More specifically, instead of utilizing a sin-
gle chain as in our approach, Smith et al employ
five parallel structured chains. One of the chains
models the sequence of word-class labels such as
noun and adjective. The other four chains model
gender, number, case, and lemma sequences, re-
spectively. Therefore, in contrast to our approach,
their system does not capture cross-dependencies
between inflectional categories, such as the de-
pendence between the word-class and case of ad-
jacent words. Unsurprisingly, Smith et al fail
to achieve improvement over a generative HMM-
based POS tagger of Haji
?
c (2001). Meanwhile,
our system outperforms the generative trigram tag-
ger HunPos (Hal?csy et al, 2007) which is an im-
262
model it. time (min) acc. OOV.
English
CRF(1, -) 8 9 97.04 88.65
CRF(1, 0) 6 17 97.02 88.44
CRF(1, 1) 8 22 97.02 88.82
CRF(2, -) 9 15 97.18 88.82
CRF(2, 0) 11 36 97.17 89.23
CRF(2, 1) 8 27 97.15 89.04
Romanian
CRF(1, -) 14 29 97.03 85.01
CRF(1, 0) 13 68 96.96 84.59
CRF(1, 1) 16 146 97.24 85.94
CRF(2, -) 7 19 97.08 85.21
CRF(2, 0) 18 99 97.02 85.42
CRF(2, 1) 12 118 97.29 86.25
Estonian
CRF(1, -) 15 28 93.39 78.66
CRF(1, 0) 17 66 93.81 80.44
CRF(1, 1) 13 129 93.77 79.37
CRF(2, -) 15 30 93.48 77.13
CRF(2, 0) 13 53 93.78 79.60
CRF(2, 1) 16 105 94.01 79.53
Czech
CRF(1, -) 6 28 89.28 70.90
CRF(1, 0) 10 112 89.94 74.44
CRF(1, 1) 10 365 90.78 76.83
CRF(2, -) 19 91 89.81 72.44
CRF(2, 0) 13 203 90.35 76.37
CRF(2, 1) 24 936 91.00 77.75
Finnish
CRF(1, -) 10 80 87.37 59.29
CRF(1, 0) 13 249 88.58 63.46
CRF(1, 1) 12 474 88.41 62.63
CRF(2, -) 11 106 86.74 56.96
CRF(2, 0) 13 272 88.52 63.46
CRF(2, 1) 12 331 88.68 63.62
Table 2: Results.
proved open-source implementation of the well-
known TnT tagger of Brants (2000). The obtained
HunPos results are presented in Table 3.
Eng Rom Est Cze Fin
HunPos 96.58 96.96 92.76 89.57 85.77
Table 3: Results using a generative HMM-based
HunPos tagger of Halacsy et al (2007).
Ceaus?u (2006) uses a maximum entropy
Markov model (MEMM) based system for tag-
ging Romanian which utilizes transitional behav-
ior between sub-labels similarly to our feature set
(6). However, in addition to ignoring the most in-
formative emission-type features (5), Ceaus?u em-
beds the MEMMs into the tiered tagging frame-
work of Tufis (1999). In tiered tagging, the full
morphological analyses are mapped into a coarser
tag set and a tagger is trained for this reduced tag
set. Subsequent to decoding, the coarser tags are
mapped into the original fine-grained morpholog-
ical analyses. There are several problems associ-
ated with this tiered tagging approach. First, the
success of the approach is highly dependent on a
well designed coarse label set. Consequently, it
requires intimate knowledge of the tag set and lan-
guage. Meanwhile, our model can be set up with
relatively little prior knowledge of the language
or the tagging scheme (see Section 3.2). More-
over, a conversion to a coarser label set is neces-
sarily lossy (at least for OOV words) and poten-
tially results in reduced accuracy since recovering
the original fine-grained tags from the coarse tags
may induce errors. Indeed, the accuracy 96.56, re-
ported by Ceaus?u on the Romanian section of the
Multext-East data set, is substantially lower than
the accuracy 97.29 we obtain. These accuracies
were obtained using identical sized training and
test sets (although direct comparison is impossible
because Ceaus?u uses a non-documented random
split).
5 Conclusions
We studied improving the accuracy of CRF-based
POS tagging by exploiting sub-label dependency
structure. The dependencies were included in the
CRF model using a relatively straightforward fea-
ture expansion scheme. Experiments on five lan-
guages showed that the approach can yield signif-
icant improvement in tagging accuracy given suf-
ficiently fine-grained label sets.
In future work, we aim to perform a more
fine-grained error analysis to gain a better under-
standing where the improvement in accuracy takes
place. One could also attempt to optimize the
compound label splits to maximize prediction ac-
curacy instead of applying a priori partitions.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the grant no
251170 (Finnish Centre of Excellence Program
(2012-2017)). We would like to thank the anony-
mous reviewers for their useful comments.
263
References
Thorsten Brants. 2000. Tnt: A statistical part-of-
speech tagger. In Proceedings of the Sixth Con-
ference on Applied Natural Language Processing,
pages 224?231.
A. Ceausu. 2006. Maximum entropy tiered tagging.
In The 11th ESSLI Student session, pages 173?179.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing (EMNLP 2002), vol-
ume 10, pages 1?8.
Toma?z Erjavec. 2010. Multext-east version 4: Multi-
lingual morphosyntactic specifications, lexicons and
corpora. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Jan Haji?c, Pavel Krbec, Pavel Kv?eto?n, Karel Oliva, and
Vladim?r Petkevi?c. 2001. Serial combination of
rules and statistics: A case study in czech tagging.
In Proceedings of the 39th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 268?
275.
Auli Hakulinen, Maria Vilkuna, Riitta Korhonen, Vesa
Koivisto, Tarja Riitta Heinonen, and Irja Alho.
2004. Iso suomen kielioppi. Suomalaisen Kirjal-
lisuuden Seura, Helsinki, Finland.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz.
2007. Hunpos: An open source trigram tagger. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 209?212.
Katri Haverinen, Jenna Nyblom, Timo Viljanen,
Veronika Laippala, Samuel Kohonen, Anna Missil?,
Stina Ojala, Tapio Salakoski, and Filip Ginter. 2013.
Building the essential resources for Finnish: the
Turku Dependency Treebank. Language Resources
and Evaluation.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, pages
282?289.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational linguis-
tics, 19(2):313?330.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of the conference on empirical methods in natu-
ral language processing, volume 1, pages 133?142.
Philadelphia, PA.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-based morphological disambiguation
with random fields. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing, pages
475?482.
Dan Tufis. 1999. Tiered tagging and combined lan-
guage models classifiers. In Proceedings of the Sec-
ond International Workshop on Text, Speech and Di-
alogue, pages 28?33.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105?151.
264
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 29?37,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Supervised Morphological Segmentation in a Low-Resource Learning
Setting using Conditional Random Fields
Teemu Ruokolainena Oskar Kohonena Sami Virpiojaa Mikko Kurimob
a Department of Information and Computer Science, Aalto University
b Department of Signal Processing and Acoustics, Aalto University
firstname.lastname@aalto.fi
Abstract
We discuss data-driven morphological
segmentation, in which word forms are
segmented into morphs, the surface forms
of morphemes. Our focus is on a low-
resource learning setting, in which only a
small amount of annotated word forms are
available for model training, while unan-
notated word forms are available in abun-
dance. The current state-of-art methods
1) exploit both the annotated and unan-
notated data in a semi-supervised man-
ner, and 2) learn morph lexicons and sub-
sequently uncover segmentations by gen-
erating the most likely morph sequences.
In contrast, we discuss 1) employing only
the annotated data in a supervised man-
ner, while entirely ignoring the unanno-
tated data, and 2) directly learning to pre-
dict morph boundaries given their local
sub-string contexts instead of learning the
morph lexicons. Specifically, we em-
ploy conditional random fields, a popular
discriminative log-linear model for seg-
mentation. We present experiments on
two data sets comprising five diverse lan-
guages. We show that the fully super-
vised boundary prediction approach out-
performs the state-of-art semi-supervised
morph lexicon approaches on all lan-
guages when using the same annotated
data sets.
1 Introduction
Modern natural language processing (NLP) appli-
cations, such as speech recognition, information
retrieval and machine translation, perform their
tasks using statistical language models. For mor-
phologically rich languages, estimation of the lan-
guage models is problematic due to the high num-
ber of compound words and inflected word forms.
A successful means of alleviating this data sparsity
problem is to segment words into meaning-bearing
sub-word units (Hirsim?ki et al, 2006; Creutz et
al., 2007; Turunen and Kurimo, 2011). In lin-
guistics, the smallest meaning-bearing units of a
language are called morphemes and their surface
forms morphs. Thus, morphs are natural targets
for the segmentation.
For most languages, existing resources contain
large amounts of raw unannotated text data, only
small amounts of manually prepared annotated
training data, and no freely available rule-based
morphological analyzers. The focus of our work is
on performing morphological segmentation in this
low-resource scenario. Given this setting, the cur-
rent state-of-art methods approach the problem by
learning morph lexicons from both annotated and
unannotated data using semi-supervised machine
learning techniques (Poon et al, 2009; Kohonen
et al, 2010). Subsequent to model training, the
methods uncover morph boundaries for new word
forms by generating their most likely morph se-
quences according to the morph lexicons.
In contrast to learning morph lexicons (Poon et
al., 2009; Kohonen et al, 2010), we study mor-
phological segmentation by learning to directly
predict morph boundaries based on their local sub-
string contexts. Specifically, we apply the linear-
chain conditional random field model, a popular
discriminative log-linear model for segmentation
presented originally by Lafferty et al (2001). Im-
portantly, we learn the segmentation model from
solely the small annotated data in a supervised
manner, while entirely ignoring the unannotated
data. Despite not using the unannotated data, we
show that by discriminatively learning to predict
the morph boundaries, we are able to outperform
the previous state-of-art.
We present experiments on Arabic and Hebrew
using the data set presented originally by Snyder
and Barzilay (2008), and on English, Finnish and
29
Turkish using the Morpho Challenge 2009/2010
data sets (Kurimo et al, 2009; Kurimo et al,
2010). The results are compared against two state-
of-art techniques, namely the log-linear model-
ing approach presented by Poon et al (2009) and
the semi-supervised Morfessor algorithm (Koho-
nen et al, 2010). We show that when employ-
ing the same small amount of annotated train-
ing data, the CRF-based boundary prediction ap-
proach outperforms these reference methods on
all languages. Additionally, since the CRF model
learns from solely the small annotated data set, its
training is computationally much less demanding
compared to the semi-supervised methods, which
utilize both the annotated and the unannotated data
sets.
The rest of the paper is organized as follows. In
Section 2, we discuss related work in morpholog-
ical segmentation and methodology. In Section 3,
we describe our segmentation method. Our exper-
imental setup is described in Section 4, and the
obtained results are presented in Section 5. In Sec-
tion 6, we discuss the method and the results. Fi-
nally, we present conclusions on the work in Sec-
tion 7.
2 Related work
The CRF model has been widely used in NLP seg-
mentation tasks, such as shallow parsing (Sha and
Pereira, 2003), named entity recognition (McCal-
lum and Li, 2003), and word segmentation (Zhao
et al, 2006). Recently, CRFs were also employed
successfully in morphological segmentation for
Arabic by Green and DeNero (2012) as a com-
ponent of an English to Arabic machine trans-
lation system. While the segmentation method
of Green and DeNero (2012) and ours is very sim-
ilar, our focuses and contributions differ in sev-
eral ways. First, while in our work we consider
the low-resource learning setting, in which a small
annotated data set is available (up to 3,130 word
types), their model is trained on the Arabic Tree-
bank (Maamouri et al, 2004) constituting sev-
eral times larger training set (588,244 word to-
kens). Second, we present empirical comparison
between the CRF approach and two state-of-art
methods (Poon et al, 2009; Kohonen et al, 2010)
on five diverse languages. Third, due to being a
component of a larger system, their presentation
on the method and experiments is rather undersp-
eficied, while here we are able to provide a more
thorough description.
In the experimental section, we compare the
CRF-based segmentation approach with two state-
of-art methods, the log-linear modeling approach
presented by Poon et al (2009) and the semi-
supervised Morfessor algorithm (Kohonen et al,
2010). As stated previously, the CRF-based seg-
mentation approach differs from these methods in
that it learns to predict morph boundaries from
a small amount of annotated data, in contrast to
learning morph lexicons from both annotated and
large amounts of unannotated data.
Lastly, there exists ample work on varying un-
supervised (and semi-supervised) morphological
segmentation methods. A useful review is given
by Hammarstr?m and Borin (2011). The funda-
mental difference between our approach and these
techniques is that our method necessarily requires
manually annotated training data.
3 Methods
In this section, we describe in detail the CRF-
based approach for supervised morphological seg-
mentation.
3.1 Morphological segmentation as a
classification task
We represent the morphological segmentation task
as a structured classification problem by assign-
ing each character to one of four classes, namely
{beginning of a multi-character morph (B), mid-
dle of a multi-character morph (M), end of a multi-
character morph (E), single character morph (S)}.
For example, consider the English word form
drivers
with a corresponding segmentation
driv + er + s .
Using the classification notation, this segmenta-
tion is represented as
START B M M E B E S STOP
<w> d r i v e r s </w>
where we have assumed additional word start
and end markers <w> and </w> with respective
classes START and STOP. As another example,
consider the Finnish word form
autoilla (with cars)
with a corresponding segmentation
auto + i + lla .
Using the classification notation, this segmenta-
tion is represented as
30
START B M M E S B M E STOP
<w> a u t o i l l a </w>
Intuitively, instead of the four class set {B, M,
E, S}, a segmentation could be accomplished us-
ing only a set of two classes {B, M} as in (Green
and DeNero, 2012). However, similarly to Chi-
nese word segmentation (Zhao et al, 2006), our
preliminary experiments suggested that using the
more fine-grained four class set {B, M, E, S} per-
formed slightly better. This result indicates that
morph segments of differerent lengths behave dif-
ferently.
3.2 Linear-chain conditional random fields
We perform the above structured classification us-
ing linear-chain conditional random fields (CRFs),
a discriminative log-linear model for tagging and
segmentation (Lafferty et al, 2001). The central
idea of the linear-chain CRF is to exploit the de-
pendencies between the output variables using a
chain structured undirected graph, also referred to
as a Markov random field, while conditioning the
output globally on the observation.
Formally, the model for input x (characters in a
word) and output y (classes corresponding to char-
acters) is written as
p (y |x;w) ?
T?
t=2
exp
(
w>f(yt?1, yt,x, t)
)
,
(1)
where t indexes the characters, T denotes word
length, w the model parameter vector, and f the
vector-valued feature extracting function.
The purpose of the feature extraction function
f is to capture the co-occurrence behavior of the
tag transitions (yt?1, yt) and a set of features de-
scribing character position t of word form x. The
strength of the CRF model lies in its capability to
utilize arbitrary, non-independent features.
3.3 Feature extraction
The quality of the segmentation depends heavily
on the choice of features defined by the feature
extraction function f . We will next describe and
motivate the feature set used in the experiments.
Our feature set consists of binary indicator func-
tions describing the position t of word x using
all left and right substrings up to a maximum
length ?. For example, consider the problem
of deciding if the letter e in the word drivers
is preceded by a morph boundary. This deci-
sion is now based on the overlapping substrings
to the left and right of this potential bound-
ary position, that is {v, iv, riv, driv, <w>driv} and
{e, er, ers, ers</w>}, respectively. The substrings
to the left and right are considered indepen-
dently. Naturally, if the maximum allowed sub-
string length ? is less than five, the longest sub-
strings are discarded accordingly. In general, the
optimum ? depends on both the amount of avail-
able training data and the language.
In addition to the substring functions, we use a
bias function which returns value 1 independent
of the input x. The bias and substring features are
combined with all the possible tag transitions.
To motivate this choice of feature set, consider
formulating an intuitive segmentation rule for the
English words talked, played and speed with the
correct segmentations talk + ed, play + ed and
speed, respectively. Now, as a right context ed
is generally a strong indicator of a boundary, one
could first formulate a rule
position t is a segment boundary
if its right context is ed.
This rule would indeed correctly segment the
words talked and played, but would incorrectly
segment speed as spe + ed. This error can be re-
solved if the left contexts are utilized as inhibitors
by expanding the above rule as
position t is a segment boundary
if its right context is ed
and the left context is not spe.
Using the feature set defined above, the CRF
model can learn to perform segmentation in this
rule-like manner according to the training data.
For example, using the above example words and
segmentations for training, the CRFs could learn
to assign a high score for a boundary given that
the right context is ed and a high score for a non-
boundary given the left context spe. Subsequent to
training, making segmentation decisions for new
word forms can then be interpreted as voting based
on these scores.
3.4 Parameter estimation
The CRF model parameters w are estimated based
on an annotated training data set. Common train-
ing criteria include the maximum likelihood (Laf-
ferty et al, 2001; Peng et al, 2004; Zhao et al,
2006), averaged structured perceptron (Collins,
2002), and max-margin (Szummer et al, 2008).
In this work, we estimate the parameters using the
perceptron algorithm (Collins, 2002).
31
In perceptron training, the required graph infer-
ence can be efficiently performed using the stan-
dard Viterbi algorithm. Subsequent to training, the
segmentations for test instances are acquired again
using Viterbi search.
Compared to other training criteria, the struc-
tured perceptron has the advantage of employing
only a single hyperparameter, namely the number
of passes over training data, making model esti-
mation fast and straightforward. We optimize the
hyperparameter using a separate development set.
Lastly, we consider the longest substring length ?
a second hyperparameter optimized using the de-
velopment set.
4 Experimental setup
This section describes the data sets, evaluation
metrics, reference methods, and other details con-
cerning the evaluation of the methods.
4.1 Data sets
We evaluate the methods on two different data sets
comprising five languages in total.
S&B data. The first data set we use is the He-
brew Bible parallel corpus introduced by Snyder
and Barzilay (2008). It contains 6,192 parallel
phrases in Hebrew, Arabic, Aramaic, and English
and their frequencies (ranging from 5 to 3517).
The phrases have been extracted using automatic
word alignment. The Hebrew and Arabic phrases
have manually annotated morphological segmen-
tations, and they are used in our experiments. The
phrases are sorted according to frequency, and ev-
ery fifth phrase starting from the first phrase is
placed in the test set, every fifth starting from the
second phrase in the development set (up to 500
phrases), and the rest of the phrases in the train-
ing set. 1 The total numbers of word types in the
sets are shown in Table 1. Finally, the word forms
in the training set are randomly permuted, and the
first 25%, 50%, 75%, and 100% of them are se-
lected as subsets to study the effect of training data
size.
MC data. The second data set is based on the
Morpho Challenge 2010 (Kurimo et al, 2010).
It includes manually prepared morphological seg-
mentations in English, Finnish and Turkish. The
1We are grateful to Dr. Hoifung Poon for providing us
instructions for dividing of the data set.
Arabic Hebrew
Training 3,130 2,770
Development 472 450
Test 1,107 1,040
Table 1: The numbers of word types in S&B data
sets (Snyder and Barzilay, 2008).
English Finnish Turkish
Unannot. 384,903 2,206,719 617,298
Training 1,000 1,000 1,000
Develop. 694 835 763
Test 10?1,000 10?1,000 10?1,000
Table 2: The numbers of word types in the MC
data sets (Kurimo et al, 2009; Kurimo et al,
2010).
additional German corpus does not have segmen-
tation annotation and is therefore excluded. The
annotated data sets include training, development,
and test sets for each language. Following Virpi-
oja et al (2011), the test set results are based on
ten randomly selected 1,000 word sets. Moreover,
we divide the annotated training sets into ten par-
titions with respective sizes of 100, 200, . . . , 1000
words so that each partition is a subset of the all
larger partitions. The data is divided so that the
smallest set had every 10th word of the original
set, the second set every 10th word and the fol-
lowing word, and so forth. For reference methods
that require unannotated data, we use the English,
Finnish and Turkish corpora from Competition 1
of Morpho Challenge 2009 (Kurimo et al, 2009).
Table 2 shows the sizes of the MC data sets.
4.2 Evaluation measures
The word segmentations are evaluated by compar-
ison with linguistic morphs using precision, recall,
and F-measure. The F-measure equals the geo-
metric mean of precision (the percentage of cor-
rectly assigned boundaries with respect to all as-
signed boundaries) and recall (the percentage of
correctly assigned boundaries with respect to the
reference boundaries). While using F-measure is
a standard procedure, the prior work differ at least
in three details: (1) whether precision and recall
are calculated as micro-average over all segmenta-
tion points or as macro-average over all the word
forms, (2) whether the evaluation is based on word
types or word tokens in a corpus, and (3) if the
32
reference segmentations have alternative correct
choices for a single word type, and how to deal
with them.
For the experiments with the S&B data sets,
we follow Poon et al (2009) and apply token-
based micro-averages. For the experiments with
the MC data sets, we follow Virpioja et al (2011)
and use type-based macro-averages. However, dif-
fering from their boundary measure, we take the
best match over the alternative reference analyses
(separately for precision and recall), since none of
the methods considered here provide multiple seg-
mentations per word type. For the models trained
with the full training set, we also report the F-
measures of the boundary evaluation method by
Virpioja et al (2011) in order to compare to the
results reported in the Morpho Challenge website.
4.3 CRF feature extraction and training
The features included in the feature vector in the
CRF model (1) are described in Section 3.3. We
include all substring features which occur in the
training data.
The CRF model is trained using the averaged
perceptron algorithm as described in Section 3.4.
The algorithm initializes the model parameters
with zero vectors. The model performance, mea-
sured using F-measure, is evaluated on the devel-
opment set after each pass over the training set,
and the training is terminated when the perfor-
mance has not improved during last 5 passes. The
maximum length of substrings ? is optimized by
considering ? = 1, 2, 3, . . . , and the search is ter-
minated when the performance has not improved
during last 5 values. Finally, the algorithm returns
the parameters yielding the highest F-measure on
the development set.
For some words, the MC training sets include
several alternative segmentations. We resolve this
ambiguity by using the first given alternative and
discarding the rest. During evaluation, the alter-
native segmentations are taken into account as de-
scribed in Section 4.2.
The experiments are run on a standard desktop
computer using our own single-threaded Python-
based implementation2.
4.4 Reference methods
We compare our method?s performance on Arabic
and Hebrew data with semi-supervised Morfessor
2Available at http://users.ics.aalto.fi/
tpruokol/
(Kohonen et al, 2010) and the results reported by
Poon et al (2009). On Finnish, English and Turk-
ish data, we compare the method only with semi-
supervised Morfessor as we have no implementa-
tion of the model by Poon et al (2009).
We use a recently released Python implemen-
tation of semi-supervised Morfessor3. Semi-
supervised Morfessor was trained separately for
each training set size, always using the full unan-
notated data sets in addition to the annotated sets.
The hyperparameters, the unannotated data weight
? and the annotated data weight ?, were optimized
with a grid search on the development set. For the
S&B data, there are no separate unannotated sets.
When the annotated training set size is varied, the
remaining parts are utilized as unannotated data.
The log-linear model described in (Poon et al,
2009) and the semi-supervised Morfessor algo-
rithm are later referred to as POON-2009 and S-
MORFESSOR for brevity.
5 Results
Method performances for Arabic and Hebrew on
the S&B data are presented in Tables 3 and 4, re-
spectively. The results for the POON-2009 model
are extracted from (Poon et al, 2009). Perfor-
mances for English, Finnish and Turkish on the
MC data set are presented in Tables 5, 6 and 7,
respectively.
On the Arabic and Hebrew data sets, the CRFs
outperform POON-2009 and S-MORFESSOR
substantially on all the considered data set sizes.
On Finnish and Turkish data, the CRFs outper-
form S-MORFESSOR except for the smallest sets
of 100 instances. On English data, the CRFs out-
perform S-MORFESSOR when the training set is
500 instances or larger.
Using our implementation of the CRF model,
obtaining the results for Arabic, Hebrew, English,
Finnish, and Turkish consumed 10, 11, 22, 32,
and 28 minutes, respectively. These CPU times
include model training and hyperparameter opti-
mization. In comparison, S-MORFESSOR train-
ing is considerably slower. For Arabic and He-
brew, the S-MORFESSOR total training times
were 24 and 22 minutes, respectively, and for En-
glish, Finnish, and Turkish 4, 22, and 10 days,
respectively. The higher training times of S-
MORFESSOR are partly because of the larger
3Available at https://github.com/
aalto-speech/morfessor
33
grids in hyperparameter optimization. Further-
more, the S-MORFESSOR training time for each
grid point grows linearly with the size of the
unannotated data set, resulting in particularly slow
training on the MC data sets. All reported times
are total CPU times for single-threaded runs, while
in practice grid searches can be parallelized.
The perceptron algorithm typically converged
after 10 passes over the training set, and never re-
quired more than 40 passes to terminate. Depend-
ing on the size of the training data, the optimized
maximum lengths of substrings varied in ranges
{3,5}, {2,7}, {3,9}, {3,6}, {3,7}, for Arabic, He-
brew, English, Finnish and Turkish, respectively.
Method %Lbl. Prec. Rec. F1
CRF 25 95.5 93.1 94.3
S-MORFESSOR 25 78.7 79.7 79.2
POON-2009 25 84.9 85.5 85.2
CRF 50 96.5 94.6 95.5
S-MORFESSOR 50 87.5 91.5 89.4
POON-2009 50 88.2 86.2 87.5
CRF 75 97.2 96.1 96.6
S-MORFESSOR 75 92.8 83.0 87.6
POON-2009 75 89.6 86.4 87.9
CRF 100 98.1 97.5 97.8
S-MORFESSOR 100 91.4 91.8 91.6
POON-2009 100 91.7 88.5 90.0
Table 3: Results for Arabic on the S&B data
set (Snyder and Barzilay, 2008). The column ti-
tled %Lbl. denotes the percentage of the annotated
data used for training. In addition to the given per-
centages of annotated data, POON-2009 and S-
MORFESSOR utilized the remainder of the data
as an unannotated set.
Finally, Table 8 shows the results of the CRF
and S-MORFESSOR models trained with the full
English, Finnish, and Turkish MC data sets and
evaluated with the boundary evaluation method of
Virpioja et al (2011). That is, these numbers are
directly comparable to the BPR-F column in the
result tables presented at the Morpho Challenge
website4. For each of the three languages, CRF
clearly outperforms all the Morpho Challenge sub-
missions that have provided morphological seg-
mentations.
4http://research.ics.aalto.fi/events/
morphochallenge/
Method %Lbl. Prec. Rec. F1
CRF 25 90.5 90.6 90.6
S-MORFESSOR 25 71.5 85.3 77.8
POON-2009 25 78.7 73.3 75.9
CRF 50 94.0 91.5 92.7
S-MORFESSOR 50 82.1 81.8 81.9
POON-2009 50 82.8 74.6 78.4
CRF 75 94.0 92.7 93.4
S-MORFESSOR 75 84.0 88.1 86.0
POON-2009 75 83.1 77.3 80.1
CRF 100 94.9 94.0 94.5
S-MORFESSOR 100 85.3 91.1 88.1
POON-2009 100 83.0 78.9 80.9
Table 4: Results for Hebrew on the S&B data
set (Snyder and Barzilay, 2008). The column ti-
tled %Lbl. denotes the percentage of the annotated
data used for training. In addition to the given per-
centages of annotated data, POON-2009 and S-
MORFESSOR utilized the remainder of the data
as an unannotated set.
6 Discussion
Intuitively, the CRF-based supervised learning ap-
proach should yield high segmentation accuracy
when there are large amounts of annotated train-
ing data available. However, perhaps surprisingly,
the CRF model yields state-of-art results already
using very small amounts of training data. This
result is meaningful since for most languages it is
infeasible to acquire large amounts of annotated
training data.
The strength of the discriminatively trained
CRF model is that overlapping, non-independent
features can be naturally employed. Importantly,
we showed that simple, language-independent
substring features are sufficient for high perfor-
mance. However, adding new, task- and language-
dependent features is also easy. One might, for ex-
ample, explore features capturing vowel harmony
in Finnish and Turkish.
The CRFs was estimated using the structured
perceptron algorithm (Collins, 2002), which has
the benefit of being computationally efficient and
easy to implement. Other training criteria, such
as maximum likelihood (Lafferty et al, 2001)
or max-margin (Szummer et al, 2008), could
also be employed. Similarly, other classifiers,
such as the Maximum Entropy Markov Models
(MEMMs) (McCallum et al, 2000), are applica-
ble. However, as the amount of information in-
34
Method Train. Prec. Rec. F1
CRF 100 80.2 74.6 77.3
S-MORFESSOR 100 88.1 79.7 83.7
CRF 200 84.7 79.2 81.8
S-MORFESSOR 200 88.1 79.5 83.6
CRF 300 86.7 79.8 83.1
S-MORFESSOR 300 88.4 80.6 84.3
CRF 400 86.5 80.6 83.4
S-MORFESSOR 400 84.6 83.6 84.1
CRF 500 88.6 80.7 84.5
S-MORFESSOR 500 86.3 82.7 84.4
CRF 600 88.1 82.6 85.3
S-MORFESSOR 600 86.7 82.5 84.5
CRF 700 87.9 83.4 85.6
S-MORFESSOR 700 86.0 82.9 84.4
CRF 800 89.1 83.2 86.1
S-MORFESSOR 800 87.1 82.5 84.8
CRF 900 89.0 82.9 85.8
S-MORFESSOR 900 86.4 82.6 84.5
CRF 1000 89.8 83.5 86.5
S-MORFESSOR 1000 88.8 80.1 84.3
Table 5: Results for English on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 384,903 word types.
corporated in the model would be unchanged, the
choice of parameter estimation criterion and clas-
sifier is unlikely to have a dramatic effect on the
method performance.
In CRF training, we focused on the supervised
learning scenario, in which no unannotated data is
exploited in addition to the annotated training sets.
However, there does exist ample work on extend-
ing CRF training to the semi-supervised setting
(for example, see Mann and McCallum (2008)
and the references therein). Nevertheless, our re-
sults strongly suggest that it is crucial to use the
few available annotated training instances as ef-
ficiently as possible before turning model train-
ing burdensome by incorporating large amounts of
unannotated data.
Following previous work (Poon et al, 2009;
Kohonen et al, 2010; Virpioja et al, 2011), we
applied the boundary F-score evaluation measure,
while Green and DeNero (2012) reported charac-
ter accuracy. We consider the boundary F-score a
better measure than accuracy, since the boundary-
Method Train. Prec. Rec. F1
CRF 100 71.4 66.0 68.6
S-MORFESSOR 100 69.8 71.0 70.4
CRF 200 76.4 71.3 73.8
S-MORFESSOR 200 75.5 68.6 71.9
CRF 300 80.4 73.9 77.0
S-MORFESSOR 300 73.1 71.8 72.5
CRF 400 81.0 76.6 78.7
S-MORFESSOR 400 73.3 74.3 73.8
CRF 500 82.9 77.9 80.3
S-MORFESSOR 500 73.5 75.1 74.3
CRF 600 82.6 80.6 81.6
S-MORFESSOR 600 76.1 73.7 74.9
CRF 700 84.3 81.4 82.8
S-MORFESSOR 700 75.0 76.6 75.8
CRF 800 85.1 83.4 84.2
S-MORFESSOR 800 74.1 78.2 76.1
CRF 900 85.2 83.8 84.5
S-MORFESSOR 900 74.2 78.5 76.3
CRF 1000 86.0 84.7 85.3
S-MORFESSOR 1000 74.2 78.8 76.4
Table 6: Results for Finnish on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 2,206,719 word
types.
tag distribution is strongly skewed towards non-
boundaries. Nevertheless, for completeness, we
computed the character accuracy for our Arabic
data set, obtaining the accuracy 99.1%, which is
close to their reported accuracy of 98.6%. How-
ever, these values are not directly comparable due
to our use of the Bible corpus by Snyder and Barzi-
lay (2008) and their use of the Penn Arabic Tree-
bank (Maamouri et al, 2004).
7 Conclusions
We have presented an empirical study in data-
driven morphological segmentation employing
supervised boundary prediction methodology.
Specifically, we applied conditional random fields,
a discriminative log-linear model for segmentation
and tagging. From a methodological perspective,
this approach differs from the previous state-of-art
methods in two fundamental aspects. First, we uti-
lize a discriminative model estimated using only
annotated data. Second, we learn to predict morph
35
Method Train. Prec. Rec. F1
CRF 100 72.4 79.6 75.8
S-MORFESSOR 100 77.9 78.5 78.2
CRF 200 83.2 82.3 82.8
S-MORFESSOR 200 80.0 83.2 81.6
CRF 300 83.9 85.9 84.9
S-MORFESSOR 300 80.1 85.6 82.8
CRF 400 86.4 86.5 86.4
S-MORFESSOR 400 80.7 87.1 83.8
CRF 500 87.5 86.4 87.0
S-MORFESSOR 500 81.0 87.2 84.0
CRF 600 87.8 88.1 87.9
S-MORFESSOR 600 80.5 89.9 85.0
CRF 700 89.1 88.3 88.7
S-MORFESSOR 700 80.9 90.7 85.5
CRF 800 88.6 90.3 89.4
S-MORFESSOR 800 81.2 91.0 85.9
CRF 900 89.2 89.8 89.5
S-MORFESSOR 900 81.4 91.2 86.0
CRF 1000 89.9 90.4 90.2
S-MORFESSOR 1000 83.0 91.5 87.0
Table 7: Results for Turkish on the Morpho Chal-
lenge 2009/2010 data set (Kurimo et al, 2009; Ku-
rimo et al, 2010). The column titled Train. de-
notes the number of annotated training instances.
In addition to the annotated data, S-MORFESSOR
utilized an unannotated set of 617,298 word types.
boundaries based on their local character substring
contexts instead of learning a morph lexicon.
We showed that our supervised method yields
improved results compared to previous state-of-
art semi-supervised methods using the same small
amount of annotated data, while not utilizing the
unannotated data used by the reference methods.
This result has two implications. First, supervised
methods can provide excellent results in morpho-
logical segmentation already when there are only
a few annotated training instances available. This
is meaningful since for most languages it is infea-
sible to acquire large amounts of annotated train-
ing data. Second, performing morphological seg-
mentation by directly modeling segment bound-
aries can be advantageous compared to modeling
morph lexicons.
A potential direction for future work includes
evaluating the morphs obtained by our method in
real world applications, such as speech recognition
and information retrieval. We are also interested
in extending the method from fully supervised to
Method English Finnish Turkish
CRF 82.0 81.9 71.5
S-MORFESSOR 79.6 73.5 70.5
Table 8: F-measures of the Morpho Chal-
lenge boundary evaluation for CRF and S-
MORFESSOR using the full annotated training
data set.
semi-supervised learning.
Acknowledgements
This work was financially supported by Langnet
(Finnish doctoral programme in language studies)
and the Academy of Finland under the Finnish
Centre of Excellence Program 2012?2017 (grant
no. 251170), project Multimodally grounded lan-
guage technology (no. 254104), and LASTU Pro-
gramme (nos. 256887 and 259934).
References
M. Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2002), volume 10,
pages 1?8. Association for Computational Linguis-
tics.
M. Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,
J. Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,
M. Sara?lar, and A Stolcke. 2007. Morph-
based speech recognition and modeling of out-of-
vocabulary words across languages. ACM Transac-
tions on Speech and Language Processing, 5(1):3:1?
3:29, December.
S. Green and J. DeNero. 2012. A class-based
agreement model for generating accurately inflected
translations. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers-Volume 1, pages 146?155.
Association for Computational Linguistics.
H. Hammarstr?m and L. Borin. 2011. Unsupervised
learning of morphology. Computational Linguistics,
37(2):309?350, June.
T. Hirsim?ki, M. Creutz, V. Siivola, M. Kurimo, S. Vir-
pioja, and J. Pylkk?nen. 2006. Unlimited vocabu-
lary speech recognition with morph language mod-
els applied to Finnish. Computer Speech and Lan-
guage, 20(4):515?541, October.
O. Kohonen, S. Virpioja, and K. Lagus. 2010. Semi-
supervised learning of concatenative morphology.
In Proceedings of the 11th Meeting of the ACL Spe-
cial Interest Group on Computational Morphology
36
and Phonology, pages 78?86, Uppsala, Sweden,
July. Association for Computational Linguistics.
M. Kurimo, S. Virpioja, V. Turunen, G. W. Blackwood,
and W. Byrne. 2009. Overview and results of Mor-
pho Challenge 2009. In Working Notes for the CLEF
2009 Workshop, Corfu, Greece, September.
M. Kurimo, S. Virpioja, and V. Turunen. 2010.
Overview and results of Morpho Challenge 2010. In
Proceedings of the Morpho Challenge 2010 Work-
shop, pages 7?24, Espoo, Finland, September. Aalto
University School of Science and Technology, De-
partment of Information and Computer Science.
Technical Report TKK-ICS-R37.
J. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional random fields: Probabilistic models for
segmenting and labeling sequence data. In Proceed-
ings of the Eighteenth International Conference on
Machine Learning, pages 282?289.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki.
2004. The penn arabic treebank: Building a large-
scale annotated arabic corpus. In NEMLAR Con-
ference on Arabic Language Resources and Tools,
pages 102?109.
G. Mann and A. McCallum. 2008. Generalized expec-
tation criteria for semi-supervised learning of con-
ditional random fields. In Proceedings of ACL-
08: HLT, pages 870?878. Association for Compu-
tational Linguistics.
A. McCallum and W. Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of the seventh conference on Natural
language learning at HLT-NAACL 2003-Volume 4,
pages 188?191. Association for Computational Lin-
guistics.
A. McCallum, D. Freitag, and F. Pereira. 2000. Max-
imum entropy Markov models for information ex-
traction and segmentation. In Pat Langley, editor,
Proceedings of the Seventeenth International Con-
ference on Machine Learning (ICML 2000), pages
591?598, Stanford, CA, USA. Morgan Kaufmann.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics (COLING 2004), page 562. Association for
Computational Linguistics.
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsuper-
vised morphological segmentation with log-linear
models. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 209?217. Association for
Computational Linguistics.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology-Volume 1, pages 134?
141. Association for Computational Linguistics.
B. Snyder and R. Barzilay. 2008. Crosslingual prop-
agation for morphological analysis. In Proceedings
of the AAAI, pages 848?854.
M. Szummer, P. Kohli, and D. Hoiem. 2008. Learn-
ing CRFs using graph cuts. Computer Vision?ECCV
2008, pages 582?595.
V. Turunen and M. Kurimo. 2011. Speech retrieval
from unsegmented Finnish audio using statistical
morpheme-like units for segmentation, recognition,
and retrieval. ACM Transactions on Speech and
Language Processing, 8(1):1:1?1:25, October.
S. Virpioja, V. Turunen, S. Spiegler, O. Kohonen, and
M. Kurimo. 2011. Empirical comparison of eval-
uation methods for unsupervised learning of mor-
phology. Traitement Automatique des Langues,
52(2):45?90.
H. Zhao, C.N. Huang, and M. Li. 2006. An improved
chinese word segmentation system with conditional
random field. In Proceedings of the Fifth SIGHAN
Workshop on Chinese Language Processing, volume
1082117. Sydney: July.
37
