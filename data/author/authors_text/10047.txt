Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 489?496
Manchester, August 2008
PNR2: Ranking Sentences with Positive and Negative Reinforcement 
for Query-Oriented Update Summarization 
 
 Abstract 
Query-oriented update summarization is 
an emerging summarization task very 
recently. It brings new challenges to the 
sentence ranking algorithms that require 
not only to locate the important and 
query-relevant information, but also to 
capture the new information when 
document collections evolve. In this 
paper, we propose a novel graph based 
sentence ranking algorithm, namely PNR2, 
for update summarization. Inspired by the 
intuition that ?a sentence receives a 
positive influence from the sentences that 
correlate to it in the same collection, 
whereas a sentence receives a negative 
influence from the sentences that 
correlates to it in the different (perhaps 
previously read) collection?, PNR2 
models both the positive and the negative 
mutual reinforcement in the ranking 
process. Automatic evaluation on the 
DUC 2007 data set pilot task 
demonstrates the effectiveness of the 
algorithm.  
 
1 Introduction 
The explosion of the WWW has brought with it a 
vast board of information. It has become virtually 
impossible for anyone to read and understand 
large numbers of individual documents that are 
abundantly available. Automatic document 
summarization provides an effective means to 
                                                 
? 2008. Licensed under the Creative Commons 
Attribution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
manage such an exponentially increased 
collection of information and to support 
information seeking and condensing goals.  
The main evaluation forum that provides 
benchmarks for researchers working on 
document summarization to exchange their ideas 
and experiences is the Document Understanding 
Conferences (DUC). The goals of the DUC 
evaluations are to enable researchers to 
participate in large-scale experiments upon the 
standard benchmark and to increase the 
availability of appropriate evaluation techniques. 
Over the past years, the DUC evaluations have 
evolved gradually from single-document 
summarization to multi-document summarization 
and from generic summarization to query-
oriented summarization. Query-oriented multi-
document summarization initiated in 2005 aims 
to produce a short and concise summary for a 
collection of topic relevant documents according 
to a given query that describes a user?s particular 
interests. 
Previous summarization tasks are all targeted 
on a single document or a static collection of 
documents on a given topic. However, the 
document collections can change (actually grow) 
dynamically when the topic evolves over time. 
New documents are continuously added into the 
topic during the whole lifecycle of the topic and 
normally they bring the new information into the 
topic. To cater for the need of summarizing a 
dynamic collection of documents, the DUC 
evaluations piloted update summarization in 2007. 
The task of update summarization differs from 
previous summarization tasks in that the latter 
aims to dig out the salient information in a topic 
while the former cares the information not only 
salient but also novel.  
Up to the present, the predominant approaches 
in document summarization regardless of the 
nature and the goals of the tasks have still been 
built upon the sentence extraction framework. 
Li Wenjie1, Wei Furu1,2, Lu Qin1, He Yanxiang2 
1Department of Computing 
The Hong Kong Polytechnic University, HK 
{csfwei, cswjli, csluqin} 
@comp.polyu.edu.hk 
2Department of Computer Science 
and Technology, Wuhan University, China 
{frwei, yxhe@whu.edu.cn} 
489
Under this framework, sentence ranking is the 
issue of most concern. In general, two kinds of 
sentences need to be evaluated in update 
summarization, i.e. the sentences in an early (old) 
document collection A (denoted by SA) and the 
sentences in a late (new) document collection B 
(denoted by SB). Given the changes from SA to SB, 
an update summarization approach may be 
concerned about four ranking issues: (1) rank SA 
independently; (2) re-rank SA after SB comes; (3) 
rank SB independently; and (4) rank SB given that 
SA is provided. Among them, (4) is of most 
concern. It should be noting that both (2) and (4) 
need to consider the influence from the sentences 
in the same and different collections.  
In this study, we made an attempt to capture 
the intuition that  
?A sentence receives a positive influence from 
the sentences that correlate to it in the same 
collection, whereas a sentence receives a 
negative influence from the sentences that 
correlates to it in the different collection.? 
We represent the sentences in A or B as a text 
graph constructed using the same approach as 
was used in Erkan and Radev (2004a, 2004b). 
Different from the existing PageRank-like 
algorithms adopted in document summarization, 
we propose a novel sentence ranking algorithm, 
called PNR2 (Ranking with Positive and Negative 
Reinforcement). While PageRank models the 
positive mutual reinforcement among the 
sentences in the graph, PNR2 is capable of 
modeling both positive and negative 
reinforcement in the ranking process.  
The remainder of this paper is organized as 
follows. Section 2 introduces the background of 
the work presented in this paper, including 
existing graph-based summarization models, 
descriptions of update summarization and time-
based ranking solutions with web graph and text 
graph. Section 3 then proposes PNR2, a sentence 
ranking algorithm based on positive and negative 
reinforcement and presents a query-oriented 
update summarization model. Next, Section 4 
reports experiments and evaluation results. 
Finally, Section 5 concludes the paper. 
2 Background and Related Work 
2.1 Previous Work in Graph-based 
Document Summarization 
Graph-based ranking algorithms such as 
Google?s PageRank (Brin and Page, 1998) and 
Kleinberg?s HITS (Kleinberg, 1999) have been 
successfully used in the analysis of the link 
structure of the WWW. Now they are springing 
up in the community of document summarization. 
The major concerns in graph-based 
summarization researches include how to model 
the documents using text graph and how to 
transform existing web page ranking algorithms 
to their variations that could accommodate 
various summarization requirements. 
Erkan and Radev (2004a and 2004b) 
represented the documents as a weighted 
undirected graph by taking sentences as vertices 
and cosine similarity between sentences as the 
edge weight function. An algorithm called 
LexRank, adapted from PageRank, was applied 
to calculate sentence significance, which was 
then used as the criterion to rank and select 
summary sentences. Meanwhile, Mihalcea and 
Tarau (2004) presented their PageRank variation, 
called TextRank, in the same year. Besides, they 
reported experimental comparison of three 
different graph-based sentence ranking 
algorithms obtained from Positional Power 
Function, HITS and PageRank (Mihalcea and 
Tarau, 2005). Both HITS and PageRank 
performed excellently. 
Likewise, the use of PageRank family was also 
very popular in event-based summarization 
approaches (Leskovec et al, 2004; Vanderwende 
et al, 2004; Yoshioka and Haraguchi, 2004; Li et 
al., 2006). In contrast to conventional sentence-
based approaches, newly emerged event-based 
approaches took event terms, such as verbs and 
action nouns and their associated named entities 
as graph nodes, and connected nodes according 
to their co-occurrence information or semantic 
dependency relations. They were able to provide 
finer text representation and thus could be in 
favor of sentence compression which was 
targeted to include more informative contents in a 
fixed-length summary. Nevertheless, these 
advantages lied on appropriately defining and 
selecting event terms.  
All above-mentioned representative work was 
concerned with generic summarization. Later on, 
graph-based ranking algorithms were introduced 
in query-oriented summarization too when this 
new challenge became a hot research topic 
recently. For example, a topic-sensitive version 
of PageRank was proposed in (OtterBacher et al, 
2005). The same algorithm was followed by Wan 
et al (2006) and Lin et al (2007) who further 
investigated on its application in query-oriented 
update summarization.  
490
2.2 The DUC 2007 Update Summarization 
Task Description 
The DUC 2007 update summarization pilot task 
is to create short (100 words) multi-document 
summaries under the assumption that the reader 
has already read some number of previous 
documents. Each of 10 topics contains 25 
documents. For each topic, the documents are 
sorted in chronological order and then partitioned 
into three collections, ?A?, ?B? and ?C?. The 
participants are then required to generate (1) a 
summary for ?A?; (2) an update summary for 
?B? assuming documents in ?A? have already 
been read; and (3) an update summary for ?C? 
assuming documents in ?A? and ?B? have 
already been read. Growing out of the DUC 2007, 
the Text Analysis Conference (TAC) 2008 
planed to keep only the DUC 2007 task (1) and 
(2). 
Each topic collection in the DUC 2007 (will 
also in the TAC 2008) is accompanied with a 
query that describes a user?s interests and focuses. 
System-generated summaries should include as 
many responses relevant to the given query as 
possible. Here is a query example from the DUC 
2007 document collection ?D0703A?.  
<topic> 
<num> D0703A </num> 
<title> Steps toward introduction of the 
Euro. </title> 
<narr> Describe steps taken and worldwide 
reaction prior to introduction of the Euro on 
January 1, 1999. Include predictions and 
expectations reported in the press. </narr> 
</topic>                                          [D0703A] 
Update summarization is definitely a time-
related task. An appropriate ranking algorithm 
must be the one capable of coping with the 
change or the time issues.  
2.3 Time-based Ranking Solutions with 
Web Graph and Text Graph 
Graph based models in document summarization 
are inspired by the idea behind web graph models 
which have been successfully used by current 
search engines. As a matter of fact, adding time 
dimension into the web graph has been 
extensively studied in recent literature. 
Basically, the evolution in the web graph stems 
from (1) adding new edges between two existing 
nodes; (2) adding new nodes in the existing graph 
(consequently adding new edges between the 
existing nodes and the new nodes or among the 
new nodes); and (3) deleting existing edges or 
nodes. Berberich et al (2004 and 2005) 
developed two link analysis methods, i.e. T-Rank 
Light and T-Rank, by taking into account two 
temporal aspects, i.e. freshness (i.e. timestamp of 
most recent update) and activity (i.e. update rates) 
of the pages and the links. They modeled the web 
as an evolving graph in which each nodes and 
edges (i.e. web pages and hyperlinks) were 
annotated with time information. The time 
information in the graph indicated different kinds 
of events in the lifespan of the nodes and edges, 
such as creation, deletion and modifications. 
Then they derived a subgraph of the evolving 
graph with respect to the user?s temporal interest. 
Finally, the time information of the nodes and the 
edges were used to modify the random walk 
model as was used in PageRank. Specifically, 
they used it to modify the random jump 
probabilities (in both T-Rank Light and T-Rank) 
and the transition probabilities (in T-Rank only).  
Meanwhile, Yu et al (2004 and 2005) 
introduced a time-weighted PageRank, called 
TimedPageRank, for ranking in a network of 
scientific publications. In their approach, 
citations were weighted based on their ages. Then 
a post-processing step decayed the authority of a 
publication based on the publication?s age. Later, 
Yang et al (2007) proposed TemporalRank, 
based on which they computed the page 
importance from two perspectives: the 
importance from the current web graph snapshot 
and the accumulated historical importance from 
previous web graph snapshot. They used a kinetic 
model to interpret TemporalRank and showed it 
could be regarded as a solution to an ordinary 
differential equation.  
In conclusion, Yu et al tried to cope with the 
problem that PageRank favors over old pages 
whose in-degrees are greater than those of new 
pages. They worked on a static single snapshot of 
the web graph, and their algorithm could work 
well on all pages in the web graph. Yang et al, 
on the other hand, worked on a series of web 
graphs at different snapshots. Their algorithm 
was able to provide more robust ranking of the 
web pages, but could not alleviate the problem 
carried by time dimension at each web graph 
snapshot. This is because they directly applied 
the original PageRank to rank the pages. In other 
words, the old pages still obtained higher scores 
while the newly coming pages still got lower 
scores. Berberich et al focused their efforts on 
the evolution of nodes and edges in the web 
graph. However, their algorithms did not work 
491
when the temporal interest of the user (or query) 
was not available.  
As for graph based update summarization, 
Wan (2007) presented the TimedTextRank 
algorithm by following the same idea presented 
in the work of Yu et al Given three collections of 
chronologically ordered documents, Lin et al 
(2007) proposed to construct the TimeStamped 
graph (TSG) graph by incrementally adding the 
sentences to the graph. They modified the 
construction of the text graph, but the ranking 
algorithm was the same as the one proposed by 
OtterBacher et al  
Nevertheless, the text graph is different from 
the web graph. The evolution in the text graph is 
limited to the type (2) in the web graph. The 
nodes and edges can not be deleted or modified 
once they are inserted. In other words, we are 
only interested in the changes caused when new 
sentences are introduced into the existing text 
graph. As a result, the ideas from Berberich et al 
cannot be adopted directly in the text graph. 
Similarly, the problem in web graph as stated in 
the work of Yu et al (i.e. ?new pages, which may 
be of high quality, have few or no in-links and 
are left behind.?) does not exist in the text graph 
at all. More precisely, the new coming sentences 
are equally treated as the existing sentences, and 
the degree (in or out) of the new sentences are 
also equally accumulated as the old sentences. 
Directly applying the ideas from the work of Yu 
et al does not always make sense in the text 
graph. Recall that the main task for sentence 
ranking in update summarization is to rank SB 
given SA. So the idea from Yang et al is also not 
applicable.  
In fact, the key points include not only 
maximizing the importance in the current new 
document collection but also minimizing the 
redundancy to the old document collection when 
ranking the sentences for update summarization. 
Time dimension does contribute here, but it is not 
the only way to consider the changes. Unlike the 
web graph, the easily-captured content 
information in a text graph can provide additional 
means to analyze the influence of the changes.  
To conclude the previous discussions, adding 
temporal information to the text graph is different 
from it in the web graph. Capturing operations 
(such as addition, deletion, modification of web 
pages and hyperlinks) is most concerned in the 
web graph; however, prohibiting redundant 
information from the old documents is the most 
critical issue in the text graph. 
3 Positive and Negative Reinforcement 
Ranking for Update Summarization 
Existing document summarization approaches 
basically follow the same processes: (1) first 
calculate the significance of the sentences with 
reference to the given query with/without using 
some sorts of sentence relations; (2) then rank the 
sentences according to certain criteria and 
measures; (3) finally extract the top-ranked but 
non-redundant sentences from the original 
documents to create a summary. Under this 
extractive framework, undoubtedly the two 
critical processes involved are sentence ranking 
and sentence selection. In the following sections, 
we will first introduce the sentence ranking 
algorithm based on ranking with positive and 
negative reinforcement, and then we present the 
sentence selection strategy. 
3.1 Ranking with Positive and Negative 
Reinforcement (PNR2) 
Previous graph-based sentence ranking 
algorithms is capable to model the fact that a 
sentence is important if it correlates to (many) 
other important sentences. We call this positive 
mutual reinforcement. In this paper, we study two 
kinds of reinforcement, namely positive and 
negative reinforcement, among two document 
collections, as illustrated in Figure 1.  
 
Figure 1 Positive and Negative Reinforcement 
In Figure 1, ?A? and ?B? denote two document 
collections about the same topics (?A? is the old 
document collection, ?B? is the new document 
collection), SA and SB denote the sentences in 
?A? and ?B?. We assume: 
1. SA performs positive reinforcement on its 
own internally; 
2. SA performs negative reinforcement on SB 
externally; 
3. SB performs negative reinforcement on SA 
externally;  
4. SB performs positive reinforcement on its 
own internally. 
Positive reinforcement captures the intuition 
that a sentence is more important if it associates 
to the other important sentences in the same 
collection. Negative reinforcement, on the other 
hand, reflects the fact that a sentence is less 
A B + + 
- 
- 
492
important if it associates to the important 
sentences in the other collection, since such a 
sentence might repeat the same or very similar 
information which is supposed to be included in 
the summary generated for the other collection.  
Let RA and RB denote the ranking of the 
sentences in A and B, the reinforcement can be 
formally described as 
??
??
?
?+??+??=
?+??+??=
+
+
B
k
BBB
k
ABA
k
B
A
k
BAB
k
AAA
k
A
pRMRMR
pRMRMR
r
r
2
)(
2
)(
2
)1(
1
)(
1
)(
1
)1(
???
???
 (1) 
where the four matrices MAA, MBB, MAB and MBA 
are the affinity matrices of the sentences in SA, in 
SB, from SA to SB and from SB to SA. 
??
?
??
?
=
22
11
??
??
W  is a weight matrix to balance the 
reinforcement among different sentences. Notice 
that 0, 21 <??  such that they perform negative 
reinforcement. Ap
r
 and Bp
r
 are two bias vectors, 
with 1,0 21 << ??  as the damping factors. [ ]
1
1
?
=
n
A n
pr , where n is the order of MAA. Bp
r
 is 
defined in the same way. We will further define 
the affinity matrices in section 3.2 later. With the 
above reinforcement ranking equation, it is also 
true that 
1. A sentence in SB correlates to many new 
sentences in SB is supposed to receive a high 
ranking from RB, and 
2. A sentence in SB correlates to many old 
sentences in SA is supposed to receive a low 
ranking from RB. 
Let [ ]TBA RRR =  and [ ]TBA ppp rrr ??= 21 ?? , then 
the above iterative equation (1) corresponds to 
the linear system, 
( ) pRMI r=??                            (2) 
where, ??
?
??
?
=
BBBA
ABAA
MM
MM
M
22
11
??
??
. 
Up to now, the PNR2 is still query-independent. 
That means only the content of the sentences is 
considered. However, for the tasks of query-
oriented summarization, the reinforcement should 
obviously bias to the user?s query. In this work, 
we integrate query information into PNR2 by 
defining the vector pr  as ( )qsrelp ii |=r , where 
( )qsrel i |  denotes the relevance of the sentence si 
to the query q. 
To guarantee the solution of the linear system 
Equation (2), we make the following two 
transformations on M. First M is normalized by 
columns. If all the elements in a column are zero, 
we replace zero elements with n1  (n is the total 
number of the elements in that column). Second, 
M is multiplied by a decay factor ? ( 10 <<? ), 
such that each element in M is scaled down but 
the meaning of M will not be changed.  
Finally, Equation (2) is rewritten as, 
( ) pRMI r=??? ?                        (3) 
The matrix ( )MI ???  is a strictly diagonally 
dominant matrix now, and the solution of the 
linear system Equation (3) exists.  
3.2 Sentence Ranking based on PNR2 
We use the above mentioned PNR2 framework to 
rank the sentences in both SA and SB 
simultaneously. Section 3.2 defines the affinity 
matrices and presents the ranking algorithm. 
The affinity (i.e. similarity) between two 
sentences is measured by the cosine similarity of 
the corresponding two word vectors, i.e.  
[ ] ( )ji sssimjiM ,, =                     (4) 
where ( )
ji
ji
ji
ss
ss
sssim rr
rr
?
?
=,
. However, when 
calculating the affinity matrices MAA and MBB, the 
similarity of a sentence to itself is defined as 0, 
i.e. 
[ ] ( )
??
?
=
?
= ji
jisssimjiM ji
             0
,
,              (5) 
Furthermore, the relevance of a sentence to the 
query q is defined as 
( )
qs
qs
qsrel
i
i
i rr
rr
?
?
=,                     (6) 
Algorithm 1. RankSentence(SA, SB, q) 
Input: The old sentence set SA, the new 
sentence set SB, and the query q. 
Output: The ranking vectors R of SA and SB. 
1: Construct the affinity matrices, and set the 
weight matrix W; 
2: Construct the matrix ( )MIA ??= ? .  
3: Choose (randomly) the initial non-negative 
vectors TR ]11[)0( L= ; 
4: 0?k , 0?? ; 
5: Repeat 
6:     ( )? ?< >++ ??= ij ij kjijkjiji
ij
k
i RaRap
a
R )()1()1( 1 r ; 
7:     ( ))()1(max kk RR ??? + ; 
8:  )1( +kR is normalized such that the maximal 
element in )1( +kR is 1. 
493
9:     1+? kk ; 
10: Until ?<? 1; 
11: )(kRR ? ; 
12: Return. 
Now, we are ready to adopt the Gauss-Seidel 
method to solve the linear system Equation (3), 
and an iterative algorithm is developed to rank 
the sentences in SA and SB. 
After sentence ranking, the sentences in SB 
with higher ranking will be considered to be 
included in the final summary.  
3.3 Sentence Selection by Removing 
Redundancy 
When multiple documents are summarized, the 
problem of information redundancy is more 
severe than it is in single document 
summarization. Redundancy removal is a must. 
Since our focus is designing effective sentence 
ranking approach, we apply the following simple 
sentence selection algorithm. 
Algorithm 2. GenerateSummary(S, length) 
Input: sentence collection S (ranked in 
descending order of significance) and length 
(the given summary length limitation) 
Output: The generated summary ?  
{}?? ; 
?l length; 
For i ?  0 to |S| do 
     threshold ? ( )( )??ssssim i   ,max ; 
     If threshold <= 0.92 do 
          isU??? ; 
          ll ? - ( )islen ;  
          If ( l <= 0) break; 
     End 
End 
Return ? . 
 
4 Experimental Studies 
4.1 Data Set and Evaluation Metrics 
The experiments are set up on the DUC 2007 
update pilot task data set. Each collection of 
documents is accompanied with a query 
description representing a user?s information 
need. We simply focus on generating a summary 
for the document collection ?B? given that the 
                                                 
1
 ?  is a pre-defined small real number as the 
convergence threshold. 
2
 In fact, this is a tunable parameter in the algorithm. 
We use the value of 0.9 by our intuition. 
user has read the document collection ?A?, which 
is a typical update summarization task.  
Table 1 below shows the basic statistics of the 
DUC 2007 update data set. Stop-words in both 
documents and queries are removed 3  and the 
remaining words are stemmed by Porter 
Stemmer 4 . According to the task definition, 
system-generated summaries are strictly limited 
to 100 English words in length. We incrementally 
add into a summary the highest ranked sentence 
of concern if it doesn?t significantly repeat the 
information already included in the summary 
until the word limitation is reached. 
 A B 
Average number of documents 10 10 
Average number of sentences 237.6 177.3 
Table 1. Basic Statistics of DUC2007 Update Data Set 
As for the evaluation metric, it is difficult to 
come up with a universally accepted method that 
can measure the quality of machine-generated 
summaries accurately and effectively. Many 
literatures have addressed different methods for 
automatic evaluations other than human judges. 
Among them, ROUGE5 (Lin and Hovy, 2003) is 
supposed to produce the most reliable scores in 
correspondence with human evaluations. Given 
the fact that judgments by humans are time-
consuming and labor-intensive, and more 
important, ROUGE has been officially adopted 
for the DUC evaluations since 2005, like the 
other researchers, we also choose it as the 
evaluation criteria. 
In the following experiments, the sentences 
and the queries are all represented as the vectors 
of words. The relevance of a sentence to the 
query is calculated by cosine similarity. Notice 
that the word weights are normally measured by 
the document-level TF*IDF scheme in 
conventional vector space models. However, we 
believe that it is more reasonable to use the 
sentence-level inverse sentence frequency (ISF) 
rather than document-level IDF when dealing 
with sentence-level text processing. This has 
been verified in our early study. 
4.2 Comparison of Positive and Negative 
Reinforcement Ranking Strategy 
The aim of the following experiments is to 
investigate the different reinforcement ranking 
strategies. Three algorithms (i.e. PR(B), 
                                                 
3
 A list of 199 words is used to filter stop-words. 
4
 http://www.tartarus.org/~martin/PorterStemmer. 
5
 ROUGE version 1.5.5 is used. 
494
PR(A+B), PR(A+B/A)) are implemented as 
reference. These algorithms are all based on the 
query-sensitive LexRank (OtterBacher et al, 
2005). The differences are two-fold: (1) the 
document collection(s) used to build the text 
graph are different; and (2) after ranking, the 
sentence selection strategies are different. In 
particular, PR(B) only uses the sentences in ?B? 
to build the graph, and the other two consider the 
sentences in both ?A? and in ?B?. Only the 
sentences in ?B? are considered to be selected in 
PR(B) and PR(A+B/A), but all the sentences in 
?A? and ?B? have the same chance to be selected 
in PR(A+B). Only the sentences from B are 
considered to be selected in the final summaries 
in PNR2 as well. In the following experiments, 
the damping factor is set to 0.85 in the first three 
algorithms as the same in PageRank. The weight 
matrix W is set to ??
?
??
?
?
?
15.0
5.01
 in the proposed 
algorithm (i.e. PNR2) and 5.021 == ?? . We have 
obtained reasonable good results with the decay 
factor ?  between 0.3 and 0.8. So we set it to 0.5 
in this paper.  
Notice that the three PageRank-like graph-
based ranking algorithms can be viewed as only 
the positive reinforcement among the sentences is 
considered, while both positive and negative 
reinforcement are considered in PNR2 as 
mentioned before. Table 2 below shows the 
results of recall scores of ROUGE-1, ROUGE-2 
and ROUGE-SU4 along with their 95% 
confidential internals within square brackets.  
 ROUGE 
-1 
ROUGE 
-2 
ROUGE-
SU4 
PR(B) 0.3323 [0.3164,0.3501] 
0.0814 
[0.0670,0.0959] 
0.1165 
0.1053,0.1286] 
PR(A+B) 0.3059 [0.2841,0.3256] 
0.0746 
[0.0613,0.0893] 
0.1064 
[0.0938,0.1186] 
PR(A+B/A) 0.3376 [0.3186,0.3572] 
0.0865 
[0.0724,0.1007] 
0.1222 
[0.1104,0.1304] 
PNR2 0.3616 [0.3464,0.3756] 
0.0895 
[0.0810,0.0987] 
0.1291 
[0.1208,0.1384] 
Table 2. Experiment Results 
We come to the following three conclusions. 
First, it is not surprising that PR(B) and 
PR(A+B/A) outperform PR(A+B), because the 
update task obviously prefers the sentences from 
the new documents (i.e. ?B?). Second, 
PR(A+B/A) outperforms PR(B) because the 
sentences in ?A? can provide useful information 
in ranking the sentences in ?B?, although we do 
not select the sentences ranked high in ?A?. Third, 
PNR2 achieves the best performance. PNR2 is 
above PR(A+B/A) by 7.11% of ROUGE-1, 
3.47% of ROUGE-2, and 5.65% of ROUGE-SU4. 
This result confirms the idea and algorithm 
proposed in this work. 
4.3 Comparison with DUC 2007 Systems 
Twenty-four systems have been submitted to the 
DUC for evaluation in the 2007 update task. 
Table 3 compares our PNR2 with them. For 
reference, we present the following representative 
ROUGE results of (1) the best and worst 
participating system performance, and (2) the 
average ROUGE scores (i.e. AVG). We can then 
easily locate the positions of the proposed models 
among them. 
 PNR2 Mean Best / Worst 
ROUGE-1 0.3616 0.3262 0.3768/0.2621 
ROUGE2 0.0895 0.0745 0.1117/0.0365 
ROUGE-SU4 0.1291 0.1128 0.1430/0.0745 
Table 3. System Comparison 
4.4 Discussion 
In this work, we use the sentences in the same 
sentence set for positive reinforcement and 
sentences in the different set for negative 
reinforcement. Precisely, the old sentences 
perform negative reinforcement over the new 
sentences while the new sentences perform 
positive reinforcement over each other. This is 
reasonable although we may have a more 
comprehensive alternation. Old sentences may 
express old topics, but they may also express 
emerging new topics. Similarly, new sentences 
are supposed to express new topics, but they may 
also express the continuation of old topics. As a 
result, it will be more comprehensive to classify 
the whole sentences (both new sentences and old 
sentences together) into two categories, i.e. old 
topics oriented sentences and new topic oriented 
sentences, and then to apply these two sentence 
sets in the PNR2 framework. This will be further 
studied in our future work. 
Moreover, in the update summarization task, 
the summary length is restricted to about 100 
words. In this situation, we find that sentence 
simplification is even more important in our 
investigations. We will also work on this issue in 
our forthcoming studies. 
5 Conclusion 
In this paper, we propose a novel sentence 
ranking algorithm, namely PNR2, for update 
summarization. As our pilot study, we simply 
assume to receive two chronologically ordered 
document collections and evaluate the summaries 
495
generated for the collection given later. With 
PNR2, sentences from the new (i.e. late) 
document collection perform positive 
reinforcement among each other but they receive 
negative reinforcement from the sentences in the 
old (i.e. early) document collection. Positive and 
negative reinforcement are concerned 
simultaneously in the ranking process. As a result, 
PNR2 favors the sentences biased to the sentences 
that are important in the new collection and 
meanwhile novel to the sentences in the old 
collection. As a matter of fact, this positive and 
negative ranking scheme is general enough and 
can be used in many other situations, such as 
social network analysis etc. 
 
Acknowledgements 
The research work presented in this paper was 
partially supported by the grants from RGC of 
HKSAR (Project No: PolyU5217/07E), NSF of 
China (Project No: 60703008) and the Hong 
Kong Polytechnic University (Project No: A-
PA6L). 
 
References 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2004. G.T-Rank: Time-Aware Authority 
Ranking. In Algorithms and Models for the Web-
Graph: Third International Workshop, WAW, pp 
131-141. 
Klaus Berberich, Michalis Vazirgiannis, and Gerhard 
Weikum. 2005. Time-Aware Authority Ranking. 
Journal of Internet Mathematics, 2(3): 301-332. 
Klaus Lorenz Berberich. 2004. Time-aware and 
Trend-based Authority Ranking. Master Thesis, 
Saarlandes  University, Germany. 
Sergey Brin and Lawrence Page. 1998. The Anatomy 
of a Large-scale Hypertextual Web Search Engine. 
Computer Networks and ISDN Systems, 30(1-
7):107-117. 
Gunes Erkan and Dragomir R. Radev. 2004a. 
LexPageRank: Prestige in Multi-Document Text 
Summarization, in Proceedings of EMNLP, pp365-
371. 
Gunes Erkan and Dragomir R. Radev. 2004b. 
LexRank: Graph-based Centrality as Salience in 
Text Summarization, Journal of Artificial 
Intelligence Research 22:457-479. 
Jon M. Kleinberg. 1999. Authoritative Sources in 
Hyperlinked Environment, Journal of the ACM, 
46(5):604-632. 
Jure Leskovec, Marko Grobelnik and Natasa Milic-
Frayling. 2004. Learning Sub-structures of 
Document Semantic Graphs for Document 
Summarization, in Proceedings of LinkKDD 
Workshop, pp133-138. 
Wenjie Li, Mingli Wu, Qin Lu, Wei Xu and Chunfa 
Yuan. 2006. Extractive Summarization using Intra- 
and Inter-Event Relevance, in Proceedings of 
ACL/COLING, pp369-376. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics, in Proceedings of HLT-
NAACL, pp71-78. 
Ziheng Lin, Tat-Seng Chua, Min-Yen Kan, Wee Sun 
Lee, Long Qiu, and Shiren Ye. 2007. NUS at DUC 
2007: Using Evolutionary Models for Text. In 
Proceedings of Document Understanding 
Conference (DUC) 2007. 
Rada Mihalcea and Paul Tarau. 2004. TextRank - 
Bringing Order into Text, in Proceedings of 
EMNLP, pp404-411. 
Rada Mihalcea. 2004. Graph-based Ranking 
Algorithms for Sentence Extraction, Applied to 
Text Summarization, in Proceedings of ACL 
(Companion Volume). 
Jahna OtterBacher, Gunes Erkan, Dragomir R. Radev. 
2005. Using Random Walks for Question-focused 
Sentence Retrieval, in Proceedings of 
HLT/EMNLP, pp915-922. 
Lucy Vanderwende, Michele Banko and Arul 
Menezes. 2004. Event-Centric Summary 
Generation, in Working Notes of DUC 2004. 
Xiaojun Wan, Jianwu Yang and Jianguo Xiao. 2006. 
Using Cross-Document Random Walks for Topic-
Focused Multi-Document Summarization, in 
Proceedings of the 2006 IEEE/WIC/ACM 
International Conference on Web Intelligence, 
pp1012-1018. 
Xiaojun Wan. 2007. TimedTextRank: Adding the 
Temporal Dimension to Multi-document 
Summarization. In Proceedings of 30th ACM 
SIGIR, pp 867-868. 
Lei Yang, Lei Qi, Yan-Ping Zhao, Bin Gao, and Tie-
Yan Liu. 2007. Link Analysis using Time Series of 
Web Graphs. In Proceedings of CIKM?07. 
Masaharu Yoshioka and Makoto Haraguchi. 2004. 
Multiple News Articles Summarization based on 
Event Reference Information, in Working Notes of 
NTCIR-4. 
Philip S. Yu, Xin Li, and Bing Liu. 2004. On the 
Temporal Dimension of Search. In Proceedings of 
the 13th International World Wide Web Conference 
on Alternate Track Papers and Posters, pp 448-449. 
Philip S. Yu, Xin Li, and Bing Liu. 2005. Adding the 
Temporal Dimension to Search ? A Case Study in 
Publication Search. In Proceedings of the 2005 
IEEE/WIC/ACM International Conference on Web 
Intelligence. 
496
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 117?120,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
 
Co-Feedback Ranking for Query-Focused Summarization 
Furu Wei1,2,3  Wenjie Li1 and Yanxiang He2 
1 The Hong Kong Polytechnic University, Hong Kong 
{csfwei,cswjli}@comp.polyu.edu.hk 
2 Wuhan University, China 
{frwei,yxhe}@whu.edu.cn
3 IBM China Research Laboratory, Beijing, China 
 
Abstract 
In this paper, we propose a novel ranking 
framework ? Co-Feedback Ranking (Co-
FRank), which allows two base rankers to 
supervise each other during the ranking 
process by providing their own ranking results 
as feedback to the other parties so as to boost 
the ranking performance. The mutual ranking 
refinement process continues until the two 
base rankers cannot learn from each other any 
more. The overall performance is improved by 
the enhancement of the base rankers through 
the mutual learning mechanism. We apply this 
framework to the sentence ranking problem in 
query-focused summarization and evaluate its 
effectiveness on the DUC 2005 data set. The 
results are promising.  
1 Introduction and Background 
Sentence ranking is the issue of most concern in 
extractive summarization. Feature-based 
approaches rank the sentences based on the 
features elaborately designed to characterize the 
different aspects of the sentences. They have 
been extensively investigated in the past due to 
their easy implementation and the ability to 
achieve promising results. The use of feature-
based ranking has led to many successful (e.g. 
top five) systems in DUC 2005-2007 query-
focused summarization (Over et al, 2007). A 
variety of statistical and linguistic features, such 
as term distribution, sentence length, sentence 
position, and named entity, etc., can be found in 
literature. Among them, query relevance, 
centroid (Radev et al, 2004) and signature term 
(Lin and Hovy, 2000) are most remarkable.  
There are two alternative approaches to 
integrate the features. One is to combine features 
into a unified representation first, and then use it 
to rank the sentences. The other is to utilize rank 
fusion or rank aggregation techniques to combine 
the ranking results (orders, ranks or scores) 
produced by the multiple ranking functions into a 
unified rank. The most popular implementation 
of the latter approaches is to linearly combine the 
features to obtain an overall score which is then 
used as the ranking criterion. The weights of the 
features are either experimentally tuned or 
automatically derived by applying learning-based 
mechanisms. However, both of the above-
mentioned ?combine-then-rank? and ?rank-then-
combine? approaches have a common drawback. 
They do not make full use of the information 
provided by the different ranking functions and 
neglect the interaction among them before 
combination. We believe that each individual 
ranking function (we call it base ranker) is able 
to provide valuable information to the other base 
rankers such that they learn from each other by 
means of mutual ranking refinement, which in 
turn results in overall improvement in ranking. 
To the best of our knowledge, this is a research 
area that has not been well addressed in the past.  
The inspiration for the work presented in this 
paper comes from the idea of Co-Training (Blum 
and Mitchell, 1998), which is a very successful 
paradigm in the semi-supervised learning 
framework for classification. In essence, co-
training employs two weak classifiers that help 
augment each other to boost the performance of 
the learning algorithms. Two classifiers mutually 
cooperate with each other by providing their own 
labeling results to enrich the training data for the 
other parties during the supervised learning 
process. Analogously, in the context of ranking, 
although each base ranker cannot decide the 
overall ranking well on itself, its ranking results 
indeed reflect its opinion towards the ranking 
from its point of view. The two base rankers can 
then share their own opinions by providing the 
ranking results to each other as feedback. For 
each ranker, the feedback from the other ranker 
contains additional information to guide the 
refinement of its ranking results if the feedback 
is defined and used appropriately. This process 
continues until the two base rankers can not learn 
from each other any more. We call this ranking 
paradigm Co-Feedback Ranking (Co-FRank). 
The way how to use the feedback information 
117
 varies depending on the nature of a ranking task. 
In this paper, we particularly consider the task of 
query-focused summarization. We design a new 
sentence ranking algorithm which allows a 
query-dependent ranker and a query-independent 
ranker mutually learn from each other under the 
Co-FRank framework. 
2 Co-Feedback Ranking for Query-
Focused Summarization 
2.1 Co-Feedback Ranking Framework 
Given a set of objects O, one can define two base 
ranker f1 and f2:     Ooofof ??o?o ,, 21 . The 
ranking results produced by f1 and f2 individually 
are by no means perfect but the two rankers can 
provide relatively reasonable ranking 
information to supervise each other so as to 
jointly improve themselves. One way to do Co-
Feedback ranking is to take the most confident 
ranking results (e.g. highly ranked instances 
based on orders, ranks or scores) from one base 
ranker as feedback to update the other?s ranking 
results, and vice versa. This process continues 
iteratively until the termination condition is 
reached, as depicted in Procedure 1. While the 
standard Co-Training algorithm requires two 
sufficient and redundant views, we suggest f1 and 
f2 be two independent rankers which emphasize 
two different aspects of the objects in O. 
Procedure 1. Co-FRank(f1, f2, O) 
1:  Rank O with f1 and obtain the ranking results r1; 
2:  Rank O with f2 and obtain the ranking results r2; 
3:  Repeat  
4:  Select the top N ranked objects 1W  from r1 as 
feedback to supervise f2, and re-rank O using f2 
and 1W ; Update r2; 
5:  Select the top N ranked objects 2W  from r2 as 
feedback to supervise f1, and re-rank O using f1 
and 2W ; Update r1; 
5:  Until I(O). 
The termination condition I(O) can be defined 
according to different application scenarios. For 
example, I(O) may require the top K ranked 
objects in r1 and r2 to be identical if one is 
particularly interested in the top ranked objects. 
It is also very likely that r1 and r2 do not change 
any more after several iterations (or the top K 
objects do not change). In this case, the two base 
rankers can not learn from each other any more, 
and the Co-Feedback ranking process should 
terminate either. The final ranking results can be 
easily determined by combining the two base 
rankers without any parameter, because they 
have already learnt from each other and can be 
equally treated.  
2.2 Query-Focused Summarization based 
on Co-FRank  
The task of query-focused summarization is to 
produce a short summary (250 words in length) 
for a set of related documents D with respect to 
the query q that reflects a user?s information 
need. We follow the traditional extractive 
summarization framework in this study, where 
the two critical processes involved are sentence 
ranking and sentence selection, yet we focus 
more on the sentence ranking algorithm based on 
Co-FRank. As for sentence selection, we 
incrementally add into the summary the highest 
ranked sentence if it doesn?t significantly repeat1 
the information already included in the summary 
until the word limitation is reached. 
In the context of query-focused summarization, 
two kinds of features, i.e. query-dependent and 
query-independent features are necessary and 
they are supposed to complement each other. We 
then use these two kinds of features to develop 
the two base rankers. The query-dependent 
feature (i.e. the relevance of the sentence s to the 
query q) is defined as the cosine similarity 
between s and q.  
    qsqsqsqsrelf ?x  ? /,cos,1  (1) 
The words in s and q vectors are weighted by 
tf*isf. Meanwhile, the query-independent feature 
(i.e. the sentence significance based on word 
centroid) is defined as 
    swcscf
sw
/2 ? ? ?   (2) 
where c(w) is the centroid weight of the word w 
in s and     DSDs ws Nisftfwc w? ? ? . DSN  is the total 
number of the sentences in D, s
w
tf  is the 
frequency of w in s, and  wDw sfNisf Slog  is the 
inverse sentence frequency (ISF) of w, where sfw  
is the sentence frequency of w in D. The sentence 
ranking algorithm based on Co-FRank is detailed 
in the following Algorithm 1.  
Algorithm 1. Co-FRank(f1, f2, D, q) 
1:  Extract sentences S={s1, ? sm} from D;  
2:  Rank S with f1 and obtain the ranking results r1; 
3:  Rank S with f2 and obtain the ranking results r2; 
4:  Normalize r1,            11111 minmaxmin rrrsrsr ii  ;
5:  Normalize r2,            22222 minmaxmin rrrsrsr ii  ; 
6:  Repeat  
                                                 
1 A sentence is discarded if the cosine similarity of it to any 
sentence already selected into the summary is greater than 
0.9. 
118
 7:  Select the top N ranked sentences at round n n
1
W  
from r1 as feedback for f2, and re-rank S using f2 
and n
1
W ,                                                
        nssims n
k
k
ii /,
1
2 1? m WS ,     22 222 minmax
min
SS
SSS 
  
            iii ssfsr 222 1 SKK ??m                         (3)
8: Select the top N ranked sentences at round n n2W  
from r2 as feedback for f1, and re-rank S using f1
and n2W ;  
         nssims n
k
k
ii /,
1
1 2? m WS ,     11 111 minmax
min
SS
SSS 
    
            iii ssfsr 111 1 SKK ??m                              (4) 
9: Until the top K sentences in r1 and r2 are the same, 
both r1 and r2 do not change any more, or 
maximum iteration round is achieved. 
10: Calculate the final ranking results, 
            221 iii srsrsr  .                                        (5) 
The update strategies used in Algorithm 1, as 
formulated in Formulas (3) and (4), are designed 
based on the intuition that the new ranking of the 
sentence s from one base ranker (say f1) consists 
of two parts. The first part is the initial ranking 
produced by f1. The second part is the similarity 
between s and the top N feedback provided by 
the other ranker (say f2), and vice versa. The top 
K ranked sentences by f2 are supposed to be 
highly supported by f2. As a result, a sentence 
that is similar to those top ranked sentences 
should deserve a high rank as well.  nissim 2,W  
captures the effect of such feedback at round n 
and the definition of it may vary with regard to 
the application background. For example, it can 
be defined as the maximum, the minimum or the 
average similarity value between si and a set of 
feedback sentences in 2W . Through this mutual 
interaction, the two base rankers supervise each 
other and are expected as a whole to produce 
more reliable ranking results.  
We assume each base ranker is most confident 
with its first ranked sentence and set N to 1. 
Accordingly,  nissim 2,W is defined as the similarity 
between si and the one sentence in n
2
W . K  is a 
balance factor which can be viewed as the 
proportion of the dependence of the new ranking 
results on its initial ranking results. K is set to 10 
as 10 sentences are basically sufficient for the 
summarization task we work on. We carry out at 
most 5 iterations in the current implementation. 
3 Experimental Study   
We take the DUC 2005 data set as the evaluation 
corpus in this preliminary study. ROUGE (Lin 
and Hovy, 2003), which has been officially 
adopted in the DUC for years is used as the 
evaluation criterion. For the purpose of 
comparison, we implement the following two 
basic ranking functions and the linear 
combination of them for reference, i.e. the query 
relevance based ranker (denoted by QRR, same 
as f1) and the word centroid based ranker 
(denoted by WCR, same as f2), and the linear 
combined ranker, LCR= O QRR+(1- O )WCR, 
where O  is a combination parameter. QRR and 
WCR are normalized by    minmaxmin x , 
where x, max and min denote the original ranking 
score, the maximum ranking score and minimum 
ranking score produced by a ranker, respectively. 
Table 1 shows the results of the average recall 
scores of ROUGE-1, ROUGE-2 and ROUGE-
SU4 along with their 95% confidence intervals 
included within square brackets. Among them, 
ROUGE-2 is the primary DUC evaluation 
criterion.  
 ROUGE-1 ROUGE-2 ROUGE-SU4
QRR 0.3597 [0.3540, 0.3654]
0.0664 
[0.0630, 0.0697] 
0.1229 
[0.1196, 0.1261]
WCR 0.3504 [0.3436, 0.3565]
0.0644 
[0.0614, 0.0675] 
0.1171 
[0.1138, 0.1202]
LCR* 0.3513 [0.3449, 0.3572]
0.0645 
[0.0613, 0.0676] 
0.1177 
[0.1145, 0.1209]
Co- 
FRank+
0.3769 
[0.3712, 0.3829]
0.0762 
[0.0724, 0.0799] 
0.1317 
[0.1282, 0.1351]
LCR** 
0.3753 
[0.3692, 0.3813]
0.0757 
[0.0719, 0.0796] 
0.1302 
[0.1265, 0.1340]
Co- 
FRank++
0.3783 
[0.3719, 0.3852]
0.0775 
[0.0733, 0.0810] 
0.1323 
[0.1293, 0.1360]
* The worst results produced by LCR when O  = 0.1 
+ The worst results produced by Co-FRank when K  = 0.6 
** The best results produced by LCR when O  = 0.4 
++ The best results produced by Co-FRank when K  = 0.8 
Table 1 Compare different ranking strategies 
Note that the improvement of LCR over QRR 
and WCR is rather significant if the combination 
parameter O  is selected appropriately. Besides, 
Co-FRank is always superior to LCR regardless 
of the best or the worst ouput, and the 
improvement is visible. The reason is that both 
QRR and WCR are enhanced step by step in Co-
FRank, which in turn results in the increased 
overall performance. The trend of the 
improvement has been clearly observed in the 
experiments. This observation validates our 
motivation and the rationality of the algorithm 
proposed in this paper and motivates our further 
investigation on this topic.  
We continue to examine the parameter settings 
in LCR and Co-FRank. Table 2 shows the results 
of LCR when the value of O  changes from 0.1 to 
119
 1.0, and Table 3 shows the results of Co-FRank 
with K  ranging from 0.5 to 0.9. Notice that K  is 
not a combination parameter. We believe that a 
base ranker should have at least half belief in its 
initial ranking results and thus the value of the K  
should be greater than 0.5. We find that LCR 
heavily depends on O . LCR produces relatively 
good and stable results with O  varying from 0.4 
to 0.6. However, the ROUGE scores drop 
apparently when O  heading towards its two end 
values, i.e. 0.1 and 1.0. 
O  ROUGE-1 ROUGE-2 ROUGE-SU4
0.1 0.3513 [0.3449, 0.3572] 
0.0645 
[0.0613, 0.0676] 
0.1177 
[0.1145, 0.1209] 
0.2 0.3623 [0.3559, 0.3685] 
0.0699 
[0.0662, 0.0736] 
0.1235 
[0.1197, 0.1271] 
0.3 0.3721 [0.3660, 0.3778] 
0.0741 
[0.0706, 0.0778] 
0.1281 
[0.1246, 0.1318] 
0.4 0.3753 [0.3692, 0.3813] 
0.0757 
[0.0719, 0.0796] 
0.1302 
[0.1265, 0.1340] 
0.5 0.3756 [0.3698, 0.3814] 
0.0755 
[0.0717, 0.0793] 
0.1307 
[0.1272, 0.1342] 
0.6 0.3770 [0.3710, 0.3826] 
0.0754 
[0.0716, 0.0791] 
0.1323 
[0.1286, 0.1357] 
0.7 0.3698 [0.3636, 0.3759] 
0.0718 
[0.0680, 0.0756] 
0.1284 
[0.1246, 0.1318] 
0.8 0.3672 [0.3613, 0.3730] 
0.0706 
[0.0669, 0.0743] 
0.1271 
[0.1234, 0.1305] 
0.9 0.3651 [0.3591, 0.3708] 
0.0689 
[0.0652, 0.0726] 
0.1258 
[0.1220, 0.1293] 
Table 2 LCR with different O  values 
As shown in Table 3, the Co-FRank can 
always produce stable and promising results 
regardless of the change of K . More important, 
even the worst result produced by Co-FRank still 
outperforms the best result produced by LCR. 
K  ROUGE-1 ROUGE-2 ROUGE-SU4
0.5 0.3750 [0.3687, 0.3810] 
0.0766 
[0.0727, 0.0804] 
0.1308 
[0.1270, 0.1344] 
0.6 0.3769 [0.3712, 0.3829] 
0.0762 
[0.0724, 0.0799] 
0.1317 
[0.1282, 0.1351]
0.7 0.3775 [0.3713, 0.3835] 
0.0763 
[0.0724, 0.0801] 
0.1319 
[0.1282, 0.1354]
0.8 0.3783 [0.3719, 0.3852] 
0.0775 
[0.0733, 0.0810] 
0.1323 
[0.1293, 0.1360] 
0.9 0.3779 [0.3722, 0.3835] 
0.0765 
[0.0728, 0.0803] 
0.1319 
[0.1285, 0.1354 
Table 3 Co-FRank with different K  values 
We then compare our results to the DUC 
participating systems. We present the following 
representative ROUGE results of (1) the top 
three DUC participating systems according to 
ROUGE-2 scores (S15, S17 and S10); and (2) 
the NIST baseline which simply selects the first 
sentences from the documents. 
 ROUGE-1 ROUGE-2 ROUGE-SU4
Co-FRank 0.3783 0.0775 0.1323 
S15 - 0.0725 0.1316 
S17 - 0.0717 0.1297 
S10 - 0.0698 0.1253 
Baseline   0.0403 0.0872 
Table 4 Compare with DUC participating systems 
It is clearly shown in Table 4 that Co-FRank 
can produce a very competitive result, which 
significantly outperforms the NIST baseline and 
meanwhile it is superior to the best participating 
system in the DUC 2005. 
4 Conclusion and Future Work 
In this paper, we propose a novel ranking 
framework, namely Co-Feedback Ranking (Co-
FRank), and examine its effectiveness in query-
focused summarization. There is still a lot of 
work to be done on this topic. Although we show 
the promising achievements of Co-Frank from 
the perspective of experimental studies, we 
expect a more theoretical analysis on Co-FRank. 
Meanwhile, we would like to investigate more 
appropriate techniques to use feedback, and we 
are interested in applying Co-FRank to the other 
applications, such as opinion summarization 
where the integration of opinion-biased and 
document-biased ranking is necessary. 
Acknowledgments 
The work described in this paper was supported 
by the Hong Kong Polytechnic University 
internal the grants (G-YG80 and G-YH53) and 
the China NSF grant (60703008). 
References  
Avrim Blum and Tom Mitchell. 1998. Combining 
Labeled and Unlabeled Data with Co-Training. In 
Proceedings of the Eleventh Annual Conference on 
Computational Learning Theory, pp92-100. 
Chin-Yew Lin and Eduard Hovy. 2000. The 
Automated Acquisition of Topic Signature for Text 
Summarization. In Proceedings of COLING, 
pp495-501. 
Chin-Yew Lin and Eduard Hovy. 2003. Automatic 
Evaluation of Summaries Using N-gram Co-
occurrence Statistics. In Proceedings of HLT-
NAACL, pp71-78. 
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, 
and Daniel Tam. 2004. Centroid-based 
Summarization of Multiple Documents. 
Information Processing and Management, 40:919-
938. 
Paul Over, Hoa Dang and Donna Harman. 2007. DUC 
in Context. Information Processing and 
Management, 43(6):1506-1520. 
120
