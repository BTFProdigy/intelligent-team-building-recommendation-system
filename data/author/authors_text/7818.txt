Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 817?824
Manchester, August 2008
Topic Identification for Fine-Grained Opinion Analysis
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
{stoyanov,cardie}@cs.cornell.edu
Abstract
Within the area of general-purpose fine-
grained subjectivity analysis, opinion topic
identification has, to date, received little
attention due to both the difficulty of the
task and the lack of appropriately anno-
tated resources. In this paper, we pro-
vide an operational definition of opinion
topic and present an algorithm for opinion
topic identification that, following our new
definition, treats the task as a problem in
topic coreference resolution. We develop a
methodology for the manual annotation of
opinion topics and use it to annotate topic
information for a portion of an existing
general-purpose opinion corpus. In exper-
iments using the corpus, our topic identi-
fication approach statistically significantly
outperforms several non-trivial baselines
according to three evaluation measures.
1 Introduction
Subjectivity analysis is concerned with extract-
ing information about attitudes, beliefs, emotions,
opinions, evaluations, sentiment and other private
states expressed in texts. In contrast to the prob-
lem of identifying subjectivity or sentiment at the
document level (e.g. Pang et al (2002), Turney
(2002)), we are interested in fine-grained subjec-
tivity analysis, which is concerned with subjec-
tivity at the phrase or clause level. We expect
fine-grained subjectivity analysis to be useful for
question-answering, summarization, information
extraction and search engine support for queries of
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the form ?How/what does entity X feel/think about
topic Y??, for which document-level opinion anal-
ysis methods can be problematic.
Fine-grained subjectivity analyses typically
identify SUBJECTIVE EXPRESSIONS in context, charac-
terize their POLARITY (e.g. positive, neutral or neg-
ative) and INTENSITY (e.g. weak, medium, strong,
extreme), and identify the associated SOURCE, or
OPINION HOLDER, as well as the TOPIC, or TARGET, of
the opinion. While substantial progress has been
made in automating some of these tasks, opinion
topic identification has received by far the least at-
tention due to both the difficulty of the task and the
lack of appropriately annotated resources.
1
This paper addresses the problem of topic iden-
tification for fine-grained opinion analysis of gen-
eral text.
2
We begin by providing a new, opera-
tional definition of opinion topic in which the topic
of an opinion depends on the context in which
its associated opinion expression occurs. We also
present a novel method for general-purpose opin-
ion topic identification that, following our new def-
inition, treats the problem as an exercise in topic
coreference resolution. We evaluate the approach
using the existing MPQA corpus (Wiebe et al,
2005), which we extend with manual annotations
that encode topic information (and refer to here-
after as the MPQA
TOPIC
corpus).
Inter-annotator agreement results for the manual
annotations are reasonably strong across a num-
ber of metrics and the results of experiments that
evaluate our topic identification method in the con-
text of fine-grained opinion analysis are promising:
1
Section 3 on related work provides additional discussion.
2
The identification of products and their components and
attributes from product reviews is a related, but quite different
task from that addressed here. Section 3 briefly discusses, and
provides references, to the most relevant research in that area.
817
using either automatically or manually identified
topic spans, we achieve topic coreference scores
that statistically significantly outperform two topic
segmentation baselines across three coreference
resolution evaluation measures (B
3
, ? and CEAF).
For the B
3
metric, for example, the best base-
line achieves a topic coreference score on the
MPQA
TOPIC
corpus of 0.55 while our topic coref-
erence algorithm scores 0.57 and 0.71 using au-
tomatically, and manually, identified topic spans,
respectively.
In the remainder of the paper, we define opin-
ion topics (Section 2), present related work (Sec-
tion 3), and motivate and describe the key idea
of topic coreference that underlies our methodol-
ogy for both the manual and automatic annota-
tion of opinion topics (Section 4). Creation of
the MPQA
TOPIC
corpus is described in Section 5
and our topic identification algorithm, in Section 6.
The evaluation methodology and results are pre-
sented in Sections 7 and 8, respectively.
2 Definitions and Examples
Consider the following opinion sentences:
(1)[
OH
John] adores [
TARGET+TOPIC SPAN
Marseille] and
visits it often.
(2)[
OH
Al] thinks that [
TARGET SPAN
[
TOPIC SPAN?
the
government] should [
TOPIC SPAN?
tax gas] more in order to
[
TOPIC SPAN?
curb [
TOPIC SPAN?
CO
2
emissions]]].
A fine-grained subjectivity analysis should iden-
tify: the OPINION EXPRESSION
3
as ?adores? in Exam-
ple 1 and ?thinks? in Example 2; the POLARITY as
positive in Example 1 and neutral in Example 2;
the INTENSITY as medium and low, respectively; and
the OPINION HOLDER (OH) as ?John? and ?Al?, re-
spectively. To be able to discuss the opinion TOPIC
in each example, we begin with three definitions:
? Topic. The TOPIC of a fine-grained opinion is
the real-world object, event or abstract entity that is
the subject of the opinion as intended by the opin-
ion holder.
? Topic span. The TOPIC SPAN associated with an
OPINION EXPRESSION is the closest, minimal span of
text that mentions the topic.
? Target span. In contrast, we use TARGET SPAN
to denote the span of text that covers the syntactic
3
For simplicity, we will use the term opinion throughout
the paper to cover all types of private states expressed in sub-
jective language.
surface form comprising the contents of the opin-
ion.
In Example 1, for instance, ?Marseille? is both
the TOPIC SPAN and the TARGET SPAN associated with
the city of Marseille, which is the TOPIC of the opin-
ion. In Example 2, the TARGET SPAN consists of the
text that comprises the complement of the subjec-
tive verb ?thinks?. Example 2 illustrates why opin-
ion topic identification is difficult: within the sin-
gle target span of the opinion, there are multiple
potential topics, each identified with its own topic
span. Without more context, however, it is impos-
sible to know which phrase indicates the intended
topic. If followed by sentence 3, however,
(3)Although he doesn?t like government-imposed taxes, he
thinks that a fuel tax is the only effective solution.
the topic of Al?s opinion in 2 is much clearer ? it
is likely to be fuel tax, denoted via the TOPIC SPAN
?tax gas? or ?tax?.
3 Related Work
As previously mentioned, there has been much re-
cent progress in extracting fine-grained subjectiv-
ity information from general text. Previous efforts
have focused on the extraction of opinion expres-
sions in context (e.g. Bethard et al (2004), Breck
et al (2007)), the assignment of polarity to these
expressions (e.g. Wilson et al (2005), Kim and
Hovy (2006)), source extraction (e.g. Bethard et
al. (2004), Choi et al (2005)), and identification of
the source-expresses-opinion relation (e.g. Choi et
al. (2006)), i.e. linking sources to the opinions that
they express.
Not surprisingly, progress has been driven by
the creation of language resources. In this regard,
Wiebe et al?s (2005) opinion annotation scheme
for subjective expressions was used to create the
MPQA corpus, which consists of 535 documents
manually annotated for phrase-level expressions of
opinions, their sources, polarities, and intensities.
Although other opinion corpora exist (e.g. Bethard
et al (2004), Voorhees and Buckland (2003), the
product review corpora of Liu
4
), we are not aware
of any corpus that rivals the scale and depth of the
MPQA corpus.
In the related area of opinion extraction from
product reviews, several research efforts have fo-
cused on the extraction of the topic of the opin-
ion (e.g. Kobayashi et al (2004), Yi et al (2003),
4
http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html
818
Popescu and Etzioni (2005), Hu and Liu (2004)).
For this specialized text genre, it has been suf-
ficient to limit the notion of topic to mentions
of product names and components and their at-
tributes. Thus, topic extraction has been effec-
tively substituted with a lexicon look-up and tech-
niques have focused on how to learn or acquire an
appropriate lexicon for the task. While the tech-
niques have been very successful for this genre
of text, they have not been applied outside the
product reviews domain. Further, there are anal-
yses (Wiebe et al, 2005) and experiments (Wilson
et al, 2005) that indicate that lexicon-lookup ap-
proaches to subjectivity analysis will have limited
success on general texts.
Outside the product review domain, there has
been little effort devoted to opinion topic annota-
tion. The MPQA corpus, for example, was orig-
inally intended to include topic annotations, but
the task was abandoned after confirming that it
was very difficult (Wiebe, 2005; Wilson, 2005),
although target span annotation is currently under-
way. While useful, target spans alone will be insuf-
ficient for many applications: they neither contain
information indicating which opinions are about
the same topic, nor provide a concise textual rep-
resentation of the topics.
Due to the lack of appropriately annotated cor-
pora, the problem of opinion topic extraction has
been largely unexplored in NLP. A notable excep-
tion is the work of Kim and Hovy (2006). They
propose a model that extracts opinion topics for
subjective expressions signaled by verbs and ad-
jectives. Their model relies on semantic frames
and extracts as the topic the syntactic constituent
at a specific argument position for the given verb
or adjective. In other words, Kim and Hovy extract
what we refer to as the target spans, and do so for
a subset of the opinion-bearing words in the text.
Although on many occasions target spans coincide
with opinion topics (as in Example 1), we have ob-
served that on many other occasions this is not the
case (as in Example 2). Furthermore, hampered by
the lack of resources with manually annotated tar-
gets, Kim and Hovy could provide only a limited
evaluation.
As we have defined it, opinion topic identifica-
tion bears some resemblance to topic segmenta-
tion, the goal of which is to partition a text into
a linear sequence of topically coherent segments.
Existing methods for topic segmentation typically
assume that fragments of text (e.g. sentences or
sequences of words of a fixed length) with sim-
ilar lexical distribution are about the same topic;
the goal of these methods is to find the boundaries
where the lexical distribution changes (e.g. Choi
(2000), Malioutov and Barzilay (2006)). Opin-
ion topic identification differs from topic segmen-
tation in that opinion topics are not necessarily spa-
tially coherent ? there may be two opinions in
the same sentence on different topics, as well as
opinions that are on the same topic separated by
opinions that do not share that topic. Nevertheless,
we will compare our topic identification approach
to a state-of-the-art topic segmentation algorithm
(Choi, 2000) in the evaluation.
Other work has successfully adopted the use of
clustering to discover entity relations by identify-
ing entities that appear in the same sentence and
clustering the intervening context (e.g. Hasegawa
et al (2004), Rosenfeld and Feldman (2007)). This
work, however, considers named entities and heads
of proper noun phrases rather than topic spans,
and the relations learned are those commonly held
between NPs (e.g. senator-of-state, city-of-state,
chairman-of-organization) rather than a more gen-
eral coreference relation.
4 A Coreference Approach to Topic
Identification
Given our initial definition of opinion topics (Sec-
tion 2), the next task is to determine which com-
putational approaches might be employed for au-
tomatic opinion topic identification. We begin this
exercise by considering some of the problematic
characteristics of opinion topics.
Multiple potential topics. As noted earlier via
Example 2, a serious problem in opinion topic
identification is the mention of multiple potential
topics within the target span of the opinion. Al-
though an issue for all opinions, this problem is
typically more pronounced in opinions that do not
carry sentiment (as in Example 2). Our current
definition of opinion topic requires the NLP sys-
tem (or a human annotator) to decide which of the
entities described in the target span, if any, refers
to the intended topic. This decision can be aided
by the following change to our definition of opin-
ion topic, which introduces the idea of a context-
dependent information focus: the TOPIC of an opin-
ion is the real-world entity that is the subject of the
opinion as intended by the opinion holder based
819
on the discourse context.
With this modified definition in hand, and given
Example 3 as the succeeding context for Example
2, we argue that the intended subject, and hence
the TOPIC, of Al?s opinion in 2 can be quickly iden-
tified as the FUEL TAX, which is denoted by the TOPIC
SPANS ?tax gas? in 2 and ?fuel tax? in 3.
Opinion topics not always explicitly mentioned.
In stark contrast to the above, on many occasions
the topic is not mentioned explicitly at all within
the target span, as in the following example:
(5)[
OH
John] identified the violation of Palestinian human
rights as one of the main factors. TOPIC: ISRAELI-
PALESTINIAN CONFLICT
We have further observed that the opinion topic
is often not mentioned within the same paragraph
and, on a few occasions, not even within the same
document as the opinion expression.
4.1 Our Solution: Topic Coreference
With the above examples and problems in mind,
we hypothesize that the notion of topic corefer-
ence will facilitate both the manual and automatic
identification of opinion topics: We say that two
opinions are topic-coreferent if they share the
same opinion topic. In particular, we conjec-
ture that judging whether or not two opinions are
topic-coreferent is easier than specifying the topic
of each opinion (due to the problems described
above).
5 Constructing the MPQA
TOPIC
Corpus
Relying on the notion of topic coreference, we next
introduce a newmethodology for the manual anno-
tation of opinion topics in text:
1. The annotator begins with a corpus of documents that
has been annotated w.r.t. OPINION EXPRESSIONS. With
each opinion expression, the corpus provides POLARITY and
OPINION HOLDER information. (We use the aforementioned
MPQA corpus.)
2. The annotator maintains a list of the opinion expressions
that remain to be annotated (initially, all opinion expressions
in the document) as well as a list of the current groupings (i.e.
clusters) of opinion expressions that have been identified as
topic-coreferent (initially this list is empty).
3. For each opinion expression, in turn, the annotator decides
whether the opinion is on the same topic as the opinions in
one of the existing clusters or should start a new cluster, and
inserts the opinion in the appropriate cluster.
4. The annotator labels each cluster with a string that de-
scribes the opinion topic that covers all opinions in the cluster.
5. The annotator marks the TOPIC SPAN of each opinion.
(This can be done at any point in the process.)
The manual annotation procedure is de-
scribed in a set of instructions available at
http://www.cs.cornell.edu/?ves. In addition, we
created a GUI that facilitates the annotation proce-
dure. With the help of these resources, one person
annotated opinion topics for a randomly selected
set of 150 of the 535 documents in the MPQA
corpus. In addition, 20 of the 150 documents were
selected at random and annotated by a second
annotator for the purposes of an inter-annotator
agreement study, the results of which are presented
in Section 8.1. The MPQA
TOPIC
and the procedure
by which it was created are described in more
detail in (Stoyanov and Cardie, 2008).
6 The Topic Coreference Algorithm
As mentioned in Section 4, our computational ap-
proach to opinion topic identification is based on
topic coreference: For each document (1) find the
clusters of coreferent opinions, and (2) label the
clusters with the name of the topic. In this paper
we focus only on the first task, topic coreference
resolution ? the most critical step for topic identi-
fication. We conjecture that the second step can be
performed through frequency analysis of the terms
in each of the clusters and leave it for future work.
Topic coreference resolution resembles another
well-known problem in NLP ? noun phrase (NP)
coreference resolution. Therefore, we adapt a
standard machine learning-based approach to NP
coreference resolution (Soon et al, 2001; Ng and
Cardie, 2002) for our purposes. Our adaptation has
three steps: (i) identify the topic spans; (ii) perform
pairwise classification of the associated opinions
as to whether or not they are topic-coreferent; and,
(iii) cluster the opinions according to the results of
(ii). Each step is discussed in more detail below.
6.1 Identifying Topic Spans
Decisions about topic coreference should depend
on the text spans that express the topic. Ideally,
we would be able to recover the topic span of each
opinion and use its content for the topic corefer-
ence decision. However, the topic span depends on
the topic itself, so it is unrealistic that topic spans
can be recovered with simple methods. Neverthe-
less, in this initial work, we investigate two sim-
820
ple methods for automatic topic span identification
and compare them to two manual approaches:
? Sentence. Assume that the topic span is the
whole sentence containing the opinion.
? Automatic. A rule-based method for identi-
fying the topic span (developed using MPQA
documents that are not part of MPQA
TOPIC
).
Rules depend on the syntactic constituent
type of the opinion expression and rely on
syntactic parsing and grammatical role label-
ing.
? Manual. Use the topic span marked by the
human annotator. We included this method
to provide an upper bound on performance of
the topic span extractor.
? Modified Manual. Meant to be a more real-
istic use of the manual topic span annotations,
this method returns the manually identified
topic span only when it is within the sentence
of the opinion expression. When this span
is outside the sentence boundary, this method
returns the opinion sentence.
Of the 4976 opinions annotated across the 150
documents of MPQA
TOPIC
, the topic spans associ-
ated with 4293 were within the same sentence as
the opinion; 3653 were within the span extracted
by our topic span extractor. Additionally, the topic
spans of 173 opinions were outside of the para-
graph containing the opinion.
6.2 Pairwise Topic Coreference Classification
The heart of our method is a pairwise topic coref-
erence classifier. Given a pair of opinions (and
their associated polarity and opinion holder infor-
mation), the goal of the classifier is to determine
whether the opinions are topic-coreferent. We use
the manually annotated data to automatically learn
the pairwise classifier. Given a training document,
we construct a training example for every pair of
opinions in the document (each pair is represented
as a feature vector). The pair is labeled as a posi-
tive example if the two opinions belong to the same
topic cluster, and a negative example otherwise.
Pairwise coreference classification relies criti-
cally on the expressiveness of the features used
to describe the opinion pair. We use three cate-
gories of features: positional, lexico-semantic and
opinion-based features.
Positional features These features are intended
to exploit the fact that opinions that are close to
each other are more likely to be on the same topic.
We use six positional features:
? Same Sentence/Paragraph
5
True if the two
opinions are in the same sentence/paragraph.
? Consecutive Sentences/Paragraphs True if
the two opinions are in consecutive sen-
tences/paragraphs.
? Number of Sentences/Paragraphs The
number of sentences/paragraphs that separate
the two opinions.
TOPIC SPAN-based lexico-semantic features The
features in this group rely on the topic spans and
are recomputed w.r.t. each of the four topic span
methods. The intuition behind this group of fea-
tures is that topic-coreferent opinions are likely to
exhibit lexical and semantic similarity within the
topic span.
? tf.idf The cosine similarity of the tf.idf
weighted vectors of the terms contained in the
two spans.
? Word overlap True if the two topic spans
contain any contain words in common.
? NP coref True if the two spans contain NPs
that are determined to be coreferent by a sim-
ple rule-based coreference system.
? NE overlap True if the two topic spans con-
tain named entities that can be considered
aliases of each other.
Opinion features The features in this group de-
pend on the attributes of the opinion. In the cur-
rent work, we obtain these features directly from
the manual annotations of the MPQA
TOPIC
corpus,
but they might also be obtained from automatically
identified opinion information using the methods
referenced in Section 3.
? Source Match True if the two opinions have
the same opinion holder.
? Polarity Match True if the two opinions have
the same polarity.
5
We use sentence/paragraph to describe two features ? one
based on the sentence and one on the paragraph.
821
? Source-PolarityMatch False if the two opin-
ions have the same opinion holder but con-
flicting polarities (since it is unlikely that a
source will have two opinions with conflict-
ing polarities on the same topic).
We employ three classifiers for pairwise corefer-
ence classification ? an averaged perceptron (Fre-
und and Schapire, 1998), SVM
light
(Joachims,
1998) and a rule-learner ? RIPPER (Cohen, 1995).
However, we report results only for the averaged
perceptron, which exhibited the best performance.
6.3 Clustering
Pairwise classification provides an estimate of the
likelihood that two opinions are topic-coreferent.
To form the topic clusters, we follow the pairwise
classification with a clustering step. We selected
a simple clustering algorithm ? single-link cluster-
ing, which has shown good performance for NP
coreference. Given a threshold, single-link cluster-
ing proceeds by assigning pairs of opinions with a
topic-coreference score above the threshold to the
same topic cluster and then performs transitive clo-
sure of the clusters.
6
7 Evaluation Methodology
For training and evaluation we use the 150-
document MPQA
TOPIC
corpus. All machine learn-
ing methods were tested via 10-fold cross valida-
tion. In each round of cross validation, we use
eight of the data partitions for training and one for
parameter estimation (we varied the threshold for
the clustering algorithm), and test on the remaining
partition. We report results for the three evaluation
measures of Section 7 using the four topic span
extraction methods introduced in Section 6. The
threshold is tuned separately for each evaluation
measure. As noted earlier, all runs obtain opinion
information from the MPQA
TOPIC
corpus (i.e. this
work does not incorporate automatic opinion ex-
traction).
7.1 Topic Coreference Baselines
We compare our topic coreference system to four
baselines. The first two are the ?default? baselines:
? one topic ? assigns all opinions to the same
cluster.
6
Experiments using best-first and last-first clustering ap-
proaches provided similar or worse results.
? one opinion per cluster ? assigns each opin-
ion to its own cluster.
The other two baselines attempt to perform topic
segmentation (discussed in Section 3) and assign
all opinions within the same segment to the same
opinion topic:
? same paragraph ? simple topic segmenta-
tion by splitting documents into segments at
paragraph boundaries.
? Choi 2000 ? Choi?s (2000) state-of-the-art
approach to finding segment boundaries. We
use the freely available C99 software de-
scribed in Choi (2000), varying a parameter
that allows us to control the average number
of sentences per segment and reporting the
best result on the test data.
7.2 Evaluation Metrics
Because there is disagreement among researchers
w.r.t. the proper evaluation measure for NP coref-
erence resolution, we use three generally accepted
metrics
7
to evaluate our topic coreference system.
B-CUBED. B-CUBED (B
3
) is a commonly
used NP coreference metric (Bagga and Baldwin,
1998). It calculates precision and recall for each
item (in our case, each opinion) based on the num-
ber of correctly identified coreference links, and
then computes the average of the item scores in
each document. Precision/recall for an item i is
computed as the proportion of items in the inter-
section of the response (system-generated) and key
(gold standard) clusters containing i divided by the
number of items in the response/key cluster.
CEAF. As a representative of another group of
coreference measures that rely on mapping re-
sponse clusters to key clusters, we selected Luo?s
(2005) CEAF score (short for Constrained Entity-
Alignment F-Measure). Similar to the ACE (2005)
score, CEAF operates by computing an optimal
mapping of response clusters to key clusters and
assessing the goodness of the match of each of the
mapped clusters.
Krippendorff?s ?. Finally, we use Passonneau?s
(2004) generalization of Krippendorff?s (1980) ?
? a standard metric employed for inter-annotator
7
The MUC scoring algorithm (Vilain et al, 1995) was
omitted because it led to an unjustifiably high MUC F-score
(.920) for the ONE TOPIC baseline.
822
B3
? CEAF
All opinions .6424 .5476 .6904
Sentiment opinions .7180 .7285 .7967
Strong opinions .7374 .7669 .8217
Table 1: Inter-annotator agreement results.
reliability studies. Krippendorff?s ? is based
on a probabilistic interpretation of the agreement
of coders as compared to agreement by chance.
While Passonneau?s innovation makes it possible
to apply Krippendorff?s ? to coreference clusters,
the probabilistic interpretation of the statistic is un-
fortunately lost.
8 Results
8.1 Inter-annotator Agreement
As mentioned previously, out of the 150 anno-
tated documents, 20 were annotated by two anno-
tators for the purpose of studying the agreement
between coders. Inter-annotator agreement results
are shown in Table 1. We compute agreement for
three subsets of opinions: all available opinions,
only the sentiment-bearing opinions and the sub-
set of sentiment-bearing opinions judged to have
polarity of medium or higher.
The results support our conjecture that topics
of sentiment-bearing opinions are much easier to
identify: inter-annotator agreement for opinions
with non-neutral polarity (SENTIMENT OPINIONS) im-
proves by a large margin for all measures. As in
other work in subjectivity annotation, we find that
strong sentiment-bearing opinions are easier to an-
notate than sentiment-bearing opinions in general.
Generally, the ? score aims to probabilistically
capture the agreement of annotation data and sep-
arate it from chance agreement. It is generally ac-
cepted that an ? score of .667 indicates reliable
agreement. The score that we observed for the
overall agreement was an? of .547, which is below
the generally accepted level, while ? for the two
subsets of sentiment-bearing opinions is above .72.
However, as discussed above, due to the way that
it is adapted to the problem of coreference resolu-
tion, the ? score loses its probabilistic interpreta-
tion. For example, the ? score requires that a pair-
wise distance function between clusters is speci-
fied. We used one sensible choice for such a func-
tion (we measured the distance between clusters A
and B as dist(A,B) = (2? |A?B|)/(|A|+ |B|)),
B
3
? CEAF
One topic .3739 -.1017 .2976
One opinion per cluster .2941 .2238 .2741
Same paragraph .5542 .3123 .5090
Choi .5399 .3734 .5370
Sentence .5749 .4032 .5393
Rule-based .5730 .4056 .5420
Modified manual .6416 .5134 .6124
Manual .7097 .6585 .6184
Table 2: Results for the topic coreference algo-
rithms.
but other sensible choices for the distance lead to
much higher scores. Furthermore, we observed
that the behavior of the ? score can be rather er-
ratic ? small changes in one of the clusterings can
lead to big differences in the score.
Perhaps a better indicator of the reliability of
the coreference annotation is a comparison with
the baselines, shown in the top half of Table 2.
All baselines score significantly lower than the
inter-annotator agreement scores. With one excep-
tion, the inter-annotator agreement scores are also
higher than those for the learning-based approach
(results shown in the lower half of Table 2), as
would typically be expected. The exception is the
classifier that uses the manual topic spans, but as
we argued earlier these spans carry significant in-
formation about the decision of the annotator.
8.2 Baselines
Results for the four baselines are shown in the first
four rows of Table 2. As expected, the two base-
lines performing topic segmentation show substan-
tially better scores than the two ?default? base-
lines.
8.3 Learning methods
Results for the learning-based approaches are
shown in the bottom half of Table 2. First, we
see that each of the learning-based methods out-
performs the baselines. This is the case even when
sentences are employed as a coarse substitute for
the true topic span. A Wilcoxon Signed-Rank test
shows that differences from the baselines for the
learning-based runs are statistically significant for
the B
3
and ? measures (p < 0.01); for CEAF,
using sentences as topic spans for the learning al-
gorithm outperforms the SAME PARAGRAPH baseline
(p < 0.05), but the results are inconclusive when
823
compared with the system of CHOI.
In addition, relying on manual topic span infor-
mation (MANUAL and MODIFIED MANUAL) allows the
learning-based approach to perform significantly
better than the two runs that use automatically
identified spans (p < 0.01, for all three measures).
The improvement in the scores hints at the impor-
tance of improving automatic topic span extrac-
tion, which will be a focus of our future work.
9 Conclusions
We presented a new, operational definition of opin-
ion topics in the context of fine-grained subjec-
tivity analysis. Based on this definition, we in-
troduced an approach to opinion topic identifi-
cation that relies on the identification of topic-
coreferent opinions. We further employed the
opinion topic definition for the manual annotation
of opinion topics to create the MPQA
TOPIC
corpus.
Inter-annotator agreement results show that opin-
ion topic annotation can be performed reliably.
Finally, we proposed an automatic approach for
identifying topic-coreferent opinions, which sig-
nificantly outperforms all baselines across three
coreference evaluation metrics.
Acknowledgments The authors of this paper
would like to thank Janyce Wiebe and Theresa
Wilson for many insightful discussions. This work
was supported in part by National Science Foun-
dation Grants BCS- 0624277 and IIS-0535099 and
by DHS Grant N0014-07-1-0152.
References
ACE. 2005. The NIST ACE evaluation website.
http://www.nist.gov/speech/tests/ace/.
Bagga, A. and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In In Proceedings of MUC7.
Bethard, S., H. Yu, A. Thornton, V. Hativassiloglou, and
D. Jurafsky. 2004. Automatic extraction of opinion propo-
sitions and their holders. In 2004 AAAI Spring Symposium
on Exploring Attitude and Affect in Text.
Breck, E., Y. Choi, and C. Cardie. 2007. Identifying expres-
sions of opinion in context. In Proceedings of IJCAI.
Choi, Y., C. Cardie, E. Riloff, and S. Patwardhan. 2005. Iden-
tifying sources of opinions with conditional random fields
and extraction patterns. In Proceedings of EMNLP.
Choi, Y., E. Breck, and C. Cardie. 2006. Joint extraction of
entities and relations for opinion recognition. In Proceed-
ings of EMNLP.
Choi, F. 2000. Advances in domain independent linear text
segmentation. Proceedings of NAACL.
Cohen, W. 1995. Fast effective rule induction. In Proceed-
ings of ICML.
Freund, Y. and R. Schapire. 1998. Large margin classifi-
cation using the perceptron algorithm. In Proceedings of
Computational Learing Theory.
Hasegawa, T., S. Sekine, and R. Grishman. 2004. Discover-
ing relations among named entities from large corpora. In
Proceedings of ACL.
Hu, M. and B. Liu. 2004. Mining opinion features in cus-
tomer reviews. In AAAI.
Joachims, T. 1998. Making large-scale support vector ma-
chine learning practical. In B. Sch?olkopf, C. Burges,
A. Smola, editor, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA.
Kim, S. and E. Hovy. 2006. Extracting opinions, opinion
holders, and topics expressed in online news media text.
In Proceedings of ACL/COLING Workshop on Sentiment
and Subjectivity in Text.
Kobayashi, N., K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expressions
for opinion extraction. In Proceedings of IJCNLP.
Krippendorff, K. 1980. Content Analysis: An Introduction to
Its Methodology. Sage Publications, Beverly Hills, CA.
Luo, X. 2005. On coreference resolution performance met-
rics. In Proceedings of EMNLP.
Malioutov, I. and R. Barzilay. 2006. Minimum cut model
for spoken lecture segmentation. In Proceedings of
ACL/COLING.
Ng, V. and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In In Proceedings of
ACL.
Pang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP.
Passonneau, R. 2004. Computing reliability for coreference
annotation. In Proceedings of LREC.
Popescu, A. and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
HLT/EMNLP.
Rosenfeld, B. and R. Feldman. 2007. Clustering for unsuper-
vised relation identification. In Proceedings of CIKM.
Soon, W., H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. Com-
putational Linguistics, 27(4).
Stoyanov, V. and C. Cardie. 2008. Annotating topics of opin-
ions. In Proceedings of LREC.
Turney, P. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proceedings of ACL.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. In Proceedings of the MUC6.
Voorhees, E. and L. Buckland. 2003. Overview of the
TREC 2003 Question Answering Track. In Proceedings
of TREC 12.
Wiebe, J., T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Language
Resources and Evaluation, 1(2).
Wiebe, J. 2005. Personal communication.
Wilson, T., J. Wiebe, and P. Hoffmann. 2005. Recognizing
contextual polarity in phrase-level sentiment analysis. In
Proceedings of HLT/EMNLP.
Wilson, T. 2005. Personal communication.
Yi, J., T. Nasukawa, R. Bunescu, and W. Niblack. 2003. Sen-
timent analyzer: Extracting sentiments about a given topic
using natural language processing techniques. In Proceed-
ings of ICDM.
824
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 923?930, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Multi-Perspective Question Answering Using the OpQA Corpus
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
{ves,cardie}@cs.cornell.edu
Janyce Wiebe
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
wiebe@cs.pitt.edu
Abstract
We investigate techniques to support the
answering of opinion-based questions.
We first present the OpQA corpus of opin-
ion questions and answers. Using the cor-
pus, we compare and contrast the proper-
ties of fact and opinion questions and an-
swers. Based on the disparate characteris-
tics of opinion vs. fact answers, we argue
that traditional fact-based QA approaches
may have difficulty in an MPQA setting
without modification. As an initial step
towards the development of MPQA sys-
tems, we investigate the use of machine
learning and rule-based subjectivity and
opinion source filters and show that they
can be used to guide MPQA systems.
1 Introduction
Much progress has been made in recent years in
automatic, open-domain question answering (e.g.,
Voorhees (2001), Voorhees (2002), Voorhees and
Buckland (2003)). The bulk of the research in this
area, however, addresses fact-based questions like:
?When did McDonald?s open its first restaurant??
or ?What is the Kyoto Protocol??. To date, how-
ever, relatively little research been done in the area
of Multi-Perspective Question Answering (MPQA),
which targets questions of the following sort:
? How is Bush?s decision not to ratify the Kyoto Protocol
looked upon by Japan and other US allies?
? How do the Chinese regard the human rights record of the
United States?
In comparison to fact-based question answering
(QA), researchers understand far less about the prop-
erties of questions and answers in MPQA, and have
yet to develop techniques to exploit knowledge of
those properties. As a result, it is unclear whether
approaches that have been successful in the domain
of fact-based QA will work well for MPQA.
We first present the OpQA corpus of opinion ques-
tions and answers. Using the corpus, we compare
and contrast the properties of fact and opinion ques-
tions and answers. We find that text spans identi-
fied as answers to opinion questions: (1) are approx-
imately twice as long as those of fact questions, (2)
are much more likely (37% vs. 9%) to represent par-
tial answers rather than complete answers, (3) vary
much more widely with respect to syntactic cate-
gory ? covering clauses, verb phrases, prepositional
phrases, and noun phrases; in contrast, fact answers
are overwhelming associated with noun phrases, and
(4) are roughly half as likely to correspond to a sin-
gle syntactic constituent type (16-38% vs. 31-53%).
Based on the disparate characteristics of opinion
vs. fact answers, we argue that traditional fact-based
QA approaches may have difficulty in an MPQA
setting without modification. As one such modifi-
cation, we propose that MPQA systems should rely
on natural language processing methods to identify
information about opinions. In experiments in opin-
ion question answering using the OpQA corpus, we
find that filtering potential answers using machine
learning and rule-based NLP opinion filters substan-
tially improves the performance of an end-to-end
MPQA system according to both a mean reciprocal
rank (MRR) measure (0.59 vs. a baseline of 0.42)
923
and a metric that determines the mean rank of the
first correct answer (MRFA) (26.2 vs. a baseline of
61.3). Further, we find that requiring opinion an-
swers to match the requested opinion source (e.g.,
does <source> approve of the Kyoto Protocol) dra-
matically improves the performance of the MPQA
system on the hardest questions in the corpus.
The remainder of the paper is organized as fol-
lows. In the next section we summarize related
work. Section 3 describes the OpQA corpus. Sec-
tion 4 uses the OpQA corpus to identify poten-
tially problematic issues for handling opinion vs.
fact questions. Section 5 briefly describes an opin-
ion annotation scheme used in the experiments. Sec-
tions 6 and 7 explore the use of opinion information
in the design of MPQA systems.
2 Related Work
There is a growing interest in methods for the auto-
matic identification and extraction of opinions, emo-
tions, and sentiments in text. Much of the relevant
research explores sentiment classification, a text cat-
egorization task in which the goal is to assign to
a document either positive (?thumbs up?) or nega-
tive (?thumbs down?) polarity (e.g. Das and Chen
(2001), Pang et al (2002), Turney (2002), Dave et
al. (2003), Pang and Lee (2004)). Other research
has concentrated on analyzing opinions at, or below,
the sentence level. Recent work, for example, indi-
cates that systems can be trained to recognize opin-
ions, their polarity, their source, and their strength
to a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et al
(2004), Pang and Lee (2004), Wilson et al (2004),
Yu and Hatzivassiloglou (2003), Wiebe and Riloff
(2005)).
Related work in the area of corpus development
includes Wiebe et al?s (2005) opinion annotation
scheme to identify subjective expressions ? expres-
sions used to express opinions, emotions, sentiments
and other private states in text. Wiebe et al have
applied the annotation scheme to create the MPQA
corpus consisting of 535 documents manually an-
notated for phrase-level expressions of opinion. In
addition, the NIST-sponsored TREC evaluation has
begun to develop data focusing on opinions ? the
2003 Novelty Track features a task that requires sys-
tems to identify opinion-oriented documents w.r.t. a
specific issue (Voorhees and Buckland, 2003).
While all of the above work begins to bridge
the gap between text categorization and question
answering, none of the approaches have been em-
ployed or evaluated in the context of MPQA.
3 OpQA Corpus
To support our research in MPQA, we created the
OpQA corpus of opinion and fact questions and an-
swers. Additional details on the construction of the
corpus as well as results of an interannotator agree-
ment study can be found in Stoyanov et al (2004).
3.1 Documents and Questions
The OpQA corpus consists of 98 documents that ap-
peared in the world press between June 2001 and
May 2002. All documents were taken from the
aforementioned MPQA corpus (Wilson and Wiebe,
2003)1 and are manually annotated with phrase-
level opinion information, following the annotation
scheme of Wiebe et al (2005), which is briefly
summarized in Section 5. The documents cover
four general (and controversial) topics: President
Bush?s alternative to the Kyoto protocol (kyoto); the
US annual human rights report (humanrights); the
2002 coup d?etat in Venezuela (venezuela); and the
2002 elections in Zimbabwe and Mugabe?s reelec-
tion (mugabe). Each topic is covered by between 19
and 33 documents that were identified automatically
via IR methods.
Both fact and opinion questions for each topic
were added to the OpQA corpus by a volunteer not
associated with the current project. The volunteer
was provided with a set of instructions for creat-
ing questions together with two documents on each
topic selected at random. He created between six
and eight questions on each topic, evenly split be-
tween fact and opinion. The 30 questions are given
in Table 1 sorted by topic.
3.2 Answer annotations
Answer annotations were added to the corpus by two
annotators according to a set of annotation instruc-
1The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
The OpQA corpus is available upon request.
924
Kyoto
1 f What is the Kyoto Protocol about?
2 f When was the Kyoto Protocol adopted?
3 f Who is the president of the Kiko Network?
4 f What is the Kiko Network?
5 o Does the president of the Kiko Network approve of the US action concerning the Kyoto Protocol?
6 o Are the Japanese unanimous in their opinion of Bush?s position on the Kyoto Protocol?
7 o How is Bush?s decision not to ratify the Kyoto Protocol looked upon by Japan and other US allies?
8 o How do European Union countries feel about the US opposition to the Kyoto protocol?
Human Rights
1 f What is the murder rate in the United States?
2 f What country issues an annual report on human rights in the United States?
3 o How do the Chinese regard the human rights record of the United States?
4 f Who is Andrew Welsdan?
5 o What factors influence the way in which the US regards the human rights records of other nations?
6 o Is the US Annual Human Rights Report received with universal approval around the world?
Venezuela
1 f When did Hugo Chavez become President?
2 f Did any prominent Americans plan to visit Venezuela immediately following the 2002 coup?
3 o Did anything surprising happen when Hugo Chavez regained power in Venezuela after he was
removed by a coup?
4 o Did most Venezuelans support the 2002 coup?
5 f Which governmental institutions in Venezuela were dissolved by the leaders of the 2002 coup?
6 o How did ordinary Venezuelans feel about the 2002 coup and subsequent events?
7 o Did America support the Venezuelan foreign policy followed by Chavez?
8 f Who is Vice-President of Venezuela?
Mugabe
1 o What was the American and British reaction to the reelection of Mugabe?
2 f Where did Mugabe vote in the 2002 presidential election?
3 f At which primary school had Mugabe been expected to vote in the 2002 presidential election?
4 f How long has Mugabe headed his country?
5 f Who was expecting Mugabe at Mhofu School for the 2002 election?
6 o What is the basis for the European Union and US critical attitude and adversarial action toward
Mugabe?
7 o What did South Africa want Mugabe to do after the 2002 election?
8 o What is Mugabe?s opinion about the West?s attitude and actions towards the 2002 Zimbabwe elec-
tion?
Table 1: Questions in the OpQA collection by topic.
f in column 1 indicates a fact question; o, an opinion
question.
tions.2 Every text segment that contributes to an
answer to any of the 30 questions is annotated as
an answer. In particular, answer annotations include
segments that constitute a partial answer. Partial an-
swers either (1) lack the specificity needed to consti-
tute a full answer (e.g., ?before May 2004? partially
answers the question When was the Kyoto protocol
ratified? when a specific date is known) or (2) need
to be combined with at least one additional answer
segment to fully answer the question (e.g., the ques-
tion Are the Japanese unanimous in their opposition
of Bush?s position on the Kyoto protocol? is an-
swered only partially by a segment expressing a sin-
gle opinion). In addition, annotators mark the min-
imum answer spans (e.g., ?a Tokyo organization,?
vs. ?a Tokyo organization representing about 150
Japanese groups?).
4 Characteristics of opinion answers
Next, we use the OpQA corpus to analyze and com-
pare the characteristics of fact vs. opinion questions.
Based on our findings, we believe that QA systems
based solely on traditional QA techniques are likely
2The annotation instructions are available
at http://www.cs.cornell.edu/ ves/
Publications/publications.htm.
to be less effective at MPQA than they are at tradi-
tional fact-based QA.
4.1 Traditional QA architectures
Despite the wide variety of approaches implied by
modern QA systems, almost all systems rely on the
following two steps (subsystems), which have em-
pirically proven to be effective:
? IR module. The QA system invokes an IR subsystem that
employs traditional text similarity measures (e.g., tf/idf)
to retrieve and rank document fragments (sentences or
paragraphs) w.r.t. the question (query).
? Linguistic filters. QA systems employ a set of filters
and text processing components to discard some docu-
ment fragments. The following filters have empirically
proven to be effective and are used universally:
Semantic filters prefer an answer segment that matches
the semantic class(es) associated with the question type
(e.g., date or time for when questions; person or organi-
zation for who questions).
Syntactic filters are also configured on the type of ques-
tion. The most common and effective syntactic filters se-
lect a specific constituent (e.g., noun phrase) according to
the question type (e.g., who question).
QA systems typically interleave the above two
subsystems with a variety of different processing
steps of both the question and the answer. The goal
of the processing is to identify text fragments that
contain an answer to the question. Typical QA sys-
tems do not perform any further text processing;
they return the text fragment as it occurred in the
text. 3
4.2 Corpus-based analysis of opinion answers
We hypothesize that QA systems that conform to
this traditional architecture will have difficulty han-
dling opinion questions without non-trivial modifi-
cation. In support of this hypothesis, we provide
statistics from the OpQA corpus to illustrate some of
the characteristics that distinguish answers to opin-
ion vs. fact questions, and discuss their implications
for a traditional QA system architecture.
Answer length. We see in Table 2 that the aver-
age length of opinion answers in the OpQA corpus
3This architecture is seen mainly in QA systems designed
for TREC?s ?factoid? and ?list? QA tracks. Systems competing
in the relatively new ?definition? or ?other? tracks have begun
to introduce new approaches. However, most such systems still
rely on the IR step and return the text fragment as it occurred in
the text.
925
Number of answers Length Number of partials
fact 124 5.12 12 (9.68%)
opinion 415 9.24 154 (37.11%)
Table 2: Number of answers, average answer length
(in tokens), and number of partial answers for
fact/opinion questions.
is 9.24 tokens, almost double that of fact answers.
Unfortunately, longer answers could present prob-
lems for some traditional QA systems. In particu-
lar, some of the more sophisticated algorithms that
perform additional processing steps such as logi-
cal verifiers (Moldovan et al, 2002) may be less ac-
curate or computationally infeasible for longer an-
swers. More importantly, longer answers are likely
to span more than a single syntactic constituent, ren-
dering the syntactic filters, and very likely the se-
mantic filters, less effective.
Partial answers. Table 2 also shows that over 37%
of the opinion answers were marked as partial vs.
9.68% of the fact answers. The implications of par-
tial answers for the traditional QA architecture are
substantial: an MPQA system will require an an-
swer generator to (1) distinguish between partial
and full answers; (2) recognize redundant partial an-
swers; (3) identify which subset of the partial an-
swers, if any, constitutes a full answer; (4) determine
whether additional documents need to be examined
to find a complete answer; and (5) asemble the final
answer from partial pieces of information.
Syntactic constituent of the answer. As discussed
in Section 4.1, traditional QA systems rely heav-
ily on the predicted syntactic and semantic class of
the answer. Based on answer lengths, we specu-
lated that opinion answers are unlikely to span a sin-
gle constituent and/or semantic class. This specula-
tion is confirmed by examining the phrase type as-
sociated with OpQA answers using Abney?s (1996)
CASS partial parser.4 For each question, we count
the number of times an answer segment for the ques-
tion (in the manual annotations) matches each con-
stituent type. We consider four constituent types
? noun phrase (n), verb phrase (v), prepositional
phrase (p), and clause (c) ? and three matching cri-
teria:
4The parser is available from
http://www.vinartus.net/spa/.
Fact OpinionQues- # of Matching Criteria syn Ques- # of Matching Criteria syn
tion answers ex up up/dn type tion answers ex up up/dn type
H 1 1 0 0 0 H 3 15 5 5 5 c
H 2 4 2 2 2 n H 5 24 5 5 10 n
H 4 1 0 0 0 H 6 123 17 23 52 n
K 1 48 13 14 24 n K 5 3 0 0 1
K 2 38 13 13 19 n K 6 34 6 5 12 c
K 3 1 1 1 1 c n K 7 55 9 8 19 c
K 4 2 1 1 1 n K 8 25 4 4 10 v
M 2 3 0 0 1 M 1 74 10 12 29 v
M 3 1 0 0 1 M 6 12 3 5 7 n
M 4 10 2 2 5 n M 7 1 0 0 0
M 5 3 1 1 2 c M 8 3 0 0 1V 1 4 3 3 4 n V 3 1 1 0 1 cV 2 1 1 1 1 n V 4 13 2 2 2 cV 5 3 0 1 1 V 6 9 2 2 5 c nV 8 4 2 4 4 n V 7 23 3 1 5
Cov- 124 39 43 66 Cov- 415 67 70 159
erage 31% 35% 53% erage 16% 17% 38%
Table 3: Syntactic Constituent Type for Answers in
the OpQA Corpus
1. The exact match criterion is satisfied only by answer seg-
ments whose spans exactly correspond to a constituent in
the CASS output.
2. The up criterion considers an answer to match a CASS
constituent if the constituent completely contains the an-
swer and no more than three additional (non-answer) to-
kens.
3. The up/dn criterion considers an answer to match a
CASS constituent if it matches according to the up crite-
rion or if the answer completely contains the constituent
and no more than three additional tokens.
The counts for the analysis of answer segment
syntactic type for fact vs. opinion questions are sum-
marized in Table 3. Results for the 15 fact ques-
tions are shown in the left half of the table, and
for the 15 opinion questions in the right half. The
leftmost column in each half provides the question
topic and number, and the second column indicates
the total number of answer segments annotated for
the question. The next three columns show, for each
of the ex, up, and up/dn matching criteria, respec-
tively, the number of annotated answer segments
that match the majority syntactic type among an-
swer segments for that question/criterion pair. Us-
ing a traditional QA architecture, the MPQA sys-
tem might filter answers based on this majority type.
The syn type column indicates the majority syntac-
tic type using the exact match criterion; two values
in the column indicate a tie for majority syntactic
type, and an empty syntactic type indicates that no
answer exactly matched any of the four constituent
types. With only a few exceptions, the up and up/dn
matching criteria agreed in majority syntactic type.
Results in Table 3 show a significant disparity be-
tween fact and opinion questions. For fact ques-
926
tions, the syntactic type filter would keep 31%, 35%,
or 53% of the correct answers, depending on the
matching criterion. For opinion questions, there is
unfortunately a two-fold reduction in the percentage
of correct answers that would remain after filtering
? only 16%, 17% or 38%, depending on the match-
ing criterion. More importantly, the majority syntac-
tic type among answers for fact questions is almost
always a noun phrase, while no single constituent
type emerges as a useful syntactic filter for opinion
questions (see the syn phrase columns in Table 3).
Finally, because semantic class information is gener-
ally tied to a particular syntactic category, the effec-
tiveness of traditional semantic filters in the MPQA
setting is unclear.
In summary, identifying answers to questions in
an MPQA setting within a traditional QA architec-
ture will be difficult. First, the implicit and explicit
assumptions inherent in standard linguistic filters are
consistent with the characteristics of fact- rather than
opinion-oriented QA. In addition, the presence of
relatively long answers and partial answers will re-
quire a much more complex answer generator than
is typically present in current QA systems.
In Sections 6 and 7, we propose initial steps to-
wards modifying the traditional QA architecture for
use in MPQA. In particular, we propose and evaluate
two types of opinion filters for MPQA: subjectiv-
ity filters and opinion source filters. Both types of
linguistic filters rely on phrase-level and sentence-
level opinion information, which has been manually
annotated for our corpus; the next section briefly de-
scribes the opinion annotation scheme.
5 Manual Opinion Annotations
Documents in our OpQA corpus come from the
larger MPQA corpus, which contains manual opin-
ion annotations. The annotation framework is de-
scribed in detail in (Wiebe et al, 2005). Here we
give a high-level overview.
The annotation framework provides a basis for
subjective expressions: expressions used to express
opinions, emotions, and sentiments. The framework
allows for the annotation of both directly expressed
private states (e.g., afraid in the sentence ?John is
afraid that Sue might fall,?) and opinions expressed
by the choice of words and style of language (e.g.,
it is about time and oppression in the sentence ?It is
about time that we end Saddam?s oppression?). In
addition, the annotations include several attributes,
including the intensity (with possible values low,
medium, high, and extreme) and the source of the
private state. The source of a private state is the per-
son or entity who holds or experiences it.
6 Subjectivity Filters for MPQA Systems
This section describes three subjectivity filters
based on the above opinion annotation scheme. Be-
low (in Section 6.3), the filters are used to remove
fact sentences from consideration when answering
opinion questions, and the OpQA corpus is used to
evaluate their effectiveness.
6.1 Manual Subjectivity Filter
Much previous research on automatic extraction of
opinion information performed classifications at the
sentence level. Therefore, we define sentence-level
opinion classifications in terms of the phrase-level
annotations. For our gold standard of manual opin-
ion classifications (dubbed MANUAL for the rest of
the paper) we will follow Riloff and Wiebe?s (2003)
convention (also used by Wiebe and Riloff (2005))
and consider a sentence to be opinion if it contains
at least one opinion of intensity medium or higher,
and to be fact otherwise.
6.2 Two Automatic Subjectivity Filters
As discussed in section 2, several research efforts
have attempted to perform automatic opinion clas-
sification on the clause and sentence level. We in-
vestigate whether such information can be useful for
MPQA by using the automatic sentence level opin-
ion classifiers of Riloff and Wiebe (2003) and Wiebe
and Riloff (2005).
Riloff and Wiebe (2003) use a bootstrapping al-
gorithm to perform a sentence-based opinion classi-
fication on the MPQA corpus. They use a set of high
precision subjectivity and objectivity clues to iden-
tify subjective and objective sentences. This data
is then used in an algorithm similar to AutoSlog-
TS (Riloff, 1996) to automatically identify a set of
extraction patterns. The acquired patterns are then
used iteratively to identify a larger set of subjective
and objective sentences. In our experiments we use
927
precision recall F
MPQA corpus RULEBASED 90.4 34.2 46.6
NAIVE BAYES 79.4 70.6 74.7
Table 4: Precision, recall, and F-measure for the two
classifiers.
the classifier that was created by the reimplemen-
tation of this bootstrapping process in Wiebe and
Riloff (2005). We will use RULEBASED to denote
the opinion information output by this classifier.
In addition, Wiebe and Riloff used the RULE-
BASED classifier to produce a labeled data set for
training. They trained a Naive Bayes subjectivity
classifier on the labeled set. We will use NAIVE
BAYES to refer to Wiebe and Riloff?s naive Bayes
classifier.5 Table 4 shows the performance of the
two classifiers on the MPQA corpus as reported by
Wiebe and Riloff.
6.3 Experiments
We performed two types of experiments using the
subjectivity filters.
6.3.1 Answer rank experiments
Our hypothesis motivating the first type of exper-
iment is that subjectivity filters can improve the an-
swer identification phase of an MPQA system. We
implement the IR subsystem of a traditional QA sys-
tem, and apply the subjectivity filters to the IR re-
sults. Specifically, for each opinion question in the
corpus 6 , we do the following:
1. Split all documents in our corpus into sentences.
2. Run an information retrieval algorithm7 on the set of all
sentences using the question as the query to obtain a
ranked list of sentences.
3. Apply a subjectivity filter to the ranked list to remove all
fact sentences from the ranked list.
We test each of the MANUAL, RULEBASED, and
NAIVE BAYES subjectivity filters. We compare the
rank of the first answer to each question in the
5Specifically, the one they label Naive Bayes 1.
6We do not evaluate the opinion filters on the 15 fact ques-
tions. Since opinion sentences are defined as containing at least
one opinion of intensity medium or higher, opinion sentences
can contain factual information and sentence-level opinion fil-
ters are not likely to be effective for fact-based QA.
7We use the Lemur toolkit?s standard tf.idf implementation
available from http://www.lemurproject.org/.
Topic Qnum Baseline Manual NaiveBayes Rulebased
Kyoto 5 1 1 1 1
6 5 4 4 3
7 1 1 1 1
8 1 1 1 1
Human 3 1 1 1 1
Rights 5 10 6 7 5
6 1 1 1 1
Venezuela 3 106 81 92 35
4 3 2 3 1
6 1 1 1 1
7 3 3 3 2
Mugabe 1 2 2 2 2
6 7 5 5 4
7 447 291 317 153
8 331 205 217 182
MRR : 0.4244 0.5189 0.5078 0.5856
MRFA: 61.3333 40.3333 43.7333 26.2
Table 5: Results for the subjectivity filters.
ranked list before the filter is applied, with the rank
of the first answer to the question in the ranked list
after the filter is applied.
Results. Results for the opinion filters are compared
to a simple baseline, which performs the informa-
tion retrieval step with no filtering. Table 5 gives the
results on the 15 opinion questions for the baseline
and each of the three subjectivity filters. The table
shows two cumulative measures ? the mean recip-
rocal rank (MRR) 8 and the mean rank of the first
answer (MRFA). 9
Table 5 shows that all three subjectivity filters out-
perform the baseline: for all three filters, the first
answer in the filtered results for all 15 questions is
ranked at least as high as in the baseline. As a result,
the three subjectivity filters outperform the baseline
in both MRR and MRFA. Surprisingly, the best per-
forming subjectivity filter is RULEBASED, surpass-
ing the gold standard MANUAL, both in MRR (0.59
vs. 0.52) and MRFA (40.3 vs. 26.2). Presum-
ably, the improvement in performance comes from
the fact that RULEBASED identifies subjective sen-
tences with the highest precision (and lowest recall).
Thus, the RULEBASED subjectivity filter discards
non-subjective sentences most aggressively.
6.3.2 Answer probability experiments
The second experiment, answer probability, be-
gins to explore whether opinion information can be
8The MRR is computed as the average of 1/r, where r is
the rank of the first answer.
9MRR has been accepted as the standard performance mea-
sure in QA, since MRFA can be strongly affected by outlier
questions. However, the MRR score is dominated by the results
in the high end of the ranking. Thus, MRFA may be more ap-
propriate for our experiments because the filters are an interme-
diate step in the processing, the results of which other MPQA
components may improve.
928
sentence
fact opinion
Manual fact 56 (46.67%) 64 (53.33%)
opinion 42 (10.14%) 372 (89.86%)
question Naive Bayes fact 49 (40.83%) 71 (59.17%)
opinion 57 (13.77%) 357 (86.23%)
Rulebased fact 96 (80.00%) 24 (20.00%)
opinion 184 (44.44%) 230 (55.56%)
Table 6: Answer probability results.
used in an answer generator. This experiment con-
siders correspondences between (1) the classes (i.e.,
opinion or fact) assigned by the subjectivity filters to
the sentences containing answers, and (2) the classes
of the questions the answers are responses to (ac-
cording to the OpQA annotations). That is, we com-
pute the probabilities (where ans = answer):
P(ans is in a C1 sentence | ans is the answer to a C2 ques-
tion) for all four combinations of C1=opinion, fact and
C2=opinion, fact.
Results. Results for the answer probability experi-
ment are given in Table 6. The rows correspond to
the classes of the questions the answers respond to,
and the columns correspond to the classes assigned
by the subjectivity filters to the sentences contain-
ing the answers. The first two rows, for instance,
give the results for the MANUAL criterion. MANUAL
placed 56 of the answers to fact questions in fact
sentences (46.67% of all answers to fact questions)
and 64 (53.33%) of the answers to fact questions in
opinion sentences. Similarly, MANUAL placed 42
(10.14%) of the answers to opinion questions in fact
sentences, and 372 (89.86%) of the answers to opin-
ion questions in opinion sentences.
The answer probability experiment sheds some
light on the subjectivity filter experiments. All three
subjectivity filters place a larger percentage of an-
swers to opinion questions in opinion sentences than
they place in fact sentences. However, the differ-
ent filters exhibit different degrees of discrimination.
Answers to opinion questions are almost always
placed in opinion sentences by MANUAL (89.86%)
and NAIVE BAYES (86.23%). While that aspect of
their performance is excellent, MANUAL and NAIVE
BAYES place more answers to fact questions in opin-
ion rather than fact sentences (though the percent-
ages are in the 50s). This is to be expected, because
MANUAL and NAIVE BAYES are more conservative
and err on the side of classifying sentences as opin-
ions: for MANUAL, the presence of any subjective
expression makes the entire sentence opinion, even
if parts of the sentence are factual; NAIVE BAYES
shows high recall but lower precision in recognizing
opinion sentences (see Table 4). Conversely, RULE-
BASED places 80% of the fact answers in fact sen-
tences and only 56% of the opinion answers in opin-
ion sentences. Again, the lower number of assign-
ments to opinion sentences is to be expected, given
the high precision and low recall of the classifier.
But the net result is that, for RULEBASED, the off-
diagonals are all less than 50%: it places more an-
swers to fact questions in fact rather than opinion
sentences (80%), and more answers to opinion ques-
tions in opinion rather than fact sentences (56%).
This is consistent with its superior performance in
the subjectivity filtering experiment.
In addition to explaining the performance of
the subjectivity filters, the answer rank experiment
shows that the automatic opinion classifiers can be
used directly in an answer generator module. The
two automatic classifiers rely on evidence in the sen-
tence to predict the class (the information extraction
patterns used by RULEBASED and the features used
by NAIVE BAYES). In ongoing work we investigate
ways to use this evidence to extract and summarize
the opinions expressed in text, which is a task simi-
lar to that of an answer generator module.
7 Opinion Source Filters for MPQA
Systems
In addition to subjectivity filters, we also define an
opinion source filter based on the manual opinion
annotations. This filter removes all sentences that
do not have an opinion annotation with a source that
matches the source of the question10. For this filter
we only used the MANUAL source annotations since
we did not have access to automatically extracted
source information. We employ the same Answer
Rank experiment as in 6.3.1, substituting the source
filter for a subjectivity filter.
Results. Results for the source filter are mixed.
The filter outperforms the baseline on some ques-
tions and performs worst on others. As a result the
MRR for the source filter is worse than the base-
10We manually identified the sources of each of the 15 opin-
ion questions.
929
line (0.4633 vs. 0.4244). However, the source fil-
ter exhibits by far the best results using the MRFA
measure, a value of 11.267. The performance im-
provement is due to the filter?s ability to recognize
the answers to the hardest questions, for which the
other filters have the most trouble (questions mu-
gabe 7 and 8). For these questions, the rank of the
first answer improves from 153 to 21, and from 182
to 11, respectively. With the exception of question
venezuela 3, which does not contain a clear source
(and is problematic altogether because there is only
a single answer in the corpus and the question?s
qualification as opinion is not clear) the source filter
always ranked an answer within the first 25 answers.
Thus, source filters can be especially useful in sys-
tems that rely on the presence of an answer within
the first few ranked answer segments and then in-
voke more sophisticated analysis in the additional
processing phase.
8 Conclusions
We began by giving a high-level overview of the
OpQA corpus. Using the corpus, we compared the
characteristics of answers to fact and opinion ques-
tions. Based on the different characteristics, we sur-
mise that traditional QA approaches may not be as
effective for MPQA as they have been for fact-based
QA. Finally, we investigated the use of machine
learning and rule-based opinion filters and showed
that they can be used to guide MPQA systems.
Acknowledgments We would like to thank Di-
ane Litman for her work eliciting the questions for
the OpQA corpus, and the anonymous reviewers
for their helpful comments.This work was supported
by the Advanced Research and Development Activ-
ity (ARDA), by NSF Grants IIS-0208028 and IIS-
0208798, by the Xerox Foundation, and by a NSF
Graduate Research Fellowship to the first author.
References
Steven Abney. 1996. Partial parsing via finite-state cascades.
In Proceedings of the ESSLLI ?96 Robust Parsing Workshop.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and D. Ju-
rafsky. 2004. Automatic extraction of opinion propositions
and their holders. In 2004 AAAI Spring Symposium on Ex-
ploring Attitude and Affect in Text.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting mar-
ket sentiment from stock message boards. In Proceedings of
the 8th Asia Pacific Finance Association Annual Conference.
Kushal Dave, Steve Lawrence, and David Pennock. 2003. Min-
ing the peanut gallery: Opinion extraction and semantic clas-
sification of product reviews. In International World Wide
Web Conference, pages 519?528.
D. Moldovan, S. Harabagiu, R. Girju, P. Morarescu, F. Laca-
tusu, A. Novischi, A. Badulescu, and O. Bolohan. 2002.
LCC tools for question answering. In Proceedings of TREC
2002.
Bo Pang and Lillian Lee. 2004. A sentimental education: Sen-
timent analysis using subjectivity summarization based on
minimum cuts. In Proceedings of the ACL, pages 271?278.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002.
Thumbs up? sentiment classification using machine learning
techniques. In Proceedings of EMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns for
subjective expressions. In Proceesings of EMNLP.
Ellen Riloff. 1996. Automatically generating extraction pat-
terns from untagged text. Proceedings of AAAI.
V. Stoyanov, C. Cardie, J. Wiebe, and D. Litman. 2004. Eval-
uating an opinion annotation scheme using a new Multi-
Perspective Question and Answer corpus. In 2004 AAAI
Spring Symposium on Exploring Attitude and Affect in Text.
Peter Turney. 2002. Thumbs up or thumbs down? Semantic
orientation applied to unsupervised classification of reviews.
In Proceedings of the ACL, pages 417?424.
E. Voorhees and L. Buckland. 2003. Overview of the
TREC 2003 Question Answering Track. In Proceedings of
TREC 12.
Ellen Voorhees. 2001. Overview of the TREC 2001 Question
Answering Track. In Proceedings of TREC 10.
Ellen Voorhees. 2002. Overview of the 2002 Question Answer-
ing Track. In Proceedings of TREC 11.
Janyce Wiebe and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts. In
Proceedings of CICLing.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005. An-
notating expressions of opinions and emotions in language.
Language Resources and Evaluation, 1(2).
Theresa Wilson and Janyce Wiebe. 2003. Annotating opinions
in the world press. 4th SIGdial Workshop on Discourse and
Dialogue (SIGdial-03).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are you?
Finding strong and weak opinion clauses. In Proceedings of
AAAI.
H. Yu and V. Hatzivassiloglou. 2003. Towards answering opin-
ion questions: Separating facts from opinions and identi-
fying the polarity of opinion sentences. In Proceedings of
EMNLP.
930
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 656?664,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Conundrums in Noun Phrase Coreference Resolution:
Making Sense of the State-of-the-Art
Veselin Stoyanov
Cornell University
Ithaca, NY
ves@cs.cornell.edu
Nathan Gilbert
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
Claire Cardie
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Ellen Riloff
University of Utah
Salt Lake City, UT
riloff@cs.utah.edu
Abstract
We aim to shed light on the state-of-the-art in NP
coreference resolution by teasing apart the differ-
ences in the MUC and ACE task definitions, the as-
sumptions made in evaluation methodologies, and
inherent differences in text corpora. First, we exam-
ine three subproblems that play a role in coreference
resolution: named entity recognition, anaphoric-
ity determination, and coreference element detec-
tion. We measure the impact of each subproblem on
coreference resolution and confirm that certain as-
sumptions regarding these subproblems in the eval-
uation methodology can dramatically simplify the
overall task. Second, we measure the performance
of a state-of-the-art coreference resolver on several
classes of anaphora and use these results to develop
a quantitative measure for estimating coreference
resolution performance on new data sets.
1 Introduction
As is common for many natural language process-
ing problems, the state-of-the-art in noun phrase
(NP) coreference resolution is typically quantified
based on system performance on manually anno-
tated text corpora. In spite of the availability of
several benchmark data sets (e.g. MUC-6 (1995),
ACE NIST (2004)) and their use in many formal
evaluations, as a field we can make surprisingly
few conclusive statements about the state-of-the-
art in NP coreference resolution.
In particular, it remains difficult to assess the ef-
fectiveness of different coreference resolution ap-
proaches, even in relative terms. For example, the
91.5 F-measure reported by McCallum and Well-
ner (2004) was produced by a system using perfect
information for several linguistic subproblems. In
contrast, the 71.3 F-measure reported by Yang et
al. (2003) represents a fully automatic end-to-end
resolver. It is impossible to assess which approach
truly performs best because of the dramatically
different assumptions of each evaluation.
Results vary widely across data sets. Corefer-
ence resolution scores range from 85-90% on the
ACE 2004 and 2005 data sets to a much lower 60-
70% on the MUC 6 and 7 data sets (e.g. Soon et al
(2001) and Yang et al (2003)). What accounts for
these differences? Are they due to properties of
the documents or domains? Or do differences in
the coreference task definitions account for the dif-
ferences in performance? Given a new text collec-
tion and domain, what level of performance should
we expect?
We have little understanding of which aspects
of the coreference resolution problem are handled
well or poorly by state-of-the-art systems. Ex-
cept for some fairly general statements, for exam-
ple that proper names are easier to resolve than
pronouns, which are easier than common nouns,
there has been little analysis of which aspects of
the problem have achieved success and which re-
main elusive.
The goal of this paper is to take initial steps to-
ward making sense of the disparate performance
results reported for NP coreference resolution. For
our investigations, we employ a state-of-the-art
classification-based NP coreference resolver and
focus on the widely used MUC and ACE corefer-
ence resolution data sets.
We hypothesize that performance variation
within and across coreference resolvers is, at least
in part, a function of (1) the (sometimes unstated)
assumptions in evaluation methodologies, and (2)
the relative difficulty of the benchmark text cor-
pora. With these in mind, Section 3 first examines
three subproblems that play an important role in
coreference resolution: named entity recognition,
anaphoricity determination, and coreference ele-
ment detection. We quantitatively measure the im-
pact of each of these subproblems on coreference
resolution performance as a whole. Our results
suggest that the availability of accurate detectors
for anaphoricity or coreference elements could
substantially improve the performance of state-of-
the-art resolvers, while improvements to named
entity recognition likely offer little gains. Our re-
sults also confirm that the assumptions adopted in
656
MUC ACE
Relative Pronouns no yes
Gerunds no yes
Nested non-NP nouns yes no
Nested NEs no GPE & LOC premod
Semantic Types all 7 classes only
Singletons no yes
Table 1: Coreference Definition Differences for MUC and
ACE. (GPE refers to geo-political entities.)
some evaluations dramatically simplify the resolu-
tion task, rendering it an unrealistic surrogate for
the original problem.
In Section 4, we quantify the difficulty of a
text corpus with respect to coreference resolution
by analyzing performance on different resolution
classes. Our goals are twofold: to measure the
level of performance of state-of-the-art corefer-
ence resolvers on different types of anaphora, and
to develop a quantitative measure for estimating
coreference resolution performance on new data
sets. We introduce a coreference performance pre-
diction (CPP) measure and show that it accurately
predicts the performance of our coreference re-
solver. As a side effect of our research, we pro-
vide a new set of much-needed benchmark results
for coreference resolution under common sets of
fully-specified evaluation assumptions.
2 Coreference Task Definitions
This paper studies the six most commonly used
coreference resolution data sets. Two of those are
from the MUC conferences (MUC-6, 1995; MUC-
7, 1997) and four are from the Automatic Con-
tent Evaluation (ACE) Program (NIST, 2004). In
this section, we outline the differences between the
MUC and ACE coreference resolution tasks, and
define terminology for the rest of the paper.
Noun phrase coreference resolution is the pro-
cess of determining whether two noun phrases
(NPs) refer to the same real-world entity or con-
cept. It is related to anaphora resolution: a NP is
said to be anaphoric if it depends on another NP
for interpretation. Consider the following:
John Hall is the new CEO. He starts on Monday.
Here, he is anaphoric because it depends on its an-
tecedent, John Hall, for interpretation. The two
NPs also corefer because each refers to the same
person, JOHN HALL.
As discussed in depth elsewhere (e.g. van
Deemter and Kibble (2000)), the notions of coref-
erence and anaphora are difficult to define pre-
cisely and to operationalize consistently. Further-
more, the connections between them are extremely
complex and go beyond the scope of this paper.
Given these complexities, it is not surprising that
the annotation instructions for the MUC and ACE
data sets reflect different interpretations and sim-
plifications of the general coreference relation. We
outline some of these differences below.
Syntactic Types. To avoid ambiguity, we will
use the term coreference element (CE) to refer
to the set of linguistic expressions that participate
in the coreference relation, as defined for each of
the MUC and ACE tasks.1 At times, it will be im-
portant to distinguish between the CEs that are in-
cluded in the gold standard ? the annotated CEs
? from those that are generated by the corefer-
ence resolution system ? the extracted CEs.
At a high level, both the MUC and ACE eval-
uations define CEs as nouns, pronouns, and noun
phrases. However, the MUC definition excludes
(1) ?nested? named entities (NEs) (e.g. ?Amer-
ica? in ?Bank of America?), (2) relative pronouns,
and (3) gerunds, but allows (4) nested nouns (e.g.
?union? in ?union members?). The ACE defini-
tion, on the other hand, includes relative pronouns
and gerunds, excludes all nested nouns that are not
themselves NPs, and allows premodifier NE men-
tions of geo-political entities and locations, such
as ?Russian? in ?Russian politicians?.
Semantic Types. ACE restricts CEs to entities
that belong to one of seven semantic classes: per-
son, organization, geo-political entity, location, fa-
cility, vehicle, and weapon. MUC has no semantic
restrictions.
Singletons. The MUC data sets include annota-
tions only for CEs that are coreferent with at least
one other CE. ACE, on the other hand, permits
?singleton? CEs, which are not coreferent with
any other CE in the document.
These substantial differences in the task defini-
tions (summarized in Table 1) make it extremely
difficult to compare performance across the MUC
and ACE data sets. In the next section, we take a
closer look at the coreference resolution task, ana-
lyzing the impact of various subtasks irrespective
of the data set differences.
1We define the term CE to be roughly equivalent to (a)
the notion of markable in the MUC coreference resolution
definition and (b) the structures that can be mentions in the
descriptions of ACE.
657
3 Coreference Subtask Analysis
Coreference resolution is a complex task that
requires solving numerous non-trivial subtasks
such as syntactic analysis, semantic class tagging,
pleonastic pronoun identification and antecedent
identification to name a few. This section exam-
ines the role of three such subtasks ? named en-
tity recognition, anaphoricity determination, and
coreference element detection ? in the perfor-
mance of an end-to-end coreference resolution
system. First, however, we describe the corefer-
ence resolver that we use for our study.
3.1 The RECONCILEACL09 Coreference
Resolver
We use the RECONCILE coreference resolution
platform (Stoyanov et al, 2009) to configure a
coreference resolver that performs comparably to
state-of-the-art systems (when evaluated on the
MUC and ACE data sets under comparable as-
sumptions). This system is a classification-based
coreference resolver, modeled after the systems of
Ng and Cardie (2002b) and Bengtson and Roth
(2008). First it classifies pairs of CEs as coreferent
or not coreferent, pairing each identified CE with
all preceding CEs. The CEs are then clustered
into coreference chains2 based on the pairwise de-
cisions. RECONCILE has a pipeline architecture
with four main steps: preprocessing, feature ex-
traction, classification, and clustering. We will
refer to the specific configuration of RECONCILE
used for this paper as RECONCILEACL09.
Preprocessing. The RECONCILEACL09 prepro-
cessor applies a series of language analysis tools
(mostly publicly available software packages) to
the source texts. The OpenNLP toolkit (Baldridge,
J., 2005) performs tokenization, sentence splitting,
and part-of-speech tagging. The Berkeley parser
(Petrov and Klein, 2007) generates phrase struc-
ture parse trees, and the de Marneffe et al (2006)
system produces dependency relations. We em-
ploy the Stanford CRF-based Named Entity Rec-
ognizer (Finkel et al, 2004) for named entity
tagging. With these preprocessing components,
RECONCILEACL09 uses heuristics to correctly ex-
tract approximately 90% of the annotated CEs for
the MUC and ACE data sets.
Feature Set. To achieve roughly state-of-the-
art performance, RECONCILEACL09 employs a
2A coreference chain refers to the set of CEs that refer to
a particular entity.
dataset docs CEs chains CEs/ch tr/tst split
MUC6 60 4232 960 4.4 30/30 (st)
MUC7 50 4297 1081 3.9 30/20 (st)
ACE-2 159 2630 1148 2.3 130/29 (st)
ACE03 105 3106 1340 2.3 74/31
ACE04 128 3037 1332 2.3 90/38
ACE05 81 1991 775 2.6 57/24
Table 2: Dataset characteristics including the number of
documents, annotated CEs, coreference chains, annotated
CEs per chain (average), and number of documents in the
train/test split. We use st to indicate a standard train/test split.
fairly comprehensive set of 61 features introduced
in previous coreference resolution systems (see
Bengtson and Roth (2008)). We briefly summarize
the features here and refer the reader to Stoyanov
et al (2009) for more details.
Lexical (9): String-based comparisons of the two
CEs, such as exact string matching and head noun
matching.
Proximity (5): Sentence and paragraph-based
measures of the distance between two CEs.
Grammatical (28): A wide variety of syntactic
properties of the CEs, either individually or as a
pair. These features are based on part-of-speech
tags, parse trees, or dependency relations. For ex-
ample: one feature indicates whether both CEs are
syntactic subjects; another indicates whether the
CEs are in an appositive construction.
Semantic (19): Capture semantic information
about one or both NPs such as tests for gender and
animacy, semantic compatibility based on Word-
Net, and semantic comparisons of NE types.
Classification and Clustering. We configure
RECONCILEACL09 to use the Averaged Percep-
tron learning algorithm (Freund and Schapire,
1999) and to employ single-link clustering (i.e.
transitive closure) to generate the final partition-
ing.3
3.2 Baseline System Results
Our experiments rely on the MUC and ACE cor-
pora. For ACE, we use only the newswire portion
because it is closest in composition to the MUC
corpora. Statistics for each of the data sets are
shown in Table 2. When available, we use the
standard test/train split. Otherwise, we randomly
split the data into a training and test set following
a 70/30 ratio.
3In trial runs, we investigated alternative classification
and clustering models (e.g. C4.5 decision trees and SVMs;
best-first clustering). The results were comparable.
658
Scoring Algorithms. We evaluate using two
common scoring algorithms4 ? MUC and B3.
The MUC scoring algorithm (Vilain et al, 1995)
computes the F1 score (harmonic mean) of preci-
sion and recall based on the identifcation of unique
coreference links. We use the official MUC scorer
implementation for the two MUC corpora and an
equivalent implementation for ACE.
The B3 algorithm (Bagga and Baldwin, 1998)
computes a precision and recall score for each CE:
precision(ce) = |Rce ?Kce|/|Rce|
recall(ce) = |Rce ?Kce|/|Kce|,
where Rce is the coreference chain to which ce is
assigned in the response (i.e. the system-generated
output) and Kce is the coreference chain that con-
tains ce in the key (i.e. the gold standard). Pre-
cision and recall for a set of documents are com-
puted as the mean over all CEs in the documents
and the F1 score of precision and recall is reported.
B3 Complications. Unlike the MUC score,
which counts links between CEs, B3 presumes
that the gold standard and the system response are
clusterings over the same set of CEs. This, of
course, is not the case when the system automat-
ically identifies the CEs, so the scoring algorithm
requires a mapping between extracted and anno-
tated CEs. We will use the term twin(ce) to refer
to the unique annotated/extracted CE to which the
extracted/annotated CE is matched. We say that
a CE is twinless (has no twin) if no corresponding
CE is identified. A twinless extracted CE signals
that the resolver extracted a spurious CE, while an
annotated CE is twinless when the resolver fails to
extract it.
Unfortunately, it is unclear how the B3 score
should be computed for twinless CEs. Bengtson
and Roth (2008) simply discard twinless CEs, but
this solution is likely too lenient ? it doles no pun-
ishment for mistakes on twinless annotated or ex-
tracted CEs and it would be tricked, for example,
by a system that extracts only the CEs about which
it is most confident.
We propose two different ways to deal with
twinless CEs for B3. One option, B3all, retains
all twinless extracted CEs. It computes the preci-
4We also experimented with the CEAF score (Luo, 2005),
but excluded it due to difficulties dealing with the extracted,
rather than annotated, CEs. CEAF assigns a zero score to
each twinless extracted CE and weights all coreference chains
equally, irrespective of their size. As a result, runs with ex-
tracted CEs exhibit very low CEAF precision, leading to un-
reliable scores.
sion as above when ce has a twin, and computes
the precision as 1/|Rce| if ce is twinless. (Simi-
larly, recall(ce) = 1/|Kce| if ce is twinless.)
The second option, B30, discards twinless
extracted CEs, but penalizes recall by setting
recall(ce) = 0 for all twinless annotated CEs.
Thus, B30 presumes that all twinless extracted
CEs are spurious.
Results. Table 3, box 1 shows the performance
of RECONCILEACL09 using a default (0.5) coref-
erence classifier threshold. The MUC score is
highest for the MUC6 data set, while the four ACE
data sets show much higher B3 scores as com-
pared to the two MUC data sets. The latter occurs
because the ACE data sets include singletons.
The classification threshold, however, can be
gainfully employed to control the trade-off be-
tween precision and recall. This has not tradi-
tionally been done in learning-based coreference
resolution research ? possibly because there is
not much training data available to sacrifice as a
validation set. Nonetheless, we hypothesized that
estimating a threshold from just the training data
might be effective. Our results (BASELINE box
in Table 3) indicate that this indeed works well.5
With the exception of MUC6, results on all data
sets and for all scoring algorithms improve; more-
over, the scores approach those for runs using an
optimal threshold (box 3) for the experiment as de-
termined by using the test set. In all remaining ex-
periments, we learn the threshold from the training
set as in the BASELINE system.
Below, we resume our investigation of the role
of three coreference resolution subtasks and mea-
sure the impact of each on overall performance.
3.3 Named Entities
Previous work has shown that resolving corefer-
ence between proper names is relatively easy (e.g.
Kameyama (1997)) because string matching func-
tions specialized to the type of proper name (e.g.
person vs. location) are quite accurate. Thus, we
would expect a coreference resolution system to
depend critically on its Named Entity (NE) extrac-
tor. On the other hand, state-of-the-art NE taggers
are already quite good, so improving this compo-
nent may not provide much additional gain.
To study the influence of NE recognition,
we replace the system-generated NEs of
5All experiments sample uniformly from 1000 threshold
values.
659
ReconcileACL09 MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
1. DEFAULT THRESHOLD (0.5)
MUC 70.40 58.20 65.76 66.73 56.75 64.30
B3all 69.91 62.88 77.25 77.56 73.03 72.82
B30 68.55 62.80 76.59 77.27 72.99 72.43
2. BASELINE
MUC 68.50 62.80 65.99 67.87 62.03 67.41
= THRESHOLD ESTIMATION
B3all 70.88 65.86 78.29 79.39 76.50 73.71
B30 68.43 64.57 76.63 77.88 75.41 72.47
3. OPTIMAL THRESHOLD
MUC 71.20 62.90 66.83 68.35 62.11 67.41
B3all 72.31 66.52 78.50 79.41 76.53 74.25
B30 69.49 64.64 76.83 78.27 75.51 72.94
4. BASELINE with
MUC 69.90 - 66.37 70.35 62.88 67.72
perfect NEs
B3all 72.31 - 78.06 80.22 77.01 73.92
B30 67.91 - 76.55 78.35 75.22 72.90
5. BASELINE with
MUC 85.80* 81.10* 76.39 79.68 76.18 79.42
perfect CEs
B3all 76.14 75.88 78.65 80.58 77.79 76.49
B30 76.14 75.88 78.65 80.58 77.79 76.49
6. BASELINE with
MUC 82.20* 71.90* 86.63 85.58 83.33 82.84
anaphoric CEs
B3all 72.52 69.26 80.29 79.71 76.05 74.33
B30 72.52 69.26 80.29 79.71 76.05 74.33
Table 3: Impact of Three Subtasks on Coreference Resolution Performance. A score marked with a * indicates that a 0.5
threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e. one that
places all CEs into a single coreference chain.
RECONCILEACL09 with gold-standard NEs
and retrain the coreference classifier. Results
for each of the data sets are shown in box 4 of
Table 3. (No gold standard NEs are available for
MUC7.) Comparison to the BASELINE system
(box 2) shows that using gold standard NEs
leads to improvements on all data sets with the
exception of ACE2 and ACE05, on which perfor-
mance is virtually unchanged. The improvements
tend to be small, however, between 0.5 to 3
performance points. We attribute this to two
factors. First, as noted above, although far from
perfect, NE taggers generally perform reasonably
well. Second, only 20 to 25% of the coreference
element resolutions required for these data sets
involve a proper name (see Section 4).
Conclusion #1: Improving the performance of NE tag-
gers is not likely to have a large impact on the performance
of state-of-the-art coreference resolution systems.
3.4 Coreference Element Detection
We expect CE detection to be an important sub-
problem for an end-to-end coreference system.
Results for a system that assumes perfect CEs
are shown in box 5 of Table 3. For these runs,
RECONCILEACL09 uses only the annotated CEs
for both training and testing. Using perfect CEs
solves a large part of the coreference resolution
task: the annotated CEs divulge anaphoricity in-
formation, perfect NP boundaries, and perfect in-
formation regarding the coreference relation de-
fined for the data set.
We see that focusing attention on all and only
the annotated CEs leads to (often substantial) im-
provements in performance on all metrics over
all data sets, especially when measured using the
MUC score.
Conclusion #2: Improving the ability of coreference re-
solvers to identify coreference elements would likely improve
the state-of-the-art immensely ? by 10-20 points in MUC F1
score and from 2-12 F1 points for B3.
This finding explains previously published re-
sults that exhibit striking variability when run with
annotated CEs vs. system-extracted CEs. On the
MUC6 data set, for example, the best published
MUC score using extracted CEs is approximately
71 (Yang et al, 2003), while multiple systems
have produced MUC scores of approximately 85
when using annotated CEs (e.g. Luo et al (2004),
McCallum and Wellner (2004)).
We argue that providing a resolver with the an-
notated CEs is a rather unrealistic evaluation: de-
termining whether an NP is part of an annotated
coreference chain is precisely the job of a corefer-
ence resolver!
Conclusion #3: Assuming the availability of CEs unre-
alistically simplifies the coreference resolution task.
3.5 Anaphoricity Determination
Finally, several coreference systems have suc-
cessfully incorporated anaphoricity determination
660
modules (e.g. Ng and Cardie (2002a) and Bean
and Riloff (2004)). The goal of the module is to
determine whether or not an NP is anaphoric. For
example, pleonastic pronouns (e.g. it is raining)
are special cases that do not require coreference
resolution.
Unfortunately, neither the MUC nor the ACE
data sets include anaphoricity information for all
NPs. Rather, they encode anaphoricity informa-
tion implicitly for annotated CEs: a CE is consid-
ered anaphoric if is not a singleton.6
To study the utility of anaphoricity informa-
tion, we train and test only on the ?anaphoric? ex-
tracted CEs, i.e. the extracted CEs that have an
annotated twin that is not a singleton. Note that
for the MUC datasets all extracted CEs that have
twins are considered anaphoric.
Results for this experiment (box 6 in Table 3)
are similar to the previous experiment using per-
fect CEs: we observe big improvements across the
board. This should not be surprising since the ex-
perimental setting is quite close to that for perfect
CEs: this experiment also presumes knowledge
of when a CE is part of an annotated coreference
chain. Nevertheless, we see that anaphoricity info-
mation is important. First, good anaphoricity iden-
tification should reduce the set of extracted CEs
making it closer to the set of annotated CEs. Sec-
ond, further improvements in MUC score for the
ACE data sets over the runs using perfect CEs (box
5) reveal that accurately determining anaphoric-
ity can lead to substantial improvements in MUC
score. ACE data includes annotations for single-
ton CEs, so knowling whether an annotated CE is
anaphoric divulges additional information.
Conclusion #4: An accurate anaphoricity determina-
tion component can lead to substantial improvement in coref-
erence resolution performance.
4 Resolution Complexity
Different types of anaphora that have to be han-
dled by coreference resolution systems exhibit dif-
ferent properties. In linguistic theory, binding
mechanisms vary for different kinds of syntactic
constituents and structures. And in practice, em-
pirical results have confirmed intuitions that differ-
ent types of anaphora benefit from different clas-
sifier features and exhibit varying degrees of diffi-
culty (Kameyama, 1997). However, performance
6Also, the first element of a coreference chain is usually
non-anaphoric, but we do not consider that issue here.
evaluations rarely include analysis of where state-
of-the-art coreference resolvers perform best and
worst, aside from general conclusions.
In this section, we analyze the behavior of
our coreference resolver on different types of
anaphoric expressions with two goals in mind.
First, we want to deduce the strengths and weak-
nesses of state-of-the-art systems to help direct
future research. Second, we aim to understand
why current coreference resolvers behave so in-
consistently across data sets. Our hypothesis is
that the distribution of different types of anaphoric
expressions in a corpus is a major factor for coref-
erence resolution performance. Our experiments
confirm this hypothesis and we use our empirical
results to create a coreference performance predic-
tion (CPP) measure that successfully estimates the
expected level of performance on novel data sets.
4.1 Resolution Classes
We study the resolution complexity of a text cor-
pus by defining resolution classes. Resolution
classes partition the set of anaphoric CEs accord-
ing to properties of the anaphor and (in some
cases) the antecedent. Previous work has stud-
ied performance differences between pronominal
anaphora, proper names, and common nouns, but
we aim to dig deeper into subclasses of each of
these groups. In particular, we distinguish be-
tween proper and common nouns that can be re-
solved via string matching, versus those that have
no antecedent with a matching string. Intuitively,
we expect that it is easier to resolve the cases
that involve string matching. Similarly, we par-
tition pronominal anaphora into several subcate-
gories that we expect may behave differently. We
define the following nine resolution classes:
Proper Names: Three resolution classes cover
CEs that are named entities (e.g. the PER-
SON, LOCATION, ORGANIZATION and DATE
classes for MUC and ACE) and have a prior ref-
erent7 in the text. These three classes are distin-
guished by the type of antecedent that can be re-
solved against the proper name.
(1) PN-e: a proper name is assigned to this exact string match
class if there is at least one preceding CE in its gold standard
coreference chain that exactly matches it.
(2) PN-p: a proper name is assigned to this partial string
match class if there is at least one preceding CE in its gold
standard chain that has some content words in common.
(3) PN-n: a proper name is assigned to this no string match
7We make a rough, but rarely inaccurate, assumption that
there are no cataphoric expressions in the data.
661
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05 Avg
# % scr # % scr # % scr # % scr # % scr # % scr % scr
PN-e 273 17 .87 249 19 .79 346 24 .94 435 25 .93 267 16 .88 373 31 .92 22 .89
PN-p 157 10 .68 79 6 .59 116 8 .86 178 10 .87 194 11 .71 125 10 .71 9 .74
PN-n 18 1 .18 18 1 .28 85 6 .19 79 4 .15 66 4 .21 89 7 .27 4 .21
CN-e 292 18 .82 276 21 .65 84 6 .40 186 11 .68 165 10 .68 134 11 .79 13 .67
CN-p 229 14 .53 239 18 .49 147 10 .26 168 10 .24 147 9 .40 147 12 .43 12 .39
CN-n 194 12 .27 148 11 .15 152 10 .50 148 8 .90 266 16 .32 121 10 .20 11 .18
1+2Pr 48 3 .70 65 5 .66 122 8 .73 76 4 .73 158 9 .77 51 4 .61 6 .70
G3Pr 160 10 .73 50 4 .79 181 12 .83 237 13 .82 246 14 .84 69 60 .81 10 .80
U3Pr 175 11 .49 142 11 .49 163 11 .45 122 7 .48 153 9 .49 91 7 .49 9 .48
Table 4: Frequencies and scores for each resolution class.
class if no preceding CE in its gold standard chain has any
content words in common with it.
Common NPs: Three analogous string match
classes cover CEs that have a common noun as a
head: (4) CN-e (5) CN-p (6) CN-n.
Pronouns: Three classes cover pronouns:
(7) 1+2Pr: The anaphor is a 1st or 2nd person pronoun.
(8) G3Pr: The anaphor is a gendered 3rd person pronoun
(e.g. ?she?, ?him?).
(9) U3Pr: The anaphor is an ungendered 3rd person pro-
noun.
As noted above, resolution classes are defined for
annotated CEs. We use the twin relationship to
match extracted CEs to annotated CEs and to eval-
uate performance on each resolution class.
4.2 Scoring Resolution Classes
To score each resolution class separately, we de-
fine a new variant of the MUC scorer. We compute
a MUC-RC score (for MUC Resolution Class) for
class C as follows: we assume that all CEs that do
not belong to class C are resolved correctly by tak-
ing the correct clustering for them from the gold
standard. Starting with this correct partial cluster-
ing, we run our classifier on all ordered pairs of
CEs for which the second CE is of class C, es-
sentially asking our coreference resolver to deter-
mine whether each member of class C is corefer-
ent with each of its preceding CEs. We then count
the number of unique correct/incorrect links that
the system introduced on top of the correct par-
tial clustering and compute precision, recall, and
F1 score. This scoring function directly measures
the impact of each resolution class on the overall
MUC score.
4.3 Results
Table 4 shows the results of our resolution class
analysis on the test portions of the six data sets.
The # columns show the frequency counts for each
resolution class, and the % columns show the dis-
tributions of the classes in each corpus (i.e. 17%
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
0.92 0.95 0.91 0.98 0.97 0.96
Table 5: Correlations of resolution class scores with respect
to the average.
of all resolutions in the MUC6 corpus were in the
PN-e class). The scr columns show the MUC-
RC score for each resolution class. The right-hand
side of Table 4 shows the average distribution and
scores across all data sets.
These scores confirm our expectations about the
relative difficulty of different types of resolutions.
For example, it appears that proper names are eas-
ier to resolve than common nouns; gendered pro-
nouns are easier than 1st and 2nd person pronouns,
which, in turn, are easier than ungendered 3rd per-
son pronouns. Similarly, our intuition is confirmed
that many CEs can be accurately resolved based on
exact string matching, whereas resolving against
antecedents that do not have overlapping strings is
much more difficult. The average scores in Table 4
show that performance varies dramatically across
the resolution classes, but, on the surface, appears
to be relatively consistent across data sets.
None of the data sets performs exactly the same,
of course, so we statistically analyze whether the
behavior of each resolution class is similar across
the data sets. For each data set, we compute the
correlation between the vector of MUC-RC scores
over the resolution classes and the average vec-
tor of MUC-RC scores for the remaining five data
sets. Table 5 contains the results, which show high
correlations (over .90) for all six data sets. These
results indicate that the relative performance of the
resolution classes is consistent across corpora.
4.4 Coreference Performance Prediction
Next, we hypothesize that the distribution of res-
olution classes in a corpus explains (at least par-
tially) why performance varies so much from cor-
662
MUC6 MUC7 ACE2 ACE03 ACE04 ACE05
P 0.59 0.59 0.62 0.65 0.59 0.62
O 0.67 0.61 0.66 0.68 0.62 0.67
Table 6: Predicted (P) vs Observed (O) scores.
pus to corpus. To explore this issue, we create a
Coreference Performance Prediction (CPP) mea-
sure to predict the performance on new data sets.
The CPP measure uses the empirical performance
of each resolution class observed on previous data
sets and forms a predicton based on the make-up
of resolution classes in a new corpus. The distribu-
tion of resolution classes for a new corpus can be
easily determined because the classes can be rec-
ognized superficially by looking only at the strings
that represent each NP.
We compute the CPP score for each of our six
data sets based on the average resolution class per-
formance measured on the other five data sets.
The predicted score for each class is computed as
a weighted sum of the observed scores for each
resolution class (i.e. the mean for the class mea-
sured on the other five data sets) weighted by the
proportion of CEs that belong to the class. The
predicted scores are shown in Table 6 and com-
pared with the MUC scores that are produced by
RECONCILEACL09.8
Our results show that the CPP measure is a
good predictor of coreference resolution perfor-
mance on unseen data sets, with the exception
of one outlier ? the MUC6 data set. In fact,
the correlation between predicted and observed
scores is 0.731 for all data sets and 0.913 exclud-
ing MUC6. RECONCILEACL09?s performance on
MUC6 is better than predicted due to the higher
than average scores for the common noun classes.
We attribute this to the fact that MUC6 includes
annotations for nested nouns, which almost al-
ways fall in the CN-e and CN-p classes. In ad-
dition, many of the features were first created for
the MUC6 data set, so the feature extractors are
likely more accurate than for other data sets.
Overall, results indicate that coreference perfor-
mance is substantially influenced by the mix of
resolution classes found in the data set. Our CPP
measure can be used to produce a good estimate
of the level of performance on a new corpus.
8Observed scores for MUC6 and 7 differ slightly from Ta-
ble 3 because this part of the work did not use the OPTIONAL
field of the key, employed by the official MUC scorer.
5 Related Work
The bulk of the relevant related work is described
in earlier sections, as appropriate. This paper stud-
ies complexity issues for NP coreference resolu-
tion using a ?good?, i.e. near state-of-the-art, sys-
tem. For state-of-the-art performance on the MUC
data sets see, e.g. Yang et al (2003); for state-of-
the-art performance on the ACE data sets see, e.g.
Bengtson and Roth (2008) and Luo (2007). While
other researchers have evaluated NP coreference
resolvers with respect to pronouns vs. proper
nouns vs. common nouns (Ng and Cardie, 2002b),
our analysis focuses on measuring the complexity
of data sets, predicting the performance of coref-
erence systems on new data sets, and quantify-
ing the effect of coreference system subcompo-
nents on overall performance. In the related area
of anaphora resolution, researchers have studied
the influence of subsystems on the overall per-
formance (Mitkov, 2002) as well as defined and
evaluated performance on different classes of pro-
nouns (e.g. Mitkov (2002) and Byron (2001)).
However, due to the significant differences in task
definition, available datasets, and evaluation met-
rics, their conclusions are not directly applicable
to the full coreference task.
Previous work has developed methods to predict
system performance on NLP tasks given data set
characteristics, e.g. Birch et al (2008) does this for
machine translation. Our work looks for the first
time at predicting the performance of NP corefer-
ence resolvers.
6 Conclusions
We examine the state-of-the-art in NP coreference
resolution. We show the relative impact of perfect
NE recognition, perfect anaphoricity information
for coreference elements, and knowledge of all
and only the annotated CEs. We also measure the
performance of state-of-the-art resolvers on sev-
eral classes of anaphora and use these results to
develop a measure that can accurately estimate a
resolver?s performance on new data sets.
Acknowledgments. We gratefully acknowledge
technical contributions from David Buttler and
David Hysom in creating the Reconcile corefer-
ence resolution platform. This research was sup-
ported in part by the Department of Homeland
Security under ONR Grant N0014-07-1-0152 and
Lawrence Livermore National Laboratory subcon-
tract B573245.
663
References
A. Bagga and B. Baldwin. 1998. Algorithms for Scor-
ing Coreference Chains. In In Linguistic Corefer-
ence Workshop at LREC 1998.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
D. Bean and E. Riloff. 2004. Unsupervised Learn-
ing of Contextual Role Knowledge for Coreference
Resolution. In Proceedings of the Annual Meeting
of the North American Chapter of the Association
for Computational Linguistics (HLT/NAACL 2004).
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294?303. Association for Computational Linguis-
tics.
Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008. Predicting Success in Machine Translation.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
745?754. Association for Computational Linguis-
tics.
Donna Byron. 2001. The Uncommon Denomina-
tor: A Proposal for Consistent Reporting of Pro-
noun Resolution Results. Computational Linguis-
tics, 27(4):569?578.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
LREC.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, and
C. Manning. 2004. Exploiting Context for Biomed-
ical Entity Recognition: From Syntax to the Web. In
Joint Workshop on Natural Language Processing in
Biomedicine and its Applications at COLING 2004.
Yoav Freund and Robert E. Schapire. 1999. Large
Margin Classification Using the Perceptron Algo-
rithm. In Machine Learning, pages 277?296.
Megumi Kameyama. 1997. Recognizing Referential
Links: An Information Extraction Perspective. In
Workshop On Operational Factors In Practical Ro-
bust Anaphora Resolution For Unrestricted Texts.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A
Mention-Synchronous Coreference Resolution Al-
gorithm Based on the Bell Tree. In Proceedings
of the 42nd Annual Meeting of the Association for
Computational Linguistics.
X. Luo. 2005. On Coreference Resolution Perfor-
mance Metrics. In Proceedings of the 2005 Human
Language Technology Conference / Conference on
Empirical Methods in Natural Language Process-
ing.
Xiaoqiang Luo. 2007. Coreference or Not: A Twin
Model for Coreference Resolution. In Proceedings
of the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL 2007).
A. McCallum and B. Wellner. 2004. Conditional Mod-
els of Identity Uncertainty with Application to Noun
Coreference. In 18th Annual Conference on Neural
Information Processing Systems.
Ruslan Mitkov. 2002. Anaphora Resolution. Long-
man, London.
MUC-6. 1995. Coreference Task Definition. In Pro-
ceedings of the Sixth Message Understanding Con-
ference (MUC-6), pages 335?344.
MUC-7. 1997. Coreference Task Definition. In
Proceedings of the Seventh Message Understanding
Conference (MUC-7).
V. Ng and C. Cardie. 2002a. Identifying Anaphoric
and Non-Anaphoric Noun Phrases to Improve
Coreference Resolution. In Proceedings of the 19th
International Conference on Computational Lin-
guistics (COLING 2002).
V. Ng and C. Cardie. 2002b. Improving Machine
Learning Approaches to Coreference Resolution. In
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
NIST. 2004. The ACE Evaluation Plan.
S. Petrov and D. Klein. 2007. Improved Inference for
Unlexicalized Parsing. In Proceedings of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (HLT/NAACL
2007).
W. Soon, H. Ng, and D. Lim. 2001. A Machine
Learning Approach to Coreference of Noun Phrases.
Computational Linguistics, 27(4):521?541.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, Ellen
Riloff, David Buttler, and David Hysom. 2009.
Reconcile: A Coreference Resolution Research Plat-
form. Computer Science Technical Report, Cornell
University, Ithaca, NY.
Kees van Deemter and Rodger Kibble. 2000. On
Coreferring: Coreference in MUC and Related
Annotation Schemes. Computational Linguistics,
26(4):629?637.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Corefer-
ence Scoring Theme. In Proceedings of the Sixth
Message Understanding Conference (MUC-6).
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference Resolution Us-
ing Competition Learning Approach. In ACL ?03:
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 176?183.
664
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 9?14,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Toward Opinion Summarization: Linking the Sources
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
{ves,cardie}@cs.cornell.edu
Abstract
We target the problem of linking source
mentions that belong to the same entity
(source coreference resolution), which is
needed for creating opinion summaries. In
this paper we describe how source coref-
erence resolution can be transformed into
standard noun phrase coreference resolu-
tion, apply a state-of-the-art coreference
resolution approach to the transformed
data, and evaluate on an available corpus
of manually annotated opinions.
1 Introduction
Sentiment analysis is concerned with the extrac-
tion and representation of attitudes, evaluations,
opinions, and sentiment from text. The area of
sentiment analysis has been the subject of much
recent research interest driven by two primary mo-
tivations. First, there is a desire to provide appli-
cations that can extract, represent, and allow the
exploration of opinions in the commercial, gov-
ernment, and political domains. Second, effec-
tive sentiment analysis might be used to enhance
and improve existing NLP applications such as in-
formation extraction, question answering, summa-
rization, and clustering (e.g. Riloff et al (2005),
Stoyanov et al (2005)).
Several research efforts (e.g. Riloff and Wiebe
(2003), Bethard et al (2004), Wilson et al (2004),
Yu and Hatzivassiloglou (2003), Wiebe and Riloff
(2005)) have shown that sentiment information
can be extracted at the sentence, clause, or indi-
vidual opinion expression level (fine-grained opin-
ion information). However, little has been done to
develop methods for combining fine-grained opin-
ion information to form a summary representa-
tion in which expressions of opinions from the
same source/target1 are grouped together, multi-
ple opinions from a source toward the same tar-
get are accumulated into an aggregated opinion,
and cumulative statistics are computed for each
source/target. A simple opinion summary2 is
shown in Figure 1. Being able to create opinion
summaries is important both for stand-alone ap-
plications of sentiment analysis as well as for the
potential uses of sentiment analysis as part of other
NLP applications.
In this work we address the dearth of ap-
proaches for summarizing opinion information.
In particular, we focus on the problem of source
coreference resolution, i.e. deciding which source
mentions are associated with opinions that belong
to the same real-world entity. In the example from
Figure 1 performing source coreference resolution
amounts to determining that Stanishev, he, and he
refer to the same real-world entities. Given the
associated opinion expressions and their polarity,
this source coreference information is the critical
knowledge needed to produce the summary of Fig-
ure 1 (although the two target mentions, Bulgaria
and our country, would also need to be identified
as coreferent).
Our work is concerned with fine-grained ex-
pressions of opinions and assumes that a system
can rely on the results of effective opinion and
source extractors such as those described in Riloff
and Wiebe (2003), Bethard et al (2004), Wiebe
and Riloff (2005) and Choi et al (2005). Presented
with sources of opinions, we approach the prob-
lem of source coreference resolution as the closely
1We use source to denote an opinion holder and target to
denote the entity toward which the opinion is directed.
2For simplicity, the example summary does not contain
any source/target statistics or combination of multiple opin-
ions from the same source to the same target.
9
? [Target Delaying of Bulgaria?s accession to the EU] would
be a serious mistake? [Source Bulgarian Prime Minister
Sergey Stanishev] said in an interview for the German daily
Suddeutsche Zeitung. ?[Target Our country] serves as a
model and encourages countries from the region to follow
despite the difficulties?, [Source he] added.
[Target Bulgaria] is criticized by [Source the EU] because of
slow reforms in the judiciary branch, the newspaper notes.
Stanishev was elected prime minister in 2005. Since then,
[Source he] has been a prominent supporter of [Target his
country?s accession to the EU].
Stanishev Accession
EU
Bulgaria
Delaying
+
? ?
+
Figure 1: Example of text containing opinions
(above) and a summary of the opinions (below).
In the text, sources and targets of opinions are
marked and opinion expressions are shown in
italic. In the summary graph, + stands for positive
opinion and - for negative.
related task of noun phrase coreference resolu-
tion. However, source coreference resolution dif-
fers from traditional noun phrase (NP) coreference
resolution in two important aspects discussed in
Section 4. Nevertheless, as a first attempt at source
coreference resolution, we employ a state-of-the-
art machine learning approach to NP coreference
resolution developed by Ng and Cardie (2002).
Using a corpus of manually annotated opinions,
we perform an extensive evaluation and obtain
strong initial results for the task of source coref-
erence resolution.
2 Related Work
Sentiment analysis has been a subject of much re-
cent research. Several efforts have attempted to
automatically extract opinions, emotions, and sen-
timent from text. The problem of sentiment ex-
traction at the document level (sentiment classifi-
cation) has been tackled as a text categorization
task in which the goal is to assign to a document
either positive (?thumbs up?) or negative (?thumbs
down?) polarity (e.g. Das and Chen (2001), Pang
et al (2002), Turney (2002), Dave et al (2003),
Pang and Lee (2004)). In contrast, the problem of
fine-grained opinion extraction has concentrated
on recognizing opinions at the sentence, clause,
or individual opinion expression level. Recent
work has shown that systems can be trained to rec-
ognize opinions, their polarity, and their strength
at a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et
al. (2004), Pang and Lee (2004), Wilson et al
(2004), Yu and Hatzivassiloglou (2003), Wiebe
and Riloff (2005)). Additionally, researchers have
been able to effectively identify sources of opin-
ions automatically (Bethard et al, 2004; Choi et
al., 2005; Kim and Hovy, 2005). Finally, Liu et al
(2005) summarize automatically generated opin-
ions about products and develop interface that al-
lows the summaries to be vizualized.
Our work also draws on previous work in the
area of coreference resolution, which is a rela-
tively well studied NLP problem. Coreference
resolution is the problem of deciding what noun
phrases in the text (i.e. mentions) refer to the same
real-world entities (i.e. are coreferent). Generally,
successful approaches have relied machine learn-
ing methods trained on a corpus of documents
annotated with coreference information (such as
the MUC and ACE corpora). Our approach to
source coreference resolution is inspired by the
state-of-the-art performance of the method of Ng
and Cardie (2002).
3 Data set
We begin our discussion by describing the data set
that we use for development and evaluation.
As noted previously, we desire methods that
work with automatically identified opinions and
sources. However, for the purpose of developing
and evaluating our approaches we rely on a corpus
of manually annotated opinions and sources. More
precisely, we rely on the MPQA corpus (Wilson
and Wiebe, 2003)3, which contains 535 manu-
ally annotated documents. Full details about the
corpus and the process of corpus creation can be
found in Wilson and Wiebe (2003); full details
of the opinion annotation scheme can be found in
Wiebe et al (2005). For the purposes of the dis-
cussion in this paper, the following three points
suffice.
First, the corpus is suitable for the domains and
genres that we target ? all documents have oc-
curred in the world press over an 11-month period,
between June 2001 and May 2002. Therefore, the
3The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
10
corpus is suitable for the political and government
domains as well as a substantial part of the com-
mercial domain. However, a fair portion of the
commercial domain is concerned with opinion ex-
traction from product reviews. Work described in
this paper does not target the genre of reviews,
which appears to differ significantly from news-
paper articles.
Second, all documents are manually annotated
with phrase-level opinion information. The an-
notation scheme of Wiebe et al (2005) includes
phrase level opinions, their sources, as well as
other attributes, which are not utilized by our ap-
proach. Additionally, the annotations contain in-
formation that allows coreference among source
mentions to be recovered.
Finally, the MPQA corpus contains no corefer-
ence information for general NPs (which are not
sources). This might present a problem for tradi-
tional coreference resolution approaches, as dis-
cussed throughout the paper.
4 Source Coreference Resolution
In this Section we define the problem of source
coreference resolution, describe its challenges,
and provide an overview of our general approach.
We define source coreference resolution as the
problem of determining which mentions of opin-
ion sources refer to the same real-world entity.
Source coreference resolution differs from tradi-
tional supervised NP coreference resolution in two
important aspects. First, sources of opinions do
not exactly correspond to the automatic extrac-
tors? notion of noun phrases (NPs). Second, due
mainly to the time-consuming nature of corefer-
ence annotation, NP coreference information is in-
complete in our data set: NP mentions that are not
sources of opinion are not annotated with coref-
erence information (even when they are part of
a chain that contains source NPs)4. In this pa-
per we address the former problem via a heuris-
tic method for mapping sources to NPs and give
statistics for the accuracy of the mapping process.
We then apply state-of-the-art coreference resolu-
tion methods to the NPs to which sources were
4This problem is illustrated in the example of Figure 1
The underlined Stanishev is coreferent with all of the Stan-
ishev references marked as sources, but, because it is used
in an objective sentence rather than as the source of an opin-
ion, the reference would be omitted from the Stanishev source
coreference chain. Unfortunately, this proper noun might be
critical in establishing coreference of the final source refer-
ence he with the other mentions of the source Stanishev.
Single Match Multiple Matches No Match
Total 7811 3461 50
Exact 6242 1303 0
Table 1: Statistics for matching sources to noun
phrases.
mapped (source noun phrases). The latter prob-
lem of developing methods that can work with in-
complete supervisory information is addressed in
a subsequent effort (Stoyanov and Cardie, 2006).
Our general approach to source coreference res-
olution consists of the following steps:
1. Preprocessing: We preprocess the corpus by running
NLP components such as a tokenizer, sentence split-
ter, POS tagger, parser, and a base NP finder. Sub-
sequently, we augment the set of the base NPs found
by the base NP finder with the help of a named en-
tity finder. The preprocessing is done following the NP
coreference work by Ng and Cardie (2002). From the
preprocessing step, we obtain an augmented set of NPs
in the text.
2. Source to noun phrase mapping: The problem
of mapping (manually or automatically annotated)
sources to NPs is not trivial. We map sources to NPs
using a set of heuristics.
3. Coreference resolution: Finally, we restrict our atten-
tion to the source NPs identified in step 2. We extract
a feature vector for every pair of source NPs from the
preprocessed corpus and perform NP coreference reso-
lution.
The next two sections give the details of Steps 2
and 3, respectively. We follow with the results of
an evaluation of our approach in Section 7.
5 Mapping sources to noun phrases
This section describes our method for heuristically
mapping sources to NPs. In the context of source
coreference resolution we consider a noun phrase
to correspond to (or match) a source if the source
and the NP cover the exact same span of text. Un-
fortunately, the annotated sources did not always
match exactly a single automatically extracted NP.
We discovered the following problems:
1. Inexact span match. We discovered that often (in
3777 out of the 11322 source mentions) there is no
noun phrase whose span matches exactly the source al-
though there are noun phrases that overlap the source.
In most cases this is due to the way spans of sources
are marked in the data. For instance, in some cases
determiners are not included in the source span (e.g.
?Venezuelan people? vs. ?the Venezuelan people?). In
other cases, differences are due to mistakes by the NP
extractor (e.g. ?Muslims rulers? was not recognized,
while ?Muslims? and ?rulers? were recognized). Yet in
other cases, manually marked sources do not match the
definition of a noun phrase. This case is described in
more detail next.
11
Measure Overall Method and Instance B3 MUC Positive Identification Actual Pos. Identification
rank parameters selection score Prec. Recall F1 Prec. Recall F1
B3 1 svm C10 ?0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2
400 5 ripper asc L2 soon2 80.7 72.2 74.5 45.2 56.3 55.1 62.1 58.4
Training MUC Score 1 svm C10 ?0.01 soon1 77.3 74.2 67.4 51.7 58.5 37.8 70.9 49.3
Documents 4 ripper acs L1.5 soon2 78.4 73.6 68.3 49.0 57.0 40.0 69.9 50.9
Positive 1 svm C10 ?0.05 soon1 72.7 73.9 60.0 57.2 58.6 37.8 71.0 49.3
identification 4 ripper acs L1.5 soon1 78.9 73.6 68.8 48.9 57.2 40.0 69.9 50.9
Actual pos. 1 svm C10 ?0.01 none 81.8 71.7 80.2 43.7 56.6 57.5 62.9 60.2
identification 2 ripper asc L4 soon2 73.9 69.9 81.1 40.2 53.9 69.8 52.5 60.0
B3 1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6
9 svm C10 ?0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9
200 MUC Score 1 svm C1 ?0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9
Training 5 ripper acs L1 soon1 77.9 0.732 71.4 46.5 56.3 37.7 69.7 48.9
Documents Positive 1 svm C1 ?0.1 soon1 74.8 73.8 63.2 55.2 58.9 32.1 74.4 44.9
identification 4 ripper acs L1 soon1 75.3 72.4 69.1 48.0 56.7 33.3 72.3 45.6
Actual pos. 1 ripper acs L4 none 81.8 67.8 91.4 32.7 48.2 72.0 52.5 60.6
identification 10 svm C10 ?0.01 none 81.4 70.3 81.6 40.8 54.4 58.4 61.6 59.9
Table 2: Performance of the best runs. For SVMs, ? stands for RBF kernel with the shown ? parameter.
2. Multiple NP match. For 3461 of the 11322 source
mentions more than one NP overlaps the source. In
roughly a quarter of these cases the multiple match is
due to the presence of nested NPs (introduced by the
NP augmentation process introduced in Section 3). In
other cases the multiple match is caused by source an-
notations that spanned multiple NPs or included more
than only NPs inside its span. There are three gen-
eral classes of such sources. First, some of the marked
sources are appositives such as ?the country?s new pres-
ident, Eduardo Duhalde?. Second, some sources con-
tain an NP followed by an attached prepositional phrase
such as ?Latin American leaders at a summit meeting in
Costa Rica?. Third, some sources are conjunctions of
NPs such as ?Britain, Canada and Australia?. Treat-
ment of the latter is still a controversial problem in
the context of coreference resolution as it is unclear
whether conjunctions represent entities that are distinct
from the conjuncts. For the purpose of our current work
we do not attempt to address conjunctions.
3. No matching NP. Finally, for 50 of the 11322 sources
there are no overlapping NPs. Half of those (25 to
be exact) included marking of the word ?who? such
as in the sentence ?Carmona named new ministers,
including two military officers who rebelled against
Chavez?. From the other 25, 19 included markings of
non-NPs including question words, qualifiers, and ad-
jectives such as ?many?, ?which?, and ?domestically?.
The remaining six are rare NPs such as ?lash? and
?taskforce? that are mistakenly not recognized by the
NP extractor.
Counts for the different types of matches of
sources to NPs are shown in Table 1. We deter-
mine the match in the problematic cases using a
set of heuristics:
1. If a source matches any NP exactly in span, match that
source to the NP; do this even if multiple NPs overlap
the source ? we are dealing with nested NP?s.
2. If no NP matches matches exactly in span then:
? If a single NP overlaps the source, then map the
source to that NP.Most likely we are dealing with
differently marked spans.
? If multiple NPs overlap the source, determine
whether the set of overlapping NPs include any
non-nested NPs. If all overlapping NPs are
nested with each other, select the NP that is
closer in span to the source ? we are still dealing
with differently marked spans, but now we also
have nested NPs. If there is more than one set
of nested NPs, then most likely the source spans
more than a single NP. In this case we select the
outermost of the last set of nested NPs before any
preposition in the span. We prefer: the outermost
NP because longer NPs contain more informa-
tion; the last NP because it is likely to be the head
NP of a phrase (also handles the case of expla-
nation followed by a proper noun); NP?s before
preposition, because a preposition signals an ex-
planatory prepositional phrase.
3. If no NP overlaps the source, select the last NP before
the source. In half of the cases we are dealing with the
word who, which typically refers to the last preceding
NP.
6 Source coreference resolution as
coreference resolution
Once we isolate the source NPs, we apply corefer-
ence resolution using the standard combination of
classification and single-link clustering (e.g. Soon
et al (2001) and Ng and Cardie (2002)).
We compute a vector of 57 features for every
pair of source noun phrases from the preprocessed
corpus. We use the training set of pairwise in-
stances to train a classifier to predict whether a
source NP pair should be classified as positive (the
NPs refer to the same entity) or negative (different
entities). During testing, we use the trained clas-
sifier to predict whether a source NP pair is pos-
itive and single-link clustering to group together
sources that belong to the same entity.
7 Evaluation
For evaluation we randomly split the MPQA cor-
pus into a training set consisting of 400 documents
12
and a test set consisting of the remaining 135 doc-
uments. We use the same test set for all evalua-
tions, although not all runs were trained on all 400
training documents as discussed below.
The purpose of our evaluation is to create a
strong baseline utilizing the best settings for the
NP coreference approach. As such, we try the
two reportedly best machine learning techniques
for pairwise classification ? RIPPER (for Re-
peated Incremental Pruning to Produce Error Re-
duction) (Cohen, 1995) and support vector ma-
chines (SVMs) in the SVM light implementation
(Joachims, 1998). Additionally, to exclude pos-
sible effects of parameter selection, we try many
different parameter settings for the two classifiers.
For RIPPER we vary the order of classes and the
positive/negative weight ratio. For SVMs we vary
C (the margin tradeoff) and the type and parameter
of the kernel. In total, we use 24 different settings
for RIPPER and 56 for SVM light.
Additionally, Ng and Cardie reported better re-
sults when the training data distribution is bal-
anced through instance selection. For instance
selection they adopt the method of Soon et al
(2001), which selects for each NP the pairs with
the n preceding coreferent instances and all in-
tervening non-coreferent pairs. Following Ng and
Cardie (2002), we perform instance selection with
n = 1 (soon1 in the results) and n = 2 (soon2).
With the three different instance selection algo-
rithms (soon1, soon2, and none), the total number
of settings is 72 for RIPPER and 168 for SVMa.
However, not all SVM runs completed in the time
limit that we set ? 200 min, so we selected half
of the training set (200 documents) at random and
trained all classifiers on that set. We made sure
to run to completion on the full training set those
SVM settings that produced the best results on the
smaller training set.
Table 2 lists the results of the best performing
runs. The upper half of the table gives the re-
sults for the runs that were trained on 400 docu-
ments and the lower half contains the results for
the 200-document training set. We evaluated us-
ing the two widely used performance measures for
coreference resolution ? MUC score (Vilain et al,
1995) and B3 (Bagga and Baldwin, 1998). In ad-
dition, we used performance metrics (precision,
recall and F1) on the identification of the posi-
tive class. We compute the latter in two different
ways ? either by using the pairwise decisions as
the classifiers outputs them or by performing the
clustering of the source NPs and then considering
a pairwise decision to be positive if the two source
NPs belong to the same cluster. The second option
(marked actual in Table 2) should be more repre-
sentative of a good clustering, since coreference
decisions are important only in the context of the
clusters that they create.
Table 2 shows the performance of the best RIP-
PER and SVM runs for each of the four evaluation
metrics. The table also lists the rank for each run
among the rest of the runs.
7.1 Discussion
The absolute B3 and MUC scores for source
coreference resolution are comparable to reported
state-of-the-art results for NP coreference resolu-
tions. Results should be interpreted cautiously,
however, due to the different characteristics of our
data. Our documents contained 35.34 source NPs
per document on average, with coreference chains
consisting of only 2.77 NPs on average. The low
average number of NPs per chain may be produc-
ing artificially high score for the B3 and MUC
scores as the modest results on positive class iden-
tification indicate.
From the relative performance of our runs, we
observe the following trends. First, SVMs trained
on the full training set outperform RIPPER trained
on the same training set as well as the correspond-
ing SVMs trained on the 200-document training
set. The RIPPER runs exhibit the opposite be-
havior ? RIPPER outperforms SVMs on the 200-
document training set and RIPPER runs trained
on the smaller data set exhibit better performance.
Overall, the single best performance is observed
by RIPPER using the smaller training set.
Another interesting observation is that the B3
measure correlates well with good ?actual? perfor-
mance on positive class identification. In contrast,
good MUC performance is associated with runs
that exhibit high recall on the positive class. This
confirms some theoretical concerns that MUC
score does not reward algorithms that recognize
well the absence of links. In addition, the results
confirm our conjecture that ?actual? precision and
recall are more indicative of the true performance
of coreference algorithms.
13
8 Conclusions
As a first step toward opinion summarization we
targeted the problem of source coreference resolu-
tion. We showed that the problem can be tackled
effectively as noun coreference resolution.
One aspect of source coreference resolution that
we do not address is the use of unsupervised infor-
mation. The corpus contains many automatically
identified non-source NPs, which can be used to
benefit source coreference resolution in two ways.
First, a machine learning approach could use the
unlabeled data to estimate the overall distributions.
Second, some links between sources may be real-
ized through a non-source NPs (see the example
of figure 1). As a follow-up to the work described
in this paper we developed a method that utilizes
the unlabeled NPs in the corpus using a structured
rule learner (Stoyanov and Cardie, 2006).
Acknowledgements
The authors would like to thank Vincent Ng and Art Munson
for providing coreference resolution code, members of the
Cornell NLP group (especially Yejin Choi and Art Munson)
for many helpful discussions, and the anonymous reviewers
for their insightful comments. This work was supported by
the Advanced Research and Development Activity (ARDA),
by NSF Grants IIS-0535099 and IIS-0208028, by gifts from
Google and the Xerox Foundation, and by an NSF Graduate
Research Fellowship to the first author.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model. In
Proceedings of COLING/ACL.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and
D. Jurafsky. 2004. Automatic extraction of opinion
propositions and their holders. In 2004 AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Iden-
tifying sources of opinions with conditional random fields
and extraction patterns. In Proceedings of EMNLP.
W. Cohen. 1995. Fast effective rule induction. In Proceed-
ings of ICML.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting
market sentiment from stock message boards. In Proceed-
ings of APFAAC.
K. Dave, S. Lawrence, and D. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifi-
cation of product reviews. In Proceedings of IWWWC.
T. Joachims. 1998. Making large-scale support vector
machine learning practical. In A. Smola B. Scho?lkopf,
C. Burges, editor, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA.
S. Kim and E. Hovy. 2005. Identifying opinion holders for
question answering in opinion texts. In Proceedings of
AAAI Workshop on Question Answering in Restricted Do-
mains.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer: An-
alyzing and comparing opinions on the web. In Proceed-
ings of International World Wide Web Conference.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings of
ACL.
B. Pang and L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns
for subjective expressions. In Proceesings of EMNLP.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction.
In Proceedings of AAAI.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases. Com-
putational Linguistics, 27(4).
V. Stoyanov and C. Cardie. 2006. Partially supervised
coreference resolution for opinion summarization through
structured rule learning. In Proceedings of EMNLP.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
Perspective question answering using the OpQA corpus.
In Proceedings of EMNLP.
P. Turney. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proceedings of ACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference scor-
ing scheme. In Proceedings of MUC-6.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In Pro-
ceedings of CICLing.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 1(2).
T. Wilson and J. Wiebe. 2003. Annotating opinions in the
world press. 4th SIGdial Workshop on Discourse and Di-
alogue (SIGdial-03).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are
you? Finding strong and weak opinion clauses. In Pro-
ceedings of AAAI.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proceed-
ings of EMNLP.
14
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 336?344,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Partially Supervised Coreference Resolution for Opinion Summarization
through Structured Rule Learning
Veselin Stoyanov and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY 14850, USA
{ves,cardie}@cs.cornell.edu
Abstract
Combining fine-grained opinion informa-
tion to produce opinion summaries is im-
portant for sentiment analysis applica-
tions. Toward that end, we tackle the
problem of source coreference resolution
? linking together source mentions that re-
fer to the same entity. The partially super-
vised nature of the problem leads us to de-
fine and approach it as the novel problem
of partially supervised clustering. We pro-
pose and evaluate a new algorithm for the
task of source coreference resolution that
outperforms competitive baselines.
1 Introduction
Sentiment analysis is concerned with extracting
attitudes, opinions, evaluations, and sentiment
from text. Work in this area has been motivated
by the desire to provide information analysis ap-
plications in the arenas of government, business,
and politics (e.g. Coglianese (2004)). Addition-
ally, sentiment analysis can augment existing NLP
applications such as question answering, informa-
tion retrieval, summarization, and clustering by
providing information about sentiment (e.g. Stoy-
anov et al (2005), Riloff et al (2005)). To date,
research in the area (see Related Work section)
has focused on the problem of extracting senti-
ment both at the document level (coarse-grained
sentiment information), and at the level of sen-
tences, clauses, or individual expressions (fine-
grained sentiment information).
In contrast, our work concerns the summa-
rization of fine-grained information about opin-
ions. In particular, while recent research ef-
forts have shown that fine-grained opinions (e.g.
Riloff and Wiebe (2003), Bethard et al (2004),
Wiebe and Riloff (2005)) as well as their sources
(e.g. Bethard et al (2004), Choi et al (2005),
Kim and Hovy (2005)) can be extracted auto-
matically, little has been done to create opin-
ion summaries, where opinions from the same
source/target are combined, statistics are com-
puted for each source/target and multiple opinions
from the same source to the same target are ag-
gregated. A simple opinion summary is shown in
figure 1.1 We expect that this type of opinion sum-
mary, based on fine-grained opinion information,
will be important for information analysis applica-
tions in any domain where the analysis of opinions
is critical.
This paper addresses the problem of opinion
summarization by considering the creation of sim-
ple opinion summaries like those of figure 1. We
propose source coreference resolution ? the task
of determining which mentions of opinion sources
refer to the same entity ? as the primary mecha-
nism for identifying the set of opinions attributed
to each real-world source. For this type of sum-
mary, source coreference resolution constitutes an
integral step in the process of generating full opin-
ion summaries. For example, given the opinion
expressions of figure 1, their polarity, and the asso-
ciated opinion sources and targets, the bulk of the
resulting summary can be produced by recogniz-
ing that source mentions ?Zacarias Moussaoui?,
?he?, ?my?, and ?Mr. Moussaoui? all refer to
the same person; and that source mentions ?Mr.
Zerkin? and ?Zerkin? refer to the same person.2
1For simplicity, the example summary does not contain
any source/target statistics.
2In addition, the summary would require the closely re-
lated task of target coreference resolution and a means for ag-
gregating the conflicting opinions from Zerkin toward Mous-
saoui.
336
At first glance, source coreference resolution
appears equivalent to the task of noun phrase
coreference resolution and therefore amenable to
traditional coreference resolution techniques (e.g.
Ng and Cardie (2002), Morton (2000)). We hy-
pothesize in Section 3, however, that the task is
likely to succumb to a better solution by treating
it in the context of a new machine learning set-
ting that we refer to as partially supervised clus-
tering. In particular, due to high coreference an-
notation costs, data sets that are annotated with
opinion information (like ours) do not typically in-
clude supervisory coreference information for all
noun phrases in a document (as would be required
for the application of traditional coreference reso-
lution techniques), but only for noun phrases that
act as opinion sources (or targets).
As a result, we define the task of partially su-
pervised clustering, the goal of which is to learn
a clustering function from a set of partially spec-
ified clustering examples (Section 4). We are not
aware of prior work on the problem of partially
supervised clustering and argue that it differs sub-
stantially from that of semi-supervised clustering.
We propose an algorithm for partially supervised
clustering that extends a rule learner with structure
information and is generally applicable to prob-
lems that fit the partially supervised clustering def-
inition (Section 5). We apply the algorithm to
the source coreference resolution task and evalu-
ate its performance on a standard sentiment analy-
sis data set that includes source coreference chains
(Section 6). We find that our algorithm outper-
forms highly competitive baselines by a consid-
erable margin ? B3 score of 83.2 vs. 81.8 and
67.1 vs. 60.9 F1 score for the identification of
positive source coreference links.
2 Related Work
Work relevant to our problem can be split into
three main areas ? sentiment analysis, traditional
noun phrase coreference resolution, and super-
vised and weakly supervised clustering. Related
work in the former two areas is summarized briefly
below. Supervised and weakly supervised cluster-
ing approaches are discussed in Section 4.
Sentiment analysis. Much of the relevant re-
search in sentiment analysis addresses sentiment
classification, a text categorization task of extract-
ing opinion at the coarse-grained document level.
The goal in sentiment classification is to assign to
[Source Zacarias Moussaoui] [? complained] at length today
about [Target his own lawyer], telling a federal court jury that
[Target he] was [? more interested in achieving fame than sav-
ing Moussaoui?s life].
Mr. Moussaoui said he was appearing on the witness stand to
tell the truth. And one part of the truth, [Source he] said, is that
[Target sending him to prison for life] would be ?[? a greater
punishment] than being sentenced to death.?
?[? [Target You] have put your interest ahead of [Source my]
life],? [Source Mr. Moussaoui] told his court-appointed lawyer
Gerald T. Zerkin.
...
But, [Source Mr. Zerkin] pressed [Target Mr. Moussaoui], was
it [? not true] that he told his lawyers earlier not to involve
any Muslims in the defense, not to present any evidence that
might persuade the jurors to spare his life?
...
[Source Zerkin] seemed to be trying to show the jurors
that while [Target the defendant] is generally [+ an honest
individual], his conduct shows [Target he] is [? not stable
mentally], and thus [? undeserving] of [Target the ultimate
punishment].
Moussaoui
Zerkin
prison for life
ultimate punishment
?
?
?
?/ +
Figure 1: Example text containing opinions
(above) and a summary of the opinions (be-
low). Sources and targets of opinions are brack-
eted; opinion expressions are shown in italics and
bracketed with associated polarity, either positive
(+) or negative (-). The underlined phrase will be
explained later in the paper.
a document either positive (?thumbs up?) or nega-
tive (?thumbs down?) polarity (e.g. Das and Chen
(2001), Pang et al (2002), Turney (2002), Dave
et al (2003)). Other research has concentrated
on analyzing fine-grained opinions at, or below,
the sentence level. Recent work, for example, in-
dicates that systems can be trained to recognize
opinions and their polarity, strength, and sources
to a reasonable degree of accuracy (e.g. Dave et
al. (2003), Riloff and Wiebe (2003), Bethard et
al. (2004), Wilson et al (2004), Yu and Hatzivas-
siloglou (2003), Choi et al (2005), Kim and Hovy
(2005), Wiebe and Riloff (2005)). Our work ex-
tends research on fine-grained opinion extraction
by augmenting the opinions with additional infor-
mation that allows the creation of concise opinion
summaries. In contrast to the opinion extracts pro-
duced by Pang and Lee (2004), our summaries are
not text extracts, but rather explicitly identify and
337
characterize the relations between opinions and
their sources.
Coreference resolution. Coreference resolution
is a relatively well studied NLP problem (e.g.
Morton (2000), Ng and Cardie (2002), Iida et al
(2003), McCallum and Wellner (2003)). Corefer-
ence resolution is defined as the problem of decid-
ing which noun phrases in the text (mentions) re-
fer to the same real world entities (are coreferent).
Generally, successful approaches to coreference
resolution have relied on supervised classification
followed by clustering. For supervised classifica-
tion these approaches learn a pairwise function to
predict whether a pair of noun phrases is corefer-
ent. Subsequently, when making coreference res-
olution decisions on unseen documents, the learnt
pairwise NP coreference classifier is run, followed
by a clustering step to produce the final clusters
(coreference chains) of coreferent NPs. For both
training and testing, coreference resolution algo-
rithms rely on feature vectors for pairs of noun
phrases that encode linguistic information about
the NPs and their local context. Our general ap-
proach to source coreference resolution is inspired
by the state-of-the-art performance of one such ap-
proach to coreference resolution, which relies on a
rule learner and single-link clustering as described
in Ng and Cardie (2002).
3 Source Coreference Resolution
In this section we introduce the problem of source
coreference resolution in the context of opinion
summarization and argue for the need for novel
methods for the task.
The task of source coreference resolution is to
decide which mentions of opinion sources refer to
the same entity. Much like traditional coreference
resolution, we employ a learning approach; how-
ever, our approach differs from traditional coref-
erence resolution in its definition of the learn-
ing task. Motivated by the desire to utilize unla-
beled examples (discussed later), we define train-
ing as an integrated task in which pairwise NP
coreference decisions are learned together with
the clustering function as opposed to treating each
NP pair as a training example. Thus, our train-
ing phase takes as input a set of documents with
manually annotated opinion sources together with
coreference annotations for the sources; it outputs
a classifier that can produce source coreference
chains for previously unseen documents contain-
ing marked (manually or automatically) opinion
sources. More specifically, the source coreference
resolution training phase proceeds through the fol-
lowing steps:
1. Source-to-NP mapping: We preprocess
each document by running a tokenizer, sen-
tence splitter, POS tagger, parser, and an NP
finder. Subsequently, we augment the set of
NPs found by the NP finder with the help of
a system for named entity detection. We then
map the sources to the NPs. Since there is
no one-to-one correspondence, we use a set
of heuristics to create the mapping. More de-
tails about why heuristics are needed and the
process used to map sources to NPs can be
found in Stoyanov and Cardie (2006).
2. Feature vector creation: We extract a fea-
ture vector for every pair of NPs from the pre-
processed corpus. We use the features intro-
duced by Ng and Cardie (2002) for the task
of coreference resolution.
3. Classifier construction: Using the feature
vectors from step 2, we construct a training
set containing one training example per doc-
ument. Each training example consists of the
feature vectors for all pairs of NPs in the doc-
ument, including those that do not map to
sources, together with the available corefer-
ence information for the source noun phrases
(i.e. the noun phrases to which sources are
mapped). The training instances are pro-
vided as input to a learning algorithm (see
Section 5), which constructs a classifier that
can take the instances associated with a new
(previously unseen) document and produce a
clustering over all NPs in the document.
The testing phase employs steps 1 and 2 as de-
scribed above, but replaces step 3 by a straightfor-
ward application of the learnt classifier. Since we
are interested in coreference information only for
the source NPs, we simply discard the non-source
NPs from the resulting clustering.
The approach to source coreference resolution
described here would be identical to traditional
coreference resolution when provided with train-
ing examples containing coreference information
for all NPs. However, opinion corpora in general,
and our corpus in particular, contain no corefer-
ence information about general NPs. Neverthe-
less, after manual sources are mapped to NPs in
338
step 1 above, our approach can rely on the avail-
able coreference information for the source NPs.
Due to the high cost of coreference annotation, we
desire methods that can work in the presence of
only this limited amount of coreference informa-
tion.
A possible workaround the absence of full NP
coreference information is to train a traditional
coreference system only on the labeled part of the
data (indeed that is one of the baselines against
which we compare). However, we believe that
an effective approach to source coreference res-
olution has to utilize the unlabeled noun phrases
because links between sources might be realized
through non-source mentions. This problem is il-
lustrated in figure 1. The underlined Moussaoui
is coreferent with all of the Moussaoui references
marked as sources, but, because it is used in an
objective sentence rather than as the source of
an opinion, the reference would be omitted from
the Moussaoui source chain. Unfortunately, this
proper noun phrase might be critical in establish-
ing the coreference of the final source reference he
with the other mentions of the source Moussaoui.
As mentioned previously, in order to utilize
the unlabeled data, our approach differs from tra-
ditional coreference resolution, which uses NP
pairs as training instances. We instead follow the
framework of supervised clustering (Finley and
Joachims, 2005; Li and Roth, 2005) and consider
each document as a training example. As in super-
vised clustering, this framework has the additional
advantage that the learning algorithm can consider
the clustering algorithm when making decisions
about pairwise classification, which could lead to
improvements in the classifier. In the next section
we describe our approach to classifier construction
for step 3 and compare our problem to traditional
weakly supervised clustering, characterizing it as
an instance of the novel problem of partially su-
pervised clustering.
4 Partially Supervised Clustering
In our desire to perform effective source corefer-
ence resolution we arrive at the following learning
problem ? the learning algorithm is presented with
a set of partially specified examples of clusterings
and acquires a function that can cluster accurately
an unseen set of items, while taking advantage of
the unlabeled information in the examples.
This setting is to be contrasted with semi-
supervised clustering (or clustering with con-
straints), which has received much research at-
tention (e.g. Demiriz et al (1999), Wagstaff and
Cardie (2000), Basu (2005), Davidson and Ravi
(2005)). Semi-supervised clustering can be de-
fined as the problem of clustering a set of items
in the presence of limited supervisory informa-
tion such as pairwise constraints (e.g. two items
must/cannot be in the same cluster) or labeled
points. In contrast to our setting, in the semi-
supervised case there is no training phase ? the
algorithm receives all examples (labeled and un-
labeled) at the same time together with some dis-
tance or cost function and attempts to find a clus-
tering that optimizes a given measure (usually
based on the distance or cost function).
Source coreference resolution might alterna-
tively be approached as a supervised clustering
problem. Traditionally, approaches to supervised
clustering have treated the pairwise link decisions
as a classification problem. These approaches first
learn a distance metric that optimizes the pairwise
decisions; and then follow the pairwise classifica-
tion with a clustering step. However, these tradi-
tional approaches have no obvious way of utilizing
the available unlabeled information.
In contrast, we follow recent approaches to su-
pervised clustering that propose ways to learn
the distance measure in the context of the clus-
tering decisions (Li and Roth, 2005; Finley and
Joachims, 2005; McCallum and Wellner, 2003).
This provides two advantages for the problem of
source coreference resolution. First, it allows the
algorithm to take advantage of the complexity of
the rich structural dependencies introduced by the
clustering problem. Viewed traditionally as a hur-
dle, the structural complexity of clustering may be
beneficial in the partially supervised case. We be-
lieve that provided with a few partially specified
clustering examples, an algorithm might be able
to generalize from the structural dependencies to
infer correctly the whole clustering of the items.
In addition, considering pairwise decisions in the
context of the clustering can arguably lead to more
accurate classifiers.
Unfortunately, none of the supervised cluster-
ing approaches is readily applicable to the partially
supervised case. However, by adapting the for-
mal supervised clustering definition, which we do
next, we can develop approaches to partially su-
pervised clustering that take advantage of the un-
339
labeled portions of the data.
Formal definition. For partially supervised
clustering we extend the formal definition of su-
pervised clustering given by Finley and Joachims
(2005). In the fully supervised setting, an al-
gorithm is given a set S of n training examples
(x1, y1), ..., (xn, yn) ? X ? Y , where X is the
set of all possible sets of items and Y is the set of
all possible clusterings of these sets. For a train-
ing example (x, y), x = {x1, x2, ..., xk} is a set
of k items and y = {y1, y2, ..., yr} is a clustering
of the items in x with each yi ? x. Addition-
ally, each item can be in no more than one cluster
(?i, j.yi ? yj = ?) and in the fully supervised case
each item is in at least one cluster (x = ? yi).
The goal of the learning algorithm is to acquire a
function h : X ? Y that can accurately cluster a
(previously unseen) set of items.
In the context of source coreference resolution
the training set contains one example for each doc-
ument. The items in each training example are the
NPs and the clustering over the items is the equiv-
alence relation defined by the coreference infor-
mation. For source coreference resolution, how-
ever, clustering information is unavailable for the
non-source NPs. Thus, to be able to deal with this
unlabeled component of the data we arrive to the
setting of partially supervised clustering, in which
we relax the condition that each item is in at least
one cluster (x = ? yi) and replace it with the con-
dition x ?
?
yi. The items with no linking infor-
mation (items in x \? yi) constitute the unlabeled
(unsupervised) component of the partially super-
vised clustering.
5 Structured Rule Learner
We develop a novel method for partially super-
vised clustering, which is motivated by the success
of a rule learner (RIPPER) for coreference resolu-
tion (Ng and Cardie, 2002). We extend RIPPER
so that it can learn rules in the context of single-
link clustering, which both suits our task (i.e. pro-
nouns link to their single antecedent) and has ex-
hibited good performance for coreference resolu-
tion (Ng and Cardie, 2002). We begin with a brief
overview of RIPPER followed by a description of
the modifications that we implemented. For ease
of presentation, we assume that we are in the fully
supervised case. We end this section by describing
the changes for the partially supervised case.
procedure StRip(TrainData){
GrowData, PruneData = Split(TrainData);
//Keep instances from the same document together
while(there are positive uncovered instances) {
r = growRule(GrowData);
r = pruneRule(r, PruneData);
DL = relativeDL(Ruleset);
if(DL ? minDL + d bits)
Ruleset.add(r);
Mark examples covered by r as +;
else
exit loop with Ruleset
}
}
procedure growRule(growData){
r = empty rule;
for(every unused feature f){
if (f is nominal feature) {
for(every possible value v of f) {
mark all instances that have values of v for f with +;
compute the transitive closure of the positive instances
//(including instances marked + from previous rules);
compute the infoGain for the future/value combination;
}
} else{ //Numeric feature
create one bag for each feature value and split the instances into bags;
do a forward and a backward pass over the bags keeping a running
clustering and compute the information gain for each value;
}
}
add the future/value pair with the best infoGain to r;
growData = growData - all negative instances;
return r;
}
procedure pruneRule(r, pruneData){
for(all antecedents a in the rule){
apply all antecedents in r up to a to pruneData;
compute the transitive closure of the positive instances;
compute A(a) ? the accuracy of the rule up to antecedent a;
}
Remove all antecedents after the antecedent for which A(a) is maximum.
}
Figure 2: The StRip algorithm. Additions to RIP-
PER are shown in bold.
5.1 The RIPPER Algorithm
RIPPER (for Repeated Incremental Pruning to
Produce Error Reduction) was introduced by Co-
hen (1995) as an extension of an existing rule
induction algorithm. Cohen (1995) showed that
RIPPER produces error rates competitive with
C4.5, while exhibiting better running times. RIP-
PER consists of two phases ? a ruleset is grown
and then optimized.
The ruleset creation phase begins by ran-
domly splitting the training data into a rule-
growing set (2/3 of the training data) and a pruning
set (the remaining 1/3). A rule is then grown on
the former set by repeatedly adding the antecedent
(the feature value test) with the largest information
gain until the accuracy of the rule becomes 1.0 or
there are no remaining potential antecedents. Next
the rule is applied to the pruning data and any rule-
final sequence that reduces the accuracy of the rule
is removed.
The optimization phase uses the full training
340
set to first grow a replacement rule and a revised
rule for each rule in the ruleset. For each rule,
the algorithm then considers the original rule, the
replacement rule, and the revised rule, and keeps
the rule with the smallest description length in the
context of the ruleset. After all rules are con-
sidered, RIPPER attempts to grow residual rules
that cover data not already covered by the rule-
set. Finally, RIPPER deletes any rules from the
ruleset that reduce the overall minimum descrip-
tion length of the data plus the ruleset. RIPPER
performs two rounds of this optimization phase.
5.2 The StRip Algorithm
The property of partially supervised clustering that
we want to explore is the structured nature of the
decisions. That is, each decision of whether two
items (say a and b) belong to the same cluster has
an implication for all items a? that belong to a?s
cluster and all items b? that belong to b?s cluster.
We target modifications to RIPPER that will al-
low StRip (for Structured RIPPER) to learn rules
that produce good clusterings in the context of
single-link clustering. We extend RIPPER so that
every time it makes a decision about a rule, it con-
siders the effect of the rule on the overall clus-
tering of items (as opposed to considering the in-
stances that the rule classifies as positive/negative
in isolation). More precisely, we precede every
computation of rule performance (e.g. informa-
tion gain or description length) by a transitive clo-
sure (i.e. single link clustering) of the data w.r.t. to
the pairwise classifications. Following the transi-
tive closure, all pairs of items that are in the same
cluster are considered covered by the rule for per-
formance computation.
The StRip algorithm is given in figure 2, with
modifications to the original RIPPER algorithm
shown in bold. Due to space limitations the op-
timization stage of the algorithm is omitted. Our
modifications to the optimization stage of RIPPER
are in the spirit of the rest of the StRip algorithm.
Partially supervised case. So far we described
StRip only for the fully supervised case. We
use a very simple modification to handle the par-
tially supervised setting: we exclude the unla-
beled pairs when computing the performance of
the rules. Thus, the unlabeled items do not count
as correct or incorrect classifications when acquir-
ing or pruning a rule, although they do participate
in the transitive closure. Links in the unlabeled
data are inferred entirely through the indirect links
between items in the labeled component that they
introduce. In the example of figure 1, the two
problematic unlabeled links are the link between
the source mention ?he? and the underlined non-
source NP ?Mr. Moussaoui? and the link between
the underlined ?Mr. Moussaoui? to any source
mention of Moussaoui. While StRip will not re-
ward any rule (or rule set) that covers these two
links directly, such rules will be rewarded indi-
rectly since they put the source he in the chain for
the source Moussaoui.
StRip running time. StRip?s running time is
generally comparable to that of RIPPER. We com-
pute transitive closure by using a Union-Find
structure, which runs in time O(log?n), which for
practical purposes can be considered linear (O(n))
3
. However, when computing the best information
gain for a nominal feature, StRip has to make a
pass over the data for each value that the feature
takes, while RIPPER can split the data into bags
and perform the computation in one pass.
6 Evaluation and Results
This section describes the source coreference data
set, the baselines, our implementation of StRip,
and the results of our experiments.
6.1 Data set
For evaluation we use the MPQA corpus (Wiebe
et al, 2005).4 The corpus consists of 535 doc-
uments from the world press. All documents in
the collection are manually annotated with phrase-
level opinion information following the annota-
tion scheme of Wiebe et al (2005). Discussion
of the annotation scheme is beyond the scope of
this paper; for our purposes it suffices to say that
the annotations include the source of each opin-
ion and coreference information for the sources
(e.g. source coreference chains). The corpus con-
tains no additional noun phrase coreference infor-
mation.
For our experiments, we randomly split the data
set into a training set consisting of 400 documents
and a test set consisting of the remaining 135 doc-
uments. We use the same test set for all experi-
3For the transitive closure, n is the number of items in a
document, which is O(
?
k), where k is the number of NP
pairs. Thus, transitive closure is sublinear in the number of
training instances.
4The MPQA corpus is available at
http://nrrc.mitre.org/NRRC/publications.htm.
341
ments, although some learning runs were trained
on 200 training documents (see next Subsection).
The test set contains a total of 4736 source NPs
(average of 35.34 source NPs per document) split
into 1710 total source NP chains (average of 12.76
chains per document) for an average of 2.77 source
NPs per chain.
6.2 Implementation
We implemented the StRip algorithm by modify-
ing JRip ? the java implementation of RIPPER in-
cluded in the WEKA toolkit (Witten and Frank,
2000). The WEKA implementation follows the
original RIPPER specification. We changed the
implementation to incorporate the modifications
suggested by the StRip algorithm; we also mod-
ified the underlying data representations and data
handling techniques for efficiency. Also due to ef-
ficiency considerations, we train StRip only on the
200-document training set.
6.3 Competitive baselines
We compare the results of the new method to three
fully supervised baseline systems, each of which
employs the same traditional coreference resolu-
tion approach. In particular, we use the afore-
mentioned algorithm proposed by Ng and Cardie
(2002), which combines a pairwise NP corefer-
ence classifier with single-link clustering.
For one baseline, we train the coreference reso-
lution algorithm on the MPQA src corpus ? the
labeled portion of the MPQA corpus (i.e. NPs
from the source coreference chains) with unla-
beled instances removed.
The second and third baselines investigate
whether the source coreference resolution task can
benefit from NP coreference resolution training
data from a different domain. Thus, we train the
traditional coreference resolution algorithm on the
MUC6 and MUC7 coreference-annotated corpora5
that contain documents similar in style to those in
the MPQA corpus (e.g. newspaper articles), but
emanate from different domains.
For all baselines we targeted the best possi-
ble systems by trying two pairwise NP classifiers
(RIPPER and an SVM in the SV M light imple-
mentation (Joachims, 1998)), many different pa-
rameter settings for the classifiers, two different
feature sets, two different training set sizes (the
5We train each baseline using both the development set
and the test set from the corresponding MUC corpus.
full training set and a smaller training set consist-
ing of half of the documents selected at random),
and three different instance selection algorithms6.
This variety of classifier and training data settings
was motivated by reported differences in perfor-
mance of coreference resolution approaches w.r.t.
these variations (Ng and Cardie, 2002). More de-
tails on the different parameter settings and in-
stance selection algorithms as well as trends in the
performance of different settings can be found in
Stoyanov and Cardie (2006). In the experiments
below we report the best performance of each of
the two learning algorithms on the MPQA test
data.
6.4 Evaluation
In addition to the baselines described above, we
evaluate StRip both with and without unlabeled
data. That is, we train on the MPQA corpus StRip
using either all NPs or just opinion source NPs.
We use the B3 (Bagga and Baldwin, 1998) eval-
uation measure as well as precision, recall, and
F1 measured on the (positive) pairwise decisions.
B3 is a measure widely used for evaluating coref-
erence resolution algorithms. The measure com-
putes the precision and recall for each NP mention
in a document, and then averages them to produce
combined results for the entire output. More pre-
cisely, given a mention i that has been assigned
to chain ci, the precision for mention i is defined
as the number of correctly identified mentions in
ci divided by the total number of mentions in ci.
Recall for i is defined as the number of correctly
identified mentions in ci divided by the number of
mentions in the gold standard chain for i.
Results are shown in Table 1. The first six
rows of results correspond to the fully supervised
baseline systems trained on different corpora ?
MUC6, MUC7, and MPQA src. The seventh row
of results shows the performance of StRip using
only labeled data. The final row of the table shows
the results for partially supervised learning with
unlabeled data. The table lists results from the best
performing run for each algorithm.
Performance among the baselines trained on the
MUC data is comparable. However, the two base-
line runs trained on the MPQA src corpus (i.e. re-
sults rows five and six) show slightly better perfor-
mance on the B3 metric than the baselines trained
6The goal of the instance selection algorithms is to bal-
ance the data, which contains many more negative than posi-
tive instances
342
ML Framework Training set Classifier B3 precision recall F1
Fully supervised MUC6 SVM 81.2 72.6 52.5 60.9
RIPPER 80.7 57.4 63.5 60.3
MUC7 SVM 81.7 65.6 55.9 60.4
RIPPER 79.7 71.6 48.5 57.9
MPQA src SVM 81.8 57.5 62.9 60.2
RIPPER 81.8 72.0 52.5 60.6
StRip 82.3 76.5 56.1 64.6
Partially supervised MPQA all StRip 83.2 77.1 59.4 67.1
Table 1: Results for Source Coreference. MPQA src stands for the MPQA corpus limited to only source
NPs, while MPQA full contains the unlabeled NPs.
on the MUC data, which indicates that for our
task the similarity of the documents in the train-
ing and test sets appears to be more important
than the presence of complete supervisory infor-
mation. (Improvements over the RIPPER runs
trained on the MUC corpora are statistically sig-
nificant7, while improvements over the SVM runs
are not.)
Table 1 also shows that StRip outperforms the
baselines on both performance metrics. StRip?s
performance is better than the baselines when
trained on MPQA src (improvement not statisti-
cally significant, p > 0.20) and even better when
trained on the full MPQA corpus, which includes
the unlabeled NPs (improvement over the base-
lines and the former StRip run statistically signif-
icant). These results confirm our hypothesis that
StRip improves due to two factors: first, consider-
ing pairwise decisions in the context of the clus-
tering function leads to improvements in the clas-
sifier; and, second, StRip can take advantage of
the unlabeled portion of the data.
StRip?s performance is all the more impressive
considering the strength of the SVM and RIPPER
baselines, which which represent the best runs
across the 336 different parameter settings tested
for SV M light and 144 different settings tested for
RIPPER. In contrast, all four of the StRip runs us-
ing the full MPQA corpus (we vary the loss ratio
for false positive/false negative cost) outperform
those baselines.
7 Future Work
Source coreference resolution is only one aspect
of opinion summarization. Additionally, an opin-
ion summarization system will need to handle
7Statistical significance is measured using both a 2-tailed
paired t-test and the Wilcoxon matched-pairs signed-ranks
test (p < 0.05). The two tests agreed on all significance
judgements, so we will not report them separately.
the closely related task of target coreference res-
olution in order to cluster targets of opinions8
and combine multiple conflicting opinions from a
source to the same targets. Furthermore, a fully
automatic opinion summarizer requires automatic
source and opinion extractors. While we antici-
pate that target coreference resolution will be sub-
ject to error rates similar to those of source coref-
erence resolution, incorporating these imperfect
opinions and sources will further impair the per-
formance of the opinion summarizer. We are not
aware of any measure that can be directly used
to assess the goodness of opinion summaries, but
plan to develop such in future work in conjunc-
tion with the development of methods for creating
opinion summaries completely automatically. The
evaluation metrics will likely have to depend on
the task for which the summaries are used.
A limitation of our approach to partially super-
vised clustering is that we do not directly optimize
for the performance measure (e.g. B3). Other ef-
forts in the area of supervised clustering (Finley
and Joachims, 2005; Li and Roth, 2005) have sug-
gested ways to learn distance measures that can
optimize directly for a desired performance mea-
sure. We plan to investigate algorithms that can di-
rectly optimize for complex measures (such as B3)
for the problem of partially supervised clustering.
Unfortunately, a measure as complex as B3 makes
extending existing approaches far from trivial due
to the difficulty of establishing the connection be-
tween individual pairwise decisions (the distance
metric) and the score of the clustering algorithm.
Acknowledgements
The authors would like to thank Vincent Ng and
Art Munson for providing coreference resolution
8We did not tackle the task of target coreference resolu-
tion in this paper because the MPQA corpus did not contain
target annotations at the time of publication.
343
code, members of the Cornell NLP group (es-
pecially Yejin Choi and Art Munson) for many
helpful discussions, and the anonymous review-
ers for their insightful comments. This work was
supported by the Advanced Research and Devel-
opment Activity (ARDA), by NSF Grants IIS-
0535099 and IIS-0208028, by gifts from Google
and the Xerox Foundation, and by an NSF Gradu-
ate Research Fellowship to the first author.
References
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model. In
In Proceedings of COLING/ACL.
S. Basu. 2005. Semi-supervised Clustering: Probabilistic
Models, Algorithms and Experiments. Ph.D. thesis, De-
partment of Computer Sciences, UT at Austin.
S. Bethard, H. Yu, A. Thornton, V. Hativassiloglou, and
D. Jurafsky. 2004. Automatic extraction of opinion
propositions and their holders. In 2004 AAAI Spring Sym-
posium on Exploring Attitude and Affect in Text.
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005. Iden-
tifying sources of opinions with conditional random fields
and extraction patterns. In Proceedings of EMNLP.
C. Coglianese. 2004. E-rulemaking: Information technology
and regulatory policy: New directions in digital govern-
ment research. Technical report, Harvard University, J. F.
Kennedy School of Government.
W. Cohen. 1995. Fast effective rule induction. In Proceed-
ings of ICML.
S. Das and M. Chen. 2001. Yahoo for amazon: Extracting
market sentiment from stock message boards. In Proceed-
ings of APFAAC.
K. Dave, S. Lawrence, and D. Pennock. 2003. Mining the
peanut gallery: Opinion extraction and semantic classifi-
cation of product reviews. In Proceedings of IWWWC.
I. Davidson and S. Ravi. 2005. Clustering with constraints:
Feasibility issues and the k-means algorithm. In Proceed-
ings of SDM.
A. Demiriz, K. P. Bennett, and M. J. Embrechts. 1999. Semi-
supervised clustering using genetic algorithms. In Pro-
ceeding of ANNIE.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of ICML.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003. In-
corporating contextual cues in trainable models for coref-
erence resolution. In Proceedings of the EACL Workshop
on The Computational Treatment of Anaphora.
T. Joachims. 1998. Making large-scale support vector
machine learning practical. In A. Smola B. Scho?lkopf,
C. Burges, editor, Advances in Kernel Methods: Support
Vector Machines. MIT Press, Cambridge, MA.
S. Kim and E. Hovy. 2005. Identifying opinion holders for
question answering in opinion texts. In Proceedings of
AAAI Workshop on Question Answering in Restricted Do-
mains.
X. Li and D. Roth. 2005. Discriminative training of cluster-
ing functions: Theory and experiments with entity identi-
fication. In Proceedings of CoNLL.
A. McCallum and B. Wellner. 2003. Toward conditional
models of identity uncertainty with application to proper
noun coreference. In Proceedings of the IJCAI Workshop
on Information Integration on the Web.
T. Morton. 2000. Coreference for NLP applications. In Pro-
ceedings of ACL.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In In Proceedings
of ACL.
B. Pang and L. Lee. 2004. A sentimental education: Senti-
ment analysis using subjectivity summarization based on
minimum cuts. In Proceedings of ACL.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning tech-
niques. In Proceedings of EMNLP.
E. Riloff and J. Wiebe. 2003. Learning extraction patterns
for subjective expressions. In Proceesings of EMNLP.
E. Riloff, J. Wiebe, and W. Phillips. 2005. Exploiting sub-
jectivity classification to improve information extraction.
In Proceedings of AAAI.
V. Stoyanov and C. Cardie. 2006. Toward opinion summa-
rization: Linking the sources. In Proceedings of the ACL
Workshop on Sentiment and Subjectivity in Text.
V. Stoyanov, C. Cardie, and J. Wiebe. 2005. Multi-
Perspective question answering using the OpQA corpus.
In Proceedings of EMNLP.
P. Turney. 2002. Thumbs up or thumbs down? Semantic ori-
entation applied to unsupervised classification of reviews.
In Proceedings of ACL.
K. Wagstaff and C. Cardie. 2000. Clustering with instance-
level constraints. In Proceedings of the 17-th National
Conference on Artificial Intelligence and 12-th Confer-
ence on Innovative Applications of Artificial Intelligence.
J. Wiebe and E. Riloff. 2005. Creating subjective and objec-
tive sentence classifiers from unannotated texts. In Pro-
ceedings of CICLing.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 1(2).
T. Wilson, J. Wiebe, and R. Hwa. 2004. Just how mad are
you? Finding strong and weak opinion clauses. In Pro-
ceedings of AAAI.
I.H. Witten and E. Frank. 2000. Data Mining: Practical ma-
chine learning tools with Java implementations. Morgan
Kaufmann, San Francisco.
H. Yu and V. Hatzivassiloglou. 2003. Towards answer-
ing opinion questions: Separating facts from opinions and
identifying the polarity of opinion sentences. In Proceed-
ings of EMNLP.
344
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 120?130,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Minimum-Risk Training of Approximate CRF-Based NLP Systems
Veselin Stoyanov and Jason Eisner
HLTCOE and Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{ves, jason}@cs.jhu.edu
Abstract
Conditional Random Fields (CRFs) are a pop-
ular formalism for structured prediction in
NLP. It is well known how to train CRFs with
certain topologies that admit exact inference,
such as linear-chain CRFs. Some NLP phe-
nomena, however, suggest CRFs with more
complex topologies. Should such models be
used, considering that they make exact infer-
ence intractable? Stoyanov et al (2011) re-
cently argued for training parameters to min-
imize the task-specific loss of whatever ap-
proximate inference and decoding methods
will be used at test time. We apply their
method to three NLP problems, showing that
(i) using more complex CRFs leads to im-
proved performance, and that (ii) minimum-
risk training learns more accurate models.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are often used to model dependencies among
linguistic variables. CRF-based models have im-
proved the state of the art in a number of natural
language processing (NLP) tasks ranging from part-
of-speech tagging to information extraction and sen-
timent analysis (Lafferty et al, 2001; Peng and Mc-
Callum, 2006; Choi et al, 2005).
Robust and theoretically sound training proce-
dures have been developed for CRFs when the
model can be used with exact inference and de-
coding.1 However, some NLP problems seem to
1?Inference? typically refers to computing posterior
marginal or max-marginal probability distributions of output
random variables, given some evidence. ?Decoding? derives
a single structured output from the results of inference.
call for higher-treewidth graphical models in which
exact inference is expensive or intractable. These
?loopy? CRFs have cyclic connections among the
output and/or latent variables. Alas, standard learn-
ing procedures assume exact inference: they do not
compensate for approximations that will be used at
test time, and can go surprisingly awry if approxi-
mate inference is used at training time (Kulesza and
Pereira, 2008).
While NLP research has been consistently evolv-
ing toward more richly structured models, one may
hesitate to add dependencies to a graphical model if
there is a danger that this will end up hurting per-
formance through approximations. In this paper we
illustrate how to address this problem, even for ex-
tremely interconnected models in which every pair
of output variables is connected.
Wainwright (2006) showed that if approximate in-
ference will be used at test time, it may be beneficial
to use a learning procedure that does not converge to
the true model but to one that performs well under
the approximations. Stoyanov et al (2011) argue for
minimizing a certain non-convex training objective,
namely the empirical risk of the entire system com-
prising the CRF together with whatever approximate
inference and decoding procedures will be used at
test time. They regard this entire system as sim-
ply a complex decision rule, analogous to a neu-
ral network, and show how to use back-propagation
to tune its parameters to locally minimize the em-
pirical risk (i.e., the average task-specific loss on
training data). Stoyanov et al (2011) show that
on certain synthetic-data problems, this frequentist
training regimen significantly reduced test-data loss
120
compared to approximate maximum likelihood esti-
mation (MLE). However, this method has not been
evaluated on real-world problems until now.
We will refer to the Stoyanov et al (2011) ap-
proach as ?ERMA??Empirical Risk Minimization
under Approximations. ERMA is attractive for NLP
because the freedom to use arbitrarily structured
graphical models makes it possible to include latent
linguistic variables, predict complex structures such
as parses (Smith and Eisner, 2008), and do collec-
tive prediction in relational domains (Ji and Grish-
man, 2011; Benson et al, 2011; Dreyer and Eis-
ner, 2009). In training, ERMA considers not only
the approximation method but also the task-specific
loss function. This means that ERMA is careful to
use the additional variables and dependencies only
in ways that help training set performance. (Overfit-
ting on the enlarged parameter set should be avoided
through regularization.)
We have developed a simple syntax for specify-
ing CRFs with complex structures, and a software
package (available from http://www.clsp.
jhu.edu/?ves/software.html) that allows
ERMA training of these CRFs for several popular
loss functions (e.g., accuracy, mean-squared error,
F-measure). In this paper, we use these tools to re-
visit three previously studied NLP applications that
can be modeled naturally with approximate CRFs
(we will use approximate CRFs to refer to CRF-
based systems that are used with approximations in
inference or decoding). We show that (i) natural lan-
guage can be modeled more effectively with CRFs
that are not restricted to a linear structure and (ii)
that ERMA training represents an improvement over
previous learning methods.
The first application, predicting congressional
votes, has not been previously modeled with CRFs.
By using a more principled probabilistic approach,
we are able to improve the state-of-the-art accuracy
from 71.2% to 78.2% when training to maximize the
approximate log-likelihood of the training data. By
switching to ERMA training, we improve this result
further to 85.1%.
The second application, information extraction
from seminar announcements, has been modeled
previously with skip-chain CRFs (Sutton and Mc-
Callum, 2005; Finkel et al, 2005). The skip-chain
CRF introduces loops and requires approximate in-
ference, which motivates minimum risk training.
Our results show that ERMA training improves F-
measures from 89.5 to 90.9 (compared to 87.1 for
the model without skip-chains).
Finally, for our third application, we perform col-
lective multi-label text classification. We follow pre-
vious work (Ghamrawi and McCallum, 2005; Finley
and Joachims, 2008) and use a fully connected CRF
to model all pairwise dependencies between labels.
We observe similar trends for this task: switching
from a maximum entropy model that does not model
label dependencies to a loopy CRF leads to an im-
provement in F-measure from 81.6 to 84.0, and us-
ing ERMA leads to additional improvement (84.7).
2 Preliminaries
2.1 Conditional Random Fields
A conditional random field (CRF) is an undirected
graphical model defined by a tuple (X ,Y,F , f, ?).
X = (X1, X2, . . .) is a set of random variables and
Y = (Y1, Y2, . . .) is a set of output random vari-
ables.2 We use x = (x1, x2, . . .), to denote a possi-
ble assignment of values to X , and similarly for y,
with xy denoting the joint assignment. Each ? ? F
is a subset of the random variables, ? ? X ? Y ,
and we write xy? to denote the restriction of xy to
?. Finally, for each ? ? F , the CRF specifies a
function ~f? that extracts a feature vector ? Rd from
the restricted assignment xy?. We define the over-
all feature vector ~f(x,y) =
?
??F
~f?(xy?) ? R
d.
The model defines conditional probabilities
p?(y|x) =
exp ~? ? ~f(x,y)
?
y? exp
~? ? ~f(x,y?)
(1)
where ~? ? Rd is a global weight vector (to be
learned). This is a log-linear model; the denomina-
tor (traditionally denoted Zx) sums over all possible
output assignments to normalize the distribution.
Provided that all probabilities needed at training
or test time are conditioned on an observation of the
form X = x, CRFs can include arbitrary overlap-
ping features of the input without having to explic-
itly model input feature dependencies.
2Stoyanov et al (2011) distinguished some of the Y vari-
ables as latent (i.e., unsupervised and ignored by the loss func-
tion). We omit this possibility, to simplify the notation.
121
2.2 Inference in CRFs
Inference in general CRFs is intractable (Koller and
Friedman, 2009). Nevertheless, there exist several
approximate algorithms that have theoretical moti-
vation and tend to exhibit good performance in prac-
tice. Those include variational methods such as
loopy belief propagation (BP) (Murphy et al, 1999)
and mean-field, as well as Markov Chain Monte
Carlo methods.
ERMA training is applicable to any approxima-
tion that corresponds to a differentiable function,
even if the function has no simple closed form but is
computed by an iterative update algorithm. In this
paper we select BP, which is exact when the fac-
tor graph is a tree, such as a linear-chain CRF, but
whose results can be somewhat distorted by loops
in the factor graph, as in our settings. BP computes
beliefs about the marginal distribution of each ran-
dom variable using iterative updates. We standardly
approximate the posterior CRF marginals given the
input observations by running BP over a CRF that
enforces those observations.
2.3 Decoding
Conditional random fields are models of probabil-
ity. A decoder is a procedure for converting these
probabilities into system outputs. Given x, the de-
coder would ideally choose y to minimize the loss
`(y,y?), where ` compares a candidate assignment
y to the true assignment y?. But of course we do not
know the truth at test time. Instead we can average
over possible values y? of the truth:
argmin
y
?
y?
p(y? | x) ? `(y,y?) (2)
This is the minimum Bayes risk (MBR) principle
from statistical decision theory: choose y to mini-
mize the expected loss (i.e., the risk) according to
the CRF?s posterior beliefs given x.
In the NLP literature, CRFs are often decoded by
choosing y to be the maximum posterior probabil-
ity assignment (e.g., Sha and Pereira (2003), Sutton
et al (2007)). This is the MBR procedure for the
0-1 loss function that simply tests whether y = y?.
For other loss functions, however, the corresponding
MBR procedure is preferable. For some loss func-
tions it is tractable given the posterior marginals of
p, while in other cases approximations are needed.
In our experiments we use MBR decoding (or a
tractable approximation) but substitute the approx-
imate posterior marginals of p as computed by BP.
For example, if the loss of y is the number of incor-
rectly recovered output variables, MBR says to sep-
arately pick the most probable value for each output
variable, according to its (approximate) marginal.
3 Minimum-Risk CRF Training
This section briefly describes the ERMA training al-
gorithm from Stoyanov et al (2011) and compares it
to related structured learning methods. We assume
a standard ML setting, with a set of training inputs
xi and corresponding correct outputs yi?. All the
methods below are regularized in practice, but we
omit mention of regularizers for simplicity.
3.1 Related Structured Learning Methods
When inference and decoding can be performed ex-
actly, the CRF parameters ~? are often trained by
maximum likelihood estimation (MLE):
argmax
?
?
i
log p?(y
i? | xi) (3)
The gradient of each summand log p?(yi? | xi)
can be computed by performing inference in two set-
tings, one with xi,yi? observed and one with only
the conditioning events xi observed. The gradient
emerges as the difference between the feature ex-
pectations in the two cases. If exact inference is
intractable, one can compute approximate feature
expectations by loopy BP. Computing the approx-
imate gradient in this way, and training the CRF
with some gradient-based optimization method, has
been shown to work relatively well in practice (Vish-
wanathan et al, 2006; Sutton and McCallum, 2005).
The above method takes into account neither the
loss function that will be used for evaluation, nor
the approximate algorithms that have been selected
for inference and decoding at test time. Other struc-
ture learning methods do consider loss, though it is
not obvious how to make them consider approxima-
tions. Those include maximum margin (Taskar et
al., 2003; Finley and Joachims, 2008) and softmax-
margin (Gimpel and Smith, 2010). The idea of
margin-based methods is to choose weights ~? so that
the correct alternative yi? always gets a better score
122
than each possible alternative yi ? Y . The loss is
incorporated in these methods by requiring the mar-
gin (~? ? ~f(xi,yi?)? ~? ? ~f(xi,yi)) ? `(yi,yi?), with
penalized slack in these constraints. The softmax-
margin method uses a different criterion?it resem-
bles MLE but modifies the denominator of (1) to
Zx =
?
y??Y exp(
~? ? ~f(x,y?) + `(y?,y?)).
In our experiments we compare against MLE
training (which is common) and softmax-margin,
which incorporates loss and which Gimpel and
Smith (2010) show is either better or competitive
when compared to other margin methods on an NLP
task. We adapt these methods to the loopy case in
the obvious way, by replacing exact inference with
loopy BP and keeping everything else the same.
3.2 Minimum-Risk Training
We wish to consider the approximate inference and
decoding algorithms and the loss function that will
be used during testing. Thus, we want ? to minimize
the expected loss under the true data distribution P :
argmin
?
Exy?P [`(??(x),y)] (4)
where ?? is the decision rule (parameterized by ?),
which decodes the results of inference under p?.
In practice, we do not know the true data distri-
bution, but we can do empirical risk minimization
(ERM), instead averaging the loss over our sample
of (xi,yi) pairs. ERM for structured prediction was
first introduced in the speech community (Bahl et
al., 1988) and later used in NLP (Och, 2003; Kakade
et al, 2002; Suzuki et al, 2006; Li and Eisner, 2009,
etc.). Previous applications of risk minimization as-
sume exact inference, having defined the hypothe-
sis space by a precomputed n-best list, lattice, or
packed forest over which exact inference is possible.
The ERMA approach (Stoyanov et al, 2011)
works with approximate inference and computes ex-
act gradients of the output loss (or a differentiable
surrogate) in the context of the approximate infer-
ence and decoding algorithms. To determine the gra-
dient of `(??(xi),yi) with respect to ?, the method
relies on automatic differentiation in the reverse
mode (Griewank and Corliss, 1991), a general tech-
nique for sensitivity analysis in computations. The
intuition behind automatic differentiation is that the
entire computation is a sequence of elementary dif-
ferentiable operations. For each elementary opera-
tion, given that we know the input and result values,
and the partial derivative of the loss with respect to
the result, we can compute the partial derivative of
the loss with respect to the inputs to the step. Dif-
ferentiating the whole complicated computation can
be carried out in backward pass in this step-by-step
manner as long as we record intermediate results
during the computation of the function (the forward
pass). At the end, we accumulate the partials of the
loss with respect to each parameter ?i.
ERMA is similar to back-propagation used in re-
current neural networks, which involve cyclic up-
dates like those in belief propagation (Williams and
Zipser, 1989). It considers an ?unrolled? version of
the forward pass, in which ?snapshots? of a vari-
able at times t and t + 1 are treated as distinct vari-
ables, with one perhaps influencing the other. The
forward pass computes `(??(xi),yi) by performing
approximate inference, then decoding, then evalu-
ation. These steps convert (xi, ?) ? marginals ?
decision? loss. The backward pass rewinds the en-
tire computation, differentiating each phase in term.
The total time required by this algorithm is roughly
twice the time of the forward pass, so its complexity
is comparable to approximate inference.
In this paper, we do not advocate any particular
test-time inference or decoding procedures. It is rea-
sonable to experiment with several choices that may
produce faster or more accurate systems. We sim-
ply recommend doing ERMA training to match each
selected test-time condition. Stoyanov et al (2011)
specifically showed how to train a system that will
use sum-product BP for inference at test time (un-
like margin-based methods). This may be advanta-
geous for some tasks because it marginalizes over la-
tent variables. However, it is popular and sometimes
faster to do 1-best decoding, so we also include ex-
periments where the test-time system returns a 1-
best value of y (or an approximation to this if the
CRF is loopy), based on max-product BP inference.
Although 1-best systems are not differentiable func-
tions, we can approach their behavior during ERM
training by annealing the training objective (Smith
and Eisner, 2006). In the annealed case we evaluate
(4) and its gradient under sum-product BP, except
that we perform inference under p(?/T ) instead of p?.
123
We gradually reduce the temperature T ? R from 1
to 0 as training proceeds, which turns sum-product
inference into max-product by moving all the prob-
ability mass toward the highest-scoring assignment.
4 Modeling Natural Language with CRFs
This section describes three NLP problems that can
be naturally modeled with approximate CRFs. The
first problem, modeling congressional votes, has not
been previously modeled with a CRF. We show that
by switching to the principled CRF framework we
can learn models that are much more accurate when
evaluated on test data, though using the same (or less
expressive) features as previous work. The other
two problems, information extraction from semi-
structured text and collective multi-label classifica-
tion, have been modeled with loopy CRFs before.
For all three models, we show that ERMA training
results in better test set performance.3
4.1 Modeling Congressional Votes
The Congressional Vote (ConVote) corpus was cre-
ated by Thomas et al (2006) to study whether votes
of U.S. congressional representatives can be pre-
dicted from the speeches they gave when debating
a bill. The corpus consists of transcripts of con-
gressional floor debates split into speech segments.
Each speech segment is labeled with the represen-
tative who is speaking and the recorded vote of that
representative on the bill. We aim to predict a high
percentage of the recorded votes correctly.
Speakers often reference one another (e.g., ?I
thank the gentleman from Utah?), to indicate agree-
ment or disagreement. The ConVote corpus manu-
ally annotates each phrase such as ?the gentleman
from Utah? with the representative that it denotes.
Thomas et al (2006) show that classification us-
ing the agreement/disagreement information in the
local context of such references, together with the
rest of the language in the speeches, can lead to sig-
nificant improvement over using either of these two
3We also experimented with a fourth application, joint POS
tagging and shallow parsing (Sutton et al, 2007) and observed
the same overall trend (i.e., minimum risk training improved
performance significantly). We do not include those experi-
ments, however, because we were unable to make our baseline
results replicate (Sutton et al, 2007).
sources of information in isolation. The original ap-
proach of Thomas et al (2006) is based on training
two Support Vector Machine (SVM) classifiers?
one for classifying speeches as supporting/opposing
the legislation and another for classifying references
as agreement/disagreement. Both classifiers rely on
bag-of-word (unigram) features of the document and
the context surrounding the link respectively. The
scores produced by the two SVMs are used to weight
a global graph whose vertices are the representa-
tives; then the min-cut algorithm is applied to par-
tition the vertices into ?yea? and ?nay? voters.
While the approach of Thomas et al (2006)
leads to significant improvement over using the first
SVM alone, it does not admit a probabilistic in-
terpretation and the two classifiers are not trained
jointly. We also remark that the min-cut technique
would not generalize beyond binary random vari-
ables (yea/nay).
We observe that congressional votes together with
references between speakers can be naturally mod-
eled with a CRF. Figure 1 depicts the CRF con-
structed for one of the debates in the development
part of the ConVote corpus. It contains a random
variable for each representative?s vote. In addition,
each speech is an observed input random variable:
it is connected by a factor to its speaker?s vote and
encourages it to be ?yea? or ?nay? according to fea-
tures of the text of the speech. Finally, each ref-
erence in each speech is an observed input random
variable connected by a factor to two votes?those
of the speaker and the referent?which it encourages
to agree or disagree according to features of the text
surrounding the reference. Just as in (Thomas et al,
2006), the score of a global assignment to all votes is
defined by considering both kinds of factors. How-
ever, unlike min-cut, CRF inference finds a proba-
bility distribution over assignments, not just a sin-
gle best assignment. This fact allows us to train the
two kinds of factors jointly (on the set of training
debates where the votes are known) to predict the
correct votes accurately (as defined by accuracy).
As Figure 1 shows, the reference factors introduce
arbitrary loops, making exact inference intractable
and thus motivating ERMA. Our experiments de-
scribed in section 5.2 show that switching to a CRF
model (keeping the same features) leads to a sizable
improvement over the previous state of the art?
124
Figure 1: An example of a debate structure from the Con-
Vote corpus. Each black square node represents a factor
and is connected to the variables in that factor, shown
as round nodes. Unshaded variables correspond to the
representatives? votes and depict the output variables that
we learn to jointly predict. Shaded variables correspond
to the observed input data? the text of all speeches of a
representative (in dark gray) or all local contexts of refer-
ences between two representatives (in light gray).
and that ERMA further significantly improves per-
formance, particularly when it properly trains with
the same inference algorithm (max-product vs. sum-
product) to be used at test time.
Baseline. As an exact baseline, we compare
against the results of Thomas et al (2006). Their
test-time Min-Cut algorithm is exact in this case: bi-
nary variables and a two-way classification.
4.2 Information Extraction from
Semi-Structured Text
We utilize the CMU seminar announcement corpus
of Freitag (2000) consisting of emails with seminar
announcements. The task is to extract four fields that
describe each seminar: speaker, location, start time
and end time. The corpus annotates the document
with all mentions of these four fields.
Sequential CRFs have been used successfully for
semi-structured information extraction (Sutton and
McCallum, 2005; Finkel et al, 2005). However,
they cannot model non-local dependencies in the
data. For example, in the seminar announcements
corpus, if ?Sutner? is mentioned once in an email
in a context that identifies him as a speaker, it is
Sutner
S
Who:
O
Prof.
S
Klaus
S
will
O
Prof.
S
Sutner
S? ?? ?
Figure 2: Skip-chain CRF for semi-structured informa-
tion extraction.
likely that other occurrences of ?Sutner? in the same
email should be marked as speaker. Hence Finkel et
al. (2005) and Sutton and McCallum (2005) propose
adding non-local edges to a sequential CRF to repre-
sent soft consistency constraints. The model, called
a ?skip-chain CRF? and shown in Figure 2, contains
a factor linking each pair of capitalized words with
the same lexical form. The skip-chain CRF model
exhibits better empirical performance than its se-
quential counterpart (Sutton and McCallum, 2005;
Finkel et al, 2005).
The non-local skip links make exact inference
intractable. To train the full model, Finkel et al
(2005) estimate the parameters of a sequential CRF
and then manually select values for the weights of
the non-local edges. At test time, they use Gibbs
sampling to perform inference. Sutton and McCal-
lum (2005) use max-product loopy belief propaga-
tion for test-time inference, and compare a train-
ing procedure that uses a piecewise approximation
of the partition function against using sum-product
loopy belief propagation to compute output variable
marginals. They find that the two training regimens
perform similarly on the overall task. All of these
training procedures try to approximately maximize
conditional likelihood, whereas we will aim to mini-
mize the empirical loss of the approximate inference
and decoding procedures.
Baseline. As an exact (non-loopy) baseline, we
train a model without the skip chains. We give two
baseline numbers in Table 1?for training the exact
CRF with MLE and with ERM. The ERM setting re-
sulted in a statistically significant improvement even
in the exact case, thanks to the use of the loss func-
tion at training time.
4.3 Multi-Label Classification
Multi-label classification is the problem of assign-
ing multiple labels to a document. For example, a
news article can be about both ?Libya? and ?civil
125
war.? The most straightforward approach to multi-
label classification employs a binary classifier for
each class separately. However, previous work has
shown that incorporating information about label de-
pendencies can lead to improvement in performance
(Elisseeff and Weston, 2001; Ghamrawi and McCal-
lum, 2005; Finley and Joachims, 2008).
For this task we follow Ghamrawi and McCallum
(2005) and Finley and Joachims (2008) and model
the label interactions by constructing a fully con-
nected CRF between the output labels. That is, for
every document, we construct a CRF that contains
a binary random variable for each label (indicating
that the corresponding label is on/off for the doc-
ument) and one binary edge for every unique pair
of labels. This architecture can represent dependen-
cies between labels, but leads to a setting in which
the output variables form one massive clique. The
resulting intractability of inference (and decoding)
motivates the use of ERMA training.
Baseline. We train a model without any of the
pairwise edges (i.e., a separate logistic regression
model for each class). We report the single best
baseline number, since MLE and ERM training re-
sulted in statistically indistinguishable results.
5 Experiments
5.1 Learning Methodology
For all experiments we split the data into
train/development/test sets using the standard splits
when available. We tune optimization algorithm pa-
rameters (initial learning rate, batch size and meta-
parameters ? and ? for stochastic meta descent) on
the training set based on training objective conver-
gence rates. We tune the regularization parameter
? (below) on development data when available, oth-
erwise we use a default value of 0.1?performance
was generally robust for small changes in the value
of ?. All statistical significance testing is performed
using paired permutation tests (Good, 2000).
Gradient-based Optimization. Gradient infor-
mation from the back-propagation procedure can be
used in a local optimization method to minimize em-
pirical loss. In this paper we use stochastic meta
descent (SMD) (Schraudolph, 1999). SMD is a
second-order method that requires vector-Hessian
products. For computing those, we do not need to
maintain the full Hessian matrix. Instead, we apply
more automatic differentiation magic?this time in
the forward mode. Computing the vector-Hessian
product and utilizing it in SMD does not add to the
asymptotic runtime, it requires about twice as many
arithmetic operations, and leads to much faster con-
vergence of the learner in our experience. See Stoy-
anov et al (2011) for details.
Since the empirical risk objective could overfit
the training data, we add an L2 regularizer ?
?
j ?
2
j
that prefers parameter values close to 0. This im-
proves generalization, like the margin constraints in
margin-based methods.
Training Procedure Stoyanov et al (2011) ob-
served that the minimum-risk objective tends to be
highly non-convex in practice. The usual approx-
imate log likelihood training objective appeared to
be smoother over the parameter space, but exhibited
global maxima at parameter values that were rela-
tively good, but sub-optimal for other loss functions.
Mean-squared error (MSE) also gave a smoother ob-
jective than other loss functions. These observations
motivated Stoyanov et al (2011) to use a contin-
uation method. They optimized approximate log-
likelihood for a few iterations to get to a good part of
the parameter space, then switched to using the hy-
brid loss function ?`(y, y?)+(1??)`MSE(y, y?). The
coefficient ? changed gradually from 0 to 1 during
training, which morphs from optimizing a smoother
loss to optimizing the desired bumpy test loss. We
follow the same procedure.
Experiments in this paper use two evaluation met-
rics: percentage accuracy and F-measure. For both
of these losses we decode by selecting the most
probable value under the marginal distribution of
each random variable. This is an exact MBR de-
code for accuracy but an approximate one for the
F-measure; our ERMA training will try to compen-
sate for this approximate decoder. This decoding
procedure is not differentiable due to the use of the
argmax function. To make the decoder differen-
tiable, we replace argmax with a stochastic (soft-
max) version during training, averaging loss over all
possible values v in proportion to their exponenti-
ated probability p(yi = v | x)1/Tdecode . This de-
coder loses smoothness and approaches an argmax
126
Problem Congressional Vote Semi-structured IE Multi-label class.
Loss function Accuracy Token-wise F-score F-score
Non-loopy Baseline 71.2 86.2 (87.1) 81.6
Loopy CRF models INFERENCE:
T
R
A
IN
IN
G
: maxprod sumprod maxprod sumprod maxprod sumprod
MLE 78.2 78.2 89.0 89.5 84.2 84.0
Softmax-margin 79.0 79.0 90.1 90.2 84.3 83.8
Min-risk (maxprod) 85.1 80.1 90.9 90.7 84.5 84.4
Min-risk (sumprod) 83.6 84.5 90.3 90.9 84.7 84.6
Table 1: Results. The top of the table lists the loss function used for each problem and the score for the best exact
baseline. The bottom lists results for the full models used with loopy BP. Models are tested with either sum-product
BP (sumprod) or max-product BP (maxprod) and trained with MLE or the minimum risk criterion. Min-risk training
runs are either annealed (maxprod), which matches max-product test, or not (sumprod), which matches sum-product
test; grey cells in the table indicate matched training and test settings. In each column, we boldface the best result as
well as all results that are not significantly worse (paired permutation test, p < 0.05).
decoder as Tdecode decreases toward 0. For simplic-
ity, our experiments just use a single fixed value of
0.1 for Tdecode. Annealing the decoder slowly did not
lead to significant differences in early experiments
on development data.
5.2 Results
Table 1 lists results of our evaluation. For all three
of our problems, using approximate CRFs results
in statistically significant improvement over the ex-
act baselines, for any of the training procedures.
But among the training procedures for approximate
CRFs, our ERMA procedure?minimizing empiri-
cal risk with the training setting matched to the test
setting?improves over the two baselines, namely
MLE and softmax-margin. MLE and softmax-
margin training were statistically indistinguishable
in our experiments with the exception of semi-
structured IE. ERMA?s improvements over them are
statistically significant at the p < .05 level for the
Congressional Vote and Semi-Structured IE prob-
lems and at the p < .1 level for the Multi-label clas-
sification problem (comparing each matched min-
risk setting shown in a gray cell in Table 1 vs. MLE).
When minimizing risk, we also observe that
matching training and test-time procedures can re-
sult in improved performance in one of the three
problems, Congressional Vote. For this problem, the
matched training condition performs better than the
alternatives (accuracy of 85.1 vs. 83.6 for the an-
nealed max-product testing and 84.5 vs 80.1 for the
sum-product setting), significant at p < .01). We
observe the same effect for semi-structured IE when
testing using max-product inference. For the other
remaining three problem setting training with either
minimal risk training regiment.
Finally, we hypothesized that sum-product infer-
ence may produce more accurate results in certain
cases as it allows more information about differ-
ent parts of the model to be exchanged. How-
ever, our results show that for these three problems,
sum-product and max-product inference yield statis-
tically indistinguishable results. This may be be-
cause the particular CRFs we used included no la-
tent variables (in constrast to the synthetic CRFs
in Stoyanov et al (2011)). As expected, we found
that max-product BP converges in fewer iterations?
sum-product BP required as many as twice the num-
ber of iterations for some of the runs.
Results in this paper represent a new state-of-the-
art for the first two of the problems, Congressional
Vote and Semi-structured IE. For Multi-Label classi-
fication, comparing against the SVM-based method
of Finley and Joachims (2008) goes beyond the
scope of this paper.
6 Related Work
Minimum-risk training has been used in speech
recognition (Bahl et al, 1988), machine translation
(Och, 2003), and energy-based models generally
(LeCun et al, 2006). In graphical models, methods
have been proposed to directly minimize loss in tree-
127
shaped or linear chain MRFs and CRFs (Kakade et
al., 2002; Suzuki et al, 2006; Gross et al, 2007).
All of the above focus on exact inference. Our
approach can be seen as generalizing these methods
to arbitrary graph structures, arbitrary loss functions
and approximate inference.
Lacoste-Julien et al (2011) also consider the ef-
fects of approximate inference on loss. However,
they assume the parameters are given, and modify
the approximate inference algorithm at test time to
consider the loss function.
Using empirical risk minimization to train graph-
ical models was independently proposed by Domke
(2010; 2011). Just as in our own paper (Stoy-
anov et al, 2011), Domke took a decision-theoretic
stance and proposed ERM as a way of calibrating
the graphical model for use with approximate infer-
ence, or for use with data that do not quite match the
modeling assumptions.4
In particular, (Domke, 2011) is similar to (Stoy-
anov et al, 2011) in using ERMA to train model pa-
rameters to be used with ?truncated? inference that
will be run for only a fixed number of iterations. For
a common pixel-labeling benchmark in computer vi-
sion, Domke (2011) shows that this procedure im-
proves training time by orders of magnitude, and
slightly improves accuracy if the same number of
message-passing iterations is used at test time.
Stoyanov and Eisner (2011) extend the ERMA
objective function by adding an explicit runtime
term. This allows them to tune model parameters
and stopping criteria to learn models that obtain a
given speed-accuracy tradeoff. Their approach im-
proves this hybrid objective over a range of coeffi-
cients when compared to the traditional way of in-
ducing sparse structures through L1 regularization.
Eisner and Daume? III (2011) propose the same lin-
ear combination of speed and accuracy as a rein-
forcement learning objective. In general, our pro-
posed ERMA setting resembles the reinforcement
learning problem of trying to directly learn a policy
that minimizes loss or maximizes reward.
We have been concerned with the fact that ERMA
training objectives may suffer from local optima and
non-differentiability. Stoyanov et al (2011) studied
4However, he is less focused than we are on matching train-
ing conditions to test conditions (by including the decoder and
task loss in the ERMA objective).
several such settings, graphed the difficult objective,
and identified some practical workarounds that are
used in the present paper. Although these methods
have enabled us to get strong results by reducing the
empirical risk, we suspect that ERMA training ob-
jectives will benefit from more sophisticated opti-
mization methods. This is true even when the ap-
proximate inference itself is restricted to be some-
thing as simple as a convex minimization. While
that simplified setting can make it slightly more con-
venient to compute the gradient of the inference re-
sult with respect to the parameters (Domke, 2008;
Domke, 2012), there is still no guarantee that follow-
ing that gradient will minimize the empirical risk.
Convex inference does not imply convex training.
7 Conclusions
Motivated by the recently proposed method of Stoy-
anov et al (2011) for minimum-risk training of
CRF-based systems, we revisited three NLP do-
mains that can naturally be modeled with approx-
imate CRF-based systems. These include appli-
cations that have not been modeled with CRFs
before (the ConVote corpus), as well as applica-
tions that have been modeled with loopy CRFs
trained to minimize the approximate log-likelihood
(semi-structured information extraction and collec-
tive multi-label classification). We show that (i)
the NLP models are improved by moving to richer
CRFs that require approximate inference, and (ii)
empirical performance is always significantly im-
proved by training to reduce the loss that would be
achieved by approximate inference, even compared
to another state-of-the-art training method (softmax-
margin) that also considers loss and uses approxi-
mate inference. The general software package that
implements the algorithms in this paper is avail-
able at http://www.clsp.jhu.edu/?ves/
software.html.
Acknowledgments
This material is based upon work supported by the
National Science Foundation under Grant #0937060
to the Computing Research Association for the
CIFellows Project.
128
References
L. Bahl, P. Brown, P. de Souza, and R. Mercer. 1988.
A new algorithm for the estimation of hidden Markov
model parameters. In Proceedings of ICASSP, pages
493?496.
E. Benson, A. Haghighi, and R. Barzilay. 2011. Event
discovery in social media feeds. In Proceedings of
ACL-HLT, pages 389?398.
Y. Choi, C. Cardie, E. Riloff, and S. Patwardhan. 2005.
Identifying sources of opinions with conditional ran-
dom fields and extraction patterns. In Proceedings of
HLT/EMNLP, pages 355?362.
J. Domke. 2008. Learning convex inference of
marginals. In Proceedings of UAI.
J. Domke. 2010. Implicit differentiation by perturba-
tion. In Advances in Neural Information Processing
Systems, pages 523?531.
J. Domke. 2011. Parameter learning with truncated
message-passing. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR).
J. Domke. 2012. Generic methods for optimization-
based modeling. In Proceedings of AISTATS.
M. Dreyer and J. Eisner. 2009. Graphical models over
multiple strings. In Proceedings of EMNLP, pages
101?110.
J. Eisner and Hal Daume? III. 2011. Learning speed-
accuracy tradeoffs in nondeterministic inference al-
gorithms. In COST: NIPS 2011 Workshop on Com-
putational Trade-offs in Statistical Learning, Sierra
Nevada, Spain, December.
A. Elisseeff and J. Weston. 2001. Kernel methods for
multi-labelled classification and categorical regression
problems. In Advances in Neural Information Pro-
cessing Systems, pages 681?687.
J.R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information ex-
traction systems by Gibbs sampling. In Proceedings of
ACL, pages 363?370.
T. Finley and T. Joachims. 2008. Training structural
SVMs when exact inference is intractable. In Proceed-
ings of ICML, pages 304?311.
D. Freitag. 2000. Machine learning for information
extraction in informal domains. Machine learning,
39(2).
N. Ghamrawi and A. McCallum. 2005. Collective multi-
label classification. In Proceedings of CIKM, pages
195?200.
K. Gimpel and N.A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In Proceedings of ACL, pages 733?736.
P. I. Good. 2000. Permutation Tests. Springer.
A. Griewank and G. Corliss, editors. 1991. Automatic
Differentiation of Algorithms. SIAM, Philadelphia.
S. Gross, O. Russakovsky, C. Do, and S. Batzoglou.
2007. Training conditional random fields for maxi-
mum labelwise accuracy. Advances in Neural Infor-
mation Processing Systems, 19:529.
H. Ji and R. Grishman. 2011. Knowledge base popula-
tion: Successful approaches and challenges. In Pro-
ceedings of ACL-HLT, pages 1148?1158.
S. Kakade, Y.W. Teh, and S. Roweis. 2002. An alternate
objective function for Markovian fields. In Proceed-
ings of ICML, pages 275?282.
D. Koller and N. Friedman. 2009. Probabilistic Graph-
ical Models: Principles and Techniques. The MIT
Press.
A. Kulesza and F. Pereira. 2008. Structured learning
with approximate inference. In Advances in Neural
Information Processing Systems, pages 785?792.
S. Lacoste-Julien, F. Huszr, and Z. Ghahramani.
2011. Approximate inference for the loss-calibrated
Bayesian. In Proceedings of AISTATS.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, pages 282?289.
Y. LeCun, S. Chopra, R. Hadsell, M.A. Ranzato, and F.-
J. Huang. 2006. A tutorial on energy-based learning.
In G. Bakir, T. Hofman, B. Schlkopf, A. Smola, and
B. Taskar, editors, Predicting Structured Data. MIT
Press.
Z. Li and J. Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
EMNLP, pages 40?51.
K. P. Murphy, Y. Weiss, and M. I. Jordan. 1999. Loopy
belief propagation for approximate inference: An em-
pirical study. In Proceedings of UAI.
F. Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL, pages
160?167.
F. Peng and A. McCallum. 2006. Information extraction
from research papers using conditional random fields.
Information Processing & Management, 42(4):963?
979.
N.N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. In Proceedings of ANN,
pages 569?574.
F. Sha and F. Pereira. 2003. Shallow parsing with con-
ditional random fields. In Proceedings of ACL/HLT,
pages 134?141.
D.A. Smith and J. Eisner. 2006. Minimum risk annealing
for training log-linear models. In Proceedings of the
COLING/ACL, pages 787?794.
129
D. Smith and J. Eisner. 2008. Dependency parsing by
belief propagation. In Proceedings of EMNLP, pages
145?156.
V. Stoyanov and J. Eisner. 2011. Learning cost-aware,
loss-aware approximate inference policies for proba-
bilistic graphical models. In COST: NIPS 2011 Work-
shop on Computational Trade-offs in Statistical Learn-
ing, Sierra Nevada, Spain, December.
V. Stoyanov, A. Ropson, and J. Eisner. 2011. Empirical
risk minimization of graphical model parameters given
approximate inference, decoding, and model structure.
In Proceedings of AISTATS.
C. Sutton and A. McCallum. 2005. Piecewise training
of undirected models. In Proceedings of UAI, pages
568?575.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized proba-
bilistic models for labeling and segmenting sequence
data. The Journal of Machine Learning Research,
8:693?723.
J. Suzuki, E. McDermott, and H. Isozaki. 2006. Train-
ing conditional random fields with multivariate eval-
uation measures. In Proceedings of COLING/ACL,
pages 217?224.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. Proceedings of NIPS, pages 25?32.
M. Thomas, B. Pang, and L. Lee. 2006. Get out the vote:
Determining support or opposition from congressional
floor-debate transcripts. In Proceedings of EMNLP,
pages 327?335.
S. Vishwanathan, N. Schraudolph, M. Schmidt, and
K. Murphy. 2006. Accelerated training of conditional
random fields with stochastic gradient methods. In
Proceedings of ICML, pages 969?976.
M. Wainwright. 2006. Estimating the ?wrong? graphi-
cal model: Benefits in the computation-limited setting.
Journal of Machine Learning Research, 7:1829?1859,
September.
R.J. Williams and D. Zipser. 1989. A learning algo-
rithm for continually running fully recurrent neural
networks. Neural Computation, 1(2):270?280.
130
Proceedings of the ACL 2010 Conference Short Papers, pages 156?161,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Coreference Resolution with Reconcile
Veselin Stoyanov
Center for Language
and Speech Processing
Johns Hopkins Univ.
Baltimore, MD
ves@cs.jhu.edu
Claire Cardie
Department of
Computer Science
Cornell University
Ithaca, NY
cardie@cs.cornell.edu
Nathan Gilbert
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
ngilbert@cs.utah.edu
riloff@cs.utah.edu
David Buttler
David Hysom
Lawrence Livermore
National Laboratory
Livermore, CA
buttler1@llnl.gov
hysom1@llnl.gov
Abstract
Despite the existence of several noun phrase coref-
erence resolution data sets as well as several for-
mal evaluations on the task, it remains frustratingly
difficult to compare results across different corefer-
ence resolution systems. This is due to the high cost
of implementing a complete end-to-end coreference
resolution system, which often forces researchers
to substitute available gold-standard information in
lieu of implementing a module that would compute
that information. Unfortunately, this leads to incon-
sistent and often unrealistic evaluation scenarios.
With the aim to facilitate consistent and realis-
tic experimental evaluations in coreference resolu-
tion, we present Reconcile, an infrastructure for the
development of learning-based noun phrase (NP)
coreference resolution systems. Reconcile is de-
signed to facilitate the rapid creation of corefer-
ence resolution systems, easy implementation of
new feature sets and approaches to coreference res-
olution, and empirical evaluation of coreference re-
solvers across a variety of benchmark data sets and
standard scoring metrics. We describe Reconcile
and present experimental results showing that Rec-
oncile can be used to create a coreference resolver
that achieves performance comparable to state-of-
the-art systems on six benchmark data sets.
1 Introduction
Noun phrase coreference resolution (or simply
coreference resolution) is the problem of identi-
fying all noun phrases (NPs) that refer to the same
entity in a text. The problem of coreference res-
olution is fundamental in the field of natural lan-
guage processing (NLP) because of its usefulness
for other NLP tasks, as well as the theoretical in-
terest in understanding the computational mech-
anisms involved in government, binding and lin-
guistic reference.
Several formal evaluations have been conducted
for the coreference resolution task (e.g., MUC-6
(1995), ACE NIST (2004)), and the data sets cre-
ated for these evaluations have become standard
benchmarks in the field (e.g., MUC and ACE data
sets). However, it is still frustratingly difficult to
compare results across different coreference res-
olution systems. Reported coreference resolu-
tion scores vary wildly across data sets, evaluation
metrics, and system configurations.
We believe that one root cause of these dispar-
ities is the high cost of implementing an end-to-
end coreference resolution system. Coreference
resolution is a complex problem, and successful
systems must tackle a variety of non-trivial sub-
problems that are central to the coreference task ?
e.g., mention/markable detection, anaphor identi-
fication ? and that require substantial implemen-
tation efforts. As a result, many researchers ex-
ploit gold-standard annotations, when available, as
a substitute for component technologies to solve
these subproblems. For example, many published
research results use gold standard annotations to
identify NPs (substituting for mention/markable
detection), to distinguish anaphoric NPs from non-
anaphoric NPs (substituting for anaphoricity de-
termination), to identify named entities (substitut-
ing for named entity recognition), and to identify
the semantic types of NPs (substituting for seman-
tic class identification). Unfortunately, the use of
gold standard annotations for key/critical compo-
nent technologies leads to an unrealistic evalua-
tion setting, and makes it impossible to directly
compare results against coreference resolvers that
solve all of these subproblems from scratch.
Comparison of coreference resolvers is further
hindered by the use of several competing (and
non-trivial) evaluation measures, and data sets that
have substantially different task definitions and
annotation formats. Additionally, coreference res-
olution is a pervasive problem in NLP and many
NLP applications could benefit from an effective
coreference resolver that can be easily configured
and customized.
To address these issues, we have created a plat-
form for coreference resolution, called Reconcile,
that can serve as a software infrastructure to sup-
port the creation of, experimentation with, and
evaluation of coreference resolvers. Reconcile
was designed with the following seven desiderata
in mind:
? implement the basic underlying software ar-
156
chitecture of contemporary state-of-the-art
learning-based coreference resolution sys-
tems;
? support experimentation on most of the stan-
dard coreference resolution data sets;
? implement most popular coreference resolu-
tion scoring metrics;
? exhibit state-of-the-art coreference resolution
performance (i.e., it can be configured to cre-
ate a resolver that achieves performance close
to the best reported results);
? can be easily extended with new methods and
features;
? is relatively fast and easy to configure and
run;
? has a set of pre-built resolvers that can be
used as black-box coreference resolution sys-
tems.
While several other coreference resolution sys-
tems are publicly available (e.g., Poesio and
Kabadjov (2004), Qiu et al (2004) and Versley et
al. (2008)), none meets all seven of these desider-
ata (see Related Work). Reconcile is a modular
software platform that abstracts the basic archi-
tecture of most contemporary supervised learning-
based coreference resolution systems (e.g., Soon
et al (2001), Ng and Cardie (2002), Bengtson and
Roth (2008)) and achieves performance compara-
ble to the state-of-the-art on several benchmark
data sets. Additionally, Reconcile can be eas-
ily reconfigured to use different algorithms, fea-
tures, preprocessing elements, evaluation settings
and metrics.
In the rest of this paper, we review related work
(Section 2), describe Reconcile?s organization and
components (Section 3) and show experimental re-
sults for Reconcile on six data sets and two evalu-
ation metrics (Section 4).
2 Related Work
Several coreference resolution systems are cur-
rently publicly available. JavaRap (Qiu et al,
2004) is an implementation of the Lappin and
Leass? (1994) Resolution of Anaphora Procedure
(RAP). JavaRap resolves only pronouns and, thus,
it is not directly comparable to Reconcile. GuiTaR
(Poesio and Kabadjov, 2004) and BART (Versley
et al, 2008) (which can be considered a succes-
sor of GuiTaR) are both modular systems that tar-
get the full coreference resolution task. As such,
both systems come close to meeting the majority
of the desiderata set forth in Section 1. BART,
in particular, can be considered an alternative to
Reconcile, although we believe that Reconcile?s
approach is more flexible than BART?s. In addi-
tion, the architecture and system components of
Reconcile (including a comprehensive set of fea-
tures that draw on the expertise of state-of-the-art
supervised learning approaches, such as Bengtson
and Roth (2008)) result in performance closer to
the state-of-the-art.
Coreference resolution has received much re-
search attention, resulting in an array of ap-
proaches, algorithms and features. Reconcile
is modeled after typical supervised learning ap-
proaches to coreference resolution (e.g. the archi-
tecture introduced by Soon et al (2001)) because
of the popularity and relatively good performance
of these systems.
However, there have been other approaches
to coreference resolution, including unsupervised
and semi-supervised approaches (e.g. Haghighi
and Klein (2007)), structured approaches (e.g.
McCallum and Wellner (2004) and Finley and
Joachims (2005)), competition approaches (e.g.
Yang et al (2003)) and a bell-tree search approach
(Luo et al (2004)). Most of these approaches rely
on some notion of pairwise feature-based similar-
ity and can be directly implemented in Reconcile.
3 System Description
Reconcile was designed to be a research testbed
capable of implementing most current approaches
to coreference resolution. Reconcile is written in
Java, to be portable across platforms, and was de-
signed to be easily reconfigurable with respect to
subcomponents, feature sets, parameter settings,
etc.
Reconcile?s architecture is illustrated in Figure
1. For simplicity, Figure 1 shows Reconcile?s op-
eration during the classification phase (i.e., assum-
ing that a trained classifier is present).
The basic architecture of the system includes
five major steps. Starting with a corpus of docu-
ments together with a manually annotated corefer-
ence resolution answer key1, Reconcile performs
1Only required during training.
157
Figure 1: The Reconcile classification architecture.
the following steps, in order:
1. Preprocessing. All documents are passed
through a series of (external) linguistic pro-
cessors such as tokenizers, part-of-speech
taggers, syntactic parsers, etc. These com-
ponents produce annotations of the text. Ta-
ble 1 lists the preprocessors currently inter-
faced in Reconcile. Note that Reconcile in-
cludes several in-house NP detectors, that
conform to the different data sets? defini-
tions of what constitutes a NP (e.g., MUC
vs. ACE). All of the extractors utilize a syn-
tactic parse of the text and the output of a
Named Entity (NE) extractor, but extract dif-
ferent constructs as specialized in the corre-
sponding definition. The NP extractors suc-
cessfully recognize about 95% of the NPs in
the MUC and ACE gold standards.
2. Feature generation. Using annotations pro-
duced during preprocessing, Reconcile pro-
duces feature vectors for pairs of NPs. For
example, a feature might denote whether the
two NPs agree in number, or whether they
have any words in common. Reconcile in-
cludes over 80 features, inspired by other suc-
cessful coreference resolution systems such
as Soon et al (2001) and Ng and Cardie
(2002).
3. Classification. Reconcile learns a classifier
that operates on feature vectors representing
Task Systems
Sentence UIUC (CC Group, 2009)
splitter OpenNLP (Baldridge, J., 2005)
Tokenizer OpenNLP (Baldridge, J., 2005)
POS OpenNLP (Baldridge, J., 2005)
Tagger + the two parsers below
Parser Stanford (Klein and Manning, 2003)
Berkeley (Petrov and Klein, 2007)
Dep. parser Stanford (Klein and Manning, 2003)
NE OpenNLP (Baldridge, J., 2005)
Recognizer Stanford (Finkel et al, 2005)
NP Detector In-house
Table 1: Preprocessing components available in
Reconcile.
pairs of NPs and it is trained to assign a score
indicating the likelihood that the NPs in the
pair are coreferent.
4. Clustering. A clustering algorithm consoli-
dates the predictions output by the classifier
and forms the final set of coreference clusters
(chains).2
5. Scoring. Finally, during testing Reconcile
runs scoring algorithms that compare the
chains produced by the system to the gold-
standard chains in the answer key.
Each of the five steps above can invoke differ-
ent components. Reconcile?s modularity makes it
2Some structured coreference resolution algorithms (e.g.,
McCallum and Wellner (2004) and Finley and Joachims
(2005)) combine the classification and clustering steps above.
Reconcile can easily accommodate this modification.
158
Step Available modules
Classification various learners in the Weka toolkit
libSVM (Chang and Lin, 2001)
SVMlight (Joachims, 2002)
Clustering Single-link
Best-First
Most Recent First
Scoring MUC score (Vilain et al, 1995)
B3 score (Bagga and Baldwin, 1998)
CEAF score (Luo, 2005)
Table 2: Available implementations for different
modules available in Reconcile.
easy for new components to be implemented and
existing ones to be removed or replaced. Recon-
cile?s standard distribution comes with a compre-
hensive set of implemented components ? those
available for steps 2?5 are shown in Table 2. Rec-
oncile contains over 38,000 lines of original Java
code. Only about 15% of the code is concerned
with running existing components in the prepro-
cessing step, while the rest deals with NP extrac-
tion, implementations of features, clustering algo-
rithms and scorers. More details about Recon-
cile?s architecture and available components and
features can be found in Stoyanov et al (2010).
4 Evaluation
4.1 Data Sets
Reconcile incorporates the six most commonly
used coreference resolution data sets, two from the
MUC conferences (MUC-6, 1995; MUC-7, 1997)
and four from the ACE Program (NIST, 2004).
For ACE, we incorporate only the newswire por-
tion. When available, Reconcile employs the stan-
dard test/train split. Otherwise, we randomly split
the data into a training and test set following a
70/30 ratio. Performance is evaluated according
to the B3 and MUC scoring metrics.
4.2 The Reconcile2010 Configuration
Reconcile can be easily configured with differ-
ent algorithms for markable detection, anaphoric-
ity determination, feature extraction, etc., and run
against several scoring metrics. For the purpose of
this sample evaluation, we create only one partic-
ular instantiation of Reconcile, which we will call
Reconcile2010 to differentiate it from the general
platform. Reconcile2010 is configured using the
following components:
1. Preprocessing
(a) Sentence Splitter: OpenNLP
(b) Tokenizer: OpenNLP
(c) POS Tagger: OpenNLP
(d) Parser: Berkeley
(e) Named Entity Recognizer: Stanford
2. Feature Set - A hand-selected subset of 60 out of the
more than 80 features available. The features were se-
lected to include most of the features from Soon et al
Soon et al (2001), Ng and Cardie (2002) and Bengtson
and Roth (2008).
3. Classifier - Averaged Perceptron
4. Clustering - Single-link - Positive decision threshold
was tuned by cross validation of the training set.
4.3 Experimental Results
The first two rows of Table 3 show the perfor-
mance of Reconcile2010. For all data sets, B3
scores are higher than MUC scores. The MUC
score is highest for the MUC6 data set, while B3
scores are higher for the ACE data sets as com-
pared to the MUC data sets.
Due to the difficulties outlined in Section 1,
results for Reconcile presented here are directly
comparable only to a limited number of scores
reported in the literature. The bottom three
rows of Table 3 list these comparable scores,
which show that Reconcile2010 exhibits state-of-
the-art performance for supervised learning-based
coreference resolvers. A more detailed study of
Reconcile-based coreference resolution systems
in different evaluation scenarios can be found in
Stoyanov et al (2009).
5 Conclusions
Reconcile is a general architecture for coreference
resolution that can be used to easily create various
coreference resolvers. Reconcile provides broad
support for experimentation in coreference reso-
lution, including implementation of the basic ar-
chitecture of contemporary state-of-the-art coref-
erence systems and a variety of individual mod-
ules employed in these systems. Additionally,
Reconcile handles all of the formatting and scor-
ing peculiarities of the most widely used coref-
erence resolution data sets (those created as part
of the MUC and ACE conferences) and, thus,
allows for easy implementation and evaluation
across these data sets. We hope that Reconcile
will support experimental research in coreference
resolution and provide a state-of-the-art corefer-
ence resolver for both researchers and application
developers. We believe that in this way Recon-
cile will facilitate meaningful and consistent com-
parisons of coreference resolution systems. The
full Reconcile release is available for download at
http://www.cs.utah.edu/nlp/reconcile/.
159
System Score Data sets
MUC6 MUC7 ACE-2 ACE03 ACE04 ACE05
Reconcile2010
MUC 68.50 62.80 65.99 67.87 62.03 67.41
B3 70.88 65.86 78.29 79.39 76.50 73.71
Soon et al (2001) MUC 62.6 60.4 ? ? ? ?
Ng and Cardie (2002) MUC 70.4 63.4 ? ? ? ?
Yang et al (2003) MUC 71.3 60.2 ? ? ? ?
Table 3: Scores for Reconcile on six data sets and scores for comparable coreference systems.
Acknowledgments
This research was supported in part by the Na-
tional Science Foundation under Grant # 0937060
to the Computing Research Association for the
CIFellows Project, Lawrence Livermore National
Laboratory subcontract B573245, Department of
Homeland Security Grant N0014-07-1-0152, and
Air Force Contract FA8750-09-C-0172 under the
DARPA Machine Reading Program.
The authors would like to thank the anonymous
reviewers for their useful comments.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Linguistic Coreference Workshop
at the Language Resources and Evaluation Conference.
Baldridge, J. 2005. The OpenNLP project.
http://opennlp.sourceforge.net/.
E. Bengtson and D. Roth. 2008. Understanding the value of
features for coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP).
CC Group. 2009. Sentence Segmentation Tool.
http://l2r.cs.uiuc.edu/ cogcomp/atool.php?tkey=SS.
C. Chang and C. Lin. 2001. LIBSVM: a Li-
brary for Support Vector Machines. Available at
http://www.csie.ntu.edu.tw/cjlin/libsvm.
J. Finkel, T. Grenager, and C. Manning. 2005. Incorporating
Non-local Information into Information Extraction Sys-
tems by Gibbs Sampling. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics and
44th Annual Meeting of the ACL.
T. Finley and T. Joachims. 2005. Supervised clustering with
support vector machines. In Proceedings of the Twenty-
second International Conference on Machine Learning
(ICML 2005).
A. Haghighi and D. Klein. 2007. Unsupervised Coreference
Resolution in a Nonparametric Bayesian Model. In Pro-
ceedings of the 45th Annual Meeting of the ACL.
T. Joachims. 2002. SVMLight, http://svmlight.joachims.org.
D. Klein and C. Manning. 2003. Fast Exact Inference with
a Factored Model for Natural Language Parsing. In Ad-
vances in Neural Information Processing (NIPS 2003).
S. Lappin and H. Leass. 1994. An algorithm for pronom-
inal anaphora resolution. Computational Linguistics,
20(4):535?561.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla, and
S. Roukos. 2004. A mention-synchronous coreference
resolution algorithm based on the bell tree. In Proceed-
ings of the 42nd Annual Meeting of the ACL.
X. Luo. 2005. On Coreference Resolution Performance
Metrics. In Proceedings of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing (HLT/EMNLP).
A. McCallum and B. Wellner. 2004. Conditional Models
of Identity Uncertainty with Application to Noun Coref-
erence. In Advances in Neural Information Processing
(NIPS 2004).
MUC-6. 1995. Coreference Task Definition. In Proceedings
of the Sixth Message Understanding Conference (MUC-
6).
MUC-7. 1997. Coreference Task Definition. In Proceed-
ings of the Seventh Message Understanding Conference
(MUC-7).
V. Ng and C. Cardie. 2002. Improving Machine Learning
Approaches to Coreference Resolution. In Proceedings of
the 40th Annual Meeting of the ACL.
NIST. 2004. The ACE Evaluation Plan. NIST.
S. Petrov and D. Klein. 2007. Improved Inference for Un-
lexicalized Parsing. In Proceedings of the Joint Meeting
of the Human Language Technology Conference and the
North American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2007).
M. Poesio and M. Kabadjov. 2004. A general-purpose,
off-the-shelf anaphora resolution module: implementation
and preliminary evaluation. In Proceedings of the Lan-
guage Resources and Evaluation Conference.
L. Qiu, M.-Y. Kan, and T.-S. Chua. 2004. A public reference
implementation of the rap anaphora resolution algorithm.
In Proceedings of the Language Resources and Evaluation
Conference.
W. Soon, H. Ng, and D. Lim. 2001. A Machine Learning Ap-
proach to Coreference of Noun Phrases. Computational
Linguistics, 27(4):521?541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009. Co-
nundrums in noun phrase coreference resolution: Mak-
ing sense of the state-of-the-art. In Proceedings of
ACL/IJCNLP.
160
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler, and
D. Hysom. 2010. Reconcile: A coreference resolution
research platform. Technical report, Cornell University.
Y. Versley, S. Ponzetto, M. Poesio, V. Eidelman, A. Jern,
J. Smith, X. Yang, and A. Moschitti. 2008. BART: A
modular toolkit for coreference resolution. In Proceed-
ings of the Language Resources and Evaluation Confer-
ence.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A Model-Theoretic Coreference
Scoring Theme. In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
X. Yang, G. Zhou, J. Su, and C. Tan. 2003. Coreference
resolution using competition learning approach. In Pro-
ceedings of the 41st Annual Meeting of the ACL.
161
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 312?320, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SemEval-2013 Task 2: Sentiment Analysis in Twitter
Preslav Nakov
QCRI, Qatar Foundation
pnakov@qf.org.qa
Zornitsa Kozareva
USC Information Sciences Institute
kozareva@isi.edu
Alan Ritter
University of Washington
aritter@cs.washington.edu
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Veselin Stoyanov
JHU HLTCOE
ves@cs.jhu.edu
Theresa Wilson
JHU HLTCOE
taw@jhu.edu
Abstract
In recent years, sentiment analysis in social
media has attracted a lot of research interest
and has been used for a number of applica-
tions. Unfortunately, research has been hin-
dered by the lack of suitable datasets, com-
plicating the comparison between approaches.
To address this issue, we have proposed
SemEval-2013 Task 2: Sentiment Analysis in
Twitter, which included two subtasks: A, an
expression-level subtask, and B, a message-
level subtask. We used crowdsourcing on
Amazon Mechanical Turk to label a large
Twitter training dataset alng with additional
test sets of Twitter and SMS messages for both
subtasks. All datasets used in the evaluation
are released to the research community. The
task attracted significant interest and a total
of 149 submissions from 44 teams. The best-
performing team achieved an F1 of 88.9% and
69% for subtasks A and B, respectively.
1 Introduction
In the past decade, new forms of communication,
such as microblogging and text messaging have
emerged and become ubiquitous. Twitter messages
(tweets) and cell phone messages (SMS) are often
used to share opinions and sentiments about the sur-
rounding world, and the availability of social con-
tent generated on sites such as Twitter creates new
opportunities to automatically study public opinion.
Working with these informal text genres presents
new challenges for natural language processing be-
yond those encountered when working with more
traditional text genres such as newswire.
Tweets and SMS messages are short in length: a
sentence or a headline rather than a document. The
language they use is very informal, with creative
spelling and punctuation, misspellings, slang, new
words, URLs, and genre-specific terminology and
abbreviations, e.g., RT for re-tweet and #hashtags.1
How to handle such challenges so as to automati-
cally mine and understand the opinions and senti-
ments that people are communicating has only very
recently been the subject of research (Jansen et al,
2009; Barbosa and Feng, 2010; Bifet et al, 2011;
Davidov et al, 2010; O?Connor et al, 2010; Pak and
Paroubek, 2010; Tumasjan et al, 2010; Kouloumpis
et al, 2011).
Another aspect of social media data, such as Twit-
ter messages, is that they include rich structured in-
formation about the individuals involved in the com-
munication. For example, Twitter maintains infor-
mation about who follows whom. Re-tweets (re-
shares of a tweet) and tags inside of tweets provide
discourse information. Modeling such structured in-
formation is important because it provides means for
empirically studying social interactions where opin-
ion is conveyed, e.g., we can study the properties of
persuasive language or those associated with influ-
ential users.
Several corpora with detailed opinion and senti-
ment annotation have been made freely available,
e.g., the MPQA corpus (Wiebe et al, 2005) of
newswire text. These corpora have proved very
valuable as resources for learning about the lan-
guage of sentiment in general, but they did not focus
on social media.
1Hashtags are a type of tagging for Twitter messages.
312
Twitter RT @tash jade: That?s really sad, Charlie RT ?Until tonight I never realised how fucked up I was? -
Charlie Sheen #sheenroast
SMS Glad to hear you are coping fine in uni... So, wat interview did you go to? How did it go?
Table 1: Examples of sentences from each corpus that contain subjective phrases.
While some Twitter sentiment datasets have al-
ready been created, they were either small and pro-
prietary, such as the i-sieve corpus (Kouloumpis
et al, 2011), or they were created only for Span-
ish like the TASS corpus2 (Villena-Roma?n et al,
2013), or they relied on noisy labels obtained from
emoticons and hashtags. They further focused on
message-level sentiment, and no Twitter or SMS
corpus with expression-level sentiment annotations
has been made available so far.
Thus, the primary goal of our SemEval-2013 task
2 has been to promote research that will lead to a
better understanding of how sentiment is conveyed
in Tweets and SMS messages. Toward that goal,
we created the SemEval Tweet corpus, which con-
tains Tweets (for both training and testing) and SMS
messages (for testing only) with sentiment expres-
sions annotated with contextual phrase-level polar-
ity as well as an overall message-level polarity. We
used this corpus as a testbed for the system evalua-
tion at SemEval-2013 Task 2.
In the remainder of this paper, we first describe
the task, the dataset creation process, and the evalu-
ation methodology. We then summarize the charac-
teristics of the approaches taken by the participating
systems and we discuss their scores.
2 Task Description
We had two subtasks: an expression-level subtask
and a message-level subtask. Participants could
choose to participate in either or both subtasks. Be-
low we provide short descriptions of the objectives
of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked instance
of a word or a phrase, determine whether that
instance is positive, negative or neutral in that
context. The boundaries for the marked in-
stance were provided: this was a classification
task, not an entity recognition task.
2http://www.daedalus.es/TASS/corpus.php
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment. For
messages conveying both a positive and a
negative sentiment, whichever is the stronger
one was to be chosen.
Each participating team was allowed to submit re-
sults for two different systems per subtask: one con-
strained, and one unconstrained. A constrained sys-
tem could only use the provided data for training,
but it could also use other resources such as lexi-
cons obtained elsewhere. An unconstrained system
could use any additional data as part of the training
process; this could be done in a supervised, semi-
supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to the
data used to train a classifier. For example, if other
data (excluding the test data) was used to develop
a sentiment lexicon, and the lexicon was used to
generate features, the system would still be con-
strained. However, if other data (excluding the test
data) was used to develop a sentiment lexicon, and
this lexicon was used to automatically label addi-
tional Tweet/SMS messages and then used with the
original data to train the classifier, then such a sys-
tem would be unconstrained.
3 Dataset Creation
In the following sections we describe the collection
and annotation of the Twitter and SMS datasets.
3.1 Data Collection
Twitter is the most common micro-blogging site on
the Web, and we used it to gather tweets that express
sentiment about popular topics. We first extracted
named entities using a Twitter-tuned NER system
(Ritter et al, 2011) from millions of tweets, which
we collected over a one-year period spanning from
January 2012 to January 2013; we used the public
streaming Twitter API to download tweets.
313
Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective,
positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark
the position of its start and end in the text boxes below. The number above each word indicates its position. The
word/phrase will be generated in the adjacent textbox so that you can confirm that you chose the correct range.
Choose the polarity of the word or phrase by selecting one of the radio buttons: positive, negative, or neutral. If a
sentence is not subjective please select the checkbox indicating that ?There are no subjective words/phrases?. Please
read the examples and invalid responses before beginning if this is your first time answering this hit.
Figure 1: Instructions provided to workers on Mechanical Turk followed by a screenshot.
Average # of Total Phrase Count Vocabulary
Corpus Words Characters Positive Negative Neutral Size
Twitter - Training 25.4 120.0 5,895 3,131 471 20,012
Twitter - Dev 25.5 120.0 648 430 57 4,426
Twitter - Test 25.4 121.2 2,734 1,541 160 11,736
SMS - Test 24.5 95.6 1,071 1,104 159 3,562
Table 2: Statistics for Subtask A.
We then identified popular topics as those named
entities that are frequently mentioned in association
with a specific date (Ritter et al, 2012). Given this
set of automatically identified topics, we gathered
tweets from the same time period which mentioned
the named entities. The testing messages had differ-
ent topics from training and spanned later periods.
To identify messages that express sentiment to-
wards these topics, we filtered the tweets us-
ing SentiWordNet (Baccianella et al, 2010). We
removed messages that contained no sentiment-
bearing words, keeping only those with at least one
word with positive or negative sentiment score that
is greater than 0.3 in SentiWordNet for at least one
sense of the words. Without filtering, we found class
imbalance to be too high.3
Twitter messages are rich in social media features,
including out-of-vocabulary (OOV) words, emoti-
cons, and acronyms; see Table 1. A large portion of
the OOV words are hashtags (e.g., #sheenroast)
and mentions (e.g., @tash jade).
3Filtering based on an existing lexicon does bias the dataset
to some degree; however, note that the text still contains senti-
ment expressions outside those in the lexicon.
Corpus Positive Negative Objective
/ Neutral
Twitter - Training 3,662 1,466 4,600
Twitter - Dev 575 340 739
Twitter - Test 1,573 601 1,640
SMS - Test 492 394 1,208
Table 3: Statistics for Subtask B.
We annotated the same Twitter messages with an-
notations for subtask A and subtask B. However,
the final training and testing datasets overlap only
partially between the two subtasks since we had
to throw away messages with low inter-annotator
agreement, and this differed between the subtasks.
For testing, we also annotated SMS messages, taken
from the NUS SMS corpus4 (Chen and Kan, 2012).
Tables 2 and 3 show statistics about the corpora we
created for subtasks A and B.
4http://wing.comp.nus.edu.sg/SMSCorpus/
314
A B
Lower Avg. Upper Avg.
Twitter - Train 64.7 82.4 90.8 82.7
Twitter - Dev 51.2 74.7 87.8 78.4
Twitter - Test 68.8 83.6 90.9 76.9
SMS - Test 66.5 88.5 81.2 77.6
Table 4: Bounds for datasets in subtasks A and B.
3.2 Annotation Guidelines
The instructions provided to the annotators, along
with an example, are shown in Figure 1. We pro-
vided several additional examples to the annotators,
shown in Table 5.
In addition, we filtered spammers by considering
the following kinds of annotations invalid:
? containing overlapping subjective phrases;
? subjective but without a subjective phrase;
? marking every single word as subjective;
? not having the overall sentiment marked.
3.3 Annotation Process
Our datasets were annotated for sentiment on Me-
chanical Turk. Each sentence was annotated by five
Mechanical Turk workers (Turkers). In order to
qualify for the hits, the Turker had to have an ap-
proval rate greater than 95% and have completed 50
approved hits. Each Turker was paid three cents
per hit. The Turker had to mark all the subjec-
tive words/phrases in the sentence by indicating their
start and end positions and say whether each subjec-
tive word/phrase was positive, negative, or neutral
(subtask A). They also had to indicate the overall
polarity of the sentence (subtask B).
Figure 1 shows the instructions and an exam-
ple provided to the Turkers. The first five rows
of Table 6 show an example of the subjective
words/phrases marked by each of the workers.
For subtask A, we combined the annotations of
each of the workers using intersection as indicated
in the last row of Table 6. A word had to appear
in 2/3 of the annotations in order to be considered
subjective. Similarly, a word had to be labeled with
a particular polarity (positive, negative, or neutral)
2/3 of the time in order to receive that label.
We also experimented with combining annota-
tions by computing the union of the sentences, and
taking the sentence of the worker who annotated the
most hits, but we found that these methods were
not as accurate. Table 4 shows the lower, average,
and upper bounds for all the hits by computing the
bounds for each hit and averaging them together.
This gives a good indication about how well we can
expect the systems to perform. For example, even if
we used the best annotator each time, it would still
not be possible to get perfect accuracy.
For subtask B, the polarity of the entire sentence
was determined based on the majority of the labels.
If there was a tie, the sentence was discarded. In
order to reduce the number of sentences lost, we
combined the objective and the neutral labels, which
Turkers tended to mix up. Table 4 shows the aver-
age bound for subtask B by computing the bounds
for each hit and averaging them together. Since the
polarity is chosen based on the majority, the upper
bound is 100%.
4 Scoring
For both subtasks, the participating systems were
required to perform a three-way classification ? a
particular marked phrase (for subtask A) or an en-
tire message (for subtask B) was to be classified as
positive, negative, or objective. For each system,
we computed a score for predicting positive/negative
phrases/messages vs. the other two classes.
For instance, to compute positive precision, Ppos,
we find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we divide
that number by the total number of messages it pre-
dicted to be positive. To compute recall, for the pos-
itive class, Rpos, we find the number of messages
correctly predicted to be positive and we divide that
number by the total number of positive messages in
the gold standard.
We then calculate F-score for the positive labels,
the harmonic average of precision and recall as fol-
lows Fpos = 2
PposRpos
Ppos+Rpos
. We carry out a similar
computation to calculate Fneg, which is F1 for neg-
ative messages.
The overall score for each system run is then
given by the average of the F1-scores for the posi-
tive and negative classes: F = (Fpos + Fneg)/2.
315
Authorities are only too aware that Kashgar is 4,000 kilometres (2,500 miles) from Beijing but only a tenth of
the distance from the Pakistani border, and are desperate to ensure instability or militancy does not leak over the
frontiers.
Taiwan-made products stood a good chance of becoming even more competitive thanks to wider access to overseas
markets and lower costs for material imports, he said.
?March appears to be a more reasonable estimate while earlier admission cannot be entirely ruled out,? according
to Chen, also Taiwan?s chief WTO negotiator.
friday evening plans were great, but saturday?s plans didnt go as expected ? i went dancing & it was an ok club,
but terribly crowded :-(
WHY THE HELL DO YOU GUYS ALL HAVE MRS. KENNEDY! SHES A FUCKING DOUCHE
AT&T was okay but whenever they do something nice in the name of customer service it seems like a favor, while
T-Mobile makes that a normal everyday thin
obama should be impeached on TREASON charges. Our Nuclear arsenal was TOP Secret. Till HE told our enemies
what we had. #Coward #Traitor
My graduation speech: ?I?d like to thanks Google, Wikipedia and my computer! :D #iThingteens
Table 5: List of example sentences with annotations that were provided to the annotators. All subjective phrases are
italicized. Positive phrases are in green, negative phrases are in red, and neutral phrases are in blue.
Worker 1 I would love to watch Vampire Diaries :) and some Heroes! Great combination 9/13
Worker 2 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Worker 3 I would love to watch Vampire Diaries :) and some Heroes! Great combination 10/13
Worker 4 I would love to watch Vampire Diaries :) and some Heroes! Great combination 13/13
Worker 5 I would love to watch Vampire Diaries :) and some Heroes! Great combination 11/13
Intersection I would love to watch Vampire Diaries :) and some Heroes! Great combination
Table 6: Example of a sentence annotated for subjectivity on Mechanical Turk. Words and phrases that were marked as
subjective are italicized and highlighted in bold. The first five rows are annotations provided by Turkers, and the final
row shows their intersection. The final column shows the accuracy for each annotation compared to the intersection.
Note that ignoring Fneutral does not reduce the
task to predicting positive vs. negative labels only
(even though some participants have chosen to do
so) since the gold standard still contains neutral
labels which are to be predicted: Fpos and Fneg
would suffer if these examples are labeled as posi-
tive and/or negative instead of neutral.
We provided participants with a scorer. In addi-
tion to outputting the overall F-score, it produced
a confusion matrix for the three prediction classes
(positive, negative, and objective), and it also vali-
dated the data submission format.
5 Participants and Results
The results for subtask A are shown in Tables 7 and
8 for Twitter and for SMS messages, respectively;
those for subtask B are shown in Table 9 for Twit-
ter and in Table 10 for SMS messages. Systems are
ranked by their scores for the constrained runs; the
ranking based on scores for unconstrained runs is
shown as a subindex.
For both subtasks, there were teams that only sub-
mitted results for the Twitter test set. Some teams
submitted both a constrained and an unconstrained
version (e.g., AVAYA and teragram). As one would
expect, the results on the Twitter test set tended to be
better than those on the SMS test set since the SMS
data was out-of-domain with respect to the training
(Twitter) data.
Moreover, the results for subtask A were signifi-
cantly better than those for subtask B, which shows
that it is a much easier task, probably because there
is less ambiguity at the phrase-level.
5.1 Subtask A: Contextual Polarity
Table 7 shows that subtask A, Twitter, attracted 23
teams, who submitted 21 constrained and 7 uncon-
strained systems. Five teams submitted both a con-
strained and an unconstrained system, and two other
teams submitted constrained systems that are on
the boundary between being constrained and uncon-
strained.
316
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 88.93 yes yes
AVAYA 86.98 87.38(1) yes yes
BOUNCE 86.79 yes yes
LVIC-LIMSI 85.70 yes yes
FBM 85.50 yes semi
GU-MLT-LT 85.19 yes yes
UNITOR 84.60 yes yes
USNA 81.31 yes yes
Serendio 80.04 yes yes
ECNUCS 79.48 80.15(2) yes yes
TJP 78.16 yes yes
?columbia-nlp 74.94 yes yes
teragram 74.89(3) yes yes
sielers 74.41 yes yes
KLUE 73.74 yes yes
OPTWIMA 69.17 36.91(6) yes yes
swatcs 67.19 63.86(5) no yes
Kea 63.94 yes yes
senti.ue-en 62.79 71.38(4) yes yes
uottawa 60.20 yes yes
IITB 54.80 yes yes
SenselyticTeam 53.88 yes yes
SU-sentilab 34.73(7) no yes
Majority Baseline 38.10 N/A N/A
Table 7: Results for subtask A on the Twitter dataset. The
? marks a team that includes a task coorganizer, and the
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
One system was semi-supervised, and the rest
were supervised. The supervised systems used clas-
sifiers such as SVM (8 systems), Naive Bayes (7 sys-
tems), and Maximum Entropy (3 systems). Other
approaches used include an ensemble of classifiers,
manual rules, and a linear classifier. Two of the sys-
tems chose not to predict neutral as a possible clas-
sification label.
The average F1-measure on the Twitter test set
was 74.1% for constrained systems and 60.5% for
unconstrained ones; this does not mean that using
additional data does not help, it just shows that the
best teams only participated with a constrained sys-
tem. NRC-Canada had the best constrained system
with an F1-measure of 88.9%, and AVAYA had the
best unconstrained one with F1=87.4%.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
GU-MLT-LT 88.37 yes yes
NRC-Canada 88.00 yes yes
?AVAYA 83.94 85.79(1) yes yes
UNITOR 82.49 yes yes
TJP 81.23 yes yes
LVIC-LIMSI 80.16 yes yes
USNA 79.82 yes yes
ECNUCS 76.69 77.34(2) yes yes
sielers 73.48 yes yes
FBM 72.95 no semi
teragram 72.83 72.83(4) yes yes
KLUE 70.54 yes yes
?columbia-nlp 70.30 yes yes
senti.ue-en 66.09 74.13(3) yes yes
swatcs 66.00 67.68(5) no yes
Kea 63.27 yes yes
uottawa 55.89 yes yes
SU-sentilab 55.38(6) no yes
SenselyticTeam 51.13 yes yes
OPTWIMA 37.32 36.38(7) yes yes
Majority Baseline 31.50 N/A N/A
Table 8: Results for subtask A on the SMS dataset. The
? indicates a late submission, the ? marks a team that
includes a task co-organizer, and the  indicates a sys-
tem submitted as constrained but which used additional
Tweets or additional sentiment-annotated text to collect
statistics that were then used as a feature.
Table 8 shows the results for the SMS test set,
where 20 teams submitted 19 constrained and 7 un-
constrained systems (again, this included two teams
that submitted boundary systems, marked accord-
ingly). The average F-measure on this test set
was 70.8% for constrained systems and 65.7% for
unconstrained systems. The best constrained sys-
tem was that of GU-MLT-LT with an F-measure of
88.4%, and AVAYA had the best unconstrained sys-
tem with an F1 of 85.8%.
5.2 Subtask B: Message Polarity
Table 9 shows that subtask B, Twitter, attracted 38
teams, who submitted 36 constrained and 15 uncon-
strained systems (and two boundary ones).
The average F1-measure was 53.7% for the con-
strained and 54.6% for the unconstrained systems.
317
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 69.02 yes yes
GU-MLT-LT 65.27 yes yes
teragram 64.86 64.86(1) yes yes
BOUNCE 63.53 yes yes
KLUE 63.06 yes yes
AMI&ERIC 62.55 61.17(3) yes yes/semi
FBM 61.17 yes yes
AVAYA 60.84 64.06(2) yes yes/semi
SAIL 60.14 61.03(4) yes yes
UT-DB 59.87 yes yes
FBK-irst 59.76 yes yes
nlp.cs.aueb.gr 58.91 yes yes
UNITOR 58.27 59.50(5) yes semi
LVIC-LIMSI 57.14 yes yes
Umigon 56.96 yes yes
NILC USP 56.31 yes yes
DataMining 55.52 yes semi
ECNUCS 55.05 58.42(6) yes yes
nlp.cs.aueb.gr 54.73 yes yes
ASVUniOfLeipzig 54.56 yes yes
SZTE-NLP 54.33 53.10(9) yes yes
CodeX 53.89 yes yes
Oasis 53.84 yes yes
NTNU 53.23 50.71(10) yes yes
UoM 51.81 45.07(15) yes yes
SSA-UO 50.17 yes no
SenselyticTeam 50.10 yes yes
UMCC DLSI (SA) 49.27 48.99(12) yes yes
bwbaugh 48.83 54.37(8) yes yes/semi
senti.ue-en 47.24 47.85(13) yes yes
SU-sentilab 45.75(14) yes yes
OPTWIMA 45.40 54.51(7) yes yes
REACTION 45.01 yes yes
uottawa 42.51 yes yes
IITB 39.80 yes yes
IIRG 34.44 yes yes
sinai 16.28 49.26(11) yes yes
Majority Baseline 29.19 N/A N/A
Table 9: Results for subtask B on the Twitter dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
These averages are much lower than those for sub-
task A, which indicates that subtask B is harder,
probably because a message can contain parts ex-
pressing both positive and negative sentiment.
Run Const- Unconst- Use Super-
rained rained Neut.? vised?
NRC-Canada 68.46 yes yes
GU-MLT-LT 62.15 yes yes
KLUE 62.03 yes yes
AVAYA 60.00 59.47(1) yes yes/semi
teragram 59.10(2) yes yes
NTNU 57.97 54.55(6) yes yes
CodeX 56.70 yes yes
FBK-irst 54.87 yes yes
AMI&ERIC 53.63 52.62(7) yes yes/semi
ECNUCS 53.21 54.77(5) yes yes
UT-DB 52.46 yes yes
SAIL 51.84 51.98(8) yes yes
UNITOR 51.22 48.88(10) yes semi
SZTE-NLP 51.08 55.46(3) yes yes
SenselyticTeam 51.07 yes yes
NILC USP 50.12 yes yes
REACTION 50.11 yes yes
SU-sentilab 49.57(9) no yes
nlp.cs.aueb.gr 49.41 55.28(4) yes yes
LVIC-LIMSI 49.17 yes yes
FBM 47.40 yes yes
ASVUniOfLeipzig 46.50 yes yes
senti.ue-en 44.65 46.72(12) yes yes
SSA UO 44.39 yes no
UMCC DLSI (SA) 43.39 40.67(14) yes yes
UoM 42.22 35.22(15) yes yes
OPTWIMA 40.98 47.15(11) yes yes
uottawa 40.51 yes yes
bwbaugh 39.73 43.43(13) yes yes/semi
IIRG 22.16 yes yes
Majority Baseline 19.03 N/A N/A
Table 10: Results for subtask B on the SMS dataset. The
 indicates a system submitted as constrained but which
used additional Tweets or additional sentiment-annotated
text to collect statistics that were then used as a feature.
Once again, NRC-Canada had the best con-
strained system with an F1-measure of 69%, fol-
lowed by teragram, which had the best uncon-
strained system with an F1-measure of 64.9%.
As Table 10 shows, the average F1-measure on
the SMS test set was 50.2% for constrained and
50.3% for unconstrained systems. NRC-Canada had
the best constrained system with an F1=68.5%, and
AVAYA had the best unconstrained one with F1-
measure of 59.5%.
318
5.3 Overall
Overall, the results achieved by the best teams were
very strong, especially for the simpler subtask A:
? F1=88.93, NRC-Canada on subtask A, Twitter;
? F1=88.37, GU-MLT-LT on subtask A, SMS;
? F1=69.02, NRC-Canada on subtask B, Twitter;
? F1=68.46, NRC-Canada on subtask B, SMS.
We can see that the strongest team overall was that
of NRC-Canada, which was ranked first on three of
the four conditions; and it was second on subtask A,
SMS. There were two other teams that were strong
across both tasks and on both test sets: GU-MLT-LT
and AVAYA. Three other teams, namely teragram,
BOUNCE and KLUE, were ranked in the top-3 in at
least one subtask and test set.
6 Discussion
We have seen that most participants restricted them-
selves to the provided data and submitted con-
strained systems. Indeed, the best systems for each
of the two subtasks and for each of the two testing
datasets were constrained systems; of course, this
does not mean that additional data would not be use-
ful. Curiously, in some cases where a team submit-
ted a constrained and unconstrained run, the uncon-
strained run actually performed worse.
Not surprisingly, most systems were supervised;
there were only five semi-supervised systems, and
there was only one unsupervised system. One ad-
ditional team declared their system as unsupervised
since it was not making use of the training data; we
still classified it as supervised though since it did use
supervision ? in the form of manual rules.
Most participants predicted all three labels (posi-
tive, negative and neutral), even though some partic-
ipants opted for not predicting neutral, which made
some sense since the final F1-score was averaged
over the positive and the negative predictions only.
The most popular classifiers included SVM, Max-
Ent, linear classifier, Naive Bayes; in some cases,
manual rules or ensembles of classifiers were used.
A variety of features were used, including word-
related (e.g., words, stems, n-grams, word clus-
ters), word-shape (e.g., punctuation, capitalization),
syntactic (e.g., POS tags, dependency relations),
Twitter-specific (e.g., repeated characters, emoti-
cons, URLs, hashtags, slang, abbreviations), and
sentiment-related (e.g., negation); one team also
used discourse relations. Almost all participants re-
lied heavily of various sentiment lexicons, the most
popular ones being MPQA and SentiWordNet, as
well as AFINN and Bing Liu?s Opinion Lexicon;
some participants used their own lexicons ? preex-
isting or built from the provided data.
Given that Twitter messages are noisy, most par-
ticipants did some preprocessing, including tok-
enization, stemming, lemmatization, stopword re-
moval, normalization/removal of URLs, hashtags,
users, slang, emoticons, repeated vowels, punctua-
tion; some even did pronoun resolution.
7 Conclusion
We have described a new task that entered SemEval-
2013: task 2 on Sentiment Analysis on Twitter. The
task has attracted a very high number of participants:
149 submissions from 44 teams.
We believe that the datasets that we have created
as part of the task and which we have released to the
community5 under a Creative Commons Attribution
3.0 Unported License,6 will be found useful by re-
searchers beyond SemEval.
Acknowledgments
The authors would like to thank Kathleen McKeown
for her insight in creating the Amazon Mechanical
Turk annotation task.
Funding for the Amazon Mechanical Turk anno-
tations was provided by the JHU Human Language
Technology Center of Excellence and by the Of-
fice of the Director of National Intelligence (ODNI),
Intelligence Advanced Research Projects Activity
(IARPA), through the U.S. Army Research Lab. All
statements of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
5http://www.cs.york.ac.uk/semeval-2013/task2/
6http://creativecommons.org/licenses/by/3.0/
319
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Nicoletta Calzolari (chair), Khalid Choukri,
Bente Maegaard, Joseph Mariani, Jan Odijk, Stelios
Piperidis, Mike Rosner, and Daniel Tapias, editors,
Proceedings of the Seventh International Conference
on Language Resources and Evaluation, LREC ?10,
pages 2200?2204, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on Twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and
Ricard Gavalda`. 2011. Detecting sentiment change in
Twitter streaming data. Journal of Machine Learning
Research - Proceedings Track, 17:5?11.
Tao Chen and Min-Yen Kan. 2012. Creating a live, pub-
lic short message service corpus: the NUS SMS cor-
pus. Language Resources and Evaluation, pages 1?
37.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
Twitter and Amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ?10, pages 107?116, Upp-
sala, Sweden.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The Good
the Bad and the OMG! In Lada A. Adamic, Ricardo A.
Baeza-Yates, and Scott Counts, editors, Proceedings of
the Fifth International Conference on Weblogs and So-
cial Media, ICWSM? 11, pages 538?541, Barcelona,
Spain.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From tweets to polls: Linking text sentiment to
public opinion time series. In William W. Cohen and
Samuel Gosling, editors, Proceedings of the Fourth
International Conference on Weblogs and Social
Media, ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter for disambiguating senti-
ment ambiguous adjectives. In Proceedings of the 5th
International Workshop on Semantic Evaluation, Se-
mEval ?10, pages 436?439, Los Angeles, CA, USA.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: an exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?11, pages 1524?1534, Edinburgh, United
Kingdom.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD ?12, pages 1104?1112, Beijing, China.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about po-
litical sentiment. In William W. Cohen and Samuel
Gosling, editors, Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, pages 178?185, Washington, DC, USA.
The AAAI Press.
Julio Villena-Roma?n, Sara Lana-Serrano, Euge-
nio Mart??nez-Ca?mara, and Jose? Carlos Gonza?lez
Cristo?bal. 2013. TASS - Workshop on Sentiment
Analysis at SEPLN. Procesamiento del Lenguaje
Natural, 50:37?44.
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.
Annotating expressions of opinions and emotions in
language. Language Resources and Evaluation, 39(2-
3):165?210.
320
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 73?80,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 9: Sentiment Analysis in Twitter
Sara Rosenthal
Columbia University
sara@cs.columbia.edu
Preslav Nakov
Qatar Computing Research Institute
pnakov@qf.org.qa
Alan Ritter
Carnegie Mellon University
rittera@cs.cmu.edu
Veselin Stoyanov
Johns Hopkins University
ves@cs.jhu.edu
Abstract
We describe the Sentiment Analysis in
Twitter task, ran as part of SemEval-2014.
It is a continuation of the last year?s task
that ran successfully as part of SemEval-
2013. As in 2013, this was the most popu-
lar SemEval task; a total of 46 teams con-
tributed 27 submissions for subtask A (21
teams) and 50 submissions for subtask B
(44 teams). This year, we introduced three
new test sets: (i) regular tweets, (ii) sarcas-
tic tweets, and (iii) LiveJournal sentences.
We further tested on (iv) 2013 tweets, and
(v) 2013 SMS messages. The highest F1-
score on (i) was achieved by NRC-Canada
at 86.63 for subtask A and by TeamX at
70.96 for subtask B.
1 Introduction
In the past decade, new forms of communica-
tion have emerged and have become ubiquitous
through social media. Microblogs (e.g., Twitter),
Weblogs (e.g., LiveJournal) and cell phone mes-
sages (SMS) are often used to share opinions and
sentiments about the surrounding world, and the
availability of social content generated on sites
such as Twitter creates new opportunities to au-
tomatically study public opinion.
Working with these informal text genres
presents new challenges for natural language pro-
cessing beyond those encountered when work-
ing with more traditional text genres such as
newswire. The language in social media is very
informal, with creative spelling and punctuation,
misspellings, slang, new words, URLs, and genre-
specific terminology and abbreviations, e.g., RT
for re-tweet and #hashtags
1
.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
Hashtags are a type of tagging for Twitter messages.
Moreover, tweets and SMS messages are short:
a sentence or a headline rather than a document.
How to handle such challenges so as to automat-
ically mine and understand people?s opinions and
sentiments has only recently been the subject of
research (Jansen et al., 2009; Barbosa and Feng,
2010; Bifet et al., 2011; Davidov et al., 2010;
O?Connor et al., 2010; Pak and Paroubek, 2010;
Tumasjan et al., 2010; Kouloumpis et al., 2011).
Several corpora with detailed opinion and sen-
timent annotation have been made freely avail-
able, e.g., the MPQA newswire corpus (Wiebe et
al., 2005), the movie reviews corpus (Pang et al.,
2002), or the restaurant and laptop reviews cor-
pora that are part of this year?s SemEval Task 4
(Pontiki et al., 2014). These corpora have proved
very valuable as resources for learning about the
language of sentiment in general, but they do not
focus on tweets. While some Twitter sentiment
datasets were created prior to SemEval-2013, they
were either small and proprietary, such as the i-
sieve corpus (Kouloumpis et al., 2011) or focused
solely on message-level sentiment.
Thus, the primary goal of our SemEval task is
to promote research that will lead to better un-
derstanding of how sentiment is conveyed in So-
cial Media. Toward that goal, we created the Se-
mEval Tweet corpus as part of our inaugural Sen-
timent Analysis in Twitter Task, SemEval-2013
Task 2 (Nakov et al., 2013). It contains tweets
and SMS messages with sentiment expressions an-
notated with contextual phrase-level and message-
level polarity. This year, we extended the corpus
by adding new tweets and LiveJournal sentences.
Another interesting phenomenon that has been
studied in Twitter is the use of the #sarcasm hash-
tag to indicate that a tweet should not be taken lit-
erally (Gonz?alez-Ib?a?nez et al., 2011; Liebrecht et
al., 2013). In fact, sarcasm indicates that the mes-
sage polarity should be flipped. With this in mind,
this year, we also evaluate on sarcastic tweets.
73
In the remainder of this paper, we first describe
the task, the dataset creation process and the eval-
uation methodology. We then summarize the char-
acteristics of the approaches taken by the partici-
pating systems, and we discuss their scores.
2 Task Description
As SemEval-2013 Task 2, we included two sub-
tasks: an expression-level subtask and a message-
level subtask. Participants could choose to partici-
pate in either or both. Below we provide short de-
scriptions of the objectives of these two subtasks.
Subtask A: Contextual Polarity Disambiguation
Given a message containing a marked in-
stance of a word or a phrase, determine
whether that instance is positive, negative or
neutral in that context. The instance bound-
aries were provided: this was a classification
task, not an entity recognition task.
Subtask B: Message Polarity Classification
Given a message, decide whether it is of
positive, negative, or neutral sentiment.
For messages conveying both positive and
negative sentiment, the stronger one is to be
chosen.
Each participating team was allowed to submit
results for two different systems per subtask: one
constrained, and one unconstrained. A constrained
system could only use the provided data for train-
ing, but it could also use other resources such as
lexicons obtained elsewhere. An unconstrained
system could use any additional data as part of
the training process; this could be done in a super-
vised, semi-supervised, or unsupervised fashion.
Note that constrained/unconstrained refers to
the data used to train a classifier. For example,
if other data (excluding the test data) was used to
develop a sentiment lexicon, and the lexicon was
used to generate features, the system would still
be constrained. However, if other data (excluding
the test data) was used to develop a sentiment lexi-
con, and this lexicon was used to automatically la-
bel additional Tweet/SMS messages and then used
with the original data to train the classifier, then
such a system would be considered unconstrained.
3 Datasets
In this section, we describe the process of collect-
ing and annotating the 2014 testing tweets, includ-
ing the sarcastic ones, and LiveJournal sentences.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 5,895 3,131 471
Twitter2013-dev 648 430 57
Twitter2013-test 2,734 1,541 160
SMS2013-test 1,071 1,104 159
Twitter2014-test 1,807 578 88
Twitter2014-sarcasm 82 37 5
LiveJournal2014-test 660 511 144
Table 1: Dataset statistics for Subtask A.
3.1 Datasets Used
For training and development, we released the
Twitter train/dev/test datasets from SemEval-2013
task 2, as well as the SMS test set, which uses mes-
sages from the NUS SMS corpus (Chen and Kan,
2013), which we annotated for sentiment in 2013.
We further added a new 2014 Twitter test set,
as well as a small set of tweets that contained
the #sarcasm hashtag to determine how sarcasm
affects the tweet polarity. Finally, we included
sentences from LiveJournal in order to determine
how systems trained on Twitter perform on other
sources. The statistics for each dataset and for
each subtask are shown in Tables 1 and 2.
Corpus Positive Negative Objective
/ Neutral
Twitter2013-train 3,662 1,466 4,600
Twitter2013-dev 575 340 739
Twitter2013-test 1,572 601 1,640
SMS2013-test 492 394 1,207
Twitter2014-test 982 202 669
Twitter2014-sarcasm 33 40 13
LiveJournal2014-test 427 304 411
Table 2: Dataset statistics for Subtask B.
3.2 Annotation
We annotated the new tweets as in 2013: by iden-
tifying tweets from popular topics that contain
sentiment-bearing words by using SentiWordNet
(Baccianella et al., 2010) as a filter. We altered the
annotation task for the sarcastic tweets, displaying
them to the Mechanical Turk annotators without
the #sarcasm hashtag; the Turkers had to deter-
mine whether the tweet is sarcastic on their own.
Moreover, we asked Turkers to indicate the degree
of sarcasm as (a) definitely sarcastic, (b) probably
sarcastic, and (c) not sarcastic.
As in 2013, we combined the annotations using
intersection, where a word had to appear in 2/3
of the annotations to be accepted. An annotated
example from each source is shown in Table 3.
74
Source Example Polarity
Twitter Why would you [still]- wear shorts when it?s this cold?! I [love]+ how Britain see?s a
bit of sun and they?re [like ?OOOH]+ LET?S STRIP!?
positive
SMS [Sorry]- I think tonight [cannot]- and I [not feeling well]- after my rest. negative
LiveJournal [Cool]+ posts , dude ; very [colorful]+ , and [artsy]+ . positive
Twitter Sarcasm [Thanks]+ manager for putting me on the schedule for Sunday negative
Table 3: Example of polarity for each source of messages. The target phrases are marked in [. . .], and
are followed by their polarity; the sentence-level polarity is shown in the last column.
3.3 Tweets Delivery
We did not deliver the annotated tweets to the par-
ticipants directly; instead, we released annotation
indexes, a list of corresponding Twitter IDs, and
a download script that extracts the correspond-
ing tweets via the Twitter API.
2
We provided the
tweets in this manner in order to ensure that Twit-
ter?s terms of service are not violated. Unfor-
tunately, due to this restriction, the task partici-
pants had access to different number of training
tweets depending on when they did the download-
ing. This varied between a minimum of 5,215
tweets and the full set of 10,882 tweets. On av-
erage the teams were able to collect close to 9,000
tweets; for teams that did not participate in 2013,
this was about 8,500. The difference in training
data size did not seem to have had a major impact.
In fact, the top two teams in subtask B (coooolll
and TeamX) trained on less than 8,500 tweets.
4 Scoring
The participating systems were required to per-
form a three-way classification for both subtasks.
A particular marked phrase (for subtask A) or an
entire message (for subtask B) was to be classi-
fied as positive, negative or objective/neutral. We
scored the systems by computing a score for pre-
dicting positive/negative phrases/messages. For
instance, to compute positive precision, p
pos
, we
find the number of phrases/messages that a sys-
tem correctly predicted to be positive, and we di-
vide that number by the total number it predicted
to be positive. To compute positive recall, r
pos
,
we find the number of phrases/messages correctly
predicted to be positive and we divide that number
by the total number of positives in the gold stan-
dard. We then calculate F1-score for the positive
class as follows F
pos
=
2(p
pos
+r
pos
)
p
pos
?r
pos
. We carry
out a similar computation for F
neg
, for the nega-
tive phrases/messages. The overall score is then
F = (F
pos
+ F
neg
)/2.
2
https://dev.twitter.com
We used the two test sets from 2013 and the
three from 2014, which we combined into one test
set and we shuffled to make it hard to guess which
set a sentence came from. This guaranteed that
participants would submit predictions for all five
test sets. It also allowed us to test how well sys-
tems trained on standard tweets generalize to sar-
castic tweets and to LiveJournal sentences, with-
out the participants putting extra efforts into this.
The participants were also not informed about the
source the extra test sets come from.
We provided the participants with a scorer that
outputs the overall score F and a confusion matrix
for each of the five test sets.
5 Participants and Results
The results are shown in Tables 4 and 5, and the
team affiliations are shown in Table 6. Tables 4
and 5 contain results on the two progress test sets
(tweets and SMS messages), which are the official
test sets from the 2013 edition of the task, and on
the three new official 2014 testsets (tweets, tweets
with sarcasm, and LiveJournal). The tables fur-
ther show macro- and micro-averaged results over
the 2014 datasets. There is an index for each re-
sult showing the relative rank of that result within
the respective column. The participating systems
are ranked by their score on the Twitter-2014 test-
set, which is the official ranking for the task; all
remaining rankings are secondary.
As we mentioned above, the participants were
not told that the 2013 test sets would be included
in the big 2014 test set, so that they do not over-
tune their systems on them. However, the 2013
test sets were made available for development, but
it was explicitly forbidden to use them for training.
Still, some participants did not notice this restric-
tion, which resulted in their unusually high scores
on Twitter2013-test; we did our best to identify
all such cases, and we asked the authors to submit
corrected runs. The tables mark such resubmis-
sions accordingly.
75
Most of the submissions were constrained, with
just a few unconstrained: 7 out of 27 for subtask
A, and 8 out of 50 for subtask B. In any case, the
best systems were constrained. Some teams par-
ticipated with both a constrained and an uncon-
strained system, but the unconstrained system was
not always better than the constrained one: some-
times it was worse, sometimes it performed the
same. Thus, we decided to produce a single rank-
ing, including both constrained and unconstrained
systems, where we mark the latter accordingly.
5.1 Subtask A
Table 4 shows the results for subtask A, which at-
tracted 27 submissions from 21 teams. There were
seven unconstrained submissions: five teams sub-
mitted both a constrained and an unconstrained
run, and two teams submitted an unconstrained
run only. The best systems were constrained. All
participating systems outperformed the majority
class baseline by a sizable margin.
5.2 Subtask B
The results for subtask B are shown in Table 5.
The subtask attracted 50 submissions from 44
teams. There were eight unconstrained submis-
sions: six teams submitted both a constrained and
an unconstrained run, and two teams submitted an
unconstrained run only. As for subtask A, the best
systems were constrained. Again, all participating
systems outperformed the majority class baseline;
however, some systems were very close to it.
6 Discussion
Overall, we observed similar trends as in
SemEval-2013 Task 2. Almost all systems used
supervised learning. Most systems were con-
strained, including the best ones in all categories.
As in 2013, we observed several cases of a team
submitting a constrained and an unconstrained run
and the constrained run performing better.
It is unclear why unconstrained systems did not
outperform constrained ones. It could be because
participants did not use enough external data or
because the data they used was too different from
Twitter or from our annotation method. Or it could
be due to our definition of unconstrained, which
labels as unconstrained systems that use additional
tweets directly, but considers unconstrained those
that use additional tweets to build sentiment lexi-
cons and then use these lexicons.
As in 2013, the most popular classifiers were
SVM, MaxEnt, and Naive Bayes. Moreover, two
submissions used deep learning, coooolll (Harbin
Institute of Technology) and ThinkPositive (IBM
Research, Brazil), which were ranked second and
tenth on subtask B, respectively.
The features used were quite varied, includ-
ing word-based (e.g., word and character n-
grams, word shapes, and lemmata), syntactic, and
Twitter-specific such as emoticons and abbrevia-
tions. The participants still relied heavily on lex-
icons of opinion words, the most popular ones
being the same as in 2013: MPQA, SentiWord-
Net and Bing Liu?s opinion lexicon. Popular this
year was also the NRC lexicon (Mohammad et
al., 2013), created by the best-performing team in
2013, which is top-performing this year as well.
Preprocessing of tweets was still a popular tech-
nique. In addition to standard NLP steps such
as tokenization, stemming, lemmatization, stop-
word removal and POS tagging, most teams ap-
plied some kind of Twitter-specific processing
such as substitution/removal of URLs, substitu-
tion of emoticons, word normalization, abbrevi-
ation lookup, and punctuation removal. Finally,
several of the teams used Twitter-tuned NLP tools
such as part of speech and named entity taggers
(Gimpel et al., 2011; Ritter et al., 2011).
The similarity of preprocessing techniques,
NLP tools, classifiers and features used in 2013
and this year is probably partially due to many
teams participating in both years. As Table 6
shows, 18 out of the 46 teams are returning teams.
Comparing the results on the progress Twit-
ter test in 2013 and 2014, we can see that NRC-
Canada, the 2013 winner for subtask A, have
now improved their F1 score from 88.93 to 90.14,
which is the 2014 best score. The best score on the
Progress SMS in 2014 of 89.31 belongs to ECNU;
this is a big jump compared to their 2013 score of
76.69, but it is less compared to the 2013 best of
88.37 achieved by GU-MLT-LT. For subtask B, on
the Twitter progress testset, the 2013 winner NRC-
Canada improves their 2013 result from 69.02 to
70.75, which is the second best in 2014; the win-
ner in 2014, TeamX, achieves 72.12. On the SMS
progress test, the 2013 winner NRC-Canada im-
proves its F1 score from 68.46 to 70.28. Overall,
we see consistent improvements on the progress
testset for both subtasks: 0-1 and 2-3 points abso-
lute for subtasks A and B, respectively.
76
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 NRC-Canada 90.14
1
88.03
4
86.63
1
77.13
5
85.49
2
83.08
2
85.61
1
2 SentiKLUE 90.11
2
85.16
8
84.83
2
79.32
3
85.61
1
83.25
1
85.15
2
3 CMUQ-Hybrid
?
88.94
4
87.98
5
84.40
3
76.99
6
84.21
3
81.87
3
84.05
3
4 CMU-Qatar
?
89.85
3
88.08
3
83.45
4
78.07
4
83.89
5
81.80
4
83.56
4
5 ECNU X 87.29
6
89.26
2
82.93
5
73.71
8
81.69
7
79.44
7
81.85
6
6 ECNU 87.28
7
89.31
1
82.67
6
73.71
9
81.67
8
79.35
8
81.75
7
7 Think Positive X 88.06
5
87.65
6
82.05
7
76.74
7
80.90
12
79.90
6
81.15
9
8 Kea
?
84.83
10
84.14
10
81.22
8
65.94
17
81.16
11
76.11
13
80.70
10
9 Lt 3 86.28
8
85.26
7
81.02
9
70.76
13
80.44
13
77.41
11
80.33
13
10 senti.ue 84.05
11
78.72
16
80.54
10
82.75
1
81.90
6
81.73
5
81.47
8
11 LyS 85.69
9
81.44
12
79.92
11
71.67
10
83.95
4
78.51
10
82.21
5
12 UKPDIPF 80.45
15
79.05
14
79.67
12
65.63
18
81.42
9
75.57
14
80.33
11
13 UKPDIPF X 80.45
16
79.05
15
79.67
13
65.63
19
81.42
10
75.57
15
80.33
12
14 TJP 81.13
14
84.41
9
79.30
14
71.20
12
78.27
15
76.26
12
78.39
15
15 SAP-RI 80.32
17
80.26
13
77.26
15
70.64
14
77.68
18
75.19
17
77.32
16
16 senti.ue
?
X 83.80
12
82.93
11
77.07
16
80.02
2
79.70
14
78.93
9
78.83
14
17 SAIL 78.47
18
74.46
20
76.89
17
65.56
20
70.62
22
71.02
21
72.57
21
18 columbia nlp

81.50
13
74.55
19
76.54
18
61.76
22
78.19
16
72.16
19
77.11
18
19 IIT-Patna 76.54
20
75.99
18
76.43
19
71.43
11
77.99
17
75.28
16
77.26
17
20 Citius X 76.59
19
69.31
21
75.21
20
68.40
15
75.82
20
73.14
18
75.38
19
21 Citius 74.71
21
61.44
25
73.03
21
65.18
21
71.64
21
69.95
22
71.90
22
22 IITPatna 70.91
23
77.04
17
72.25
22
66.32
16
76.03
19
71.53
20
74.45
20
23 SU-sentilab 74.34
22
62.58
24
68.26
23
53.31
25
69.53
23
63.70
24
68.59
23
24 Univ. Warwick
?
62.25
26
60.12
26
67.28
24
58.08
24
64.89
25
63.42
25
65.48
25
25 Univ. Warwick
?
X 64.91
25
63.01
23
67.17
25
60.59
23
67.46
24
65.07
23
67.14
24
26 DAEDALUS 67.42
24
63.92
22
60.98
26
45.27
27
61.01
26
55.75
26
60.50
26
27 DAEDALUS X 61.95
27
55.97
27
58.11
27
49.19
26
58.65
27
55.32
27
58.17
27
Majority baseline 38.1 31.5 42.2 39.8 33.4
Table 4: Results for subtask A. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
Finally, note that for both subtasks, the best sys-
tems on the Twitter-2014 dataset are those that per-
formed best on the 2013 progress Twitter dataset:
NRC-Canada for subtask A, and TeamX (Fuji Xe-
rox Co., Ltd.) for subtask B.
It is interesting to note that the best results
for Twitter2014-test are lower than those for
Twitter2013-test for both subtask A (86.63 vs.
90.14) and subtask B (70.96 vs 72.12). This is
so despite the baselines for Twitter2014-test be-
ing higher than those for Twitter2013-test: 42.2 vs.
38.1 for subtask A, and 34.6 vs. 29.2 for subtask
B. Most likely, having access to Twitter2013-test
at development time, teams have overfitted on it. It
could be also the case that some of the sentiment
dictionaries that were built in 2013 have become
somewhat outdated by 2014.
Finally, note that while some teams such as
NRC-Canada performed well across all test sets,
other such as TeamX, which used a weighting
scheme tuned specifically for class imbalances in
tweets, were only strong on Twitter datasets.
7 Conclusion
We have described the data, the experimental
setup and the results for SemEval-2014 Task 9.
As in 2013, our task was the most popular one at
SemEval-2014, attracting 46 participating teams:
21 in subtask A (27 submissions) and 44 in sub-
task B (50 submissions).
We introduced three new test sets for 2014: an
in-domain Twitter dataset, an out-of-domain Live-
Journal test set, and a dataset of tweets contain-
ing sarcastic content. While the performance on
the LiveJournal test set was mostly comparable
to the in-domain Twitter test set, for most teams
there was a sharp drop in performance for sarcas-
tic tweets, highlighting better handling of sarcas-
tic language as one important direction for future
work in Twitter sentiment analysis.
We plan to run the task again in 2015 with the
inclusion of a new sub-evaluation on detecting sar-
casm with the goal of stimulating research in this
area; we further plan to add one more test domain.
77
Uncon- 2013: Progress 2014: Official 2014: Average
# System strain.? Tweet SMS Tweet Tweet Live- Macro Micro
sarcasm Journal
1 TeamX 72.12
1
57.36
26
70.96
1
56.50
3
69.44
15
65.63
3
69.99
5
2 coooolll 70.40
3
67.68
2
70.14
2
46.66
24
72.90
5
63.23
12
70.51
2
3 RTRGO 69.10
5
67.51
3
69.95
3
47.09
23
72.20
6
63.08
13
70.15
3
4 NRC-Canada 70.75
2
70.28
1
69.85
4
58.16
1
74.84
1
67.62
1
71.37
1
5 TUGAS 65.64
13
62.77
11
69.00
5
52.87
12
69.79
13
63.89
6
68.84
8
6 CISUC KIS
?
67.56
8
65.90
6
67.95
6
55.49
5
74.46
2
65.97
2
70.02
4
7 SAIL 66.80
11
56.98
28
67.77
7
57.26
2
69.34
17
64.79
4
68.06
10
8 SWISS-CHOCOLATE 64.81
18
66.43
5
67.54
8
49.46
16
73.25
4
63.42
10
69.15
6
9 Synalp-Empathic 63.65
23
62.54
12
67.43
9
51.06
15
71.75
9
63.41
11
68.57
9
10 Think Positive X 68.15
7
63.20
9
67.04
10
47.85
21
66.96
24
60.62
18
66.47
15
11 SentiKLUE 69.06
6
67.40
4
67.02
11
43.36
30
73.99
3
61.46
14
68.94
7
12 JOINT FORCES X 66.61
12
62.20
13
66.79
12
45.40
26
70.02
12
60.74
17
67.39
12
13 AMI ERIC 70.09
4
60.29
20
66.55
13
48.19
20
65.32
26
60.02
21
65.58
20
14 AUEB 63.92
21
64.32
8
66.38
14
56.16
4
70.75
11
64.43
5
67.71
11
15 CMU-Qatar
?
65.11
17
62.95
10
65.53
15
40.52
38
65.63
25
57.23
27
64.87
24
16 Lt 3 65.56
14
64.78
7
65.47
16
47.76
22
68.56
20
60.60
19
66.12
17
17 columbia nlp

64.60
19
59.84
21
65.42
17
40.02
40
68.79
19
58.08
25
65.96
19
18 LyS 66.92
10
60.45
19
64.92
18
42.40
33
69.79
14
59.04
22
66.10
18
19 NILC USP 65.39
15
61.35
16
63.94
19
42.06
34
69.02
18
58.34
24
65.21
21
20 senti.ue 67.34
9
59.34
23
63.81
20
55.31
6
71.39
10
63.50
7
66.38
16
21 UKPDIPF 60.65
29
60.56
17
63.77
21
54.59
7
71.92
7
63.43
8
66.53
13
22 UKPDIPF X 60.65
30
60.56
18
63.77
22
54.59
8
71.92
8
63.43
9
66.53
14
23 SU-FMI
?
60.96
28
61.67
15
63.62
23
48.34
19
68.24
21
60.07
20
64.91
23
24 ECNU 62.31
27
59.75
22
63.17
24
51.43
14
69.44
16
61.35
15
65.17
22
25 ECNU X 63.72
22
56.73
29
63.04
25
49.33
17
64.08
31
58.82
23
63.04
27
26 Rapanakis 58.52
32
54.02
35
63.01
26
44.69
27
59.71
37
55.80
31
61.28
32
27 Citius X 63.25
24
58.28
24
62.94
27
46.13
25
64.54
29
57.87
26
63.06
26
28 CMUQ-Hybrid
?
63.22
25
61.75
14
62.71
28
40.95
37
65.14
27
56.27
30
63.00
28
29 Citius 62.53
26
57.69
25
61.92
29
41.00
36
62.40
33
55.11
33
61.51
31
30 KUNLPLab 58.12
33
55.89
31
61.72
30
44.60
28
63.77
32
56.70
29
62.00
29
31 senti.ue
?
X 65.21
16
56.16
30
61.47
31
54.09
9
68.08
22
61.21
16
63.71
25
32 UPV-ELiRF 63.97
20
55.36
33
59.33
32
37.46
42
64.11
30
53.63
37
60.49
33
33 USP Biocom 58.05
34
53.57
36
59.21
33
43.56
29
67.80
23
56.86
28
61.96
30
34 DAEDALUS X 58.94
31
54.96
34
57.64
34
35.26
44
60.99
35
51.30
39
58.26
35
35 IIT-Patna 52.58
40
51.96
37
57.25
35
41.33
35
60.39
36
52.99
38
57.97
36
36 DejaVu 57.43
36
55.57
32
57.02
36
42.46
32
64.69
28
54.72
34
59.46
34
37 GPLSI 57.49
35
46.63
42
56.06
37
53.90
10
57.32
41
55.76
32
56.47
37
38 BUAP 56.85
37
44.27
44
55.76
38
51.52
13
53.94
44
53.74
36
54.97
39
39 SAP-RI 50.18
44
49.00
41
55.47
39
48.64
18
57.86
40
53.99
35
56.17
38
40 UMCC DLSI Sem 51.96
41
50.01
38
55.40
40
42.76
31
53.12
45
50.43
40
54.20
42
41 IBM EG 54.51
38
46.62
43
52.26
41
34.14
46
59.24
38
48.55
43
54.34
41
42 Alberta 53.85
39
49.05
40
52.06
42
40.40
39
52.38
46
48.28
44
51.85
44
43 lsis lif 46.38
46
38.56
47
52.02
43
34.64
45
61.09
34
49.25
41
54.90
40
44 SU-sentilab 50.17
45
49.60
39
49.52
44
31.49
47
55.11
42
45.37
47
51.09
45
45 SINAI 50.59
42
57.34
27
49.50
45
31.15
49
58.33
39
46.33
46
52.26
43
46 IITPatna 50.32
43
40.56
46
48.22
46
36.73
43
54.68
43
46.54
45
50.29
46
47 Univ. Warwick 39.17
48
29.50
49
45.56
47
39.77
41
39.60
49
41.64
48
43.19
48
48 UMCC DLSI Graph 43.24
47
36.66
48
45.49
48
53.15
11
47.81
47
48.82
42
46.56
47
49 Univ. Warwick X 34.23
50
24.63
50
45.11
49
31.40
48
29.34
50
35.28
49
38.88
49
50 DAEDALUS 36.57
49
40.86
45
33.03
50
28.96
50
40.83
48
34.27
50
35.81
50
Majority baseline 29.2 19.0 34.6 27.7 27.2
Table 5: Results for subtask B. The
?
indicates system resubmissions (because they initially trained on
Twitter2013-test), and the

indicates a system that includes a task co-organizer as a team member. The
systems are sorted by their score on the Twitter2014 test dataset; the rankings on the individual datasets
are indicated with a subscript. The last two columns show macro- and micro-averaged results across the
three 2014 test datasets.
In the 2015 edition of the task, we might also
remove the constrained/unconstrained distinction.
Finally, as there are multiple opinions about a
topic in Twitter, we would like to focus on detect-
ing the sentiment trend towards a topic.
Acknowledgements
We would like to thank Kathleen McKeown and
Smaranda Muresan for funding the 2014 Twitter
test sets. We also thank the anonymous reviewers.
78
Subtasks Team Affiliation 2013?
B Alberta University of Alberta
B AMI ERIC AMI Software R&D and Universit?e de Lyon (ERIC LYON 2) yes
B AUEB Athens University of Economics and Business yes
B BUAP Benem?erita Universidad Aut?onoma de Puebla
B CISUC KIS University of Coimbra
A, B Citius University of Santiago de Compostela
A, B CMU-Qatar Carnegie Mellon University, Qatar
A, B CMUQ-Hybrid Carnegie Mellon University, Qatar (different from the above)
A, B columbia nlp Columbia University yes
B cooolll Harbin Institute of Technology
A, B DAEDALUS Daedalus
B DejaVu Indian Institute of Technology, Kanpur
A, B ECNU East China Normal University yes
B GPLSI University of Alicante
B IBM EG IBM Egypt
A, B IITPatna Indian Institute of Technology, Patna
A, B IIT-Patna Indian Institute of Technology, Patna (different from the above)
B JOINT FORCES Zurich University of Applied Sciences
A Kea York University, Toronto yes
B KUNLPLab Koc? University
B lsis lif Aix-Marseille University yes
A, B Lt 3 Ghent University
A, B LyS Universidade da Coru?na
B NILC USP University of S?ao Paulo yes
A, B NRC-Canada National Research Council Canada yes
B Rapanakis Stamatis Rapanakis
B RTRGO Retresco GmbH and University of Gothenburg yes
A, B SAIL Signal Analysis and Interpretation Laboratory yes
A, B SAP-RI SAP Research and Innovation
A, B senti.ue Universidade de
?
Evora yes
A, B SentiKLUE Friedrich-Alexander-Universit?at Erlangen-N?urnberg yes
B SINAI University of Ja?en yes
B SU-FMI Sofia University
A, B SU-sentilab Sabanci University yes
B SWISS-CHOCOLATE ETH Zurich
B Synalp-Empathic University of Lorraine
B TeamX Fuji Xerox Co., Ltd.
A, B Think Positive IBM Research, Brazil
A TJP University of Northumbria at Newcastle Upon Tyne yes
B TUGAS Instituto de Engenharia de Sistemas e Computadores, yes
Investigac??ao e Desenvolvimento em Lisboa
A, B UKPDIPF Ubiquitous Knowledge Processing Lab
B UMCC DLSI Graph Universidad de Matanzas and Univarsidad de Alicante yes
B UMCC DLSI Sem Universidad de Matanzas and Univarsidad de Alicante (different from above) yes
A, B Univ. Warwick University of Warwick
B UPV-ELiRF Universitat Polit`ecnica de Val`encia
B USP Biocom University of S?ao Paulo and Federal University of S?ao Carlos
Table 6: Participating teams, their affiliations, subtasks they have taken part in, and an indication about
whether the team participated in SemEval-2013 Task 2.
79
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An enhanced
lexical resource for sentiment analysis and opinion
mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation,
LREC ?10, Valletta, Malta.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 36?44, Beijing, China.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavald`a. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine Learning Research, Proceedings Track, 17:5?
11.
Tao Chen and Min-Yen Kan. 2013. Creating a
live, public short message service corpus: the NUS
SMS corpus. Language Resources and Evaluation,
47(2):299?335.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcasm in Twitter
and Amazon. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, CoNLL ?10, pages 107?116, Uppsala, Sweden.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech tagging
for Twitter: Annotation, features, and experiments.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies, ACL-HLT ?11, pages 42?
47, Portland, Oregon, USA.
Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, and
Nina Wacholder. 2011. Identifying sarcasm in Twit-
ter: a closer look. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies - Short Pa-
pers, ACL-HLT ?11, pages 581?586, Portland, Ore-
gon, USA.
Bernard Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter power: Tweets as elec-
tronic word of mouth. J. Am. Soc. Inf. Sci. Technol.,
60(11):2169?2188.
Efthymios Kouloumpis, Theresa Wilson, and Johanna
Moore. 2011. Twitter sentiment analysis: The
good the bad and the OMG! In Proceedings of
the Fifth International Conference on Weblogs and
Social Media, ICWSM ?11, Barcelona, Catalonia,
Spain.
Christine Liebrecht, Florian Kunneman, and Antal
Van den Bosch. 2013. The perfect solution for de-
tecting sarcasm in tweets #not. In Proceedings of
the 4th Workshop on Computational Approaches to
Subjectivity, Sentiment and Social Media Analysis,
pages 29?37, Atlanta, Georgia, USA.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of tweets. In Proceed-
ings of the Seventh international workshop on Se-
mantic Evaluation Exercises, SemEval-2013, pages
321?327, Atlanta, Georgia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 task 2: Sentiment analysis in
Twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on
Semantic Evaluation, SemEval ?13, pages 312?320,
Atlanta, Georgia, USA.
Brendan O?Connor, Ramnath Balasubramanyan, Bryan
Routledge, and Noah Smith. 2010. From tweets
to polls: Linking text sentiment to public opinion
time series. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Alexander Pak and Patrick Paroubek. 2010. Twit-
ter based system: Using Twitter for disambiguating
sentiment ambiguous adjectives. In Proceedings of
the 5th International Workshop on Semantic Evalu-
ation, SemEval ?10, pages 436?439, Uppsala, Swe-
den.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing - Volume 10, EMNLP ?02, pages
79?86.
Maria Pontiki, Harris Papageorgiou, Dimitrios Gala-
nis, Ion Androutsopoulos, John Pavlopoulos, and
Suresh Manandhar. 2014. SemEval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation, SemEval ?14, Dublin, Ireland.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?11, pages 1524?1534, Edinburgh,
Scotland, UK.
Andranik Tumasjan, Timm Sprenger, Philipp Sandner,
and Isabell Welpe. 2010. Predicting elections with
Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the Fourth Inter-
national Conference on Weblogs and Social Media,
ICWSM ?10, Washington, DC, USA.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
80
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 122?126,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Reconciling OntoNotes: Unrestricted Coreference Resolution in
OntoNotes with Reconcile
Veselin Stoyanov
CLSP
Johns Hopkins University
Baltimore, MD
Uday Babbar and Pracheer Gupta and Claire Cardie
Department of Computer Science
Cornell University
Ithaca, NY
Abstract
This paper describes our entry to the 2011 CoNLL
closed task (Pradhan et al, 2011) on modeling un-
restricted coreference in OntoNotes. Our system is
based on the Reconcile coreference resolution re-
search platform. Reconcile is a general software in-
frastructure for the development of learning-based
noun phrase (NP) coreference resolution systems.
Our entry for the CoNLL closed task is a configura-
tion of Reconcile intended to do well on OntoNotes
data. This paper describes our configuration of Rec-
oncile as well as the changes that we had to imple-
ment to integrate with the OntoNotes task definition
and data formats. We also present and discuss the
performance of our system under different testing
conditions on a withheld validation set.
1 Introduction
Noun phrase (NP) coreference resolution is one of
the fundamental tasks of the field of Natural Lan-
guage Processing (NLP). Recently, the creation of
the OntoNotes corpus (Pradhan et al, 2007) has
provided researchers with a large standard data
collection with which to create and empirically
compare coreference resolution systems.
Reconcile (Stoyanov et al, 2010b) is a general
coreference resolution research platform that aims
to abstract the architecture of different learning-
based coreference systems and to provide infras-
tructure for their quick implementation. Recon-
cile is distributed with several state-of-the art NLP
components and a set of optimized feature imple-
mentations. We decided to adapt Reconcile for
the OntoNotes corpus and enter it in the 2011
CoNLL shared task with three goals in mind: (i) to
compare the architecture and components of Rec-
oncile with other state-of-the-art coreference sys-
tems, (ii) to implement and provide the capabil-
ity of running Reconcile on the OntoNotes corpus,
and, (iii) to provide a baseline for future algorithm
implementations in Reconcile that evaluate on the
OntoNotes corpus.
Although Reconcile can be easily adapted to
new corpora, doing so requires introducing new
components. More precisely, the system has to
be modified to be consistent with the specific def-
inition of the coreference task embodied in the
OntoNotes annotation instructions. Additionally,
different corpora use different data formats, so the
system needs to implement capabilities for dealing
with these new formats. Finally, Reconcile can be
configured with different features and components
to create an instantiation that models well the par-
ticular data.
In this paper we describe, ReconcileCoNLL,
our entry to the 2011 CoNLL shared task based on
the Reconcile research platform. We begin by de-
scribing the general Reconcile architecture (Sec-
tion 2), then describe the changes that we incor-
porated in order to enable Reconcile to work on
OntoNotes data (Sections 3 and 4). Finally, we
describe our experimental set up and results from
running ReconcileCoNLL under different condi-
tions (Section 5).
2 Overview of Reconcile
In this section we give a high-level overview of the
Reconcile platform. We refer the reader for more
details to Stoyanov et al (2010a) and Stoyanov et
al. (2010b). Results from running a Reconcile-
based coreference resolution system on different
corpora can be found in Stoyanov et al (2009).
Reconcile was developed to be a coreference
resolution research platform that allows for quick
implementation of coreference resolution systems.
The platform abstracts the major processing steps
(components) of current state-of-the-art learning-
based coreference resolution systems. A descrip-
tion of the steps and the available components can
be found in the referenced papers.
3 The ReconcileCoNLL System
To participate in the 2011 CoNLL shared task, we
configured Reconcile to conform to the OntoNotes
general coreference resolution task. We will use
the name ReconcileCoNLL, to refer to this par-
ticular instantiation of the general Reconcile plat-
form. The remainder of this section describe the
changes required to enable ReconcileCoNLL to
run (accurately) on OntoNotes data.
122
ReconcileCoNLL employs the same basic
pipelined architecture as Reconcile. We describe
the specific components used in each step.
1. Preprocessing. Documents in the OntoNotes
corpus are manually (or semi-automatically) an-
notated with many types of linguistic information.
This information includes tokens, part-of-speech
tags, and named entity information as well as a
constituent syntactic parse of the text. For the pur-
pose of participating in the shared task, we rely on
these manual annotations, when available. Thus,
we do not run most of the standard Reconcile pre-
processing components. One type of information
not provided in the OntoNotes corpus is a depen-
dency parse. Several of Reconcile?s features rely
on a dependency parse of the text. Thus, we ran
the Stanford dependency parser (Klein and Man-
ning, 2003), which performs a constituent parse
and uses rules to convert to a dependency format.1
Two additional changes to the preprocessing
step were necessary for running on the OntoNotes
data. The first is the implementation of compo-
nents that can convert data from the OntoNotes
format to the Reconcile internal format. The sec-
ond is adaptation of the Coreference Element (CE)
extractor to conform to the OntoNotes definition
of what can constitute a CE. Our implementations
for these two tasks are briefly described in Sec-
tions 4.1 and 4.2, respectively.
2. Feature generation. ReconcileCoNLL was
configured with 61 features that have proven suc-
cessful for coreference resolution on other data
sets. Due to the lack of time we performed
no feature engineering or selection specific to
OntoNotes. We used a new component for gener-
ating the pairwise CEs that comprise training and
test instances, which we dub SMARTPG (for smart
pair generator). This is described in Section 4.3.
3. Classification. We train a linear classifier us-
ing the averaged perceptron algorithm (Freund and
Schapire, 1999). We use a subset of 750 randomly
selected documents for training, since training on
the entire set required too much memory.2 As a
result, we had ample validation data for tuning
thresholds, etc.
1A better approach would be to use the rules to create the
dependency parse from the manual constituent parse. We de-
cided against this approach due to implementation overhead.
2It is easy to address the memory issue in the on-line per-
ceptron setting, but in the interest of time we chose to reduce
the size of the training data. Training on the set of 750 docu-
ments is done efficiently in memory by allocating 4GB to the
Java virtual machine.
4. Clustering. We use Reconcile?s single-link
clustering algorithm. In other words, we compute
the transitive closure of the positive pairwise pre-
dictions. Note that what constitutes a positive pre-
diction depends on a threshold set for the classifier
from the previous step. This clustering threshold
is optimized using validation data. More details
about the influence of the validation process can
be found in Section 5.
5. Scoring. The 2011 CoNLL shared task pro-
vides a scorer that computes a set of commonly
used coreference resolution evaluation metrics.
We report results using this scorer in Section 5.
However, we used the Reconcile-internal versions
of scorers to optimize the threshold. This was
done for pragmatic reasons ? time pressure pre-
vented us from incorporating the CoNLL scorer in
the system. We also report the Reconcile-internal
scores in the experiment section.
This concludes the high-level description of
the ReconcileCoNLL system. Next, we describe
in more detail the main changes implemented to
adapt to the OntoNotes data.
4 Adapting to OntoNotes
The first two subsection below describe the two
main tasks that need to be addressed when running
Reconcile on a new data set: annotation conver-
sion and CE extraction. The third subsection de-
scribes the new Smart CE Pairwise instance gen-
erator ? a general component that can be used for
any coreference data set.
4.1 Annotation Conversion
There are fundamental differences between the an-
notation format used by OntoNotes and that used
internally by Reconcile. While OntoNotes relies
on token-based representations, Reconcile uses a
stand-off bytespan annotation. A significant part
of the development of ReconcileCoNLL was de-
voted to conversion of the OntoNotes manual to-
ken, parse, named-entity and coreference annota-
tions. In general, we prefer the stand-off bytespan
format because it allows the reference text of the
document to remain unchanged while annotation
layers are added as needed.
4.2 Coreference Element Extraction
The definition of what can constitute an element
participating in the coreference relation (i.e., a
Coreference Element or CE) depends on the par-
ticular dataset. Optimizing the CE extraction com-
123
Optimized Thres- B- CEAF MUC
Metric hold Cubed
BCubed 0.4470 0.7112 0.1622 0.6094
CEAF 0.4542 0.7054 0.1650 0.6141
MUC 0.4578 0.7031 0.1638 0.6148
Table 1: Reconcile-internal scores for different
thresholds. The table lists the best threshold for
the validation data and results using that threshold.
Pair Gen. BCubed CEAFe MUC
SMARTPG 0.6993 0.1634 0.6126
All Pairs 0.6990 0.1603 0.6095
Table 3: Influence of different pair generators.
ponent for the particular task definition can result
in dramatic improvements in performance. An ac-
curate implementation limits the number of ele-
ments that the coreference system needs to con-
sider while keeping the recall high.
The CE extractor that we implemented for
OntoNotes extends the existing Reconcile ACE05
CE extractor (ACE05, 2005) via the following
modifications:
Named Entities: We exclude named entities of
type CARDINAL NUMBER, MONEY and NORP,
the latter of which captures nationality, religion,
political and other entities.
Possessives: In the OntoNotes corpus, posses-
sives are included as coreference elements, while
in ACE they are not.
ReconcileCoNLL ignores the fact that verbs can
also be CEs for the OntoNotes coreference task as
this change would have constituted a significant
implementation effort.
Overall, our CE extractor achieves recall of over
96%, extracting roughly twice the number of CEs
in the answer key (precision is about 50%). High
recall is desirable for the CE extractor at the cost of
precision since the job of the coreference system is
to further narrow down the set of anaphoric CEs.
4.3 Smart Pair Generator
Like most current coreference resolution systems,
at the heart of Reconcile lies a pairwise classifier.
The job of the classifier is to decide whether or not
two CEs are coreferent or not. We use the term
pair generation to refer to the process of creating
the CE pairs that the classifier considers. The most
straightforward way of generating pairs is by enu-
merating all possible unique combinations. This
approach has two undesirable properties ? it re-
quires time in the order of O(n2) for a given doc-
ument (where n is the number of CEs in the docu-
ment) and it produces highly imbalanced data sets
with the number of positive instances (i.e., coref-
erent CEs) being a small fraction of the number of
negative instances. The latter issue has been ad-
dressed by a technique named instance generation
(Soon et al, 2001): during training, each CE is
matched with the first preceding CE with which it
corefers and all other CEs that reside in between
the two. During testing, a CE is compared to all
preceding CEs until a coreferent CE is found or
the beginning of the document is reached. This
technique reduces class imbalance, but it has the
same worst-case runtime complexity of O(n2).
We employ a new type of pair generation that
aims to address both the class imbalance and
improves the worst-case runtime. We will use
SMARTPG to refer to this component. Our pair
generator relies on linguistic intuitions and is
based on the type of each CE. For a given CE,
we use a rule-based algorithm to guess its type.
Based on the type, we restrict the scope of possi-
ble antecedents to which the CE can refer in the
following way:
Proper Name (Named Entity): A proper name
is compared against all proper names in the 20 pre-
ceding sentences. In addition, it is compared to all
other CEs in the two preceding sentences.
Definite noun phrase: Compared to all CEs in
the six preceding sentences.
Common noun phrase: Compared to all CEs
in the two preceding sentences.
Pronoun: Compared to all CEs in the two pre-
ceding sentences unless it is a first person pronoun.
First person pronouns are additionally compared
to first person pronouns in the preceding 20 sen-
tences.
During development, we used SMARTPG
on coreference resolution corpora other than
OntoNotes and determined that the pair generator
tends to lead to more accurate results. It also has
runtime linear in the number of CEs in a docu-
ment, which leads to a sizable reduction in run-
ning time for large documents. Training files gen-
erated by SMARTPG also tend to be more bal-
anced. Finally, by omitting pairs that are un-
likely to be coreferent, SMARTPG produces much
smaller training sets. This leads to faster learning
and allows us to train on more documents.
124
Optimized Metric Threshold BCubed CEAFe MUC BLANC CEAFm Combined
BCubed 0.4470 0.6651 0.4134 0.6156 0.6581 0.5249 0.5647
CEAF 0.4542 0.6886 0.4336 0.6206 0.7012 0.5512 0.5809
MUC 0.4578 0.6938 0.4353 0.6215 0.7108 0.5552 0.5835
Table 2: CoNLL scores for different thresholds on validation data.
CoNLL Official Test Scores BCubed CEAFe MUC BLANC CEAFm Combined
Closed Task 0.6144 0.3588 0.5843 0.6088 0.4608 0.5192
Gold Mentions 0.6248 0.3664 0.6154 0.6296 0.4808 0.5355
Table 4: Official CoNLL 2011 test scores. Combined score is the average of MUC, BCubed and CEAFe.
5 Experiments
In this section we present and discuss the results
for ReconcileCoNLLwhen trained and evaluated
on OntoNotes data. For all experiments, we train
on a set of 750 randomly selected documents from
the OntoNotes corpus. We use another 674 ran-
domly selected documents for validation. We re-
port scores using the scorers implemented inter-
nally in Reconcile as well as the scorers supplied
by the CoNLL shared task.
In the rest of the section, we describe our results
when controlling two aspects of the system ? the
threshold of the pairwise CE classifier, which is
tuned on training data, and the method used for
pair generation. We conclude by presenting the
official results for the CoNLL shared task.
Influence of Classifier Threshold As previ-
ously mentioned, the threshold above which the
decision of the classifier is considered positive
provides us with a knob that controls the preci-
sion/recall trade-off. Reconcile includes a mod-
ule that can automatically search for a threshold
value that optimizes a particular evaluation met-
ric. Results using three Reconcile-internal scor-
ers (BCubed, CEAF, MUC) are shown in Table
1. First, we see that the threshold that optimizes
performance on the validation data also exhibits
the best results on the test data. The same does
not hold when using the CoNLL scorer for test-
ing, however: as Table 2 shows, the best results
for almost all of the CoNLL scores are achieved at
the threshold that optimizes the Reconcile-internal
MUC score. Note that we did not optimize thresh-
olds for the external scorer in the name of sav-
ing implementation effort. Unfortunately, the re-
sults that we submitted for the official evaluations
were for the suboptimal threshold that optimizes
Reconcile-internal BCubed score.
Influence of Pair Generation Strategies Next,
we evaluate the performance of SMARTPG pair
generators. We run the same system set-up as
above substituting the pair generation module. Re-
sults (using the internal scorer), displayed in Table
3, show our SMARTPG performs identically to the
generator producing all pairs, while it runs in time
linear in the number of CEs.
Official Scores for the CoNLL 2011 Shared
Task Table 4 summarizes the official scores of
ReconcileCoNLL on the CoNLL shared task. Sur-
prisingly, the scores are substationally lower than
the scores on our held-out training set. So far, we
have no explanation for these differences in perfor-
mance. We also observe that using gold-standard
instead of system-extracted CEs leads to improve-
ment in score of about point and a half.
The official score places us 8th out of 21 sys-
tems on the closed task. We note that because
of the threshold optimization mix-up we suffered
about 2 points in combined score performance.
Realistically our system should score around 0.54
placing us 5th or 6th on the task.
6 Conclusions
In this paper, we presented ReconcileCoNLL, our
system for the 2011 CoNLL shared task based on
the Reconcile research platform. We described
the overall Reconcile platform, our configuration
for the CoNLL task and the changes that we im-
plemented specific to the task. We presented the
results of an empirical evaluation performed on
held-out training data. We discovered that results
for our system on this data are quite different from
the official score that our system achieved.
Acknowledgments
This material is based upon work supported by
the National Science Foundation under Grant #
0937060 to the Computing Research Association
for the CIFellows Project.
125
References
ACE05. 2005. NIST ACE evaluation website. In
http://www.nist.gov/speech/tests/ace/2005.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
In Machine Learning, pages 277?296.
D. Klein and C. Manning. 2003. Fast Exact Inference
with a Factored Model for Natural Language Pars-
ing. In Advances in Neural Information Processing
(NIPS 2003).
Sameer S. Pradhan, Lance Ramshaw, Ralph
Weischedel, Jessica MacBride, and Linnea Micci-
ulla. 2007. Unrestricted coreference: Identifying
entities and events in ontonotes. In Proceedings
of the International Conference on Semantic
Computing.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. Conll-2011 shared task: Modeling un-
restricted coreference in ontonotes. In Proceedings
of the Fifteenth Conference on Computational Nat-
ural Language Learning (CoNLL 2011), Portland,
Oregon, June.
W. Soon, H. Ng, and D. Lim. 2001. A Machine
Learning Approach to Coreference of Noun Phrases.
Computational Linguistics, 27(4):521?541.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
Making sense of the state-of-the-art. In Proceedings
of ACL/IJCNLP.
V. Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. But-
tler, and D. Hysom. 2010a. Reconcile: A corefer-
ence resolution research platform. Technical report,
Cornell University.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2010b.
Coreference resolution with reconcile. In Proceed-
ings of the ACL 2010.
126
