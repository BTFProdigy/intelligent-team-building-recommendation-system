Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 907?916,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Learning Graph Walk Based Similarity Measures for Parsed Text
Einat Minkov?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
einat@cs.cmu.edu
William W. Cohen
Machine Learning Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
wcohen@cs.cmu.edu
Abstract
We consider a parsed text corpus as an in-
stance of a labelled directed graph, where
nodes represent words and weighted directed
edges represent the syntactic relations be-
tween them. We show that graph walks, com-
bined with existing techniques of supervised
learning, can be used to derive a task-specific
word similarity measure in this graph. We also
propose a new path-constrained graph walk
method, in which the graph walk process is
guided by high-level knowledge about mean-
ingful edge sequences (paths). Empirical eval-
uation on the task of named entity coordinate
term extraction shows that this framework is
preferable to vector-based models for small-
sized corpora. It is also shown that the path-
constrained graph walk algorithm yields both
performance and scalability gains.
1 Introduction
Graph-based similarity measures have been used
for a variety of language processing applications.
In this paper we assume directed graphs, where
typed nodes denote entities and labelled directed
and weighted edges denote the relations between
them. In this framework, graph walks can be ap-
plied to draw a measure of similarity between the
graph nodes. Previous works have applied graph
walks to draw a notion of semantic similarity over
such graphs that were carefully designed and man-
ually tuned, based on WordNet reations (Toutanova
?Current address: Nokia Research Center Cambridge, Cam-
bridge, MA 02142, USA.
et al, 2004; Collins-Thompson and Callan, 2005;
Hughes and Ramage, 2007).
While these and other researchers have used
WordNet to evaluate similarity between words, there
has been much interest in extracting such a measure
from text corpora (e.g., (Snow et al, 2005; Pado?
and Lapata, 2007)). In this paper, we suggest pro-
cessing dependency parse trees within the general
framework of directed labelled graphs. We construct
a graph that directly represents a corpus of struc-
tured (parsed) text. In the suggested graph scheme,
nodes denote words and weighted edges represent
the dependency relations between them. We apply
graph walks to derive an inter-word similarity mea-
sure. We further apply learning techniques, adapted
to this framework, to improve the derived corpus-
based similarity measure.
The learning methods applied include existing
learning techniques, namely edge weight tuning,
where weights are associated with the edge types,
and discriminative reranking of graph nodes, using
features that describe the possible paths between a
graph node and the initial ?query nodes? (Minkov
and Cohen, 2007).
In addition, we outline in this paper a novel
method for learning path-constrained graph walks.
While reranking allows use of high-level features
that describe properties of the traversed paths, the
suggested algorithm incorporates this information in
the graph walk process. More specifically, we allow
the probability flow in the graph to be conditioned
on the history of the walk. We show that this method
results in improved performance as it directs proba-
bility flow to meaningful paths. In addition, it leads
907
to substantial gains in terms of runtime performance.
The graph representation and the set of learning
techniques suggested are empirically evaluated on
the task of coordinate term extraction1 from small
to moderately sized corpora, where we compare
them against vector-based models, including a state-
of-the-art syntactic distributional similarity method
(Pado? and Lapata, 2007). It is shown that the graph
walk based approach gives preferable results for the
smaller datasets (and comparable otherwise), where
learning yields significant gains in accuracy.
There are several contributions of this paper.
First, we represent dependency-parsed corpora
within a general graph walk framework, and derive
inter-word similarity measures using graph walks
and learning techniques available in this framework.
To our knowledge, the application of graph walks to
parsed text in general, and to the extraction of coor-
dinate terms in particular, is novel. Another main
contribution of this paper is the path-constrained
graph walk variant, which is a general learning tech-
nique for calculating the similarity between graph
nodes in directed and labelled graphs.
Below we first outline our proposed scheme for
representing a dependency-parsed text corpus as a
graph, and provide some intuitions about the asso-
ciated similarity metric (Section 2). We then give
an overview of the graph-walk based similarity met-
ric (Section 3), as well as the known edge weight
tuning and reranking learning techniques (Section
4). We next present the proposed algorithm of path-
constrained graph walks (Section 5). The paper pro-
ceeds with a review of related work (Section 6), a
discussion of the coordinate term extraction task,
empirical evaluation and our conclusions (Sections
7-9).
2 Representing a Corpus as a Graph
A typed dependency parse tree consists of directed
links between words, where dependencies are la-
belled with the relevant grammatical relation (e.g.,
nominal subject, indirect object etc.). We suggest
representing a text corpus as a connected graph
of dependency structures, according to the scheme
shown in Figure 1. The graph shown in the figure
1In particular, we focus on the extraction of named entity
classes.
Figure 1: The suggested graph schema, demonstrated for
a two-sentence corpus.
includes the dependency analysis of two sentences:
?boys like playing with all kinds of cars?, and ?girls
like playing with dolls?. In the graph, each word
mention is represented as a node, which includes the
index of the sentence in which it appears, as well
as its position within the sentence. Word mentions
are marked as circles in the figure. The ?type? of
each word ? henceforth a term node ? is denoted by
a square in the figure. Each word mention is linked
to the corresponding term; for example, the nodes
?like1? and ?like2? represent distinct word mentions
and both nodes are linked to the term ?like?. For
every edge in the graph, we add another edge in the
opposite direction (not shown in the figure); for ex-
ample, an inverse edge exists from ?like1? to ?girls1?
with an edge labelled as ?nsubj-inv?. The resulting
graph is highly interconnected and cyclic.
We will apply graph walks to derive an extended
measure of similarity, or relatedness, between word
terms (as defined above). For example, starting from
the term ?girls?, we will reach the semantically re-
lated term ?boys? via the following two paths:
(1) girls mention?? girls1 nsubj?? like1 as?term?? like mention??
like2 nsubj?inverse?? boys2 as?term?? boys
(2) girls mention?? girls1 nsubj?? like1 partmod?? playing1
as?term?? playing mention?? playing2 partmod?inverse?? like2
nsubj?inverse?? boys2 as?term?? boys .
Intuitively, in a graph representing a large cor-
pus, terms that are more semantically related will
be linked by a larger number of connecting paths. In
addition, shorter connecting paths may be in general
more meaningful. In the next section we show that
908
the graph walk paradigm addresses both of these re-
quirements. Further, different edge types, as well as
the paths traversed, are expected to have varying im-
portance in different types of word similarity (for ex-
ample, verbs and nouns are associated with different
connectivity patterns). These issues are addressed
using learning.
3 Graph Walks and Similarity Queries
This section provides a quick overview of the graph
walk induced similarity measure. For details, the
reader is referred to previous publications (e.g.,
(Toutanova et al, 2004; Minkov and Cohen, 2007)).
In summary, similarity between two nodes in the
graph is defined by a weighted graph walk process,
where an edge of type ` is assigned an edge weight,
?`, determined by its type.2 The transition proba-
bility of reaching node y from node x over a single
time step, Pr(x ?? y), is defined as the weight
of their connecting edge, ?l, normalized by the to-
tal outgoing weight from x. Given these transition
probabilities, and starting from an initial distribu-
tion Vq of interest (a query), we perform a graph
walk for a finite number of steps K. Further, at each
step of the walk, a proportion ? of the probability
mass at every node is emitted. Thus, this model ap-
plies exponential decay on path length. The final
probability distribution of this walk over the graph
nodes, which we denote as R, is computed as fol-
lows: R = ?Ki=1 ?iVqMi, where M is the transi-
tion matrix.3 The answer to a query, Vq, is a list of
nodes, ranked by the scores in the final distribution
R. In this multi-step walk, nodes that are reached
from the query nodes by many shorter paths will be
assigned a higher score than nodes connected over
fewer longer paths.
4 Learning
We consider a supervised setting, where we are
given a dataset of example queries and labels over
the graph nodes, indicating which nodes are relevant
to which query. For completeness, we describe here
two methods previously described by Minkov and
2In this paper, we consider either uniform edge weights; or,
learn the set of weights ? from examples.
3We tune K empirically and set ? = 0.5, as in (Minkov and
Cohen, 2007).
Cohen (Minkov and Cohen, 2007): a hill-climbing
method that tunes the graph weights; and a reranking
method. We also specify the feature set to be used by
the reranking method in the domain of parsed text.
4.1 Weight Tuning
There are several motivations for learning the graph
weights ? in this domain. First, some dependency
relations ? foremost, subject and object ? are in gen-
eral more salient than others (Lin, 1998; Pado? and
Lapata, 2007). In addition, dependency relations
may have varying importance per different notions
of word similarity (e.g., noun vs. verb similarity
(Resnik and Diab, 2000)). Weight tuning allows the
adaption of edge weights to each task (i.e., distribu-
tion of queries).
The weight tuning method implemented in this
work is based on an error backpropagation hill
climbing algorithm (Diligenti et al, 2005). The al-
gorithm minimizes the following cost function:
E = 1N
?
z?N
ez =
1
N
?
z?N
1
2(pz ? p
Opt
z )2
where ez is the error for a target node z defined as the
squared difference between the final score assigned
to z by the graph walk, pz , and some ideal score ac-
cording to the example?s labels, pOptz .4 Specifically,
pOptz is set to 1 in case that the node z is relevant
or 0 otherwise. The error is averaged over a set of
example instantiations of size N . The cost function
is minimized by gradient descent where the deriva-
tive of the error with respect to an edge weight ?`
is derived by decomposing the walk into single time
steps, and considering the contribution of each node
traversed to the final node score.
4.2 Node Reranking
Reranking of the top candidates in a ranked list
has been successfully applied to multiple NLP tasks
(Collins, 2002; Collins and Koo, 2005). In essence,
discriminative reranking allows the re-ordering of
results obtained by methods that perform some form
of local search, using features that encode higher
level information.
4For every example query, a handful of the retrieved nodes
are considered, including both relevant and irrelevant nodes.
909
A number of features describing the set of paths
from Vq can be conveniently computed in the pro-
cess of executing the graph walk, and it has been
shown that reranking using these features can im-
prove results significantly. It has also been shown
that reranking is complementary to weight tuning
(Minkov and Cohen, 2007), in the sense that the
two techniques can be usefully combined by tuning
weights, and then reranking the results.
In the reranking approach, for every training ex-
ample i (1 ? i ? N ), the reranking algorithm is
provided with the corresponding output ranked list
of li nodes. Let zij be the output node ranked at rank
j in li, and let pzij be the probability assigned to zij
by the graph walk. Each output node zij is repre-
sented through m features, which are computed by
pre-defined feature functions f1, . . . , fm. The rank-
ing function for node zij is defined as:
F (zij , ??) = ?0log(pzij ) +
m
?
k=1
?kfk(zij)
where ?? is a vector of real-valued parameters. Given
a new test example, the output of the model is the
output node list reranked by F (zij , ??). To learn the
parameter weights ??, we here applied a boosting
method (Collins and Koo, 2005) (see also (Minkov
et al, 2006)).
4.2.1 Features
We evaluate the following feature templates.
Edge label sequence features indicate whether a par-
ticular sequence of edge labels `i occurred, in a
particular order, within the set of paths leading to
the target node zij . Lexical unigram feature indi-
cate whether a word mention whose lexical value
is tk was traversed in the set of paths leading to
zij . Finally, the Source-count feature indicates the
number of different source query nodes that zij was
reached from. The intuition behind this last fea-
ture is that nodes linked to multiple query nodes,
where applicable, are more relevant. For exam-
ple, for the query term ?girl? in the graph depicted
in Figure 1, the target node ?boys? is described
by the features (denoted as feature-name.feature-
value): sequence.nsubj.nsubj-inv (where mention
and as-term edges are omitted) , lexical.??like? etc.
In this work, the features encoded are binary.
However, features can be assugned numeric weights
that corresponds to the probability of the indicator
being true for any path between x and zij (Cohen
and Minkov, 2006).
5 Path-Constrained Graph Walk
While node reranking allows the incorporation of
high-level features that describe the traversed paths,
it is desirable to incorporate such information ear-
lier in the graph walk process. In this paper, we
suggest a variant of a graph-walk, which is con-
strained by path information. Assume that prelim-
inary knowledge is available that indicates the prob-
ability of reaching a relevant node after following a
particular edge type sequence (path) from the query
distribution Vq to some node x. Rather than fix the
edge weights ?, we can evaluate the weights of the
outgoing edges from node x dynamically, given the
history of the walk (the path) up to this node. This
should result in gains in accuracy, as paths that lead
mostly to irrelevant nodes can be eliminated in the
graph walk process. In addition, scalability gains
are expected, for the same reason.
We suggest a path-constrained graph walk algo-
rithm, where path information is maintained in a
compact path-tree structure constructed based on
training examples. Each vertex in the path tree de-
notes a particular walk history. In applying the graph
walk, the nodes traversed are represented as a set of
node pairs, comprised of the graph node and the cor-
responding vertices in the path tree. The outgoing
edge weights from each node pair will be estimated
according to the respective vertex in the path tree.
This approach needs to address two subtasks: learn-
ing of the path-tree; and updating of the graph walk
paradigm to co-sample from the graph and the path
tree. We next describe these two components in de-
tail.
The Path-Tree
We construct a path-tree T using a training set
of N example queries. Let a path p be a sequence
of k < K edge types (where K is the maximum
number of graph walk steps). For each training ex-
ample, we recover all of the connecting paths lead-
ing to the top M (correct and incorrect) nodes. We
consider only acyclic paths. Let each path p be as-
sociated with its count, within the paths leading to
the correct nodes, denoted as C+p . Similarly, the
910
Figure 2: An example path-tree.
count within paths leading to the negatively labelled
nodes is denoted C?p . The full set of paths observed
is then represented as a tree.5 The leaves of the
tree are assigned a Laplace-smoothed probability:
Pr(p) = C
+
p +1
C+p +C?p +2
.
Given path probabilities, they are propagated
backwards to all tree vertices, applying the MAX
operator.6 Consider the example given in Figure 2.
The path-tree in the figure includes three paths (con-
structed from edge types k, l,m, n). The top part
of the figure gives the paths? associated counts, and
the bottom part of the figure gives the derived outgo-
ing edge probabilities at each vertex. This path-tree
specifies, for example, that given an edge of type l
was traversed from the root, the probability of reach-
ing a correct target node is 0.9 if an edge of type n
is followed, whereas the respective probability if an
edge of type m is followed is estimated at a lower
0.2.
A Concurrent Graph-walk
Given a generated path tree, we apply path-
constrained graph walks that adhere both to the
topology of the graph G, and to the path tree T .
Walk histories of each node x visited in the walk
are compactly represented as pairs < t, x >, where
t denotes the relevant vertex in the path tree. For
example, suppose that after one walk step, the main-
tained node-history pairs include < T (l), x1 > and
< T (m), x2 >. If x3 is reached in the next walk step
5The conversion to a tree is straight-forward, where identical
path prefixes are merged.
6Another possibility is to average the downstream cumula-
tive counts at each vertex. The MAX operation gave better re-
sults in our experiments.
Given: graph G, path-tree T , query distribution V0,
number of steps K
Initialize: for each xi ? V0, assign a pair
< root(T ), xi >
Repeat for steps k = 0 to K:
For each < ti, xi >? Vk:
Let L be the set of outgoing edge labels from xi, in G.
For each lm ? L:
For each xj ? G s.t., xi lm?? xj , add < tj , xj > to
Vk+1, where tj ? T , s.t. ti lm?? tj , with probability
Pr(xi|Vk) ? Pr(lm|ti, T ). (The latter probabilities
should be normalized with respect to xi.)
If ti is a terminal node in T , emit xi with probability
Pr(xi|Vk)? Pr(ti|T ).
Figure 3: Pseudo-code for path-constrained graph walk
from both x1 and x2 over paths included in the path-
tree, it will be represented by multiple node pairs,
e.g., < T (l ? n), x3 > and < T (m ? l, x3 >.
A pseudo-code for a path-constrained graph walk is
given in Figure 3. It is straight-forward to discard
paths in T that are associated with a lower proba-
bility than some threshold. A threshold of 0.5, for
example, implies that only paths that led to a major-
ity of positively labelled nodes in the training set are
followed.
6 Related Work
Graph walks over typed graphs have been applied
to derive semantic similarity for NLP problems us-
ing WordNet as a primary information source. For
instance, Hughes and Ramage (2007) constructed a
graph which represented various types of word re-
lations from WordNet, and compared random-walk
similarity to similarity assessments from human-
subject trials. Random-walk similarity has also been
used for lexical smoothing for prepositional word
attachment (Toutanova et al, 2004) and query ex-
pansion (Collins-Thompson and Callan, 2005). In
contrast to these works, our graph representation de-
scribes parsed text and has not been (consciously)
engineered for a particular task. Instead, we in-
clude learning techniques to optimize the graph-
walk based similarity measure. The learning meth-
ods described in this paper can be readily applied to
911
other directed and labelled entity-relation graphs.7
The graph representation described in this paper
is perhaps most related to syntax-based vector space
models, which derive a notion of semantic similar-
ity from statistics associated with a parsed corpus
(Grefenstette, 1994; Lin, 1998; Pado? and Lapata,
2007). In most cases, these models construct vectors
to represent each word wi, where each element in the
vector for wi corresponds to particular ?context? c,
and represents a count or an indication of whether
wi occurred in context c. A ?context? can refer to
simple co-occurrence with another word wj , to a
particular syntactic relation to another word (e.g., a
relation of ?direct object? to wj), etc. Given these
word vectors, inter-word similarity is evaluated us-
ing some appropriate similarity measure for the vec-
tor space, such as cosine vector similarity, or Lin?s
similarity (Lin, 1998).
Recently, Pado? and Lapata (Pado? and Lapata,
2007) have suggested an extended syntactic vector
space model called dependency vectors, in which
rather than simple counts, the components of a
word vector of contexts consist of weighted scores,
which combine both co-occurrence frequency and
the importance of a context, based on properties of
the connecting dependency paths. They considered
two different weighting schemes: a length weight-
ing scheme, assigning lower weight to longer con-
necting paths; and an obliqueness weighting hierar-
chy (Keenan and Comrie, 1977), assigning higher
weight to paths that include grammatically salient
relations. In an evaluation of word pair similar-
ity based on statistics from a corpus of about 100
million words, they show improvements over sev-
eral previous vector space models. Below we will
compare our framework to that of Pado? and Lap-
ata. One important difference is that while Pado? and
Lapata make manual choices (regarding the set of
paths considered and the weighting scheme), we ap-
ply learning to adjust the analogous parameters.
7 Extraction of Coordinate Terms
We evaluate the text representation schema and the
proposed set of graph-based similarity measures on
the task of coordinate term extraction. In particular,
7We refer the reader to the TextGraph workshop proceed-
ings, http://textgraphs.org.
we evaluate the extraction of named entities, includ-
ing city names and person names from newswire
data, using word similarity measures. Coordinate
terms reflect a particular type of word similarity
(relatedness), and are therefore an appropriate test
case for our framework. While coordinate term ex-
traction is often addressed by a rule-based (tem-
plates) approach (Hearst, 1992), this approach was
designed for very large corpora such as the Web,
where the availability of many redundant documents
allows use of high-precision and low-recall rules.
In this paper we focus on relatively small corpora.
Small limited text collections may correspond to
documents residing on a personal desktop, email
collections, discussion groups and other specialized
sets of documents.
The task defined in the experiments is to retrieve
a ranked list of city or person names given a small
set of seeds. This task is implemented in the graph
as a query, where we let the query distribution Vq be
uniform over the given seeds (and zero elsewhere).
Ideally, the resulting ranked list will be populated
with many additional city, or person, names.
We compare graph walks to dependency vec-
tors (DV) (Pado? and Lapata, 2007),8 as well as to
a vector-based bag-of-words co-occurrence model.
DV is a state-of-the-art syntactic vector-based model
(see Section 6). The co-occurrence model represents
a more traditional approach, where text is processed
as a stream rather than syntactic structures. In ap-
plying the vector-space based methods, we compute
a similarity score between every candidate from the
corpus and each of the query terms, and then aver-
age these scores (as the query distributions are uni-
form) to construct a ranked list. For efficiency, in
the vector-based models we limit the considered set
of candidates to named entities. Similarly, the graph
walk results are filtered to include named entities.9
Corpora. As the experimental corpora, we use
the training set portion of the MUC-6 dataset (MUC,
1995) as well as articles from the Associated Press
(AP) extracted from the AQUAINT corpus (Bilotti
8We used the code from http://www.coli.uni-
saarland.de/ pado/dv.html, and converted the underlying
syntactic patterns to the Stanford dependency parser conven-
tions.
9In general, graph walk results can be filtered by various
word properties, e.g., capitalization pattern, or part-of-speech.
912
Corpus words nodes edges unique NEs
MUC 140K 82K 244K 3K
MUC+AP 2,440K 1,030K 3,550K 36K
Table 1: Corpus statistics
et al, 2007), all parsed using the Stanford depen-
dency parser (de Marneffe et al, 2006).10 The MUC
corpus provides true named entity tags, while the
AQUAINT corpus includes automatically generated,
noisy, named entity tags. Statistics on the experi-
mental corpora and their corresponding graph rep-
resentation are detailed in Table 1. As shown, the
MUC corpus contains about 140 thousand words,
whereas the MUC+AP experimental corpus is sub-
stantially larger, containing about 2.5 million words.
We generated 10 queries, each comprised of 4 city
names selected randomly according to the distribu-
tion of city name mentions in MUC-6. Similarly,
we generated a set of 10 queries that include 4 per-
son names selected randomly from the MUC corpus.
(The MUC corpus was appended to AP, so that the
same query sets are applicable in both cases.) For
each task, we use 5 queries for training and tuning
and the remaining queries for testing.
8 Experimental Results
Experimental setup. We evaluated cross-validation
performance over the training queries in terms of
mean average precision for varying walk lengths K.
We found that beyond K = 6 improvements were
small (and in fact deteriorated for K = 9). We there-
fore set K = 6. Weight tuning was trained using
the training queries and two dozens of target nodes
overall. In reranking, we set a feature count cutoff
of 3, in order to avoid over-fitting. Reranking was
applied to the top 200 ranked nodes output by the
graph walk using the tuned edge weights. Finally,
path-trees were constructed using the top 20 correct
nodes and 20 incorrect nodes retrieved by the uni-
formly weighted graph walk. In the experiments,
we apply a threshold of 0.5 to the path constrained
graph walk method.
We note that for learning, true labels were used for
the fully annotated MUC corpus (we hand labelled
all of the named entities of type location in the cor-
pus as to whether they were city names). However,
10http://nlp.stanford.edu/software/lex-parser.shtml; sen-
tences longer than 70 words omitted.
noisy negative examples were considered for the
larger automatically annotated AP corpus. (Specif-
ically, for cities, we only considered city names in-
cluded in the MUC corpus as correct answers.)
A co-occurrence vector-space model was applied
using a window of two tokens to the right and to
the left of the focus word. Inter-word similarity
was evaluated in this model using cosine similar-
ity, where the underlying co-occurrence counts were
normalized by log-likelihood ratio (Pado? and Lap-
ata, 2007). The parameters of the DV method were
set based on a cross validation evaluation (using the
MUC+AP corpus). The medium set of dependency
paths and the oblique edge weighting scheme were
found to perform best. We experimented with co-
sine as well as Lin similarity measure in combina-
tion with the dependency vectors representation. Fi-
nally, given the large number of candidates in the
MUC+AP corpus (Table 1), we show the results of
applying the considered vector-space models to the
top, high-quality, entities retrieved with reranking
for this corpus.11
Test set results. Figure 4 gives results for the city
name (top) and the person name (bottom) extraction
tasks. The left part of the figure shows results us-
ing the MUC corpus, and its right part ? using the
MUC+AP corpus. The curves show precision as a
function of rank in the ranked list, up to rank 100.
(For this evaluation, we hand-labeled all the top-
ranked results as to whether they are city names or
person names.) Included in the figure are the curves
of the graph-walk method with uniform weights
(G:Uw), learned weights (G:Lw), graph-walk with
reranking (Rerank) and a path-constrained graph-
walk (PCW). Also given are the results of the co-
occurrence model (CO), and the syntactic vector-
space DV model, using the Lin similarity measure
(DV:Lin). Performance of the DV model using co-
sine similarity was found comparable or inferior to
using the Lin measure, and is omitted from the fig-
ure for clarity.
Several trends can be observed from the results.
With respect to the graph walk methods, the graph
walk using the learned edge weights consistently
outperforms the graph walk with uniform weights.
Reranking and the path-constrained graph walk,
11We process the union of the top 200 results per each query.
913
MUC MUC+AP
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
G:Uw
G:Lw
CO
DV:Lin
PCW
Rerank
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  10  20  30  40  50  60  70  80  90  100
Pr
ec
is
io
n
Rank
Figure 4: Test results: Precision at the top 100 ranks, for the city name extraction task (top) and person name extraction
task (bottom).
however, yield superior results. Both of these learn-
ing methods utilize a richer set of features than the
graph walk and weight tuning, which can consider
only local information. In particular, while the graph
walk paradigm assigns lower importance to longer
connecting paths (as described in Section 3), rerank-
ing and the path-constrained walker allow to dis-
card short yet irrelevant paths, and by that eliminate
noise at the top ranks of the retrieved list. In gen-
eral, the results show that edge sequences carry ad-
ditional meaning compared with the individual edge
label segments traversed.
Out of the vector-based models, the co-
occurrence model is preferable for the city name
extraction task, and the syntactic dependency vec-
tors model gives substantially better performance
for person name extraction. We conjecture that city
name mentions are less structured in the underlying
text. In addition, the syntactic weighting scheme of
the DV model is probably not optimal for the case of
city names. For example, a conjunction relation was
found highly indicative for city names (see below).
However, this relation is not emphasized by the DV
weighting schema. As expected, the performance of
the vector-based models improves for larger corpora
(Terra and Clarke, 2003). These models demonstrate
good performance for the larger MUC+AP corpus,
but only mediocre performance for the smaller MUC
corpus.
Contrasting the graph-based methods with the
vector-based models, the difference in performance
in favor of reranking and PCW, especially for the
smaller corpus, can be attributed to two factors. The
first factor is learning, which optimizes performance
for the underlying data. A second factor is the incor-
poration of non-local information, encoding proper-
ties of the traversed paths.
Models. Following is a short description of the
models learned by the different methods and tasks.
Weight tuning assigned high weights to edge types
such as conj-and, prep-in and prep-from, nn, ap-
pos and amod for the city extraction task. For per-
914
 2
 4
 6
 8
 10
 12
 14
 16
 18
 20
 0  1  2  3  4  5  6Nu
m
be
r o
f g
ra
ph
 n
od
es
 v
isi
te
d 
[lo
g_
2]
Walk steps
G:U
PCW:0
PCW:0.5
PCW:0.8
Figure 5: The graph walk exponential spread is bounded
by the path constrained walk.
son extraction, prominent edge types included subj,
obj, poss and nn. (The latter preferences are sim-
ilar to the linguistically motivated weights of DV.)
High weight features assigned by reranking for city
name extraction included, for example, lexical fea-
tures such as ?based? and ?downtown?, and edge bi-
grams such as ?prep-in-Inverse?conj-and? or ?nn-
Inverse?nn?. Positive highly predictive paths in
the constructed path tree included many symmetric
paths, such as ...?conj andInverse...?.conj and...,
...?prep inInverse...?.prep in..., for the city name
extraction task.
Scalability. Figure 5 shows the number of graph
nodes maintained in each step of the graph walk
(logarithm scale) for a typical city extraction query
and the MUC+AP corpus. As shown by the solid
line, the number of graph nodes visited using the
weighted graph walk paradigm grows exponentially
with the length of the walk. Applying a path-
constrained walk with a threshold of 0 (PCW:0) re-
duces the maximal number of nodes expanded (as
paths not observed in the training set are discarded).
As shown, increasing the threshold leads to signifi-
cant gains in scalability. Overall, query processing
time averaged at a few minutes, using a commodity
PC.
9 Conclusion and Future Directions
In this paper we make several contributions. First,
we have explored a novel but natural representation
for a corpus of dependency-parsed text, as a labelled
directed graph. We have evaluated the task of coor-
dinate term extraction using this representation, and
shown that this task can be performed using similar-
ity queries in a general-purpose graph-walk based
query language. Further, we have successfully ap-
plied learning techniques that tune weights assigned
to different dependency relations, and re-score can-
didates using features derived from the graph walk.
Another orthogonal contribution of this paper is
a path-constrained graph walk variant, where the
graph walk is guided by high level knowledge about
meaningful paths, learned from training examples.
This method was shown to yield improved perfor-
mance for the suggested graph representation, and
improved scalability compared with the local graph
walk. The method is general, and can be readily ap-
plied in similar settings.
Empirical evaluation of the coordinate term ex-
traction task shows that the graph-based framework
performs better than vector-space models for the
smaller corpus, and comparably otherwise. Over-
all, we find that the suggested model is suitable for
deep (syntactic) processing of small specialized cor-
pora. In preliminary experiments where we evalu-
ated this framework on the task of extracting general
word synonyms, using a relatively large corpus of
15 million words, we found the graph-walk perfor-
mance to be better than DV using cosine similarity
measures, but second to DV using Lin?s similarity
measure. While this set of results is incomplete, we
find that it is consistent with the results reported in
this paper.
The framework presented can be enhanced in sev-
eral ways. For instance, WordNet edges and mor-
phology relations can be readily encoded in the
graph. We believe that this framework can be ap-
plied for the extraction of more specialized no-
tions of word relatedness, as in relation extraction
(Bunescu and Mooney, 2005). The path-constrained
graph walk method proposed may be enhanced by
learning edge probabilities, using a rich set of fea-
tures. We are also interested in exploring a possi-
ble relation between the path-constrained walk ap-
proach and reinforcement learning.
Acknowledgments
The authors wish to thank the anonymous reviewers
and Hanghang Tong for useful advice. This material
is based upon work supported by Yahoo! Research.
915
References
Matthew W. Bilotti, Paul Ogilvie, Jamie Callan, and Eric
Nyberg. 2007. Structured retrieval for question an-
swering. In SIGIR.
Razvan C. Bunescu and Raymond J. Mooney. 2005. A
shortest path dependency kernel for relation extrac-
tion. In HLT-EMNLP.
William W. Cohen and Einat Minkov. 2006. A graph-
search framework for associating gene identifiers with
documents. BMC Bioinformatics, 7(440).
Michael Collins and Terry Koo. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31(1):25?69.
Kevyn Collins-Thompson and Jamie Callan. 2005.
Query expansion using random walk models. In
CIKM.
Michael Collins. 2002. Ranking algorithms for named-
entity extraction: Boosting and the voted perceptron.
In ACL.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Michelangelo Diligenti, Marco Gori, and Marco Mag-
gini. 2005. Learning web page scores by error back-
propagation. In IJCAI.
Gregory Grefenstette. 1994. Explorations in Automatic
Thesaurus Discovery. Kluwer Academic Publishers,
Dordrecht.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING.
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In EMNLP.
Edward Keenan and Bernard Comrie. 1977. Noun
phrase accessibility and universal grammar. Linguis-
tic Inquiry, 8.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Einat Minkov and William W. Cohen. 2007. Learning to
rank typed graph walks: Local and global approaches.
In WebKDD/KDD-SNA workshop.
Einat Minkov, William W. Cohen, and Andrew Y. Ng.
2006. Contextual search and name disambiguation in
email using graphs. In SIGIR.
1995. Proceedings of the sixth message understanding
conference (muc-6). In Morgan Kaufmann Publish-
ers, Inc. Columbia, Maryland.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In CogSci.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS.
Egidio Terra and C. L. A. Clarke. 2003. Frequency
estimates for statistical word similarity measures. In
NAACL.
Kristina Toutanova, Christopher D. Manning, and An-
drew Y. Ng. 2004. Learning random walk models for
inducing word dependency distributions. In ICML.
916
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 443?450, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Extracting Personal Names from Email: Applying Named Entity
Recognition to Informal Text
Einat Minkov and Richard C. Wang
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15217
{einat,rcwang}@cs.cmu.edu
William W. Cohen
Ctr for Automated Learning & Discovery
Carnegie Mellon University
Pittsburgh, PA 15217
wcohen@cs.cmu.edu
Abstract
There has been little prior work on Named
Entity Recognition for ?informal? docu-
ments like email. We present two meth-
ods for improving performance of per-
son name recognizers for email: email-
specific structural features and a recall-
enhancing method which exploits name
repetition across multiple documents.
1 Introduction
Named entity recognition (NER), the identification
of entity names in free text, is a well-studied prob-
lem. In most previous work, NER has been applied
to news articles (e.g., (Bikel et al, 1999; McCal-
lum and Li, 2003)), scientific articles (e.g., (Craven
and Kumlien, 1999; Bunescu and Mooney, 2004)),
or web pages (e.g., (Freitag, 1998)). These genres of
text share two important properties: documents are
written for a fairly broad audience, and writers take
care in preparing documents. Important genres that
do not share these properties include instant messag-
ing logs, newsgroup postings and email messages.
We refer to these genres as ?informal? text.
Informal text is harder to process automatically.
Informal documents do not obey strict grammatical
conventions. They contain grammatical and spelling
errors. Further, since the audience is more restricted,
informal documents often use group- and task-
specific abbreviations and are not self-contained.
Because of these differences, existing NER methods
may require modifications to perform well on infor-
mal text.
In this paper, we investigate NER for informal
text with an experimental study of the problem of
recognizing personal names in email?a task that is
both useful and non-trivial. An application of in-
terest is corpus anonymization. Automatic or semi-
automatic email anonymization should allow using
large amounts of informal text for research purposes,
for example, of medical files. Person-name extrac-
tion and other NER tasks are helpful for automatic
processing of informal text for a large variety of ap-
plications (Culotta et al, 2004; Cohen et al, 2005).
We first present four corpora of email text, anno-
tated with personal names, each roughly compara-
ble in size to the MUC-6 corpus1. We experimen-
tally evaluate the performance of conditional ran-
dom fields (CRF) (Lafferty et al, 2001), a state-
of-the art machine-learning based NER methods on
these corpora. We then turn to examine the special
attributes of email text (vs. newswire) and suggest
venues for improving extraction performance. One
important observation is that email messages often
include some structured, easy-to-recognize names,
such as names within a header, names appearing in
automatically-generated phrases, as well as names in
signature files or sign-offs. We therefore suggest a
set of specialized structural features for email; these
features are shown to significantly improve perfor-
mance on our corpora.
We also present and evaluate a novel method for
exploiting repetition of names in a test corpus. Tech-
niques for exploiting name repetition within docu-
ments have been recently applied to newswire text
1Two of these are publicly available. The others can not be
distributed due to privacy considerations.
443
(e.g., (Humphreys et al, 1998)), scientific abstracts
(e.g., (Bunescu and Mooney, 2004)) and seminar an-
nouncements (Sutton and Mccallum, 2004); how-
ever, these techniques rely on either NP analysis or
capitalization information to pre-identify candidate
coreferent name mentions, features which are not re-
liable in email. Furthermore, we argue that name
repetition in email should be inferred by examining
multiple documents in a corpus, which is not com-
mon practice. We therefore present an alternative
efficient scheme for increasing recall in email, us-
ing the whole corpus. This technique is shown to
always improve recall substantially, and to almost
always improve F1 performance.
2 Corpora
Two email corpora used in our experiments were
extracted from the CSpace email corpus (Kraut et
al., 2004), which contains email messages collected
from a management course conducted at Carnegie
Mellon University in 1997. In this course, MBA stu-
dents, organized in teams of four to six members,
ran simulated companies in different market scenar-
ios. We believe this corpus to be quite similar to
the work-oriented mail of employees of a small or
medium-sized company. This text corpus contains
three header fields: ?From?, ?Subject?, and ?Time?.
Mgmt-Game is a subcorpora consisting of all emails
written over a five-day period. In the experiments,
the first day worth of email was used as a training
set, the fourth for tuning and the fifth day as a test
set. Mgmt-Teams forms another split of this data,
where the training set contains messages between
different teams than in the test set; hence in Mgmt-
Teams, the person names appearing in the test set
are generally different than those that appear in the
training set.
The next two collections of email were extracted
from the Enron corpus (Klimt and Yang, 2004). The
first subset, Enron-Meetings, consists of messages in
folders named ?meetings? or ?calendar?2 . Most but
not all of these messages are meeting-related. The
second subset, Enron-Random, was formed by re-
peatedly sampling a user name (uniformly at random
among 158 users), and then sampling an email from
2with two exceptions: (a) six very large files were removed,
and (b) one very large ?calendar? folder was excluded.
that user (uniformly at random).
Annotators were instructed to include nicknames
and misspelled names, but exclude person names
that are part of an email address and names that are
part of a larger entity name like an organization or
location (e.g., ?David Tepper School of Business?).
The sizes of the corpora are given in Table 1. We
limited training size to be relatively small, reflecting
a real-world scenario.
Corpus # Documents #Words #NamesTrain Tune Test x1000
Mgmt-Teams 120 82 83 105 2,792
Mgmt-Game 120 216 264 140 2,993
Enron-Meetings 244 242 247 204 2,868
Enron-Random 89 82 83 286 5,059
Table 1: Summary of the corpora used in the experiments.
The number of words and names refer to the whole annotated
corpora.
3 Existing NER Methods
In our first set of experiments we apply CRF, a
machine-learning based probabilistic approach to la-
beling sequences of examples, and evaluate it on the
problem of extracting personal names from email.
Learning reduces NER to the task of tagging (i.e.,
classifying) each word in a document. We use a set
of five tags, corresponding to (1) a one-token entity,
(2) the first token of a multi-token entity, (3) the last
token of a multi-token entity, (4) any other token of
a multi-token entity and (5) a token that is not part
of an entity.
The sets of features used are presented in Table
2. All features are instantiated for the focus word, as
well as for a window of 3 tokens to the left and to the
right of the focus word. The basic features include
the lower-case value of a token t, and its capital-
ization pattern, constructed by replacing all capital
letters with the letter ?X?, all lower-case letters with
?x?, all digits with ?9? and compressing runs of the
same letter with a single letter. The dictionary fea-
tures define various categories of words including
common words, first names, last names 3 and ?roster
names? 4 (international names list, where first and
3We used US Census? lists of the most com-
mon first and last names in the US, available from
http://www.census.gov/genealogy/www/freqnames.html
4A dictionary of 16,623 student names across the country,
obtained as part of the RosterFinder project (Sweeney, 2003)
444
Basic Features
t, lexical value, lowercase (binary form, e.g. f(t=?hello?)=1)
capitalization pattern of t (binary form, e.g. f(t.cap=x+)=1)
Dictionary Features
inCommon: t in common words dictionary
inFirst: t in first names dictionary
inLast: t in last names dictionary
inRoster: t in roster names dictionary
First: inFirst ? ?isLast ? ?inCommon
Last: ?inFirst ? inLast ? ?inCommon
Name: (First ? Last ? inRoster) ? ? inCommon
Title: t in a personal prefixes/suffixes dictionary
Org: t in organization suffixes dictionary
Loc: t in location suffixes dictionary
Email Features
t appears in the header
t appears in the ?from? field
t is a probable ?signoff?
(? after two line breaks and near end of message)
t is part of an email address (regular expression)
does the word starts a new sentence
(? capitalized after a period, question or exclamation mark)
t is a probable initial (X or X.)
t followed by the bigram ?and I?
t capitalized and followed by a pronoun within 15 tokens
Table 2: Feature sets
last names are mixed.) In addition, we constructed
some composite dictionary features, as specified in
Table 2: for example, a word that is in the first-name
dictionary and is not in the common-words or last-
name dictionaries is designated a ?sure first name?.
The common-words dictionary used consists of
base forms, conjugations and plural forms of com-
mon English words, and a relatively small ad-hoc
dictionary representing words especially common in
email (e.g., ?email?, ?inbox?). We also use small
manually created word dictionaries of prefixes and
suffixes indicative of persons (e.g., ?mr?, ?jr?), loca-
tions (e.g., ?ave?) and organizations (e.g., ?inc?).
Email structure features: We perform a simplified
document analysis of the email message and use this
to construct some additional features. One is an in-
dicator as to whether a token t is equal to some to-
ken in the ?from? field. Another indicates whether
a token t in the email body is equal to some token
appearing in the whole header. An indicator feature
based on a regular expression is used to mark tokens
that are part of a probable ?sign-off? (i.e., a name at
the end of a message). Finally, since the annotation
rules do not consider email addresses to be names,
we added an indicator feature for tokens that are in-
side an email address.
l.2.mr l.1.president
l.2.mrs l.2.dr
l.1.jr r.2.who
l.1.judge r.2.jr
r.3.staff l.3.by
l.2.ms r.3.president
r.2.staff l.3.by
r.1.family l.3.rep
l.3.says l.2.rep
r.3.reporter r.1.administration
l.1.by r.2.home
l.2.by r.1.or
l.3.name l.1.with
l.2.name l.1.thanks
l.3.by r.1.picked
r.3.his l.3.meet
r.1.ps r.1.started
r.3.home r.1.told
r.1.and l.2.prof
l.1.called l.2.email
Figure 1: Predictive contexts for personal-name words for
MUC-6 (left) and Mgmt-Game (right) corpora. A features is
denoted by its direction comparing to the focus word (l/r), offset
and lexical value.
We experimented with features derived from POS
tags and NP-chunking of the email, but found the
POS assignment too noisy to be useful. We did in-
clude some features based on approximate linguistic
rules. One rule looks for capitalized words that are
not common words and are followed by a pronoun
within a distance of up to 15 tokens. (As an exam-
ple, consider ?Contact Puck tomorrow. He should be
around.?). Another rule looks for words followed by
the bigram ?and I?. As is common for hand-coded
NER rules, both these rules have high precision and
low recall.
3.1 Email vs Newswire
In order to explore some of the differences between
email and newswire NER problems, we stripped all
header fields from the Mgmt-Game messages, and
trained a model (using basic features only) from the
resulting corpus of email bodies. Figure 1 shows the
features most indicative of a token being part of a
name in the models trained for the Mgmt-Game and
MUC-6 corpora. To make the list easier to interpret,
it includes only the features corresponding to tokens
surrounding the focus word.
As one might expect, the important features from
the MUC-6 dataset are mainly formal name titles
such as ?mr?, ?mrs?, and ?jr?, as well as job ti-
tles and other pronominal modifiers such as ?pres-
ident? and ?judge?. However, for the Mgmt-Game
corpus, most of the important features are related
to email-specific structure. For example, the fea-
tures ?left.1.by? and ?left.2.by? are often associated
with a quoted excerpt from another email message,
which in the Mgmt-Game corpus is often marked
by mailers with text like ?Excerpts from mail: 7-
445
Sep-97 Re: paper deadline by Richard Wang?. Sim-
ilarly, features like ?left.1.thanks? and ?right.1.ps?
indicate a ?signoff? section of an email, as does
?right.2.home? (which often indicates proximity to
a home phone number appearing in a signature).
3.2 Experimental Results
We now turn to evaluate the usefulness of the fea-
ture sets described above. Table 3 gives entity-level
F1 performance 5 for CRF trained models for all
datasets, using the basic features alone (B); the ba-
sic and email-tailored features (B+E); the basic and
dictionary features (B+D); and, all of the feature sets
combined (B+D+E). All feature sets were tuned us-
ing the Mgmt-Game validation subset. The given
results relate to previously unseen test sets.
Dataset B B+E B+D B+D+E
Mgmt-Teams 68.1 75.7 82.0 87.9
Mgmt-Game 79.2 84.2 90.7 91.9
Enron-Meetings 59.0 71.5 78.6 76.9
Enron-Random 68.1 70.2 72.9 76.2
Table 3: F1 entity-leavel performance for the sets of features,
across all datasets, with CRF training.
The results show that the email-specific features
are very informative. In addition, they show that
the dictionary features are especially useful. This
can be explained by the relatively weak contextual
evidence in email. While dictionaries are useful in
named entities extraction in general, they are in fact
more essential when extracting names from email
text, where many name mentions are part of headers,
names lists etc. Finally, the results for the combined
feature set are superior in most cases to any subset
of the features.
Overall the level of performance using all fea-
tures is encouraging, considering the limited training
set size. Performance on Mgmt-Teams is somewhat
lower than for Mgmt-Game mainly because (by de-
sign) there is less similarity between training and
test sets with this split. Enron emails seem to be
harder than Mgmt-Game emails, perhaps because
they include fewer structured instances of names.
Enron-Meetings emails also contain a number of
constructs that were not encountered in the Mgmt-
Game corpus, notably lists (e.g., of people attending
a meeting), and also include many location and or-
5No credit awarded for partially correct entity boundaries.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 5  10  15  20  25  30  35
Mgmt Game
Enron-Meetings
Enron-Random
MUC-6
Figure 2: Cumulative percentage of person-name tokens w
that appear in at most K distinct documents as a function of K.
ganization names, which are rare in Mgmt-Game. A
larger set of dictionaries might improve performance
for the Enron corpora.
4 Repetition of named entities in email
In the experiments described above, the extractors
have high precision, but relatively low recall. This
typical behavior suggests that some sort of recall-
enhancing procedure might improve overall perfor-
mance.
One family of recall-enhancing techniques are
based on looking for multiple occurrences of names
in a document, so that names which occur in am-
biguous contexts will be more likely to be recog-
nized. It is an intuitive assumption that the ways in
which names repeat themselves in a corpus will be
different in email and newswire text. In news stories,
one would expect repetitions within a single docu-
ment to be common, as a means for an author to es-
tablish a shared context with the reader. In an email
corpus, one would expect names to repeat more fre-
quently across the corpus, in multiple documents?
at least when the email corpus is associated with a
group that works together closely. In this section we
support this conjecture with quantitative analysis.
In a first experiment, we plotted the percentage
of person-name tokens w that appear in at most
K distinct documents as a function of K. Figure
2 shows this function for the Mgmt-Game, MUC-
6, Enron-Meetings, and Enron-Random datasets.
There is a large separation between MUC-6 and
Mgmt-Game, the most workgroup-oriented email
corpus. In MUC-6, for instance, almost 80% of the
446
Single-Document Repetition
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Mgmt Game Mgmt
Teams
Enron
Meetings
Enron
Random
MUC-6
To
ke
n
 
R
ec
al
l
SDR
CRF
SDR+CRF
(a) SDR
Multiple-Document Repetition
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
100%
Mgmt Game Mgmt
Teams
Enron
Meetings
Enron
Random
MUC-6
To
ke
n
 
R
ec
al
l
MDR
CRF
MDR+CRF
(b) MDR
Figure 3: Upper bounds on recall and recall improvements
associated with methods that look for terms that re-occur within
a single document (SDR) or across multiple documents (MDR).
names appear in only a single document, while in
Mgmt-Game, only 30% of the names appear in only
a single document. At the other extreme, in MUC-6,
only 1.3% of the names appear in 10 or more docu-
ments, while in Mgmt-Game, almost 20% do. The
Enron-Random and Enron-Meetings datasets show
distributions of names that are intermediate between
Mgmt-Game and MUC-6.
As a second experiment, we implemented two
very simple extraction rules. The single document
repetition (SDR) rule marks every token that oc-
curs more than once inside a single document as a
name. Adding tokens marked by the SDR rule to
the tokens marked by the learned extractor generates
a new extractor, which we will denote SDR+CRF.
Thus, the recall of SDR+CRF serves as an upper
bound on the token recall6 of any recall-enhancing
6Token level recall is recall on the task of classifying tokens
as inside or outside an entity name.
method that improves the extractor by exploiting
repetition within a single document. Analogously,
the multiple document repetition (MDR) rule marks
every token that occurs in more than one document
as a name. Again, the token recall of MDR+CRF
rule is an upper bound on the token recall of any
recall-enhancing method that exploits token repeti-
tion across multiple documents.
The left bars in Figure 3 show the recall obtained
by the SDR (top) and the MDR rule (bottom). The
MDR rule has highest recall for the two Mgmt cor-
pora, and lowest recall for the MUC-6 corpus. Con-
versely, for the SDR rule, the highest recall level
obtained is for MUC-6. The middle bars show the
token recall obtained by the CRF extractor, using
all features. The right bars show the token recall
of the SDR+CRF and MDR+CRF extractors. Com-
paring them to the other bars, we see that the maxi-
mal potential recall gain from a SDR-like method is
on MUC-6. For MDR-like methods, there are large
potential gains on the Mgmt corpora as well as on
Enron-Meetings and Enron-Random to a lesser de-
gree. This probably reflects the fact that the Enron
corpora are from a larger and more weakly interact-
ing set of users, compared to the Mgmt datasets.
These results demonstrate the importance of ex-
ploiting repetition of names across multiple docu-
ments for entity extraction from email.
5 Improving Recall With Inferred
Dictionaries
Sequential learners of the sort used here classify to-
kens from each document independently; moreover,
the classification of a word w is independent of the
classification of other occurrences of w elsewhere in
the document. That is, the fact that a word w has ap-
peared somewhere in a context that clearly indicates
that it is a name does not increase the probability that
it will be classified as a name in other, more ambigu-
ous contexts.
Recently, sequential learning methods have been
extended to directly utilize information about name
co-occurrence in learning the sequential classifier.
This approach provides an elegant solution to mod-
eling repetition within a single document. However,
it requires identifying candidate related entities in
advance, applying some heuristic. Thus, Bunescu &
447
Mooney (2004) link between similar NPs (requiring
their head to be identical), and Sutton and Mccallum
(2004) connect pairs of identical capitalized words.
Given that in email corpora capitalization patterns
are not followed to a large extent, there is no ad-
equate heuristic that would link candidate entities
prior to extraction. Further, it is not clear if a col-
lective classification approach can scale to modeling
multiple-document repetition.
We suggest an alternative approach of recall-
enhancing name matching, which is appropriate for
email. Our approach has points of similarity to
the methods described by Stevenson and Gaizauskas
(2000), who suggest matching text against name dic-
tionaries, filtering out names that are also common
words or appear as non-names in high proportion
in the training data. The approach described here
is more systematic and general. In a nutshell, we
suggest applying the noisy dictionary of predicted
names over the test corpus, and use the approximate
(predicted) name to non-name proportions over the
test set itself to filter out ambiguous names. There-
fore, our approach does not require large amount of
annotated training data. It also does not require word
distribution to be similar between train and test data.
We will now describe our approach in detail.
5.1 Matching names from dictionary
First, we construct a dictionary comprised of all
spans predicted as names by the learned model. For
personal names, we suggest expanding this dictio-
nary further, using a transformation scheme. Such a
scheme would construct a family of possible varia-
tions of a name n: as an example, Figure 4 shows
name variations created for the name span ?Ben-
jamin Brown Smith?. Once a dictionary is formed,
a single pass is made through the corpus, and ev-
ery longest match to some name-variation is marked
as a name7. It may be that a partial name span n1
identified by the extractor is subsumed by the full
name span n2 identified by the dictionary-matching
scheme. In this case, entity-level precision is in-
creased, having corrected the entity?s boundaries.
7Initials-only variants of a name, e.g., ?bs? in Figure 4 are
marked as a name only if the ?inSignoff? feature holds?i.e., if
they appear near the end of a message in an apparent signature.
benjamin brown smith benjamin-brown-s. b. brown s. bbs
benjamin-brown smith benjamin-b. s. b. b. smith bs
benjamin brown-smith benjamin-smith b. brown-s.
benjamin-brown-smith benjamin smith benjamin
benjamin brown s. b. brown smith brown
benjamin-b. smith benjamin b. s. smith
benjamin b. smith b. brown-smith b. smith
benjamin brown-s. benjamin-s. b. b. s
benjamin-brown s. benjamin s. b. s.
Figure 4: Names variants created from the name ?Benjamin
Brown Smith?
5.2 Dictionary-filtering schemes
The noisy dictionary-matching scheme is suscepti-
ble to false positives. That is, some words predicted
by the extractor to be names are in fact non-names.
Presumably, these non-names could be removed by
simply eliminating low-confidence predictions of
the extractor; however, ambiguous words ?that are
not exclusively personal names in the corpus? may
need to be identified and removed as well. We note
that ambiguity better be evaluated in the context of
the corpus. For example, ?Andrew? is a common
first name, and may be confidently (and correctly)
recognized as one by the extractor. However, in the
Mgmt-Game corpus, ?Andrew? is also the name of
an email server, and most of the occurrences of this
name in this corpus are not personal names. The
high frequency of the word ?Andrew? in the cor-
pus, coupled with the fact that it is only sometimes a
name, means that adding this word to the dictionary
leads to a substantial drop in precision.
We therefore suggest a measure for filtering the
dictionary. This measure combines two metrics. The
first metric, predicted frequency (PF), estimates the
degree to which a word appears to be used consis-
tently as a name throughout the corpus:
PF (w) ? cpf(w)ctf(w)
where cpf(w) denotes the number of times that a
word w is predicted as part of a name by the extrac-
tor, and ctf(w) is the number of occurrences of the
word w in the entire test corpus (we emphasize that
estimating this statistic based on test data is valid, as
it is fully automatic ?blind? procedure).
Predicted frequency does not assess the likely cost
of adding a word to a dictionary: as noted above,
ambiguous or false dictionary terms that occur fre-
quently will degrade accuracy. A number of statis-
tics could be used here; for instance, practitioners
448
sometimes filter a large dictionary by simply dis-
carding all words that occur more than k times in a
test corpus. We elected to use the inverse document
frequency (IDF) of w to measure word frequency:
IDF (w) ?
log(N+0.5df(w) )
log(N + 1)
Here df(w) is the number of documents that contain
a word w, and N is the total number of documents
in the corpus. Inverse document frequency is often
used in the field of information retrieval (Allan et al,
1998), and the formula above has the virtue of being
scaled between 0 and 1 (like our PF metric) and of
including some smoothing. In addition to bounding
the cost of a dictionary entry, the IDF formula is in
itself a sensible filter, since personal names will not
appear as frequently as common English words.
The joint filter combines these two multiplica-
tively, with equal weights:
PF.IDF (w) : PF (w) ? IDF (w)
PF.IDF takes into consideration both the probability
of a word being a name, and how common it is in
the entire corpus. Words that get low PF.IDF scores
are therefore either words that are highly ambiguous
in the corpus (as derived from the extractors? pre-
dictions) or are common words, which were inaccu-
rately predicted as names by the extractor.
In the MDR method of Figure 3, we imposed
an artificial requirement that words must appear in
more than one document. In the method described
here, there is no such requirement: indeed, words
that appear in a small number of documents are
given higher weights, due to the IDF factor. Thus
this approach exploits both single-document and
multiple-document repetitions.
In a set of experiments that are not described here,
the PF.IDF measure was found to be robust to pa-
rameter settings, and also preferable to its separate
components in improving recall at minimal cost in
precision. As described, the PF.IDF values per word
range between 0 and 1. One can vary the threshold,
under which a word is to be removed from the dic-
tionary, to control the precision-recall trade-off. We
tuned the PF.IDF threshold using the validation sub-
sets, optimizing entity-level F1 (a threshold of 0.16
was found optimal).
In summary, our recall-enhancing strategy is as
follows:
1. Learn an extractor E from the training corpus Ctrain .
2. Apply the extractor E to a test corpus Ctest to assign a
preliminary labeling.
3. Build a dictionary S?? including the names n such that
(a) n is extracted somewhere in the preliminary label-
ing of the test corpus, or is derived from an extracted
name applying the name transformation scheme and (b)
PF.IDF (n) > ??.
4. Apply the dictionary-matching scheme of Section 5.1, us-
ing the dictionary S?? to augment the preliminary label-
ing, and output the result.
5.3 Experiments with inferred dictionaries
Table 4 shows results using the method described
above. We consider all of the email corpora and the
CRF learner, trained with the full feature set. The
results are given in terms of relative change, com-
pared to the baseline results generated by the extrac-
tors (scoreresult/scorebaseline ? 1) and final value.
As expected, recall is always improved. Entity-
level F1 is increased as well, as recall is increased
more than precision is decreased. The largest im-
provements are for the Mgmt corpora ?the two e-
mail datasets shown to have the largest potential im-
provement from MDR-like methods in Figure 3. Re-
call improvements are more modest for the Enron
datasets, as was anticipated by the MDR analysis.
Another reason for the gap is that extractor baseline
performance is lower for the Enron datasets, so that
the Enron dictionaries are noisier.
As detailed in Section 2, the Mgmt-Teams dataset
was constructed so that the names in the training
and test set have only minimal overlap. The perfor-
mance improvement on this dataset shows that rep-
etition of mostly-novel names can be detected using
our method. This technique is highly effective when
names are novel, or dense, and is optimal when ex-
tractor baseline precision is relatively high.
Dataset Precision Recall F1
Mgmt-Teams -0.9% / 92.9 +8.5% / 89.8 +3.9% / 91.3
Mgmt-Game -0.8% / 94.5 +8.4% / 96.2 +3.8% / 95.4
Enron-Meetings -2.5% / 81.1 +4.7% / 74.9 +1.2% / 77.9
Enron-Random -3.8% / 79.2 +4.9% / 74.3 +0.7% / 76.7
Table 4: Entity-level relative improvement and final result,
applying name-matching on models trained with CRF and the
full feature set (F1 baseline given in Table 3).
449
6 Conclusion
This work applies recently-developed sequential
learning methods to the task of extraction of named
entities from email. This problem is of interest as an
example of NER from informal text?text that has
been prepared quickly for a narrow audience.
We showed that informal text has different char-
acteristics from formal text such as newswire. Anal-
ysis of the highly-weighted features selected by the
learners showed that names in informal text have
different (and less informative) types of contextual
evidence. However, email also has some structural
regularities which make it easier to extract personal
names. We presented a detailed description of a set
of features that address these regularities and signif-
icantly improve extraction performance on email.
In the second part of this paper, we analyzed
the way in which names repeat in different types
of corpora. We showed that repetitions within a
single document are more common in newswire
text, and that repetitions that span multiple docu-
ments are more common in email corpora. Addi-
tional analysis confirms that the potential gains in
recall from exploiting multiple-document repetition
is much higher than the potential gains from exploit-
ing single-document repetition.
Based on this insight, we introduced a simple and
effective method for exploiting multiple-document
repetition to improve an extractor. One drawback of
the recall-enhancing approach is that it requires the
entire test set to be available: however, our test sets
are of only moderate size (83 to 264 documents),
and it is likely that a similar-size sample of unlabeled
data would be available in many practical applica-
tions. The approach substantially improves recall
and often improves F1 performance; furthermore, it
can be easily used with any NER method.
Taken together, extraction performance is sub-
stantially improved by this approach. The improve-
ments seem to be strongest for email corpora col-
lected from closely interacting groups. On the
Mgmt-Teams dataset, which was designed to reduce
the value of memorizing specific names appearing
in the training set, F1 performance is improved from
68.1% for the out-of-the-box system (or 82.0% for
the dictionary-augmented system) to 91.3%. For the
less difficult Mgmt-Game dataset, F1 performance
is improved from 79.2% for an out-of-the-box CRF-
based NER system (or 90.7% for a CRF-based sys-
tem that uses several large dictionaries) to 95.4%.
As future work, experiments should be expanded to
include additional entity types and other types of in-
formal text, such as blogs and forum postings.
References
J. Allan, J. Callan, W.B. Croft, L. Ballesteros, D. Byrd,
R. Swan, and J. Xu. 1998. Inquery does battle with trec-
6. In TREC-6.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel. 1999. An
algorithm that learns what?s in a name. Machine Learning,
34:211?231.
R. Bunescu and R. J. Mooney. 2004. Relational markov net-
works for collective information extraction. In ICML-2004
Workshop on Statistical Relational Learning.
W. W. Cohen, E. Minkov, and A. Tomasic. 2005. Learning to
undertand website update requests. In IJCAI-05.
M. Craven and J. Kumlien. 1999. Constructing biologi-
cal knowledge bases by extracting information from text
sources. In ISMB-99.
A. Culotta, R. Bekkerman, and A. McCallum. 2004. Extracting
social networks and contact information from email and the
web. In CEAS-04.
D. Freitag. 1998. Information extraction from html: applica-
tion of a general machine learning approach. In AAAI-98.
K. Humphreys, R. Gaizauskas, S. Azzam, C. Huyck,
B. Mitchell, H. Cunningham, and Y. Wilks. 1998. Descrip-
tion of the LASIE-II system as used for MUC-7.
B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In
CEAS-04.
R. E. Kraut, S. R. Fussell, F. J. Lerch, and J. A. Espinosa. 2004.
Coordination in teams: evi-dence from a simulated manage-
ment game. To appear in the Journal of Organizational Be-
havior.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML-01.
A. McCallum and W. Li. 2003. Early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In CoNLL-2003.
M. Stevenson and R. Gaizauskas. 2000. Using corpus-derived
names lists for named entities recognition. In NAACL-2000.
C. Sutton and A. Mccallum. 2004. Collective segmentation
and labeling of distant entities in information extraction. In
ICML workshop on Statistical Relational Learning.
L. Sweeney. 2003. Finding lists of people on the web.
Technical Report CMU-CS-03-168, CMU-ISRI-03-104.
http://privacy.cs.cmu.edu/dataprivacy/ projects/rosterfinder/.
450
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 93?96,
New York, June 2006. c?2006 Association for Computational Linguistics
NER Systems that Suit User?s Preferences: Adjusting the Recall-Precision
Trade-off for Entity Extraction
Einat Minkov, Richard C. Wang
Language Technologies
Institute
Carnegie Mellon University
einat,rcwang@cs.cmu.edu
Anthony Tomasic
Inst. for Software Research
International
Carnegie Mellon University
tomasic@cs.cmu.edu
William W. Cohen
Machine Learning Dept.
Carnegie Mellon University
wcohen@cs.cmu.edu
Abstract
We describe a method based on ?tweak-
ing? an existing learned sequential classi-
fier to change the recall-precision tradeoff,
guided by a user-provided performance
criterion. This method is evaluated on
the task of recognizing personal names in
email and newswire text, and proves to be
both simple and effective.
1 Introduction
Named entity recognition (NER) is the task of iden-
tifying named entities in free text?typically per-
sonal names, organizations, gene-protein entities,
and so on. Recently, sequential learning methods,
such as hidden Markov models (HMMs) and con-
ditional random fields (CRFs), have been used suc-
cessfully for a number of applications, including
NER (Sha and Pereira, 2003; Pinto et al, 2003; Mc-
callum and Lee, 2003). In practice, these methods
provide imperfect performance: precision and re-
call, even for well-studied problems on clean well-
written text, reach at most the mid-90?s. While
performance of NER systems is often evaluated in
terms of F1 measure (a harmonic mean of preci-
sion and recall), this measure may not match user
preferences regarding precision and recall. Further-
more, learned NER models may be sub-optimal also
in terms of F1, as they are trained to optimize other
measures (e.g., loglikelihood of the training data for
CRFs).
Obviously, different applications of NER have
different requirements for precision and recall. A
system might require high precision if it is designed
to extract entities as one stage of fact-extraction,
where facts are stored directly into a database. On
the other hand, a system that generates candidate ex-
tractions which are passed to a semi-automatic cu-
ration system might prefer higher recall. In some
domains, such as anonymization of medical records,
high recall is essential.
One way to manipulate an extractor?s precision-
recall tradeoff is to assign a confidence score to each
extracted entity and then apply a global threshold to
confidence level. However, confidence thresholding
of this sort cannot increase recall. Also, while confi-
dence scores are straightforward to compute in many
classification settings, there is no inherent mecha-
nism for computing confidence of a sequential ex-
tractor. Culotta and McCallum (2004) suggest sev-
eral methods for doing this with CRFs.
In this paper, we suggest an alternative simple
method for exploring and optimizing the relation-
ship between precision and recall for NER systems.
In particular, we describe and evaluate a technique
called ?extractor tweaking? that optimizes a learned
extractor with respect to a specific evaluation met-
ric. In a nutshell, we directly tweak the threashold
term that is part of any linear classifier, including se-
quential extractors. Though simple, this approach
has not been empirically evaluated before, to our
knowledge. Further, although sequential extractors
such as HMMs and CRFs are state-of-the-art meth-
ods for tasks like NER, there has been little prior re-
search about tuning these extractors? performance to
suit user preferences. The suggested algorithm op-
timizes the system performance per a user-provided
93
evaluation criterion, using a linear search procedure.
Applying this procedure is not trivial, since the un-
derlying function is not smooth. However, we show
that the system?s precision-recall rate can indeed be
tuned to user preferences given labelled data using
this method. Empirical results are presented for a
particular NER task?recognizing person names, for
three corpora, including email and newswire text.
2 Extractor tweaking
Learning methods such as VP-HMM and CRFs op-
timize criteria such as margin separation (implicitly
maximized by VP-HMMs) or log-likelihood (ex-
plicitly maximized by CRFs), which are at best indi-
rectly related to precision and recall. Can such learn-
ing methods be modified to more directly reward a
user-provided performance metric?
In a non-sequential classifier, a threshold on confi-
dence can be set to alter the precision-recall tradeoff.
This is nontrivial to do for VP-HMMs and CRFs.
Both learners use dynamic programming to find the
label sequence y = (y1, . . . , yi, . . . , yN ) for a word
sequence x = (x1, . . . , xi, . . . , xN ) that maximizes
the function W ?
?
i f(x, i, yi?1, yi) , where W is
the learned weight vector and f is a vector of fea-
tures computed from x, i, the label yi for xi, and the
previous label yi?1. Dynamic programming finds
the most likely state sequence, and does not output
probability for a particular sub-sequence. (Culotta
and McCallum, 2004) suggest several ways to gen-
erate confidence estimation in this framework. We
propose a simpler approach for directly manipulat-
ing the learned extractor?s precision-recall ratio.
We will assume that the labels y include one label
O for ?outside any named entity?, and let w0 be the
weight for the feature f0, defined as follows:
f0(x, i, yi?1, yi) ?
{
1 if yi = O
0 else
If no such feature exists, then we will create one.
The NER based on W will be sensitive to the value
of w0: large negative values will force the dynamic
programming method to label tokens as inside enti-
ties, and large positive values will force it to label
fewer entities1.
1We clarify that w0 will refer to feature f0 only, and not to
other features that may incorporate label information.
We thus propose to ?tweak? a learned NER by
varying the single parameter w0 systematically so as
to optimize some user-provided performance metric.
Specifically, we tune w0 using a a Gauss-Newton
line search, where the objective function is itera-
tively approximated by quadratics.2 We terminate
the search when two adjacent evaluation results are
within a 0.01% difference3.
A variety of performance metrics might be imag-
ined: for instance, one might wish to optimize re-
call, after applying some sort of penalty for pre-
cision below some fixed threshold. In this paper
we will experiment with performance metrics based
on the (complete) F-measure formula, which com-
bines precision and recall into a single numeric value
based on a user-provided parameter ?:
F (?, P,R) = (?
2 + 1)PR
?2P +R
A value of ? > 1 assigns higher importance to re-
call. In particular, F2 weights recall twice as much
as precision. Similarly, F0.5 weights precision twice
as much as recall.
We consider optimizing both token- and entity-
level F? ? awarding partial credit for partially ex-
tracted entities and no credit for incorrect entity
boundaries, respectively. Performance is optimized
over the dataset on which W was trained, and tested
on a separate set. A key question our evaluation
should address is whether the values optimized for
the training examples transfer well to unseen test ex-
amples, using the suggested approximate procedure.
3 Experiments
3.1 Experimental Settings
We experiment with three datasets, of both email
and newswire text. Table 1 gives summary statis-
tics for all datasets. The widely-used MUC-6 dataset
includes news articles drawn from the Wall Street
Journal. The Enron dataset is a collection of emails
extracted from the Enron corpus (Klimt and Yang,
2004), where we use a subcollection of the mes-
sages located in folders named ?meetings? or ?cal-
endar?. The Mgmt-Groups dataset is a second email
2from http://billharlan.com/pub/code/inv.
3In the experiments, this is usually within around 10 itera-
tions. Each iteration requires evaluating a ?tweaked? extractor
on a training set.
94
collection, extracted from the CSpace email cor-
pus, which contains email messages sent by MBA
students taking a management course conducted at
Carnegie Mellon University in 1997. This data was
split such that its test set contains a different mix of
entity names comparing to training exmaples. Fur-
ther details about these datasets are available else-
where (Minkov et al, 2005).
# documents # names
Train Test # tokens per doc.
MUC-6 347 30 204,071 6.8
Enron 833 143 204,423 3.0
Mgmt-Groups 631 128 104,662 3.7
Table 1: Summary of the corpora used in the experiments
We used an implementation of Collins? voted-
percepton method for discriminatively training
HMMs (henceforth, VP-HMM) (Collins, 2002) as
well as CRF (Lafferty et al, 2001) to learn a NER.
Both VP-HMM and CRF were trained for 20 epochs
on every dataset, using a simple set of features such
as word identity and capitalization patterns for a
window of three words around each word being clas-
sified. Each word is classified as either inside or out-
side a person name.4
3.2 Extractor tweaking Results
Figure 1 evaluates the effectiveness of the optimiza-
tion process used by ?extractor tweaking? on the
Enron dataset. We optimized models for F? with
different values of ?, and also evaluated each op-
timized model with different F? metrics. The top
graph shows the results for token-level F? , and the
bottom graph shows entity-level F? behavior. The
graph illustates that the optimized model does in-
deed roughly maximize performance for the target
? value: for example, the token-level F? curve for
the model optimized for ? = 0.5 indeed peaks at
? = 0.5 on the test set data. The optimization is
only roughly accurate5 for several possible reasons:
first, there are differences between train and test sets;
in addition, the line search assumes that the perfor-
mance metric is smooth and convex, which need
not be true. Note that evaluation-metric optimiza-
tion is less successful for entity-level performance,
4This problem encoding is basic. However, in the context of
this paper we focus on precision-recall trade-off in the general
case, avoiding settings? optimization.
5E.g, the token-level F2 curve peaks at ? = 5.
 50
 55
 60
 65
 70
 75
 80
 85
 90
5.02.01.00.50.2
F(
Be
ta)
Beta
0.2
0.5
1.0
2.0
5.0
 50
 55
 60
 65
 70
 75
 80
 85
 90
5.02.01.00.50.2
F(
Be
ta)
Beta
0.2
0.5
1.0
2.0
5.0
Figure 1: Results of token-level (top) and entity-level (bot-
tom) optimization for varying values of ?, for the Enron dataset,
VP-HMM. The y-axis gives F in terms of ?. ? (x-axis) is given
in a logarithmic scale.
which behaves less smoothly than token-level per-
formance.
Token Entity
? Prec Recall Prec Recall
Baseline 93.3 76.0 93.6 70.6
0.2 100 53.2 98.2 57.0
0.5 95.3 71.1 94.4 67.9
1.0 88.6 79.4 89.2 70.9
2.0 81.0 83.9 81.8 70.9
5.0 65.8 91.3 69.4 71.4
Table 2: Sample optimized CRF results, for the MUC-6
dataset and entity-level optimization.
Similar results were obtained optimizing baseline
CRF classifiers. Sample results (for MUC-6 only,
due to space limitations) are given in Table 2, opti-
mizing a CRF baseline for entity-level F? . Note that
as ? increases, recall monotonically increases and
precision monotonically falls.
The graphs in Figure 2 present another set of re-
sults with a more traditional recall-precision curves.
The top three graphs are for token-level F? opti-
mization, and the bottom three are for entity-level
optimization. The solid lines show the token-level
and entity-level precision-recall tradeoff obtained by
95
MUC-6 Enron M.Groups
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Token-level
Entity-level
Token-level baseline
Entity-level baseline
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Pr
ec
is
io
n
Recall
Token-level
Entity-level
Token-level baseline
Entity-level baseline
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
 50
 60
 70
 80
 90
 100
 50  60  70  80  90  100
Recall
Figure 2: Results for the evaluation-metric model optimization. The top three graphs are for token-level F (?) optimization,
and the bottom three are for entity-level optimization. Each graph shows the baseline learned VP-HMM and evaluation-metric
optimization for different values of ?, in terms of both token-level and entity-level performance.
varying6 ? and optimizing the relevant measure for
F? ; the points labeled ?baseline? show the precision
and recall in token and entity level of the baseline
model, learned by VP-HMM. These graphs demon-
strate that extractor ?tweaking? gives approximately
smooth precision-recall curves, as desired. Again,
we note that the resulting recall-precision trade-
off for entity-level optimization is generally less
smooth.
4 Conclusion
We described an approach that is based on mod-
ifying an existing learned sequential classifier to
change the recall-precision tradeoff, guided by a
user-provided performance criterion. This approach
not only allows one to explore a recall-precision
tradeoff, but actually allows the user to specify a
performance metric to optimize, and optimizes a
learned NER system for that metric. We showed
that using a single free parameter and a Gauss-
Newton line search (where the objective is itera-
tively approximated by quadratics), effectively op-
timizes two plausible performance measures, token-
6We varied ? over the values 0.2, 0.5, 0.8, 1, 1.2, 1.5, 2, 3
and 5
level F? and entity-level F? . This approach is in
fact general, as it is applicable for sequential and/or
structured learning applications other than NER.
References
M. Collins. 2002. Discriminative training methods for hidden
markov models: Theory and experiments with perceptron al-
gorithms. In EMNLP.
A. Culotta and A. McCallum. 2004. Confidence estimation for
information extraction. In HLT-NAACL.
B. Klimt and Y. Yang. 2004. Introducing the Enron corpus. In
CEAS.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and la-
beling sequence data. In ICML.
A. Mccallum and W. Lee. 2003. early results for named entity
recognition with conditional random fields, feature induction
and web-enhanced lexicons. In CONLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Extracting
personal names from emails: Applying named entity recog-
nition to informal text. In HLT-EMNLP.
D. Pinto, A. Mccallum, X. Wei, and W. B. Croft. 2003. table
extraction using conditional random fields. In ACM SIGIR.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In HLT-NAACL.
96
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128?135,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generating Complex Morphology for Machine Translation
Einat Minkov?
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA, USA
einatm@cs.cmu.edu
Kristina Toutanova
Microsoft Research
Redmond, WA, USA
kristout@microsoft.com
Hisami Suzuki
Microsoft Research
Redmond, WA, USA
hisamis@microsoft.com
Abstract
We present a novel method for predicting in-
flected word forms for generating morpho-
logically rich languages in machine trans-
lation. We utilize a rich set of syntactic
and morphological knowledge sources from
both source and target sentences in a prob-
abilistic model, and evaluate their contribu-
tion in generating Russian and Arabic sen-
tences. Our results show that the proposed
model substantially outperforms the com-
monly used baseline of a trigram target lan-
guage model; in particular, the use of mor-
phological and syntactic features leads to
large gains in prediction accuracy. We also
show that the proposed method is effective
with a relatively small amount of data.
1 Introduction
Machine Translation (MT) quality has improved
substantially in recent years due to applying data
intensive statistical techniques. However, state-of-
the-art approaches are essentially lexical, consider-
ing every surface word or phrase in both the source
sentence and the corresponding translation as an in-
dependent entity. A shortcoming of this word-based
approach is that it is sensitive to data sparsity. This is
an issue of importance as aligned corpora are an ex-
pensive resource, which is not abundantly available
for many language pairs. This is particularly prob-
lematic for morphologically rich languages, where
word stems are realized in many different surface
forms, which exacerbates the sparsity problem.
? This research was conducted during the author?s intern-
ship at Microsoft Research.
In this paper, we explore an approach in which
words are represented as a collection of morpholog-
ical entities, and use this information to aid in MT
for morphologically rich languages. Our goal is two-
fold: first, to allow generalization over morphology
to alleviate the data sparsity problem in morphology
generation. Second, to model syntactic coherence in
the form of morphological agreement in the target
language to improve the generation of morphologi-
cally rich languages. So far, this problem has been
addressed in a very limited manner in MT, most typ-
ically by using a target language model.
In the framework suggested in this paper, we train
a model that predicts the inflected forms of a se-
quence of word stems in a target sentence, given
the corresponding source sentence. We use word
and word alignment information, as well as lexi-
cal resources that provide morphological informa-
tion about the words on both the source and target
sides. Given a sentence pair, we also obtain syntactic
analysis information for both the source and trans-
lated sentences. We generate the inflected forms of
words in the target sentence using all of the available
information, using a log-linear model that learns the
relevant mapping functions.
As a case study, we focus on the English-Russian
and English-Arabic language pairs. Unlike English,
Russian and Arabic have very rich systems of mor-
phology, each with distinct characteristics. Trans-
lating from a morphology-poor to a morphology-
rich language is especially challenging since de-
tailed morphological information needs to be de-
coded from a language that does not encode this in-
formation or does so only implicitly (Koehn, 2005).
We believe that these language pairs are represen-
128
tative in this respect and therefore demonstrate the
generality of our approach.
There are several contributions of this work. First,
we propose a general approach that shows promise
in addressing the challenges of MT into morpholog-
ically rich languages. We show that the use of both
syntactic and morphological information improves
translation quality. We also show the utility of
source language information in predicting the word
forms of the target language. Finally, we achieve
these results with limited morphological resources
and training data, suggesting that the approach is
generally useful for resource-scarce language pairs.
2 Russian and Arabic Morphology
Table 1 describes the morphological features rele-
vant to Russian and Arabic, along with their possible
values. The rightmost column in the table refers to
the morphological features that are shared by Rus-
sian and Arabic, including person, number, gender
and tense. While these features are fairly generic
(they are also present in English), note that Rus-
sian includes an additional gender (neuter) and Ara-
bic has a distinct number notion for two (dual). A
central dimension of Russian morphology is case
marking, realized as suffixation on nouns and nom-
inal modifiers1. The Russian case feature includes
six possible values, representing the notions of sub-
ject, direct object, location, etc. In Arabic, like other
Semitic languages, word surface forms may include
proclitics and enclitics (or prefixes and suffixes as
we refer to them in this paper), concatenated to in-
flected stems. For nouns, prefixes include conjunc-
tions (wa: ?and?, fa: ?and, so?), prepositions (bi:
?by, with?, ka: ?like, such as?, li: ?for, to?) and a de-
terminer, and suffixes include possessive pronouns.
Verbal prefixes include conjunction and negation,
and suffixes include object pronouns. Both object
and possessive pronouns are captured by an indica-
tor function for its presence or absence, as well as
by the features that indicate their person, number
and gender. As can be observed from the table, a
large number of surface inflected forms can be gen-
erated by the combination of these features, making
1Case marking also exists in Arabic. However, in many in-
stances, it is realized by diacritics which are ignored in standard
orthography. In our experiments, we include case marking in
Arabic only when it is reflected in the orthography.
the morphological generation of these languages a
non-trivial task.
Morphologically complex languages also tend to
display a rich system of agreements. In Russian, for
example, adjectives agree with head nouns in num-
ber, gender and case, and verbs agree with the sub-
ject noun in person and number (past tense verbs
agree in gender and number). Arabic has a similarly
rich system of agreement, with unique characteris-
tics. For example, in addition to agreement involv-
ing person, number and gender, it also requires a de-
terminer for each word in a definite noun phrase with
adjectival modifiers; in a noun compound, a deter-
miner is attached to the last noun in the chain. Also,
non-human subject plural nouns require the verb to
be inflected in a singular feminine form. Generating
these morphologically complex languages is there-
fore more difficult than generating English in terms
of capturing the agreement phenomena.
3 Related Work
The use of morphological features in language mod-
elling has been explored in the past for morphology-
rich languages. For example, (Duh and Kirchhoff,
2004) showed that factored language models, which
consider morphological features and use an opti-
mized backoff policy, yield lower perplexity.
In the area of MT, there has been a large body
of work attempting to modify the input to a transla-
tion system in order to improve the generated align-
ments for particular language pairs. For example,
it has been shown (Lee, 2004) that determiner seg-
mentation and deletion in Arabic sentences in an
Arabic-to-English translation system improves sen-
tence alignment, thus leading to improved over-
all translation quality. Another work (Koehn and
Knight, 2003) showed improvements by splitting
compounds in German. (Nie?en and Ney, 2004)
demonstrated that a similar level of alignment qual-
ity can be achieved with smaller corpora applying
morpho-syntactic source restructuring, using hierar-
chical lexicon models, in translating from German
into English. (Popovic? and Ney, 2004) experimented
successfully with translating from inflectional lan-
guages into English making use of POS tags, word
stems and suffixes in the source language. More re-
cently, (Goldwater and McClosky, 2005) achieved
improvements in Czech-English MT, optimizing a
129
Features Russian Arabic Both
POS (11 categories) (18 categories)
Person 1,2,3
Number dual sing(ular), pl(ural)
Gender neut(er) masc(uline), fem(inine)
Tense gerund present, past, future, imperative
Mood subjunctive, jussive
Case dat(ive), prep(ositional), nom(inative), acc(usative), gen(itive)
instr(umental)
Negation yes, no
Determiner yes, no
Conjunction wa, fa, none
Preposition bi, ka, li, none
ObjectPronoun yes, no
Pers/Numb/Gend of pronoun, none
PossessivePronoun Same as ObjectPronoun
Table 1: Morphological features used for Russian and Arabic
set of possible source transformations, incorporat-
ing morphology. In general, this line of work fo-
cused on translating from morphologically rich lan-
guages into English; there has been limited research
in MT in the opposite direction. Koehn (2005) in-
cludes a survey of statistical MT systems in both di-
rections for the Europarl corpus, and points out the
challenges of this task. A recent work (El-Kahlout
and Oflazer, 2006) experimented with English-to-
Turkish translation with limited success, suggesting
that inflection generation given morphological fea-
tures may give positive results.
In the current work, we suggest a probabilistic
framework for morphology generation performed as
post-processing. It can therefore be considered as
complementary to the techniques described above.
Our approach is general in that it is not specific to
a particular language pair, and is novel in that it al-
lows modelling of agreement on the target side. The
framework suggested here is most closely related to
(Suzuki and Toutanova, 2006), which uses a proba-
bilistic model to generate Japanese case markers for
English-to-Japanese MT. This work can be viewed
as a generalization of (Suzuki and Toutanova, 2006)
in that our model generates inflected forms of words,
and is not limited to generating a small, closed set of
case markers. In addition, the morphology genera-
tion problem is more challenging in that it requires
handling of complex agreement phenomena along
multiple morphological dimensions.
4 Inflection Prediction Framework
In this section, we define the task of of morphologi-
cal generation as inflection prediction, as well as the
lexical operations relevant for the task.
4.1 Morphology Analysis and Generation
Morphological analysis can be performed by ap-
plying language specific rules. These may include
a full-scale morphological analysis with contextual
disambiguation, or, when such resources are not
available, simple heuristic rules, such as regarding
the last few characters of a word as its morphogical
suffix. In this work, we assume that lexicons LS and
LT are available for the source and translation lan-
guages, respectively. Such lexicons can be created
manually, or automatically from data. Given a lexi-
con L and a surface word w, we define the following
operations:
? Stemming - let Sw = {s1, ..., sl} be the set of
possible morphological stems (lemmas) of w
according to L.2
? Inflection - let Iw = {i1, ..., im} be the set of
surface form words that have the same stem as
w. That is, i ? Iw iff Si?Sw 6= ?.
? Morphological analysis - let Aw = {a1, ..., av}
be the set of possible morphological analyses
for w. A morphological analysis a is a vector of
categorical values, where the dimensions and
possible values for each dimension in the vector
representation space are defined by L.
4.2 The Task
We assume that we are given aligned sentence pairs,
where a sentence pair includes a source and a tar-
2Multiple stems are possible due to ambiguity in morpho-
logical analysis.
130
NN+sg+nom+neut
the
DET
allocation of resources has completed
NN+sg PREP NN+pl AUXV+sg VERB+pastpart
?????????????
NN+sg+gen+pl+masc
????????
VERB+perf+pass+part+neut+sg
?????????
raspredelenie resursov zaversheno
Figure 1: Aligned English-Russian sentence pair
with syntactic and morphological annotation
get sentence, and lexicons LS and LT that support
the operations described in the section above. Let
a sentence w1, ...wt, ...wn be the output of a MT
system in the target language. This sentence can
be converted into the corresponding stem set se-
quence S1, ...St, ...Sn, applying the stemming op-
eration. Then the task is, for every stem set St in
the output sentence, to predict an inflection yt from
its inflection set It. The predicted inflections should
both reflect the meaning conveyed by the source sen-
tence, and comply with the agreement rules of the
target language. 3
Figure 1 shows an example of an aligned English-
Russian sentence pair: on the source (English) side,
POS tags and word dependency structure are indi-
cated by solid arcs. The alignments between En-
glish and Russian words are indicated by the dot-
ted lines. The dependency structure on the Russian
side, indicated by solid arcs, is given by a treelet MT
system in our case (see Section 6.1), projected from
the word dependency structure of English and word
alignment information. Note that the Russian sen-
tence displays agreement in number and gender be-
tween the subject noun (raspredelenie) and the pred-
icate (zaversheno); note also that resursov is in gen-
itive case, as it modifies the noun on its left.
5 Models for Inflection Prediction
5.1 A Probabilistic Model
Our learning framework uses a Maximum Entropy
Markov model (McCallum et al, 2000). The model
decomposes the overall probability of a predicted
inflection sequence into a product of local proba-
bilities for individual word predictions. The local
3That is, assuming that the stem sequence that is output by
the MT system is correct.
probabilities are conditioned on the previous k pre-
dictions. The model implemented here is of second
order: at any decision point t we condition the prob-
ability distribution over labels on the previous two
predictions yt?1 and yt?2 in addition to the given
(static) word context from both the source and tar-
get sentences. That is, the probability of a predicted
inflection sequence is defined as follows:
p(y | x) =
n
?
t=1
p(yt | yt?1, yt?2, xt), yt ? It
where xt denotes the given context at position t
and It is the set of inflections corresponding to St,
from which the model should choose yt.
The features we constructed pair up predicates on
the context ( x?, yt?1, yt?2) and the target label (yt).
In the suggested framework, it is straightforward to
encode the morphological properties of a word, in
addition to its surface inflected form. For example,
for a particular inflected word form yt and its con-
text, the derived paired features may include:
?k =
{
1 if surface word yt is y? and s? ? St+1
0 otherwise
?k+1 =
{ 1 if Gender(yt) =?Fem? and Gender(yt?1) =?Fem?
0 otherwise
In the first example, a given neighboring stem set
St+1 is used as a context feature for predicting the
target word yt. The second feature captures the gen-
der agreement with the previous word. This is possi-
ble because our model is of second order. Thus, we
can derive context features describing the morpho-
logical properties of the two previous predictions.4
Note that our model is not a simple multi-class clas-
sifier, because our features are shared across mul-
tiple target labels. For example, the gender fea-
ture above applies to many different inflected forms.
Therefore, it is a structured prediction model, where
the structure is defined by the morphological proper-
ties of the target predictions, in addition to the word
sequence decomposition.
5.2 Feature Categories
The information available for estimating the distri-
bution over yt can be split into several categories,
4Note that while we decompose the prediction task left-to-
right, an appealing alternative is to define a top-down decompo-
sition, traversing the dependency tree of the sentence. However,
this requires syntactic analysis of sufficient quality.
131
corresponding to feature source. The first ma-
jor distinction is monolingual versus bilingual fea-
tures: monolingual features refer only to the context
(and predicted label) in the target language, while
bilingual features have access to information in the
source sentences, obtained by traversing the word
alignment links from target words to a (set of) source
words, as shown in Figure 1.
Both monolingual and bilingual features can be
further split into three classes: lexical, morpholog-
ical and syntactic. Lexical features refer to surface
word forms, as well as their stems. Since our model
is of second order, our monolingual lexical fea-
tures include the features of a standard word trigram
language model. Furthermore, since our model is
discriminative (predicting word forms given their
stems), the monolingual lexical model can use stems
in addition to predicted words for the left and cur-
rent position, as well as stems from the right con-
text. Morphological features are those that refer to
the features given in Table 1. Morphological infor-
mation is used in describing the target label as well
as its context, and is intended to capture morpho-
logical generalizations. Finally, syntactic features
can make use of syntactic analyses of the source
and target sentences. Such analyses may be derived
for the target language, using the pre-stemmed sen-
tence. Without loss of generality, we will use here
a dependency parsing paradigm. Given a syntactic
analysis, one can construct syntactic features; for ex-
ample, the stem of the parent word of yt. Syntactic
features are expected to be useful in capturing agree-
ment phenomena.
5.3 Features
Table 2 gives the full set of suggested features for
Russian and Arabic, detailed by type. For monolin-
gual lexical features, we consider the stems of the
predicted word and its immediately adjacent words,
in addition to traditional word bigram and trigram
features. For monolingual morphological features,
we consider the morphological attributes of the two
previously predicted words and the current predic-
tion; for monolingual syntactic features, we use the
stem of the parent node.
The bilingual features include the set of words
aligned to the focus word at position t, where they
are treated as bag-of-words, i.e., each aligned word
Feature categories Instantiations
Monolingual lexical
Word stem st?1,st?2,st,st+1
Predicted word yt, yt?1, yt?2
Monolingual morphological
f : POS, Person, Number, Gender, Tense f(yt?2),f(yt?1),f(yt)
Neg, Det, Prep, Conj, ObjPron, PossPron
Monolingual syntactic
Parent stem sHEAD(t)
Bilingual lexical
Aligned word set Al Alt, Alt?1, Alt+1
Bilingual morph & syntactic
f : POS, Person, Number, Gender, Tense f(Alt), f(Alt?1),
Neg, Det, Prep, Conj, ObjPron, PossPron, f(Alt+1), f(AlHEAD(t))
Comp
Table 2: The feature set suggested for English-
Russian and English-Arabic pairs
is assigned a separate feature. Bilingual lexical fea-
tures can refer to words aligned to yt as all as words
aligned to its immediate neighbors yt?1 and yt+1.
Bilingual morphological and syntactic features re-
fer to the features of the source language, which
are expected to be useful for predicting morphol-
ogy in the target language. For example, the bilin-
gual Det (determiner) feature is computed accord-
ing to the source dependency tree: if a child of a
word aligned to wt is a determiner, then the fea-
ture value is assigned its surface word form (such
as a or the). The bilingual Prep feature is com-
puted similarly, by checking the parent chain of the
word aligned to wt for the existence of a preposi-
tion. This feature is hoped to be useful for predict-
ing Arabic inflected forms with a prepositional pre-
fix, as well as for predicting case marking in Rus-
sian. The bilingual ObjPron and PossPron features
represent any object pronoun of the word aligned to
wt and a preceding possessive pronoun, respectively.
These features are expected to map to the object and
possessive pronoun features in Arabic. Finally, the
bilingual Compound feature checks whether a word
appears as part of a noun compound in the English
source. f this is the case, the feature is assigned the
value of ?head? or ?dependent?. This feature is rel-
evant for predicting a genitive case in Russian and
definiteness in Arabic.
6 Experimental Settings
In order to evaluate the effectiveness of the sug-
gested approach, we performed reference experi-
ments, that is, using the aligned sentence pairs of
132
Data Eng-Rus Eng-Ara
Avg. sentlen Eng Rus Eng Ara
Training 1M 470K
14.06 12.90 12.85 11.90
Development 1,000 1,000
13.73 12.91 13.48 12.90
Test 1,000 1,000
13.61 12.84 8.49 7.50
Table 3: Data set statistics: corpus size and average
sentence length (in words)
reference translations rather than the output of an
MT system as input.5 This allows us to evaluate
our method with a reduced noise level, as the words
and word order are perfect in reference translations.
These experiments thus constitute a preliminary step
for tackling the real task of inflecting words in MT.
6.1 Data
We used a corpus of approximately 1 million aligned
sentence pairs for English-Russian, and 0.5 million
pairs for English-Arabic. Both corpora are from a
technical (software manual) domain, which we be-
lieve is somewhat restricted along some morpho-
logical dimensions, such as tense and person. We
used 1,000 sentence pairs each for development and
testing for both language pairs. The details of the
datasets used are given in Table 3.
The sentence pairs were word-aligned using
GIZA++ (Och and Ney, 2000) and submitted to a
treelet-based MT system (Quirk et al, 2005), which
uses the word dependency structure of the source
language and projects word dependency structure to
the target language, creating the structure shown in
Figure 1 above.
6.2 Lexicon
Table 4 gives some relevant statistics of the lexicons
we used. For Russian, a general-domain lexicon was
available to us, consisting of about 80,000 lemmas
(stems) and 9.4 inflected forms per stem.6 Limiting
the lexicon to word types that are seen in the train-
ing set reduces its size substantially to about 14,000
stems, and an average of 3.8 inflections per stem.
We will use this latter ?domain-adapted? lexicon in
our experiments.
5In this case, yt should equal wt, according to the task defi-
nition.
6The averages reported in Table 4 are by type and do not
consider word frequencies in the data.
Source Stems Avg(| I |) Avg(| S |)
Rus. Lexicon 79,309 9.4
Lexicon ? Train 13,929 3.8 1.6
Ara. Lexicon ? Train 12,670 7.0 1.7
Table 4: Lexicon statistics
For Arabic, as a full-size Arabic lexicon was not
available to us, we used the Buckwalter morpholog-
ical analyzer (Buckwalter, 2004) to derive a lexicon.
To acquire the stemming and inflection operators, we
submit all words in our training data to the Buckwal-
ter analyzer. Note that Arabic displays a high level
of ambiguity, each word corresponding to many pos-
sible segmentations and morphological analyses; we
considered all of the different stems returned by the
Buckwalter analyzer in creating a word?s stem set.
The lexicon created in this manner contains 12,670
distinct stems and 89,360 inflected forms.
For the generation of word features, we only con-
sider one dominant analysis for any surface word
for simplicity. In case of ambiguity, we considered
only the first (arbitrary) analysis for Russian. For
Arabic, we apply the following heuristic: use the
most frequent analysis estimated from the gold stan-
dard labels in the Arabic Treebank (Maamouri et al,
2005); if a word does not appear in the treebank, we
choose the first analysis returned by the Buckwal-
ter analyzer. Ideally, the best word analysis should
be provided as a result of contextual disambiguation
(e.g., (Habash and Rambow, 2005)); we leave this
for future work.
6.3 Baseline
As a baseline, we pick a morphological inflection yt
at random from It. This random baseline serves as
an indicator for the difficulty of the problem. An-
other more competitive baseline we implemented
is a word trigram language model (LM). The LMs
were trained using the CMU language modelling
toolkit (Clarkson and Rosenfeld, 1997) with default
settings on the training data described in Table 3.
6.4 Experiments
In the experiments, our primary goal is to evaluate
the effectiveness of the proposed model using all
features available to us. Additionally, we are inter-
ested in knowing the contribution of each informa-
tion source, namely of morpho-syntactic and bilin-
gual features. Therefore, we study the performance
133
of models including the full feature schemata as well
as models that are restricted to feature subsets ac-
cording to the feature types as described in Section
5.2. The models are as follows: Monolingual-Word,
including LM-like and stem n-gram features only;
Bilingual-Word, which also includes bilingual lex-
ical features;7 Monolingual-All, which has access
to all the information available in the target lan-
guage, including morphological and syntactic fea-
tures; and finally, Bilingual-All, which includes all
feature types from Table 2.
For each model and language, we perform feature
selection in the following manner. The features are
represented as feature templates, such as ?POS=X?,
which generate a set of binary features correspond-
ing to different instantiations of the template, as in
?POS=NOUN?. In addition to individual features, con-
junctions of up to three features are also considered
for selection (e.g., ?POS=NOUN & Number=plural?).
Every conjunction of feature templates considered
contains at least one predicate on the prediction yt,
and up to two predicates on the context. The feature
selection algorithm performs a greedy forward step-
wise feature selection on the feature templates so as
to maximize development set accuracy. The algo-
rithm is similar to the one described in (Toutanova,
2006). After this process, we performed some man-
ual inspection of the selected templates, and finally
obtained 11 and 36 templates for the Monolingual-
All and Bilingual-All settings for Russian, respec-
tively. These templates generated 7.9 million and
9.3 million binary feature instantiations in the fi-
nal model, respectively. The corresponding num-
bers for Arabic were 27 feature templates (0.7 mil-
lion binary instantiations) and 39 feature templates
(2.3 million binary instantiations) for Monolingual-
All and Bilingual-All, respectively.
7 Results and Discussion
Table 5 shows the accuracy of predicting word forms
for the baseline and proposed models. We report ac-
curacy only on words that appear in our lexicons.
Thus, punctuation, English words occurring in the
target sentence, and words with unknown lemmas
are excluded from the evaluation. The reported ac-
curacy measure therefore abstracts away from the is-
7Overall, this feature set approximates the information that
is available to a state-of-the-art statistical MT system.
Model Eng-Rus Eng-Ara
Random 31.7 16.3
LM 77.6 31.7
Monolingual Word 85.1 69.6
Bilingual Word 87.1 71.9
Monolingual All 87.1 71.6
Bilingual All 91.5 73.3
Table 5: Accuracy (%) results by model
sue of incomplete coverage of the lexicon. When
we encounter these words in the true MT scenario,
we will make no predictions about them, and simply
leave them unmodified. In our current experiments,
in Russian, 68.2% of all word tokens were in Cyril-
lic, of which 93.8% were included in our lexicon.
In Arabic, 85.5% of all word tokens were in Arabic
characters, of which 99.1% were in our lexicon.8
The results in Table 5 show that the suggested
models outperform the language model substantially
for both languages. In particular, the contribution of
both bilingual and non-lexical features is notewor-
thy: adding non-lexical features consistently leads
to 1.5% to 2% absolute gain in both monolingual
and bilingual settings in both language pairs. We
obtain a particularly large gain in the Russian bilin-
gual case, in which the absolute gain is more than
4%, translating to 34% error rate reduction. Adding
bilingual features has a similar effect of gaining
about 2% (and 4% for Russian non-lexical) in ac-
curacy over monolingual models. The overall accu-
racy is lower in Arabic than in Russian, reflecting
the inherent difficulty of the task, as indicated by the
random baseline (31.7 in Russian vs. 16.3 in Ara-
bic).
In order to evaluate the effectiveness of the model
in alleviating the data sparsity problem in morpho-
logical generation, we trained inflection prediction
models on various subsets of the training data de-
scribed in Table 3, and tested their accuracy. The
results are given in Figure 2. We can see that with as
few as 5,000 training sentences pairs, the model ob-
tains much better accuracy than the language model,
which is trained on data that is larger by a few orders
of magnitude. We also note that the learning curve
8For Arabic, the inflection ambiguity was extremely high:
there were on average 39 inflected forms per stem set in our
development corpus (per token), as opposed to 7 in Russian.
We therefore limited the evaluation of Arabic to those stems that
have up to 30 inflected forms, resulting in 17 inflected forms per
stem set on average in the development data.
134
50
55
60
65
70
75
80
85
90
5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95 10
0
Training data size (x1,000)
Ac
cu
ra
cy
 
(%
)
RUS-bi-word
RUS-bi-all
ARA-bi-word
ARA-bi-all
Figure 2: Accuracy, varying training data size
becomes less steep as we use more training data,
suggesting that the models are successfully learning
generalizations.
We have also manually examined some repre-
sentative cases where the proposed model failed to
make a correct prediction. In both Russian and Ara-
bic, a very common pattern was a mistake in pre-
dicting the gender (as well as number and person in
Arabic) of pronouns. This may be attributed to the
fact that the correct choice of the pronoun requires
coreference resolution, which is not available in our
model. A more thorough analysis of the results will
be helpful to bring further improvements.
8 Conclusions and Future Work
We presented a probabilistic framework for mor-
phological generation given aligned sentence pairs,
incorporating morpho-syntactic information from
both the source and target sentences. The re-
sults, using reference translations, show that the pro-
posed models achieve substantially better accuracy
than language models, even with a relatively small
amount of training data. Our models using morpho-
syntactic information also outperformed models us-
ing only lexical information by a wide margin. This
result is very promising for achieving our ultimate
goal of improving MT output by using a special-
ized model for target language morphological gener-
ation. Though this goal is clearly outside the scope
of this paper, we conducted a preliminary experi-
ment where an English-to-Russian MT system was
trained on a stemmed version of the aligned data and
used to generate stemmed word sequences, which
were then inflected using the suggested framework.
This simple integration of the proposed model with
the MT system improved the BLEU score by 1.7.
The most obvious next step of our research, there-
fore, is to further pursue the integration of the pro-
posed model to the end-to-end MT scenario.
There are multiple paths for obtaining further im-
provements over the results presented here. These
include refinement in feature design, word analysis
disambiguation, morphological and syntactic anal-
ysis on the source English side (e.g., assigning se-
mantic role tags), to name a few. Another area of
investigation is capturing longer-distance agreement
phenomena, which can be done by implementing a
global statistical model, or by using features from
dependency trees more effectively.
References
Tim Buckwalter. 2004. Buckwalter arabic morphological ana-
lyzer version 2.0.
Philip Clarkson and Roni Rosenfeld. 1997. Statistical language
modelling using the CMU cambridge toolkit. In Eurospeech.
Kevin Duh and Kathrin Kirchhoff. 2004. Automatic learning of
language model structure. In COLING.
Ilknur Durgar El-Kahlout and Kemal Oflazer. 2006. Initial ex-
plorations in English to Turkish statistical machine transla-
tion. In NAACL workshop on statistical machine translation.
Sharon Goldwater and David McClosky. 2005. Improving sta-
tistical MT through morphological analysis. In EMNLP.
Nizar Habash and Owen Rambow. 2005. Arabic tokenization,
part-of-speech tagging and morphological disambiguation in
one fell swoop. In ACL.
Philipp Koehn and Kevin Knight. 2003. Empirical methods for
compound splitting. In EACL.
Philipp Koehn. 2005. Europarl: A parallel corpus for statistical
machine translation. In MT Summit.
Young-Suk Lee. 2004. Morphological analysis for statistical
machine translation. In HLT-NAACL.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Hubert
Jin. 2005. Arabic Treebank: Part 1 v 3.0. Linguistic Data
Consortium.
Andrew McCallum, Dayne Freitag, and Fernando C. N. Pereira.
2000. Maximum entropy markov models for information
extraction and segmentation. In ICML.
Sonja Nie?en and Hermann Ney. 2004. Statistical machine
translation with scarce resources using morpho-syntactic in-
formation. Computational Linguistics, 30(2):181?204.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In ACL.
Maja Popovic? and Hermann Ney. 2004. Towards the use of
word stems and suffixes for statistical machine translation.
In LREC.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Depen-
dency tree translation: Syntactically informed phrasal SMT.
In ACL.
Hisami Suzuki and Kristina Toutanova. 2006. Learning to pre-
dict case markers in Japanese. In COLING-ACL.
Kristina Toutanova. 2006. Competitive generative models with
structure learning for NLP classification tasks. In EMNLP.
135
Workshop on TextGraphs, at HLT-NAACL 2006, pages 1?8,
New York City, June 2006. c?2006 Association for Computational Linguistics
A Graphical Framework for Contextual Search and Name Disambiguation
in Email
Einat Minkov
Language Technologies Inst.
Carnegie Mellon University
Pittsburgh, PA 15213
einatm@cs.cmu.edu
William W. Cohen
Machine Learning Dept.
Carnegie Mellon University
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Andrew Y. Ng
Computer Science Dept.
Stanford University
Stanford, CA 94305
ang@cs.stanford.edu
Abstract
Similarity measures for text have histor-
ically been an important tool for solving
information retrieval problems. In this pa-
per we consider extended similarity met-
rics for documents and other objects em-
bedded in graphs, facilitated via a lazy
graph walk. We provide a detailed in-
stantiation of this framework for email
data, where content, social networks and
a timeline are integrated in a structural
graph. The suggested framework is evalu-
ated for the task of disambiguating names
in email documents. We show that rerank-
ing schemes based on the graph-walk sim-
ilarity measures often outperform base-
line methods, and that further improve-
ments can be obtained by use of appropri-
ate learning methods.
1 Introduction
Many tasks in information retrieval can be per-
formed by clever application of textual similarity
metrics. In particular, The canonical IR problem of
ad hoc retrieval is often formulated as the task of
finding documents ?similar to? a query. In modern
IR settings, however, documents are usually not iso-
lated objects: instead, they are frequently connected
to other objects, via hyperlinks or meta-data. (An
email message, for instance, is connected via header
information to other emails in the same thread and
also to the recipient?s social network.) Thus it is
important to understand how text-based document
similarity measures can be extended to documents
embedded in complex structural settings.
Our similarity metric is based on a lazy graph
walk, and is closely related to the well-known
PageRank algorithm (Page et al, 1998). PageRank
and its variants are based on a graph walk of infi-
nite length with random resets. In a lazy graph walk,
there is a fixed probability of halting the walk at each
step. In previous work (Toutanova et al, 2004), lazy
walks over graphs were used for estimating word
dependency distributions: in this case, the graph
was one constructed especially for this task, and the
edges in the graph represented different flavors of
word-to-word similarity. Other recent papers have
also used walks over graphs for query expansion (Xi
et al, 2005; Collins-Thompson and Callan, 2005).
In these tasks, the walk propagates similarity to a
start node through edges in the graph?incidentally
accumulating evidence of similarity over multiple
connecting paths.
In contrast to this previous work, we consider
schemes for propogating similarity across a graph
that naturally models a structured dataset like an
email corpus: entities correspond to objects includ-
ing email addresses and dates, (as well as the usual
types of documents and terms), and edges corre-
spond to relations like sent-by. We view the simi-
larity metric as a tool for performing search across
this structured dataset, in which related entities that
are not directly similar to a query can be reached via
multi-step graph walk.
In this paper, we formulate and evaluate this ex-
tended similarity metric. The principal problem we
1
consider is disambiguating personal names in email,
which we formulate as the task of retrieving the per-
son most related to a particular name mention. We
show that for this task, the graph-based approach im-
proves substantially over plausible baselines. After
retrieval, learning can be used to adjust the ranking
of retrieved names based on the edges in the paths
traversed to find these names, which leads to an ad-
ditional performance improvement. Name disam-
biguation is a particular application of the suggested
general framework, which is also applicable to any
real-world setting in which structural data is avail-
able as well as text.
This paper proceeds as follows. Sections 2 and
3 formalize the general framework and its instanti-
ation for email. Section 4 gives a short summary
of the learning approach. Section 5 includes experi-
mental evaluation, describing the corpora and results
for the person name disambiguation task. The paper
concludes with a review of related work, summary
and future directions.
2 Email as a Graph
A graph G consists of a set of nodes, and a set of la-
beled directed edges. Nodes will be denoted by let-
ters like x, y, or z, and we will denote an edge from
x to y with label ` as x `?? y. Every node x has
a type, denoted T (x), and we will assume that there
are a fixed set of possible types. We will assume for
convenience that there are no edges from a node to
itself (this assumption can be easily relaxed.)
We will use these graphs to represent real-world
data. Each node represents some real-world entity,
and each edge x `?? y asserts that some binary
relation `(x, y) holds. The entity types used here
to represent an email corpus are shown in the left-
most column of Table 1. They include the tradi-
tional types in information retrieval systems, namely
file and term. In addition, however, they include the
types person, email-address and date. These enti-
ties are constructed from a collection of email mes-
sages in the obvious way?for example, a recipient of
?Einat Minkov <einat@cs.cmu.edu>? indicates the
existence of a person node ?Einat Minkov? and an
email-address node ?einat@cs.cmu.edu?. (We as-
sume here that person names are unique identifiers.)
The graph edges are directed. We will assume
that edge labels determine the source and target
node types: i.e., if x `?? z and w `?? y then
T (w) = T (x) and T (y) = T (z). However, mul-
tiple relations can hold between any particular pair
of nodes types: for instance, it could be that x `?? y
or x `
?
?? y, where ` 6= `?. (For instance, an email
message x could be sent-from y, or sent-to y.) Note
also that edges need not denote functional relations:
for a given x and `, there may be many distinct nodes
y such that x `?? y. For instance, for a file x, there
are many distinct terms y such that x has-term?? y holds.
In representing email, we also create an inverse
label `?1 for each edge label (relation) `. Note that
this means that the graph will definitely be cyclic.
Table 1 gives the full set of relations used in our
email represention scheme.
3 Graph Similarity
3.1 Edge weights
Similarity between two nodes is defined by a lazy
walk process, and a walk on the graph is controlled
by a small set of parameters ?. To walk away from
a node x, one first picks an edge label `; then, given
`, one picks a node y such that x `?? y. We assume
that the probability of picking the label ` depends
only on the type T (x) of the node x, i.e., that the
outgoing probability from node x of following an
edge type ` is:
Pr(` | x) = Pr(` | Ti) ? ?`,Ti
Let STi be the set of possible labels for an edge leav-
ing a node of type Ti. We require that the weights
over all outgoing edge types given the source node
type form a probability distribution, i.e., that
?
`?STi
?`,Ti = 1
In this paper, we will assume that once ` is picked,
y is chosen uniformly from the set of all y such that
x `?? y. That is, the weight of an edge of type l
connecting source node x to node y is:
Pr(x `?? y | `) = ?`,Ti
| y : x `?? y |
This assumption could easily be generalized, how-
ever: for instance, for the type T (x) = file and
2
source type edge type target type
file sent-from person
sent-from-email email-address
sent-to person
sent-to-email email-address
date-of date
has-subject-term term
has-term term
person sent-from inv. file
sent-to?1 file
alias email-address
has-term term
email-address sent-to-email?1 file
sent-from-email?1 file
alias-inverse person
is-email?1 term
term has-term?1 file
has subject-term?1 file
is-email email-address
has-term?1 person
date date-of?1 file
Table 1: Graph structure: Node and relation types
` = has-term, weights for terms y such that x `?? y
might be distributed according to an appropriate lan-
guage model (Croft and Lafferty, 2003).
3.2 Graph walks
Conceptually, the edge weights above define the
probability of moving from a node x to some other
node y. At each step in a lazy graph walk, there
is also some probability ? of staying at x. Putting
these together, and denoting byMxy the probability
of being at node y at time t + 1 given that one is at
x at time t in the walk, we define
Mxy =
{
(1 ? ?)
?
` Pr(x
`?? y|`) ? Pr(`|T (x)) x 6= y
? x = y
If we associate nodes with integers, and makeM
a matrix indexed by nodes, then a walk of k steps
can then be defined by matrix multiplication: specif-
ically, if V0 is some initial probability distribution
over nodes, then the distribution after a k-step walk
is proportional to Vk = V0Mk. Larger values of ?
increase the weight given to shorter paths between
x and y. In the experiments reported here, we con-
sider small values of k, and this computation is car-
ried out directly using sparse-matrix multiplication
methods.1 If V0 gives probability 1 to some node x0
1We have also explored an alternative approach based on
sampling; this method scales better but introduces some addi-
tional variance into the procedure, which is undesirable for ex-
perimentation.
and probability 0 to all other nodes, then the value
given to y in Vk can be interpreted as a similarity
measure between x and y.
In our framework, a query is an initial distribu-
tion Vq over nodes, plus a desired output type Tout ,
and the answer is a list of nodes y of type Tout ,
ranked by their score in the distribution Vk. For in-
stance, for an ordinary ad hoc document retrieval
query (like ?economic impact of recycling tires?)
would be an appropriate distribution Vq over query
terms, with Tout = file . Replacing Tout with person
would find the person most related to the query?
e.g., an email contact heavily associated with the
retread economics. Replacing Vq with a point dis-
tribution over a particular document would find the
people most closely associated with the given docu-
ment.
3.3 Relation to TF-IDF
It is interesting to view this framework in compar-
ison to more traditional IR methods. Suppose we
restrict ourselves to two types, terms and files, and
allow only in-file edges. Now consider an initial
query distribution Vq which is uniform over the two
terms ?the aardvark?. A one-step matrix multiplica-
tion will result in a distribution V1, which includes
file nodes. The common term ?the? will spread
its probability mass into small fractions over many
file nodes, while the unusual term ?aardvark? will
spread its weight over only a few files: hence the
effect will be similar to use of an IDF weighting
scheme.
4 Learning
As suggested by the comments above, this graph
framework could be used for many types of tasks,
and it is unlikely that a single set of parameter val-
ues will be best for all tasks. It is thus important to
consider the problem of learning how to better rank
graph nodes.
Previous researchers have described schemes for
adjusting the parameters ? using gradient descent-
like methods (Diligenti et al, 2005; Nie et al, 2005).
In this paper, we suggest an alternative approach of
learning to re-order an initial ranking. This rerank-
ing approach has been used in the past for meta-
search (Cohen et al, 1999) and also several natural-
3
language related tasks (Collins and Koo, 2005). The
advantage of reranking over parameter tuning is that
the learned classifier can take advantage of ?global?
features that are not easily used in walk.
Note that node reranking, while can be used as
an alternative to weight manipulation, it is better
viewed as a complementary approach, as the tech-
niques can be naturally combined by first tuning the
parameters ?, and then reranking the result using a
classifier which exploits non-local features. This hy-
brid approach has been used successfully in the past
on tasks like parsing (Collins and Koo, 2005).
We here give a short overview of the reranking ap-
proach, that is described in detail elsewhere (Collins
and Koo, 2005). The reranking algorithm is pro-
vided with a training set containing n examples. Ex-
ample i (for 1 ? i ? n) includes a ranked list of
li nodes. Let wij be the jth node for example i,
and let p(wij) be the probability assigned to wij by
the graph walk. A candidate node wij is represented
through m features, which are computed by m fea-
ture functions f1, . . . , fm. We will require that the
features be binary; this restriction allows a closed
form parameter update. The ranking function for
node x is defined as:
F (x, ??) = ?0L(x) +
m
?
k=1
?kfk(x)
where L(x) = log(p(x)) and ?? is a vector of real-
value parameters. Given a new test example, the out-
put of the model is the given node list re-ranked by
F (x, ??).
To learn the parameter weights ??, we use a boost-
ing method (Collins and Koo, 2005), which min-
imizes the following loss function on the training
data:
ExpLoss(??) =
?
i
li
?
j=2
e?(F (xi,1,??)?F (xi,j ,??))
where xi,1 is, without loss of generality, a correct
target node. The weights for the function are learned
with a boosting-like method, where in each itera-
tion the feature fk that has the most impact on the
loss function is chosen, and ?k is modified. Closed
form formulas exist for calculating the optimal ad-
ditive updates and the impact per feature (Schapire
and Singer, 1999).
5 Evaluation
We experiment with three separate corpora.
The Cspace corpus contains email messages col-
lected from a management course conducted at
Carnegie Mellon University in 1997 (Minkov et
al., 2005). In this course, MBA students, orga-
nized in teams of four to six members, ran simu-
lated companies in different market scenarios. The
corpus we used here includes the emails of all
teams over a period of four days. The Enron cor-
pus is a collection of mail from the Enron cor-
pus that has been made available for the research
community (Klimt and Yang, 2004). Here, we
used the saved email of two different users.2 To
eliminate spam and news postings we removed
email files sent from email addresses with suf-
fix ?.com? that are not Enron?s; widely distributed
email files (sent from ?enron.announcement?, to
?all.employees@enron.com? etc.). Text from for-
warded messages, or replied-to messages were also
removed from the corpus.
Table 2 gives the size of each processed corpus,
and the number of nodes in the graph representation
of it. In deriving terms for the graph, terms were
Porter-stemmed and stop words were removed. The
processed Enron corpora are available from the first
author?s home page.
corpus Person set
files nodes train test
Cspace 821 6248 26 80
Sager-E 1632 9753 11 51
Shapiro-R 978 13174 11 49
Table 2: Corpora Details
5.1 Person Name Disambiguation
5.1.1 Task definition
Consider an email message containing a common
name like ?Andrew?. Ideally an intelligent mailer
would, like the user, understand which person ?An-
drew? refers to, and would rapidly perform tasks like
retrieving Andrew?s prefered email address or home
page. Resolving the referent of a person name is also
an important complement to the ability to perform
named entity extraction for tasks like social network
analysis or studies of social interaction in email.
2Specifially, we used the ?all documents? folder, including
both incoming and outgoing files.
4
However, although the referent of the name is
unambiguous to the recipient of the email, it can
be non-trivial for an automated system to find out
which ?Andrew? is indicated. Automatically de-
termining that ?Andrew? refers to ?Andrew Y. Ng?
and not ?Andrew McCallum? (for instance) is espe-
cially difficult when an informal nickname is used,
or when the mentioned person does not appear in the
email header. As noted above, we model this prob-
lem as a search task: based on a name-mention in an
email message m, we formulate query distribution
Vq, and then retrieve a ranked list of person nodes.
5.1.2 Data preparation
Unfortunately, building a corpus for evaluating
this task is non-trivial, because (if trivial cases are
eliminated) determining a name?s referent is often
non-trivial for a human other than the intended re-
cipient. We evaluated this task using three labeled
datasets, as detailed in Table 2.
The Cspace corpus has been manually annotated
with personal names (Minkov et al, 2005). Addi-
tionally, with the corpus, there is a great deal of
information available about the composition of the
individual teams, the way the teams interact, and
the full names of the team members. Using this
extra information it is possible to manually resolve
name mentions. We collected 106 cases in which
single-token names were mentioned in the the body
of a message but did not match any name from the
header. Instances for which there was not suffi-
cient information to determine a unique person en-
tity were excluded from the example set. In addition
to names that refer to people that are simply not in
the header, the names in this corpus include people
that are in the email header, but cannot be matched
because they are referred to using: initials?this is
commonly done in the sign-off to an email; nick-
names, including common nicknames (e.g., ?Dave?
for ?David?), unusual nicknames (e.g., ?Kai? for
?Keiko?); or American names adopted in place of
a foreign name (e.g., ?Jenny? for ?Qing?).
For Enron, two datasets were generated automat-
ically. We collected name mentions which corre-
spond uniquely a names that is in the email ?Cc?
header line; then, to simulate a non-trivial matching
task, we eliminate the collected person name from
the email header. We also used a small dictionary of
16 common American nicknames to identify nick-
names that mapped uniquely to full person names
on the ?Cc? header line.
For each dataset, some examples were picked ran-
domly and set aside for learning and evaluation pur-
poses.
initials nicknames other
Cspace 11.3% 54.7% 34.0%
Sager-E - 10.2% 89.8%
Shapiro-R - 15.0% 85.0%
Table 3: Person Name Disambiguation Datasets
5.2 Results for person name disambiguation
5.2.1 Evaluation details
All of the methods applied generate a ranked list
of person nodes, and there is exactly one correct an-
swer per example.3 Figure 1 gives results4 for two
of the datasets as a function of recall at rank k, up
to rank 10. Table 4 shows the mean average preci-
sion (MAP) of the ranked lists as well as accuracy,
which we define as the percentage of correct answers
at rank 1 (i.e., precision at rank 1.)
5.2.2 Baseline method
To our knowledge, there are no previously re-
ported experiments for this task on email data. As a
baseline, we apply a reasonably sophisticated string
matching method (Cohen et al, 2003). Each name
mention in question is matched against all of the per-
son names in the corpus. The similarity score be-
tween the name term and a person name is calculated
as the maximal Jaro similarity score (Cohen et al,
2003) between the term and any single token of the
personal name (ranging between 0 to 1). In addition,
we incorporate a nickname dictionary5, such that if
the name term is a known nickname of a name, the
similarity score of that pair is set to 1.
The results are shown in Figure 1 and Table 4. As
can be seen, the baseline approach is substantially
less effective for the more informal Cspace dataset.
Recall that the Cspace corpus includes many cases
such as initials, and also nicknames that have no
literal resemblance to the person?s name (section
3If a ranking contains a block of items with the same score,
a node?s rank is counted as the average rank of the ?block?.
4Results refer to test examples only.
5The same dictionary that was used for dataset generation.
5
5.1.2), which are not handled well by the string sim-
ilarity approach. For the Enron datasets, the base-
line approach perfoms generally better (Table 4). In
all the corpora there are many ambiguous instances,
e.g., common names like ?Dave? or ?Andy? that
match many people with equal strength.
5.2.3 Graph walk methods
We perform two variants of graph walk, corre-
sponding to different methods of forming the query
distribution Vq. Unless otherwise stated, we will use
a uniform weighting of labels?i.e., ?`,T = 1/ST ;
? = 1/2; and a walk of length 2.
In the first variant, we concentrate all the prob-
ability in the query distribution on the name term.
The column labeled term gives the results of the
graph walk from this probability vector. Intuitively,
using this variant, the name term propagates its
weight to the files in which it appears. Then, weight
is propagated to person nodes which co-occur fre-
quently with these files. Note that in our graph
scheme there is a direct path between terms to per-
son names, so that they recieve weight as well.
As can be seen in the results, this leads to very
effective performance: e.g., it leads to 61.3% vs.
41.3% accuracy for the baseline approach on the
CSpace dataset. However, it does not handle am-
biguous terms as well as one would like, as the query
does not include any information of the context in
which the name occurred: the top-ranked answer for
ambiguous name terms (e.g., ?Dave?) will always
be the same person. To solve this problem, we also
used a file+term walk, in which the query Vq gives
equal weight to the name term node and the file in
which it appears.
We found that adding the file node to Vq provides
useful context for ambiguous instances?e.g., the
correct ?David? would in general be ranked higher
than other persons with this same name. On the
other hand, though, adding the file node reduces
the the contribution of the term node. Although the
MAP and accuracy are decreased, file+term has bet-
ter performance than term at higher recall levels, as
can be seen in Figure 1.
5.2.4 Reranking the output of a walk
We now examine reranking as a technique for im-
proving the results. After some preliminary exper-
imentation, we adopted the following types of fea-
tures f for a node x. The set of features are fairly
generic. Edge unigram features indicate, for each
edge label `, whether ` was used in reaching x from
Vq. Edge bigram features indicate, for each pair of
edge labels `1, `2, whether `1 and `2 were used (in
that order) in reaching x from Vq. Top edge bigram
features are similar but indicate if `1, `2 were used
in one of the two highest-scoring paths between Vq
and x (where the ?score? of a path is the product of
Pr(y `?? z) for all edges in the path.)
We believe that these features could all be com-
puted using dynamic programming methods. Cur-
rently, however, we compute features by using a
method we call path unfolding, which is simi-
lar to the back-propagation through time algorithm
(Haykin, 1994; Diligenti et al, 2005) used in train-
ing recurrent neural networks. Graph unfolding is
based on a backward breadth-first visit of the graph,
starting at the target node at time step k, and expand-
ing the unfolded paths by one layer per each time
step. This procedure is more expensive, but offers
more flexibility in choosing alternative features, and
was useful in determining an optimal feature set.
In addition, we used for this task some addi-
tional problem-specific features. One feature indi-
cates whether the set of paths leading to a node orig-
inate from one or two nodes in Vq. (We conjecture
that in the file+term walk, nodes are connected to
both the source term and file nodes are more rele-
vant comparing to nodes that are reached from the
file node or term node only.) We also form features
that indicate whether the given term is a nickname of
the person name, per the nicknames dictionary; and
whether the Jaro similarity score between the term
and the person name is above 0.8. This information
is similar to that used by the baseline method.
The results (for the test set, after training on the
train set) are shown in Table 4 and (for two represen-
tative cases) Figure 1. In each case the top 10 nodes
were reranked. Reranking substantially improves
performance, especially for the file+term walk. The
accuracy rate is higher than 75% across all datasets.
The features that were assigned the highest weights
by the re-ranker were the literal similarity features
and the source count feature.
6
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20
Cu
m
ul
at
ive
 R
at
e
CSPACE
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  5  10  15  20  25
Cu
m
ul
at
ive
 R
at
e
Rank
SHAPIRO-R
baseline
term
term reranked
file+term
file+term re-ranked
Figure 1: Person name disambiguation results: Re-
call at rank k
6 Related Work
As noted above, the similarity measure we use is
based on graph-walk techniques which have been
adopted by many other researchers for several dif-
ferent tasks. In the information retrieval commu-
nity, infinite graph walks are prevalent for deter-
mining document centrality (e.g., (Page et al, 1998;
Diligenti et al, 2005; Kurland and Lee, 2005)). A
related venue of research is of spreading activa-
tion over semantic or association networks, where
the underlying idea is to propagate activation from
source nodes via weighted links through the network
(Berger et al, 2004; Salton and Buckley, 1988).
The idea of representing structured data as a
graph is widespread in the data mining community,
which is mostly concerned with relational or semi-
structured data. Recently, the idea of PageRank
MAP Accuracy
Cspace
Baseline 49.0 41.3
Graph - term 72.6 61.3
Graph - file+term 66.3 48.8
Reranking - term 85.6 72.5
Reranking - file+term 89.0 83.8
Sager-E
Baseline 67.5 39.2
Graph - term 82.8 66.7
Graph - file+term 61.7 41.2
Reranking - term 83.2 68.6
Reranking - file+term 88.9 80.4
Shapiro-R
Baseline 60.8 38.8
Graph - term 84.1 63.3
Graph - file+term 56.5 38.8
Reranking - term 87.9 65.3
Reranking - file+term 85.5 77.6
Table 4: Person Name Disambiguation Results
has been applied to keyword search in structured
databases (Balmin et al, 2004). Analysis of inter-
object relationships has been suggested for entity
disambiguation for entities in a graph (Kalashnikov
et al, 2005), where edges are unlabelled. It has been
suggested to model similarity between objects in re-
lational data in terms of structural-context similarity
(Jeh and Widom, 2002).
We propose the use of learned re-ranking schemes
to improve performance of a lazy graph walk.
Earlier authors have considered instead using hill-
climbing approaches to adjust the parameters of a
graph-walk (Diligenti et al, 2005). We have not
compared directly with such approaches; prelimi-
nary experiments suggest that the performance gain
of such methods is limited, due to their inability to
exploit the global features we used here6. Related
research explores random walks for semi supervised
learning (Zhu et al, 2003; Zhou et al, 2005).
The task of person disambiguation has been stud-
ied in the field of social networks (e.g., (Malin et
al., 2005)). In particular, it has been suggested to
perform name disambiguation in email using traf-
fic information, as derived from the email headers
(Diehl et al, 2006). Our approach differs in that it
allows integration of email content and a timeline in
addition to social network information in a unified
6For instance, re-ranking using a set of simple locally-
computable features only modestly improved performance of
the ?random? weight set for the CSpace threading task.
7
framework. In addition, we incorporate learning to
tune the system parameters automatically.
7 Conclusion
We have presented a scheme for representing a cor-
pus of email messages with a graph of typed entities,
and an extension of the traditional notions of docu-
ment similarity to documents embedded in a graph.
Using a boosting-based learning scheme to rerank
outputs based on graph-walk related, as well as other
domain-specific, features provides an additional per-
formance improvement. The final results are quite
strong: for the explored name disambiguation task,
the method yields MAP scores in the mid-to-upper
80?s. The person name identification task illustrates
a key advantage of our approach?that context can
be easily incorporated in entity disambiguation.
In future work, we plan to further explore the
scalability of the approach, and also ways of inte-
grating this approach with language-modeling ap-
proaches for document representation and retrieval.
An open question with regard to contextual (multi-
source) graph walk in this framework is whether it is
possible to further focus probability mass on nodes
that are reached from multiple source nodes. This
may prove beneficial for complex queries.
References
Andrey Balmin, Vagelis Hristidis, and Yannis Papakonstanti-
nou. 2004. ObjectRank: Authority-based keyword search in
databases. In VLDB.
Helmut Berger, Michael Dittenbach, and Dieter Merkl. 2004.
An adaptive information retrieval system. based on associa-
tive networks. In APCCM.
William W. Cohen, Robert E. Schapire, and Yoram Singer.
1999. Learning to order things. Journal of Artificial Intelli-
gence Research (JAIR), 10:243?270.
William W. Cohen, Pradeep Ravikumar, and Stephen Fienberg.
2003. A comparison of string distance metrics for name-
matching tasks. In IIWEB.
Michael Collins and Terry Koo. 2005. Discriminative rerank-
ing for natural language parsing. Computational Linguistics,
31(1):25?69.
Kevyn Collins-Thompson and Jamie Callan. 2005. Query ex-
pansion using random walk models. In CIKM.
W. Bruce Croft and John Lafferty. 2003. Language Modeling
for Information Retrieval. Springer.
Christopher P. Diehl, Lise Getoor, and Galileo Namata. 2006.
Name reference resolution in organizational email archives.
In SIAM.
Michelangelo Diligenti, Marco Gori, and Marco Maggini.
2005. Learning web page scores by error back-propagation.
In IJCAI.
Simon Haykin. 1994. Neural Networks. Macmillan College
Publishing Company.
Glen Jeh and Jennifer Widom. 2002. Simrank: A measure of
structural-context similarity. In SIGKDD.
Dmitri Kalashnikov, Sharad Mehrotra, and Zhaoqi Chen. 2005.
Exploiting relationship for domain independent data clean-
ing. In SIAM.
Brown Klimt and Yiming Yang. 2004. The enron corpus: A
new dataset for email classification research. In ECML.
Oren Kurland and Lillian Lee. 2005. Pagerank without hyper-
links: Structural re-ranking using links induced by language
models. In SIGIR.
Bradely Malin, Edoardo M. Airoldi, and Kathleen M. Carley.
2005. A social network analysis model for name disam-
biguation in lists. Journal of Computational and Mathemat-
ical Organization Theory, 11(2).
Einat Minkov, Richard Wang, and William Cohen. 2005. Ex-
tracting personal names from emails: Applying named entity
recognition to informal text. In HLT-EMNLP.
Zaiqing Nie, Yuanzhi Zhang, Ji-Rong Wen, and Wei-Ying Ma.
2005. Object-level ranking: Bringing order to web objects.
In WWW.
Larry Page, Sergey Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the web. In
Technical Report, Computer Science department, Stanford
University.
Gerard Salton and Chris Buckley. 1988. On the use of spread-
ing activation methods in automatic information retrieval. In
SIGIR.
Robert E. Schapire and Yoram Singer. 1999. Improved boost-
ing algorithms using confidence-rated predictions. Machine
Learning, 37(3):297?336.
Kristina Toutanova, Christopher D. Manning, and Andrew Y.
Ng. 2004. Learning random walk models for inducing word
dependency distributions. In ICML.
Wensi Xi, Edward Allan Fox, Weiguo Patrick Fan, Benyu
Zhang, Zheng Chen, Jun Yan, and Dong Zhuang. 2005.
Simfusion: Measuring similarity using unified relationship
matrix. In SIGIR.
Dengyong Zhou, Bernhard Scholkopf, and Thomas Hofmann.
2005. Semi-supervised learning on directed graphs. In
NIPS.
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 2003.
Semi-supervised learning using gaussian fields and harmonic
functions. In ICML.
8
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 845?853,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Discriminative Learning for Joint Template Filling
Einat Minkov
Information Systems
University of Haifa
Haifa 31905, Israel
einatm@is.haifa.ac.il
Luke Zettlemoyer
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
lsz@cs.washington.edu
Abstract
This paper presents a joint model for tem-
plate filling, where the goal is to automati-
cally specify the fields of target relations such
as seminar announcements or corporate acqui-
sition events. The approach models mention
detection, unification and field extraction in
a flexible, feature-rich model that allows for
joint modeling of interdependencies at all lev-
els and across fields. Such an approach can,
for example, learn likely event durations and
the fact that start times should come before
end times. While the joint inference space is
large, we demonstrate effective learning with
a Perceptron-style approach that uses simple,
greedy beam decoding. Empirical results in
two benchmark domains demonstrate consis-
tently strong performance on both mention de-
tection and template filling tasks.
1 Introduction
Information extraction (IE) systems recover struc-
tured information from text. Template filling is an IE
task where the goal is to populate the fields of a tar-
get relation, for example to extract the attributes of a
job posting (Califf and Mooney, 2003) or to recover
the details of a corporate acquisition event from a
news story (Freitag and McCallum, 2000).
This task is challenging due to the wide range
of cues from the input documents, as well as non-
textual background knowledge, that must be consid-
ered to find the best joint assignment for the fields
of the extracted relation. For example, Figure 1
shows an extraction from CMU seminar announce-
ment corpus (Freitag and McCallum, 2000). Here,
the goal is to perform mention detection and extrac-
tion, by finding all of the text spans, or mentions,
Date 5/5/1995
Start Time 3:30PM
Location Wean Hall 5409
Speaker Raj Reddy
Title Some Necessary Conditions for a Good User Interface
End Time ?
Figure 1: An example email and its template. Field men-
tions are highlighted in the text, grouped by color.
that describe field values, unify these mentions by
grouping them according to target field, and normal-
izing the results within each group to provide the
final extractions. Each of these steps requires sig-
nificant knowledge about the target relation. For ex-
ample, in Figure 1, the mention ?3:30? appears three
times and provides the only reference to a time. We
must infer that this is the starting time, that the end
time is never explicitly mentioned, and also that the
event is in the afternoon. Such inferences may not
hold in more general settings, such as extraction for
medical emergencies or related events.
In this paper, we present a joint modeling and
learning approach for the combined tasks of men-
tion detection, unification, and template filling, as
described above. As we will see in Section 2, pre-
vious work has mostly focused on learning tagging
845
models for mention detection, which can be diffi-
cult to aggregate into a full template extraction, or
directly learning template field value extractors, of-
ten in isolation and with no reasoning across differ-
ent fields in the same relation. We present a simple,
feature-rich, discriminative model that readily incor-
porates a broad range of possible constraints on the
mentions and joint field assignments.
Such an approach allows us to learn, for each tar-
get relation, an integrated model to weight the dif-
ferent extraction options, including for example the
likely lengths for events, or the fact that start times
should come before end times. However, there are
significant computation challenges that come with
this style of joint learning. We demonstrate empiri-
cally that these challenges can be solved with a com-
bination of greedy beam decoding, performed di-
rectly in the joint space of possible mention clusters
and field assignments, and structured Perceptron-
style learning algorithm (Collins, 2002).
We report experimental evaluations on two bench-
mark datasets in different genres, the CMU semi-
nar announcements and corporate acquisitions (Fre-
itag and McCallum, 2000). In each case, we evalu-
ated both template extraction and mention detection
performance. Our joint learning approach provides
consistently strong results across every setting, in-
cluding new state-of-the-art results. We also demon-
strate, through ablation studies on the feature set, the
need for joint modeling and the relative importance
of the different types of joint constraints.
2 Related Work
Research on the task of template filling has focused
on the extraction of field value mentions from the
underlying text. Typically, these values are extracted
based on local evidence, where the most likely entity
is assigned to each slot (Roth and Yih, 2001; Siefkes,
2008). There has been little effort towards a compre-
hensive approach that includes mention unification,
as well as considers the structure of the target rela-
tional schema to create semantically valid outputs.
Recently, Haghighi and Klein (2010) presented
a generative semi-supervised approach for template
filling. In their model, slot-filling entities are first
generated, and entity mentions are then realized in
text. Thus, their approach performs coreference at
slot level. In addition to proper nouns (named en-
tity mentions) that are considered in this work, they
also account for nominal and pronominal noun men-
tions. This work presents a discriminative approach
to this problem. An advantage of a discriminative
framework is that it allows the incorporation of rich
and possibly overlapping features. In addition, we
enforce label consistency and semantic coherence at
record level.
Other related works perform structured relation
discovery for different settings of information ex-
traction. In open IE, entities and relations may be in-
ferred jointly (Roth and Yih, 2002; Yao et al, 2011).
In this IE task, the target relation must agree with the
entity types assigned to it; e.g., born-in relation re-
quires a place as its argument. In addition, extracted
relations may be required to be consistent with an
existing ontology (Carlson et al, 2010). Compared
with the extraction of tuples of entity mention pairs,
template filling is associated with a more complex
target relational schema.
Interestingly, several researchers have attempted
to model label consistency and high-level relational
constraints using state-of-the-art sequential models
of named entity recognition (NER). Mainly, pre-
determined word-level dependencies were repre-
sented as links in the underlying graphical model
(Sutton and McCallum, 2004; Finkel et al, 2005).
Finkel et al (2005) further modelled high-level se-
mantic constraints; for example, using the CMU
seminar announcements dataset, spans labeled as
start time or end time were required to be seman-
tically consistent. In the proposed framework we
take a bottom-up approach to identifying entity men-
tions in text, where given a noisy set of candidate
named entities, described using rich semantic and
surface features, discriminative learning is applied
to label these mentions. We will show that this ap-
proach yields better performance on the CMU semi-
nar announcement dataset when evaluated in terms
of NER. Our approach is complimentary to NER
methods, as it can consolidate noisy overlapping
predictions from multiple systems into coherent sets.
3 Problem Setting
In the template filling task, a target relation r is pro-
vided, comprised of attributes (also referred to as
846
Figure 2: The relational schema for the seminars domain.
Figure 3: A record partially populated from text.
fields, or slots) A(r). Given a document d, which
is known to describe a tuple of the underlying re-
lation, the goal is to populate the fields with values
based on the text.
The relational schema. In this work, we describe
domain knowledge through an extended relational
database schema R. In this schema, every field of
the target relation maps to a tuple of another rela-
tion, giving rise to a hierarchical view of template
filling. Figure 2 describes a relational schema for
the seminar announcement domain. As shown, each
field of the seminar relation maps to another rela-
tion; e.g., speaker?s values correspond to person tu-
ples. According to the outlined schema, most re-
lations (e.g., person) consist of a single attribute,
whereas the date and time relations are characterised
with multiple attributes; for example, the time rela-
tion includes the fields of hour, minutes and ampm.
We will make use of limited domain knowledge,
expressed as relation-level constraints that are typi-
cally realized in a database. Namely, the following
tests are supported for each relation.
Tuple validity. This test reflects data integrity. The
attributes of a relation may be defined as mandatory
or optional. Mandatory attributes are denoted with a
solid boundary in Figure 2 (e.g., seminar.date), and
optional attributes are denoted with a dashed bound-
ary (e.g., seminar.title). Similar constraints can be
posed on a set of attributes; e.g., either day-of-month
or day-of-week must be populated in the date rela-
tion. Finally, a combination of field values may be
required to be valid, e.g., the values of day, month,
year and day-of-week must be consistent.
Tuple contradiction. This function checks
whether two valid tuples v1 and v2 are inconsis-
tent, implying a negation of possible unification of
these tuples. In this work, we consider date and time
tuples as contradictory if they contain semantically
different values for some field; tuples of location,
person and title are required to have minimal over-
lap in their string values to avoid contradiction.
Template filling. Given document d, the hierar-
chical schema R is populated in a bottom-up fash-
ion. Generally, parent-free relations in the hierar-
chy correspond to generic entities, realized as en-
tity mentions in the text. In Figure 2, these relations
are denoted by double-line boundary, including lo-
cation, person, title, date and time; every tuple of
these relations maps to a named entity mention.1
Figure 3 demonstrates the correct mapping of
named entity mentions to tuples, as well as tuple uni-
fication, for the example shown in Figure 1. For ex-
ample, the mentions ?Wean 5409? and ?Wean Hall
5409? correspond to tuples of the location relation,
where the two tuples are resolved into a unified set.
To complete template filling, the remaining relations
of the schema are populated bottom-up, where each
field links to a unified set of populated tuples. For
example, in Figure 3, the seminar.location field is
linked to {?Wean Hall 5409?,?Wean 5409?}.
Value normalization of the unified tuples is an-
other component of template filling. We partially ad-
dress normalization: tuples of semantically detailed
(multi-attribute) relations, e.g., date and time, are re-
solved into their semantic union, while textual tuples
(e.g., location) are normalized to the longest string
in the set. In this work, we assume that each tem-
plate slot contains at most one value. This restriction
can be removed, at the cost of increasing the size of
the decoding search space.
1In the multi-attribute relations of date and time, each at-
tribute maps to a text span, where the set of spans at tuple-level
is required to be sequential (up to a small distance d).
847
4 Structured Learning
Next, we describe how valid candidate extrac-
tions are instantiated (Sec. 4.1) and how learning
is applied to assess the quality of the candidates
(Sec. 4.2), where beam search is used to find the top
scoring candidates efficiently (Sec. 4.3).
4.1 Candidate Generation
Named entity recognition. A set of candidate men-
tions Sd(a) is extracted from document d per each
attribute a of a relation r ? L, where L is the set
of parent-free relations in T . We aim at high-recall
extractions; i.e., Sd(a) is expected to contain the cor-
rect mentions with high probability. Various IE tech-
niques, as well as an ensemble of methods, can be
employed for this purpose. For each relation r ? L,
valid candidate tuples Ed(r) are constructed from
the candidate mentions that map to its attributes.
Unification. For every relation r ? L, we con-
struct candidate sets of unified tuples, {Cd(r) ?
Ed(r)}. Naively, the number of candidate sets is
exponential in the size of Ed(t). Importantly, how-
ever, the tuples within a candidate unification set are
required to be non-contradictory. In addition, the
text spans that comprise the mentions within each
set must not overlap. Finally, we do not split tuples
with identical string values between different sets.
Candidate tuples. To construct the space of candi-
date tuples of the target relation, the remaining rela-
tions r ? {T?L} are visited bottom-up, where each
field a ? A(r) is mapped in turn to a (possibly uni-
fied) populated tuple of its type. The valid (and non-
overlapping) combinations of field mappings consti-
tute a set of candidate tuples of r.
The candidate tuples generated using this proce-
dure are structured entities, constructed using typed
named entity recognition, unification, and hierarchi-
cal assignment of field values (Figure 3). We will
derive features that describe local and global prop-
erties of the candidate tuples, encoding both surface
and semantic information.
4.2 Learning
We employ a discriminative learning algorithm, fol-
lowing Collins (2002). Our goal is to find the candi-
Algorithm 1: The beam search procedure
1. Populate every low-level relation r ? L from text d:
? Construct a set of candidate valid tuples Ed(r) given
high-recall typed candidate text spans Sd(a), a ? A(r).
? Group Ed(r) into possibly overlapping unified sets,
{Cd(r) ? Ed(r)}.
2. Iterate bottom-up through relations r ? {T ? L}:
? Initialize the set of candidate tuples Ed(r) to an empty
set.
? Iterate through attributes a ? A(r):
? Retrieve the set of candidate tuples (or unified tuple
sets) Ed(r?), where r? is the relation that attribute a
links to in T . Add an empty tuple to the set.
? For every pair of candidate tuples e ? Ed(r) and
e? ? Ed(r?), modify e by linking attribute a(e) to
tuple e?.
? Add the modified tuples, if valid, to Ed(r).
? Apply Equation 1 to rank the partially filled candi-
date tuples e ? Ed(r). Keep the k top scoring can-
didates in Ed(r), and discard the rest.
3. Apply Equation 1 to output a ranked list of extracted records
Ed(r?), where r? is the target relation.
date that maximizes:
F (y, ??) =
m
?
j=1
?jfj(y, d, T ) (1)
where fj(d, y, T ), j = 1, ..,m, are pre-defined fea-
ture functions describing a candidate record y of the
target relation given document d and the extended
schema T . The parameter weights ?j are to be
learned from labeled instances. The training pro-
cedure involves initializing the weights ?? to zero.
Given ??, an inference procedure is applied to find
the candidate that maximizes Equation 1. If the top-
scoring candidate is different from the correct map-
ping known, then: (i) ?? is incremented with the fea-
ture vector of the correct candidate, and (ii) the fea-
ture vector of the top-scoring candidate is subtracted
from ??. This procedure is repeated for a fixed num-
ber of epochs. Following Collins, we employ the av-
eraged Perceptron online algorithm (Collins, 2002;
Freund and Schapire, 1999) for weight learning.
4.3 Beam Search
Unfortunately, optimal local decoding algorithms
(such as the Viterbi algorithm in tagging problems
(Collins, 2002)) can not be applied to our prob-
lem. We therefore propose using beam search to ef-
ficiently find the top scoring candidate. This means
848
that rather than instantiate the full space of valid can-
didate records (Section 4.1), we are interested in in-
stantiating only those candidates that are likely to be
assigned a high score by F . Algorithm 1 outlines
the proposed beam search procedure. As detailed,
only a set of top scoring tuples of size k (beam size)
is maintained per relation r ? T during candidate
generation. A given relation is populated incremen-
tally, having each of its attributes a ? A(r) map in
turn to populated tuples of its type, and using Equa-
tion 1 to find the k highest scoring partially popu-
lated tuples; this limits the number of candidate tu-
ples evaluated to k2 per attribute, and to nk2 for a
relation with n attributes. While beam search is effi-
cient, performance may be compromised compared
with an unconstrained search. The beam size k al-
lows controlling the trade-off between performance
and cost. An advantage of the proposed approach is
that rather than output a single prediction, a list of
coherent candidate tuples may be generated, ranked
according to Equation 1.
5 Seminar Extraction Task
Dataset The CMU seminar announcement dataset
(Freitag and McCallum, 2000) includes 485 emails
containing seminar announcements. The dataset has
been originally annotated with text spans referring to
four slots: speaker, location, stime, and etime. We
have annotated this dataset with two additional at-
tributes: date and title.2 We consider this corpus as
an example of semi-structured text, where some of
the field values appear in the email header, in a tabu-
lar structure, or using special formatting (Califf and
Mooney, 1999; Minkov et al, 2005).3
We used a set of rules to extract candidate named
entities per the types specified in Figure 2.4 The
rules encode information typically used in NER, in-
cluding content and contextual patterns, as well as
lookups in available dictionaries (Finkel et al, 2005;
Minkov et al, 2005). The extracted candidates are
high-recall and overlapping. In order to increase
recall further, additional candidates were extracted
based on document structure (Siefkes, 2008). The
2A modified dataset is available on the author?s homepage.
3Such structure varies across messages. Otherwise, the
problem would reduce to wrapper learning (Zhu et al, 2006).
4The rule language used is based on cascaded finite state
machines (Minorthird, 2008).
recall for the named entities of type date and time is
near perfect, and is estimated at 96%, 91% and 90%
for location, speaker and title, respectively.
Features The categories of the features used are
described below. All features are binary and typed.5
Lexical. These features indicate the value and
pattern of words within the text spans correspond-
ing to each field. For example, lexical features per
Figure 1 include location.content.word.wean, loca-
tion.pattern.capitalized. Similar features are derived
for a window of three words to the right and to the
left of the included spans. In addition, we observe
whether the words that comprise the text spans ap-
pear in relevant dictionaries: e.g., whether the spans
assigned to the location field include words typi-
cal of location, such as ?room? or ?hall?. Lex-
ical features of this form are commonly used in
NER (Finkel et al, 2005; Minkov et al, 2005).
Structural. It has been previously shown that
the structure available in semi-structured documents
such as email messages is useful for information ex-
traction (Minkov et al, 2005; Siefkes, 2008). As
shown in Figure 1, an email message includes a
header, specifying textual fields such as topic, dates
and time. In addition, space lines and line breaks are
used to emphasize blocks of important information.
We propose a set of features that model correspon-
dence between the text spans assigned to each field
and document structure. Specifically, these features
model whether at least one of the spans mapped to
each field appears in the email header; captures a
full line in the document; is indent; appears within
space lines; or in a tabular format. In Figure 1, struc-
tural active features include location.inHeader, lo-
cation.fullLine, title.withinSpaceLines, etc.
Semantic. These features refer to the semantic
interpretation of field values. According to the re-
lational schema (Figure 2), date and time include
detailed attributes, whereas other relations are rep-
resented as strings. The semantic features encoded
therefore refer to date and time only. Specifically,
these features indicate whether a unified set of tu-
ples defines a value for all attributes; for example,
in Figure 1, the union of entities that map to the
date field specify all of the attribute values of this
relation, including day-of-month, month, year, and
5Real-value features were discretized into segments.
849
Date Stime Etime Location Speaker Title
Full model 96.1 99.3 98.7 96.4 87.5 69.5
No structural features 94.9 99.1 98.0 96.1 83.8 65.1
No semantic features 96.1 98.7 95.4 96.4 87.5 69.5
No unification 87.2 97.0 95.1 94.5 76.0 62.7
Individual fields 96.5 97.2 - 96.4 86.8 64.5
Table 1: Seminar extraction results (5-fold CV): Field-level F1
Date Stime Etime Location Speaker Title
SNOW (Roth and Yih, 2001) - 99.6 96.3 75.2 73.8 -
BIEN (Peshkin and Pfeffer, 2003) - 96.0 98.8 87.1 76.9 -
Elie (Finn, 2006) - 98.5 96.4 86.5 88.5 -
TIE (Siefkes, 2008) - 99.3 97.1 81.7 85.4 -
Full model 96.3 99.1 98.0 96.9 85.8 67.7
Table 2: Seminar extraction results (5-fold CV, trained on 50% of corpus): Field-level F1
day-of-week. Another feature encodes the size of the
most semantically detailed named entity that maps
to a field; for example, the most detailed entity men-
tion of type stime in Figure 1 is ?3:30?, compris-
ing of two attribute values, namely hour and min-
utes. Similarly, the total number of semantic units
included in a unified set is represented as a feature.
These features were designed to favor semantically
detailed mentions and unified sets. Finally, domain-
specific semantic knowledge is encoded as features,
including the duration of the seminar, and whether a
time value is round (minutes divide by 5).
In addition to the features described, one may
be interested in modeling cross-field information.
We have experimented with features that encode
the shortest distance between named entity mentions
mapping to different fields (measured in terms of
separating lines or sentences), based on the hypoth-
esis that field values typically co-appear in the same
segments of the document. These features were not
included in the final model since their contribution
was marginal. We leave further exploration of cross-
field features in this domain to future work.
Experiments We conducted 5-fold cross vali-
dation experiments using the seminar extraction
dataset. As discussed earlier, we assume that a sin-
gle record is described in each document, and that
each field corresponds to a single value. These
assumptions are violated in a minority of cases.
In evaluating the template filling task, only exact
matches are accepted as true positives, where partial
matches are counted as errors (Siefkes, 2008). No-
tably, the annotated labels as well as corpus itself are
not error-free; for example, in some announcements
the date and day-of-week specified are inconsistent.
Our evaluation is strict, where non-empty predicted
values are counted as errors in such cases.
Table 1 shows the results of our full model us-
ing beam size k = 10, as well as model variants.
In order to evaluate the contribution of the proposed
features, we eliminated every feature group in turn.
As shown in the table, removing the structural fea-
tures hurt performance consistently across fields. In
particular, structure is informative for the title field,
which is otherwise characterised with low content
and contextual regularity. Removal of the semantic
features affected performance on the stime and etime
fields, modeled by these features. In particular, the
optional etime field, which has fewer occurrences in
the dataset, benefits from modeling semantics.
An important question to be addressed in evalu-
ation is to what extent the joint modeling approach
contributes to performance. In another experiment
we therefore mimic the typical scenario of template
filling, in which the value of the highest scoring
named entity is assigned to each field. In our frame-
work, this corresponds to a setting in which a unified
set includes no more than a single entity. The results
are shown in Table 1 (?no unification?). Due to re-
duced evidence given a single entity versus a a coref-
erent set of entities, this results in significantly de-
graded performance. Finally, we experimented with
populating every field of the target schema indepen-
dently of the other fields. While results are overall
comparable on most fields, this had negative impact
on the title field. This is largely due to erroneous as-
signments of named entities of other types (mainly,
person) as titles; such errors are avoided in the full
joint model, where tuple validity is enforced.
Table 2 provides a comparison of the full model
850
Date Stime Etime Location Speaker Title
(Sutton and McCallum, 2004) - 96.7 97.2 88.1 80.4 -
(Finkel et al, 2005) - 97.1 97.9 90.0 84.2 -
Full model 95.4 97.1 97.9 97.0 86.5 75.5
Table 3: Seminar extraction results: Token-level F1
against previous state-of-the-art results. These re-
sults were all obtained using half of the corpus for
training, and its remaining half for evaluation; the
reported figures were averaged over five random
splits. For comparison, we used 5-fold cross vali-
dation, where only a subset of each train fold that
corresponds to 50% of the corpus was used for train-
ing. Due to the reduced training data, the results are
slightly lower than in Table 1. (Note that we used the
same test examples in both cases.) The best results
per field are marked in boldface. The proposed ap-
proach yields the best or second-best performance
on all target fields, and gives the best performance
overall. While a variety of methods have been ap-
plied in previous works, none has modeled template
filling in a joint fashion. As argued before, joint
modeling is especially important for irregular fields,
such as title; we provide first results on this field.
Previously, Sutton and McCallum (2004) and
later Finkel et-al. (2005), applied sequential models
to perform NER on this dataset, identifying named
entities that pertain to the template slots. Both of
these works incorporated coreference and high-level
semantic information to a limited extent. We com-
pare our approach to their work, having obtained and
used the same 5-fold cross validation splits as both
works. Table 3 shows results in terms of token F1.
Our results evaluated on the named mention recogni-
tion task are superior overall, giving comparable or
best performance on all fields. We believe that these
results demonstrate the benefit of performing men-
tion recognition as part of a joint model that takes
into account detailed semantics of the underlying re-
lational schema, when available.
Finally, we evaluate the global quality of the ex-
tracted records. Rather than assess performance at
field-level, this stricter evaluation mode considers a
whole tuple, requiring the values assigned to all of
its fields to be correct. Overall, our full model (Table
1) extracts globally correct records for 52.6% of the
examples. To our knowledge, this is the first work
that provides this type of evaluation on this dataset.
Importantly, an advantage of the proposed approach
Figure 4: The relational schema for acquisitions.
is that it readily outputs a ranked list of coherent pre-
dictions. While the performance at the top of the
output lists was roughly comparable, increasing k
gives higher oracle recall: the correct record was
included in the output k-top list 69.7%, 76.1% and
80.4% of the time, for k = 5, 10, 20 respectively.
6 Corporate Acquisitions
Dataset The corporate acquisitions corpus con-
tains 600 newswire articles, describing factual or po-
tential corporate acquisition events. The corpus has
been annotated with the official names of the parties
to an acquisition: acquired, purchaser and seller, as
well as their corresponding abbreviated names and
company codes.6 We describe the target schema us-
ing the relational structure depicted in Figure 4. The
schema includes two relations: the corp relation de-
scribes a corporate entity, including its full name,
abbreviated name and code as attributes; the target
acquisition relation includes three role-designating
attributes, each linked to a corp tuple.
Candidate name mentions in this strictly gram-
matical genre correspond to noun phrases. Docu-
ments were pre-processed to extract noun phrases,
similarly to Haghighi and Klein (2010).
Features We model syntactic features, following
Haghighi and Klein (2010). In order to compen-
sate for parsing errors, shallow syntactic features
were added, representing the values of neighboring
verbs and prepositions (Cohen et al, 2005). While
newswire documents are mostly unstructured, struc-
tural features are used to indicate whether any of the
purchaser, acquired and seller text spans appears in
6In this work, we ignore other fields annotated, as they are
inconsistently defined, have low number of occurrences in the
corpus, and are loosely inter-related semantically.
851
purname purabr purcode acqname acqabr acqcode sellname sellabr sellcode
TIE (batch) 55.7 58.1 - 53.5 55.0 - 31.8 25.8 -
TIE (inc) 51.6 55.3 - 49.2 51.7 - 26.0 24.0 -
Full model 48.9 55.0 70.2 50.7 55.2 67.2 33.2 36.8 55.4
Model variants:
No inter-type and struct. ftrs 45.1 50.5 66.8 49.8 53.9 66.4 34.9 42.2 56.0
No semantic features 42.6 38.4 58.1 40.5 36.5 44.8 32.2 26.6 46.6
Individual roles 43.9 48.7 62.5 45.0 47.2 52.7 34.1 40.3 47.8
Table 4: Corp. acquisition extraction results: Field-level F1
purname purabr purcode acqname acqabr acqcode sellname sellabr sellcode
TIE (batch) 52.6 40.5 - 49.2 43.7 28.7 16.4 -
TIE (inc) 48.4 38.6 - 44.7 42.7 - 23.6 14.5 -
Full model 45.0 48.3 69.8 46.4 59.5 66.9 31.6 33.0 55.0
Table 5: Corp. acquisition extraction results: Entity-level F1
the article?s header. Semantic features are applied
to corp tuples: we model whether the abbreviated
name is a subset of the full name; whether the cor-
porate code forms exact initials of the full or abbre-
viated names; or whether it has high string similarity
to any of these values. Finally, cross-type features
encode the shortest string between spans mapping
to different roles in the acquisition relation.
Experiments We applied beam search, where
corp tuples are extracted first, and acquisition tuples
are constructed using the top scoring corp entities.
We used a default beam size k = 10. The dataset is
split into a 300/300 train/test subsets.
Table 4 shows results of our full model in terms of
field-level F1, compared against TIE, a state-of-the-
art discriminative system (Siefkes, 2008). Unfortu-
nately, we can not directly compare against a gener-
ative joint model evaluated on this dataset (Haghighi
and Klein, 2010).7 The best results per attribute are
shown in boldface. Our full model performs bet-
ter overall than TIE trained incrementally (similarly
to our system), and is competitive with TIE using
batch learning. Interestingly, the performance of our
model on the code fields is high; these fields do
not involve boundary prediction, and thus reflect the
quality of role assignment.
Table 4 also shows the results of model vari-
ants. Removing the inter type and structural fea-
tures mildly hurt performance, on average. In con-
trast, the semantic features, which account for the
semantic cohesiveness of the populated corp tuples,
are shown to be necessary. In particular, remov-
7They report average performance on a different set of
fields; in addition, their results include modeling of pronouns
and nominal mentions, which are not considered here.
ing them degrades the extraction of the abbreviated
names; these features allow prediction of abbrevi-
ated names jointly with the full corporate names,
which are more regular (e.g., include a distinctive
suffix). Finally, we show results of predicting each
role filler individually. Inferring the roles jointly
(?full model?) significantly improves performance.
Table 5 further shows results on NER, the task of
recovering the sets of named entity mentions per-
taining to each target field. As shown, the proposed
joint approach performs overall significantly better
than previous results reported. These results are con-
sistent with the case study of seminar extraction.
7 Summary and Future Work
We presented a joint approach for template filling
that models mention detection, unification, and field
extraction in a flexible, feature-rich model. This ap-
proach allows for joint modeling of interdependen-
cies at all levels and across fields. Despite the com-
putational challenges of this joint inference space,
we obtained effective learning with a Perceptron-
style approach and simple beam decoding.
An interesting direction of future research is
to apply reranking to the output list of candidate
records using additional evidence, such as support-
ing evidence on the Web (Banko et al, 2008). Also,
modeling additional features or feature combina-
tions in this framework as well as effective feature
selection or improved parameter estimation (Cram-
mer et al, 2009) may boost performance. Finally,
it is worth exploring scaling the approach to unre-
stricted event extraction, and jointly model extract-
ing more than one relation per document.
852
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2008. Open in-
formation extraction from the web. In Proceedings of
IJCAI.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI/IAAI.
Mary Elaine Califf and Raymond J. Mooney. 2003.
Bottom-up relational learning of pattern matching
rules for information extraction. Journal of Machine
Learning Research, 4.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of WSDM.
William W. Cohen, Einat Minkov, and Anthony Toma-
sic. 2005. Learning to understand web site update re-
quests. In Proceedings of the international joint con-
ference on Artificial intelligence (IJCAI).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Conference on
Empirical Methods in Natural Language Processing
(EMNLP).
Koby Crammer, Alex Kulesza, and Mark Dredze. 2009.
Adaptive regularization of weight vectors. In Ad-
vances in Neural Information Processing Systems
(NIPS).
Jenny Rose Finkel, Trond Grenager, , and Christopher D.
Manning. 2005. Incorporating non-local information
into information extraction systems by gibbs sampling.
In Proceedings of ACL.
Aidan Finn. 2006. A multi-level boundary classification
approach to information extraction. In PhD thesis.
Dayne Freitag and Andrew McCallum. 2000. In-
formation extraction with hmm structures learned by
stochastic optimization. In AAAI/IAAI.
Yoav Freund and Rob Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3).
Aria Haghighi and Dan Klein. 2010. An entity-level ap-
proach to information extraction. In Proceedings of
ACL.
Einat Minkov, Richard C. Wang, and William W. Cohen.
2005. Extracting personal names from emails: Ap-
plying named entity recognition to informal text. In
HLT/EMNLP.
Minorthird. 2008. Methods for identifying names and
ontological relations in text using heuristics for in-
ducing regularities from data. http://http://
minorthird.sourceforge.net.
Leonid Peshkin and Avi Pfeffer. 2003. Bayesian infor-
mation extraction network. In Proceedings of the in-
ternational joint conference on Artificial intelligence
(IJCAI).
Dan Roth and Wen-tau Yih. 2001. Relational learning
via propositional algorithms: An information extrac-
tion case study. In Proceedings of the international
joint conference on Artificial intelligence (IJCAI).
Dan Roth and Wen-tau Yih. 2002. Probabilistic reason-
ing for entity and relation recognition. In COLING.
Christian Siefkes. 2008. In An Incrementally Trainable
Statistical Approach to Information Extraction. VDM
Verlag.
Charles Sutton and Andrew McCallum. 2004. Collec-
tive segmentation and labeling of distant entities in in-
formation extraction. In Technical Report no. 04-49,
University of Massachusetts.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In Proceedings of EMNLP.
Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, and Wei-
Ying Ma. 2006. Simultaneous record detection and
attribute labeling in web data extraction. In Proc. of
the ACM SIGKDD Intl. Conf. on Knowledge Discovery
and Data Mining (KDD).
853
Proceedings of the TextGraphs-7 Workshop at ACL, pages 20?24,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Graph Based Similarity Measures for Synonym Extraction from Parsed Text
Einat Minkov
Dep. of Information Systems
University of Haifa
Haifa 31905, Israel
einatm@is.haifa.ac.il
William W. Cohen
School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15213
wcohen@cs.cmu.edu
Abstract
We learn graph-based similarity measures for
the task of extracting word synonyms from a
corpus of parsed text. A constrained graph
walk variant that has been successfully ap-
plied in the past in similar settings is shown to
outperform a state-of-the-art syntactic vector-
based approach on this task. Further, we show
that learning specialized similarity measures
for different word types is advantageous.
1 Introduction
Many applications of natural language processing
require measures of lexico-semantic similarity. Ex-
amples include summarization (Barzilay and El-
hadad, 1999), question answering (Lin and Pantel,
2001), and textual entailment (Mirkin et al, 2006).
Graph-based methods have been successfully ap-
plied to evaluate word similarity using available on-
tologies, where the underlying graph included word
senses and semantic relationships between them
(Hughes and Ramage, 2007). Another line of re-
search aims at eliciting semantic similarity measures
directly from freely available corpora, based on the
distributional similarity assumption (Harria, 1968).
In this domain, vector-space methods give state-of-
the-art performance (Pado? and Lapata, 2007).
Previously, a graph based framework has been
proposed that models word semantic similarity from
parsed text (Minkov and Cohen, 2008). The un-
derlying graph in this case describes a text cor-
pus as connected dependency structures, accord-
ing to the schema shown in Figure 1. The toy
graph shown includes the dependency analysis of
two sentences: ?a major environmental disaster is
Figure 1: A joint graph of dependency structures
under way?, and ?combat the environmental catas-
trophe?. In the graph, word mentions (in circles)
and word types (in squares) are both represented
as nodes. Each word mention is linked to its
corresponding word type; for example, the nodes
?environmental3? and ?environmental204? represent
distinct word mentions and both nodes are linked
to the word type ?environmental?.1 For every edge
in the graph, there exists an edge in the oppo-
site direction (not shown in the figure). In this
graph, the terms disaster and catastrophe are re-
lated due to the connecting path disaster ?? disaster3
amod?inverse?? environmental3 ?? environmental ??
environmental204 amod?? catastrophe204 ?? catastrophe .
Given a query, which consists of a word of inter-
est (e.g., ?disaster?), various graph-based similarity
metrics can be used to assess inter-node relatedness,
so that a list of nodes ranked by their similarity to
the query is returned to the user. An advantage of
graph-based similarity approaches is that they pro-
duce similarity scores that reflect structural infor-
1We will sometimes refer to word types as terms.
20
mation in the graph (Liben-Nowell and Kleinberg,
2003). Semantically similar terms are expected to
share connectivity patterns with the query term in
the graph, and thus appear at the top of the list.
Notably, different edge types, as well as the paths
traversed, may have varying importance for differ-
ent types of similarity sought. For example, in the
parsed text domain, noun similarity and verb sim-
ilarity are associated with different syntactic phe-
nomena (Resnik and Diab, 2000). To this end, we
consider a path constrained graph walk (PCW) al-
gorithm, which allows one to learn meaningful paths
given a small number of labeled examples and incor-
porates this information in assessing node related-
ness in the graph (Minkov and Cohen, 2008). PCW
have been successfully applied to the extraction of
named entity coordinate terms, including city and
person names, from graphs representing newswire
text (Minkov and Cohen, 2008), where the special-
ized measures learned outperformed the state-of-
the-art dependency vectors method (Pado? and Lap-
ata, 2007) for small- and medium-sized corpora.
In this work, we apply the path constrained graph
walk method to the task of eliciting general word
relatedness from parsed text, conducting a set of ex-
periments on the task of synonym extraction. While
the tasks of named entity extraction and synonym
extraction from text have been treated separately in
the literature, this work shows that both tasks can be
addressed using the same general framework. Our
results are encouraging: the PCW model yields su-
perior results to the dependency vectors approach.
Further, we show that learning specialized similar-
ity measures per word type (nouns, verbs and adjec-
tives) is preferable to applying a uniform model for
all word types.
2 Path Constrained Graph Walks
PCW is a graph walk variant proposed recently that
is intended to bias the random walk process to fol-
low meaningful edge sequences (paths) (Minkov
and Cohen, 2008). In this approach, rather than as-
sume fixed (possibly, uniform) edge weight param-
eters ? for the various edge types in the graph, the
probability of following an edge of type ? from node
x is evaluated dynamically, based on the history of
the walk up to x.
The PCW algorithm includes two components.
First, it should provide estimates of edge weights
conditioned on the history of a walk, based on train-
ing examples. Second, the random walk algorithm
has to be modified to maintain historical information
about the walk compactly.
In learning, a dataset of N labelled example
queries is provided. The labeling schema is binary,
where a set of nodes considered as relevant answers
to an example query ei, denoted as Ri, is specified,
and graph nodes that are not explicitly included in
Ri are assumed irrelevant to ei. As a starting point,
an initial graph walk is applied to generate a ranked
list of graph nodes li for every example query ei. A
path-tree T is then constructed that includes all of
the acyclic paths up to length k leading to the top
M+ correct and M? incorrect nodes in each of the
retrieved lists li. Every path p is associated with
a maximum likelihood probability estimate Pr(p)
of reaching a correct node based on the number of
times the path was observed in the set of correct and
incorrect target nodes. These path probabilities are
propagated backwards in the path tree to reflect the
probability of reaching a correct node, given an out-
going edge type and partial history of the walk.
Given a new query, a constrained graph walk vari-
ant is applied that adheres both to the topology of the
graph G and the path tree T . In addition to tracking
the graph node that the random walker is at, PCW
maintains pointers to the nodes of the path tree that
represent the walk histories in reaching that graph
node. In order to reduce working memory require-
ments, one may prune paths that are associated with
low probability of reaching a correct node. This of-
ten leads to gains in accuracy.
3 Synonym Extraction
We learn general word semantic similarity measures
from a graph that represents a corpus of parsed text
(Figure 1). In particular, we will focus on evalu-
ating word synonymy, learning specialized models
for different word types. In the experiments, we
mainly compare PCW against the dependency vec-
tors model (DV), due to Pado? and Lapata (2007).
In the latter approach, a word wi is represented
as a vector of weighted scores, which reflect co-
occurrence frequency with words wj , as well as
21
properties of the dependency paths that connect the
word wi to word wj . In particular, higher weight
is assigned to connecting paths that include gram-
matically salient relations, based on the obliqueness
weighting hierarchy (Keenan and Comrie, 1977).
For example, co-occurrence of word wi with word
wj over a path that includes the salient subject rela-
tion receives higher credit than co-occurrences over
a non-salient relation such as preposition. In addi-
tion, Pado? and Lapata suggest to consider only a
subset of the paths observed that are linguistically
meaningful. While the two methods incorporate
similar intuitions, PCW learns meaningful paths that
connect the query and target terms from examples,
whereas DV involves manual choices that are task-
independent.
3.1 Dataset
To allow effective learning, we constructed a dataset
that represents strict word synonymy relations for
multiple word types. The dataset consists of 68 ex-
amples, where each example query consists of a sin-
gle term of interest, with its synonym defined as a
single correct answer. The dataset includes noun
synonym pairs (22 examples), adjectives (24) and
verbs (22). Example synonym pairs are shown in
Table 1. A corpus of parsed text was constructed
using the British National Corpus (Burnard, 1995).
The full BNC corpus is a 100-million word col-
lection of samples of written and spoken contem-
porary British English texts. We extracted rele-
vant sentences, which contained the synonymous
words, from the BNC corpus. (The number of ex-
tracted sentences was limited to 2,000 per word.)
For infrequent words, we extracted additional ex-
ample sentences from Associated Press (AP) arti-
cles included in the AQUAINT corpus (Bilotti et al,
2007). (Sentence count was complemented to 300
per word, where applicable.) The constructed cor-
pus, BNC+AP, includes 1.3 million words overall.
This corpus was parsed using the Stanford depen-
dency parser (de Marneffe et al, 2006).2. The parsed
corpus corresponds to a graph that includes about
0.5M nodes and 1.7M edges.
2http://nlp.stanford.edu/software/lex-parser.shtml
Nouns movie : film
murderer : assassin
Verbs answered : replied
enquire : investigate
Adjectives contemporary : modern
infrequent : rare
Table 1: Example word synonym pairs: the left words are
used as the query terms.
3.2 Experiments
Given a query like {term=?movie?}, we would like
to get synonymous words, such as film, to appear
at the top of the retrieved list. In our experimental
setting, we assume that the word type of the query
term is known. Rather than rank all words (terms) in
response to a query, we use available (noisy) part of
speech information to narrow down the search to the
terms of the same type as the query term, e.g. for the
query ?film? we retrieve nodes of type ? =noun.
We applied the PCW method to learn separate
models for noun, verb and adjective queries. The
path trees were constructed using the paths leading
to the node known to be a correct answer, as well
as to the otherwise irrelevant top-ranked 10 terms.
We required the paths considered by PCW to in-
clude exactly 6 segments (edges). Such paths rep-
resent distributional similarity phenomena, allowing
a direct comparison against the DV method. In con-
ducting the constrained walk, we applied a thresh-
old of 0.5 to truncate paths associated with lower
probability of reaching a relevant response, follow-
ing on previous work (Minkov and Cohen, 2008).
We implemented DV using code made available by
its authors,3 where we converted the syntactic pat-
terns specified to Stanford dependency parser con-
ventions. The parameters of the DV method were
set to medium context and oblique edge weighting
scheme, which were found to perform best (Pado?
and Lapata, 2007). In applying a vector-space based
method, a similarity score needs to be computed be-
tween every candidate from the corpus and the query
term to construct a ranked list. In practice, we used
the union of the top 300 words retrieved by PCW as
candidate terms for DV.
We evaluate the following variants of DV: hav-
3http://www.coli.uni-saarland.de/? pado/dv.html
22
Nouns Verbs Adjs All
CO-Lin 0.34 0.37 0.37 0.37
DV-Cos 0.24 0.36 0.26 0.29
DV-Lin 0.45 0.49 0.54 0.50
PCW 0.47 0.55 0.47 0.49
PCW-P 0.53 0.68 0.55 0.59
PCW-P-U 0.49 0.65 0.50 0.54
Table 2: 5-fold cross validation results: MAP
ing inter-word similarity computed using Lin?s mea-
sure (Lin, 1998) (DV-Lin), or using cosine similarity
(DV-Cos). In addition, we consider a non-syntactic
variant, where a word?s vector consists of its co-
occurrence counts with other terms (using a win-
dow of two words); that is, ignoring the dependency
structure (CO-Lin).
Finally, in addition to the PCW model described
above (PCW), we evaluate the PCW approach in set-
tings where random, noisy, edges have been elimi-
nated from the underlying graph. Specifically, de-
pendency links in the graph may be associated with
pointwise mutual information (PMI) scores of the
linked word mention pairs (Manning and Schu?tze,
1999); edges with low scores are assumed to rep-
resent word co-occurrences of low significance, and
so are removed. We empirically set the PMI score
threshold to 2.0, using cross validation (PCW-P).4
In addition to the specialized PCW models, we also
learned a uniform model over all word types in these
settings; that is, this model is trained using the union
of all training examples, being learned and tested us-
ing a mixture of queries of all types (PCW-P-U).
3.3 Results
Table 2 gives the results of 5-fold cross-validation
experiments in terms of mean average precision
(MAP). Since there is a single correct answer per
query, these results correspond to the mean recipro-
cal rank (MRR).5 As shown, the dependency vec-
tors model applied using Lin similarity (DV-Lin)
performs best among the vector-based models. The
improvement achieved due to edge weighting com-
4Eliminating low PMI co-occurrences has been shown to be
beneficial in modeling lexical selectional preferences recently,
using a similar threshold value (Thater et al, 2010).
5The query?s word inflections and words that are seman-
tically related but not synonymous were discarded from the
ranked list manually for evaluation purposes.
pared with the co-occurrence model (CO-Lin) is
large, demonstrating that syntactic structure is very
informative for modeling word semantics (Pado? and
Lapata, 2007). Interestingly, the impact of applying
the Lin similarity measure versus cosine (DV-Cos)
is even more profound. Unlike the cosine measure,
Lin?s metric was designed for the task of evaluating
word similarity from corpus statistics; it is based on
the mutual information measure, and allows one to
downweight random word co-occurrences.
Among the PCW variants, the specialized PCW
models achieve performance that is comparable to
the state-of-the-art DV measure (DV-Lin). Further,
removing noisy word co-occurrences from the graph
(PCW-P) leads to further improvements, yielding
the best results over all word types. Finally, the
graph walk model that was trained uniformly for all
word types (PCW-P-U) outperforms DV-Lin, show-
ing the advantage of learning meaningful paths. No-
tably, the uniformly trained model is inferior to
PCW trained separately per word type in the same
settings (PCW-P). This suggests that learning spe-
cialized word similarity metrics is beneficial.
4 Discussion
We applied a path constrained graph walk variant to
the task of extracting word synonyms from parsed
text. In the past, this graph walk method has been
shown to perform well on a related task, of extract-
ing named entity coordinate terms from text. While
the two tasks are typically treated distinctly, we have
shown that they can be addressed using the same
framework. Our results on a medium-sized cor-
pus were shown to exceed the performance of de-
pendency vectors, a syntactic state-of-the-art vector-
space method. Compared to DV, the graph walk ap-
proach considers higher-level information about the
connecting paths between word pairs, and are adap-
tive to the task at hand. In particular, we showed that
learning specialized graph walk models for different
word types is advantageous. The described frame-
work can be applied towards learning other flavors
of specialized word relatedness models (e.g., hyper-
nymy). Future research directions include learning
word similarity measures from graphs that integrate
corpus statistics with word ontologies, as well as im-
proved scalability (Lao and Cohen, 2010).
23
References
Regina Barzilay and Michael Elhadad. 1999. Text sum-
marizations with lexical chains, in Inderjeet Mani and
Mark Maybury, editors, Advances in Automatic Text
Summarization. MIT.
Matthew W. Bilotti, Paul Ogilvie, Jamie Callan, and Eric
Nyberg. 2007. Structured retrieval for question an-
swering. In SIGIR.
Lou Burnard. 1995. Users Guide for the British National
Corpus. British National Corpus Consortium, Oxford
University Computing Service, Oxford, UK.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC.
Zellig Harria. 1968. Mathematical Structures of Lan-
guage. Wiley, New York.
Thad Hughes and Daniel Ramage. 2007. Lexical seman-
tic relatedness with random graph walks. In EMNLP.
Edward Keenan and Bernard Comrie. 1977. Noun
phrase accessibility and universal grammar. Linguis-
tic Inquiry, 8.
Ni Lao and William W. Cohen. 2010. Fast query exe-
cution for retrieval models based on path constrained
random walks. In KDD.
Liben-Nowell and J. Kleinberg. 2003. The link predic-
tion problem for social networks. In CIKM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4).
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In COLING-ACL.
Chris Manning and Hinrich Schu?tze. 1999. Founda-
tions of Statistical Natural Language Processing. MIT
Press.
Einat Minkov and William W. Cohen. 2008. Learning
graph walk based similarity measures for parsed text.
In EMNLP.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similarity
methods for lexical entailment acquisition. In ACL.
Sebastian Pado? and Mirella Lapata. 2007. Dependency-
based construction of semantic space models. Compu-
tational Linguistics, 33(2).
Philip Resnik and Mona Diab. 2000. Measuring verb
similarity. In Proceedings of the Annual Meeting of
the Cognitive Science Society.
Stefan Thater, Hagen F??urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In ACL.
24
