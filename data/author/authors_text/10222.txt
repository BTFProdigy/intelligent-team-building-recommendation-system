BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 94?95,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics 1
Conditional Random Fields and Support Vector Machines for Disorder 
Named Entity Recognition in Clinical Texts 
Dingcheng Li Karin Kipper-Schuler Guergana Savova 
University of Minnesota Mayo Clinic College of Medicine Mayo Clinic College of Medicine 
Minneapolis, Minnesota, USA Rochester, Minnesota, USA Rochester, Minnesota, USA 
lixxx345@umn.edu schuler.karin@mayo.edu savova.guergana@mayo.edu 
 
Abstract 
We present a comparative study between 
two machine learning methods, Conditional 
Random Fields and Support Vector Ma-
chines for clinical named entity recognition. 
We explore their applicability to clinical 
domain. Evaluation against a set of gold 
standard named entities shows that CRFs 
outperform SVMs. The best F-score with 
CRFs is 0.86 and for the SVMs is 0.64 as 
compared to a baseline of 0.60. 
1 Introduction and background 
Named entity recognition (NER) is the discovery 
of named entities (NEs), or textual mentions that 
belong to the same semantic class. In the biomedi-
cal domain NEs are diseases, signs/symptoms, ana-
tomical signs, and drugs. NER performance is high 
as applied to scholarly text and newswire narra-
tives (Leaman et al, 2008). Clinical free-text, on 
the other hand, exhibits characteristics of both in-
formal and formal linguistic styles which, in turn, 
poses challenges for clinical NER. Conditional 
Random Fields (CRFs) (Lafferty et al, 2001) and 
and Support Vector Machines (SVMs) (Cortes and 
Vapnik, 1995) are machine learning techniques 
which can handle multiple features during learn-
ing. CRFs? main strength lies in their ability to in-
clude various unrelated features, while SVMs? in 
the inclusion of overlapping features.  Our goal is 
to compare CRFs and SVMs performance for 
clinical NER with focus on disease/disorder NEs. 
2 Dataset and features 
Our dataset is a gold standard corpus of 1557 sin-
gle- and multi-word disorder annotations (Ogren et 
al., 2008). For training and testing the CRF and 
SVM models the IOB (inside-outside-begin) nota-
tion (Leaman, 2008) was applied. In our project, 
we used 1265 gold standard annotations for train-
ing and 292 for testing. The features used for the 
learning process are described as follows. Diction-
ary look-up is a binary value feature that represents 
if the NE is in the dictionary (SNOMED-CT). Bag 
of Words (BOW) is a representation of the context 
by the unique words in it. Part-of-speech tags 
(POS) of BOW is the pos tags of the context 
words. Window size is the number of tokens repre-
senting context surrounding the target word. Ori-
entation(left or right) is the location of the feature 
in regard to the target word. Distance is the prox-
imity of the feature in regard to the target word 
Capitalization has one of the four token-based val-
ues: all upper case, all lower case, mixed_case and 
initial upper case. Number features refer to the 
presence or absence of related numbers. Feature 
sets are in Table 1. 
3 Results and discussion 
Figure 1 shows the CRF results. The F-scores, re-
call and precision for the baseline dictionary look-
up are 0.604, 0.468 and 0.852 respectively. When 
BOW is applied in feature combination 2 results 
improve sharply adding 0.15, 0.17 and 0.08 points 
respectively. The F-score, recall and precision im-
prove even further with the capitalization feature to 
0.858, 0.774 and 0.963 respectively. Figure 2 
shows SVM results. The addition of more features 
to the model did not show an upward trend. The 
best results are with feature combination 1 and 3. 
The F-score reaches 0.643, which although an im-
provement over the baseline greatly underperforms 
CRF results. BOW features seem not discrimina-
tive with SVMs. When the window size increases 
to 5, performance decreases as demonstrated in 
feature combinations 2, 4 and 8. Results with fea-
ture combination 4, in particular, has a pronounced 
downward trend. Its F-score is 0.612, a decrease by 
0.031 compared with Test 1 or Test 3. Its recall 
and precision are 0.487 and 0.822 respectively, a 
decrease by 0.036 and 0.01 respectively. This sup-
ports the results achieved with CRFs where a 
smaller window size yields better performance. 
 
94
 2
No Features 
1 dictionary look-up (baseline) 
2 dictionary look-up+BOW+Orientation+distance (Win-
dow 5) 
3 dictionary look-up + BOW + Orientation + distance 
(Window 3) 
4 dictionary look-up + BOW  + POS + Orientation + 
distance (Window 5) 
5 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) 
6 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + bullet number 
7 dictionary look-up + BOW + POS + Orientation + 
distance(Window 3) + measurement 
8 dictionary look-up + BOW + POS + Orientation + 
distance  (Window 5) + neighboring number 
9 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3) + neighboring number 
10 dictionary look-up + BOW +POS + Orientation + dis-
tance (Window 3)+neighboring number+measurement 
11 dictionary look-up+BOW+POS+Orientation (Window 
3)+neighboring number+bullet number + measurement 
12 dictionary look-up + BOW +POS + Orientation 
+distance (Window 3) + neighboring number + bullet 
number + measurement + capitalization 
Table 1: Feature combinations 
 
 
Figure 1: CRF evaluation results 
 
Figure 2: SVM evaluation results 
 
As the results show, context represented by the 
BOW feature plays an important role indicating the 
importance of the words surrounding NEs. On the 
other hand, POS tag features did not bring much 
improvement, which perhaps hints at a hypothesis 
that grammatical roles are not as important as con-
text in clinical text. Thirdly, a small window size is 
more discriminative. Clinical notes are unstruc-
tured free text with short sentences. If a larger win-
dow size is used, many words will share similar 
features. Fourthly, capitalization is highly dis-
criminative. Fifthly, as a finite state machine de-
rived from HMMs, CRFs can naturally consider 
state-to-state dependences and feature-to-state de-
pendences. On the other hand, SVMs do not con-
sider such dependencies. SVMs separate the data 
into categories via a kernel function. They imple-
ment this by mapping the data points onto an opti-
mal linear separating hyperplane. Finally, SVMs 
do not behave well for large number of feature 
values. For large number of feature values, it 
would be more difficult to find discriminative lines 
to categorize the labels. 
4 Conclusion and future work 
We investigated the use of CRFs and SVMs for 
disorder NER in clinical free-text. Our results 
show that, in general, CRFs outperformed SVMs. 
We demonstrated that well-chosen features along 
with dictionary-based features tend to improve the 
CRF model?s performance but not the SVM?s.  
Acknowledgements 
The work was partially supported by a Biomedical 
Informatics and Computational Biology scholar-
ship from the University of Minnesota. 
References 
Corinna Cortes and Vladimir Vapnik. Support-vector 
network. Machine Learning, 20:273-297, 1995. 
John Lafferty, Andrew McCallum and Fernando 
Pereira. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Data. 
In Proceedings of the Eighteenth International Con-
ference on Machine Learning (ICML-2001), 2001. 
Robert Leaman and Graciela Gonzalez. BANNER: an 
Executable Survey of Advances in Biomedical 
Named Entity Recognition. Pacific Symposium on 
Biocomputing 13:652-663. 2008. 
Philip Ogren, Guergana Savova and Christopher G 
Chute. Constructing evaluation corpora for auto-
mated clinical named entity recognition. Proc LREC 
2008. 
95
Proceedings of NAACL HLT 2009: Tutorials, pages 13?14,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
VerbNet overview, extensions, mappings and applications 
(Karin Kipper Schuler, Anna Korhonen, Susan Brown) 
 
Abstract: 
The goal of this tutorial is to introduce and discuss VerbNet, a broad coverage verb lexicon 
freely available on-line. VerbNet contains explicit syntactic and semantic information for classes 
of verbs and has mappings to several other widely-used lexical resources, including WordNet, 
PropBank, and FrameNet.  Since its first release in 2005 VerbNet is being used by a large 
number of researchers as a means of characterizing verbs and verb classes.  
The first part of the tutorial will include an overview of the original Levin verb classification; 
introduce the main VerbNet components, such as thematic roles and syntactic and semantic 
representations, and present a comparison with other available lexical resources.  
During the second part of the tutorial, we will explore VerbNet extensions (how new classes 
were derived and created through manual and semi-automatic processes), and we will present 
on-going work on automatic acquisition of Levin-style classes in corpora. The latter is useful for 
domain-adaptation and tuning of VerbNet for real-world applications which require this.  
The last part of the tutorial will be devoted to discussing the current status of VerbNet; 
including recent work mapping to other lexical resources, such as PropBank, FrameNet, 
WordNet, OntoNotes sense groupings, and the Omega ontology.  We will also present changes 
designed to regularize the syntactic frames and to make the naming conventions more 
transparent and user friendly.  Finally, we will describe some applications in which VerbNet has 
been used. 
Tutorial Outline:  
VerbNet overview: 
- Original Levin classes 
- VerbNet components (roles, syntactic and semantic descriptions) 
- Related work in lexical resources 
VerbNet extensions: 
- Manual and semi-automatic extension of VerbNet with new classes  
- On-going work on automatic acquisition of Levin-style classes in corpora 
13
VerbNet mappings and applications: 
- Mappings to other resources (PropBank, FrameNet, WordNet, OntoNotes sense 
groupings, Omega ontology) 
- Current status of VerbNet 
- Ongoing improvements 
- VerbNet applications 
Short biographical description of the presenters: 
Karin Kipper Schuler  
University of Colorado 
kipper@verbs.colorado.edu 
 
Karin Kipper Schuler received her PhD from the Computer Science Department of the 
University of Pennsylvania. Her primary research is aimed at the development and 
evaluation of large-scale computational lexical resources. During the past couple of 
years she worked for the Mayo Clinic where she was involved in applications related to 
bio/medical informatics including automatic extraction of named entities and relations 
from clinical data and development of knowledge models to guide this data extraction.  
 
Anna Korhonen 
University of Cambridge 
alk23@cam.ac.uk 
 
Anna Korhonen received her PhD from the Computer Laboratory of the University of 
Cambridge in the UK. Her research focuses mainly on automatic acquisition of lexical 
information from texts. She has developed techniques and tools for automatic lexical 
acquisition for English and other languages, and has used them to acquire large lexical 
resources , extend manually built resources, and help NLP application tasks (e.g. parsing, 
word sense disambiguation, information extraction) . She has applied the techniques to 
different domains (e.g. biomedical) and has also used them to advance on research in 
related fields (e.g. cognitive sciences).  
 
Susan Brown 
University of Colorado 
susan.brown@colorado.edu 
 
Susan Brown is currently a PhD student in Linguistics and Cognitive Science at the 
University of Colorado under Dr. Martha Palmer?s supervision. Susan?s research focuses 
on lexical ambiguity, especially as it pertains to natural language processing tasks.  Her 
research methodologies include psycholinguistic experimentation, corpus study, and 
annotation analysis.  She has also been involved in the development of large-scale 
lexical resources and the design and construction of a lexically based ontology. 
14
