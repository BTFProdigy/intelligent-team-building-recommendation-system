Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1985?1995, Dublin, Ireland, August 23-29 2014.
Empirical analysis of exploiting review helpfulness for extractive
summarization of online reviews
Wenting Xiong
University of Pittsburgh
Department of Computer Science
wex12@cs.pitt.edu
Diane Litman
University of Pittsburgh
Department of Computer Science & LRDC
litman@cs.pitt.edu
Abstract
We propose a novel unsupervised extractive approach for summarizing online reviews by ex-
ploiting review helpfulness ratings. In addition to using the helpfulness ratings for review-level
filtering, we suggest using them as the supervision of a topic model for sentence-level content
scoring. The proposed method is metadata-driven, requiring no human annotation, and generaliz-
able to different kinds of online reviews. Our experiment based on a widely used multi-document
summarization framework shows that our helpfulness-guided review summarizers significantly
outperform a traditional content-based summarizer in both human evaluation and automated eval-
uation.
1 Introduction
Multi-document summarization has great potential in online reviews, as manually reading comments
provided by other users is time consuming if not impossible. While extractive techniques are generally
preferred over abstractive ones (as abstraction can introduce disfluency), existing extractive summarizers
are either supervised or based on heuristics of certain desired characteristics of the summarization result
(e.g., maximize n-gram coverage (Nenkova and Vanderwende, 2005), etc.). However, when it comes
to online reviews, there are problems with both approaches: the first one requires manual annotation
and is thus less generalizable; the second one might not capture the salient information in reviews from
different domains (camera reviews vs. movie reviews), because the heuristics are designed for traditional
genres (e.g., news articles) while the utility of reviews might vary with the review domain.
We propose to exploit review metadata, that is review helpfulness ratings
1
, to facilitate review sum-
marization. Because this is user-provided feedback on review helpfulness which naturally reflects users?
interest in online review exploration, our approach captures domain-dependent salient information adap-
tively. Furthermore, as this metadata is widely available online (e.g., Amazon.com, IMDB.com)
2
, our
approach is unsupervised in the sense that no manual annotation is needed for summarization purposes.
Therefore, we hypothesize that summarizers guided by review helpfulness will outperform systems based
on textual features/heuristics designed for traditional genres. To build such helpfulness-guided summa-
rizers, we introduce review helpfulness during content selection in two ways: 1) using the review-level
helpfulness ratings directly to filter out unhelpful reviews, 2) using sentence-level helpfulness features
derived from review-level helpfulness ratings for sentence scoring. As we observe in our pilot study
that supervised LDA (sLDA) (Blei and McAuliffe, 2010) trained with review helpfulness ratings has
potential in differentiating review helpfulness at the sentence level, we develop features based on the
inferred hidden topics from sLDA to capture the helpfulness of a review sentence for summarization pur-
poses. We implement our helpfulness-guided review summarizers based on an widely used open-source
multi-document extractive summarization framework (MEAD (Radev et al., 2004)). Both human and
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are
added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
This is the percentage of readers who found the review to be helpful (Kim et al., 2006).
2
If it is not available, the review helpfulness can be assessed fully automatically (Kim et al., 2006; Liu et al., 2008).
1985
automated evaluations show that our helpfulness-guided summarizers outperform a strong baseline that
MEAD provides across multiple review domains. Further analysis on the human summaries shows that
some effective heuristics proposed for traditional genres might not work well for online reviews, which
indirectly supports our use of review metadata as supervision. The presented work also extrinsically
demonstrates that the helpfulness-related topics learned from the review-level supervision can capture
review helpfulness at the sentence-level.
2 Related Work
In multi-document extractive summarization, various unsupervised approaches have been proposed to
avoid manual annotation. A key task in extractive summarization is to identify important text units.
Prior successful extractive summarizers score a sentence based on n-grams within the sentence: by the
word frequency (Nenkova and Vanderwende, 2005), bigram coverage (Gillick and Favre, 2009), topic
signatures (Lin and Hovy, 2000) or latent topic distribution of the sentence (Haghighi and Vanderwende,
2009), which all aim to capture the ?core? content of the text input. Other approaches regard the n-
gram distribution difference (e.g., Kullback-Lieber (KL) divergence) between the input documents and
the summary (Lin et al., 2006), or based on a graph-representation of the document content (Erkan and
Radev, 2004; Leskovec13 et al., 2005), with an implicit goal to maximize the output representativeness.
In comparison, while our approach follows the same extractive summarization paradigm, it is metadata
driven, identifying important text units through the guidance of user-provided review helpfulness assess-
ment.
When it comes to online reviews, the desired characteristics of a review summary are different from
traditional text genres (e.g., news articles), and could vary from one review domain to another. Thus
different review summarizers have been proposed to focus on different desired properties of review sum-
maries, primarily based on opinion mining and sentiment analysis (Carenini et al., 2006; Lerman et al.,
2009; Lerman and McDonald, 2009; Kim and Zhai, 2009). Here the desired property varies from the
coverage of product aspects (Carenini et al., 2006; Lerman et al., 2009) to the degree of agreement on
aspect-specific sentiment (Lerman et al., 2009; Lerman and McDonald, 2009; Kim and Zhai, 2009).
While there is a large overlap between text summarization and review opinion mining, most work fo-
cuses on sentiment-oriented aspect extraction and the output is usually a set of topics words plus their
representative text units (Hu and Liu, 2004; Zhuang et al., 2006). However, such a topic-based summa-
rization framework is beyond the focus of our work, as we aim to adapt traditional extractive techniques
to the review domain by introducing review helpfulness ratings as guidance.
In this paper, we utilize review helpfulness via using sLDA. The idea of using sLDA in text summa-
rization is not new. However, the model is previously applied at the sentence level (Li and Li, 2012),
which requires human annotation on the sentence importance. In comparison, our use of sLDA is at
the document (review) level, using existing metadata of the document (review helpfulness ratings) as the
supervision, and thus requiring no annotation at all. With respect to the use of review helpfulness ratings,
early work of review summarization (Liu et al., 2007) only consider it as a filtering criteria during input
preprocessing. Other researchers use it as the gold-standard for automated review helpfulness prediction,
a predictor of product sales (Ghose and Ipeirotis, 2011), a measurement of reviewers? authority in social
network analysis (Lu et al., 2010), etc.
3 Helpfulness features for sentence scoring
While the most straightforward way to utilize review helpfulness for content selection is through filter-
ing (Liu et al., 2007) (further discussed in Section 4.3), we also propose to take into account review
helpfulness during sentence scoring by learning helpfulness-related review topics in advance. Because
sLDA learns the utility of the topics for predicting review-level helpfulness ratings (decomposing review
helpfulness ratings by topics), we develop novel features (rHelpSum and sHelpSum) based on the in-
ferred topics of the words in a sentence to capture its helpfulness in various perspectives. We later use
them for sentence scoring in a helpfulness-guided summarizer (Section 4.3).
Compared with LDA (Blei et al., 2003), sLDA (Blei and McAuliffe, 2010) introduces a response
1986
variable y
i
? Y to each document D
i
during topic discovery. The model not only learns the topic
assignment z
1:N
for words w
1:N
in D
i
, it also learns a function from the posterior distribution of z in D
to Y . When Y is the review-level helpfulness gold-standard, the model leans a set of topics predictive of
review helpfulness, as well as the utility of z in predicting review helpfulness y
i
, denoted as ?. (Both z
and ? are K-dimensional.)
At each inference step, sLDA assigns a topic ID to each word in every review. z
l
= k means that
the topic ID for word at position l in sentence s is k. Given the topic assignments z
1:L
to words w
1:L
in a review sentence s, we estimate the contribution of s to the helpfulness of the review it belongs to
(Formula 1), as well as the average topic importance in s (Formula 2). While rHelpSum is sensitive to
the review length, sHelpSum is sensitive to the sentence length.
rHelpSum(s) =
1
N
l=L
?
l=1
?
k
?
k
p(z
l
= k) (1)
sHelpSum(s) =
1
L
l=L
?
l=1
?
k
?
k
p(z
l
= k) (2)
As the topic assignment in each inference iteration might not be the same, Riedl and Biemann (Riedl
and Biemann, 2012) proposed the mode method in their application of LDA for text segmentation ? use
the most frequently assigned topic for each word in all iterations as the final topic assignment ? to address
the instability issue. Inspired by their idea, we also use the mode method to infer the topic assignment in
our task, but only apply the mode method to the last 10 iterations, because the topic distribution might
not be well learned at the beginning.
4 Experimental setup
To investigate the utility of exploiting user-provided review helpfulness ratings for content selection in
extractive summarization, we develop two helpfulness-guided summarizers based on the MEAD frame-
work (HelpfulFilter and HelpfulSum). We compare our systems? performance against a strong unsu-
pervised extractive summarizer that MEAD supports as our baseline (MEAD+LexRank). To focus
on sentence scoring only, we use the same MEAD word-based MMR (Maximal Marginal Relevance)
reranker (Carbonell and Goldstein, 1998) for all summarizers, and set the length of the output to be 200
words.
4.1 Data
Our data consists of two kinds of online reviews: 4050 Amazon camera reviews provided by Jindal and
Liu (2008) and 280 IMDB movie reviews that we collected by ourselves. Both corpora were used in
our prior work of automatically predicting review helpfulness, in which every review has at least three
helpfulness votes. On average, the helpfulness of camera reviews is .80 and that of movie reviews is .74.
Summarization test sets. Because the proposed approach method is purely unsupervised, and we do
not optimize our summarization parameters during learning, we evaluate our approach based on a subset
of review items directly: we randomly sample 18 reviews for each review item (a camera or movie) and
randomly select 3 items for each review domain. In total there are 6 summarization test sets (3 items ?
2 domains), where each contains 18 reviews to be summarized (i.e. ?summarizing 18 camera reviews
for Nikon D3200?). In the summarization test sets, the average number of sentences per review is 9
for camera reviews, and 18 for movie reviews; the average number of words per sentence in the camera
reviews and movie reviews are 25 and 27, respectively.
4.2 sLDA training
We implement sLDA based on the topic modeling framework of Mallet (McCallum, 2002) using 20
topics (K = 20) and the best hyper-parameters (topic distribution priors ? and word distribution priors
1987
?) that we learned in our pilot study on LDA.
3
Since our summarization approach is unsupervised, we learn the topic assignment for each review
word using the corresponding sLDA model trained on all reviews of that domain (4050 reviews for
camera and 280 reviews for movie).
4
4.3 Three summarizers
Baseline (MEAD+LexRank): The default feature set of MEAD includes Position, Length, and Centroid.
Here Length is a word-count threshold, which gives score 0 to sentences shorter than the threshold. As
we observe that short review sentences sometimes can be very informative as well (e.g., ?This camera is
so amazing!?, ?The best film I have ever seen!?), we adjust Length to 5 from its default value 9. MEAD
also provides scripts to compute LexRank (Erkan and Radev, 2004), which is a more advanced feature
using graph-based algorithm for computing relative importance of textual units. We supplement the
default feature set with LexRank to get the best summarizer from MEAD, yielding the sentence scoring
function F
baseline
(s), in which s is a given sentence and all features are assigned equal weights (same as
in the other two summarizers).
F
baseline
(s) =
{
Position+ Centroid+ LexRank if Length ? 5
0 if Length < 5
(3)
HelpfulFilter: This summarizer is a direct extension of the baseline, which considers review-level help-
fulness ratings (hRating) as an additional filtering criteria in its sentence scoring function F
HelpfulF ilter
.
(In our study, we omit the automated prediction (Kim et al., 2006; Liu et al., 2008) and filter reviews
by their helpfulness gold-standard directly.) We set the cutting threshold to be the average helpful-
ness rating of all the reviews that we used to train the topic model for the corresponding domain
(hRatingAve(domain)).
F
HelpfulF ilter
(s) =
{
F
baseline
(s) if hRating(s) ? hRatingAve(domain)
0 if hRating(s) < hRatingAve(domain)
(4)
HelpfulSum: To isolate the contribution of review helpfulness, the second summarizer only uses help-
fulness related features in its sentence scoring function F
HelpfulSum
. The features are rHelpSum ? the
contribution of a sentence to the overall helpfulness of its corresponding review, sHelpSum ? the average
topic weight in a sentence for predicting the overall helpfulness of the review (Formula 1 and 2), plus
hRating for filtering. Note that there is no overlap between features used in the baseline and Helpful-
Sum, as we wonder if the helpfulness information alone is good enough for discovering salient review
sentences.
F
HelpfulSum
(s) =
{
rHelpSum(s) + sHelpSum(s) if hRating(s) ? hRatingAve(domain)
0 if hRating(s) < hRatingAve(domain)
(5)
5 Evaluation
For evaluation, we will first present our human evaluation user study and then present the automated
evaluation result based on human summaries collected from the user study.
3
In our pilot study, we experimented with various hyper-parameter settings, and trained the model with 100 sampling
iterations in both the Estimation and the Maximization steps. As we found the best results are more likely to be achieved when
? = 0.5, ? = 0.1, we use this setting to train the sLDA model in our summarization experiment.
4
In practice, this means that we need to (re)train the topic model after given the summarization test set.
1988
5.1 Human evaluation
The goal of our human evaluation is to compare the effectiveness of 1) using a traditional content selec-
tion method (MEAD+LexRank), 2) using the traditional method enhanced by review-level helpfulness
filtering (HelpfulFilter), and 3) using sentence helpfulness features estimated by sLDA plus review-level
helpfulness filtering (HelpfulSum) for building an extractive multi-document summarization system for
online reviews. Therefore, we use a within-subject design in our user study for each review domain,
considering the summarizer as the main effect on human evaluation results.
The user study is carried out in the form of online surveys (one survey per domain) hosted by Quadrics.
In total, 36 valid users participated in our online-surveys.
5
We randomly assigned 18 of them to the
camera reviews, and the rest 18 to the movie reviews.
5.1.1 Experimental procedures
Each online survey contains three summarization sets. The human evaluation on each one is taken in
three steps:
Step 1: We first require users to perform manual summarization, by selecting 10 sentences from the
input reviews (displayed in random order for each visit). This ensures that users are familiar with the
input text so that they can have fair judgement on machine-generated results. To help users select the
sentences, we provide an introductory scenario at the beginning of the survey to illustrate the potential
application in accordance with the domain (e.g., Figure 1).
Figure 1: Scenario for summarizing camera reviews
Figure 2: Content evaluation
Step 2: We then ask users to perform pairwise comparison on summaries generated by the three sys-
tems. The three pairs are generated in random order; and the left-or-right display position (in Figure 3)
of the two summaries in each pair is also randomly selected. Here we use the same 5-level preference
ratings used in (Lerman et al., 2009), and translate them into integers from -2 to 2 in our result analysis.
Step 3: Finally, we ask users to evaluate the three summaries in isolation regarding the summary quality
in three content-related aspects: recall, precision and accuracy (top, middle and bottom in Figure 2,
respectively), which were used in (Carenini et al., 2006). In this content evaluation, the three summaries
are randomly visited and the users rate the proposed statements (one for each aspect) on a 5-point scale.
5.1.2 Results
Pairwise comparison. We use a mixed linear model to analyze user preference over the three summary
pairs separately, in which ?summarizer? is a between-subject factor, ?review item? is the repeated factor,
and ?user? is a random effect. Results are summarized in Table 1. (Positive preference ratings on ?A
over B? means A is preferred over B; negative ratings means B is preferred over A.) As we can see,
HelpfulSum is the best: it is consistently preferred over the other two summarizers across domains and
the preference is significant throughout conditions except when compared with HelpfulFilter on movie
reviews. HelpfulFilter is significantly preferred over the baseline (MEAD+LexRank) for movie reviews,
while it does not outperform the baseline on camera reviews. A further look at the compression rate
(cRate) of the three systems (Table 2) shows that on average HelpfulFilter generates shortest summaries
5
All participants are older than eighteen, recruited via university mailing lists, on-campus flyers as well as social networks
online. While we also considered educational peer reviews as a third domain, about half of the participants dropped out in the
middle of the survey. Thus we only consider the two e-commerce domains in this paper.
1989
Figure 3: Example of pairwise comparison for summarizing camera reviews (left:HelpfulSum, right: the
baseline).
among the three summarizers on camera reviews
6
, which makes it naturally harder for HelpfulFilter to
beat the other two (Napoles et al., 2011).
Pair Domain Est. Mean Sig.
HelpfulFilter Camera -.602 .001
over MEAD+LexRank Movie .621 .000
HelpfulSum Camera .424 .011
over MEAD+LexRank Movie .601 .000
HelpfulSum Camera 1.18 .000
over HelpfulFilter Movie .160 .310
Table 1: Mixed-model analysis of user preference ratings
in pairwise comparison across domains. Confidence inter-
val = 95%. The preference rating is ranged from -2 to 2.
Summarizer Camera Movie
MEAD+LexRank 6.07% 2.64%
HelpfulFilter 3.25% 2.39%
HelpfulSum 5.94% 2.69%
Human (Ave.) 6.11% 2.94%
Table 2: Compression rate of the three
systems across domains.
Content evaluation. We summarize the average quality ratings (Figure 2) received by each summarizer
across review items and users for each review domain in Table 3. We carry out paired T-tests for every
pair of summarizers on each quality metric. While no significant difference is found among the three
summarizers on any quality metric for movie reviews, there are differences for camera reviews. In terms
of both accuracy and recall, HelpfulSum is significantly better than HelpfulFilter (p=.008 for accuracy,
p=.034 for recall) and the baseline is significantly better than HelpfulFilter (p=.005 for accuracy, p=.005
for recall), but there is no difference between HelpfulSum and the baseline. For precision, no significant
6
While we limit the summarization output to be 200 words in MEAD, as the content selection is at the sentence level, the
summaries can have different number of words in practice. Considering that word-based MMR controls the redundancy in the
selected summary sentences (? = 0.5 as suggested), there might be enough content to select using F
HelpfulFilter
.
1990
difference is observed in either domain.
Summarizer Camera Movie
Metric Precision Recall Accuracy Precision Recall Accuracy
MEAD+LexRank 2.63 3.24 3.57 2.50 2.59 2.93
HelpfulFilter 2.78 2.74 3.11 2.44 2.61 2.96
HelpfulSum 2.41 3.19 3.69 2.52 2.67 3.02
Table 3: Human ratings for content evaluation. The best result on each metric is bolded for every review
domain (the higher the better).
With respect to pairwise evaluation, content evaluation yields consistent results on camera reviews
between HelpfulFilter vs. the baseline and HelpfulSum vs. HelpfulFilter. However, only pairwise com-
parison (preference ratings) shows significant difference between HelpfulSum vs. the baseline and the
difference in the summarizers? performance on movie reviews. This confirms that pairwise comparison
is more suitable than content evaluation for human evaluation (Lerman et al., 2009).
5.2 Automated evaluation based on ROUGE metrics
Although human evaluation is generally preferred over automated metrics for summarization evaluation,
we report our automated evaluation results based on ROUGE scores (Lin, 2004) using references col-
lected from the user study. For each summarization test set, we have 3 machine generated summaries
and 18 human summaries. We compute the ROUGE scores in a leave-1-out fashion: for each machine
generated summary, we compare it against 17 out of the 18 human summaries and report the score aver-
age across the 17 runs; for each human summary, we compute the score using the other 17 as references,
and report the average human summarization performance.
Evaluation results are summarized in Table 4 and Table 5, in which we report the F-measure for R-
1 (unigram), R-2 (bigram) and R-SU4 (skip-bigram with maximum gap length of 4)
7
, following the
convention in the summarization community. Here we observe slightly different results with respect
to human evaluation: for camera reviews, no significant result is observed, while HelpfulSum achieves
the best R-1 score and HelpfulFilter works best regarding R-2 and R-SU4. In both cases the baseline
is never the best. For movie reviews, HelpfulSum significantly outperforms the other summarizers on
all ROUGE measurements, and the improvement is over 100% on R-2 and R-SU4, almost the same as
human does. This is consistent with the result of pairwise comparison in that HelpfulSum works better
than both HelpfulFilter and the baseline on movie reviews.
Summarizer R-1 R-2 R-SU4
MEAD+LexRank .333 .117 .110
HelpfulFilter .346 .121 .111
HelpfulSum .350 .110 .101
Human .360 .138 .126
Table 4: ROUGE evaluation on camera reviews
Summarizer R-1 R-2 R-SU4
MEAD+LexRank .281 .044 .047
HelpfulFilter .273 .040 .041
HelpfulSum .325 .095 .090
Human .339 .093 .093
Table 5: ROUGE evaluation on movie reviews
6 Human summary analysis
To get a comprehensive understanding of the challenges in extractive review summarization, we analyze
the agreement in human summaries collected in our user study at different levels of granularity, regarding
heuristics that are widely used in existing extractive summarizers.
Average word/sentence counts. Figure 4 illustrates the trend of average number of words and sentences
shared by different number of users across review items for each domain. As it shows, no sentence is
7
Because ROUGE requires all summaries to have equal length (word counts), we only consider the first 100 words in every
summary.
1991
agreed by over 10 users, which suggests that it is hard to make humans agree on the informativeness of
review sentences.
Figure 4: Average number of words (w) and sentences (s) in agreed
human summaries
Figure 5: Average probability of
words used in human summaries
Word frequency. We then compute the average probability of word (in the input) used by different
number of human summarizers to see if the word frequency pattern found in news articles (words that
human summarizers agreed to use in their summaries are of high frequency in the input text (Nenkova
and Vanderwende, 2005)) holds for online reviews. Figure 5 confirms this. However, the average word
probability is below 0.01 in those shared by 14 out of 18 summaries
8
; the flatness of the curve seems to
suggest that word frequency alone is not enough for capturing the salient information in input reviews.
KL-divergence. Another widely used heuristic in multi-document summarization is minimizing the
distance of unigram distribution between the summary and the input text (Lin et al., 2006). We wonder
if this applies to online review summarization. For each testing set, we group review sentences by the
number of users who selected them in their summaries, and compute the KL-divergence (KLD) between
each sentence group and the input. The average KL-divergence of each group across review items are
visualized in Figure 6, showing that this intuition is incorrect for our review domains. Actually, the
pattern is quite the opposite, especially when the number of users who share the sentences is less than 8.
Thus traditional methods that aim to minimize KL-divergence might not work well for online reviews.
Figure 6: Average KL-Divergence between
input and sentences used in human summaries
Figure 7: Average BigramSum of sentences
used in human summaries
Bigram coverage. Recent studies proposed a simple but effective criteria for extractive summarization
based on bigram coverage (Nenkova and Vanderwende, 2005; Gillick and Favre, 2009). The coverage
of a given bigram in a summary is defined as the number of input documents the bigram appears in, and
presumably good summaries should have larger sum of bigram coverage (BigramSum). However, as
shown in Figure 7, this criteria might not work well in our case either. For instance, the BigramSum of
the sentences that are shared by 3 human judges is smaller than those shared by 1 or 2 judges.
8
The average probability of words used by all 4 human summarizers are 0.01 across the 30 DUC03 sets (Nenkova and
Vanderwende, 2005).
1992
7 Conclusion and future work
We propose a novel unsupervised extractive approach for summarizing online reviews by exploiting re-
view helpfulness ratings for content selection. We demonstrate that the helpfulness metadata can not
only be directly used for review-level filtering, but also be used as the supervision of sLDA for sentence
scoring. This approach leverages the existing metadata of online reviews, requiring no annotation and
generalizable to multiple review domains. Our experiment based on the MEAD framework shows that
HelpfulFilter is preferred over the baseline (MEAD+LexRank) on camera reviews in human evaluation.
HelpfulSum, which utilizes review helpfulness at both the review and sentence level, significantly out-
performs the baseline in both human and automated evaluation. Our analysis on the collected human
summaries reveals the limitation of traditional summarization heuristics (proposed for news articles) for
being used in review domains.
In this study, we consider the ground truth of review helpfulness as the percentage of helpful votes
over all votes, where the helpfulness votes could be biased in various ways (Danescu-Niculescu-Mizil et
al., 2009). In the future, we would like to explore more sophisticated models of review helpfulness to
eliminate such biases, or even automatic review helpfulness predictions based on just review text. We also
would like to build a fully automated summarizer by replacing the review helpfulness gold-standard with
automated predictions as the filtering criteria. Given the collected human summaries, we will experiment
with different feature combinations for sentence scoring and we will compare our helpfulness features
with other content features as well. Finally, we want to further analyze the impact of the number of
human judges on our automated evaluation results based on ROUGE scores.
Acknowledgements
This research is supported by the Institute of Education Sciences, U.S. Department of Education, through
Grant R305A120370 to the University of Pittsburgh. The opinions expressed are those of the authors and
do not necessarily represent the views of the Institute or the U.S. Department of Education. We thank
Dr. Jingtao Wang and Dr. Christian Schunn for giving us suggestions on the user study design.
References
David M Blei and Jon D McAuliffe. 2010. Supervised topic models. arXiv preprint arXiv:1003.0783.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents
and producing summaries. In Proceedings of the 21st annual international ACM SIGIR conference on Research
and development in information retrieval, pages 335?336. ACM.
Giuseppe Carenini, Raymond T Ng, and Adam Pauls. 2006. Multi-document summarization of evaluative text. In
In Proceedings of the 11st Conference of the European Chapter of the Association for Computational Linguis-
tics.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets, Jon Kleinber g, and Lillian Lee. 2009. How opinions are
received by online communities: A case study on Amazon .com helpfulness votes. In Proceedings of the 18th
International Conference on World Wide Web, pages 141?150.
G?unes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summa-
rization. J. Artif. Intell. Res.(JAIR), 22(1):457?479.
Anindya Ghose and Panagiotis G Ipeirotis. 2011. Estimating the helpfulness and economic impact of product
reviews: Mining text and reviewer characteristics. IEEE Transactions on Knowledge and Data Engineering,
23(10):1498?1512.
Dan Gillick and Benoit Favre. 2009. A scalable global model for summarization. In Proceedings of the Workshop
on Integer Linear Programming for Natural Langauge Processing, pages 10?18. Association for Computational
Linguistics.
1993
Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. In
Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics, pages 362?370. Association for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 168?177. ACM.
Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of the international conference on
Web search and web data mining, pages 219?230.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generating comparative summaries of contradictory opinions in
text. In Proceedings of the 18th ACM conference on Information and knowledge management, pages 385?394.
ACM.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,
pages 423?430. Association for Computational Linguistics.
Kevin Lerman and Ryan McDonald. 2009. Contrastive summarization: an experiment with consumer reviews.
In Proceedings of human language technologies: The 2009 annual conference of the North American chapter
of the association for computational linguistics, companion volume: Short papers, pages 113?116. Association
for Computational Linguistics.
Kevin Lerman, Sasha Blair-Goldensohn, and Ryan McDonald. 2009. Sentiment summarization: Evaluating and
learning user preferences. In Proceedings of the 12th Conference of the European Chapter of the Association
for Computational Linguistics, pages 514?522.
Jure Leskovec13, Natasa Milic-Frayling, and Marko Grobelnik. 2005. Impact of linguistic analysis on the seman-
tic graph coverage and learning of document extracts.
Jiwei Li and Sujian Li. 2012. A novel feature-based bayesian model for query focused multi-document summa-
rization. arXiv preprint arXiv:1212.2006.
Chin-Yew Lin and Eduard Hovy. 2000. The automated acquisition of topic signatures for text summarization. In
Proceedings of the 18th conference on Computational linguistics, volume 1 of COLING ?00, pages 495?501.
Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie. 2006. An information-theoretic approach to
automatic evaluation of summaries. In Proceedings of the main conference on Human Language Technology
Conference of the North American Chapter of the Association of Computational Linguistics, pages 463?470.
Association for Computational Linguistics.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text Summarization Branches
Out: Proceedings of the ACL-04 Workshop, pages 74?81.
Jingjing Liu, Yunbo Cao, Chin yew Lin, Yalou Huang, and Ming zhou. 2007. Low-quality product review
detection in opinion summarization. In Proceedings of the 2007 Conference on Empirical Methods in Natural
Language Processing.
Yang Liu, Xiangji Huang, Aijun An, and Xiaohui Yu. 2008. Modeling and predicting the helpfulness of online
reviews. In Data Mining, 2008. ICDM?08. Eighth IEEE International Conference on, pages 443?452. IEEE.
Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas, and Livia Polanyi. 2010. Exploiting social context for review
quality prediction. In Proceedings of the 19th international conference on World wide web, pages 691?700.
Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.
Courtney Napoles, Benjamin Van Durme, and Chris Callison-Burch. 2011. Evaluating sentence compression:
Pitfalls and suggested remedies. In Proceedings of the Workshop on Monolingual Text-To-Text Generation,
pages 91?97. Association for Computational Linguistics.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-101.
Dragomir Radev, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda Celebi, Stanko Dimitrov, Elliott
Drabek, Ali Hakim, Wai Lam, Danyu Liu, et al. 2004. Mead-a platform for multidocument multilingual text
summarization. Proceedings of the 4th International Conference on Language Resources and Evaluation (LREC
2004).
1994
Martin Riedl and Chris Biemann. 2012. How text segmentation algorithms gain from topic models. In Proceed-
ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 553?557. Association for Computational Linguistics.
Li Zhuang, Feng Jing, and Xiao-Yan Zhu. 2006. Movie review mining and summarization. In Proceedings of the
15th ACM international conference on Information and knowledge management, pages 43?50. ACM.
1995
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 77?83,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Helpfulness-Guided Review Summarization
Wenting Xiong
University of Pittsburgh
210 South Bouquet Street, Pittsburgh, PA 15260
wex12@cs.pitt.edu
Abstract
Review mining and summarization has been
a hot topic for the past decade. A lot of ef-
fort has been devoted to aspect detection and
sentiment analysis under the assumption that
every review has the same utility for related
tasks. However, reviews are not equally help-
ful as indicated by user-provided helpfulness
assessment associated with the reviews. In
this thesis, we propose a novel review sum-
marization framework which summarizes re-
view content under the supervision of auto-
mated assessment of review helpfulness. This
helpfulness-guided framework can be easily
adapted to traditional review summarization
tasks, for a wide range of domains.
1 Introduction
Nowadays, as reviews thrive on the web, more and
more people wade through these online resources
to inform their own decision making. Due to the
rapid growth of the review volume, the ability of
automatically summarizing online reviews becomes
critical to allowing people to make use of them.
This makes review mining and summarization an
increasingly hot topic over the past decade. Gen-
erally speaking, there are two main paradigms in
review summarization. One is aspect-based opin-
ion summarization, which aims to differentiate and
summarize opinions regarding specific subject as-
pects. It usually involves fine-grained analysis of
both review topics and review sentiment. The other
is more summarization-oriented, prior work under
this category either assumes a shared topic or aims to
produce general summaries. In this case, the focus
is the summarization, extracting salient information
from reviews and organizing them properly. Com-
pared with traditional text summarizers, sentiment-
informed summarizers generally perform better as
shown by human evaluation results (Carenini et al,
2006; Lerman et al, 2009).
However, one implicit assumption shared by most
prior work is that all reviews are of the same util-
ity in review summarization tasks, while reviews
that comment on the same aspect and are associ-
ated with the same rating may have difference in-
fluence to users, as indicated by user-provided help-
fulness assessment (e.g. ?helpful? votes on Ama-
zon.com). We believe that user-generated helpful-
ness votes/ratings suggest people?s point of interest
in review exploration. Intuitively, when users re-
fer to online reviews for guidance, reviews that are
considered helpful by more people naturally receive
more attention and credit, and thus should be given
more weight in review summarization. Following
this intuition, we hypothesize that introducing re-
view helpfulness information into review summa-
rization can yield more useful review summaries.
In addition, we are also motivated by the chal-
lenges that we faced when summarizing educational
peer reviews in which the review entity is also text.
In the peer-review domain, traditional algorithms
of identifying review aspects may suffer as reviews
contain both reviewers? evaluations of a paper and
reviewers? references to the paper. Such heteroge-
neous sources of review content bring challenges to
aspect identification, and the educational perspective
of peer review directly affects the characteristics of
77
desired summaries, which has not ye been taken into
consideration in any of the current summarization
techniques. We expect the helpfulness assessment
of peer reviews can identify important information
that should be captured in peer-review summaries.
2 Related work
The proposed work is grounded in the following
areas: review-helpfulness analysis, review summa-
rization and supervised topic modeling. In this sec-
tion, we will discuss existing work in the literature
and explain how the proposed work relates to them.
2.1 Review-helpfulness analysis
In the literature, most researchers take a supervised
approach in modeling review helpfulness. They ei-
ther aggregate binary helpfulness votes for each re-
view into a numerical score, or directly use numer-
ical helpfulness ratings. Kim et. al (2006) took the
first attempt, using regression to model review help-
fulness based on various linguistic features. They
reported that the combination of review length, re-
view unigrams and product rating statistics per-
formed best. Along this line, other studies showed
the perceived review helpfulness depends not only
on the review content, but also on some other fac-
tors. Ghose et. al (2008) found that the reviewer?s
reviewing history also matters. However, they ob-
served that review-subjectivity, review-readability
and other reviewer-related features are interchange-
able for predicting review helpfulness. In addition,
the empirical study on Amazon reviews conducted
by Danescu-Niculescu-Mizil et. al (2009) revealed
that the perceived helpfulness is also affected by
how a review relates to the other reviews of the same
product. However, given our goal of using review
helpfulness assessment to guide summarization to-
wards generating more useful summaries rather than
to explain each individual helpfulness rating, we
will ignore the interaction of helpfulness assessment
among reviews of the same target.
Furthermore, the utility of features in modeling
review helpfulness may vary with the review do-
main. Mudambi et. al (2010) showed that for
product reviews, the product type moderates both
the product ratings and review length on the per-
ceived review helpfulness. For educational peer re-
views, in X (2011) we showed that cognitive con-
structs which predict feedback implementation can
further improve our helpfulness model upon general
linguistic features. These findings seem to suggest
that the review helpfulness model should be domain-
dependent, due to the specific semantics of ?helpful-
ness? defined in context of the domain.
2.2 Review summarization
One major paradigm of review summarization is
aspect-based summarization, which is based on
identifying aspects and associating opinion senti-
ment with them. (Although this line of work is
closely related to sentiment analysis, it is not the
focus of this proposed work.) While initially peo-
ple use information retrieval techniques to recog-
nize aspect terms and opinion expressions (Hu and
Liu, 2004; Popescu and Etzioni, 2005), recent work
seems to favor generative statistical models more
(Mei et al, 2007; Lu and Zhai, 2008; Titov and Mc-
Donald, 2008b; Titov and McDonald, 2008a; Blei
and McAuliffe, 2010; Brody and Elhadad, 2010;
Mukherjee and Liu, 2012; Sauper and Barzilay,
2013). One typical problem with these models is
that many discovered aspects are not meaningful to
end-users. Some of these studies focus on distin-
guishing aspects in terms of sentiment variation by
modeling aspects together with sentiment (Titov and
McDonald, 2008a; Lu and Zhai, 2008; Mukherjee
and Liu, 2012; Sauper and Barzilay, 2013). How-
ever, little attention is given to differentiating review
content directly regarding their utilities in review
exploration. Mukherjee and Liu (2012) attempted
to address this issue by introducing user-provided
aspect terms as seeds for learning review aspects,
though this approach might not be easily generalized
to other domains, as users? point of interest could
vary with the review domain.
Another paradigm of review summarization is
more summarization-oriented. In contrast, such ap-
proaches do not require the step of identifying as-
pects, instead, they either assume the input text share
the same aspect or aim to produce general sum-
maries. These studies are closely related to the tra-
ditional NLP task of text summarization. Generally
speaking, the goal of text summarization is to retain
the most important points of the input text within a
shorter length. Either extractively or abstractively,
78
one important task is to determine the informative-
ness of a text element. In addition to reducing in-
formation redundancy, different heuristics were pro-
posed within the context of opinion summarization.
Stoyanov and Cardie (2008) focused on identifying
opinion entities (opinion, source, target) and pre-
senting them in a structured way (templates or di-
agrams). Lerman et. al (2009) reported that users
preferred sentiment informed summaries based on
their analysis of human evaluation of various sum-
marization models, while Kim and Zhai (2009) fur-
ther considered an effective review summary as rep-
resentative contrastive opinion pairs. Different from
all above, Ganesan et. al (2010) represented text
input as token-based graphs based on the token or-
der in the string. They rank summary candidates by
scoring paths after removing redundant information
from the graph. For any summarization framework
discussed above, the helpfulness of the review ele-
ments (e.g. sentences, opinion entities, or words),
which can be derived from the review overall help-
fulness, captures informativeness from another di-
mension that has not been taken into account yet.
2.3 Supervised content modeling
As review summarization is meant to help users ac-
quire useful information effectively, what and how
to summarize may vary with user needs. To discover
user preferences, Ando and Ishizaki (2012) man-
ually analyzed travel reviews to identify the most
influential review sentences objectively and subjec-
tively, while Mukherjee and Liu (2012) extract and
categorize review aspects through semi-supervised
modeling using user-provided seeds (categories of
terms). In contrast, we are interested in using user-
provided helpfulness ratings for guidance. As these
helpfulness ratings are existing meta data of reviews,
we will need no additional input from users. Specif-
ically, we propose to use supervised LDA (Blei and
McAuliffe, 2010) to model review content under the
supervision of review helpfulness ratings. Similar
approach is widely adopted in sentiment analysis,
where review aspects are learned in the presence
of sentiment predictions (Blei and McAuliffe, 2010;
Titov and McDonald, 2008a). Furthermore, Brana-
van et. al (2009) showed that joint modeling of text
and user annotations benefits extractive summariza-
tion. Therefore, we hypothesize modeling review
content together with review helpfulness is benefi-
cial to review summarization as well.
3 Data
We plan to experiment on three representative re-
view domains: product reviews, book reviews and
peer reviews. The first one is mostly studied, while
the later two types are more complex, as the review
content consists of both reviewer?s evaluations of the
target and reviewer?s references to the target, which
is also text. This property makes review summariza-
tion more challenging.
For product reviews and book reviews, we plan
to use Amazon reviews provided by Jindal and Liu
(2008), which is a widely used data set in review
mining and sentiment analysis. We consider the
helpfulness assessment of an Amazon review as the
ratio of ?helpful? votes over all votes (Kim et al,
2006). For educational peer reviews, we plan to use
an annotated corpus (Nelson and Schunn, 2009) col-
lected from an online peer-review reciprocal system,
which we used in our prior work (Xiong and Litman,
2011). Two experts (a writing instructor and a con-
tent instructor) were asked to rate the helpfulness of
each peer review on a scale from one to five (Pearson
correlation r = 0.425, p ? 0.01). For our study, we
consider the average ratings given by the two experts
(which roughly follow a normal distribution) as the
gold standard of review helpfulness ratings. To be
consistent with the other review domains, we nor-
malize peer-review helpfulness ratings in the range
between 0 and 1.
4 Proposed work
The proposed thesis work consists of three parts:
1) review content analysis using user-provided help-
fulness ratings, 2) automatically predicting review
helpfulness and 3) a helpfulness-guided review sum-
marization framework.
4.1 Review content analysis
Before advocating the proposed idea, we would test
our two hypothesis: 1) user-provided review help-
fulness assessment reflects review content differ-
ence. 2) Considering review content in terms of in-
ternal content (e.g. reviewers? opinions) vs. exter-
nal content (e.g. book content), the internal content
79
influences the perceived review helpfulness more
than the external content.
We propose to use two kind of instruments, one is
Linguistic Inquiry Word Count (LIWC)1, which is
a manually created dictionary of words; the other is
the set of review topics learned by Latent Dirich-
let Allocation (LDA) (Blei et al, 2003; Blei and
McAuliffe, 2010). LIWC analyzes text input based
on language usages both syntactically and semanti-
cally, which reveals review content patterns at a high
level; LDA can be used to model sentence-level re-
view topics which are domain specific.
For the LIWC-based analysis, we test whether
each category count has a significant effect on the
numerical helpfulness ratings using paired T-test.
For LDA-based analysis, we demonstrate the dif-
ference by show how the learned topics vary when
helpfulness information is introduced as supervi-
sion. Specifically, by comparing the topics learned
from the unsupervised LDA and those learned from
the supervised LDA (with helpfulness ratings), we
expect to show that the supervision of helpfulness
ratings can yield more meaningful aspect clusters.
It is important to note that in both approaches
a review is considered as a bag of words, which
might be problematic if the review has both internal
and external content. Considering this, we hypoth-
esize that the content difference captured by user-
provided helpfulness ratings is mainly in the review-
ers? evaluation rather than in the content of external
sources (hypothesis 2). We plan to test this hypoth-
esis on both book reviews and peer reviews by ana-
lyzing review content in two conditions: in the first
condition (the control condition), all content is pre-
served; in the second condition, the external content
is excluded. If we observe more content variance
in the second condition than the first one, the sec-
ond hypothesis is true. Thus we will separate review
internal and external content in the later summariza-
tion step. For simplification, in the second condi-
tion, we only consider the topic words of the exter-
nal content; we plan to use a corpus-based approach
to identify these topic terms and filter them out to
reduce the impact of external content.
1Url: http://www.liwc.net. We are using LIWC2007.
4.2 Automated review helpfulness assessment
Considering how review usefulness would be inte-
grated in the proposed summarization framework,
we propose two models for predicting review help-
fulness at different levels of granularity.
A discriminative model to learn review global
helpfulness. Previously we (2011) built a discrim-
inative model for predicting the helpfulness of ed-
ucational peer reviews based on prior work of au-
tomatically predicting review helpfulness of prod-
uct reviews (Kim et al, 2006). We considered both
domain-general features and domain-specific fea-
tures. The domain-general features include structure
features (e.g. review length), semantic features, and
descriptive statistics of the product ratings (Kim et
al., 2006); the domain-specific features include the
percentage of external content in reviews and cog-
nitive and social science features that are specific
to the peer-review domain. To extend this idea to
other types of reviews: for product reviews, we con-
sider product aspect-related terms as the topic words
of the external content; for book reviews, we take
into account author?s profile information (number
of books, the mean average book ratings). As we
showed that replacing review unigrams with manu-
ally crafted keyword categories can further improve
the helpfulness model of peer reviews, we plan to
investigate whether review unigrams are generally
replaceable by review LIWC features for modeling
review helpfulness.
A generative model to learn review local help-
fulness. In order to utilize user-provided helpfulness
information in a decomposable fashion, we propose
to use sLDA (Blei and McAuliffe, 2010) to model
review content with review helpfulness information
at the review level, so that the learned latent topics
will be predictive of review helpfulness. In addition
to evaluating the model?s predictive power and the
quality of the learned topics, we will also investi-
gate the extent to which the model?s performance is
affected by the size of the training set, as we may
need to use automatically predicted review helpful-
ness instead, if user-provided helpfulness informa-
tion is not available.
80
4.3 Helpfulness-guided review summarization
In the proposed work, we plan to investigate various
methods of supervising an extractive review summa-
rizer using the proposed helpfulness models. The
simplest method (M1) is to control review helpful-
ness of the summarization input by removing re-
views that are predicted of low helpfulness. A sim-
ilar method (M2) is to use post-processing rather
than pre-processing ? reorder the selected summary
candidates (e.g. sentences) based on their predicted
helpfulness. The helpfulness of a summary sentence
can be either inferred from the local-helpfulness
model (sLDA), or aggregated from review-level
helpfulness ratings of the review(s) from which the
sentence is extracted. The third one (M3) works
together with a specific summarization algorithm,
interpolating traditional informativeness assessment
with novel helpfulness metrics based on the pro-
posed helpfulness models.
For demonstration, we plan to prototype the pro-
posed framework based on MEAD* (Carenini et al,
2006), which is an extension of MEAD (an open-
source framework for multi-document summariza-
tion (Radev et al, 2004)) for summarizing evalu-
ative text. MEAD* defines sentence informative-
ness based on features extracted through standard
aspect-based review mining (Hu and Liu, 2004). As
a human-centric design, we plan to evaluate the pro-
posed framework in a user study in terms of pair-
wise comparison of the reviews generated by differ-
ent summarizers (M1, M2, M3 and MEAD*). Al-
though fully automated summarization metrics are
available (e.g. Jensen-Shannon Divergence (Louis
and Nenkova, 2009)), they favor summaries that
have a similar word distribution to the input and thus
do not suit our task of review summarization.
To show the generality of the proposed ideas, we
plan to evaluate the utility of introducing review
helpfulness in aspect ranking as well, which is an
important sub-task of review opinion analysis. If
our hypothesis (1) is true, we would expect aspect
ranking based on helpfulness-involved metrics out-
performing the baseline which does not use review
helpfulness (Yu et al, 2011). This evaluation will
be done on product reviews and peer reviews, as the
previous work was based on product reviews, while
peer reviews tend to have an objective aspect rank-
ing (provided by domain experts).
5 Contributions
The proposed thesis mainly contributes to review
mining and summarization.
1. Investigate the impact of the source of review
content on review helpfulness. While a lot of
studies focus on product reviews, we based our
analysis on a wider range of domains, including
peer reviews, which have not been well studied
before.
2. Propose two models to automatically assess re-
view helpfulness at different levels of granu-
larity. While the review-level global helpful-
ness model takes into account domain-specific
semantics of helpfulness of reviews, the lo-
cal helpfulness model learns review helpfulness
jointly with review topics. This local helpful-
ness model allows us to decompose overall re-
view helpfulness into small elements, so that
review helpfulness can be easily combined with
metrics of other dimensions in assessing the
importance of summarization candidates.
3. Propose a user-centric review summarization
framework that utilizes user-provided helpful-
ness assessment as supervision. Compared
with previous work, we take a data driven ap-
proach in modeling review helpfulness as well
as helpfulness-related topics, which requires
no extra human input of user-preference and
can be adapted to typical review summarization
tasks such as aspect selection/ranking, sum-
mary sentence ordering, etc.
References
M. Ando and S. Ishizaki. 2012. Analysis of travel re-
view data from readers point of view. WASSA 2012,
page 47.
D.M. Blei and J.D. McAuliffe. 2010. Supervised topic
models. arXiv preprint arXiv:1003.0783.
D.M. Blei, A.Y. Ng, and M.I. Jordan. 2003. Latent
dirichlet alocation. the Journal of machine Learning
research, 3:993?1022.
SRK Branavan, H. Chen, J. Eisenstein, and R. Barzilay.
2009. Learning document-level semantic properties
81
from free-text annotations. Journal of Artificial Intel-
ligence Research, 34(2):569.
S. Brody and N. Elhadad. 2010. An unsupervised aspect-
sentiment model for online reviews. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, pages 804?812. Associa-
tion for Computational Linguistics.
G. Carenini, R. Ng, and A. Pauls. 2006. Multi-document
summarization of evaluative text. In In Proceedings
of the 11st Conference of the European Chapter of the
Association for Computational Linguistics. Citeseer.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinber g, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon .com helpfulness votes. In Proceedings of
WWW, pages 141?150.
Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.
2010. Opinosis: a graph-based approach to abstrac-
tive summarization of highly redundant opinions. In
Proceedings of the 23rd International Conference on
Computational Linguistics, COLING ?10, pages 340?
348, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Anindya Ghose and Panagiotis G. Ipeirotis. 2008. Esti-
mating the socio-economic impact of product reviews.
In NYU Stern Research Working Paper CeDER.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 168?177. ACM.
N. Jindal and B. Liu. 2008. Opinion spam and analysis.
In Proceedings of the international conference on Web
search and web data mining, pages 219?230.
H.D. Kim and C.X. Zhai. 2009. Generating comparative
summaries of contradictory opinions in text. In Pro-
ceedings of the 18th ACM conference on Information
and knowledge management, pages 385?394. ACM.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423?430, Sydney, Australia,
July.
K. Lerman, S. Blair-Goldensohn, and R. McDonald.
2009. Sentiment summarization: Evaluating and
learning user preferences. In Proceedings of the 12th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics, pages 514?522.
Association for Computational Linguistics.
Annie Louis and Ani Nenkova. 2009. Automatically
evaluating content selection in summarization without
human models. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing: Volume 1-Volume 1, pages 306?314. Associ-
ation for Computational Linguistics.
Y. Lu and C. Zhai. 2008. Opinion integration through
semi-supervised topic modeling. In Proceedings of
the 17th international conference on World Wide Web,
pages 121?130. ACM.
Q. Mei, X. Ling, M. Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th interna-
tional conference on World Wide Web, pages 171?180.
ACM.
S.M. Mudambi and D. Schuff. 2010. What makes a
helpful online review? a study of customer reviews
on amazon. com. MIS quarterly, 34(1):185?200.
A. Mukherjee and B. Liu. 2012. aspect extraction
through semi-supervised modeling. In Proceedings of
50th anunal meeting of association for computational
Linguistics (acL-2012)(accepted for publication).
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. In Instructional Sci-
ence, volume 37, pages 375?401.
A.M. Popescu and O. Etzioni. 2005. Extracting product
features and opinions from reviews. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 339?346. Association for Computational Lin-
guistics.
D. Radev, T. Allison, S. Blair-Goldensohn, J. Blitzer,
A. Celebi, S. Dimitrov, E. Drabek, A. Hakim, W. Lam,
D. Liu, et al 2004. Mead-a platform for multidocu-
ment multilingual text summarization. In Proceedings
of LREC, volume 2004.
C. Sauper and R. Barzilay. 2013. Automatic aggregation
by joint modeling of aspects and values. Journal of
Artificial Intelligence Research, 46:89?127.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proceedings of
the 22nd International Conference on Computational
Linguistics-Volume 1, pages 817?824. Association for
Computational Linguistics.
I. Titov and R. McDonald. 2008a. A joint model of text
and aspect ratings for sentiment summarization. Ur-
bana, 51:61801.
I. Titov and R. McDonald. 2008b. Modeling online re-
views with multi-grain topic models. In Proceedings
of the 17th international conference on World Wide
Web, pages 111?120. ACM.
Wenting Xiong and Diane Litman. 2011. Automatically
predicting peer-review helpfulness. In Proceedings of
82
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 502?507.
J. Yu, Z.J. Zha, M. Wang, and T.S. Chua. 2011. Aspect
ranking: identifying important product aspects from
online consumer reviews. Computational Linguistics,
pages 1496?1505.
83
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 502?507,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Automatically Predicting Peer-Review Helpfulness
Wenting Xiong
University of Pittsburgh
Department of Computer Science
Pittsburgh, PA, 15260
wex12@cs.pitt.edu
Diane Litman
University of Pittsburgh
Department of Computer Science &
Learning Research and Development Center
Pittsburgh, PA, 15260
litman@cs.pitt.edu
Abstract
Identifying peer-review helpfulness is an im-
portant task for improving the quality of feed-
back that students receive from their peers. As
a first step towards enhancing existing peer-
review systems with new functionality based
on helpfulness detection, we examine whether
standard product review analysis techniques
also apply to our new context of peer reviews.
In addition, we investigate the utility of in-
corporating additional specialized features tai-
lored to peer review. Our preliminary results
show that the structural features, review uni-
grams and meta-data combined are useful in
modeling the helpfulness of both peer reviews
and product reviews, while peer-review spe-
cific auxiliary features can further improve
helpfulness prediction.
1 Introduction
Peer reviewing of student writing has been widely
used in various academic fields. While existing
web-based peer-review systems largely save instruc-
tors effort in setting up peer-review assignments and
managing document assignment, there still remains
the problem that the quality of peer reviews is of-
ten poor (Nelson and Schunn, 2009). Thus to en-
hance the effectiveness of existing peer-review sys-
tems, we propose to automatically predict the help-
fulness of peer reviews.
In this paper, we examine prior techniques that
have been used to successfully rank helpfulness for
product reviews, and adapt them to the peer-review
domain. In particular, we use an SVM regression al-
gorithm to predict the helpfulness of peer reviews
based on generic linguistic features automatically
mined from peer reviews and students? papers, plus
specialized features based on existing knowledge
about peer reviews. We not only demonstrate that
prior techniques from product reviews can be suc-
cessfully tailored to peer reviews, but also show the
importance of peer-review specific features.
2 Related Work
Prior studies of peer review in the Natural Lan-
guage Processing field have not focused on help-
fulness prediction, but instead have been concerned
with issues such as highlighting key sentences in pa-
pers (Sandor and Vorndran, 2009), detecting impor-
tant feedback features in reviews (Cho, 2008; Xiong
and Litman, 2010), and adapting peer-review assign-
ment (Garcia, 2010). However, given some simi-
larity between peer reviews and other review types,
we hypothesize that techniques used to predict re-
view helpfulness in other domains can also be ap-
plied to peer reviews. Kim et al (2006) used re-
gression to predict the helpfulness ranking of prod-
uct reviews based on various classes of linguistic
features. Ghose and Ipeirotis (2010) further exam-
ined the socio-economic impact of product reviews
using a similar approach and suggested the useful-
ness of subjectivity analysis. Another study (Liu
et al, 2008) of movie reviews showed that helpful-
ness depends on reviewers? expertise, their writing
style, and the timeliness of the review. Tsur and
Rappoport (2009) proposed RevRank to select the
most helpful book reviews in an unsupervised fash-
ion based on review lexicons. However, studies of
Amazon?s product reviews also show that the per-
502
Class Label Features
Structural STR review length in terms of tokens, number of sentences, percentage of sentences
that end with question marks, number of exclamatory sentences.
Lexical UGR, BGR tf-idf statistics of review unigrams and bigrams.
Syntactic SYN
percentage of tokens that are nouns, verbs, verbs conjugated in the
first person, adjectives / adverbs and open classes, respectively.
Semantic
TOP, counts of topic words,
posW, negW counts of positive and negative sentiment words.
Meta-data MET
the overall ratings of papers assigned by reviewers, and the absolute
difference between the rating and the average score given by all reviewers.
Table 1: Generic features motivated by related work of product reviews (Kim et al, 2006).
ceived helpfulness of a review depends not only on
its review content, but also on social effects such as
product qualities, and individual bias in the presence
of mixed opinion distribution (Danescu-Niculescu-
Mizil et al, 2009).
Nonetheless, several properties distinguish our
corpus of peer reviews from other types of reviews:
1) The helpfulness of our peer reviews is directly
rated using a discrete scale from one to five instead
of being defined as a function of binary votes (e.g.
the percentage of ?helpful? votes (Kim et al, 2006));
2) Peer reviews frequently refer to the related stu-
dents? papers, thus review analysis needs to take into
account paper topics; 3) Within the context of edu-
cation, peer-review helpfulness often has a writing
specific semantics, e.g. improving revision likeli-
hood; 4) In general, peer-review corpora collected
from classrooms are of a much smaller size com-
pared to online product reviews. To tailor existing
techniques to peer reviews, we will thus propose
new specialized features to address these issues.
3 Data and Features
In this study, we use a previously annotated peer-
review corpus (Nelson and Schunn, 2009; Patchan
et al, 2009), collected using a freely available web-
based peer-review system (Cho and Schunn, 2007)
in an introductory college history class. The corpus
consists of 16 papers (about six pages each) and 267
reviews (varying from twenty words to about two
hundred words). Two experts (a writing instructor
and a content instructor) (Patchan et al, 2009) were
asked to rate the helpfulness of each peer review
on a scale from one to five (Pearson correlation
r = 0.425, p < 0.01). For our study, we consider
the average ratings given by the two experts (which
roughly follow a normal distribution) as the gold
standard of review helpfulness. Two example rated
peer reviews (shown verbatim) follow:
A helpful peer review of average-rating 5:
The support and explanation of the ideas could use
some work. broading the explanations to include all
groups could be useful. My concerns come from some
of the claims that are put forth. Page 2 says that the
13th amendment ended the war. is this true? was there
no more fighting or problems once this amendment was
added? ...
The arguments were sorted up into paragraphs,
keeping the area of interest clear, but be careful about
bringing up new things at the end and then simply leaving
them there without elaboration (ie black sterilization at
the end of the paragraph).
An unhelpful peer review of average-rating 1:
Your paper and its main points are easy to find and to
follow.
As shown in Table 1, we first mine generic
linguistic features from reviews and papers based
on the results of syntactic analysis of the texts,
aiming to replicate the feature sets used by Kim et
al. (2006). While structural, lexical and syntactic
features are created in the same way as suggested
in their paper, we adapt the semantic and meta-data
features to peer reviews by converting the mentions
of product properties to mentions of the history
topics and by using paper ratings assigned by peers
instead of product scores.1
1We used MSTParser (McDonald et al, 2005) for syntactic
analysis. Topic words are automatically extracted from all stu-
503
In addition, the following specialized features are
motivated by an empirical study in cognitive sci-
ence (Nelson and Schunn, 2009), which suggests
that students? revision likelihood is significantly cor-
related with certain feedback features, and by our
prior work (Xiong and Litman, 2010; Xiong et
al., 2010) for detecting these cognitive science con-
structs automatically:
Cognitive-science features (cogS): For a given
review, cognitive-science constructs that are signifi-
cantly correlated with review implementation likeli-
hood are manually coded for each idea unit (Nel-
son and Schunn, 2009) within the review. Note,
however, that peer-review helpfulness is rated for
the whole review, which can include multiple idea
units.2 Therefore in our study, we calculate the dis-
tribution of feedbackType values (praise, problem,
and summary) (kappa = .92), the percentage of
problems that have problem localization ?the pres-
ence of information indicating where the problem is
localized in the related paper? (kappa = .69), and
the percentage of problems that have a solution ?
the presence of a solution addressing the problem
mentioned in the review? (kappa = .79) to model
peer-review helpfulness. These kappa values (Nel-
son and Schunn, 2009) were calculated from a sub-
set of the corpus for evaluating the reliability of hu-
man annotations3. Consider the example of the help-
ful review presented in Section 3 which was manu-
ally separated into two idea units (each presented in
a separate paragraph). As both ideas are coded as
problem with the presence of problem localization
and solution, the cognitive-science features of this
review are praise%=0, problem%=1, summary%=0,
localization%=1, and solution%=1.
Lexical category features (LEX2): Ten cate-
gories of keyword lexicons developed for automat-
ically detecting the previously manually annotated
feedback types (Xiong et al, 2010). The categories
are learned in a semi-supervised way based on syn-
tactic and semantic functions, such as suggestion
dents? papers using topic signature (Lin and Hovy, 2000) soft-
ware kindly provided by Annie Louis. Positive and negative
sentiment words are extracted from the General Inquirer Dic-
tionaries (http://www.wjh.harvard.edu/ inquirer/homecat.htm).
2Details of different granularity levels of annotation can be
found in (Nelson and Schunn, 2009).
3These annotators are not the same experts who rated the
peer-review helpfulness.
modal verbs (e.g. should, must, might, could, need),
negations (e.g. not, don?t, doesn?t), positive and neg-
ative words, and so on. We first manually created
a list of words that were specified as signal words
for annotating feedbackType and problem localiza-
tion in the coding manual; then we supplemented
the list with words selected by a decision tree model
learned using a Bag-of-Words representation of the
peer reviews. These categories will also be helpful
for reducing the feature space size as discussed be-
low.
Localization features (LOC): Five features de-
veloped in our prior work (Xiong and Litman, 2010)
for automatically identifying the manually coded
problem localization tags, such as the percentage of
problems in reviews that could be matched with a
localization pattern (e.g. ?on page 5?, ?the section
about?), the percentage of sentences in which topic
words exist between the subject and object, etc.
4 Experiment and Results
Following Kim et al (2006), we train our helpful-
ness model using SVM regression with a radial ba-
sis function kernel provided by SVMlight (Joachims,
1999). We first evaluate each feature type in iso-
lation to investigate its predictive power of peer-
review helpfulness; we then examine them together
in various combinations to find the most useful fea-
ture set for modeling peer-review helpfulness. Per-
formance is evaluated in 10-fold cross validation
of our 267 peer reviews by predicting the absolute
helpfulness scores (with Pearson correlation coeffi-
cient r) as well as by predicting helpfulness rank-
ing (with Spearman rank correlation coefficient rs).
Although predicted helpfulness ranking could be di-
rectly used to compare the helpfulness of a given set
of reviews, predicting helpfulness rating is desirable
in practice to compare helpfulness between existing
reviews and new written ones without reranking all
previously ranked reviews. Results are presented re-
garding the generic features and the specialized fea-
tures respectively, with 95% confidence bounds.
4.1 Performance of Generic Features
Evaluation of the generic features is presented in
Table 2, showing that all classes except syntac-
tic (SYN) and meta-data (MET) features are sig-
504
nificantly correlated with both helpfulness rating
(r) and helpfulness ranking (rs). Structural fea-
tures (bolded) achieve the highest Pearson (0.60)
and Spearman correlation coefficients (0.59) (al-
though within the significant correlations, the dif-
ference among coefficients are insignificant). Note
that in isolation, MET (paper ratings) are not sig-
nificantly correlated with peer-review helpfulness,
which is different from prior findings of product re-
views (Kim et al, 2006) where product scores are
significantly correlated with product-review help-
fulness. However, when combined with other fea-
tures, MET does appear to add value (last row).
When comparing the performance between predict-
ing helpfulness ratings versus ranking, we observe
r ? rs consistently for our peer reviews, while Kim
et al (2006) reported r < rs for product reviews.4
Finally, we observed a similar feature redundancy
effect as Kim et al (2006) did, in that simply com-
bining all features does not improve the model?s per-
formance. Interestingly, our best feature combina-
tion (last row) is the same as theirs. In sum our
results verify our hypothesis that the effectiveness
of generic features can be transferred to our peer-
review domain for predicting review helpfulness.
Features Pearson r Spearman rs
STR 0.60? 0.10* 0.59? 0.10*
UGR 0.53? 0.09* 0.54? 0.09*
BGR 0.58? 0.07* 0.57? 0.10*
SYN 0.36? 0.12 0.35? 0.11
TOP 0.55? 0.10* 0.54? 0.10*
posW 0.57? 0.13* 0.53? 0.12*
negW 0.49? 0.11* 0.46? 0.10*
MET 0.22? 0.15 0.23? 0.12
All-combined 0.56? 0.07* 0.58? 0.09*
STR+UGR+MET
0.61? 0.10* 0.61? 0.10*
+TOP
STR+UGR+MET 0.62? 0.10* 0.61? 0.10*
Table 2: Performance evaluation of the generic features
for predicting peer-review helpfulness. Significant results
are marked by * (p ? 0.05).
4.2 Analysis of the Specialized Features
Evaluation of the specialized features is shown in
Table 3, where all features examined are signifi-
4The best performing single feature type reported (Kim et
al., 2006) was review unigrams: r = 0.398 and rs = 0.593.
cantly correlated with both helpfulness rating and
ranking. When evaluated in isolation, although
specialized features have weaker correlation coeffi-
cients ([0.43, 0.51]) than the best generic features,
these differences are not significant, and the special-
ized features have the potential advantage of being
theory-based. The use of features related to mean-
ingful dimensions of writing has contributed to va-
lidity and greater acceptability in the related area of
automated essay scoring (Attali and Burstein, 2006).
When combined with some generic features, the
specialized features improve the model?s perfor-
mance in terms of both r and rs compared to
the best performance in Section 4.1 (the baseline).
Though the improvement is not significant yet, we
think it still interesting to investigate the potential
trend to understand how specialized features cap-
ture additional information of peer-review helpful-
ness. Therefore, the following analysis is also pre-
sented (based on the absolute mean values), where
we start from the baseline feature set, and gradually
expand it by adding our new specialized features:
1) We first replace the raw lexical unigram features
(UGR) with lexical category features (LEX2), which
slightly improves the performance before rounding
to the significant digits shown in row 5. Note that
the categories not only substantially abstract lexical
information from the reviews, but also carry simple
syntactic and semantic information. 2) We then add
one semantic class ? topic words (row 6), which en-
hances the performance further. Semantic features
did not help when working with generic lexical fea-
tures in Section 4.1 (second to last row in Table 2),
but they can be successfully combined with the lexi-
cal category features and further improve the perfor-
mance as indicated here. 3) When cognitive-science
and localization features are introduced, the predic-
tion becomes even more accurate, which reaches a
Pearson correlation of 0.67 and a Spearman correla-
tion of 0.67 (Table 3, last row).
5 Discussion
Despite the difference between peer reviews and
other types of reviews as discussed in Section 2,
our work demonstrates that many generic linguistic
features are also effective in predicting peer-review
helpfulness. The model?s performance can be alter-
505
Features Pearson r Spearman rs
cogS 0.43? 0.09 0.46? 0.07
LEX2 0.51? 0.11 0.50? 0.10
LOC 0.45? 0.13 0.47? 0.11
STR+MET+UGR
0.62? 0.10 0.61? 0.10
(Baseline)
STR+MET+LEX2 0.62? 0.10 0.61? 0.09
STR+MET+LEX2+
0.65? 0.10 0.66? 0.08
TOP
STR+MET+LEX2+
0.66? 0.09 0.66? 0.08
TOP+cogS
STR+MET+LEX2+
0.67? 0.09 0.67? 0.08
TOP+cogS+LOC
Table 3: Evaluation of the model?s performance (all sig-
nificant) after introducing the specialized features.
natively achieved and further improved by adding
auxiliary features tailored to peer reviews. These
specialized features not only introduce domain ex-
pertise, but also capture linguistic information at an
abstracted level, which can help avoid the risk of
over-fitting. Given only 267 peer reviews in our
case compared to more than ten thousand product
reviews (Kim et al, 2006), this is an important con-
sideration.
Though our absolute quantitative results are
not directly comparable to the results of Kim et
al. (2006), we indirectly compared them by ana-
lyzing the utility of features in isolation and com-
bined. While STR+UGR+MET is found as the best
combination of generic features for both types of
reviews, the best individual feature type is differ-
ent (review unigrams work best for product reviews;
structural features work best for peer reviews). More
importantly, meta-data, which are found to signif-
icantly affect the perceived helpfulness of product
reviews (Kim et al, 2006; Danescu-Niculescu-Mizil
et al, 2009), have no predictive power for peer re-
views. Perhaps because the paper grades and other
helpfulness ratings are not visible to the reviewers,
we have less of a social dimension for predicting
the helpfulness of peer reviews. We also found that
SVM regression does not favor ranking over predict-
ing helpfulness as in (Kim et al, 2006).
6 Conclusions and Future Work
The contribution of our work is three-fold: 1) Our
work successfully demonstrates that techniques used
in predicting product review helpfulness ranking can
be effectively adapted to the domain of peer reviews,
with minor modifications to the semantic and meta-
data features. 2) Our qualitative comparison shows
that the utility of generic features (e.g. meta-data
features) in predicting review helpfulness varies be-
tween different review types. 3) We further show
that prediction performance could be improved by
incorporating specialized features that capture help-
fulness information specific to peer reviews.
In the future, we would like to replace the man-
ually coded peer-review specialized features (cogS)
with their automatic predictions, since we have al-
ready shown in our prior work that some impor-
tant cognitive-science constructs can be successfully
identified automatically.5 Also, it is interesting to
observe that the average helpfulness ratings assigned
by experts (used as the gold standard in this study)
differ from those given by students. Prior work on
this corpus has already shown that feedback fea-
tures of review comments differ not only between
students and experts, but also between the writing
and the content experts (Patchan et al, 2009). While
Patchan et al (2009) focused on the review com-
ments, we hypothesize that there is also a difference
in perceived peer-review helpfulness. Therefore, we
are planning to investigate the impact of these dif-
ferent helpfulness ratings on the utilities of features
used in modeling peer-review helpfulness. Finally,
we would like to integrate our helpfulness model
into a web-based peer-review system to improve the
quality of both peer reviews and paper revisions.
Acknowledgements
This work was supported by the Learning Research
and Development Center at the University of Pitts-
burgh. We thank Melissa Patchan and Christian D.
Schunn for generously providing the manually an-
notated peer-review corpus. We are also grateful to
Christian D. Schunn, Janyce Wiebe, Joanna Drum-
mond, and Michael Lipschultz who kindly gave us
valuable feedback while writing this paper.
5The accuracy rate is 0.79 for predicting feedbackType, 0.78
for problem localization, and 0.81 for solution on the same his-
tory data set.
506
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. In Michael Russell, editor,
The Journal of Technology, Learning and Assessment
(JTLA), volume 4, February.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. In Computers
and Education, volume 48, pages 409?426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings of the First In-
ternational Conference on Educational Data Mining
(EDM2008), pages 192?196.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinber g, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon .com helpfulness votes. In Proceedings of
WWW, pages 141?150.
Raquel M. Crespo Garcia. 2010. Exploring document
clustering techniques for personalized peer assessment
in exploratory courses. In Proceedings of Computer-
Supported Peer Review in Education (CSPRED) Work-
shop in the Tenth International Conference on Intelli-
gent Tutoring Systems (ITS 2010).
Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Esti-
mating the helpfulness and economic impact of prod-
uct reviews: Mining text and reviewer characteristics.
In IEEE Transactions on Knowledge and Data Engi-
neering, volume 99, Los Alamitos, CA, USA. IEEE
Computer Society.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT Press, Cambridge, MA,
USA.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423?430, Sydney, Australia,
July.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th conference on
Computational linguistics, volume 1 of COLING ?00,
pages 495?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.
2008. Modeling and predicting the helpfulness of on-
line reviews. In Proceedings of the Eighth IEEE Inter-
national Conference on Data Mining, pages 443?452,
Los Alamitos, CA, USA.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 91?98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. In Instructional Sci-
ence, volume 37, pages 375?401.
Melissa M. Patchan, Davida Charney, and Christian D.
Schunn. 2009. A validation study of students? end
comments: Comparing comments by students, a writ-
ing instructor, and a content instructor. In Journal of
Writing Research, volume 1, pages 124?152. Univer-
sity of Antwerp.
Agnes Sandor and Angela Vorndran. 2009. Detect-
ing key sentences for automatic assistance in peer-
reviewing research articles in educational sciences. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), pages 36?44.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most helpful
book reviews. In Proceedings of the Third Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM2009), pages 36?44.
Wenting Xiong and Diane J. Litman. 2010. Identifying
problem localization in peer-review feedback. In Pro-
ceedings of Tenth International Conference on Intelli-
gent Tutoring Systems (ITS2010), volume 6095, pages
429?431.
Wenting Xiong, Diane J. Litman, and Christian D.
Schunn. 2010. Assessing reviewers performance
based on mining problem localization in peer-review
data. In Proceedings of the Third International Con-
ference on Educational Data Mining (EDM2010),
pages 211?220.
507
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 10?19,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Understanding Differences in Perceived Peer-Review Helpfulness using
Natural Language Processing
Wenting Xiong
University of Pittsburgh
Department of Computer Science
Pittsburgh, PA, 15260
wex12@cs.pitt.edu
Diane Litman
University of Pittsburgh
Department of Computer Science &
Learning Research and Development Center
Pittsburgh, PA, 15260
litman@cs.pitt.edu
Abstract
Identifying peer-review helpfulness is an im-
portant task for improving the quality of feed-
back received by students, as well as for help-
ing students write better reviews. As we tailor
standard product review analysis techniques to
our peer-review domain, we notice that peer-
review helpfulness differs not only between
students and experts but also between types
of experts. In this paper, we investigate how
different types of perceived helpfulness might
influence the utility of features for automatic
prediction. Our feature selection results show
that certain low-level linguistic features are
more useful for predicting student perceived
helpfulness, while high-level cognitive con-
structs are more effective in modeling experts?
perceived helpfulness.
1 Introduction
Peer review of writing is a commonly recommended
technique to include in good writing instruction. It
not only provides more feedback compared to what
students might get from their instructors, but also
provides opportunities for students to practice writ-
ing helpful reviews. While existing web-based peer-
review systems facilitate peer review from the logis-
tic aspect (e.g. collecting papers from authors, as-
signing reviewers, and sending reviews back), there
still remains the problem that the quality of peer
reviews varies, and potentially good feedback is
not written in a helpful way. To address this is-
sue, we propose to add a peer-review helpfulness
model to current peer-review systems, to automat-
ically predict peer-review helpfulness based on fea-
tures mined from textual reviews using Natural Lan-
guage Processing (NLP) techniques. Such an intel-
ligent component could enable peer-review systems
to 1) control the quality of peer reviews that are sent
back to authors, so authors can focus on the help-
ful ones; and 2) provide feedback to reviewers with
respect to their reviewing performance, so students
can learn to write better reviews.
In our prior work (Xiong and Litman, 2011), we
examined whether techniques used for predicting the
helpfulness of product reviews (Kim et al, 2006)
could be tailored to our peer-review domain, where
the definition of helpfulness is largely influenced by
the educational context of peer review. While previ-
ously we used the average of two expert-provided
ratings as our gold standard of peer-review help-
fulness1, there are other types of helpfulness rating
(e.g. author perceived helpfulness) that could be the
gold standard, and that could potentially impact the
features used to build the helpfulness model. In fact,
we observe that peer-review helpfulness seems to
differ not only between students and experts (exam-
ple 1), but also between types of experts (example
2).
In the following examples, students judge helpful-
ness with discrete ratings from one to seven; experts
judge it using a one to five scale. Higher ratings on
both scales correspond to the most helpful reviews.
Example 1:
Student rating = 7, Average expert rating = 2 The
1Averaged ratings are considered more reliable since they
are less noisy.
10
author also has great logic in this paper. How can
we consider the United States a great democracy
when everyone is not treated equal. All of the main
points were indeed supported in this piece.
Student rating = 3, Average expert rating = 5 I
thought there were some good opportunities to
provide further data to strengthen your argument.
For example the statement ?These methods of
intimidation, and the lack of military force offered
by the government to stop the KKK, led to the
rescinding of African American democracy.?
Maybe here include data about how . . . (126 words)
Example 2:
Writing-expert rating = 2, Content-expert rating = 5
Your over all arguements were organized in some
order but was unclear due to the lack of thesis in
the paper. Inside each arguement, there was no
order to the ideas presented, they went back and
forth between ideas. There was good support to
the arguements but yet some of it didnt not fit your
arguement.
Writing-expert rating = 5, Content-expert rating = 2
First off, it seems that you have difficulty writing
transitions between paragraphs. It seems that you
end your paragraphs with the main idea of each
paragraph. That being said, . . . (173 words) As a
final comment, try to continually move your paper,
that is, have in your mind a logical flow with every
paragraph having a purpose.
To better understand such differences and inves-
tigate their impact on automatically assessing peer-
review helpfulness, in this paper, we compare help-
fulness predictions using our many different pos-
sibilities for gold standard ratings. In particular,
we compare the predictive ability of features across
gold standard ratings by examining the most use-
ful features and feature ranks using standard feature
selection techniques. We show that paper ratings
and lexicon categories that suggest clear transitions
and opinions are most useful in predicting helpful-
ness as perceived by students, while review length
is generally effective in predicting expert helpful-
ness. While the presence of praise and summary
comments are more effective in modeling writing-
expert helpfulness, providing solutions is more use-
ful in predicting content-expert helpfulness.
2 Related Work
To our knowledge, no prior work on peer review
from the NLP community has attempted to auto-
matically predict peer-review helpfulness. Instead,
the NLP community has focused on issues such as
highlighting key sentences in papers (Sandor and
Vorndran, 2009), detecting important feedback fea-
tures in reviews (Cho, 2008; Xiong and Litman,
2010), and adapting peer-review assignment (Gar-
cia, 2010). However, many NLP studies have been
done on the helpfulness of other types of reviews,
such as product reviews (Kim et al, 2006; Ghose
and Ipeirotis, 2010), movie reviews (Liu et al,
2008), book reviews (Tsur and Rappoport, 2009),
etc. Kim et al (2006) used regression to predict the
helpfulness ranking of product reviews based on var-
ious classes of linguistic features. Ghose and Ipeiro-
tis (2010) further examined the socio-economic im-
pact of product reviews using a similar approach
and suggested the usefulness of subjectivity analy-
sis. Another study (Liu et al, 2008) of movie re-
views showed that helpfulness depends on review-
ers? expertise, their writing style, and the timeliness
of the review. Tsur and Rappoport (2009) proposed
RevRank to select the most helpful book reviews in
an unsupervised fashion based on review lexicons.
To tailor the utility of this prior work on help-
fulness prediction to educational peer reviews, we
will draw upon research on peer review in cognitive
science. One empirical study of the nature of peer-
review feedback (Nelson and Schunn, 2009) found
that feedback implementation likelihood is signif-
icantly correlated with five feedback features. Of
these features, problem localization ?pinpointing
the source of the problem and/or solution in the orig-
inal paper? and solution ?providing a solution to
the observed problem? were found to be most im-
portant. Researchers (Cho, 2008; Xiong and Lit-
man, 2010) have already shown that some of these
constructs can be automatically learned from tex-
tual input using Machine Learning and NLP tech-
niques. In addition to investigating what proper-
ties of textual comments make peer-review helpful,
researchers also examined how the comments pro-
duced by students versus by different types of ex-
perts differ (Patchan et al, 2009). Though focusing
on differences between what students and experts
11
produce, such work sheds light on our study of stu-
dents? and experts? helpfulness ratings of the same
student comments (i.e. what students and experts
value).
Our work in peer-review helpfulness prediction
integrates the NLP techniques and cognitive-science
approaches mentioned above. We will particularly
focus on examining the utility of features motivated
by related work from both areas, with respect to dif-
ferent types of gold standard ratings of peer-review
helpfulness for automatic prediction.
3 Data
In this study, we use a previously annotated peer-
review corpus (Nelson and Schunn, 2009; Patchan
et al, 2009) that was collected in an introduc-
tory college history class using the freely available
web-based peer-review SWoRD (Scaffolded Writ-
ing and Rewriting in the Discipline) system (Cho
and Schunn, 2007). The corpus consists of 16 pa-
pers (about six pages each) and 189 reviews (vary-
ing from twenty words to about two hundred words)
accompanied by numeric ratings of the papers. Each
review was manually segmented into idea units (de-
fined as contiguous feedback referring to a single
topic) (Nelson and Schunn, 2009), and these idea
units were then annotated by two independent an-
notators for various coding categories, such as feed-
back type (praise, problem, and summary), problem
localization, solution, etc. For example, the sec-
ond case in Example 1, which only has one idea
unit, was annotated as feedbackType = problem,
problemlocalization = True, and solution =
True. The agreement (Kappa) between the two an-
notators is 0.92 for FeedbackType, 0.69 for localiza-
tion, and 0.89 for solution.2
Our corpus also contains author provided back
evaluations. At the end of the peer-review assign-
ment, students were asked to provide back evalu-
ation on each review that they received by rating
review helpfulness using a discrete scale from one
to seven. After the corpus was collected, one writ-
2For Kappa value interpretation, Landis and Koch (1977)
propose the following agreement standard: 0.21-0.40 = ?Fair?;
0.41-0.60 = ?Moderate?; 0.61-0.80 = ?Substantial?; 0.81-1.00
= ?Almost Perfect?. Thus, while localization signals are more
difficult to annotate, the inter-annotator agreement is still sub-
stantial.
ing expert and one content expert were also asked to
rate review helpfulness with a slightly different scale
from one to five. For our study, we will also com-
pute the average ratings given by the two experts,
yielding four types of possible gold-standard ratings
of peer-review helpfulness for each review. Figure 1
shows the rating distribution of each type. Interest-
ingly, we observed that expert ratings roughly follow
a normal distribution, while students are more likely
to give higher ratings (as illustrated in Figure 1).
4 Features
Our features are motivated by the prior work in-
troduced in Section 2, in particular, NLP work on
predicting product-review helpfulness (Kim et al,
2006), as well as work on automatically learning
cognitive-science constructs (Nelson and Schunn,
2009) using NLP (Cho, 2008; Xiong and Litman,
2010). The complete list of features is shown in Ta-
ble 3 and described below. The computational lin-
guistic features are automatically extracted based
on the output of syntactic analysis of reviews and
papers3. These features represent structural, lexi-
cal, syntactic and semantic information of the tex-
tual content, and also include information for identi-
fying certain important cognitive constructs:
? Structural features consider the general struc-
ture of reviews, which includes review length in
terms of tokens (reviewLength), number of sen-
tences (sentNum), the average sentence length
(sentLengthAve), percentage of sentences that
end with question marks (question%), and
number of exclamatory sentences (exclams).
? Lexical features are counts of ten lexical cat-
egories (Table 1), where the categories were
learned in a semi-supervised way from review
lexicons in a pilot study. We first manually cre-
ated a list of words that were specified as signal
words for annotating feedbackType and prob-
lem localization in the coding manual; then
we supplemented the list with words selected
by a decision tree model learned using a Bag-
of-Words representation of the peer reviews.
3We used MSTParser (McDonald et al, 2005) for syntactic
analysis.
12
Figure 1: Distribution of peer-review helpfulness when rated by students and experts
Tag Meaning Word list
SUG suggestion should, must, might, could, need, needs, maybe, try, revision, want
LOC location page, paragraph, sentence
ERR problem error, mistakes, typo, problem, difficulties, conclusion
IDE idea verb consider, mention
LNK transition however, but
NEG negative words fail, hard, difficult, bad, short, little, bit, poor, few, unclear, only, more
POS positive words great, good, well, clearly, easily, effective, effectively, helpful, very
SUM summarization main, overall, also, how, job
NOT negation not, doesn?t, don?t
SOL solution revision specify correction
Table 1: Ten lexical categories
Compared with commonly used lexical uni-
grams and bigrams (Kim et al, 2006), these
lexical categories are equally useful in model-
ing peer-review helpfulness, and significantly
reduce the feature space.4
? Syntactic features mainly focus on nouns and
verbs, and include percentage of tokens that are
nouns, verbs, verbs conjugated in the first per-
son (1stPVerb%), adjectives/adverbs, and open
classes, respectively.
? Semantic features capture two important peer-
4Lexical categories help avoid the risk of over-fitting, given
only 189 peer reviews in our case compared to more than ten
thousand Amazon.com reviews used for predicting product re-
view helpfulness (Kim et al, 2006).
review properties: their relevance to the main
topics in students? papers, and their opinion
sentiment polarities. Kim et al (2006) ex-
tracted product property keywords from exter-
nal resources based on their hypothesis that
helpful product reviews refer frequently to cer-
tain product properties. Similarly, we hypothe-
size that helpful peer reviews are closely related
to domain topics that are shared by all students
papers in an assignment. Our domain topic set
contains 288 words extracted from the collec-
tion of student papers using topic-lexicon ex-
traction software5; our feature (domainWord)
5The software extracts topic words based on topic signa-
tures (Lin and Hovy, 2000), and was kindly provided by Annie
Louis.
13
Feature Description
regTag% The percentage of problems in reviews that could be matched with a localization pattern.
soDomain% The percentage of sentences where any domain word appears between the subject and the object.
dDeterminer The number of demonstrative determiners.
windowSize For each review sentence, we search for the most likely referred window of words in the relatedpaper, and windowSize is the average number of words of all windows.
Table 2: Localization features
counts how many words of a given review be-
long to the extracted set. For sentiment po-
larities, we extract positive and negative sen-
timent words from the General Inquirer Dictio-
naries 6, and count their appearance in reviews
in terms of their sentiment polarity (posWord,
negWord).
? Localization features are motivated by lin-
guistic features that are used for automatically
predicting problem localization (an important
cognitive construct for feedback understand-
ing and implementation) (Nelson and Schunn,
2009), and are presented in Table 2. To illus-
trate how these features are computed, consider
the following critique:
The section of the essay on African
Americans needs more careful at-
tention to the timing and reasons
for the federal governments decision
to stop protecting African American
civil and political rights.
The review has only one sentence, in which one
regular expression is matched with ?the section
of? thus regTag% = 1; no demonstrative de-
terminer, thus dDeterminer = 0; ?African?
and ?Americans? are domain words appearing
between the subject ?section? and the object
?attention?, so soDomain is true for this sen-
tence and thus soDomain% = 1 for the given
review.
In addition to the low-level linguistic features pre-
sented above, we also examined non-linguistic fea-
tures that are derived from the ratings and prior
manual annotations of the corpus, described in Sec-
tion 3.
6http://www.wjh.harvard.edu/ inquirer/homecat.htm
? Cognitive-science features are motivated by
an empirical study (Nelson and Schunn, 2009)
which suggests significant correlation between
certain cognitive constructs (e.g. feedbackType,
problem localization, solution) and review im-
plementation likelihood. Intuitively, helpful
reviews are more likely to get implemented,
thus we introduced these features to capture
desirable high-level characteristics of peer re-
views. Note that in our corpus these cogni-
tive constructs are manually coded at the idea-
unit level (Nelson and Schunn, 2009), how-
ever, peer-review helpfulness is rated at the re-
view level.7 Our cognitive-science features ag-
gregate the annotations up to the review-level
by reporting the percentage of idea-units in
a review that exhibit each characteristic: the
distribution of review types (praise%, prob-
lem%, summary%), the percentage of problem-
localized critiques (localization%), as well as
the percentage of solution-provided ones (solu-
tion%).
? Social-science features introduce elements re-
flecting interactions between students in a peer-
review assignment. As suggested in related
work on product review helpfulness (Kim
et al, 2006; Danescu-Niculescu-Mizil et al,
2009), some social dimensions (e.g. customer
opinion on related product quality) are of great
influence in the perceived helpfulness of prod-
uct reviews. Similarly, in our case, we intro-
duced related paper ratings (pRating) ? to con-
sider whether and how helpfulness ratings are
affected by the rating that the paper receives8
? and the absolute difference between the rat-
7Details of different granularity levels of annotation can be
found in (Nelson and Schunn, 2009).
8That is, to examine whether students give higher ratings to
peers who gave them higher paper ratings in the first place.
14
ing and the average score given by all review-
ers (pRatingDiff ) ? to measure the variation in
perceived helpfulness of a given review.
5 Experiments
We take a machine learning approach to model dif-
ferent types of perceived helpfulness (student help-
fulness, writing-expert helpfulness, content-expert
helpfulness, average-expert helpfulness) based on
combinations of linguistic and non-linguistic fea-
tures extracted from our peer-review corpus. Then
we compare the different helpfulness types in terms
of the predictive power of features used in their cor-
responding models. For comparison purpose, we
consider the linguistic and non-linguistic features
both separately and in combination, which generates
three set of features: 1) linguistic features, 2) non-
linguistic features, and 3) all features. For each set
of features, we train four models, each correspond-
ing to a different kind of helpfulness rating. For each
learning task (three by four), we use two standard
feature selection algorithms to find the most useful
features based on 10-fold cross validation. First, we
perform Linear Regression with Greedy Stepwise
search (stepwise LR) to select the most useful fea-
tures when testing in each of the ten folds, and count
how many times each features is selected in the ten
trials. Second, we use Relief Feature Evaluation9
with Ranker (Relief) (Kira and Rendell, 1992; Wit-
ten and Frank, 2005) to rank all used features based
on their average merits (the ability of the given fea-
ture to differentiate between two example pairs) of
ten trials.10
Although both methods are supervised, the wrap-
per is ?more aggressive? because its feature evalu-
ation is based on the performance of the regression
model and thus the resulting feature set is tailored
to the learning algorithm. In contrast, Relief does
not optimize feature sets directly for classifier per-
formance, thus it takes into account class informa-
tion in a ?less aggressive? manner than the Wrapper
method. We use both methods in our experiment to
9Relief evaluates the worth of an attribute by repeatedly
sampling an instance and changing the value of the given at-
tribute based on the nearest instance of the same and different
class.
10Both algorithms are provided by Weka
(http://www.cs.waikato.ac.nz/ml/weka/).
provide complementary perspectives. While the for-
mer can directly tell us what features are most use-
ful, the latter gives feature ranks which provide more
detailed information about differences between fea-
tures. To compare the feature selection results, we
examine the four kind of helpfulness models for
each of the three feature sets separately, as presented
below. Note that the focus of this paper is compar-
ing feature utilities in different helpfulness models
rather than predicting those types of helpfulness rat-
ings. (Details of how the average-expert model per-
forms can be found in our prior work (Xiong and
Litman, 2011).)
5.1 Feature Selection of Linguistic Features
Table 4 presents the feature selection results of com-
putational linguistic features used in modeling the
four different types of peer-review helpfulness. The
first row lists the four sources of helpfulness ratings,
and each column represents a corresponding model.
The second row presents the most useful features
in each model selected by stepwise LR, where ?#
of folds? refers to the number of trials in which the
given feature appears in the resulting feature set dur-
ing the 10-fold cross validation. Here we only report
features that are selected by no less than five folds
(half the time). The third row presents feature ranks
computed using Relief, where we only report the top
six features due to the space limit. Features are or-
dered in descending ranks, and the average merit and
its standard deviation is reported for each one of the
features.
The selection result of stepwise LR shows that
reviewLength is most useful for predicting expert
helpfulness in general, while specific lexicon cate-
gories (i.e. LNK, and NOT) and positive words (pos-
Word) are more useful in predicting student helpful-
ness. When looking at the ranking result, we observe
that transition cues (LNK) and posWord are also
ranked high in the student-helpfulness model, al-
though question% and suggestion words (SUG) are
ranked highest. For expert-helpfulness models, win-
dowSize and posWord, which are not listed in the se-
lected features for expert helpfulness (although they
are selected for students), are actually ranked high
for modeling average-expert helpfulness. While ex-
clamatory sentence number (exclams) and summa-
rization cues are ranked top for the writing expert,
15
Type Features
Structural reviewLength, sentNum, sentLengthAve, question%, exclams
Lexical SUG, LOC, ERR, IDE, LNK, NEG, POS, SUM, NOT, SOL (Table 1)
Syntactic noun%, verb%, 1stPVerb%, adj+adv%, opClass%
Semantic domainWord, posWord, negWord
Localization regTag%, soDomain%, dDeterminer, windowSize (Table 2)
Cognitive-science praise%, problem%, summary%, localization%, solution%
Social-science pRating, pRatingDiff
Table 3: Summary of features
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
LNK 9 reviewLength 8 reviewLength 10 reviewLength 10
Stepwise posWord 8 question% 6 sentNum 8
LR NOT 6 sentNum 5 question% 8
windowSize 6 1stPVerb% 5
POS 5
Relief
Feature Merit Feature Merit Feature Merit Feature Merit
question% .019? .002 exclams .010? .003 question% .010? .004 exclams .010? .003
SUG .015? .003 SUM .008? .004 ERR .009? .003 question% .011? .004
LNK .014? .003 NEG .006? .004 SUG .009? .004 windowSize .008? .002
sentLengthAve .012? .003 negWord .005? .002 posWord .007? .002 posWord .006? .002
POS .011? .002 windowSize .004? .002 exclams .006? .001 reviewLength .004? .001
posWord .010? .001 sentNum .003? .001 1stPVerb% .007? .004 sentLengthAve .004? .001
Table 4: Feature selection based on linguistic features
the percentage of questions (question%) and error
cues (ERR) are ranked top for the content-expert. In
addition, the percentage of words that are verbs con-
jugated in the first person (1stPVerb%) is both se-
lected and ranked high in the content-expert helpful-
ness model. Out of the four models, SUG are ranked
high for predicting both students and content-expert
helpfulness. These observations indicate that both
students and experts value questions (question%)
and suggestions (SUG) in reviews, and students par-
ticularly favor clear signs of logic flow in review ar-
guments (LNK), positive words (posWord), as well
as reference of their paper content which provides
explicit context information (windowSize). In addi-
tion, experts in general prefer longer reviews (re-
viewLength), and the writing expert thinks clear
summary signs (SUM) are important indicators of
helpful peer reviews.
5.2 Feature Selection of non-Linguistic
Features
When switching to the high-level non-linguistic fea-
tures (Table 5), we find that solution% is always se-
lected (in all ten trials) as a most useful feature for
predicting all four kind of helpfulness, and is also
ranked high for content-expert and student helpful-
ness. Especially for the content-expert, solution%
has a much higher merit (0.013) compared to all the
other features (? 0.03). This agrees with our ob-
servation in section 5.1 that SUG are ranked high in
both cases. localization% is selected as one of the
most useful features in the content-expert helpful-
ness model, which is also ranked top in the student
model (though not selected frequently by stepwise
LR). For modeling the writing-expert helpfulness,
praise (praise%) is more important than problem
and summary, and the paper rating (pRating) loses
its predictive power compared to how it works in the
other models. In contrast, pRating is both selected
and ranked high for predicting students? perceived
helpfulness.
5.3 Feature Selection of All Features
When considering all features together as reported
in Table 6, pRating is only selected in the student-
helpfulness model, and still remains to be the most
important feature for predicting students? perceived
helpfulness. As for experts, the structural feature
16
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
Stepwise pRating 10 solution% 10 localization% 10 solution% 10
LR solution% 10 solution% 10 pRating 10
problem% 9 pRating 10 localization% 9
Relief
Feature Merit Feature Merit Feature Merit Feature Merit
localization% .012? .003 praise% .008? .002 solution% .013? .005 problem% .004? .002
pRatingDiff .010? .002 problem% .007? .002 pRating .003? .002 localization% .004? .006
pRating .007? .002 summary% .001? .004 praise% .001? .002 praise% .003? .003
solution% .006? .005 localization% .001? .005 localization% .001? .004 solution% .002? .004
problem% .004? .002 pRating .004? .004 problem% .001? .002 pRating .005? .003
summary% .004? .003 pRatingDiff .007? .002 pRating .002? .003 pRatingDiff .006? .005
Table 5: Feature selection based on non-linguistic features
Source Students Writing expert Content expert Expert average
Feature # of folds Feature # of folds Feature # of folds Feature # of folds
Stepwise pRating 10 reviewLength 10 reviewLength 10 reviewLength 10
LR dDeterminer 7 problem% 8 problem% 6
pRatingDiff 5
sentNum 5
Relief
Feature Merit Feature Merit Feature Merit Feature Merit
pRating .030? .006 exclams .016? .003 solution% .025? .003 exclams .015? .004
NOT .019? .004 praise% .015? .003 domainWord .012? .002 question% .012? .004
pRatingDiff .019? .005 SUM .013? .004 regTag% .012? .007 LOC .007? .002
sentNum .014? .002 summary% .008? .003 reviewLength .009? .002 sentNum .007? .002
question% .014? .003 problem% .009? .003 question% .010? .003 reviewLength .007? .001
NEG .013? .002 reviewLength .004? .001 sentNum .008? .002 praise% .008? .004
Table 6: Feature selection based on all features
reviewLength stands out from all other features in
both the writing-expert and the content-expert mod-
els. Interestingly, it is the number of sentences (sent-
Num) rather than review length of structure features
that is useful in the student-helpfulness model. And
demonstrative determiners (dDeterminer) is also se-
lected, which indicates that having a clear sign of
comment targets is considered important from the
students? perspective. When examining the model?s
ranking result, we find that more lexicon categories
are ranked high for students compared to other kind
of helpfulness. Specifically, NOT appears high
again, suggesting clear expression of opinion is im-
portant in predicting student-helpfulness. Across
four types of helpfulness, again, we observed that
the writing expert tends to value praise and summary
(indicated by both SUM and summary%) in reviews
while the content-expert favors critiques, especially
solution provided critiques.
5.4 Discussion
Based on our observations from the above three
comparisons, we summarize our findings with re-
spect to different feature types and provide inter-
pretation: 1) review length (in tokens) is generally
effective in predicting expert perceived helpfulness,
while number of sentences is more useful in mod-
eling student perceived helpfulness. Interestingly,
there is a strong correlation between these two fea-
tures (r = 0.91, p ? 0.001), and why one is selected
over the other in different helpfulness models needs
further investigation. 2) Lexical categories such as
transition cues, negation, and suggestion words are
of more importance in modeling student perceived
helpfulness. This might indicate that students pre-
fer clear expression of problem, reference and even
opinion in terms of specific lexicon clues, the lack of
which is likely to result in difficulty in their under-
standing of the reviews. 3) As for cognitive-science
features, solution is generally an effective indica-
tor of helpful peer reviews. Within the three feed-
back types of peer reviews, praise is valued high
by the writing expert. (It is interesting to notice
that although praise is shown to be more impor-
tant than problem and summary for modeling the
writing-expert helpfulness, positive sentiment words
do not appear to be more predictive than negative
sentiments.) In contrast, problem is more desirable
17
from the content expert?s point of view. Although
students assign less importance to the problem them-
selves, solution provided peer reviews could be help-
ful for them with respect to the learning goal of peer-
review assignments. 4) Paper rating is a very ef-
fective feature for predicting review helpfulness per-
ceived by students, which is not the case for either
expert. This supports the argument of social aspects
in people?s perception of review helpfulness, and it
also reflects the fact that students tend to be nice to
each other in such peer-review interactions. How-
ever, this dimension might not correspond with the
real helpfulness of the reviews, at least from the per-
spective of both the writing expert and content ex-
pert.
6 Conclusion and Future Work
We have shown that the type of helpfulness to be
predicted does indeed influence the utility of dif-
ferent feature types for automatic prediction. Low-
level general linguistic features are more predic-
tive when modeling students? perceived helpfulness;
high-level theory supported constructs are more use-
ful in experts? models. However, in the related area
of automated essay scoring (Attali and Burstein,
2006), others have suggested the need for the use
of validated features related to meaningful dimen-
sions of writing, rather than low-level (but easy to
automate) features. In this perspective, our work
similarly poses challenge to the NLP community in
terms of how to take into account the education-
oriented dimensions of helpfulness when applying
traditional NLP techniques of automatically pred-
icating review helpfulness. In addition, it is im-
portant to note that predictive features of perceived
helpfulness are not guaranteed to capture the nature
of ?truly? helpful peer reviews (in contrast to the
perceived ones).
In the future, we would like to investigate how
to integrate useful dimensions of helpfulness per-
ceived by different audiences in order to come up
with a ?true? helpfulness gold standard. We would
also like to explore more sophisticated features and
other NLP techniques to improve our model of peer-
review helpfulness. As we have already built models
to automatically predict certain cognitive constructs
(problem localization and solution), we will replace
the annotated cognitive-science features used here
with their automatic predictions, so that we can build
our helpfulness model fully automatically. Finally,
we would like to integrate our helpfulness model
into a real peer-review system and evaluate its per-
formance extrinsically in terms of improving stu-
dents? learning and reviewing performance in future
peer-review assignments.
Acknowledgments
This work was supported by the Learning Research
and Development Center at the University of Pitts-
burgh. We thank Melissa Patchan and Chris Schunn
for generously providing the manually annotated
peer-review corpus. We are also grateful to Michael
Lipschultz and Chris Schunn for their feedback
while writing this paper.
References
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. The Journal of Technology,
Learning and Assessment (JTLA), 4(3), February.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. Computers and
Education, 48:409?426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings of the First In-
ternational Conference on Educational Data Mining
(EDM2008), pages 192?196.
Cristian Danescu-Niculescu-Mizil, Gueorgi Kossinets,
Jon Kleinberg, and Lillian Lee. 2009. How opin-
ions are received by online communities: A case study
on Amazon.com helpfulness votes. In Proceedings of
WWW, pages 141?150.
Raquel M. Crespo Garcia. 2010. Exploring document
clustering techniques for personalized peer assessment
in exploratory courses. In Proceedings of Computer-
Supported Peer Review in Education (CSPRED) Work-
shop in the Tenth International Conference on Intelli-
gent Tutoring Systems (ITS 2010).
Anindya Ghose and Panagiotis G. Ipeirotis. 2010. Esti-
mating the helpfunless and economic impact of prod-
uct reviews: Mining text and reviewer characteristics.
IEEE Transactions on Knowledge and Data Engineer-
ing, 99.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and Marco
Pennacchiotti. 2006. Automatically assessing review
helpfulness. In Proceedings of the 2006 Conference
18
on Empirical Methods in Natural Language Process-
ing (EMNLP2006), pages 423?430, Sydney, Australia,
July.
Kenji Kira and Larry A. Rendell. 1992. A practical
approach to feature selection. In Derek H. Sleeman
and Peter Edwards, editors, ML92: Proceedings of
the Ninth International Conference on Machine Learn-
ing, pages 249?256, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33:159?174.
Chin-Yew Lin and Eduard Hovy. 2000. The auto-
mated acquisition of topic signatures for text summa-
rization. In Proceedings of the 18th conference on
Computational linguistics, volume 1 of COLING ?00,
pages 495?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Yang Liu, Xiangji Guang, Aijun An, and Xiaohui Yu.
2008. Modeling and predicting the helpfulness of on-
line reviews. In Proceedings of the Eighth IEEE Inter-
national Conference on Data Mining, pages 443?452,
Los Alamitos, CA, USA.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics, ACL ?05,
pages 91?98, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. Instructional Sci-
ence, 37(4):375?401.
Melissa M. Patchan, Davida Charney, and Christian D.
Schunn. 2009. A validation study of students? end
comments: Comparing comments by students, a writ-
ing instructor, and a content instructor. Journal of
Writing Research, 1(2):124?152.
Agnes Sandor and Angela Vorndran. 2009. Detect-
ing key sentences for automatic assistance in peer-
reviewing research articles in educational sciences. In
Proceedings of the 47th Annual Meeting of the Associ-
ation for Computational Linguistics and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing (ACL-IJCNLP), pages 36?44.
Oren Tsur and Ari Rappoport. 2009. Revrank: A fully
unsupervised algorithm for selecting the most helpful
book reviews. In Proceedings of the Third Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM2009), pages 36?44.
IH Witten and E. Frank. 2005. Data Mining: Practi-
cal Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, San Francisco, CA.
Wenting Xiong and Diane Litman. 2010. Identifying
problem localization in peer-review feedback. In Pro-
ceedings of Tenth International Conference on Intelli-
gent Tutoring Systems (ITS2010), volume 6095, pages
429?431.
Wenting Xiong and Diane Litman. 2011. Automatically
predicting peer-review helpfulness. In Proceedings
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies
(ACL/HLT), Portland, Oregon, June.
19
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 174?179,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
An Interactive Analytic Tool for Peer-Review Exploration
Wenting Xiong1,2, Diane Litman1,2, Jingtao Wang1,2 and Christian Schunn2
1 Department of Computer Science & 2 Learning Research and Development Center
University of Pittsburgh, Pittsburgh, PA, 15260
wex12@cs.pitt.edu
Abstract
This paper presents an interactive analytic tool
for educational peer-review analysis. It em-
ploys data visualization at multiple levels of
granularity, and provides automated analytic
support using clustering and natural language
processing. This tool helps instructors dis-
cover interesting patterns in writing perfor-
mance that are reflected through peer reviews.
1 Introduction
Peer review is a widely used educational approach
for coaching writing in many domains (Topping,
1998; Topping, 2009). Because of the large number
of review comments to examine, instructors giving
peer review assignments find it difficult to examine
peer comments. While there are web-based peer-
review systems that help instructors set up peer-
review assignments, no prior work has been done
to support instructors? comprehension of the textual
review comments.
To address this issue, we have designed and devel-
oped an interactive analytic interface (RevExplore)
on top of SWoRD1 (Cho and Schunn, 2007), a web-
based peer-review reciprocal system that has been
used by over 12,000 students over the last 8 years.
In this paper, we show how RevExplore visualizes
peer-review information in multiple dimensions and
various granularity levels to support investigative
exploration, and applies natural language process-
ing (NLP) techniques to facilitate review compre-
hension and comparison.
1https://sites.google.com/site/swordlrdc/
2 Design Goals
Instructors face challenges when they try to make
sense of the peer-review data collected by SWoRD
for their assignments. Instructors we have inter-
viewed have complained that peer reviews are time-
consuming to read and almost ?impossible? to in-
terpret: 1) to understand the pros and cons of one
student?s paper, they need to synthesize all the peer
reviews received by that student by reading them one
by one; 2) furthermore, if instructors would like to
discover general patterns regarding students? writ-
ing performance, they have to additionally compare
peer reviews across multiple students which requires
their simultaneously remembering various opinions
for many students; 3) in the initial stage of peer re-
view analysis, instructors have no clear idea of what
potential patterns they should be looking for (?cold
start?).
These challenges motivate our design of RevEx-
plore, a peer-review analytic tool that is a plugin
to SWoRD. We set our design goals to address the
challenges mentioned above, respectively: 1) cre-
ate a simple and informative representation of peer-
review data which automatically aggregates peer-
reviews at the level of student; 2) provide intelligent
support of text mining and semantic abstraction for
the purpose of comparison; 3) enable an overview of
key characteristics of peer reviews for initial explo-
ration.
To fulfill our design goals, we design an inter-
active visualization system to ease the exploration
process, following the pattern of overview plus de-
tail (Card et al, 1999). In the overview, RevExplore
174
provides a high level of visualization of overall peer-
review information at the student level for initial ex-
ploration. In the detail-view, RevExplore automati-
cally abstracts the semantic information of peer re-
views at the topic-word level, with the original texts
visible on demand. In addition, we introduce clus-
tering and NLP techniques to support automated an-
alytics.
3 Related Work
One major goal of peer review studies in educational
research is to understand how to better improve stu-
dent learning, directly or indirectly. Empirical stud-
ies of textual review comments based on manual
coding have discovered that certain review features
(e.g., whether the solution to a problem is explicitly
stated in a comment) can predict both whether the
problem will be understand and the feedback imple-
mented (Nelson and Schunn, 2009). Our previous
studies used machine learning and NLP techniques
to automatically identify the presence of such useful
features in review comments (Xiong et al, 2010);
similar techniques have also been used to determine
review comment helpfulness (Xiong and Litman,
2011; Cho, 2008). With respect to paper analysis,
Sa?ndor and Vorndran (2009) used NLP to highlight
key sentences, in order to focus reviewer attention
on important paper aspects. Finally, Giannoukos et
al. (2010) focused on peer matching based on stu-
dents? profile information to maximize learning out-
comes, while Crespo Garcia and Pardo (2010) ex-
plored the use of document clustering to adaptively
guide the assignment of papers to peers. In contrast
to the prior work above, the research presented here
is primarily motivated by the needs of instructors,
instead of the needs of students. In particular, the
goal of RevExplore is to utilize the information in
peer reviews and papers, to help instructors better
understand student performance in the peer-review
assignments for their courses.
Many computer tools have already been de-
veloped to support peer review activities in var-
ious types of classrooms, from programming
courses (Hyyrynen et al, 2010) to courses involving
writing in the disciplines (Nelson and Schunn, 2009;
Yang, 2011). Within the writing domain, systems
such as SWoRD (Cho and Schunn, 2007) mainly as-
sist instructors by providing administrative manage-
ment support and/or (optional) automatic grading
services. While peer review systems especially de-
signed for instructors do exist, their goal is typically
to create a collaborative environment for instructors
to improve their professional skills (Fu and Hawkes,
2010). In terms of artificial intelligence support, to
our knowledge no current peer review system has the
power to provide instructors with insights about the
semantic content of peer reviews, due to the diver-
sity and complexity of the textual review comments.
For example, SWoRD currently provides teachers a
numerical summary view that includes the number
of reviews received for each paper, and the mean
and standard deviation of numerical reviewing rat-
ings for each paper. SWoRD also allows instruc-
tors to automatically compute a grade based on a
student?s writing and reviewing quality; the grading
algorithm uses the numerical ratings but not the as-
sociated text comments. In this work, we attempted
to address the lack of semantic insight both by hav-
ing humans in the loop to identify points of interest
for interactive data exploration, and by adapting ex-
isting natural language processing techniques to the
peer review domain to support automated analytics.
4 RevExplore
As an example for illustration, we will use data col-
lected in a college level history class (Nelson and
Schunn, 2009): the instructor created the writing
assignment through SWoRD and provided a peer-
review rubric which required students to assess a
history paper?s quality on three dimensions (logic,
flow and insight) separately, by giving a numeric
rating on a scale of 1-7 in addition to textual com-
ments. While reviewing dimensions and associated
guidelines (see below) are typically created by an in-
structor for a particular assignment, instructors can
also set up their rubric using a library provided by
SWoRD.
For instance, the instructor created the following
guidance for commenting on the ?logic? dimension:
?Provide specific comments about the logic of the
author?s argument. If points were just made without
support, describe which ones they were. If the sup-
port provided doesn?t make logical sense, explain
what that is. If some obvious counter-argument was
175
not considered, explain what that counter-argument
is. Then give potential fixes to these problems if you
can think of any. This might involve suggesting that
the author change their argument.?
Instructor guidance for numerically rating the log-
ical arguments of the paper based on the comments
was also given. For this history assignment, the
highest rating of 7 (?Excellent?) was described as
?All arguments strongly supported and no logical
flaws in the arguments.? The lowest rating of 1
(?Disastrous?) was described as ?No support pre-
sented for any arguments, or obvious flaws in all
arguments.?
24 students submitted their papers online through
SWoRD and then reviewed 6 peers? papers assigned
to them in a ?double blind? manner (review exam-
ples are available in Figure 2). When peer review
is finished, RevExplore loads all papers and peer
reviews, both textual comments and numeric rat-
ings, and then goes through several text processing
steps to prepare for interactive analytics. This pre-
processing includes computing the domain words,
sentence simplification, domain-word masking, syn-
tactic analysis, and key noun-phrase extraction.
4.1 Overview ? Student Clustering
RevExplore starts with a student-centric visualiza-
tion overview. It uses a visual node of a bar chart
to represent each student, visualizing the average of
the student?s peer ratings in gray, as well as the rat-
ing histogram with gradient colors (from red to blue)
that are mapped to the rating scale from 1 to 7 (de-
noted by the legend in Figure 1).
To investigate students? writing performance, in-
structors can manually group similar nodes together
into one stacked bar chart, or use automatic group-
ing options that RevExplore supports to inform ini-
tial hypotheses about peer review patterns. In the
auto-mode, RevExplore can group students regard-
ing a certain property (e.g. rating average); it can
also cluster students using standard clustering algo-
rithms2 based on either rating statistics or Bag-Of-
Words extracted from the relevant peer reviews.
If a instructor is curious about the review content
for certain students during exploration, the instruc-
2RevExplore implements both K-Means and a hierarchical
clustering algorithm.
Figure 1: RevExplore overview. Stacked bar charts rep-
resent student groups. The tooltip shows the ID of the
current student, writing performance (average peer rat-
ings), review helpfulness (average helpfulness ratings), as
well as the main issues in the descending order of their
frequency, which are extracted from the peer reviews re-
ceived by a highlighted student using NLP techniques.
tor can read the main issues, in the form of noun
phrases (NPs) of a student?s peer reviews in a tooltip
by mouse hovering on the bar squares which the stu-
dent corresponds to. For example, Figure 1 shows
that the peer reviews received by this student are
mainly focused on the argumentation and the intro-
duction part of the paper.
To extract peer-review main issues, RevExplore
syntactically simplifies each review sentence (Heil-
man and Smith, 2010), parses each simplified sen-
tence using the Stanford dependency parser (de
Marneffe et al, 2006), and then traverses each de-
pendency tree to find the key NP in a rule-based
manner.3 Due to reviewers? frequent references to
the relevant paper, most of the learned NPs are do-
main related facts used in the paper, rather than eval-
uative texts that suggest problems or suggestions. To
avoid the interference of the domain content, we ap-
ply domain-word masking (explained in Section 4.2)
to the simplified sentences before parsing, and elim-
inate any key NP that contains the mask.
4.2 Detail-View ? Topic Comparison
When two groups of students are selected in the
overview, their textual peer reviews can be further
3Rules are constructed purely based on our intuition.
176
Figure 2: Peer-review exploration using RevExplore, for mining differences between strong and weak students.
compared with respect to specific reviewing dimen-
sions using a list of topic words that are automati-
cally computed in real-time.
Extracting topic words of peer reviews for com-
parison purposes is different from most traditional
topic-word extraction tasks that are commonly in-
volved in text summarization. In traditional text
summarization, the informativeness measurement
is designed to extract the common themes, while
in our case of comparison, instructors are more
concerned with the uniqueness of each target set
of peer reviews compared to the others. Thus a
topic-signature acquisition algorithm (Lin and Hovy,
2000), which extracts topic words through compar-
ing the vocabulary distribution of a target corpus
against that of a generic background corpus using
a statistic metric, suits our application better than
other approaches, such as probabilistic graphical
models (e.g. LDA) and frequency based methods.
Therefore, RevExplore considers topic signatures as
the topic words for a group of reviews, using all peer
reviews as the background corpus.4 Again, to min-
imize the impact of the domain content of the rele-
vant papers, we apply topic-masking which replaces
all domain words5 with ?ddd? before computing the
topic signatures.
As the software outputs topic signatures together
with their associated weights which reflect signature
importance, RevExplore uses this weight informa-
tion to order the topic words as a list, and visualizes
the weight as the font size and foreground color of
the relevant topic word. These lists are placed in
two rows regarding their group membership dimen-
sion by dimension. For each dimension, the cor-
responding lists of both rows are aligned vertically
with the same background color to indicate that di-
mension (e.g. Topic-list detail view of Figure 2).
To further facilitate the comparison within a dimen-
sion, RevExplore highlights the topic words that are
unique to one group with a darker background color.
4We use TopicS (Nenkova and Louis, 2008) provided by An-
nie Louis.
5learned from all student papers against 5000 documents
from the English Gigaword Corpus using TopicS.
177
If the user cannot interpret the topic that an ex-
tracted word might imply, the user can click on the
word to read the relevant original reviews, with that
word highlighted in red (e.g. Original reviews pane
of Figure 2).
5 Analysis Example
Figure 2 shows how RevExplore is used to discover
the difference between strong and weak students
with respect to their writing performance on ?logic?
in the history peer-review assignment introduced in
Section 4.
First we group students into strong versus weak
regarding their writing performance on logic by se-
lecting the K-Means algorithm to cluster students
into two groups based on their rating histogram on
logic. As shown in the Overview pane of Figure 2,
we then label them as A and B for further topic com-
parison.
Next, in the topic-list detail view, we check
?praise? and ?problem?6, and fire the ?enter? but-
ton to start extracting topic words for group A and B
on every selected dimension. Note that ?logic? will
be automatically selected since the focus has already
been narrowed down to logic in the overview.
To first compare the difference in general logic is-
sues between these two groups, we refer to the two
lists on ?logic? (in the middle of the topic-list de-
tail view, Figure 2). As we can see, the weak stu-
dents? reviews (Group A) are more about the logic
of statements and the usage of facts (indicated by the
unique words ?examples? and ?details?); the strong
students? peer reviews (group B) focus more on ar-
gumentation (noted by ?counter? and ?supporting?).
To further compare the two groups regarding dif-
ferent review sentiment, we look at the lists corre-
sponding to ?problem? and ?praise? (left and right
columns). For instance, we can see that strong stu-
dents? suffer more from context specific problems,
which is indicated by the bigger font size of the
domain-word mask. Meanwhile, to understand what
a topic word implies, say, ?logic? in group A?s topic
list on ?problem?, we can click the word to bring out
the relevant peer reviews, in which all occurrences
6Although ?praise? and ?problem? are manually annotated
in this corpus (Nelson and Schunn, 2009), Xiong et al (2010)
have shown that they can be automatically learned in a data-
driven fashion.
of ?logic? are colored in red (original reviews pane
in Figure 2).
6 Ongoing Evaluation
We are currently evaluating our work along two di-
mensions. First, we are interested in examining
the utility of RevExplore for instructors. After re-
ceiving positive feedback from several instructors
at the University of Pittsburgh, as an informal pilot
study, we deployed RevExplore for some of these
instructors during the Spring 2012 semester and let
them explore the peer reviews of their own ongo-
ing classes. Instructors did observe interesting pat-
terns using this tool after a short time of exploration
(within two or three passes from the overview to the
topic-word detail view). In addition, we are con-
ducting a formal user study of 40 subjects to validate
the topic-word extraction component for comparing
reviews in groups. Our preliminary result shows that
our use of topic signatures is significantly better than
a frequency-based baseline.
7 Summary and Future work
RevExplore demonstrates the usage of data visual-
ization in combination with NLP techniques to help
instructors interactively make sense of peer review
data, which was almost impracticable before. In the
future we plan to further analyze the data collected
in our formal user study, to validate the helpful-
ness of our proposed topic-word approach for mak-
ing sense of large quantities of peer reviews. We
also plan to incorporate NLP information beyond the
word and NP level, to support additional types of re-
view comparisons. In addition, we plan to summa-
rize the interview data that we informally collected
from several instructors, and will mine the log files
of their interactions with RevExplore to understand
how the tool would (and should) be used by instruc-
tors in general. Last but not least, we will continue
revising our design of RevExplore based on instruc-
tor feedback, and plan to conduct a more formal
evaluation with instructors.
Acknowledgments
Thanks to Melissa Patchan for providing the history
peer-review corpus. We are also grateful to LRDC
for financial support.
178
References
Stuart K. Card, Jock D. Mackinlay, and Ben Shneider-
man. 1999. Readings in information visualization:
using vision to think. San Francisco, CA, USA.
Kwangsu Cho and Christian D. Schunn. 2007. Scaf-
folded writing and rewriting in the discipline: A web-
based reciprocal peer review system. Computers and
Education, 48(3):409?426.
Kwangsu Cho. 2008. Machine classification of peer
comments in physics. In Proceedings First Interna-
tional Conference on Educational Data Mining, pages
192?196.
Raquel M Crespo Garcia and Abelardo Pardo. 2010. A
supporting system for adaptive peer review based on
learners? profiles. In Proceedings of Computer Sup-
ported Peer Review in Education Workshop, pages 22?
31.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006.
Hongxia Fu and Mark Hawkes. 2010. Technology-
supported peer assessment as a means for teacher
learning. In Proceedings of the 2010 Workshop on
Computer Supported Peer Review in Education.
Ioannis Giannoukos, Ioanna Lykourentzou, Giorgos
Mpardis, Vassilis Nikolopoulos, Vassilis Loumos, and
Eleftherios Kayafas. 2010. An adaptive mechanism
for author-reviewer matching in online peer assess-
ment. In Semantics in Adaptive and Personalized Ser-
vices, pages 109?126.
Michael Heilman and Noah A. Smith. 2010. Extracting
simplified statements for factual question generation.
In Proceedings of the 3rd Workshop on Question Gen-
eration.
Ville Hyyrynen, Harri Ha?ma?la?inen, Jouni Ikonen, and
Jari Porras. 2010. Mypeerreview: an online peer-
reviewing system for programming courses. In Pro-
ceedings of the 10th Koli Calling International Con-
ference on Computing Education Research, pages 94?
99.
Chin-Yew Lin and Eduard Hovy. 2000. The automated
acquisition of topic signatures for text summarization.
In Proceedings COLING.
Melissa M. Nelson and Christian D. Schunn. 2009. The
nature of feedback: how different types of peer feed-
back affect writing performance. Instructional Sci-
ence, 37:375?401.
Ani Nenkova and Annie Louis. 2008. Can you summa-
rize this? Identifying correlates of input difficulty for
generic multi-document summarization. In Proceed-
ings of Association for Computational Linguistics.
?Agnes Sa?ndor and Angela Vorndran. 2009. Detecting
key sentences for automatic assistance in peer review-
ing research articles in educational sciences. In Pro-
ceedings of the 2009 Workshop on Text and Citation
Analysis for Scholarly Digital Libraries, pages 36?44,
Suntec City, Singapore, August. Association for Com-
putational Linguistics.
Keith Topping. 1998. Peer assessment between students
in colleges and universities. Review of Educational
Research, 68(3):249?276.
Keith J. Topping. 2009. Peer assessment. Theory Into
Practice, 48(1):20?27.
Wenting Xiong and Diane Litman. 2011. Automatically
predicting peer-review helpfulness. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers - Volume 2, HLT ?11, pages 502?
507.
Wenting Xiong, Diane J. Litman, and Christian D.
Schunn. 2010. Assessing reviewers performance
based on mining problem localization in peer-review
data. In Proceedings Third International Conference
on Educational Data Mining.
Yu-Fen Yang. 2011. A reciprocal peer review system to
support college students? writing. British Journal of
Educational Technology, 42(4):687?700.
179
