Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 372?381,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Triplet Lexicon Models for Statistical Machine Translation
Sas?a Hasan, Juri Ganitkevitch, Hermann Ney, Jesu?s Andre?s-Ferrer??
Human Language Technology and Pattern Recognition, RWTH Aachen University, Germany
?Universidad Polite?cnica de Valencia, Dept. Sist. Informa?ticos y Computacio?n
{hasan,ganitkevitch,ney}@cs.rwth-aachen.de jandres@dsic.upv.es
Abstract
This paper describes a lexical trigger model
for statistical machine translation. We present
various methods using triplets incorporating
long-distance dependencies that can go be-
yond the local context of phrases or n-gram
based language models. We evaluate the pre-
sented methods on two translation tasks in a
reranking framework and compare it to the re-
lated IBM model 1. We show slightly im-
proved translation quality in terms of BLEU
and TER and address various constraints to
speed up the training based on Expectation-
Maximization and to lower the overall num-
ber of triplets without loss in translation per-
formance.
1 Introduction
Data-driven methods have been applied very suc-
cessfully within the machine translation domain
since the early 90s. Starting from single-word-
based translation approaches, significant improve-
ments have been made through advances in mod-
eling, availability of larger corpora and more pow-
erful computers. Thus, substantial progress made
in the past enables today?s MT systems to achieve
acceptable results in terms of translation quality for
specific language pairs such as Arabic-English. If
sufficient amounts of parallel data are available, sta-
tistical MT systems can be trained on millions of
?The work was carried out while the author was at the Hu-
man Language Technology and Pattern Recognition group at
RWTH Aachen University and partly supported by the Valen-
cian Conselleria d?Empresa, Universitat i Cie`ncia under grants
CTBPRA/2005/ and BEFPI/2007/014.
target
source
e e?
f
Figure 1: Triplet example: a source word f is triggered
by two target words e and e?, where one of the words is
within and the other outside the considered phrase pair
(indicated by the dashed line).
sentence pairs and use an extended level of context
based on bilingual groups of words which denote
the building blocks of state-of-the-art phrase-based
SMT systems.
Due to data sparseness, statistical models are of-
ten trained on local context only. Language mod-
els are derived from n-grams with n ? 5 and bilin-
gual phrase pairs are extracted with lengths up to
10 words on the target side. This captures the local
dependencies of the data in detail and is responsi-
ble for the success of data-driven phrase-based ap-
proaches.
In this work, we will introduce a new statistical
model based on lexicalized triplets (f, e, e?) which
we will also refer to as cross-lingual triggers of
the form (e, e? ? f). This can be understood
as two words in one language triggering one word
in another language. These triplets, modeled by
p(f |e, e?), are closely related to lexical translation
probabilities based on the IBM model 1, i.e. p(f |e).
Several constraints and setups will be described later
on in more detail, but as an introduction one can
372
think of the following interpretation which is de-
picted in Figure 1: Using a phrase-based MT ap-
proach, a source word f is triggered by its trans-
lation e which is part of the phrase being consid-
ered, whereas another target word e? outside this
phrase serves as an additional trigger in order to al-
low for more fine-grained distinction of a specific
word sense. Thus, this cross-lingual trigger model
can be seen as a combination of a lexicon model (i.e.
f and e) and a model similar to monolingual long-
range (i.e. distant bigram) trigger models (i.e. e and
e?, although these dependencies are reflected indi-
rectly via e? ? f ) which uses both local (in-phrase)
and global (in-sentence) information for the scoring.
The motivation behind this approach is to get non-
local information outside the current context (i.e. the
currently considered bilingual phrase pair) into the
translation process. The triplets are trained via the
EM algorithm, as will be shown later in more detail.
2 Related Work
In the past, a significant number of methods has
been presented that try to capture long-distance de-
pendencies, i.e. use dependencies in the data that
reach beyond the local context of n-grams or phrase
pairs. In language modeling, monolingual trigger
approaches have been presented (Rosenfeld, 1996;
Tillmann and Ney, 1997) as well as syntactical meth-
ods that parse the input and model long-range de-
pendencies on the syntactic level by conditioning on
the predecessing words and their corresponding par-
ent nodes (Chelba and Jelinek, 2000; Roark, 2001).
The latter approach was shown to reduce perplex-
ities and improve the WER in speech recognition
systems. One drawback is that the parsing process
might slow down the system significantly and the
approach is complicated to be integrated directly in
the search process. Thus, the effect is often shown
offline in reranking experiments using n-best lists.
One of the simplest models that can be seen in
the context of lexical triggers is the IBM model 1
(Brown et al, 1993) which captures lexical depen-
dencies between source and target words. It can be
seen as a lexicon containing correspondents of trans-
lations of source and target words in a very broad
sense since the pairs are trained on the full sentence
level. The model presented in this work is very close
to the initial IBM model 1 and can be seen as taking
another word into the conditioning part, i.e. the trig-
gering items.1 Furthermore, since the second trig-
ger can come from any part of the sentence, we also
have a link to long-range monolingual triggers as
presented above.
A long-range trigram model is presented in
(Della Pietra et al, 1994) where it is shown how to
derive a probabilistic link grammar in order to cap-
ture long-range dependencies in English using the
EM algorithm. Expectation-Maximization is used
in the presented triplet model as well which is de-
scribed in more detail in Section 3. Instead of deriv-
ing a grammar automatically (based on POS tags of
the words), we rely on a fully lexicalized approach,
i.e. the training is taking place at the word level.
Related work in the context of fine-tuning lan-
guage models by using cross-lingual lexical triggers
is presented in (Kim and Khudanpur, 2003). The
authors show how to use cross-lingual triggers on a
document level in order to extract translation lexi-
cons and domain-specific language models using a
mutual information criterion.
Recently, word-sense disambiguation (WSD)
methods have been shown to improve translation
quality (Chan et al, 2007; Carpuat and Wu, 2007).
Chan et al (2007) use an SVM based classifier for
disambiguating word senses which are directly in-
corporated in the decoder through additional fea-
tures that are part of the log-linear combination of
models. They use local collocations based on sur-
rounding words left and right of an ambiguous word
including the corresponding parts-of-speech. Al-
though no long-range dependencies are modeled, the
approach yields an improvement of +0.6% BLEU on
the NIST Chinese-English task. In Carpuat and Wu
(2007), another state-of-the-art WSD engine (a com-
bination of naive Bayes, maximum entropy, boost-
ing and Kernel PCA models) is used to dynamically
determine the score of a phrase pair under consid-
eration and, thus, let the phrase selection adapt to
the context of the sentence. Although the baseline is
significantly lower than in the work of Chan et al,
this setup reaches an improvement of 0.5% BLEU
on the NIST CE task and up to 1.1% BLEU on the
1Thus, instead of p(f |e) we model p(f |e, e?) with different
additional constraints as explained later on.
373
IWSLT?06 test sets.
The work in this paper tries to complement the
WSD approaches by using long-range dependen-
cies. If triggers from a local context determine dif-
ferent lexical choice for the word being triggered,
the setting is comparable to the mentioned WSD
approaches (although local dependencies might al-
ready be reflected sufficiently in the phrase models).
A distant second trigger, however, might have a ben-
eficial effect for specific languages, e.g. by captur-
ing word splits (as it is the case in German for verbs
with separable prefixes) or, as already mentioned, al-
lowing for a more fine-grained lexical choice of the
word being triggered, namely based on another word
which is not part of the current local, i.e. phrasal,
context.
The basic idea of triplets of the form (e, f ? ? f),
called multi-word extensions, is also mentioned in
(Tillmann, 2001) but neither evaluated nor investi-
gated in further detail.
In the following sections, we will describe the
model proposed in this work. In Section 3, a de-
tailed introduction is given, as well as the EM train-
ing and variations of the model. The different set-
tings will be evaluated in Section 4, where we show
experiments on the IWSLT Chinese-English and
TC-STAR EPPS English-Spanish/Spanish-English
tracks. A discussion of the results and further ex-
amples are given in Section 5. Final remarks and
future work are addressed in Section 6.
3 Model
As an extension to commonly used lexical word
pair probabilities p(f |e) as introduced in (Brown
et al, 1993), we define our model to operate on
word triplets. A triplet (f, e, e?) is assigned a value
?(f |e, e?) ? 0 with the constraint such that
?e, e? :
?
f
?(f |e, e?) = 1.
Throughout this paper, e and e? will be referred to as
the first and the second trigger, respectively. In view
of its triggers f will be termed the effect.
For a given bilingual sentence pair (fJ1 , e
I
1), the
probability of a source word fj given the whole tar-
get sentence eI1 for the triplet model is defined as:
pall (fj |e
I
1) =
1
Z
I?
i=1
I?
k=i+1
?(fj |ei, ek), (1)
where Z denotes a normalization factor based on the
corresponding target sentence length, i.e.
Z =
I(I ? 1)
2
. (2)
The introduction of a second trigger (i.e. ek in
Eq. 1) enables the model to combine local (i.e. word
or phrase level) and global (i.e. sentence level) infor-
mation.
In the following, we will describe the training pro-
cedure of the model via maximum likelihood esti-
mation for the unconstrained case.
3.1 Training
The goal of the training procedure is to maximize the
log-likelihood Fall of the triplet model for a given
bilingual training corpus {(fJ1 , e
I
1)}
N
1 consisting of
N sentence pairs:
Fall :=
N?
n=1
Jn?
j=1
log pall (fj |e
In
1 ),
where Jn and In are the lengths of the nth source
and target sentences, respectively. As there is no
closed form solution for the maximum likelihood es-
timate, we resort to iterative training via the EM al-
gorithm (Dempster et al, 1977). We define the aux-
iliary function Q(?; ??) based on Fall where ?? is the
new estimate within an iteration which is to be de-
rived from the current estimate ?. Here, ? stands for
the entire set of model parameters to be estimated,
i.e. the set of all {?(f |e, e?)}. Thus, we obtain
Q
(
{?(f |e, e?)}; {??(f |e, e?)}
)
=
N?
n=1
Jn?
j=1
In?
i=1
In?
k=i+1
[
Z?1n ?(fj |ei, ek)
pall (fj |e
In
1 )
? (3)
log
(
Z?1n ??(fj |ei, ek)
)
]
,
where Zn is defined as in Eq. 2. Using the
method of Lagrangian multipliers for the normaliza-
tion constraint, we take the derivative with respect to
374
??(f |e, e?) and obtain:
??(f |e, e?) =
A(f, e, e?)
?
f ? A(f
?, e, e?)
(4)
where A(f, e, e?) is a relative weight accumulator
over the parallel corpus:
A(f, e, e?) =
N?
n=1
Jn?
j=1
?(f, fj)
Z?1n ?(f |e, e
?)
pall (fj |e
In
1 )
Cn(e, e
?) (5)
and
Cn(e, e
?) =
In?
i=1
In?
k=i+1
?(e, ei)?(e
?, ek).
The function ?(?, ?) denotes the Kronecker delta.
The resulting training procedure is analogous to the
one presented in (Brown et al, 1993) and (Tillmann
and Ney, 1997).
The next section presents variants of the ba-
sic unconstrained model by putting restrictions on
the valid regions of triggers (in-phrase vs. out-of-
phrase) and using alignments obtained from either
GIZA++ training or forced alignments in order to
reduce the model size and to incorporate knowledge
already obtained in previous training steps.
3.2 Model variations
Based on the unconstrained triplet model presented
in Section 3, we introduce additional constraints,
namely the phrase-bounded and the path-aligned
triplet model in the following. The former reduces
the number of possible triplets by posing constraints
on the position of where valid triggers may originate
from. In order to obtain phrase boundaries on the
training data, we use forced alignments, i.e. translate
the whole training data by constraining the transla-
tion hypotheses to the target sentences of the training
corpus.
Path-aligned triplets use an alignment constraint
from the word alignments that are trained with
GIZA++. Here, we restrict the first trigger pair (f, e)
to the alignment path as based on the alignment ma-
trix produced by IBM model 4.
These variants require information in addition to
the bilingual sentence pair (fJ1 , e
I
1), namely a corre-
sponding phrase segmentation ? = {piij} with
piij =
{
1 ? a phrase pair that covers ei and fj
0 otherwise
for the phrase-bounded method and, similarly, a
word alignment A = {aij} where
aij =
{
1 if ei is aligned to fj
0 otherwise
.
3.2.1 Phrase-bounded triplets
The phrase-bounded triplet model (referred to as
pphr in the following), restricts the first trigger e to
the same phrase as f , whereas the second trigger e?
is set outside the phrase, resulting in
pphr (fj |e
I
1,?) =
1
Zj
I?
i=1
I?
k=1
piij(1 ? pikj)?(fj |ei, ek). (6)
3.2.2 Path-aligned triplet
The path-aligned triplet model (denoted by palign
in the following), restricts the scope of e to words
aligned to f by A, yielding:
palign(fj |e
I
1, A) =
1
Zj
I?
i=1
I?
k=1
aij?(fj |ei, ek) (7)
where the Zj are, again, the appropriate normaliza-
tion terms.
Also, to account for non-aligned words (analo-
gously to the IBM models), the empty word e0 is
considered in all three model variations. We show
the effect of the empty word in the experiments (Sec-
tion 4). Furthermore, we can train the presented
models in the inverse direction, i.e. p(e|f, f ?), and
combine the two directions in the rescoring frame-
work. The next section presents a set of experiments
that evaluate the performance of the presented triplet
model and its variations.
4 Experiments
In this section, we describe the system setup used in
this work, including the translation tasks and the cor-
responding training corpora. The experiments are
based on an n-best list reranking framework.
375
4.1 System
The experiments were carried out using a state-of-
the-art phrase-based SMT system. The dynamic
programming beam search decoder uses several
models during decoding by combining them log-
linearly. We incorporate phrase translation and word
lexicon models in both directions, a language model,
as well as phrase and word penalties including a
distortion model for the reordering. While gener-
ating the hypotheses, a word graph is created which
compactly represents the most likely translation hy-
potheses. Out of this word graph, we generate n-
best lists and use them to test the different setups as
described in Section 3.
In the experiments, we use 10,000-best lists con-
taining unique translation hypotheses, i.e. duplicates
generated due to different phrase segmentations are
reduced to one single entry. The advantage of this
reranking approach is that we can directly test the
obtained models since we already have fully gener-
ated translations. Thus, we can apply the triplet lex-
icon model based on p(f |e, e?) and its inverse coun-
terpart p(e|f, f ?) directly. During decoding, since e?
could be from anywhere outside the current phrase,
i.e. even from a part which lies beyond the current
context which has not yet been generated, we would
have to apply additional constraints during training
(i.e. make further restrictions such as i? < i for a
trigger pair (ei, ei?)).
Optimization of the model scaling factors is car-
ried out using minimum error rate training (MERT)
on the development sets. The optimization criterion
is 100-BLEU since we want to maximize the BLEU
score.
4.2 Tasks
4.2.1 IWSLT
For the first part of the experiments, we use
the corpora that were released for the IWSLT?07
evaluation campaign. The training corpus con-
sists of approximately 43K Chinese-English sen-
tence pairs, mainly coming from the BTEC cor-
pus (Basic Travel Expression Corpus). This is a
multilingual speech corpus which contains tourism-
related material, such as transcribed conversations
about making reservations, asking for directions or
conversations as taking place in restaurants. For the
experiments, we use the clean data track, i.e. tran-
scriptions of read speech. As the development set
which is used for tuning the parameters of the base-
line system and the reranking framework, we use
the IWSLT?04 evaluation set (500 sentence pairs).
The two blind test sets which are used to evaluate
the final performance of the models are the official
evaluation sets from IWSLT?05 (506 sentences) and
IWSLT?07 (489 sentences).
The average sentence length of the training cor-
pus is 10 words. Thus, the task is somewhat lim-
ited and very domain-specific. One of the advan-
tages of this setting is that preliminary experiments
can be carried out quickly in order to analyze the ef-
fects of the different models in detail. This and the
small vocabulary size (12K entries) makes the cor-
pus ideal for first ?rapid application development?-
style setups without having to care about possible
constraints due to memory requirements or CPU
time restrictions.
4.2.2 EPPS
Furthermore, additional experiments are based on
the EPPS corpus (European Parliament Plenary Ses-
sions) as used within the FTE (Final Text Edition)
track of the TC-STAR evaluations. The corpus con-
tains speeches held by politicians at plenary sessions
of the European Parliament that have been tran-
scribed, ?corrected? to make up valid written texts
and translated into several target languages. The lan-
guage pairs considered in the experiments here are
Spanish-English and English-Spanish.
The training corpus consists of roughly 1.3M sen-
tence pairs with 35.5M running words on the En-
glish side. The vocabulary sizes are considerably
larger than for the IWSLT task, namely around 170K
on the target side. As development set, we use
the development data issued for the 2006 evaluation
(1122 sentences), whereas the two blind test sets are
the official evaluation data from 2006 (TC-Star?06,
1117 sentences) and 2007 (TC-Star?07, 1130 sen-
tences).
4.3 Results
4.3.1 IWSLT experiments
One of the first questions that arises is how many
EM iterations should be carried out during training
of the triplet model. Since the IWSLT task is small,
376
 56.6
 56.8
 57
 57.2
 57.4
 57.6
 0  10  20  30  40  50 34.6
 34.8
 35
 35.2
 35.4
 35.6
BL
EU
 sc
ore
TE
R s
cor
e
EM iterations
IWSLT?04 BLEUIWSLT?04 TER
Figure 2: Effect of EM iterations on IWSLT?04, left axis
shows BLEU (higher numbers better), right axis (dashed
graph) shows TER score (lower numbers better).
IWSLT?04 IWSLT?05
BLEU TER BLEU TER
baseline 56.7 35.49 61.1 30.59
pall(e|f, f ?) 57.1 35.03 61.3 30.55
w/ singletons 57.3 35.04 61.3 30.61
w/ empties 57.3 35.00 61.2 30.65
+ pall(f |e, e?) 57.5 34.69 61.7 30.24
Table 1: Different setups showing the effect of singletons
and empty words for IWSLT CE IWSLT?04 (dev) and
IWSLT?05 (test) sets, pall triplets, 20 EM iterations.
we can quickly run the experiments on a full uncon-
strained triplet model without any cutoff or further
constraints. Figure 2 shows the rescoring perfor-
mance for different numbers of EM iterations. The
first 10 iterations significantly improve the triplet
model performance for the IWSLT task. After that,
there are no big changes. The performance even de-
grades a little bit after 30 iterations. For the IWSLT
task, we therefore set a fixed number of 20 EM iter-
ations for the following experiments since it shows a
good performance in terms of both BLEU and TER
score. The oracle TER scores of the 10k-best lists
are 14.18% for IWSLT?04, 11.36% for IWSLT?05
and 18.85% for IWSLT?07, respectively.
The next chain of experiments on the IWSLT task
investigates the impact of changes to the setup of
training an unconstrained triplet model, such as the
addition of the empty word and the inclusion of sin-
gletons (i.e. triplets that were only seen once in the
IWSLT?05 IWSLT?07
BLEU TER BLEU TER
baseline 61.1 30.59 38.9 45.60
IBM model 1 61.5 30.29 39.4 45.31
trip fe+ef pall 61.7 30.24 39.7 45.24
trip fe+ef pphr 61.5 30.32 39.1 45.36
trip fe+ef palign 61.2 30.60 39.7 45.02
Table 2: Comparison of triplet variants on IWSLT CE test
sets, 20 EM iterations, with singletons and empty words.
training data). This might show the importance of
rare events in order to derive strategies when mov-
ing to larger tasks where it is not feasible to train all
possible triplets, such as e.g. on the EPPS task (as
shown later) or the Chinese-English NIST task. The
results for the unconstrained model are shown in Ta-
ble 1, beginning with a full triplet model in reverse
direction, pall (e|f, f ?), that contains no singletons
and no empty words for the triggering side. In this
setting, singletons seem to help on dev but there is no
clear improvement on one of the test sets, whereas
empty words do not make a significant difference but
can be used since they do not harm either. The base-
line can be improved by +0.6% BLEU and around
-0.5% in TER on the IWSLT?04 set. For the vari-
ous setups, there are no big differences in the TER
score which might be an effect of optimization on
BLEU. Therefore, for further experiments using the
constraints from Section 3.2, we use both singletons
and empty words as the default.
Adding the other direction p(f |e, e?) results in an-
other increase, with a total of +0.8% BLEU and
-0.8% TER, which shows that the combination of
both directions helps overall translation quality. The
results on the two test sets are shown in Table 2.
As can be seen, we arrive at similar improvements,
namely +0.6% BLEU and -0.3% TER on IWSLT?05
and +0.8% BLEU and -0.4% TER on IWSLT?07, re-
spectively. The constrained models, i.e. the phrase-
bounded (pphr ) and path-aligned (palign ) triplets are
outperformed by the full unconstrained case, al-
though on IWSLT?07 both unconstrained and path-
aligned models are close.
For a fair comparison, we added a classical IBM
model 1 in the rescoring framework. It can be seen
that the presented triplet models slightly outperform
377
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 52.3 34.57 50.4 36.46
trip fe+ef pall 52.9 34.32 50.6 36.34
+ max dist 10 52.9 34.20 50.8 36.22
Table 3: Effect of using maximum distance constraint for
pall on EPPS Spanish-English test sets, occ3, 4 EM iter-
ations due to time constraints.
the simple IBM model 1. Note that IBM model 1
is a special case of the triplet lexicon model if the
second trigger is the empty word.
4.3.2 EPPS experiments
Since EPPS is a considerably harder task (larger
vocabulary and longer sentences), the training of a
full unconstrained triplet model cannot be done due
to memory restrictions. One possibility to reduce
the number of extracted triplets is to apply a max-
imum distance constraint in the training procedure,
i.e. only trigger pairs are considered where the dis-
tance between first and second trigger is below or
equal to the specified maximum.
Table 3 shows the effect of a maximum distance
constraint for the Spanish-English direction. Due
to the large amount of triplets (we extract roughly
two billion triplets2 for the EPPS data), we drop all
triplets that occur less than 3 times which results in
640 million triplets. Also, due to time restrictions3,
we only train 4 iterations and compare it to 4 itera-
tions of the same setting with the maximum distance
set to 10. The training with the maximum distance
constraints ends with a total of 380 million triplets.
As can be seen (Table 3), the performance is compa-
rable while cutting down the computation time from
9.2 to 3.1 hours. The experiments were carried out
on a 2.2GHz Opteron machine with 16 GB of mem-
ory. The overall gain is +0.4?0.6% BLEU and up to
-0.4% in TER. We even observe a slight increase in
BLEU for the TC-Star?07 set which might be a ran-
dom effect due to optimization on the development
set where the behavior is the same as for TC-Star?06.
2Extraction can be easily done in parallel by splitting the
corpus and merging identical triplets iteratively in a separate
step for two chunks at a time.
3One iteration needs more than 12 hours for the uncon-
strained case.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
trip fe+ef pphr 50.2 37.01 51.5 35.38
+ occ2 50.2 37.06 51.8 35.32
Table 4: Results on EPPS, English-Spanish, pphr com-
bined, occ3, 10 EM iterations.
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
using FA 50.0 37.18 51.7 35.52
using IBM4 50.0 37.12 51.7 35.43
+ occ2 50.2 36.84 52.0 35.10
+ max dist 1 50.0 37.10 51.7 35.51
Table 5: Results on EPPS, English-Spanish, maximum
approximation, palign combined, occ3, 10 EM iterations.
Results on EPPS English-Spanish for the phrase-
bounded triplet model are presented in Table 4.
Since the number of triplets is less than for the un-
constrained model, we can lower the cutoff from 3
to 2 (denoted in the table by occ3 and occ2 , respec-
tively). There is a small additional gain on the TC-
Star?07 test set by this step, with a total of +0.7%
BLEU for TC-Star?06 and +0.8% BLEU for TC-
Star?07.
Table 5 shows results for a variation of the path-
aligned triplet model palign that restricts the first trig-
ger to the best aligned word as estimated in the IBM
model 1, thus using a maximum-approximation of
the given word alignment. The model was trained
on two word alignments, firstly the one contained in
the forced alignments on the training data, and sec-
ondly on an IBM-4 word alignment generated using
GIZA++. For this second model we also demon-
strate the improvement obtained when increasing the
triplet lexicon size by using less trimming.
Another experiment was carried out to investigate
the effect of immediate neighboring words used as
triggers within the palign setting. This is equivalent
to using a ?maximum distance of 1? constraint. We
obtained worse results, namely a 0.2-0.3% drop in
BLEU and a 0.3-0.4% raise in TER (cf. Table 5,
last row), although the training is significantly faster
with this setup, namely roughly 30 minutes per it-
378
TC-Star?06 TC-Star?07
BLEU TER BLEU TER
baseline 49.5 37.65 51.0 36.03
IBM model 1 50.0 37.12 51.8 35.51
pall , occ3 50.0 37.17 51.8 35.43
pphr , occ2 50.2 37.06 51.8 35.32
palign , occ2 50.2 36.84 52.0 35.10
Table 6: Final results on EPPS English-Spanish, con-
strained triplet models, 10 EM iterations, compared to
standard IBM model 1.
eration using less than 2 GB of memory. However,
this shows that triggers outside the immediate con-
text help overall translation quality. Additionally, it
supports the claim that the presented methods are a
complementary alternative to the WSD approaches
mentioned in Section 2 which only consider the im-
mediate context of a single word.
Finally, we compare the constrained models to an
unconstrained setting and, again, to a standard IBM
model 1. Table 6 shows that the palign model con-
strained on using the IBM-4 word alignments yields
+0.7% in BLEU on TC-Star?06 which is +0.2%
more than with a standard IBM model 1. TER de-
creases by -0.3% when compared to model 1. For
the TC-Star?07 set, the observations are similar.
The oracle TER scores of the development n-best
list are 25.16% for English-Spanish and 27.0% for
Spanish-English, respectively.
5 Discussion
From the results of our reranking experiments, we
can conclude that the presented triplet lexicon model
outperforms the baseline single-best hypotheses of
the decoder. When comparing to a standard IBM
model 1, the improvements are significantly smaller
though measurable. So far, since IBM model 1
is considered one of the stronger rescoring mod-
els, these results look promising. An unconstrained
triplet model has the best performance if training is
feasible since it also needs the most memory and
time to be trained, at least for larger tasks.
In order to cut down computational requirements,
we can apply phrase-bounded and path-aligned
training constraints that restrict the possibilities of
selecting triplet candidates (in addition to simple
f e e? ?(f |e, e?)
pagar taxpayer bill 0.76
factura taxpayer bill 0.11
contribuyente taxpayer bill 0.10
f e ? pibm1 (f |e)
contribuyente taxpayer 0.40
contribuyentes taxpayer 0.18
europeo taxpayer 0.08
factura bill 0.19
ley bill 0.18
proyecto bill 0.11
Table 7: Example of triplets and related IBM model 1
lexical probabilities. The triggers ?taxpayer? and ?bill?
have a new effect (?pagar?), previously not seen in the
top ranks of the lexicon.
thresholding). Although no clear effect could be
observed for adding empty words on the trigger-
ing side, it does not harm and, thus, we get a sim-
ilar functionality to IBM model 1 being ?integrated?
in the triplet lexicon model. The phrase-bounded
training variant uses forced alignments computed
on the whole training data (i.e. search constrained
to producing the target sentences of the bilingual
corpus) but could not outperform the path-aligned
model which reuses the alignment path information
obtained in regular GIZA++ training.
Additionally, we observe a positive impact from
triggers lying outside the immediate context of one
predecessor or successor word.
5.1 Examples
Table 7 shows an excerpt of the top entries for
(e, e?) = (taxpayer , bill) and compares it to the top
entries of a lexicon based on IBM model 1. We ob-
serve a triggering effect since the Spanish word pa-
gar (to pay) is triggered at top position by the two
English words taxpayer and bill. The average dis-
tance of taxpayer and bill is 5.4 words. The models
presented in this work try to capture this property
and apply it in the scoring of hypotheses in order to
allow for better lexical choice in specific contexts.
In Table 8, we show an example translation where
rescoring with the triplet model achieves higher n-
gram coverage on the reference translation than the
variant based on IBM model 1 rescoring. The differ-
ing phrases are highlighted.
379
Source sen-
tence
. . . respecto de la Posicio?n Comu?n
del Consejo con vistas a la adopcio?n
del Reglamento del Parlamento Eu-
ropeo y del Consejo relativo al . . .
IBM-1
rescoring
. . . on the Council common position
with a view to the adoption of the
Rules of Procedure of the European
Parliament and of the Council . . .
Triplet
rescoring
. . . on the common position of the
Council with a view to the adop-
tion of the regulation of the Euro-
pean Parliament and of the Council
. . .
Reference
translation
. . . as regards the Common Position
of the Council with a view to the
adoption of a European Parliament
and Council Regulation as regards
the . . .
Table 8: A translation example on TC-Star?07 Spanish-
English comparing the effect of the triplet model to a
standard IBM-1 model.
6 Outlook
We have presented a new lexicon model based on
triplets extracted on a sentence level and trained it-
eratively using the EM algorithm. The motivation of
this approach is to add an additional second trigger
to a translation lexicon component which can come
from a more global context (on a sentence level) and
allow for a more fine-grained lexical choice given a
specific context. Thus, the method is related to word
sense disambiguation approaches.
We showed improvements by rescoring n-best
lists of the IWSLT Chinese-English and EPPS
Spanish-English/English-Spanish task. In total, we
achieve up to +1% BLEU for some of the test sets in
comparison to the decoder baseline and up to +0.3%
BLEU compared to IBM model 1.
Future work will address an integration into the
decoder since the performance of the current rescor-
ing framework is limited by the quality of the n-
best lists. For the inverse model, p(e|f, f ?), an in-
tegration into the search is directly possible. Further
experiments will be conducted, especially on large
tasks such as the NIST Chinese-English and Arabic-
English task. Training on these huge databases will
only be possible with an appropriate selection of
promising triplets.
Acknowledgments
This material is partly based upon work supported
by the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023,
and was partly realized as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
The authors would like to thank the anonymous
reviewers for their valuable comments.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?311,
June.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL 2007),
Prague, Czech Republic, June.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 33?40, Prague, Czech Republic, June. As-
sociation for Computational Linguistics.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stephen A. Della Pietra, Vincent J. Della Pietra, John R.
Gillett, John D. Lafferty, Harry Printz, and Lubos?
Ures?. 1994. Inference and estimation of a long-range
trigram model. In J. Oncina and R. C. Carrasco, ed-
itors, Grammatical Inference and Applications, Sec-
ond International Colloquium, ICGI-94, volume 862,
pages 78?92, Alicante, Spain. Springer Verlag.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Se-
ries B, 39(1):1?22.
Woosung Kim and Sanjeev Khudanpur. 2003. Cross-
lingual lexical triggers in statistical language model-
ing. In Proceedings of the 2003 Conference on Empir-
ical Methods in Natural Language Processing, pages
17?24, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Brian Roark. 2001. Probabilistic top-down parsing
and language modeling. Computational Linguistics,
27(2):249?276.
380
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech and Language, 10(3):187?228.
Christoph Tillmann and Hermann Ney. 1997. Word trig-
gers and the EM algorithm. In Proc. Special Interest
Group Workshop on Computational Natural Language
Learning (ACL), pages 117?124, Madrid, Spain, July.
Christoph Tillmann. 2001. Word Re-Ordering and Dy-
namic Programming based Search Algorithm for Sta-
tistical Machine Translation. Ph.D. thesis, RWTH
Aachen University, Aachen, Germany, May.
381
Proceedings of the ACL-IJCNLP 2009 Software Demonstrations, pages 25?28,
Suntec, Singapore, 3 August 2009.
c?2009 ACL and AFNLP
Demonstration of Joshua: An Open Source Toolkit
for Parsing-based Machine Translation
?
Zhifei Li, Chris Callison-Burch, Chris Dyer
?
, Juri Ganitkevitch
+
, Sanjeev Khudanpur,
Lane Schwartz
?
, Wren N. G. Thornton, Jonathan Weese, and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University
? Computational Linguistics and Information Processing Lab, University of Maryland
+ Human Language Technology and Pattern Recognition Group, RWTH Aachen University
? Natural Language Processing Lab, University of Minnesota
Abstract
We describe Joshua (Li et al, 2009a)
1
,
an open source toolkit for statistical ma-
chine translation. Joshua implements all
of the algorithms required for transla-
tion via synchronous context free gram-
mars (SCFGs): chart-parsing, n-gram lan-
guage model integration, beam- and cube-
pruning, and k-best extraction. The toolkit
also implements suffix-array grammar ex-
traction and minimum error rate training.
It uses parallel and distributed computing
techniques for scalability. We also pro-
vide a demonstration outline for illustrat-
ing the toolkit?s features to potential users,
whether they be newcomers to the field
or power users interested in extending the
toolkit.
1 Introduction
Large scale parsing-based statistical machine
translation (e.g., Chiang (2007), Quirk et al
(2005), Galley et al (2006), and Liu et al (2006))
has made remarkable progress in the last few
years. However, most of the systems mentioned
above employ tailor-made, dedicated software that
is not open source. This results in a high barrier
to entry for other researchers, and makes experi-
ments difficult to duplicate and compare. In this
paper, we describe Joshua, a Java-based general-
purpose open source toolkit for parsing-based ma-
chine translation, serving the same role as Moses
(Koehn et al, 2007) does for regular phrase-based
machine translation.
?
This research was supported in part by the Defense Ad-
vanced Research Projects Agency?s GALE program under
Contract No. HR0011-06-2-0001 and the National Science
Foundation under grants No. 0713448 and 0840112. The
views and findings are the authors? alone.
1
Please cite Li et al (2009a) if you use Joshua in your
research, and not this demonstration description paper.
2 Joshua Toolkit
When designing our toolkit, we applied general
principles of software engineering to achieve three
major goals: Extensibility, end-to-end coherence,
and scalability.
Extensibility: Joshua?s codebase consists of
a separate Java package for each major aspect
of functionality. This way, researchers can focus
on a single package of their choosing. Fuur-
thermore, extensible components are defined by
Java interfaces to minimize unintended inter-
actions and unseen dependencies, a common hin-
drance to extensibility in large projects. Where
there is a clear point of departure for research,
a basic implementation of each interface is
provided as an abstract class to minimize
work necessary for extensions.
End-to-end Cohesion: An MT pipeline con-
sists of many diverse components, often designed
by separate groups that have different file formats
and interaction requirements. This leads to a large
number of scripts for format conversion and to
facilitate interaction between the components, re-
sulting in untenable and non-portable projects, and
hindering repeatability of experiments. Joshua, on
the other hand, integrates the critical components
of an MT pipeline seamlessly. Still, each compo-
nent can be used as a stand-alone tool that does not
rely on the rest of the toolkit.
Scalability: Joshua, especially the decoder, is
scalable to large models and data sets. For ex-
ample, the parsing and pruning algorithms are im-
plemented with dynamic programming strategies
and efficient data structures. We also utilize suffix-
array grammar extraction, parallel/distributed de-
coding, and bloom filter language models.
Joshua offers state-of-the-art quality, having
been ranked 4th out of 16 systems in the French-
English task of the 2009 WMT evaluation, both in
automatic (Table 1) and human evaluation.
25
System BLEU-4
google 31.14
lium 26.89
dcu 26.86
joshua 26.52
uka 25.96
limsi 25.51
uedin 25.44
rwth 24.89
cmu-statxfer 23.65
Table 1: BLEU scores for top primary systems on
the WMT-09 French-English Task from Callison-
Burch et al (2009), who also provide human eval-
uation results.
2.1 Joshua Toolkit Features
Here is a short description of Joshua?s main fea-
tures, described in more detail in Li et al (2009a):
? Training Corpus Sub-sampling: We sup-
port inducing a grammar from a subset
of the training data, that consists of sen-
tences needed to translate a particular test
set. To accomplish this, we make use of the
method proposed by Kishore Papineni (per-
sonal communication), outlined in further de-
tail in (Li et al, 2009a). The method achieves
a 90% reduction in training corpus size while
maintaining state-of-the-art performance.
? Suffix-array Grammar Extraction: Gram-
mars extracted from large training corpora
are often far too large to fit into available
memory. Instead, we follow Callison-Burch
et al (2005) and Lopez (2007), and use a
source language suffix array to extract only
rules that will actually be used in translating
a particular test set. Direct access to the suffix
array is incorporated into the decoder, allow-
ing rule extraction to be performed for each
input sentence individually, but it can also be
executed as a standalone pre-processing step.
? Grammar formalism: Our decoder as-
sumes a probabilistic synchronous context-
free grammar (SCFG). It handles SCFGs
of the kind extracted by Hiero (Chiang,
2007), but is easily extensible to more gen-
eral SCFGs (as in Galley et al (2006)) and
closely related formalisms like synchronous
tree substitution grammars (Eisner, 2003).
? Pruning: We incorporate beam- and cube-
pruning (Chiang, 2007) to make decoding
feasible for large SCFGs.
? k-best extraction: Given a source sentence,
the chart-parsing algorithm produces a hy-
pergraph representing an exponential num-
ber of derivation hypotheses. We implement
the extraction algorithm of Huang and Chi-
ang (2005) to extract the k most likely deriva-
tions from the hypergraph.
? Oracle Extraction: Even within the large
set of translations represented by a hyper-
graph, some desired translations (e.g. the ref-
erences) may not be contained due to pruning
or inherent modeling deficiency. We imple-
ment an efficient dynamic programming al-
gorithm (Li and Khudanpur, 2009) for find-
ing the oracle translations, which are most
similar to the desired translations, as mea-
sured by a metric such as BLEU.
? Parallel and distributed decoding: We
support parallel decoding and a distributed
language model that exploit multi-core and
multi-processor architectures and distributed
computing (Li and Khudanpur, 2008).
? Language Models: We implement three lo-
cal n-gram language models: a straightfor-
ward implementation of the n-gram scoring
function in Java, capable of reading stan-
dard ARPA backoff n-gram models; a na-
tive code bridge that allows the decoder to
use the SRILM toolkit to read and score n-
grams
2
; and finally a Bloom Filter implemen-
tation following Talbot and Osborne (2007).
? Minimum Error Rate Training: Joshua?s
MERT module optimizes parameter weights
so as to maximize performance on a develop-
ment set as measured by an automatic evalu-
ation metric, such as BLEU. The optimization
consists of a series of line-optimizations us-
ing the efficient method of Och (2003). More
details on the MERT method and the imple-
mentation can be found in Zaidan (2009).
3
2
The first implementation allows users to easily try the
Joshua toolkit without installing SRILM. However, users
should note that the basic Java LM implementation is not as
scalable as the SRILM native bridge code.
3
The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
26
? Variational Decoding: spurious ambiguity
causes the probability of an output string
among to be split among many derivations.
The goodness of a string is measured by
the total probability of its derivations, which
means that finding the best output string is
computationally intractable. The standard
Viterbi approximation is based on the most
probable derivation, but we also implement
a variational approximation, which considers
all the derivations but still allows tractable
decoding (Li et al, 2009b).
3 Demonstration Outline
The purpose of the demonstration is 4-fold: 1) to
give newcomers to the field of statistical machine
translation an idea of the state-of-the-art; 2) to
show actual, live, end-to-end operation of the sys-
tem, highlighting its main components, targeting
potential users; 3) to illustrate, through visual aids,
the underlying algorithms, for those interested in
the technical details; and 4) to explain how those
components can be extended, for potential power
users who want to be familiar with the code itself.
The first component of the demonstration will
be an interactive user interface, where arbitrary
user input in a source language is entered into a
web form and then translated into a target lan-
guage by the system. This component specifically
targets newcomers to SMT, and demonstrates the
current state of the art in the field. We will have
trained multiple systems (for multiple language
pairs), hosted on a remote server, which will be
queried with the sample source sentences.
Potential users of the system would be inter-
ested in seeing an actual operation of the system,
in a similar fashion to what they would observe
on their own machines when using the toolkit. For
this purpose, we will demonstrate three main mod-
ules of the toolkit: the rule extraction module, the
MERT module, and the decoding module. Each
module will have a separate terminal window ex-
ecuting it, hence demonstrating both the module?s
expected output as well as its speed of operation.
In addition to demonstrating the functionality
of each module, we will also provide accompa-
nying visual aids that illustrate the underlying al-
gorithms and the technical operational details. We
will provide visualization of the search graph and
(Software and documentation at: http://cs.jhu.edu/
?
ozaidan/zmert.)
the 1-best derivation, which would illustrate the
functionality of the decoder, as well as alterna-
tive translations for phrases of the source sentence,
and where they were learned in the parallel cor-
pus, illustrating the functionality of the grammar
rule extraction. For the MERT module, we will
provide figures that illustrate Och?s efficient line
search method.
4 Demonstration Requirements
The different components of the demonstration
will be spread across at most 3 machines (Fig-
ure 1): one for the live ?instant translation? user
interface, one for demonstrating the different com-
ponents of the system and algorithmic visualiza-
tions, and one designated for technical discussion
of the code. We will provide the machines our-
selves and ensure the proper software is installed
and configured. However, we are requesting that
large LCD monitors be made available, if possi-
ble, since that would allow more space to demon-
strate the different components with clarity than
our laptop displays would provide. We will also
require Internet connectivity for the live demon-
stration, in order to gain access to remote servers
where trained models will be hosted.
References
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Jason Eisner. 2003. Learning non-isomorphic tree
mappings for machine translation. In Proceedings
of ACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the ACL/Coling.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of the International Work-
shop on Parsing Technologies.
27
We will rely on 3 workstations: 
one for the instant translation 
demo, where arbitrary input is 
translated from/to a language pair 
of choice (top); one for runtime 
demonstration of the system, with 
a terminal window for each of the 
three main components of the 
systems, as well as visual aids, 
such as derivation trees (left); and 
one (not shown) designated for 
technical discussion of the code.
Remote server 
hosting trained 
translation models
JHU
Grammar extraction
Decoder
M
E
R
T
Figure 1: Proposed setup of our demonstration. When this paper is viewed as a PDF, the reader may
zoom in further to see more details.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the ACL-2007 Demo and Poster Ses-
sions.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
Proceedings Workshop on Syntax and Structure in
Statistical Translation.
Zhifei Li and Sanjeev Khudanpur. 2009. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 135?139, Athens, Greece, March. As-
sociation for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In Proceedings of ACL.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of EMNLP-
CoLing.
Franz Josef Och. 2003. Minimum error rate training
for statistical machine translation. In Proceedings
of ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of ACL.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In Proceedings of ACL.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
28
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1168?1179,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Learning Sentential Paraphrases from Bilingual Parallel Corpora
for Text-to-Text Generation
Juri Ganitkevitch, Chris Callison-Burch, Courtney Napoles, and Benjamin Van Durme
Center for Language and Speech Processing, and HLTCOE
Johns Hopkins University
Abstract
Previous work has shown that high quality
phrasal paraphrases can be extracted from
bilingual parallel corpora. However, it is not
clear whether bitexts are an appropriate re-
source for extracting more sophisticated sen-
tential paraphrases, which are more obviously
learnable from monolingual parallel corpora.
We extend bilingual paraphrase extraction to
syntactic paraphrases and demonstrate its abil-
ity to learn a variety of general paraphrastic
transformations, including passivization, da-
tive shift, and topicalization. We discuss how
our model can be adapted to many text gener-
ation tasks by augmenting its feature set, de-
velopment data, and parameter estimation rou-
tine. We illustrate this adaptation by using
our paraphrase model for the task of sentence
compression and achieve results competitive
with state-of-the-art compression systems.
1 Introduction
Paraphrases are alternative ways of expressing the
same information (Culicover, 1968). Automatically
generating and detecting paraphrases is a crucial as-
pect of many NLP tasks. In multi-document sum-
marization, paraphrase detection is used to collapse
redundancies (Barzilay et al, 1999; Barzilay, 2003).
Paraphrase generation can be used for query expan-
sion in information retrieval and question answer-
ing systems (McKeown, 1979; Anick and Tipirneni,
1999; Ravichandran and Hovy, 2002; Riezler et al,
2007). Paraphrases allow for more flexible matching
of system output against human references for tasks
like machine translation and automatic summariza-
tion (Zhou et al, 2006; Kauchak and Barzilay, 2006;
Madnani et al, 2007; Snover et al, 2010).
Broadly, we can distinguish two forms of para-
phrases: phrasal paraphrases denote a set of surface
text forms with the same meaning:
the committee?s second proposal
the second proposal of the committee
while syntactic paraphrases augment the surface
forms by introducing nonterminals (or slots) that are
annotated with syntactic constraints:
the NP1?s NP2
the NP2 of the NP1
It is evident that the latter have a much higher poten-
tial for generalization and for capturing interesting
paraphrastic transformations.
A variety of different types of corpora (and se-
mantic equivalence cues) have been used to auto-
matically induce paraphrase collections for English
(Madnani and Dorr, 2010). Perhaps the most nat-
ural type of corpus for this task is a monolingual
parallel text, which allows sentential paraphrases to
be extracted since the sentence pairs in such corpora
are perfect paraphrases of each other (Barzilay and
McKeown, 2001; Pang et al, 2003). While rich syn-
tactic paraphrases have been learned from monolin-
gual parallel corpora, they suffer from very limited
data availability and thus have poor coverage.
Other methods obtain paraphrases from raw
monolingual text by relying on distributional simi-
larity (Lin and Pantel, 2001; Bhagat and Ravichan-
dran, 2008). While vast amounts of data are
readily available for these approaches, the distri-
butional similarity signal they use is noisier than
the sentence-level correspondency in parallel cor-
pora and additionally suffers from problems such as
mistaking cousin expressions or antonyms (such as
{boy , girl} or {rise, fall}) for paraphrases.
1168
Abundantly available bilingual parallel corpora
have been shown to address both these issues, ob-
taining paraphrases via a pivoting step over foreign
language phrases (Bannard and Callison-Burch,
2005). The coverage of paraphrase lexica extracted
from bitexts has been shown to outperform that
obtained from other sources (Zhao et al, 2008a).
While there have been efforts pursuing the extrac-
tion of more powerful paraphrases (Madnani et
al., 2007; Callison-Burch, 2008; Cohn and Lapata,
2008; Zhao et al, 2008b), it is not yet clear to what
extent sentential paraphrases can be induced from
bitexts. In this paper we:
? Extend the bilingual pivoting approach to para-
phrase induction to produce rich syntactic para-
phrases.
? Perform a thorough analysis of the types of
paraphrases we obtain and discuss the para-
phrastic transformations we are capable of cap-
turing.
? Describe how training paradigms for syntac-
tic/sentential paraphrase models should be tai-
lored to different text-to-text generation tasks.
? Demonstrate our framework?s suitability for a
variety of text-to-text generation tasks by ob-
taining state-of-the-art results on the example
task of sentence compression.
2 Related Work
Madnani and Dorr (2010) survey a variety of data-
driven paraphrasing techniques, categorizing them
based on the type of data that they use. These
include large monolingual texts (Lin and Pantel,
2001; Szpektor et al, 2004; Bhagat and Ravichan-
dran, 2008), comparable corpora (Barzilay and Lee,
2003; Dolan et al, 2004), monolingual parallel cor-
pora (Barzilay and McKeown, 2001; Pang et al,
2003), and bilingual parallel corpora (Bannard and
Callison-Burch, 2005; Madnani et al, 2007; Zhao et
al., 2008b). We focus on the latter type of data.
Paraphrase extraction using bilingual parallel cor-
pora was proposed by Bannard and Callison-Burch
(2005) who induced paraphrases using techniques
from phrase-based statistical machine translation
(Koehn et al, 2003). After extracting a bilingual
phrase table, English paraphrases are obtained by
pivoting through foreign language phrases. Since
many paraphrases can be extracted for a phrase,
Bannard and Callison-Burch rank them using a para-
phrase probability defined in terms of the translation
model probabilities p(f |e) and p(e|f):
p(e2|e1) =
?
f
p(e2, f |e1) (1)
=
?
f
p(e2|f, e1)p(f |e1) (2)
?
?
f
p(e2|f)p(f |e1). (3)
Several subsequent efforts extended the bilin-
gual pivoting technique, many of which introduced
elements of more contemporary syntax-based ap-
proaches to statistical machine translation. Mad-
nani et al (2007) extended the technique to hier-
archical phrase-based machine translation (Chiang,
2005), which is formally a synchronous context-free
grammar (SCFG) and thus can be thought of as a
paraphrase grammar. The paraphrase grammar can
paraphrase (or ?decode?) input sentences using an
SCFG decoder, like the Hiero, Joshua or cdec MT
systems (Chiang, 2007; Li et al, 2009; Dyer et al,
2010). Like Hiero, Madnani?s model uses just one
nonterminal X instead of linguistic nonterminals.
Three additional efforts incorporated linguistic
syntax. Callison-Burch (2008) introduced syntac-
tic constraints by labeling all phrases and para-
phrases (even non-constituent phrases) with CCG-
inspired slash categories (Steedman and Baldridge,
2011), an approach similar to Zollmann and Venu-
gopal (2006)?s syntax-augmented machine transla-
tion (SAMT). Callison-Burch did not formally de-
fine a synchronous grammar, nor discuss decoding,
since his presentation did not include hierarchical
rules. Cohn and Lapata (2008) used the GHKM
extraction method (Galley et al, 2004), which is
limited to constituent phrases and thus produces
a reasonably small set of syntactic rules. Zhao
et al (2008b) added slots to bilingually extracted
paraphrase patterns that were labeled with part-of-
speech tags, but not larger syntactic constituents.
Before the shift to statistical natural language pro-
cessing, paraphrasing was often treated as syntactic
transformations or by parsing and then generating
1169
from a semantic representation (McKeown, 1979;
Muraki, 1982; Meteer and Shaked, 1988; Shem-
tov, 1996; Yamamoto, 2002). Indeed, some work
generated paraphrases using (non-probabilistic) syn-
chronous grammars (Shieber and Schabes, 1990;
Dras, 1997; Dras, 1999; Kozlowski et al, 2003).
After the rise of statistical machine translation, a
number of its techniques were repurposed for para-
phrasing. These include sentence alignment (Gale
and Church, 1993; Barzilay and Elhadad, 2003),
word alignment and noisy channel decoding (Brown
et al, 1990; Quirk et al, 2004), phrase-based models
(Koehn et al, 2003; Bannard and Callison-Burch,
2005), hierarchical phrase-based models (Chiang,
2005; Madnani et al, 2007), log-linear models and
minimum error rate training (Och, 2003a; Madnani
et al, 2007; Zhao et al, 2008a), and here syntax-
based machine translation (Wu, 1997; Yamada and
Knight, 2001; Melamed, 2004; Quirk et al, 2005).
Beyond cementing the ties between paraphrasing
and syntax-based statistical machine translation, the
novel contributions of our paper are (1) an in-depth
analysis of the types of structural and sentential
paraphrases that can be extracted with bilingual piv-
oting, (2) a discussion of how our English?English
paraphrase grammar should be adapted to specific
text-to-text generation tasks (Zhao et al, 2009) with
(3) a concrete example of the adaptation procedure
for the task of paraphrase-based sentence compres-
sion (Knight and Marcu, 2002; Cohn and Lapata,
2008; Cohn and Lapata, 2009).
3 SCFGs in Translation
The model we use in our paraphrasing approach is
a syntactically informed synchronous context-free
grammar (SCFG). The SCFG formalism (Aho and
Ullman, 1972) was repopularized for statistical ma-
chine translation by Chiang (2005). Formally, a
probabilistic SCFG G is defined by specifying
G = ?N , TS , TT ,R, S?,
whereN is a set of nonterminal symbols, TS and TT
are the source and target language vocabularies, R
is a set of rules and S ? N is the root symbol. The
rules inR take the form:
C ? ??, ?,?, w?,
PP/NN ? mit einer  |  with a
NP ? das leck  |  the leak
VP ?  NP PP/NN detonation zu schliessen  |  closing NP PP/NN blast 
they
VP
VP
PRP VBD NNDTNN
NP NPNP
closing tried the   
S
sie versuchten das zu schliessen
leak
leck
with a   blast
DT IN
PP
VBG
einermit detonation
Figure 1: Synchronous grammar rules for translation are
extracted from sentence pairs in a bixtext which have
been automatically parsed and word-aligned. Extraction
methods vary on whether they extract only minimal rules
for phrases dominated by nodes in the parse tree, or more
complex rules that include non-constituent phrases.
where the rule?s left-hand side C ? N is a nonter-
minal, ? ? (N?TS)? and ? ? (N?TT )? are strings
of terminal and nonterminal symbols with an equal
number of nonterminals cNT (?) = cNT (?) and
?: {1 . . . cNT (?)} ? {1 . . . cNT (?)}
constitutes a one-to-one correspondency function
between the nonterminals in ? and ?. A non-
negative weight w ? 0 is assigned to each rule, re-
flecting the likelihood of the rule.
Rule Extraction Phrase-based approaches to sta-
tistical machine translation (and their successors)
extract pairs of (e, f) phrases from automatically
word-aligned parallel sentences. Och (2003b)
described various heuristics for extracting phrase
alignments from the Viterbi word-level alignments
that are estimated using Brown et al (1993) word-
alignment models.
These phrase extraction heuristics have been ex-
tended so that they extract synchronous grammar
rules (Galley et al, 2004; Chiang, 2005; Zollmann
and Venugopal, 2006; Liu et al, 2006). Most of
these extraction methods require that one side of the
parallel corpus be parsed. This is typically done au-
tomatically with a statistical parser.
Figure 1 shows examples of rules obtained from
a sentence pair. To extract a rule, we first choose a
source side span f like das leck. Then we use phrase
extraction techniques to find target spans e that are
consistent with the word alignment (in this case the
1170
leak is consistent with our f ). The nonterminal sym-
bol that is the left-hand side of the SCFG rule is then
determined by the syntactic constituent that domi-
nates e (in this case NP). To introduce nonterminals
into the right-hand side of the rule, we can apply
rules extracted over sub-phrases of f , synchronously
substituting the corresponding nonterminal symbol
for the sub-phrases on both sides. The synchronous
substitution applied to f and e then yields the corre-
spondency ?.
One significant differentiating factor between the
competing ways of extracting SCFG rules is whether
the extraction method generates rules only for con-
stituent phrases that are dominated by a node in
the parse tree (Galley et al, 2004; Cohn and
Lapata, 2008) or whether they include arbitrary
phrases, including non-constituent phrases (Zoll-
mann and Venugopal, 2006; Callison-Burch, 2008).
We adopt the extraction for all phrases, including
non-constituents, since it allows us to cover a much
greater set of phrases, both in translation and para-
phrasing.
Feature Functions Rather than assigning a single
weight w, we define a set of feature functions ~? =
{?1...?N} that are combined in a log-linear model:
w = ?
N?
i=1
?i log?i. (4)
The weights ~? of these feature functions are set to
maximize some objective function like BLEU (Pap-
ineni et al, 2002) using a procedure called minimum
error rate training (MERT), owing to Och (2003a).
MERT iteratively adjusts the weights until the de-
coder produces output that best matches reference
translations in a development set, according to the
objective function. We will examine appropriate ob-
jective functions for text-to-text generation tasks in
Section 6.2.
Typical features used in statistical machine trans-
lation include phrase translation probabilities (cal-
culated using maximum likelihood estimation over
all phrase pairs enumerable in the parallel cor-
pus), word-for-word lexical translation probabili-
ties (which help to smooth sparser phrase transla-
tion estimates), a ?rule application penalty? (which
governs whether the system prefers fewer longer
they can not be dangerous to the rest of the village
VP/PP
VB+JJ
S
NP
NP/NN
sie k?nnengef?hrlich werdennichtdem rest des dorfes
VP/PP
VB+JJ
S
NP
NP/NN
NP/NN ? dem rest des  |   the rest of the
NP ? NP/NN dorfes  |  NP/NN village
VP/PP ? nicht VB+JJ k?nnen  |  can not VB+JJ
VB+JJ ? gef?hrlich werden  |  be dangerous
S ? sie NP VP/PP  |  they VP/PP to NP
Figure 2: An example derivation produced by a syntactic
machine translation system. Although the synchronous
trees are unlike the derivations found in the Penn Tree-
bank, their yield is a good translation of the German.
phrases or a greater number of shorter phrases), and
a language model probability.
Decoding Given an SCFG and an input source
sentence, the decoder performs a search for the sin-
gle most probable derivation via the CKY algorithm.
In principle the best translation should be the En-
glish sentence e that is the most probable after sum-
ming over all d ? D derivations, since many deriva-
tions yield the same e. In practice, we use a Viterbi
approximation and return the translation that is the
yield of the single best derivation:
e? = arg max
e?Trans(f)
?
d?D(e,f)
p(d, e|f)
? yield(arg max
d?D(e,f)
p(d, e|f)). (5)
Derivations are simply successive applications of the
SCFG rules such as those given in Figure 2.
4 SCFGs in Paraphrasing
Rule Extraction To create a paraphrase grammar
from a translation grammar, we extend the syntac-
tically informed pivot approach of Callison-Burch
(2008) to the SCFG model. For this purpose, we
assume a grammar that translates from a given for-
eign language to English. For each pair of trans-
lation rules where the left-hand side C and foreign
1171
string ? match:
C ? ??, ?1,?1, ~?1?
C ? ??, ?2,?2, ~?2?,
we create a paraphrase rule:
C ? ??1, ?2,?, ~??,
where the nonterminal correspondency relation ?
has been set to reflect the combined nonterminal
alignment:
? = ??11 ? ?2 .
Feature Functions In the computation of the fea-
tures ~? from ~?1 and ~?2 we follow the approximation
in Equation 3, which yields lexical and phrasal para-
phrase probability features. Additionally, we add a
boolean indicator for whether the rule is an iden-
tity paraphrase, ?identity . Another indicator feature,
?reorder , fires if the rule swaps the order of two non-
terminals, which enables us to promote more com-
plex paraphrases that require structural reordering.
Decoding With this, paraphrasing becomes an
English-to-English translation problem which can
be formulated similarly to Equation 5 as:
e?2 ? yield(arg max
d?D(e2,e1)
p(d, e2|e1)).
Figure 3 shows an example derivation produced as a
result of applying our paraphrase rules in the decod-
ing process. Another advantage of using the decoder
from statistical machine translation is that n-gram
language models, which have been shown to be use-
ful in natural language generation (Langkilde and
Knight, 1998), are already well integrated (Huang
and Chiang, 2007).
5 Analysis
A key motivation for the use of syntactic paraphrases
over their phrasal counterparts is their potential to
capture meaning-preserving linguistic transforma-
tions in a more general fashion. A phrasal system is
limited to memorizing fully lexicalized transforma-
tions in its paraphrase table, resulting in poor gener-
alization capabilities. By contrast, a syntactic para-
phrasing system intuitively should be able to address
this issue and learn well-formed and generic patterns
that can be easily applied to unseen data.
twelve cartoons insulting the
prophet
mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
12 the prophet mohammad
CD
NNS
JJ DT
NNP
NP
NP
VP
NP
DT+NNP
cartoons offensive
Foreign Pivot PhraseParaphrase Rule
JJ ? offensive  |   insulting
Lexical paraphrase:
NP ? NP that VP  |  NP VP
Reduced relative clause:
NP ? CD of the NNS  |  CD NNS
Partitive construction: 
VP ? are JJ to NP  |  JJ NP
Pred. adjective copula deletion:
JJ -> beleidigend  |  offensive
JJ -> beleidigend  |  insulting
NP -> NP die VP  |  NP VP
NP -> NP die VP  |  NP that VP
NP -> CD der NNS  |  CD of the NNS
NP -> CD der NNS  |  CD NNS
VP ? sind JJ f?r NP  |  are JJ to NP
VP ? sind JJ f?r NP  |  JJ NP
of the that are to
Figure 3: An example of a synchronous paraphrastic
derivation. A few of the rules applied in the parse are
show in the left column, with the pivot phrases that gave
rise to them on the right.
To put this expectation to the test, we investigate
how our grammar captures a number of well-known
paraphrastic transformations.1 Table 1 shows the
transformations along with examples of the generic
grammar rules our system learns to represent them.
When given a transformation to extract a syntactic
paraphrase for, we want to find rules that neither
under- nor over-generalize. This means that, while
replacing the maximum number of syntactic argu-
ments with nonterminals, the rules ideally will both
retain enough lexicalization to serve as sufficient ev-
idence for the applicability of the transformation and
impose constraints on the nonterminals to ensure the
arguments? well-formedness.
The paraphrases implementing the possessive rule
and the dative shift shown in Table 1 are a good
examples of this: the two noun-phrase arguments
to the expressions are abstracted to nonterminals
while each rule?s lexicalization provides an appro-
priate frame of evidence for the transform. This is
important for a good representation of dative shift,
which is a reordering transformation that fully ap-
plies to certain ditransitive verbs while other verbs
are uncommon in one of the forms:
1The data and software used to extract the grammar we draw
these examples from is described in Section 6.5.
1172
Possessive rule NP ? the NN of the NNP | the NNP ?s NNNP ? the NNS 1 made by NNS 2 | the NNS 2?s NNS 1
Dative shift VP ? give NN to NP | give NP the NNVP ? provide NP1 to NP2 | give NP2 NP1
Adv./adj. phrase move S/VP ? ADVP they VBP | they VPB ADVPS ? it is ADJP VP | VP is ADJP
Verb particle shift VP ? VB NP up | VB up NP
Reduced relative clause SBAR/S ? although PRP VBP that | although PRP VBPADJP ? very JJ that S | JJ S
Partitive constructions NP ? CD of the NN | CD NNNP ? all DT\NP | all of the DT\NP
Topicalization S ? NP , VP . | VP , NP .
Passivization SBAR? that NP had VBN | which was VBN by NP
Light verbs VP ? take action ADVP | to act ADVPVP ? TO take a decision PP | TO decide PP
Table 1: A selection of meaning-preserving transformations and hand-picked examples of syntactic paraphrases that
our system extracts capturing these.
give decontamination equipment to Japan
give Japan decontamination equipment
provide decontamination equipment to Japan
? provide Japan decontamination equipment
Note how our system extracts a dative shift rule for
to give and a rule that both shifts and substitutes a
more appropriate verb for to provide.
The use of syntactic nonterminals in our para-
phrase rules to capture complex transforms also
makes it possible to impose constraints on their ap-
plication. For comparison, as Madnani et al (2007)
do not impose any constraints on how the nontermi-
nal X can be realized, their equivalent of the topi-
calization rule would massively overgeneralize:
S ? X1, X2 . | X2, X1 .
Additional examples of transforms our use of syn-
tax allows us to capture are the adverbial phrase
shift and the reduction of a relative clause, as well
as other phenomena listed in Table 1.
Unsurprisingly, syntactic information alone is not
sufficient to capture all transformations. For in-
stance it is hard to extract generic paraphrases for all
instances of passivization, since our syntactic model
currently has no means of representing the morpho-
logical changes that the verb undergoes:
the reactor leaks radiation
radiation is leaking from the reactor .
Still, for cases where the verb?s morphology does
not change, we manage to learn a rule:
the radiation that the reactor had leaked
the radiation which leaked from the reactor .
Another example of a deficiency in our synchronous
grammar models are light verb constructs such as:
to take a walk
to walk .
Here, a noun is transformed into the corresponding
verb ? something our synchronous syntactic CFGs
are not able to capture except through memorization.
Our survey shows that we are able to extract ap-
propriately generic representations for a wide range
of paraphrastic transformations. This is a surpris-
ing result which shows that bilingual parallel cor-
pora can be used to learn sentential paraphrases, and
that they are a viable alternative to other data sources
like monolingual parallel corpora, which more obvi-
ously contain sentential paraphrases, but are scarce.
6 Text-to-Text Applications
The core of many text-to-text generation tasks is
sentential paraphrasing, augmented with specific
constraints or goals. Since our model borrows much
of its machinery from statistical machine translation
? a sentential rewriting problem itself ? it is straight-
forward to use our paraphrase grammars to generate
new sentences using SMT?s decoding and param-
eter optimization techniques. Our framework can
be adapted to many different text-to-text generation
tasks. These could include text simplification, sen-
1173
tence compression, poetry generation, query expan-
sion, transforming declarative sentences into ques-
tions, and deriving hypotheses for textual entail-
ment. Each individual text-to-text application re-
quires that our framework be adapted in several
ways, by specifying:
? A mechanism for extracting synchronous
grammar rules (in this paper we argue that
pivot-based paraphrasing is widely applicable).
? An appropriate set of rule-level features that
capture information pertinent to the task (e.g.
whether a rule simplifies a phrase).
? An appropriate ?objective function? that scores
the output of the model, i.e. a task-specific
equivalent to the BLEU metric in SMT.
? A development set with examples of the sen-
tential transformations that we are modeling.
? Optionally, a way of injecting task-specific
rules that were not extracted automatically.
In the remainder of this section, we illustrate how
our bilingually extracted paraphrases can be adapted
to perform sentence compression, which is the task
of reducing the length of sentence while preserving
its core meaning. Most previous approaches to sen-
tence compression focused only on the deletion of
a subset of words from the sentence (Knight and
Marcu, 2002). Our approach follows Cohn and La-
pata (2008), who expand the task to include substi-
tutions, insertions and reorderings that are automat-
ically learned from parallel texts.
6.1 Feature Design
In Section 4 we discussed phrasal probabilities.
While these help quantify how good a paraphrase
is in general, they do not make any statement on
task-specific things such as the change in language
complexity or text length. To make this information
available to the decoder, we enhance our paraphrases
with four compression-targeted features. We add the
count features csrc and ctgt , indicating the number of
words on either side of the rule as well as two differ-
ence features: cdcount = ctgt ? csrc and the anal-
ogously computed difference in the average word
length in characters, cdavg .
6.2 Objective Function
Given our paraphrasing system?s connection to
SMT, the naive/obvious choice for parameter op-
timization would be to optimize for BLEU over a
set of paraphrases, for instance parallel English ref-
erence translations for a machine translation task
(Madnani et al, 2007). For a candidate C and a ref-
erence R, (with lengths c and r) BLEU is defined as:
BLEUN (C,R)
=
{
e(1?c/r) ? e
?N
n=1 logwnpn if c/r ? 1
e
?N
n=1 logwnpn otherwise ,
where pn is the modified n-gram precision of C
against R, with typically N = 4 and wn = 1N . The
?brevity penalty? term e(1?c/r) is added to prevent
short candidates from achieving perfect scores.
Naively optimizing for BLEU, however, will re-
sult in a trivial paraphrasing system heavily biased
towards producing identity ?paraphrases?. This is
obviously not what we are looking for. Moreover,
BLEU does not provide a mechanism for directly
specifying a per-sentence compression rate, which
is desirable for the compression task.
Instead, we propose PRE?CIS, an objective func-
tion tailored to the text compression task:
PRE?CIS?,?(I, C,R)
=
{
e?(??c/i) ? BLEU(C,R) if c/i ? ?
BLEU(C,R) otherwise
For an input sentence I , an output C and ref-
erence compression R (with lengths i, c and r),
PRE?CIS combines the precision estimate of BLEU
with an additional ?verbosity penalty? that is ap-
plied to compressions that fail to meet a given target
compression rate ?. We rely on the BLEU brevity
penalty to prevent the system from producing overly
aggressive compressions. The scaling term ? deter-
mines how severely we penalize deviations from ?.
In our experiments we use ? = 10.
It is straightforward to find similar adaptations for
other tasks. For text simplification, for instance, the
penalty term can include a readability metric. For
poetry generation we can analogously penalize out-
puts that break the meter (Greene et al, 2010).
6.3 Development Data
To tune the parameters of our paraphrase system for
sentence compression, we need an appropriate cor-
1174
pus of reference compressions. Since our model is
designed to compress by paraphrasing rather than
deletion, the commonly used deletion-based com-
pression data sets like the Ziff-Davis corpus are not
suitable. We have thus created a corpus of com-
pression paraphrases. Beginning with 9570 tuples
of parallel English?English sentences obtained from
multiple reference translations for machine transla-
tion evaluation, we construct a parallel compression
corpus by selecting the longest reference in each tu-
ple as the source sentence and the shortest reference
as the target sentence. We further retain only those
sentence pairs where the compression rate cr falls in
the range 0.5 < cr ? 0.8. From these, we randomly
select 936 sentences for the development set, as well
as 560 sentences for a test set that we use to gauge
the performance of our system.
6.4 Grammar Augmentations
As we discussed in Section 5, the paraphrase gram-
mar we induce is capable of representing a wide va-
riety of transformations. However, the formalism
and extraction method are not explicitly geared to-
wards a compression application. For instance, the
synchronous nature of our grammar does not allow
us to perform deletions of constituents as done by
Cohn and Lapata (2007)?s tree transducers. One way
to extend the grammar?s capabilities towards the re-
quirements of a given task is by injecting additional
rules designed to capture appropriate operations.
For the compression task, this could include
adding rules to delete target-side nonterminals:
JJ ? JJ | ?
This would render the grammar asynchronous and
require adjustments to the decoding process. Al-
ternatively, we can generate rules that specifically
delete particular adjectives from the corpus:
JJ ? superfluous | ? .
In our experiments we evaluate the latter approach
by generating optional deletion rules for all adjec-
tives, adverbs and determiners.
6.5 Experimental Setup
We extracted a paraphrase grammar from the
French?English Europarl corpus (v5). The bitext
was aligned using the Berkeley aligner and the En-
glish side was parsed with the Berkeley parser. We
Grammar # Rules
total 42,353,318
w/o identity 23,641,016
w/o complex constituents 6,439,923
w/o complex const. & identity 5,097,250
Table 2: Number and distribution of rules in our para-
phrase grammar. Note the significant number of identity
paraphrases and rules with complex nonterminal labels.
obtained the initial translation grammar using the
SAMT toolkit (Venugopal and Zollmann, 2009).
The grammars we extract tend to be extremely
large. To keep their size manageable, we only con-
sider translation rules that have been seen more than
3 times and whose translation probability exceeds
10?4 for pivot recombination. Additionally, we only
retain the top 25 most likely paraphrases of each
phrase, ranked by a uniformly weighted combina-
tion of phrasal and lexical paraphrase probabilities.
We tuned the model parameters to our PRE?CIS
objective function, implemented in the Z-MERT
toolkit (Zaidan, 2009). For decoding we used the
Joshua decoder (Li et al, 2010). The language
model used in our paraphraser and the Clarke and
Lapata (2008) baseline system is a Kneser-Ney dis-
counted 5-gram model estimated on the Gigaword
corpus using the SRILM toolkit (Stolcke, 2002).
6.6 Evaluation
To assess the output quality of the resulting sentence
compression system, we compare it to two state-of-
the-art sentence compression systems. Specifically,
we compare against our implementation of Clarke
and Lapata (2008)?s compression model which uses
a series of constraints in an integer linear program-
ming (ILP) solver, and Cohn and Lapata (2007)?s
tree transducer toolkit (T3) which learns a syn-
chronous tree substitution grammar (STSG) from
paired monolingual sentences. Unlike SCFGs, the
STSG formalism allows changes to the tree topol-
ogy. Cohn and Lapata argue that this is a natural
fit for sentence compression, since deletions intro-
duce structural mismatches. We trained the T3 soft-
ware2 on the 936 ?full, compressed? sentence pairs
that comprise our development set. This is equiva-
lent in size to the training corpora that Cohn and La-
pata (2007) used (their training corpora ranged from
2www.dcs.shef.ac.uk/people/T.Cohn/t3/
1175
882?1020 sentence pairs), and has the advantage of
being in-domain with respect to our test set. Both
these systems reported results outperforming previ-
ous systems such as McDonald (2006). To showcase
the value of the adaptations discussed above, we also
compare variants of our paraphrase-based compres-
sion systems: using Hiero instead of syntax, using
syntax with or without compression features, using
an augmented grammar with optional deletion rules.
We solicit human judgments of the compres-
sions along two five-point scales: grammaticality
and meaning. Judges are instructed to decide how
much the meaning from a reference translation is
retained in the compressed sentence, with a score
of 5 indicating that all of the important information
is present, and 1 being that the compression does
not retain any of the original meaning. Similarly, a
grammar score of 5 indicates perfect grammaticality,
and a grammar score of 1 is assigned to sentences
that are entirely ungrammatical. To ensure fairness,
we perform pairwise system comparisons with com-
pression rates strictly tied on the sentence-level. For
any comparison, a sentence is only included in the
computation of average scores if the difference be-
tween both systems? compression rates is < 0.05.3
Table 4 shows a set of pairwise comparisons for
compression rates ? 0.5. We see that going from
a Hiero-based to a syntactic paraphrase grammar
yields a significant improvement in grammatical-
ity. Adding compression-specific features improves
grammaticality even further. Further augmenting the
grammar with deletion rules significantly helps re-
tain the core meaning at compression rates this high,
however compared to the un-augmented syntactic
system grammaticality scores drop. While our ap-
proach significantly outperforms the T3 system, we
are not able to match ILP?s results in grammaticality.
In Table 3 we compare our system to the ILP ap-
proach at a modest compression rate of? 0.8. Here,
we significantly outperform ILP in meaning reten-
tion while achieving comparable results in gram-
maticality. This improvement is significant at p <
0.0001, using the sign test, while the better gram-
maticality score of the ILP system is not statisti-
3Because evaluation quality correlates linearly with com-
pression rate, the community-accepted practice of not compar-
ing based on a closely tied compression rate is potentially sub-
ject to erroneous interpretation (Napoles et al, 2011).
CR Meaning Grammar
Reference 0.73 4.26 4.35
Syntax+Feat. 0.80 3.67 3.38
ILP 0.80 3.50 3.49
Random Deletions 0.50 1.94 1.57
Table 3: Results of the human evaluation on longer com-
pressions: pairwise compression rates (CR), meaning and
grammaticality scores. Bold indicates a statistically sig-
nificance difference at p < 0.05.
CR Meaning Grammar
Hiero 0.56 2.57 2.35
Syntax 0.56 2.76 2.67
Syntax 0.53 2.70 2.49
Syntax+Feat. 0.53 2.71 2.54
Syntax+Feat. 0.54 2.79 2.71
Syntax+Aug. 0.54 2.96 2.52
Syntax+Aug. 0.52 2.87 2.40
ILP 0.52 2.83 3.09
Syntax+Aug. 0.50 2.41 2.20
T3 0.50 2.01 1.93
Table 4: Human evaluation for shorter compressions and
for variations of our paraphrase system. +Feat. includes
the compression features from Section 6.1, +Aug. in-
cludes optional deletion rules from Section 6.4.
cally significant (p < 0.088). These results indi-
cate that, over a variety of compression rates, our
framework for text-to-text generation is performing
as well as or better than specifically tailored state-
of-the-art methods.
Table 5 shows an example sentence drawn from
our test set and the compressions produced by the
different systems. We see that both the paraphrase
and ILP systems produce good quality results, with
the paraphrase system retaining the meaning of the
source sentence more accurately.
7 Conclusion
In this work we introduced a method to learn syntac-
tically informed paraphrases from bilingual parallel
texts. We discussed the expressive power and limita-
tions of our formalism and outlined straightforward
adaptation strategies for applications in text-to-text
generation. We demonstrated when our paraphras-
ing system was adapted to do sentence compression,
it achieved results competitive with state-of-the-art
compression systems with only minimal effort.
1176
Source he also expected that he would have a role in the future at the level of the islamic movementacross the palestinian territories , even if he was not lucky enough to win in the elections .
Reference he expects to have a future role in the islamic movement in the palestinian territories if he isnot successful in the elections .
Syntax+Feat. he also expected that he would have a role in the future of the islamic movement in thepalestinian territories , although he was not lucky enough to win elections .
ILP he also expected that he would have a role at the level of the islamic movement , even if hewas not lucky enough to win in the elections .
Source in this war which has carried on for the last 12 days , around 700 palestinians , which includea large number of women and children , have died .
Reference about 700 palestinians , mostly women and children , have been killed in the israeli offensiveover the last 12 days .
Syntax+Feat. in this war has done for the last 12 days , around 700 palestinians , including women andchildren , died .
ILP in this war which has carried for the days palestinians , which include a number of womenand children died .
Source hala speaks arabic most of the time with her son , taking into consideration that he can speakenglish with others .
Reference hala speaks to her son mostly in arabic , as he can speak english to others .
Syntax+Feat. hala speaks arabic most of the time with her son , considering that he can speak english withothers .
ILP hala speaks arabic most of the time , taking into consideration that he can speak english withothers .
Table 5: Example compressions produced by the two systems in Table 3 for three input sentences from our test data.
Acknowledgments
We would like to thank Trevor Cohn for kindly pro-
viding us with the T3 compression system. This re-
search was supported by the NSF under grant IIS-
0713448. Opinions, interpretations, and conclusions
are the authors? alone.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Peter G. Anick and Suresh Tipirneni. 1999. The para-
phrase search assistant: terminological feedback for
iterative information seeking. In Proceedings of SI-
GIR.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay and Noemie Elhadad. 2003. Sentence
alignment for monolingual comparable corpora. In
Proceedings of EMNLP.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of ACL.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings of
ACL.
Regina Barzilay. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Genera-
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Peter Brown, John Cocke, Stephen Della Pietra, Vincent
Della Pietra, Frederick Jelinek, Robert Mercer, and
1177
Paul Poossin. 1990. A statistical approach to language
translation. Computational Linguistics, 16(2), June.
Peter Brown, Stephen Della Pietra, Vincent Della Pietra,
and Robert Mercer. 1993. The mathematics of ma-
chine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311, June.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2007. Large margin
synchronous generation and its application to sentence
compression. In Proceedings of EMNLP-CoLing.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research (JAIR), 34:637?674.
Peter W. Culicover. 1968. Paraphrase generation and
information retrieval from stored text. Mechani-
cal Translation and Computational Linguistics, 11(1-
2):78?88.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Mark Dras. 1997. Representing paraphrases using syn-
chronous tree adjoining grammars. In Proceedings of
ACL.
Mark Dras. 1999. Tree Adjoining Grammar and the Re-
luctant Paraphrasing of Text. Ph.D. thesis, Macquarie
University, Australia.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proceed-
ings of ACL.
William Gale and Kenneth Church. 1993. A program
for aligning sentences in bilingual corpora. Compu-
atational Linguistics, 19(1):75?90.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Pro-
ceedings of HLT/NAACL.
Erica Greene, Tugba Bodrumlu, and Kevin Knight. 2010.
Automatic analysis of rhythmic poetry with applica-
tions to generation and translation. In Proceedings of
EMNLP.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proceedings of ACL.
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
EMNLP.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Raymond Kozlowski, Kathleen McCoy, and K. Vijay-
Shanker. 2003. Generation of single-sentence
paraphrases from predicate/argument structure using
lexico-grammatical resources. In Workshop On Para-
phrasing.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Workshop On
Natural Language Generation, Ontario, Canada.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proceedings of WMT09.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren Thornton, Ziyuan Wang, Jonathan
Weese, and Omar Zaidan. 2010. Joshua 2.0: A
toolkit for parsing-based machine translation with syn-
tax, semirings, discriminative training and other good-
ies. In Proceedings of WMT10.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment templates for statistical machine
translation. In Proceedings of the ACL/Coling.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parameter
tuning in statistical machine translation. In Proceed-
ings of WMT07.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic evidence. In Proceedings
of EACL.
1178
Kathleen R. McKeown. 1979. Paraphrasing using given
and new information in a question-answer system. In
Proceedings of ACL.
Dan Melamed. 2004. Statistical machine translation by
parsing. In Proceedings of ACL.
Marie W. Meteer and Varda Shaked. 1988. Strategies for
effective paraphrasing. In Proceedings of COLING.
Kazunori Muraki. 1982. On a semantic model for multi-
lingual paraphrasing. In Proceedings of COLING.
Courtney Napoles, Chris Callison-Burch, and Ben-
jamin Van Durme. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Work-
shop on Monolingual Text-To-Text Generation.
Franz Josef Och. 2003a. Minimum error rate training for
statistical machine translation. In Proceedings of ACL.
Franz Josef Och. 2003b. Minimum error rate training in
statistical machine translation. In Proceedings of ACL.
Karolina Owczarzak, Declan Groves, Josef Van Gen-
abith, and Andy Way. 2006. Contextual bitext-derived
paraphrases in automatic MT evaluation. In Proceed-
ings of WMT06.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proceedings of HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of ACL.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
sufrace text patterns for a question answering system.
In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Hadar Shemtov. 1996. Generation of paraphrases from
ambiguous logical forms. In Proceedings of COLING.
Stuart Shieber and Yves Schabes. 1990. Generation and
synchronous tree-adjoining grammars. In Workshop
On Natural Language Generation.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Non-Transformational
Syntax: Formal and Explicit Models of Grammar.
Wiley-Blackwell.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceeding of the International
Conference on Spoken Language Processing.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition of
entailment relations. In Proceedings of EMNLP, Pro-
ceedings of EMNLP.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. Prague Bul-
letin of Mathematical Linguistics, 91.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3).
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of ACL.
Kazuhide Yamamoto. 2002. Machine translation by in-
teraction between paraphraser and transfer. In Pro-
ceedings of COLING.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008a. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008b. Pivot approach for extracting paraphrase
patterns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of WMT06.
1179
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1363?1372,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Watermarking the Outputs of Structured Prediction with an
application in Statistical Machine Translation.
Ashish Venugopal1 Jakob Uszkoreit1 David Talbot1 Franz J. Och1 Juri Ganitkevitch2
1Google, Inc. 2Center for Language and Speech Processing
1600 Amphitheatre Parkway Johns Hopkins University
Mountain View, 94303, CA Baltimore, MD 21218, USA
{avenugopal,uszkoreit,talbot,och}@google.com juri@cs.jhu.edu
Abstract
We propose a general method to water-
mark and probabilistically identify the
structured outputs of machine learning al-
gorithms. Our method is robust to lo-
cal editing operations and provides well
defined trade-offs between the ability to
identify algorithm outputs and the qual-
ity of the watermarked output. Unlike
previous work in the field, our approach
does not rely on controlling the inputs to
the algorithm and provides probabilistic
guarantees on the ability to identify col-
lections of results from one?s own algo-
rithm. We present an application in statis-
tical machine translation, where machine
translated output is watermarked at mini-
mal loss in translation quality and detected
with high recall.
1 Motivation
Machine learning algorithms provide structured
results to input queries by simulating human be-
havior. Examples include automatic machine
translation (Brown et al, 1993) or automatic
text and rich media summarization (Goldstein
et al, 1999). These algorithms often estimate
some portion of their models from publicly avail-
able human generated data. As new services
that output structured results are made avail-
able to the public and the results disseminated
on the web, we face a daunting new challenge:
Machine generated structured results contam-
inate the pool of naturally generated human
data. For example, machine translated output
and human generated translations are currently
both found extensively on the web, with no auto-
matic way of distinguishing between them. Al-
gorithms that mine data from the web (Uszko-
reit et al, 2010), with the goal of learning to
simulate human behavior, will now learn mod-
els from this contaminated and potentially self-
generated data, reinforcing the errors commit-
ted by earlier versions of the algorithm.
It is beneficial to be able to identify a set of
encountered structured results as having been
generated by one?s own algorithm, with the pur-
pose of filtering such results when building new
models.
Problem Statement: We define a struc-
tured result of a query q as r = {z1 ? ? ? zL} where
the order and identity of elements zi are impor-
tant to the quality of the result r. The structural
aspect of the result implies the existence of alter-
native results (across both the order of elements
and the elements themselves) that might vary in
their quality.
Given a collection of N results, CN =
r1 ? ? ? rN , where each result ri has k ranked alter-
natives Dk(qi) of relatively similar quality and
queries q1 ? ? ? qN are arbitrary and not controlled
by the watermarking algorithm, we define the
watermarking task as:
Task. Replace ri with r?i ? Dk(qi) for some sub-
set of results in CN to produce a watermarked
collection C?N
such that:
? C?N is probabilistically identifiable as having
been generated by one?s own algorithm.
1363
? the degradation in quality from CN to the
watermarked C?N should be analytically con-
trollable, trading quality for detection per-
formance.
? C?N should not be detectable as water-
marked content without access to the gen-
erating algorithms.
? the detection of C?N should be robust to sim-
ple edit operations performed on individual
results r ? C?N .
2 Impact on Statistical Machine
Translation
Recent work(Resnik and Smith, 2003; Munteanu
and Marcu, 2005; Uszkoreit et al, 2010) has
shown that multilingual parallel documents can
be efficiently identified on the web and used as
training data to improve the quality of statisti-
cal machine translation.
The availability of free translation services
(Google Translate, Bing Translate) and tools
(Moses, Joshua), increase the risk that the con-
tent found by parallel data mining is in fact gen-
erated by a machine, rather than by humans. In
this work, we focus on statistical machine trans-
lation as an application for watermarking, with
the goal of discarding documents from training
if they have been generated by one?s own algo-
rithms.
To estimate the magnitude of the problem,
we used parallel document mining (Uszkoreit et
al., 2010) to generate a collection of bilingual
document pairs across several languages. For
each document, we inspected the page content
for source code that indicates the use of trans-
lation modules/plug-ins that translate and pub-
lish the translated content.
We computed the proportion of the content
within our corpus that uses these modules. We
find that a significant proportion of the mined
parallel data for some language pairs is gener-
ated via one of these translation modules. The
top 3 languages pairs, each with parallel trans-
lations into English, are Tagalog (50.6%), Hindi
(44.5%) and Galician (41.9%). While these
proportions do not reflect impact on each lan-
guage?s monolingual web, they are certainly high
enough to affect machine translations systems
that train on mined parallel data. In this work,
we develop a general approach to watermark
structured outputs and apply it to the outputs
of a statistical machine translation system with
the goal of identifying these same outputs on the
web. In the context of the watermarking task
defined above, we output selecting alternative
translations for input source sentences. These
translations often undergo simple edit and for-
matting operations such as case changes, sen-
tence and word deletion or post editing, prior to
publishing on the web. We want to ensure that
we can still detect watermarked translations de-
spite these edit operations. Given the rapid pace
of development within machine translation, it
is also important that the watermark be robust
to improvements in underlying translation qual-
ity. Results from several iterations of the system
within a single collection of documents should be
identifiable under probabilistic bounds.
While we present evaluation results for sta-
tistical machine translation, our proposed ap-
proach and associated requirements are applica-
ble to any algorithm that produces structured
results with several plausible alternatives. The
alternative results can arise as a result of inher-
ent task ambiguity (for example, there are mul-
tiple correct translations for a given input source
sentence) or modeling uncertainty (for example,
a model assigning equal probability to two com-
peting results).
3 Watermark Structured Results
Selecting an alternative r? from the space of al-
ternatives Dk(q) can be stated as:
r? = arg max
r?Dk(q)
w(r,Dk(q), h) (1)
where w ranks r ? Dk(q) based on r?s presen-
tation of a watermarking signal computed by a
hashing operation h. In this approach, w and
its component operation h are the only secrets
held by the watermarker. This selection crite-
rion is applied to all system outputs, ensuring
that watermarked and non-watermarked version
of a collection will never be available for compar-
ison.
1364
A specific implementation of w within our wa-
termarking approach can be evaluated by the
following metrics:
? False Positive Rate: how often non-
watermarked collections are falsely identi-
fied as watermarked.
? Recall Rate: how often watermarked col-
lections are correctly identified as water-
marked.
? Quality Degradation: how significantly
does C?N differ from CN when evaluated by
task specific quality metrics.
While identification is performed at the col-
lection level, we can scale these metrics based
on the size of each collection to provide more
task sensitive metrics. For example, in machine
translation, we count the number of words in
the collection towards the false positive and re-
call rates.
In Section 3.1, we define a random hashing
operation h and a task independent implemen-
tation of the selector function w. Section 3.2
describes how to classify a collection of water-
marked results. Section 3.3 and 3.4 describes re-
finements to the selection and classification cri-
teria that mitigate quality degradation. Follow-
ing a comparison to related work in Section 4,
we present experimental results for several lan-
guages in Section 5.
3.1 Watermarking: CN ? C?N
We define a random hashing operation h that is
applied to result r. It consists of two compo-
nents:
? A hash function applied to a structured re-
sult r to generate a bit sequence of a fixed
length.
? An optional mapping that maps a single
candidate result r to a set of sub-results.
Each sub-result is then hashed to generate
a concatenated bit sequence for r.
A good hash function produces outputs whose
bits are independent. This implies that we can
treat the bits for any input structured results
as having been generated by a binomial distri-
bution with equal probability of generating 1s
vs 0s. This condition also holds when accu-
mulating the bit sequences over a collection of
results as long as its elements are selected uni-
formly from the space of possible results. There-
fore, the bits generated from a collection of un-
watermarked results will follow a binomial dis-
tribution with parameter p = 0.5. This result
provides a null hypothesis for a statistical test
on a given bit sequence, testing whether it is
likely to have been generated from a binomial
distribution binomial(n, p) where p = 0.5 and n
is the length of the bit sequence.
For a collection CN = r1 ? ? ? rN , we can define
a watermark ranking function w to systemati-
cally select alternatives r?i ? Dk(q), such that
the resulting C?N is unlikely to produce bit se-
quences that follow the p = 0.5 binomial distri-
bution. A straightforward biasing criteria would
be to select the candidate whose bit sequence ex-
hibits the highest ratio of 1s. w can be defined
as:
w(r,Dk(q), h) =
#(1, h(r))
|h(r)| (2)
where h(r) returns the randomized bit sequence
for result r, and #(x, ~y) counts the number of
occurrences of x in sequence ~y. Selecting alter-
natives results to exhibit this bias will result in
watermarked collections that exhibit this same
bias.
3.2 Detecting the Watermark
To classify a collection CN as watermarked or
non-watermarked, we apply the hashing opera-
tion h on each element in CN and concatenate
the sequences. This sequence is tested against
the null hypothesis that it was generated by a
binomial distribution with parameter p = 0.5.
We can apply a Fisherian test of statistical sig-
nificance to determine whether the observed dis-
tribution of bits is unlikely to have occurred by
chance under the null hypothesis (binomial with
p = 0.5).
We consider a collection of results that rejects
the null hypothesis to be watermarked results
generated by our own algorithms. The p-value
under the null hypothesis is efficiently computed
1365
by:
p? value = Pn(X ? x) (3)
=
n?
i=x
(n
i
)
pi(1? p)n?i (4)
where x is the number of 1s observed in the col-
lection, and n is the total number of bits in the
sequence. Comparing this p-value against a de-
sired significance level ?, we reject the null hy-
pothesis for collections that have Pn(X ? x) <
?, thus deciding that such collections were gen-
erated by our own system.
This classification criteria has a fixed false
positive rate. Setting ? = 0.05, we know that
5% of non-watermarked bit sequences will be
falsely labeled as watermarked. This parameter
? can be controlled on an application specific ba-
sis. By biasing the selection of candidate results
to produce more 1s than 0s, we have defined
a watermarking approach that exhibits a fixed
false positive rate, a probabilistically bounded
detection rate and a task independent hashing
and selection criteria. In the next sections, we
will deal with the question of robustness to edit
operations and quality degradation.
3.3 Robustness and Inherent Bias
We would like the ability to identify water-
marked collections to be robust to simple edit
operations. Even slight modifications to the ele-
ments within an item r would yield (by construc-
tion of the hash function), completely different
bit sequences that no longer preserve the biases
introduced by the watermark selection function.
To ensure that the distributional biases intro-
duced by the watermark selector are preserved,
we can optionally map individual results into a
set of sub-results, each one representing some lo-
cal structure of r. h is then applied to each sub-
result and the results concatenated to represent
r. This mapping is defined as a component of
the h operation.
While a particular edit operation might af-
fect a small number of sub-results, the majority
of the bits in the concatenated bit sequence for
r would remain untouched, thereby limiting the
damage to the biases selected during watermark-
ing. This is of course no defense to edit opera-
tions that are applied globally across the result;
our expectation is that such edits would either
significantly degrade the quality of the result or
be straightforward to identify directly.
For example, a sequence of words r = z1 ? ? ? zL
can be mapped into a set of consecutive n-gram
sequences. Operations to edit a word zi in r will
only affect events that consider the word zi. To
account for the fact that alternatives in Dk(q)
might now result in bit sequences of different
lengths, we can generalize the biasing criteria to
directly reflect the expected contribution to the
watermark by defining:
w(r,Dk(q), h) = Pn(X ? #(1, h(r))) (5)
where Pn gives probabilities from binomial(n =
|h(r)|, p = 0.5).
Inherent collection level biases: Our null
hypothesis is based on the assumption that col-
lections of results draw uniformly from the space
of possible results. This assumption might not
always hold and depends on the type of the re-
sults and collection. For example, considering
a text document as a collection of sentences,
we can expect that some sentences might repeat
more frequently than others.
This scenario is even more likely when ap-
plying a mapping into sub-results. n-gram se-
quences follow long-tailed or Zipfian distribu-
tions, with a small number of n-grams contribut-
ing heavily toward the total number of n-grams
in a document.
A random hash function guarantees that in-
puts are distributed uniformly at random over
the output range. However, the same input will
be assigned the same output deterministically.
Therefore, if the distribution of inputs is heav-
ily skewed to certain elements of the input space,
the output distribution will not be uniformly
distributed. The bit sequences resulting from
the high frequency sub-results have the potential
to generate inherently biased distributions when
accumulated at the collection level. We want to
choose a mapping that tends towards generating
uniformly from the space of sub-results. We can
empirically measure the quality of a sub-result
mapping for a specific task by computing the
1366
false positive rate on non-watermarked collec-
tions. For a given significance level ?, an ideal
mapping would result in false positive rates close
to ? as well.
Figure 1 shows false positive rates from 4 al-
ternative mappings, computed on a large corpus
of French documents (see Table 1 for statistics).
Classification decisions are made at the collec-
tion level (documents) but the contribution to
the false positive rate is based on the number
of words in the classified document. We con-
sider mappings from a result (sentence) into its
1-grams, 1 ? 5-grams and 3 ? 5 grams as well
as the non-mapping case, where the full result
is hashed.
Figure 1 shows that the 1-grams and 1 ? 5-
gram generate sub-results that result in heav-
ily biased false positive rates. The 3 ? 5 gram
mapping yields false positive rates close to their
theoretically expected values. 1 Small devia-
tions are expected since documents make differ-
ent contributions to the false positive rate as a
function of the number of words that they repre-
sent. For the remainder of this work, we use the
3-5 gram mapping and the full sentence map-
ping, since the alternatives generate inherently
distributions with very high false positive rates.
3.4 Considering Quality
The watermarking described in Equation 3
chooses alternative results on a per result basis,
with the goal of influencing collection level bit
sequences. The selection criteria as described
will choose the most biased candidates available
in Dk(q). The parameter k determines the ex-
tent to which lesser quality alternatives can be
chosen. If all the alternatives in each Dk(q) are
of relatively similar quality, we expect minimal
degradation due to watermarking.
Specific tasks however can be particularly sen-
sitive to choosing alternative results. Discrimi-
native approaches that optimize for arg max se-
lection like (Och, 2003; Liang et al, 2006; Chi-
ang et al, 2009) train model parameters such
1In the final version of this paper we will perform sam-
pling to create a more reliable estimate of the false posi-
tive rate that is not overly influenced by document length
distributions.
that the top-ranked result is well separated from
its competing alternatives. Different queries also
differ in the inherent ambiguity expected from
their results; sometimes there really is just one
correct result for a query, while for other queries,
several alternatives might be equally good.
By generalizing the definition of the w func-
tion to interpolate the estimated loss in quality
and the gain in the watermarking signal, we can
trade-off the ability to identify the watermarked
collections against quality degradation:
w(r,Dk(q), fw) = ? ? gain(r,Dk(q), fw)
?(1? ?) ? loss(r,Dk(q))
(6)
Loss: The loss(r,Dk(q)) function reflects the
quality degradation that results from selecting
alternative r as opposed to the best ranked can-
didate in Dk(q)). We will experiment with two
variants:
lossrank(r,Dk(q)) = (rank(r)? k)/k
losscost(r,Dk(q)) = (cost(r)?cost(r1))/ cost(r1)
where:
? rank(r): returns the rank of r within Dk(q).
? cost(r): a weighted sum of features (not
normalized over the search space) in a log-
linear model such as those mentioned in
(Och, 2003).
? r1: the highest ranked alternative in Dk(q).
lossrank provides a generally applicable criteria
to select alternatives, penalizing selection from
deep within Dk(q). This estimate of the qual-
ity degradation does not reflect the generating
model?s opinion on relative quality. losscost con-
siders the relative increase in the generating
model?s cost assigned to the alternative trans-
lation.
Gain: The gain(r,Dk(q), fw) function reflects
the gain in the watermarking signal by selecting
candidate r. We simply define the gain as the
Pn(X ? #(1, h(r))) from Equation 5.
1367
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(a) 1-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(b) 1? 5-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(c) 3? 5-grams mapping
 0 0.2 0.4 0.6 0.8 1 0.05 0.10 0.25 0.5portion of documents p-value thresholdexpectedobserved
(d) Full result hashing
Figure 1: Comparison of expected false positive rates against observed false positive rates for different
sub-result mappings.
4 Related Work
Using watermarks with the goal of transmitting
a hidden message within images, video, audio
and monolingual text media is common. For
structured text content, linguistic approaches
like (Chapman et al, 2001; Gupta et al, 2006)
use language specific linguistic and semantic
expansions to introduce hidden watermarks.
These expansions provide alternative candidates
within which messages can be encoded. Re-
cent publications have extended this idea to ma-
chine translation, using multiple systems and
expansions to generate alternative translations.
(Stutsman et al, 2006) uses a hashing function
to select alternatives that encode the hidden
message in the lower order bits of the transla-
tion. In each of these approaches, the water-
marker has control over the collection of results
into which the watermark is to be embedded.
These approaches seek to embed a hidden
message into a collection of results that is se-
lected by the watermarker. In contrast, we ad-
dress the condition where the input queries are
not in the watermarker?s control.
The goal is therefore to introduce the water-
mark into all generated results, with the goal of
probabilistically identifying such outputs. Our
approach is also task independent, avoiding the
need for templates to generate additional al-
ternatives. By addressing the problem directly
within the search space of a dynamic program-
ming algorithm, we have access to high quality
alternatives with well defined models of qual-
ity loss. Finally, our approach is robust to local
word editing. By using a sub-result mapping, we
increase the level of editing required to obscure
the watermark signal; at high levels of editing,
the quality of the results themselves would be
significantly degraded.
5 Experiments
We evaluate our watermarking approach applied
to the outputs of statistical machine translation
under the following experimental setup.
A repository of parallel (aligned source and
target language) web documents is sampled to
produce a large corpus on which to evaluate the
watermarking classification performance. The
1368
corpora represent translations into 4 diverse tar-
get languages, using English as the source lan-
guage. Each document in this corpus can be
considered a collection of un-watermarked struc-
tured results, where source sentences are queries
and each target sentence represents a structured
result.
Using a state-of-the-art phrase-based statisti-
cal machine translation system (Och and Ney,
2004) trained on parallel documents identified
by (Uszkoreit et al, 2010), we generate a set
of 100 alternative translations for each source
sentence. We apply the proposed watermarking
approach, along with the proposed refinements
that address task specific loss (Section 3.4) and
robustness to edit operations (Section 3.3) to
generate watermarked corpora.
Each method is controlled via a single param-
eter (like k or ?) which is varied to generate
alternative watermarked collections. For each
parameter value, we evaluate the Recall Rate
and Quality Degradation with the goal of find-
ing a setting that yields a high recall rate, min-
imal quality degradation. False positive rates
are evaluated based on a fixed classification sig-
nificance level of ? = 0.05. The false posi-
tive and recall rates are evaluated on the word
level; a document that is misclassified or cor-
rectly identified contributes its length in words
towards the error calculation. In this work, we
use ? = 0.05 during classification corresponding
to an expected 5% false positive rate. The false
positive rate is a function of h and the signifi-
cance level ? and therefore constant across the
parameter values k and ?.
We evaluate quality degradation on human
translated test corpora that are more typical for
machine translation evaluation. Each test cor-
pus consists of 5000 source sentences randomly
selected from the web and translated into each
respective language.
We chose to evaluate quality on test corpora
to ensure that degradations are not hidden by
imperfectly matched web corpora and are con-
sistent with the kind of results often reported for
machine translation systems. As with the clas-
sification corpora, we create watermarked ver-
sions at each parameter value. For a given pa-
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 
l
o
s
s
recall
FRENCH max K-BestFRENCH model cost gainFRENCH rank gain
Figure 2: BLEU loss against recall of watermarked
content for the baseline approach (max K-best),
rank and cost interpolation.
rameter value, we measure false positive and re-
call rates on the classification corpora and qual-
ity degradation on the evaluation corpora.
Table 1 shows corpus statistics for the classi-
fication and test corpora and non-watermarked
BLEU scores for each target language. All
source texts are in English.
5.1 Loss Interpolated Experiments
Our first set of experiments demonstrates base-
line performance using the watermarking crite-
ria in Equation 5 versus the refinements sug-
gested in Section 3.4 to mitigate quality degra-
dation. The h function is computed on the full
sentence result r with no sub-event mapping.
The following methods are evaluated in Figure 2.
? Baseline method (labeled ?max K-best?):
selects r? purely based on gain in water-
marking signal (Equation 5) and is param-
eterized by k: the number of alternatives
considered for each result.
? Rank interpolation: incorporates rank into
w, varying the interpolation parameter ?.
? Cost interpolation: incorporates cost into
w, varying the interpolation parameter ?.
The observed false positive rate on the French
classification corpora is 1.9%.
1369
Classification Quality
Target # words # sentences # documents # words # sentences BLEU %
Arabic 200107 15820 896 73592 5503 12.29
French 209540 18024 600 73592 5503 26.45
Hindi 183676 13244 1300 73409 5489 20.57
Turkish 171671 17155 1697 73347 5486 13.67
Table 1: Content statistics for classification and quality degradation corpora. Non-watermarked BLEU
scores are reported for the quality corpora.
We consider 0.2% BLEU loss as a thresh-
old for acceptable quality degradation. Each
method is judged by its ability to achieve high
recall below this quality degradation threshold.
Applying cost interpolation yields the best
results in Figure 2, achieving a recall of 85%
at 0.2% BLEU loss, while rank interpolation
achieves a recall of 76%. The baseline approach
of selecting the highest gain candidate within a
depth of k candidates does not provide sufficient
parameterization to yield low quality degrada-
tion. At k = 2, this method yields almost 90%
recall, but with approximately 0.4% BLEU loss.
5.2 Robustness Experiments
In Section 5.2, we proposed mapping results into
sub-events or features. We considered alterna-
tive feature mappings in Figure 1, finding that
mapping sentence results into a collection of 3-
5 grams yields acceptable false positive rates at
varied levels of ?.
Figure 3 presents results that compare mov-
ing from the result level hashing to the 3-5 gram
sub-result mapping. We show the impact of the
mapping on the baseline max K-best method as
well as for cost interpolation. There are sub-
stantial reductions in recall rate at the 0.2%
BLEU loss level when applying sub-result map-
pings in cases. The cost interpolation method
recall drops from 85% to 77% when using the
3-5 grams event mapping. The observed false
positive rate of the 3-5 gram mapping is 4.7%.
By using the 3-5 gram mapping, we expect
to increase robustness against local word edit
operations, but we have sacrificed recall rate due
to the inherent distributional bias discussed in
Section 3.3.
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.5  0.6  0.7  0.8  0.9  1
B
L
E
U
 
l
o
s
s
recall
FRENCH nbest max K-best full sentenceFRENCH max K-best 3-5 gramsFRENCH cost interp. full sentenceFRENCH cost interp. 3-5 grams
Figure 3: BLEU loss against recall of watermarked
content for the baseline and cost interpolation meth-
ods using both result level and 3-5 gram mapped
events.
5.3 Multilingual Experiments
The watermarking approach proposed here in-
troduces no language specific watermarking op-
erations and it is thus broadly applicable to
translating into all languages. In Figure 4, we
report results for the baseline and cost interpola-
tion methods, considering both the result level
and 3-5 gram mapping. We set ? = 0.05 and
measure recall at 0.2% BLEU degradation for
translation from English into Arabic, French,
Hindi and Turkish. The observed false posi-
tive rates for full sentence hashing are: Arabic:
2.4%, French: 1.8%, Hindi: 5.6% and Turkish:
5.5%, while for the 3-5 gram mapping, they are:
Arabic: 5.8%, French: 7.5%, Hindi:3.5% and
Turkish: 6.2%. Underlying translation qual-
ity plays an important role in translation qual-
ity degradation when watermarking. Without
a sub-result mapping, French (BLEU: 26.45%)
1370
 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.95 1 Arabic French Hindi Turkishrecall sentence-level3-to-5 grams
Figure 4: Loss of recall when using 3-5 gram mapping
vs sentence level mapping for Arabic, French, Hindi
and Turkish translations.
achieves recall of 85% at 0.2% BLEU loss, while
the other languages achieve over 90% recall at
the same BLEU loss threshold. Using a sub-
result mapping degrades quality for each lan-
guage pair, but changes the relative perfor-
mance. Turkish experiences the highest rela-
tive drop in recall, unlike French and Arabic,
where results are relatively more robust to using
sub-sentence mappings. This is likely a result of
differences in n-gram distributions across these
languages. The languages considered here all
use space separated words. For languages that
do not, like Chinese or Thai, our approach can
be applied at the character level.
6 Conclusions
In this work we proposed a general method
to watermark and probabilistically identify the
structured outputs of machine learning algo-
rithms. Our method provides probabilistic
bounds on detection ability, analytic control on
quality degradation and is robust to local edit-
ing operations. Our method is applicable to
any task where structured outputs are generated
with ambiguities or ties in the results. We ap-
plied this method to the outputs of statistical
machine translation, evaluating each refinement
to our approach with false positive and recall
rates against BLEU score quality degradation.
Our results show that it is possible, across sev-
eral language pairs, to achieve high recall rates
(over 80%) with low false positive rates (between
5 and 8%) at minimal quality degradation (0.2%
BLEU), while still allowing for local edit opera-
tions on the translated output. In future work
we will continue to investigate methods to mit-
igate quality loss.
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Minimum error rate
training in statistical machine translation. In Pro-
ceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning.
Peter F. Brown, Vincent J.Della Pietra, Stephen
A. Della Pietra, and Robert. L. Mercer. 1993.
The mathematics of statistical machine transla-
tion: Parameter estimation. Computational Lin-
guistics, 19:263?311.
Mark Chapman, George Davida, and Marc
Rennhardway. 2001. A practical and effec-
tive approach to large-scale automated linguistic
steganography. In Proceedings of the Information
Security Conference.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine trans-
lation. In North American Chapter of the Associa-
tion for Computational Linguistics - Human Lan-
guage Technologies (NAACL-HLT).
Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, and
Jaime Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics.
In Research and Development in Information Re-
trieval, pages 121?128.
Gaurav Gupta, Josef Pieprzyk, and Hua Xiong
Wang. 2006. An attack-localizing watermarking
scheme for natural language documents. In Pro-
ceedings of the 2006 ACM Symposium on Informa-
tion, computer and communications security, ASI-
ACCS ?06, pages 157?165, New York, NY, USA.
ACM.
Percy Liang, Alexandre Bouchard-Cote, Dan Klein,
and Ben Taskar. 2006. An end-to-end dis-
criminative approach to machine translation. In
Proceedings of the Joint International Conference
on Computational Linguistics and Association of
Computational Linguistics (COLING/ACL, pages
761?768.
Dragos Stefan Munteanu and Daniel Marcu. 2005.
Improving machine translation performance by ex-
ploiting non-parallel corpora. Computational Lin-
guistics.
Franz Josef Och and Hermann Ney. 2004. The
1371
alignment template approach to statistical ma-
chine translation. Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 2003 Meeting of the Asssociation of Com-
putational Linguistics.
Philip Resnik and Noah A. Smith. 2003. The web as
a parallel corpus. computational linguistics. Com-
putational Linguistics.
Ryan Stutsman, Mikhail Atallah, Christian
Grothoff, and Krista Grothoff. 2006. Lost
in just the translation. In Proceedings of the 2006
ACM Symposium on Applied Computing.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and
Moshe Dubiner. 2010. Large scale parallel doc-
ument mining for machine translation. In Pro-
ceedings of the 2010 COLING.
1372
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 192?201,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
PARADIGM: Paraphrase Diagnostics through Grammar Matching
Jonathan Weese and Juri Ganitkevitch
Johns Hopkins University
Chris Callison-Burch
University of Pennsylvania
Abstract
Paraphrase evaluation is typically done ei-
ther manually or through indirect, task-
based evaluation. We introduce an in-
trinsic evaluation PARADIGM which mea-
sures the goodness of paraphrase col-
lections that are represented using syn-
chronous grammars. We formulate two
measures that evaluate these paraphrase
grammars using gold standard sentential
paraphrases drawn from a monolingual
parallel corpus. The first measure calcu-
lates how often a paraphrase grammar is
able to synchronously parse the sentence
pairs in the corpus. The second mea-
sure enumerates paraphrase rules from the
monolingual parallel corpus and calculates
the overlap between this reference para-
phrase collection and the paraphrase re-
source being evaluated. We demonstrate
the use of these evaluation metrics on para-
phrase collections derived from three dif-
ferent data types: multiple translations
of classic French novels, comparable sen-
tence pairs drawn from different newspa-
pers, and bilingual parallel corpora. We
show that PARADIGM correlates with hu-
man judgments more strongly than BLEU
on a task-based evaluation of paraphrase
quality.
1 Introduction
Paraphrases are useful in a wide range of natu-
ral language processing applications. A variety
of data-driven approaches have been proposed to
generate paraphrase resources (see Madnani and
Dorr (2010) for a survey of these methods). Few
objective metrics have been established to evalu-
ate these resources. Instead, paraphrases are typi-
cally evaluated using subjective manual evaluation
or through task-based evaluations.
Different researchers have used different crite-
ria for manual evaluations. For example, Barzilay
and McKeown (2001) evaluated their paraphrases
by asking judges whether paraphrases were ?ap-
proximately conceptually equivalent.? Ibrahim
et al. (2003) asked judges whether their para-
phrases were ?roughly interchangeable given the
genre.? Bannard and Callison-Burch (2005) re-
placed phrases with paraphrases in a number of
sentences and asked judges whether the substitu-
tions ?preserved meaning and remained grammat-
ical.? The results of these subjective evaluations
are not easily reusable.
Other researchers have evaluated their para-
phrases through task-based evaluations. Lin and
Pantel (2001) measured their potential impact on
question-answering. Cohn and Lapata (2007)
evaluate their applicability in the text-to-text gen-
eration task of sentence compression. Zhao et al.
(2009) use them to perform sentence compression
and simplification and to compute sentence simi-
larity. Several researchers have demonstrated that
paraphrases can improve machine translation eval-
uation (c.f. Kauchak and Barzilay (2006), Zhou
et al. (2006), Madnani (2010) and Snover et al.
(2010)).
We introduce an automatic evaluation met-
ric called PARADIGM, PARAphrase DIagnostics
through Grammar Matching. This metric eval-
uates paraphrase collections that are represented
using synchronous grammars. Synchronous tree-
adjoining grammars (STAGs), synchronous tree
substitution grammars (STSGs), and synchronous
context free grammars (SCFGs) are popular for-
malisms for representing paraphrase rules (Dras,
1997; Cohn and Lapata, 2007; Madnani, 2010;
Ganitkevitch et al., 2011). We present two mea-
sures that evaluate these paraphrase grammars us-
ing gold standard sentential paraphrases drawn
from a monolingual parallel corpus, which have
been previously proposed as a good resource
192
for paraphrase evaluation (Callison-Burch et al.,
2008; Cohn et al., 2008).
The first of our two proposed metrics calculates
how often a paraphrase grammar is able to syn-
chronously parse the sentence pairs in a test set.
The second measure enumerates paraphrase rules
from a monolingual parallel corpus and calculates
the overlap between this reference paraphrase col-
lection, and the paraphrase resource being evalu-
ated.
2 Related work and background
The most closely related work is ParaMetric
(Callison-Burch et al., 2008), which is a set of
objective measures for evaluating the quality of
phrase-based paraphrases. ParaMetric extracts a
set of gold-standard phrasal paraphrases from sen-
tential paraphrases that have been manually word-
aligned. The sentential paraphrases used in Para-
Metric were drawn from a data set originally cre-
ated to evaluate machine translation output using
the BLEU metric. Cohn et al. (2008) argue that
these sorts of monolingual parallel corpora are ap-
propriate for evaluating paraphrase systems, be-
cause they are naturally occurring sources of para-
phrases.
Callison-Burch et al. (2008) calculated three
types of metrics in ParaMetric. The manual word
alignments were used to calculate how well an
automatic paraphrasing technique is able to align
the paraphrases in a sentence pair. This measure
is limited to a class of paraphrasing techniques
that perform alignment (like MacCartney et al.
(2008)). Most methods produce a list of para-
phrases for a given input phrase. So Callison-
Burch et al. (2008) calculate two more gener-
ally applicable measures by comparing the para-
phrases in an automatically extracted resource to
gold standard paraphrases extracted via the align-
ments. These allow a lower-bound on precision
and relative recall to be calculated.
Liu et al. (2010) introduce the PEM metric as an
alternative to BLEU, since BLEU prefers iden-
tical paraphrases. PEM uses a second language
as a pivot to judge semantic equivalence. This re-
quires use of some bilingual data. Chen and Dolan
(2011) suggest using BLEU together with their
metric PINC, which uses n-grams to measure lex-
ical difference between paraphrases.
PARADIGM extends the ideas in ParaMetric
from lexical and phrasal paraphrasing techniques
to paraphrasing techniques that also generate syn-
tactic templates, such as Zhao et al. (2008), Cohn
and Lapata (2009), Madnani (2010) and Ganitke-
vitch et al. (2011). Instead of extracting gold stan-
dard paraphrases using techniques from phrase-
based machine translation, we use grammar ex-
traction techniques (Weese et al., 2011) to ex-
tract gold standard paraphrase grammar rules from
ParaMetric?s word-aligned sentential paraphrases.
Using these rules, we calculate the overlap be-
tween a gold standard paraphrase grammar and an
automatically generated paraphrase grammar.
Moreover, like ParaMetric, PARADIGM is able
to do further analysis on a restricted class of para-
phrasing models. In this case, PARADIGM evalu-
ates how well certain models are able to produce
synchronous parses of sentence pairs drawn from
monolingual parallel corpora. PARADIGM?s dif-
ferent metrics are explained in Section 4, but first
we give background on synchronous parsing and
synchronous grammars.
2.1 Synchronous parsing with SCFGs
Synchronous context-free grammars
An SCFG (Lewis and Stearns, 1968; Aho and
Ullman, 1972) is similar to a context-free gram-
mar, except that it generates pairs of strings
in correspondence. Each production rule in an
SCFG rewrites a non-terminal symbol as a pair of
phrases, which may have contain a mix of words
and non-terminals symbols. The grammar is syn-
chronous because both phrases in the pair must
have an identical set of non-terminals (though they
can come in different orders), and corresponding
non-terminals must be rewritten using the same
rule.
Much recent work in MT (and, by extension,
paraphrasing approaches that use MT machinery)
has been focused on choosing an appropriate set of
non-terminal symbols. The Hiero model (Chiang,
2007) used a single non-terminal symbolX . Other
approaches have read symbols from constituent
parses of the training data (Galley et al., 2004;
Galley et al., 2006; Zollmann and Venugopal,
2006). Labels based combinatory categorial gram-
mar (Steedman and Baldridge, 2011) have also
been used (Almaghout et al., 2010; Weese et al.,
2012).
Synchronous parsing
Wu (1997) introduced a parsing algorithm using
a variant of CKY. Dyer recently showed (2010)
193
an
d
h
i
m
i
m
p
e
a
c
h
t
o
w
a
n
t
s
o
m
e
.d
o
w
n
s
t
e
p
t
o
h
i
m
e
x
p
e
c
t
o
t
h
e
r
s
.
resign
to
him
want
others
while
,
him
impeach
to
propose
people
some
D
T
NP
V
B
P
V
B
P
R
P
C
C
N
N
S
V
B
P
P
R
P
V
B
P
R
T
VP
VP
S
VP
S
NP
VP
VP
NP
S
VP
S
S
.
Figure 1: PARADIGM extracts lexical, phrasal and
syntactic paraphrases from parsed, word-aligned
sentence pairs.
that the average parse time can be significantly im-
proved by using a two-pass algorithm.
The question of whether a source-reference pair
is reachable under a model must be addressed in
end-to-end discriminative training in MT (Liang
et al., 2006a; Gimpel and Smith, 2012). Auli et
al. (2009) showed that only approximately 30% of
training pairs are reachable under a phrase-based
model. This result is confirmed by our results in
paraphrasing.
3 Paraphrase grammar extraction
Like ParaMetric, PARADIGM extracts gold stan-
dard paraphrases from word-aligned sentential
paraphrases. PARADIGM goes further by parsing
one of the two input sentences, and uses the parse
tree to extract syntactic paraphrase rules, follow-
ing recent advances in syntactic approaches to ma-
chine translation (like Galley et al. (2004), Zoll-
mann and Venugopal (2006), and others). Figure 1
shows an example of a parsed sentence pair. From
that pair it is possible to extract a wide variety
of non-identical paraphrases, which include lexi-
cal paraphrases (single word synonyms), phrasal
paraphrases, and syntactic paraphrases that in-
clude a mix of words and syntactic non-terminal
CC? and while
VBP? want propose
VBP? expect want
DT? some some people
S? him to step down him to resign
VP? step down resign
VP? to step down to resign
VP? want to impeach him propose to impeach him
VP? want VP propose VP
VP? want to impeach PRP propose to impeach PRP
VP? VBP him to step down VBP him to resign
S? PRP to step down PRP to resign
Figure 2: Four examples each of lexical, phrasal,
and syntactic paraphrases that can be extracted
from the sentence pair in Figure 1.
symbols. Figure 2 shows a set of four examples
for each type that can be extracted from Figure 1.
These rules are formulated as SCFG rules,
with a syntactic left-hand nonterminal symbol
and two English right-hand sides representing the
paraphrase. The examples above include non-
terminal symbols that represent whole syntac-
tic constituents. It is also possible to create
more complex non-terminal symbols that describe
CCG-like non-constituent phrases. For example,
we could extract a rule like
S/VP? <NNS want him to, NNS expect him to>
Using constituents only, we are able to ex-
tract 45 paraphrase rules from Figure 1. Adding
CCG-style slashed constituents yields 66 addi-
tional rules.
4 PARADIGM: Evaluating paraphrase
grammars
By considering a paraphrase model as a syn-
chronous context-free grammar, we propose to
measure the model?s goodness using the following
criteria:
1. What percentage of sentential paraphrases
are reachable under the model? That is, given
a collection of sentence pairs (a
i
, b
i
) and an
SCFG G, where each pair of a and b are sen-
tential paraphrases, how many of the pairs are
in the language of G? We evaluate this by
producing a synchronous parse for the pairs,
as shown in Figure 3.
2. Given a collection of gold-standard para-
phrase rules, how many of those paraphrases
exist as rules in G? To calculate this, we
look at the overlap of grammars (described in
194
of the 
twelve cartoons
insulting
mohammad
CD
NNS
JJ
NP
NP
VP
NP
12 the islamic prophet
CD
NNS
JJ
NP
NP
VP
NP
cartoons offensivethat were to sparked riots
violent unrest was caused by
NP
NPVBD
VBD
VP
VP
S
S
Figure 3: We measure the goodness of paraphrase
grammars by determine how often they can be
used to synchronously parse gold-standard sen-
tential paraphrases. Note we do not require the
synchronous derivation to match a gold-standard
parse tree.
Section 4.2 below), examining different cate-
gories of rules and thresholding based on how
frequently the rule was used in the gold stan-
dard data.
These criteria correspond to properties that we
think are desirable in paraphrase models. They
also have the advantage that they do not depend
on human judgments and so can be calculated au-
tomatically.
4.1 Synchronous parse coverage
Paraphrase grammars should be able to explain
sentential paraphrases. For example, Figure
3 shows a sentence pair that is synchronously
parseable by one paraphrase grammar. In general,
we say that the more such sentence pairs that a
paraphrase grammar can synchronously parse, the
better it is.
The synchronous derivation allows us to draw
inferences about parts of the sentence pair that are
in correspondence; for instance, in Figure 3, vi-
olent unrest corresponds to riots and mohammad
corresponds to the islamic prophet.
4.2 Grammar overlap defined
We measure grammar overlap by comparing the
sets of production rules for two different gram-
mars. If the grammars contain rules that are equiv-
alent, the equivalent rules are in the grammars?
overlap.
We consider two types of overlapping, which
we will call strict and non-strict overlap. For strict
overlap, we say that two rules are equivalent if
they are identical, that is, if they have the same
left-hand side non-terminal symbol, their source
sides are identical strings, and their target sides are
identical strings. (This includes identical indexing
on non-terminal symbols on the right hand sides
of the rule.)
To calculate non-strict overlap, we ignore the
identities of non-terminal symbols in the left-hand
and right-hand sides of the rules. That is, two rules
are considered equivalent if they are identical after
all the non-terminal symbols have been replaced
by one equivalent symbol.
For example, in non-strict overlap, the syntactic
rule
NP ? ?N
1
?s N
2
; the N
2
of N
1
?
would match the Hiero rule
X ? ?X
1
?s X
2
; the X
2
of X
1
?
If we are considering two Hiero grammars,
strict and non-strict intersection are the same op-
eration since they only have on non-terminal X .
4.3 Precision lower bound and relative recall
Callison-Burch et al. (2008) use the notion of over-
lap between two paraphrase sets to define two met-
rics, precision lower bound and relative recall.
These are calculated the same way as standard
precision and recall. Relative recall is qualified
as ?relative? because it is calculated on a poten-
tially incomplete set of gold standard paraphrases.
There may exist valid paraphrases that do not oc-
cur in that set. Similarly, only a lower bound on
precision can be calculated because the candidate
set may contain valid paraphrases that do not oc-
cur in the gold standard set.
5 Experiments
5.1 Data
We extracted paraphrase grammars from a vari-
ety of different data sources, including four collec-
tions of sentential paraphrases. These included:
? Multiple translation corpora that were
compiled by the Linguistics Data Consortium
(LDC) for the purposes of evaluating ma-
chine translation quality with the BLEU met-
ric. We collected eight LDC corpora that all
have multiple English translations.
1
1
LDC Catalog numbers LDC2002T01, LDC2005T05,
LDC2010T10, LDC2010T11, LDC2010T12, LDC2010T14,
LDC2010T17, and LDC2010T23.
195
sentence total
Corpus pairs words
LDC Multiple Translations 83,284 2,254,707
Classic French Literature 75,106 682,978
MSR Paraphrase Corpus 5,801 219,492
ParaMetric 970 21,944
Table 1: Amount of English?English parallel data.
LDC data has 4 parallel translations per sentence.
Literature data is from Barzilay and McKeown
(2001). MSR data is from Quirk et al. (2004)
and Dolan et al. (2004). ParaMertic data is from
Callison-Burch et al. (2008).
? Classic French Literature that were trans-
lated by different translators, and which were
compiled by Barzilay and McKeown (2001).
? The MSR Paraphrase corpus which con-
sists of sentence pairs drawn from compara-
ble news articles drawn from different web
sites in the same date rate. The sentence pairs
were aligned heuristically aligned and then
manually judged to be paraphrases.
? The ParaMetric data which consists of 900
manually word-aligned sentence pairs col-
lected by Cohn et al. (2008). 300 sentence
pairs were drawn from each of the 3 above
sources. We use this to extract the gold stan-
dard paraphrase grammar.
The size of the data from each source is summa-
rized in Table 1.
For each dataset, after tokenizing and normaliz-
ing, we parsed one sentence in each English pair
using the Berkeley constituency parser (Liang et
al., 2006b). We then obtained word-level align-
ments, either using GIZA++ (Och and Ney, 2000)
or, in the case of ParaMetric, using human annota-
tions.
We used the Thrax grammar extractor (Weese
et al., 2011) to extract Hiero-style and syntactic
SCFGs from the paraphrase data. In the syntac-
tic setting we allowed labeling of rules with ei-
ther constituent labels or CCG-style slashed cat-
egories. The size of the extracted grammars is
shown in Table 2.
We also used version 0.2 of the SCFG-based
paraphrase collection known as the ParaPhrase
DataBase or PPDB (Ganitkevitch et al., 2013).
The PPDB paraphrases were extracted using the
pivoting technique (Bannard and Callison-Burch,
Grammar Rules
LDC Hiero 52,784,462
Lit. Hiero 3,288,546
MSR Hiero 2,456,513
ParaMetric Hiero 584,944
LDC Syntax 23,978,477
Lit. Syntax 715,154
MSR Syntax 406,115
ParaMetric Syntax 317,772
PPDB-v0.2-small 1,292,224
PPDB-v0.2-large 9,456,356
PPDB-v0.2-xl 46,592,161
Table 2: Size of various paraphrase grammars.
Grammar freq. ? 1 freq. ? 2
ParaMetric Syntax 317,772 21,709
LDC Hiero 5,840 (1.8%) 416 (1.9%)
Lit. Hiero 6,152 (1.9%) 359 (1.7%)
MSR Hiero 10,012 (3.2%) 315 (1.5%)
LDC Syntax 48,833 (15.3%) 7,748 (35.6%)
Lit. Syntax 14,431 (4.5%) 1,960 (9.0%)
MSR Syntax 21,197 (6.7%) 2,053 (9.5%)
PPDB-v0.2-small 15,831 (5.0%) 5,673 (26.1%)
PPDB-v0.2-large 31,277 (9.8%) 8,245 (37.9%)
PPDB-v0.2-xl 47,720 (15.0%) 10,049 (46.2%)
Table 3: Size of strict overlap (number of rules and
% of the gold standard) of each grammar with a
syntactic grammar derived from ParaMetric. freq.
? 2 means we first removed all rules that ap-
peared only once from the ParaMetric grammar.
The number in parentheses shows the percentage
of ParaMetric rules that are present in the overlap.
2005) on bilingual parallel corpora containing
over 42 million sentence pairs.
The PPDB release includes a tool for pruning
the grammar to a smaller size by retaining only
high-precision paraphrases. We include PPDB
grammars for several different pruning settings in
our analysis.
5.2 Experimental setup
We calculated our two metrics for each of the
grammars listed in Table 2.
To perform synchronous parsing, we used the
Joshua decoder (Post et al., 2013), which includes
an implementation of Dyer?s two-pass parsing al-
gorithm (2010). After splitting the LDC data into
10 equal pieces, we trained paraphrase models on
nine-tenths of the data and parsed the other tenth.
Grammars trained from other sources (the MSR
corpus, French literature domain, and PPDB) were
also evaluated on the held-out tenth of LDC data.
196
Grammar freq. ? 1 freq. ? 2
ParaMetric Syntax 200,385 20,699
LDC Hiero 41,346 (20.6%) 5,323 (25.8%)
Lit. Hiero 36,873 (18.4%) 4,606 (22.3%)
MSR Hiero 58,970 (29.4%) 6,741 (32.6%)
LDC Syntax 37,231 (11.7%) 5,055 (24.5%)
Lit. Syntax 19,530 (9.7%) 3,121 (15.1%)
MSR Syntax 28,016 (14.0%) 3,564 (17.2%)
PPDB-v0.2-small 13,003 (6.5%) 3,661 (17.7%)
PPDB-v0.2-large 22,431 (11.2%) 4,837 (23.4%)
PPDB-v0.2-xl 31,294 (15.6%) 5,590 (27.0%)
Table 4: Size of non-strict overlap of each gram-
mar with the syntactic grammar derived from
ParaMetric. The number in parentheses shows the
percentage of ParaMetric rules that are present in
the overlap.
Grammar syntactic phrasal lexical
ParaMetric 238,646 73,320 5,806
LDC
Syn
36,375 (15%) 8,806 (12%) 3,652 (62%)
MSR
Syn
7,734 (3%) 11,254 (15%) 2,209 (38%)
PPDB-xl 40,822 (17%) 3,765 (5%) 3,142 (54%)
Table 5: Number of paraphrases of each type
in each grammar?s strict overlap with the syntac-
tic ParaMetric grammar. Numbers in parentheses
show the percentage of ParaMetric rules of each
type.
Note that the LDC data contains 4 independent
translations of each foreign sentence, giving 6 pos-
sible (unordered) paraphrase pairs. We evaluated
coverage in two ways (corresponding to the two
columns in Table 6): first, considering all possible
sentence pairs from the test data, how many were
able to be parsed?
Secondly, if we consider all the English sen-
tences that correspond to one foreign sentence,
how many foreign sentences had at least one pair
of English translations that could be parsed syn-
chronously?
For grammar overlap, we perform both strict
and non-strict calculations (see Section 4.2)
against a syntactic grammar derived from hand-
aligned ParaMetric data.
5.3 Grammar overlap results
In Table 5 we see a breakdown of the types of para-
phrases in the overlap for three of the models. Al-
though the PPDB-xl overlap is much larger than
the other two, about 80% of its rules are syntac-
tic transformations. The LDC and MSR models
have a much larger proportion of phrasal and lexi-
cal rules.
Next we will look at the grammar overlap num-
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 10 100 1k 10k 100k  0 0.04 0.08 0.12 0.16Precision Lower Bound Relative RecallNumber of rules RecallPrec.
Figure 4: Precision lower bound and relative recall
when overlapping different sizes of PPDB with the
syntactic ParaMetric grammar.
bers presented in Table 3 and Table 4.
Note the non-intuitive result that for some
grammars (notably PPDB), the non-strict overlap
is smaller than the strict overlap. This is because
rules with different non-terminals only count once
in the non-strict overlap; for example, in PPDB-
small,
NN?? answer ; reply ?
VB?? answer ; reply ?
count as separate entries when calculating strictly,
but when ignoring non-terminals, they count as
only one type of rule.
The fact that the non-strict overlaps are smaller
means that there must be many rules in PPDB that
are identical except for non-terminal labels.
5.4 Precision and recall results
Figure 4 shows relative recall and precision lower
bound calculated for various sizes of PPDB rela-
tive to the ParaMetric grammar. The x-axis rep-
resents the size of the grammar as we vary from
keeping only the most probable rules to including
less probable ones. Restricting to high probability
rules makes the grammar much smaller, resulting
in higher precision.
5.5 Synchronous parsing results
Table 6 shows the percentage of sentence pairs that
were reachable in a held-out portion of the LDC
multiple-translation data.
We find that a grammar trained on LDC data
vastly outperforms data from any other domain.
This is not surprising ? we shouldn?t expect a
model trained on French literature to be able to
197
Grammar % (all) % (any)
LDC Hiero 9.5 33.0
Lit. Hiero 1.8 9.6
MSR Hiero 1.7 9.2
LDC Syntax 9.1 30.2
Lit. Syntax 2.0 10.7
MSR Syntax 1.9 10.4
PM Syntax 1.7 9.8
PPDB-v0.2-small 1.8 3.3
PPDB-v0.2-large 2.5 4.5
PPDB-v0.2-xl 3.5 6.2
Table 6: Parse coverage on held-out LDC data.
The all column considers every possible sentential
paraphrase in the test set. The any column consid-
ers a sentence parsed if any of its paraphrases was
able to parsed.
handle some of the vocabulary found in news sto-
ries that were originally in Arabic or Chinese.
The PPDB data outperforms both French litera-
ture and MSR models if we look all possible sen-
tence pairs from test data (the column labeled ?all?
in the table). However, when we consider whether
any pair from a set of 4 translations can be trans-
lated, the PPDB models do not do as well. This
implies that PPDB tends to be able to reach many
pairs from the same set of translations, but there
are many translations that it cannot handle at all.
By contrast, the literature- and MSR-trained mod-
els can reach at least one pair from 10% of the
test examples, even though the absolute number
of pairs they can reach is lower.
5.6 Effects of grammar size and choice of
syntactic labels
Table 2 shows that the PPDB-derived grammars
are much larger than the syntactic models derived
from other domains. It may seem surprising that
they should perform worse, but adding more rules
to the grammar just by varying non-terminal labels
isn?t likely to help overall parse coverage. This
suggests a new pruning method: keep only the top
k label variations for each rule type.
If we compare the syntactic models to the Hi-
ero models trained from the same data, we see
that their overall reachability performance is not
very different. This implies that paraphrases can
be annotated with linguistic information without
necessarily hurting their ability to explain partic-
ular sentence pairs. Contrast this result, with, for
example, those of Koehn et al. (2003), showing
that restricting translation models to only syntac-
tic phrases hurts overall translation performance.
The comparable performance between Hiero and
syntactic models seems to hold regardless of do-
main.
6 Correlation with human judgments
To validate PARADIGM, we calculated its correla-
tion with human judgments of paraphrase quality
on the sentence compression text-to-text genera-
tion task, which has been used to evaluate para-
phrase grammars in previous research (Cohn and
Lapata, 2007; Zhao et al., 2009; Ganitkevitch et
al., 2011; Napoles et al., 2011). We created sen-
tence compression systems for five of the para-
phrase grammars described in Section 5.1. We fol-
lowed the methodology outlined by Ganitkevitch
et al. (2011) and did the following:
? Each paraphrase grammar was augmented
with an appropriate set of rule-level features
that capture information pertinent to the task.
In this case, the paraphrase rules were given
two additional features that shows how the
number of words and characters changed af-
ter applying the rule.
? Similarly to how the weights of the mod-
els are set using minimum error rate training
in statistical machine translation, the weights
for each of the paraphrase grammars using
the PRO tuning method (Hopkins and May,
2011).
? Instead of optimizing to the BLEU metric, as
is done in machine translation, we optimized
to PR
?
ECIS, a metric developed for sentence
compression that adapts BLEU so that it in-
cludes a ?verbosity penalty? (Ganitkevitch et
al., 2011) to encourage the compression sys-
tems to produce shorter output.
? We created a development set with sentence
compressions by selecting 1000 pairs of sen-
tences from the multiple translation corpus
where two English translations of the same
foreign sentences differed in each other by a
length ratio of 0.67?0.75.
? We decoded a test set of 1000 sentences us-
ing each of the grammars and its optimized
198
weights with the Joshua decoder (Ganitke-
vitch et al., 2012). The selected in the same
fashion as the dev sentences, so each one had
a human-created reference compression.
We conducted a human evaluation to judge the
meaning and grammaticality of the sentence com-
pressions derived from each paraphrase grammar.
We presented workers on Mechanical Turk with
the input sentence to the compression sentence
(the long sentence), along with 5 shortened out-
puts from our compression systems. To ensure
that workers were producing reliable judgments
we also presented them with a positive control (a
reference compression written by a person) and a
negative controls (a compressed output that was
generated by randomly deleted words). We ex-
cluded judgments from workers who did not per-
form well on the positive and negative controls.
Meaning and grammaticality were scored on
5-point scales where 5 is best. These human
scores were averaged over 2000 judgments (1000
sentences x 2 annotators) for each system. The
systems? outputs were then scored with BLEU,
PR
?
ECIS, and their paraphrase grammars were
scored PARADIGM?s relative recall and precision
lower-bound estimates. For each grammar, we
also calculated the average length of parseable
sentences.
We calculated the correlation between the hu-
man judgements and the automatic scores, using
Spearman?s rank correlation coefficient ?. This
is methodology is the same that is used to quan-
tify the goodness of automatic evaluation metrics
in the machine translation literature (Przybocki et
al., 2008; Callison-Burch et al., 2010). The pos-
sible values of ? range between 1 (where all sys-
tems are ranked in the same order) and ?1 (where
the systems are ranked in the reverse order). Thus
an automatic evaluation metric with a higher abso-
lute value for ? is making predictions that are more
similar to the human judgments than an automatic
evaluation metric with a lower absolute ?.
Table 7 shows that our PARADIGM scores cor-
relate more highly with human judgments than ei-
ther BLEU or PR
?
ECIS for the 5 systems in our eval-
uation. This suggests that it may be a better predic-
tor of the goodness of paraphrase grammars than
MT metrics, when the paraphrase grammars are
used for text-to-text generation tasks.
MEANING GRAMMAR
BLEU -0.7 -0.1
PR
?
ECIS -0.6 +0.2
PINC +0.1 +0.4
PARADIGM
precision
+0.6 +0.1
PARADIGM
recall
+0.1 +0.4
PARADIGM
avg?len
-0.3 +0.4
Table 7: The correlation (Spearman?s ?) of dif-
ferent automatic evaluation metrics with human
judgments of paraphrase quality for the text-to-
text generation task of sentence compression.
7 Summary
We have introduced two new metrics for evaluat-
ing paraphrase grammars, and looked at several
models from a variety of domains. Using these
metrics we can perform a variety of analyses about
SCFG-based paraphrase models:
? Automatically-extracted grammars can parse
a small fraction of held-out data (?30%).
This is comparable to results in MT (Auli et
al., 2009).
? In-domain training data is necessary in or-
der to parse held-out data. A model trained
on newswire data parsed 30% of held-out
newswire sentence pairs, versus to <10% for
literature or parliamentary data.
? SCFGs with syntactic labels perform just as
well as simpler models with a single non-
terminal label.
? Automatically-extracted syntactic grammars
tend to have a reasonable overlap with gram-
mars derived from human-aligned data, in-
cluding more 45% of the gold-standard gram-
mar?s paraphrase rules that occurred at least
twice.
? We showed that PARADIGM more strongly
correlates with human judgments of the
meaning and grammaticality of paraphrases
produced by sentence compression systems
than standard automatic evaluation measures
like BLEU.
PARADIGM will help researchers developing
paraphrase resources to perform similar diagnos-
tics on their models, and quickly evaluate their
systems.
199
Acknowledgements
This material is based on research sponsored by
the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling. Pren-
tice Hall.
Hala Almaghout, Jie Jiang, and Andy Way. 2010.
CCG augmented hierarchical phrase-based machine
translation. In Proc. of IWSLT.
Michael Auli, Adam Lopez, Hieu Hoang, and Philipp
Koehn. 2009. A systematic analysis of translation
model search spaces. In Proc. WMT.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of ACL.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proc. of ACL.
Chris Callison-Burch, Trevor Cohn, and Mirella Lap-
ata. 2008. ParaMetric: An automatic evaluation
metric for paraphrasing. In Proc. of COLING.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Fourth Workshop
on Statistical Machine Translation (WMT10).
David L. Chen and William Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proc. of ACL.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Mirella Lapata. 2007. Large mar-
gin synchronous generation and its application to
sentence compression. In Proceedings of EMNLP-
CoLing.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial
Intelligence Research (JAIR), 34:637?674.
Trevor Cohn, Chris Callison-Burch, and Mirella Lap-
ata. 2008. Constructing corpora for the develop-
ment and evaluation of paraphrase systems. Com-
putational Linguistics, 34(4).
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrases cor-
pora: Exploiting massively parallel news sources. In
Proc. of COLING.
Mark Dras. 1997. Representing paraphrases using
synchronous tree adjoining grammars. In Proceed-
ings of the 35th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 516?518,
Madrid, Spain, July. Association for Computational
Linguistics.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL 2004: Main Proceedings, pages
273?280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve Deneefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
of ACL, pages 961?968.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learn-
ing sentential paraphrases from bilingual parallel
corpora for text-to-text generation. In Proceedings
of EMNLP.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283?291, Montr?eal, Canada, June. As-
sociation for Computational Linguistics.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proc. NAACL.
Kevin Gimpel and Noah A. Smith. 2012. Structured
ramp loss minimization for machine translation. In
Proc. of NAACL.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Ali Ibrahim, Boris Katz, and Jimmy Lin. 2003. Ex-
tracting structural paraphrases from aligned mono-
lingual corpora. In Proc. of the Second International
Workshop on Paraphrasing.
200
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings
of EMNLP.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Morristown, NJ, USA.
Association for Computational Linguistics.
Philip M. Lewis and Richard E. Stearns. 1968.
Syntax-directed transduction. Journal of the ACM,
15(3):465?488.
Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proc. of
ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 104?111, New
York City, USA, June. Association for Computa-
tional Linguistics.
Dekang Lin and Patrick Pantel. 2001. Discovery of
inference rules from text. Natural Language Engi-
neering, 7(3):343?360.
Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2010. PEM: a paraphrase evaluation metric exploit-
ing parallel texts. In Proc. of EMNLP.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model
for natural language inference. In Proceedings of
the 2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 802?811, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. the-
sis, Department of Computer Science, University of
Maryland College Park.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence com-
pression: Pitfalls and suggested remedies. In Pro-
ceedings of the Workshop on Monolingual Text-To-
Text Generation, pages 91?97, Portland, Oregon,
June. Association for Computational Linguistics.
Franz Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440?447, Hong Kong,
China, October.
Matt Post, Juri Ganitkevitch, Luke Orland, Jonathan
Weese, Yuan Cao, and Chris Callison-Burch. 2013.
Joshua 5.0: Sparser, better, faster, server. In Proc. of
WMT.
Mark Przybocki, Kay Peterson, and Sebastian Bron-
sart. 2008. Official results of the NIST 2008 ?Met-
rics for MAchine TRanslation? challenge (Metrics-
MATR08). In AMTA-2008 workshop on Metrics for
Machine Translation.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monlingual machine translation for paraphrase gen-
eration. In Proc. of EMNLP.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Mark Steedman and Jason Baldridge. 2011. Combi-
natory categorial grammar. In Robert Borsley and
Kersti B?orjars, editors, Non-Transformational Syn-
tax. Wiley-Blackwell.
Idan Szpektor, Eyal Shnarch, and Ido Dagan. 2007.
Instance-based evaluation of entailment rule acqui-
sition. In Proc. of ACL.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the thrax
grammar extractor. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
478?484, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Jonathan Weese, Chris Callison-Burch, and Adam
Lopez. 2012. Using categorial grammar to label
translation rules. In Proc. of WMT.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?404.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using para-
phrases to evaluate summaries automatically. In
Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation, pages 138?141, New York City,
June. Association for Computational Linguistics.
201
Proceedings of NAACL-HLT 2013, pages 758?764,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
PPDB: The Paraphrase Database
Juri Ganitkevitch1 Benjamin Van Durme1,2 Chris Callison-Burch2,3
1Center for Language and Speech Processing, Johns Hopkins University
2Human Language Technology Center of Excellence, Johns Hopkins University
3Computer and Information Science Department, University of Pennsylvania
Abstract
We present the 1.0 release of our para-
phrase database, PPDB. Its English portion,
PPDB:Eng, contains over 220 million para-
phrase pairs, consisting of 73 million phrasal
and 8 million lexical paraphrases, as well as
140 million paraphrase patterns, which cap-
ture many meaning-preserving syntactic trans-
formations. The paraphrases are extracted
from bilingual parallel corpora totaling over
100 million sentence pairs and over 2 billion
English words. We also release PPDB:Spa, a
collection of 196 million Spanish paraphrases.
Each paraphrase pair in PPDB contains a
set of associated scores, including paraphrase
probabilities derived from the bitext data and a
variety of monolingual distributional similar-
ity scores computed from the Google n-grams
and the Annotated Gigaword corpus. Our re-
lease includes pruning tools that allow users to
determine their own precision/recall tradeoff.
1 Introduction
Paraphrases, i.e. differing textual realizations of the
same meaning, have proven useful for a wide vari-
ety of natural language processing applications. Past
paraphrase collections include automatically derived
resources like DIRT (Lin and Pantel, 2001), the
MSR paraphrase corpus and phrase table (Dolan
et al, 2004; Quirk et al, 2004), among others.
Although several groups have independently ex-
tracted paraphrases using Bannard and Callison-
Burch (2005)?s bilingual pivoting technique (see
Zhou et al (2006), Riezler et al (2007), Snover et
al. (2010), among others), there has never been an
official release of this resource.
In this work, we release version 1.0 of the Para-
Phrase DataBase PPDB,1 a collection of ranked En-
glish and Spanish paraphrases derived by:
? Extracting lexical, phrasal, and syntactic para-
phrases from large bilingual parallel corpora
(with associated paraphrase probabilities).
? Computing distributional similarity scores for
each of the paraphrases using the Google n-
grams and the Annotated Gigaword corpus.
In addition to the paraphrase collection itself, we
provide tools to filter PPDB to only retain high pre-
cision paraphrases, scripts to limit the collection to
phrasal or lexical paraphrases (synonyms), and soft-
ware that enables users to extract paraphrases for
languages other than English.
2 Extracting Paraphrases from Bitexts
To extract paraphrases we follow Bannard and
Callison-Burch (2005)?s bilingual pivoting method.
The intuition is that two English strings e1 and e2
that translate to the same foreign string f can be as-
sumed to have the same meaning. We can thus pivot
over f and extract ?e1, e2? as a pair of paraphrases,
as illustrated in Figure 1. The method extracts a di-
verse set of paraphrases. For thrown into jail, it ex-
tracts arrested, detained, imprisoned, incarcerated,
jailed, locked up, taken into custody, and thrown
into prison, along with a set of incorrect/noisy para-
phrases that have different syntactic types or that are
due to misalignments.
For PPDB, we formulate our paraphrase collec-
tion as a weighted synchronous context-free gram-
mar (SCFG) (Aho and Ullman, 1972; Chiang, 2005)
1Freely available at http://paraphrase.org.
758
... f?nf Landwirte , weil
... 5 farmers were in Ireland ...
...
oder wurden , gefoltert
or have been , tortured
festgenommen 
thrown into jail
festgenommen
imprisoned
...
... ...
...
Figure 1: Phrasal paraphrases are extracted via bilingual
pivoting.
with syntactic nonterminal labels, similar to Cohn
and Lapata (2008) and Ganitkevitch et al (2011).
An SCFG rule has the form:
r
def= C ? ?f, e,?, ~??,
where the left-hand side of the rule,C, is a nontermi-
nal and the right-hand sides f and e are strings of ter-
minal and nonterminal symbols. There is a one-to-
one correspondence, ?, between the nonterminals
in f and e: each nonterminal symbol in f has to
also appear in e. Following Zhao et al (2008), each
rule r is annotated with a vector of feature functions
~? = {?1...?N} which are combined in a log-linear
model (with weights ~?) to compute the cost of ap-
plying r:
cost(r) = ?
N?
i=1
?i log?i. (1)
To create a syntactic paraphrase grammar we
first extract a foreign-to-English translation gram-
mar from a bilingual parallel corpus, using tech-
niques from syntactic machine translation (Koehn,
2010). Then, for each pair of translation rules where
the left-hand side C and foreign string f match:
r1
def= C ? ?f, e1,?1, ~?1?
r2
def= C ? ?f, e2,?2, ~?2?,
we pivot over f to create a paraphrase rule rp:
rp
def= C ? ?e1, e2,?p, ~?p?,
with a combined nonterminal correspondency func-
tion ?p. Note that the common source side f im-
plies that e1 and e2 share the same set of nonterminal
symbols.
The paraphrase rules obtained using this method
are capable of making well-formed generalizations
of meaning-preserving rewrites in English. For
instance, we extract the following example para-
phrase, capturing the English possessive rule:
NP ? the NP1 of NNS 2 | the NNS2 ?s NP1.
The paraphrase feature vector ~?p is computed
from the translation feature vectors ~?1 and ~?2 by
following the pivoting idea. For instance, we esti-
mate the conditional paraphrase probability p(e2|e1)
by marginalizing over all shared foreign-language
translations f :
p(e2|e1) ?
?
f
p(e2|f)p(f |e1). (2)
3 Scoring Paraphrases Using Monolingual
Distributional Similarity
The bilingual pivoting approach anchors para-
phrases that share an interpretation because of a
shared foreign phrase. Paraphrasing methods based
on monolingual text corpora, like DIRT (Lin and
Pantel, 2001), measure the similarity of phrases
based on distributional similarity. This results in a
range of different types of phrases, including para-
phrases, inference rules and antonyms. For instance,
for thrown into prison DIRT extracts good para-
phrases like arrested, detained, and jailed. How-
ever, it also extracts phrases that are temporarily
or causally related like began the trial of, cracked
down on, interrogated, prosecuted and ordered the
execution of, because they have similar distribu-
tional properties. Since bilingual pivoting rarely ex-
tracts these non-paraphrases, we can use monolin-
gual distributional similarity to re-rank paraphrases
extracted from bitexts (following Chan et al (2011))
or incorporate a set of distributional similarity scores
as features in our log-linear model.
Each similarity score relies on precomputed dis-
tributional signatures that describe the contexts that
a phrase occurs in. To describe a phrase e, we gather
counts for a set of contextual features for each oc-
currence of e in a corpus. Writing the context vector
for the i-th occurrence of e as ~se,i, we can aggre-
gate over all occurrences of e, resulting in a distri-
butional signature for e, ~se =
?
i ~se,i. Following the
intuition that phrases with similar meanings occur in
759
the long-term
achieve25
goals 23
plans 97
investment 10
confirmed64
revise43 the long-term
the long-term
the long-term
the long-term
the long-term
.
.
.
.
L-achieve = 25
L-confirmed
= 64
L-revise = 43
?
R-goals 
= 23
R-plans  = 97
R-investment 
= 10
?
the long-term
?
=~sig
?
(a) The n-gram corpus records the long-term as preceded
by revise (43 times), and followed by plans (97 times). We
add corresponding features to the phrase?s distributional
signature retaining the counts of the original n-grams.
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
(b) Here, position-aware lexical and part-of-speech n-
gram features, labeled dependency links , and features
reflecting the phrase?s CCG-style label NP/NN are in-
cluded in the context vector.
Figure 2: Features extracted for the phrase the long term from the n-gram corpus (2a) and Annotated Gigaword (2b).
similar contexts, we can then quantify the goodness
of e? as a paraphrase of e by computing the cosine
similarity between their distributional signatures:
sim(e, e?) = ~se ? ~se?
|~se||~se? |
.
A wide variety of features have been used to de-
scribe the distributional context of a phrase. Rich,
linguistically informed feature-sets that rely on de-
pendency and constituency parses, part-of-speech
tags, or lemmatization have been proposed in work
such as by Church and Hanks (1991) and Lin and
Pantel (2001). For instance, a phrase is described by
the various syntactic relations such as: ?what verbs
have this phrase as the subject??, or ?what adjectives
modify this phrase??. Other work has used simpler
n-gram features, e.g. ?what words or bigrams have
we seen to the left of this phrase??. A substantial
body of work has focussed on using this type of
feature-set for a variety of purposes in NLP (Lapata
and Keller, 2005; Bhagat and Ravichandran, 2008;
Lin et al, 2010; Van Durme and Lall, 2010).
For PPDB, we compute n-gram-based context
signatures for the 200 million most frequent phrases
in the Google n-gram corpus (Brants and Franz,
2006; Lin et al, 2010), and richer linguistic signa-
tures for 175 million phrases in the Annotated Gi-
gaword corpus (Napoles et al, 2012). Our features
extend beyond those previously used in the work by
Ganitkevitch et al (2012). They are:
? n-gram based features for words seen to the left
and right of a phrase.
? Position-aware lexical, lemma-based, part-of-
speech, and named entity class unigram and bi-
gram features, drawn from a three-word win-
dow to the right and left of the phrase.
? Incoming and outgoing (wrt. the phrase) de-
pendency link features, labeled with the corre-
sponding lexical item, lemmata and POS.
? Syntactic features for any constituents govern-
ing the phrase, as well as for CCG-style slashed
constituent labels for the phrase.
Figure 2 illustrates the feature extraction for an ex-
ample phrase.
4 English Paraphrases ? PPDB:Eng
We combine several English-to-foreign bitext cor-
pora to extract PPDB:Eng: Europarl v7 (Koehn,
2005), consisting of bitexts for the 19 European lan-
guages, the 109 French-English corpus (Callison-
Burch et al, 2009), the Czech, German, Span-
ish and French portions of the News Commen-
tary data (Koehn and Schroeder, 2007), the United
Nations French- and Spanish-English parallel cor-
pora (Eisele and Chen, 2010), the JRC Acquis cor-
pus (Steinberger et al, 2006), Chinese and Arabic
760
Identity Paraphrases Total
Lexical 0.6M 7.6M 8.1M
Phrasal 4.9M 68.4M 73.2M
Syntactic 46.5M 93.6M 140.1M
All 52.0M 169.6M 221.4M
Table 1: A breakdown of PPDB:Eng size by paraphrase
type. We distinguish lexical (i.e. one-word) paraphrases,
phrasal paraphrases and syntactically labeled paraphrase
patterns.
newswire corpora used for the GALE machine trans-
lation campaign,2 parallel Urdu-English data from
the NIST translation task,3 the French portion of
the OpenSubtitles corpus (Tiedemann, 2009), and a
collection of Spanish-English translation memories
provided by TAUS.4
The resulting composite parallel corpus has more
than 106 million sentence pairs, over 2 billion En-
glish words, and spans 22 pivot languages. To ap-
ply the pivoting technique to this multilingual data,
we treat the various pivot languages as a joint Non-
English language. This simplifying assumption al-
lows us to share statistics across the different lan-
guages and apply Equation 2 unaltered.
Table 1 presents a breakdown of PPDB:Eng by
paraphrase type. We distinguish lexical (a single
word), phrasal (a continuous string of words), and
syntactic paraphrases (expressions that may con-
tain both words and nonterminals), and separate
out identity paraphrases. While we list lexical and
phrasal paraphrases separately, it is possible that a
single word paraphrases as a multi-word phrase and
vice versa ? so long they share the same syntactic
label.
5 Spanish Paraphrases ? PPDB:Spa
We also release a collection of Spanish paraphrases:
PPDB:Spa is extracted analogously to its English
counterpart and leverages the Spanish portions of the
bitext data available to us, totaling almost 355 mil-
lion Spanish words, in nearly 15 million sentence
pairs. The paraphrase pairs in PPDB:Spa are anno-
2http://projects.ldc.upenn.edu/gale/
data/Catalog.html
3LDC Catalog No. LDC2010T23
4http://www.translationautomation.com/
Identity Paraphrases Total
Lexical 1.0M 33.1M 34.1M
Phrasal 4.3M 73.2M 77.5M
Syntactic 29.4M 55.3M 84.7M
All 34.7M 161.6M 196.3M
Table 2: An overview of PPDB:Spa. Again, we parti-
tion the resource into lexical (i.e. one-word) paraphrases,
phrasal paraphrases and syntactically labeled paraphrase
patterns.
expect
NNS VBP
NP
VP
the data
NP VP
S
to show
JJ
economistsfew
......
S
...
RelArg0 Arg1
Figure 3: To inspect our coverage, we use the Penn
Treebank?s parses to map from Propbank annotations to
PPDB?s syntactic patterns. For the above annotation
predicate, we extract VBP ? expect, which is matched
by paraphrase rules like VBP ? expect | anticipate
and VBP ? expect | hypothesize. To search for
the entire relation, we replace the argument spans
with syntactic nonterminals. Here, we obtain S ?
NP expect S, for which PPDB has matching rules like
S ? NP expect S | NP would hope S, and S ?
NP expect S | NP trust S. This allows us to apply so-
phisticated paraphrases to the predicate while capturing
its arguments in a generalized fashion.
tated with distributional similarity scores based on
lexical features collected from the Spanish portion
of the multilingual release of the Google n-gram
corpus (Brants and Franz, 2009), and the Spanish
Gigaword corpus (Mendonca et al, 2009). Table 2
gives a breakdown of PPDB:Spa.
6 Analysis
To estimate the usefulness of PPDB as a resource
for tasks like semantic role labeling or parsing, we
analyze its coverage of Propbank predicates and
predicate-argument tuples (Kingsbury and Palmer,
2002). We use the Penn Treebank (Marcus et
al., 1993) to map Propbank annotations to patterns
which allow us to search PPDB:Eng for paraphrases
that match the annotated predicate. Figure 3 illus-
761
 1 3 5-30 -25 -20 -15 -10 -5  0Avg. Score Pruning Threshold 0 0.5 1-30 -25 -20 -15 -10 -5  0  0 50 100 150Coverage PP / Type
(a) PPDB:Eng coverage of Propbank predicates
(top), and average human judgment score (bottom)
for varying pruning thresholds.
 0 0.2 0.4 0.6 0.8 1-30 -25 -20 -15 -10 -5  0  0 20 40 60 80 100 120 140 160Coverage Paraphrases / TypePruning ThresholdRelation Tokens CoveredParaphrases / TypeRelation Types Covered
(b) PPDB:Eng?s coverage of Propbank predicates
with up to two arguments. Here we consider rules
that paraphrase the full predicate-argument expres-
sion.
Figure 4: An illustration of PPDB?s coverage of the manually annotated Propbank predicate phrases (4a) and binary
relations with argument non-terminals (4b). The curves indicate the coverage on tokens (solid) and types (dotted), as
well as the average number of paraphrases per covered type (dashed) at the given pruning level.
trates this mapping.
In order to quantify PPDB?s precision-recall
tradeoff in this context, we perform a sweep
over our collection, beginning with the full set of
paraphrase pairs and incrementally discarding the
lowest-scoring ones. We choose a simple estimate
for each paraphrase pair?s score by uniformly com-
bining its paraphrase probability features in Eq. 1.
The top graph in Figure 4a shows PPDB?s cover-
age of predicates (e.g. VBP ? expect) at the type
level (i.e. counting distinct predicates), as well as
the token level (i.e. counting predicate occurrences
in the corpus). We also keep track of average num-
ber of paraphrases per covered predicate type for
varying pruning levels. We find that PPDB has a
predicate type recall of up to 52% (accounting for
97.5% of tokens). Extending the experiment to full
predicate-argument relations with up to two argu-
ments (e.g. S ? NNS expect S), we obtain a 27%
type coverage rate that accounts for 40% of tokens
(Figure 4b). Both rates hold even as we prune the
database down to only contain high precision para-
phrases. Our pruning method here is based on a sim-
ple uniform combination of paraphrase probabilities
and similarity scores.
To gauge the quality of our paraphrases, the au-
thors judged 1900 randomly sampled predicate para-
phrases on a scale of 1 to 5, 5 being the best. The
bottom graph in Figure 4a plots the resulting human
score average against the sweep used in the cover-
age experiment. It is clear that even with a simple
weighing approach, the PPDB scores show a clear
correlation with human judgements. Therefore they
can be used to bias the collection towards greater re-
call or higher precision.
7 Conclusion and Future Work
We present the 1.0 release of PPDB:Eng and
PPDB:Spa, two large-scale collections of para-
phrases in English and Spanish. We illustrate the
resource?s utility with an analysis of its coverage of
Propbank predicates. Our results suggest that PPDB
will be useful in a variety of NLP applications.
Future releases of PPDB will focus on expand-
ing the paraphrase collection?s coverage with regard
to both data size and languages supported. Further-
more, we intend to improve paraphrase scoring by
incorporating additional sources of information, as
well as by better utilizing information present in the
data, like domain or topic. We will also address
points of refinement such as handling of phrase am-
biguity, and effects specific to individual pivot lan-
guages. Our aim is for PPDB to be a continuously
updated and improving resource.
Finally, we will explore extensions to PPDB to in-
clude aspects of related large-scale resources such as
lexical-semantic hierarchies (Snow et al, 2006), tex-
tual inference rules (Berant et al, 2011), relational
patterns (Nakashole et al, 2012), and (lexical) con-
ceptual networks (Navigli and Ponzetto, 2012).
762
Acknowledgements
We would like to thank Frank Ferraro for his Prop-
bank processing tools. This material is based
on research sponsored by the NSF under grant
IIS-1249516 and DARPA under agreement num-
ber FA8750-13-2-0017 (the DEFT program). The
U.S. Government is authorized to reproduce and dis-
tribute reprints for Governmental purposes. The
views and conclusions contained in this publication
are those of the authors and should not be interpreted
as representing official policies or endorsements of
DARPA or the U.S. Government.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Jonathan Berant, Jacob Goldberger, and Ido Dagan.
2011. Global learning of typed entailment rules. In
Proceedings of ACL.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Thorsten Brants and Alex Franz. 2009. Web 1T 5-gram,
10 european languages version 1. Linguistic Data
Consortium, Philadelphia.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In Pro-
ceedings of WMT, pages 1?28, Athens, Greece, March.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
EMNLP Workshop on GEMS.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 6(1):22?29.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Pro-
ceedings of the COLING.
Andreas Eisele and Yu Chen. 2010. MultiUN: A multi-
lingual corpus from united nation documents. In Pro-
ceedings of LREC, Valletta, Malta.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Monolingual distributional
similarity for text-to-text generation. In Proceedings
of *SEM. Association for Computational Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of LREC.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in domain adaptation for statistical machine transla-
tion. In Proceedings of WMT, Prague, Czech Repub-
lic, June. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1).
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: the Penn Treebank. Computational
Linguistics, 19(2).
Angelo Mendonca, David Andrew Graff, and Denise
DiPersio. 2009. Spanish Gigaword Second Edition.
Linguistic Data Consortium.
Ndapandula Nakashole, Gerhard Weikum, and Fabian
Suchanek. 2012. PATTY: a taxonomy of rela-
tional patterns with semantic types. In Proceedings
of EMNLP.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In Proceedings of EMNLP.
763
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of the 45th Annual Meeting of
the ACL.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. Ter-plus: paraphrase, se-
mantic, and alignment enhancements to translation
edit rate. Machine Translation, 23(2-3):117?127.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of the ACL/Coling.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and Da?niel
Varga. 2006. The JRC-Acquis: A multilingual
aligned parallel corpus with 20+ languages. In Pro-
ceedings of LREC, Genoa, Italy.
Jo?rg Tiedemann. 2009. News from OPUS: A collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume 5.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Shiqi Zhao, Cheng Niu, Ming Zhou, Ting Liu, and Sheng
Li. 2008. Combining multiple resources to improve
SMT-based paraphrasing model. In Proceedings of
ACL/HLT.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of HLT/NAACL.
764
Proceedings of the NAACL HLT 2013 Student Research Workshop, pages 62?68,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Large-Scale Paraphrasing for Natural Language Understanding
Juri Ganitkevitch
Center for Language and Speech Processing
Johns Hopkins University
juri@cs.jhu.edu
Abstract
We examine the application of data-driven
paraphrasing to natural language understand-
ing. We leverage bilingual parallel corpora
to extract a large collection of syntactic para-
phrase pairs, and introduce an adaptation
scheme that allows us to tackle a variety of
text transformation tasks via paraphrasing. We
evaluate our system on the sentence compres-
sion task. Further, we use distributional sim-
ilarity measures based on context vectors de-
rived from large monolingual corpora to anno-
tate our paraphrases with an orthogonal source
of information. This yields significant im-
provements in our compression system?s out-
put quality, achieving state-of-the-art perfor-
mance. Finally, we propose a refinement of
our paraphrases by classifying them into nat-
ural logic entailment relations. By extend-
ing the synchronous parsing paradigm towards
these entailment relations, we will enable our
system to perform recognition of textual en-
tailment.
1 Introduction
In this work, we propose an extension of current
paraphrasing methods to tackle natural language un-
derstanding problems. We create a large set of para-
phrase pairs in a data-driven fashion, rank them
based on a variety of similarity metrics, and attach
an entailment relation to each pair, facilitating nat-
ural logic inference. The resulting resource has po-
tential applications to a variety of NLP applications,
including summarization, query expansion, question
answering, and recognizing textual entailment.
Specifically, we build on Callison-Burch (2007)?s
pivot-based paraphrase extraction method, which
uses bilingual parallel data to learn English phrase
pairs that share the same meaning. Our approach ex-
tends the pivot method to learn meaning-preserving
syntactic transformations in English. We repre-
sent these using synchronous context-free grammars
(SCFGs). This representation allows us to re-use
a lot of machine translation machinery to perform
monolingual text-to-text generation. We demon-
strate the method on a sentence compression task
(Ganitkevitch et al, 2011).
To improve the system, we then incorporate fea-
tures based on monolingual distributional similar-
ity. This orthogonal source of signal allows us to
re-scores the bilingually-extracted paraphrases us-
ing information drawn from large monolingual cor-
pora. We show that the monolingual distributional
scores yield significant improvements over a base-
line that scores paraphrases only with bilingually-
extracted features (Ganitkevitch et al, 2012).
Further, we propose a semantics for paraphras-
ing by classifying each paraphrase pair with one
of the entailment relation types defined by natural
logic (MacCartney, 2009). Natural logic is used
to perform inference over pairs of natural language
phrases, like our paraphrase pairs. It defines a set of
relations including, equivalence (?), forward- and
backward-entailments (@, A), antonyms (?), and
others. We will build a classifier for our paraphrases
that uses features extracted from annotated resources
like WordNet and distributional information gath-
ered over large text corpora to assign one or more
entailment relations to each paraphrase pair. We will
evaluate the entailment assignments by applying this
enhanced paraphrasing system to the task of recog-
nizing textual entailment (RTE).
2 Extraction of Syntactic Paraphrases
from Bitexts
A variety of different types of corpora have been
used to automatically induce paraphrase collections
for English (see Madnani and Dorr (2010) for a sur-
62
... f?nf Landwirte , weil
... 5 farmers were in Ireland ...
...
oder wurden , gefoltert
or have been , tortured
festgenommen 
thrown into jail
festgenommen
imprisoned
...
... ...
...
Figure 1: An example of pivot-based phrasal paraphrase
extraction ? we assume English phrases that translate to
a common German phrase to be paraphrases. Thus we
extract ?imprisoned? as a paraphrase of ?thrown into jail.?
vey of these methods). Bannard and Callison-Burch
(2005) extracted phrasal paraphrases from bitext by
using foreign language phrases as a pivot: if two
English phrases e1 and e2 both translate to a for-
eign phrase f , they assume that e1 and e2 are para-
phrases of one another. Figure 1 gives an example
of a phrasal paraphrase extracted by Bannard and
Callison-Burch (2005).
Since ?thrown into jail? is aligned to multiple
German phrases, and since each of those German
phrases align back to a variety of English phrases,
the method extracts a wide range of possible para-
phrases including good paraphrase like: imprisoned
and thrown into prison. It also produces less good
paraphrases like: in jail and put in prison for, and
bad paraphrases, such as maltreated and protec-
tion, because of noisy/inaccurate word alignments
and other problems. To rank these, Bannard and
Callison-Burch (2005) derive a paraphrase probabil-
ity p(e1|e2):
p(e2|e1) ?
?
f
p(e2|f)p(f |e1), (1)
where the p(ei|f) and p(f |ei) are translation proba-
bilities estimated from the bitext (Brown et al, 1990;
Koehn et al, 2003).
We extend this method to extract syntactic para-
phrases (Ganitkevitch et al, 2011). Table 1
shows example paraphrases produced by our sys-
tem. While phrasal systems memorize phrase pairs
without any further generalization, a syntactic para-
phrasing system can learn more generic patterns.
These can be better applied to unseen data. The
paraphrases implementing the possessive rule and
Possessive rule
NP ? the NN of the NNP the NNP ?s NN
NP ? the NP made by NN the NN ?s NP
Dative shift
VP ? give NN to NP give NP the NN
VP ? provide NP1 to NP2 give NP2 NP1
Partitive constructions
NP ? CD of the NN CD NN
NP ? all NN all of the NN
Reduced relative clause
SBAR/S ? although PRP VBP that although PRP VBP
ADJP ? very JJ that S JJ S
Table 1: A selection of example paraphrase patterns ex-
tracted by our system. These rules demonstrate that, us-
ing the pivot approach from Figure 1, our system is capa-
ble of learning meaning-preserving syntactic transforma-
tions in English.
the dative shift shown in Table 1 are good examples
of this: the two noun-phrase arguments to the ex-
pressions are abstracted to nonterminals while each
rule?s lexicalization provides an appropriate frame
of evidence for the transform.
2.1 Formal Representation
In this proposal we focus on a paraphrase
model based on synchronous context-free gram-
mar (SCFG). The SCFG formalism (Aho and Ull-
man, 1972) was repopularized for statistical ma-
chine translation by (Chiang, 2005). An probabilis-
tic SCFG G contains rules r of the form r = C ?
??, ?,?, w?. A rule r?s left-hand side C is a nonter-
minal, while its right-hands sides ? and ? can be
mixed strings of words and nonterminal symbols.
There is a one-to-one correspondency between the
nonterminals in ? and ?. Each rule is assigned a
cost wr ? 0, reflecting its likelihood.
To compute the cost wr of the application of a
rule r, we define a set of feature functions ~? =
{?1...?N} that are combined in a log-linear model.
The model weights are set to maximize a task-
dependent objective function.
2.2 Syntactic Paraphrase Rules via Bilingual
Pivoting
Our paraphrase acquisition method is based on the
extraction of syntactic translation rules in statistical
machine translation (SMT). In SMT, SCFG rules are
extracted from English-foreign sentence pairs that
are automatically parsed and word-aligned. For a
63
CR Meaning Grammar
Reference 0.80 4.80 4.54
ILP 0.74 3.44 3.41
PP 0.78 3.53 2.98
PP + n-gram 0.80 3.65 3.16
PP + syntax 0.79 3.70 3.26
Random Deletions 0.78 2.91 2.53
Table 2: Results of the human evaluation on longer com-
pressions: pairwise compression ratios (CR), meaning
and grammaticality scores. Bold indicates a statistically
significant best result at p < 0.05. The scores range from
1 to 5, 5 being perfect.
foreign phrase the corresponding English phrase is
found via the word alignments. This phrase pair
is turned into an SCFG rule by assigning a left-
hand side nonterminal symbol, corresponding to
the syntactic constituent that dominates the English
phrase. To introduce nonterminals into the right-
hand sides of the rule, we can replace correspond-
ing sub-phrases in the English and foreign phrases
with nonterminal symbols. Doing this for all sen-
tence pairs in a bilingual parallel corpus results in a
translation grammar that serves as the basis for syn-
tactic machine translation.
To create a paraphrase grammar from a transla-
tion grammar, we extend the syntactically informed
pivot approach of (Callison-Burch, 2008) to the
SCFG model: for each pair of translation rules r1
and r2 with matching left-hand side nonterminal C
and foreign language right-hand side ?: r1 = C ?
??, ?1,?1, ~?1? and r2 = C ? ??, ?2,?2, ~?2?,
we pivot over ? and create a paraphrase rule rp:
rp = C ? ??1, ?2,?, ~??. We estimate the cost
for rp following Equation 1.
2.3 Task-Based Evaluation
Sharing its SCFG formalism permits us to re-use
much of SMT?s machinery for paraphrasing appli-
cations, including decoding and minimum error rate
training. This allows us to easily tackle a variety of
monolingual text-to-text generation tasks, which can
be cast as sentential paraphrasing with task-specific
constraints or goals.
For our evaluation, we apply our paraphrase sys-
tem to sentence compression. However, to success-
fully use paraphrases for sentence compression, we
need to adapt the system to suit the task. We intro-
duce a four-point adaptation scheme for text-to-text
twelve
cartoons insulting the prophet mohammad
CD NNS JJ
DT
NNP
NP
NP
VP
NP
DT+NNP
12
the prophet mohammad
CD NNS JJ DT NNP
NP
NP
VP
NP
DT+NNP
cartoons offensiveof the that are to
Figure 2: An example of a synchronous paraphrastic
derivation in sentence compression.
generation via paraphrases, suggesting:
? The use task-targeted features that capture in-
formation pertinent to the text transformation.
For sentence compression the features include
word count and length-difference features.
? An objective function that takes into account
the contraints imposed by the task. We use
PRE?CIS, an augmentation of the BLEU metric,
which introduces a verbosity penalty.
? Development data that represents the precise
transformations we seek to model. We use a set
of human-made example compressions mined
from translation references.
? Optionally, grammar augmentations that allow
for the incorporation of effects that the learned
paraphrase grammar cannot capture. We exper-
imented with automatically generated deletion
rules.
Applying the above adaptations to our generic para-
phraser (PP), quickly yields a sentence compression
system that performs on par with a state-of-the-art
integer linear programming-based (ILP) compres-
sion system (Clarke and Lapata, 2008). As Table 2
shows, human evaluation results suggest that our
system outperforms the contrast system in meaning
retention. However, it suffers losses in grammatical-
ity. Figure 2 shows an example derivation produced
as a result of applying our paraphrase rules in the
decoding process.
3 Integrating Monolingual Distributional
Similarity into Bilingually Extracted
Paraphrases
Distributional similarity-based methods (Lin and
Pantel, 2001; Bhagat and Ravichandran, 2008) rely
64
on the assumption that similar expressions appear
in similar contexts ? a signal that is orthogonal to
bilingual pivot information we have considered thus
far. However, the monolingual distributional signal
is noisy: it suffers from problems such as mistaking
cousin expressions or antonyms (such as ?rise, fall?
or ?boy , girl?) for paraphrases. We circumvent this
issue by starting with a paraphrase grammar ex-
tracted from bilingual data and reranking it with in-
formation based on distributional similarity (Gan-
itkevitch et al, 2012).
3.1 Distributional Similarity
In order to compute the similarity of two expressions
e1 and e2, their respective occurrences across a cor-
pus are aggregated in context vectors ~c1 and ~c2. The
~ci are typically vectors in a high-dimensional fea-
ture space with features like counts for words seen
within a window of an ei. For parsed data more so-
phisticated features based on syntax and dependency
structure around an occurrence are possible. The
comparison of e1 and e2 is then made by comput-
ing the cosine similarity between ~c1 and ~c2.
Over large corpora the context vectors for even
moderately frequent ei can grow unmanageably
large. Locality sensitive hashing provides a way of
dealing with this problem: instead of retaining the
explicit sparse high-dimensional ~ci, we use a ran-
dom projection h(?) to convert them into compact bit
signatures in a dense b-dimensional boolean space
in which approximate similarity calculation is pos-
sible.
3.2 Integrating Similarity with Syntactic
Paraphrases
In order to incorporate distributional similarity in-
formation into the paraphrasing system, we need
to calculate similarity scores for the paraphrastic
SCFG rules in our grammar. For rules with purely
lexical right-hand sides e1 and e2 this is a simple
task, and the similarity score sim(e1, e2) can be di-
rectly included in the rule?s feature vector ~?. How-
ever, if e1 and e2 are long, their occurrences be-
come sparse and their similarity can no longer be
reliably estimated. In our case, the right-hand sides
of our rules also contain non-terminal symbols and
re-ordered phrases, so computing a similarity score
is not straightforward.
the long-term
achieve25
goals 23
plans 97
investment 10
confirmed64
revise43 the long-term
the long-term
the long-term
the long-term
the long-term
.
.
.
.
L-achieve = 25
L-confirmed
= 64
L-revise = 43
?
R-goals 
= 23
R-plans  = 97
R-investment 
= 10
?
the long-term
?
=~sig
?
Figure 3: An example of the n-gram feature extraction
on an n-gram corpus. Here, ?the long-term? is seen pre-
ceded by ?revise? (43 times) and followed by ?plans? (97
times).
Our solution is to decompose the discontinuous
patterns that make up the right-hand sides of a rule r
into pairs of contiguous phrases, for which we then
look up distributional signatures and compute sim-
ilarity scores. To avoid comparing unrelated pairs,
we require the phrase pairs to be consistent with a to-
ken alignment a, defined and computed analogously
to word alignments in machine translation.
3.3 Data Sets and Types of Distributional
Signatures
We investigate the impact of the data and feature set
used to construct distributional signatures. In partic-
ular we contrast two approaches: a large collection
of distributional signatures with a relatively simple
feature set, and a much smaller set of signatures with
a rich, syntactically informed feature set.
The larger n-gram model is drawn from a web-
scale n-gram corpus (Brants and Franz, 2006; Lin et
al., 2010). Figure 3 illustrates this feature extraction
approach. The resulting collection comprises distri-
butional signatures for the 200 million most frequent
1-to-4-grams in the n-gram corpus.
For the syntactically informed model, we use
the constituency and dependency parses provided
in the Annotated Gigaword corpus (Napoles et al,
2012). Figure 4 illustrates this model?s feature ex-
traction for an example phrase occurrence. Using
this method we extract distributional signatures for
over 12 million 1-to-4-gram phrases.
3.4 Evaluation
For evaluation, we follow the task-based approach
taken in Section 2 and apply the similarity-scored
65
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 4: An example of the syntactic feature-set. The
phrase ?the long-term? is annotated with position-aware
lexical and part-of-speech n-gram features, labeled de-
pendency links, and features derived from the phrase?s
CCG label (NP/NN ).
paraphrases to sentence compression. The distri-
butional similarity scores are incorporated into the
paraphrasing system as additional rule features into
the log-linear model. The task-targeted parameter
tuning thus results in a reranking of the rules that
takes into consideration, the distributional informa-
tion, bilingual alignment-based paraphrase probabil-
ities, and compression-centric features.
Table 2 shows comparison of the bilingual base-
line paraphrase grammar (PP), the reranked gram-
mars based on signatures extracted from the Google
n-grams (n-gram), the richer signatures drawn from
Annotated Gigaword (Syntax), and Clarke and La-
pata (2008)?s compression system (ILP). In both
cases, the inclusion of distributional similarity in-
formation results in significantly better output gram-
maticality and meaning retention. Despite its lower
coverage (12 versus 200 million phrases), the syn-
tactic distributional similarity outperforms the sim-
pler Google n-gram signatures.
3.5 PPDB
To facilitate a more widespread use of paraphrases,
we release a collection of ranked paraphrases ob-
tained by the methods outlined in Sections 2 and 3
to the public (Ganitkevitch et al, 2013).
4 Paraphrasing with Natural Logic
In the previously derived paraphrase grammar it is
assumed that all rules imply the semantic equiva-
lence of two textual expressions. The varying de-
grees of confidence our system has in this relation-
ship are evidenced by the paraphrase probabilities
and similarity scores. However, the grammar can
also contain rules that in fact represent a range of se-
mantic relationships, including hypernym- hyponym
relationships, such as India ? this country.
To better model such cases we propose an anno-
tation of each paraphrase rule with explicit relation
labels based on natural logic. Natural logic (Mac-
Cartney, 2009) defines a set of pairwise relations be-
tween textual expressions, such as equivalence (?),
forward (@) and backward (A) entailment, negation
?) and others. These relations can be used to not
only detect semantic equivalence, but also infer en-
tailment. Our resulting system will be able to tackle
tasks like RTE, where the more a fine-grained reso-
lution of semantic relationships is crucial to perfor-
mance.
We favor a classification-based approach to this
problem: for each pair of paraphrases in the gram-
mar, we extract a feature vector that aims to capture
information about the semantic relationship in the
rule. Using a manually annotated development set
of paraphrases with relation labels, we train a clas-
sifier to discriminate between the different natural
logic relations.
We propose to leverage both labeled and unla-
beled data resources to extract useful features for
the classification. Annotated resources like Word-
Net can be used to derive a catalog of word and
phrase pairs with known entailment relationships,
for instance ?India, country ,@?. Using word align-
ments between our paraphrase pairs, we can estab-
lish what portions of a pair have labels in WordNet
and retain corresponding features.
To leverage unlabeled data, we propose extending
our notion of distributional similarity. Previously,
we used cosine similarity to compare the signatures
of two phrases. However, cosine similarity is a sym-
metric measure, and it is unlikely to prove helpful
for determining the (asymmetric) entailment direc-
tionality of a paraphrase pair (i.e. whether it is a
hypo- or hypernym relation). We therefore propose
to extract a variety of asymmetric similarity fea-
tures from distributional contexts. Specifically, we
seek a measure that compares both the similarity and
the ?breadth? of two vectors. Assuming that wider
breadth implies a hypernym, i.e. a @-entailment, the
scores produced by such a measure can be highly
66
twelve illustrations insulting
muhammad
CD NNS JJ
NP
NP
VP
NP
the prophet
NNS JJ
NP
NP
VP
NP
cartoons offensiveeditorial that were to12
CD
VB
NP
S
caused unrest
VB
NP
S
sparked  riots bywere 
NP
in Denmark
PP
NP PPJJ
JJ
""
NP
NP
of the
Paraphrase rules Entailment classification
CD ? twelve | 12 twelve ? 12
JJ ?  | editorial  A editorial
NNS ? illustrations | cartoons illustrations A cartoons
JJ ? insulting | offensive insulting ? / @ offensive
NP ? the prophet | muhammad the prophet ? muhammad
VB ? caused | sparked caused A sparked
NP ? unrest | riots unrest A riots
PP ?  | in Denmark  A in Denmark
NP ? CD(?) NNS(A) | CD(?) of the NNS(A) twelve illustrations A 12 of the cartoons
Figure 5: Our system will use synchronous parsing and paraphrase grammars to perform natural language inference.
Each paraphrase transformation will be classified with a natural logic entailment relation. These will be joined bottom-
up, as illustrated by the last rule, where the join of the smaller constituents ? ./ A results in A for the larger phrase
pairs. This process will be propagated up the trees to determine if the hypothesis can be inferred from the premise.
informative for our classification problem. Asym-
metric measures like Tversky indices (Tolias et al,
2001) appear well-suited to the problem. We will
investigate application of Tversky indices to our dis-
tributional signatures and their usefulness for entail-
ment relation classification.
4.1 Task-Based Evaluation
We propose evaluating the resulting system on tex-
tual entailment recognition. To do this, we cast the
RTE task as a synchronous parsing problem, as illus-
trated in Figure 5. We will extend the notion of syn-
chronous parsing towards resolving entailments, and
define and implement a compositional join operator
./ to compute entailment relations over synchronous
derivations from the individual rule entailments.
While the assumption of a synchronous parse
structure is likely to be valid for translations and
paraphrases, we do not expect it to straightforwardly
hold for entailment recognition. We will thus in-
vestigate the limits of the synchronous assumption
over RTE data. Furthermore, to expand the sys-
tem?s coverage in a first step, we propose a simple
relaxation of the synchronousness requirement via
entailment-less ?glue rules.? These rules, similar to
out-of-vocabulary rules in translation, will allow us
to include potentially unrelated or unrecognized por-
tions of the input into the synchronous parse.
5 Conclusion
We have described an extension of the state of the art
in paraphrasing in a number of important ways: we
leverage large bilingual data sets to extract linguis-
tically expressive high-coverage paraphrases based
on an SCFG formalism. On an example text-to-
text generation task, sentence compression, we show
that an easily adapted paraphrase system achieves
state of the art meaning retention. Further, we in-
clude a complementary data source, monolingual
corpora, to augment the quality of the previously
obtained paraphrase grammar. The resulting sys-
tem is shown to perform significantly better than
the purely bilingual paraphrases, in both meaning
retention and grammaticality, achieving results on
par with the state of the art. Finally, we propose
an extension of SCFG-based paraphrasing towards
a more fine grained semantic representation using a
classification-based approach. In extending the syn-
chronous parsing methodology, we outline the ex-
pansion of the paraphraser towards a system capable
of tackling entailment recognition tasks.
67
Acknowledgements
The ideas described in this paper were developed in
collaboration with Benjamin Van Durme and Chris
Callison-Burch. This material is based on research
sponsored by the NSF under grant IIS-1249516
and DARPA under agreement number FA8750-13-
2-0017 (the DEFT program). The U.S. Government
is authorized to reproduce and distribute reprints for
Governmental purposes. The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing official
policies or endorsements of DARPA, the NSF, or the
U.S. Government.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Peter Brown, John Cocke, Stephen Della Pietra, Vincent
Della Pietra, Frederick Jelinek, Robert Mercer, and
Paul Poossin. 1990. A statistical approach to language
translation. Computational Linguistics, 16(2), June.
Chris Callison-Burch. 2007. Paraphrasing and Trans-
lation. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2012. Monolingual distributional
similarity for text-to-text generation. In Proceedings
of *SEM. Association for Computational Linguistics.
Juri Ganitkevitch, Chris Callison-Burch, and Benjamin
Van Durme. 2013. Ppdb: The paraphrase database. In
Proceedings of HLT/NAACL.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Bill MacCartney. 2009. Natural language inference.
Ph.D. thesis, Stanford University.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
Yannis A. Tolias, Stavros M. Panas, and Lefteri H.
Tsoukalas. 2001. Generalized fuzzy indices for simi-
larity matching. Fuzzy Sets and Systems, 120(2):255?
270.
68
Proceedings of the ACL 2010 System Demonstrations, pages 7?12,
Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational Linguistics
cdec: A Decoder, Alignment, and Learning Framework for
Finite-State and Context-Free Translation Models
Chris Dyer
University of Maryland
redpony@umd.edu
Adam Lopez
University of Edinburgh
alopez@inf.ed.ac.uk
Juri Ganitkevitch
Johns Hopkins University
juri@cs.jhu.edu
Jonathan Weese
Johns Hopkins University
jweese@cs.jhu.edu
Ferhan Ture
University of Maryland
fture@cs.umd.edu
Phil Blunsom
Oxford University
pblunsom@comlab.ox.ac.uk
Hendra Setiawan
University of Maryland
hendra@umiacs.umd.edu
Vladimir Eidelman
University of Maryland
vlad@umiacs.umd.edu
Philip Resnik
University of Maryland
resnik@umiacs.umd.edu
Abstract
We present cdec, an open source frame-
work for decoding, aligning with, and
training a number of statistical machine
translation models, including word-based
models, phrase-based models, and models
based on synchronous context-free gram-
mars. Using a single unified internal
representation for translation forests, the
decoder strictly separates model-specific
translation logic from general rescoring,
pruning, and inference algorithms. From
this unified representation, the decoder can
extract not only the 1- or k-best transla-
tions, but also alignments to a reference,
or the quantities necessary to drive dis-
criminative training using gradient-based
or gradient-free optimization techniques.
Its efficient C++ implementation means
that memory use and runtime performance
are significantly better than comparable
decoders.
1 Introduction
The dominant models used in machine transla-
tion and sequence tagging are formally based
on either weighted finite-state transducers (FSTs)
or weighted synchronous context-free grammars
(SCFGs) (Lopez, 2008). Phrase-based models
(Koehn et al, 2003), lexical translation models
(Brown et al, 1993), and finite-state conditional
random fields (Sha and Pereira, 2003) exemplify
the former, and hierarchical phrase-based models
the latter (Chiang, 2007). We introduce a soft-
ware package called cdec that manipulates both
classes in a unified way.1
Although open source decoders for both phrase-
based and hierarchical translation models have
been available for several years (Koehn et al,
2007; Li et al, 2009), their extensibility to new
models and algorithms is limited by two sig-
nificant design flaws that we have avoided with
cdec. First, their implementations tightly couple
the translation, language model integration (which
we call rescoring), and pruning algorithms. This
makes it difficult to explore alternative transla-
tion models without also re-implementing rescor-
ing and pruning logic. In cdec, model-specific
code is only required to construct a translation for-
est (?3). General rescoring (with language models
or other models), pruning, inference, and align-
ment algorithms then apply to the unified data
structure (?4). Hence all model types benefit im-
mediately from new algorithms (for rescoring, in-
ference, etc.); new models can be more easily pro-
totyped; and controlled comparison of models is
made easier.
Second, existing open source decoders were de-
signed with the traditional phrase-based parame-
terization using a very small number of dense fea-
tures (typically less than 10). cdec has been de-
signed from the ground up to support any parame-
terization, from those with a handful of dense fea-
tures up to models with millions of sparse features
(Blunsom et al, 2008; Chiang et al, 2009). Since
the inference algorithms necessary to compute a
training objective (e.g. conditional likelihood or
expected BLEU) and its gradient operate on the
unified data structure (?5), any model type can be
trained using with any of the supported training
1The software is released under the Apache License, ver-
sion 2.0, and is available from http://cdec-decoder.org/ .
7
criteria. The software package includes general
function optimization utilities that can be used for
discriminative training (?6).
These features are implemented without com-
promising on performance. We show experimen-
tally that cdec uses less memory and time than
comparable decoders on a controlled translation
task (?7).
2 Decoder workflow
The decoding pipeline consists of two phases. The
first (Figure 1) transforms input, which may be
represented as a source language sentence, lattice
(Dyer et al, 2008), or context-free forest (Dyer
and Resnik, 2010), into a translation forest that has
been rescored with all applicable models.
In cdec, the only model-specific logic is con-
fined to the first step in the process where an
input string (or lattice, etc.) is transduced into
the unified hypergraph representation. Since the
model-specific code need not worry about integra-
tion with rescoring models, it can be made quite
simple and efficient. Furthermore, prior to lan-
guage model integration (and distortion model in-
tegration, in the case of phrase based translation),
pruning is unnecessary for most kinds of mod-
els, further simplifying the model-specific code.
Once this unscored translation forest has been
generated, any non-coaccessible states (i.e., states
that are not reachable from the goal node) are re-
moved and the resulting structure is rescored with
language models using a user-specified intersec-
tion/pruning strategy (?4) resulting in a rescored
translation forest and completing phase 1.
The second phase of the decoding pipeline (de-
picted in Figure 2) computes a value from the
rescored forest: 1- or k-best derivations, feature
expectations, or intersection with a target language
reference (sentence or lattice). The last option
generates an alignment forest, from which a word
alignment or feature expectations can be extracted.
Most of these values are computed in a time com-
plexity that is linear in the number of edges and
nodes in the translation hypergraph using cdec?s
semiring framework (?5).
2.1 Alignment forests and alignment
Alignment is the process of determining if and
how a translation model generates a ?source, tar-
get? string pair. To compute an alignment under
a translation model, the phase 1 translation hyper-
graph is reinterpreted as a synchronous context-
free grammar and then used to parse the target
sentence.2 This results in an alignment forest,
which is a compact representation of all the deriva-
tions of the sentence pair under the translation
model. From this forest, the Viterbi or maximum a
posteriori word alignment can be generated. This
alignment algorithm is explored in depth by Dyer
(2010). Note that if the phase 1 forest has been
pruned in some way, or the grammar does not de-
rive the sentence pair, the target intersection parse
may fail, meaning that an alignment will not be
recoverable.
3 Translation hypergraphs
Recent research has proposed a unified repre-
sentation for the various translation and tagging
formalisms that is based on weighted logic pro-
gramming (Lopez, 2009). In this view, trans-
lation (or tagging) deductions have the structure
of a context-free forest, or directed hypergraph,
where edges have a single head and 0 or more tail
nodes (Nederhof, 2003). Once a forest has been
constructed representing the possible translations,
general inference algorithms can be applied.
In cdec?s translation hypergraph, a node rep-
resents a contiguous sequence of target language
words. For SCFG models and sequential tag-
ging models, a node also corresponds to a source
span and non-terminal type, but for word-based
and phrase-based models, the relationship to the
source string (or lattice) may be more compli-
cated. In a phrase-based translation hypergraph,
the node will correspond to a source coverage vec-
tor (Koehn et al, 2003). In word-based models, a
single node may derive multiple different source
language coverages since word based models im-
pose no requirements on covering all words in the
input. Figure 3 illustrates two example hyper-
graphs, one generated using a SCFG model and
other from a phrase-based model.
Edges are associated with exactly one syn-
chronous production in the source and target lan-
guage, and alternative translation possibilities are
expressed as alternative edges. Edges are further
annotated with feature values, and are annotated
with the source span vector the edge corresponds
to. An edge?s output label may contain mixtures
of terminal symbol yields and positions indicating
where a child node?s yield should be substituted.
2The parser is smart enough to detect the left-branching
grammars generated by lexical translation and tagging mod-
els, and use a more efficient intersection algorithm.
8
SCFG parser
FST transducer
Tagger
Lexical transducer
Phrase-based 
transducer
Source CFG
Source 
sentence
Source lattice
Unscored 
hypergraph
Input Transducers
Cube pruning
Full intersection
FST rescoring
Translation 
hypergraph
Output
Cube growing
No rescoring
Figure 1: Forest generation workflow (first half of decoding pipeline). The decoder?s configuration
specifies what path is taken from the input (one of the bold ovals) to a unified translation hypergraph.
The highlighted path is the workflow used in the test reported in ?7.
Translation 
hypergraph
Target 
reference
Viterbi extraction
k-best extraction
max-translation 
extraction
feature 
expectations
intersection by 
parsing
Alignment 
hypergraph
feature 
expectations
max posterior 
alignment
Viterbi alignment
Translation outputs Alignment outputs
Figure 2: Output generation workflow (second half of decoding pipeline). Possible output types are
designated with a double box.
In the case of SCFG grammars, the edges corre-
spond simply to rules in the synchronous gram-
mar. For non-SCFG translation models, there are
two kinds of edges. The first have zero tail nodes
(i.e., an arity of 0), and correspond to word or
phrase translation pairs (with all translation op-
tions existing on edges deriving the same head
node), or glue rules that glue phrases together.
For tagging, word-based, and phrase-based mod-
els, these are strictly arranged in a monotone, left-
branching structure.
4 Rescoring with weighted FSTs
The design of cdec separates the creation of a
translation forest from its rescoring with a lan-
guage models or similar models.3 Since the struc-
ture of the unified search space is context free (?3),
we use the logic for language model rescoring de-
scribed by Chiang (2007), although any weighted
intersection algorithm can be applied. The rescor-
3Other rescoring models that depend on sequential con-
text include distance-based reordering models or Markov fea-
tures in tagging models.
ing models need not be explicitly represented as
FSTs?the state space can be inferred.
Although intersection using the Chiang algo-
rithm runs in polynomial time and space, the re-
sulting rescored forest may still be too large to rep-
resent completely. cdec therefore supports three
pruning strategies that can be used during intersec-
tion: full unpruned intersection (useful for tagging
models to incorporate, e.g., Markov features, but
not generally practical for translation), cube prun-
ing, and cube growing (Huang and Chiang, 2007).
5 Semiring framework
Semirings are a useful mathematical abstraction
for dealing with translation forests since many
useful quantities can be computed using a single
linear-time algorithm but with different semirings.
A semiring is a 5-tuple (K,?,?, 0, 1) that indi-
cates the set from which the values will be drawn,
K, a generic addition and multiplication operation,
? and ?, and their identities 0 and 1. Multipli-
cation and addition must be associative. Multi-
plication must distribute over addition, and v ? 0
9
Goal
JJ NN
1 2
a
s
m
a
l
l
l
i
t
t
l
e
h
o
u
s
e
s
h
e
l
l
Goal
010
100 101
110
a
s
m
a
l
l
l
i
t
t
l
e
1
a
1
house
1
shell
1
little
1
small
1
house
1
shell
1
little
1
small
Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines
(small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distor-
tion limit of 1 (right).
must equal 0. Values that can be computed using
the semirings include the number of derivations,
the expected translation length, the entropy of the
translation posterior distribution, and the expected
values of feature functions (Li and Eisner, 2009).
Since semirings are such a useful abstraction,
cdec has been designed to facilitate implementa-
tion of new semirings. Table 1 shows the C++ rep-
resentation used for semirings. Note that because
of our representation, built-in types like double,
int, and bool (together with their default op-
erators) are semirings. Beyond these, the type
prob t is provided which stores the logarithm of
the value it represents, which helps avoid under-
flow and overflow problems that may otherwise
be encountered. A generic first-order expectation
semiring is also provided (Li and Eisner, 2009).
Table 1: Semiring representation. T is a C++ type
name.
Element C++ representation
K T
? T::operator+=
? T::operator*=
0 T()
1 T(1)
Three standard algorithms parameterized with
semirings are provided: INSIDE, OUTSIDE, and
INSIDEOUTSIDE, and the semiring is specified us-
ing C++ generics (templates). Additionally, each
algorithm takes a weight function that maps from
hypergraph edges to a value in K, making it possi-
ble to use many different semirings without alter-
ing the underlying hypergraph.
5.1 Viterbi and k-best extraction
Although Viterbi and k-best extraction algorithms
are often expressed as INSIDE algorithms with
the tropical semiring, cdec provides a separate
derivation extraction framework that makes use of
a < operator (Huang and Chiang, 2005). Thus,
many of the semiring types define not only the el-
ements shown in Table 1 but T::operator< as
well. The k-best extraction algorithm is also pa-
rameterized by an optional predicate that can filter
out derivations at each node, enabling extraction
of only derivations that yield different strings as in
Huang et al (2006).
6 Model training
Two training pipelines are provided with cdec.
The first, called Viterbi envelope semiring train-
ing, VEST, implements the minimum error rate
training (MERT) algorithm, a gradient-free opti-
mization technique capable of maximizing arbi-
trary loss functions (Och, 2003).
6.1 VEST
Rather than computing an error surface using k-
best approximations of the decoder search space,
cdec?s implementation performs inference over
the full hypergraph structure (Kumar et al, 2009).
In particular, by defining a semiring whose values
are sets of line segments, having an addition op-
eration equivalent to union, and a multiplication
operation equivalent to a linear transformation of
the line segments, Och?s line search can be com-
puted simply using the INSIDE algorithm. Since
the translation hypergraphs generated by cdec
may be quite large making inference expensive,
the logic for constructing error surfaces is fac-
tored according to the MapReduce programming
paradigm (Dean and Ghemawat, 2004), enabling
parallelization across a cluster of machines. Im-
plementations of the BLEU and TER loss functions
are provided (Papineni et al, 2002; Snover et al,
2006).
10
6.2 Large-scale discriminative training
In addition to the widely used MERT algo-
rithm, cdec also provides a training pipeline for
discriminatively trained probabilistic translation
models (Blunsom et al, 2008; Blunsom and Os-
borne, 2008). In these models, the translation
model is trained to maximize conditional log like-
lihood of the training data under a specified gram-
mar. Since log likelihood is differentiable with
respect to the feature weights in an exponential
model, it is possible to use gradient-based opti-
mization techniques to train the system, enabling
the parameterization of the model using millions
of sparse features. While this training approach
was originally proposed for SCFG-based transla-
tion models, it can be used to train any model
type in cdec. When used with sequential tagging
models, this pipeline is identical to traditional se-
quential CRF training (Sha and Pereira, 2003).
Both the objective (conditional log likelihood)
and its gradient have the form of a difference in
two quantities: each has one term that is com-
puted over the translation hypergraph which is
subtracted from the result of the same computa-
tion over the alignment hypergraph (refer to Fig-
ures 1 and 2). The conditional log likelihood is
the difference in the log partition of the translation
and alignment hypergraph, and is computed using
the INSIDE algorithm. The gradient with respect
to a particular feature is the difference in this fea-
ture?s expected value in the translation and align-
ment hypergraphs, and can be computed using ei-
ther INSIDEOUTSIDE or the expectation semiring
and INSIDE. Since a translation forest is generated
as an intermediate step in generating an alignment
forest (?2) this computation is straightforward.
Since gradient-based optimization techniques
may require thousands of evaluations to converge,
the batch training pipeline is split into map and
reduce components, facilitating distribution over
very large clusters. Briefly, the cdec is run as the
map function, and sentence pairs are mapped over.
The reduce function aggregates the results and per-
forms the optimization using standard algorithms,
including LBFGS (Liu et al, 1989), RPROP (Ried-
miller and Braun, 1993), and stochastic gradient
descent.
7 Experiments
Table 2 compares the performance of cdec, Hi-
ero, and Joshua 1.3 (running with 1 or 8 threads)
decoding using a hierarchical phrase-based trans-
lation grammar and identical pruning settings.4
Figure 4 shows the cdec configuration and
weights file used for this test.
The workstation used has two 2GHz quad-core
Intel Xenon processors, 32GB RAM, is running
Linux kernel version 2.6.18 and gcc version 4.1.2.
All decoders use SRI?s language model toolkit,
version 1.5.9 (Stolcke, 2002). Joshua was run on
the Sun HotSpot JVM, version 1.6.0 12. A hierar-
chical phrase-based translation grammar was ex-
tracted for the NIST MT03 Chinese-English trans-
lation using a suffix array rule extractor (Lopez,
2007). A non-terminal span limit of 15 was used,
and all decoders were configured to use cube prun-
ing with a limit of 30 candidates at each node and
no further pruning. All decoders produced a BLEU
score between 31.4 and 31.6 (small differences are
accounted for by different tie-breaking behavior
and OOV handling).
Table 2: Memory usage and average per-sentence
running time, in seconds, for decoding a Chinese-
English test set.
Decoder Lang. Time (s) Memory
cdec C++ 0.37 1.0Gb
Joshua (1?) Java 0.98 1.5Gb
Joshua (8?) Java 0.35 2.5Gb
Hiero Python 4.04 1.1Gb
formalism=scfg
grammar=grammar.mt03.scfg.gz
add pass through rules=true
scfg max span limit=15
feature function=LanguageModel \
en.3gram.pruned.lm.gz -o 3
feature function=WordPenalty
intersection strategy=cube pruning
cubepruning pop limit=30
LanguageModel 1.12
WordPenalty -4.26
PhraseModel 0 0.963
PhraseModel 1 0.654
PhraseModel 2 0.773
PassThroughRule -20
Figure 4: Configuration file (above) and feature
weights file (below) used for the decoding test de-
scribed in ?7.
4http://sourceforge.net/projects/joshua/
11
8 Future work
cdec continues to be under active development.
We are taking advantage of its modular design to
study alternative algorithms for language model
integration. Further training pipelines are un-
der development, including minimum risk train-
ing using a linearly decomposable approximation
of BLEU (Li and Eisner, 2009), and MIRA train-
ing (Chiang et al, 2009). All of these will be
made publicly available as the projects progress.
We are also improving support for parallel training
using Hadoop (an open-source implementation of
MapReduce).
Acknowledgements
This work was partially supported by the GALE
program of the Defense Advanced Research
Projects Agency, Contract No. HR0011-06-2-001.
Any opinions, findings, conclusions or recommen-
dations expressed in this paper are those of the au-
thors and do not necessarily reflect the views of the
sponsors. Further support was provided the Euro-
Matrix project funded by the European Commis-
sion (7th Framework Programme). Discussions
with Philipp Koehn, Chris Callison-Burch, Zhifei
Li, Lane Schwarz, and Jimmy Lin were likewise
crucial to the successful execution of this project.
References
P. Blunsom and M. Osborne. 2008. Probalistic inference for
machine translation. In Proc. of EMNLP.
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrimina-
tive latent variable model for statistical machine transla-
tion. In Proc. of ACL-HLT.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational Lin-
guistics, 19(2):263?311.
D. Chiang, K. Knight, and W. Wang. 2009. 11,001 new
features for statistical machine translation. In Proc. of
NAACL, pages 218?226.
D. Chiang. 2007. Hierarchical phrase-based translation.
Comp. Ling., 33(2):201?228.
J. Dean and S. Ghemawat. 2004. MapReduce: Simplified
data processing on large clusters. In Proc. of the 6th Sym-
posium on Operating System Design and Implementation
(OSDI 2004), pages 137?150.
C. Dyer and P. Resnik. 2010. Context-free reordering, finite-
state translation. In Proc. of HLT-NAACL.
C. Dyer, S. Muresan, and P. Resnik. 2008. Generalizing
word lattice translation. In Proc. of HLT-ACL.
C. Dyer. 2010. Two monolingual parses are better than one
(synchronous parse). In Proc. of HLT-NAACL.
L. Huang and D. Chiang. 2005. Better k-best parsing. In In
Proc. of IWPT, pages 53?64.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. ACL.
L. Huang, K. Knight, and A. Joshi. 2006. A syntax-directed
translator with extended domain of locality. In Proc. of
AMTA.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT/NAACL, pages 48?54.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of ACL, Demonstration Ses-
sion, pages 177?180, June.
S. Kumar, W. Macherey, C. Dyer, and F. Och. 2009. Efficient
minimum error rate training and minimum B ayes-risk de-
coding for translation hypergraphs and lattices. In Proc.
of ACL, pages 163?171.
Z. Li and J. Eisner. 2009. First- and second-order expectation
semirings with applications to minimum-risk training on
translation forests. In Proc. of EMNLP, pages 40?51.
Z. Li, C. Callison-Burch, C. Dyer, J. Ganitkevitch, S. Khu-
danpur, L. Schwartz, W. N. G. Thornton, J. Weese, and
O. F. Zaidan. 2009. Joshua: an open source toolkit for
parsing-based machine translation. In Proc. of the Fourth
Workshop on Stat. Machine Translation, pages 135?139.
D. C. Liu, J. Nocedal, D. C. Liu, and J. Nocedal. 1989. On
the limited memory BFGS method for large scale opti-
mization. Mathematical Programming B, 45(3):503?528.
A. Lopez. 2007. Hierarchical phrase-based translation with
suffix arrays. In Proc. of EMNLP, pages 976?985.
A. Lopez. 2008. Statistical machine translation. ACM Com-
puting Surveys, 40(3), Aug.
A. Lopez. 2009. Translation as weighted deduction. In Proc.
of EACL, pages 532?540.
M.-J. Nederhof. 2003. Weighted deductive parsing and
Knuth?s algorithm. Comp. Ling., 29(1):135?143, Mar.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proc. of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
M. Riedmiller and H. Braun. 1993. A direct adaptive method
for faster backpropagation learning: The RPROP algo-
rithm. In Proc. of the IEEE international conference on
neural networks, pages 586?591.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL, pages 134?141.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A study of translation edit rate with
targeted human annotation. In Proc. AMTA.
A. Stolcke. 2002. SRILM ? an extensible language modeling
toolkit. In Intl. Conf. on Spoken Language Processing.
12
Transactions of the Association for Computational Linguistics, 1 (2013) 165?178. Action Editor: David Chiang.
Submitted 11/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Learning to translate with products of novices: a suite of open-ended
challenge problems for teaching MT
Adam Lopez1, Matt Post1, Chris Callison-Burch1,2, Jonathan Weese, Juri Ganitkevitch,
Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,
Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,
Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao?
Department of Computer Science, Johns Hopkins University
1Human Language Technology Center of Excellence, Johns Hopkins University
2Computer and Information Science Department, University of Pennsylvania
Abstract
Machine translation (MT) draws from several
different disciplines, making it a complex sub-
ject to teach. There are excellent pedagogical
texts, but problems in MT and current algo-
rithms for solving them are best learned by
doing. As a centerpiece of our MT course,
we devised a series of open-ended challenges
for students in which the goal was to im-
prove performance on carefully constrained
instances of four key MT tasks: alignment,
decoding, evaluation, and reranking. Students
brought a diverse set of techniques to the prob-
lems, including some novel solutions which
performed remarkably well. A surprising and
exciting outcome was that student solutions
or their combinations fared competitively on
some tasks, demonstrating that even newcom-
ers to the field can help improve the state-of-
the-art on hard NLP problems while simulta-
neously learning a great deal. The problems,
baseline code, and results are freely available.
1 Introduction
A decade ago, students interested in natural lan-
guage processing arrived at universities having been
exposed to the idea of machine translation (MT)
primarily through science fiction. Today, incoming
students have been exposed to services like Google
Translate since they were in secondary school or ear-
lier. For them, MT is science fact. So it makes sense
to teach statistical MT, either on its own or as a unit
? The first five authors were instructors and the remaining au-
thors were students in the worked described here. This research
was conducted while Chris Callison-Burch was at Johns Hop-
kins University.
in a class on natural language processing (NLP), ma-
chine learning (ML), or artificial intelligence (AI). A
course that promises to show students how Google
Translate works and teach them how to build some-
thing like it is especially appealing, and several uni-
versities and summer schools now offer such classes.
There are excellent introductory texts?depending
on the level of detail required, instructors can choose
from a comprehensive MT textbook (Koehn, 2010),
a chapter of a popular NLP textbook (Jurafsky and
Martin, 2009), a tutorial survey (Lopez, 2008), or
an intuitive tutorial on the IBM Models (Knight,
1999b), among many others.
But MT is not just an object of academic study.
It?s a real application that isn?t fully perfected, and
the best way to learn about it is to build an MT sys-
tem. This can be done with open-source toolkits
such as Moses (Koehn et al, 2007), cdec (Dyer et
al., 2010), or Joshua (Ganitkevitch et al, 2012), but
these systems are not designed for pedagogy. They
are mature codebases featuring tens of thousands of
source code lines, making it difficult to focus on
their core algorithms. Most tutorials present them
as black boxes. But our goal is for students to learn
the key techniques in MT, and ideally to learn by
doing. Black boxes are incompatible with this goal.
We solve this dilemma by presenting students
with concise, fully-functioning, self-contained com-
ponents of a statistical MT system: word alignment,
decoding, evaluation, and reranking. Each imple-
mentation consists of a na??ve baseline algorithm in
less than 150 lines of Python code. We assign them
to students as open-ended challenges in which the
goal is to improve performance on objective eval-
uation metrics as much as possible. This setting
mirrors evaluations conducted by the NLP research
165
community and by the engineering teams behind
high-profile NLP projects such as Google Translate
and IBM?s Watson. While we designate specific al-
gorithms as benchmarks for each task, we encour-
age creativity by awarding more points for the best
systems. As additional incentive, we provide a web-
based leaderboard to display standings in real time.
In our graduate class on MT, students took a va-
riety of different approaches to the tasks, in some
cases devising novel algorithms. A more exciting re-
sult is that some student systems or combinations of
systems rivaled the state of the art on some datasets.
2 Designing MT Challenge Problems
Our goal was for students to freely experiment with
different ways of solving MT problems on real data,
and our approach consisted of two separable com-
ponents. First, we provided a framework that strips
key MT problems down to their essence so students
could focus on understanding classic algorithms or
invent new ones. Second, we designed incentives
that motivated them to improve their solutions as
much as possible, encouraging experimentation with
approaches beyond what we taught in class.
2.1 Decoding, Reranking, Evaluation, and
Alignment for MT (DREAMT)
We designed four assignments, each corresponding
to a real subproblem in MT: alignment, decoding,
evaluation, and reranking.1 From the more general
perspective of AI, they emphasize the key problems
of unsupervised learning, search, evaluation design,
and supervised learning, respectively. In real MT
systems, these problems are highly interdependent,
a point we emphasized in class and at the end of each
assignment?for example, that alignment is an exer-
cise in parameter estimation for translation models,
that model choice is a tradeoff between expressivity
and efficient inference, and that optimal search does
not guarantee optimal accuracy. However, present-
ing each problem independently and holding all else
constant enables more focused exploration.
For each problem we provided data, a na??ve solu-
tion, and an evaluation program. Following Bird et
al. (2008) and Madnani and Dorr (2008), we imple-
mented the challenges in Python, a high-level pro-
1http://alopez.github.io/dreamt
gramming language that can be used to write very
concise programs resembling pseudocode.2,3 By de-
fault, each baseline system reads the test data and
generates output in the evaluation format, so setup
required zero configuration, and students could be-
gin experimenting immediately. For example, on re-
ceipt of the alignment code, aligning data and eval-
uating results required only typing:
> align | grade
Students could then run experiments within minutes
of beginning the assignment.
Three of the four challenges also included unla-
beled test data (except the decoding assignment, as
explained in ?4). We evaluated test results against a
hidden key when assignments were submitted.
2.2 Incentive Design
We wanted to balance several pedagogical goals: un-
derstanding of classic algorithms, free exploration
of alternatives, experience with typical experimental
design, and unhindered collaboration.
Machine translation is far from solved, so we ex-
pected more than reimplementation of prescribed al-
gorithms; we wanted students to really explore the
problems. To motivate exploration, we made the as-
signments competitive. Competition is a powerful
force, but must be applied with care in an educa-
tional setting.4 We did not want the consequences
of ambitious but failed experiments to be too dire,
and we did not want to discourage collaboration.
For each assignment, we guaranteed a passing
grade for matching the performance of a specific tar-
get alorithm. Typically, the target was important
but not state-of-the-art: we left substantial room for
improvement, and thus competition. We told stu-
dents the exact algorithm that produced the target ac-
curacy (though we expected them to derive it them-
selves based on lectures, notes, or literature). We
did not specifically require them to implement it, but
the guarantee of a passing grade provided a power-
ful incentive for this to be the first step of each as-
signment. Submissions that beat this target received
additional credit. The top five submissions received
full credit, while the top three received extra credit.
2http://python.org
3Some well-known MT systems have been implemented in
Python (Chiang, 2007; Huang and Chiang, 2007).
4Thanks to an anonymous reviewer for this turn of phrase.
166
This scheme provided strong incentive to continue
experimentation beyond the target alorithm.5
For each assignment, students could form teams
of any size, under three rules: each team had to pub-
licize its formation to the class, all team members
agreed to receive the same grade, and teams could
not drop members. Our hope was that these require-
ments would balance the perceived competitive ad-
vantage of collaboration against a reluctance to take
(and thus support) teammates who did not contribute
to the competitive effort.6 This strategy worked: out
of sixteen students, ten opted to work collaboratively
on at least one assignment, always in pairs.
We provided a web-based leaderboard that dis-
played standings on the test data in real time, iden-
tifying each submission by a pseudonymous han-
dle known only to the team and instructors. Teams
could upload solutions as often as they liked before
the assignment deadline. The leaderboard displayed
scores of the default and target alorithms. This in-
centivized an early start, since teams could verify
for themselves when they met the threshold for a
passing grade. Though effective, it also detracted
from realism in one important way: it enabled hill-
climbing on the evaluation metric. In early assign-
ments, we observed a few cases of this behavior,
so for the remaining assignments, we modified the
leaderboard so that changes in score would only be
reflected once every twelve hours. This strategy
trades some amount of scientific realism for some
measure of incentive, a strategy that has proven
effective in other pedagogical tools with real-time
feedback (Spacco et al, 2006).
To obtain a grade, teams were required to sub-
mit their results, share their code privately with the
instructors, and publicly describe their experimen-
tal process to the class so that everyone could learn
from their collective effort. Teams were free (but not
required) to share their code publicly at any time.
5Grades depend on institutional norms. In our case, high grades
in the rest of class combined with matching all assignment tar-
get alorithms would earn a B+; beating two target alorithms
would earn an A-; top five placement on any assignment would
earn an A; and top three placement compensated for weaker
grades in other course criteria. Everyone who completed all
four assignments placed in the top five at least once.
6The equilibrium point is a single team, though this team would
still need to decide on a division of labor. One student contem-
plated organizing this team, but decided against it.
Some did so after the assignment deadline.
3 The Alignment Challenge
The first challenge was word alignment: given a par-
allel text, students were challenged to produce word-
to-word alignments with low alignment error rate
(AER; Och and Ney, 2000). This is a variant of a
classic assignment not just in MT, but in NLP gen-
erally. Klein (2005) describes a version of it, and we
know several other instructors who use it.7 In most
of these, the object is to implement IBM Model 1
or 2, or a hidden Markov model. Our version makes
it open-ended by asking students to match or beat an
IBM Model 1 baseline.
3.1 Data
We provided 100,000 sentences of parallel data from
the Canadian Hansards, totaling around two million
words.8 This dataset is small enough to align in
a few minutes with our implementation?enabling
rapid experimentation?yet large enough to obtain
reasonable results. In fact, Liang et al (2006) report
alignment accuracy on data of this size that is within
a fraction of a point of their accuracy on the com-
plete Hansards data. To evaluate, we used manual
alignments of a small fraction of sentences, devel-
oped by Och and Ney (2000), which we obtained
from the shared task resources organized by Mihal-
cea and Pedersen (2003). The first 37 sentences
of the corpus were development data, with manual
alignments provided in a separate file. Test data con-
sisted of an additional 447 sentences, for which we
did not provide alignments.9
3.2 Implementation
We distributed three Python programs with the
data. The first, align, computes Dice?s coefficient
(1945) for every pair of French and English words,
then aligns every pair for which its value is above an
adjustable threshold. Our implementation (most of
7Among them, Jordan Boyd-Graber, John DeNero, Philipp
Koehn, and Slav Petrov (personal communication).
8http://www.isi.edu/natural-language/download/hansard/
9This invited the possibility of cheating, since alignments of the
test data are publicly available on the web. We did not adver-
tise this, but as an added safeguard we obfuscated the data by
distributing the test sentences randomly throughout the file.
167
Listing 1 The default aligner in DREAMT: thresh-
olding Dice?s coefficient.
for (f, e) in bitext:
for f_i in set(f):
f_count[f_i] += 1
for e_j in set(e):
fe_count[(f_i,e_j)] += 1
for e_j in set(e):
e_count[e_j] += 1
for (f_i, e_j) in fe_count.keys():
dice[(f_i,e_j)] = \
2.0 * fe_count[(f_i, e_j)] / \
(f_count[f_i] + e_count[e_j])
for (f, e) in bitext:
for (i, f_i) in enumerate(f):
for (j, e_j) in enumerate(e):
if dice[(f_i,e_j)] >= cutoff:
print "%i-%i " % (i,j)
which is shown in Listing 1) is quite close to pseu-
docode, making it easy to focus on the algorithm,
one of our pedagogical goals. The grade program
computes AER and optionally prints an alignment
grid for sentences in the development data, showing
both human and automatic alignments. Finally the
check program verifies that the results represent
a valid solution, reporting an error if not?enabling
students to diagnose bugs in their submissions.
The default implementation enabled immediate
experimentation. On receipt of the code, students
were instructed to align the first 1,000 sentences and
compute AER using a simple command.
> align -n 1000 | grade By varying the
number of input sentences and the threshold for an
alignment, students could immediately see the effect
of various parameters on alignment quality.
We privately implemented IBM Model 1 (Brown
et al, 1993) as the target alorithm for a passing
grade. We ran it for five iterations with English
as the target language and French as the source.
Our implementation did not use null alignment
or symmetrization?leaving out these common im-
provements offered students the possibility of dis-
covering them independently, and thereby rewarded.
A
E
R
?
10
0
20
30
40
50
60
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 1: Submission history for the alignment challenge.
Dashed lines represent the default and baseline system
performance. Each colored line represents a student, and
each dot represents a submission. For clarity, we show
only submissions that improved the student?s AER.
3.3 Challenge Results
We received 209 submissions from 11 teams over a
period of two weeks (Figure 1). Everyone eventually
matched or exceeded IBM Model 1 AER of 31.26.
Most students implemented IBM Model 1, but we
saw many other solutions, indicating that many truly
experimented with the problem:
? Implementing heuristic constraints to require
alignment of proper names and punctuation.
? Running the algorithm on stems rather than sur-
face words.
? Initializing the first iteration of Model 1 with
parameters estimated on the observed align-
ments in the development data.
? Running Model 1 for many iterations. Most re-
searchers typically run Model 1 for five itera-
tions or fewer, and there are few experiments
in the literature on its behavior over many iter-
ations, as there are for hidden Markov model
taggers (Johnson, 2007). Our students carried
out these experiments, reporting runs of 5, 20,
100, and even 2000 iterations. No improve-
ment was observed after 20 iterations.
168
? Implementing various alternative approaches
from the literature, including IBM Model 2
(Brown et al, 1993), competitive linking
(Melamed, 2000), and smoothing (Moore,
2004).
One of the best solutions was competitive linking
with Dice?s coefficient, modified to incorporate the
observation that alignments tend to be monotonic by
restricting possible alignment points to a window of
eight words around the diagonal. Although simple,
it acheived an AER of 18.41, an error reduction over
Model 1 of more than 40%.
The best score compares unfavorably against a
state-of-the-art AER of 3.6 (Liu et al, 2010). But
under a different view, it still represents a significant
amount of progress for an effort taking just over two
weeks: on the original challenge from which we ob-
tained the data (Mihalcea and Pedersen, 2003) the
best student system would have placed fifth out of
fifteen systems. Consider also the combined effort of
all the students: when we trained a perceptron clas-
sifier on the development data, taking each student?s
prediction as a feature, we obtained an AER of 15.4,
which would have placed fourth on the original chal-
lenge. This is notable since none of the systems
incorporated first-order dependencies on the align-
ments of adjacent words, long noted as an impor-
tant feature of the best alignment models (Och and
Ney, 2003). Yet a simple system combination of stu-
dent assignments is as effective as a hidden Markov
Model trained on a comparable amount of data (Och
and Ney, 2003).
It is important to note that AER does not neces-
sarily correlate with downstream performance, par-
ticularly on the Hansards dataset (Fraser and Marcu,
2007). We used the conclusion of the assignment as
an opportunity to emphasize this point.
4 The Decoding Challenge
The second challenge was decoding: given a fixed
translation model and a set of input sentences, stu-
dents were challenged to produce translations with
the highest model score. This challenge introduced
the difficulties of combinatorial optimization under
a deceptively simple setup: the model we provided
was a simple phrase-based translation model (Koehn
et al, 2003) consisting only of a phrase table and tri-
gram language model. Under this simple model, for
a French sentence f of length I , English sentence
e of length J , and alignment a where each element
consists of a span in both e and f such that every
word in both e and f is aligned exactly once, the
conditional probability of e and a given f is as fol-
lows.10
p(e, a|f) =
?
?i,i?,j,j???a
p(f i?i |ej
?
j )
J+1?
j=1
p(ej |ej?1, ej?2)
(1)
To evaluate output, we compute the conditional
probability of e as follows.
p(e|f) =
?
a
p(e, a|f) (2)
Note that this formulation is different from the typ-
ical Viterbi objective of standard beam search de-
coders, which do not sum over all alignments, but
approximate p(e|f) by maxa p(e, a|f). Though the
computation in Equation 2 is intractable (DeNero
and Klein, 2008), it can be computed in a few min-
utes via dynamic programming on reasonably short
sentences. We ensured that our data met this crite-
rion. The corpus-level probability is then the prod-
uct of all sentence-level probabilities in the data.
The model includes no distortion limit or distor-
tion model, for two reasons. First, leaving out the
distortion model slightly simplifies the implementa-
tion, since it is not necessary to keep track of the last
word translated in a beam decoder; we felt that this
detail was secondary to understanding the difficulty
of search over phrase permutations. Second, it actu-
ally makes the problem more difficult, since a simple
distance-based distortion model prefers translations
with fewer permutations; without it, the model may
easily prefer any permutation of the target phrases,
making even the Viterbi search problem exhibit its
true NP-hardness (Knight, 1999a; Zaslavskiy et al,
2009).
Since the goal was to find the translation with the
highest probability, we did not provide a held-out
test set; with access to both the input sentences and
10For simplicity, this formula assumes that e is padded with two
sentence-initial symbols and one sentence-final symbol, and
ignores the probability of sentence segmentation, which we
take to be uniform.
169
the model, students had enough information to com-
pute the evaluation score on any dataset themselves.
The difficulty of the challenge lies simply in finding
the translation that maximizes the evaluation. In-
deed, since the problem is intractable, even the in-
structors did not know the true solution.11
4.1 Data
We chose 48 French sentences totaling 716 words
from the Canadian Hansards to serve as test data.
To create a simple translation model, we used the
Berkeley aligner to align the parallel text from the
first assignment, and extracted a phrase table using
the method of Lopez (2007), as implemented in cdec
(Dyer et al, 2010). To create a simple language
model, we used SRILM (Stolcke, 2002).
4.2 Implementation
We distributed two Python programs. The first,
decode, decodes the test data monotonically?
using both the language model and translation
model, but without permuting phrases. The imple-
mentation is completely self-contained with no ex-
ternal dependencies: it implements both models and
a simple stack decoding algorithm for monotonic
translation. It contains only 122 lines of Python?
orders of magnitude fewer than most full-featured
decoders. To see its similarity to pseudocode, com-
pare the decoding algorithm (Listing 2) with the
pseudocode in Koehn?s (2010) popular textbook (re-
produced here as Algorithm 1). The second pro-
gram, grade, computes the log-probability of a set
of translations, as outline above.
We privately implemented a simple stack decoder
that searched over permutations of phrases, similar
to Koehn (2004). Our implementation increased the
codebase by 44 lines of code and included param-
eters for beam size, distortion limit, and the maxi-
mum number of translations considered for each in-
put phrase. We posted a baseline to the leaderboard
using values of 50, 3, and 20 for these, respectively.
11We implemented a version of the Lagrangian relaxation algo-
rithm of Chang and Collins (2011), but found it difficult to
obtain tight (optimal) solutions without iteratively reintroduc-
ing all of the original constraints. We suspect this is due to
the lack of a distortion penalty, which enforces a strong pref-
erence towards translations with little reordering. However,
the solution found by this algorithm is only approximates the
objective implied by Equation 2, which sums over alignments.
We also posted an oracle containing the most prob-
able output for each sentence, selected from among
all submissions received so far. The intent of this
oracle was to provide a lower bound on the best pos-
sible output, giving students additional incentive to
continue improving their systems.
4.3 Challenge Results
We received 71 submissions from 10 teams (Fig-
ure 2), again exhibiting variety of solutions.
? Implementation of greedy decoder which at
each step chooses the most probable translation
from among those reachable by a single swap
or retranslation (Germann et al, 2001; Langlais
et al, 2007).
? Inclusion of heuristic estimates of future cost.
? Implementation of a private oracle. Some stu-
dents observed that the ideal beam setting was
not uniform across the corpus. They ran their
decoder under different settings, and then se-
lected the most probable translation of each
sentence.
Many teams who implemented the standard stack
decoding algorithm experimented heavily with its
pruning parameters. The best submission used ex-
tremely wide beam settings in conjunction with a
reimplementation of the future cost estimate used in
Moses (Koehn et al, 2007). Five of the submissions
beat Moses using its standard beam settings after it
had been configured to decode with our model.
We used this assignment to emphasize the im-
portance of good models: the model score of the
submissions was generally inversely correlated with
BLEU, possibly because our simple model had no
distortion limits. We used this to illustrate the differ-
ence between model error and search error, includ-
ing fortuitous search error (Germann et al, 2001)
made by decoders with less accurate search.
5 The Evaluation Challenge
The third challenge was evaluation: given a test cor-
pus with reference translations and the output of sev-
eral MT systems, students were challenged to pro-
duce a ranking of the systems that closely correlated
with a human ranking.
170
Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.
stacks = [{} for _ in f] + [{}]
stacks[0][lm.begin()] = initial_hypothesis
for i, stack in enumerate(stacks[:-1]):
for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:
for j in xrange(i+1,len(f)+1):
if f[i:j] in tm:
for phrase in tm[f[i:j]]:
logprob = h.logprob + phrase.logprob
lm_state = h.lm_state
for word in phrase.english.split():
(lm_state, word_logprob) = lm.score(lm_state, word)
logprob += word_logprob
logprob += lm.end(lm_state) if j == len(f) else 0.0
new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
if lm_state not in stacks[j] or \
stacks[j][lm_state].logprob < logprob:
stacks[j][lm_state] = new_hypothesis
winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
def extract_english(h):
return "" if h.predecessor is None else "%s%s " %
(extract_english(h.predecessor), h.phrase.english)
print extract_english(winner)
Algorithm 1 Basic stack decoding algorithm,
adapted from Koehn (2010), p. 165.
place empty hypothesis into stack 0
for all stacks 0...n? 1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hypothesis
place in stack
recombine with existing hypothesis
prune stack if too big
5.1 Data
We chose the English-to-German translation sys-
tems from the 2009 and 2011 shared task at the an-
nual Workshop for Machine Translation (Callison-
Burch et al, 2009; Callison-Burch et al, 2011), pro-
viding the first as development data and the second
as test data. We chose these sets because BLEU
(Papineni et al, 2002), our baseline metric, per-
formed particularly poorly on them; this left room
for improvement in addition to highlighting some
lo
g 1
0
p(
e|f
)?
C
-1200
-1250
-1300
-1350
-1400
-20
days
-18
days
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 2: Submission history for the decoding challenge.
The dotted green line represents the oracle over submis-
sions.
deficiencies of BLEU. For each dataset we pro-
vided the source and reference sentences along with
anonymized system outputs. For the development
data we also provided the human ranking of the sys-
171
tems, computed from pairwise human judgements
according to a formula recommended by Bojar et al
(2011).12
5.2 Implementation
We provided three simple Python programs:
evaluate implements a simple ranking of the sys-
tems based on position-independent word error rate
(PER; Tillmann et al, 1997), which computes a bag-
of-words overlap between the system translations
and the reference. The grade program computes
Spearman?s ? between the human ranking and an
output ranking. The check program simply ensures
that a submission contains a valid ranking.
We were concerned about hill-climbing on the test
data, so we modified the leaderboard to report new
results only twice a day. This encouraged students to
experiment on the development data before posting
new submissions, while still providing intermittent
feedback.
We privately implemented a version of BLEU,
which obtained a correlation of 38.6 with the human
rankings, a modest improvement over the baseline
of 34.0. Our implementation underperforms the one
reported in Callison-Burch et al (2011) since it per-
forms no tokenization or normalization of the data.
This also left room for improvement.
5.3 Evaluation Challenge Results
We received 212 submissions from 12 teams (Fig-
ure 3), again demonstrating a wide range of tech-
niques.
? Experimentation with the maximum n-gram
length and weights in BLEU.
? Implementation of smoothed versions of BLEU
(Lin and Och, 2004).
? Implementation of weighted F-measure to bal-
ance both precision and recall.
? Careful normalization of the reference and ma-
chine translations, including lowercasing and
punctuation-stripping.
12This ranking has been disputed over a series of papers (Lopez,
2012; Callison-Burch et al, 2012; Koehn, 2012). The paper
which initiated the dispute, written by the first author, was di-
rectly inspired by the experience of designing this assignment.
Sp
ea
rm
an
?s
?
0.8
0.6
0.4
-7
days
-6
days
-5
days
-4
days
-3
days
-2
days
-1
days
due
Figure 3: Submission history for the evaluation chal-
lenge.
? Implementation of several techniques used in
AMBER (Chen and Kuhn, 2005).
The best submission, obtaining a correlation of
83.5, relied on the idea that the reference and ma-
chine translation should be good paraphrases of each
other (Owczarzak et al, 2006; Kauchak and Barzi-
lay, 2006). It employed a simple paraphrase sys-
tem trained on the alignment challenge data, us-
ing the pivot technique of Bannard and Callison-
Burch (2005), and computing the optimal alignment
between machine translation and reference under a
simple model in which words could align if they
were paraphrases. When compared with the 20
systems submitted to the original task from which
the data was obtained (Callison-Burch et al, 2011),
this system would have ranked fifth, quite near the
top-scoring competitors, whose correlations ranged
from 88 to 94.
6 The Reranking Challenge
The fourth challenge was reranking: given a test cor-
pus and a large N -best list of candidate translations
for each sentence, students were challenged to select
a candidate translation for each sentence to produce
a high corpus-level BLEU score. Due to an error
our data preparation, this assignment had a simple
solution that was very difficult to improve on. Nev-
ertheless, it featured several elements that may be
useful for future courses.
172
6.1 Data
We obtained 300-best lists from a Spanish-English
translation system built with the Joshua toolkit
(Ganitkevitch et al, 2012) using data and resources
from the 2011 Workshop on Machine Translation
(Callison-Burch et al, 2011). We provided 1989
training sentences, consisting of source and refer-
ence sentences along with the candidate translations.
We also included a test set of 250 sentences, for
which we provided only the source and candidate
translations. Each candidate translation included six
features from the underlying translation system, out
of an original 21; our hope was that students might
rediscover some features through experimentation.
6.2 Implementation
We conceived of the assignment as one in which stu-
dents could apply machine learning or feature engi-
neering to the task of reranking the systems, so we
provided several tools. The first of these, learn,
was a simple program that produced a vector of
feature weights using pairwise ranking optimization
(PRO; Hopkins and May, 2011), with a perceptron
as the underlying learning algorithm. A second,
rerank, takes a weight vector as input and reranks
the sentences; both programs were designed to work
with arbitrary numbers of features. The grade pro-
gram computed the BLEU score on development
data, while check ensured that a test submission
is valid. Finally, we provided an oracle program,
which computed a lower bound on the achievable
BLEU score on the development data using a greedy
approximation (Och et al, 2004). The leaderboard
likewise displayed an oracle on test data. We did
not assign a target alorithm, but left the assignment
fully open-ended.
6.3 Reranking Challenge Outcome
For each assignment, we made an effort to create
room for competition above the target alorithm.
However, we did not accomplish this in the rerank-
ing challenge: we had removed most of the features
from the candidate translations, in hopes that stu-
dents might reinvent some of them, but we left one
highly predictive implicit feature in the data: the
rank order of the underlying translation system. Stu-
dents discovered that simply returning the first can-
didate earned a very high score, and most of them
quickly converged to this solution. Unfortunately,
the high accuracy of this baseline left little room for
additional competition. Nevertheless, we were en-
couraged that most students discovered this by acci-
dent while attempting other strategies to rerank the
translations.
? Experimentation with parameters of the PRO
algorithm.
? Substitution of alternative learning algorithms.
? Implementation of a simplified minimum
Bayes risk reranker (Kumar and Byrne, 2004).
Over a baseline of 24.02, the latter approach ob-
tained a BLEU of 27.08, nearly matching the score
of 27.39 from the underlying system despite an im-
poverished feature set.
7 Pedagogical Outcomes
Could our students have obtained similar results by
running standard toolkits? Undoubtedly. However,
our goal was for students to learn by doing: they
obtained these results by implementing key MT al-
gorithms, observing their behavior on real data, and
improving them. This left them with much more in-
sight into how MT systems actually work, and in
this sense, DREAMT was a success. At the end of
class, we requested written feedback on the design
of the assignments. Many commented positively on
the motivation provided by the challenge problems:
? The immediate feedback of the automatic grad-
ing was really nice.
? Fast feedback on my submissions and my rela-
tive position on the leaderboard kept me both
motivated to start the assignments early and to
constantly improve them. Also knowing how
well others were doing was a good way to
gauge whether I was completely off track or not
when I got bad results.
? The homework assignments were very engag-
ing thanks to the clear yet open-ended setup
and their competitive aspects.
Students also commented that they learned a lot
about MT and even research in general:
173
Question 1 2 3 4 5 N/A
Feedback on my work for this course is useful - - - 4 9 3
This course enhanced my ability to work effectively in a team 1 - 5 8 2 -
Compared to other courses at this level, the workload for this course is high - 1 7 6 1 1
Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
? I learned the most from the assignments.
? The assignments always pushed me one step
more towards thinking out loud how the par-
ticular task can be completed.
? I appreciated the setup of the homework prob-
lems. I think it has helped me learn how to
set up and attack research questions in an or-
ganized way. I have a much better sense for
what goes into an MT system and what prob-
lems aren?t solved.
We also received feedback through an anonymous
survey conducted at the end of the course before
posting final grades. Each student rated aspects
of the course on a five point Likert scale, from 1
(strongly disagree) to 5 (strongly agree). Several
questions pertained to assignments (Table 1), and al-
lay two possible concerns about competition: most
students felt that the assignments enhanced their col-
laborative skills, and that their open-endedness did
not result in an overload of work. For all survey
questions, student satisfaction was higher than av-
erage for courses in our department.
8 Discussion
DREAMT is inspired by several different ap-
proaches to teaching NLP, AI, and computer sci-
ence. Eisner and Smith (2008) teach NLP using
a competitive game in which students aim to write
fragments of English grammar. Charniak et al
(2000) improve the state-of-the-art in a reading com-
prehension task as part of a group project. Christo-
pher et al (1993) use NACHOS, a classic tool for
teaching operating systems by providing a rudimen-
tary system that students then augment. DeNero and
Klein (2010) devise a series of assignments based
on Pac-Man, for which students implement several
classic AI techniques. A crucial element in such ap-
proaches is a highly functional but simple scaffold-
ing. The DREAMT codebase, including grading and
validation scripts, consists of only 656 lines of code
(LOC) over four assignments: 141 LOC for align-
ment, 237 LOC for decoding, 86 LOC for evalua-
tion, and 192 LOC for reranking. To simplify imple-
mentation further, the optional leaderboard could be
delegated to Kaggle.com, a company that organizes
machine learning competitions using a model sim-
ilar to the Netflix Challenge (Bennet and Lanning,
2007), and offers pro bono use of its services for
educational challenge problems. A recent machine
learning class at Oxford hosted its assignments on
Kaggle (Phil Blunsom, personal communication).
We imagine other uses of DREAMT. It could be
used in an inverted classroom, where students view
lecture material outside of class and work on prac-
tical problems in class. It might also be useful in
massive open online courses (MOOCs). In this for-
mat, course material (primarily lectures and quizzes)
is distributed over the internet to an arbitrarily large
number of interested students through sites such as
coursera.org, udacity.com, and khanacademy.org. In
many cases, material and problem sets focus on spe-
cific techniques. Although this is important, there is
also a place for open-ended problems on which stu-
dents apply a full range of problem-solving skills.
Automatic grading enables them to scale easily to
large numbers of students.
On the scientific side, the scale of MOOCs might
make it possible to empirically measure the effec-
tiveness of hands-on or competitive assignments,
by comparing course performance of students who
work on them against that of those who do not.
Though there is some empirical work on competi-
tive assignments in the computer science education
literature (Lawrence, 2004; Garlick and Akl, 2006;
Regueras et al, 2008; Ribeiro et al, 2009), they
generally measure student satisfaction and retention
rather than the more difficult question of whether
such assignments actually improve student learning.
However, it might be feasible to answer such ques-
174
tions in large, data-rich virtual classrooms offered
by MOOCs. This is an interesting potential avenue
for future work.
Because our class came within reach of state-of-
the-art on each problem within a matter of weeks,
we wonder what might happen with a very large
body of competitors. Could real innovation oc-
cur? Could we solve large-scale problems? It may
be interesting to adopt a different incentive struc-
ture, such as one posed by Abernethy and Frongillo
(2011) for crowdsourcing machine learning prob-
lems: rather than competing, everyone works to-
gether to solve a shared task, with credit awarded
proportional to the contribution that each individual
makes. In this setting, everyone stands to gain: stu-
dents learn to solve problems as they are found in
the real world, instructors learn new insights into the
problems they pose, and, in the long run, users of
AI technology benefit from overall improvements.
Hence it is possible that posing open-ended, real-
world problems to students might be a small piece
of the puzzle of providing high-quality NLP tech-
nologies.
Acknowledgments
We are grateful to Colin Cherry and Chris Dyer
for testing the assignments in different settings and
providing valuable feedback, and to Jessie Young
for implementing a dual decomposition solution to
the decoding assignment. We thank Jason Eis-
ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,
Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-
ney Napoles, Michael Rushanan, Joanne Selinski,
Svitlana Volkova, and the anonymous reviewers for
lively discussion and helpful comments on previous
drafts of this paper. Any errors are our own.
References
J. Abernethy and R. M. Frongillo. 2011. A collaborative
mechanism for crowdsourcing prediction problems. In
Proc. of NIPS.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
J. Bennet and S. Lanning. 2007. The netflix prize. In
Proc. of the KDD Cup and Workshop.
S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008.
Multidisciplinary instruction with the natural language
toolkit. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
O. Bojar, M. Ercegovc?evic?, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2).
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 work-
shop on statistical machine translation. In Proc. of
WMT.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. In Proc. of EMNLP.
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,
M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,
W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Read-
ing comprehension programs in a statistical-language-
processing class. In Proc. of Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
B. Chen and R. Kuhn. 2005. AMBER: A modified
BLEU, enhanced ranking metric. In Proc. of WMT.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
W. A. Christopher, S. J. Procter, and T. E. Anderson.
1993. The nachos instructional operating system. In
Proc. of USENIX.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL.
J. DeNero and D. Klein. 2010. Teaching introductory
articial intelligence with Pac-Man. In Proc. of Sym-
posium on Educational Advances in Artificial Intelli-
gence.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
J. Eisner and N. A. Smith. 2008. Competitive grammar
writing. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
175
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3).
J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and
C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and paraphrases. In Proc. of WMT.
R. Garlick and R. Akl. 2006. Intra-class competitive
assignments in CS2: A one-year study. In Proc. of
International Conference on Engineering Education.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. of ACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing. Prentice Hall, 2nd edition.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
D. Klein. 2005. A core-tools statistical NLP course. In
Proc. of Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL.
K. Knight. 1999a. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4).
K. Knight. 1999b. A statistical MT tutorial workbook.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. of AMTA.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn. 2012. Simulating human judgment in machine
translation evaluation campaigns. In Proc. of IWSLT.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
P. Langlais, A. Patry, and F. Gotti. 2007. A greedy de-
coder for phrase-based statistical machine translation.
In Proc. of TMI.
R. Lawrence. 2004. Teaching data structures using
competitive games. IEEE Transactions on Education,
47(4).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3).
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3).
A. Lopez. 2012. Putting human assessments of machine
translation systems in order. In Proc. of WMT.
N. Madnani and B. Dorr. 2008. Combining open-source
with research to re-engineer a hands-on introductory
NLP course. In Proc. of Workshop on Issues in Teach-
ing Computational Linguistics.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2).
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proc. on Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2004. Improving IBM word alignment
model 1. In Proc. of ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
NAACL.
K. Owczarzak, D. Groves, J. V. Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
L. Regueras, E. Verdu?, M. Verdu?, M. Pe?rez, J. de Castro,
and M. Mun?oz. 2008. Motivating students through
on-line competition: An analysis of satisfaction and
learning styles.
P. Ribeiro, M. Ferreira, and H. Simo?es. 2009. Teach-
ing artificial intelligence and logic programming in a
competitive environment. Informatics in Education,
(Vol 8 1):85.
J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,
N. Padua-Perez, and F. Emad. 2006. Experiences with
marmoset: Designing and using an advanced submis-
sion and testing system for programming courses. In
Proc. of Innovation and technology in computer sci-
ence education.
176
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In Proc. of European Conf. on Speech
Communication and Technology.
M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009.
Phrase-based statistical machine translation as a trav-
eling salesman problem. In Proc. of ACL.
177
178
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 256?264,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Monolingual Distributional Similarity for Text-to-Text Generation
Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch
Center for Language and Speech Processing
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
Abstract
Previous work on paraphrase extraction and
application has relied on either parallel
datasets, or on distributional similarity met-
rics over large text corpora. Our approach
combines these two orthogonal sources of in-
formation and directly integrates them into
our paraphrasing system?s log-linear model.
We compare different distributional similar-
ity feature-sets and show significant improve-
ments in grammaticality and meaning reten-
tion on the example text-to-text generation
task of sentence compression, achieving state-
of-the-art quality.
1 Introduction
A wide variety of applications in natural language
processing can be cast in terms of text-to-text gen-
eration. Given input in the form of natural lan-
guage, a text-to-text generation system produces
natural language output that is subject to a set of
constraints. Compression systems, for instance, pro-
duce shorter sentences. Paraphrases, i.e. differ-
ing textual realizations of the same meaning, are a
crucial components of text-to-text generation sys-
tems, and have been successfully applied to tasks
such as multi-document summarization (Barzilay et
al., 1999; Barzilay, 2003), query expansion (An-
ick and Tipirneni, 1999; Riezler et al, 2007), ques-
tion answering (McKeown, 1979; Ravichandran and
Hovy, 2002), sentence compression (Cohn and La-
pata, 2008; Zhao et al, 2009), and simplification
(Wubben et al, 2012).
Paraphrase collections for text-to-text generation
have been extracted from a variety of different cor-
pora. Several approaches rely on bilingual paral-
lel data (Bannard and Callison-Burch, 2005; Zhao
et al, 2008; Callison-Burch, 2008; Ganitkevitch et
al., 2011), while others leverage distributional meth-
ods on monolingual text corpora (Lin and Pantel,
2001; Bhagat and Ravichandran, 2008). So far, how-
ever, only preliminary studies have been undertaken
to combine the information from these two sources
(Chan et al, 2011).
In this paper, we describe an extension of Gan-
itkevitch et al (2011)?s bilingual data-based ap-
proach. We augment the bilingually-sourced para-
phrases using features based on monolingual distri-
butional similarity. More specifically:
? We show that using monolingual distributional
similarity features improves paraphrase quality
beyond what we can achieve with features esti-
mated from bilingual data.
? We define distributional similarity for para-
phrase patterns that contain constituent-level
gaps, e.g.
sim(one JJ instance of NP , a JJ case of NP).
This generalizes over distributional similarity
for contiguous phrases.
? We compare different types of monolingual
distributional information and show that they
can be used to achieve significant improve-
ments in grammaticality.
? Finally, we compare our method to several
strong baselines on the text-to-text generation
task of sentence compression. Our method
shows state-of-the-art results, beating a purely
bilingually sourced paraphrasing system.
256
... ihre Pl?ne w?rden
their plans would
...
ohne aufzugeben
without
langfristigen
in the long term
langfristigen
long-term
...
... ...
...Pl?ne
plans
seine
giving up his
......
Figure 1: Pivot-based paraphrase extraction for con-
tiguous phrases. Two phrases translating to the same
phrase in the foreign language are assumed to be
paraphrases of one another.
2 Background
Approaches to paraphrase extraction differ based on
their underlying data source. In Section 2.1 we out-
line pivot-based paraphrase extraction from bilin-
gual data, while the contextual features used to de-
termine closeness in meaning in monolingual ap-
proaches is described in Section 2.2.
2.1 Paraphrase Extraction via Pivoting
Following Ganitkevitch et al (2011), we formulate
our paraphrases as a syntactically annotated syn-
chronous context-free grammar (SCFG) (Aho and
Ullman, 1972; Chiang, 2005). An SCFG rule has
the form:
r = C ? ?f, e,?, ~??,
where the left-hand side of the rule, C, is a nonter-
minal and the right-hand sides f and e are strings
of terminal and nonterminal symbols. There is a
one-to-one correspondency between the nontermi-
nals in f and e: each nonterminal symbol in f has
to also appear in e. The function ? captures this bi-
jective mapping between the nonterminals. Drawing
on machine translation terminology, we refer to f as
the source and e as the target side of the rule.
Each rule is annotated with a feature vector of fea-
ture functions ~? = {?1...?N} that, using a corre-
sponding weight vector ~?, are combined in a log-
linear model to compute the cost of applying r:
cost(r) = ?
N?
i=1
?i log?i. (1)
A wide variety of feature functions can be formu-
lated. We detail the feature-set used in our experi-
ments in Section 4.
NP NN
NP
EU
NN NP
NP
intentions
's
EUder 
......?
h i
's
in the long term
in the long term 
langfristigen Pl?ne
the long-term of
Europeofthe long-term plans
IBM goals
IBM 's
's in the long term 
l?ngerfristige Ziele
IBMofthe long-term ambitions
..
Figure 2: Extraction of syntactic paraphrases via the
pivoting approach: We aggregate over different sur-
face realizations, matching the lexicalized portions
of the rule and generalizing over the nonterminals.
To extract paraphrases we follow the intuition that
two English strings e1 and e2 that translate to the
same foreign string f can be assumed to have the
same meaning, as illustrated in Figure 1.1
First, we use standard machine translation meth-
ods to extract a foreign-to-English translation gram-
mar from a bilingual parallel corpus (Koehn, 2010).
Then, for each pair of translation rules where the
left-hand side C and foreign string f match:
r1 = C ? ?f, e1,?1, ~?1?
r2 = C ? ?f, e2,?2, ~?2?,
we pivot over f to create a paraphrase rule rp:
rp = C ? ?e1, e2,?p, ~?p?,
with a combined nonterminal correspondency func-
tion ?p. Note that the common source side f im-
plies that e1 and e2 share the same set of nonterminal
symbols.
The paraphrase feature vector ~?p is computed
from the translation feature vectors ~?1 and ~?2 by
following the pivoting idea. For instance, we esti-
mate the conditional paraphrase probability p(e2|e1)
by marginalizing over all shared foreign-language
translations f :
p(e2|e1) =
?
f
p(e2, f |e1) (2)
=
?
f
p(e2|f, e1)p(f |e1) (3)
?
?
f
p(e2|f)p(f |e1). (4)
1See Yao et al (2012) for an analysis of this assumption.
257
twelve cartoons insulting the prophet mohammad
CD NNS JJ DT
NNP
NP
NP
VP
NP
DT+NNP
12 the prophet mohammad
CD NNS JJ
DT
NNP
NP
NP
VP
NP
DT+NNP
cartoons offensiveof the that are to
Figure 3: An example of a synchronous paraphras-
tic derivation, here a sentence compression. Shaded
words are deleted in the indicated rule applications.
Figure 2 illustrates syntax-constrained pivoting and
feature aggregation over multiple foreign language
translations for a paraphrase pattern.
After the SCFG has been extracted, it can be used
within standard machine translation machinery, such
as the Joshua decoder (Ganitkevitch et al, 2012).
Figure 3 shows an example for a synchronous para-
phrastic derivation produced as a result of applying
our paraphrase grammar in the decoding process.
The approach outlined relies on aligned bilingual
texts to identify phrases and patterns that are equiva-
lent in meaning. When extracting paraphrases from
monolingual text, we have to rely on an entirely dif-
ferent set of semantic cues and features.
2.2 Monolingual Distributional Similarity
Methods based on monolingual text corpora mea-
sure the similarity of phrases based on contextual
features. To describe a phrase e, we define a set of
features that capture the context of an occurrence of
e in our corpus. Writing the context vector for the
i-th occurrence of e as ~se,i, we can aggregate over
all occurrences of e, resulting in a distributional sig-
nature for e, ~se =
?
i ~se,i. Following the intuition
that phrases with similar meanings occur in similar
contexts, we can then quantify the goodness of e? as
a paraphrase of e by computing the cosine similarity
between their distributional signatures:
sim(e, e?) = ~se ? ~se?
|~se||~se? |
.
A wide variety of features have been used to de-
scribe the distributional context of a phrase. Rich,
linguistically informed feature-sets that rely on de-
pendency and constituency parses, part-of-speech
tags, or lemmatization have been proposed in widely
known work such as by Church and Hanks (1991)
and Lin and Pantel (2001). For instance, a phrase
is described by the various syntactic relations it has
with lexical items in its context, such as: ?for what
verbs do we see with the phrase as the subject??, or
?what adjectives modify the phrase??.
However, when moving to vast text collections or
collapsed representations of large text corpora, lin-
guistic annotations can become impractically expen-
sive to produce. A straightforward and widely used
solution is to fall back onto lexical n-gram features,
e.g. ?what words or bigrams have we seen to the left
of this phrase?? A substantial body of work has fo-
cussed on using this type of feature-set for a variety
of purposes in NLP (Lapata and Keller, 2005; Bha-
gat and Ravichandran, 2008; Lin et al, 2010; Van
Durme and Lall, 2010).
2.3 Other Related Work
Recently, Chan et al (2011) presented an initial in-
vestigation into combining phrasal paraphrases ob-
tained through bilingual pivoting with monolingual
distributional information. Their work investigated
a reranking approach and evaluated their method via
a substitution task, showing that the two sources of
information are complementary and can yield im-
provements in paraphrase quality when combined.
3 Incorporating Distributional Similarity
In order to incorporate distributional similarity in-
formation into the paraphrasing system, we need
to calculate similarity scores for the paraphrastic
SCFG rules in our grammar. For rules with purely
lexical right-hand sides e1 and e2 this is a simple
task, and the similarity score sim(e1, e2) can be di-
rectly included in the rule?s feature vector ~?. How-
ever, if e1 and e2 are long, their occurrences become
sparse and their similarity can no longer be reliably
estimated. In our case, the right-hand sides of our
rules often contain gaps and computing a similarity
score is less straightforward.
Figure 4 shows an example of such a discontin-
uous rule and illustrates our solution: we decom-
pose the discontinuous patterns that make up the
258
NP
the's NP
ofNP
long
long-term
term
the
inNN
NN
the long-term
in the long term
's
of
?
+ sim
? ?!
sim(r) = 12
 
sim
?
Figure 4: Scoring a rule by extracting and scoring
contiguous phrases consistent with the alignment.
The overall score of the rule is determined by av-
eraging across all pairs of contiguous subphrases.
right-hand sides of a rule r into pairs of contiguous
phrases P(r) = {?e, e??}, for which we can look
up distributional signatures and compute similarity
scores. This decomposition into phrases is non-
trivial, since our sentential paraphrase rules often
involve significant reordering or structural changes.
To avoid comparing unrelated phrase pairs, we re-
quire P(r) to be consistent with a token alignment
a. The alignment is defined analogously to word
alignments in machine translation, and computed by
treating the source and target sides of our paraphrase
rules as a parallel corpus.
We define the overall similarity score of the rule
to be the average of the similarity scores of all ex-
tracted phrase pairs:
sim(r,a) = 1
|P(a)|
?
(e,e?)?P(a)
sim(e, e?).
Since the distributional signatures for long, rare
phrases may be computed from only a handful of
occurrences, we additionally query for the shorter
sub-phrases that are more likely to have been ob-
served often enough to have reliable signatures and
thus similarity estimates.
Our definition of the similarity of two discon-
tinuous phrases substantially differs from others in
the literature. This difference is due to a differ-
ence in motivation. Lin and Pantel (2001), for in-
stance, seek to find new paraphrase pairs by compar-
ing their arguments. In this work, however, we try
to add orthogonal information to existing paraphrase
pairs. Both our definition of pattern similarity and
our feature-set (see Section 4.3) are therefore geared
towards comparing the substitutability and context
similarity of a pair of paraphrases.
Our two similarity scores are incorporated into
the paraphraser as additional rule features in ~?,
simngram and simsyn , respectively. We estimate the
corresponding weights along with the other ?i as de-
tailed in Section 4.
4 Experimental Setup
4.1 Task: Sentence Compression
To evaluate our method on a real text-to-text appli-
cation, we use the sentence compression task. To
tune the parameters of our paraphrase system for
sentence compression, we need an appropriate cor-
pus of reference compressions. Since our model is
designed to compress by paraphrasing rather than
deletion, the commonly used deletion-based com-
pression data sets like the Ziff-Davis corpus are not
suitable. We thus use the dataset introduced in our
previous work (Ganitkevitch et al, 2011).
Beginning with 9570 tuples of parallel English?
English sentences obtained from multiple reference
translations for machine translation evaluation, we
construct a parallel compression corpus by select-
ing the longest reference in each tuple as the source
sentence and the shortest reference as the target sen-
tence. We further retain only those sentence pairs
where the compression ratio cr falls in the range
0.5 < cr ? 0.8. From these, we select 936 sen-
tences for the development set, as well as 560 sen-
tences for a test set that we use to gauge the perfor-
mance of our system.
We contrast our distributional similarity-informed
paraphrase system with a pivoting-only baseline, as
well as an implementation of Clarke and Lapata
(2008)?s state-of-the-art compression model which
uses a series of constraints in an integer linear pro-
gramming (ILP) solver.
4.2 Baseline Paraphrase Grammar
We extract our paraphrase grammar from the
French?English portion of the Europarl corpus (ver-
sion 5) (Koehn, 2005). The Berkeley aligner (Liang
et al, 2006) and the Berkeley parser (Petrov and
Klein, 2007) are used to align the bitext and parse
the English side, respectively. The paraphrase gram-
mar is produced using the Hadoop-based Thrax
259
the long-term
achieve25
goals 23
plans 97
investment 10
confirmed64
revise43
Left Right
the long-term
the long-term
the long-term
the long-term
the long-term
.
.
.
.
L-achieve = 25
L-confirmed
= 64
L-revise = 43
?
R-goals 
= 23
R-plans  = 97
R-investment 
= 10
?
the long-term
?
=~signgram
?
Figure 5: An example of the n-gram feature extrac-
tion on an n-gram corpus. Here, ?the long-term? is
seen preceded by ?revise? (43 times) and followed
by ?plans? (97 times). The corresponding left- and
right-side features are added to the phrase signature
with the counts of the n-grams that gave rise to them.
grammar extractor?s paraphrase mode (Ganitkevitch
et al, 2012). The syntactic nonterminal labels we
allowed in the grammar were limited to constituent
labels and CCG-style slashed categories. Paraphrase
grammars extracted via pivoting tend to grow very
large. To keep the grammar size manageable, we
pruned away all paraphrase rules whose phrasal
paraphrase probabilities p(e1|e2) or p(e2|e1) were
smaller than 0.001.
We extend the feature-set used in Ganitkevitch et
al. (2011) with a number of features that aim to bet-
ter describe a rule?s compressive power: on top of
the word count features wcountsrc and wcount tgt
and the word count difference feature wcountdiff ,
we add character based count and difference features
ccountsrc , ccount tgt , and ccountdiff , as well as log-
compression ratio features wordcr = log wcount tgtwcountsrc
and the analogously defined charcr = log ccount tgtccountsrc .
For model tuning and decoding we used the
Joshua machine translation system (Ganitkevitch et
al., 2012). The model weights are estimated using an
implementation of the PRO tuning algorithm (Hop-
kins and May, 2011), with PRE?CIS as our objective
function (Ganitkevitch et al, 2011). The language
model used in our paraphraser and the Clarke and
Lapata (2008) baseline system is a Kneser-Ney dis-
counted 5-gram model estimated on the Gigaword
corpus using the SRILM toolkit (Stolcke, 2002).
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sigsyntax
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 6: An example of the syntactic feature-
set. The phrase ?the long-term? is annotated with
position-aware lexical and part-of-speech n-gram
features (e.g. ?on to? on the left, and ?investment?
and ?NN? to its right), labeled dependency links
(e.g. amod ? investment) and features derived
from the phrase?s CCG label NP/NN .
4.3 Distributional Similarity Model
To investigate the impact of the feature-set used to
construct distributional signatures, we contrast two
approaches: a high-coverage collection of distribu-
tional signatures with a relatively simple feature-set,
and a much smaller set of signatures with a rich, syn-
tactically informed feature-set.
4.3.1 n-gram Model
The high-coverage model (from here on: n-gram
model) is drawn from a web-scale n-gram corpus
(Brants and Franz, 2006; Lin et al, 2010). We ex-
tract signatures for phrases up to a length of 4. For
each phrase p we look at n-grams of the form wp
and pv, where w and v are single words. We then
extract the corresponding features wleft and vright .
The feature count is set to the count of the n-gram,
reflecting the frequency with which p was preceded
or followed, respectively, by w and v in the data the
n-gram corpus is based on. Figure 5 illustrates this
feature extraction approach. The resulting collection
comprises distributional signatures for the 200 mil-
lion most frequent 1-to-4-grams in the n-gram cor-
pus.
260
4.3.2 Syntactic Model
For the syntactically informed signature model
(from here on: syntax model), we use the
constituency and dependency parses provided in
the Annotated Gigaword corpus (Napoles et al,
2012). We limit ourselves to the Los Angeles
Times/Washington Post portion of the corpus and
extract phrases up to a length of 4. The following
feature set is used to compute distributional signa-
tures for the extracted phrases:
? Position-aware lexical and part-of-speech uni-
gram and bigram features, drawn from a three-
word window to the right and left of the phrase.
? Features based on dependencies for both links
into and out of the phrase, labeled with the cor-
responding lexical item and POS. If the phrase
corresponds to a complete subtree in the con-
stituency parse we additionally include lexical
and POS features for its head word.
? Syntactic features for any constituents govern-
ing the phrase, as well as for CCG-style slashed
constituent labels for the phrase. The latter are
split in governing constituent and missing con-
stituent (with directionality).
Figure 6 illustrates the syntax model?s feature ex-
traction for an example phrase occurrence. Using
this method we extract distributional signatures for
over 12 million 1-to-4-gram phrases.
4.3.3 Locality Sensitive Hashing
Collecting distributional signatures for a large
number of phrases quickly leads to unmanageably
large datasets. Storing the syntax model?s 12 mil-
lion signatures in a compressed readable format,
for instance, requires over 20GB of disk space.
Like Ravichandran et al (2005) and Bhagat and
Ravichandran (2008), we rely on locality sensitive
hashing (LSH) to make the use of these large collec-
tions practical.
In order to avoid explicitly computing the fea-
ture vectors, which can be memory intensive for fre-
quent phrases, we chose the online LSH variant de-
scribed by Van Durme and Lall (2010), as imple-
mented in the Jerboa toolkit (Van Durme, 2012).
This method, based on the earlier work of Indyk and
Motwani (1998) and Charikar (2002), approximates
the cosine similarity between two feature vectors
based on the Hamming distance in a dimensionality-
reduced bitwise representation. Two feature vec-
tors u, v each of dimension d are first projected
through a d?b random matrix populated with draws
from N (0, 1). We then convert the resulting b-
dimensional vectors into bit-vectors by setting each
bit of the signature conditioned on whether the cor-
responding projected value is less than 0. Now,
given the bit signatures h(~u) and h(~v), we can ap-
proximate the cosine similarity of u and v as:
sim ?(u, v) = cos
(D(h(~u), h(~v))
b pi
)
,
where d(?, ?) is the Hamming distance. In our ex-
periments we use 256-bit signatures. This reduces
the memory requirements for the syntax model to
around 600MB.
5 Evaluation Results
To rate the quality of our output, we solicit human
judgments of the compressions along two five-point
scales: grammaticality and meaning preservation.
Judges are instructed to decide how much the mean-
ing from a reference translation is retained in the
compressed sentence, with a score of 5 indicating
that all of the important information is present, and
1 being that the compression does not retain any of
the original meaning. Similarly, a grammar score
of 5 indicates perfect grammaticality, while a score
of 1 is assigned to sentences that are entirely un-
grammatical. We ran our evaluation on Mechani-
cal Turk, where a total of 126 judges provided 3 re-
dundant judgments for each system output. To pro-
vide additional quality control, our HITs were aug-
mented with both positive and negative control com-
pressions. For the positive control we used the refer-
ence compressions from our test set. Negative con-
trol was provided by adding a compression model
based on random word deletions to the mix.
In Table 1 we compare our distributional
similarity-augmented systems to the plain pivoting-
based baseline and the ILP approach. The compres-
sion ratios of the paraphrasing systems are tuned to
match the average compression ratio seen on the de-
velopment and test set. The ILP system is config-
261
ured to loosely match this ratio, as to not overly con-
strain its search space. Our results indicate that the
paraphrase approach significantly outperforms ILP
on meaning retention. However, the baseline sys-
tem shows notable weaknesses in grammaticality.
Adding the n-gram distributional similarity model
to the paraphraser recovers some of the difference in
grammaticality while simultaneously yielding some
gain in the compressions? meaning retention. Mov-
ing to distributional similarity estimated on the syn-
tactic feature-set yields additional improvement, de-
spite the model?s lower coverage.
It is known that human evaluation scores correlate
linearly with the compression ratio produced by a
sentence compression system (Napoles et al, 2011).
Thus, to ensure fairness in our comparisons, we pro-
duce a pairwise comparison breakdown that only
takes into account compressions of almost identical
length.2 Figure 7 shows the results of this analysis,
detailing the number of wins and ties in the human
judgements.
We note that the gains in meaning retention over
both the baseline and the ILP system are still present
in the pairwise breakdown. The gains over the
paraphrasing baseline, as well as the improvement
in meaning over ILP are statistically significant at
p < 0.05 (using the sign test).
We can observe that there is substantial overlap
between the baseline paraphraser and the n-gram
model, while the syntax model appears to yield no-
ticeably different output far more often.
Table 2 shows two example sentences drawn from
our test set and the compressions produced by the
different systems. It can be seen that both the
paraphrase-based and ILP systems produce good
quality results, with the paraphrase system retaining
the meaning of the source sentence more accurately.
6 Conclusion
We presented a method to incorporate monolingual
distributional similarity into linguistically informed
paraphrases extracted from bilingual parallel data.
Having extended the notion of similarity to dis-
contiguous pattern with multi-word gaps, we inves-
tigated the effect of using feature-sets of varying
2We require the compressions to be within ?10% length of
one another.
Score 0
50100
150200
250300
050
100150
200250
300
Syntax :: ILP Syntax :: n?gram n?gram :: PP
Grammar
Meaning
Figure 7: A pairwise breakdown of the human judg-
ments comparing the systems. Dark grey regions
show the number of times the two systems were tied,
and light grey shows how many times one system
was judged to be better than the other.
CR Meaning Grammar
Reference 0.80 4.80 4.54
ILP 0.74 3.44 3.41
PP 0.78 3.53 2.98
PP + n-gram 0.80 3.65 3.16
PP + syntax 0.79 3.70 3.26
Random Deletions 0.78 2.91 2.53
Table 1: Results of the human evaluation on longer
compressions: pairwise compression rates (CR),
meaning and grammaticality scores. Bold indicates
a statistically significance difference at p < 0.05.
complexity to compute distributional similarity for
our paraphrase collection. We conclude that, com-
pared to a simple large-scale model, a rich, syntax-
based feature-set, even with significantly lower cov-
erage, noticeably improves output quality in a text-
to-text generation task. Our syntactic method sig-
nificantly improves grammaticality and meaning re-
tention over a strong paraphrastic baseline, and of-
fers substantial gains in meaning retention over a
deletion-based state-of-the-art system.
Acknowledgements This research was supported
in part by the NSF under grant IIS-0713448 and
in part by the EuroMatrixPlus project funded by
the European Commission (7th Framework Pro-
gramme). Opinions, interpretations, and conclu-
sions are the authors? alone.
262
Source should these political developments have an impact on sports ?
Reference should these political events affect sports ?
Syntax should these events have an impact on sports ?
n-gram these political developments impact on sports ?
PP should these events impact on sports ?
ILP political developments have an impact
Source now we have to think and make a decision about our direction and choose only one way .
thanks .
Reference we should ponder it and decide our path and follow it , thanks .
Syntax now we think and decide on our way and choose one way . thanks .
n-gram now we have and decide on our way and choose one way . thanks .
PP now we have and decide on our way and choose one way . thanks .
ILP we have to think and make a decision and choose way thanks
Table 2: Example compressions produced by our systems and the baselines Table 1 for three input sentences
from our test data.
References
Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory
of Parsing, Translation, and Compiling. Prentice Hall.
Peter G. Anick and Suresh Tipirneni. 1999. The para-
phrase search assistant: terminological feedback for
iterative information seeking. In Proceedings of SI-
GIR.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
Regina Barzilay, Kathleen R. McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In Proceedings of
ACL.
Regina Barzilay. 2003. Information Fusion for Mutli-
document Summarization: Paraphrasing and Genera-
tion. Ph.D. thesis, Columbia University, New York.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL/HLT.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
version 1.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin Van
Durme. 2011. Reranking bilingually extracted para-
phrases using monolingual distributional similarity. In
EMNLP Workshop on GEMS.
Moses Charikar. 2002. Similarity estimation techniques
from rounding algorithms. In Proceedings of STOC.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
ACL.
Kenneth Church and Patrick Hanks. 1991. Word asso-
ciation norms, mutual information and lexicography.
Computational Linguistics, 6(1):22?29.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:273?381.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of the
COLING.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
263
and Chris Callison-Burch. 2012. Joshua 4.0: Packing,
PRO, and paraphrases. In Proceedings of WMT12.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Piotr Indyk and Rajeev Motwani. 1998. Approximate
nearest neighbors: towards removing the curse of di-
mensionality. In Proceedings of STOC.
Philipp Koehn. 2005. Europarl: A parallel corpus for sta-
tistical machine translation. In MT summit, volume 5.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Mirella Lapata and Frank Keller. 2005. Web-based mod-
els for natural language processing. ACM Transac-
tions on Speech and Language Processing, 2(1).
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of LREC.
Kathleen R. McKeown. 1979. Paraphrasing using given
and new information in a question-answer system. In
Proceedings of ACL.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphrastic
sentence compression with a character-based metric:
Tightening without deletion. Workshop on Monolin-
gual Text-To-Text Generation.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In Proceedings of
HLT/NAACL.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
sufrace text patterns for a question answering system.
In Proceedings of ACL.
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.
2005. Randomized Algorithms and NLP: Using Lo-
cality Sensitive Hash Functions for High Speed Noun
Clustering. In Proceedings of ACL.
Stefan Riezler, Alexander Vasserman, Ioannis Tsochan-
taridis, Vibhu Mittal, and Yi Liu. 2007. Statistical
machine translation for query expansion in answer re-
trieval. In Proceedings of ACL.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceeding of the International
Conference on Spoken Language Processing.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Benjamin Van Durme. 2012. Jerboa: A toolkit for
randomized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Excel-
lence, Johns Hopkins University.
Sander Wubben, Antal van den Bosch, and Emiel Krah-
mer. 2012. Sentence simplification by monolingual
machine translation. In Proceedings of ACL.
Xuchen Yao, Benjamin Van Durme, and Chris Callison-
Burch. 2012. Expectations of word sense in parallel
corpora. In Proceedings of HLT/NAACL.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2008. Pivot approach for extracting paraphrase pat-
terns from bilingual corpora. In Proceedings of
ACL/HLT.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of ACL.
264
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 93?98,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Enriched MT Grammar for Under $100
Omar F. Zaidan and Juri Ganitkevitch
Dept. of Computer Science, Johns Hopkins University
Baltimore, MD 21218, USA
{ozaidan,juri}@cs.jhu.edu
Abstract
We propose a framework for improving out-
put quality of machine translation systems, by
operating on the level of grammar rule fea-
tures. Our framework aims to give a boost to
grammar rules that appear in the derivations
of translation candidates that are deemed to
be of good quality, hence making those rules
more preferable by the system. To that end, we
ask human annotators on Amazon Mechanical
Turk to compare translation candidates, and
then interpret their preferences of one candi-
date over another as an implicit preference for
one derivation over another, and therefore as
an implicit preference for one or more gram-
mar rules. Our framework also allows us to
generalize these preferences to grammar rules
corresponding to a previously unseen test set,
namely rules for which no candidates have
been judged.
1 Introduction
When translating between two languages, state-
of-the-art statistical machine translation sys-
tems (Koehn et al, 2007; Li et al, 2009) generate
candidate translations by relying on a set of relevant
grammar (or phrase table) entries. Each of those
entries, or rules, associates a string in the source
language with a string in the target language, with
these associations typically learned by examining
a large parallel bitext. By the very nature of the
translation process, a target side sentence e can
be a candidate translation for a source sentence f
only if e can be constructed using a small subset
of the grammar, namely the subset of rules with
source side sequences relevant to the word sequence
of f . However, even this limited set of candidates
(call it E(f)) is quite large, with |E(f)| growing
exponentially in the length of f . The system is able
to rank the translations within E(f) by assigning a
score s(e) to each candidate translation. This score
is the dot product:
s(e) = ~?(e) ? ~w (1)
where ~?(e) is a feature vector characterizing e, and
~w is a system-specific weight vector characterizing
the system?s belief of how much the different fea-
tures reflect translation quality. The features of a
candidate e are computed by examining the way e is
constructed (or derived), and so if we let d(e) be the
derivation of e, the feature vector can be denoted:1
~?(d(e)) = ??1(d(e)), . . . , ?m(d(e))? (2)
where ?i(d(e)) is the value of ith feature function
of d(e) (with a corresponding weight wi in ~w).
To compute the score for a candidate, we examine
its derivation d(e), enumerating the grammar rules
used to construct e: d(e) = (r1, . . . , rk). Typically,
each of the rules will itself have a vector of m fea-
tures, and we calculate the value of a derivation fea-
ture ?i(d(e)) as the sum of the ith feature over all
rules in the derivation:
?i(d(e)) =
?
r?d(e)
?i(r) (3)
1There are other features computed directly, without ex-
amining the derivation (e.g. candidate length, language model
score), but we omit these features from the motivation discus-
sion for clarity.
93
These features are usually either relative frequen-
cies estimated from the training corpus, relating the
rule?s source and target sides, or features that char-
acterize the structure of the rule itself, independently
from the corpus.
Either way, the weightwi is chosen so as to reflect
some belief regarding the correlation between the ith
feature and translation quality. This is usually done
by choosing weights that maximize performance on
a tuning set separate from the training bitext. Un-
like system weights, the grammar rule feature val-
ues are fixed once extracted, and are not modified
during this tuning phase. In this paper, we propose a
framework to augment the feature set to incorporate
additional intuition about how likely a rule is to pro-
duce a translation preferred by a human annotator.
This knowledge is acquired by directly asking hu-
man judges to compare candidate translations, there-
fore determining which subset of grammar rules an-
notators seem to prefer over others. We also seek to
generalize this intuition to rules for which no can-
didates were judged, hence allowing us to impact a
much larger set of rules than just those used in trans-
lating the tuning set.
The paper is organized as follows. We first give
a general description of our framework. We then
discuss our data collection efforts on Amazon Me-
chanical Turk for an Urdu-English translation task,
and make explicit the type of judgments we col-
lect and how they can be used to augment grammar
rules. Before concluding, we propose a framework
for generalizing judgments to unseen grammar rules,
and analyze the data collection process.
2 The General Framework
As initially mentioned, when tuning a SMT system
on a development set, we typically only perform
high-level optimization of the system weights. In
this section we outline an approach that could allow
for lower-level optimization, on the level of individ-
ual grammar rules.
We kick off the process by soliciting judgments
from human annotators regarding the quality of a
subset of candidates (the following section outlines
how candidates are chosen). The resulting judg-
ments on sentences are interpreted to be judgments
on individual grammar rules used in the derivations
of these candidates. And so, if an annotator declares
a candidate to be of high quality, this is considered
a vote of confidence on the individual rules giving
rise to this candidate, and if an annotator declares a
candidate to be of lowl quality, this is considered a
vote of no confidence on the individual rules.
To make use of the collected judgments, we ex-
tend the set of features used in the decoder by a new
feature ?:
~?? = ??1, . . . , ?m, ?? (4)
This feature is the cornerstone of our framework,
as it will hold the quantified and cumulated judg-
ments for each rule, and will be used by the system
at decoding time, in addition to the existing m fea-
tures, incorporating the annotators? judgments into
the translation process.2
The range of possible values for this feature, and
how the feature is computed, depends on how one
chooses to ask annotators to score candidates, and
what form those judgments assume (i.e. are those
judgments scores on a scale? Are they ?better?
vs. ?worse? judgments, and if so, compared to how
many other possibilities?). At this point, we will
only emphasize that the value of ? should reflect
the annotators? preference for the rule, and that it
should be computed from the collected judgments.
We will propose one such method of computing ? in
Section 4, after describing the type of judgments we
collected.
3 Data Collection
We apply our approach to an Urdu-to-English trans-
lation task. We used a syntactically rich SAMT
grammar (Venugopal and Zollmann, 2006), where
each rule in the grammar is characterized by 12 fea-
tures. The grammar was provided by Chris Callison-
Burch (personal communication), and was extracted
from a parallel corpus of 88k sentence pairs.3 One
system using this grammar produced significantly
improved output over submissions to the NIST 2009
Urdu-English task (Baker et al, 2009).
We use the Joshua system (Li et al, 2009)
as a decoder, with system weights tuned using
2In fact, the collected judgments can only cover a small por-
tion of the grammar. We address this coverage problem in Sec-
tion 4.
3LDC catalog number LDC2009E12.
94
Z-MERT (Zaidan, 2009) on a tuning set of 981 sen-
tences, a subset of the 2008 NIST Urdu-English test
set.4 We choose candidates to be judged from the
300-best candidate lists.5
Asking a worker to make a quantitative judgment
of the quality of a particular candidate translation
(e.g. on a 1?7 scale) is a highly subjective and
annotator-dependent process. Instead, we present
workers with pairs of candidates, and ask them to
judge which candidate is of better quality.
How are candidate pairs chosen? We would like
a judgment to have the maximum potential for be-
ing informative about specific grammar rules. In es-
sense, we prefer a pair of candidates if they have
highly similar derivations, yet differ noticeably in
terms of how the decoder ranks them. In other
words, if a relatively minimal change in derivation
causes a relatively large difference in the score as-
signed by the decoder, we are likely to attribute the
difference to very few rule comparisons (or perhaps
only one), hence focusing the comparison on indi-
vidual rules, all the while shielding the annotators
from having to compare grammar rules directly.
Specifically, each pair of candidates (e, e?) is as-
signed a potential score pi(e, e?), defined as:
pi(e, e?) =
s(e)s(e?)
lev(d(e),d(e?))
, (5)
where s(e) is the score assigned by the decoder,
and lev(d,d?) is a distance measure between two
derivations which we will now descibe in more de-
tail. In Joshua, the derivation of a candidate is cap-
tured fully and exactly by a derivation tree, and so
we define lev(d,d?) as a tree distance metric as fol-
lows. We first represent the trees as strings, using the
familiar nested string representation, then compute
the word-based Levenshtein edit distance between
the two strings. An edit has a cost of 1 in general, but
we assign a cost of zero to edit operations on termi-
nals, since we want to focus on the structure of the
derivation trees, rather than on terminal-level lexi-
cal choices.6 Furthermore, we ignore differences in
4LDC catalog number LDC2009E11.
5We exclude source sentences shorter than 4 words long or
that have fewer than 4 candidate translations. This eliminates
roughly 6% of the development set.
6This is not to say that lexical choices are not important, but
lexical choice is heavily influenced by context, which is not cap-
?pure? pre-terminal rules, that only have terminals
as their right-hand side. These decisions effectively
allow us to focus our efforts on grammar rules with
at least one nonterminal in their right-hand side.
We perform the above potential computation on
all pairs formed by the cross product of the top 10
candidates and the top 300 candidates, and choose
the top five pairs ranked by potential.
Our HIT template is rather simple. Each HIT
screen corresponds to a single source sentence,
which is shown to the worker along with the five
chosen candidate pairs. To aid workers who are not
fluent in Urdu7 better judge translation quality, the
HIT also displays one of the available references
for that source sentence. To eliminate potential bias
associated with the order in which candidates are
presented (an annotator might be biased to choos-
ing the first presented candidate, for example), we
present the two candidates in random or- der. Fur-
thermore, for quality assurance, we embed a sixth
candidate pair to be judged, where we pair up a ran-
domly chosen candidate with another reference for
that sentence.8 Presumably, a faithful worker would
be unlikely to prefer a random candidate over the
reference, and so this functions as an embedded self-
verification test. The order of this test, relative to the
five original pairs, is chosen randomly.
4 Incorporating the Judgements
4.1 Judgement Quantification
The judgments we obtain from the procedure de-
scribed in the previous section relate pairs of can-
didate translations. However, we have defined the
accumulation feature ? as a feature for each rule.
Thus, in order to compute ?, we need to project the
judgments onto the rules that tell the two candidates
apart. A simple way to do this is the following: for
a judged candidate pair (e, e?) let U(e) be the set of
tured well by grammar rules. Furthermore, lexical choice is a
phenomenon already well captured by the score assigned to the
candidate by the language model, a feature typically included
when designing ~?.
7We exclude workers from India and restrict the task to
workers with an existing approval rating of 90% or higher.
8The tuning set contains at least three different human refer-
ences for each source sentence, and so the reference ?candidate?
shown to the worker is not the same as the sentence already
identified as a reference.
95
rules that appear in d(e) but not in d(e?), and vice
versa.9 We will assume that the jugdment obtained
for (e, e?) applies for every rule pair in the cartesian
product of U(e) and U(e?). This expansion yields
a set of judged grammar rule pairs J = {(a, b)}
with associated vote counts va>b and vb>a, captur-
ing how often the annotators preferred a candidate
that was set apart by a over a candidate containing
b, and vice versa.
So, following our prior definiton as an expression
of the judges? preference, we can calculate the value
of ? for a rule r as the relative frequency of favorable
judgements:
?(r) =
?
(r,b)?J vr>b
?
(r,b)?J vb>r + vr>b
(6)
4.2 Generalization to Unseen Rules
This approach has a substantial problem: ?, com-
puted as given above, is undefined for a rule that
was never judged (i.e. a rule that never set apart
a pair of candidates presented to the annotators).
Furthermore, as described, the coverage of the col-
lected judgments will be limited to a small subset
of the entire grammar, meaning that when the sys-
tem is asked to translate a new source sentence, it
is highly unlikely that the relevant grammar rules
would have already been judged by an annotator.
Therefore, it is necessary to generalize the collected
judgments/votes and propagate them to previously
unexamined rules.
In order to do this, we propose the following gen-
eral approach: when observing a judgment for a pair
of rules (a, b) ? J , we view that judgement not as
a vote on one of them specifically, but rather as a
comparison of rules similar to a versus rules similar
to b. When calculating ?(r) for any rule r we use
a distance measure over rules, ?, to estimate how
each judgment in J projects to r. This leads to the
following modified computatio of ?(r):
?(r) =
?
(a,b)?J
?(a, r)v?b>a + ?(b, r)v
?
a>b
?(a, r) + ?(b, r)
(7)
9The way we select candidate pairs ensures that U(e) and
U(e?) are both small and expressive in terms of impact on the
decoder ranking. On our data U(e) contained an average of 4
rules.
where v?a>b (and analogously v
?
b>a) is defined as the
relative frequency of a being preferred over b:
v?a>b =
va>b
va>b + vb>a
4.3 A Vector Space Realization
Having presented a general framework for judgment
generalization, we will now briefly sketch a concrete
realization of this approach.
In order to be able to use the common distance
metrics on rules, we define a rule vector space. The
basis of this space will be a new set of rule features
designed specifically for the purpose of describing
the structure of a rule, ~? = ??1, . . . , ?k?. Provided
the exact features chosen are expressive and well-
distributed over the grammar, we expect any con-
ventional distance metric to correlate with rule sim-
ilarity.
We deem a particular ?i good if it quantifies a
quality of the rule that describes the rule?s nature
rather than the particular lexical choices it makes,
i.e. a statistic (such as the rule length, arity, number
of lexical items in the target or source side or the av-
erage covered span in the training corpus), informa-
tion relevant to the rule?s effect on a derivation (such
as nonterminals occuring in the rule and wheter they
are re-ordered) or features that capture frequent lex-
ical cues that carry syntactic information (such as
the co-occurrence of function words in source and
target language, possibly in conjunction with certain
nonterminal types).
5 Results and Analysis
The judgments were collected over a period of
about 12 days (Figure 1). A total of 16,374 labels
were provided (2,729 embedded test labels + 13,645
?true? labels) by 658 distinct workers over 83.1 hours
(i.e. each worker completed an average of 4.2 HITs
over 7.6 minutes). The reward for each HIT was
$0.02, with an additional $0.005 incurred for Ama-
zon Fees. Since each HIT provides five labels, we
obtain 200 (true) labels on the dollar. Each HIT
took an average of 1.83 minutes to complete, for a
labeling rate of 164 true labels/hour, and an effec-
tive wage of $0.66/hour. The low reward does not
seem to have deterred Turkers from completing our
HITs faithfully, as the success rate on the embedded
96
 0 10 20 30 40 50 60 70 80 90 100
 1  2  3  4  5  6  7  8  9  10  11% Completed Time (Days)
Figure 1: Progress of HIT submission over time. There
was a hiatus of about a month during which we collected
no data, which we are omitting for clairty.
True Questions Validation Questions
Preferred % Preferred %
High-
Ranked
40.0% Reference 83.7%
Low-
Ranked
24.1% Random
Candidate
11.7%
No
Difference
35.9% No
Difference
4.65%
Table 1: Distributions of the collected judgments over
the true questions and over the embedded test questions.
?High-Ranked? (resp. ?Low-Ranked?) refers to whether
the decoder assigned a high (low) score to the candidate.
And so, annotators agreed with the decoder 40.0% of the
time, and disagreed 24.1% of the time.
questions was quite high (Table 1).10 From our set
of comparatively judged candidate translations we
extracted competing rule pairs. To reduce the in-
fluence of lexical choices and improve comparabil-
ity, we excluded pure preterminal rules and limited
the extraction to rules covering the same span in the
Urdu source. Figure 3 shows an interesting example
of one such rule pair. While the decoder demon-
strates a clear preference for rule (a) (including it
into its higher-ranked translation 100% of the time),
the Turkers tend to prefer translations generated us-
ing rule (b), disagreeing with the SMT system 60%
of the time. This indicates that preferring the second
rule in decoding may yield better results in terms of
human judgment, in this case potentially due to the
10It should be mentioned that the human references them-
selves are of relatively low quality.
0
5
10
15
20
25
30
1 2 3 4 5 6 7 8 9 10
Candidate Rank
%
 
T
i
m
e
 
C
h
o
s
e
n
 
f
o
r
 
C
o
m
p
a
r
i
s
o
n
Figure 2: Histogram of the rank of the higher-ranked
candidate chosen in pair comparisons. For instance, in
about 29% of chosen pairs, the higher-ranked candidate
was the top candidate (of 300) by decoder score.
(a)  [NP] ! " [NP] [NN+IN] !" # the [NN+IN] [NP] $
(b)  [NP] ! " [NP] !" [NN] !"  # [NN] of [NP] $
Figure 3: A example pair of rules for which judgements
were obtained. The first rule is preferred by the decoder,
while human annotators favor the second rule.
cleaner separation of noun phrases from the prepo-
sitional phrase.
We also examine the distribution of the chosen
candidates. Recall that each pair consists of a high-
ranked candidate from the top-ten list, and a low-
ranked candidate from the top-300 list. The His-
togram of the higher rank (Figure 2) shows that the
high-ranked candidate is in fact a top-three candi-
date over 50% of the time. We also see (Figure 4)
that the low-ranked candidate tends to be either close
in rank to the top-ten list, or far away. This again
makes sense given our definition of potential for a
pair: potential is high if the derivations are very
close (left mode) or if the decoder scores differ con-
siderably (right mode).
Finally, we examine inter-annotator agreement,
since we collect multiple judgments per query. We
find that there is full agreement among the anno-
tators in 20.6% of queries. That is, in 20.6% of
queries, all three annotators answering that query
gave the same answer (out of the three provided
answers). This complete agreement rate is signif-
icantly higher than a rate caused by pure chance
(11.5%). This is a positive result, especially given
97
05
10
15
20
1
-
2
0
2
1
-
4
0
4
1
-
6
0
6
1
-
8
0
8
1
-
1
0
0
1
0
1
-
1
2
0
1
2
1
-
1
4
0
1
4
1
-
1
6
0
1
6
1
-
1
8
0
1
8
1
-
2
0
0
2
0
1
-
2
2
0
2
2
1
-
2
4
0
2
4
1
-
2
6
0
2
6
1
-
2
8
0
2
8
1
-
3
0
0
Candidate Rank
%
 
T
i
m
e
 
C
h
o
s
e
n
 
f
o
r
 
C
o
m
p
a
r
i
s
o
n
Figure 4: Histogram of the rank of the lower-ranked can-
didate chosen in pair comparisons. For instance, in about
16% of chosen candidate pairs, the lower-ranked candi-
date was ranked in the top 20.
how little diversity usually exists in n-best lists,
a fact (purposely) exacerbated by our strategy of
choosing highly similar pairs of candidates. On the
other hand, we observe complete disagreement in
only 14.9% of queries, which is significantly lower
than a rate caused by pure chance (which is 22.2%).
One thing to note is that these percentages are
calculated after excluding the validation questions,
where the complete agreement rate is an expectedly
even higher 64.9%, and the complete disagreement
rate is an expectedly even lower 3.60%.
6 Conclusions and Outlook
We presented a framework that allows us to ?tune?
MT systems on a finer level than system-level fea-
ture weights, going instead to the grammar rule level
and augmenting the feature set to reflect collected
human judgments. A system relying on this new fea-
ture during decoding is expected to have a slightly
different ranking of translation candidates that takes
human judgment into account. We presented one
particular judgment collection procedure that relies
on comparing candidate pairs (as opposed to eval-
uating a candidate in isolation) and complemented
it with one possible method of propagating human
judgments to cover grammar rules relevant to new
sentences.
While the presented statistics over the collected
data suggest that the proposed candidate selection
procedure yields consistent and potentially informa-
tive data, the quantitative effects on a machine trans-
lation system remain to be seen.
Additionally, introducing ? as a new feature
makes it necessary to find a viable weight for it.
While this can be done trivially in running MERT
on arbitrary development data, it may be of interest
to extend the weight optimization procedure in or-
der to preserve the partial ordering induced by the
judgments as best as possible.
Acknowledgments
This research was supported by the EuroMatrix-
Plus project funded by the European Commission,
by the DARPA GALE program under Contract No.
HR0011-06-2-0001, and the NSF under grant IIS-
0713448.
References
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf
Brown, Chris Callison-Burch, Glen Coppersmith,
Bonnie Dorr, Wes Filardo, Kendall Giles, Anni Irvine,
Mike Kayser, Lori Levin, Justin Martineau, Jim May-
field, Scott Miller, Aaron Phillips, Andrew Philpot,
Christine Piatko, Lane Schwartz, and David Zajic.
2009. Semantically informed machine translation
(SIMT). In SCALE 2009 Summer Workshop Final Re-
port, pages 135?139.
Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondr?ej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical machine
translation. In Proc. of ACL, Demonstration Session,
pages 177?180, June.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
N. G. Thornton, Jonathan Weese, and Omar F. Zaidan.
2009. Joshua: An open source toolkit for parsing-
based machine translation. In Proc. of the Fourth
Workshop on Statistical Machine Translation, pages
135?139.
Ashish Venugopal and Andreas Zollmann. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proc. of the NAACL 2006 Workshop on Statistical
Machine Translation, pages 138?141. Association for
Computational Linguistics.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
98
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 133?137,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Joshua 2.0: A Toolkit for Parsing-Based Machine Translation
with Syntax, Semirings, Discriminative Training and Other Goodies
Zhifei Li, Chris Callison-Burch, Chris Dyer,? Juri Ganitkevitch,
Ann Irvine, Sanjeev Khudanpur, Lane Schwartz,? Wren N.G. Thornton,
Ziyuan Wang, Jonathan Weese and Omar F. Zaidan
Center for Language and Speech Processing, Johns Hopkins University, Baltimore, MD
? Computational Linguistics and Information Processing Lab, University of Maryland, College Park, MD
? Natural Language Processing Lab, University of Minnesota, Minneapolis, MN
Abstract
We describe the progress we have made in
the past year on Joshua (Li et al, 2009a),
an open source toolkit for parsing based
machine translation. The new functional-
ity includes: support for translation gram-
mars with a rich set of syntactic nonter-
minals, the ability for external modules to
posit constraints on how spans in the in-
put sentence should be translated, lattice
parsing for dealing with input uncertainty,
a semiring framework that provides a uni-
fied way of doing various dynamic pro-
gramming calculations, variational decod-
ing for approximating the intractable MAP
decoding, hypergraph-based discrimina-
tive training for better feature engineering,
a parallelized MERT module, document-
level and tail-based MERT, visualization
of the derivation trees, and a cleaner
pipeline for MT experiments.
1 Introduction
Joshua is an open-source toolkit for parsing-based
machine translation that is written in Java. The
initial release of Joshua (Li et al, 2009a) was a
re-implementation of the Hiero system (Chiang,
2007) and all its associated algorithms, includ-
ing: chart parsing, n-gram language model inte-
gration, beam and cube pruning, and k-best ex-
traction. The Joshua 1.0 release also included
re-implementations of suffix array grammar ex-
traction (Lopez, 2007; Schwartz and Callison-
Burch, 2010) and minimum error rate training
(Och, 2003; Zaidan, 2009). Additionally, it in-
cluded parallel and distributed computing tech-
niques for scalability (Li and Khudanpur, 2008).
This paper describes the additions to the toolkit
over the past year, which together form the 2.0 re-
lease. The software has been heavily used by the
authors and several other groups in their daily re-
search, and has been substantially refined since the
first release. The most important new functions in
the toolkit are:
? Support for any style of synchronous context
free grammar (SCFG) including syntax aug-
ment machine translation (SAMT) grammars
(Zollmann and Venugopal, 2006)
? Support for external modules to posit transla-
tions for spans in the input sentence that con-
strain decoding (Irvine et al, 2010)
? Lattice parsing for dealing with input un-
certainty, including ambiguous output from
speech recognizers or Chinese word seg-
menters (Dyer et al, 2008)
? A semiring architecture over hypergraphs
that allows many inference operations to be
implemented easily and elegantly (Li and
Eisner, 2009)
? Improvements to decoding through varia-
tional decoding and other approximate meth-
ods that overcome intractable MAP decoding
(Li et al, 2009b)
? Hypergraph-based discriminative training for
better feature engineering (Li and Khudan-
pur, 2009b)
? A parallelization of MERT?s computations,
and supporting document-level and tail-based
optimization (Zaidan, 2010)
? Visualization of the derivation trees and hy-
pergraphs (Weese and Callison-Burch, 2010)
? A convenient framework for designing and
running reproducible machine translation ex-
periments (Schwartz, under review)
The sections below give short descriptions for
each of these new functions.
133
2 Support for Syntax-based Translation
The initial release of Joshua supported only
Hiero-style SCFGs, which use a single nontermi-
nal symbol X. This release includes support for ar-
bitrary SCFGs, including ones that use a rich set
of linguistic nonterminal symbols. In particular
we have added support for Zollmann and Venu-
gopal (2006)?s syntax-augmented machine trans-
lation. SAMT grammar extraction is identical to
Hiero grammar extraction, except that one side of
the parallel corpus is parsed, and syntactic labels
replace the X nonterminals in Hiero-style rules.
Instead of extracting this Hiero rule from the bi-
text
[X]? [X,1] sans [X,2] | [X,1] without [X,2]
the nonterminals can be labeled according to
which constituents cover the nonterminal span on
the parsed side of the bitext. This constrains what
types of phrases the decoder can use when produc-
ing a translation.
[VP]? [VBN] sans [NP] | [VBN] without [NP]
[NP]? [NP] sans [NP] | [NP] without [NP]
Unlike GHKM (Galley et al, 2004), SAMT has
the same coverage as Hiero, because it allows
non-constituent phrases to get syntactic labels us-
ing CCG-style slash notation. Experimentally, we
have found that the derivations created using syn-
tactically motivated grammars exhibit more coher-
ent syntactic structure than Hiero and typically re-
sult in better reordering, especially for languages
with word orders that diverge from English, like
Urdu (Baker et al, 2009).
3 Specifying Constraints on Translation
Integrating output from specialized modules
(like transliterators, morphological analyzers, and
modality translators) into the MT pipeline can
improve translation performance, particularly for
low-resource languages. We have implemented
an XML interface that allows external modules
to propose alternate translation rules (constraints)
for a particular word span to the decoder (Irvine
et al, 2010). Processing that is separate from
the MT engine can suggest translations for some
set of source side words and phrases. The XML
format allows for both hard constraints, which
must be used, and soft constraints, which compete
with standard extracted translation rules, as well
as specifying associated feature weights. In ad-
dition to specifying translations, the XML format
allows constraints on the lefthand side of SCFG
rules, which allows constraints like forcing a par-
ticular span to be translated as an NP. We modi-
fied Joshua?s chart-based decoder to support these
constraints.
4 Semiring Parsing
In Joshua, we use a hypergraph (or packed forest)
to compactly represent the exponentially many
derivation trees generated by the decoder for an
input sentence. Given a hypergraph, we may per-
form many atomic inference operations, such as
finding one-best or k-best translations, or com-
puting expectations over the hypergraph. For
each such operation, we could implement a ded-
icated dynamic programming algorithm. How-
ever, a more general framework to specify these
algorithms is semiring-weighted parsing (Good-
man, 1999). We have implemented the in-
side algorithm, the outside algorithm, and the
inside-outside speedup described by Li and Eis-
ner (2009), plut the first-order expectation semir-
ing (Eisner, 2002) and its second-order version (Li
and Eisner, 2009). All of these use our newly im-
plemented semiring framework.
The first- and second-order expectation semi-
rings can also be used to compute many interesting
quantities over hypergraphs. These quantities in-
clude expected translation length, feature expec-
tation, entropy, cross-entropy, Kullback-Leibler
divergence, Bayes risk, variance of hypothesis
length, gradient of entropy and Bayes risk, covari-
ance and Hessian matrix, and so on.
5 Word Lattice Input
We generalized the bottom-up parsing algorithm
that generates the translation hypergraph so that
it supports translation of word lattices instead of
just sentences. Our implementation?s runtime and
memory overhead is proportional to the size of the
lattice, rather than the number of paths in the lat-
tice (Dyer et al, 2008). Accepting lattice-based
input allows the decoder to explore a distribution
over input sentences, allowing it to select the best
translation from among all of them. This is es-
pecially useful when Joshua is used to translate
the output of statistical preprocessing components,
such as speech recognizers or Chinese word seg-
menters, which can encode their alternative analy-
ses as confusion networks or lattices.
134
6 Variational Decoding
Statistical models in machine translation exhibit
spurious ambiguity. That is, the probability of an
output string is split among many distinct deriva-
tions (e.g., trees or segmentations) that have the
same yield. In principle, the goodness of a string
is measured by the total probability of its many
derivations. However, finding the best string dur-
ing decoding is then NP-hard. The first version of
Joshua implemented the Viterbi approximation,
which measures the goodness of a translation us-
ing only its most probable derivation.
The Viterbi approximation is efficient, but it ig-
nores most of the derivations in the hypergraph.
We implemented variational decoding (Li et al,
2009b), which works as follows. First, given a for-
eign string (or lattice), the MT system produces a
hypergraph, which encodes a probability distribu-
tion p over possible output strings and their deriva-
tions. Second, a distribution q is selected that ap-
proximates p as well as possible but comes from
a family of distributions Q in which inference is
tractable. Third, the best string according to q
(instead of p) is found. In our implementation,
the q distribution is parameterized by an n-gram
model, under which the second and third steps can
be performed efficiently and exactly via dynamic
programming. In this way, variational decoding
considers all derivations in the hypergraph but still
allows tractable decoding.
7 Hypergraph-based Discriminative
Training
Discriminative training with a large number of
features has potential to improve the MT perfor-
mance. We have implemented the hypergraph-
based minimum risk training (Li and Eisner,
2009), which minimizes the expected loss of the
reference translations. The minimum-risk objec-
tive can be optimized by a gradient-based method,
where the risk and its gradient can be computed
using a second-order expectation semiring. For
optimization, we use both L-BFGS (Liu et al,
1989) and Rprop (Riedmiller and Braun, 1993).
We have also implemented the average Percep-
tron algorithm and forest-reranking (Li and Khu-
danpur, 2009b). Since the reference translation
may not be in the hypergraph due to pruning or in-
herent defficiency of the translation grammar, we
need to use an oracle translation (i.e., the transla-
tion in the hypergraph that is most simmilar to the
reference translation) as a surrogate for training.
We implemented the oracle extraction algorithm
described by Li and Khudanpur (2009a) for this
purpose.
Given the current infrastructure, other training
methods (e.g., maximum conditional likelihood or
MIRA as used by Chiang et al (2009)) can also be
easily supported with minimum coding. We plan
to implement a large number of feature functions
in Joshua so that exhaustive feature engineering is
possible for MT.
8 Minimum Error Rate Training
Joshua?s MERT module optimizes parameter
weights so as to maximize performance on a de-
velopment set as measuered by an automatic eval-
uation metric, such as Bleu (Och, 2003).
We have parallelized our MERT module in
two ways: parallelizing the computation of met-
ric scores, and parallelizing the search over pa-
rameters. The computation of metric scores is
a computational concern when tuning to a met-
ric that is slow to compute, such as translation
edit rate (Snover et al, 2006). Since scoring a
candidate is independent from scoring any other
candidate, we parallelize this computation using a
multi-threaded solution1. Similarly, we parallelize
the optimization of the intermediate initial weight
vectors, also using a multi-threaded solution.
Another feature is the module?s awareness of
document information, and the capability to per-
form optimization of document-based variants of
the automatic metric (Zaidan, 2010). For example,
in document-based Bleu, a Bleu score is calculated
for each document, and the tuned score is the aver-
age of those document scores. The MERT module
can furthermore be instructed to target a specific
subset of those documents, namely the tail subset,
where only the subset of documents with the low-
est document Bleu scores are considered.2
More details on the MERT method and the im-
plementation can be found in Zaidan (2009).3
1Based on sample code by Kenneth Heafield.
2This feature is of interest to GALE teams, for instance,
since GALE?s evaluation criteria place a lot of focus on trans-
lation quality of tail documents.
3The module is also available as a standalone applica-
tion, Z-MERT, that can be used with other MT systems.
(Software and documentation at: http://cs.jhu.edu/
?ozaidan/zmert.)
135
9 Visualization
We created tools for visualizing two of the
main data structures used in Joshua (Weese and
Callison-Burch, 2010). The first visualizer dis-
plays hypergraphs. The user can choose from a
set of input sentences, then call the decoder to
build the hypergraph. The second visualizer dis-
plays derivation trees. Setting a flag in the con-
figuration file causes the decoder to output parse
trees instead of strings, where each nonterminal is
annotated with its source-side span. The visual-
izer can read in multiple n-best lists in this format,
then display the resulting derivation trees side-by-
side. We have found that visually inspecting these
derivation trees is useful for debugging grammars.
We would like to add visualization tools for
more parts of the pipeline. For example, a chart
visualizer would make it easier for researchers to
tell where search errors were happening during
decoding, and why. An alignment visualizer for
aligned parallel corpora might help to determine
how grammar extraction could be improved.
10 Pipeline for Running MT
Experiments
Reproducing other researchers? machine transla-
tion experiments is difficult because the pipeline is
too complex to fully detail in short conference pa-
pers. We have put together a workflow framework
for designing and running reproducible machine
translation experiments using Joshua (Schwartz,
under review). Each step in the machine transla-
tion workflow (data preprocessing, grammar train-
ing, MERT, decoding, etc) is modeled by a Make
script that defines how to run the tools used in that
step, and an auxiliary configuration file that de-
fines the exact parameters to be used in that step
for a particular experimental setup. Workflows
configured using this framework allow a complete
experiment to be run ? from downloading data and
software through scoring the final translated re-
sults ? by executing a single Makefile.
This framework encourages researchers to sup-
plement research publications with links to the
complete set of scripts and configurations that
were actually used to run the experiment. The
Johns Hopkins University submission for the
WMT10 shared translation task was implemented
in this framework, so it can be easily and exactly
reproduced.
Acknowledgements
Research funding was provided by the NSF un-
der grant IIS-0713448, by the European Commis-
sion through the EuroMatrixPlus project, and by
the DARPA GALE program under Contract No.
HR0011-06-2-0001. The views and findings are
the authors? alone.
References
Kathy Baker, Steven Bethard, Michael Bloodgood,
Ralf Brown, Chris Callison-Burch, Glen Copper-
smith, Bonnie Dorr, Wes Filardo, Kendall Giles,
Anni Irvine, Mike Kayser, Lori Levin, Justin Mar-
tineau, Jim Mayfield, Scott Miller, Aaron Phillips,
Andrew Philpot, Christine Piatko, Lane Schwartz,
and David Zajic. 2009. Semantically informed ma-
chine translation (SIMT). SCALE summer work-
shop final report, Human Language Technology
Center Of Excellence.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In NAACL, pages 218?226.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL-08: HLT, pages 1012?
1020, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic finite-state transducers. In ACL.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In HLT-NAACL.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Ann Irvine, Mike Kayser, Zhifei Li, Wren Thornton,
and Chris Callison-Burch. 2010. Integrating out-
put from specialized modules in machine transla-
tion: Transliteration in joshua. The Prague Bulletin
of Mathematical Linguistics, 93:107?116.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
EMNLP, Singapore.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
ACL SSST, pages 10?18.
Zhifei Li and Sanjeev Khudanpur. 2009a. Efficient
extraction of oracle-best translations from hyper-
graphs. In Proceedings of NAACL.
136
Zhifei Li and Sanjeev Khudanpur. 2009b. Forest
reranking for machine translation with the percep-
tron algorithm. In GALE book chapter on ?MT
From Text?.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri
Ganitkevitch, Sanjeev Khudanpur, Lane Schwartz,
Wren Thornton, Jonathan Weese, and Omar. Zaidan.
2009a. Joshua: An open source toolkit for parsing-
based machine translation. In WMT09.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009b. Variational decoding for statistical machine
translation. In ACL.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge
Nocedal. 1989. On the limited memory bfgs
method for large scale optimization. Mathematical
Programming, 45:503?528.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP-CoNLL.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Martin Riedmiller and Heinrich Braun. 1993. A
direct adaptive method for faster backpropagation
learning: The rprop algorithm. In IEEE INTER-
NATIONAL CONFERENCE ON NEURAL NET-
WORKS, pages 586?591.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua.
The Prague Bulletin of Mathematical Linguistics,
93:157?166.
Lane Schwartz. under review. Reproducible results in
parsing-based machine translation: The JHU shared
task submission. In WMT10.
Matthew Snover, Bonnie J. Dorr, and Richard
Schwartz. 2006. A study of translation edit rate
with targeted human annotation. In AMTA.
Jonathan Weese and Chris Callison-Burch. 2010. Vi-
sualizing data structures in parsing-based machine
translation. The Prague Bulletin of Mathematical
Linguistics, 93:127?136.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Omar F. Zaidan. 2010. Document- and tail-based min-
imum error rate training of machine translation sys-
tems. In preparation.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the NAACL-2006 Workshop
on Statistical Machine Translation (WMT-06), New
York, New York.
137
Workshop on Monolingual Text-To-Text Generation, pages 84?90,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 84?90,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Paraphrastic Sentence Compression with a Character-based Metric:
Tightening without Deletion
Courtney Napoles1 and Chris Callison-Burch1 and Juri Ganitkevitch1 and
Benjamin Van Durme1,2
1Department of Computer Science
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present a substitution-only approach to
sentence compression which ?tightens? a sen-
tence by reducing its character length. Replac-
ing phrases with shorter paraphrases yields
paraphrastic compressions as short as 60% of
the original length. In support of this task,
we introduce a novel technique for re-ranking
paraphrases extracted from bilingual corpora.
At high compression rates1 paraphrastic com-
pressions outperform a state-of-the-art dele-
tion model in an oracle experiment. For fur-
ther compression, deleting from oracle para-
phrastic compressions preserves more mean-
ing than deletion alone. In either setting, para-
phrastic compression shows promise for sur-
passing deletion-only methods.
1 Introduction
Sentence compression is the process of shortening a
sentence while preserving the most important infor-
mation. Because it was developed in support of ex-
tractive summarization (Knight and Marcu, 2000),
much of the previous work considers deletion-based
models, which extract a subset of words from a long
sentence to create a shorter sentence such that mean-
ing and grammar are maximally preserved. This
framework imposes strict constraints on the task and
does not accurately model human-written compres-
sions, which tend to be abstractive rather than ex-
tractive (Marsi et al, 2010). This is one sense in
which paraphrastic compression can improve exist-
ing compression methodologies.
1Compression rate is defined as the compression length over
original length, so lower values indicate shorter sentences.
We distinguish two non-identical notions of sen-
tence compression: making a sentence substantially
shorter versus ?tightening? a sentence by remov-
ing unnecessary verbiage. We propose a method to
tighten sentences with just substitution and no dele-
tion operations. Using paraphrases extracted from
bilingual text and re-ranked on monolingual data,
our system selects the set of paraphrases that min-
imizes the character length of a sentence.
While not currently the standard, character-based
lengths have been considered before in compres-
sion, and we believe that it is relevant for current
and future applications. Character lengths have been
used for document summarization (DUC 2004, Over
and Yen (2004)), summarizing for mobile devices
(Corston-Oliver, 2001), and subtitling (Glickman et
al., 2006). Although in the past strict word limits
have been imposed for various documents, informa-
tion transmitted electronically is often limited by the
number of bytes, which directly relates to the num-
ber of characters. Mobile devices, SMS messages,
and microblogging sites such as Twitter are increas-
ingly important for quickly spreading information.
In this context, it is important to consider character-
based constraints.
We examine whether paraphrastic compression
allows more information to be conveyed in the same
number of characters as deletion-only compressions.
For example, the length constraint of Twitter posts or
tweets is 140 characters, and many article lead sen-
tences exceed this limit. A paraphrase substitution
oracle compresses the sentence in the table below to
76% of its original length (162 to 123 characters; the
first is the original). The compressed tweet is 140
84
characters, including spaces 17-character shortened
link to the original article.2
Congressional leaders reached a last-gasp agreement
Friday to avert a shutdown of the federal government,
after days of haggling and tense hours of brinksman-
ship.
Congress made a final agreement Fri. to avoid govern-
ment shutdown, after days of haggling and tense hours
of brinkmanship. on.wsj.com/h8N7n1
In contrast, using deletion to compress to the same
length may not be as expressive:
Congressional leaders reached agreement Friday to
avert a shutdown of federal government, after haggling
and tense hours. on.wsj.com/h8N7n1
This work presents a model that makes paraphrase
choices to minimize the character length of a sen-
tence. An oracle paraphrase-substitution experiment
shows that human judges rate paraphrastic compres-
sions higher than deletion-based compressions. To
achieve further compression, we shortened the or-
acle compressions using a deletion model to yield
compressions 80% of the original sentence length
and compared these to compressions generated us-
ing just deletions. Manual evaluation found that
the oracle-then-deletion compressions to preserve
more meaning than deletion-only compressions at
uniform compression rates.
2 Related work
Most of the previous research on sentence compres-
sion focuses on deletion using syntactic informa-
tion, (e.g., Galley and McKeown (2007), Knight
and Marcu (2002), Nomoto (2009), Galanis and An-
droutsopoulos (2010), Filippova and Strube (2008),
McDonald (2006), Yamangil and Shieber (2010),
Cohn and Lapata (2008), Cohn and Lapata (2009),
Turner and Charniak (2005)). Woodsend et al
(2010) incorporate paraphrase rules into a deletion
model. Previous work in subtitling has made one-
word substitutions to decrease character length at
high compression rates (Glickman et al, 2006).
More recent approaches in steganography have used
paraphrase substitution to encode information in text
but focus on grammaticality, not meaning preserva-
tion (Chang and Clark, 2010). Zhao et al (2009) ap-
plied an adaptable paraphrasing pipeline to sentence
2Taken from the main page of http://wsj.com, April 9, 2011.
compression, optimizing for F-measure over a man-
ually annotated set of gold standard paraphrases.
Sentence compression has been considered be-
fore in contexts outside of summarization, such as
headline, title, and subtitle generation (Dorr et al,
2003; Vandeghinste and Pan, 2004; Marsi et al,
2009). Corston-Oliver (2001) deleted characters
from words to shorten the character length of sen-
tences. To our knowledge character-based compres-
sion has not been examined before with the surging
popularity and utility of Twitter.
3 Sentence Tightening
The distinction between tightening and compression
can be illustrated by considering how much space
needs to be preserved. In the case of microblogging,
often a sentence has just a few too many characters
and needs to be ?tightened?. On the other hand, if a
sentence is much longer than a desired length, more
drastic compression is necessary. The first subtask
is relevant in any context with strict word or charac-
ter limits. Some sentences may not be compressible
beyond a certain limit. For example, we found that
near 10% of the compressions generated by Clarke
and Lapata (2008) were identical to the original sen-
tence. In situations where the sentence must meet
a minimum length, tightening can be used to meet
these requirements.
Multi-reference translations provide an instance
of the natural length variation of human-generated
sentences. These translations represent different
ways to express the foreign same sentence, so there
should be no meaning lost between the different ref-
erence translations. The character-based length of
different translations of a given sentence varies on
average by 80% when compared to the shortest sen-
tence in a set.3 This provides evidence that sen-
tences can be tightened to some extent without los-
ing any meaning.
Through the lens of sentence tightening, we con-
sider whether paraphrase substitutions alone can
yield compressions competitive with a deletion at
the same length. A character-based compression
rate is crucial in this framework, as two compres-
3This value will vary by collection and with the number of
references: for example, the NIST05 Arabic reference set has a
mean compression rate of 0.92 with 4 references per set.
85
sions having the same character-based compres-
sion rate may have different word-based compres-
sion rates. The advantage of a character-based sub-
stitution model is in choosing shorter words when
possible, freeing space for more content words. Go-
ing by word length alone would exclude the many
paraphrases with fewer characters than the original
phrase and the same number of words (or more).
3.1 Paraphrase Acquisition
To generate paraphrases for use in our experiments,
we took the approach described by Bannard and
Callison-Burch (2005), which extracts paraphrases
from bilingual parallel corpora. Figure 1 illustrates
the process. A phrase to be paraphrased, like thrown
into jail, is found in a German-English parallel cor-
pus. The corresponding foreign phrase (festgenom-
men) is identified using word alignment and phrase
extraction techniques from phrase-based statistical
machine translation (Koehn et al, 2003). Other oc-
currences of the foreign phrase in the parallel corpus
may align to another English phrase like jailed. Fol-
lowing Bannard and Callison-Burch, we treated any
English phrases that share a common foreign phrase
as potential paraphrases of each other.
As the original phrase occurs several times and
aligns with many different foreign phrases, each of
these may align to a variety of other English para-
phrases. Thus, thrown into jail not only paraphrases
as jailed, but also as arrested, detained, impris-
oned, incarcerated, locked up, taken into custody,
and thrown into prison . Moreover, because the
method relies on noisy and potentially inaccurate
word alignments, it is prone to generate many bad
paraphrases, such as maltreated, thrown, cases, cus-
tody, arrest, owners, and protection.
To rank candidates, Bannard and Callison-Burch
defined the paraphrase probability p(e2|e1) based
on the translation model probabilities p(e|f) and
p(f |e) from statistical machine translation. Follow-
ing Callison-Burch (2008), we refine selection by re-
quiring both the original phrase and paraphrase to
be of the same syntactic type, which leads to more
grammatical paraphrases.
Although many excellent paraphrases are ex-
tracted from parallel corpora, many others are un-
suitable and the translation score does not always
accurately distinguish the two. Therefore, we re-
Paraphrase Monlingual Bilingual
study in detail 1.00 0.70
scrutinise 0.94 0.08
consider 0.90 0.20
keep 0.83 0.03
learn 0.57 0.10
study 0.42 0.07
studied 0.28 0.01
studying it in detail 0.16 0.05
undertook 0.06 0.06
Table 1: Candidate paraphrases for study in detail with
corresponding approximate cosine similarity (Monolin-
gual) and translation model (Bilingual) scores.
ranked our candidates based on monolingual distri-
butional similarity, employing the method described
by Van Durme and Lall (2010) to derive approxi-
mate cosine similarity scores over feature counts us-
ing single token, independent left and right contexts.
Features were computed from the web-scale n-gram
collection of Lin et al (2010). As 5-grams are the
highest order of n-gram in this collection, the al-
lowable set of paraphrases have at most four words
(which allows at least one word of context).
To our knowledge this is the first time such tech-
niques have been used in combination in order to
derive higher quality paraphrase candidates. See Ta-
ble 1 for an example.
The monolingual-filtering technique we describe
is by no means limited to paraphrases extracted from
bilingual corpora. It could be applied to other data-
driven paraphrasing techniques (see Madnani and
Dorr (2010) for a survey). Although it is particularly
well suited to the bilingual extracted corpora, since
the information that it adds is orthogonal to that
model, it would presumably add less to paraphras-
ing techniques that already take advantage of mono-
lingual distributional similarity (Pereira et al, 1993;
Lin and Pantel, 2001; Barzilay and Lee, 2003).
In order to evaluate the paraphrase candidates
and scoring techniques, we randomly selected 1,000
paraphrase sets where the source phrase was present
in the corpus described in Clarke and Lapata (2008).
For each phrase and set of candidate paraphrases, we
extracted all of the contexts from the corpus in which
the source phrase appeared. Human judges were
presented each sentence with the original phrase and
the same sentences with each paraphrase candidate
86
... letzteWoche wurden in Irland f?nf Landwirte festgenommen , weil sie verhindern wollten
... last week five farmers were thrown into jail in Ireland because they resisted ...
...
Zahlreiche Journalisten sind verschwunden oder wurden festgenommen , gefoltert und get?tet .
Quite a few journalists have disappeared or have been imprisoned , tortured and killed .
Figure 1: Using a bilingual parallel corpus to extract paraphrases.
substituted in. Each paraphrase substitution was
graded based on the extent to which it preserved
the meaning and affected the grammaticality of the
sentence. While both the bilingual translation score
and monolingual cosine similarity positively corre-
lated with human judgments, the monolingual score
proved a stronger predictor of quality in both dimen-
sions. Using Kendall?s tau correlation coefficient,
the agreement between the ranking imposed by the
monolingual score and human ratings surpassed that
of the original ranking as derived during the bilin-
gual extraction, for both meaning and grammar.4 In
our substitution framework, we ignore the transla-
tion probabilities and use only the approximate co-
sine similarity in the paraphrase decision task.
4 Framework for Sentence Tightening
Our sentence tightening approach uses a dynamic
programming strategy to find the combination of
non-overlapping paraphrases that minimizes a sen-
tence?s character length. The threshold of the mono-
lingual score for paraphrases can be varied to widen
or narrow the search space, which may be further in-
creased by considering any lexical paraphrases not
subject to syntactic constraints. Sentences with a
compression rate as low as 0.6 can be generated
without thresholding the paraphrase scores. Because
the system can generate multiple paraphrased sen-
tences of equal length, we apply two layers of filter-
ing to generate a single output. First we calculate a
word-overlap score between the original and candi-
date sentences to favor compressions similar to the
original sentence; then, from among the sentences
4For meaning and grammar respectively, ? = 0.28 and 0.31
for monolingual scores and 0.19 and 0.15 for bilingual scores.
with the highest word overlap, we select the com-
pression with the best language model score.
Higher paraphrase thresholds guarantee more ap-
propriate paraphrases but yield longer compressions.
Using a cosine-similarity threshold of 0.95, the av-
erage compression rate is 0.968, which is consider-
ably longer than the compressions using no thresh-
old (0.60). In these experiments we did not syntac-
tically constrain paraphrases. However, we believe
that our monolingual refining of paraphrase sets im-
proves paraphrase selection and is a reasonable al-
ternative to using syntactic constraints.
In case judges favor compressions that have high
word overlap with the original sentence, we com-
pressed the longest sentence from each set of ref-
erence translations (Huang et al, 2002) and ran-
domly chose a sentence from the set of reference
translations to use as the standard for comparison.
Paraphrastic compressions were generated at cosine-
similarity thresholds ranging from 0.60 to 0.95.
We implemented a state-of-the-art deletion model
(Clarke and Lapata, 2008) to generate deletion-only
compressions. We fixed the compression length
to ?5 characters of the length of each paraphras-
tic compression, in order to isolate the compression
quality from the effect of compression rate (Napoles
et al, 2011). Manual evaluation used Amazon?s
Mechanical Turk with three-way redundancy and
positive and negative controls to filter bad workers.
Meaning and grammar judgments were collected us-
ing two 5-point scales (5 being the highest score).
5 Evaluation
The initial results of our substitution system show
room for improvement in future work (Table 2). We
believe this is due to erroneous paraphrase substi-
87
System Grammar Meaning CompR Cos.
Substitution 3.8 3.7 0.97 0.95
Deletion 4.1 4.0 0.97 -
Substitution 3.4 3.2 0.89 0.85
Deletion 4.0 3.8 0.89 -
Substitution 3.1 3.0 0.85 0.75
Deletion 3.9 3.7 0.85 -
Substitution 2.9 2.9 0.82 0.65
Deletion 3.8 3.5 0.82 -
Table 2: Mean ratings of compressions using just deletion
or substitution at different paraphrase thresholds (Cos.).
Deletion performed better in all settings.
tutions, since phrases with the same syntactic cate-
gory and distributional similarity are not necessarily
semantically identical. Illustrative examples include
WTO for United Nations and east or west for south.
Because the quality of the multi-reference transla-
tions is not uniformly high, for the following exper-
iment we used a dataset of English newspaper arti-
cles.
To control against these errors and test the viabil-
ity of a substitution-only approach, we generated all
possible paraphrase substitutions above a threshold
of 0.80 within a set of 20 randomly chosen sentences
from the written corpus of Clarke and Lapata (2008).
We solicited humans to make a ternary decision of
whether a paraphrase was acceptable in the context
(good, bad, or not sure). We applied our model to
generate compressions using only paraphrase substi-
tutions on which all three annotators agreed that the
paraphrase was good. The oracle generated com-
pressions with an average compression rate of 0.90.
On the same set of original sentences, we used
the deletion model to generate compressions con-
strained to ?5 characters of the length of the ora-
cle compression. Next, we examined whether apply-
ing the deletion model to paraphrastic compressions
would improve compression quality. In manual eval-
uation along the dimensions of grammar and mean-
ing, both the oracle compressions and oracle-plus-
deletion compressions outperformed the deletion-
only compressions at uniform lengths (Table 3)5.
These results suggest that improvements in para-
phrase acquisition will make our system competitive
with deletion-only models.
5Paraphrastic compressions were rated significantly higher
for meaning, p < 0.05
Model Grammar Meaning CompR
Oracle 4.1 4.3 0.90
Deletion 4.0 4.1 0.90
Gold 4.3 3.8 0.75
Oracle+deletion 3.4 3.7 0.80
Deletion 3.2 3.4 0.80
Table 3: Mean ratings of compressions generated by a
substitution oracle, deletion only, deletion on the oracle
compression, and the gold standard. Being able to choose
the best paraphrases would enable our substitution model
to outperform the deletion model.
6 Conclusion
This work shows promise for the use of only sub-
stitution in the task of sentence tightening. There
are myriad possible extensions and improvements
to this method, most notably richer features be-
yond paraphrase length. We do not currently use
syntactic information in our paraphrastic compres-
sion model because it places limits on the number
of paraphrases available for a sentence and thereby
limits the possible compression rate. The current
method for paraphrase extraction does not include
certain types of rewriting, such as passivization, and
should be extended to incorporate even more short-
ening paraphrases. Future work can directly apply
these methods to Twitter and extract additional para-
phrases and abbreviations from Twitter and/or SMS
data. Our substitution approach can be improved by
applying more sophisticated techniques to choosing
the best candidate compression, or by framing it as
an optimization problem over more than just mini-
mal length. Overall, we find these results to be en-
couraging for the possibility of sentence compres-
sion without deletion.
Acknowledgments
We are grateful to John Carroll for helping us obtain
the RASP parser. This research was partially funded
by the JHU Human Language Technology Center of
Excellence. This research was funded in part by the
NSF under grant IIS-0713448. The views and find-
ings are the authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
88
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Ching-Yun Chang and Stephen Clark. 2010. Linguis-
tic steganography using automatically generated para-
phrases. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
591?599. Association for Computational Linguistics.
James Clarke and Mirella Lapata. 2008. Global infer-
ence for sentence compression: An integer linear pro-
gramming approach. Journal of Artificial Intelligence
Research, 31:399?429.
Trevor Cohn and Mirella Lapata. 2008. Sentence com-
pression beyond word deletion. In Proceedings of
COLING.
Trevor Cohn and Mirella Lapata. 2009. Sentence com-
pression as tree transduction. Journal of Artificial In-
telligence Research, 34:637?674.
Simon Corston-Oliver. 2001. Text compaction for dis-
play on very small screens. In Proceedings of the
NAACL Workshop on Automatic Summarization.
Bonnie Dorr, David Zajic, and Richard Schwartz. 2003.
Hedge trimmer: A parse-and-trim approach to head-
line generation. In Proceedings of the HLT-NAACL
Workshop on Text summarization Workshop.
Katja Filippova and Michael Strube. 2008. Dependency
tree based sentence compression. In Proceedings of
the Fifth International Natural Language Generation
Conference. Association for Computational Linguis-
tics.
Dimitrios Galanis and Ion Androutsopoulos. 2010. An
extractive supervised two-stage method for sentence
compression. In Proceedings of NAACL.
Michel Galley and Kathleen R. McKeown. 2007. Lex-
icalized Markov grammars for sentence compression.
the Proceedings of NAACL/HLT.
Oren Glickman, Ido Dagan, Mikaela Keller, Samy Ben-
gio, and Walter Daelemans. 2006. Investigating lexi-
cal substitution scoring for subtitle generation. In Pro-
ceedings of the Tenth Conference on Computational
Natural Language Learning, pages 45?52. Associa-
tion for Computational Linguistics.
Shudong Huang, David Graff, and George Doddington.
2002. Multiple-Translation Chinese Corpus. Linguis-
tic Data Consortium.
Kevin Knight and Daniel Marcu. 2000. Statistics-based
summarization ? Step one: Sentence compression. In
Proceedings of AAAI.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139:91?107.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of HLT/NAACL.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules from text. Natural Language Engineering,
7(3):343?360.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New Tools for Web-Scale
N-grams. In Proceedings of LREC.
Nitin Madnani and Bonnie Dorr. 2010. Generat-
ing phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?388.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2009. Is sentence compression an NLG
task? In Proceedings of the 12th European Workshop
on Natural Language Generation.
Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Walter
Daelemans. 2010. On the limits of sentence com-
pression by deletion. Empirical Methods in Natural
Language Generation, pages 45?66.
Ryan McDonald. 2006. Discriminative sentence com-
pression with soft syntactic constraints. In In Proceed-
ings of EACL.
Courtney Napoles, Benjamin Van Durme, and Chris
Callison-Burch. 2011. Evaluating sentence compres-
sion: Pitfalls and suggested remedies. In Proceedings
of ACL, Workshop on Monolingual Text-To-Text Gen-
eration.
Tadashi Nomoto. 2009. A comparison of model free ver-
sus model intensive approaches to sentence compres-
sion. In Proceedings of EMNLP.
Paul Over and James Yen. 2004. An introduction to
DUC 2004: Intrinsic evaluation of generic news text
summarization systems. In Proceedings of DUC 2004
Document Understanding Workshop, Boston.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of English words. In ACL-93.
Jenine Turner and Eugene Charniak. 2005. Supervised
and unsupervised learning for sentence compression.
In Proceedings of ACL.
Benjamin Van Durme and Ashwin Lall. 2010. Online
generation of locality sensitive hash signatures. In
Proceedings of ACL, Short Papers.
Vincent Vandeghinste and Yi Pan. 2004. Sentence com-
pression for automated subtitling: A hybrid approach.
In Proceedings of the ACL workshop on Text Summa-
rization.
89
Kristian Woodsend, Yansong Feng, and Mirella Lapata.
2010. Generation with quasi-synchronous grammar.
In Proceedings of EMNLP.
Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
synchronous tree-substitution grammar induction and
its application to sentence compression. In Proceed-
ings of ACL.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
90
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478?484,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joshua 3.0: Syntax-based Machine Translation
with the Thrax Grammar Extractor
Jonathan Weese1, Juri Ganitkevitch1, Chris Callison-Burch1, Matt Post2 and Adam Lopez1,2
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present progress on Joshua, an open-
source decoder for hierarchical and syntax-
based machine translation. The main fo-
cus is describing Thrax, a flexible, open
source synchronous context-free grammar ex-
tractor. Thrax extracts both hierarchical (Chi-
ang, 2007) and syntax-augmented machine
translation (Zollmann and Venugopal, 2006)
grammars. It is built on Apache Hadoop for
efficient distributed performance, and can eas-
ily be extended with support for new gram-
mars, feature functions, and output formats.
1 Introduction
Joshua is an open-source1 toolkit for hierarchical
machine translation of human languages. The origi-
nal version of Joshua (Li et al, 2009) was a reim-
plementation of the Python-based Hiero machine-
translation system (Chiang, 2007); it was later ex-
tended (Li et al, 2010) to support richer formalisms,
such as SAMT (Zollmann and Venugopal, 2006).
The main focus of this paper is to describe this
past year?s work in developing Thrax (Weese, 2011),
an open-source grammar extractor for Hiero and
SAMT grammars. Grammar extraction has shown
itself to be something of a black art, with decod-
ing performance depending crucially on a variety
of features and options that are not always clearly
described in papers. This hindered direct com-
parison both between and within grammatical for-
malisms. Thrax standardizes Joshua?s grammar ex-
1http://github.com/joshua-decoder/joshua
traction procedures by providing a flexible and con-
figurable means of specifying these settings. Sec-
tion 3 presents a systematic comparison of the two
grammars using identical feature sets.
In addition, Joshua now includes a single pa-
rameterized script that implements the entire MT
pipeline, from data preparation to evaluation. This
script is built on top of a module called CachePipe.
CachePipe is a simple wrapper around shell com-
mands that uses SHA-1 hashes and explicitly-
provided lists of dependencies to determine whether
a command needs to be run, saving time both in run-
ning and debugging machine translation pipelines.
2 Thrax: grammar extraction
In modern machine translation systems such as
Joshua (Li et al, 2009) and cdec (Dyer et al, 2010),
a translation model is represented as a synchronous
context-free grammar (SCFG). Formally, an SCFG
may be considered as a tuple
(N,S, T?, T? , G)
where N is a set of nonterminal symbols of the
grammar, S ? N is the goal symbol, T? and T?
are the source- and target-side terminal symbol vo-
cabularies, respectively, and G is a set of production
rules of the grammar.
Each rule in G is of the form
X ? ??, ?,??
where X ? N is a nonterminal symbol, ? is a se-
quence of symbols from N ? T?, ? is a sequence of
478
symbols from N ? T? , and ? is a one-to-one cor-
respondence between the nonterminal symbols of ?
and ?.
The language of an SCFG is a set of ordered pairs
of strings. During decoding, the set of candidate
translations of an input sentence f is the set of all
e such that the pair (f, e) is licensed by the transla-
tion model SCFG. Each candidate e is generated by
applying a sequence of production rules (r1 . . . rn).
The cost of applying each rule is:
w(X ? ??, ??) =
?
i
?i(X ? ??, ??)
?i (1)
where each ?i is a feature function and ?i is the
weight for ?i. The total translation model score of
a candidate e is the product of the rules used in its
derivation. This translation model score is then com-
bined with other features (such as a language model
score) to produce an overall score for each candidate
translation.
2.1 Hiero and SAMT
Throughout this work, we will reference two par-
ticular SCFG types known as Hiero and Syntax-
Augmented Machine Translation (SAMT).
A Hiero grammar (Chiang, 2007) is an SCFG
with only one type of nonterminal symbol, tradi-
tionally labeled X . A Hiero grammar can be ex-
tracted from a parallel corpus of word-aligned sen-
tence pairs as follows: If (f ji , e
l
k) is a sub-phrase
of the sentence pair, we say it is consistent with
the pair?s alignment if none of the words in f ji are
aligned to words outside of elk, and vice-versa. The
consistent sub-phrase may be extracted as an SCFG
rule. Furthermore, if a consistent phrase is contained
within another one, a hierarchical rule may be ex-
tracted by replacing the smaller piece with a nonter-
minal.
An SAMT grammar (Zollmann and Venugopal,
2006) is similar to a Hiero grammar, except that the
nonterminal symbol set is much larger, and its la-
bels are derived from a parse tree over either the
source or target side in the following manner. For
each rule, if the target side is spanned by one con-
stituent of the parse tree, we assign that constituent?s
label as the nonterminal symbol for the rule. Other-
wise, we assign an extended category of the form
C1 + C2, C1/C2, or C2 \C1 ? indicating that the
das begr??e ich sehr .
i
very
much
welcome this
.
PRP
NP
S
RB RB
ADVP
VP
VBP DT
NP
.
Figure 1: An aligned sentence pair.
target side spans two adjacent constituents, is a C1
missing a C2 to the right, or is a C1 missing a C2
on the left, respectively. Table 1 contains a list of
Hiero and SAMT rules extracted from the training
sentence pair in Figure 1.
2.2 System overview
The following were goals in the design of Thrax:
? the ability to extract different SCFGs (such as
Hiero and SAMT), and to adjust various extrac-
tion parameters for the grammars;
? the ability to easily change and extend the fea-
ture sets for each rule
? scalability to arbitrarily large training corpora.
Thrax treats the grammar extraction and scoring
as a series of dependent Hadoop jobs. Hadoop
(Venugopal and Zollmann, 2009) is an implementa-
tion of Google?s MapReduce (Dean and Ghemawat,
2004), a framework for distributed processing of
large data sets. Hadoop jobs have two parts. In the
map step, a set of key/value pairs is mapped to a set
of intermediate key/value pairs. In the reduce step,
all intermediate values associated with an interme-
diate key are merged.
The first step in the Thrax pipeline is to extract all
the grammar rules. The map step in this job takes as
input word-aligned sentence pairs and produces a set
of ordered pairs (r, c) where r is a rule and c is the
number of times it was extracted. During the reduce
step, these rule counts are summed, so the result is
a set of rules, along with the total number of times
each rule was extracted from the entire corpus.
479
Span Hiero SAMT
[1, 3] X ? ?sehr, very much? ADV P ? ?sehr, very much?
[0, 3] X ? ?X sehr, X very much? PRP +ADV P ? ?PRP sehr, PRP very much?
[3, 4] X ? ?begru??e,welcome? V BP ? ?begru??e,welcome?
[0, 6] X ? ?X ich sehr ., i very much X .? S ? ?V P ich sehr ., i very much V P .?
[0, 6] X ? ?X ., X .? S ? ?S/. ., S/. .?
Table 1: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.
Given the rules and their counts, a separate
Hadoop job is run for each feature. These jobs can
all be submitted at once and run in parallel, avoid-
ing the linear sort-and-score workflow. The output
from each feature job is the same set of pairs (r, c)
as the input, except each rule r has been annotated
with some feature score f .
After the feature jobs have been completed, we
have several copies of the grammar, each of which
has been scored with one feature. A final Hadoop
job combines all these scores to produce the final
grammar.
Some users may not have access to a Hadoop
cluster. Thrax can be run in standalone or pseudo-
distributed mode on a single machine. It can also
be used with Amazon Elastic MapReduce,2 a web
service that provides computation time on a Hadoop
cluster on-demand.
2.3 Extraction
The first step in the Thrax workflow is the extraction
of grammar rules from an input corpus. As men-
tioned above, Hiero and SAMT grammars both re-
quire a parallel corpus with word-level alignments.
SAMT additionally requires that the target side of
the corpus be parsed.
There are several parameters that can make a sig-
nificant difference in a grammar?s overall translation
performance. Each of these parameters is easily ad-
justable in Thrax by changing its value in a configu-
ration file.
? maximum rule span
? maximum span of consistent phrase pairs
? maximum number of nonterminals
? minimum number of aligned terminals in rule
2http://aws.amazon.com/elasticmapreduce/
? whether to allow adjacent nonterminals on
source side
? whether to allow unaligned words at the edges
of consistent phrase pairs
Chiang (2007) gives reasonable heuristic choices
for these parameters when extracting a Hiero gram-
mar, and Lopez (2008) confirms some of them (max-
imum rule span of 10, maximum number of source-
side symbols at 5, and maximum number of non-
terminals at 2 per rule). ?) provided comparisons
among phrase-based, hierarchical, and syntax-based
models, but did not report extensive experimentation
with the model parameterizations.
When extracting Hiero- or SAMT-style gram-
mars, the first Hadoop job in the Thrax workflow
takes in a parallel corpus and produces a set of rules.
But in fact Thrax?s extraction mechanism is more
general than that; all it requires is a function that
maps a string to a set of rules. This makes it easy
to implement new grammars and extract them using
Thrax.
2.4 Feature functions
Thrax considers feature functions of two types: first,
there are features that can be calculated by looking
at each rule in isolation. Such features do not re-
quire a Hadoop job to calculate their scores, since
we may inspect the rules in any order. (In practice,
we calculate the scores at the very last moment be-
fore outputting the final grammar.) We call these
features simple features. Thrax implements the fol-
lowing simple features:
? a binary indicator functions denoting:
? whether the rule is purely abstract (i.e.,
has no terminal symbols)
480
? the rule is purely lexical (i.e., has no non-
terminals)
? the rule is monotonic or has reordering
? the rule has adjacent nonterminals on the
source side
? counters for
? the number of unaligned words in the rule
? the number of terminals on the target side
of the rule
? a constant phrase penalty
In addition to simple features, Thrax also imple-
ments map-reduce features. These are features that
require comparing rules in a certain order. Thrax
uses Hadoop to sort the rules efficiently and calcu-
late these feature functions. Thrax implements the
following map-reduce features:
? Phrasal translation probabilities p(?|?) and
p(?|?), calculated with relative frequency:
p(?|?) =
C(?, ?)
C(?)
(2)
(and vice versa), where C(?) is the number of
times a given event was extracted.
? Lexical weighting plex(?|?,A) and
plex(?|?,A). We calculate these weights
as given in (Koehn et al, 2003): let A be the
alignment between ? and ?, so (i, j) ? A if
and only if the ith word of ? is aligned to the
jth word of ?. Then we can define plex(?|?) as
n?
i=1
1
|{j : (i, j) ? A}|
?
(i,j)?A
w(?j |?i) (3)
where ?i is the ith word of ?, ?j is the jth word
of ?, and w(y|x) is the relative frequency of
seeing word y given x.
? Rarity penalty, given by
exp(1? C(X ? ??, ??)) (4)
where again C(?) is a count of the number of
times the rule was extracted.
The above features are all implemented and can
be turned on or off with a keyword in the Thrax con-
figuration file.
It is easy to extend Thrax with new feature func-
tions. For simple features, all that is needed is to im-
plement Thrax?s SIMPLEFEATURE interface defin-
ing a method that takes in a rule and calculates a
feature score. Map-reduce features are slightly more
complex: to subclass MAPREDUCEFEATURE, one
must define a mapper and reducer, but also a sort
comparator to determine in what order the rules are
compared during the reduce step.
2.5 Related work
Joshua includes a simple Hiero extractor (Schwartz
and Callison-Burch, 2010). The extractor runs as a
single Java process, which makes it difficult to ex-
tract larger grammars, since the host machine must
have enough memory to hold all of the rules at once.
Joshua?s extractor scores each rule with three feature
functions ? lexical probabilities in two directions,
and one phrasal probability score p(?|?).
The SAMT implementation of Zollmann and
Venugopal (2006) includes a several-thousand-line
Perl script to extract their rules. In addition to
phrasal and lexical probabilities, this extractor im-
plements several other features that are also de-
scribed in section 2.4.
Finally, the cdec decoder (Dyer et al, 2010) in-
cludes a grammar extractor that performs well only
when all rules can be held in memory.
Memory usage is a limitation of both the Joshua
and cdec extractors. Translation models can be very
large, and many feature scores require accumulation
of statistical data from the entire set of extracted
rules. Since it is impractical to keep the entire gram-
mar in memory, rules are usually sorted on disk and
then read sequentially. Different feature calcula-
tions may require different sort orders, leading to a
linear workflow that alternates between sorting the
grammar and calculating a feature score. To cal-
culate more feature scores, more sorts have to be
performed. This discourages the implementation of
new features. For example, Joshua?s built-in rule ex-
tractor calculates the phrasal probability p(?|?) for
each rule but, to save time, does not calculate its ob-
vious counterpart p(?|?), which would require an-
other sort.
481
Language pair sentences (K) words (M)
cs?en 332 4.7
de?en 279 5.5
en?cs 487 6.9
en?de 359 7.2
en?fr 682 12.5
fr?en 792 14.4
Table 2: Training data size after subsampling.
The SAMT extractor does not have a problem
with large data sets; SAMT can run on Hadoop, as
Thrax does.
The Joshua and cdec extractors only extract Hiero
grammars, and Zollmann and Venugopal?s extractor
can only extract SAMT-style grammars. They are
not designed to score arbitrary feature sets, either.
Since variation in translation models and feature sets
can have a significant effect on translation perfor-
mance, we have developed Thrax in order to make it
easy to build and test new models.
3 Experiments
We built systems for six language pairs for the WMT
2011 shared task: cz-en, en-cz, de-en, en-de, fr-en,
and en-fr.3 For each language pair, we built both
SAMT and hiero grammars.4 Table 3 contains the
results on the complete WMT 2011 test set.
To train the translation models, we used the pro-
vided Europarl and news commentary data. For cz-
en and en-cz, we also used sections of the CzEng
parallel corpus (Bojar and Z?abokrtsky?, 2009). The
parallel data was subsampled using Joshua?s built-
in subsampler to select sentences with n-grams rel-
evant to the tuning and test set. We used SRILM
to train a 5-gram language model with Kneser-Ney
smoothing using the appropriate side of the paral-
lel data. For the English LM, we also used English
Gigaword Fourth Edition.5
Before extracting an SCFG with Thrax, we used
the provided Perl scripts to tokenize and normalize
3fr=French, cz=Czech, de=German, en=English.
4Except for fr-en and en-fr. We were unable to decode with
SAMT grammars for these language pairs due to their large size.
We have since resolved this issue and will have scores for the
final version of the paper.
5LDC2009T13
pair hiero SAMT improvement
cz-en 21.1 21.7 +0.6
en-cz 16.8 16.9 +0.1
de-en 18.9 19.5 +0.6
en-de 14.3 14.9 +0.6
fr-en 28.0 - -
en-fr 30.4 - -
Table 3: Single-reference BLEU-4 scores.
the data. We also removed any sentences longer than
50 tokens (after tokenization). For SAMT grammar
extraction, we parsed the English training data us-
ing the Berkeley Parser (Petrov et al, 2006) with the
provided Treebank-trained grammar.
We tuned the model weights against the
WMT08 test set (news-test2008) using Z-
MERT (Zaidan, 2009), an implementation of mini-
mum error-rate training included with Joshua. We
decoded the test set to produce a 300-best list of
unique translations, then chose the best candidate for
each sentence using Minimum Bayes Risk reranking
(Kumar and Byrne, 2004). Figure 2 shows an exam-
ple derivation with an SAMT grammar. To re-case
the 1-best test set output, we trained a true-case 5-
gram language model using the same LM training
data as before, and used an SCFG translation model
to translate from the lowercased to true-case output.
The translation model used rules limited to five to-
kens in length, and contained no hierarchical rules.
4 CachePipe: Cached pipeline runs
Machine translation pipelines involve the specifica-
tion and execution of many different datasets, train-
ing procedures, and pre- and post-processing tech-
niques that can have large effects on translation out-
come, and which make direct comparisons between
systems difficult. The complexity of managing these
pipelines and experimental environments has led to a
number of different experimental management sys-
tems, such as Experiment.perl,6 Joshua 2.0?s Make-
file system (Li et al, 2010), and LoonyBin (Clark
and Lavie, 2010). In addition to managing the
pipeline, these scripts employ different techniques
to avoid expensive recomputation by caching steps.
6http://www.statmt.org/moses/?n=
FactoredTraining.EMS
482
the
reactor type will be operated with uranium
VBN
DT+NP
GLUE
VP
PP
der reaktortyp , das nicht
angereichert
wird zwar mit uran betrieben
, which is
not
enriched
ist .
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
.
S
VBN
DT+NP
GLUE
VP
PP
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
S
Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals
and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.
However, these approaches are based on simple but
unreliable heuristics (such as timestamps or file ex-
istence) to make the caching determination.
Our solution to the caching dependency problem
is CachePipe. CachePipe is designed with the fol-
lowing goals: (1) robust content-based dependency
checking and (2) ease of use, including minimal
editing of existing scripts. CachePipe is essentially
a wrapper around command invocations. Presented
with a command to run and a list of file dependen-
cies, it computes SHA-1 hashes of the dependencies
and of the command invocation and stores them; the
command is executed only if any of those hashes are
different from previous runs. A basic invocation in-
volves specifying (1) a name or identifier associated
with the command or step, (2) the command to run,
and (3) a list of file dependencies. For example, to
copy file a to b from a shell prompt, the following
command could be used:
cachecmd copy "cp a b" a b
The first time the command is run, the file would be
copied; afterwards, the command would be skipped
after CachePipe verified that the contents of the de-
pendencies a and b had not changed.
CachePipe is open-source software, distributed
with Joshua or available separately.7 It currently
provides both a shell script interface and a program-
matic API for Perl. It accepts a number of other
arguments and dependency types. It also serves as
the foundation of a new script in Joshua 3.0 that im-
plements the complete Joshua pipeline, from data
preparation to evaluation.
5 Future work
Thrax is currently limited to SCFG-based translation
models. A natural development would be to extract
GHKM grammars (Galley et al, 2004) or more re-
cent tree-to-tree models (Zhang et al, 2008; Liu et
al., 2009; Chiang, 2010). We also hope that Thrax
will continue to be extended with more feature func-
tions as researchers develop and contribute them.
Acknowledgements
This research was supported by in part by the Eu-
roMatrixPlus project funded by the European Com-
mission (7th Framework Programme), and by the
NSF under grant IIS-0713448. Opinions, interpre-
tations, and conclusions are the authors? alone.
7https://github.com/joshua-decoder/
cachepipe
483
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, Uppsala, Sweden,
July.
Jonathan H. Clark and Alon Lavie. 2010. Loony-
bin: Keeping language technologists sane through au-
tomated management of experimental (hyper) work-
flows. In Proc. LREC.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In OSDI.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc.
ACL 2010 System Demonstrations, pages 7?12.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
ACL, Suntec, Singapore, August.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. COLING, Manchester, UK,
August.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL, Sydney, Aus-
tralia, July.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua:
Suffix arrays and prefix trees. The Prague Bulletin of
Mathematical Linguistics, 93:157?166, January.
Mark Steedman. 1999. Alternating quantifier scope in
ccg. In Proc. ACL, Stroudsburg, PA, USA.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67?78.
Jonathan Weese. 2011. A systematic comparison of syn-
chronous context-free grammars for machine transla-
tion. Master?s thesis, Johns Hopkins University, May.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. NAACL Workshop on Statistcal Machine Trans-
lation, New York, New York.
484
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 283?291,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joshua 4.0: Packing, PRO, and Paraphrases
Juri Ganitkevitch1, Yuan Cao1, Jonathan Weese1, Matt Post2, and Chris Callison-Burch1
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present Joshua 4.0, the newest version
of our open-source decoder for parsing-based
statistical machine translation. The main con-
tributions in this release are the introduction
of a compact grammar representation based
on packed tries, and the integration of our
implementation of pairwise ranking optimiza-
tion, J-PRO. We further present the exten-
sion of the Thrax SCFG grammar extractor
to pivot-based extraction of syntactically in-
formed sentential paraphrases.
1 Introduction
Joshua is an open-source toolkit1 for parsing-based
statistical machine translation of human languages.
The original version of Joshua (Li et al, 2009) was
a reimplementation of the Python-based Hiero ma-
chine translation system (Chiang, 2007). It was later
extended to support grammars with rich syntactic
labels (Li et al, 2010a). More recent efforts in-
troduced the Thrax module, an extensible Hadoop-
based extraction toolkit for synchronous context-
free grammars (Weese et al, 2011).
In this paper we describe a set of recent exten-
sions to the Joshua system. We present a new com-
pact grammar representation format that leverages
sparse features, quantization, and data redundancies
to store grammars in a dense binary format. This al-
lows for both near-instantaneous start-up times and
decoding with extremely large grammars. In Sec-
tion 2 we outline our packed grammar format and
1joshua-decoder.org
present experimental results regarding its impact on
decoding speed, memory use and translation quality.
Additionally, we present Joshua?s implementation
of the pairwise ranking optimization (Hopkins and
May, 2011) approach to translation model tuning.
J-PRO, like Z-MERT, makes it easy to implement
new metrics and comes with both a built-in percep-
tron classifier and out-of-the-box support for widely
used binary classifiers such as MegaM and Max-
Ent (Daume? III and Marcu, 2006; Manning and
Klein, 2003). We describe our implementation in
Section 3, presenting experimental results on perfor-
mance, classifier convergence, and tuning speed.
Finally, we introduce the inclusion of bilingual
pivoting-based paraphrase extraction into Thrax,
Joshua?s grammar extractor. Thrax?s paraphrase ex-
traction mode is simple to use, and yields state-of-
the-art syntactically informed sentential paraphrases
(Ganitkevitch et al, 2011). The full feature set of
Thrax (Weese et al, 2011) is supported for para-
phrase grammars. An easily configured feature-level
pruning mechanism allows to keep the paraphrase
grammar size manageable. Section 4 presents de-
tails on our paraphrase extraction module.
2 Compact Grammar Representation
Statistical machine translation systems tend to per-
form better when trained on larger amounts of bilin-
gual parallel data. Using tools such as Thrax, trans-
lation models and their parameters are extracted
and estimated from the data. In Joshua, translation
models are represented as synchronous context-free
grammars (SCFGs). An SCFG is a collection of
283
rules {ri} that take the form:
ri = Ci ? ??i, ?i,?i, ~?i?, (1)
where left-hand side Ci is a nonterminal symbol, the
source side ?i and the target side ?i are sequences
of both nonterminal and terminal symbols. Further,
?i is a one-to-one correspondence between the non-
terminal symbols of ?i and ?i, and ~?i is a vector of
features quantifying the probability of ?i translat-
ing to ?i, as well as other characteristics of the rule
(Weese et al, 2011). At decoding time, Joshua loads
the grammar rules into memory in their entirety, and
stores them in a trie data structure indexed by the
rules? source side. This allows the decoder to effi-
ciently look up rules that are applicable to a particu-
lar span of the (partially translated) input.
As the size of the training corpus grows, so does
the resulting translation grammar. Using more di-
verse sets of nonterminal labels ? which can signifi-
cantly improve translation performance ? further ag-
gravates this problem. As a consequence, the space
requirements for storing the grammar in memory
during decoding quickly grow impractical. In some
cases grammars may become too large to fit into the
memory on a single machine.
As an alternative to the commonly used trie struc-
tures based on hash maps, we propose a packed trie
representation for SCFGs. The approach we take is
similar to work on efficiently storing large phrase
tables by Zens and Ney (2007) and language mod-
els by Heafield (2011) and Pauls and Klein (2011) ?
both language model implementations are now inte-
grated with Joshua.
2.1 Packed Synchronous Tries
For our grammar representation, we break the SCFG
up into three distinct structures. As Figure 1 in-
dicates, we store the grammar rules? source sides
{?i}, target sides {?i}, and feature data {~?i} in sep-
arate formats of their own. Each of the structures
is packed into a flat array, and can thus be quickly
read into memory. All terminal and nonterminal
symbols in the grammar are mapped to integer sym-
bol id?s using a globally accessible vocabulary map.
We will now describe the implementation details for
each representation and their interactions in turn.
2.1.1 Source-Side Trie
The source-side trie (or source trie) is designed
to facilitate efficient lookup of grammar rules by
source side, and to allow us to completely specify a
matching set of rule with a single integer index into
the trie. We store the source sides {?i} of a grammar
in a downward-linking trie, i.e. each trie node main-
tains a record of its children. The trie is packed into
an array of 32-bit integers. Figure 1 illustrates the
composition of a node in the source-side trie. All
information regarding the node is stored in a con-
tiguous block of integers, and decomposes into two
parts: a linking block and a rule block.
The linking block stores the links to the child trie
nodes. It consists of an integer n, the number of chil-
dren, and n blocks of two integers each, containing
the symbol id aj leading to the child and the child
node?s address sj (as an index into the source-side
array). The children in the link block are sorted by
symbol id, allowing for a lookup via binary or inter-
polation search.
The rule block stores all information necessary to
reconstruct the rules that share the source side that
led to the current source trie node. It stores the num-
ber of rules, m, and then a tuple of three integers
for each of the m rules: we store the symbol id of
the left-hand side, an index into the target-side trie
and a data block id. The rules in the data block are
initially in an arbitrary order, but are sorted by ap-
plication cost upon loading.
2.1.2 Target-Side Trie
The target-side trie (or target trie) is designed to
enable us to uniquely identify a target side ?i with a
single pointer into the trie, as well as to exploit re-
dundancies in the target side string. Like the source
trie, it is stored as an array of integers. However,
the target trie is a reversed, or upward-linking trie:
a trie node retains a link to its parent, as well as the
symbol id labeling said link.
As illustrated in Figure 1, the target trie is ac-
cessed by reading an array index from the source
trie, pointing to a trie node at depth d. We then fol-
low the parent links to the trie root, accumulating
target side symbols gj into a target side string gd1 as
we go along. In order to match this traversal, the tar-
get strings are entered into the trie in reverse order,
i.e. last word first. In order to determine d from a
284
# children
# rules
child symbol
child address
rule left-hand side
target address
data block id
n ?
m ?
a
j
s
j
+
1
C
j
t
j
b
j
.
.
.
.
.
.
n
m
.
.
.
.
.
.
parent symbol
parent address
g
j
t
j-1
.
.
.
.
.
.
# features
feature id
feature value
n ?
f
j
v
j
.
.
.
n
.
.
.
.
.
Feature block 
index
Feature byte
buffer
Target trie
array
Source trie
array
f
j
.
.
.
.
.
.
Quantization
b
j
.
.
.
.
.
.
f
j
q
j
Figure 1: An illustration of our packed grammar data structures. The source sides of the grammar rules are
stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries
for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full
target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain
a data block id, which identifies feature data attached to the rule. The features are encoded according to a
type/quantization specification and stored as variable-length blocks of data in a byte buffer.
pointer into the target trie, we maintain an offset ta-
ble in which we keep track of where each new trie
level begins in the array. By first searching the offset
table, we can determine d, and thus know how much
space to allocate for the complete target side string.
To further benefit from the overlap there may be
among the target sides in the grammar, we drop the
nonterminal labels from the target string prior to in-
serting them into the trie. For richly labeled gram-
mars, this collapses all lexically identical target sides
that share the same nonterminal reordering behavior,
but vary in nonterminal labels into a single path in
the trie. Since the nonterminal labels are retained in
the rules? source sides, we do not lose any informa-
tion by doing this.
2.1.3 Features and Other Data
We designed the data format for the grammar
rules? feature values to be easily extended to include
other information that we may want to attach to a
rule, such as word alignments, or locations of occur-
rences in the training data. In order to that, each rule
ri has a unique block id bi associated with it. This
block id identifies the information associated with
the rule in every attached data store. All data stores
are implemented as memory-mapped byte buffers
that are only loaded into memory when actually re-
quested by the decoder. The format for the feature
data is detailed in the following.
The rules? feature values are stored as sparse fea-
tures in contiguous blocks of variable length in a
byte buffer. As shown in Figure 1, a lookup table
is used to map the bi to the index of the block in the
buffer. Each block is structured as follows: a sin-
gle integer, n, for the number of features, followed
by n feature entries. Each feature entry is led by an
integer for the feature id fj , and followed by a field
of variable length for the feature value vj . The size
of the value is determined by the type of the feature.
Joshua maintains a quantization configuration which
maps each feature id to a type handler or quantizer.
After reading a feature id from the byte buffer, we
retrieve the responsible quantizer and use it to read
the value from the byte buffer.
Joshua?s packed grammar format supports Java?s
standard primitive types, as well as an 8-bit quan-
tizer. We chose 8 bit as a compromise between
compression, value decoding speed and transla-
285
Grammar Format Memory
Hiero (43M rules)
Baseline 13.6G
Packed 1.8G
Syntax (200M rules)
Baseline 99.5G
Packed 9.8G
Packed 8-bit 5.8G
Table 1: Decoding-time memory use for the packed
grammar versus the standard grammar format. Even
without lossy quantization the packed grammar rep-
resentation yields significant savings in memory
consumption. Adding 8-bit quantization for the real-
valued features in the grammar reduces even large
syntactic grammars to a manageable size.
tion performance (Federico and Bertoldi, 2006).
Our quantization approach follows Federico and
Bertoldi (2006) and Heafield (2011) in partitioning
the value histogram into 256 equal-sized buckets.
We quantize by mapping each feature value onto the
weighted average of its bucket. Joshua allows for an
easily per-feature specification of type. Quantizers
can be share statistics across multiple features with
similar value distributions.
2.2 Experiments
We assess the packed grammar representation?s
memory efficiency and impact on the decoding
speed on the WMT12 French-English task. Ta-
ble 1 shows a comparison of the memory needed
to store our WMT12 French-English grammars at
runtime. We can observe a substantial decrease in
memory consumption for both Hiero-style gram-
mars and the much larger syntactically annotated
grammars. Even without any feature value quantiza-
tion, the packed format achieves an 80% reduction
in space requirements. Adding 8-bit quantization
for the log-probability features yields even smaller
grammar sizes, in this case a reduction of over 94%.
In order to avoid costly repeated retrievals of indi-
vidual feature values of rules, we compute and cache
the stateless application cost for each grammar rule
at grammar loading time. This, alongside with a lazy
approach to rule lookup allows us to largely avoid
losses in decoding speed.
Figure shows a translation progress graph for the
WMT12 French-English development set. Both sys-
 0 500 1000 1500 2000 2500
 0  500  1000  1500  2000  2500Sentences Translated Seconds PassedStandardPacked
Figure 2: A visualization of the loading and decod-
ing speed on the WMT12 French-English develop-
ment set contrasting the packed grammar represen-
tation with the standard format. Grammar loading
for the packed grammar representation is substan-
tially faster than that for the baseline setup. Even
with a slightly slower decoding speed (note the dif-
ference in the slopes) the packed grammar finishes
in less than half the time, compared to the standard
format.
tems load a Hiero-style grammar with 43 million
rules, and use 16 threads for parallel decoding. The
initial loading time for the packed grammar repre-
sentation is dramatically shorter than that for the
baseline setup (a total of 176 seconds for loading and
sorting the grammar, versus 1897 for the standard
format). Even though decoding speed is slightly
slower with the packed grammars (an average of 5.3
seconds per sentence versus 4.2 for the baseline), the
effective translation speed is more than twice that of
the baseline (1004 seconds to complete decoding the
2489 sentences, versus 2551 seconds with the stan-
dard setup).
3 J-PRO: Pairwise Ranking Optimization
in Joshua
Pairwise ranking optimization (PRO) proposed by
(Hopkins and May, 2011) is a new method for dis-
criminative parameter tuning in statistical machine
translation. It is reported to be more stable than the
popular MERT algorithm (Och, 2003) and is more
scalable with regard to the number of features. PRO
treats parameter tuning as an n-best list reranking
problem, and the idea is similar to other pairwise
ranking techniques like ranking SVM and IR SVMs
286
(Li, 2011). The algorithm can be described thusly:
Let h(c) = ?w,?(c)? be the linear model score
of a candidate translation c, in which ?(c) is the
feature vector of c and w is the parameter vector.
Also let g(c) be the metric score of c (without loss
of generality, we assume a higher score indicates a
better translation). We aim to find a parameter vector
w such that for a pair of candidates {ci, cj} in an n-
best list,
(h(ci)? h(cj))(g(ci)? g(cj)) =
?w,?(ci)??(cj)?(g(ci)? g(cj)) > 0,
namely the order of the model score is consistent
with that of the metric score. This can be turned into
a binary classification problem, by adding instance
??ij = ?(ci)??(cj)
with class label sign(g(ci) ? g(cj)) to the training
data (and symmetrically add instance
??ji = ?(cj)??(ci)
with class label sign(g(cj) ? g(ci)) at the same
time), then using any binary classifier to find the w
which determines a hyperplane separating the two
classes (therefore the performance of PRO depends
on the choice of classifier to a large extent). Given
a training set with T sentences, there are O(Tn2)
pairs of candidates that can be added to the training
set, this number is usually much too large for effi-
cient training. To make the task more tractable, PRO
samples a subset of the candidate pairs so that only
those pairs whose metric score difference is large
enough are qualified as training instances. This fol-
lows the intuition that high score differential makes
it easier to separate good translations from bad ones.
3.1 Implementation
PRO is implemented in Joshua 4.0 named J-PRO.
In order to ensure compatibility with the decoder
and the parameter tuning module Z-MERT (Zaidan,
2009) included in all versions of Joshua, J-PRO is
built upon the architecture of Z-MERT with sim-
ilar usage and configuration files(with a few extra
lines specifying PRO-related parameters). J-PRO in-
herits Z-MERT?s ability to easily plug in new met-
rics. Since PRO allows using any off-the-shelf bi-
nary classifiers, J-PRO provides a Java interface that
enables easy plug-in of any classifier. Currently, J-
PRO supports three classifiers:
? Perceptron (Rosenblatt, 1958): the percep-
tron is self-contained in J-PRO, no external re-
sources required.
? MegaM (Daume? III and Marcu, 2006): the clas-
sifier used by Hopkins and May (2011).2
? Maximum entropy classifier (Manning and
Klein, 2003): the Stanford toolkit for maxi-
mum entropy classification.3
The user may specify which classifier he wants to
use and the classifier-specific parameters in the J-
PRO configuration file.
The PRO approach is capable of handling a large
number of features, allowing the use of sparse dis-
criminative features for machine translation. How-
ever, Hollingshead and Roark (2008) demonstrated
that naively tuning weights for a heterogeneous fea-
ture set composed of both dense and sparse features
can yield subpar results. Thus, to better handle the
relation between dense and sparse features and pro-
vide a flexible selection of training schemes, J-PRO
supports the following four training modes. We as-
sume M dense features and N sparse features are
used:
1. Tune the dense feature parameters only, just
like Z-MERT (M parameters to tune).
2. Tune the dense + sparse feature parameters to-
gether (M +N parameters to tune).
3. Tune the sparse feature parameters only with
the dense feature parameters fixed, and sparse
feature parameters scaled by a manually speci-
fied constant (N parameters to tune).
4. Tune the dense feature parameters and the scal-
ing factor for sparse features, with the sparse
feature parameters fixed (M+1 parameters to
tune).
J-PRO supports n-best list input with a sparse fea-
ture format which enumerates only the firing fea-
tures together with their values. This enables a more
compact feature representation when numerous fea-
tures are involved in training.
2hal3.name/megam
3nlp.stanford.edu/software
287
0 10 20 300
10
20
30
40
Iteration
BLE
U
Dev set MT03 (10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT04(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT05(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Dev set MT03 (1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT04(1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT05(1026 features)
 
 
PercepMegaMMax?Ent
Figure 3: Experimental results on the development and test sets. The x-axis is the number of iterations (up to
30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper
row: results trained using only dense features (10 features); Lower row: results trained using dense+sparse
features (1026 features). Left column: development set (MT03); Middle column: test set (MT04); Right
column: test set (MT05).
Datasets Z-MERT
J-PRO
Percep MegaM Max-Ent
Dev (MT03) 32.2 31.9 32.0 32.0
Test (MT04) 32.6 32.7 32.7 32.6
Test (MT05) 30.7 30.9 31.0 30.9
Table 2: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).
3.2 Experiments
We did our experiments using J-PRO on the NIST
Chinese-English data, and BLEU score was used as
the quality metric for experiments reported in this
section.4 The experimental settings are as the fol-
lowing:
Datasets: MT03 dataset (998 sentences) as devel-
opment set for parameter tuning, MT04 (1788 sen-
tences) and MT05 (1082 sentences) as test sets.
Features: Dense feature set include the 10 regular
features used in the Hiero system; Sparse feature set
4We also experimented with other metrics including TER,
METEOR and TER-BLEU. Similar trends as reported in this
section were observed. These results are omitted here due to
limited space.
includes 1016 target-side rule POS bi-gram features
as used in (Li et al, 2010b).
Classifiers: Perceptron, MegaM and Maximum
entropy.
PRO parameters: ? = 8000 (number of candidate
pairs sampled uniformly from the n-best list), ? = 1
(sample acceptance probability), ? = 50 (number of
top candidates to be added to the training set).
Figure 3 shows the BLEU score curves on the
development and test sets as a function of itera-
tions. The upper and lower rows correspond to
the results trained with 10 dense features and 1026
dense+sparse features respectively. We intentionally
selected very bad initial parameter vectors to verify
the robustness of the algorithm. It can be seen that
288
with each iteration, the BLEU score increases mono-
tonically on both development and test sets, and be-
gins to converge after a few iterations. When only 10
features are involved, all classifiers give almost the
same performance. However, when scaled to over a
thousand features, the maximum entropy classifier
becomes unstable and the curve fluctuates signifi-
cantly. In this situation MegaM behaves well, but
the J-PRO built-in perceptron gives the most robust
performance.
Table 2 compares the results of running Z-MERT
and J-PRO. Since MERT is not able to handle nu-
merous sparse features, we only report results for
the 10-feature setup. The scores for both setups
are quite close to each other, with Z-MERT doing
slightly better on the development set but J-PRO
yielding slightly better performance on the test set.
4 Thrax: Grammar Extraction at Scale
4.1 Translation Grammars
In previous years, our grammar extraction methods
were limited by either memory-bounded extractors.
Moving towards a parallelized grammar extraction
process, we switched from Joshua?s formerly built-
in extraction module to Thrax for WMT11. How-
ever, we were limited to a simple pseudo-distributed
Hadoop setup. In a pseudo-distributed cluster, all
tasks run on separate cores on the same machine
and access the local file system simultaneously, in-
stead of being distributed over different physical ma-
chines and harddrives. This setup proved unreliable
for larger extractions, and we were forced to reduce
the amount of data that we used to train our transla-
tion models.
For this year, however, we had a permanent clus-
ter at our disposal, which made it easy to extract
grammars from all of the available WMT12 data.
We found that on a properly distributed Hadoop
setup Thrax was able to extract both Hiero gram-
mars and the much larger SAMT grammars on the
complete WMT12 training data for all tested lan-
guage pairs. The runtimes and resulting (unfiltered)
grammar sizes for each language pair are shown in
Table 3 (for Hiero) and Table 4 (for SAMT).
Language Pair Time Rules
Cs ? En 4h41m 133M
De ? En 5h20m 219M
Fr ? En 16h47m 374M
Es ? En 16h22m 413M
Table 3: Extraction times and grammar sizes for Hi-
ero grammars using the Europarl and News Com-
mentary training data for each listed language pair.
Language Pair Time Rules
Cs ? En 7h59m 223M
De ? En 9h18m 328M
Fr ? En 25h46m 654M
Es ? En 28h10m 716M
Table 4: Extraction times and grammar sizes for
the SAMT grammars using the Europarl and News
Commentary training data for each listed language
pair.
4.2 Paraphrase Extraction
Recently English-to-English text generation tasks
have seen renewed interest in the NLP commu-
nity. Paraphrases are a key component in large-
scale state-of-the-art text-to-text generation systems.
We present an extended version of Thrax that im-
plements distributed, Hadoop-based paraphrase ex-
traction via the pivoting approach (Bannard and
Callison-Burch, 2005). Our toolkit is capable of
extracting syntactically informed paraphrase gram-
mars at scale. The paraphrase grammars obtained
with Thrax have been shown to achieve state-of-the-
art results on text-to-text generation tasks (Ganitke-
vitch et al, 2011).
For every supported translation feature, Thrax im-
plements a corresponding pivoted feature for para-
phrases. The pivoted features are set up to be aware
of the prerequisite translation features they are de-
rived from. This allows Thrax to automatically de-
tect the needed translation features and spawn the
corresponding map-reduce passes before the pivot-
ing stage takes place. In addition to features use-
ful for translation, Thrax also offers a number of
features geared towards text-to-text generation tasks
such as sentence compression or text simplification.
Due to the long tail of translations in unpruned
289
Source Bitext Sentences Words Pruning Rules
Fr ? En 1.6M 45M p(e1|e2), p(e2|e1) > 0.001 49M
{Da + Sv + Cs + De + Es + Fr} ? En 9.5M 100M
p(e1|e2), p(e2|e1) > 0.02 31M
p(e1|e2), p(e2|e1) > 0.001 91M
Table 5: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word
counts refer to the English side of the bitexts used.
translation grammars and the combinatorial effect
of pivoting, paraphrase grammars can easily grow
very large. We implement a simple feature-level
pruning approach that allows the user to specify up-
per or lower bounds for any pivoted feature. If a
paraphrase rule is not within these bounds, it is dis-
carded. Additionally, pivoted features are aware of
the bounding relationship between their value and
the value of their prerequisite translation features
(i.e. whether the pivoted feature?s value can be guar-
anteed to never be larger than the value of the trans-
lation feature). Thrax uses this knowledge to dis-
card overly weak translation rules before the pivot-
ing stage, leading to a substantial speedup in the ex-
traction process.
Table 5 gives a few examples of large paraphrase
grammars extracted from WMT training data. With
appropriate pruning settings, we are able to obtain
paraphrase grammars estimated over bitexts with
more than 100 million words.
5 Additional New Features
? With the help of the respective original au-
thors, the language model implementations by
Heafield (2011) and Pauls and Klein (2011)
have been integrated with Joshua, dropping
support for the slower and more difficult to
compile SRILM toolkit (Stolcke, 2002).
? We modified Joshua so that it can be used as
a parser to analyze pairs of sentences using a
synchronous context-free grammar. We imple-
mented the two-pass parsing algorithm of Dyer
(2010).
6 Conclusion
We present a new iteration of the Joshua machine
translation toolkit. Our system has been extended to-
wards efficiently supporting large-scale experiments
in parsing-based machine translation and text-to-text
generation: Joshua 4.0 supports compactly repre-
sented large grammars with its packed grammars,
as well as large language models via KenLM and
BerkeleyLM.We include an implementation of PRO,
allowing for stable and fast tuning of large feature
sets, and extend our toolkit beyond pure translation
applications by extending Thrax with a large-scale
paraphrase extraction module.
Acknowledgements This research was supported
by in part by the EuroMatrixPlus project funded
by the European Commission (7th Framework Pro-
gramme), and by the NSF under grant IIS-0713448.
Opinions, interpretations, and conclusions are the
authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Compu-
tational Linguistics.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of WMT06, pages
94?101. Association for Computational Linguistics.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguistics.
290
Kristy Hollingshead and Brian Roark. 2008. Rerank-
ing with baseline system scores and ranks as features.
Technical report, Center for Spoken Language Under-
standing, Oregon Health & Science University.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010a. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Zhifei Li, Ziyuan Wang, and Sanjeev Khudanpur. 2010b.
Unsupervised discriminative language model training
for machine translation using simulated confusion sets.
In Proceedings of COLING, Beijing, China, August.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Morgan &
Claypool Publishers.
Chris Manning and Dan Klein. 2003. Optimization,
maxent models, and conditional estimation without
magic. In Proceedings of HLT/NAACL, pages 8?8. As-
sociation for Computational Linguistics.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41rd
Annual Meeting of the Association for Computational
Linguistics (ACL-2003), Sapporo, Japan.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of WMT11.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Richard Zens and Hermann Ney. 2007. Efficient phrase-
table representation for machine translation with appli-
cations to online MT and speech translation. In Pro-
ceedings of HLT/NAACL, pages 492?499, Rochester,
New York, April. Association for Computational Lin-
guistics.
291
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 206?212,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joshua 5.0: Sparser, better, faster, server
Matt Post1 and Juri Ganitkevitch2 and Luke Orland1 and Jonathan Weese2 and Yuan Cao2
1Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Sciences Department
University of Pennsylvania
Abstract
We describe improvements made over the
past year to Joshua, an open-source trans-
lation system for parsing-based machine
translation. The main contributions this
past year are significant improvements in
both speed and usability of the grammar
extraction and decoding steps. We have
also rewritten the decoder to use a sparse
feature representation, enabling training of
large numbers of features with discrimina-
tive training methods.
1 Introduction
Joshua is an open-source toolkit1 for hierarchical
and syntax-based statistical machine translation
of human languages with synchronous context-
free grammars (SCFGs). The original version of
Joshua (Li et al, 2009) was a port (from Python to
Java) of the Hiero machine translation system in-
troduced by Chiang (2007). It was later extended
to support grammars with rich syntactic labels (Li
et al, 2010). Subsequent efforts produced Thrax,
the extensible Hadoop-based extraction tool for
synchronous context-free grammars (Weese et al,
2011), later extended to support pivoting-based
paraphrase extraction (Ganitkevitch et al, 2012).
Joshua 5.0 continues our yearly update cycle.
The major components of Joshua 5.0 are:
?3.1 Sparse features. Joshua now supports an
easily-extensible sparse feature implementa-
tion, along with tuning methods (PRO and
kbMIRA) for efficiently setting the weights
on large feature vectors.
1joshua-decoder.org
?3.2 Significant speed increases. Joshua 5.0 is up
to six times faster than Joshua 4.0, and also
does well against hierarchical Moses, where
end-to-end decoding (including model load-
ing) of WMT test sets is as much as three
times faster.
?3.3 Thrax 2.0. Our reengineered Hadoop-based
grammar extractor, Thrax, is up to 300%
faster while using significantly less interme-
diate disk space.
?3.4 Many other features. Joshua now includes a
server mode with fair round-robin scheduling
among and within requests, a bundler for dis-
tributing trained models, improvements to the
Joshua pipeline (for managing end-to-end ex-
periments), and better documentation.
2 Overview
Joshua is an end-to-end statistical machine trans-
lation toolkit. In addition to the decoder com-
ponent (which performs the actual translation), it
includes the infrastructure needed to prepare and
align training data, build translation and language
models, and tune and evaluate them.
This section provides a brief overview of the
contents and abilities of this toolkit. More infor-
mation can be found in the online documentation
(joshua-decoder.org/5.0/).
2.1 The Pipeline: Gluing it all together
The Joshua pipeline ties together all the infrastruc-
ture needed to train and evaluate machine transla-
tion systems for research or industrial purposes.
Once data has been segmented into parallel train-
ing, development, and test sets, a single invocation
of the pipeline script is enough to invoke this entire
infrastructure from beginning to end. Each step is
206
broken down into smaller steps (e.g., tokenizing a
file) whose dependencies are cached with SHA1
sums. This allows a reinvoked pipeline to reliably
skip earlier steps that do not need to be recom-
puted, solving a common headache in the research
and development cycle.
The Joshua pipeline is similar to other ?ex-
periment management systems? such as Moses?
Experiment Management System (EMS), a much
more general, highly-customizable tool that al-
lows the specification and parallel execution of
steps in arbitrary acyclic dependency graphs
(much like the UNIX make tool, but written with
machine translation in mind). Joshua?s pipeline
is more limited in that the basic pipeline skeleton
is hard-coded, but reduced versatility covers many
standard use cases and is arguably easier to use.
The pipeline is parameterized in many ways,
and all the options below are selectable with
command-line switches. Pipeline documentation
is available online.
2.2 Data preparation, alignment, and model
building
Data preparation involves data normalization (e.g.,
collapsing certain punctuation symbols) and tok-
enization (with the Penn treebank or user-specified
tokenizer). Alignment with GIZA++ (Och and
Ney, 2000) and the Berkeley aligner (Liang et al,
2006b) are supported.
Joshua?s builtin grammar extractor, Thrax, is
a Hadoop-based extraction implementation that
scales easily to large datasets (Ganitkevitch et al,
2013). It supports extraction of both Hiero (Chi-
ang, 2005) and SAMT grammars (Zollmann and
Venugopal, 2006) with extraction heuristics eas-
ily specified via a flexible configuration file. The
pipeline also supports GHKM grammar extraction
(Galley et al, 2006) using the extractors available
from Michel Galley2 or Moses.
SAMT and GHKM grammar extraction require
a parse tree, which are produced using the Berke-
ley parser (Petrov et al, 2006), or can be done out-
side the pipeline and supplied as an argument.
2.3 Decoding
The Joshua decoder is an implementation of the
CKY+ algorithm (Chappelier et al, 1998), which
generalizes CKY by removing the requirement
2nlp.stanford.edu/?mgalley/software/
stanford-ghkm-latest.tar.gz
that the grammar first be converted to Chom-
sky Normal Form, thereby avoiding the complex-
ities of explicit binarization schemes (Zhang et
al., 2006; DeNero et al, 2009). CKY+ main-
tains cubic-time parsing complexity (in the sen-
tence length) with Earley-style implicit binariza-
tion of rules. Joshua permits arbitrary SCFGs, im-
posing no limitation on the rank or form of gram-
mar rules.
Parsing complexity is still exponential in the
scope of the grammar,3 so grammar filtering re-
mains important. The default Thrax settings ex-
tract only grammars with rank 2, and the pipeline
implements scope-3 filtering (Hopkins and Lang-
mead, 2010) when filtering grammars to test sets
(for GHKM).
Joshua uses cube pruning (Chiang, 2007) with
a default pop limit of 100 to efficiently explore the
search space. Other decoder options are too nu-
merous to mention here, but are documented on-
line.
2.4 Tuning and testing
The pipeline allows the specification (and optional
linear interpolation) of an arbitrary number of lan-
guage models. In addition, it builds an interpo-
lated Kneser-Ney language model on the target
side of the training data using KenLM (Heafield,
2011; Heafield et al, 2013), BerkeleyLM (Pauls
and Klein, 2011) or SRILM (Stolcke, 2002).
Joshua ships with MERT (Och, 2003) and PRO
implementations. Tuning with k-best batch MIRA
(Cherry and Foster, 2012) is also supported via
callouts to Moses.
3 What?s New in Joshua 5.0
3.1 Sparse features
Until a few years ago, machine translation systems
were for the most part limited in the number of fea-
tures they could employ, since the line-based op-
timization method, MERT (Och, 2003), was not
able to efficiently search over more than tens of
feature weights. The introduction of discrimina-
tive tuning methods for machine translation (Liang
et al, 2006a; Tillmann and Zhang, 2006; Chiang
et al, 2008; Hopkins and May, 2011) has made
it possible to tune large numbers of features in
statistical machine translation systems, and open-
3Roughly, the number of consecutive nonterminals in a
rule (Hopkins and Langmead, 2010).
207
source implementations such as Cherry and Foster
(2012) have made it easy.
Joshua 5.0 has moved to a sparse feature rep-
resentation internally. First, to clarify terminol-
ogy, a feature as implemented in the decoder is
actually a template that can introduce any number
of actual features (in the standard machine learn-
ing sense). We will use the term feature function
for these templates and feature for the individual,
traditional features that are induced by these tem-
plates. For example, the (typically dense) features
stored with the grammar on disk are each separate
features contributed by the PHRASEMODEL fea-
ture function template. The LANGUAGEMODEL
template contributes a single feature value for each
language model that was loaded.
For efficiency, Joshua does not store the en-
tire feature vector during decoding. Instead, hy-
pergraph nodes maintain only the best cumulative
score of each incoming hyperedge, and the edges
themselves retain only the hyperedge delta (the in-
ner product of the weight vector and features in-
curred by that edge). After decoding, the feature
vector for each edge can be recomputed and ex-
plicitly represented if that information is required
by the decoder (for example, during tuning).
This functionality is implemented via the fol-
lowing feature function interface, presented here
in simplified pseudocode:
interface FeatureFunction:
apply(context, accumulator)
The context comprises fixed pieces of the input
sentence and hypergraph:
? the hypergraph edge (which represents the
SCFG rule and sequence of tail nodes)
? the complete source sentence
? the input span
The accumulator object?s job is to accumulate
feature (name,value) pairs fired by a feature func-
tion during the application of a rule, via another
interface:
interface Accumulator:
add(feature_name, value)
The accumulator generalization4 permits the use
of a single feature-gathering function for two ac-
cumulator objects: the first, used during decoding,
maintains only a weighted sum, and the second,
4Due to Kenneth Heafield.
used (if needed) during k-best extraction, holds
onto the entire sparse feature vector.
For tuning large sets of features, Joshua sup-
ports both PRO (Hopkins and May, 2011), an in-
house version introduced with Joshua 4.0, and k-
best batch MIRA (Cherry and Foster, 2012), im-
plemented via calls to code provided by Moses.
3.2 Performance improvements
We introduced many performance improvements,
replacing code designed to get the job done under
research timeline constraints with more efficient
alternatives, including smarter handling of locking
among threads, more efficient (non string-based)
computation of dynamic programming state, and
replacement of fixed class-based array structures
with fixed-size literals.
We used the following experimental setup to
compare Joshua 4.0 and 5.0: We extracted a large
German-English grammar from all sentences with
no more than 50 words per side from Europarl v.7
(Koehn, 2005), News Commentary, and the Com-
mon Crawl corpora using Thrax default settings.
After filtering against our test set (newstest2012),
this grammar contained 70 million rules. We then
trained three language models on (1) the target
side of our grammar training data, (2) English
Gigaword, and (3) the monolingual English data
released for WMT13. We tuned a system using
kbMIRA and decoded using KenLM (Heafield,
2011). Decoding was performed on 64-core 2.1
GHz AMD Opteron processors with 256 GB of
available memory.
Figure 1 plots the end-to-end runtime5 as a
function of the number of threads. Each point in
the graph is the minimum of at least fifteen runs
computed at different times over a period of a few
days. The main point of comparison, between
Joshua 4.0 and 5.0, shows that the current version
is up to 500% faster than it was last year, espe-
cially in multithreaded situations.
For further comparison, we took these models,
converted them to hierarchical Moses format, and
then decoded with the latest version.6 We com-
piled Moses with the recommended optimization
settings7 and used the in-memory (SCFG) gram-
5i.e., including model loading time and grammar sorting
6The latest version available on Github as of June 7, 2013
7With tcmalloc and the following compile flags:
--max-factors=1 --kenlm-max-order=5
debug-symbols=off
208
 500 1000 2000 3000 4000
 5000 10000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 4.0 (in-memory)Moses (in-memory)Joshua 4.0 (packed)Joshua 5.0 (packed)
Figure 1: End-to-end runtime as a function of the
number of threads. Each data point is the mini-
mum of at least fifteen different runs.
 200 300 400 500 1000 2000
 3000 4000 5000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 5.0Moses
Figure 2: Decoding time alone.
mar format. BLEU scores were similar.8 In this
end-to-end setting, Joshua is about 200% faster
than Moses at high thread counts (Figure 1).
Figure 2 furthers the Moses and Joshua com-
parison by plotting only decoding time (subtract-
ing out model loading and sorting times). Moses?
decoding speed is 2?3 times faster than Joshua?s,
suggesting that the end-to-end gains in Figure 1
are due to more efficient grammar loading.
3.3 Thrax 2.0
The Thrax module of our toolkit has undergone
a similar overhaul. The rule extraction code was
822.88 (Moses), 22.99 (Joshua 4), and 23.23 (Joshua 5).
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 3: Here, position-aware lexical and part-of-
speech n-gram features, labeled dependency links,
and features reflecting the phrase?s CCG-style la-
bel NP/NN are included in the context vector.
rewritten to be easier to understand and extend, al-
lowing, for instance, for easy inclusion of alterna-
tive nonterminal labeling strategies.
We optimized the data representation used for
the underlying map-reduce framework towards
greater compactness and speed, resulting in a
300% increase in extraction speed and an equiv-
alent reduction in disk I/O (Table 1). These
gains enable us to extract a syntactically labeled
German-English SAMT-style translation grammar
from a bitext of over 4 million sentence pairs in
just over three hours. Furthermore, Thrax 2.0 is
capable of scaling to very large data sets, like
the composite bitext used in the extraction of the
paraphrase collection PPDB (Ganitkevitch et al,
2013), which counted 100 million sentence pairs
and over 2 billion words on the English side.
Furthermore, Thrax 2.0 contains a module fo-
cused on the extraction of compact distributional
signatures over large datasets. This distribu-
tional mode collects contextual features for n-
gram phrases, such as words occurring in a win-
dow around the phrase, as well as dependency-
based and syntactic features. Figure 3 illustrates
the feature space. We then compute a bit signature
from the resulting feature vector via a randomized
locality-sensitive hashing projection. This yields a
compact representation of a phrase?s typical con-
text. To perform this projection Thrax relies on
the Jerboa toolkit (Van Durme, 2012). As part of
the PPDB effort, Thrax has been used to extract
rich distributional signatures for 175 million 1-
to-4-gram phrases from the Annotated Gigaword
corpus (Napoles et al, 2012), a parsed and pro-
209
Cs-En Fr-En De-En Es-En
Rules 112M 357M 202M 380M
Space Time Space Time Space Time Space Time
Joshua 4.0 120GB 112 min 364GB 369 min 211GB 203 min 413GB 397 min
Joshua 5.0 31GB 25 min 101GB 81 min 56GB 44 min 108GB 84 min
Difference -74.1% -77.7% -72.3% -78.0% -73.5% -78.3% -73.8% -78.8%
Table 1: Comparing Hadoop?s intermediate disk space use and extraction time on a selection of Europarl
v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax?s final
grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers
and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.
cessed version of the English Gigaword (Graff et
al., 2003).
Thrax is distributed with Joshua and is also
available as a separate download.9
3.4 Other features
Joshua 5.0 also includes many features designed
to increase its usability. These include:
? A TCP/IP server architecture, designed to
handle multiple sets of translation requests
while ensuring fairness in thread assignment
both across and within these connections.
? Intelligent selection of translation and lan-
guage model training data using cross-
entropy difference to rank training candidates
(Moore and Lewis, 2010; Axelrod et al,
2011) (described in detail in Orland (2013)).
? A bundler for easy packaging of trained mod-
els with all of its dependencies.
? A year?s worth of improvements to the
Joshua pipeline, including many new features
and supported options, and increased robust-
ness to error.
? Extended documentation.
4 WMT Submissions
We submitted a constrained entry for all tracks ex-
cept English-Czech (nine in total). Our systems
were constructed in a straightforward fashion and
without any language-specific adaptations using
the Joshua pipeline. For each language pair, we
trained a Hiero system on all sentences with no
more than fifty words per side in the Europarl,
News Commentary, and Common Crawl corpora.
9github.com/joshua-decoder/thrax
We built two interpolated Kneser-Ney language
models: one from the monolingual News Crawl
corpora (2007?2012), and another from the tar-
get side of the training data. For systems translat-
ing into English, we added a third language model
built on Gigaword. Language models were com-
bined linearly into a single language model using
interpolation weights from the tuning data (new-
stest2011). We tuned our systems with kbMIRA.
For truecasing, we used a monolingual translation
system built on the training data, and finally deto-
kenized with simple heuristics.
5 Summary
The 5.0 release of Joshua is the result of a signif-
icant year-long research, engineering, and usabil-
ity effort that we hope will be of service to the
research community. User-friendly packages of
Joshua are available from joshua-decoder.
org, while developers are encouraged to partic-
ipate via github.com/joshua-decoder/
joshua. Mailing lists, linked from the main
Joshua page, are available for both.
Acknowledgments Joshua?s sparse feature rep-
resentation owes much to discussions with Colin
Cherry, Barry Haddow, Chris Dyer, and Kenneth
Heafield at MT Marathon 2012 in Edinburgh.
This material is based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
210
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362, Edinburgh, Scotland, UK., July.
J.C. Chappelier, M. Rajman, et al 1998. A generalized
CYK algorithm for parsing stochastic CFG. In First
Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133?137.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL-HLT, pages 427?436, Montre?al,
Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, Waikiki, Hawaii, USA, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, Adam Pauls, and Dan Klein. 2009.
Asynchronous binarization for synchronous gram-
mars. In Proceedings of ACL, Suntec, Singapore,
August.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL/COLING, Sydney, Australia, July.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, PRO, and paraphrases. In Proceedings of
the Workshop on Statistical Machine Translation.
Juri Ganitkevitch, Chris Callison-Burch, and Benjamin
Van Durme. 2013. Ppdb: The paraphrase database.
In Proceedings of HLT/NAACL.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2003.
English gigaword. Linguistic Data Consortium,
Philadelphia.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of ACL, Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguis-
tics.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proceedings of
EMNLP, pages 646?655.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation, Athens,
Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proceedings of the Workshop on
Statistical Machine Translation.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of ACL/COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of
NAACL, pages 104?111, New York City, USA, June.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of ACL (short papers), pages 220?224.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL, pages 440?
447, Hong Kong, China, October.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
Sapporo, Japan.
Luke Orland. 2013. Intelligent selection of trans-
lation model training data for machine translation
with TAUS domain data: A summary. Master?s the-
sis, Johns Hopkins University, Baltimore, Maryland,
June.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
pages 258?267, Portland, Oregon, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL,
Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Seventh International
Conference on Spoken Language Processing.
211
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721?728, Syd-
ney, Australia, July.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Ex-
cellence, Johns Hopkins University.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the
Thrax grammar extractor. In Proceedings of the
Workshop on Statistical Machine Translation.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, New York, New York.
212
