A language?independent shallow?parser Compiler 
Alexandra Kinyon
CIS Dpt. .
University of Pennsylvania
kinyon@linc.cis.upenn.edu
http://www.cis.upenn.edu/?kinyon
Abstract
We present a rule?based shallow?
parser compiler, which allows to
generate a robust shallow?parser for
any language, even in the absence of
training data, by resorting to a very
limited number of rules which aim at
identifying constituent boundaries.
We contrast our approach to other
approaches used for shallow?parsing
(i.e. finite?state and probabilistic
methods). We present an evaluation
of our tool for English (Penn
Treebank) and for French (newspaper
corpus "LeMonde") for several tasks
(NP?chunking & "deeper" parsing) .
1 Introduction
Full syntactic parsers of unrestricted text are
costly to develop, costly to run and often yield
errors, because of lack of robustness of wide?
coverage grammars and problems of attachment.
This has led, as early as 1958 (Joshi & Hopely
97), to the development of shallow?parsers,
which aim at identifying as quickly and
accurately as possible, main constituents  (and
possibly syntactic functions) in an input,
without dealing with the most difficult problems
encountered with "full?parsing". Hence,
shallow?parsers are very practical tools. There
are two main techniques used to develop
shallow?parsers:
1?   Probabilistic techniques (e.g. Magerman
94, Ratnaparkhi 97, Daelmans & al. 99) 
2? Finite?state techniques (e.g. Grefenstette 96)
Probabilistic techniques require large amounts
of syntactically?annotated training data1, which
makes them very unsuitable for languages for
1 We are leaving aside unsupervised learning techniques
here, since to our knowledge they have not proved a
successful  for  developing practical shallow?parsers. 
which no such data is available (i.e. most
languages except English) and also, they are not
domain?independent nor "style?independent"
(e.g. they do not allow to successfully shallow?
parse speech, if no annotated data is available
for that ?style?).  Finally, a shallow?parser
developed using these techniques will have to
mirror the information contained in the training
data. For instance, if one trains such a tool on
data were only non recursive NP chunks are
marked2, then one will not be able to obtain
richer information such as chunks of other
categories, embeddings, syntactic functions... 
On the other hand, finite?state techniques rely
on the development of a large set of rules (often
based on regular expressions) to capture all the
ways a constituent can expend. So for example
for detecting English NPs, one could write the
following rules :
NP ? Det adj* noun adj* 
NP ?  Det adj         (for noun ellipsis)  
NP ?  ProperNoun etc ....          
But this is time consuming and difficult since
one needs to foresee all possible rewriting cases,
and if some rule is forgotten, or if too many
POS errors are left, robustness and/or accuracy
will suffer.
Then these regular expressions have to be
manipulated i.e. transformed into automata,
which will be determinized and minimized
(both being costly operations). And even though
determinization and minimization must be done
only once (in theory) for a given set of rules, it
is still costly to port such tools to a new set of
rules (e.g. for a new language, a new domain) or
to change some existing rules. 
In this paper, we argue that in order to
accomplish the same task, it is unnecessary to
develop full sets of regular expression : instead 
2
 See (Abney 91) for the definition of a chunk.
of specifying all the ways a constituent can be
rewritten, it is sufficient to express how it
begins and/or ends. This allows to achieve
similar results but with far fewer rules, and
without a need for determinization or
minimization because rules which are written
that way are de?facto deterministic. So in a
sense, our approach bears some similarities with
the constraint?based formalism because we
resort to ?local rules? (Karlsson & al. 95), but
we focus on identifying constituent boundaries
(and not syntactic functions), and allow any
level of embedding thanks to the use of a stack.
In the first part of this paper, we present our
tool: a shallow?parser compiler. In a second
part, we present output samples as well as
several evaluations  for French and for English,
where the tool has been used to develop both an
NP?chunker and a richer shallow?parser. We
also explain why our approach is more tolerant
to POS?tagging errors. Finally, we discuss
some other practical uses which are made of this
shallow?parser compiler. 
2   Presentation of the compiler
Our tool has been developed using JavaCC (a
compiler compiler similar to Lex & Yacc, but
for java). The program takes as input a file
containing rules. These rules aim at identifying
constituent boundaries for a given language.
(For example for English, one such rule could
say  "When encountering a preposition, start a
PP"), either by relying on function words, or on
morphological information (e.g. gender) if it is
appropriate for the language which is being
considered.
 These rule files specify :
? A mapping between the "abstract" morpho?
syntactic tags, used in the rules, and "real"
morpho?syntactic tags as they will appear
in the input.
? A declaration of the syntactic constituents
which will be detected (e.g. NP, VP, PP ...)
? A set of unordered rules 
From this rule file, the compiler generates a java
program, which is a shallow?parser based on
the rule file. One can then run this shallow?
parser on an input to obtain a shallow?parsed
text3.
The compiler itself is quite simple, but we have
decided to compile the rules rather than interpret
them essentially for efficiency reasons. Also, it
3 The input is generally POS?tagged, although this is not
an intrinsic requirement of the compiler.
is language independent since a rule file may be
written for any given language, and compiled
into a shallow?parser for that language.
Each rule is of the form: 
{Preamble} disjunction of patterns then actions
2.1 A concrete example : compiling a
simple NP?chunker for English
In this section we present a very simple "toy"
example which aims at identifying some NPs in
the Penn Treebank4 (Marcus & al 93).
In order to do so, we write a rule file, shown on
figure 1. The top of the file declares a mapping
between the abstract tagset we use in our rules,
and the tagset of the PennTreebank. For
example commonN corresponds to the 3 tags NN,
NNS, NNPS in the PennTreebank. It then
declares the labels of the constituents which will
be detected (here there is only one: NP). Finally,
it declares 3 rules. 
%% A small NP?chunker for the Penn?treebank
tagmap <QuantityAdv:any,some,many>;
tagmap<ProperN:NNP>;
tagmap<det:DT,PDT>;
tagmap<commonN:NN,NNS,NNPS>;
tagmap<DemPro:D*>;
tagmap<Adj:JJ*>;
tagmap<OtherTags:V*,P,C*,RB*.,:,,>;
label NP;
%% rule 1
{} (:$det) | ($QuantityAdv:) | (:$DemPro) then
close(),open(NP);
%% rule 2
{!NP} (:$commonN) | (:$Adj) | (:$ProperN) then
close(),open(NP);
%% rule 3
{} (:$OtherTags) then close();
FIGURE 1 : An example of a  rule?file
Rule 1 says that when a determiner, a quantity
adverb or a demonstrative pronoun is
encountered, the current constituent must be
closed, and an NP must be opened. Rule 2 says
that, when not inside an NP, if a common noun,
an adjective or a proper noun is encountered,
then the current constituent should be closed and
an NP should be opened. Finally, Rule 3 says
that when some other tag is encountered (i.e. a
verb, a preposition, a punctuation, a conjunction
4 This example is kept very simple for sake of clarity. It
does not aim at yielding a very accurate result.
or  an adverb) then the current constituent
should be closed.
This rule file is then compiled into an NP?
chunker. If one inputs  (a) to the NP?chunker, it
will then output (b)
(a) The/DT cat/NNS eats/VBZ the/DT
mouse/NNS ./.
(b) <NP> The/DT cat/NNS </NP> eats/VBZ <NP>
the/DT mouse/NNS </NP> ./.
In our compiler, rules access  a limited context :
? The constituent(s) being built
? The previous form and POS
? The current form and POS
? The next form and POS
So contrary to standard finite?state techniques,
only constituent boundaries are explicited, and
it is not necessary (or even possible) to specify
all the possible ways a constituent may be
realized .
As shown in section 3, this reduces greatly the
number of rules in the system (from several
dozens to less than 60 for a wide?coverage
shallow?parser). Also, focussing only on
constituent boundaries  ensures determinism :
there is no need for determinizing nor
minimizing the automata we obtain from our
rules.  
Our tool is robust : it never fails to provide an
output and can be used to create a parser for any
text from any domain in any language.
It is also important to note that the parsing is
done incrementally : the input is scanned strictly
from left to right, in one single pass. And for
each pattern matched, the associated actions are
taken (i.e. constituent boundaries are added).
Since there is no backtracking, this allows an
output in linear time. If several patterns match,
the longest one is applied. Hence our rules are
declarative and unordered. Although in theory
conflicts could appear between 2 patterns of
same length (as shown in (c1) and (c2)), this has
never happened in practice. Of course the case is
nonetheless dealt with in the implementation,
and a warning is then issued to the user.
(c1)  {} (:a) (:b) then close();
(c2) {} (:a) (:b) then open(X);
As is seen on figure 1, one can write
disjunctions of patterns for a given rule.  
In this very simple example, only non recursive
NP?chunks are marked, by choice. But it is not
an intrinsic limitation of the tool, since any
amount of embedding can be obtained (as
shown in section 3 below), through the use of a
Stack. From a formal point of view, our tool has
the power of a deterministic push?down
automaton.
When there is a match between the input and the
pattern in a rule, the following actions may be
taken :
? close(): closes the constituent last opened by
inserting </X> in the output, were X is the
syntactic label at the top of the Stack.
? open(X): opens a new constituent by
inserting label <X> in the output
? closeWhenOpen(X,Y):  delays the closing
of constituent labeled X, until constituent
labeled Y is opened. 
? closeWhenClose(X,Y): delays the closing of
constituent labeled X, until constituent
labeled Y is closed. 
? doNothing(): used to "neutralize" a shorter
match.
Examples for the actions open() and close() were
provided on figure 1. The actions
closeWhenOpen(X,Y) and closeWhenClose(X,Y)
allow to perform some attachments. For
example a rule for English could say :
{NP} (:$conjCoord) then close(), open(NPcoord),
closeWhenClose(NPcoord,NP);
This rule says that  when inside an NP, a
coordinating conjunction is encountered, a
NPcoord should be opened, and should be
closed only when the next NP to the right will
be closed. This allows, for example, to obtain
output (d) for a coordination of NPs5.
 (d) John likes 
<NP>Apples</NP> 
<NPcoord> and 
<NP> green beans </NP>
 </NPcoord>
An example of the action doNothing() for
English could be:
{} (:$prep) then open(PP);
{} P(:$prep) (:$prep) then doNothing()  ;
The first rule says that when a preposition is
encountered, a PP should be opened. The second
rule says that when a preposition is encountered,
if the previous tag was also a preposition,
nothing should be done. Since the pattern for
5
 This is shown as an example as to how this action can be
used, it does not aim at imposing this structure to
coordinations, which could be dealt with differently using
other rules.
rule 2 is longer than the pattern for rule 1, it will
apply when the second preposition in a row is
encountered, hence "neutralizing" rule 1. This
allows to obtain "flatter" structures for PPs,
such as the one in (e1). Without this rule, one
would obtain the structure in (e2) for the same
input.
(e1) This costs <PP> up  to 1000 $ </PP>
(e2) This costs 
<PP> up 
<PP> to 1000 $ </PP>
</PP>
3. Some "real world" applications
In this section, we present some uses which
have been made of this Shallow?Parser
compiler. First we explain how the tool has been
used to develop a 1 million word Treebank for
French, along with an evaluation. Then we
present an evaluation for English. 
It is well known that evaluating a Parser is a
difficult task, and this is even more true for
Shallow?Parsers, because there is no real
standard task (some Shallow?parsers have
embedded constituents, some encode syntactic
functions, some encode constituent information,
some others dependencies or even a mixture of
the 2) There also isn?t standard evaluation
measures for such tools. To perform evaluation,
one can compare the output of the parser to a
well?established Treebank developed
independently (assuming one is available for the
language considered), but the result is unfair to
the parser because generally in Treebanks all
constituents are attached. One can also compare
the output of the parser to a piece of text which
has been manually annotated just for the
purpose of the evaluation. But then it is difficult
to ensure an objective measure (esp. if the
person developing the parser and the person
doing the annotation are the same). Finally, one
can automatically extract, from a well?
established Treebank, information that is
relevant to a given , widely agreed on, non
ambiguous task such as identifying bare non?
recursive NP?chunks, and compare the output
of the parser for that task to the extracted
information. But this yields an evaluation that is
valid only for this particular task and may not
well reflect the overall performance of the
parser.  In what follows, in order to be as
objective as possible, we use these 3 types of
evaluation, both for French and for English6, and
use standard measures of recall and precision.
Please bear in mind though that these metric
measures, although very fashionable, have their
limits7. Our goal is not to show that our tool is
the one which provides the best results when
compared to other shallow?parsers, but rather to
show that it obtains similar results, although in a
much simpler way, with a limited number of
rules compared to finite?state techniques and
more tolerance to POS errors,  and even in the
absence of available training data (i.e. cases
were probabilistic techniques could not be
used). To achieve this goal, we also present
samples of parsed outputs we obtain, so that the
reader may judge for himself/herself.
3.1. A shallow?parser for French
We used our compiler to create a shallow?parser
for French. Contrary to English, very few
shallow?parsers exist for French, and no
Treebank actually exist to train a probabilistic
parser (although one is currently being built
using our tool c.f. (Abeill? & al. 00)).
Concerning shallow?parsers, one can mention
(Bourigault 92) who aims at isolating NPs
representing technical terms, whereas we wish
to have information on other constituents as
well, and (Ait?Moktar & Chanod 97)  whose
tool is not publicly available. One can also
mention (Vergne 99), who developed a parser
for French which also successfully relies on
function words to identify constituent
boundaries. But contrary to us, his tool does not
embed constituents8. And it is also not publicly
available.
In order to develop a set of rules for French, we
had to examine the linguistic characteristics of
this language. It turns out that although French
has a richer morphology than English (e.g.
gender for nouns, marked tense for verbs), most
constituents are nonetheless triggered by the
occurrence of a function word. Following the
linguistic tradition, we consider as function
words all words associated to a POS which
labels a closed?class i.e. : determiners,
prepositions, clitics, auxiliaries, pronouns
(relative, demonstrative), conjunctions
6 Of course, manual annotation was done by a different
person than the one who developed the rules.
7 For instance in a rule?based system, performance may
often be increased by adding more rules.
8 Instead, it identifies chunks and then assigns some
syntactic functions to these chunks. 
(subordinating, coordinating), auxiliaries,
punctutation marks and adverbs belonging to a
closed class (e.g. negation adverbs "ne" "pas")9.
The presence of function words makes the
detection of the beginning of a constituent rather
easy. For instance, contrary to English,
subordinating conjunctions (que/that) are never
omitted when a subordinating clause starts.
Similarly,  determiners are rarely omitted at the
beginning of an NP. 
Our aim was to develop a shallow?parser which
dealt with some embedding, but did not commit
to attach potentially ambiguous phrases such as
PPs and verb complements. We wanted to
identify the following constituents : NP, PP, VN
(verbal nucleus), VNinf (infinitivals introduced
by a preposition), COORD (for coordination),
SUB (sentential complements), REL (relative
clauses), SENT (sentence boundaries), INC (for
constituents of unknown category), AdvP
(adverbial phrases).
We wanted NPs to include all adjectives but not
other postnominal  modifiers (i.e. postposed
relative clauses and PPs), in order to obtain a
structure similar to (f).
(f) <NP> Le beau livre bleu </NP> 
      <PP> de  <NP>ma cousine</NP> </PP> ?
         (my cousin?s beautiful blue book)
Relative clauses also proved easy to identify
since they begin when a relative pronoun is
encountered. The ending of clauses occurs
essentially when a punctuation mark or a
conjunction of coordination is encountered or
when another clause begins, or when a sentence
ends (g1) . These rules for closing clauses work
fairly well in practice (see evaluation below) but
could be further refined, since they will yield a
wrong closing boundary for the relative in a
sentence like  (g2)
(g1) <SENT> <NP> Jean </NP> 
      <VN> voit</VN> 
     <NP>la femme </NP>
      <REL> qui 
             <VN> pense </VN>
             <SUB> que 
                       <NP> Paul </NP> 
                       <VN> viendra </VN> 
             </SUB> 
        </REL>  .  </SENT> 
  (John sees the woman who thinks that Paul will
come)
9 Considering punctuation marks as function words may
be "extending" the linguistic tradition. Nonetheless, it is a
closed class, since there is a small finite number of
punctuation marks.
(g2) * <SENT> <NP> Jean </NP> 
          <VN> pense</VN>
                   <SUB> que 
                           <NP> la femme </NP> 
                            <REL> que 
                                       <NP> Pierre </NP> 
                                       <VN> voit</VN> 
                                        <VN> aime </VN> 
                                       <NP>  Donald </NP> 
                            </REL> 
                    </SUB>  . </SENT>
(*John thinks that the woman [REL that Peter sees
likes Donald])
Concerning clitics, we have decided to group
them with the verb (h1) even when dealing with
subject clitics (h2).  One motivation is the
possible inversion of the subject clitic (h3).
(h1)  <SENT><NP> JEAN </NP> 
        <VN> le lui donne</VN> . </SENT> 
(J. gives it to him).
(h2) <SENT> <VN> Il le voit </VN>  . </SENT>
                  (He sees it)
(h3) <SENT><VN> L?as tu vu </VN>  ? </SENT>
             (Him did you see ?).
Sentences are given a flat structure, that is
complements are not included in a verbal phrase10
(i). From a practical point of view this eases our
task. From a theoretical point of view,  the
traditional VP (with complements) is subject to
much linguistic debate and is often
discontinuous in French as is shown in (j1) and
(j2): In (j1) the NP subject (IBM) is postverbal
and precedes the locative complement (sur le
march?). In (j2), the adverb certainement is also
postverbal and precedes the NP object  (une
augmentation de capital).
(i) <SENT><NP> JEAN </NP> 
     <VN> donne</VN> 
     <NP>une pomme</NP> 
     <PP> ? <NP> Marie </NP> </PP> . </SENT> 
            (John gives an apple to Mary)
(j1) les actions qu?a mises IBM sur le march?
(the shares that IBM put on the market)
(j2) Les actionnaires d?cideront certainement une
augmentation de capital (the stock holders will
certainly decide on a raise of capital)
3.1.1 Evaluation for French
10
 Hence the use of VN(for verbal nucleus) instead of VP.
When we began our task, we had at our disposal
a 1 million word POS tagged and hand?
corrected corpus (Abeill? & Cl?ment 98). The
corpus was meant to be syntactically annotated
for constituency. To achieve this, precise
annotation guidelines for constituency had been
written and a portion of the corpus had been
hand?annotated (independently of the
development of the shallow?parser) to test the
guidelines (approx. 25 000 words) . 
To evaluate the shallow parser, we
performed as described at the beginning of
section 3 : We parsed the 1 million words. We
set aside 500 sentences (approx. 15 000 words)
for quickly tuning our rules. We also set aside
the 25 000 words that had been independently
annotated in order to compare the output of the
parser to a portion of the final Treebank. In
addition, an annotator hand?corrected the output
of the shallow?parser on 1000 new randomly
chosen sentences (approx. 30 000 words).
Contrary to the 25 000 words which constituted
the beginning of the Treebank, for these 30 000
words verb arguments, PPs and modifiers were
not attached. Finally, we extracted bare non?
recursive NPs from the 25 000 words, in order
to evaluate how the parser did on this particular
task.
When compared to the hand?corrected
parser?s output, for opening brackets we  obtain
a recall of 94.3 % and a precision of 95.2%. For
closing brackets, we obtain a precision of 92.2
% and a recall of 91.4 %. Moreover, 95.6 % of
the correctly placed brackets are labeled
correctly, the remaining 4.4% are not strictly
speaking labeled incorrectly, since they are
labeled INC (i.e. unknown) These unknown
constituents, rather then errors, constitute a
mechanism of underspecification (the idea being
to assign as little wrong information as
possible)11. 
When compared to the 25 000 words of the
Treebank,  For opening brackets, the recall is
92.9% and the precision is 94%. For closing
brackets, the recall is 62,8% and the precision is
65%. These lower results are normal, since the
Treebank contains attachments that the parser is
not supposed to make.
Finally, on the specific task of identifying non?
recursive NP?chunks, we obtain a recall of 96.6
% and a precision of 95.8 %. for opening
11 These underspecified label can be removed at a deeper
parsing stage, or one can add a guesser  .
<SENT>  <NP> La:Dfs proportion:NC </NP> 
              <PP> d?:P <NP> ?tudiants:NC </NP> </PP> 
              <PP> par_rapport_?:P 
<NP> la:Ddef population:NC</NP> </PP> 
              <PONCT> ,:PONCT </PONCT> 
              <PP> dans:P <NP> notre:Dposs
pays:NC</NP> </PP> 
             <PONCT> ,:PONCT</PONCT> 
             <VN> est:VP inf?rieure:Aqual </VN> 
            <PP> ?:P <NP> ce:PROdem</NP> </PP> 
             <REL> qu:PROR3ms 
          <VN> elle:CL est:VP </VN> 
<COORD> <PP> ?:P <NP> les:Ddef Etats?
Unis:NP </NP> </PP>   ou:CC 
<PP> ?:P <NP> le:Ddef
Japon:NP</NP></PP> </COORD> 
             </REL> <PONCT> .:PONCT</PONCT>
</SENT> 
<SENT>  <NP> Les:Dmp pays:NC</NP> 
<NP> les:Ddef plus:ADV efficaces:Aqual
?conomiquement:ADV</NP> 
                <VN> sont:VP</VN> 
                <NP> ceux:PROdem</NP> 
                <REL> qui:PROR 
                      <VN> ont:VP</VN> 
<NP> les:Ddef travailleurs:NC les:Ddef
mieux:ADV</NP>
                      <VN> form?s:VK</VN> 
                </REL> <PONCT> .:PONCT</PONCT>
</SENT> 
<SENT>  <ADVP> D?autre_part:ADV</ADVP> 
<PONCT> ,:PONCT </PONCT> 
              <SUB> si:CS 
           <VN> nous:CL voulons:VP demeurer:VW
</VN> 
            <NP> une:Dind grande_puissance:NC</NP> 
             </SUB> <PONCT> ,:PONCT</PONCT> 
              <VN> nous:CL devons:VP rester:VW</VN> 
               <NP> un:Dind pays:NC</NP> 
            <REL> qui:PROR 
                          <VN> cr?e:VP</VN> 
                         <NP> le:Ddef savoir:NC</NP> 
          </REL><PONCT> .:PONCT</PONCT>
</SENT> 
<SENT>  <COORD> Et:CC 
<PP> pour:P <NP> cela:PROdem</NP>
</PP> </COORD> 
               <PONCT> ,:PONCT </PONCT> 
              <VN> il:CL faut:VP</VN> 
<NP> un:Dind
enseignement_sup?rieur:NC fort:Aqual</NP> 
<PONCT> .:PONCT</PONCT> </SENT> 
<SENT>  <COORD> Mais:CC 
                <PP> en_dehors_de:P 
                <NP> ces:Ddem raisons:NC
?conomiques:Aqual ou:CC
philosophiques:Aqual </NP> </PP>
</COORD> 
              <PONCT> ,:PONCT </PONCT> 
              <VN> il:CL y:CL a:VP </VN> 
              <NP> la:Ddef r?alit?:NC </NP> 
             <NP> les:Ddef ?tudiants:NC</NP> 
              <VN> sont:VP</VN> 
              <PP> ?:P <NP> notre:Dposs porte:NC</NP>
</PP>  <PONCT> .:PONCT</PONCT> </SENT> 
FIGURE 2 : Sample output for French
brackets, and a recall and precision of resp.
94.3% and 92.9 % for closing brackets.
To give an idea about the coverage of the
parser, sentences are on average 30 words long
and comprise 20.6 opening brackets (and thus as
many closing brackets). Errors difficult to
correct with access to a limited context involve
mainly "missing" brackets (e.g. "comptez vous
* ne pas le traiter" (do you expect not to treat
him) appears as single constituent, while there
should be 2) , while "spurious" brackets can
often be eliminated by adding more rules (e.g.
for multiple prepositions  : "de chez"). Most
errors for closing brackets are due to clause
boundaries(i.e. SUB, COORD and REL).
To obtain these results, we had to write only
48 rules. 
Concerning speed, as argued in (Tapanainen
& J?rvinen, 94), we found that rule?based
systems are not necessarily slow, since the 1
million words are parsed in 3mn 8 seconds.
One can compare this to (Ait?Moktar &
Chanod 97), who, in order to shallow?parse
French resort to 14 networks and parse
150words /sec (Which amounts to approx. 111
minutes for one million words)12. It is difficult to
compare our result to other results, since  most
Shallow?parsers pursue different tasks, and use
different evaluation metrics. However to give an
idea, standard techniques typically produce an
output for one million words in 20 mn and
report a precision and a recall ranging from 70%
to 95% depending on the language, kind of text
and task. Again, we are not saying that our
technique obtains best results, but simply that it
is fast and  easy to use for unrestricted text for
any language. To give a better idea to the reader,
we provide an output of the Shallow?parser for
French on figure 2. 
In order to improve our tool and our rules, a
demo is available online on the author?s
homepage.
3.2 A Shallow?Parser for English
We wanted to evaluate our compiler on more
than one language, to make sure that our results
were easily replicable. So we wrote a new set of
rules for English using the PennTreebank tagset,
both for POS and for constituent labels.
12 They report a recall ranging from 82.6% and 92.6%
depending on the type of texts, and a precision of 98% for
subject recognition, but their results are not directly
comparable to ours, since the task is different.
We sat aside sections 00 and 01 of the WSJ for
evaluation (i.e. approx. 3900 sentences), and
used other sections of the WSJ for tuning our
rules.
Contrary to the French Treebank, the Penn
Treebank contains non?surfastic constructions
such as empty nodes, and constituents that are
not triggered by a lexical items.
Therefore, before evaluating our new shallow?
parser, we automatically removed from the test
sentences all opening brackets that were not
immediately followed by a lexical item, with
their corresponding closing brackets, as well as
all the constituents which contained an empty
element. We also removed all information on
pseudo?attachment. We then evaluated the
output of the shallow?parser to the test
sentences. For bare NPs, we compared our
output to the POS tagged version of the test
sentences (since bare?NPs are marked there). 
For the shallow?parsing task, we obtain a
precision of 90.8% and a recall of 91% for
opening brackets, a precision of 65.7% and
recall of 66.1% for closing brackets.  For the
NP?chunking task, we obtain a precision of
91% and recall of 93.2%, using an ?exact
match? measure (i.e. both the opening and
closing boundaries of an NP must match to be
counted as correct).  
The results, were as satisfactory as for French.
Concerning linguistic choices when writing the
rules, we didn?t really make any, and simply
followed closely those of the Penn Treebank
syntactic annotation guidelines (modulo the
embeddings, the empty categories and pseudo?
attachments mentioned above).
Concerning the number of rules, we used 54 of
them in order to detect all constituents, and 27
rules for NP?chunks identification. . In sections
00 and 01 of the wsj there were 24553 NPs,
realized as 1200 different POS patterns (ex : CD
NN,  DT $ JJ NN, DT NN?). Even though
these 1200 patterns corresponded to a lower
number of regular expressions, a standard
finite?state approach would have to resort to
more than 27 rules. One can also compare this
result to the one reported in (Ramshaw &
Marcus 95) who, obtain up to 93.5% recall and
93.1% precision on the same task, but using
between 500 and 2000 rules.
3.3 Tolerance to POS errors
To test the tolerance to POS tagging errors,
we have extracted the raw text from the English
corpus from section 3.2., and retagged it using
the publicly available tagger TreeTagger
(Schmid, 94). without retraining it. The authors
of the tagger advertise an error?rate between 3
and 4%. We then ran the NP?chunker on the
output of the tagger, and still obtain a precision
of 90.2% and a recall of 92% on the ?exact
match? NP identification task: the fact that our
tool does not rely on regular expressions
describing "full constituent patterns" allows to
ignore some POS errors since mistagged words
which do not appear at constituent boundaries
(i.e. essentially lexical words)  have no
influence on the output. This improves accuracy
and robustness. For example, if "first" has been
mistagged noun instead of adjective in [NP the
first man ] on the moon ..., it won?t prevent
detecting the NP, as long as the determiner has
been tagged correctly. 
Conclusion
We have presented a tool which allows to
generate a shallow?parser for unrestricted text
in any language. This tool is based on the use of
a imited number of rules which aim at
identifying constituent boundaries. We then
presented evaluations on French and on English,
and concluded that our tools obtains results
similar to other shallow?parsing techniques, but
in a much simpler and economical way. 
We are interested in developing new sets of
rules for new languages (e.g. Portuguese and
German) and new style (e.g. French oral texts).
It would also be interesting to test the tool on
inflectional languages.
The shallow?parser for French is also being
used in the SynSem project which aims at
syntactically and semantically annotating
several millions words of French texts
distributed by ELRA13. Future improvements of
the tool will consist in adding a module to
annotate syntactic functions, and complete
valency information for verbs, with the help of a
lexicon (Kinyon, 00).
Finally, from a theoretical point of view, it may
be interesting to see if our rules could be
acquired automatically from raw text (although
this might not be worth it in practice,
considering the small number of rules we use,
and the fact that acquiring the rules in such a
way would most likely introduce errors).
13
 European Language Ressources Association
Acknowledgement We especially thank F.
Toussenel, who has performed most of the
evaluation for French presented in section 3.1.1. 
References
Abeill? A. Cl?ment L. 1999 : A tagged reference corpus
for French. Proc. LINC?EACL?99. Bergen
Abeill? A., Cl?ment L., Kinyon A., Toussenel F. 2001
Building a Treebank for French. In Treebanks (A
Abeill? ed.). Kluwer academic publishers.
Abney S. 1991. Parsing by chunks. In Principle?based
Parsing. (R. Berwick, S. Abney and C. Tenny eds),
Kluwer academic publishers.
A?t?Mokhtar S. & Chanod J.P. 1997. Incremental Finite?
State Parsing. Proc. ANLP?97, Washington,
Bourigault 1992 : Surface Grammatical analysis for the
extraction of terminological noun phrases. Proc.
COLING?92. Vol 3,  pp. 977?981
Brants T., Skut W.,  Uszkoreit H., 1999. Syntactic
Annotation of a German Newspaper Corpus. Proc.
ATALA Treebank Workshop. Paris, France.
Daelemans W., Buchholz S., Veenstra J.. Memory?Based
Shallow Parsing.Proc.CoNLL?EACL?99
Grefenstette G.. 1996. Light Parsing as Finite?State
Filtering. Proc. ECAI ?96 workshop on "Extended
finite state models of  language".  
Joshi A.K. Hopely P. 1997. A parser from antiquity. In
Extended Finite State Models of Language. (A. Kornai
ed.). University Press.
Karlsson F., Voutilainen A., Heikkil J., Antilla A. (eds.)
1995. Constraint Grammar: a language?independent
system for parsing unrestricted text. Mouton de
Gruyer.
Kinyon A. 2000. Hypertags. Proc. COLING?00.
Sarrebrucken.
Magerman D.M., 1994  Natural language parsing as
statistical pattern recognition. PhD Dissertation,
Stanford University.
Marcus M., Santorini B., and Marcinkiewicz M.A. 1993.
Building a large annotated corpus of english: The penn
treebank. Computational Linguistics, 19:313??330.
Ramshaw, L.A. & Marcus, M.P., 1995. Text Chunking
using Transformation?Based Learning, ACL Third
Workshop on Very Large Corpora, pp.82?94, 1995.   
Ratnaparkhi A. 1997. linear observed time statistical
parser based on maximum entropy models. Technical
Report cmp-lg/9706014.
Tapanainen P. and J?rvinen T., 1994, Syntactic Analysis
of a Natural Language Using Linguistic Rules and
Corpus?Based Patterns. Proc. COLING?94. Vol I, pp
629?634. Kyoto.
Schmid H. 1994 Probabilistic Part?Of?Speech Tagging
Using Decision Trees. Proc. NEMLAP?94.
Vergne J. 1999. Etude et mod?lisation de la syntaxe des
langues ? l?aide de l?ordinateur. Analyse syntaxique
automatique non combinatoire. Dossier d?habilitation
? diriger des recherches. Univ. de Caen.
A Classification of Grammar Development Strategies
Alexandra Kinyon Carlos A. Prolo
Computer and Information Science Department
University of Pennsylvania
Suite 400A, 3401 Walnut Street
Philadelphia, PA, USA, 19104-6228
 
kinyon,prolo  @linc.cis.upenn.edu
Abstract
In this paper, we propose a classification of gram-
mar development strategies according to two crite-
ria : hand-written versus automatically acquired
grammars, and grammars based on a low versus
high level of syntactic abstraction. Our classifica-
tion yields four types of grammars. For each type,
we discuss implementation and evaluation issues.
1 Introduction: Four grammar
development strategies
There are several potential strategies to build wide-
coverage grammars, therefore there is a need for
classifying these various strategies. In this paper,
we propose a classification of grammar develop-
ment strategies according to two criteria :
 Hand-crafted versus Automatically ac-
quired grammars
 Grammars based on a low versus high level of
syntactic abstraction.
As summarized in table 1, our classification
yields four types of grammars, which we call re-
spectively type A, B, C and D.
Of these four types, three have already been im-
plemented to develop wide-coverage grammars for
English within the Xtag project, and an implemen-
tation of the fourth type is underway 1. Most of
our examples are based on the development of wide
coverage Tree Adjoining Grammars (TAG), but it is
important to note that the classification is relevant
within other linguistic frameworks as well (HPSG,
GPSG, LFG etc.) and is helpful to discuss portabil-
ity among several syntactic frameworks.
We devote a section for each type of grammar in
our classification. We discuss the advantages and
drawbacks of each approach, and especially focus
1We do not discuss here shallow-parsing approaches, but
only f ull grammar development. Due to space limitations, we
do not introduce the TAG formalism and refer to (Joshi, 1987)
for an introduction.
on how each type performs w.r.t. grammar cov-
erage, linguistic adequacy, maintenance, over- and
under- generation as well as to portability to other
syntactic frameworks. We discuss grammar repli-
cation as a mean to compare these approaches. Fi-
nally, we argue that the fourth type, which is cur-
rently being implemented, exhibits better develop-
ment properties.
2 TYPE A Grammars: hand-crafted
The limitations of Type A grammars (hand-crafted)
are well known : although linguistically moti-
vated, developing and maintaining a totally hand-
crafted grammar is a challenging (perhaps unreal-
istic ?) task. Such a large hand-crafted grammar
for TAGs is described for English in (XTAG Re-
search Group, 2001). Smaller hand-crafted gram-
mars for TAGs have been developed for other lan-
guages (e.g. French (Abeille, 1991)), with similar
problems. Of course, the limitations of hand-crafted
grammars are not specific to the TAG framework
(see e.g. (Clement and Kinyon, 2001) for LFG).
2.1 Coverage issues
The Xtag grammar for English, which is freely
downloadable from the project homepage 2 (along
with tools such as a parser and an extensive doc-
umentation), has been under constant development
for approximately 15 years. It consists of more than
1200 elementary trees (1000 for verbs) and has been
tested on real text and test suites. For instance, (Do-
ran et al, 1994) report that 61% of 1367 grammat-
ical sentences from the TSNLP test-suite (Lehman
and al, 1996) were parsed with an early version of
the grammar. More recently, (Prasad and Sarkar,
2000) evaluated the coverage of the grammar on
?the weather corpus?, which contained rather com-
plex sentences with an average length of 20 words
per sentence, as well as on the ?CSLI LKB test
suite? (Copestake, 1999). In addition, in order to
2http://www.cis.upenn.edu/ xtag/
High level of syntactic ab-
straction
Low level of syntactic ab-
straction
Hand-crafted Type A: Type C:
Traditional hand-crafted
grammars
Hand-crafted level of syn-
tactic abstraction
Automatically generated
grammars
Automatically acquired Type B: Type D:
Traditional treebank ex-
tracted grammars
Automatically acquired
level of syntactic abstrac-
tion
Automatically generated
grammar
Table 1: A classification of grammars
evaluate the range of syntactic phenomena covered
by the Xtag grammar, an internal test-suite which
contains all the example sentences (grammatical
and ungrammatical) from the continually updated
documentation of the grammar is distributed with
the grammar. (Prasad and Sarkar, 2000) argue that
constant evaluation is useful not only to get an idea
of the coverage of a grammar, but also as a way to
continuously improve and enrich the grammar 3.
Parsing failures were due, among other things,
to POS errors, missing lexical items, missing trees
(i.e. grammar rules), feature clashes, bad lexicon
grammar interaction (e.g. lexical item anchoring the
wrong tree(s)) etc.
2.2 Maintenance issues
As a hand-crafted grammar grows , consistency is-
sues arise and one then needs to develop mainte-
nance tools. (Sarkar and Wintner, 1999) describe
such a maintenance tool for the Xtag grammar for
English, which aims at identifying problems such
as typographical errors (e.g. a typo in a feature
can prevent unification at parse time and hurt per-
formance), undocumented features (features from
older versions of the grammar, that no longer ex-
ist), type-errors (e.g. English verb nodes should not
be assigned a gender feature), etc. But even with
such maintenance tools, coverage, consistency and
maintenance issues still remain.
3For instance, at first, Xtag parsed only 20% of the sen-
tences in the weather corpus because this corpus contained fre-
quent free relative constructions not handled by the grammar.
After augmenting the grammar, 89.6% of the sentences did get
a parse.
2.3 Are hand-crafted grammars useful ?
Some degree of automation in grammar develop-
ment is unavoidable for any real world application
: small and even medium-size hand-crafted gram-
mar are not useful for practical applications because
of their limited coverage, but larger grammars give
way to maintenance issues. However, despite the
problems of coverage and maintenance encountered
with hand-crafted grammars, such experiments are
invaluable from a linguistic point of view. In par-
ticular, the Xtag grammar for English comes with
a very detailed documentation, which has proved
extremely helpful to devise increasingly automated
approaches to grammar development (see sections
below) 4.
3 TYPE B Grammars: Automatically
extracted
To remedy some of these problems, Type B gram-
mars (i.e. automatically acquired, mostly from an-
notated corpora) have been developed. For instance
(Chiang, 2000), (Xia, 2001) (Chen, 2001) all auto-
matically acquire large TAGs for English from the
Penn Treebank (Marcus et al, 1993). However, de-
spite an improvement in coverage, new problems
arise with this type of grammars : availability of an-
notated data which is large enough to avoid sparse
data problems, possible lack of linguistic adequacy,
extraction of potentially unreasonably large gram-
mars (slows down parsing and increases ambiguity),
4Perhaps fully hand-crafted grammars can be used in prac-
tice on limited domains, e.g. the weather corpus. However, a
degree of automation is useful even in those cases, if only to
insure consistency and avoid some maintenance problems.
lack of domain and framework independence (e.g. a
grammar extracted from the Penn Treebank will re-
flect the linguistic choices and the annotation errors
made when annotating the treebank).
We give two examples of problems encountered
when automatically extracting TAG grammars: The
extraction of a wrong domain of locality; And The
problem of sparse-data regarding the integration of
the lexicon with the grammar.
3.1 Wrong domain of locality
Long distance dependencies are difficult to detect
accurately in annotated corpora, even when such
dependencies can be adequately modeled by the
grammar framework used for extraction (which is
the case for TAGs, but not for instance for Context
Free Grammars). For example, (Xia, 2001) extracts
two elementary trees from a sentence such as Which
dog does Hillary Clinton think that Chelsea prefers.
These trees are shown on figure 1. Unfortunately,
because of the potentially unbounded dependency,
the two trees exhibit an incorrect domain of local-
ity: the Wh-extracted element ends up in the wrong
elementary tree, as an argument of ?think?, instead
of as an argument of ?prefer? 5





 





	
(Which dog)










(Hillary) 






(think)












(Chelsea) 








(prefers)
Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 17?24,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The Metagrammar Goes Multilingual:
A Cross-Linguistic Look at the V2-Phenomenon
Alexandra Kinyon
Department of CIS
University of Pennsylvania
kinyon@linc.cis.upenn.edu
Tatjana Scheffler
Department of Linguistics
University of Pennsylvania
tatjana@ling.upenn.edu
Aravind K. Joshi
Department of CIS
University of Pennsylvania
joshi@linc.cis.upenn.edu
Owen Rambow
CCLS
Columbia University
rambow@cs.columbia.edu
SinWon Yoon
UFRL
Universite? Paris 7
swyoon@linguist.jussieu.fr
Abstract
We present an initial investigation into
the use of a metagrammar for explic-
itly sharing abstract grammatical specifi-
cations among languages. We define a
single class hierarchy for a metagrammar
which allows us to automatically gener-
ate grammars for different languages from
a single compact metagrammar hierarchy.
We use as our linguistic example the verb-
second phenomenon, which shows con-
siderable variation while retaining a ba-
sic property, namely the fact that the verb
can appear in one of two positions in the
clause.
1 An Overview of Metagrammars
A metagrammar (MG) factors common properties
of TAG elementary trees to avoid redundancy, ease
grammar development, and expand coverage with
minimal effort: typically, from a compact man-
ually encoded MG of a few dozen classes, one
or more TAGs with several hundreds of elemen-
tary trees are automatically generated. This is
appealing from a grammar engineering point of
view, and also from a linguistic point of view:
cross-linguistic generalizations are expressed di-
rectly in the MG. In this paper, we extend some
earlier work on multilingual MGs (Candito, 1998;
Kinyon and Rambow, 2003) by proposing cross-
linguistic and framework-neutral syntactic invari-
ants, which we apply to TAG. We focus on the
verb-second phenomenon as a prototypical exam-
ple of cross-language variation.
The notion of Metagrammar Metagrammars
were first introduced by Candito (1996) to manu-
ally encode syntactic knowledge in a compact and
abstract class hierarchy which supports multiple
inheritance, and from which a TAG is automati-
cally generated offline. Candito?s class hierarchy
imposes a general organization of syntax into three
dimensions:
  Dimension 1: to encode initial subcategoriza-
tion frames i.e. TAG tree families
  Dimension 2: to encode valency alternations
/ redistribution of syntactic functions
  Dimension 3: to encode the surface realiza-
tion of arguments.
Each class in the MG hierarchy is associated
with a partial tree description The tool computes
a set of well-formed classes by combining exactly
one terminal class from dimension 1, one termi-
nal class from dimension 2, and  terminal classes
from dimensions 3 ( being the number of argu-
ments subcategorized by the lexical head anchor-
ing the elementary tree(s) generated). The con-
junction of the tree descriptions associated with
each well-formed class in the set yields a minimal
satisfying description, which results in the gener-
ation of one or more elementary trees. Candito?s
tool was used to develop a large TAG for French
as well as a medium-size TAG for Italian Candito
(1999), so multilinguality was addressed from the
start, but each language had its dedicated hierar-
chy, with no sharing of classes despite the obvious
similarities between Italian and French. A related
approach was proposed by (Xia, 2001); the work
of Evans, Gazdar, and Weir (2000) also has some
common elements with MG.
Framework- and language-neutral syntactic
invariants Using a MG, and following Can-
dito, we can postulate cross-linguistic and cross-
framework syntactic invariants such as:
17
  The notion of subcategorization
  The existence of a finite number of syntactic
functions (subject, object etc.)
  The existence of a finite number of syntactic
categories (NP, PP, etc.)
  The existence of valency alternations (Can-
dito?s dimension 2)
  The existence, orthogonal to valency alterna-
tions, of syntactic phenomena which do not
alter valency, such as wh-movement (Can-
dito?s dimension 3).
These invariants ? unlike other framework-
specific syntactic assumptions such as the exis-
tence of ?movement? or ?wh-traces? ? are ac-
cepted by most if not all existing frameworks, even
though the machinery of a given framework may
not necessarily account explicitly for each invari-
ant. For instance, TAG does not have an explicit
notion of syntactic function: although by conven-
tion node indices tend to reflect a function, it is not
enforced by the framework?s machinery.1
Hypertags Based on such framework- and
language-neutral syntactic properties, Kinyon
(2000) defined the notion of Hypertag (HT), a
combination of Supertags (ST) Srinivas (1997)
and of the MG. A ST is a TAG elementary tree,
which provides richer information than standard
POS tagging, but in a framework-specific man-
ner (TAG), and also in a grammar-specific manner
since a ST tagset can?t be ported from one TAG
to another TAG. A HT is an abstraction of STs,
where the main syntactic properties of any given
ST is encoded in a general readable Feature Struc-
ture (FS), by recording which MG classes a ST in-
herited from when it was generated. Figure 1 illus-
trates the  ST, HT pair for Par qui sera accom-
pagne?e Marie ?By whom will Mary be accompa-
nied?. We see that a HT feature structure directly
reflects the MG organization, by having 3 features
?Dimension 1?, ?Dimension 2? and ?Dimension
3?, where each feature takes its value from the MG
terminal classes used to generate a given ST.
The XMG Tool Candito?s tool brought a sig-
nificant linguistic insight, therefore we essentially
retain the above-mentioned syntactic invariants.
However, more recent MG implementations have
been developed since, each adding its significant
contribution to the underlying metagrammatical
hypothesis.
In this paper, we use the eXtensible MetaGram-
mar (XMG) tool which was developed by Crabbe?
1But several attempts have been made to explicitly add
functions to TAG, e.g. by Kameyama (1986) to retain the
benefits of both TAG and LFG, or by Prolo (2006) to account
for the coordination of constituents of different categories,
yet sharing the same function.
S
PP
P
par
NWh
(qui)
S
Aux
sera
V
accompagne?e
N 
(Marie)


DIMENSION1 STRICTTRANSITIVE
DIMENSION2 PERSONALFULLPASSIVE
DIMENSION3 SUBJECT INVERTEDSUBJECTCOMPLEMENT WHQUESTIONEDBYCOMPLEMENT	



Figure 1: A  SuperTag, HyperTag pair for ac-
compagne?e (?accompanied?) obtained with Can-
dito?s MetaGrammar compiler
(2005). In XMG, an MG consists of a set of
classes similar to those in object-oriented pro-
gramming, which are structured into a multiple
inheritance hierarchy. Each class specifies a par-
tial tree description (expressed by dominance and
precedence constraints). The nodes of these tree
fragment descriptions may be annotated with fea-
tures. Classes may instantiate each other, and they
may be parametrized (e.g., to hand down features
like the grammatical function of a substitution
node). The compiler unifies the instantiations of
tree descriptions that are called. This unification
is additionally guided by node colors, constraints
that specify that a node must not be unified with
any other node (red), must be unified (white), or
may be unified, but only with a white node (black).
XMG allows us to implement a hierarchy similar
to that of Candito, but it also allows us to modify
and extend it, as no structural assumptions about
the class hierarchy are hard-coded.
2 The V2 Phenomenon
The Verb-Second (V2) phenomenon is a well-
known set of data that demonstrates small-scale
cross-linguistic variation. The examples in (1)
show German, a language with a V2-constraint:
(1a) is completely grammatical, while (1b) is not.
This is considered to be due to the fact that the
finite verb is required to be located in ?second po-
sition? (V2) in German. Other languages with a
V2 constraint include Dutch, Yiddish, Frisian, Ice-
landic, Mainland Scandinavian, and Kashmiri.
(1) a. Auf
on
dem
the
Weg
path
sieht
sees
der
the
Junge
boy
eine
a
Ente.
duck
?On the path, the boy sees a duck.?
18
b. * Auf
on
dem
the
Weg
path
der
the
Junge
boy
sieht
sees
eine
a
Ente.
duck
Int.: ?On the path, the boy sees a duck.?
Interestingly, these languages differ with re-
spect to how exactly the constraint is realized.
Rambow and Santorini (1995) present data from
the mentioned languages and provide a set of pa-
rameters that account for the exhibited variation.
In the following, for the sake of brevity, we will
confine the discussion to two languages: German,
and Yiddish. The German data is as follows (we
do not repeat (1a) from above):
(2) a. Der
the
Junge
boy
sieht
sees
eine
a
Ente
duck
auf
on
dem
the
Weg.
path
?On the path, the boy sees a duck.?
b. . . . ,
. . . ,
dass
that
der
the
Junge
boy
auf
on
dem
the
Weg
path
eine
a
Ente
duck
sieht.
sees
?. . . , that the boy sees a duck on the path.?
c. Eine
a
Ente
duck
sieht
sees
der
the
Junge.
boy
?The boy sees a duck.?
The Yiddish data:
(3) a. Dos
the
yingl
boy
zet
sees
oyfn
on-the
veg
path
a
a
katshke.
duck
?On the path, the boy sees a duck.?
b. Oyfn
on-the
veg
path
zet
sees
dos
the
yingl
boy
a
a
katshke.
duck.
?On the path, the boy sees a duck.?
c. . . . ,
. . . ,
az
that
dos
the
yingl
boy
zet
sees
a
a
katshke
duck
?. . . , that the boy sees a duck.?
While main clauses exhibit V2 in German, embed-
ded clauses with complementizers are verb-final
(2b). In contrast, Yiddish embedded clauses must
also be V2 (3c).
3 Handling V2 in the Metagrammar
It is striking that the basic V2 phenomenon is the
same in all of these languages: the verb can ap-
pear in either its underlying position, or in sec-
ond position (or, in some cases, third). We claim
that what governs the appearance of the verb
in these different positions (and thus the cross-
linguistic differences) is that the heads?the verbal
head and functional heads such as auxiliaries and
complementizers?interact in specific ways. For
example, in German a complementizer is not com-
patible with a verbal V2 head, while in Yiddish it
is. We express the interaction among heads by as-
signing the heads different values for a set of fea-
tures. Which heads can carry which feature values
is a language-specific parameter. Our implementa-
tion is based on the previous pen-and-pencil anal-
ysis of Rambow and Santorini (1995), which we
have modified and extended.
The work we present in this paper thus has
a threefold interest: (1) we show how to han-
dle an important syntactic phenomenon cross-
linguistically in a MG framework; (2) we partially
validate, correct, and extend a previously proposed
linguistically-motivated analysis; and (3) we pro-
vide an initial fragment of a MG implementa-
tion from which we generate TAGs for languages
which are relatively less-studied and for which no
TAG currently exists (Yiddish).
4 Elements of Our Implementation
In this paper, we only address verbal elementary
trees. We define a verbal realization to be a com-
bination of three classes (or ?dimensions? in Can-
dito?s terminology): a subcategorization frame,
a redistribution of arguments/valency alternation
(in our case, voice, which we do not further dis-
cuss), and a topology, which encodes the posi-
tion and characteristics of the verbal head. Thus,
we reinterpret Candito?s ?Dimension 3? to con-
centrate on the position of the verbal heads, with
the different argument realizations (topicalized,
base position) depending on the available heads,
rather than defined as first-class citizens. The sub-
cat and argument redistributions result in a set of
structures for arguments which are left- or right-
branching (depending on language and grammat-
ical function). Figure 2 shows some argument
structures for German. The topology reflects the
basic clause structure, that is, the distribution of ar-
guments and adjuncts, and the position of the verb
(initial, V2, final, etc.). Our notion of sentence
topology is thus similar to the notion formalized
by Gerdes (2002). Specifically, we see positions
of arguments and adjuncts as defined by the posi-
tions of their verbal heads. However, while Gerdes
(2002) assumes as basic underlying notions the
fields created by the heads (the traditional Vorfeld
for the topicalized element and the Mittelfeld be-
tween the verb in second position and the verb in
clause-final position), we only use properties of
the heads. The fields are epiphenomenal for us.As
mentioned above, we use the following set of fea-
tures to define our MG topology:
  I (finite tense and subject-verb agreement):
creates a specifier position for agreement
which must be filled in a derivation, but al-
lows recursion (i.e., adjunction at IP).
  Top (topic): a feature which creates a spec-
ifier position for the topic (semantically rep-
resented in a lambda abstraction) which must
be filled in a derivation, and which does not
allow recursion.
  M (mood): a feature with semantic content
(to be defined), but no specifier.
  C (complementizer): a lexical feature intro-
duced only by complementizers.
We can now define our topology in more detail.
It consists of two main parts:
19
German:
What Features Introduced Directionality
1 Verb (clause-final) +I head-final
2 Verb (V2, subject-inital) +M, +Top, +I head-initial
3 Verb (V2, non-subject-initial) +M, +Top head-initial
4 Complementizer +C, +M head-initial
Yiddish:
What Features Introduced Directionality
1 Verb +I head-initial
2 Verb (V2, subject-inital) +M, +Top, +I head-initial
3 Verb (V2, non-subject-initial) +M, +Top head-initial
4 Complementizer +C head-initial
Figure 4: Head inventories for German and Yiddish.
1:

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
black












 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











v
2:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











3:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white











4:

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M +
C +
black











comp

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white











Figure 5: Head structures for German corresponding to the table in Figure 4 (above)
1:

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











2:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white











3:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white











4:

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C +
black











comp

 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
white











Figure 6: Head structures for Yiddish corresponding to the table in Figure 4 (below)
20
 
 
 

CAT V
I +
TOP +
black






NP 

 
 
 

CAT V
I +
TOP +
white







 
 
 

CAT V
I +
TOP 
black






NP 

 
 
 

CAT V
I +
TOP 
white







 
 
 

CAT V
I 
TOP +
black






NP 

 
 
 

CAT V
I 
TOP +
white







 
 
 

CAT V
I 
TOP 
black






NP 

 
 
 

CAT V
I 
TOP 
white






Figure 2: The argument structures

CAT V
white 

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
black











	
Figure 3: The projection structure; feature values
can be filled in at the top feature structure to con-
trol the derivation.
  The projection includes the origin of the
verb in the phrase structure (with an empty
head since we assume it is no longer there)
and its maximal projection. It is shown in
Figure 3. The maximal projection expresses
the expected feature content. For example,
if we want to model non-finite clauses, the
maximal projection will have [I], while root
V2 clauses will have [+Top], and embedded
finite clauses with complementizers will have
[+I,+C].
  Structures for heads, which can be head-
initial or head-final. They introduce catego-
rial features. Languages differ in what sort of
heads they have. Which heads are available
for a given language is captured in a head in-
ventory, i.e., a list of possible heads for that
language (which use the head structure just
mentioned). Two such lists are shown in Fig-
ure 4, for German and Yiddish. The corre-
sponding head structures are shown in Fig-
ures 5 and 6.
A topology is a combination of the projection
and any combination of heads allowed by the
language-specific head inventory. This is hard
to express in XMG, so instead we list the spe-
cific combinations allowed. One might ask how
we derive trees for language without the V2 phe-
nomenon. Languages without V2 will usually
have a smaller set of possible heads. We are work-
ing on a metagrammar for Korean in parallel with
our work on the V2 languages. Korean is very
much like German without the V2 phenomenon:
the verbal head can only be in clause-final position
(i.e., head 1 from Figure 5. However, passiviza-
tion and scrambling can be treated the same way
in Korean and German, since these phenomena are
independent of V2.
5 Sample Derivation
Given a feature ordering (C  M  Top  I) and
language-specific head inventories as in Figure 4,
we compile out MGs for German (Figure 5) and
Yiddish (Figure 6).2 The projection and the ar-
gument realizations do not differ between the two
languages: thus, these parts of the MG can be
reused. The features, which were introduced for
descriptive reasons, now guide the TAG compila-
tion: only certain heads can be combined. Further-
more, subjects and non-subjects are distinguished,
as well as topicalized and non-topicalized NPs
(producing 4 kinds of arguments so far). The com-
piler picks out any number of compatible elements
from the Metagrammar and performs the unifica-
tions of nodes that are permitted (or required) by
2All terminal nodes are ?red?; spine nodes have been an-
notated with their color.
21
the node descriptions and the colors. By way of
example, the derivations of elementary trees which
can be used in a TAG analysis of German (2c) and
Yiddish (3c) are shown in Figures 7 and 8, respec-
tively.
6 Conclusion and Future work
This paper showed how cross-linguistic general-
izations (in this case, V2) can be incorporated into
a multilingual MG. This allows not only the reuse
of MG parts for new (often, not well-studied) lan-
guages, but it also enables us to study small-scale
parametric variation between languages in a con-
trolled and formal way. We are currently modify-
ing and extending our implementation in several
ways.
The Notion of Projection In our current ap-
proach, the verb is never at the basis of the pro-
jection, it has always been removed into a new
location. This may seem unmotivated in certain
cases, such as German verb-final sentences. We
are looking into using the XMG unification to ac-
tually place the verb at the bottom of the projection
in these cases.
Generating Top and Bottom Features The
generated TAG grammar currently does not have
top and bottom feature sets, as one would expect
in a feature-based TAG. These are important for
us so we can force adjunction in adjunct-initial V2
sentences (where the element in clause-initial po-
sition is not an argument of the verb). We intend
to follow the approach laid out in Crabbe? (2005) in
order to generate top and bottom feature structures
on the nodes of the TAG grammar.
Generating test-suites to document our
grammars Since XMG offers more complex
object-oriented functionalities, including in-
stances, and therefore recursion, it is now
straightforward to directly generate parallel mul-
tilingual sentences directly from XMG, without
any intermediate grammar generation step. The
only obstacle remains the explicit encoding of
Hypertags into XMG.
Acknowledgments
We thank Yannick Parmentier, Joseph Leroux,
Bertrand Gaiffe, Benoit Crabbe?, the LORIA XMG
team, and Julia Hockenmaier for their invaluable
help; Eric de la Clergerie, Carlos Prolo and the
Xtag group for their helpful feedback, comments
and suggestions on different aspects of this work;
and Marie-He?le`ne Candito for her insights. This
work was supported by NSF Grant 0414409 to the
University of Pennsylvania.
References
Candito, M. H. 1998. Building parallel LTAG for French and
Italian. In Proc. ACL-98. Montreal.
Candito, M.H. 1996. A principle-based hierarchical repre-
sentation of LTAGs. In Proc. COLING-96. Copenhagen.
Candito, M.H. 1999. Repre?sentation modulaire et
parame?trable de grammaires e?lectroniques lexicalise?es.
Doctoral Dissertation, Univ. Paris 7.
Cle?ment, L., and A. Kinyon. 2003. Generating parallel mul-
tilingual LFG-TAG grammars using a MetaGrammar. In
Proc. ACL-03. Sapporo.
Clergerie, E. De La. 2005. From metagrammars to factorized
TAG/TIG parsers. In IWPT-05. Trento.
Crabbe?, B. 2005. Repre?sentation informatique de grammaires
fortement lexicalise?es. Doctoral Dissertation, Univ. Nancy
2.
Evans, R., G. Gazdar, and D. Weir. 2000. Lexical rules
are just lexical rules. In Tree Adjoining Grammars, ed.
A. Abeille? and O. Rambow. CSLI.
Gerdes, K. 2002. DTAG. attempt to generate a useful TAG for
German using a metagrammar. In Proc. TAG+6. Venice.
Kameyama, M. 1986. Characterising LFG in terms of TAG.
In Unpublished report. Univ. of Pennsylvania.
Kinyon, A. 2000. Hypertags. In Proc. COLING-00. Sar-
rebrucken.
Kinyon, A., and O. Rambow. 2003. Generating cross-
language and cross-framework annotated test-suites using
a MetaGrammar. In Proc. LINC-EACL-03. Budapest.
Prolo, C. 2006. Handling unlike coordinated phrases in TAG
by mixing Syntactic Category and Grammatical Function.
In Proc. TAG+8. Sidney.
Rambow, Owen, and Beatrice Santorini. 1995. Incremental
phrase structure generation and a universal theory of V2.
In Proceedings of NELS 25, ed. J.N. Beckman, 373?387.
Amherst, MA: GSLA.
Srinivas, B. 1997. Complexity of lexical descriptions and its
relevance for partial parsing. Doctoral Dissertation, Univ.
of Pennsylvania.
Xia, F. 2001. Automatic grammar generation from two per-
spectives. Doctoral Dissertation, Univ. of Pennsylvania.
XTAG Research Group. 2001. A lexicalized tree adjoin-
ing grammar for English. Technical Report IRCS-01-03,
IRCS, University of Pennsylvania.
22
 
 
 

CAT V
I 
TOP +
black






NP 

 
 
 

CAT V
I 
TOP +
white







 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
white












 
 
 

CAT V
I +
TOP 
black






NP 

 
 
 

CAT V
I +
TOP 
white







 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M 
C 
black












 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white












CAT V
white 

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
black











Object-Topicalized + Head 3 + Subject-Non-Topicalized + Head 1 + Projection
(White and Black nodes next to each other are unified.)
Figure 7: Derivation of the German elementary tree NP

 V NP  (2d).
23
 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M +
C +
black











comp

 
 
 
 
 
 
 
 

CAT V
I +
TOP 
M +
C 
white












 
 
 

CAT V
I +
TOP +
black






NP 

 
 
 

CAT V
I +
TOP +
white







 
 
 
 
 
 
 
 

CAT V
I +
TOP +
M +
C 
black











v

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
white












 
 
 

CAT V
I 
TOP 
black






NP 

 
 
 

CAT V
I 
TOP 
white







CAT V
white 

 
 
 
 
 
 
 
 

CAT V
I 
TOP 
M 
C 
black











Head 4 (Comp) + Subject-Topicalized + Head 2 + Object-Non-Topicalized + Projection
Figure 8: Derivation of the Yiddish elementary tree Comp NP  V NP

 (3c).
24
Hypertags 
Alexandra KINYON 
Talana / Lattice, Univ. Paris 7 
UFRL case 7003 
2 pl Jussieu 75005 Paris, France 
Alex andra.Kinyon @ linguist.j ussieu.fr 
Abstract 
Srinivas (97) enriches traditional 
morpho-syntactic POS tagging with 
syntactic information by introducing 
Supertags. Unfortunately, words are 
assigned on average a much higher number 
of Supertags than traditional POS. In this 
paper, we develop the notion of Hypertag, 
first introduced in Kinyon (00a) and in 
Kinyon (00b), which allows to factor the 
information contained in ~everal Supertags 
into a single structure and to encode 
flmctional information in a systematic 
lnanner. We show why other possible 
solutions based on mathematical properties 
of trees are unsatisfactory and also discuss 
the practical usefulness of this approach. 
Introduction 
As a first step prior to parsing, traditional Part 
of Speech (POS) tagging assigns limited 
morpho-syntactic nformation to lexical items. 
These labels can be more or less fine-grained 
depending on the tagset , but syntactic 
information is often absent or limited. Also, most 
lexical items are assigned several POS. Although 
lexical ambiguities are dealt with by POS taggers, 
either in a rule-based or in probabilistic manner, it 
is useful to delay this decision at a further parsing 
step (e.g. Giguet (98) shows that knowing 
constituent boundaries is crucial for solving 
lexical ambiguity correctly). In order to do so, it 
would help to be able to encode several POS into 
one compact representation. 
In order to assign richer syntactic information 
to lexical items Joshi & Srinivas (94) and 
Srinivas (97) introduce the notion of Supertags, 
developed within the fiamework of Tree 
Adjoining Grammars (TAG). The idea behind 
Supertags is to assign to each word in a sentence, 
instead of a traditional POS, an "elementary 
tree", which constitutes a primitive syntactic 
structure within the TAG frmnework. A 
supertagged text can then be inputed to a parser 
or shallow parser, thus alleviating the task of the 
parser. Several problems remain though: 
? Even when no lexical ambiguity occurs, each 
word can anchor several trees (several hundreds 
for some verbs) I. On average for English a word 
is associated with 1.5 POS and with 9 supertags 
(Joshi (99)). One common solution to the 
problem is to only retain the "best" supertag for 
each word, or eventually the 3 best supertags for 
each word, but then early decision has an adverse 
effect on the quality of parsing if the wrong 
supertag(s) have been kept : one typically obtains 
between 75% and 92% accuracy when 
supertagging, depending on the type of text being 
supertagged and on the technique used) (cf 
Srinivas (97), Chen & al (99), Srinivas & Joshi 
(99)). This means that it may be the case that 
every word in 4 will be assigned the wrong 
supertag, whereas typical POS taggers usually 
achieve an accuracy above 95%. 
? Supertagged texts rely heavily on the TAG 
framework and therefore may be difficult to 
exploit without being familiar with this 
fornaal ism. 
? Supertagged texts are difficult to read and 
thus difficult to annotate manually. 
? Some structural information contained in 
Supertags is redundant 
? Some information is missing, especially with 
respect to syntactic functions 2.
So our idea is to investigate how supertags can 
be underspecified so that instead of associating a
set of supertags to each word, one could associate 
one single structure, which we call hypertag, and 
which contains the same information as a set of 
supertags as well as functional information 
Our practical goal is fourfolds :
a) delaying decision for parsing 
b) obtaining a compact and readable 
representation, which can be manually annotated 
1 See Barrier & al. (00) for precise data for French, using 
the FFAG wide-coverage grammar developped at 
TALANA, University of I'aris 7. 
2 The usefulness offunctional information ill POS tagging 
has also been discussed within the reductionist paradigm 
(cf Voutilainen &Tapanainen (93)). 
446 
as a step towards building a treebank for French 
(cf Abeill6 & al. (00a), Cl6ment & Kinyon (00)). 
c) extracting linguistic information on a large 
scale such as lcxical preferences for verb 
subcategorization frames. (cf Kinyon (99a)) 
(1) Building an efficient, but nonetheless 
psycholinguistically motivated, processing model 
for TAGs (cf Kinyon (99b)) 
Ttms, in addition of being well-defined 
computational objects (Point a), hypertags hould 
I)e "readable" (point b) and also motivated from a 
linguistic point of view (Points c & d). 
In the first part of this paper, we briefly 
introduce the LTAG frmnework and give 
exmnples of supertags. In a second part, we 
investigate several potential ways to underspecify 
supertags, and show why these solutions are 
unsatisfactory. In a third part, we explain the 
solution we have adopted, building up on the 
notion of MetaGrammar introduced by Candito 
(96) and Candito (99). Finally, we discuss how 
this approach can be used in practice, and why it 
is interesting for frameworks other than LTAGs. 
1 Br ief  Overv iew of LTAGs  
A LTAG consists of a t'inite set of 
elementary trees of finite depth. Each 
elementary tree nmst "anchor" one or more 
lcxical item(s). The principal anchor is called 
"head", other anchors are called "co-heads". All 
leaves in elementary trees are either "anchor", 
"foot node" (noted *) or "substitution node" 
(noted $).. These trees are of 2 types ? auxiliary 
or initial 3. A tree has at most 1 foot-node. A tree 
with a foot node is an auxiliary tree. Trees that 
are not auxiliary are initial. Elementary trees 
combine with 2 operations : substitution and 
adjunction, but we won't develop this point since 
it is orthogonal to our concern and refer to Joshi 
(87) for more details. Morphosyntactic features 
are encoded in atomic feature structures 
associated to nodes in elementary trees, in order 
to handle phenomena such as agreement. 
Moreover, linguistic constraints on the well- 
formedness of elementary trees have been 
formulated : 
? Predicate Argulnent Cooccurence Principle : 
there must be a leaf node for each realized 
argument of the head of an elementary tree. 
? Semantic consistency : No elementary tree is 
semantically void 
? Semantic minimality : an elementary tree 
corresponds at most to one semantic unit 
Figure 1 shows a non exhaustive set of 
Supertags (i.e. elementary trees) which can be 
assigned to "beats ''4 , which is a verb in trees ctl 
(canonical tree), ~2 (object extraction), 131 
(ob.iect relative) and \[32 (subject relative) and a 
noun in tree oG. So an LTAG can be seen as a 
large dictionary, were in addition of traditional 
POS, lexical entries are associated with several 
structures encoding their nlorphological as well 
as some of their syntactic properties, these 
structures being very similar to small constituent 
trees. 
e?l c?2 
S S 
~05 v NI,I. s' s' 
beats expl V NI,I, Cl~mp NO,I- V 
I I I I 
(Vb : "J beats M.") it is that beats 
(rb: "It is Mary that J. beats") 
N N 
NI* S' NO* S' 
N 
Comp NO$ V Camp V NI$ I 
I \[ I I 1,eats 
that beats who beats 
(Vb : "The man that (Vb : "The man rho (Noaa :"3 beats") 
M. beats .,.") beats 31 ...") 
HGURE 1 : some supertags fi)r "beats"  
2 Underspeci fy ing Supertags 
The idea of underspecifying constituent rees 
(and thus elementary trees) is not new. Several 
solutions have been proposed in the past. We will 
now investigate how these solutions could 
potentially be used to encode a set of supertags in 
a compact manner. 
2.1 Parse forest 
Since elementary trees are constituent 
structures, one could represent a set of elementary 
trees with a graph instead of a tree (cf. Tomita 
(91)). This approach is not particularly interesting 
though. For example, if one considers the trees 
czl and 131 fi'om figure 1, it is obvious that they 
hardly have any structural information in 
common, not even the category of their root. 
Therefore, representing these 2 structures in a 
graph would not help. Moreover, packed 
3 Traditionally initial trees arc called a, and auxiliary lines 4 For sake of readability, morphological features arc not 
S\]IOWI1. 
447 
structures are notoriously difficult to manipulate 
and yield unreadable output. 
2.2 Logical formulae 
With this approach, developped for instance in 
Kalhneyer (99), a tree can be represented by a 
logical formula, where each pair of nodes is either 
in relation of dominance, or in relation of 
precedance. This allows to resort to 1 ~' order 
logic to represent a set of trees by 
underspecifying dominance and/or precedence 
relations . Unfortunately, this yields an output 
which is difficult to read. Also, the approach 
relies only on mathematical properties of trees 
(i.e. no linguistic motivations) 
2.3 Linear types of trees 
This approach, introduced in Srinivas (97), 
used in other work (e.g. Halber (99)) is more 
specific to TAGs. The idea is to relax constraints 
on the order of nodes in a tree as well as on 
internal nodes. A linear type consists in a 7-tuple 
<A,B,C,D,E,F,G> where A is the root of the tree, 
B is the category of the anchor, C is the lexical 
anchor, D is a set of nodes which can receive an 
adjunction, E is a set of co-anchors, F a set of 
nodes marked for substitution, and G a potential 
foot node (or nil in case the tree is initial). In 
addition, elements of E and F are marked + if 
they are to the left of the anchor, - if they are to 
the right. 
czl or2 
S S 
NO P NO,,L V PP NI,,L 
I / \  I /",, 
donne a N2-1- donne il N2,,\[- 
FIGURE 2 : 
two trees with the same linear type 
For example, the tree NOdonneNl'~N2 for 
"Jean donne une pomme gl Marie" (J. gives an 
apple to M.) and the tree N0donne~lN2Nl for 
"Jean donne & Marie une pomme" (J. gives M. an 
apple) which are shown on Figure 2, yield the 
unique linear type (a) 
(a) <S,V,donnc, { S,V,PP}, { h+ }, { N0-,NI +,N2+ }, nil> 
(b) <S,V,gives, {S,V,PP}, { to+ }, { N0-,N1 +,N2+} ,nil> 
This approach is robust, but not really 
linguistic : it will allow to refer to trees that are 
not initially in the grammar. For instance, the 
linear type (b) will correctly allow the sentence 
"John gives an apple to Mary", but also 
incorrectly allow "*John gives to Mary an apple". 
Moreover, linear types are not easily readable s. 
Finally, trees that have more structural 
differences than just the ordering of branches will 
yield different linear types. So, the tree 
N0giveNltoN2 (J. gives an apple to M.) yields 
the linear type (b), whereas the tree N0giveN2Nl 
(J. gives M. an apple) yields a different linear 
type (c), and thus both linear types should label 
"gives". Therefore, it is impossible to label 
"gives" with one unique linear type. 
(c) <S,V,gives, { S,V}, { }, { N0-,N 1 +,N2+} ,nil> 
2.4. Partition approach 
This approach, which we have investigated, 
consists in building equivalence classes to 
partition the grammar, each lexical item then 
anchors one class instead of a set of trees. But 
building such a partition is prohibitively costly : a 
wide coverage grammar for French contains 
approx. 5000 elementary trees (cf Abeilld & al. 
(99), (00b)), which means that we have 25~'~ 
possible subsets. Also, it does not work from a 
linguistic point of view : 
(a) Quand Jean a brisd la glace ? 
(When did J. break the ice ?) 
(b) Jean a brisd la glace (J. broke the ice) 
(c) Quelle chaise Jean a brisd ce matin ? 
(Which chair did J. break this morning ?) 
In (a) brisd potentially anchors N0briseNI 
(canonical transitive), WhN0brise (object 
extraction) and NOBriseGlace (tree for idiom). 
But in (b), we would like brim not to anchor 
WhN0brise since there is no Wh element in the 
sentence, therefore these three trees should not 
belong to the same equivallence class : We can 
have class A={N0briseN1,NOBriseGlace} and 
ClassB={WhN0brise}. But then, in (c), brisd 
potentially anchors WhN0brise and N0briseNI 
but not NOBriseGlace since glace does not appear 
in the sentence. So NOVN1 and NOBriseGlace 
should not be in the same equivalence class. This 
hints that the only realistic partition of the 
grammar would be the one were each class 
contains only one tree, which is pretty useless. 
4. Exploiting a MetaGrammar 
Candito (96), (99) has developed a tool to 
generate semi-automatically elementary trees She 
use an additional ayer of linguistic description, 
called the metagrammar (MG), which imposes a 
general organization for syntactic information in 
a 3 dimensional hierarchy : 
5 This type of format was considered as a step towards 
creating a trccbank for French (of Abcilld & al 00a), but 
unfommatcly proved impossible to manually annotate. 
448 
? Dimension 1: initial subcategorization 
? Dimension 2: redistribution of functions and 
transitivity alternations 
? D imension 3: surface realization of 
arguments, clause type and word order 
Each terminal class in dimension 1 describes a
possible initial subcategorization (i.e. a tree 
family). Each terminal class it\] dimension 2 
describes a list of ordered redistributions of 
functions (e.g. it allows to add an argument for 
causatives). Finally, each terminal class in 
dimension 3 represents the surface realization of a 
(final) flmction (e.g. cliticized, extracted ...). 
Each class in the hierarchy corresponds to the 
partial description of a tree (cf. Rogers & Vijay- 
Shanker (94)). An elementary tree is generated by 
inheriting from one terminal class in dimension 1, 
fi'om one terminal class in dimension 2 and fl'olll 
U terulinal classes ill dinlension 3 (were n is the 
number of arguments of the elementary tree). 6 
The hierarchy is partially handwritten. Then 
crossing of linguistic phenomena (e.g. passive + 
extraction), terminal classes, and from there 
elementary trees are generated automatically off 
line. This allows to obtain a grammar which cat\] 
then be used to parse online. When the grau\]mar 
is generated, it is straight forward to keep track of 
the terminal classes each elementary tree 
inherited from : Figure 3 shows seven elementary 
trees which can superiag "domw" (gives), as well 
as the inheritance patterns 7 associated to each of 
these supertags. All the exainples below will refer 
to this figure. 
The key idea then is to represent a set of 
elementary trees by a disjunction for each 
dilnension of the hierarchy. Therefore, a hypertag 
consists in 3 disjunctions (one for dimension 1, 
one for dinlension 2 and one for dimension 3). 
The cross-product of the disiunctions can then be 
perforined automatically and from there the set of 
elementary trees referred to by the hypertag will 
6 The idea to use the MG to obtain a colnpact 
representation f a set of SuperTags was briefly sketched 
in Candito (99) and Abeill6 & al. (99), by resorting to 
MetaFeatures, but the approach ere is slightly different 
since only inlbrmation about he classes in the hierarchy is
used. 
7 We call inheritance patterns Ihe structure used to store all 
the terminal classes atree has inherited from. 
{ ) CA . . . . .  . . . . . .  
oq 
,~ V~)i .... . . .  ioII 1 : It0 '?li i ('Ill 2) q 
N ~ ' P  ~ II,i ....... i,,, 2 . . . . . . .  lislril,,,,ioi, .1  
\[~)imension3: suhj:tlominai.canonlcal \[\[ 
\[ / /~ .x  ollj ....... ina, ........ |cal II 
donne h N2-L ~l'O )j : II(lltl IIn -canon efll\]\[ 
(J donne lille pOlllMt~ h M. / 
J gives an apple to M) 
c~2 
N ~ N  I.L \[~)i ....... ioi, 1: n0vnl (hi12) q 
I Dimc.sio. 2 : .o redistril>ution t 
I lii ....... ion 3 :\] suhj ....... inal ........ ieal II 
~ / I obJ ....... i,,,,I ........ i~.l II 
(J, donlIc \[I ~.~. \[\]Dt3 pOlllmO / 
J gi','es to M an apll\]?) 
0'.~ F)  i ....... ion |: I, OVlll(~ln2) "~1 
s ~ I Dimension 2 : hObj-clnply / 
N~NI .L~ \] I)i ....... ion 3 :\[ sulij ....... inal ........ ical I\[ 
I L_  I ebj : nominal-canonical I\] 
define 
(J. dOlitle LilLe pOlliille/ 
J gives an apple) 
\[;4 ~l i  ....... ioi, I : nll~ nl(hi,2) 
~ ~  I I)hllensi(ll l 2 : no redistribution \[ 
Conll~ %' Nll$ PP \[ I obj : relallvizcd-object I I  
I I / \  L i.o,,J: ...... ,,,,,, . . . . . . . .  ++.~ 
que donne l'\[ep N2.\[- 
/ a 
(1 peru n ? que donne J. h M, I 
The allph, wDich gives J. to M.) 
t\]5 \[~)i ....... ioi, 1: n0viil(hn2) q N 
N I * ~ S _ '  ~ I Dime.d.n 2 :no redistril}utlon .1  
~ ~  l l) i  ....... i0113 :l st lhj  ........ ilHll . . . . . . . . .  i ' l  II 
Cotnl> N0-L V PP l \] obj : rehttivized-olljcct I I  
I I ~ L l,,-.,,J ........ i,,,,: .......... i,.,~ 
que donne P\[cp N2,~ 
/ 
/l 
(l,a pomme que J. donne D M. I 
The allph, whictl J. gil'es to M.J 
I}6N ~) i  ....... ion I: n0vnl(',,,2) q 
I m ....... io,, 2 :aohj-cmply t NI* - -  ~ ~ I I ) i  ....... ion 3 :l suhj ........ iual-i.verled \ ] l  / A 
Comp V NI)~ L i ob.i : relali;ized-object .~J 
I i 
qlte dotlttd 
(La pomme que donne J. / 
The apph' which give.i J.) 
\[:17 UI i  ....... ion 11 n(l',nl(hn2) q 
y s' ~ I I)imc.sio. 2 : Mll, j-elnpty . l  
NI* l m ....... io. 3 :l +,,i,j ........ inal .......... it,,: \]I 
c , , ~  v L I obj : relali+ized.ohject 11 
I i 
qtte donne 
(La potlltJl# qtte J. dotltte / 
The allPh' which J. gil.es ) 
F IGURE 3 : SuperTags and associated 
inher itance patterns 
be automatically retrieved We will now ilhlstrate 
this, first by showing how hypertags are built, and 
then by explaining how a set of trees (and thus of 
supertags) is retrieved from the information 
contained in a hypemig. 
4.1 Bui ld ing hypertags : a detailed example 
Let us start with a simple exemple were we 
want "donner" to be assigned the supertags o~1 (J. 
dmme tree pomme D M.I J. gives an apple to M.) 
and o~2 (J donne h M. tree l)omme/J, gives M. an 
449 
apple). On figure 3, one notices that these 2 trees 
inherited exactly fi'om the same classes : the 
relative order of the two complements is left 
unspecified in the hierarchy, thus one same 
description will yield both trees. In this case, the 
hypertag will thus simply be identical to the 
inheritance pattern of these 2 trees : 
Dimension 1 : n0vnl (hn2) 
Dimension 2 : no redistribution 
Dimension 3 subj :nominal-canonical \[ 
obj : nominal-canonical \] 
\[ a-~'}bj: nominal-canonical\[ 
Let's now add tree o{3 (J. donne une pomme / 
J. gives an apple) to this hypertag. This tree had 
its second object declared empty in dimension 2 
(thus it inherits only two terminal classes from 
dimension 3, since it has only 2 arguments 
realized). The hypertag now becomes 8 : 
Dim. 1: n0vnl(an2) 
Dim. 2 : no redistribution OR StObj- empty  
I)im. 3 lsubj :nonainal-canonical \[ 
obj : nominal-canonical 
a-obj: nominal-canonical 
Let's now add the tree 134 for the object 
relative to this hypertag. This tree has been 
generated by inheriting in dimension 3 fi'om the 
terminal class "nominal inverted" for its subject 
and from the class "relativized object" for its 
object. This information is simply added in the 
hypertag, which now becomes : 
I 
)i,l~. : n0wll (~.12) 
ira. 2 : no redistribution 0P, il0bj- empty l 
ira. 3 subj :nominal-canonical OR nominal-inverledl I 
obj : nominal-canonical OR relativized-oblect I I 
I a-0bj: n0minal-canonical ii 
Also note that for this last example the 
structural properties of 134 were quite different 
than those of ?~1, 0{2 and cG (for instance, it has a 
root of category N and not S). But this has little 
importance since a generalization is made in 
linguistic terms without explicitly relying on the 
shape of trees. 
it is also clear that hypertags are built in a 
monotonic fashion : each supertag added to a 
hypertag just adds information. Hypertags allow 
to label each word with a unique structure 9. and 
8 What has been added to a supertag is shown in bold 
characters. 
9 We presented a simple example for sake of clarity, but 
traditional POS ambiguity is handled in the same way, 
except hat disjunctions are then added in dimension 1 as 
contain rich syntactic and ftmctional information 
about lexical items (For our example here the 
word donne~gives). They are linguistically 
motivated, but also yield a readable output. They 
can be enriched or modified by Imman annotators 
or easily fed to a parser or shallow parser. 
4.2 Retrieving information from hypertags 
Retrieving inforlnation from hypertags is 
pretty straightforward. For example, to recover 
the set of supertags contained in a hypertag, one 
just needs to perform the cross-product between 
tile 3 dimensions of the hypertag, as shown orl 
Figure 4, in order to obtain all inheritance 
patterns. These inheritance patterns are then 
matched with tile inheritance patterns contained 
in the grammar (i.e. tile right colunm in Figure 3) 
to recover all the appropriate supertags. 
Inheritance patterns which are generated but don't 
match any existing trees in tile grammar are 
simply discarded. 
We observe that the 4 supertags 0{1, c~2 and 
0{3 and \]34 which we had explicitly added to tile 
hypertag in 4.1 are correctly retrieved. But also, 
the supertags 135, 136 and 137 arc retrieved, which 
we did not explicitly intend since we never added 
them to the hypertag. But if a word can anchor 
the 4 first trees, then it will also necessarily 
anchor tile three last ones : for instance we had 
added the canonical tree without a second object 
realized into the hypertag (tree or2 ), as well as 
the tree for tile object relative with a second 
object realized realized (tree 134 ), so it is 
expected that tile tree for the object relative 
without a second object realized can be retrieved 
from the hypertag (tree 136) even though we never 
explicitly added it. In fact, the automatic rossing 
of disjunctions in the hypertag insures 
consistency. 
Also note that no particular" mechanism is 
needed for dimension 3 to handle arguments 
which are not realized : if hObj-empty is inherited 
from dilnension 2, then only subject and object 
will inherit from dimeusiou three (since only 
arguments that are realized inherit from that 
dimension when the grammar is generated). 
Information can be modified at runtime in a 
hypertag, depending on the context of lexical 
items. For example "relativized-object" can be 
suppressed in dimension 2 from the hypertag 
shown on Figure 4, in case no Wh element is 
encountered in a sentence. Then, the correct set 
of supertags will still be retrieved from the 
well. 
450 
Content of the llypertag 
Dim ension2 Dim en sio n3 
Subject Object a-obj 
1 i I  
l .-/\ ~ ~ 1 1 1 i 1 
~1 0<2 \[~5 N o \[~ cs3 \[~6 N. \[7,7 (Jorresllonding Corresl)onl(lhig 
I,'~'~' \[%qsertagsc?r-resls?'idi"glol'il-ierilancelJatter'sl t r~, e 
\[ (el Figure 3) I 
FIGURE 4 : Retrieving Inheritance patterns and Supertags 
fronl a Hypertag 
hypertag by automatic rossing (that is, trees o~1, 
(;~2 and o'.3), since the other inheritance l)atterns 
generated won't refer to any tree ill the grainmar 
(here, tie tree inherits in diillension 3 
,vuhject:in, verted-nominal, without inheriting also 
objecl: IwlalivizeU-oluect) 
4.3 Practical use 
We have seen that an LTAG can be seen as a 
dictionary, in which each lexical entry is 
associated to a set of elementary trees. With 
hypertags, each lexical entry is now paired with 
one unique structure. Therefore, automatically 
hypertagging a text is easy and involves a simple 
dictionary lookup. The equiwllent of finding the 
"right" supertag for each lexical item in a lext (i.e. 
reducing ambiguity) then consists in dynamically 
removing information from hypertags (i.e. 
suppressing elements in disjunctions). This can be 
achieved by specific rules, which are currently 
being developed. The resulting output carl then 
easily be manually annotated in order to build a 
gold-standard corpus : manually removing 
linguistically relevant pieces fronl information in 
a disjunction from a single structure is simpler 
than dealing with a set of trees. In addition of 
obvious advantages in terms of display (tlee 
structures, especially when presented in a non 
graphical way, are unreadable), the task itself 
becomes easier because topological problems are 
solved automatically: annotators need just 
answer questions uch as "does this verb have an 
extracted object ?", "is the subject of this verb 
inverted ?" to decide which terminal classe(s) 
nlust be kept i? .We believe that these questions 
are easier to iulswcr than "Which of these trees 
have a node N I marked wh+ at address 1.1 9" 
(for an extracted object). 
Moreover, supertagged text are difficult to use 
outside of an LTAG framework, contrary to 
hypertagged texts, which contain higher level 
general inguistic information. An example would 
be searching and extracting syntactic data oil a 
large scale : suppose one wants to extract all tile 
occurrences where a given verb V has a 
relativized object. To do so on a hypertagged text 
simply involves performing a "grep" ell all lines 
coutainhig a V whose hypertag contains 
dimension .7 : objet:relalivized-object , without 
knowing anything about the LTAG framework. 
Performing the same task with a supertagged text 
involves knowing how LTAGs encode relativized 
objects in elementary trees and scanning potential 
trees associated with V. Another examl)le would 
be using a hypertagged text as an input to a parser 
based oil a framework other than LTAGs : for 
instance, information in hypertags could be used 
by an LFG parser to constrain the construction of 
an IV-structure, whereas it's uuclear how tills could 
be achieved with supertags. 
10 This of course implies that one must be very careful in 
choosing evocative names for terminal classes. 
451 
The need to "featurize" Supertags, in order to 
pack ambiguity and add functional information 
has also been discussed for text generation ill 
Danlos (98) and more recently in Srinivas & 
Rainbow (00). It would be interesting to compare 
their approach with that of hypertags. 
Conclusion 
We have introduced the notion of Hypertags. 
Hypertags allow to assign one unique structure to 
lexical items. Moreover this structure is readable, 
linguistically and computationally motivated, 
and contains much richer syntactic information 
than traditional POS, thus a hypertagger would be 
a good candidate as the front end of a parser. It 
allows in practice to build large annotated 
resources which are useful for extracting 
syntactic information on a large scale, without 
being dependant on a ~iven grammatical 
formalism. 
We have shown how hypertags are built, how 
information can be retrieved from them. Further 
work will investigate how hypertags can be 
combined irectly. 
Referenees 
Abeilld A., Candito M.H., Kinyon A. (1999) FFAG : 
current status & parsing scheme. Prec. Vextal'99. 
Venice. 
Abeilld A., Cldment L., Kinyon A. (2000a) Building a 
Treebank for French. Prec. LREC'2000. Athens. 
Abeilld A., Candito M.H., Kinyon A. (2000b) Current 
status of FTAG. Proc TAG+5. Paris. 
Barrier N. Barrier S. Kinyon A. (2000). Lcxik : a 
maintenance tool for FTAG. Prec. TAG+5. Paris. 
Candito M-H. (1996) A principle-based hierarchical 
representation of LTAGs, Prec. COLING'96 
Kopenhagen. 
Candito M.-H, (1999) Reprdsentation modulairc ct 
paramdtrable de grammaircs 61ectroniques 
lexicalisdes. Application au frangais e ta  l'italien. 
PhD dissertation. University Paris 7. 
Chen J., Srinivas B., Vijay-Shanker K. 1999 New 
Models for Improving Supertag Disambiguation. 
Prec. EACL'99 pp. 188-195. Bergen. 
Cldment L, Kinyon A. (2000) Chunking, marking and 
searching a morphosyntactically annotated corpus 
for French. Prec. ACIDCA'2000. Monastir. 
Danlos L (1998) GTAG : un formalisme lexicalis6 
pour la gdndration automatique d  TAG. TAL 39:2. 
Giguet E. (1998) Mdthodes pour l'analyse automatique 
de structures formelles sur documents 
multilingues.PhD thesis. Universitd e Cacn. 
Halber A. (1999) Stratdgie d'analyse pour la 
comprdhension de la parole : vers une approche 
base de Grammaires d'Arbres Adjoints Lexicalisdes. 
PhD thesis. ENST. Paris 
Joshi A. (1987) An introduction to Tree Adjoining 
Grammars. In Mathematics of Language. A. 
Manaster-Ramer (eds). John Benjamins Publishing 
Company. Amsterdam.Philadelphia. pp. 87-114. 
Joshi A. (1999) Explorations era domain of locality. 
CLIN'99. Utrecht. 
Joshi A. Srinivas B. (1994) Disambiguation of Super 
Parts of Speech (or Supertags) : Ahnost parsing. 
Proceeding COLING'94. Kyoto. 
Kalhneyer L 0999) Tree Description Grammars and 
Undcrspecified Representations. PhD thesis, 
Universit'At Ti.ibingen. 
Kinyon A. (1999a) Parsing preferences with LTAGs : 
exploiting the derivation tree. Prec. ACL'99.College 
Park, Md 
Kinyon A. (1999b) Some remarks about the 
psycholinguistic relevance of LTAGs. Prec. 
CLIN'99. Utrecht. 
Srinivas B. (1997) Complexity of lexical descriptions 
and its relevance tbr partial parsing, PhD thesis, 
Univ. of Pennsylvania. 
Srinivas B., Joshi A. (1999) Supertagging : An 
approach to almost parsing. Computational 
Linguistics 25:2. 
Srinivas B. Rainbow O. (2000) Using TAGs, a Trce 
Model, and a Language Model for Generation. Prec. 
TAG+5. Paris. 
Tomita M. (1991) Generalized LR Parsing. Masaru 
Tomita (eds). Kluwer academic publishers.. 
Rogers J., Vijay-Shanker K. (1994) Obtaining trees 
froln their descriptions : an application to TAGs. 
Computational Intelligence, 10:4 pp 401-421. 
Voutilainen A. Tapanainen P. (1993) Ambiguity 
resolution in a reductionistic parser. Prec. EACL'93. 
452 
