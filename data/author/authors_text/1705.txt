Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 37?40,
New York, June 2006. c?2006 Association for Computational Linguistics
Gesture Improves Coreference Resolution
Jacob Eisenstein and Randall Davis
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139 USA
{jacobe+davis}@csail.mit.edu
Abstract
Coreference resolution, like many problems
in natural language processing, has most of-
ten been explored using datasets of written
text. While spontaneous spoken language
poses well-known challenges, it also offers ad-
ditional modalities that may help disambiguate
some of the inherent disfluency. We explore
features of hand gesture that are correlated
with coreference. Combining these features
with a traditional textual model yields a statis-
tically significant improvement in overall per-
formance.
1 Introduction
Although the natural language processing community has
traditionally focused largely on text, face-to-face spoken
language is ubiquitous, and offers the potential for break-
through applications in domains such as meetings, lec-
tures, and presentations. We believe that in face-to-face
discourse, it is important to consider the possibility that
non-verbal communication may offer features that are
critical to language understanding. However, due to the
long-standing emphasis on text datasets, there has been
relatively little work on non-textual features in uncon-
strained natural language (prosody being the most no-
table exception).
Multimodal research in NLP has typically focused
on dialogue systems for human-computer interaction
(e.g., (Oviatt, 1999)); in contrast, we are interested in
the applicability of multimodal features to unconstrained
human-human dialogues. We believe that such features
will play an essential role in bringing NLP applications
such as automatic summarization and segmentation to
multimedia documents, such as lectures and meetings.
More specifically, in this paper we explore the possi-
bility of applying hand gesture features to the problem
of coreference resolution, which is thought to be fun-
damental to these more ambitious applications (Baldwin
and Morton, 1998). To motivate the need for multimodal
features in coreference resolution, consider the following
transcript:
?[This circle (1)] is rotating clockwise and [this
piece of wood (2)] is attached at [this point (3)]
and [this point (4)] but [it (5)] can rotate. So as
[the circle (6)] rotates, [this (7)] moves in and
out. So [this whole thing (8)] is just going back
and forth.?
Even given a high degree of domain knowledge (e.g.,
that ?circles? often ?rotate? but ?points? rarely do), de-
termining the coreference in this excerpt seems difficult.
The word ?this? accompanied by a gesture is frequently
used to introduce a new entity, so it is difficult to deter-
mine from the text alone whether ?[this (7)]? refers to
?[this piece of wood (2)],? or to an entirely different part
of the diagram. In addition, ?[this whole thing (8)]? could
be anaphoric, or it might refer to a new entity, perhaps
some superset of predefined parts.
The example text was drawn from a small corpus of di-
alogues, which has been annotated for coreference. Par-
ticipants in the study had little difficulty understanding
what was communicated. While this does not prove that
human listeners are using gesture or other multimodal
features, it suggests that these features merit further in-
vestigation. We extracted hand positions from the videos
in the corpus, using computer vision. From the raw hand
positions, we derived gesture features that were used to
supplement traditional textual features for coreference
resolution. For a description of the study?s protocol, auto-
matic hand tracking, and a fuller examination of the ges-
ture features, see (Eisenstein and Davis, 2006). In this pa-
per, we present results showing that these features yield a
significant improvement in performance.
37
2 Implementation
A set of commonly-used linguistic features were selected
for this problem (Table 1). The first five features apply
to pairs of NPs; the next set of features are applied indi-
vidually to both of the NPs that are candidates for coref-
erence. Thus, we include two features each, e.g., J is
PRONOUN and I is PRONOUN, indicating respectively
whether the candidate anaphor and candidate antecedent
are pronouns. We include separate features for each of
the four most common pronouns: ?this?, ?it?, ?that?, and
?they,? yielding features such as J=?this?.
2.1 Gesture Features
The gesture features shown in Table 1 are derived from
the raw hand positions using a simple, deterministic sys-
tem. Temporally, all features are computed at the mid-
point of each candidate NP; for a further examination
of the sensitivity to temporal offset, see (Eisenstein and
Davis, 2006).
At most one hand is determined to be the ?focus hand,?
according to the following heuristic: select the hand far-
thest from the body in the x-dimension, as long as the
hand is not occluded and its y-position is not below the
speaker?s waist. If neither hand meets these criteria, than
no hand is said to be in focus. Occluded hands are also
not permitted to be in focus; the listener?s perspective was
very similar to that of the camera, so it seemed unlikely
that the speaker would occlude a meaningful gesture. In
addition, our system?s estimates of the position of an oc-
cluded hand are unlikely to be accurate.
If focus hands can be identified during both mentions,
the Euclidean distance between focus points is computed.
The distance is binned, using the supervised method de-
scribed in (Fayyad and Irani, 1993). An advantage of
binning the continuous features is that we can create a
special bin for missing data, which occurs whenever a fo-
cus hand cannot be identified.
If the same hand is in focus during both NPs, then the
value of WHICH HAND is set to ?same?; if a different
hand is in focus then the value is set to ?different?; if a
focus hand cannot be identified in one or both NPs, then
the value is set to ?missing.? This multi-valued feature is
automatically converted into a set of boolean features, so
that all features can be represented as binary variables.
2.2 Coreference Resolution Algorithm
(McCallum and Wellner, 2004) formulates coreference
resolution as a Conditional Random Field, where men-
tions are nodes, and their similarities are represented as
weighted edges. Edge weights range from ?? to ?,
with larger values indicating greater similarity. The op-
timal solution is obtained by partitioning the graph into
cliques such that the sum of the weights on edges within
cliques is maximized, and the sum of the weights on
edges between cliques is minimized:
y? = argmaxy
?
i,j,i6=j
yi,js(xi, xj) (1)
In equation 1, x is a set of mentions and y is a corefer-
ence partitioning, such that yi,j = 1 if mentions xi and xj
corefer, and yi,j = ?1 otherwise. s(xi, xj) is a similarity
score computed on mentions xi and xj .
Computing the optimal partitioning y? is equivalent to
the problem of correlation clustering, which is known to
be NP-hard (Demaine and Immorlica, to appear). De-
maine and Immorlica (to appear) propose an approxima-
tion using integer programming, which we are currently
investigating. However, in this research we use average-
link clustering, which hierarchically groups the mentions
x, and then forms clusters using a cutoff chosen to maxi-
mize the f-measure on the training set.
We experiment with both pipeline and joint models for
computing s(xi, xj). In the pipeline model, s(xi, xj) is
the posterior of a classifier trained on pairs of mentions.
The advantage of this approach is that any arbitrary clas-
sifier can be used; the downside is that minimizing the er-
ror on all pairs of mentions may not be equivalent to min-
imizing the overall error of the induced clustering. For
experiments with the pipeline model, we found best re-
sults by boosting shallow decision trees, using the Weka
implementation (Witten and Frank, 1999).
Our joint model is based on McCallum and Well-
ner?s (2004) adaptation of the voted perceptron to corefer-
ence resolution. Here, s is given by the product of a vec-
tor of weights ? with a set of boolean features ?(xi, xj)
induced from the pair of noun phrases: s(xi, xj) =
??(xi, xj). The maximum likelihood weights can be ap-
proximated by a voted perceptron, where, in the iteration
t of the perceptron training:
?t = ?t?1 +
?
i,j,i6=j
?(xi, xj)(y
?
i,j ? y?i,j) (2)
In equation 2, y? is the ground truth partitioning from
the labeled data. y? is the partitioning that maximizes
equation 1 given the set of weights ?t?1. As before,
average-link clustering with an adaptive cutoff is used to
partition the graph. The weights are then averaged across
all iterations of the perceptron, as in (Collins, 2002).
3 Evaluation
The results of our experiments are computed using
mention-based CEAF scoring (Luo, 2005), and are re-
ported in Table 2. Leave-one-out evaluation was used to
form 16 cross-validation folds, one for each document in
the corpus. Using a planned, one-tailed pairwise t-test,
the gesture features improved performance significantly
38
MARKABLE DIST The number of markables between the candidate NPs
EXACT MATCH True if the candidate NPs have identical surface forms
STR MATCH True if the candidate NPs match after removing articles
NONPRO MATCH True if the candidate NPs are not pronouns and have identical surface forms
NUMBER MATCH True if the candidate NPs agree in number
PRONOUN True if the NP is a pronoun
DEF NP True if the NP begins with a definite article, e.g. ?the box?
DEM NP True if the NP is not a pronoun and begins with the word ?this?
INDEF NP True if the NP begins an indefinite article, e.g. ?a box?
pronouns Individual features for each of the four most common pronouns: ?this?, ?it?, ?that?, and
?they?
FOCUS DIST Distance between the position of the in-focus hand during j and i (see text)
WHICH HAND Whether the hand in focus during j is the same as in i (see text)
Table 1: The feature set
System Feature set F1
AdaBoost Gesture + Speech 54.9
AdaBoost Speech only 52.8
Voted Perceptron Gesture + Speech 53.7
Voted Perceptron Speech only 52.9
Baseline EXACT MATCH only 50.2
Baseline None corefer 41.5
Baseline All corefer 18.8
Table 2: Results
for the boosted decision trees (t(15) = 2.48, p < .02),
though not for the voted perceptron (t(15) = 1.07, p =
.15).
In the ?all corefer? baseline, all NPs are grouped into
a single cluster; in the ?none corefer?, each NP gets its
own cluster. In the ?EXACT MATCH? baseline, two NPs
corefer when their surface forms are identical. All ex-
perimental systems outperform all baselines by a statis-
tically significant amount. There are few other reported
results for coreference resolution on spontaneous, uncon-
strained speech; (Strube and Mu?ller, 2003) similarly finds
low overall scores for pronoun resolution on the Switch-
board Corpus, albeit by a different scoring metric. Unfor-
tunately, they do not compare performance to equivalent
baselines.
For the AdaBoost method, 50 iterations of boosting are
performed on shallow decision trees, with a maximum
tree depth of three. For the voted perceptron, 50 training
iterations were performed. The performance of the voted
perceptron on this task was somewhat unstable, varying
depending on the order in which the documents were
presented. This may be because a small change in the
weights can lead to a very different partitioning, which
in turn affects the setting of the weights in the next per-
ceptron iteration. For these results, the order of presenta-
tion of the documents was randomized, and the scores for
the voted perceptron are the average of 10 different runs
(? = 0.32% with gestures, 0.40% without).
Although the AdaBoost method minimizes pairwise
error rather than the overall error of the partitioning, its
performance was superior to the voted perceptron. One
possible explanation is that by boosting small decision
trees, AdaBoost was able to take advantage of non-linear
combinations of features. We tested the voted perceptron
using all pairwise combinations of features, but this did
not improve performance.
4 Discussion
If gesture features play a role in coreference resolu-
tion, then one might expect the probability of corefer-
ence to vary significantly when conditioned on features
describing the gesture. As shown in Table 3, the pre-
diction holds: the binned FOCUS DIST gesture feature
has the fifth highest ?2 value, and the relationship be-
tween coreference and all gesture features was significant
(?2 = 727.8, dof = 4, p < .01). Note also that although
FOCUS DIST ranks fifth, three of the features above it
are variants of a string-match feature, and so are highly
redundant.
The WHICH HAND feature is less strongly corre-
lated with coreference, but the conditional probabilities
do correspond with intuition. If the NPs corefer, then
the probability of using the same hand to gesture during
both NPs is 59.9%; if not, then the likelihood is 52.8%.
The probability of not observing a focus hand is 20.3%
when the NPs corefer, 25.1% when they do not; in other
words, gesture is more likely for both NPs of a corefer-
ent pair than for the NPs of a non-coreferent pair. The
relation between the WHICH HAND feature and coref-
erence is also significantly different from the null hypoth-
esis (?2 = 57.2, dof = 2, p < .01).
39
Rank Feature ?2
1. EXACT MATCH 1777.9
2. NONPRO MATCH 1357.5
3. STR MATCH 1201.8
4. J = ?it? 732.8
5. FOCUS DIST 727.8
6. MARKABLE DIST 619.6
7. J is PRONOUN 457.5
8. NUMBER 367.9
9. I = ?it? 238.6
10. I is PRONOUN 132.6
11. J is INDEF NP 79.3
12. SAME FOCUS HAND 57.2
Table 3: Top 12 Features By Chi-Squared
5 Related Work
Research on multimodality in the NLP community
has usually focused on multimodal dialogue systems
(e.g., (Oviatt, 1999)). These systems differ fundamen-
tally from ours in that they address human-computer in-
teraction, whereas we address human-human interaction.
Multimodal dialogue systems tackle interesting and dif-
ficult challenges, but the grammar, vocabulary, and rec-
ognized gestures are often pre-specified, and dialogue is
controlled at least in part by the computer. In our data, all
of these things are unconstrained.
Prosody has been shown to improve performance on
several NLP problems, such as topic and sentence seg-
mentation (e.g., (Shriberg et al, 2000)). We are aware of
no equivalent work showing statistically significant im-
provement on unconstrained speech using hand gesture
features. (Nakano et al, 2003) shows that body posture
predicts turn boundaries, but does not show that these
features improve performance beyond a text-only system.
(Chen et al, 2004) shows that gesture may improve sen-
tence segmentation; however, in this study, the improve-
ment afforded by gesture is not statistically significant,
and evaluation was performed on a subset of their original
corpus that was chosen to include only the three speakers
who gestured most frequently. Still, this work provides a
valuable starting point for the integration of gesture fea-
ture into NLP systems.
6 Conclusion
We have described how gesture features can be used to
improve coreference resolution on a corpus of uncon-
strained speech. Hand position and hand choice corre-
late significantly with coreference, explaining this gain in
performance. We believe this is the first example of hand
gesture features improving performance by a statistically
significant margin on unconstrained speech.
References
Breck Baldwin and Thomas Morton. 1998. Dy-
namic coreference-based summarization. In Proc. of
EMNLP.
Lei Chen, Yang Liu, Mary P. Harper, and Eliza-
beth Shriberg. 2004. Multimodal model integra-
tion for sentence unit detection. In Proceedings of
International Conference on Multimodal Interfaces
(ICMI?04). ACM Press.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
EMNLP.
Erik D. Demaine and Nicole Immorlica. to appear. Cor-
relation clustering in general weighted graphs. Theo-
retical Computer Science.
Jacob Eisenstein and Randall Davis. 2006. Gesture fea-
tures for coreference resolution. In Workshop on Mul-
timodal Interaction and Related Machine Learning Al-
gorithms.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-
interval discretization of continuousvalued attributes
for classification learning. In Proceedings of IJCAI-
93.
Xiaoqiang Luo. 2005. On coreference resolution perfor-
mance metrics. In Proc. of HLT-EMNLP, pages 25?32.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Neural Information Processing Sys-
tems.
Yukiko Nakano, Gabe Reinstein, Tom Stocky, and Jus-
tine Cassell. 2003. Towards a model of face-to-face
grounding. In Proceedings of ACL?03.
Sharon L. Oviatt. 1999. Mutual disambiguation of
recognition errors in a multimodel architecture. In Hu-
man Factors in Computing Systems (CHI?99), pages
576?583.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur,
and Gokhan Tur. 2000. Prosody-based automatic seg-
mentation of speech into sentences and topics. Speech
Communication, 32.
Michael Strube and Christoph Mu?ller. 2003. A machine
learning approach to pronoun resolution in spoken di-
alogue. In Proceedings of ACL ?03, pages 168?175.
Ian H. Witten and Eibe Frank. 1999. Data Mining: Prac-
tical Machine Learning Tools and Techniques with
Java Implementations. Morgan Kaufmann.
40
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 352?359,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Conditional Modality Fusion for Coreference Resolution
Jacob Eisenstein and Randall Davis
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139 USA
{jacobe,davis}@csail.mit.edu
Abstract
Non-verbal modalities such as gesture can
improve processing of spontaneous spoken
language. For example, similar hand ges-
tures tend to predict semantic similarity, so
features that quantify gestural similarity can
improve semantic tasks such as coreference
resolution. However, not all hand move-
ments are informative gestures; psycholog-
ical research has shown that speakers are
more likely to gesture meaningfully when
their speech is ambiguous. Ideally, one
would attend to gesture only in such cir-
cumstances, and ignore other hand move-
ments. We present conditional modality
fusion, which formalizes this intuition by
treating the informativeness of gesture as a
hidden variable to be learned jointly with
the class label. Applied to coreference
resolution, conditional modality fusion sig-
nificantly outperforms both early and late
modality fusion, which are current tech-
niques for modality combination.
1 Introduction
Non-verbal modalities such as gesture and prosody
can increase the robustness of NLP systems to the
inevitable disfluency of spontaneous speech. For ex-
ample, consider the following excerpt from a dia-
logue in which the speaker describes a mechanical
device:
?So this moves up, and it ? everything moves up.
And this top one clears this area here, and goes all
the way up to the top.?
The references in this passage are difficult to
disambiguate, but the gestures shown in Figure 1
make the meaning more clear. However, non-verbal
modalities are often noisy, and their interactions
with speech are complex (McNeill, 1992). Ges-
ture, for example, is sometimes communicative, but
other times merely distracting. While people have
little difficulty distinguishing between meaningful
gestures and irrelevant hand motions (e.g., self-
touching, adjusting glasses) (Goodwin and Good-
win, 1986), NLP systems may be confused by such
seemingly random movements. Our goal is to in-
clude non-verbal features only in the specific cases
when they are helpful and necessary.
We present a model that learns in an unsupervised
fashion when non-verbal features are useful, allow-
ing it to gate the contribution of those features. The
relevance of the non-verbal features is treated as a
hidden variable, which is learned jointly with the
class label in a conditional model. We demonstrate
that this improves performance on binary corefer-
ence resolution, the task of determining whether a
noun phrases refers to a single semantic entity. Con-
ditional modality fusion yields a relative increase of
73% in the contribution of hand-gesture features.
The model is not specifically tailored to gesture-
speech integration, and may also be applicable to
other non-verbal modalities.
2 Related work
Most of the existing work on integrating non-verbal
features relates to prosody. For example, Shriberg
et al (2000) explore the use of prosodic features for
sentence and topic segmentation. The first modal-
352
And this top one clears this area here, and goes
all the way up to the top...
2
So this moves up. And it ? everything moves up.
1
Figure 1: An example where gesture helps to disambiguate meaning.
ity combination technique that they consider trains a
single classifier with all modalities combined into a
single feature vector; this is sometimes called ?early
fusion.? Shriberg et al also consider training sepa-
rate classifiers and combining their posteriors, either
through weighted addition or multiplication; this is
sometimes called ?late fusion.? Late fusion is also
employed for gesture-speech combination in (Chen
et al, 2004). Experiments in both (Shriberg et al,
2000) and (Kim et al, 2004) find no conclusive win-
ner among early fusion, additive late fusion, and
multiplicative late fusion.
Toyama and Horvitz (2000) introduce a Bayesian
network approach to modality combination for
speaker identification. As in late fusion, modality-
specific classifiers are trained independently. How-
ever, the Bayesian approach also learns to predict
the reliability of each modality on a given instance,
and incorporates this information into the Bayes
net. While more flexible than the interpolation tech-
niques described in (Shriberg et al, 2000), training
modality-specific classifiers separately is still sub-
optimal compared to training them jointly, because
independent training of the modality-specific classi-
fiers forces them to account for data that they can-
not possibly explain. For example, if the speaker is
not gesturing meaningfully, it is counterproductive
to train a gesture-modality classifier on the features
at this instant; doing so can lead to overfitting and
poor generalization.
Our approach combines aspects of both early and
late fusion. As in early fusion, classifiers for all
modalities are trained jointly. But as in Toyama and
Horvitz?s Bayesian late fusion model, modalities can
be weighted based on their predictive power for spe-
cific instances. In addition, our model is trained to
maximize conditional likelihood, rather than joint
likelihood.
3 Conditional modality fusion
The goal of our approach is to learn to weight the
non-verbal features xnv only when they are rele-
vant. To do this, we introduce a hidden variable
m ? {?1, 1}, which governs whether the non-
verbal features are included. p(m) is conditioned on
a subset of features xm, which may belong to any
modality; p(m|xm) is learned jointly with the class
label p(y|x), with y ? {?1, 1}. For our coreference
resolution model, y corresponds to whether a given
pair of noun phrases refers to the same entity.
In a log-linear model, parameterized by weights
w, we have:
p(y|x;w) =
?
m
p(y,m|x;w)
=
?
m exp(?(y,m,x;w))?
y?,m exp(?(y
?,m,x;w))
.
Here, ? is a potential function representing the
compatibility between the label y, the hidden vari-
able m, and the observations x; this potential is pa-
rameterized by a vector of weights, w. The numera-
tor expresses the compatibility of the label y and ob-
servations x, summed over all possible values of the
hidden variablem. The denominator sums over both
m and all possible labels y?, yielding the conditional
probability p(y|x;w). The use of hidden variables
353
in a conditionally-trained model follows (Quattoni
et al, 2004).
This model can be trained by a gradient-based
optimization to maximize the conditional log-
likelihood of the observations. The unregularized
log-likelihood and gradient are given by:
l(w) =
X
i
ln(p(yi|xi;w)) (1)
=
X
i
ln
P
m exp(?(yi,m,xi;w))P
y?,m exp(?(y
?,m,xi;w))
(2)
?li
?wj
=
X
m
p(m|yi,xi;w)
?
?wj
?(yi,m,xi;w)
?
X
y?,m
p(m, y?|xi;w)
?
?wj
?(y?,m,xi;w)
The form of the potential function ? is where our
intuitions about the role of the hidden variable are
formalized. Our goal is to include the non-verbal
features xnv only when they are relevant; conse-
quently, the weight for these features should go to
zero for some settings of the hidden variable m. In
addition, verbal language is different when used in
combination with meaningful non-verbal commu-
nication than when it is used unimodally (Kehler,
2000; Melinger and Levelt, 2004). Thus, we learn
a different set of feature weights for each case: wv,1
when the non-verbal features are included, and wv,2
otherwise. The formal definition of the potential
function for conditional modality fusion is:
?(y,m,x;w) ?
{
y(wTv,1xv +w
T
nvxnv) +w
T
mxm m = 1
ywTv,2xv ?w
T
mxm m = ?1.
(3)
4 Application to coreference resolution
We apply conditional modality fusion to corefer-
ence resolution ? the problem of partitioning the
noun phrases in a document into clusters, where all
members of a cluster refer to the same semantic en-
tity. Coreference resolution on text datasets is well-
studied (e.g., (Cardie and Wagstaff, 1999)). This
prior work provides the departure point for our in-
vestigation of coreference resolution on spontaneous
and unconstrained speech and gesture.
4.1 Form of the model
The form of the model used in this application is
slightly different from that shown in Equation 3.
When determining whether two noun phrases core-
fer, the features at each utterance must be consid-
ered. For example, if we are to compare the simi-
larity of the gestures that accompany the two noun
phrases, it should be the case that gesture is relevant
during both time periods.
For this reason, we create two hidden variables,
m1 and m2; they indicate the relevance of ges-
ture over the first (antecedent) and second (anaphor)
noun phrases, respectively. Since gesture similarity
is only meaningful if the gesture is relevant during
both NPs, the gesture features are included only if
m1 = m2 = 1. Similarly, the linguistic feature
weights wv,1 are used when m1 = m2 = 1; oth-
erwise the weights wv,2 are used. This yields the
model shown in Equation 4.
The vector of meta features xm1 includes all
single-phrase verbal and gesture features from Ta-
ble 1, computed at the antecedent noun phrase;
xm2 includes the single-phrase verbal and gesture
features, computed at the anaphoric noun phrase.
The label-dependent verbal features xv include both
pairwise and single phrase verbal features from the
table, while the label-dependent non-verbal features
xnv include only the pairwise gesture features. The
single-phrase non-verbal features were not included
because they were not thought to be informative as
to whether the associated noun-phrase would partic-
ipate in coreference relations.
4.2 Verbal features
We employ a set of verbal features that is similar
to the features used by state-of-the-art coreference
resolution systems that operate on text (e.g., (Cardie
and Wagstaff, 1999)). Pairwise verbal features in-
clude: several string-match variants; distance fea-
tures, measured in terms of the number of interven-
ing noun phrases and sentences between the candi-
date NPs; and some syntactic features that can be
computed from part of speech tags. Single-phrase
verbal features describe the type of the noun phrase
(definite, indefinite, demonstrative (e.g., this ball),
or pronoun), the number of times it appeared in
the document, and whether there were any adjecti-
354
?(y,m1,m2,x;w) ?
{
y(wTv,1xv +w
T
nvxnv) +m1w
T
mxm1 +m2w
T
mxm2 , m1 = m2 = 1
ywTv,2xv +m1w
T
mxm1 +m2w
T
mxm2 , otherwise.
(4)
val modifiers. The continuous-valued features were
binned using a supervised technique (Fayyad and
Irani, 1993).
Note that some features commonly used for coref-
erence on the MUC and ACE corpora are not appli-
cable here. For example, gazetteers listing names of
nations or corporations are not relevant to our cor-
pus, which focuses on discussions of mechanical de-
vices (see section 5). Because we are working from
transcripts rather than text, features dependent on
punctuation and capitalization, such as apposition,
are also not applicable.
4.3 Non-verbal features
Our non-verbal features attempt to capture similar-
ity between the speaker?s hand gestures; similar ges-
tures are thought to suggest semantic similarity (Mc-
Neill, 1992). For example, two noun phrases may
be more likely to corefer if they are accompanied by
identically-located pointing gestures. In this section,
we describe features that quantify various aspects of
gestural similarity.
The most straightforward measure of similarity is
the Euclidean distance between the average hand po-
sition during each noun phrase ? we call this the
FOCUS-DISTANCE feature. Euclidean distance cap-
tures cases in which the speaker is performing a ges-
tural ?hold? in roughly the same location (McNeill,
1992).
However, Euclidean distance may not correlate
directly with semantic similarity. For example,
when gesturing at a detailed part of a diagram,
very small changes in hand position may be se-
mantically meaningful, while in other regions posi-
tional similarity may be defined more loosely. Ide-
ally, we would compute a semantic feature cap-
turing the object of the speaker?s reference (e.g.,
?the red block?), but this is not possible in gen-
eral, since a complete taxonomy of all possible ob-
jects of reference is usually unknown. Instead, we
use a hidden Markov model (HMM) to perform a
spatio-temporal clustering on hand position and ve-
locity. The SAME-CLUSTER feature reports whether
the hand positions during two noun phrases were
usually grouped in the same cluster by the HMM.
JS-DIV reports the Jensen-Shannon divergence, a
continuous-valued feature used to measure the simi-
larity in cluster assignment probabilities between the
two gestures (Lin, 1991).
The gesture features described thus far capture the
similarity between static gestures; that is, gestures
in which the hand position is nearly constant. How-
ever, these features do not capture the similarity be-
tween gesture trajectories, which may also be used
to communicate meaning. For example, a descrip-
tion of two identical motions might be expressed
by very similar gesture trajectories. To measure the
similarity between gesture trajectories, we use dy-
namic time warping (Huang et al, 2001), which
gives a similarity metric for temporal data that is
invariant to speed. This is reported in the DTW-
DISTANCE feature.
All features are computed from hand and body
pixel coordinates, which are obtained via computer
vision; our vision system is similar to (Deutscher et
al., 2000). The feature set currently supports only
single-hand gestures, using the hand that is farthest
from the body center. As with the verbal feature set,
supervised binning was applied to the continuous-
valued features.
4.4 Meta features
The role of the meta features is to determine whether
the gesture features are relevant at a given point in
time. To make this determination, both verbal and
non-verbal features are applied; the only require-
ment is that they be computable at a single instant
in time (unlike features that measure the similarity
between two NPs or gestures).
Verbal meta features Meaningful gesture has
been shown to be more frequent when the associated
speech is ambiguous (Melinger and Levelt, 2004).
Kehler finds that fully-specified noun phrases are
less likely to receive multimodal support (Kehler,
2000). These findings lead us to expect that pro-
355
Pairwise verbal features
edit-distance a numerical measure of the string simi-
larity between the two NPs
exact-match true if the two NPs have identical sur-
face forms
str-match true if the NPs are identical after re-
moving articles
nonpro-str true if i and j are not pronouns, and str-
match is true
pro-str true if i and j are pronouns, and str-
match is true
j-substring-i true if the anaphor j is a substring of
the antecedent i
i-substring-j true if i is a substring of j
overlap true if there are any shared words be-
tween i and j
np-dist the number of noun phrases between i
and j in the document
sent-dist the number of sentences between i and
j in the document
both-subj true if both i and j precede the first verb
of their sentences
same-verb true if the first verb in the sentences for
i and j is identical
number-match true if i and j have the same number
Single-phrase verbal features
pronoun true if the NP is a pronoun
count number of times the NP appears in the
document
has-modifiers true if the NP has adjective modifiers
indef-np true if the NP is an indefinite NP (e.g.,
a fish)
def-np true if the NP is a definite NP (e.g., the
scooter)
dem-np true if the NP begins with this, that,
these, or those
lexical features lexical features are defined for the most
common pronouns: it, that, this, and
they
Pairwise gesture features
focus-distance the Euclidean distance in pixels be-
tween the average hand position during
the two NPs
DTW-agreement a measure of the agreement of the hand-
trajectories during the two NPs, com-
puted using dynamic time warping
same-cluster true if the hand positions during the two
NPs fall in the same cluster
JS-div the Jensen-Shannon divergence be-
tween the cluster assignment likeli-
hoods
Single-phrase gesture features
dist-to-rest distance of the hand from rest position
jitter sum of instantaneous motion across NP
speed total displacement over NP, divided by
duration
rest-cluster true if the hand is usually in the cluster
associated with rest position
movement-cluster true if the hand is usually in the cluster
associated with movement
Table 1: The feature set
nouns should be likely to co-occur with meaningful
gestures, while definite NPs and noun phrases that
include adjectival modifiers should be unlikely to do
so. To capture these intuitions, all single-phrase ver-
bal features are included as meta features.
Non-verbal meta features Research on gesture
has shown that semantically meaningful hand mo-
tions usually take place away from ?rest position,?
which is located at the speaker?s lap or sides (Mc-
Neill, 1992). Effortful movements away from these
default positions can thus be expected to predict that
gesture is being used to communicate. We iden-
tify rest position as the center of the body on the
x-axis, and at a fixed, predefined location on the y-
axis. The DIST-TO-REST feature computes the av-
erage Euclidean distance of the hands from the rest
position, over the duration of the NP.
As noted in the previous section, a spatio-
temporal clustering was performed on the hand po-
sitions and velocities, using an HMM. The REST-
CLUSTER feature takes the value ?true? iff the most
frequently occupied cluster during the NP is the
cluster closest to rest position. In addition, pa-
rameter tying in the HMM forces all clusters but
one to represent static hold, with the remaining
cluster accounting for the transition movements be-
tween holds. Only this last cluster is permitted to
have an expected non-zero speed; if the hand is
most frequently in this cluster during the NP, then
the MOVEMENT-CLUSTER feature takes the value
?true.?
4.5 Implementation
The objective function (Equation 1) is optimized
using a Java implementation of L-BFGS, a quasi-
Newton numerical optimization technique (Liu and
Nocedal, 1989). Standard L2-norm regulariza-
tion is employed to prevent overfitting, with cross-
validation to select the regularization constant. Al-
though standard logistic regression optimizes a con-
vex objective, the inclusion of the hidden variable
renders our objective non-convex. Thus, conver-
gence to a global minimum is not guaranteed.
5 Evaluation setup
Dataset Our dataset consists of sixteen short dia-
logues, in which participants explained the behavior
356
of mechanical devices to a friend. There are nine
different pairs of participants; each contributed two
dialogues, with two thrown out due to recording er-
rors. One participant, the ?speaker,? saw a short
video describing the function of the device prior
to the dialogue; the other participant was tested on
comprehension of the device?s behavior after the di-
alogue. The speaker was given a pre-printed dia-
gram to aid in the discussion. For simplicity, only
the speaker?s utterances were included in these ex-
periments.
The dialogues were limited to three minutes in du-
ration, and most of the participants used the entire
allotted time. ?Markable? noun phrases ? those that
are permitted to participate in coreference relations
? were annotated by the first author, in accordance
with the MUC task definition (Hirschman and Chin-
chor, 1997). A total of 1141 ?markable? NPs were
transcribed, roughly half the size of the MUC6 de-
velopment set, which includes 2072 markable NPs
over 30 documents.
Evaluation metric Coreference resolution is of-
ten performed in two phases: a binary classifi-
cation phase, in which the likelihood of corefer-
ence for each pair of noun phrases is assessed;
and a partitioning phase, in which the clusters of
mutually-coreferring NPs are formed, maximizing
some global criterion (Cardie and Wagstaff, 1999).
Our model does not address the formation of noun-
phrase clusters, but only the question of whether
each pair of noun phrases in the document corefer.
Consequently, we evaluate only the binary classifi-
cation phase, and report results in terms of the area
under the ROC curve (AUC). As the small size of
the corpus did not permit dedicated test and devel-
opment sets, results are computed using leave-one-
out cross-validation, with one fold for each of the
sixteen documents in the corpus.
Baselines Three types of baselines were compared
to our conditional modality fusion (CMF) technique:
? Early fusion. The early fusion baseline in-
cludes all features in a single vector, ignor-
ing modality. This is equivalent to standard
maximum-entropy classification. Early fusion
is implemented with a conditionally-trained
linear classifier; it uses the same code as the
CMF model, but always includes all features.
? Late fusion. The late fusion baselines train
separate classifiers for gesture and speech, and
then combine their posteriors. The modality-
specific classifiers are conditionally-trained lin-
ear models, and again use the same code as the
CMF model. For simplicity, a parameter sweep
identifies the interpolation weights that maxi-
mize performance on the test set. Thus, it is
likely that these results somewhat overestimate
the performance of these baseline models. We
report results for both additive and multiplica-
tive combination of posteriors.
? No fusion. These baselines include the fea-
tures from only a single modality, and again
build a conditionally-trained linear classifier.
Implementation uses the same code as the CMF
model, but weights on features outside the tar-
get modality are forced to zero.
Although a comparison with existing state-of-the-
art coreference systems would be ideal, all such
available systems use verbal features that are inap-
plicable to our dataset, such as punctuation, capital-
ization, and gazetteers. The verbal features that we
have included are a representative sample from the
literature (e.g., (Cardie and Wagstaff, 1999)). The
?no fusion, verbal features only? baseline thus pro-
vides a reasonable representation of prior work on
coreference, by applying a maximum-entropy clas-
sifier to this set of typical verbal features.
Parameter tuning Continuous features are
binned separately for each cross-validation fold,
using only the training data. The regularization
constant is selected by cross-validation within each
training subset.
6 Results
Conditional modality fusion outperforms all other
approaches by a statistically significant margin (Ta-
ble 2). Compared with early fusion, CMF offers an
absolute improvement of 1.20% in area under the
ROC curve (AUC).1 A paired t-test shows that this
1AUC quantifies the ranking accuracy of a classifier. If the
AUC is 1, all positively-labeled examples are ranked higher than
all negative-labeled ones.
357
model AUC
Conditional modality fusion .8226
Early fusion .8109
Late fusion, multiplicative .8103
Late fusion, additive .8068
No fusion (verbal features only) .7945
No fusion (gesture features only) .6732
Table 2: Results, in terms of areas under the ROC
curve
2 3 4 5 6 7 80.79
0.795
0.8
0.805
0.81
0.815
0.82
0.825
0.83
log of regularization constant
AU
C
CMF
Early FusionSpeech Only
Figure 2: Conditional modality fusion is robust to
variations in the regularization constant.
result is statistically significant (p < .002, t(15) =
3.73). CMF obtains higher performance on fourteen
of the sixteen test folds. Both additive and multi-
plicative late fusion perform on par with early fu-
sion.
Early fusion with gesture features is superior to
unimodal verbal classification by an absolute im-
provement of 1.64% AUC (p < 4 ? 10?4, t(15) =
4.45). Thus, while gesture features improve coref-
erence resolution on this dataset, their effectiveness
is increased by a relative 73% when conditional
modality fusion is applied. Figure 2 shows how per-
formance varies with the regularization constant.
7 Discussion
The feature weights learned by the system to deter-
mine coreference largely confirm our linguistic in-
tuitions. Among the textual features, a large pos-
itive weight was assigned to the string match fea-
tures, while a large negative weight was assigned to
features such as number incompatibility (i.e., sin-
pronoun def dem indef "this" "it" "that" "they"modifiers?0.6
?0.5
?0.4
?0.3
?0.2
?0.1
0
0.1
0.2
0.3
0.4 Weights learned with verbal meta features
Figure 3: Weights for verbal meta features
gular versus plural). The system also learned that
gestures with similar hand positions and trajectories
were likely to indicate coreferring noun phrases; all
of our similarity metrics were correlated positively
with coreference. A chi-squared analysis found that
the EDIT DISTANCE was the most informative ver-
bal feature. The most informative gesture feature
was DTW-AGREEMENT feature, which measures
the similarity between gesture trajectories.
As described in section 4, both textual and gestu-
ral features are used to determine whether the ges-
ture is relevant. Among textual features, definite
and indefinite noun phrases were assigned nega-
tive weights, suggesting gesture would not be use-
ful to disambiguate coreference for such NPs. Pro-
nouns were assigned positive weights, with ?this?
and the much less frequently used ?they? receiving
the strongest weights. ?It? and ?that? received lower
weights; we observed that these pronouns were fre-
quently used to refer to the immediately preceding
noun phrase, so multimodal support was often un-
necessary. Last, we note that NPs with adjectival
modifiers were assigned negative weights, support-
ing the finding of (Kehler, 2000) that fully-specified
NPs are less likely to receive multimodal support. A
summary of the weights assigned to the verbal meta
features is shown in Figure 3. Among gesture meta
features, the weights learned by the system indicate
that non-moving hand gestures away from the body
are most likely to be informative in this dataset.
358
8 Future work
We have assumed that the relevance of gesture to
semantics is dependent only on the currently avail-
able features, and not conditioned on prior history.
In reality, meaningful gestures occur over contigu-
ous blocks of time, rather than at randomly dis-
tributed instances. Indeed, the psychology literature
describes a finite-state model of gesture, proceed-
ing from ?preparation,? to ?stroke,? ?hold,? and then
?retraction? (McNeill, 1992). These units are called
movement phases. The relevance of various gesture
features may be expected to depend on the move-
ment phase. During strokes, the trajectory of the
gesture may be the most relevant feature, while dur-
ing holds, static features such as hand position and
hand shape may dominate; during preparation and
retraction, gesture features are likely to be irrelevant.
The identification of these movement phases
should be independent of the specific problem of
coreference resolution. Thus, additional labels for
other linguistic phenomena (e.g., topic segmenta-
tion, disfluency) could be combined into the model.
Ideally, each additional set of labels would transfer
performance gains to the other labeling problems.
9 Conclusions
We have presented a new method for combining
multiple modalities, which we feel is especially rel-
evant to non-verbal modalities that are used to com-
municate only intermittently. Our model treats the
relevance of the non-verbal modality as a hidden
variable, learned jointly with the class labels. Ap-
plied to coreference resolution, this model yields a
relative increase of 73% in the contribution of the
gesture features. This gain is attained by identify-
ing instances in which gesture features are especially
relevant, and weighing their contribution more heav-
ily. We next plan to investigate models with a tem-
poral component, so that the behavior of the hidden
variable is governed by a finite-state transducer.
Acknowledgments We thank Aaron Adler, Regina
Barzilay, S. R. K. Branavan, Sonya Cates, Erdong Chen,
Michael Collins, Lisa Guttentag, Michael Oltmans, and Tom
Ouyang. This research is supported in part by MIT Project Oxy-
gen.
References
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase corefer-
ence as clustering. In Proceedings of EMNLP, pages 82?89.
Lei Chen, Yang Liu, Mary P. Harper, and Elizabeth Shriberg.
2004. Multimodal model integration for sentence unit de-
tection. In Proceedings of ICMI, pages 121?128.
Jonathan Deutscher, Andrew Blake, and Ian Reid. 2000. Artic-
ulated body motion capture by annealed particle filtering. In
Proceedings of CVPR, volume 2, pages 126?133.
Usama M. Fayyad and Keki B. Irani. 1993. Multi-interval
discretization of continuousvalued attributes for classifica-
tion learning. In Proceedings of IJCAI-93, volume 2, pages
1022?1027. Morgan Kaufmann.
M.H. Goodwin and C. Goodwin. 1986. Gesture and co-
participation in the activity of searching for a word. Semiot-
ica, 62:51?75.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-7 coref-
erence task definition. In Proceedings of the Message Un-
derstanding Conference.
Xuedong Huang, Alex Acero, and Hsiao-Wuen Hon. 2001.
Spoken Language Processing. Prentice Hall.
Andrew Kehler. 2000. Cognitive status and form of reference
in multimodal human-computer interaction. In Proceedings
of AAAI, pages 685?690.
Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf.
2004. Detecting structural metadata with decision trees
and transformation-based learning. In Proceedings of HLT-
NAACL?04. ACL Press.
Jianhua Lin. 1991. Divergence measures based on the shannon
entropy. IEEE transactions on information theory, 37:145?
151.
Dong C. Liu and Jorge Nocedal. 1989. On the limited memory
BFGS method for large scale optimization. Mathematical
Programming, 45:503?528.
David McNeill. 1992. Hand and Mind. The University of
Chicago Press.
Alissa Melinger and Willem J. M. Levelt. 2004. Gesture and
communicative intention of the speaker. Gesture, 4(2):119?
141.
Ariadna Quattoni, Michael Collins, and Trevor Darrell. 2004.
Conditional random fields for object recognition. In Neural
Information Processing Systems, pages 1097?1104.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur, and
Gokhan Tur. 2000. Prosody-based automatic segmentation
of speech into sentences and topics. Speech Communication,
32.
Kentaro Toyama and Eric Horvitz. 2000. Bayesian modal-
ity fusion: Probabilistic integration of multiple vision al-
gorithms for head tracking. In Proceedings of ACCV ?00,
Fourth Asian Conference on Computer Vision.
359
Proceedings of ACL-08: HLT, pages 852?860,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Gestural Cohesion for Topic Segmentation
Jacob Eisenstein, Regina Barzilay and Randall Davis
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
77 Massachusetts Ave., Cambridge MA 02139
{jacobe, regina, davis}@csail.mit.edu
Abstract
This paper explores the relationship between
discourse segmentation and coverbal gesture.
Introducing the idea of gestural cohesion, we
show that coherent topic segments are char-
acterized by homogeneous gestural forms and
that changes in the distribution of gestural
features predict segment boundaries. Gestu-
ral features are extracted automatically from
video, and are combined with lexical features
in a Bayesian generative model. The resulting
multimodal system outperforms text-only seg-
mentation on both manual and automatically-
recognized speech transcripts.
1 Introduction
When people communicate face-to-face, discourse
cues are expressed simultaneously through multiple
channels. Previous research has extensively studied
how discourse cues correlate with lexico-syntactic
and prosodic features (Hearst, 1994; Hirschberg and
Nakatani, 1998; Passonneau and Litman, 1997); this
work informs various text and speech processing
applications, such as automatic summarization and
segmentation. Gesture is another communicative
modality that frequently accompanies speech, yet it
has not been exploited for computational discourse
analysis.
This paper empirically demonstrates that gesture
correlates with discourse structure. In particular,
we show that automatically-extracted visual fea-
tures can be combined with lexical cues in a sta-
tistical model to predict topic segmentation, a fre-
quently studied form of discourse structure. Our
method builds on the idea that coherent discourse
segments are characterized by gestural cohesion; in
other words, that such segments exhibit homoge-
neous gestural patterns. Lexical cohesion (Halliday
and Hasan, 1976) forms the backbone of many ver-
bal segmentation algorithms, on the theory that seg-
mentation boundaries should be placed where the
distribution of words changes (Hearst, 1994). With
gestural cohesion, we explore whether the same idea
holds for gesture features.
The motivation for this approach comes from a
series of psycholinguistic studies suggesting that
gesture supplements speech with meaningful and
unique semantic content (McNeill, 1992; Kendon,
2004). We assume that repeated patterns in gesture
are indicative of the semantic coherence that charac-
terizes well-defined discourse segments. An advan-
tage of this view is that gestures can be brought to
bear on discourse analysis without undertaking the
daunting task of recognizing and interpreting indi-
vidual gestures. This is crucial because coverbal
gesture ? unlike formal sign language ? rarely fol-
lows any predefined form or grammar, and may vary
dramatically by speaker.
A key implementational challenge is automati-
cally extracting gestural information from raw video
and representing it in a way that can applied to dis-
course analysis. We employ a representation of vi-
sual codewords, which capture clusters of low-level
motion patterns. For example, one codeword may
correspond to strong left-right motion in the up-
per part of the frame. These codewords are then
treated similarly to lexical items; our model iden-
tifies changes in their distribution, and predicts topic
852
boundaries appropriately. The overall framework is
implemented as a hierarchical Bayesian model, sup-
porting flexible integration of multiple knowledge
sources.
Experimental results support the hypothesis that
gestural cohesion is indicative of discourse struc-
ture. Applying our algorithm to a dataset of face-
to-face dialogues, we find that gesture commu-
nicates unique information, improving segmenta-
tion performance over lexical features alone. The
positive impact of gesture is most pronounced
when automatically-recognized speech transcripts
are used, but gestures improve performance by a
significant margin even in combination with manual
transcripts.
2 Related Work
Gesture and discourse Much of the work on ges-
ture in natural language processing has focused
on multimodal dialogue systems in which the ges-
tures and speech may be constrained, e.g. (Johnston,
1998). In contrast, we focus on improving discourse
processing on unconstrained natural language be-
tween humans. This effort follows basic psycho-
logical and linguistic research on the communicative
role of gesture (McNeill, 1992; Kendon, 2004), in-
cluding some efforts that made use of automatically
acquired visual features (Quek, 2003). We extend
these empirical studies with a statistical model of the
relationship between gesture and discourse segmen-
tation.
Hand-coded descriptions of body posture shifts
and eye gaze behavior have been shown to correlate
with topic and turn boundaries in task-oriented dia-
logue (Cassell et al, 2001). These findings are ex-
ploited to generate realistic conversational ?ground-
ing? behavior in an animated agent. The seman-
tic content of gesture was leveraged ? again, for
gesture generation ? in (Kopp et al, 2007), which
presents an animated agent that is capable of aug-
menting navigation directions with gestures that de-
scribe the physical properties of landmarks along
the route. Both systems generate plausible and
human-like gestural behavior; we address the con-
verse problem of interpreting such gestures.
In this vein, hand-coded gesture features have
been used to improve sentence segmentation, show-
ing that sentence boundaries are unlikely to over-
lap gestures that are in progress (Chen et al, 2006).
Features that capture the start and end of gestures
are shown to improve sentence segmentation beyond
lexical and prosodic features alone. This idea of ges-
tural features as a sort of visual punctuation has par-
allels in the literature on prosody, which we discuss
in the next subsection.
Finally, ambiguous noun phrases can be resolved
by examining the similarity of co-articulated ges-
tures (Eisenstein and Davis, 2007). While noun
phrase coreference can be viewed as a discourse pro-
cessing task, we address the higher-level discourse
phenomenon of topic segmentation. In addition, this
prior work focused primarily on pointing gestures
directed at pre-printed visual aids. The current pa-
per presents a new domain, in which speakers do not
have access to visual aids. Thus pointing gestures
are less frequent than ?iconic? gestures, in which the
form of motion is the principle communicative fea-
ture (McNeill, 1992).
Non-textual features for topic segmentation Re-
search on non-textual features for topic segmenta-
tion has primarily focused on prosody, under the as-
sumption that a key prosodic function is to mark
structure at the discourse level (Steedman, 1990;
Grosz and Hirshberg, 1992; Swerts, 1997). The ul-
timate goal of this research is to find correlates of
hierarchical discourse structure in phonetic features.
Today, research on prosody has converged on
prosodic cues which correlate with discourse struc-
ture. Such markers include pause duration, fun-
damental frequency, and pitch range manipula-
tions (Grosz and Hirshberg, 1992; Hirschberg and
Nakatani, 1998). These studies informed the devel-
opment of applications such as segmentation tools
for meeting analysis, e.g. (Tur et al, 2001; Galley et
al., 2003).
In comparison, the connection between gesture
and discourse structure is a relatively unexplored
area, at least with respect to computational ap-
proaches. One conclusion that emerges from our
analysis is that gesture may signal discourse struc-
ture in a different way than prosody does: while spe-
cific prosodic markers characterize segment bound-
aries, gesture predicts segmentation through intra-
segmental cohesion. The combination of these two
853
modalities is an exciting direction for future re-
search.
3 Visual Features for Discourse Analysis
This section describes the process of building a rep-
resentation that permits the assessment of gestural
cohesion. The core signal-level features are based
on spatiotemporal interest points, which provide a
sparse representation of the motion in the video. At
each interest point, visual, spatial, and kinematic
characteristics are extracted and then concatenated
into vectors. Principal component analysis (PCA)
reduces the dimensionality to a feature vector of
manageable size (Bishop, 2006). These feature vec-
tors are then clustered, yielding a codebook of visual
forms. This video processing pipeline is shown in
Figure 1; the remainder of the section describes the
individual steps in greater detail.
3.1 Spatiotemporal Interest Points
Spatiotemporal interest points (Laptev, 2005) pro-
vide a sparse representation of motion in video. The
idea is to select a few local regions that contain high
information content in both the spatial and tempo-
ral dimensions. The image features at these regions
should be relatively robust to lighting and perspec-
tive changes, and they should capture the relevant
movement in the video. The set of spatiotemporal
interest points thereby provides a highly compressed
representation of the key visual features. Purely spa-
tial interest points have been successful in a variety
of image processing tasks (Lowe, 1999), and spa-
tiotemporal interest points are beginning to show
similar advantages for video processing (Laptev,
2005).
The use of spatiotemporal interest points is specif-
ically motivated by techniques from the computer
vision domain of activity recognition (Efros et al,
2003; Niebles et al, 2006). The goal of activity
recognition is to classify video sequences into se-
mantic categories: e.g., walking, running, jumping.
As a simple example, consider the task of distin-
guishing videos of walking from videos of jump-
ing. In the walking videos, the motion at most of
the interest points will be horizontal, while in the
jumping videos it will be vertical. Spurious vertical
motion in a walking video is unlikely to confuse the
classifier, as long as the majority of interest points
move horizontally. The hypothesis of this paper is
that just as such low-level movement features can be
applied in a supervised fashion to distinguish activi-
ties, they can be applied in an unsupervised fashion
to group co-speech gestures into perceptually mean-
ingful clusters.
The Activity Recognition Toolbox (Dolla?r et al,
2005)1 is used to detect spatiotemporal interest
points for our dataset. This toolbox ranks interest
points using a difference-of-Gaussians filter in the
spatial dimension, and a set of Gabor filters in the
temporal dimension. The total number of interest
points extracted per video is set to equal the number
of frames in the video. This bounds the complexity
of the representation to be linear in the length of the
video; however, the system may extract many inter-
est points in some frames and none in other frames.
Figure 2 shows the interest points extracted from
a representative video frame from our corpus. Note
that the system has identified high contrast regions
of the gesturing hand. From manual inspection,
the large majority of interest points extracted in our
dataset capture motion created by hand gestures.
Thus, for this dataset it is reasonable to assume that
an interest point-based representation expresses the
visual properties of the speakers? hand gestures. In
videos containing other sources of motion, prepro-
cessing may be required to filter out interest points
that are extraneous to gestural communication.
3.2 Visual Descriptors
At each interest point, the temporal and spatial
brightness gradients are constructed across a small
space-time volume of nearby pixels. Brightness gra-
dients have been used for a variety of problems in
computer vision (Forsyth and Ponce, 2003), and pro-
vide a fairly general way to describe the visual ap-
pearance of small image patches. However, even for
a small space-time volume, the resulting dimension-
ality is still quite large: a 10-by-10 pixel box across 5
video frames yields a 500-dimensional feature vec-
tor for each of the three gradients. For this reason,
principal component analysis (Bishop, 2006) is used
to reduce the dimensionality. The spatial location of
the interest point is added to the final feature vector.
1http://vision.ucsd.edu/?pdollar/research/cuboids doc/index.html
854
Figure 1: The visual processing pipeline for the extraction of gestural codewords from video.
Figure 2: Circles indicate the interest points extracted
from this frame of the corpus.
This visual feature representation is substantially
lower-level than the descriptions of gesture form
found in both the psychology and computer science
literatures. For example, when manually annotat-
ing gesture, it is common to employ a taxonomy
of hand shapes and trajectories, and to describe the
location with respect to the body and head (Mc-
Neill, 1992; Martell, 2005). Working with automatic
hand tracking, Quek (2003) automatically computes
perceptually-salient gesture features, such as sym-
metric motion and oscillatory repetitions.
In contrast, our feature representation takes the
form of a vector of continuous values and is not eas-
ily interpretable in terms of how the gesture actu-
ally appears. However, this low-level approach of-
fers several important advantages. Most critically,
it requires no initialization and comparatively little
tuning: it can be applied directly to any video with a
fixed camera position and static background. Sec-
ond, it is robust: while image noise may cause a
few spurious interest points, the majority of inter-
est points should still guide the system to an appro-
priate characterization of the gesture. In contrast,
hand tracking can become irrevocably lost, requiring
manual resets (Gavrila, 1999). Finally, the success
of similar low-level interest point representations at
the activity-recognition task provides reason for op-
timism that they may also be applicable to unsuper-
vised gesture analysis.
3.3 A Lexicon of Visual Forms
After extracting a set of low-dimensional feature
vectors to characterize the visual appearance at each
spatiotemporal interest point, it remains only to
convert this into a representation amenable to a
cohesion-based analysis. Using k-means cluster-
ing (Bishop, 2006), the feature vectors are grouped
into codewords: a compact, lexicon-like representa-
tion of salient visual features in video. The number
of clusters is a tunable parameter, though a system-
atic investigation of the role of this parameter is left
for future work.
Codewords capture frequently-occurring patterns
of motion and appearance at a local scale ? interest
points that are clustered together have a similar vi-
sual appearance. Because most of the motion in our
videos is gestural, the codewords that appear during
a given sentence provide a succinct representation of
the ongoing gestural activity. Distributions of code-
words over time can be analyzed in similar terms
to the distribution of lexical features. A change in
the distribution of codewords indicates new visual
kinematic elements entering the discourse. Thus, the
codeword representation allows gestural cohesion to
be assessed in much the same way as lexical cohe-
sion.
4 Bayesian Topic Segmentation
Topic segmentation is performed in a Bayesian
framework, with each sentence?s segment index en-
coded in a hidden variable, written zt. The hidden
variables are assumed to be generated by a linear
segmentation, such that zt ? {zt?1, zt?1 + 1}. Ob-
servations ? the words and gesture codewords ? are
855
generated by multinomial language models that are
indexed according to the segment. In this frame-
work, a high-likelihood segmentation will include
language models that are tightly focused on a com-
pact vocabulary. Such a segmentation maximizes
the lexical cohesion of each segment. This model
thus provides a principled, probabilistic framework
for cohesion-based segmentation, and we will see
that the Bayesian approach is particularly well-
suited to the combination of multiple modalities.
Formally, our goal is to identify the best possible
segmentation S, where S is a tuple: S = ?z, ?, ??.
The segment indices for each sentence are written
zt; for segment i, ?i and ?i are multinomial lan-
guage models over words and gesture codewords re-
spectively. For each sentence, xt and yt indicate
the words and gestures that appear. We will seek to
identify the segmentation S? = argmaxSp(S,x,y),
conditioned on priors that will be defined below.
p(S,x,y) = p(x,y|S)p(S)
p(x,y|S) =
?
i
p({xt : zt = i}|?i)p({yt : zt = i}|?i)
(1)
p(S) = p(z)
?
i
p(?i)p(?i) (2)
The language models ?i and ?i are multinomial
distributions, so the log-likelihood of the obser-
vations xt is log p(xt|?i) =
?W
j n(t, j) log ?i,j ,
where n(t, j) is the count of word j in sentence t,
and W is the size of the vocabulary. An analogous
equation is used for the gesture codewords. Each
language model is given a symmetric Dirichlet prior
?. As we will see shortly, the use of different pri-
ors for the verbal and gestural language models al-
lows us to weight these modalities in a Bayesian
framework. Finally, we model the probability of
the segmentation z by considering the durations of
each segment: p(z) =
?
i p(dur(i)|?). A negative-
binomial distribution with parameter ? is applied to
discourage extremely short or long segments.
Inference Crucially, both the likelihood (equa-
tion 1) and the prior (equation 2) factor into a prod-
uct across the segments. This factorization en-
ables the optimal segmentation to be found using
a dynamic program, similar to those demonstrated
by Utiyama and Isahara (2001) and Malioutov and
Barzilay (2006). For each set of segmentation points
z, the associated language models are set to their
posterior expectations, e.g., ?i = E[?|{xt : zt =
i}, ?].
The Dirichlet prior is conjugate to the multino-
mial, so this expectation can be computed in closed
form:
?i,j =
n(i, j) + ?
N(i) +W?
, (3)
where n(i, j) is the count of word j in segment
i and N(i) is the total number of words in seg-
ment i (Bernardo and Smith, 2000). The symmetric
Dirichlet prior ? acts as a smoothing pseudo-count.
In the multimodal context, the priors act to control
the weight of each modality. If the prior for the ver-
bal language model ? is high relative to the prior for
the gestural language model ? then the verbal multi-
nomial will be smoother, and will have a weaker im-
pact on the final segmentation. The impact of the
priors on the weights of each modality is explored
in Section 6.
Estimation of priors The distribution over seg-
ment durations is negative-binomial, with parame-
ters ?. In general, the maximum likelihood estimate
of the parameters of a negative-binomial distribu-
tion cannot be found in closed form (Balakrishnan
and Nevzorov, 2003). For any given segmentation,
the maximum-likelihood setting for ? is found via
a gradient-based search. This setting is then used
to generate another segmentation, and the process
is iterated until convergence, as in hard expectation-
maximization. The Dirichlet priors on the language
models are symmetric, and are chosen via cross-
validation. Sampling or gradient-based techniques
may be used to estimate these parameters, but this is
left for future work.
Relation to other segmentation models Other
cohesion-based techniques have typically focused
on hand-crafted similarity metrics between sen-
tences, such as cosine similarity (Galley et al, 2003;
Malioutov and Barzilay, 2006). In contrast, the
model described here is probabilistically motivated,
maximizing the joint probability of the segmentation
with the observed words and gestures. Our objec-
tive criterion is similar in form to that of Utiyama
and Isahara (2001); however, in contrast to this prior
856
work, our criterion is justified by a Bayesian ap-
proach. Also, while the smoothing in our approach
arises naturally from the symmetric Dirichlet prior,
Utiyama and Isahara apply Laplace?s rule and add
pseudo-counts of one in all cases. Such an approach
would be incapable of flexibly balancing the contri-
butions of each modality.
5 Evaluation Setup
Dataset Our dataset is composed of fifteen audio-
video recordings of dialogues limited to three min-
utes in duration. The dataset includes nine differ-
ent pairs of participants. In each video one of five
subjects is discussed. The potential subjects include
a ?Tom and Jerry? cartoon, a ?Star Wars? toy, and
three mechanical devices: a latchbox, a piston, and
a candy dispenser. One participant ? ?participant A?
? was familiarized with the topic, and is tasked with
explaining it to participant B, who is permitted to
ask questions. Audio from both participants is used,
but only video of participant A is used; we do not ex-
amine whether B?s gestures are relevant to discourse
segmentation.
Video was recorded using standard camcorders,
with a resolution of 720 by 480 at 30 frames per
second. The video was reduced to 360 by 240 gray-
scale images before visual analysis is applied. Audio
was recorded using headset microphones. No man-
ual postprocessing is applied to the video.
Annotations and data processing All speech was
transcribed by hand, and time stamps were obtained
using the SPHINX-II speech recognition system for
forced alignment (Huang et al, 1993). Sentence
boundaries are annotated according to (NIST, 2003),
and additional sentence boundaries are automati-
cally inserted at all turn boundaries. Commonly-
occurring terms unlikely to impact segmentation are
automatically removed by using a stoplist.
For automatic speech recognition, the default Mi-
crosoft speech recognizer was applied to each sen-
tence, and the top-ranked recognition result was re-
ported. As is sometimes the case in real-world ap-
plications, no speaker-specific training data is avail-
able. The resulting recognition quality is very poor,
yielding a word error rate of 77%.
Annotators were instructed to select segment
boundaries that divide the dialogue into coherent
topics. Segmentation points are required to coincide
with sentence or turn boundaries. A second annota-
tor ? who is not an author on any paper connected
with this research ? provided an additional set of
segment annotations on six documents. On this sub-
set of documents, the Pk between annotators was
.306, and the WindowDiff was .325 (these metrics
are explained in the next subsection). This is simi-
lar to the interrater agreement reported byMalioutov
and Barzilay (2006).
Over the fifteen dialogues, a total of 7458 words
were transcribed (497 per dialogue), spread over
1440 sentences or interrupted turns (96 per dia-
logue). There were a total of 102 segments (6.8
per dialogue), from a minimum of four to a maxi-
mum of ten. This rate of fourteen sentences or in-
terrupted turns per segment indicates relatively fine-
grained segmentation. In the physics lecture corpus
used by Malioutov and Barzilay (2006), there are
roughly 100 sentences per segment. On the ICSI
corpus of meeting transcripts, Galley et al (2003)
report 7.5 segments per meeting, with 770 ?poten-
tial boundaries,? suggesting a similar rate of roughly
100 sentences or interrupted turns per segment.
The size of this multimodal dataset is orders of
magnitude smaller than many other segmentation
corpora. For example, the Broadcast News corpus
used by Beeferman et al (1999) and others con-
tains two million words. The entire ICSI meeting
corpus contains roughly 600,000 words, although
only one third of this dataset was annotated for seg-
mentation (Galley et al, 2003). The physics lecture
corpus that was mentioned above contains 232,000
words (Malioutov and Barzilay, 2006). The task
considered in this section is thus more difficult than
much of the previous discourse segmentation work
on two dimensions: there is less training data, and a
finer-grained segmentation is required.
Metrics All experiments are evaluated in terms
of the commonly-used Pk (Beeferman et al, 1999)
and WindowDiff (WD) (Pevzner and Hearst, 2002)
scores. These metrics are penalties, so lower val-
ues indicate better segmentations. The Pk metric
expresses the probability that any randomly chosen
pair of sentences is incorrectly segmented, if they
are k sentences apart (Beeferman et al, 1999). Fol-
lowing tradition, k is set to half of the mean seg-
857
Method Pk WD
1. gesture only .486 .502
2. ASR only .462 .476
3. ASR + gesture .388 .401
4. transcript only .382 .397
5. transcript + gesture .332 .349
6. random .473 .526
7. equal-width .508 .515
Table 1: For each method, the score of the best perform-
ing configuration is shown. Pk and WD are penalties, so
lower values indicate better performance.
ment length. The WindowDiff metric is a varia-
tion of Pk (Pevzner and Hearst, 2002), applying a
penalty whenever the number of segments within the
k-sentence window differs for the reference and hy-
pothesized segmentations.
Baselines Two na??ve baselines are evaluated.
Given that the annotator has divided the dialogue
into K segments, the random baseline arbitrary
chooses K random segmentation points. The re-
sults of this baseline are averaged over 1000 itera-
tions. The equal-width baseline places boundaries
such that all segments contain an equal number of
sentences. Both the experimental systems and these
na??ve baselines were given the correct number of
segments, and also were provided with manually an-
notated sentence boundaries ? their task is to select
the k sentence boundaries that most accurately seg-
ment the text.
6 Results
Table 1 shows the segmentation performance for a
range of feature sets, as well as the two baselines.
Given only gesture features the segmentation results
are poor (line 1), barely outperforming the baselines
(lines 6 and 7). However, gesture proves highly ef-
fective as a supplementary modality. The combina-
tion of gesture with ASR transcripts (line 3) yields
an absolute 7.4% improvement over ASR transcripts
alone (line 4). Paired t-tests show that this result
is statistically significant (t(14) = 2.71, p < .01
for both Pk and WindowDiff). Even when man-
ual speech transcripts are available, gesture features
yield a substantial improvement, reducing Pk and
WD by roughly 5%. This result is statistically sig-
nificant for both Pk (t(14) = 2.00, p < .05) and
WD (t(14) = 1.94, p < .05).
Interactions of verbal and gesture features We
now consider the relative contribution of the verbal
and gesture features. In a discriminative setting, the
contribution of each modality would be explicitly
weighted. In a Bayesian generative model, the same
effect is achieved through the Dirichlet priors, which
act to smooth the verbal and gestural multinomials ?
see equation 3. For example, when the gesture prior
is high and verbal prior is low, the gesture counts are
smoothed, and the verbal counts play a greater role
in segmentation. When both priors are very high,
the model will simply try to find equally-sized seg-
ments, satisfying the distribution over durations.
The effects of these parameters can be seen in Fig-
ure 3. The gesture model prior is held constant at
its ideal value, and the segmentation performance
is plotted against the logarithm of the verbal prior.
Low values of the verbal prior cause it to domi-
nate the segmentation; this can be seen at the left
of both graphs, where the performance of the multi-
modal and verbal-only systems are nearly identical.
High values of the verbal prior cause it to be over-
smoothed, and performance thus approaches that of
the gesture-only segmenter.
Comparison to other models While much of
the research on topic segmentation focuses on writ-
ten text, there are some comparable systems that
also aim at unsupervised segmentation of sponta-
neous spoken language. For example, Malioutov
and Barzilay (2006) segment a corpus of classroom
lectures, using similar lexical cohesion-based fea-
tures. With manual transcriptions, they report a .383
Pk and .417 WD on artificial intelligence (AI) lec-
tures, and .298 Pk and .311 WD on physics lectures.
Our results are in the range bracketed by these two
extremes; the wide range of results suggests that seg-
mentation scores are difficult to compare across do-
mains. The segmentation of physics lectures was at
a very course level of granularity, while the segmen-
tation of AI lectures was more similar to our anno-
tations.
We applied the publicly-available executable for
this algorithm to our data, but performance was
poor, yielding a .417 Pk and .465 WD even when
both verbal and gestural features were available.
858
?3 ?2.5 ?2 ?1.5 ?1 ?0.5
0.32
0.34
0.36
0.38
0.4
0.42
log verbal prior
Pk
 
 
verbal?only
multimodal
?3 ?2.5 ?2 ?1.5 ?1 ?0.5
0.32
0.34
0.36
0.38
0.4
0.42
log verbal prior
W
D
 
 
verbal?only
multimodal
Figure 3: The multimodal and verbal-only performance using the reference transcript. The x-axis shows the logarithm
of the verbal prior; the gestural prior is held fixed at the optimal value.
This may be because the technique is not de-
signed for the relatively fine-grained segmentation
demanded by our dataset (Malioutov, 2006).
7 Conclusions
This research shows a novel relationship between
gestural cohesion and discourse structure. Automat-
ically extracted gesture features are predictive of dis-
course segmentation when used in isolation; when
lexical information is present, segmentation perfor-
mance is further improved. This suggests that ges-
tures provide unique information not present in the
lexical features alone, even when perfect transcripts
are available.
There are at least two possibilities for how ges-
ture might impact topic segmentation: ?visual punc-
tuation,? and cohesion. The visual punctuation view
would attempt to identify specific gestural patterns
that are characteristic of segment boundaries. This
is analogous to research that identifies prosodic sig-
natures of topic boundaries, such as (Hirschberg and
Nakatani, 1998). By design, our model is incapable
of exploiting such phenomena, as our goal is to in-
vestigate the notion of gestural cohesion. Thus, the
performance gains demonstrated in this paper can-
not be explained by such punctuation-like phenom-
ena; we believe that they are due to the consistent
gestural themes that characterize coherent topics.
However, we are interested in pursuing the idea of
visual punctuation in the future, so as to compare the
power of visual punctuation and gestural cohesion
to predict segment boundaries. In addition, the in-
teraction of gesture and prosody suggests additional
possibilities for future research.
The videos in the dataset for this paper are fo-
cused on the description of physical devices and
events, leading to a fairly concrete set of gestures.
In other registers of conversation, gestural form may
be driven more by spatial metaphors, or may con-
sist mainly of temporal ?beats.? In such cases, the
importance of gestural cohesion for discourse seg-
mentation may depend on the visual expressivity of
the speaker. We plan to examine the extensibility of
gesture cohesion to more naturalistic settings, such
as classroom lectures.
Finally, topic segmentation provides only an out-
line of the discourse structure. Richer models of dis-
course include hierarchical structure (Grosz and Sid-
ner, 1986) and Rhetorical Structure Theory (Mann
and Thompson, 1988). The application of gestural
analysis to such models may lead to fruitful areas of
future research.
Acknowledgments
We thank Aaron Adler, C. Mario Christoudias,
Michael Collins, Lisa Guttentag, Igor Malioutov,
Brian Milch, Matthew Rasmussen, Candace Sidner,
Luke Zettlemoyer, and the anonymous reviewers.
This research was supported by Quanta Computer,
the National Science Foundation (CAREER grant
IIS-0448168 and grant IIS-0415865) and the Mi-
crosoft Research Faculty Fellowship.
859
References
Narayanaswamy Balakrishnan and Valery B. Nevzorov.
2003. A primer on statistical distributions. John Wi-
ley & Sons.
Doug Beeferman, Adam Berger, and John D. Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177?210.
Jose? M. Bernardo and Adrian F. M. Smith. 2000.
Bayesian Theory. Wiley.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning. Springer.
Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-
more, Candace L. Sidner, and Charles Rich. 2001.
Non-verbal cues for discourse structure. In Proceed-
ings of ACL, pages 106?115.
Lei Chen, Mary Harper, and Zhongqiang Huang. 2006.
Using maximum entropy (ME) model to incorporate
gesture cues for sentence segmentation. In Proceed-
ings of ICMI, pages 185?192.
Piotr Dolla?r, Vincent Rabaud, Garrison Cottrell, and
Serge Belongie. 2005. Behavior recognition via
sparse spatio-temporal features. In ICCV VS-PETS.
Alexei A. Efros, Alexander C. Berg, Greg Mori, and Ji-
tendra Malik. 2003. Recognizing action at a distance.
In Proceedings of ICCV, pages 726?733.
Jacob Eisenstein and Randall Davis. 2007. Conditional
modality fusion for coreference resolution. In Pro-
ceedings of ACL, pages 352?359.
David A. Forsyth and Jean Ponce. 2003. Computer Vi-
sion: A Modern Approach. Prentice Hall.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. Proceedings of
ACL, pages 562?569.
Dariu M. Gavrila. 1999. Visual analysis of human move-
ment: A survey. Computer Vision and Image Under-
standing, 73(1):82?98.
Barbara Grosz and Julia Hirshberg. 1992. Some into-
national characteristics of discourse structure. In Pro-
ceedings of ICSLP, pages 429?432.
Barbara Grosz and Candace Sidner. 1986. Attention,
intentions, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohesion
in English. Longman.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL.
Julia Hirschberg and Christine Nakatani. 1998. Acoustic
indicators of topic segmentation. In Proceedings of
ICSLP.
Xuedong Huang, Fileno Alleva, Mei-Yuh Hwang, and
Ronald Rosenfeld. 1993. An overview of the Sphinx-
II speech recognition system. In Proceedings of ARPA
Human Language Technology Workshop, pages 81?
86.
Michael Johnston. 1998. Unification-based multimodal
parsing. In Proceedings of COLING, pages 624?630.
Adam Kendon. 2004. Gesture: Visible Action as Utter-
ance. Cambridge University Press.
Stefan Kopp, Paul Tepper, Kim Ferriman, and Justine
Cassell. 2007. Trading spaces: How humans and hu-
manoids use speech and gesture to give directions. In
Toyoaki Nishida, editor, Conversational Informatics:
An Engineering Approach. Wiley.
Ivan Laptev. 2005. On space-time interest points. In-
ternational Journal of Computer Vision, 64(2-3):107?
123.
David G. Lowe. 1999. Object recognition from local
scale-invariant features. In Proceedings of ICCV, vol-
ume 2, pages 1150?1157.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25?32.
Igor Malioutov. 2006. Minimum cut model for spoken
lecture segmentation. Master?s thesis, Massachusetts
Institute of Technology.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8:243?281.
Craig Martell. 2005. FORM: An experiment in the anno-
tation of the kinematics of gesture. Ph.D. thesis, Uni-
versity of Pennsylvania.
David McNeill. 1992. Hand and Mind. The University
of Chicago Press.
Juan Carlos Niebles, Hongcheng Wang, and Li Fei-Fei.
2006. Unsupervised Learning of Human Action Cate-
gories Using Spatial-Temporal Words. In Proceedings
of the British Machine Vision Conference.
NIST. 2003. The Rich Transcription Fall 2003 (RT-03F)
Evaluation plan.
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1):103?139.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28(1):19?36.
Francis Quek. 2003. The catchment feature model
for multimodal language analysis. In Proceedings of
ICCV.
Mark Steedman. 1990. Structure and intonation in spo-
ken language understanding. In Proceedings of ACL,
pages 9?16.
Marc Swerts. 1997. Prosodic features at discourse
boundaries of different strength. The Journal of the
Acoustical Society of America, 101:514.
Gokhan Tur, Dilek Hakkani-Tur, Andreas Stolcke, and
Elizabeth Shriberg. 2001. Integrating prosodic and
lexical cues for automatic topic segmentation. Com-
putational Linguistics, 27(1):31?57.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
Proceedings of ACL, pages 491?498.
860
