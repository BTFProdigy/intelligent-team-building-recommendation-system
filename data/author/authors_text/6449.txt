Evaluating Cross-Language Annotation Transfer in the MultiSemCor Corpus  
Luisa BENTIVOGLI, Pamela FORNER, Emanuele PIANTA 
ITC-irst 
Via Sommarive, 18 
38050 Povo ? Trento 
 Italy 
{bentivo, forner, pianta}@itc.it 
 
Abstract 
In this paper we illustrate and evaluate an approach 
to the creation of high quality linguistically 
annotated resources based on the exploitation of 
aligned parallel corpora. This approach is based on 
the assumption that if a text in one language has 
been annotated and its translation has not, 
annotations can be transferred from the source text 
to the target using word alignment as a bridge. The 
transfer approach has been tested in the creation of 
the MultiSemCor corpus, an English/Italian 
parallel corpus created on the basis of the English 
SemCor corpus. In MultiSemCor texts are aligned 
at the word level and semantically annotated with a 
shared inventory of senses. We present some 
experiments carried out to evaluate the different 
steps involved in the methodology. The results of 
the evaluation suggest that the cross-language 
annotation transfer methodology is a promising 
solution allowing for the exploitation of existing 
(mostly English) annotated resources to bootstrap 
the creation of annotated corpora in new (resource-
poor) languages with greatly reduced human effort.  
1 Introduction 
Large-scale language resources play a crucial role 
for a steady progress in the field of Natural 
Language Processing (NLP), as they are essential 
for carrying out basic research and for building 
portable and robust systems with broad coverage. 
More specifically, given the advances of machine 
learning statistical methods for NLP, with 
supervised training methods leading the way to 
major improvements in performance on different 
tasks, a particularly valuable resource is now 
represented by large linguistically annotated 
corpora. 
Up until some years ago, linguistically annotated 
corpora were only produced through manual 
annotation, or by manual check of automatically 
produced annotations. Unfortunately, manual 
annotation is a very difficult and time-consuming 
task, and this fact has led to a shortage of manual-
quality annotated data. The scarcity of large size 
annotated corpora is more acute for languages 
different from English, for which even minimal 
amounts of data are still missing. This state of 
affairs makes it clear that any endeavour aiming at 
reducing the human effort needed to produce 
manual-quality labelled data will be highly 
beneficial to the field. 
Recent studies have shown that a valuable 
opportunity for breaking the annotated resource 
bottleneck is represented by parallel corpora, 
which can be exploited in the creation of resources 
for new languages via projection of annotations 
available in another language. This paper 
represents our contribution to the research in this 
field. We present a novel methodology to create a 
semantically annotated corpus by exploiting 
information contained in an already annotated 
corpus, using word alignment as a bridge. The 
methodology has been applied in the creation of 
the MultiSemCor corpus. MultiSemCor is an 
English/Italian parallel corpus which is being 
created on the basis of the English SemCor corpus 
and where the texts are aligned at the word level 
and semantically annotated with a shared inventory 
of senses.  
Given the promising results of a pilot study 
presented in (Bentivogli and Pianta, 2002), the 
MultiSemCor corpus is now under development. In 
this paper we focus on a thorough evaluation of the 
steps involved in the transfer methodology. We 
evaluate the performance of a new version of the 
word alignment system and the final quality of the 
annotations transferred from English to Italian. In 
Section 2 we lay out the annotation transfer 
methodology and summarize some related work. In 
Section 3 we discuss some problematic issues 
related to the methodology which will be 
extensively tested and evaluated in Section 4. In 
Section 5 we report about the state of development 
of the MultiSemCor corpus and, finally, in Section 
6 we present conclusions and our thoughts on 
future work. 
2 The Annotation Transfer Methodology 
The MultiSemCor project (Bentivogli and Pianta, 
2002) aims at building an English/Italian parallel 
corpus, aligned at the word level and annotated 
with PoS, lemma and word sense. The parallel 
corpus is created by exploiting the SemCor corpus 
(Landes et al, 1998), which is a subset of the 
English Brown corpus containing about 700,000 
running words. In SemCor all the words are tagged 
by PoS, and more than 200,000 content words are 
also lemmatized and sense-tagged with reference 
to the WordNet lexical database1 (Fellbaum, 1998).  
The main hypothesis underlying this 
methodology is that, given a text and its translation 
into another language, the semantic information is 
mostly preserved during the translation process. 
Therefore, if the texts in one language have been 
semantically annotated and their translations have 
not, annotations can be transferred from the source 
language to the target using word alignment as a 
bridge.  
The first problem to be solved in the creation of 
MultiSemCor was the fact that the Italian 
translations of the SemCor texts did not exist. Our 
solution was to have the translations made by 
professional translators. Given the high costs of 
building semantically annotated corpora, requiring 
specific skills and very specialized training, we 
think that manually translating the annotated 
corpus and automatically transferring the 
annotations may be preferable to hand-labelling a 
corpus from scratch. Not only are translators more 
easily available than linguistic annotators, but 
translations may be a more flexible and durable 
kind of annotation. Moreover, the annotation 
transfer methodology has the further advantage of 
producing a parallel corpus.  
With respect to a situation in which the 
translation of a corpus is already available, a 
corpus translated on purpose presents the 
advantage that translations can be ?controlled?, 
i.e. carried out following criteria aiming at 
maximizing alignment and annotation transfer. Our 
professional translators are asked to use, 
preferably, the same dictionaries used by the word 
aligner, and to maximize, whenever possible, the 
lexical correspondences between source and target 
texts. The translators are also told that the 
controlled translation criteria should never be 
followed to the detriment of a good Italian prose. 
Controlled translations cost the same as free 
translations, while having the advantage of 
                                                     
1
 WordNet is an English lexical database, developed 
at Princeton University, in which nouns, verbs, 
adjectives, and adverbs are organized into sets of 
synonyms (synsets) and linked to each other by means 
of various lexical and semantic relationships. In the last 
years, within the NLP community WordNet has become 
the reference lexicon for almost all tasks involving word 
sense disambiguation (see, for instance, the Senseval 
competition). 
 
enhancing the performances of the annotation 
transfer procedure. 
Once the SemCor texts have been translated, the 
strategy for creating MultiSemCor consists of (i) 
automatically aligning Italian and English texts at 
the word level, and (ii) automatically transferring 
the word sense annotations from English to the 
aligned Italian words. The final result of the 
MultiSemCor project is an Italian corpus annotated 
with PoS, lemma and word sense, but also an 
aligned parallel corpus lexically annotated with a 
shared inventory of word senses. More 
specifically, the sense inventory used is 
MultiWordNet (Pianta et al, 2002), a multilingual 
lexical database in which the Italian component is 
strictly aligned with the English WordNet. 
2.1 Related Work 
The idea of obtaining linguistic information about 
a text in one language by exploiting parallel or 
comparable texts in another language has been 
explored in the field of Word Sense 
Disambiguation (WSD) since the early 90?s, the 
most representative works being (Brown et al, 
1991), (Gale et al, 1992), and (Dagan and Itai, 
1994).  
In more recent years, Ide et al (2002) present a 
method to identify word meanings starting from a 
multilingual corpus. A by-product of applying this 
method is that once a word in one language is 
word-sense tagged, the translation equivalents in 
the parallel texts are also automatically annotated. 
Cross-language tagging is the goal of the work 
by Diab and Resnik (2002), who present a method 
for word sense tagging both the source and target 
texts of parallel bilingual corpora with the 
WordNet sense inventory.  
Parallel to the studies regarding the projection of 
semantic information, more recently the NLP 
community has also explored the possibility of 
exploiting translation to project more syntax-
oriented annotations. Yarowsky et al (2001) 
describe a successful method consisting of (i) 
automatic annotation of English texts, (ii) cross-
language projection of annotations onto target 
language texts, and (iii) induction of noise-robust 
taggers for the target language. A further step is 
made in (Hwa et al, 2002) and (Cabezas et al, 
2001), which address the task of acquiring a 
dependency treebank by bootstrapping from 
existing linguistic resources for English. Finally, in 
(Riloff et al, 2002) a method is presented for 
rapidly creating Information Extraction (IE) 
systems for new languages by exploiting existing 
IE systems via cross-language projection. 
The results of all the above mentioned studies 
show how previous major investments in English 
annotated corpora and tool development can be 
effectively leveraged across languages, allowing 
the development of accurate resources and tools in 
other languages without comparable human effort. 
3 Quality Issues 
The MultiSemCor project raises a number of 
theoretical and practical issues. For instance: is 
translational language fully representative of the 
general use of language in the same way as 
original language is? To what extent are the lexica 
of different languages comparable? These 
theoretical issues have already been presented in 
(Pianta and Bentivogli, 2003) and will not be 
discussed here. In the following, we address the 
issue of the quality of the annotation resulting from 
the application of the methodology.  
As opposed to automatic word sense 
disambiguation tasks, the MultiSemCor project 
specifically aims at producing manual-quality 
annotated data. Therefore, a potential risk which 
needs to be faced is represented by the possible 
degradation of the Italian annotation quality 
through the various steps of the annotation transfer 
procedure. A number of factors must be taken into 
account. First, annotation errors can be found in 
the original English texts. Then, the word aligner 
may align words incorrectly, and finally the 
transfer of the semantic annotations may not be 
applicable to certain translation pairs.  
SemCor quality. The English SemCor corpus has 
been manually annotated. However, some 
annotation errors can be found in the texts (see 
Fellbaum et al, 1998, for SemCor taggers? 
confidence ratings). As an example, the word 
pocket in the sentence ?He put his hands on his 
pockets? was incorrectly tagged with the WordNet 
synset {pouch, sac, sack, pocket -- an enclosed 
space} instead of the correct one {pocket -- a small 
pouch in a garment for carrying small articles}. 
Word alignment quality. The feasibility of the 
entire MultiSemCor project heavily depends on the 
availability of an English/Italian word aligner with 
very good performance in terms of recall and, 
more importantly, precision.  
Transfer quality. Even when both the original 
English annotations and the word alignment are 
correct, a number of cases still remain for which 
the transfer of the annotation is not applicable. An 
annotation is not transferable from the source 
language to the target when the translation 
equivalent does not preserve the lexical meaning of 
the source language. In these cases, if the 
alignment process puts the two expressions in 
correspondence, then the transfer of the sense 
annotation from the source to the target language is 
not correct.  
The first main cause of incorrect transfer is 
represented by translation equivalents which are 
not cross-language synonyms of the source 
language words. For example, in a sentence of the 
corpus the English word meaning is translated with 
the Italian word motivo (reason, grounds) which is 
suitable in that specific context but is not a 
synonymic translation of the English word. In this 
case, if the two words are aligned, the transfer of 
the sense annotation from English is not correct as 
the English sense annotation is not suitable for the 
Italian word. A specific case of non-synonymous 
translation occurs when a translation equivalent 
does not belong to the same lexical category of the 
source word. For example, the English verb to 
coexist in the sentence ?the possibility for man to 
coexist with animals? has been translated with the 
Italian noun coesistenza (coexistence) in ?le 
possibilit? di coesistenza tra gli uomini e gli 
animali?. Even if the translation is suitable for that 
context, the English sense of the verb cannot be 
transferred to the Italian noun. Sometimes, non-
synonymous translations are due to errors in the 
Italian translation, as in pull translated as spingere 
(push).  
A second case which offers challenge to the 
sense annotation transfer is phrasal 
correspondence, occurring when a target phrase 
has globally the same meaning as the 
corresponding source phrase, but the single words 
of the phrase are not cross-language synonyms of 
their corresponding source words. For example, the 
expression a dreamer sees has been translated as 
una persona sogna (a person dreams). The Italian 
translation maintains the synonymy at the phrase 
level but the single component words do not. 
Therefore, if the single words were aligned any 
transfer from English to Italian would be incorrect. 
Another example of phrasal correspondence, in 
which the semantic equivalence between words in 
the source and target phrase is even fuzzier, is 
given by the English phrase the days would get 
shorter and shorter translated as imminente fine 
dei tempi (imminent end of times). 
Another controversial cause of possible incorrect 
transfer is represented by the case in which the 
translation equivalent is indeed a cross-language 
synonym of the source expression but it is not a 
lexical unit. This usually happens with lexical 
gaps, i.e. when a language expresses a concept 
with a lexical unit whereas the other language 
expresses the same concept with a free 
combination of words, as for instance the English 
word successfully which can only be translated 
with the Italian free combination of words con 
successo (with success). However, it can also be 
the result of a choice made by the translator who 
decides to use a free combination of words instead 
of a possible lexical unit, as in empirically 
translated as in modo empirico (in an empirical 
manner) instead of empiricamente. In these cases 
the problem arises because in principle if the target 
expression is not a lexical unit it cannot be 
annotated as a whole. On the contrary, each 
component of the free combination of words 
should be annotated with its respective sense. 
In the next Section we will address these quality 
issues in order to assess the extent to which they 
affect the cross-language annotation transfer 
methodology. 
4 Evaluation of the Annotation Transfer 
Methodology 
A number of experiments have been carried out in 
order to test the various steps involved in the 
annotation transfer methodology. More precisely, 
we evaluated the performances of the word 
alignment system and the quality of the final 
annotation of the Italian corpus. 
4.1 Word Alignment 
Word alignment is the first crucial step in the 
methodology applied to build MultiSemCor. The 
word aligner used in the project is KNOWA 
(KNOwledge-intensive Word Aligner), an 
English/Italian word aligner, developed at ITC-irst, 
which relies mostly on information contained in 
the Collins bilingual dictionary, available in 
electronic format. KNOWA also exploits a 
morphological analyzer and a multiword 
recognizer for both English and Italian. For a 
detailed discussion of the characteristics of this 
tool, see (Pianta and Bentivogli, 2004). 
Some characteristics of the MultiSemCor 
scenario make the alignment task easier for 
KNOWA. First, in SemCor all multiwords 
included in WordNet are explicitly marked. Thus 
KNOWA does not need to recognize English 
multiwords, although it still needs to recognize the 
Italian ones. Second, within MultiSemCor word 
alignment is done with the final aim of transferring 
lexical annotations from English to Italian. Since 
only content words have word sense annotations in 
SemCor, it is more important that KNOWA 
behaves correctly on content words, which are 
easier to align than functional words. 
To evaluate the word aligner performance on the 
MultiSemCor task we created a gold standard 
composed of three English unseen texts (br-f43, 
br-l10, br-j53) taken randomly from the 
SemCor corpus. For each English text both a 
controlled and a free translation were made. Given 
the expectation that free translations are less 
suitable for word alignment, we decided to test 
KNOWA also on them in order to verify if the 
annotation transfer methodology can be applied to 
already existing parallel corpora. 
The six resulting pairs of texts were manually 
aligned following a set of alignment guidelines 
which have been defined taking into account the 
work done in similar word alignment projects 
(Melamed, 2001). Annotators were asked to align 
different kinds of units (simple words, segments of 
more than one word, parts of words) and to mark 
different kinds of semantic correspondence 
between the aligned units, e.g. full correspondence 
(synonymic), non synonymic, changes in lexical 
category, phrasal correspondence. Inter-annotator 
agreement was measured with the Dice coefficient 
proposed in (V?ronis and Langlais, 2000) and can 
be considered satisfactory as it turned out to be 
87% for free translations and 92% for controlled 
translations. As expected, controlled translations 
produced a better agreement rate between 
annotators. 
For assessing the performance of KNOWA, the 
standard notions of Precision, Recall, and 
Coverage have been used following (V?ronis and 
Langlais, 2000). See (Och and Ney, 2003) and 
Arenberg et al, 2000) for different evaluation 
metrics. The performance of KNOWA applied to 
the MultiSemCor gold standard in a full-text 
alignment task is shown in Table 1. These results, 
which compare well with those reported in the 
literature (V?ronis, 2000) show that, as expected, 
controlled translations allow for a better alignment 
but also that free translations may be satisfactorily 
aligned.  
The evaluation of KNOWA with respect to the 
English content words which have a semantic tag 
in SemCor is reported in Tables 2 and 3, for both 
free and controlled translations and broken down 
by Part of Speech.  
 Precision Recall Coverage 
Free 83.5 57.9 60.0 
Controlled 88.4 67.5  74.9 
Table 1: KNOWA on Full-text  
 Precision Recall Coverage 
Nouns 93.7 81.1 86.5 
Verbs 85.6 70.3 82.1 
Adjectives 95.6 64.7 67.7 
Adverbs 88.4 38.5 43.5 
Total 91.2 68.2 74.8 
Table 2: KNOWA on sense-tagged words only 
(Free translations)  
 Precision Recall Coverage 
Nouns 95.9 82.5 86.1 
Verbs 90.7 76.8 84.7 
Adjectives 95.2 69.9 73.5 
Adverbs 90.4 51.6 57.1 
Total 93.9 74.6 79.5 
Table 3: KNOWA on sense-tagged words only 
(Controlled translations) 
We can see that ignoring function words the 
performance of the word aligner improves in both 
precision and recall. 
4.2 Italian Annotation Quality 
As pointed out in Section 3, even in the case of a 
perfect word alignment the transfer of the 
annotations from English to the correctly aligned 
Italian words can still be a source of errors in the 
resulting Italian annotations. In order to evaluate 
the quality of the annotations automatically 
transferred to Italian, a new gold standard was 
created starting from SemCor text br-g11. The 
English text, containing 2,153 tokens and 1,054 
semantic annotations, was translated into Italian in 
a controlled modality. The resulting Italian text is 
composed of 2,351 tokens, among which 1,085 are 
content words to be annotated. The English text 
and its Italian translation were manually aligned 
and the Italian text was manually semantically 
annotated taking into account the annotations of 
the English words. Each time an English 
annotation was appropriate for the Italian 
corresponding word, the annotator used it also for 
Italian. Otherwise, the annotator did not use the 
original English annotation for the Italian word and 
looked in WordNet for a suitable annotation. 
Moreover, when the English annotations were 
not suitable for annotating the Italian words, the 
annotator explicitly distinguished between wrong 
English annotations and English annotations that 
could not be transferred to the Italian translation 
equivalents. The errors in the English annotations 
amount to 24 cases. Non-transferable annotations 
amount to 155, among which 143 are due to lack 
of synonymy at lexical level and 12 to translation 
equivalents which are not lexical units. 
The differences between the English and Italian 
text with respect to the number of tokens and 
annotations have also been analysed. The Italian 
text has about 200 tokens and 31 annotated words 
more than the English text. The difference in the 
number of tokens is due to various factors. First, 
there are grammatical characteristics specific to the 
Italian language, such as a different usage of 
articles, or a greater usage of reflexive verbs which 
leads to a higher number of clitics. For example, 
the English sentence ?as cells coalesced? must be 
translated into Italian as ?quando le cellule si 
unirono?. Then, we have single English words 
translated into Italian with free combinations of  
words (ex: down translated as verso il basso) and 
multiwords which are recognized in English and 
not recognized in Italian (e.g. one token for 
nucleic_acid in the English text and two tokens in 
the Italian text, one for acido and one for 
nucleico). As regards content words to be 
annotated, we would have expected that their 
number was the same both in English and Italian. 
In fact, the difference we found is much lower than 
the difference between tokens. This difference is 
explained by the fact that some English content 
words have not been annotated. For example, 
modal and auxiliary verbs (to have, to be, can, 
may, to have to, etc.) and partitives (some, any) 
where systematically left unannotated in the 
English text whereas they have been annotated for 
Italian. 
The automatic procedures for word alignment 
and annotation transfer were run on text br-g11 
and evaluated against the gold standard. The total 
number of transferred senses amounts to 879. 
Among them, 756 are correct and 123 are incorrect 
for the Italian words. Table 4 summarizes the 
results in terms of precision, recall and coverage 
with respect to both English annotations available 
(1,054) and Italian words to be annotated (1,085).  
We can see that the final quality of the Italian 
annotations is acceptable, the precision amounting 
to 86.0%. The annotation error rate of 14.0% has 
been analyzed in order to classify the different 
factors affecting the transfer methodology. Table 5 
reports the data about the composition of the 
incorrect transfer. 
Comparing the number of annotation errors in 
the English source, as marked up during the 
creation of the gold standard (24), with the number 
of errors in the Italian annotation due to errors in 
the original annotation (22), we can see that almost 
all of the source errors have been transferred, 
contributing in a consistent way to the overall 
Italian annotation error rate. 
As regards word alignment, br-g11 was a 
relatively easy text as the performance of KNOWA 
(i.e. 96.5%) is higher than that obtained with the 
test set (see Table 3). 
 Precision Recall Coverage 
Wrt English 86.0 71.7 83.4 
Wrt Italian 86.0 69.7 81.0 
Table 4: Annotation evaluation on text br-g11 
 # % 
English annotation errors 22 2.5 
Word alignment errors 31 3.5 
Non-transferable annotations 70 8.0 
Total incorrect transfers 123 14.0 
Table 5: Composition of the incorrect transfer  
The last source of annotation errors is 
represented by words which have been correctly 
aligned but whose word sense annotation cannot be 
transferred. This happens with (i) translation 
equivalents which are lexical units but are not 
cross-language synonyms, and (ii) translation 
equivalents which are cross-language synonyms 
but are not lexical units. In practice, given the 
difficulty in deciding what is a lexical unit and 
what is not, we decided to accept the transfer of a 
word sense from an English lexical unit to an 
Italian free combination of words (see for instance 
occhiali da sole annotated with the sense of 
sunglasses). Therefore, only the lack of synonymy 
at lexical level has been considered an annotation 
error. 
The obtained results are encouraging. Among 
the 143 non-synonymous translations marked in 
the gold standard, only 70 have been aligned by the 
word alignment system, showing that KNOWA is 
well suited to the MultiSemCor task. The reason is 
that it relies on bilingual dictionaries where non-
synonymous translations are quite rare. This can be 
an advantage with respect to statistics-based word 
aligners, which are expected to be able to align a 
great number of non-synonymous translations, thus 
introducing more errors in the transfer procedure.  
A final remark about the evaluation concerns the 
proportion of non-transferable word senses with 
respect to errors in the original English 
annotations. It is sometimes very difficult to 
distinguish between annotation errors and non-
transferable word senses, also because we are not 
English native speakers. Thus, we preferred to be 
conservative in marking English annotations as 
errors unless in very clear cases. This approach 
may have reduced the number of the errors in the 
original English corpus and augmented the number 
of non-transferable word senses, thus penalizing 
the transfer procedure itself. 
Summing up, the cross-language annotation 
transfer methodology produces an Italian corpus 
which is tagged with a final precision of 86.0%. 
After the application of the methodology 19.0% of 
the Italian words still need to be annotated (see the 
annotation coverage of 81.0%). We think that, 
given the precision and coverage rates obtained 
from the evaluation, the corpus as it results from 
the automatic procedure can be profitably used. 
However, even in the case that a manual revision is 
envisaged, we think that hand-checking the 
automatically tagged corpus and manually 
annotating the remaining 19% still results to be 
cost effective with respect to annotating the corpus 
from scratch. 
5 The MultiSemCor Corpus Up to Now 
We are currently working at the extensive 
application of the annotation transfer methodology 
for the creation of the MultiSemCor corpus. Up to 
now, MultiSemCor is composed of 29 English 
texts aligned at the word level with their 
corresponding Italian translations. Both source and 
target texts are annotated with POS, lemma, and 
word sense. More specifically, as regards English 
we have 55,935 running words among which 
29,655 words are semantically annotated (from 
SemCor). As for Italian, the corpus amounts to 
59,726 running words among which 23,095 words 
are annotated with word senses that have been 
automatically transferred from English. 
MultiSemCor can be a useful resource for a 
variety of tasks, both as a monolingual 
semantically annotated corpus and as a parallel 
aligned corpus. As an example, we are already 
using it to automatically enrich the Italian 
component of MultiWordNet, the reference lexicon 
of MultiSemCor. As a matter of fact, out of the 
23,095 Italian words automatically sense-tagged, 
5,292 are not yet present in MultiWordNet and will 
be added to it. Moreover, the Italian component of 
MultiSemCor is being used as a gold standard for 
the evaluation of Word Sense Disambiguation 
systems working on Italian. Besides NLP 
applications, MultiSemCor is also suitable to be 
consulted by humans through a Web interface 
(Ranieri et al, 2004) which is available at: 
http://tcc.itc.it/projects/multisemcor.  
6 Conclusion and future directions 
We have presented and evaluated an approach to 
the creation of high quality semantically annotated 
resources based on the exploitation of aligned 
parallel corpora. The results obtained from the 
thorough evaluation of the different steps involved 
in the methodology confirm the feasibility of the 
MultiSemCor project. The cross-lingual annotation 
transfer methodology is going to be applied also to 
the remaining 157 SemCor texts, which are 
currently being translated into Italian. 
As regards future research directions within the 
transfer annotation paradigm, it would be 
interesting to extend the methodology to other 
languages, e.g. Spanish, for which a WordNet 
exists and can be aligned with MultiWordNet. 
Moreover, as the Brown Corpus, used to create 
SemCor, has been syntactically annotated within 
the English Penn Treebank, the syntactic 
annotations of the SemCor texts are also available. 
We are planning to explore the possibility of 
transferring the syntactic annotations from the 
English to the Italian texts of MultiSemCor. 
References  
L. Ahrenberg, M. Merkel, H. Sagvall and A. J. 
Tiedemann. 2000. Evaluation of word alignment 
systems. In Proceedings of LREC 2000, Athens, 
Greece. 
L. Bentivogli and E. Pianta. 2002. Opportunistic 
Semantic Tagging. In Proceedings of LREC-
2002, Las Palmas, Canary Islands, Spain. 
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, 
and R. L. Mercer. 1991. Word-Sense 
Disambiguation using Statistical Methods. In 
Proceedings of ACL?91, Berkeley, California, 
USA. 
C. Cabezas, B. Dorr and P. Resnik. 2001. Spanish 
Language Processing at University of Maryland: 
Building Infrastructure for Multilingual 
Applications. In Proceedings of the 2nd  
International Workshop on Spanish Language 
Processing and Language Technologies, Jaen, 
Spain. 
I. Dagan and A. Itai. 1994. Word Sense 
Disambiguation Using a Second Language 
Monolingual Corpus. Computational Linguistics: 
20(4):563-596. 
M. Diab and P. Resnik. 2002. An unsupervised 
method for word sense tagging using parallel 
corpora. In In Proceedings of ACL 2002, 
Philadelphia, USA . 
C. Fellbaum, J. Grabowski and S. Landes. 1998. 
Performance and confidence in a semantic 
annotation task. In Fellbaum, C. (ed.). 1998. 
WordNet: An Electronic Lexical Database. The 
MIT Press, Cambridge (Mass.). 
C. Fellbaum (ed.). 1998. WordNet: An Electronic 
Lexical Database. The MIT Press, Cambridge 
(Mass.). 
W. A. Gale, K. W. Church and D. Yarowsky. 
1992. Using Bilingual Materials to Develop 
Word Sense Disambiguation Methods. In 
Proceedings of the Fourth International 
Conference on Theoretical and Methodological 
Issues in Machine Translation. Montreal, 
Canada. 
R. Hwa, P. Resnik and A. Weinberg. 2002. 
Breaking the Resource Bottleneck for 
Multilingual Parsing. In Proceedings of the 
LREC-2002 Workshop on "Linguistic Knowledge 
Acquisition and Representation: Bootstrapping 
Annotated Language Data'', Las Palmas, Canary 
Islands, Spain. 
N. Ide, T. Erjavec, and D. Tufis. 2002. Sense 
Discrimination with Parallel Corpora. In 
Proceedings of ACL'02 Workshop on Word 
Sense Disambiguation: Recent Successes and 
Future Directions, Philadelphia, USA.  
S. Landes C. Leacock, and R.I. Tengi. 1998. 
Building semantic concordances. In Fellbaum, C. 
(ed.) (1998) WordNet: An Electronic Lexical 
Database. The MIT Press, Cambridge (Mass.). 
I. D. Melamed. 2001. Empirical Methods for 
Exploiting Parallel Texts. The MIT Press, 
Cambridge (Mass.).  
F.J. Och and H. Ney. 2003. A systematic 
comparison of various statistical alignment 
models. Computational Linguistics, 29(1):19-53. 
E. Pianta and L. Bentivogli. 2004. Knowledge 
intensive word alignment with KNOWA. In 
Proceedings of Coling 2004, Geneva, 
Switzerland. 
E. Pianta and L. Bentivogli. 2003. Translation as 
Annotation. In Proceedings of the AI*IA 2003 
Workshop ?Topics and Perspectives of Natural 
Language Processing in Italy?, Pisa, Italy. 
E. Pianta, L. Bentivogli and C. Girardi. 2002. 
MultiWordNet: developing an aligned 
multilingual database. In Proceedings of the 
First Global WordNet Conference, Mysore, 
India.  
M. Ranieri, E. Pianta and L. Bentivogli. 2004. 
Browsing Multilingual Information with the 
MultiSemCor Web Interface. In Proceedings of 
the LREC-2004 Workshop ?The amazing utility 
of parallel and comparable corpora?, Lisbon, 
Portugal. 
E. Riloff, C. Schafer and D. Yarowsky. 2002. 
Inducing information extraction systems for new 
languages via cross-language projection. In 
Proceedings of Coling 2002, Taipei, Taiwan. 
J. V?ronis and  P. Langlais. 2000. Evaluation of 
parallel text alignment systems. In V?ronis, J. 
(ed.). 2000. Parallel Text Processing, Kluwer 
Academic Publishers, Dordrecht. 
J. V?ronis (ed.). 2000. Parallel Text Processing. 
Kluwer Academic Publishers, Dordrecht. 
D. Yarowsky, G. Ngai and R. Wicentowski. 2001. 
Inducing Multilingual Text Analysis Tools via 
Robust Projection across Aligned Corpora. In 
Proceedings of HLT 2001, San Diego, 
California, USA.  
Knowledge Intensive Word Alignment with KNOWA 
Emanuele PIANTA and Luisa BENTIVOGLI 
ITC-irst 
Via Sommarie, 18 
38050 Povo - Trento 
Italy 
{pianta,bentivo}@itc.it 
 
Abstract 
In this paper we present KNOWA, an 
English/Italian word aligner, developed at ITC-irst, 
which relies mostly on information contained in 
bilingual dictionaries. The performances of 
KNOWA are compared with those of GIZA++, a 
state of the art statistics-based alignment algorithm. 
The two algorithms are evaluated on the EuroCor 
and MultiSemCor tasks, that is on two 
English/Italian publicly available parallel corpora. 
The results of the evaluation show that, given the 
nature and the size of the available English-Italian 
parallel corpora, a language-resource-based word 
aligner such as KNOWA can outperform a fully 
statistics-based algorithm such as GIZA++. 
1 Introduction 
Aligning a text and its translation (also known as 
bitext) at the word level is a basic Natural 
Language Processing task that has found various 
applications in recent years. Word level alignments 
can be used to build bilingual concordances for 
human browsing, to feed machine learning-based 
translation algorithms, or as a basis for sense 
disambiguation algorithms or for automatic 
projection of linguistic annotations from one 
language to another.   
A number of word alignment algorithms have 
been presented in the literature, see for instance 
(V?ronis, 2000) and (Melamed, 2001). Shared 
evaluation procedures have been established, 
although there are still open issues on some 
evaluation details (Ahrenberg et al 2000).  
Most of the known alignment algorithms are 
statistics-based and do not exploit external 
linguistic resources, or use them to a very limited 
extent. The main attractive of such algorithms is 
that they are language independent, and only 
require a parallel corpus of reasonable size to be 
trained.  
However, word alignment can be used for 
different purposes and in different application 
scenarios; different kinds of alignment strategies 
produce different kinds of results (for instance in 
terms of precision/recall) which can be more or 
less suitable to the goal to be achieved. The 
requirement of having a parallel corpus of 
adequate size available for training the  statistics-
based algorithms may be difficult to meet, given 
that parallel corpora are a precious but often rare 
resource. For the most common languages, such as 
English, French, German, Chinese, etc., reference 
parallel corpora of adequate size are available, and 
indeed statistics-based algorithms are evaluated on 
such reference corpora. Unfortunately, if one needs 
to replicate in a different corpus the results 
obtained for the reference corpora, finding a 
parallel corpus of adequate size can be difficult 
even for the most common languages. Consider 
that one of the most appealing features of statistics-
based algorithms is their ability to induce 
alignment models for bitexts belonging to very 
specific domains, an ability which seems to be out 
of reach for algorithms based on generic linguistic 
resources. However, for the statistics-based 
algorithms to achieve their objective, a parallel 
corpus for the specific domain needs to be 
available, a requirement that in some cases cannot 
be met easily.  
For these reasons, we claim that in some cases 
algorithms based on external, linguistics resources, 
if available, can be a useful alternative to statistics- 
based algorithms. In the rest of this paper we will 
compare the results obtained by a statistics-based 
and a linguistic resource-based algorithm when 
applied to the EuroCor and MultiSemCor 
English/Italian corpora.  
The statistics-based algorithm to be evaluated is 
described in  (Och and Ney, 2003). For its 
evaluation we used an implementation by the 
authors themselves, called GIZA++, which is 
freely available to the scientific community (Och, 
2003). The second algorithm to be evaluated is 
crucially based on a bilingual dictionary and a 
morphological analyzer. It is called KNOWA 
(KNowledge intensive Word Aligner) and has been 
developed at ITC-irst by the authors of this paper. 
The results of the comparative evaluation show 
that, given specific application goals, and given the 
availability of Italian/English resources, KNOWA 
obtains results that are comparable or better than 
the results obtained with GIZA++. 
Section 2 describes the basic KNOWA 
algorithm. Sections 3 and 4 illustrate two enhanced 
versions of the KNOWA algorithm. Section 5 
reports an experiment in which both KNOWA and 
GIZA++ are first applied to the alignment of a 
reference parallel corpus, EuroCor, and then to the 
MultiSemCor corpus. Section 6 adds some 
conclusive remarks. 
2 KNOWA ? the basic algorithm 
KNOWA is an English/Italian word aligner, which 
relies mostly on information contained in the 
Collins bilingual dictionary, available in electronic 
format. KNOWA also exploits a morphological 
analyzer and a multiword recognizer, for both 
Italian and English. It does not require any corpus 
for training. However the input bitext must be 
sentence-aligned. 
For each sentence pair, KNOWA produces word 
alignments according to the following strategy: 
 
? The morphological analysis produces a set 
of candidate lemmas for each English and 
Italian word. 
? The candidate lemmas are ordered from the 
most to the least probable by means of a 
rule-based PoS ordering algorithm. 
? A three phase incremental alignment 
procedure takes as input the two sentences 
annotated with sets of ordered candidate 
lemmas and outputs a set of pairwise word 
alignments. 
 
The alignment procedure is crucially based on 
the relation of potential correspondence between 
English and Italian words: 
 
Given an English word wE and an Italian word 
wI, wI is the potential correspondent of wE if one of 
the candidate lemmas of wI is the translation 
equivalent of one of the candidate lemmas of wE, 
according to a bilingual dictionary. 
 
The potential correspondence relation holds 
between words, but is relative to a lemma pair. For 
instance we say that the words dreams and sogna 
are potential correspondents relative to the lemma 
pair <dream/verb, sognare/verb>. Two words can 
be potential correspondents relative to more than 
one lemma pair. For instance the words dream and 
sogno are potential correspondents relative to the 
two lemmas pairs <dream/verb, sognare/verb> and 
<dream/ noun, sogno/noun>. In fact dream and 
sogno can be either first singular person of the verb 
to dream and sognare, or singular forms of the 
noun dream and sogno respectively. 
The correspondence relation is called potential 
because in real texts, tokens that are potential 
correspondents may not in fact be translations of 
each other. Take for instance the following 
translation pair: ?ll cane e il gatto?, ?the dog and 
the cat?. The first occurrence of the Italian article 
?il? is a potential correspondent of both 
occurrences of the word ?the? in the English 
sentence, but is the translation of only the first one.  
In the first phase of the alignment procedure the 
potential correspondence relation is exploited in 
the English to Italian direction: 
 
For each English word wE in a certain position p: 
 
1. Get the most probable candidate lemma of wE. 
2. Get the Italian word wI in the same position p. 
3. Check if there is a candidate lemma of wI 
which is a potential correspondent of wE 
relative to the current English candidate 
lemma, on the basis of a bilingual lexicon. 
4. If yes, align wE and wI and record their 
lemmas. 
5. Otherwise consider the next probable 
candidate lemma of wE and go back to step 2. 
6. If no aligment is found, progressively extend 
the Italian word window and go back to step 1. 
 
By extending the Italian word window we mean 
considering Italian words in position p ? Delta, 
where p is the position of the English word and 
Delta can vary from 1 to a MaxDelta value. The 
value of MaxDelta is adjustable, but a number of 
experiments have shown that the best results are 
obtained when MaxDelta=14. Note that if the 
alignment is not found within the Italian word 
window, the English word is left unaligned. In 
Table 1 the box in the Italian column shows the 
maximal text window in which the potential 
correspondent of dream is searched (MaxDelta=5).  
The search starts from 15-precedente and ends 
after the first extension of the text window as 
sogno can be found in position p-1. 
In the second phase of the alignment procedure 
the potential correspondence relation is exploited 
from Italian to English. For each Italian word 
which has not been aligned in the first phase, the 
same procedure is applied as above. 
In the third and last phase, the algorithm tries to 
align the words which are still unaligned, resorting 
to the graphemic similarity of the Italian and 
English words. See (Yzaguirre et al, 2000) for a 
similar approach. 
Note that given the way in which the alignment 
procedure works, finding an alignment implies also 
selecting a PoS and a lemma for both English and 
Italian words. The selected PoS and lemma can be 
different from the ones that were considered most 
probable by the PoS ordering algorithm, due to the 
constraints added by the potential correspondence 
relation. 
? ? 
9-the 9-l' 
10-exact 10-esatta 
11-pattern 11-riproduzione 
12-of 12-di 
13-a 13-un 
14-previous 14-sogno 
15-dream 15-precedente 
16-we 16-abbiamo 
17-have 17-un 
18-an 18-caso 
19-instance 19-di 
20-of 20-deja_vu 
21-deja_vu 21-, 
? ? 
Table 1: An example of a maximal text window 
The KNOWA algorithm needs to be able to cope 
with at least two problematic aspects. The first are 
multiwords. To work properly, KNOWA needs to 
identify them in the source and target sentences, 
and needs knowledge about their translation 
equivalents. We have tried to exploit the 
information about multiwords contained in the 
Collins bilingual dictionary. However it is well 
known that dictionaries contain only a small part of 
multiwords actually used in language. Thus, there 
is still wide room to improve KNOWA's capability 
to handle multiwords.  
The second problematic aspect has to do with 
multiple potential correspondence relations. Given 
a source word in one language, more than one 
potential correspondent can be found within the 
maximal word window in the target language. This 
is particularly true in a full text alignment task, that 
is trying to align also functional words. Articles 
and determiners can occur repeatedly in any 
sentence, and almost any Italian preposition can be 
the translation of any English preposition; this 
makes the task of aligning determiners and 
preposition on the basis of the potential 
correspondence relation and the absolute position 
in the sentence hard. Whatever the number of 
potential correspondents, the alignment procedure 
selects the potential correspondent whose position 
is nearest to the position of the source word by first 
considering the most probable PoS of the source 
word. Unfortunately, the potential correspondent 
selected in this way is not always the right one. 
Thus multiple potential correspondents can be a 
source of alignment errors for KNOWA. In the 
following section we describe an extension of the 
basic KNOWA algorithm that tries to cope with 
this limitation. 
3 KNOWA ? the pivot extension 
In this section we illustrate a variation of the basic 
KNOWA algorithm, which tries to solve the 
problem of multiple potential correspondence 
relations. To illustrate the problem, let us consider 
the example in Table 2, where wrong alignments 
are marked with a cross. 
1-the 1-il 
2-boy 2-cane 
3-likes 3-piace 
4-the 4-al 
5-dog 5-bambino 
Table 2: Errors due to multiple potential 
correspondence relations 
In the Italian translation the order of the English 
noun phrase is inverted. This is due to the fact that 
the Italian translation of ?likes? follows a different 
verb subcategorization pattern. What is an object in 
English becomes a subject in Italian, causing a 
problem to the basic KNOWA algorithm. In fact, 
KNOWA correctly aligns 2-boy with 5-bambino, 
and 5-dog with 2-cane, even if the English and 
Italian nouns are not in the same position in the 
respective sentences, thanks to a search in the 
Italian word window. However, KNOWA would 
also align 1-the with 1-il, and 4-the with 4-al. 
Actually 1-the is a potential correspondent of both 
1-il, and 4-al (the correct translation), but 
KNOWA chooses 1-il because its position is 
nearest to 1-the. 
To solve these problems we need to use a 
different strategy. The solution is based on the 
observation that content words tend to be less 
involved in multiple potential correspondences 
than function words, and that function words tend 
to be attached to content words. Thus the basic 
idea amounts to trying first the alignment of 
content words, and only in a second phase trying 
the alignment of function words relative to the 
position of content words to which they are 
attached.  Alignments between content words act 
as pivots, around which the alignment of function 
words is tried. 
In the example above, first the algorithm finds 
the following correct alignments:  
 
2-boy <> 5-bambino 
3-likes <> 3-piace 
5-dog <> 2-cane 
 
Then, it takes the first alignment and tries to align 
the word before 2-boy and the word before 5-
bambino, finding the correct alignment between 1-
the and 4-al, and so on. 
We do not expect that all content words are 
equally good pivots. To assess the goodness of 
nouns, verbs, adjectives, and adverbs as pivot 
words, we run various experiments, taking only the 
content words of a specific PoS and some 
combinations of them as pivot words. The results 
of these experiments show that nouns, taken alone 
as pivots, produce the best results in comparison 
with other PoS or combinations of PoS. 
We also considered an alternative strategy for 
selecting pivots words. Instead of using the PoS as 
a predictor for the goodness of a word as pivot, 
which actually amounts to saying that words in a 
certain PoS can be aligned with a lower error rate 
than others, we selected as pivots the words for 
which the potential correspondence relation with 
their translation equivalents in the other language 
is one-to-one. Given a word wE in the English 
sentence and a word wI in its Italian translation, we 
select wE as a pivot word if, and only if,  wI is the 
only potential correspondent of wE, and wE is the 
only potential correspondent of wI. Of course, 
content words, and nouns in particular, tend to 
have such property much more frequently than 
words with other PoS. However, not all nouns have 
this characteristics. On the other hand certain 
function words, for instance conjunctions, may be 
involved in a one-to-one potential correspondence 
relation.  
Table 3 shows a complete English sentence with 
its translation, taken from MultiSemCor. All the 
pivot words involved in one-to-one potential 
correspondence relations, according the Collins 
dictionary, are connected by a solid line. Note that 
the relation between 2-temperatures and 2-clima is 
indeed one-to-one, but is not recorded in the 
reference dictionary, so it is marked with a dotted 
line in the table.  
Table 4 exemplifies instead typical cases of non-
pivot words: 9-rovente is the only potential 
translation of 1-sizzling, but 9-rovente can also 
translate 2-hot, so neither 1-sizzling nor 4-hot are 
selected as pivot words.  
The pivot extension of KNOWA has strong 
similarities with a strategy that is used by various 
statistics-based algorithms, aiming at selecting at 
first the translation correspondents that are most 
probably correct. Once these pivotal 
correspondences have been established, the 
remaining alignments are derived using the pivots 
as fixed points. Given that fact that these 
algorithms do not exploit bilingual dictionaries, the 
selection of the pivotal translation correspondent 
may be based on cognates, or specific frequency 
configurations. See among others (Simmard and 
Plamondon, 1998) and (Ribeiro et al, 2000). 
The results obtained by applying the one-to-one 
potential correspondence as criterion for selecting 
pivot words are illustrated further on in Section 5. 
 
1-Sizzling 
2-temperatures 
3-and 
4-hot 
5-summer 
6-pavements 
7-are 
8-anything 
9-but 
10-kind 
11-to 
12-the 
13-feet 
1-Il 
2-clima 
3-torrido 
4-e 
5-i 
6-marciapiedi 
7-dell? 
8-estate 
9-rovente 
10-non 
11-sono 
12-niente 
13-di 
14-buono 
15-per 
16-i 
17-piedi 
Table 3: pivot words involved in one-to-one 
potential correspondences 
1-Sizzling 
2-temperatures 
3-and 
4-hot 
5-summer 
6-pavements 
7-are 
8-anything 
9-but 
10-kind 
? 
1-Il 
2-clima 
3-torrido 
4-e 
5-i 
6-marciapiedi 
7-dell? 
8-estate 
9-rovente 
10-non 
? 
Table 4: typical potential correspondences for 
non-pivot words 
4 KNOWA - the breadth-first extension 
The pivot extension to the basic KNOWA 
algorithm is based on two main hypotheses: first, 
certain words, which we call pivot words and 
which are mainly content words, are easier to align 
than others; second, the position of the other 
words, mainly function words, is anchored to the 
position of pivot words. This means for instance 
that if an article is near to a noun in Italian, we 
expect the English translation of the article to be 
near the English translation of the noun.  
However if we look closer to the way the basic 
algorithm explores the search space of the potential 
correspondent in the word window, we will see 
that such strategy is inconsistent with the above 
two hypotheses. Suppose that we start from a pivot 
word wE1, in position pE1, as illustrated in Table 5, 
where pivot words are included in box. Then, we 
try to align a non-pivot word wE2 occurring in 
position pE1+1. If the correspondent of wE1, that is 
wI1, occurs in position pI1, then we expect the 
correspondent of wE2, to occur in position pI1+1. 
Now, if wI2 turns out not to be the  potential 
correspondent of wE2, possibly because wE2 has not 
been translated, KNOWA will extend the word 
window of wI2, and search the potential 
correspondents in position pI1 ? 2, pI1 ? 3, and so 
on, up to MaxDelta. We describe this by saying 
that the basic algorithm searches potential 
correspondents in the word window following a 
depth-first search strategy. Unfortunately, such 
strategy can cause alignment errors. Suppose that 
wE3 is another pivot word in position pE3, to be 
aligned with wI3 in position pI3, and that wE4 is a 
non-pivot word in position pE3+1, to be aligned 
with wI4, in position pI3+1. Suppose also that wE2 is 
a potential correspondent of wI4. Because of the 
depth-first search strategy, the basic KNOWA 
algorithm will align wE2 and wI4 wrongly. This kind 
of error can be avoided by adopting what can be 
called a breadth-first search strategy. In practice, 
for each pivot word we first search the potential 
correspondent in a word window of 0, that is in the 
expected initial position, then for each pivot word 
we search potential correspondents in a window of 
?1, and so on up to the MaxDelta. The results of 
testing these strategy are reported in the following 
section. 
 
1-  
2- wE1 
3- wE2 
4- 
5- 
6- wE3 
7- wE4 
8- 
9- 
10- 
? 
1- 
2- 
3-  
4- wI1 
5- wI2 
6- 
7- 
8- wI3 
9- wI4 
10- 
? 
Table 5: Wrong alignment caused by the first-
depth search strategy in the word window 1-9. 
5 The experiments 
We have run the experiments on two tasks, the 
EuroCor and the MultiSemCor alignment tasks. 
We call EuroCor a reduced and revised version of 
EuroParl, a multilingual corpus extracted from the 
proceedings of the European Parliament, see 
(Koehn, unpublished). EuroParl includes texts in 
11 European languages, automatically aligned at 
the sentence level, whereas EuroCor includes only 
a part of the texts in EuroParl and only for English 
and Italian. On the other hand, MultiSemCor is a 
reference English/Italian corpus being developed at 
ITC-irst, including SemCor (part of the Brown 
Corpus) and its Italian translations. MultiSemCor 
has been created with the purpose of automatically 
transfer lexical semantic annotations from English 
to Italian (Bentivogli and Pianta, 2002).  
For our experiments on EuroCor, we used as  
gold standard (and test set) a text that,  following 
the EuroParl naming conventions, can be  
identified as ep-98-09-18. The revised version of 
this text includes 385 sentences, and has been 
manually aligned at the word level. Also sentence 
alignment has been manually revised.  
For our experiments on MultiSemCor we used a 
gold standard composed of 6 files, manually 
aligned. Three of them have been exploited as 
development set and three as test set. In order to 
keep the test set as unseen as possible, the 
experiments whose main goal is tuning the 
algorithm by comparing various alignment 
strategies or parameters have been run on the 
development set. Once the best configuration has 
been obtained on the development set, we gave the 
results of running the algorithm with such 
configuration on the test set. 
In our first experiment we run GIZA++ on both 
EuroCor and MultiSemCor. At first, we run 
GIZA++ on the entire English/Italian part of 
EuroParl, including around 694,000 sentences. The 
training of GIZA++ on this big corpus took around 
two weeks only for the English-to-Italian direction, 
on a high-level Sun Spark with 4 GB of memory. 
For this reason we decided to run the subsequent 
experiments on EuroCor, a reduced version of 
EuroParl, including around 21,000 sentences. 
EuroCor includes the following texts from 
EuroParl: ep-96-05-08, ep-97-04-07, ep-98-04-01, 
ep-90-11-04, ep-99-01-14, ep-99-10-05, ep-00-06-
13, ep-00-09-04, ep-01-04-02, ep-01-04-03. the 
file in the gold standard, ep-98-09-18, should be 
added to these texts. These texts where chosen 
randomly, sampling them from as diverse periods 
of time as possible. Note that GIZA++ cannot be 
tested on a test set distinct from the training set. 
Thus we trained GIZA++ on the whole EuroCor 
corpus, including the file in the test set. Given the 
fact that we are simply using GIZA++ as a black 
box without having access to the internals of the 
alignment program, this seems acceptably safe 
from a methodological point of view. In all our 
experiments with GIZA++ we adopted a 
configuration of the system which is reported by 
the  authors to produce optimal results, that is 
15H5344454, where the number in the base refers to 
the IBM models 1, 3, 4, and 5, H refers to the 
HMM training, and the superscript figures refer to 
the number of iterations. 
5.1 The EuroCor task 
The first training of GIZA++ on EuroCor gave the 
following disappointing results on all-words 
alignment: 59.7% precision, 14.1% recall. After 
inspection of the corpus, we realized that the 
original files in EuroParl contain tokenization 
errors, and what counts more, a big number of 
sentence alignment errors. For this reason we 
produced a revised version of EuroCor, fixing 
these errors as extensively as possible.  
A new run of GIZA++ on the revised EuroCor 
gave the following result: P:62.0%, R:34.7% on all 
word alignment; P:53.2%, R:38.3% on content 
words only. These results compare badly with 
those reported by (Och and Ney, 2003) on the 
Hansard alignment task. For this task, the authors 
report a precision of 79.6%, for a training on a 
corpus of 8,000 sentences. Explaining such a 
difference is not easy. A first explanation can be 
the fact the EuroCor task is inherently harder than 
the Hansard task. Whereas in the Hansard corpus 
the texts are direct translations of each other, in the 
EuroCor corpus it happens quite frequently that the 
English and Italian texts are both translation of a 
text in a third language. As a consequence, the 
texts are much more difficult to align. A better and 
more systematic revision of the sentence 
alignments could also improve the performance of 
GIZA++. 
The basic version of KNOWA run on the 
EuroCor test file gives the results reported in Table 
6. These results confirm the difficulty of the 
EuroCor task, but are quite encouraging for 
KNOWA, given that no special tuning was made 
to obtain them. It is interesting to note that whereas 
GIZA++ performs better on the all-word task than 
on the content-only-word task, KNOWA gets 
better results on the content-word-only task. 
Although it is true that aligning function words 
seems inherently more difficult than aligning 
content word, the worse result obtained by a 
statistics-based algorithm such as GIZA++ on the 
content-words-only task may be explained by the 
fact that data about content words are more sparse 
that data about function words. 
  Precision Recall 
all 62.0 34.7 GIZA++ 
22k content 53.2 38.3 
all 63.4 41.6 KNOWA 
basic content 85.5 53.2 
Table 6: GIZA++ and KNOWA-basic on the 
EuroCor task 
 
5.2 The MultiSemCor task 
The training of GIZA++ on MultiSemCor has been 
quite problematic, due to the small dimensions of 
MultiSemCor. In the current phase of the project, 
only 2,948 sentences are available. This is a small 
corpus which allows for only an approximate 
comparison with the experiment reported by Och 
and Ney (2003) on a set of 8,000 sentences from 
the Hansard corpus. Also, the authors report an 
improvement of around 7 points in precision, in 
passing from a corpus of 8,000 to 128,000 
sentences. As the ultimate version of MultiSemCor 
is expected to include more than 20,000 sentences, 
we can expect a non negligible improvement in 
precision when GIZA++ will be applied to the 
final version of MultiSemCor.  
To simulate at least partly the improvement that 
one can expect from an increase in the size of 
MultiSemCor, we trained GIZA++ on the union of 
the available MultiSemCor and EuroCor. The 
results of the training on MultiSemCor only, and 
on the union of MultiSemCor and EuroCor are 
reported in Table 7. Besides the row for the all-
word task, the table contains also a SemCor row. 
This task concerns all the words that have been 
manually tagged in SemCor, and roughly 
corresponds to the content-word task. As the 
purpose of MultiSemCor is transferring lexical 
annotations from the English annotated words to 
the corresponding Italian words, it is particularly 
important that the alignment for the annotated 
words be correct. The results showed that GIZA++ 
works consistently better in the Italian-to-English 
direction, rather than vice versa, so we report the 
former direction. Only for the training on the union 
of the MultiSemCor and EuroCor data, we also 
report the results calculated by resorting to the 
symmetrization by intersection of the two 
alignments. Table 7 below shows that the 
MultiSemCor task is less difficult than the 
EuroCor Task; that GIZA++ consistently performs 
worse on content words; and finally that the 
increase in the dimensions of the training corpus 
produces a non marginal improvement in the 
precision, although not in the recall measure. 
Symmetrization produces a big improvement in 
precision but also an unacceptable worsening of 
the recall measure for GIZA++. 
The two last rows in the table report the 
performances of the basic version of KNOWA in 
the same two tasks. These results show that given 
the available resources, KNOWA outperforms 
GIZA++ in all tasks. This is even clearer if we 
consider the extended versions of KNOWA, as 
reported in Table 8. Finally Table 9 reports the 
results of KNOWA on the test set. 
 task Prec. Recall 
all 68.9 53.5 GIZA++ 3k 
(MSC) It ->En semcor 60.4 55.1 
all 73.4 55.2 GIZA++ 25k 
(MSC+EC) It ->En semcor 81.9 52.9 
all 95.2 38.8 GIZA++ 25k 
(MSC+EC) intersec semcor 95.8 37.1 
all 84.5 63.7 KNOWA 
basic semcor 92.0 73.4 
Table 7: GIZA++ and KNOWA-basic on the 
MultiSemCor task (development set) 
KNOWA version task Prec. Recall 
all 86.8 65.3 pivot (nouns) 
depth-first semcor 92.5 73.6 
all 88.1 66.5 pivot (1-to-1) 
depth-first semcor 92.8 74.4 
all 89.4 67.5 pivot (1-to-1) 
breadth-first semcor 93.0 74.6 
Table 8: KNOWA-enhanced on constrained 
translation (development-set) 
KNOWA version task Prec. Recall 
all 82.1 56.9 best 
(on free tran.) semcor 89.1 66.5 
all 87.0 66.6 best 
(constr. tran.) semcor 91.8 72.8 
Table 9: KNOWA-best on test set (free and 
constrained translation) 
6 Conclusion 
In this paper we compared the performances of two 
word aligners, one exclusively based on statistical 
principles, and the other intensively based on 
linguistic resources. Although statistics-based 
algorithms are very appealing, because they are 
language independent, and only need a parallel 
corpus of reasonable size to be trained, we have 
shown that, from a practical point of view, the lack 
of parallel corpora with the necessary 
characteristics can hamper the performances of the 
statistical algorithms. In these cases, an algorithm 
based on linguistic resources, if available, can 
outperform a statistics-based algorithm. 
Also, knowledge-intensive word aligners may be 
more effective when word alignment is needed for 
special purposes such as annotation transfer from 
one language to another. This is  the case for 
instance of the MultiSemCor project, in which, 
apart from a better performance in terms of 
precision and recall, a word aligner based on 
dictionaries, such as KNOWA, has the advantage 
that it will fail to align words that are not 
synonyms. The alignment of non-synonymous 
translation equivalents, which are hardly found in 
bi-lingual dictionaries, is usually a strength of 
corpus-based word aligners, but turns out to be a 
disadvantage in the MultiSemCor case, where the 
alignment of non synonyoums words causes the 
transfer of wrong word sense annotations from one 
language to the other. 
References  
Lars Ahrenberg, Magnus Merkel, Anna S?gvall 
Hein and J?rg Tiedemann. 2000. Evaluation of 
word alignment systems. In Proceedings of 
LREC 2000, Athens, Greece.  
Luisa Bentivogli and Emanuele Pianta. 2002. 
Opportunistic Semantic Tagging. In Proceedings 
of LREC-2002, Las Palmas, Canary Islands, 
Spain (2002). 
Philipp Koehn. Unpublished. Europarl: A 
Multilingual Corpus for Evaluation of Machine 
Translation, unpublised draft, available at http: 
//www.isi.edu/~koehn/publications/europarl.ps. 
Dan I. Melamed. 2001. Empirical Methods for 
Exploiting Parallel Texts. The MIT Press, 
Cambridge, Massachussets. 
Franz J. Och. 2003. GIZA++: Training of 
statistical translation models. Available at 
http://www.isi.edu/~och/GIZA++.html. 
Franz. J. Och and H. Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment 
Models. Computational Linguistics, 29(1):19-51. 
Ant?nio Ribeiro, Gabriel Lopes and Jo?o Mexia. 
2000. Using Confidence Bands for Parallel Texts 
Alignment. In Proceedings of the 38th 
Conference of the Association for Computational 
Linguistics (ACL 2000), Hong Kong, China, 
2000 October 3?6. pp. 432?439. 
Michel Simard and Pierre Plamondon. 1998. 
Bilingual Sentence Alignment: Balancing 
Robustness and Accuracy. In Machine 
Translation, 13(1):59-80. 
Jean V?ronis (ed.). 2000. Parallel Text Processing. 
Dordrecht: Kluwer Academic Publishers. 
Llu?s de Yzaguirre, M. Ribas, J. Vivaldi and M. T. 
Cabr?. 2000. Some technical aspects about 
aligning near languages. In Proceedings of 
LREC 2000, Athens, Greece 
67
68
69
70
Revising the WORDNET DOMAINS Hierarchy: semantics, coverage and 
balancing 
Luisa Bentivogli, Pamela Forner, Bernardo Magnini, Emanuele Pianta 
ITC-irst ? Istituto per la Ricerca Scientifica e Tecnologica 
Via Sommarive 18, Povo ? Trento, Italy, 38050 
email:{bentivo, forner, magnini, pianta}@itc.it 
 
Abstract 
The continuous expansion of the multilingual 
information society has led in recent years to a pressing 
demand for multilingual linguistic resources suitable to 
be used for different applications.  
In this paper we present the WordNet Domains 
Hierarchy (WDH), a language-independent resource 
composed of 164, hierarchically organized, domain 
labels (e.g. Architecture, Sport, Medicine). Although 
WDH has been successfully applied to various Natural 
Language Processing tasks, the first available version 
presented some problems, mostly related to the lack of a 
clear semantics of the domain labels. Other correlated 
issues were the coverage and the balancing of the 
domains. We illustrate a new version of WDH 
addressing these problems by an explicit and systematic 
reference to the Dewey Decimal Classification. The new 
version of WDH has a better defined semantics and is 
applicable to a wider range of tasks. 
1 Introduction 
The continuous expansion of the multilingual 
information society with a growing number of new 
languages present on the Web has led in recent 
years to a pressing demand for multilingual 
applications. To support such applications, 
multilingual language resources are needed, which 
however require a lot of human effort to be built. 
For this reason, the development of language-
independent resources which factorize what is 
common to many languages, and are possibly 
linked to the language-specific resources, could 
bring great advantages to the development of the 
multilingual information society. 
A language-independent resource, usable in 
many automatic and human applications, is 
represented by domain hierarchies. The notion of 
domain is related to similar notions such as 
semantic field, subject matter, broad topic, subject 
code, subject domain, category. These notions are 
used, sometimes interchangeably, sometimes with 
significant distinctions, in various fields such as 
linguistics, lexicography, cataloguing, text 
categorization. As far as this work is concerned, 
we define a domain as an area of knowledge which 
is somehow recognized as unitary. A domain can 
be characterized by the name of a discipline where 
a certain knowledge area is developed (e.g. 
chemistry) or by the specific object of the 
knowledge area (e.g. food). Although objects of 
knowledge and disciplines that study them are 
clearly related, the relation between these two 
points of view on domains is sometimes blurred 
and may be a source of uncertainty on their exact 
definition. 
Another interesting duality when speaking about 
domains is related to the fact that knowledge 
manifests itself in both words and texts. So the 
notion of domain can be applied both to the study 
of words, where a domain is the area of knowledge 
to which a certain lexical concept belongs, or to the 
study of texts, where the domain of a text is its 
broad topic. In this work we will assume that also 
these two points of view on domains are strictly 
intertwined.  
By their nature, domains can be organized in 
hierarchies based on a relation of specificity. For 
instance we can say that TENNIS is a more specific 
domain than SPORT, or that ARCHITECTURE is more 
general than TOWN PLANNING. 
Domain hierarchies can be usefully integrated 
into other linguistic resources and are also 
profitably used in many Natural Language 
Processing (NLP) tasks such as Word Sense 
Disambiguation (Magnini et al 2002), Text 
Categorization (Schutze, 1998), Information 
Retrieval (Walker and Amsler, 1986).  
As regards the usage of Domain hierarchies in 
the field of multilingual lexicography, an example 
is given by the EuroWordNet Domain-ontology, a 
language independent domain hierarchy to which 
interlingual concepts (ILI-records) can be assigned 
(Vossen, 1998). In the same line, see also the 
SIMPLE domain hierarchy (SIMPLE, 2000).  
Large domain hierarchies are also available on 
the Internet, mainly meant for classifying web 
documents. See for instance the Google and Yahoo 
directories. 
A large-scale application of a domain hierarchy 
to a lexicon is represented by WORDNET DOMAINS 
(Magnini and Cavagli?, 2000). WORDNET 
DOMAINS is a lexical resource developed at ITC-
irst where each WordNet synset (Fellbaum, 1998) 
is annotated with one or more domain labels 
selected from a domain hierarchy which was 
specifically created to this purpose. As the 
WORDNET DOMAINS Hierarchy (WDH) is 
language-independent, it has been possible to 
exploit it in the framework of MultiWordNet 
(Pianta et al, 2002), a multilingual lexical database 
developed at ITC-irst in which the Italian 
component is strictly aligned with the English 
WordNet. In MultiWordNet, the domain 
information has been automatically transferred 
from English to Italian, resulting in a Italian 
version of WORDNET DOMAINS. For instance, as 
the English synset {court, tribunal, judicature} was 
annotated with the domain LAW, also the Italian 
synset {corte, tribunale}, which is aligned with the 
corresponding English synset, results automatically 
annotated with the LAW domain. This procedure 
can be applied to any other WordNet (or part of it) 
aligned with Princeton WordNet (see for instance 
the Spanish WordNet). 
It is worth noticing that two of the main on-
going projects addressing the construction of 
multilingual resources, that is MEANING (Rigau 
et al 2002) and BALKANET (see web site), make 
use of WORDNET DOMAINS. Finally, WORDNET 
DOMAINS is being profitably used by the NLP 
community mainly for Word Sense 
Disambiguation tasks in various languages. 
Another application of domain hierarchies can 
be found in the field of corpus creation. In many 
existing corpora (see for instance the BNC, the 
ANC, the Brown and LOB Corpora) domain is one 
of the most used criteria for text selection and/or 
classification. Given that a domain hierarchy is 
language independent, if the same domain 
hierarchy is used to build reference corpora for 
different languages, then it would be easy to create 
(a first approximation of) comparable corpora by 
putting in correspondence corpora sections 
belonging to the same domain. 
An example of a corpus in which the complete 
representation of domains is pursued in a 
systematic way is represented by the MEANING 
Italian corpus, a large size corpus of written 
contemporary Italian in which a subset of the 
WDH labels has been chosen as the fundamental 
criterion for the selection of the texts to be 
included in the corpus (Bentivogli et al, 2003). 
Given the relevance of language-independent 
domain hierarchies for multilingual applications, it 
is of primary importance that these resources have 
a well-defined semantics and structure in order to 
be useful in various application fields. This paper 
reports the work done to improve the WDH so that 
it complies with such requirements. In particular, 
the WDH revision has been carried out with 
reference to the Dewey Decimal Classification. 
The paper is organized as follows. Section 2 
briefly introduces the WORDNET DOMAINS 
Hierarchy and its main characteristics, with a short 
overview of the Dewey Decimal Classification 
system. Section 3 describes features and properties 
of the revision. Finally, in section 4, conclusions 
are reported. 
2 The WordNet Domains Hierarchy 
The first version of the WDH was composed of 
164 domain labels selected starting from the 
subject field codes used in current dictionaries, and 
the subject codes contained in the Dewey Decimal 
Classification (DDC), a general knowledge 
organization tool which is the most widely used 
taxonomy for library organization purposes. 
Domain labels were organized in five main trees, 
reaching a maximum depth of four. Figure 1 shows 
a fragment of one of the five main trees in the 
WORDNET DOMAINS original hierarchy. 
Doctrines
Psychology
Art
Religion
Psychoanalysis
Dance
Drawing
Music
Photography
Plastic Arts
Sculpture
Numismatics
Jewellery
Painting
Philately
Philosophy
Theatre
Mythology
Occultism
Roman Catholic
Theology
Figure 1: Fragment of the original WDH 
Domain labels were initially conceived to be 
application-oriented, that is, they have been 
integrated in WordNet with the main purpose of 
allowing the categorization of word senses and to 
provide useful information during the 
disambiguation process. 
The second level of WDH, where the so-called 
Basic Domains are represented, includes labels 
such as ART, SPORT, RELIGION and HISTORY, 
while in the third level a degree of major 
specialization is reproduced, and domains, like for 
example, DRAWING, PAINTING, TENNIS, 
VOLLEYBALL, and ARCHAEOLOGY can be found. For 
NLP tasks, the set of Basic Domains has proved to 
possess a suitable level of abstraction and 
granularity. 
Although the first version of WDH found many 
applications in different scenarios, it presented 
some problems. First, the domain labels did not 
have a defined semantics. The content of the labels 
could be suggested by the lexical meaning of their 
name, but there was no explicit indication about 
their intended interpretation. 
Second, it was not clear whether the Basic 
Domains met certain requirements such as 
knowledge coverage and balancing. In fact, the 
Basic Domains are supposed to possess a 
comparable degree of granularity and, at the same 
time, to cover all human knowledge. However, 
they did not always posses such characteristics. For 
instance VETERINARY was put at the same level as 
ECONOMY, although these two domains obviously 
do not posses the same level of granularity. 
Moreover not all branches of human knowledge 
were represented (see for instance the HOME 
domain). 
The purpose of the work presented here was, 
therefore, to find a solution for such problems, in 
order to improve the applicability of WDH in a 
wider range of fields. The solution we propose is 
crucially based on the Dewey Decimal 
Classification (edition 21), which has been used as 
a reference point for defining a clear semantics, 
preventing overlapping among domains, and 
assessing the Basic Domains coverage and 
granularity issues.  
2.1 The Dewey Decimal Classification (DDC)  
The Dewey Decimal Classification (DDC) system 
(Mitchell et al 1996) is the most widely used 
taxonomy for library classification purposes 
providing a logical system for the organization of 
every item of knowledge through well-defined 
subject codes hierarchically organized. The 
semantics of each subject code is determined by a 
numeric code, a short lexical description associated 
to it, and by the hierarchical relations with the 
other subject codes. Another characteristic of the 
DDC is that a handbook is available explaining 
how texts should be classified under subject codes. 
The DDC is not just for organizing book 
collections; it has also been licensed for 
cataloguing internet resources (see for example 
BUBL http://bubl.ac.uk/link/) and it was conceived 
to accommodate the expansion and evolution of 
the body of human knowledge.  
The DDC hierarchy is arranged by disciplines 
(or fields of study), and this entails that a subject 
may appear in more than one discipline, depending 
on the aspect of the topic discussed.  
The DDC hierarchical structure allows a topic to 
be defined as part of the broader topic above it, and 
that determines the meaning of the class and its 
relation to other classes. At the broadest level, 
called Main Classes (or First summary), the DDC 
is composed of ten mutually exclusive main 
classes, which together cover the entire world of 
knowledge. Each main class is sub-divided into ten 
divisions, (the Hundred Divisions, or Second 
Summary) and each division is split into ten 
sections (the Thousand Section, also called Third 
Summary). 
Each category in the DDC is represented by a 
numeric code as the example below shows.  
 
700  Art 
 730  Plastic Arts 
  736 Carving 
   736.2 Precious Stones 
    736.23 Diamonds 
    736.25 Sapphires 
   736.4 Wood 
  738 Ceramic Arts 
  739 Art Metalwork 
 740  Drawing 
 750  Painting 
 
The first digit of the numbers indicates the main 
class, (700 is used for all Arts) the second digit 
indicates the hundred division, (730 corresponds to 
Plastic arts, 740 to Drawing, 750 to Painting) and 
the third digit indicates the section (736 represents 
Carving, 738 Ceramic arts, 739 Art metalwork). 
Moreover, almost all sub-classes are further 
subdivided. A decimal point follows the third digit 
until the degree of specification needed (736.23 
Diamonds, 736.25 Sapphires).  
3 The Revision of the WDH 
The revision of the first version of the WDH aimed 
at satisfying the following properties and 
characteristics:  
 
o semantics: each WDH label should have an 
explicit semantics and should be 
unambiguously identified; 
o disjunction: the interpretation of all WDH 
labels should not overlap; 
o basic coverage: all human knowledge should 
be covered  by the Basic Domains; 
o basic balancing: most Basic Domains should 
have a comparable degree of granularity. 
 
In the following sections we are going to show 
how a systematic mapping between WDH and 
DDC can be used to enforce each of the above 
characteristics.  
3.1 Semantics 
To give the domain labels a clear semantics so that 
they can be unambiguously identified and 
interpreted, we decided to associate each domain 
label to one or more DDC codes as shown below in 
Table 1.  
WDH Domains 
 
DDC Codes 
 
 Art 
 
[700-(790-(791.43,792,793.3), 
          710,720,745.5)] 
       Plastic arts 730 
                   Sculpture [731:735] 
                   Numismatics 737 
       Jewellery 739.27 
       Drawing [740-745.5] 
       Painting 750 
       Graphic arts 760 
                   Philately 769.56 
       Photography 770 
       Music 780 
       Cinema 791.43 
       Theatre [792-792.8] 
       Dance [792.8,793.3] 
Table 1: Fragment of the new WDH with the 
respective DDC codes 
In many cases we found a one-to-one mapping 
between a WDH label and a DDC code (e.g. 
PAINTING mapped onto 750 or CINEMA onto 
791.43). When one-to-one mappings were not 
found, artificial DDC codes were created. An 
artificial code, represented within square brackets, 
is created with reference to various DDC codes or 
parts of them. To describe artificial nodes, certain 
conventions have been adopted.  
(i) A series of non-consecutive codes is listed 
separated by a comma (see DANCE). 
(ii) A series of consecutive codes is indicated by a 
range. For instance, the series [731, 732, 733, 734, 
735] is abbreviated as [731:735] (see SCULPTURE). 
(iii) A part of a tree is represented as the difference 
between a tree and one or more of its subtrees, 
where the tree and the subtrees are identified by 
their roots (see DRAWING). 
(iv) The square brackets should be interpreted as 
meaning ?the generalities? of the composition of 
codes contained in the brackets. So, for instance, 
[731:735] should be interpreted as the generalities 
of the codes going from 731 to 735. In the original 
DDC, generalities are identified by the 0 decimal. 
For instance, the code 700 refers to the generalities 
of the codes from 710 to 790. 
To establish a mapping between labels and codes 
we exploited the names of the DDC categories and 
their description in the DDC manual. This worked 
pretty well in most cases, but there are some 
exceptions. Take for instance the TOURISM domain. 
Apparently tourism does not occur as a category in 
the DDC. On a closer inspection it came out that 
the categories which are most clearly related to 
tourism are 910.202:World travel guides and 
910.4:Accounts of travel. 
Note that a WDH domain can be mapped onto 
codes included in different DDC main classes, i.e. 
disciplines. For example ARTISANSHIP 
(745.5:Handicrafts, 338.642:Small business) maps 
onto categories located partly under 700:Art and 
partly under 300:Social Sciences. The same 
happens with SEXUALITY, a domain that following 
the DDC is studied by many different disciplines, 
e.g. philosophy, medicine, psychology, body care. 
As a consequence of the systematic specification 
of the semantics of the WDH domains, some of 
them have been re-labeled with regard to the 
previous version of the hierarchy. For instance, the 
domain BOTANY has been changed to PLANTS, 
ZOOLOGY to ANIMALS, and ALIMENTATION to FOOD. 
This change of focus from the name of the 
discipline to the name of the object of the 
discipline is not only in compliance with the new 
edition of the DDC, but it also reflects current and 
international usage (see, for example, Google 
categories). In some cases the change of the 
domain name comes along with a change of its 
intended interpretation. For instance, we have 
decided to enlarge the semantics of the domain 
ZOOTECHNICS and to call it ANIMAL HUSBANDRY, a 
more generic domain which was missing in the 
previous hierarchy.  
In most cases the hierarchical relations between 
the WDH domains are the same as the relations 
holding between the corresponding DDC codes: 
MUSIC is more specific than ART in the same way 
as 780:Music is more specific than 700:The Arts. 
To reinforce the hierarchical parallelism between 
the WDH and the DCC, we re-located some 
domains with regard to the previous WDH 
hierarchy. For example, OCCULTISM, which was 
placed under RELIGION in the old hierarchy, has 
been moved under the newly created domain 
PARANORMAL. Also, TOPOGRAPHY, previously placed 
under ASTRONOMY, has now been moved under 
GEOGRAPHY.  
In a few cases however we did not respect the 
hierarchical relations specified by the DDC, as in 
the case of the ARCHITECTURE domain shown in 
Table 2. ARCHITECTURE has been mapped onto 
720:Architecture and TOWN PLANNING onto 
710:Civic & landscape art.  
WDH Domains DDC Codes 
 Architecture  [645,690,710,720] 
 
Town Planning 710 
 
Buildings 690 
 
Furniture 645 
Table 2: A fragment of WDH for ARCHITECTURE 
However, whereas the 710 code is sibling of 720 
in the DDC, TOWN PLANNING is child of 
ARCHITECTURE in WDH. Also, ARCHITECTURE and 
TOWN PLANNING should be under ART according to 
the DDC, but they have been placed under 
APPLIED SCIENCE in WDH. 
3.2 Disjunction 
This property requires that no DDC code is 
associated to more than one WDH label. In only 
one case this requirement has not been met. 
Apparently, the DDC does not distinguish between 
the disciplines of Sociology and Anthropology, 
and reserves the codes that go from 301 to 307 to 
both of them. Although these two disciplines are 
strictly connected, it seems to us that in the current 
practice they are considered as distinct. So the 
WDH contains two distinct domains for 
SOCIOLOGY and ANTHROPOLOGY, which partially 
overlap because they both map onto the same DDC 
codes 301:307. 
3.3 Basic Coverage 
The term basic coverage refers to the ideal 
requirement that all human knowledge be covered 
by the totality of the Basic Domains (i.e. the 
domains composing the second level of WDH). 
Also in this case, we used the DDC as a gold 
standard to measure the coverage of WDH. Given 
the fact that the DDC has been used for more than 
a century to classify books and written documents 
all over the world, we can assume that the DDC 
guarantees a complete representation of all 
branches of knowledge. So the basic coverage has 
been manually checked by verifying that all (or 
almost all) the DDC categories can be assigned to 
at least one Basic Domain.  
From a practical point of view, it would be very 
complicated to check all the thousands of codes 
contained in the DDC. Thus, our check relied on 
two assumptions. First, when the Basic Domains 
are taken as a stand alone set, the semantics of a 
Basic Domain is given by its specific code together 
with the codes of its subdomains. Second, once a 
DDC code is covered by a Basic Domain, 
inductively, all the more specific categories are 
covered as well. These assumptions allowed us to 
actually check only the topmost DDC codes. For 
example, let?s take the 300 main class of the DDC. 
Table 3 below shows that all the sub-codes of the 
300 class are covered by one or more domains.  
In order to improve the overall WDH coverage, 
5 completely new domains have been introduced 
(the first three are Basic): PARANORMAL, HOME, 
HEALTH, FINANCE and GRAPHIC ARTS. 
Codes DDC Categories WDH Domains 
300 ? Social sciences 
? SOCIAL SCIENCE 
? SOCIOLOGY 
? ANTHROPOLOGY 
310 ? General statistics ? SOCIOLOGY 
320 ? Political science ? POLITICS 
330 ? Economics ? ECONOMY 
340 ? Law ? LAW 
350 ? Public administration & military service 
? ADMINISTRATION  
? MILITARY 
360 ? Social problems & 
services 
? SOCIOLOGY 
? ECONOMY 
? SEXUALITY 
370 ? Education ? PEDAGOGY 
380 
? Commerce, 
communication, 
transport 
? COMMERCE  
? TELECOMMUNICATION  
? TRANSPORT 
390 ? Customs, etiquette, folklore 
? FASHION  
? ANTHROPOLOGY 
? SEXUALITY  
Table 3: Coverage of the 300 DDC class 
We can now assume that the domain-coverage of 
the new version of WDH is almost equivalent to 
that of the DDC, thus ensuring the complete 
representation of all branches of knowledge. 
The new WDH allowed us to fix a number of 
synset classifications that were unsatisfactory in 
the previous version of WORDNET DOMAINS. For 
instance, in the first version of WORDNET 
DOMAINS the English/Italian synset {microwave 
oven, microwave}/{forno a microonde, 
microonde} was annotated with the FURNITURE 
domain, while the synset {detergent}/{detersivo} 
was annotated with FACTOTUM (i.e. no specific 
domain) as no better solution was available. The 
new WDH hierarchy allows for a more appropriate 
classification of both synsets within the new HOME 
domain. 
A few DDC codes are not covered by the new 
list of domains either. These are the codes under 
the 000:Generalities class which includes 
disciplines such as 010:Bibliography, 020:Library 
& information sciences, 030:Encyclopedic works, 
080:General collections. This section has been 
specifically created for cataloguing general and 
encyclopedic works and collections. So it is a 
idiosyncratic category which is not based on 
subject but on the genre of texts. 
Another set of codes which remains not covered 
by WDH are those going from 420 to 490 and from 
810 to 890. These DDC codes are devoted to 
specific languages and literatures of different 
countries, for example, 430:Germanic Languages, 
440:Romance Languages, 810:American Literature 
in English, etc. These codes are undoubtedly 
relevant for the classification of books, but are not 
compatible with the rationale of WDH, which is 
meant to be a language-independent resource. 
3.4 Basic Balancing 
The requirement about basic balancing is meant to 
assure that all Basic Domains have a comparable 
degree of granularity. 
Defining a granularity metrics for domains is a 
complex issue, for which only a tentative solution 
is provided here. At a first glance, three aspects 
could be taken into consideration: the number of 
publications about a domain, the number of sub-
codes in the DDC, and the relevance of a domain 
in the social life.  
As a first attempt, balancing could be evaluated 
referring to the number of publications classified 
under each Basic Domain. In fact, data are 
available about the number of texts classified 
under each of the DDC codes. Unfortunately, the 
number of books published under a certain 
category may not be indicative of its social 
relevance: very specialized domains may include a 
high number of publications, which however 
circulate in a restricted circle, with low social 
impact. For example, the number of texts classified 
in the History domain turns out to be more then ten 
times the number of texts catalogued under the 
Computer Science domain. However, if one looks 
at the number of HTML pages available on the 
Internet, or the number of magazines sold in a 
newspaper stand, or the number of terms used in 
everyday life, one cannot maintain that History is 
ten times more relevant than Computer Science. 
Another approach for evaluating the granularity 
of domains could be to take into account the 
number of DDC sub-codes corresponding to each 
Basic Domain. Unfortunately, also this approach 
gives results which are far from being satisfactory. 
The fact that a discipline has many subdivisions 
seems not to be clearly correlated with its 
relevance. For instance in the DDC manual 
(version 21) 105 pages can be put in 
correspondence with the ENGINEERING domain, 
whereas only 26 correspond to SPORT. It should 
also be said that there is no correlation between the 
number of publications and the number of sub-
categories in the DDC. For instance, 
ARCHITECTURE has a great number of publications 
classified under it, but on the contrary, the number 
of sub-categories in the DDC is very limited. 
The third criterion to evaluate the granularity of 
domains is their social relevance, which seems not 
to be captured adequately by the previous two 
criteria. Of course, social relevance is very difficult 
to evaluate. We tentatively took into consideration 
the organization of Internet hierarchies such as the 
Google and Yahoo directories, which seem to be 
closer than the DDC to represent the current social 
relevance of certain domains. See for instance the 
huge number of HTML pages classified in Google 
under the topic Television Programs. Of course 
Internet is only a partial view of the organization 
of human knowledge, so we cannot simply rely on 
the Internet to evaluate the granularity of the 
domains. 
None of the approaches analyzed so far seems to 
fit our needs. Thus we took into consideration a 
fourth criterion, which is based on the DDC as 
well. Instead of counting the number of 
subdivisions under a certain DDC code, we 
measured the depth of the code from the top of the 
hierarchy. For instance we can say that 700:Art has 
depth 1, 780:Music has depth 2, 782:Vocal Music 
has depth 3, and so on. We make the assumption 
that two DDC codes with the same depth have the 
same granularity. For instance we assume that 
782:Vocal Music and 382:Foreign Trade have the 
same granularity (both have depth 3).  
In order to evaluate the granularity of the Basic 
Domains against the DDC, we can compare WDH 
labels and DDC codes with the same depth. Given 
that the Basic Domains have depth 2, we should 
compare them to the so called Hundred Divisions 
(000, 010, 020, 030, ?, 100, 110, 120, etc.). 
Summing up, we will say that the Basic Domains 
are balanced if they can all be mapped onto the 
Hundred Divisions. Also, in the comparison we 
should take into account that the Basic Domains 
are 45, whereas the Hundred Divisions are 100. So, 
we expect that in the average, one Basic Domain 
maps onto two Hundred Divisions with a small 
degree of variance with respect to the average.  
What we have obtained from the analysis of the 
new WDH is the following: out of 45 Basic 
Domains 
 
o 4 domains map onto a Main Class (depth 1) 
o 18 domains are mapped at the Hundred 
Divisions level (depth 2) 
o 6 domains are mapped at different DDC levels, 
with the majority of DDC codes at depth 2 
o 17 domains map onto subdivisions of depth 3 
and 4. 
 
As for the average number of DDC codes 
covered by each Basic Domain, the variance is 
quite high. Certain Basic Domains cover a big 
number of codes from the Hundred Divisions. For 
instance HISTORY, and ART cover 6 codes each. 
Instead, in  most cases, one Basic Domain covers 
only one DDC code (e.g. LAW and 340:Law). 
The evaluation of the granularity of the Basic 
Domains according to the proposed criterion can 
be considered satisfactory even if the results 
diverge somewhat from what expected in principle.  
To explain this partial divergence in the 
granularity of domains, one should take into 
consideration that the DDC has been created 
relying heavily on the academic organization of 
knowledge disciplines. On the other side, in the 
practical WDH reorganization process we tried to 
balance somehow this discipline-oriented 
approach, by taking into account also the social 
relevance of domains. This has been done by 
relying on the organization of Internet directories 
and on our personal intuitions. 
Such an approach led us to put at the Basic level 
WDH labels corresponding to DDC codes with 
depth higher than 2 (more specific than the 
Hundreds Divisions). See for instance the 
positioning of RADIO+TV, FOOD, HEALTH, and 
ENVIRONMENT at the Basic level, even if they 
correspond to DDC codes of level 3 and 4.  
Instead, ANIMALS and PLANTS were not Basic in 
the previous version of WDH, but have been 
promoted to the Basic level in accordance with the 
granularity level they have in the DDC.  
Other domain labels have been placed at a lower 
level then expected with reference to the DDC. For 
instance PHILOSOPHY, ART, RELIGION, and 
LITERATURE have been put at the Basic Level, 
even if they correspond to DDC codes belonging to 
the Main Classes (depth 1). On the other side 
ASTROLOGY, ARCHAEOLOGY,  BODY CARE, and  
VETERINARY which were Basic in the previous 
version of the WDH, have been demoted at a lower 
level in accordance with the granularity they have 
in the DDC. Only in one case this process of 
demotion has led to the elimination of a sub-
domain, that is TEXTILE.  
4 Conclusions 
In this paper we described the revision of the 
WORDNET DOMAINS Hierarchy (WDH), with the 
aim of providing it with a clear semantics, and 
evaluating the coverage and balancing of a subset 
of the WDH, called Basic Domains. This has been 
done mostly by relying on the information 
available in the Dewy Decimal Classification 
(DDC). A semantics has been provided to the 
WDH labels by defining one or more pointers to 
DDC codes. The coverage of the Basic Domains 
has been evaluated by checking that each DDC 
code is covered by at least one Basic Domain. 
Finally, balancing has been evaluated mostly by 
comparing the granularity of the Basic Domains 
with the granularity of a subset of the DDC called 
the Hundred Divisions. Balancing is the aspect of 
the Basic Domains which diverges more clearly 
from the DDC. This is explained by the fact that 
we took in higher consideration the social 
relevance of domains. 
We think that the new version of the WDH is 
better suited to act as a useful language-
independent resource in the fields of computational 
lexicography, corpus building, and various NLP 
applications.  
5 Acknowledgements 
Thanks to Alfio Gliozzo for his useful comments 
and suggestions about how to improve the 
WORDNET DOMAINS Hierarchy. 
References  
BALKANET http://www.ceid.upatras.gr/Balkanet/ 
L. Bentivogli, C. Girardi and E. Pianta. 2003. The 
MEANING Italian Corpus. In Proceedings of the 
Corpus Linguistics 2003 Conference. Lancaster, 
United Kingdom. 
C. Fellbaum. 1998. WordNet. An Electronic 
Lexical Database. The MIT Press, Boston. 
B. Magnini and G. Cavagli?. 2000. Integrating 
Subject Field Codes into WordNet. In 
Proceedings of LREC-2000. Athens, Greece. 
B. Magnini, C. Strapparava, G. Pezzulo and A. 
Gliozzo. 2002. The Role of Domain Information 
in Word Sense Disambiguation. Journal of 
Natural Language Engineering (Special Issue on 
evaluating Word Sense Disambiguation 
Systems), 9(1):359:373. 
J.S. Mitchell, J. Beall, W.E. Matthews and G.R. 
New (eds). 1996. Dewey Decimal Classification  
Edition 21 (DDC 21). Forest Press, Albany, New 
York. 
E. Pianta, L. Bentivogli and C. Girardi. 2002. 
MultiWordNet: developing an aligned 
multilingual database. In Proceedings of the 
First Global WordNet Conference. Mysore, 
India. 
G. Rigau, B. Magnini, E. Agirre, P. Vossen and J. 
Carrol. 2002. MEANING: a Roadmap to 
Knowledge Technologies. In Proceedings of the 
COLING-2002 workshop "A Roadmap for 
Computational Linguistics". Taipei, Taiwan. 
H. Schutze. 1998. Automatic Word Sense 
Discrimination. Computational Linguistics, 
24(1):97-123. 
SIMPLE. 2000. Linguistic Specifications. 
Deliverable D2.1, March 2000.  
P. Vossen (ed). 1998. Computers and the 
Humanities (Special Issue on EuroWordNet), 
32(2-3). 
D.E. Walker and R.A. Amsler. 1986. Analyzing 
Language in Restricted Domain. Sublanguage 
description and Processing. Lawrence Earlbaum, 
Hillsdale NJ.  
Appendix : The first two levels of the WDH new version with the corresponding DDC codes 
 
TOP-LEVEL BASIC DOMAINS DDC 
Humanities   
 History [920:990] 
 Linguistics 410 
 Literature [800, 400] 
 Philosophy [100-(130, 150, 176)] 
 Psychology 150 
 Art [700-(710, 720, 745.5, 790-(791.43, 792, 793.3))] 
 Paranormal 130 
 Religion 200 
   
Free_Time  [790-(791.43, 792, 793.3)] 
 Radio-Tv [791.44, 791.45] 
 Play [793.4:795-794.6] 
 Sport [794.6, 796:799] 
   
Applied_Science  600 
 Agriculture [338.1, 630] 
 Food [613.2, 613.3, 641, 642] 
 Home [640-(641, 642, 645)] 
 Architecture [645, 690, 710, 720] 
 Computer_Science [004:006] 
 Engineering 620 
 Telecommunication [383, 384] 
 Medicine [610-(611, 612, 613)] 
   
Pure_Science  500 
 Astronomy  520 
 Biology [570-577, 611, 612-612.6] 
 Animals  590 
 Plants 580 
 Environment  577 
 Chemistry  540 
 Earth  [550, 560, 910-(910.4, 910.202)] 
 Mathematics 510 
 Physics  530 
   
Social_Science  [300.1:300.9] 
 Anthropology [301:307, 395, 398] 
 Health [613-(613.2, 613.3, 613.8, 613.9)] 
 Military [355:359] 
 Pedagogy 370 
 Publishing 070 
 Sociology [301:319-(305.8, 306.7), 360-(363.4, 368)] 
 Artisanship [338.642, 745.5] 
 Commerce [381, 382] 
 Industry [338-(338.1, 338.642), 660, 670, 680] 
 Transport [385:389] 
 Economy [330-(334, 338), 368, 650] 
 Administration [351:354] 
 Law 340 
 Politics 320 
 Tourism [910.202, 910.4] 
 Fashion [390-(392.6, 395, 398), 687] 
 Sexuality [155.3, 176, 306.7, 363.4, 392.6, 612.6, 613.96] 
   
 Factotum  
 
 
Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 26?32,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Ontology Population from Textual Mentions:
Task Definition and Benchmark
Bernardo Magnini, Emanuele Pianta, Octavian Popescu and
Manuela Speranza
ITC-irst, Istituto per la Ricerca Scientifica e Tecnologica
Via Sommarive 18, 38050 Povo (TN), Italy
{magnini, pianta, popescu, manspera}@itc.it
Abstract
In this paper we propose and investigate
Ontology Population from Textual Mentions
(OPTM), a sub-task of Ontology Population
from text where we assume that mentions for
several kinds of entities (e.g. PERSON,
O R G A N I Z A T I O N , LO C A T I O N , GEO-
POLITICAL_ ENTITY) are already extracted
from a document collection. On the one
hand, OPTM simplifies the general Ontology
Population task, limiting the input textual
material; on the other hand, it introduces
challenging extensions to Ontology Popula-
tion restricted to named entities, being open
to a wider spectrum of linguistic phenomena.
We describe a manually created benchmark
for OPTM and discuss several factors which
determine the difficulty of the task.
1 Introduction
Mentions are portions of text which refer to enti-
ties
1
. As an example, given a particular textual
context, both the mentions ?George W. Bush?
and ?the U.S. President.? refer to the same entity,
i.e. a particular instance of Person whose first
name is ?George?, whose middle initial is ?W.?,
whose family name is ?Bush? and whose role is
?U.S. President?.
In this paper we propose and investigate Ontol-
ogy Population from Textual Mentions (OPTM),
a sub-task of Ontology Learning and Population
                                                 
1
 The terms ?mention? and ?entity? have been intro-
duced within the ACE Program (Linguistic Data Con-
sortium, 2004). ?Mentions? are equivalent to ?refer-
ring expressions? and ?entities? are equivalent to
?referents?, as widely used in computational linguis-
tics. In this paper, we use italics for ?mentions? and
small caps for ENTITY and ENTITY_ATTRIBUTE.
(OLP) from text where we assume that mentions
for several kinds of entities (e.g. PERSON,
ORGANIZATION, LO C A T I O N , GEO-POLITICAL
_ENTITY) are already extracted from a document
collection.
We assume an ontology with a set of classes
C={c
1
, ?, c
n
} with each class c
1
 being described
by a set of attribute value pairs [a
1
, v
1
]. Given a
set of mentions M={m
1,c1
, ?,  m
n,cn
}, where each
mention m
j
 is classified into a class c
i
 in C, the
OPTM task is defined in three steps: Recognition
and Classification of Entity Attributes, Normali-
zation, and Resolution of inter-text Entity Co-
reference.
(i) Recognition and Classification of Entity
Attributes (RCEA). The textual material
expressed in a mention is extracted and dis-
tributed along the attribute-value pairs al-
ready defined for the class c
i
 of the mention;
as an example, given the PERSON mention
?U.S. President Bush?, we expect that the
attribute LAST_NAME is filled with the value
?Bush? and the attribute ROLE is filled with
the value ?U.S. President?. Note that fillers,
at this step, are still portions of text.
(ii) Normalization. The textual material ex-
tracted at step (i) is assigned to concepts and
relations already defined in the ontology; for
example, the entity BUSH is created as an in-
stance of COUNTRY_PRESIDENT, and an in-
stance of the relation PRESIDENT_OF is cre-
ated between BUSH and U.S.A. At this step
different instances are created for co-
referring mentions.
(iii) Resolution of inter-text Entity Co-
reference (REC). Each mention m
j
 has to be
assigned to a single individual entity be-
longing to a class in C . For example, we
recognize that the instances created at step
(i) for ?U.S. President Bush? and ?George
W. Bush? actually refer to the same entity.
26
In this paper we address steps (i) and (iii),
while step (ii) is work in progress. The input of
the OPTM task consists of classified mentions
and the output consists of individual entities
filled with textual material (i.e. there is no nor-
malization) with their co-reference relations. The
focus is on the definition of the task and on an
empirical analysis of the aspects that determine
its complexity, rather than on approaches and
methods for the automatic solution of OPTM.
There are several advantages of OPTM which
make it appealing for OLP. First, mentions pro-
vide an obvious simplification with respect to the
more general Ontology Population from text (cf.
Buitelaar et al 2005); in particular, mentions are
well defined and there are systems for automatic
mention recognition. Although there is no univo-
cally accepted definition for the OP task, a useful
approximation has been suggested by
(Bontcheva and Cunningham, 2005) as Ontology
Driven Information Extraction with the goal of
extracting and classifying instances of concepts
and relations defined in a Ontology, in place of
filling a template. A similar task has been ap-
proached in a variety of perspectives, including
term clustering (Lin, 1998 and Almuhareb and
Poesio, 2004) and term categorization (Avancini
et al 2003). A rather different task is Ontology
Learning, where new concepts and relations are
supposed to be acquired, with the consequence of
changing the definition of the Ontology itself
(Velardi et al 2005). However, since mentions
have been introduced as an evolution of the tra-
ditional Named Entity Recognition task (see
Tanev and Magnini, 2006), they guarantee a rea-
sonable level of difficulty, which makes OPTM
challenging both for the Computational Linguis-
tic side and the Knowledge Representation
community. Second, there already exist anno-
tated data with mentions, delivered under the
ACE (Automatic Content Extraction) initiative
(Ferro et al 2005, Linguistic Data Consortium
2004), which makes the exploitation of machine
learning based approaches possible. Finally,
having a limited scope with respect to OLP, the
OPTM task allows for a better estimation of per-
formance; in particular, it is possible to evaluate
more easily the recall of the task, i.e. the propor-
tion of information correctly assigned to an en-
tity out of the total amount of information pro-
vided by a certain mention.
In the paper we both define the OPTM task
and describe an OPTM benchmark, i.e. a docu-
ment collection annotated with mentions as well
as an ontology where information from mentions
has been manually extracted. The general archi-
tecture of the OPTM task has been sketched
above, considering three sub tasks. The docu-
ment collection we use consists of about 500
Italian news items. Currently, mentions referring
to PE R S O N , ORGANIZATION and GEO-
POLITICAL_ ENTITY have been annotated and co-
references among such mentions have been es-
tablished. As for the RCEA sub task, we have
considered mentions referring to PERSON and
have built a knowledge base of instances, each
described with a number of attribute-value pairs.
The paper is structured as follows. Section 2
provides the useful background as far as men-
tions and entities are concerned. Section 3 de-
fines the OPTM task and introduces the dataset
we have used, as well as the annotation proce-
dures and guidelines we have defined for the re-
alization of the OPTM benchmark corpus. Sec-
tion 4 reports on a number of quantitative and
qualitative analyses of the OPTM benchmark
aimed at determining the difficulty of the task.
Finally, Section 5 proposes future extensions and
developments of our work.
2 Mentions and Entities
As indicated in the ACE Entity Detection
task, the annotation of entities (e.g. PERSON,
ORGANIZAT I O N , LOCAT I O N  a n d  GEO-
POLITICAL_ENTITY) requires that the entities
mentioned in a text be detected, their syntactic
head marked, their sense disambiguated, and that
selected attributes of these entities be extracted
and merged into a unified representation for each
entity.
As it often happens that the same entity is
mentioned more than once in the same text, two
inter-connected levels of annotation have been
defined: the level of the entity, which provides a
representation of an object in the world, and the
level of the entity mention, which provides in-
formation about the textual references to that
object.  For instance, if  the entity
GEORGE_W._BUSH (e.g. the individual in the
world who is the current president of the U.S.) is
mentioned in two different sentences of a text as
?the U.S. president? and as ?the president?, these
two expressions are considered as two co-
referring entity mentions.
The kinds of reference made by entities to
something in the world are described by the fol-
lowing four classes:
? specific referential entities are those where
the entity being referred to is a unique object
27
or set of objects (e.g. ?The president of
thecompany is here?)
 2
;
? generic referential entities refer to a kind or
type of entity and not to a particular object (or
set of objects) in the world (e.g. ?The presi-
dent is elected every 5 years?);
? under-specified referential entities are non-
generic non-specific references, including im-
precise quantifications (e.g. ?everyone?) and
estimates (e.g. ?more than 10.000 people?);
? negatively quantified entities refer to the
empty set of the mentioned type of object (e.g.
?No lawyer?).
The textual extent of mentions is defined as
the entire nominal phrase used to refer to an en-
tity, thus including modifiers (e.g. ?a big fam-
ily?), prepositional phrases (e.g. ?the President of
the Republic?) and dependent clauses (e.g. ?the
girl who is working in the garden?).
The classification of entity mentions is based
on syntactic features; among the most significant
categories defined by LDD (Linguistic Data
Consortium 2004) there are:
- NAM: proper names (e.g. ?Ciampi?, ?the
UN?);
- NOM: nominal constructions (e.g. ?good chil-
dren?, ?the company?);
- PRO: pronouns, e.g. personal (?you?) and in-
definite (?someone?);
- WHQ: wh-words, such as relatives and inter-
rogatives (e.g. ?Who?s there??);
- PTV: partitive constructions (e.g. ?some of
them?, ?one of the schools?);
- APP: appositive constructions (e.g. ?Dante,
famous poet? , ?Juventus, Italian football
club?).
Since the dataset presented in this paper has
been developed for Italian, some new types of
mentions have been added to those listed in the
LDC guidelines; for instance, we have created a
specific tag, ENCLIT, to annotate the clitics
whose extension can not be identified at word-
level (e.g. ?veder[lo]?/?to see him?). Some types
of mentions, on the other hand, have been elimi-
nated; this is the case for pre-modifiers, due to
syntactic differences between English, where
both adjectives and nouns can be used as pre-
modifiers, and Italian, which only admits adjec-
tives in that position.
In extending the annotation guidelines, we
have decided to annotate all conjunctions of en-
tities, not only those which share the same modi-
fiers as indicated in the ACE guidelines, and to
mark them using a specific new tag, CONJ (e.g.
                                                 
2
 Notice that the corpus is in Italian, but we present English
examples for the sake of readability.
?mother and child?)
3
.
According to the ACE standards, each dis-
tinct person or set of people mentioned in a
document refers to an entity of type PERSON. For
example, people may be specified by name
(?John Smith?), occupation (?the butcher?),
family relation (?dad?), pronoun (?he?), etc., or
by some combination of these.
PERSON (PE), the class we have considered
for the Ontology Population from Textual Men-
tion task, is further classified with the following
subtypes:
? INDIVIDUAL_PERSON: PES which refer to a
single person (e.g. ?George W. Bush?);
? GROUP_PERSON: PES which refer to more than
one person (e.g. ?my parents?, ?your family?,
etc.);
? INDEFINITE_PERSON: a PE is classified as in-
definite when it is not possible to judge from
the context whether it refers to one or more
persons (e.g. ?I wonder who came to see me?).
3 Task definition
In Section 3.1 we first describe the document
collection we have used for the creation of the
OPTM benchmark. Then, Section 3.2 provides
details about RCEA, the first step in OPTM.
3.1 Document collection
The OPTM benchmark is built on top of a
document collection (I-CAB, Italian Content
Annotated Bank)
4
 annotated with entity men-
tions. I-CAB (Magnini et al 2006) consists of
525 news documents taken from the local news-
paper ?L?Adige?
5
. The selected news stories be-
long to four different days (September, 7th and
8th 2004 and October, 7th and 8th 2004) and are
grouped into five categories: News Stories, Cul-
tural News, Economic News, Sports News and
Local News (see Table 1).
09/07 09/08 10/07 10/08 Total
News 23 25 18 21 87
Culture 20 18 16 18 72
Economy 13 15 12 14 54
Sport 29 41 27 26 123
Local 46 43 49 51 189
TOTAL 131 142 122 130 525
Table 1: Number of news stories per category.
                                                 
3
 Appositive and conjoined mentions are complex construc-
tions. Although LDC does not identify heads for complex
constructions, we have decided to annotate all the extent as
head.
4
 A demo is available at http://ontotext.itc.it/webicab
5
 http://www.ladige.it/
28
I-CAB is further divided into training and
test sections, which contain 335 and 190 docu-
ments respectively. In total, I-CAB consists of
around 182,500 words: 113,500 and 69,000
words in the training and the test sections re-
spectively (the average length of a news story is
around 339 words in the training section and 363
words in the test section).
The annotation of I-CAB is being carried out
manually, as we intend I-CAB to become a
benchmark for various automatic Information
Extraction tasks, including recognition and nor-
malization of temporal expressions, entities, and
relations between entities (e.g. the relation af-
filiation connecting a person to the organization
to which he or she is affiliated).
3.2 Recognition and Classification
As stated in Section 1, we assume that for
each type of entity there is a set of attribute-value
pairs, which typically are used for mentioning
that entity type. The same entity may have dif-
ferent values for the same attribute and, at this
point no normalization of the data is made, so
there is no way to differentiate between different
values of the same attribute, e.g. there is no
stipulation regarding the relationship between
?politician? and ?political leader?. Finally, we
currently assume a totally flat structure among
the possible values for the attributes.
The work we describe in this Section and in
the next one concerns a pilot study on entities of
type PERSON. After an empirical investigation on
the dataset described in Section 3.1 we have as-
sumed that the attributes listed in the first column
of Table 2 constitute a proper set for this type of
entity. The second column lists some possible
values for each attribute.
The textual extent of a value is defined as the
maximal extent containing pertinent information.
For instance, if we have a person mentioned as
?the thirty-year-old sport journalist?, we will
select ?sport journalist? as value for the attribute
ACTIVITY. In fact, the age of the journalist in not
pertinent to the activity attribute and is left out,
whereas ?sport? contributes to specifying the
activity performed.
As there are always less paradigmatic values
for a given attribute, we shortly present further
the guidelines in making a decision in those
cases. Generally, articles and prepositions are not
admitted at the beginning of the textual extent of
a value, an exception being made in the case of
articles in nicknames.
Attributes Possible values
FIRST_NAME Ralph, Greg
MIDDLE_NAME J., W.
LAST_NAME McCarthy, Newton
NICKNAME Spider, Enigmista
TITLE prof., Mr.
SEX actress
ACTIVITY
AFFILIATION
ROLE
journalist, doctor
The New York Times
director, president
PROVENIENCE South American
FAMILY_RELATION father, cousin
AGE_CATEGORY boy, girl
MISCELLANEA The men with red shoes
Table 2. Attributes for PERSON.
Typical examples for the TITLE attribute are
?Mister?, ?Miss?, ?Professor?, etc. We consider
as TITLE the words which are used to address
people with special status, but which do not refer
specifically to their activity. In Italian, profes-
sions are often used to address people (e.g. ?av-
vocato/lawyer?, ?ingegnere/engineer?). In order
to avoid a possible overlapping between the
TITLE attribute and the ACTIVITY attribute, pro-
fessions are considered values for title only if
they appear in abbreviated forms (?avv.?, ?ing.?
etc.) before a proper name.
With respect to the SEX attribute, we con-
sider as values all the portions of text carrying
this information. In most cases, first and middle
names are relevant. In addition, the values of the
SEX attribute can be gendered words (e.g. ?Mis-
ter? vs. ?Mrs.?, ?husband? vs. ?wife?) and words
from grammatical categories carrying informa-
tion about gender (e.g. adjectives).
The attributes A CTIVITY, RO L E , AF -
FILIATION are three strictly connected attributes.
ACTIVITY refers to the actual activity performed
by a person, while ROLE refers to the position
they occupy. So, for instance, ?politician? is a
possible value for ACTIVITY, while ?leader of the
Labour Party? refers to a ROLE. Each group of
these three attributes is associated with a mention
and all the information within a group has to be
derived from the same mention. If different
pieces of information derive from distinct men-
tions, we will have two separate groups. Con-
sider the following three mentions of the same
entity:
29
(1) ?the journalist of Radio Liberty?
(2) ?the redactor of breaking news?
(3) ?a spare time astronomer?
These three mentions lead to three different
groups of ACTIVITY, ROLE and AFFILIATION.
The obvious inference that the first two mentions
conceptually belong to the same group is not
drawn. This step is to be taken at a further stage.
The PROVENIENCE attribute can have as
values all phrases denoting geographical/racial
origin or provenience and religious affiliation.
The attribute AGE_CATEGORY can have either
numerical values, such as ?three years old?, or
words indicating age, such as ?middle-aged?, etc.
In the next section we will analyze the occur-
rences of the values of these attributes in a news
corpus.
4 Data analysis
The difficulty of the OPTM task is directly cor-
related to four factors: (i) the extent to which the
linguistic form of mentions varies; (ii) the per-
plexity of the values of the attributes; (iii) the
size of the set of the potential co-references and
(iv) the number of different mentions per entity.
In this section we present the work we have un-
dertaken so far and the results we have obtained
regarding the above four factors.
We started with a set of 175 documents be-
longing to the I-CAB corpus (see Section 3.1).
Each document has been manually annotated
observing the specifications described in Section
3.2. We focused on mentions referring to
INDIVIDUAL PERSON (Mentions in Table 3), ex-
cluding from the dataset both mentions referring
to different entity types (e.g. ORGANIZATION)
and PERSON GROUP. In addition, for the pur-
poses of this work we decided to filter out the
following mentions: (i) mentions consisting of a
single pronoun; (ii) nested mentions, (in particu-
lar in the case where a larger mention, e.g.
?President Ciampi?, contained a smaller one, e.g.
?Ciampi?, only the larger mention was consid-
ered). The total number of remaining mentions
(Meaningful mentions in Table 3) is 2343. Fi-
nally, we filtered out repetitions of mentions (i.e.
string equal) that co-refer inside the same docu-
ment, obtaining a set of 1139 distinct mentions.
The average number of mentions for an entity
in a document is 2.09, while the mentions/entity
proportion within the whole collection is 2.68.
The detailed distribution of mentions with re-
spect to document entities is presented in Table
4. Columns 1 and 3 list the number of mentions
and columns 2 and 4 list the number of entities
which are mentioned for the respective number
of times (from 1 to 9 and more than 10). For in-
stance, in the dataset there are 741 entities which,
within a single document, have just one mention,
while there are 27 entities which are mentioned
more than 10 times in the same document. As an
indication of variability, only 14% of document
entities have been mentioned in two different
ways.
Documents 175
Words 57 033
Words in mentions 8116
Mentions 3157
Meaningful mentions 2343
Distinct mentions 1139
Document entities 1117
Collection entities 873
Table 3. Documents, mentions and entities in the
OPTM dataset.
#M/E #occ #M/E #occ
1 741 6 15
2 164 7 11
3 64 8 12
4 47 9 5
5 31 ?10 27
Table 4. Distribution of mentions per entity.
4.1 Co-reference density
We can estimate the a priori probability that two
entities selected from different documents co-
refer. Actually, this is the estimate of the prob-
ability that two entities co-refer conditioned by
the fact that they have been correctly identified
inside the documents. We can compute such
probability as the complement of the ratio be-
tween the number of different entities and the
number of the document entities in the collec-
tion.
entitiesdocument
entitiescollection
corefcrossP
?
?
?=?
#
#
1)(
From Table 3 we read these values as 873
and 1117 respectively, therefore, for this corpus,
the probability of intra-document co-reference is
approximately 0.22.
30
A cumulative factor in estimating the diffi-
culty of the co-reference task is the ratio between
the number of different entities and the number
of mentions. We call this ratio the co-reference
density and it shows the a priori expectation that
a correct identified mention refers to a new en-
tity.
mentions
entitiescollection
densitycoref
#
# ?
=?
The co-reference density takes values in the
interval with limits [0-1]. The case where the co-
reference density tends to 0 means that all the
mentions refer to the same entity, while where
the value tends to 1 it means that each mention in
the collection refers to a different entity. Both
limits render the co-reference task superfluous.
The figure for co-reference density we found in
our corpus is 873/2343 ? 0.37, and it is far from
being close to one of the extremes.
A last measure we introduce is the ratio
between the number of different entities and the
number of distinct mentions. Let?s call it pseudo
co-reference density. In fact it shows the value of
co-reference density conditioned by the fact that
one knows in advance whether two mentions that
are identical also co-refer.
mentionsdistinct
entitiescollection
densitypcoref
?
?
=?
#
#
The pseudo co-reference for our corpus is
873/1139 ? 0.76. This information is not directly
expressed in the collection, so it should be ap-
proximated. The difference between co-reference
density and pseudo co-reference density (see Ta-
ble 5) shows the increase in recall, if one consid-
ers that two identical mentions refer to the same
entity with probability 1. On the other hand, the
loss in accuracy might be too large (consider for
example the case when two different people hap-
pen to have the same first name).
co-reference density 0.37
pseudo co-reference density 0.76
cross co-reference 0.22
Table 5. A priori estimation of difficulty of co-
reference
4.2 Attribute variability
The estimation of the variability of the values for
a certain attribute is given in Table 6. The first
column indicates the attribute under considera-
tion; the second column lists the total number of
mentions of the attribute found in the corpus; the
third column lists the number of different values
that the attribute actually takes and, between pa-
rentheses, its proportion over the total number of
values; the fourth column indicates the propor-
tion of the occurrences of the attribute with re-
spect to the total number of mentions (distinct
mentions are considered).
Table 6. Variability of values for attributes.
In Table 7 we show the distribution of the at-
tributes inside one mention. That is, we calculate
how many times one entity contains more than
one attribute. Columns 1 and 3 list the number of
attributes found in a mention, and columns 2 and
4 list the number of mentions that actually con-
tain that number of values for attributes.
#attributes #mentions #attributes #mentions
1 398 5 55
2 220 6 25
3 312 7 8
4 117 8 4
Table 7. Number of attributes inside a mention.
An example of a mention from our dataset that
includes values for eight attributes is the follow-
ing:
The correspondent of Al Jazira, Amr Abdel
Hamid, an Egyptian of Russian nationality?
We conclude this section with a statistic re-
garding the coverage of attributes (miscellanea
excluded). There are 7275 words used in 1139
Attributes total
occ.
distinct
occ. (%)
occ.
prob.
FIRST_NAME 535 303 (44%) 27,0%
MIDDLE_NAME 25 25 (100%) 2,1%
LAST_NAME 772 690 (11%) 61,0%
NICKNAME 14 14 (100%) 1,2%
TITLE 12 10 (17%) 0,8%
SEX 795 573 (23%) 51,0%
ACTIVITY 145 88 (40%) 7,0%
AFFILIATION 134 121 (10%) 11,0%
ROLE 155 92 (42%) 8,0%
PROVENIENCE 120 80 (34%) 7,3%
FAMILY_REL. 17 17(100%) 1,4%
AGE_CATEGORY 31 31(100%) 2,7%
MISCELLANEA 106 106 (100%) 9,3%
31
distinct mentions, out of which 3606, approxi-
mately 49%, are included in the values of the
attributes.
5 Conclusion and future work
We have presented work in progress aiming at
a better definition of the general OLP task. In
particular we have introduced Ontology Popula-
tion from Textual Mentions (OPTM) as a simpli-
fication of OLP, where the source textual mate-
rial are already classified mentions of entities.
An analysis of the data has been conducted over
a OPTM benchmark manually built from a cor-
pus of Italian news. As a result a number of indi-
cators have been extracted that suggest the com-
plexity of the task for systems aiming at auto-
matic resolution of OPTM.
Our future work is related to the definition and
extension of the OPTM benchmark for the nor-
malization step (see Introduction). For this step it
is crucial the construction and use of a large-
scale ontology, including the concepts and rela-
tions referred by mentions. A number of inter-
esting relations between mentions and ontology
are likely to emerge.
The work presented in this paper is part of the
ONTOTEXT project, a larger initiative aimed at
developing text mining technologies to be ex-
ploited in the perspective of the Semantic Web.
The project focuses on the study and develop-
ment of innovative knowledge extraction tech-
niques for producing new or less noisy informa-
tion to be made available to the Semantic Web.
ONTOTEXT addresses three key research as-
pects: annotating documents with semantic and
relational information, providing an adequate
degree of interoperability of such relational in-
formation, and updating and extending the on-
tologies used for Semantic Web annotation. The
concrete evaluation scenario in which algorithms
will be tested with a number of large-scale ex-
periments is the automatic acquisition of infor-
mation about people from newspaper articles.
6 Acknowledgements
This work was partially funded the three-
year project ONTOTEXT
6
 funded by the Provin-
cia Autonoma di Trento. We would like to thank
Nicola Tovazzi for his contribution to the anno-
tation of the dataset.
                                                 
6
 http://tcc.itc.it/projects/ontotext
References
Almuhareb, A. and Poesio, M.. 2004. Attribute-
based and value-based clustering: An evalua-
tion. In Proceedings of EMNLP 2004, pages
158--165, Barcelona, Spain.
Avancini, H., Lavelli, A., Magnini, B.,
Sebastiani, F., Zanoli, R. (2003). Expanding
Domain-Specific Lexicons by Term Categori-
zation. In: Proceedings of SAC 2003, 793-79.
Cunningham, H. and Bontcheva, K. Knowledge
Management and Human Language: Crossing
the Chasm. Journal of Knowledge Manage-
ment, 9(5), 2005.
Buitelaar, P., Cimiano, P. and Magnini, B. (Eds.)
Ontology Learning from Text: Methods,
Evaluation and Applications. IOS Press, 2005.
Ferro, L., Gerber, L., Mani, I., Sundheim, B. and
Wilson, G. (2005). TIDES 2005 Standard for
the Annotation of Temporal Expressions.
Technical report, MITRE.
Lavelli, A., Magnini, B., Negri, M., Pianta, E.,
Speranza, M. and Sprugnoli, R. (2005). Italian
Content Annotation Bank (I-CAB): Temporal
Expressions (V. 1.0.). Technical Report T-
0505-12. ITC-irst, Trento.
Lin, D. (1998). Automatic Retrieval and Clus-
tering of Similar Words. In: Proceedings of
COLING-ACL98, Montreal, Canada, 1998.
Linguistic Data Consortium (2004). ACE
(Automatic Content Extraction) English An-
notation Guidelines for Entities, version 5.6.1
2005.05.23.
http://projects.ldc.upenn.edu/ace/docs/English-
Entities-Guidelines_v5.6.1.pdf
Magnini, B., Pianta, E., Girardi, C., Negri, M.,
Romano, L., Speranza, M., Bartalesi Lenzi, V.
and Sprugnoli, R. (2006). I-CAB: the Italian
Content Annotation Bank. Proceedings of
LREC-2006, Genova, Italy, 22-28 May, 2006.
Tanev, H. and Magnini, B. Weakly Supervised
Approaches for Ontology Population. Pro-
ceedings of EACL-2006, Trento, Italy, 3-7
April, 2006.
Velardi, P., Navigli, R., Cuchiarelli, A., Neri, F.
(2004). Evaluation of Ontolearn, a Methodol-
ogy for Automatic Population of Domain On-
tologies. In: Buitelaar, P., Cimiano, P.,
Magnini, B. (eds.): Ontology Learning from
Text: Methods, Evaluation and Applications,
IOS Press, Amsterdam, 2005.
32
Representing and Accessing Multilevel Linguistic Annotation using 
the MEANING Format  
 
 
Emanuele Pianta 
ITC-irst 
38050, Povo 
Trento, Italy 
pianta@itc.it 
Luisa Bentivogli 
ITC-irst 
38050, Povo 
Trento, Italy 
bentivo@itc.it 
Christian Girardi 
ITC-irst 
38050, Povo 
Trento, Italy 
cgirardi@itc.it 
Bernardo Magnini 
ITC-irst 
38050, Povo 
Trento, Italy 
magnini@itc.it 
 
  
 
Abstract 
We present an XML annotation format 
(MEANING Annotation Format, MAF) 
specifically designed to represent and in-
tegrate different levels of linguistic anno-
tations and a tool that provides flexible 
access to them (MEANING Browser). 
We describe our experience in integrating 
linguistic annotations coming from dif-
ferent sources, and the solutions we 
adopted to implement efficient access to 
corpora annotated with the Meaning 
Format. 
1 Introduction 
It is well known that when using XML-based 
annotation schemes to represent multi layer an-
notations, it can be difficult to handle partially 
overlapping annotations. Annotating discontinu-
ous elements may be considered as a variant of 
the same problem (Pianta and Bentivogli, 2004). 
Other difficulties can arise from the necessity of 
integrating manual and automatic annotations, as 
we will show in this paper. 
One of the most effective solutions to the 
above mentioned problems is the so called stand-
off annotation, based on the separation between 
textual data and annotations, and between vari-
ous types of annotation, possibly pointing to 
same text. This approach has been systematically 
adopted in the design of MAF, a multilayer XML 
format developed for the EU-funded MEANING 
project, in the context of the creation of the Ital-
ian MEANING Corpus (Bentivogli et al, 2003).  
In this paper we will describe our experience 
in the use of MAF, with special emphasis on how 
we solved issues related to representing annota-
tion levels which come from different sources, 
and can possibly overlap. We will also give de-
tails about the solutions we adopted to allow for 
efficient access and human browsing of MAF 
standoff annotations. 
The rest of the paper is organized as follows. 
Section 2 describes MAF and the types of anno-
tations which have been represented with it. Sec-
tion 3 reports on the integration into MAF of lin-
guistic annotations coming from different 
sources. Section 4 illustrates the strategies 
adopted to make the information encoded in 
MAF quickly accessible. Finally, Section 5 pre-
sents the MEANING Browser, a tool for access-
ing and navigating corpora linguistically anno-
tated with MAF. 
2 The MEANING Format 
Following the proposals for the ISO/TC 37/SC 4 
standard for linguistic resources (Ide and 
Romary, 2002), the MAF scheme is based on 
annotation structures and data categories. Each 
type of annotation structure (nestable <struct> 
elements) corresponds to a specific kind of lin-
guistic object (e.g. tokens, lexical units, multi-
words), and each instance of a linguistic object is 
identified by a unique identifier. Data categories 
(<feat> tags) represent attributes of the linguistic 
objects. Different representation levels are con-
tained in separate documents, or document sec-
tions. The XLink and XPointer syntax is used to 
represent relations between elements in different 
XML documents, and IDREFs attributes for rela-
tions within the same document. 
2.1 First  version 
The first version of the MEANING Format has 
been used to represent seven kinds of informa-
tion: orthographic features, the structure of the 
77
text, morphosyntactic information, multiwords, 
syntactic information, named entities, and word 
senses. 
Annotation levels are related to each other fol-
lowing a hierarchy of annotation levels, which 
reflects a theoretically grounded hierarchy of 
linguistic objects. The basic (orthographic) anno-
tation level, representing tokens, is implemented 
with pointers to the character positions in the hub 
corpus. Then the morphosyntactic level, repre-
senting word-related morphological information, 
contains pointers to the tokens, whereas the mul-
tiword level points to the words described at 
morphosyntactic level. 
The following example shows how the mor-
phosyntactic features of the Italian word ?an-
dare? (to go) are represented. 
 
<struc   type="w-level" id="w_12" 
             xlink:href="#xpointer(id('t_10'))"> 
     <feat type="lemma">andare</feat> 
     <feat type="stem">and</feat> 
     <feat type="pos">v</feat> 
     <feat type="elra-tag">VF</feat> 
     <feat type="mood">inf</feat> 
     <feat type="tense">pres</feat> 
 </struc> 
 
MAF also specifically addresses the problem  
of discontinuous units, such as for instance non-
contiguous multiwords; see ?andarci veramente 
piano? (take it really easy). A detailed study of 
how standoff annotation allows for an elegant 
treatment of this phenomenon can be found in 
(Pianta and Bentivogli 2004). 
2.2 Second version 
The first version of the MEANING Format has 
recently been extended within the FU-PAT ON-
TOTEXT project (Magnini et al 2005). 
Within this project, we are creating the Italian 
Content Annotation Bank (I-CAB), a corpus of 
Italian news stories annotated with different 
kinds of semantic information. Annotation is be-
ing carried out manually, as we intend I-CAB to 
become a benchmark for automatic Information 
Extraction and Ontology Population tasks, in-
cluding recognition and normalization of various 
types of entities, temporal expressions, relations 
between entities, and relations between entities 
and temporal expressions (e.g. the relation date-
of-birth connecting a person to a date). 
To fulfill I-CAB annotation needs, we ex-
tended MAF, by adding a number of new lin-
guistic annotation levels, i.e.: 
 
? temporal expressions  
? entities of type person and organization 
? mentions (i.e. the textual expressions re-
ferring to the entities)  
 
According to the hierarchical approach to rep-
resenting relations between annotation levels in 
the first version of the MEANING Format, tem-
poral expressions and entity mentions are repre-
sented with pointers to morphosyntactic level 
entities. Entities, instead, are represented with 
pointers to entity mentions.  
To manually annotate temporal expressions 
we followed the TIMEX2 markup standard, 
while to mark entities and mentions we relied on 
the ACE entity detection task guidelines. To per-
form the annotation task we used Callisto 
(http://callisto.mitre.org). 
3 Converting linguistic annotations into 
MAF  
The manual annotations produced through Cal-
listo, which is related to novel annotation levels 
such  as temporal expressions and entity men-
tions, had to be integrated with more traditional 
annotations which are performed automatically 
with the TextPro tool, an automatic linguistic 
analysis Tool Suite developed at ITC-irst. 
News
MEANING
Annotation 
Format
Callisto
Lucene
AIF format
Manual 
annotation
Automatic 
annotation
TextPro
TextPro format
Indexing
Data Base
Conversion
MEANING
Browser
I-CAB 
Corpus
 
As one can see in the above figure, two different 
annotation processes (automatic and manual) 
produce two different formats which must be 
converted and integrated into MAF in order to be 
accessed by the MEANING Browser (or any 
other NLP tool). 
3.1 From TextPro format to MEANING 
Format 
TextPro takes a raw text as input and carries out 
basic processing tasks such as tokenization, mor-
78
phological analysis, PoS tagging, lemmatization, 
and multiword recognition. The results of 
TextPro analyses are represented in a table, 
where each token is on a row, and columns con-
tain multiple annotation levels. Converting from 
the TextPro to the MEANING Format requires 
retrieving the character positions of tokens in the 
hub corpus, which are not directly available in 
the TextPro output. 
3.2 From AIF format to MEANING format 
The Callisto manual annotation tool produces a 
coding format called AIF (Atlas Interchange For-
mat), which implements a stand-off XML anno-
tation scheme. 
When using the Callisto graphical interface, 
all annotations of temporal expressions and en-
tity mentions are carried out by selecting a se-
quence of contiguous characters. As a conse-
quence, all AIF annotations make reference to 
character positions. 
However, from Section 2.2 we know that in 
MAF temporal expressions and entity mentions 
make reference to morphosyntactic linguistic 
objects, not characters. This implies that, to go 
from AIF to the MEANING Format, we need to 
translate annotations making reference to the po-
sition of characters into annotations that point to 
morphological entities. More precisely, we need 
to substitute pointers to character positions with 
pointers to morphosyntactic objects which have 
been marked automatically by TextPro. Carrying 
out this step will also achieve the integration of 
manual and automatic annotations.  
The integration step is possible because the 
MAF hierarchy of annotation levels points, at the 
lowest level, to character positions. By following 
the hierarchy of links relating the various annota-
tion levels it is always possible to trace back a 
linguistic object to some sequence of characters 
in the raw text, and in the opposite direction, 
given a string, we know what linguistic objects 
correspond to it. Summing up, the integration of 
AIF annotations into MAF requires that, given 
the character positions contained in the AIF an-
notation of some string, we substitute the point-
ers to characters with the pointers to the linguis-
tic objects that cover the same string. 
 
4 Data Access 
MAF turned out to be a flexible and expressive 
means to represent and integrate multiple levels 
of linguistic annotation. This was achieved 
mainly thanks to the adoption of the standoff 
annotation approach.  However accessing and 
retrieving information spread in possibly very 
large repositories (hundreds of thousands) of 
XML files may be a challenging task even for 
Database Management Systems specifically de-
signed to handle XML. To solve this problem we 
first analyzed existing native XML databases 
such as eXist, and Apache Xindice, but found 
that what was available at the time did not suited 
our needs. For this reason we approached the 
access problem through a two-fold strategy: 
? converting  XML data into a relational 
database 
? indexing XML data and accessing them 
through a  search engine (LUCENE) 
The conversion of MAF data into a relational 
database is based on the following strategy. Each 
annotation level is mapped into a table, where 
rows represent instances of the relevant linguistic 
object (e.g. words), and columns represent its 
attributes (e.g. lemma, PoS, etc). Specific col-
umns contain the object identifiers and the point-
ers to objects of other types/tables. 
Once MAF data are stored in a relational data-
base, they can be accessed quite efficiently. 
However, when the access to data requires joins 
of many tables, access times become incompati-
ble with various kinds of applications, such as 
on-line corpus browsing. For this reason we tried 
to complement the use of a relational database 
with the exploitation of the indexing capability 
of the LUCENE search engine 
(http://lucene.apache.org/). To this extent we 
modified the LUCENE analyzer so as to be able 
to parse XML structures. In this way LUCENE 
can be configured in order to index any XML 
structure. 
The fast access capabilities of a relational da-
tabase combined with the extended indexing ca-
pabilities of LUCENE enabled us to implement a 
browser of MAF annotated corpora.  
5 The MEANING Browser 
The MEANING Browser can be used by humans 
to navigate any corpus encoded with MAF. The 
browser is built upon an API which can be used 
by any automatic system.  
In the following, we are going to demonstrate 
how I-CAB texts and their annotations can be 
accessed through the MEANING Browser. 
The first kind of access to the corpus is word-
oriented, and amounts to a concordancer, i.e. a 
79
tool able to provide all the occurrences of a cer-
tain word in the corpus. The user can alterna-
tively search for all occurrences of a word form, 
or a lemma, possibly constraining the search to a 
certain PoS. Free combinations between these 
constraints are allowed. The system will return a 
KWIC-like concordance of all the tokens in the 
corpus that match the request, within a chosen 
word window. By clicking on the magnifying 
glass, one can see the sentence in which the 
searched word occurs (see Appendix 1). 
By clicking on a specific icon a new window 
is opened where the whole text is displayed and 
its linguistic annotations are made accessible. A 
number of graphical widgets allow the user to 
highlight the desired annotations: e.g. nouns, 
verbs, multiwords, temporal expressions, men-
tions of a specific entity. 
In Appendix 2 the browser is used to show 
both nouns (automatically annotated) and entity 
mentions (from manual annotation). Appendix 3 
shows time expressions and discontinuous mul-
tiwords; see how the multiword ?ha rassegnato 
? le dimissioni? (he resigned) is made discon-
tinuous by the occurrence of a time expression 
ieri (yesterday). The browser will also give mor-
phosyntactic information about single words 
composing multiwords (governo, government). 
From the same window one can access the XML 
files encoding multiple annotation levels for the 
same document. 
References  
Bentivogli, L., Girardi, C., Pianta, E. 2003. The ME-
ANING Italian Corpus. In Proceedings of the Cor-
pus Linguistics 2003 conference, Lancaster, UK. 
Ide, N. & Romary, L. 2002. Standards for Language 
Resources. In Proceedings of LREC 2002, Las 
Palmas, Canary Islands, Spain.  
Magnini, B., Negri, M., Pianta, E., Romano, L., Sper-
anza, M., Serafini, L., Girardi, C., Bartalesi, V., 
Sprugnoli, R. 2005. From Text to Knowledge for 
the Semantic Web: the ONTOTEXT Project. In 
Proceedings of  SWAP 2005 Workshop, Trento, I-
taly. 
Pianta, E. and Bentivogli, L. 2004. Annotating Dis-
continuous Structures in XML: the Multiword 
Case. In Proceedings of the LREC 2004 Satellite 
Workshop on "XML-based richly annotated cor-
pora", Lisbon, Portugal. 
 
 
 
Appendix 1   
Kwic Concordancer 
 
 
Appendix 2 
Browsing nouns (in grey, automatic annotation)  and 
entity mentions (Tony Blair, manual annotation) 
 
 
Appendix 3  
Browsing discontinuous multiwords (ha rassegnato ? 
le dimissioni, he resigned), time expressions (ieri, 
yesterday) and word information (governo) 
 
80
Proceedings of the 8th International Conference on Computational Semantics, pages 277?281,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Computing implicit entities and events
for story understanding
Rodolfo Delmonte, Emanuele Pianta
Universita` Ca? Foscari and IRST, Fondazione Bruno Kessler, Venezia
delmont@unive.it, pianta@itc.it
1 Introduction
In order to show that a system for text understanding has produced a sound
representation of the semantic and pragmatic contents of a story, it should be
able to answer questions about the participants and the events occurring in
the story. This requires processing linguistic descriptions which are lexically
expressed but also unexpressed ones, a task that, in our opinion, can only be
accomplished starting from full-fledged semantic representations. The over-
all task of story understanding requires in addition computing appropriate
coreference and cospecification for entities and events in what is usually re-
ferred to as a Discourse Model. All these tasks have been implemented in
the GETARUNS system, which is subdivided into two main meta-modules
or levels: the Low Level System, containing all modules that operate at sen-
tence level; High Level System, containing all the modules that operate at
discourse level by updating the Discourse Model. The system is divided up
into a pipeline of sequential but independent modules which realize the sub-
division of a parsing scheme as proposed in LFG theory where a c-structure
is built before the f-structure can be projected by unification into a DAG
(Direct Acyclic Graph). In this sense we try to apply phrase-structure rules
in a given sequence as they are ordered in the grammar: whenever a syntac-
tic constituent is successfully built, it is checked for semantic consistency, as
LFG grammaticality principles require [1].
GETARUNS has a highly sophisticated linguistically based semantic
module which is used to build up the Discourse Model. Semantic process-
ing is strongly modularized and distributed amongst a number of differ-
ent submodules which take care of Spatio-Temporal Reasoning, Discourse
Level Anaphora Resolution, and other subsidiary processes like Topic Hier-
archy which cooperate to find the most probable antecedent of coreferring
277
and cospecifying referential expressions when creating semantic individuals.
These are then asserted in the Discourse Model (hence the DM), which is
then the sole knowledge representation used to solve nominal coreference.
Semantic Mapping is performed in two steps: at first a Logical Form is pro-
duced which is a structural mapping from DAGs onto unscoped well-formed
formulas. These are then turned into situational semantics informational
units, infons which may become facts or sits (non factual situations). Each
unit has a relation, a list of arguments which in our case receive their se-
mantic roles from lower processing - a polarity, a temporal and a spatial
location index. Inferences can be drawn on the facts repository as will be
discussed below.
2 Implicit entities and implicatures
Conversational implicatures and implications in general, are based on an
assumption by the addressee that the speaker is obeying the conversational
maxims (see [2]), in particular the cooperative principle. We regard the
mechanism that recovers standard implicatures and conversational implica-
tions in general, as a reasoning process that uses the knowledge contained in
the semantic relations actually expressed in the utterance to recover hidden
or implied relations or events as we call them. This reasoning process can
be partially regarded as a subproduct of an inferential process that takes
spatio-temporal locations as the main component and is triggered by the
need to search for coreferent or cospecifiers to a current definite or indef-
inite NP head. This can be interpreted as bridging referential expression
entertaining some semantic relation with previously mentioned entities. If
we consider a classical example from [5] (A: Can you tell me the time? ; B:
Well, the milkman has come), we see that the request of the current time
is bound to a spatio-temporal location. Using the MILKMAN rather than
a WATCH to answer the question, is relatable to spatio-temporal triggers.
In fact, in order to infer the right approximate time, we need to situate the
COMING event of the milkman in time, given a certain spatial location.
Thus, it is just the ?pragmatic restriction? associated to SPACE and TIME
implied in the answer, that may trigger the inference.
2.1 The restaurant text
To exemplify some of the issues presented above we present a text by [7]. In
this text, entities may be scenario-dependent characters or main characters
independent thereof. Whereas the authors use the text for psychological
278
experimental reasons, we will focus on its computability.
(0) At the restaurant. (1) John went into a restaurant. (2) There was a
table in the corner. (3) The waiter took the order. (4) The atmosphere was
warm and friendly. (5) He began to read his book.
Sentence (1) introduces both JOHN as the Main Topic in the Topic Hier-
archy and RESTAURANT as the Main Location (in the role of LOCATion
argument of the governing verb GO and the preposition INTO). Sentence
(2) can potentially introduce TABLE as new main Topic. This type of sen-
tences is called presentational in the linguistic literature, and has the prag-
matic role of presenting an entity on the scene of the narration in an abrupt
manner, or, as Centering would definite it, with a SHIFT move. However,
the TABLE does not constitute a suitable entity to be presented on the
scene and the underlying import is triggering the inference that ?someone
is SITting at a TABLE?. This inference is guided by the spatio-temporal
component of the system. GETARUNS is equipped with a spatio-temporal
inferential module that asserts Main Spatio-Temporal Locations to anchor
events and facts expressed by situational infons. This happens whenever an
explicit lexical location is present in the text, as in the first sentence (the
RESTAURANT). The second sentence contains another explicit location:
the CORNER. Now, the inferential system will try to establish whether the
new location is either a deictic version of the Main Location, or it is semanti-
cally included in the Main Location, or else it is a new unconnected location
that substitutes the previous one. The CORNER is in a meronymic seman-
tic relation with RESTAURANT and thus it is understood as being a part
of it. This inference triggers the implicature that the TABLE mentioned in
sentence (2) is a metonymy for the SITting event. Consequently, the system
will not assume that the indefinite expression a table has the funciton to
present a new entity TABLE, but that an implicit entity is involved with a
related event. The entity implied is understood as the Main Topic of the
current Topic Hierarchy, i.e. JOHN.
We will now concentrate our attention onto sentence (3). To account
for the fact that whenever a waiter takes an order there is always someone
that makes the order, GETARUNS computes TAKE ORDER as a com-
pound verb with an optional implicit GOAL argument that is the person
ORDERing something. The system then looks for the current Main Topic of
discourse or the Focus as computed by the Topic Hierarchy Algorithm, and
associates the semantic identifier to the implicit entity. This latter procedure
is triggered by the existential dummy quantifier associated to the implicit
279
optional argument. However, another important process has been activated
automatically by the presence of a singular definite NP, ?the WAITER?,
which is searched at first in the Discourse Model of entities and proper-
ties asserted for the previous stretch of text. Failure in equality matching
activates the bridging mechanism for inferences which succeeds in identify-
ing the WAITER as a Social Role in a RESTAURANT, the current Main
Location.
The text includes a sentence (4) that represents a psychological state-
ment, that is it expresses the feelings and is viewed from the point of view of
one of the characters in the story. The relevance of the sentence is its role in
the assignment of the antecedent to the pronominal expressions contained in
the following sentence (5). Without such a sentence the anaphora resolution
module would have no way of computing JOHN as the legitimate antecedent
of ?He/his?. However, in order to capture such information, GETARUNS
computes the Point of View and Discourse Domain on the basis of Informa-
tional Structure and Focus Topic by means of a Topic Hierarchy algorithm
based on [3] and [8].
2.2 Common sense reasoning
GETARUNS is also able to search for unexpressed relations intervening
in the current spatio-temporal location. To solve this problem in a princi-
pled way we needed commonsense knowledge organized in a computationally
tractable way. This is what CONCEPTNET 2.1 ([6]) provides. ConceptNet
- available at www.conceptnet.org - is the largest freely available, machine-
useable commonsense resource. Organized as a network of semi-structured
natural language fragments, ConceptNet consists of over 250,000 elements
of commonsense knowledge. At present it includes instances of 19 semantic
relations, representing categories of, inter alia, temporal, spatial, causal, and
functional knowledge. The representation chosen is semi-structured natu-
ral language using lemmata rather than inflected words. The way in which
concepts are related reminds ?scripts?, where events may be decomposed in
Preconditions, Subevents and so on, and has been inspired by Cyc ([4]).
ConceptNet can be accessed in different ways; we wanted a strongly con-
strained one. We choose a list of relations from this external resource and
combine them with the information available from the processing of the text
to derive Implicit Information. In other words, we assume that what is be-
ing actually said hides additional information which however is implicitely
hinted at. ConceptNet provides the following relations: SubEventOf, First-
SubeventOf, DesiresEvent, Do, CapableOf, FunctionOf, UsedFor, EventRe-
280
quiresObject, LocationOf. Let us see how this information can be exploited
to interpret another classical example from the Pragmatics literature: A:
I?ve just run out of petrol ; B: Oh, there?s a garage just around the corner.
There are a number of missing conceptual links that need to be inferred in
this text, as follows: Inf1 : the CAR has run out of petrol; Inf2 : the CAR
NEEDS petrol; Inf3 : garages SELL PETROL for cars.
In addition, in order to use ConceptNet we need to link petrol and garage
to gas/gasoline and gas station respectively. Now we can query the ontology
and will recover the following facts. The whole process starts from the first
utterance and uses RUN OUT OF GAS: (Do ?car? ?run out of gas?). Then
we can use GAS STATION and CAR to build another query and get (Do
?car? ?get fuel at gas station?), where FUEL and GASoline are in IsA
relation. Eventually we may still get additional information on the reason
why this has to be done: (Do ?person? ?don?t want to run out of gas?),
(SubeventOf ?drive car? ?you run out of gas?), (Do ?car? ?need gas petrol
in order to function?), (Do ?gas station? ?sell fuel for automobile?). These
may all constitute additional commonsense knowledge that may be used to
further explain and clarify the implicature.
References
[1] Joan Bresnan. Lexical-Functional Syntax (Blackwell Textbooks in Linguistics).
Blackwell Publisher, September 2000.
[2] H.P. Grice. Logic and conversation. In P. Cole and J.L. Morgan, editors, Syntax
and Semantics, volume 3. New York Academic Press, 1975.
[3] B. Grosz. Focusing and description in natural language dialogues. Cambridge
University Press, 1981.
[4] Douglas B. Lenat. CYC: A large-scale investment in knowledge infrastructure.
Communications of the ACM, 38(11):33?38, 1995.
[5] Stephen Levinson. Pragmatics. Cambridge University Press, 1983.
[6] Hugo Liu and Push Singh. ConceptNet: A practical commonsense reasoning
toolkit. BT Technology Journal, 22(211?226), 2004.
[7] A.J. Sanford and S.C. Garrod. Thematic subjecthood and cognitive constraints
on discourse structure. Journal of Pragmatics, 12(5-6):519?534, 1988.
[8] C. Sidner. Focusing in the comprehension of definite anaphora. In M. Brady
and R. Berwick, editors, Computational models of discourse, pages 267?330.
MIT Press, 1983.
281
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 191?194,
Prague, June 2007. c?2007 Association for Computational Linguistics
IRST-BP: Preposition Disambiguation  
 based on  
 Chain Clarifying Relationships Contexts 
 
Octavian Popescu 
FBK-IRST, Trento (Italy) 
popescu@itc.it  
Sara Tonelli 
FBK-IRST, Trento (Italy) 
satonelli@itc.it 
Emanuele Pianta 
FBK-IRST, Trento (Italy) 
pianta@itc.it 
 
 
Abstract 
We are going to present a technique of 
preposition disambiguation based on 
sense discriminative patterns, which are 
acquired using a variant of Angluin?s al-
gorithm. They represent the essential in-
formation extracted from a particular 
type of local contexts we call Chain 
Clarifying Relationship contexts. The 
data set and the results we present are 
from the Semeval task, WSD of Preposi-
tion (Litkowski 2007). 
1 Introduction 
Word Sense Disambiguation (WSD) is a prob-
lem of finding the relevant clues in a surround-
ing context. Context is used with a wide scope in 
the NLP literature. However, there is a dichot-
omy among two types of contexts, local and 
topical contexts (Leacock et. all 1993), that is 
general enough to encompass the whole notion 
and at the same to represent a relevant distinc-
tion. 
The local context is formed by information on 
word order, distance and syntactic structure and 
it is not restricted to open-class words. A topical 
context is formed by the list of those words that 
are likely to co-occur with a particular sense of a 
word. Generally, the WSD methods have a 
marked predilection for topical context, with the 
consequence that structural clues are rarely, if 
ever, taken into account. However, it has been 
suggested (Stetina&Nagao 1997, Dekang 1997) 
that structural words, especially prepositions and 
particles, play an important role in computing 
the lexical preferences considered to be the most 
important clues for disambiguation. 
Closed class words, prepositions in particular, 
are ambiguous (Litkowski&Hargraves2006). 
Their disambiguation is essential for the correct 
processing of the meaning of a whole phrase. A 
wrong PP-attachment may render the sense of 
the whole sentence unintelligible. Consider for 
example: 
 
(1) Joe heard the gossip about you and me. 
(2) Bob rowed about his old car and his 
mother. 
 
A probabilistic context free grammar most 
likely will parse both (1) and (2) wrongly1. It 
would attach ?about? to ?to hear? in (1) and 
would consider the ?his old car and his mother? 
the object of ?about? in (2).  
The information needed for disambiguation of 
open class words is spread at all linguistics lev-
els, from lexicon to pragmatics, and can be lo-
cated within all discourse levels, from immedi-
ate collocation to paragraphs (Stevenson&Wilks 
1999). Intuitively, prepositions have a different 
behavior. Most likely, their senses are deter-
mined within the government category of their 
                                                 
1
 Indeed, Charniak?s parser, considered to be among 
the most accurate ones for English, parses wrongly 
both of them. 
191
heads. We expect the local context to play the 
most important role in the disambiguation of 
prepositions. 
We are going to present a technique of prepo-
sition disambiguation based on sense discrimina-
tive patterns, which are acquired using a variant 
of Angluin?s algorithm. These patterns represent 
the essential information extracted from a par-
ticular type of local contexts we call Chain 
Clarifying Relationship contexts. The data set 
and the results we present are from the Semeval 
task, WSD of Preposition (Litkowski 2007). 
In Section 2 we introduce the Chain Clarify-
ing Relationships, which represent particular 
types of local contexts. In Section 3 we present 
the main ideas of the Angluin algorithm. We 
show in Section 4 how it can be adapted to ac-
commodate the preposition disambiguation task. 
Section 5 is dedicated to further research. 
2 Chain Clarifying Relationships 
We think of ambiguity of natural language as a 
net - like relationship. Under certain circum-
stances, a string of words represents a unique 
collection of senses. If a different sense for one 
of these words is chosen, the result is an un-
grammatical sentence. Consider (3) below: 
 
(3) Most people do not live in a state of 
high intellectual awareness about their 
every action. 
 
Suppose one chooses the sense of ?to live? to 
be ?to populate?. Then, its complement, ?state?, 
should be synonym with location. The analysis 
crashes when ?awareness? is considered. There 
are two things we notice here: (a) the relation-
ship between ?live? and ?state? ? the only two 
acceptable sense combination out of four are 
(populate, location) and (experience, entity) ? 
and (b) the chain like relationship between 
?awareness?, ?state?, ?live? where the sense of 
any of them determines the sense of all the oth-
ers in a cascade effect, or results in ungrammati-
cality. A third thing, not directly observable in 
(3) is that the syntactic configuration is crucial in 
order for (a) and (b) to arise. Example (4) shows 
that in a different syntactic configuration the 
above sense relationship simply disappears: 
 
(4) The awareness of people about the state insti-
tutions is arguably the first condition to live 
in a democratic state. 
 
We call the relationship between ?live?, 
?state?, ?awareness? a Chain Clarifying Rela-
tionship (CCR). In that specific syntactic con-
figuration their senses are interdependent and 
independent of the rest of the sentence. To each 
CCR corresponds a sense discriminative pattern. 
Our goal is to learn which local contexts are 
CCRs. Each CCR is a pattern of words on a syn-
tactic configuration. Each slot can be filled only 
by words defined by certain lexical features. To 
learn a CCR means to discover the syntactic 
configuration and the respective features. For 
example consider (5) and (6) with their CCRs in 
(CCR5) and (CCR6) respectively:  
 
(5) Some people lived in the same state of 
disappointment/ optimism/ happiness. 
 (CCR5) (vb=live_sense_2, prep1=in_1, 
prep1_obj=state_sense_1,prep2=of_sense_1
a,prep2_obj=[State_of_Spirit])  
 (6) Some people lived in the same state of 
Africa/ Latin America/ Asia. 
(CCR6) (vb=live_sense_1, prep1=in_1, 
prep1_obj=state_sense_1,prep2=of_1b,prep
2_obj = [Location]) 
 
The lexical features of the open class words in 
a specific syntactic configuration trigger the 
senses of each word, if the context is a CCR. In 
(CCR5) any word that has the same lexical trait 
as the one required by prep2_obj slot will deter-
mine a unique sense for all the other words, in-
cluding the preposition. The same holds for 
(CCR6). The difference between (CCR5) and 
(CCR6) is part of the linguistic knowledge 
(which can be clearly shown: ?how? (5) vs. 
?where? (6)). 
The CCR approach proposes a deterministic 
approach to WSD. There are two features of 
CCRs which are interesting from a strictly prac-
tical point of view. Firstly, CCR proposal is a 
way to determine the size of the window where 
the disambiguation clues are searched for (many 
WSD algorithms arbitrarily set it apriori). Sec-
ondly, within a CCR, by construction, the sense 
of one word determines the senses of all the oth-
ers. 
192
3 Angluin Learning Algorithm  
Our working hypothesis is that we can learn the 
CCRs contexts by inferring differences via a 
regular language learning algorithm. What we 
want to learn is which features fulfil each syn-
tactic slot. First we introduce the original An-
gluin?s algorithm and then we mention a variant 
of it admitting unspecified values.  
Angluin proved that a regular set can be 
learned in polynomial time by assuming the ex-
istence of an oracle which can gives ?yes/no? 
answers and counterexamples to two types of 
queries: membership queries and conjecture que-
ries (queries about the form of the regular lan-
guage) (Angluin 1998). 
The algorithm employs an observation table 
built on prefix /suffix closed classes. To each 
word a {1, 0} value is associated, ?1? meaning 
that the word belongs to the target regular lan-
guage. Initially the table is empty and is filled 
incrementally. The table is closed if all prefixes 
of the already seen examples are in the table and 
is consistent if two rows dominated by the same 
prefix have the same value, ?0? or ?1?. 
If the table is not consistent or closed then a 
set of membership queries is made. If the table is 
consistent and closed then a conjecture query is 
made. If the oracle responds ?no?, it has to pro-
vide a counterexample and the previous steps are 
cycled till ?yes? is obtained. 
The role of the oracle for conjecture questions 
can be substituted by a stochastic process. If 
strict equality is not requested, then a probably 
approximately correct identification of language 
can be obtained (PAC identification), which 
guarantees that the two languages (the identified 
one, Li, and the target one, Lt) are equal up to a 
certain extent. The approximation is constrained 
by two parameters ? ? accuracy and ? ? confi-
dence, and the constraint is P(d(Li, Lt) ? ?) ? ?), 
where the distance between two languages is the 
probability to see a word in just one of them. 
The algorithm can be further generalized to 
work with unspecified values. The examples 
may have three values (?yes?, ?no?, ???), as in 
many domains one has to deal with partial 
knowledge The main result is that a variant of 
the above algorithm successfully halts if the 
number of counterexamples provided by the ora-
cle have O(log n) missing attributes, where n is 
the number of attributes (Goldmann et al 2003). 
4 Preposition Disambiguation Task 
The CCR extraction algorithm is supervised. 
Consider that you have a sense annotated cor-
pora. Extract the dependency paths and filter out 
the ones which are not sense discriminative. Try 
to generalize each slot and retain the minimal 
ones. What is left are CCRs. 
Unfortunately, for the preposition disam-
biguation task the training set is sense annotated 
only for prepositions. We have undertaken a dif-
ferent strategy. The training corpus can be used 
as an oracle. The main idea is to start with a set 
of few examples for each sense from the training 
set which are considered to be the most repre-
sentative ones. We try to generalize each of 
them independently and to tackle down the bor-
der cases (the cases that may correspond to two 
different senses) which are considered unspeci-
fied examples. The process stops when the ora-
cle does not bring any new information (the 
training cases have been learned). Below we 
explain this process step by step. 
Step 1. Get the seed examples. For each 
preposition and sense get the seed examples. 
This operation is performed by a human expert. 
It may be the case that the glosses or the diction-
ary definition are a good starting point (with the 
advantage that the intervention of a human is no 
more required). However, we preferred do to it 
manually for better precision. 
Besides the most frequent sense, we have con-
sidered, in average, another two senses. There is 
a practical reason for this limitation: the number 
of examples for the rest of the senses is insuffi-
cient. In total we have considered 149 senses out 
of the 241 senses present in the training set. For 
each an average of three examples has been cho-
sen. 
Step 2. Get the CCRs. For each example we 
read the lex units associated with its frame from 
FrameNet. Our goal is to identify the relevant 
syntactic and lexical features associated with 
each slot. We have undertaken two simplifying 
assumptions. Firstly, only the government cate-
gory of the head of the PP is considered (which 
can be a verb, a noun or an adjective). Secondly, 
193
the lexical features are identified with synsets 
from WordNet.  
We have used the Charniak?s parser to extract 
the structure of the PP-phrases and further we 
have used Collin?s algorithm to implement a 
head recogniser. 
A head can have many synsets. In order to 
understand which sense the word has in the re-
spective construction we look for the synset 
common to the elements extracted from lex. If 
the proposed synset uniquely identifies just one 
sense then it is considered a CCR. If not, we are 
looking for the next synset. This step corre-
sponds to membership queries in Angluin?s al-
gorithm. 
Step 3. Generalize the CCRs. At the end of 
step 2 we have a set of CCRs for each sense. We 
obtained 395 initial CCRs. We tried to extend 
the coverage by taking into account the hypero-
nyms of each synsets. Only approximately 10% 
of these new patterns have received an answer 
from the oracle. Consequently, for our ap-
proach ,a part of the training corpus has not been 
used. It serves only 15 examples in average to 
get a correct CCR. All the instances of the same 
CCR do not bring any new information to our 
approach. 
Posteriori, we have noticed that the initial pat-
terns have an almost 50% (48.57%) coverage in 
the test data. The generalized patterns obtained 
after the third step have 82% test corpus cover-
age. For the rest 18%, which are totally un-
known cases, we have chosen the most frequent 
sense. 
In table 1 we present the performances of our 
system. It achieves 0.65 (FF-score), which com-
pares favourably against baseline ? the most fre-
quent -of 0.53. On the first column of Table 1 
we write the FF score interval - more than 0.75, 
between 0.75 and 0.5, and less than 0.5 respec-
tively, - on the second column we present the 
number of cases within that interval the system 
solved and on the third column we include the 
corresponding number for baseline. 
Table 1 
 
Interval System Baseline 
1.00 - 0.75 18 8 
0.75 - 0.50 15 6 
0.00 ? 0.50 2 20 
5 Conclusion and Further Research 
Our system did not perform very well (third po-
sition out of three). Analyzing the errors, we 
have noticed that our system systematically con-
found two senses in some cases (for example 
?by? 5(2) vs. 15(3), for ?on? 4(1c) vs. 1(1) etc.). 
We would like to see whether these errors are 
due to a misclassification in training. 
 
References 
 
Angluin, D. (1987): ?Learning Regular Sets 
from Queries and Counterexamples?, Infor-
mation and Computation Volume 75 ,  Issue 2 
Goldman, S., Kwek, S., Scott, S. (2003): ?Learn-
ing from examples with unspecified attribute 
values?, Information and Computation, Vol-
ume 180 
Leacock, C., Towell, G., Voorhes, E. (1993): 
?Towards Building Contextual Representa-
tions of Word Senses Using Statistical Mod-
els?, In Proceedings, SIGLEX workshop: Ac-
quisition of Lexical Knowledge from Text 
Lin, D. (1997): ?Using syntactic dependency as 
local context to resolve word sense ambigu-
ity?.ACL/EACL-97,  Madrid 
Litkowski, K. C. (2007):?Word Sense Disam-
biguation of Prepositions? , The Semeval 
2007 WePS Track. In Proceedings of Semeval 
2007, ACL 
Litkowski, K. C., Hargraves O. (2006): ?Cover-
age and Inheritance in the Preposition Project", 
Proceedings of the Third ACL-SIGSEM 
Workshop on Prepositions, Trento, 
Stetina J, Nagao M (1997): ?Corpus based PP 
attachment ambiguity resolution with a se-
mantic dictionary.?, Proc. of the 5th Work-
shop on very large corpora, Beijing and 
Hongkong, pp 66-80 
Stevenson K., Wilks, Y.,(2001): ?The interaction 
of knowledge sources in word sense disam-
biguation?, Computational Linguistics, 
27(3):321?349. 
 
194
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 170?173,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
KX: A flexible system for Keyphrase eXtraction 
 Emanuele Pianta Fondazione Bruno Kessler Trento, Italy. pianta@fbk.eu 
Sara Tonelli Fondazione Bruno Kessler Trento, Italy. satonelli@fbk.eu     Abstract 
In this paper we present KX, a system for key-phrase extraction developed at FBK-IRST, which exploits basic linguistic annotation combined with simple statistical measures to select a list of weighted keywords from a document. The system is flexible in that it of-fers to the user the possibility of setting pa-rameters such as frequency thresholds for col-location extraction and indicators for key-phrase relevance, as well as it allows for do-main adaptation exploiting a corpus of docu-ments in an unsupervised way. KX is also eas-ily adaptable to new languages in that it re-quires only a PoS-Tagger to derive lexical pat-terns. In the SemEval task 5 ?Automatic Key-phrase Extraction from Scientific Articles?, KX performance achieved satisfactory results both in finding reader-assigned keywords and in the combined keywords subtask.  1 Introduction Keyphrases are expressions, either single words or phrases, describing the most important con-cepts of a document. As such, a list of key-phrases provides an approximate but useful char-acterization of the content of a text and can be used in a number of interesting ways both for human and automatic processing. For example, keyphrases provide a sort of quick summary of a document. This can be exploited not only in automatic summarization tasks, but also to en-able quick topic search over a number of docu-ments indexed according to their keywords, which is more precise and efficient than full-text search. Once the keywords of a document collec-tion are known, they can also be used to calculate semantic similarity between documents and to cluster the texts according to such similarity (Ricca et al 2004). Also, keyword extraction can be used as an intermediate step for automatic sense extraction (Jones et al 2002).  
For these reasons, the keyphrase extraction task proposed at SemEval 2010 raised much at-tention among NLP researchers, with 20 groups participating to the competition. In this frame-work, we presented the KX system, specifically tuned to identify keyphrases in scientific articles. In particular, the challenge comprised two sub-tasks: the extraction of reader-assigned and of author-assigned keyphrases in scientific articles from the ACM digital library. The former are assigned to the articles by annotators, who can choose only keyphrases that occur in the docu-ment, while author-assigned keyphrases are not necessarily included in the text. 2 KX architecture A previous version of the KX system, named KXPat (Pianta, 2009), was developed to extract keyphrases from patent documents in the PatEx-pert project (www.patexpert.org). The sys-tem employed in the SemEval task has additional parameters and has been tailored to identify key-phrases in scientific articles. With KX, the identification of keyphrases can be accomplished with or without the help of a reference corpus, from which some statistical measures are computed in an unsupervised way. We present here the general KX architecture, including the corpus-based pre-processing, even if in the SemEval task the information extracted from the corpus did not contribute as expected (see Section 3).  KX keyphrase extraction combines linguistic and statistical information, similar to (Frantzi et al, 2000) and is based on 4 steps. The first three steps are carried out at corpus level, whereas the fourth one extracts information specific to each single document to be processed. This means that the first three steps require a corpus C, preferably sharing the same domain of the document d from which the keyphrases should be extracted. The fourth step, instead, is focused only on the 
170
document d. The steps can be summarized as follows: Step 1: Extract from C the list NG-c of corpus n-grams, where an n-gram is any sequence of tokens in the text, for instance ?the sys-tem?, ?of the?, ?specifically built?. Step 2: Select from the list NG-c a sub-list of multiword terms MW-c, that is combina-tions of words expressing a unitary con-cept, for instance ?light beam? or ?access control?  Step 3: For each document in C, recognize and mark the multiword terms. Calculate the inverse document frequency (IDF) for all words and multiword terms in the cor-pus. Step 4: Given a document d from which a set of relevant keyphrases should be ex-tracted, count all words and multiword terms and rank them. Step 1 is aimed at building a list of all possible n-grams in C. The maximum length of the selected n-grams can be set by the user. For SemEval, beside one-token n-grams, we select 2-, 3- and 4-grams. Since n-grams occurring only a few times are very unlikely to be useful for keyphrase recognition, they are cut off from the extracted list and excluded for further processing. The fre-quency threshold can be set according to the ref-erence corpus dimensions. For SemEval, we fixed the frequency threshold to 4. In this step, a black-list was also used in order to exclude n-grams containing any of the stopwords in the list. Such stopwords include for example ?every-thing?, ?exemplary?, ?preceding?, etc. In Step 2, we select as multiword terms those n-grams that match certain lexical patterns. To this purpose, we first analyze all n-grams with the MorphoPro morphological analyzer of the TextPro toolsuite (Pianta et al, 2006). Then, we filter out the n-grams whose analysis does not correspond to a predefined set of lexical patterns. For example, one of the patterns admitted for 4-grams is the following: [N]~[O]~[ASPGLU]~[NU]. This means that a 4-gram is a candidate multiword term if it is composed by a Noun, fol-lowed by ?of? or ?for? (defined as O), followed by either an Adjective, Singular noun, Past parti-ciple, Gerund, punctuation (L) or Unknown word, followed by either a Noun or Unknown word. This is matched for example by the 4-gram ?subset [S] of [O] parent [S] peers [N]?.  
Both the lexical categories (e.g. S for singular noun) and the admissible lexical patterns can be defined by the user. In Step 3, multiword terms are recognized by combining local (document) and global (corpus) evidence. To this purpose, we do not exploit as-sociation measures such as Log-Likelihood, or Mutual Information, but a simple frequency based criterion. Two thresholds are defined: MinCorpus, which corresponds to the minimum number of occurrences of an n-gram in a refer-ence corpus, and MinDoc, i.e. the minimum number of occurrences in the current document. KX marks an n-gram in a document as a multiword term if it occurs at least MinCorpus times in the corpus or at least MinDoc times in the document. The two parameters depend on the size of the corpus and the document respectively. In SemEval, we found that the best thresholds are MinDoc=4 and MinCorpus=8. A similar, fre-quency-based, strategy is used to solve ambigui-ties in how sequences of contiguous multiwords should be segmented. For instance, given the sequence ?combined storage capability of sen-sors? we need to decide whether we recognize ?combined storage capability? or ?storage capa-bility of sensors?. To this purpose, we calculate the strength of each alternative collocation as docFrequency * corpusFrequency, and then choose the stronger one. To calculate IDF for each word and multiword term, we use the usual formula:   log( TotDocs / DocsContaningTerm ). In step 4, we take into account a new docu-ment d, possibly not included in C, from which the keyphrases should be extracted. First we rec-ognize and mark multiword terms, through the same algorithm used in Step 3. Note that KX can recognize multiwords also in isolated documents, independently of any reference corpus, by acti-vating only the MinDoc parameter (see above). Then, we count the frequency of words and multiword terms in d, obtaining a first list of keyphrases, ranked according to frequency. Thus, frequency is the baseline ranking parame-ter, based on the assumption that important con-cepts are mentioned more frequently than less important ones.  After the creation of a frequency-based list of keyphrases, various techniques are used to re-rank it according to relevance. In order to find the best ranking mechanism for the type of key-phrases we want to extract, different parameters can be set: ? Inverse document frequency (IDF): this parameter takes into account the fact that a 
171
concept that is mentioned in all documents is less relevant to our task than a concept occurring in few documents ? Keyphrase length: number of tokens in a keyphrase. Concepts expressed by longer phrases are expected to be more specific, and thus more relevant. When this pa-rameter is activated, frequency is multi-plied by the keyphrase length. ? Position of first occurrence: important concepts are expected to be mentioned be-fore less relevant ones. If the parameter is activated, the frequency score will be mul-tiplied by the PosFact factor computed as (DistFromEnd / MaxIndex)pwr2, where MaxIndex is the length of the current document and DistFromEnd is MaxIndex minus the position of the first keyphrase occurrence in the text.  ? Shorter concept subsumption: In the key-phrase list, two concepts can occur, such that one is a specification of the other. Concept subsumption and boosting are used to merge or re-rank such couples of concepts. If a keyphrase is (stringwise) in-cluded in a longer keyphrase with a higher frequency, the frequency of the shorter keyphrase is transferred to the count of the longer one. E.g.  ?grid service discov-ery?=6 and ?grid service?=4 are re-ranked as ?grid service discovery?=10 and ?grid service?=0  ? Longer concept boosting: If a keyphrase is included in a longer one with a lower frequency, the average score between the two keyphrase frequency is computed. Such score is assigned to the less frequent keyphrase and subtracted from the fre-quency score of the higher ranked one. For example, if ?grid service discovery?=4 and ?grid service?=6, the average fre-quency is 5, so that ?grid service discov-ery?=5 and ?grid service? = 6?5=1. This parameter can be activated alone or to-gether with another one that modifies the criterion for computing the boosting. With this second option, the longer keyphrase is assigned the frequency of the shorter one. For example, if ?grid service discovery?=4 and ?grid service?=6, the boosting gives ?grid service discovery?=6 and ?grid serv-ice?=6. After the list of ranked keyphrases is extracted for each document, it is finally post-processed in two steps. The post-processing phase has been 
added specifically for SemEval, because key-phrases do not usually need to be stemmed and acronym expansion is relevant only for the spe-cific genre of scientific articles. For this reason, the two processes are not part of the official sys-tem architecture.  First, acronyms are replaced by the extended form, which is automatically extracted from the current document. The algorithm for acronym detection scans for parenthetical expressions in the text and checks if a preceding text span can be considered a suitable correspondence (Nguyen and Kan, 2007). The algorithm should detect cases in which the acronym appears after or before the extended form, like in ?Immediate Predecessors Tracking (IPT)? and ?IPT (Imme-diate Predecessors Tracking)?. If the acronym and the extended form appear both in the key-phrase list, only the extended form is kept and the acronym frequency is added.  The second step is stemming with the (Porter Stemmer). Then, we check if the list of stemmed keyphrases contains duplicate entries. If yes, we sum the frequencies of the double keyphrases and remove one of the two from the list.   3 Experimental Setup In the SemEval task, 144 training files were made available before the test data release. We split them into a training/development set of 100 documents and a test set of 44 documents, in or-der to find the best parameter combination. Key-phrase assignment is a subjective task and crite-ria for keyphrase identification depend on the domain and on the goal for which the keyphrases are needed. For example in scientific articles longer keyphrases are often more informative than shorter ones, so the parameters for boosting longer concepts are particularly relevant.    We first tested all parameters in isolation to compute the improvement over the frequency-based baseline. Results are reported in Table 1. F1 is computed as the harmonic mean of preci-sion and recall over the 15 top-ranked key-phrases after stemming. We report the combined F1, as computed by the task scorer in order to combine reader-assigned and author-assigned keyword sets.   Parameter F1 (combined) Baseline(MinDoc = 2) 13.63 Baseline(MinDoc = 4) 14.84 +CorpusColloc(small) 13.48     +CorpusColloc(big) 13.33 +IDF 17.98 
172
+KeyphraseLength 16.78 +FirstPosition 16.18 +ShortConcSubsumption 16.03 +LongConcBoost(version1) 14.38 +LongConcBoost(version2) 13.93 MinDoc = 4, +FirstPosition,    +IDF,   +KeyphraseLength,   +ShortConcSubsumption,  +LongConcBoost(version1) 
 25.62 
Table 1: Parameter performance over development set  The parameter scoring the highest improvement over the baseline is IDF. Also the parameters boosting longer keyphrases and those that occur at the beginning of the text are effective. Note that the LongConcBoost parameter achieves bet-ter results in the first version, which has a higher impact on the re-ranking. Surprisingly, using a domain corpus to extract information about multiword terms, as described in Section 2, steps 1 - 3, does not achieve any improvement. This means that KX can better recognize keyphrases in single documents without any corpus refer-ence. Besides, the best setting for MinDoc, the minimum number of multiword occurrences in the current document (see Section 2) is 4. We tested the CorpusColloc parameter using two different reference corpora: one contained the 100 articles of the training set (CorpusColloc small), while the other (CorpusColloc big) in-cluded both the 100 training articles and the 200 scientific publications of the NUS Keyphrase Corpus (Nguyen and Kan, 2007). The perform-ance is worse using the larger corpus than the smaller one, and in both cases it is below the baseline obtained without any reference corpus.  In the bottom row of Table 1, the best pa-rameter combination is reported with the score obtained over the development set. The im-provement over the baseline reaches 11.99 F1. 4 Evaluation In the SemEval task, the system was run on the test set (100 articles) with the best performing parameter combination described in the previous section. The results obtained over the 15 top-ranked keyphrases are reported in Table 2.   Keyphrase type P R F1 Reader-assigned 20.33 25.33 22.56 Combined 23.60 24.15 23.87 Table 2: System performance over test set In the competition, the F1 score over reader-assigned keyphrases was ranked 3rd out of 20 
participants, while the combined measure  achieved the 7th best result out of 20. 5 Conclusions In this work we have described KX, a flexible system for keyphrase extraction, which achieved promising results in the SemEval task 5. The good KX performance is due to its adaptable ar-chitecture, based on a set of parameters that can be tailored to the document type, the preferred keyphrase length, etc. The system can also ex-ploit multiword lists (with frequency) extracted from a reference corpus, even if this feature did not improve KX performance in this specific task. However, this proved to be relevant when applied to keyphrase extraction in the patent do-main, using a large domain-specific corpus of 10.000 very long documents (Pianta, 2009). A limitation of KX in the task was that it ex-tracts only keyphrases already present in a given document, while the author-assigned subtask in the SemEval competition included also key-phrases that do not occur in the text. Another improvement, which is now being implemented, is the extraction of the best parameter combina-tion using machine-learning techniques. References  Jones, S., Lundy, S. and Paynter, G. W. 2002. Interac-tive Document Summarization Using Automati-cally Extracted Keyphrases. In Proc. of the 35th Hawaii International Conference on System Sci-ences. Frantzi, K., Ananiadou, S. and Mima, H. 2000. Automatic recognition of multi-word terms: the C-value/NC-value method. Journal on Digital Li-braries. 3 (2), pp.115-130. Thuy Dung Nguyen and Min-Yen Kan. 2007. Key-phrase Extraction in Scientific Documents. In D.H.-L. Goh et al (eds.): ICADL 2007, LNCS 4822, pp. 317-326. Pianta, E., Girardi, C and Zanoli, R. 2006. The TextPro tool suite. In Proc. of LREC. Pianta, E. 2009. Content Distillation from Patent Ma-terial, FBK Technical Report. Ricca, F., Tonella, P., Girardi, C and Pianta, E. 2004. An empirical study on Keyword-based Web Site Clustering. In Proceedings of the 12th IWPC. PorterStemmer: http://tartarus.org/~martin/PorterStemmer/perl.txt.  
173
Answering Why-Questions in
Closed Domains from a
Discourse Model
Rodolfo Delmonte
University of Venice ?Ca? Foscari? (Italy)
email: delmont@unive.it
Emanuele Pianta
Fondazione Bruno Kessler ? FBK (Italy)
email: pianta@fbk.eu
Abstract
In this paper we will present a system for Question Answering called
GETARUNS, in its deep version applicable to closed domains, that is to
say domains for which the lexical semantics is fully specified and does
not have to be induced. In addition, no ontology is needed: semantic re-
lations are derived from linguistic relations encoded in the syntax. The
main tenet of the system is that it is possible to produce consistent seman-
tic representations using a strict linguistic approach without resorting to
extralinguistic knowledge sources. The paper will briefly present the low
level component which is responsible for pronominal binding, quantifier
raising and temporal interpretation. Then it will discuss in more detail
the high level component where a Discourse Model is created from text.
The system has been evaluated on a wide variety of texts from closed
domains, producing full and accurate parsing, semantics and anaphora
resolution for all sentences.
103
104 Delmonte and Pianta
1 Introduction
In this paper we will present the system for Question Answering called GETARUNS,
in its deep version applicable to closed domains, that is to say domains for which the
lexical semantic is fully specified and does not have to be induced. GETARUNS is
a GEneral multilingual Text And Reference UNderstander which follows a linguisti-
cally based approach to text understanding and embodies a number of general strate-
gies on how to implement linguistic principles in a running system. The system ad-
dresses one main issue: how to restrict access to extralinguistic knowledge of the
world by contextual reasoning, i.e. reasoning from linguistically available cues.
Another important issue addressed by the system is multilinguality. In GETARUNS
the user may switch from one language to another by simply unloading the current lex-
icon and uploading the lexicon for the new language: at present Italian, German and
English are implemented. Multilinguality has been implemented to support the theo-
retical linguistic subdivision of Universal Grammar into a Core and a Peripheral set of
rules. The system is organized around another fundamental assumption: the architec-
ture of such a system must be modular thus requiring a pipeline of sequential feeding
processes of information, each module providing one chunk of knowledge, backtrack-
ing being allowed only within each single module. The architecture of the system
is organized in such a way as to allow for feedback into the parser from Anaphoric
Binding: however, when pronominals have been finally bound or left free, no more
changes are allowed on the f-structure output of the parser.
Thus, we can think of the system as being subdivided into two main meta-modules
or levels: Low Level System, containing all modules that operate at Sentence Level;
High Level System, containing all the modules that operate at Discourse and Text
Level by updating the Discourse Model. The deep and complete version of the sys-
tem that we present here can be used with strictly closed domains and does not need
any supporting ontology. However, it has also been used in one such context with a
different architecture, which had OWL and RDFs as final external knowledge repre-
sentation formalisms. Ontologies and Knowledge Sources should be used as Word
Sense Disambiguation tools (we have not produced results on this however).
Texts belonging to what we define as closed domains are characterized by the fact
that the system has all the semantic information which is needed process then; and
most importantly, sentences making up the texts can be fully parsed without failures.
In practice, these texts are relatively short and the length of sentences is below a certain
threshold, typically 25 words. They are used for text understanding practice in a lan-
guage learning environment. In this context, question answering is used to validate the
appropriateness of the user?s answer. Some such texts will be presented below. One
will be the text used by Mitre in 2000 to organize the Workshop on Reading Compre-
hension Tests as Evaluation for Computer-Based Language Understanding Systems
(Brill et al, 2000). The system has been evaluated on a wide variety of such texts
and has parsed fully and accurately all sentences with the appropriate Semantics and
Anaphora Resolution (Delmonte, 2007).
Answering Why-Questions in Closed Domains from a Discourse Model 105
2 The Low Level System
Even though we assume that the output of the Low Level System is mandatory for the
creation of the semantic representation needed to create a consistent Discourse Model
we will not be able comment it in depth for lack of space. We will simply show the
internal components or modules it encompasses and add a few comments. However
we stress the paramount importance of a deep linguistic analysis of the input text.
When each sentence is parsed, tense, aspect and temporal adjuncts are used to
build the basic temporal interpretation to be used by the temporal reasoner. Every
constituent is checked for semantic consistency and semantic features are added to
each semantic head in the form of generic concepts taken from WordNet and other
similar semantic lexical resources.
Eventually two important modules are implemented: Quantifier Raising and Pro-
nominal Binding. Quantifier Raising is computed on f-structure which is represented
internally as a DAG (Direct Acyclic Graph). It may introduce a pair of functional
components: an operator where the quantifier can be raised, and a pool containing the
associated variable where the quantifier is actually placed in the f-structure represen-
tation. This information may then be used by the following Higher System to inspect
quantifier scope.
Pronominal Binding is carried out at first at sentence internal level. DAGs will
be searched for binding domains and antecedents matched to the pronouns if any to
produce a list of possible bindings. Best candidates will then be chosen.
3 The Discourse Model
Informally, a DiscourseModel (DM)may be described as the set of entities "specified"
in a discourse, linked together by the relations they participate in. They are called dis-
course entities, but may also be regarded as discourse referents or cognitive elements.
A discourse entity (DE) inhabits a speaker?s discourse model and represents some-
thing the speaker has referred to. A speaker refers to something by utterances that
either evoke (if first reference) or access (if subsequent reference) its corresponding
discourse entity.
As soon as a DE is evoked, it gets a description. The initial description ID that
tags a newly evoked DE might have a special status, because it is the only information
about an entity that can be assumed to be shared (though not necessarily believed) by
both speaker and listener alike. However certain types of DE must be derived from
other ones inferentially.
Definite descriptions can be used like definite pronouns to access entities which are
presumably in the listener?s DM, or they can be used to evoke new entities into that
model.
Building a DM is clearly only a part of the overall process of understanding which
makes heavy use of background mutual knowledge on the side of the addressee in
order to carry out the complex inferences required. In order to build an adequate Dis-
course Model we rely on a version of Situation Semantics which takes perspectives or
point of view as the higher node in a hierarchical scheme in which there is a bifurca-
tion between factual and non-factual situations. Partially following Burke (1991) we
assume that the notion of perspectives is significant in situation theory insofar as the
106 Delmonte and Pianta
very same situations can be viewed by an agent (or by different agents) from different
perspectives, hence situations may support different and perhaps conflicting kinds of
information (ibid., p.134). Situations are characterized in terms of infons, or better
the infons that they support. In turn we distinguish between facts and concepts where
the former have to do with concrete ostensive entities which yield information that is
referential, in that they explicitly involve objects in the world relative to a given per-
spective. On the contrary concepts constitutes a piece of general information about
the world relative to a given perspective, which does not directly refer to any particular
entity or object, nor is it specific to particular ostensive entities.
Infons are built according to situation theory: a basic infon consists of a relation, its
argument roles , a polarity, and a couple of indices anchoring the event/state/processe
to a given spatiotemporal location.
In our system, facts may describe information relative to a subjective or an objec-
tive discourse domain: subjective facts are thus computable as situations viewed from
the perspective of a given agent?s mind, in our case also corresponding to the Main
Topic of discourse. On the contrary, objective facts are reported from the perspective
of the text?s author. However, to highlight the difference existing between subjective
and objective information in the model, we decided to call facts only objective infons;
subjective infons are called sit. Also generic facts are treated as sits.
These main constituents of situations are further described by taking as primitives
individuals, relations and locations and by using as logical notation set theory. Thus,
individuals and inferences on individuals are wrought out in set theory notation: we
use ind for a unique individual, set for a collection of individuals which can be indi-
viduated by means of membership, card for the cardinality of any set with a numerical
or indefinite quantified value, in to indicate membership, class for generic sets which
can be made up of an indefinite quantity however big enough to encompass sets, sub-
sets, classes or individuals. Each entity is assigned a constant value or id and an infon
which are uniquely individuated by a number.
Infons may express or contain a main relation: relations may be properties, so-
cial or relational roles, events or states, locational modifiers or specifiers ? that is
attributes, etc.. Simplex properties predicate some property of a semantic identifier;
complex properties take individuals and propositions as their arguments and in this
case individuals are usually associated to a semantic role. Semantic roles are inherited
from the lexical form associated to a given predicate in the lexicon and transferred
into the f-structure of the utterance under analysis. Semantic roles are paramount in
the choice and construction of questions and answers.
Inferences are produced every time a given property is reintroduced in the story in
order to ascertain whether the same property was already present in the model and
should not be reasserted, or whether it should be added to it. Properties may be an-
chored to a given location or be universally anchored: a name, is a rigid designator in
that it is computed as a property associated to a given individual and has a universal
locational anchoring, meaning that the same individual will always be individuated
by that name in the story. The same would apply to permanent properties like the
substance or matter constituting an object, like a house, or other such properties. Per-
sistence may then be computed both for entities, properties, relations and locations;
also, a Relevance Score is computed by a separate module that analyzes information
Answering Why-Questions in Closed Domains from a Discourse Model 107
structure for each simplex utterance.
4 Semantic Rules
After collecting all modifier heads, if any, of the current predicate, the rule for the
creation of semantic individuals separates previously resolved pronouns/nouns from
non resolved ones. In both cases it uses some sort of equational reasoning in order
to ascribe properties to already asserted semantic identifiers, by taking advantage of
linguistic information encoded in Function/Role, according to a linguistically well-
defined hierarchy which treats arguments and adjuncts as semantically different. New
semantic individuals are added when needed.
The module handling semantic individuals treats new individuals to be asserted in
the DM separately from already asserted ones ? in which case, the semantic index
should be inherited from properties belonging to previously asserted individuals. In
addition, quantified expressions should be treated differently from individuals or sets,
be they singleton sets, or sets with a given cardinality.
Semantic attributes are collected in the f-structure representation and come from the
SPEC subsidiary function. We use the following attributes to separate semantic types:
definiteness, partitivity and class. Definiteness applies to nominal expressions: these
may be definite (+def), indefinite (-def), or zero definite (def0), which applies both
to bare NPs and to proper nouns; partitivity is an attribute which gets a value only
in case of quantified NPs. Finally the class attribute is used to differentiate proper
nouns (-class) from common nouns (+class) which may undergo quantification,
and quantified pronouns (+me).
5 Question Answering from a Discourse Model
In order to show how the system behaves we report and focus only on one small text.
New texts are usually fully parsed: some intervention may be required to introduce
contextual classes for tag disambiguation purposes. Here below is the text and the
questions proposed for the Workshop on Text Understanding quoted above:
How Maple Syrup is Made
Maple syrup comes from sugar maple trees. At one time, maple syrup
was used to make sugar. This is why the tree is called a "sugar" maple
tree. Sugar maple trees make sap. Farmers collect the sap. The best time
to collect sap is in February and March. The nights must be cold and the
days warm. The farmer drills a few small holes in each tree. He puts
a spout in each hole. Then he hangs a bucket on the end of each spout.
The bucket has a cover to keep rain and snow out. The sap drips into the
bucket. About 10 gallons of sap come from each hole.
1. Who collects maple sap? (Farmers)
2. What does the farmer hang from a spout? (A bucket)
3. When is sap collected? (February and March)
4. Where does the maple sap come from? (Sugar maple trees)
5. Why is the bucket covered? (to keep rain and snow out)
As far as we gathered from the proceedings of the conference, none of the participants
was able to answer all the questions (Brill et al, 2000).
108 Delmonte and Pianta
This is howwe organize the system. We first compute the DM of the target question
(hereafter QDM), the whole process is carried out on the basis of the facts contained
in the question ad text DMs. Questions are classified into three types: partial or wh-
questions, why questions and complete or yes/no questions.
Recovering the answer from the DM is essentially done in four steps:
? extracting question word or question type for yes/no questions
? extracting the main predicates from the question, which are then used to
? search for identical/similar predicates in the text DM
? extraction of the argument matching the answer
As commented in the sections above, the semantic representation contained in a
DM can be basically defined as Predicate-Argument Structures or PAS, with a polarity
and pair of spatiotemporal indices. Given a short text and a question about the text,
the QA system will build a semantic model of the text where each distinct entity
is assigned a unique semantic identifier, and is represented as a pool of properties,
relations and attributes. Whenever possible, the system will also draw the necessary
inferences to assign relation and attributes of sets to the individuals composing those
sets.
Then it will completely parse the input question and produce a QDM for it, where
facts are represented as q_fact terms. Afterwards, the first move consists in recovering
the question word in the QDM by the following conjunction of queries
q_fact(K,focus,[arg:Id],1,_,_),
q_fact(_,isa,[_:Id,_:Focus],1,A,B)
where the variable Id, associated to the property "focus", is used to recover the ac-
tual Focus in the associated "isa" fact. This Focus is constituted by the question word
used to formulate the query. This is used by the system to activate specialized proce-
dures that will address specific semantic structures. As said above, why questions are
processed separately from other wh- questions. The next query fired is
get_focus_arg(Focus,Pred,Args,Answer,True-NewFact),
which will give back the contents of the answer in the variable Answer and the govern-
ing predicate in Pred. These are then used to generate the actual surface form of the
answer. Args and True-NewFact are used in case the question is a complete or yes/no
question. In order to generate the answer, tense and mood are searched in the DM;
then a logical form is build as required by the generator, and the build_reply procedure
is fired:
get_focus_tense(T,M), Form=[Pred,T,M,P,[D]],
build_reply(Out,Focus,Form), !.
We will present general wh- questions at first. They include all types of factoid
questions and also how questions. The main predicate looks for an appropriate lin-
guistic description to substitute the wh- word argument position in the appropriate
PAS. Here follows the full definition of the get_focus_arg procedure for the ?who?
case.
Answering Why-Questions in Closed Domains from a Discourse Model 109
get_focus_arg(who,Pred,Ind,D1,NewP):-
q_getevents(A,Pred),
q_fact(X,Pred,Args,1,_,L),
q_role(Y,X,Z,Role),
answer_buildarg(Role,Pred,[Idx:Z],D,Facts),
select_from_pred1(Pred,Role,Facts,NewP,D1), !.
We use a different procedure in case the question governing predicate is a copu-
lative verb, because we have to search for the associated property in the QDM, as
follows:
copulative(Pred),
q_fact(X,Pred,[prop:Y],1,_,_),
q_fact(Y,Prop,[_:K,Role:Type],1,_,_)
q_fact(_,inst_of,[_:K,_:Z],P,T,S),
q_get_ind_des(K,Propp,Ty),
Copulative predicates have a proposition as their argument and the verb itself is not
useful, being semantically empty. The predicate corresponding to the proposition is
searched through the infon Y identifying the fact. When we have recovered the Role
and the linguistic description of the property Propp indicated by the wh- question, we
pass them to the following predicate and search the associated individual in the DM:
answer_buildarg(Role,Pred,[Idx:Propp],Answer,Facts)
Suppose the wh-question is a where question with a copulative verb; then the role
will be a location and the Propp will be "in". How copulative questions will search for
class properties, i.e. not for names or individuals:
q_fact(X,how,[_:Y],1,_,_),
q_fact(Q,isa,[_:K,class:Pred],1,_,_),
q_fact(_,inst_of,[_:K,_:Z],P,T,S)
Semantic roles are irrelevant in this latter case: the only indication we use for the
search is a dummy prop role. On the contrary, when lexical verbs are governing predi-
cates, we need to use the PAS and the semantic role associated to the missing argument
to recover the appropriate answer in all other cases. Here we should also use a differ-
ent semantic strategy in case an argument is questioned and there is another argument
expressed in the question ? what, whom, who. Or else an adjunct is questioned ?
where, when, how, etc. ? or the predicate is intransitive, an argument is questioned
and there is no additional information available.
Now consider a typical search for the answer argument:
answer_buildarg(Role,Pred,Tops,Answer,Facts):-
on(Ind:Prop,Tops),
entity(Type,Id,Score,facts(Facts)),
extract_properties(Type,Ind,Facts,Def,Num,NProp,Cat),
select_allrole_facts(Role,Ind,Facts,Pred,PropLoc),
Answer=[Def,nil,Num,NProp,Cat,PropLoc], !.
110 Delmonte and Pianta
Here, extract_properties checks for the appropriate semantic type and property by
picking one entity and its properties at the time. When it succeeds, the choice is
further checked and completed by the call to select_allrole_facts. This is what ex-
tract_properties does:
extract_properties(Type, Ind, Facts, Def, Num, NProp, Gend):-
( Sclass=prop,
extrfacts(Facts,Ind,Gend,Sclass,Prop), Num=sing
; Sclass=class,
extrfacts(Facts,Ind,CGend,Sclass,Prop),
select_gend(Prop,CGend,Gend) ),
topichood_stack(Prop,Def),
( Type=ind, Num=sing
; Type=set, Num=plur ),
set_def(Sclass, Ind, Prop, Role, Def),
confirm_head(Def, Gend, Prop, NProp), !.
The procedure searches for individuals or sets filling a given semantic role in the
predicate-argument structure associated to the governing predicate. In addition, it has
the important task of setting functional and semantic features for the generator, like
gender and number. This is paramount when a pronoun has to be generated instead
of the actual basic linguistic description associated to a given semantic identifier. In
particular, gender may be already explicitly associated in the DM to the linguistic
description of a given entity or it may be derived from WordNet or other linguistic
resources handling derivational morphology. The call topichood_stack looks for static
definiteness information associated to the linguistic description in the DM. Proper
names are always "definite". On the contrary, common nouns may be used in definite
or indefinite ways. This information may be modified by the dialogue intervening
between user and system and be recorded in the user model. The decision is ulti-
mately taken by the set_def procedure which looks into the question-answering user
model knowledge base where previous mentions of the same entity might have been
recorded. Or else it does it ? by means of update_user_model? to be used in further
user-system interactions. If the entity semantic identifier is already present Def will
be set to "definite", otherwise it will remain as it has been originally set in the DM.
set_def(Def,Id,Prop,Role,Def1):-
( tknow(Id,Role1), swap_def(Def,Def1)
; tknow(Prop,Role1), swap_def(Def,Def1)
; update_user_model(Id,Role), assign_def(Def,Def1) ).
6 Computing Answers to WHY questions
Why question are usually answered by events, i.e. complete propositions. They would
in general constitute cases of rhetorical clause pairs labelled either as a Motivation-
Effect or a Cause-Result. In Delmonte et al (2007), causal relations are further de-
composed into the following finer-grained subprocesses:
? Cause-Result
? Rationale-Effect
Answering Why-Questions in Closed Domains from a Discourse Model 111
? Purpose-Outcome
? Circumstance-Outcome
? Means-Outcome
Furthermore, rationale clauses have been shown to be constituted structurally by un-
tensed Infinitival Adjuncts: on the contrary, Cause-Result pairs are usually constituted
by tensed propositions.
Consider now the pieces of knowledge needed to build the appropriate answer to the
question "Why is the tree called sugar maple tree?". Sentences involved to reconstruct
the answer are:
Maple syrup comes from sugar maple trees.
At one time, maple syrup was used to make sugar.
This is why the tree is called a "sugar" maple tree.
In other words, in order to build the appropriate answer, the system should be able
to build an adequate semantic representation for the discourse anaphora "This", which
is used to essentially relate the current sentence to the event chain of the previous
sentence. This is a fairly common way of expressing this kind of causal relation, that
we then would like to assume as a paradigmatic one. Eventually, the correct answer
would be:
Because maple syrup was used to make sugar
which as can be easily gathered is the content of the previous complex sentence. Here
below is the portion of the DM representation needed to reconstruct the answer:
ind(infon19, id8)
fact(infon20,inst_of,[ind:id8,class:edible_animal],1,univ, univ)
fact(infon21, isa,[ind:id8,class:[maple_syrup]],1, id1, id7)
set(infon23, id9)
card(infon24, id9, 5)
fact(infon25, sugar_maple, [ind:id10], 1, id1, id7)
fact(infon26, of, [arg:id10, specif:id9], 1, univ, univ)
fact(infon27,inst_of,[ind:id9,class:plant_life],1,univ, univ)
fact(infon28, isa, [ind:id9, class:tree], 1, id1, id7)
class(infon43, id13)
fact(infon44,inst_of,[ind:id13,class:substance],1,univ, univ)
fact(infon45, isa, [ind:id13, class:sugar], 1, id1, id7)
fact(id14,make,[agent:id8,theme_aff:id13],1,tes(finf_m3), id7)
fact(infon48,isa,[arg:id14,arg:ev],1,tes(finf_m3), id7)
fact(infon49, isa, [arg:id15, arg:tloc], 1, tes(finf_m3), id7)
fact(infon50, pres, [arg:id15], 1, tes(finf_m3), id7)
fact(infon51,time,[arg:id14,arg:id15], 1, tes(finf_m3), id7)
fact(id16,use,[theme_unaff:id8,prop:id14], 1, tes(sn5_m3), id7)
fact(id21,call,[actor:id9, theme_bound:id9], 1, tes(f1_m4), id7)
ent(infon61, id18)
fact(infon62,prop,[arg:id18,
disc_set:[id16:use:[theme_unaff:id8, prop:id14]]],
1, id1, id7)
ind(infon63, id19)
fact(infon66, inst_of, [ind:id19, class:abstract], 1, univ, univ)
fact(infon67, isa, [ind:id19, class:reason], 1, id1, id7)
fact(infon81, in, [arg:id21, nil:id19], 1, tes(f1_m4), id7)
fact(infon83, reason, [nil:id18, arg:id19], 1, id1, id7)
fact(id23, be, [prop:infon83], 1, tes(sn10_m4), id7)
112 Delmonte and Pianta
These three pieces of knowledge representation are built respectively when the
three sentences above are processed. When the second sentence is processed, the
semantic identifier id8 is simply inherited. Also, notice that it is transferred from USE
predicate to MAKE by means of controlling equations which are part of LFG syntactic
representations.
The system will at first look for a REASON semantic predicate associated to a
CALL predicate, as derived from the question semantic representation, which we re-
port here below:
q_loc(infon3, id1, [arg:main_tloc, arg:tr(f2_q6)])
q_ind(infon4, id2)
q_fact(infon5, tree, [nil:id2], 1, id1, univ)
q_fact(infon6, maple, [ind:id2], 1, id1, univ)
q_fact(infon7, sugar, [ind:id2], 1, id1, univ)
q_fact(infon8, of, [arg:id2, specif:id2], 1, univ, univ)
q_fact(infon9, why, [ind:id2], 1, id1, univ)
q_fact(infon10, inst_of, [ind:id2, class:plant_life], 1, univ, univ)
q_fact(infon11, isa, [ind:id2, class:tree], 1, id1, univ)
q_class(infon12, id3)
q_fact(infon13, inst_of, [ind:id3, class:substance], 1, univ, univ)
q_fact(infon14, isa, [ind:id3, class:sugar], 1, id1, univ)
q_class(infon15, id4)
q_fact(infon16, inst_of, [ind:id4, class:plant_life], 1, univ, univ)
q_fact(infon17, isa, [ind:id4, class:maple], 1, id1, univ)
q_fact(infon22, tree, [nil:id2, arg:id2], 1, id1, univ)
q_fact(id5, call, [prop:infon22], 1, tes(f2_q6), univ)
q_fact(infon23, isa, [arg:id5, arg:ev], 1, tes(f2_q6), univ)
q_fact(infon24, isa, [arg:id6, arg:tloc], 1, tes(f2_q6), univ)
q_fact(infon25, pres, [arg:id6], 1, tes(f2_q6), univ)
q_fact(infon26, time, [arg:id5, arg:id6], 1, tes(f2_q6), univ)
q_fact(infon27, focus, [arg:id7], 1, tes(f2_q6), univ)
q_fact(infon28, isa, [arg:id7, arg:why], 1, tes(f2_q6), univ)
q_fact(infon29, for, [arg:id5, motiv:id7], 1, tes(f2_q6), univ)
q_fact(infon35, perf, [arg:id8, ask:id5], 1, id1, univ)
The final part of the answer building process is constituted by the search of the
actual linguistic description to associate to the original predicate. This is done in the
pool of facts associated to the current entity which has been chosen from the inventory
of entities of the world associated to the original text.
answer_buildarg(Role,Pred,Tops,Answer,Facts,[]):-
on(Ind:Prop,Tops),
entity(Type,Id,Score,facts(Facts)),
extract_properties(Type,Ind,Facts,Def,Num,NProp,Cat),
select_allrole_facts(Role,Ind,Facts,Pred,PropLoc),
Answer=[Def,nil,Num,NProp,Cat,PropLoc],!.
select_allrole_facts(Role,Ind,Facts,Pred,PropLoc):-
selarf(Pred,Fact,Args,Pol,Id),
on(Fact,Facts),
isa_role_fatto(Args),
ind_role(Args,Inds),
on(Prop-Role1,Inds),
belongsrole(Role,Role1), !.
For instance, when searching the answer to the question "who collects the sap?",
the answer is searched in the following pool associated to the entity FARMER:
Answering Why-Questions in Closed Domains from a Discourse Model 113
entity(set,id32,28,facts([
card(infon117,id32,5),
fact(infon118,inst_of,[ind:id32,class:man],1,univ,univ),
fact(infon119,isa,[ind:id32,class:farmer],1,id31,id8),
fact(id33,collect,[agent:id32,theme_aff:id28],1,tes(f1_m6),id8),
fact(id58,drill,[agent:id32,theme_aff:id56],1,tes(f1_m9),id8),
fact(id63,put,[agent:id32,theme_aff:id61,loc_direct:id56],1,tes(f1_m10),id8),
fact(id69,hang,[agent:id32,theme_aff:id66,loc_direct:id67],1,tes(f1_m11),id8)])).
This is reached from the COLLECT and SAP entities pools, which are cross-
checked to verify that the same predicates are available.
entity(class,id28,7,facts([
fact(infon102,inst_of,[ind:id28,class:substance],1,univ,univ),
fact(infon103,isa,[ind:id28,class:sap],1,id27,id8),
fact(id29,make,[actor:id9,theme_aff:id28],1,tes(f1_m5),id8),
fact(id33,collect,[agent:id32,theme_aff:id28],1,tes(f1_m6),id8),
fact(id41,collect,[agent:id39,theme_aff:id28],1,tes(finf_m7),id8),
fact(id82,drip,[agent:id28, modal:id66],1,tes(f1_m13),id8),
fact(infon343,has,[arg:id88,theme:id28],1,id84,id85)])).
Then belongsrole checks to verify that the Role belongs to the appropriate set of
roles adequate for that slot in the PAS. In the ?why? case it has to search recursively
for events. This is the case represented by discourse anaphora of the type "this is
why/that is why", where the reason is a complex event structure:
extract_properties(Role,Ind,Facts,NewProp):-
Fact =.. [fact,Id,Pred,Args,Pol,Time,Place],
on(Fact,Fa),
on(_:Ind,Args),
on(disc_set:Disc,Args),
Disc=[Ind1:Pre:[Ro1:Id1, Ro2:Id2]],
buildarg2(Ro2,NewP,[Id1:Prop],FirstProp,Facts,MDs),
FirstProp=[Def1,nil,Num,NProp,Cat,PropLoc],
Fact1 =.. [fact,Id2,NewP,Args1,Pol1,Time1,Place1],
on(Fact1,Facts),
on(Ro3:Ind2,Args1),
Ind2$\backslash$=Id1,
buildarg2(Ro3,What,[Ind2:Prop],SecProp,Facts1,MDs),
SecondProp=[Def2,nil,Num2, NProp2,Cat2,PropLoc2],
Prop_Why=[to,What,NProp2],
NewProp=[Pre,[Def1,nil,Num,NProp,Cat,Prop_Why]], !.
Here below is the relevant DM representation of the other WHY question, the one
requesting for a Motivation through Rational clauses: ?why is the bucket covered??
loc(infon288, id73, [arg:main_tloc, arg:tes(sn7_m11)])
ind(infon289, id74)
fact(infon290, inst_of, [ind:id74, class:event], 1, univ, univ)
fact(infon291, isa, [ind:id74, class:rain], 1, univ, univ)
ind(infon292, id75)
fact(infon293, inst_of, [ind:id75, class:event], 1, univ, univ)
fact(infon294, isa, [ind:id75, class:snow], 1, univ, univ)
ind(infon295, id76)
fact(infon296, isa, [ind:id76, class:cover], 1, id73, id8)
fact(infon297, inst_of, [ind:id76, class:legal], 1, univ, univ)
fact(infon301, cover, [nil:id69, arg:id76], 1, id73, id8)
fact(id77,have,[owner:id69,prop:infon301],1,tes(sn10_m12),id8)
114 Delmonte and Pianta
fact(infon302, isa, [arg:id77, arg:st], 1, tes(sn10_m12), id8)
fact(infon303, isa, [arg:id78, arg:tloc], 1, tes(sn10_m12), id8)
fact(infon304, pres, [arg:id78], 1, tes(sn10_m12), id8)
fact(infon305, time, [arg:id77, arg:id78], 1, tes(sn10_m12), id8)
in(infon322, id74, id79)
in(infon323, id75, id79)
fact(id80,keep_out,[actor:id69,theme_aff:id79],1,tes(finf1_m12), id8)
fact(infon308, isa, [arg:id80, arg:pr], 1, tes(finf1_m12), id8)
fact(infon309, isa, [arg:id81, arg:tloc], 1, tes(finf1_m12), id8)
fact(infon310, pres, [arg:id81], 1, tes(finf1_m12), id8)
fact(infon311, time, [arg:id80, arg:id81], 1, tes(finf1_m12), id8)
fact(infon312, result, [arg:id77, arg:id80], 1, tes(sn10_m12), id8)
during(tes(sn10_m12), tes(sn7_m11))
includes(tr(sn10_m12), id73)
The relevant information is expressed as a semantic role RESULT, and is the one
connecting the two predicates, HAVE/KEEP_OUT. This is the piece of information
that will be used to answer the question.
7 Conclusions
In the paper we have shows that one can actually implement systems using deep lin-
guistic and semantic analysis to answer hard questions. Our systems employs repre-
sentations derived from Situation Semantics paradigm (Burke, 1991) and LFG syn-
tactic theory (Bresnan, 2001). We have exemplified its performance on a series of
factoid questions and we also added ?why? questions. GETARUNS has been able to
answer all questions proposed in the Mitre Workshop and also the additional seman-
tically and syntactically hard discourse bound Why question based on the recurrent
formulaic copulative expression ?this/that is why?. For a complete presentation of the
system please refer to Delmonte (2007).
References
Bresnan, J. (2001). Lexical-Functional Syntax. Oxford: Blackwell.
Brill, E., E. Charniak, M. Harper, M. Light, E. Riloff, and E. Voorhees (Eds.) (2000,
May). Reading Comprehension Tests as Evaluation for Computer-Based Language
Understanding Sytems, Seattle, Washington. ANLP-NAACL.
Burke, T. (1991). Peirce on truth and partiality. In J. Barwise, J. M. Gawron,
G. Plotkin, and S. Tutiya (Eds.), Situation Theory and its Applications. Stanford:
CSLI Publications.
Delmonte, R. (2007). Computational Linguistic Text Processing: Logical Form, Se-
mantic Interpretation, Discourse Relations and Question Answering. New York:
Nova Science Publishers.
Delmonte, R., G. Nicolae, S. Harabagiu, and C. Nicolae (2007). A linguistically-
based approach to discourse relations recognition. In B. Sharp and M. Zock (Eds.),
Natural Language Processing and Cognitive Science: Proc. of 4th NLPCS (Fun-
chal, Portugal), pp. 81?91. INSTICC PRESS.
Proceedings of the 8th International Conference on Computational Semantics, pages 342?345,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
A novel approach to mapping
FrameNet lexical units to WordNet synsets
Sara Tonelli, Emanuele Pianta
Abstract
In this paper we present a novel approach to mapping FrameNet
lexical units to WordNet synsets in order to automatically enrich the
lexical unit set of a given frame. While the mapping approaches pro-
posed in the past mainly rely on the semantic similarity between lexical
units in a frame and lemmas in a synset, we exploit the definition of
the lexical entries in FrameNet and the WordNet glosses to find the
best candidate synset(s) for the mapping. Evaluation results are also
reported and discussed.
1 FrameNet and the existing mapping approaches
The FrameNet database [1] is a lexical resource of English describing some
prototypical situations, the frames, and the frame-evoking words or expres-
sions associated with them, the lexical units (LU). Every frame corresponds
to a scenario involving a set of participants, the frame elements, that are typ-
ically the syntactic dependents of the lexical units. The FrameNet resource
is corpus-based, i.e. every lexical unit should be instantiated by at least
one example sentence, even if at the moment the definition and annotation
step is still incomplete for several LUs. FrameNet has proved to be useful
in a number of NLP tasks, from textual entailment to question answering,
but its coverage is still a major problem. In order to expand the resource,
it would be a good solution to acquire lexical knowledge encoded in other
existing resources and import it into the FrameNet database. WordNet [4],
for instance, covers the majority of nouns, verbs, adjectives and adverbs in
the English language, organized in synonym sets called synsets. Mapping
FrameNet LUs to WordNet synsets would automatically increase the num-
ber of LUs per frame by importing all synonyms from the mapped synset,
and would allow to exploit the semantic and lexical relations in WordNet to
enrich the information encoded in FrameNet.
342
Several experiments have been carried out in this direction. Johansson and
Nugues [5] created a feature representation for every WordNet lemma and
used it to train an SVM classifier for each frame that tells whether a lemma
belongs to the frame or not. Crespo and Buitelaar [3] carried out an au-
tomatic mapping of medical-oriented frames to WordNet synsets, trying to
select synsets attached to a LU that were statistically significant in a given
reference corpus. De Cao et al [2] proposed a method to detect the set
of suitable WordNet senses able to evoke the same frame by exploiting the
hypernym hierarchies that capture the largest number of LUs in the frame.
For all above mentioned approaches, a real evaluation on randomly selected
frames is missing, and accuracy was mainly computed over the new lexical
units obtained for a frame, not on a gold standard where one or more synsets
are assigned to every lexical unit in a frame. Besides, it seems that the most
common approach to carry out the mapping relies on some similarity mea-
sures that perform better on richer sets of lexical units.
2 The mapping algorithm
2.1 Motivation
We propose a mapping algorithm that is independent of the number of LUs
in a frame and from the example sentences available. In fact, we believe
that under real-usage conditions, the automatic expansion of LUs is typi-
cally required for frames with a smaller LU set, especially for those with
only one element. In the FrameNet database (v. 1.3), 33 frames out of 720
are described only by one lexical unit, and 63 are described by two. Further-
more, almost 3000 lexical units are characterized only by the lexicographic
definition and are not provided with example sentences. For this reason, we
suggest an alternative approach that makes use of usually unexploited infor-
mation collected in the FrameNet database, namely the definition associated
with every lexical unit.
Since both WordNet glosses and FrameNet definitions are manually writ-
ten by lexicographers, they usually show a high degree of similarity, and
sometimes are even identical. For example, the definition of thicken in the
Change of consistency frame is ?become thick or thicker?, which is identical
to the WordNet gloss of synset n. v#00300319. The thicken lemma occurs
in three WordNet synsets, and in each of them it is the only lemma available,
so no synonymy information could be exploited for the sense disambiguation.
343
2.2 The algorithm
We tried to devise a simple method to map a FrameNet Lexical Unit (LU)
into one or more WordNet synsets. Given a LU L from a frame F, we first
find the set of all synsets containing L (L candidate set, LCandSet). If LCan-
dSet contains only one synset, this is assigned to L. Otherwise, we look for
the synsets in LCandSet whose WN gloss has the highest similarity with the
FrameNet definition of L. We tried two baseline similarity algorithms based
respectively on stem overlap and on a modified version of the Levenshtein
algorithm taking stems as comparison unit instead of characters. Stem over-
lap turned out to perform definitely better than Levehnstein. Then we tried
to improve on simple stem overlap baseline by considering also the other
LUs in F. To this extent, we calculate the set of all synsets linked to any
LU in F (FCandSet). This is exploited in two ways. First, we boost the
similarity score of the synsets in LCandSet with the largest number of links
to other LUs in F (according to FCandSet). Secondly we assign to F the
most common WordNet Domain in FCandSet, and then boost the similarity
score of LCandSet synsets belonging to the most frequent WordNet-Domain
in F. We discard any candidate synset with a similarity score below a MIN
threshold; on the other side, we accept more than one candidate synset if
they have a similarity score higher than a MAX threshold.
3 Evaluation
We created a gold standard by manually mapping 380 LUs belonging to
as many frames to the corresponding WordNet synsets. Then, we divided
our dataset into a development set of 100 LUs and a testset of 280 LUs.
We tested the Levenshtein algorithm and the Stem Overlap algorithm (SO),
then we evaluated the improvement in performance of the latter taking into
account information about the most frequent domain (D) and the most
frequent synsets (Syn). Results are reported in Table 1.
Table 1: Mapping evaluation
Precision Recall F-measure
Levenshtein 0.50 0.49 0.49
Stem Overlap (SO) 0.66 0.56 0.61
SO+Domain (D) 0.66 0.57 0.61
SO+D+Syn 0.71 0.62 0.66
344
We carried out several tests to set the MIN and MAX threshold in order
to get the best F-measure, reported in Table 1. As for precision, the best
performance obtained with SO+D+Syn and a stricter MIN/MAX threshold
scored 0.78 (recall 0.36, f-measure 0.49).
4 Conclusions
We proposed a new method to map FrameNet LUs to WordNet synsets
by computing a similarity measure between LU definitions and WordNet
glosses. To our knowledge, this is the only approach to the task based on
this kind of similarity. The only comparable evaluation available is reported
in [5], and shows that our results are promising. De Cao at al. [2] reported a
better performance, particularly for recall, but evaluation of their mapping
algorithm relied on a gold standard of 4 selected frames having at least 10
LUs and a given number of corpus instantiations.
In the future, we plan to improve the algorithm by shallow parsing the
LU definitions and the WordNet glosses. Besides, we will exploit informa-
tion extracted from the WordNet hierarchy. We also want to evaluate the
effectiveness of the approach focusing on the new LUs to be included in the
existing frames.
References
[1] Collin F. Baker, Charles J. Fillmore, and John B. Lowe. The Berkeley
FrameNet Project. In Proceedings of the 36th ACL Meeting and 17th
ICCL Conference. Morgan Kaufmann, 1998.
[2] Diego De Cao, Danilo Croce, Marco Pennacchiotti, and Roberto Basili.
Combining Word Sense and Usage for modeling Frame Semantics. In
Proceedings of STEP 2008, Venice, Italy, 2008.
[3] Mario Crespo and Paul Buitelaar. Domain-specific English-to-Spanish
Translation of FrameNet. In Proc. of LREC 2008, Marrakech, 2008.
[4] Christiane Fellbaum, editor. WordNet: An Electronic Lexical Database.
MIT Press, 1998.
[5] R. Johansson and P. Nugues. Using WordNet to extend FrameNet cov-
erage. In Proc. of the Workshop on Building Frame-semantic Resources
for Scandinavian and Baltic Languages, at NODALIDA, Tartu, 2007.
345
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 157?161,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Feature Type Classification for Therapeutic Purposes: 
a preliminary evaluation with non-expert speakers 
 
 
Gianluca E. Lebani 
University of Trento 
gianluca.lebani@unitn.it 
Emanuele Pianta 
Fondazione Bruno Kessler 
pianta@fbk.eu 
 
  
 
Abstract 
We propose a feature type classification 
thought to be used in a therapeutic context. 
Such a scenario lays behind our need for a 
easily usable and cognitively plausible classi-
fication. Nevertheless, our proposal has both 
a practical and a theoretical outcome, and its 
applications range from computational lin-
guistics to psycholinguistics. An evaluation 
through inter-coder agreement has been per-
formed to highlight the strength of our pro-
posal and to conceive some improvements for 
the future. 
1 Introduction 
Most common therapeutic practices for anomia 
rehabilitation rely either on the therapist?s intui-
tive linguistic knowledge or on different kinds of 
resources that have to be consulted manually 
(Semenza, 1999; Raymer and Gonzalez-Rothi, 
2002; Springer, 2008). STaRS.sys (Semantic 
Task Rehabilitation Support system) is a tool 
thought for supporting the therapist in the prepa-
ration of a semantic task (cfr. Nickels, 2002).  
To be effective, such a system must lean on a 
knowledge base in which every concept is asso-
ciated with different kinds of featural descrip-
tions. The notion of feature refers to the linguis-
tic descriptions of a property that can be obtained 
by asking a subject to describe a concept. Exam-
ples of concept-feature pairings will be repre-
sented here as <concept> feature 1  couples 
such as <dog> has a tail or <dog> barks. 
                                                 
1
 Typographical conventions: concepts, categories and fea-
tures will be printed in italics courier new font. 
When reporting a concept-feature pair, the concept will be 
further enclosed by <angled brackets>. Feature types 
and classes of types will be both reported in times roman, 
but while the formers will be written in italics, type classes 
will be in SMALL CAPITALS. 
As a consequence of this scenario, an intuitive 
and cognitively plausible classification of the 
feature types that can be associated with a con-
cept is a vital component of our tool. In this pa-
per, we present a classification that meets such 
criteria, built by moving from an analysis of the 
relevant proposals available in the literature.  
We evaluated our classification by asking to a 
group of naive Italian speakers to annotate a test 
set by using our categories. The resulting agree-
ment has been interpreted both as an index of 
reliability and as a measure of ease of learning 
and use by non-expert speakers. In these prelimi-
nary phases we focus on Italian, leaving to future 
evaluations whether or how to extend the domain 
of our tool to other languages. 
These pages are organized as follows: in Sec-
tion 2 we briefly review the relevant works for 
the following discussion. In Section 3 we intro-
duce our classification and in the remaining part 
we evaluate its reliability and usability. 
2 Related Works 
2.1 Feature Norms 
In the psychological tradition, a collection of fea-
ture norms is typically built by asking to a group 
of speakers to generate short phrases (i.e. fea-
tures) to describe a given set of concepts.  
Even if normative data have been collected 
and employed for addressing a wide range of 
issues on the nature of the semantic memory, the 
only freely available resources are, to our know-
ledge, those by Garrard et al(2001), those by 
McRae et al(2005), those by Vinson and Vig-
liocco (2008), all in English, and the Dutch 
norms available in the Leuven database (De 
Deyne et al 2008). 
Moving out of the psychological domain, the 
only collection built in the lexicographic tradi-
tion is that by Kremer et al(2008), collected 
from Italian and German speakers 
157
2.2 Related Classifications 
The proposals that constitute our theoretical 
framework have been chosen for their being ei-
ther implemented in an extensive semantic re-
source, motivated by well specified theoretical 
explanations (on which there is consensus) or 
effectively used in a specific therapeutic context. 
They have originated in research fields as distant 
as lexicography, theoretical linguistics, ontology 
building, (clinical) neuropsychology and cogni-
tive psychology. Specifically, the works we 
moved from have been: 
? a type classification adopted for clinical pur-
poses in the CIMeC?s Center for Neurocogni-
tive Rehabilitation (personal communication); 
? the knowledge-type taxonomy proposed by 
Wu & Barsalou (2009), and the modified ver-
sion adopted by Cree & McRae (2003); 
? the brain region taxonomy proposed by Cree 
& McRae (2003); 
? the semantic (but not lexical) relations imple-
mented in WordNet 3.0 (Fellbaum, 1998) and 
in EuroWordNet (Alonge et al 1998); 
? the classification of part/whole relations by 
Winston et al(1987); 
? the SIMPLE-PAROLE-CLIPS Extended Qua-
lia Structures (Ruimy et al 2002). 
3 STaRS.sys feature types classification 
The properties of our classification follow from 
the practical use scenario of STaRS.sys. In de-
tails, the fact that it?s thought to be used in a the-
rapeutic context motivates our need for a classi-
fication that has to be: (1) intuitive enough to be 
easily used by therapist and (2) robust and (3) 
cognitively plausible so as to be used for prepar-
ing the relevant kinds of therapeutic tasks.  
Furthermore, being focused on features pro-
duced by human speakers, the classification ap-
plies to the linguistic description of a property, 
rather than to the property itself. Accordingly, 
then, pairings like the following: 
<plane> carries stuff 
<plane> is used for carrying stuff 
are though as instances of different types (re-
spectively, is involved in and is used for).  
Starting from an analysis of the relevant pro-
posals available in the literature, we identified a 
set of 26 feature types, most of which have been 
organized into the following six classes: 
TAXONOMIC PROPERTIES: Two types related 
to the belonging of a concept to a category have 
been isolated: the is-a and the coordinate types. 
PART-OF RELATIONS: We mainly followed 
Winston et als (1987) taxonomy in distinguish-
ing six types describing a relation between a 
concept and its part(s): has component, has 
member, has portion, made-of, has phase and 
has geographical part. 
PERCEPTUAL PROPERTIES: Inspired by the 
Cree and McRae?s (2003) brain region taxono-
my, we isolated six types of perceivable proper-
ties: has size, has shape, has texture, has taste, 
has smell, has sound, has colour. 
USAGE PROPERTIES: This class is composed 
by three types of an object?s use descriptions: is 
used for, is used by and is used with. 
LOCATIONAL PROPERTIES: We identified 
three types describing the typical situation, space 
and time associated to an object. 
ASSOCIATED EVENTS AND ATTRIBUTES: This 
class encompasses three kinds of information 
that can be associated to an object: its emotive 
property (has affective property), one of its per-
manent properties (has attribute) and the role it 
plays in an action or in a process (is involved in). 
As a matter of fact, each of the other classes is a 
specification of one of the two latter types, to 
which particular relevance has been accorded 
due to their status from a cognitive point of view.  
Others: Two feature types fall out of this 
classification, and constitute two distinct classes 
on their own. These are the has domain type, that 
specifies the semantic field of a concept, and the 
dummy is associated with, used for classifying 
all those features that falls out of any other label. 
Comparison and final remarks: A quick 
comparison between our types and the other 
classifications reveals that, apart from the is used 
with type, we didn?t introduce any new opposi-
tion. Any type of ours, indeed, has a parallel type 
or relation in at least one of the other proposals. 
Such a remark shows what is the third major ad-
vantage of our classification, together with its 
usability and its cognitive plausibility: its compa-
tibility with a wide range of well known theoreti-
cal and experimental frameworks, that allows it 
to serve as a common ground for the interplay of 
theories, insights and ideas originated from the 
above mentioned research areas. 
4 Evaluation 
Given the aims of our classification, and of 
STaRS.sys in general, we choose to evaluate our 
coding scheme by asking to a group of non ex-
perts to label a subset of the non-normalized 
Kremer et als (2008) norms and measuring the 
158
inter-coder agreement between them (Artstein 
and Poesio, 2008), adhering to the Krippen-
dorff?s (2004, 2008) recommendations.  
The choice to recruit only naive subjects has 
the positive consequence of allowing us to draw 
inferences also on the usability of our proposal. 
That is, such an evaluation can be additionally 
seen as a measure of how easily a minimally 
trained user can understand the oppositions iso-
lated in our classification. 
4.1 Experimental Setup 
Participants: 5 Italian speakers with a university 
degree were recruited for this evaluation. None 
of them had any previous experience in lexico-
graphy, nor any education in lexical semantics. 
Materials: 300 concept-feature pairs were se-
lected mainly from a non-normalized version of 
the Kremer et als (2008) norms. We choose this 
dataset because (1) it?s a collection of descrip-
tions generated by Italian speakers and (2) we 
wanted to avoid any bias due to a normalization 
procedure, so as to provide our subjects with de-
scriptions that were as plausible as possible. 
The experimental concept-attribute pairs have 
been chosen so to have the more balanced distri-
bution of concepts and feature types as possible, 
by not allowing duplicated pairs. As for the con-
cepts, an uniform distribution of features per cat-
egory (30 feature for all the ten categories of the 
original dataset) and of features per concept (i.e. 
between 4 and 7) has been easily obtained.  
The attempt to balance feature types, however, 
has revealed impracticable, mainly due to the 
nature of the concepts of the Kremer?s collection 
and to the skewness of its type distribution. 
Therefore, we fixed an arbitrary minimum thre-
shold of ten plausible features per type. Plausible 
features have been obtained from a pilot annota-
tion experiment performed by one author and an 
additional subject. We further translated 23 con-
cept-feature pairs from the McRae (11 cases) and 
from the Leuven (12 cases) datasets for balanc-
ing types as much as possible. 
Still, it has not been possible to find ten fea-
tures for the following types: has Geographical 
Part, has Phase and has Member (no features at 
all: this is a consequence of the kind of concept 
represented the dataset), has Portion (only four 
cases, again, this is a consequence of the source 
dataset), has Domain (5) and has Sound (6). We 
nevertheless decided to include these types in the 
instructions and the relevant features in the test 
set. Our decision has been motivated by the re-
sults of the pilot experiment, in which the sub-
jects made reference to such types as a secondary 
interpretation in more than ten cases. 
Procedure: The participants were asked to la-
bel every concept-feature pair with the appropri-
ate type label, relying primarily on the linguistic 
form of the feature. They received a 17-pages 
booklet providing an explanation of the annota-
tion goals, a definition and some examples for 
every type class and for every type, a decision 
flowchart and a reference table.  
Every participant was asked to read carefully 
the instructions, to complete a training set of 30 
concept-feature pairs and to discuss his/her deci-
sions with one of the two authors before starting 
the experimental session. The test set was pre-
sented as a unique excel sheet. On the average, 
labeling the 300 experimental pairs took 2 hours.  
4.2 Results 
The annotations collected from the participants 
have been normalized by conflating direct (e.g. 
is-a) and reverse (e.g. is the Category of) relation 
labels, and the agreement between their choice 
has been measured adopting Fleiss? Kappa. The 
?Kappa: annotators? column of Table 1 reports 
the general and the type-wise kappa scores2 for 
the annotations of the participants. 
 
Feature Type Kappa:  
annotators 
Kappa: 
gold/majority 
is-a 0.900 0.956 
coordination 0.788 0.913 
has component 0.786 0.864 
has portion 0.558 0.747 
made of 0.918 0.955 
has size 0.912 1 
has shape 0.812 1 
has texture 0.456 0.793 
has taste 0.852 1 
has smell 0.865 1 
has sound 0.582 0.795 
has colour 0.958 1 
is used for 0.831 0.727 
is used by 0.964 1 
is used with 0.801 0.939 
situation located 0.578 0.854 
space located 0.808 0.898 
time located 0.910 0.946 
is involved in 0.406 0.721 
has attribute 0.460 0.746 
has affective property 0.448 0.855 
has domain 0.069 0.277 
is associated with 0.141 0.415 
General 0.73 0.866 
 
Table 1: Type-wise agreement values 
 
                                                 
2
 All reported Kappa values are associated with p < 0.001. 
159
Even if there is no consensus on how to interp-
ret Kappa values in isolation, and despite the fact 
that, to our knowledge, this is the first work of 
this kind, we can nevertheless draw interesting 
conclusions from the pattern in table 1. The gen-
eral Kappa score has a value of 0.73, and the 
agreement values are above 0.8 for 12 types, not 
so distant in 2 cases, and well above 0.67 for 9 
types, 5 of which are our ?residual? categories, 
that is, those that are more ?general? that at least 
one of the other types3. 
Such a contrast between the residual and the 
other types is even more pronounced in the class-
wise analysis, where the only Kappa value below 
the 0.8 threshold is the one obtained for the AS-
SOCIATED EVENTS AND ATTRIBUTES class (? = 
0.766) 4 . Furthermore, the distribution of false 
positives in a confusion matrix between the per-
formance of the annotators and the ?majority? 
vote5 shows that part of the low agreement for 
the residual types is due to the ?summation? of 
the disagreement on the other categories. Ob-
viously, part of this variance is due also to the 
fact that such types have fuzzier boundaries, and 
so are more difficult to handle. 
As for the remaining four low agreement 
types, two of them (has affective property, has 
domain) have been signaled by the annotators to 
be difficult to handle, while the remaining two 
(has sound, has portion) have been frequently 
confused with one of the ASSOCIATED EVENTS 
AND ATTRIBUTES types and with the has compo-
nent type, respectively. Such results are not very 
puzzling for the has domain and has portion 
types, given the technicality of the former and, 
for the latter, the nature of the described con-
cepts. They do point, however, to a better defini-
tion of the remaining two types, the has sound 
and has affective property ones, in that most dif-
ficulties seem to arise from an unclear definition 
of their respective scopes. 
As pointed out by Artstein and Poesio (2008), 
agreement doesn?t ensure validity. In trying to 
evaluate how our annotators ?did it right?, we 
measured the exact Kappa coefficient (Conger, 
1980) between the majority annotation (i.e. what 
annotators should have done to agree) and the 
annotation of the same set by one of the two au-
                                                 
3
 Our residual labels are has Attribute, has Texture, is Asso-
ciated with, is Involved in and Situation Located. 
4
 The general Fleiss? Kappa value for the class-wise com-
parison is 0.766. 
5
 That is, the performance obtained by assigning the label 
chosen by the majority of the annotators. 
thors. With some approximation, we see this last 
performance as the ?right? one. 
Results are reported in the ?Kappa: gold / ma-
jority? column of Table 1. The general Kappa 
value is well above 0.8, and so it is for 15 of the 
23 types. Only two types (has domain and is as-
sociated with) are below the 0.67 minimal thre-
shold. These data further confirm the difficulties 
in handling residual types, but, more importantly, 
seem to suggest that our ?gold standard? annota-
tor have been able to learn the classification in a 
fairly correct way (at least, it did in a way similar 
as one of the two authors of this classification). 
4.3 Discussion 
We interpret the results of our evaluation as a 
demonstration of the reliability of our coding 
scheme as well as of the usability of our classifi-
cation, at least as the non residual types are con-
cerned. For the future, many improvements are 
suggested by our data. In particular, they showed 
the need of the annotators to receive a better 
training on some relations and distinctions.  
This points in the direction of both a more 
deep training on the types we?ve dubbed as ?re-
siduals?, and of a better definition of poorly un-
derstood types such as has domain and has affec-
tive property and puzzling distinctions such as 
the has smell/is Involved in ones. 
5 Conclusions and Future Directions 
In this paper we introduced a classification of the 
information types that can be expressed to de-
scribe a concrete concept. Even if we thought 
this classification mainly for therapeutic purpos-
es, its use can be broadened to include a wide 
range of possible NLP tasks.  
We evaluated our proposal by asking a group 
of naive speakers to annotate a list of concept-
feature pairs with the appropriate label. Even if 
our results can?t be interpreted as absolutely pos-
itive, we consider them promising, in that (1) the 
skeleton of the classification seems to have been 
validated by the performance of our participants 
and (2) a great part of the disagreement seems to 
be solvable through major care in the training 
phase. In the near future we are going to test our 
(improved) coding scheme with annotators from 
the population of the STaRS.sys final users, i.e. 
therapist with experience in semantic therapy. 
Finally, further research is needed to assess if 
and to what extent the semantic model underly-
ing our classification is compatible with those of 
existing lexical and/or semantic resources. 
160
Acknowledgments 
We are grateful to the annotators who gave us 
the data reported in Section 4 and to all the 
CLICers that commented our classification. In 
particular, we would like to thank dr. Federica 
Cavicchio for their statistical advice and Gerhard 
Kremer for providing us with a non-normalized 
version of his dataset.  
Reference 
Antonietta Alonge, Nicoletta Calzolari, Piek Vossen, 
Laura Bloksma, Irene Castellon, Maria A. Marti 
and Wim Peters. 1998. The linguistic design of the 
EuroWordNet database. Computer and the Human-
ities, 32: 91-115. 
Ron Artstein and Massimo Poesio. 2008. Inter-coder 
agreement for computational linguistics. Computa-
tional Linguistics, 34 (4): 555-596. 
Anthony J. Conger. 1980. Integration and generalisa-
tion of Kappas for multiple raters. Psychological 
Bulletin, 88: 322-328. 
George S. Cree and Ken MCrae. 2003. Analyzing the 
factors underlying the structure and computation of 
the meaning of chipmunk, cherry, chisel, cheese, 
and cello (and many other such concrete nouns). 
Journal of Experimental Psychology: General, 132 
(2): 163-201 
Simon De Deyne, Steven Verheyen, Eef Amel, Wolf 
Vanpaemel, Matthew J. Dry, Wouter Voorspoels 
and Gert Storm. 2008. Exemplar by feature appli-
cability matrices and other Dutch normative data 
for semantic concepts. Behavior Research Me-
thods, 40 (4): 1030-1048. 
Christiane Fellbaum. 1998. WordNet. An electronic 
lexical database. The MIT Press. Cambridge, MA. 
Joseph L. Fleiss. 1971. Measuring nominal scale 
agreement among many raters. Psychological Bul-
letin, 76 (5): 378-382. 
Peter Garrard, Matthew A. Lambon Ralph, John R. 
Hodges and Karalyn Patterson. 2001. Prototypi-
cality, distinctiveness and intercorrelation: analyses 
of the semantic attributes of living and nonliving 
concepts. Cognitive Neuropsychology, 18 (2): 125-
174. 
Gerhard Kremer, Andrea Abel and Marco Baroni. 
2008. Cognitively salient relations for multilingual 
lexicography. Proceedings of COLING-CogALex 
Workshop 2008: 94-101. 
Klaus Krippendorff. 2004. Reliability in content anal-
ysis: some common misconceptions and recom-
mendations. Human Communication Research, 30 
(3): 411-433. 
Klaus Krippendorff. 2008. Testing the reliability of 
content analysis data: what is involved and why. In 
K. Krippendorff and M.A. Bock (eds.). The Con-
tent Analysis Reader. Sage, Thousand Oaks, CA: 
350-357. 
Ken McRae, George S. Cree, Mark S. Seidenberg and 
Chris McNorgan. 2005. Semantic feature produc-
tion norms for a large set of living and nonliving 
things. Behavior Research Methods, Instruments & 
Computers, 37 (4): 547-559. 
Gregory L. Murphy. 2002. The big book of concepts. 
The MIT Press, Cambridge, MA. 
Lyndsey Nickels.2002. Therapy for naming disorders: 
revisiting, revising, and reviewing. Aphasiology, 
16 (10/11): 935-979 
Anastasia M. Raymer and Leslie J. Gonzalez-Rothi. 
2002. Clinical diagnosis and treatment of naming 
disorders. In A.E. Hillis (ed.). Handbook of Adult 
Language Disorders. Psychology Press: 163-182. 
Eleanor Rosch and Carolyn B. Mervis. 1975. Family 
resemblances: studies in the internal structure of 
categories. Cognitive Psychology, 7: 573-605. 
Nilda Ruimy, Monica Monachini, Raffaella Distante, 
Elisabetta Guazzini, Stefano Molino, Marisa Uli-
vieri, Nicoletta Calzolari and Antonio Zampolli. 
2002. Clips, a multi-level Italian computational le-
xicon: a glimpse to data. Proceedings LREC 2002: 
792-799. 
Carlo Semenza. 1999. Lexical-semantic disorders in 
aphasia. In G. Denes and L. Pizzamiglio (eds.). 
Handbook of clinical and experimental neuropsy-
chology. Psychology Press, Hove: 215-244. 
Luise Springer. 2008. Therapeutic approaches in 
aphasia rehabilitation. In B. Stemmer and H. Whi-
taker (eds.) Handbook of the Neuroscience of Lan-
guage. Elsevier Science, : 397-406. 
David P. Vinson and Gabriella Vigliocco. 2008. Se-
mantic feature production norms for a large set of 
objects and events. Behavior Research Methods, 40 
(1): 183-190. 
Morton E. Winston, Roger Chaffin and Douglas 
Herrman. 1987. A taxonomy of part-whole rela-
tion. Cognitive Science, 11:417-444. 
Ling-ling Wu and Lawrence W. Barsalou. 2009. Per-
ceptual Simulation in conceptual combination: evi-
dence from property generation. Acta Psychologi-
ca, 132: 173-189. 
161
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 12?17,
Beijing, August 2010
Exploiting Lexical Resources for Therapeutic Purposes: the 
case of WordNet and STaRS.sys 
Gianluca E. Lebani?? 
?Center for Mind/Brain Sciences 
University of Trento 
gianluca.lebani@unitn.it 
Emanuele Pianta? 
?HLT Group 
Fondazione Bruno Kessler 
pianta@fbk.eu 
 
Abstract 
In this paper, we present an on-going 
project aiming at extending the WordNet 
lexical database by encoding common 
sense featural knowledge elicited from 
language speakers. Such extension of 
WordNet is required in the framework of 
the STaRS.sys project, which has the 
goal of building tools for supporting the 
speech therapist during the preparation of 
exercises to be submitted to aphasic pa-
tients for rehabilitation purposes. We re-
view some preliminary results and illu-
strate what extensions of the existing 
WordNet model are needed to accommo-
date for the encoding of commonsense 
(featural) knowledge. 
1 Introduction 
Electronic lexical resources such as WordNet 
and FrameNet are used for a great variety of nat-
ural processing tasks, ranging from query expan-
sion, to word sense disambiguation, text classifi-
cation, or textual entailment. Some of these re-
sources are also used by human users as on-line 
dictionaries; see the Princeton WordNet1 and the 
MultiWordNet 2  on-line sites. In this paper we 
describe a novel attempt to exploit the informa-
tion contained in wordnets to build a tool de-
signed to support the therapy of language disord-
ers. In doing so, we will tackle also an interesting 
theoretical issue. Is the WordNet conceptual 
model apt to represent the common sense know-
ledge associated to concepts, which is partly lost 
in case of language disorders (aphasia) due to a 
                                                 
1
 http://wordnet.princeton.edu/ 
2
 http://multiwordnet.fbk.eu/ 
brain damage? Note that, in cognitively oriented 
studies of the lexicon such knowledge is often 
represented in the form of featural descriptions 
elicited from speakers, such as <a cat> is 
lazy 3 , <camels> are found in deserts, 
<planes> fly etc. 
Anomia is the most pervasive and persistent of 
aphasia symptoms. It has been described as ?a 
difficulty in finding high information words, 
both in fluent discourse and when called upon to 
identify an object of action by name? (Goodglass 
and Wingfield, 1997:3). The naming difficulties 
experienced by anomic patients can vary sub-
stantially, so that different ?anomias? can be cha-
racterized as arising from either a mainly lexical 
or mainly semantic breakdown. Depending on 
the kind of anomia, therapeutic approaches can 
vary, so as to employ the more appropriate tasks 
and stimuli. 
Computers can support the rehabilitation of 
language disorders in many ways: from assisting 
the administrative management to enhancing 
common assessment methods, from helping the 
clinician during the therapeutic session to alle-
viating the communicative difficulties of a pa-
tient by exploiting his unimpaired abilities (Pe-
theram, 2004). 
In these pages we introduce STaRS.sys (Se-
mantic Task Rehabilitation Support system), a 
Computer Assisted Therapy (CAT) tool designed 
to support the therapist in the preparation of se-
mantic exercises such as odd-one-out, yes/no 
attribute question answering, property generation 
and so forth. All these exercises are based on the 
kinds of information that are carried by featural 
                                                 
3
 Concepts and features will be printed in italics couri-
er new font. When reporting a concept-feature pair, the 
concept will be further enclosed by <angled brackets>. 
Feature types and concept categories will be reported in 
italics times new roman. 
12
descriptions. Such a scenario motivates the need 
for a lexical semantic resource which is richer 
and somehow more cognitively-oriented than the 
existing ones. We will argue that such needs can 
be satisfied by enhancing the WordNet model 
(WN: Fellbaum, 1998 ed) as implemented in the 
Italian MultiWordNet (MWN: Pianta et al 2002) 
lexicon. Our project is developed in collabora-
tion with the CIMeC?s Center for Neuropsycho-
logical Rehabilitation (CeRiN), and focuses on 
Italian. We leave to the future the evaluation of 
whether and how our model can be expanded to 
other languages. 
These pages are organized as follows: Sec. 2 
shows the possibilities offered by the exploita-
tion of STaRS.sys in a therapeutic context, and 
the lexical semantics requirements that such use 
poses. In Sec. 3 and 4 we illustrate specific is-
sues related to the encoding of featural know-
ledge into the MWN model. 
2 STaRS.sys in a therapeutic context 
In this section we will illustrate the semantic re-
quirements that the therapeutic use of STaRS.sys 
poses, and how we foresee the tool will be used 
in practical therapeutic scenarios. 
2.1 Semantic requirements 
An essential requirement of the STaRS.sys tool 
is the capability of managing the major variables 
that influence the performance of anomic pa-
tients in semantic therapeutic tasks (Raymer and 
Gonzalez-Rothi, 2002; Cree and McRae, 2003). 
Accordingly, we identified a minimum of five 
types of information which should be available 
for every lexical concept:  
Conceptual Taxonomy. A fully-specified 
conceptual taxonomy is an essential requirement 
for our tool, in the light of the existence of pa-
tients affected by language disorders specific to 
certain semantic categories, such as tools, or liv-
ing beings (Capitani et al 2003).  
Featural Descriptions. Featural descriptions 
are assumed to play a central role in the human 
semantic memory (Murphy, 2002) and will be 
represented here as <concept> feature 
couples, e.g. <dog> has a tail. 
This information can be exploited for selecting 
sets of concepts which are relevant in a certain 
therapeutic context, e.g. concepts sharing a fea-
ture value (?red objects?) or those for which a 
type of feature is particularly relevant (e.g. ?ani-
mals with a peculiar fur?).  
Feature Types Classification. A grouping of 
FDs into feature types is needed for selectively 
working on feature types of interest, or for the 
estimation of semantic measures such as feature 
distinctiveness, semantic relevance, concept si-
milarity and feature correlation (Cree and 
McRae, 2003; Sartori and Lombardi, 2004; Vin-
son et al 2003). As we will see in the following 
sections, feature types can be mapped onto 
WordNet-like relations. 
Prototypicality. A concept can be more or 
less representative of its category. Choosing and 
working on concepts with different levels of pro-
totypicality can be informative, for both thera-
peutic and diagnostic purposes. 
Word Frequency. Patients? performance can 
be affected by word frequency. Thereby, a criti-
cal skill for our tool is the ability to discriminate 
between words used with different frequencies. 
2.2 Use Case Scenarios 
By exploiting a lexical infrastructure encoding 
such semantic information, STaRS.sys can be 
used by a therapist for: 
? retrieving concepts; 
? retrieving information associated to concepts; 
? comparing concepts. 
These three functionalities can be illustrated 
by the preparation of three different tasks for a 
patient affected by, e.g., a semantic deficit selec-
tively affecting animal concepts. Such a kind of 
patient would show comprehension and produc-
tion difficulties restricted to concepts belonging 
to the animal category (Capitani et al 2003). 
Plausibly, furthermore, his production problems 
would manifest both as naming failure in con-
trolled conditions (i.e. in tests like the ones re-
ported below) and as a difficulty/inability to re-
trieve the intended word in spontaneous speech 
(Semenza, 1999).  
In the first scenario, the therapist looks for 
concepts that match given specifications in order 
to prepare a feature generation task. As an exam-
ple, she submits to STaRS.sys a request for con-
cepts of frequent use, referring to animals, asso-
ciated to highly distinctive color features and 
having a high mean feature distinctiveness. The 
system returns concepts such as zebra, tiger 
13
and cow. Finally the patient is asked to generate 
phrasal descriptions for these concepts. 
In a second scenario, STaRS.sys is used to re-
trieve FDs for a given set of concepts. Right and 
wrong concept-feature couples are created to 
build a questionnaire, in which the patient is re-
quired to distinguish the right from the wrong 
pairs. For instance, the therapist submits to 
STaRS.sys a query for features of the concept 
leopard that are highly relevant and either per-
ceptual or taxonomical, and obtains features such 
as is yellow with black spots and is a 
cat. 
Finally, in the third scenario the therapist uses 
STaRS.sys to find concepts for an odd-one-out 
task. That is, she looks for triples composed of 
two similar concepts plus an incoherent one that 
has to be found by the patient. As an example, 
starting from the concept lion, she looks for 
animals that typically live in a similar/different 
natural habitat, and obtains similar concepts such 
as leopard and cheetah, and a dissimilar con-
cept such as wolf. 
3 WN as semantic lexical resource for 
STaRS.sys 
The STaRS.sys application scenario motivates 
the need for a lexical semantic resources that: 
R1: is cognitively motivated; 
R2: is based on a fully-specified is-a hierarchy; 
R3: is intuitive enough to be used by a therapist; 
R4: allows for the encoding of featural properties 
and their association to concepts; 
While designing the STaRS.sys tool, we made 
the hypothesis that a semantic lexical resource 
built according to the WN model could meet 
most of the above requirements.  
In the WN model every concept is represented 
as a synset (set of synonyms) such as {hand, ma-
nus, hook, mauler, mitt, paw}. Such semantic 
units are organized in a network interconnected 
trough several relations. Examples of semantic 
relations include the is-a relation, e.g.{left_hand, 
left} is-a {hand, ?}, and the meronymy relation, 
e.g. {hand, ?} has-part {finger}. 
At a first glance, WN seems to easily meet 
three of the above criteria. First, WN was initial-
ly conceived as a model of the human lexical 
memory. Second, WordNet implement extensive 
and systematic noun hierarchies. More specifi-
cally, a preliminary analysis of the Italian MWN 
nominal hierarchy has shown that the semantic 
categories which are relevant for rehabilitation 
purposes can be easily mapped onto MWN top 
level nodes (tools, animals, people). Third, WN 
is based on a conceptual model which is relative-
ly simple and near to language use (as opposed 
to more sophisticated logics-based models). We 
expect that this feature will facilitate the use of 
STaRS.sys by therapists, which may not have all 
the formal logics awareness that is needed to use 
formal ontologies. Furthermore, MWN is ma-
nually developed trough an on-line Web applica-
tion. We expect that such application can be used 
by therapists using STaRS.sys for the shared and 
community-based development/maintenance of 
the lexical resource they need.  
A final motivation in favor of the choice of 
MWN is the fact that this Italian resource is 
strictly aligned at the semantic level to English 
and other European languages (e.g. Spanish, Por-
tuguese, Romania, Hebrew). Thus, we can envi-
sage that at least part of the semantic information 
which is encoded for Italian can be ported to the 
aligned languages and used for similar purposes. 
4 Mapping featural descriptions into 
MWN 
Our hypothesis about the usefulness of the WN 
model for the needs of STaRS.sys can be fully 
confirmed only if we find a way to encode in 
such a model all or most of the knowledge which 
is contained in feature descriptions elicited from 
Italian speakers (R4 in previous section). In more 
general terms we need to answer the following 
questions. Does MWN already contain all the 
information that is needed by the STaRS.sys re-
quirements? If we need to extend the existing 
MWN, can we simply add new synsets and in-
stances of existing relations, or do we need to 
add new relation types? Is the conceptual model 
of MWN or of any other WN variant powerful 
enough to encode all the information contained 
in feature descriptions?  
A first simple approach to representing fea-
ture descriptions in MWN is associating feature 
descriptions to synset glosses. As a consequence, 
a MWN gloss, which is currently composed of a 
definition and a list of usage examples, all 
crafted by lexicographers, would contain also a 
14
list of feature descriptions, elicited from lan-
guage speakers. 
This approach may be useful for some of the 
foreseen usages of STaRS.sys (e.g. retrieving 
feature descriptions from concepts), and can also 
be interesting for a generic use of MWN. How-
ever, to fully exploit the knowledge contained in 
FDs (e.g. for calculating concept similarity) it is 
necessary to encode that knowledge in a more 
explicit way; that is we need to map each FD in a 
wordnet-like relation between a source and a 
target concept. For instance, a pair such as <cup> 
is used for drinking can be represented as a 
is_used_for relation holding between the source 
concept {cup} and the target concept {drink}. 
Encoding the source concept is relatively easy 
given that it is usually expressed as an isolated 
word that is used as stimulus for feature elicita-
tion from subjects, e.g. ?scimmia? (?monkey?). 
The only problematic aspect in this step may be 
the choice of the right sense which was meant 
when the word has been proposed to subjects. In 
some cases this may be not trivial, even if, in 
principle, stimulus words are supposed to be 
chosen so as to avoid ambiguities; see for in-
stance the word ?cipolla? (?onion?), which in 
MWN is ambiguous between the vegetable and 
food sense. 
More complex is the encoding of the feature 
itself which is a free and possibly complex lin-
guistic description (e.g. likes eating bana-
nas). To fulfill our goal, we need to map such 
description in a wordnet-like relation and a target 
concept. Such goal can be accomplished in two 
steps. 
4.1 Mapping feature types into MWN rela-
tions 
Given the semantic requirements illustrated in 
Sec. 2.1, one the first steps in the development of 
the STaRS.sys tool has been the design of a clas-
sification of FDs in feature types; see Lebani and 
Pianta (2010). In a second moment, we realized 
that assigning a FD to a feature type is equivalent 
to assigning it to a wordnet relation, given that it 
is possible to create one-to-one mappings be-
tween features types and relations.  
The adopted feature type classification has 
been designed so as to be (1) reasonably intui-
tive, (2) robust and (3) cognitively plausible. The 
cognitive plausibility requirement has been ful-
filled by moving from an analysis of similar pro-
posals put forwards in the experimental litera-
ture, or exploited in the therapeutic practice. As 
for the former, we considered research fields as 
distant as lexicography, theoretical linguistics 
and cognitive psychology. Examples of compati-
ble proposals currently exploited in the therapeu-
tic practice are the question type of Laiacona et 
al?s (1993) semantic questionnaire, a type classi-
fication adopted by the therapists of the CIMeC?s 
CeRiN (personal communication) and the Se-
mantic Feature Analysis paradigm (Boyle and 
Coelho, 1995). 
The resulting classification only considers 
concrete objects and is composed of 25 feature 
types. All of them (except the is associated with 
relations) belong to one of the following six rela-
tions) belong to one of the following six major 
classes: taxonomic properties, part-of- relations, 
 
Feature Type Example 
has Portion <bread> cut into slices 
has Geographical Part <Africa> Egitto 
has Size <elephant> is big 
has Shape <clock> is round 
has Texture <eel> is slimy / <biscuit> is crunchy 
has Taste <lemon> is bitter 
has Smell <rose water> smells of rose 
has Sound <lighting> produces a thunder 
has Colour <lemon> is yellow 
is Used for <cup> is used for drinking 
is Used by <cleaver> is used by butchers 
is Used with <violin> is played with a bow 
Situation Located <jacket> used in occasions 
Space Located <camel> in the desert 
Time Located <pajamas> used at night 
has Origin <milk> comes from cows 
is Involved in <bird> eats seeds - is hunted 
has Attribute <subway> is fast 
has Affective Property <horror movie> is scary 
is Associated with <dog> man 
 
Table 1: STaRS.sys types not having a parallel word-
net semantic relation 
15
perceptual properties, usage properties, location-
al properties and associated events and attributes. 
A first version of this classification has been 
evaluated by asking 5 na?ve Italian speakers to 
assign the appropriate type label to 300 concept4-
feature pairs from a non-normalized version of 
the Kremer et als (2008) norms. The inter-coder 
agreement between subjects (Fleiss? Multi-pi = 
0,73) validated the skeleton of our classification, 
at the same time suggesting some minor changes 
that have been applied to the classification pro-
posed here. An evaluation of the improved clas-
sification involving therapists has been planned 
for the (very near) future.  
Note that in order to map all of the feature 
types into wordnet relations we had to create a 
number of new relations which are not available 
in existing wordnets. The list of existing MWN 
relations used to encode STaRS.sys feature types 
includes five items: hypernym, has_co-ordinate, 
has_part, has_member, has_substance. The fol-
lowing table contains the list of the 20 additional 
relations, along with examples. 
4.2 Encoding target concepts in MWN 
A second step needed in order to fully represent 
the semantics of feature descriptions in MWN is 
the encoding of target concepts.  
Target concepts can be expressed by a noun 
(e.g. has a <neck>), an adjective (e.g. is <big>) 
or a verb or a verbal construction (e.g. is used 
for <drinking>, is used to <cut bread>). 
In principle this is not problematic as WN en-
codes all these lexical categories. 
What is problematic instead is the possible 
complexity of target concepts. Whereas in WN 
synsets are bound to contain only lexical units 
(with the few exceptions of the so called artificial 
nodes), the target of a featural description can be 
a free combination of words, for instance a noun 
modified by an adjective (e.g. has a <long 
neck>), an adjective modified by an adverb (e.g. 
is <very big>) or a verb with an argument (e.g. 
is used to <cut bread>). For giving an idea of 
the phenomenon, consider that 27,6% of the fea-
tures that composes the experimental sample in 
                                                 
4
 In details, the subjects were submitted with concrete con-
cepts belonging to one of the following categories: mam-
mals, birds, fruits, vegetables, body parts, clothing, mani-
pulable tools, vehicles, furniture and buildings. 
Lebani and Pianta (2010) contain target concepts 
expressed by free combination of words  
The solution we adopted to solve this problem 
relies on the notion of phraset proposed by Ben-
tivogli and Pianta (2003; 2004), that is a data 
structure used for encoding ?sets of synonymous 
free combination of words (as opposed to lexical 
units) which are recurrently used to express a 
concept?. In the original proposal, the authors 
introduced such a data structure to cope with 
lexical gaps in multingual resources or to encode 
alternative (linguistically complex) ways of ex-
pressing an existing concept. Phrasets can be as-
sociated to existing synsets to represent alterna-
tive (non lexical) ways of expressing lexicalized 
concepts, e.g. the Italian translations of ?dish-
cloth?: 
Synset: {canovaccio, strofinaccio} 
Phraset: {strofinaccio_per_i_piatti,  
straccio_per_i_piatti} 
where ?strofinaccio per i piatti? and ?straccio per 
i piatti? and are free combinations of words. In 
alternative, they can be used to represent lexical 
gaps, such as the Italian translation equivalent of 
?breadknife?: 
Synset: {GAP} 
Phraset: {coltello_da_pane,  
coltello_per_il_pane} 
Phrasets can be annotated by exploiting the com-
poses/composed-of lexical relation linking phra-
set with the synsets corresponding to the con-
cepts that compose it. For instance the expression 
in the above phraset is linked by a hypernym and 
by a composed-of relation with the synset {col-
tello} (knife) and {pane} (bread). As far as 
FDs are concerned, the use of phrasets is com-
patible with the received view about the compo-
sitional nature of the human conceptual know-
ledge (Murphy, 2002).  
Figure 1 shows how phrasets allow for 
representing the complex FD <breadknife> is 
used to cut bread in the MWN model. 
5 Conclusion and future directions 
This paper presents the preliminary results of a 
research aiming at exploiting and extending the 
WordNet conceptual model as an essential com-
ponent of a tool for supporting the rehabilitation 
of patients with language disorders. A crucial  
 
16
  
Figure 1: Representation of the concept-feature pair 
<breadknife> is used to cut bread 
 
aspect for the use of wordnet-like resources in 
such a context is the possibility of representing 
lexical knowledge represented in the form of fea-
ture descriptions elicited from language speakers. 
Our work has illustrated the steps which are 
needed to encode feature descriptions in the WN 
model. To this purpose we introduced twenty 
new wordnet relations, and relied on phrasets for 
representing complex (non-lexicalized) concepts. 
The study presented in these pages is a neces-
sary theoretical step for the development of our 
tool. A practical evaluation of its feasibility is 
planned for the very near future, together with 
other (equally important but less relevant in this 
context) issues concerning both the population of 
our semantic knowledge base and the overall 
design of STaRS.sys. 
 
Acknowledgements. We are grateful to Rita 
Capasso and Alessia Monti for the useful discus-
sion of the application scenario sketched in these 
pages. 
References 
Luisa Bentivogli and Emanuele Pianta. 2003. Beyond 
Lexical Units: Enriching WordNets with Phrasets. 
Proceedings of EACL 2003: 67-70. 
Luisa Bentivogli and Emanuele Pianta. 2004. Extend-
ing WordNet with Syntagmatic Information. Pro-
ceedings of the 2nd International WordNet Confe-
rence: 47-53. 
Mary Boyle and Carl A. Coelho. 1995. Application of 
semantic feature analysis as a treatment for aphasia 
dysnomia. American Journal of Speech-Language 
Pathology, 4: 94-98. 
Erminio Capitani, Marcella Laiacona, Brad Z. Mahon 
and Alfonso Caramazza. 2003. What are the Facts 
of Semantic Category-Specific Deficits? A Criti-
cal Review of the Clinical Evidence. Cognitive 
Neuropsychology, 20(3): 213-261. 
George S. Cree and Ken McRae. 2003. Analyzing the 
Factors Underlying the Structure and Computa-
tion of the Meaning of Chipmunk, Cherry, Chisel, 
Cheese, and Cello (and Many Other Such Con-
crete Nouns). Journal of Experimental Psycholo-
gy: General, 132 (2): 163-201. 
Christiane Fellbaum. 1998 ed. WordNet: an electronic 
lexical database. The MIT Press.
Harold Goodglass and Arthur Wingfield. 1997. Ano-
mia: Neuroanatomical & Cognitive Correlates. 
Academic Press. 
Gerhard Kremer, Andrea Abel and Marco Baroni. 
2008. Cognitively salient relations for multilingual 
lexicography. Proceedings of COLING-CogALex 
Workshop 2008: 94-101. 
Marcella Laiacona, Riccardo Barbarotto, Cristina Tri-
velli and Erminio Capitani. 1993. Dissociazioni 
Semantiche Intercategoriali. Archivio di Psicologi-
a, Neurologia e Psichiatria, 54: 209-248. 
Gianluca E. Lebani and Emanuele Pianta. 2010. A 
Feature Type Classification for Therapeutic Pur-
poses: a preliminary evaluation with non expert 
speakers. Proceedings of ACL-LAW IV Workshop. 
Gregory L. Murphy. 2002. The big book of concepts. 
The MIT Press, Cambridge, MA. 
Brian Petheram. 2004, ed. Special Issue on Computers 
and Aphasia. Aphasiology, 18 (3): 187-282. 
Emanuele Pianta, Luisa Bentivogli and Christian Gi-
rardi. 2002. MultiWordNet: developing an aligned 
multilingual database. Proceedings of the 1st Inter-
national Conference on Global WordNet. 
Anastasia Raymer and Leslie Gonzalez-Rothi. 2002. 
Clinical Diagnosis and Treatment of Naming Dis-
orders. In A.E. Hillis (ed) The Handbook of Adult 
Language Disorders. Psychology Press: 163-182. 
Giuseppe Sartori and Luigi Lombardi. 2004. Semantic 
Relevance and Semantic Disorders. Journal of 
Cognitive Neuroscience, 16 (3): 439-452. 
Carlo Semenza. 1999. Lexical-semantic disorders in 
aphasia. In G. Denes and L. Pizzamiglio (eds.). 
Handbook of Clinical and Experimental Neuropsy-
chology. Psychology Press, Hove: 215-244. 
David P. Vinson, Gabriella Vigliocco, Stefano Cappa 
and Simona Siri. 2003. The Breakdown of Seman-
tic Knowledge: Insights from a Statistical Model of 
Meaning Representation. Brain and Language, 86: 
347-365. 
17
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 19?27,
Beijing, August 2010
Extending English ACE 2005 Corpus Annotation with Ground-truth
Links to Wikipedia
Luisa Bentivogli
FBK-Irst
bentivo@fbk.eu
Pamela Forner
CELCT
forner@celct.it
Claudio Giuliano
FBK-Irst
giuliano@fbk.eu
Alessandro Marchetti
CELCT
amarchetti@celct.it
Emanuele Pianta
FBK-Irst
pianta@fbk.eu
Kateryna Tymoshenko
FBK-Irst
tymoshenko@fbk.eu
Abstract
This paper describes an on-going annota-
tion effort which aims at adding a man-
ual annotation layer connecting an exist-
ing annotated corpus such as the English
ACE-2005 Corpus to Wikipedia. The an-
notation layer is intended for the evalua-
tion of accuracy of linking to Wikipedia in
the framework of a coreference resolution
system.
1 Introduction
Collaboratively Constructed Resources (CCR)
such as Wikipedia are starting to be used for a
number of semantic processing tasks that up to
few years ago could only rely on few manually
constructed resources such as WordNet and Sem-
Cor (Fellbaum, 1998). The impact of the new re-
sources can be multiplied by connecting them to
other existing datasets, e.g. reference corpora. In
this paper we will illustrate an on-going annota-
tion effort which aims at adding a manual anno-
tation layer connecting an existing annotated cor-
pus such as the English ACE-2005 dataset1 to a
CCR such as Wikipedia. This effort will produce
a new integrated resource which can be useful for
the coreference resolution task.
Coreference resolution is the task of identify-
ing which mentions, i.e. individual textual de-
scriptions usually realized as noun phrases or pro-
nouns, refer to the same entity. To solve this
task, especially in the case of non-pronominal co-
reference, researchers have recently started to ex-
ploit semantic knowledge, e.g. trying to calculate
1http://projects.ldc.upenn.edu/ace/
the semantic similarity of mentions (Ponzetto and
Strube, 2006) or their semantic classes (Ng, 2007;
Soon et al, 2001). Up to now, WordNet has been
one of the most frequently used sources of se-
mantic knowledge for the coreference resolution
task (Soon et al, 2001; Ng and Cardie, 2002). Re-
searchers have shown, however, that WordNet has
some limits. On one hand, although WordNet has
a big coverage of the English language in terms
of common nouns, it still has a limited coverage
of proper nouns (e.g. Barack Obama is not avail-
able in the on-line version) and entity descrip-
tions (e.g. president of India). On the other hand
WordNet sense inventory is considered too fine-
grained (Ponzetto and Strube, 2006; Mihalcea and
Moldovan, 2001). In alternative, it has been re-
cently shown that Wikipedia can be a promising
source of semantic knowledge for coreference res-
olution between nominals (Ponzetto and Strube,
2006).
Consider some possible uses of Wikipedia.
For example, knowing that the entity men-
tion ?Obama? is described on the Wikipedia
page Barack_Obama2, one can benefit from
the Wikipedia category structure. Categories as-
signed to the Barack_Obama page can be used
as semantic classes, e.g. ?21st-century presidents
of the United States?. Another example of a
useful Wikipedia feature are the links between
Wikipedia pages. For instance, some Wikipedia
pages contain links to the Barack_Obama page.
Anchor texts of these links can provide alterna-
2The links to Wikipedia pages are given displaying only
the last part of the link which corresponds to the title of the
page. The complete link can be obtained adding this part to
http://en.wikipedia.org/wiki/.
19
tive names of this entity, e.g. ?Barack Hussein
Obama? or ?Barack Obama Junior?.
Naturally, in order to obtain semantic knowl-
edge about an entity mention from Wikipedia
one should link this mention to an appropriate
Wikipedia page, i.e. to disambiguate it using
Wikipedia as a sense inventory. The accuracy
of linking entity mentions to Wikipedia is a very
important issue. For example, such linking is a
step of the approach to coreference resolution de-
scribed in (Bryl et al, 2010). In order to evaluate
this accuracy in the framework of a coreference
resolution system, a corpus of documents, where
entity mentions are annotated with ground-truth
links to Wikipedia, is required.
The possible solution of this problem is to ex-
tend the annotation of entity mentions in a corefer-
ence resolution corpus. In the recent years, coref-
erence resolution systems have been evaluated on
various versions of the English Automatic Content
Extraction (ACE) corpus (Ponzetto and Strube,
2006; Versley et al, 2008; Ng, 2007; Culotta et
al., 2007; Bryl et al, 2010). The latest publicly
available version is ACE 20053.
In this paper we present an extension of ACE
2005 non-pronominal entity mention annotations
with ground-truth links to Wikipedia. This exten-
sion is intended for evaluation of accuracy of link-
ing entity mentions to Wikipedia pages. The an-
notation is currently in progress. At the moment
of writing this paper we have completed around
55% of the work. The extension can be exploited
by coreference resolution systems, which already
use ACE 2005 corpus for development and testing
purposes, e.g. (Bryl et al, 2010). Moreover, En-
glish ACE 2005 corpus is multi-purpose and can
be used in other information extraction (IE) tasks
as well, e.g. relation extraction. Therefore, we
believe that our extension might also be useful for
other IE tasks, which exploit semantic knowledge.
In the following we start by providing a brief
overview of the existing corpora annotated with
links to Wikipedia. In Section 3 we describe some
characteristics of the English ACE 2005 corpus,
which are relevant to the creation of the extension.
Next, we describe the general annotation princi-
3http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
ples and the procedure adopted to carry out the
annotation. In Section 4 we present some anal-
yses of the annotation and statistics about Inter-
Annotator Agreement.
2 Related work
Recent approaches to linking terms to Wikipedia
pages (Cucerzan, 2007; Csomai and Mihalcea,
2008; Milne and Witten, 2008; Kulkarni et al,
2009) have used two kinds of corpora for eval-
uation of accuracy: (i) sets of Wikipedia pages
and (ii) manually annotated corpora. In Wikipedia
pages links are added to terms ?only where
they are relevant to the context?4. Therefore,
Wikipedia pages do not contain the full annotation
of all entity mentions. This observation applies
equally to the corpus used by (Milne and Wit-
ten, 2008), which includes 50 documents from the
AQUAINT corpus annotated following the same
strategy5. The corpus created by (Cucerzan, 2007)
contains annotation of named entities only6. It
contains 756 annotations, therefore for our pur-
poses it is limited in terms of size.
Kulkarni et al (2009) have annotated 109 doc-
uments collected from homepages of various sites
with as many links as possible7. Their annotation
is too extensive for our purposes, since they do not
limit annotation to the entity mentions. To tackle
this issue, one can use an automatic entity mention
detector, however it is likely to introduce noise.
3 Creating the extension
The task consists of manually annotating the
non-pronominal mentions contained in the En-
glish ACE 2005 corpus with links to appropriate
Wikipedia articles. The objective of the work is
to create an extension of ACE 2005, where all the
mentions contained in the ACE 2005 corpus are
disambiguated using Wikipedia as a sense reposi-
tory to point to. The extension is intended for the
4http://en.wikipedia.org/wiki/
Wikipedia:Manual_of_Style
5http://www.nzdl.org/wikification/
docs.html
6http://research.microsoft.com/en-us/
um/people/silviu/WebAssistant/TestData/
7http://soumen.cse.iitb.ac.in/?soumen/
doc/CSAW/
20
evaluation of accuracy of linking to Wikipedia in
the framework of a coreference resolution system.
3.1 The English ACE 2005 Corpus
The English ACE 2005 corpus is composed of
599 articles assembled from a variety of sources
selected from broadcast news programs, newspa-
pers, newswire reports, internet sources and from
transcribed audio. It contains the annotation of a
series of entities (person, location, organization)
for a total of 15,382 different entities and 43,624
mentions of these entities. A mention is an in-
stance of a textual reference to an object, which
can be either named (e.g. Barack Obama), nom-
inal (e.g. the president), or pronominal (e.g. he,
his, it). An entity is an aggregate of all the men-
tions which refer to one conceptual entity. Beyond
the annotation of entities and mentions, ACE 05
contains also the annotation of local co-reference
for the entities; this means that mentions which
refer to the same entity in a document have been
marked with the same ID.
3.2 Annotating ACE 05 with Wikipedia
Pages
For the purpose of our task, not all the
ACE 05 mentions are annotated, but only the
named (henceforth NAM) and nominal (hence-
forth NOM) mentions. The resulting additional
annotation layer will contain a total of 29,300
mentions linked to Wikipedia pages. As specif-
ically regards the annotation of NAM mentions,
information about local coreference contained in
ACE 05 has been exploited in order to speed up
the annotation process. In fact, only the first
occurrence of the NAM mentions in each doc-
ument has been annotated and the annotation is
then propagated to all the other co-referring NAM
mentions in the document.
Finally, it must be noted that in ACE 05, given
a complex entity description, both the full ex-
tent of the mention (e.g. president of the United
States) and its syntactic head (e.g. ?president?)
are marked. In our Wikipedia extension only the
head of the mention is annotated, while the full ex-
tent of the mention is available from the original
ACE 05 corpus.
3.3 General Annotation Principles
Depending on the mention type to be annotated,
i.e. NAM or NOM, a different annotation strategy
has been followed. Each mention of type NAM
is annotated with a link to a Wikipedia page de-
scribing the referred entity. For instance, ?George
Bush? is annotated with a link to the Wikipedia
page George_W._Bush.
NOM mentions are annotated with a link to the
Wikipedia page which provides a description of
its appropriate sense. For instance, in the exam-
ple ?I was driving Northwest of Baghdad and I
bumped into these guys going around the capi-
tal? the mention ?capital? is linked to the page
which provides a description of its meaning, i.e.
Capital_(political). Note that the object
of linking is the textual description of an entity,
and not the entity itself. In the example, even
though from the context it is clear that the mention
?capital? refers to Baghdad, we provide a link to
the concept of capital and not to the entity Bagdad.
As a term can have both a more generic sense
and a more specific one, depending on the context
in which it occurs, mentions of type NOM can of-
ten be linked to more than one Wikipedia page.
Whenever possible, the NOM mentions are anno-
tated with a list of links to appropriate Wikipedia
pages in the given context. In such cases, links
are sorted in order of relevance, where the first
link corresponds to the most specific sense for that
term in its context, and therefore is regarded as the
best choice. For instance, for the NOM mention
head ?President? which in the context identifies
the United States President George Bush the an-
notation?s purpose is to provide a description of
the item ?President?, so the following links are
selected as appropriate: President_of_the_
United_States and President.
The correct interpretation of the term is strictly
related to the context in which the term occurs.
While performing the annotation, the context of
the entire document has always been exploited in
order to correctly identify the specific sense of the
mention.
3.4 Annotation Procedure
The annotation procedure requires that the men-
tion string is searched in Wikipedia in order to
21
find the appropriate page(s) to be used for anno-
tating the mention. In the annotation exercise, the
annotators have always taken into consideration
the context where a mention occurs, searching for
both the generic and the most specific sense of the
mention disambiguated in the context. In fact, in
the example provided above, not only ?President?,
but also ?President of the United States? has been
queried in Wikipedia as required by the context.
Not only the context, but also some features of
Wikipedia must be mentioned as they affect the
annotation procedure:
a. One element which contributes to the choice
of the appropriate Wikipedia page(s) for
one mention is the list of links proposed in
Wikipedia?s Disambiguation pages. Disam-
biguation pages are non-article pages which
are intended to allow the user to choose from
a list of Wikipedia articles defining different
meanings of a term, when the term is am-
biguous. Disambiguation pages cannot be
used as links for the annotation as they are
not suitable for the purposes of this task. In
fact, the annotator?s task is to disambiguate
the meaning of the mention, so one link,
pointing to a specific sense, is to be cho-
sen. Disambiguation pages should always be
checked as they provide useful suggestions
in order to reach the appropriate link(s).
b. In the same way as Disambiguation pages,
Wikitionary cannot be used as linking page,
as it provides a list of possible senses for a
term and not only one specific sense which is
necessary to disambiguate the mention.
c. In Wikipedia, terms may be redirected to
other terms which are related in terms of
morphological derivation; i.e. searching for
the term ?Senator? you are automatically
redirected to ?Senate?; or querying ?citizen?
you are automatically redirected to ?citizen-
ship?. Redirections have always been con-
sidered appropriate links for the term.
Some particular rules have been followed in order
to deal with specific cases in the annotation, which
are described below:
1. As explained before in Section 3.2, as a gen-
eral rule the head of the ACE 05 mention
is annotated with Wikipedia links. In those
cases where the syntactic head of the men-
tion is a multiword lexical unit, the ACE 05
practice is to mark as head only the rightmost
item of the multiword. For instance, in the
case of the multiword ?flight attendant? only
?attendant? is marked as head of the men-
tion, although ?flight attendant? is clearly a
multiword lexical unit that should be anno-
tated as one semantic whole. In our anno-
tation we take into account the meaning of
the whole lexical unit; so, in the above exam-
ple, the generic sense of ?attendant? has not
been given, whereas Flight_attendant
is considered as the appropriate link.
2. In some cases, in ACE 2005 pronouns like
?somebody?, ?anybody?, ?anyone?, ?one?,
?others?, were incorrectly marked as NOM
(instead of PRO). Such cases, which amount
to 117, have been marked with the tag ?No
Annotation?.
3. When a page exists in Wikipedia for a given
mention but not for the specific sense in that
context the ?Missing sense? annotation has
been used. One example of ?Missing sense?
is for instance the term ?heart? which has 29
links proposed in the ?Disambiguation page?
touching different categories (sport, science,
anthropology, gaming, etc.), but there is no
link pointing to the sense of ?center or core of
something?; so, when referring to the heart
of a city, the term has been marked as ?Miss-
ing sense?.
4. When no article exists in Wikipedia for a
given mention, the tag ?No page? has been
adopted.
5. Nicknames, i.e. descriptive names used
in place of or in addition to the official
name(s) of a person, have been treated as
NAM. Thus, even if nicknames look like de-
scriptions of individuals (and their reference
should not be solved, following the general
rule), they are actually used and annotated as
22
Number of annotated mentions 16310
Number of single link mentions 13774
Number of multi-link mentions 1458
Number of ?No Page? annotations 481
Number of ?Missing Sense? 480
annotations
Number of ?No Annotation? 117
annotations
Total number of links 16851
Total number of links in multi-link 3077
mentions
Table 1: Annotation data
proper names aliases. For example, given the
mention ?Butcher of Baghdad?, whose head
?Butcher? is to be annotated, the appropriate
Wikipedia link is Saddam_Hussein, auto-
matically redirected from the searched string
?Butcher of Baghdad?. The link Butcher
is not appropriate as it provides a description
of the mention. It is interesting the fact that
Wikipedia itself redirects to the page of Sad-
dam Hussein.
4 The ACE05-WIKI Extension
Up to now, the 55% of the markable men-
tions have been annotated by one annotator,
amounting to 16,310 mentions. This annotation
has been carried out by CELCT in a period
of two months from February 22 to April 30,
2010, using the on-line version of Wikipedia,
while the remaining 45% of the ACE mentions
will be annotated during August 2010. The
complete annotation will be freely available
at: http://www.celct.it/resources.
php?id_page=acewiki2010, while the
ACE 2005 corpus is distributed by LDC8.
4.1 Annotation Data Analysis
Table 1 gives some statistics about the overall
annotation. In the following sections, mentions
annotated with one link are called ?single link?,
whereas, mentions annotated with more than one
link are named ?multi-link?.
These data refer to the annotation of each sin-
gle mention. It is not possible to give statis-
tics at the entity level, as mentions have differ-
8http://www.ldc.upenn.edu/Catalog/
CatalogEntry.jsp?catalogId=LDC2006T06
Annotation Mention Type
NAM NOM
Single link mentions 6589 7185
Multi-link mentions 79 1379
Missing sense 96 384
No Page 440 41
Table 2: Distinction of NAM and NOM in the an-
notation
ent ID depending on the documents they belong
to, and the information about the cross-document
co-reference is not available. Moreover, mentions
of type NOM are annotated with different links
depending on their disambiguated sense, making
thus impossible to group them together.
Most mentions have been annotated with only
one link; if we consider multi-link mentions, we
can say that each mention has been assigned an
average of 2,11 links (3,077/1,458).
Data about ?Missing sense? and ?No page?
are important as they provide useful information
about the coverage of Wikipedia as sense in-
ventory. Considering both ?Missing sense? and
?No page? annotations, the total number of men-
tions which have not been linked to a Wikipedia
page amounts to 6%, equally distributed between
?Missing sense? and ?No page? annotations. This
fact proves that, regarded as a sense inventory,
Wikipedia has a broad coverage. As Table 2
shows, the mentions for which more than one link
was deemed appropriate are mostly of type NOM,
while NAM mentions have been almost exclu-
sively annotated with one link only. The very few
cases in which a NAM mention is linked to more
than one Wikipedia page are primarily due to (i)
mistakes in the ACE 05 annotation (for example,
the mention ?President? was erroneously marked
as a NAM); (ii) or to cases where nouns marked
as NAM could also be considered as NOMs (see
for instance the mention ?Marine?, to mean the
Marine Corps).
Table 2 provides also statistics about the ?Miss-
ing sense? and ?No page? cases provided on men-
tions divided among the NAM and NOM type.
The ?missing sense? annotation concerns mostly
the NOM category, whereas the NAM category
is hardly affected. This attests the fact that per-
sons, locations and organizations are well repre-
23
sented in Wikipedia. This is mainly due to the
encyclopedic nature of Wikipedia where an arti-
cle may be about a person, a concept, a place,
an event, a thing etc.; instead, information about
nouns (NOM) is more likely to be found in a
dictionary, where information about the meanings
and usage of a term is provided.
4.2 Inter-Annotator Agreement
About 3,100 mentions, representing more than
10% of the mentions to be annotated, have been
annotated by two annotators in order to calculate
Inter-Annotator Agreement.
Once the annotations were completed, the
two annotators carried out a reconciliation phase
where they compared the two sets of links pro-
duced. Discrepancies in the annotation were
checked with the aim of removing only the more
rough errors and oversights. No changes have
been made in the cases of substantial disagree-
ment, which has been maintained.
In order to measure Inter-Annotator Agree-
ment, two metrics were used: (i) the Dice coeffi-
cient to measure the agreements on the set of links
used in the annotation9 and (ii) two measures of
agreement calculated at the mention level, i.e. on
the group of links associated to each mention.
The Dice coefficient is computed as follows:
Dice = 2C/(A + B)
where C is the number of common links chosen by
the two annotators, while A and B are respectively
the total number of links selected by the first and
the second annotator. Table 3 shows the results
obtained both before and after the reconciliation
9The Dice coefficient is a typical measure used to com-
pare sets in IR and is also used to calculate inter-annotator
agreement in a number of tasks where an assessor is allowed
to select a set of labels to apply to each observation. In fact,
in these cases measures such as the widely used K are not
good to calculate agreement. This is because K only offers
a dichotomous distinction between agreement and disagree-
ment, whereas what is needed is a coefficient that also allows
for partial disagreement between judgments. In fact, in our
case we often have a partial agreement on the set of links
given for each mention. Also considering only the mentions
for which a single link has been chosen, it is not possible
to calculate K statistics in a straightforward way as the cate-
gories (i.e. the possible Wikipedia pages) in some cases can-
not be determined a priori and are different for each mention.
Due to these factors chance agreement cannot be calculated
in an appropriate way.
BEFORE AFTER
reconciliation reconciliation
DICE 0.85 0.94
Table 3: Statistics about Dice coefficient
BEFORE AFTER
reconciliation reconciliation
Complete 77.98% 91.82%
On first link 84.41% 95.58%
Table 4: Agreement at the mention level
process. Agreement before reconciliation is satis-
factory and shows the feasibility of the annotation
task and the reliability of the annotation scheme.
Two measures of agreement at the mention
level are also calculated. To this purpose, we
count the number of mentions where annotators
agree, as opposed to considering the agreement on
each link separately. Mention-level agreement is
calculated as follows:
Number of mentions with annotation in agreement
Total number of annotated mentions
We calculate both ?complete? agreement and
agreement on the first link. As regards the first
measure, a mention is considered in complete
agreement if (i) it has been annotated with the
same link(s) and (ii) in the case of multi-link men-
tions, links are given in the same order. As for the
second measure, there is agreement on a mention
if both the annotators chose the same first link (i.e.
the one judged as the most appropriate), regard-
less of other possible links assigned to that men-
tion. Table 4 provides data about both complete
agreement and first link agreement, calculated be-
fore and after the annotators reconciliation.
4.3 Disagreement Analysis
Considering the 3,144 double-annotated men-
tions, the cases of disagreements amount to 692
(22,02%) before the reconciliation while they are
reduced to 257 (8,18%) after that process. It is in-
teresting to point out that the disagreements affect
the mentions of type NOM in most of the cases,
whereas mentions of type NAM are involved only
in 3,8% of the cases.
Examining the two annotations after the recon-
ciliation, it is possible to distinguish three kinds
of disagreement which are shown in Table 5 to-
24
Number of
Disagreement type Disagreements
1) No matching in the link(s)
proposed
105 (40,85%)
2) No matching on the first link,
but at least one of the other links
is the same
14 (5,45%)
3) Matching on the first link and
mismatch on the number of ad-
ditional links
138 (53,70%)
Total Disagreements 257
Table 5: Types of disagreements
gether with the data about their distribution. An
example of disagreement of type (1) is the anno-
tation of the mention ?crossing?, in the following
context: ?Marines from the 1st division have se-
cured a key Tigris River Crossing?. Searching for
the word ?river crossing? in the Wikipedia search-
box, the Disambiguation Page is opened and a
list of possible links referring to more specific
senses of the term are offered, while the generic
?river crossing? sense is missing. The annota-
tors are required to choose just one of the possi-
ble senses provided and they chose two different
links pointing to pages of more specific senses:
{Ford_%28river%29} and {Bridge}.
Another example is represented by the annota-
tion of the mention ?area? in the context : ?Both
aircraft fly at 125 miles per hour gingerly over en-
emy area?. In Wikipedia no page exists for the
specific sense of ?area? appropriate in the con-
text. Searching for ?area? in Wikipedia, the page
obtained is not suitable, and the Disambiguation
page offers a list of various possible links to either
more specific or more general senses of the term.
One annotator judged the more general Wikipedia
page Area_(subnational_entity) as ap-
propriate to annotate the mention, while the sec-
ond annotator deemed the page not suitable and
thus used the ?Missing sense? annotation.
Disagreement of type (2) refers to cases where
at least one of the links proposed by the annota-
tors is the same, but the first (i.e. the one judged
as the most suitable) is different. Given the fol-
lowing context: ?Tom, You know what Liber-
als want?, the two annotation sets provided for
the mention ?Liberal? are: {Liberalism} and
{Liberal_Party, Modern_liberalism_
in_the_United_States, Liberalism}.
The first annotator provided only one link for
the mention ?liberal?, which is different from the
first link provided by second annotator. However,
the second annotator provided also other links,
among which there is the link provided by the first
annotator.
Another example is represented by the annota-
tion of the mention ?killer?. Given the context:
?He?d be the 11th killer put to death in Texas?, the
two annotators provided the following link sets:
{Assassination, Murder} and {Murder}.
Starting from the Wikipedia disambiguation page,
the two annotators agreed on the choice of one of
the links but not on the first one.
Disagreement of type (3) refers to cases where
both annotators agree on the first link, correspond-
ing to the most specific sense, but one of them
also added link(s) considered appropriate to an-
notate the mention. Given the context: ?7th Cav-
alry has just taken three Iraqi prisoners?, the an-
notations provided for the term ?prisoners? are:
{Prisoner_of_war} and {Prisoner_of_
war, Incarceration}. This happens when
more than one Wikipedia pages are appropriate to
describe the mention.
As regards the causes of disagreement, we see
that the cases of disagreement mentioned above
are due to two main reasons:
a. The lack of the appropriate sense in
Wikipedia for the given mention
b. The different interpretation of the context in
which the mention occurs.
In cases of type (a) the annotators adopted differ-
ent strategies to perform their task, that is:
i. they selected a more general sense (i.e.
?area? which has been annotated with
Area_(subnational_entity)),
ii. they selected a more specific sense (see for
example the annotations of the mentions
?river crossing?).
iii. they selected the related senses proposed by
the Wikipedia Disambiguation page (as in
the annotation of ?killer? in the example
above).
25
Disagreement Reas. a Reas. b Tot
type (see above)
1) No match 95 10 105
2) No match on 4 10 14
first link
3) Mismatch on 138 138
additional links
Total 99 158 257
(38,5%) (61,5%)
Table 6: Distribution of disagreements according
to their cause
iv. they used the tag ?Missing sense?.
As Wikipedia is constantly evolving, adding
new pages and consequently new senses, it is
reasonable to think that the considered elements
might find the appropriate specific/general link as
time goes by.
Case (b) happens when the context is ambigu-
ous and the information provided in the text al-
lows different possible readings of the mention
to be annotated, making thus difficult to disam-
biguate its sense. These cases are independent
from Wikipedia sense repository but are related to
the subjectivity of the annotators and to the inher-
ent ambiguity of text.
Table 6 shows the distribution of disagreements
according to their cause. Disagreements of type 1
and 2 can be due to both a and b reasons, while
disagreements of type 3 are only due to b.
The overall number of disagreements shows
that the cases where the two annotators did not
agree are quite limited, amounting only to 8%.
The analyses of the disagreements show some
characteristics of Wikipedia considered as sense
repository. As reported in Table 8, in the 61,5%
of the cases of disagreement, the different anno-
tations are caused by the diverse interpretation
of the context and not by the lack of senses in
Wikipedia. It is clear that Wikipedia has a good
coverage and it proves to be a good sense disam-
biguation tool. In some cases it reveals to be too
fine-grained and in other cases it remains at a more
general level.
5 Conclusion
This paper has presented an annotation work
which connects an existing annotated corpus such
as the English ACE 2005 dataset to a Collabo-
ratively Constructed Semantic Resource such as
Wikipedia. Thanks to this connection Wikipedia
becomes an essential semantic resource for the
task of coreference resolution. On one hand, by
taking advantage of the already existing annota-
tions, with a relatively limited additional effort,
we enriched an existing corpus and made it useful
for a new NLP task which was not planned when
the corpus was created. On the other hand, our
work allowed us to explore and better understand
certain characteristics of the Wikipedia resource.
For example we were able to demonstrate in quan-
titative terms that Wikipedia has a very good cov-
erage, at least as far as the kind of entity men-
tions which are contained in the ACE 2005 dataset
(newswire) is concerned.
Acknowledgments
The research leading to these results has re-
ceived funding from the ITCH project (http://
itch.fbk.eu), sponsored by the Italian Min-
istry of University and Research and by the Au-
tonomous Province of Trento and the Copilosk
project (http://copilosk.fbk.eu), a Joint
Research Project under Future Internet - Internet
of Content program of the Information Technol-
ogy Center, Fondazione Bruno Kessler.
We thank Giovanni Moretti from CELCT for
technical assistance.
References
Bryl, Volha, Claudio Giuliano, Luciano Serafini, and
Kateryna Tymoshenko. 2010. Using background
knowledge to support coreference resolution. In
Proceedings of the 19th European Conference on
Artificial Intelligence (ECAI 2010), August.
Csomai, Andras and Rada Mihalcea. 2008. Linking
documents to encyclopedic knowledge. IEEE Intel-
ligent Systems, 23(5):34?41.
Cucerzan, Silviu. 2007. Large-scale named entity
disambiguation based on Wikipedia data. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 708?716, Prague, Czech Republic,
June. Association for Computational Linguistics.
26
Culotta, Aron, Michael L. Wick, and Andrew McCal-
lum. 2007. First-order probabilistic models for
coreference resolution. In Human Language Tech-
nology Conference of the North American Chap-
ter of the Association of Computational Linguistics,
pages 81?88.
Fellbaum, Christiane, editor. 1998. WordNet: an elec-
tronic lexical database. MIT Press.
Kulkarni, Sayali, Amit Singh, Ganesh Ramakrishnan,
and Soumen Chakrabarti. 2009. Collective anno-
tation of wikipedia entities in web text. In KDD
?09: Proceedings of the 15th ACM SIGKDD inter-
national conference on Knowledge discovery and
data mining, pages 457?466, New York, NY, USA.
ACM.
Mihalcea, Rada and Dan I. Moldovan. 2001.
Ez.wordnet: Principles for automatic generation of
a coarse grained wordnet. In Russell, Ingrid and
John F. Kolen, editors, FLAIRS Conference, pages
454?458. AAAI Press.
Milne, David and Ian H. Witten. 2008. Learning
to link with wikipedia. In CIKM ?08: Proceed-
ing of the 17th ACM conference on Information and
knowledge management, pages 509?518, New York,
NY, USA. ACM.
Ng, Vincent and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In ACL ?02: Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics,
pages 104?111.
Ng, Vincent. 2007. Semantic class induction and
coreference resolution. In ACL 2007, Proceed-
ings of the 45th Annual Meeting of the Association
for Computational Linguistics, June 23-30, 2007,
Prague, Czech Republic, pages 536?543.
Ponzetto, S. P. and M. Strube. 2006. Exploiting se-
mantic role labeling, WordNet and Wikipedia for
coreference resolution. Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 192?
199.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistic, 27(4):521?544.
Versley, Yannick, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
Bart: a modular toolkit for coreference resolution.
In Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies, pages 9?12.
27
Proceedings of the 2nd Workshop on ?Collaboratively Constructed Semantic Resources?, Coling 2010, pages 55?62,
Beijing, August 2010
Identifying and Ranking Topic Clusters in the Blogosphere 
M. Atif Qureshi 
Korea Advanced Institute of  
Science and Technology  
atifms@kaist.ac.kr 
Arjumand Younus                    
Korea Advanced Institute of 
Science and Technology  
arjumandms@kaist.ac.kr 
Muhammad Saeed 
University of Karachi  
saeed@uok.edu.pk 
Nasir Touheed                    
Institute of Business Administra-
tion 
ntouheed@iba.edu.pk 
Abstract 
The blogosphere is a huge collaboratively 
constructed resource containing diverse 
and rich information. This diversity and 
richness presents a significant research 
challenge to the Information Retrieval 
community. This paper addresses this 
challenge by proposing a method for 
identification of ?topic clusters? within 
the blogosphere where topic clusters 
represent the concept of grouping togeth-
er blogs sharing a common interest i.e. 
topic, the algorithm takes into account 
both the hyperlinked social network of 
blogs along with the content in the blog 
posts. Additionally we use various forms 
and parts-of-speech of the topic to pro-
vide a broader coverage of the blogos-
phere. The next step of the method is to 
assign topic-specific ranks to each blog 
in the cluster using a metric called ?Topic 
Discussion Rank,? that helps in identify-
ing the most influential blog for a specif-
ic topic. We also perform an experimen-
tal evaluation of our method on real blog 
data and show that the proposed method 
reaches a high level of accuracy. 
1 Introduction 
With a proliferation of Web 2.0 services and ap-
plications there has been a major paradigm shift 
in the way we envision the World Wide Web 
(Anderson, 2007; O?Reilly, 2005). Previously the 
Web was considered as a medium to access in-
formation in a read-only fashion. Weblogs or 
blogs is one such application that has played an 
effective role in making the Web a social gather-
ing point for masses. The most appealing aspect 
of blogs is the empowerment they provide to 
people on the World Wide Web by enabling 
them to publish their own opinions, ideas, and 
thoughts on many diverse topics of their own 
interest generally falling into politics, economics, 
sports, technology etc.  A blog is usually like a 
personal diary (Sorapure, 2003) with the differ-
ence that it's now online and accessible to remote 
people, it consists of posts arranged chronologi-
cally by date and it can be updated on a regular 
basis by the author of the blog known as blogger. 
Moreover bloggers have the option to link to 
other blogs thereby creating a social network 
within the world of blogs called the blogos-
phere ? in short the blogosphere is a collabora-
tively constructed resource with rich information 
on a wide spectrum of topics having characteris-
tics very different from the traditional Web. 
   However with these differing characteristics of 
blogs arise many research challenges and this is 
in particular the case for the Information Retriev-
al domain. One important problem that arises 
within this huge blogosphere (Sifry, 2009) is 
with respect to identification of topic clusters. 
Such a task involves identification of the key 
blog clusters that share a common interest point 
(i.e., topic) reflected quite frequently through 
their blog posts. This is a special type of cluster-
55
ing problem with useful applications in the do-
main of blog search as Mishne and de Rijke 
(2006) point out in their study of blog search 
about the concept queries submitted by users of 
blog search systems.  
   Moreover ranking these bloggers with respect 
to their interest in the topic is also a crucial task 
in order to recognize the most influential blogger 
for that specific topic. However the blog ranking 
problem has a completely different nature than 
the web page ranking problem and link populari-
ty based algorithms cannot be applied for ranking 
blogs. The reasons for why link based methods 
cannot be used for blog ranking are as follows:  
 
Blogs have very few links when com-
pared to web pages; Leskovec et al 
report that average number of links 
per blog post is only 1.6 links (2007). 
This small number of links per blog 
results in formation of very sparse 
network especially when trying to find 
blogs relevant to a particular topic. 
Blog posts are associated with a time-
stamp and they need some time for 
getting in-links. In most of the cases 
when they receive the links the topics 
which they talk about die out. 
When link based ranking techniques are 
used for blogs, bloggers at times as-
sume the role of spammers and try to 
exploit the system to boost rank of 
their blogs.  
   In this paper we propose a solution for identifi-
cation of topic clusters from within the blogos-
phere for any topic of interest. We also devise a 
way to assign topic-specific ranks for each iden-
tified blog within the topic cluster. The cluster is 
identified by the calculation of a metric called 
?Topic Discussion Isolation Rank (TDIR).? Each 
blog in the cluster is also assigned a topic rank 
by further calculation of another metric ?Topic 
Discussion Rank (TDR).? The first metric 
"TDIR" is applied to a blog in isolation for the 
topic under consideration and the second metric 
"TDR" takes into account the blog?s role in its 
neighborhood for that specific topic. Our work 
differs from past approaches (Kumar et al, 2003; 
Gruhl et al, 2004; Chi et al, 2007; Li et al, 2009) 
in that it takes into consideration both the links 
between the blogs as well as the content in the 
blog posts whereas a majority of the past me-
thods follow only link structure. Furthermore we 
make use of some natural language processing 
techniques to ensure better coverage of our clus-
ter-finding and ranking methodology. We also 
perform an experimental evaluation of our pro-
posed solution and release the resultant data of 
blog clusters and the ranks as an XML corpus.  
   The remainder of this paper is organized as 
follows. Section 2 presents a brief summary of 
related work in this dimension and explains how 
our proposed methodology differs from these 
works. Section 3 explains the concept of ?topic 
clusters? in detail along with a description of our 
solution for clustering and ranking blogs on basis 
of topics. Section 4 explains our experimental 
methodology and presents our experimental 
evaluations on a corpus of 50,471 blog posts ga-
thered from 102 blogs. Section 5 concludes the 
paper with a discussion of future work in this 
direction.  
2 Related Work 
Given the vast amount of useful information in 
the blogosphere there have been many research 
efforts for mining and analysis of the 
blogosphere. This section reviews some of the 
works that are relevant to our study.  
   There have been several works with respect to 
community detection in the blogosphere: one of 
the oldest works in this dimension is by Kumar 
et al who studied the bursty nature of the 
blogosphere by extracting communities using the 
hyperlinks between the blogs (2003). Gruhl et al 
proposed a transmission graph to study the flow 
of information in the blogosphere and the 
proposed model is based on disease-propagation 
model in epidemic studies (2004). Chi et al 
studied the evolution of blog communities over 
time and introduced the concept of community 
factorization (2007). A fairly recent work is by 
Li et al that studies the information propagation 
pattern in the blogosphere through cascade 
affinity which is an inclination of a blogger to 
join a particular blog community (2009). Apart 
from detection of communities within the 
blogosphere another related study which has 
recently attracted much interest is of identifying 
influentials within a ?blog community? 
(Nakajima et al, 2005; Agarwal et al, 2008). All 
56
these works base their analysis on link structure 
of the blogosphere whereas our analytical model 
differs from these works in that it assigns topic 
based ranks to the blogs by taking into account 
both links and blog post?s contents.  
Along with the community detection problem 
in the blogosphere there has also been an increas-
ing interest in ranking blogs. Fujimura et al 
point out the weak nature of hyperlinks in the 
web blogs and due to that nature they devise a 
ranking algorithm for blog entries that uses the 
structural characteristic of blogs; the algorithm 
enables a new blog entry or other entries that 
have no in-links to be rated according to the past 
performance of the blogger (2005). There is a 
fairly recent work closely related to ours per-
formed by Hassan et al(2009) and this work 
identifies the list of particularly important blogs 
with recurring interest in a specific topic; their 
approach is based on lexical similarity and ran-
dom walks. 
3 Cluster Finding and Ranking Metho-
dology 
In this section we explain the concept of ?topic 
clusters? in detail and go into the details of why 
we deviate from the traditional term of ?blog 
community? in the literature. After this signifi-
cant discussion we then move on to explain our 
proposed method for identification and ranking 
of the ?topic clusters? in the blogosphere: two 
metrics ?topic discussion isolation rank? and 
?topic discussion rank? are used for this purpose.  
3.1 Topic Clusters 
As explained in section 2 the problem of group-
ing together blogs has been referred to as the 
?community detection problem? in the literature. 
However an aspect ignored by most of these 
works is the contents of the blogs. Additionally 
most of the works in this dimension find a blog 
community by following blog threads? discus-
sions/conversations (Nakajima et al, 2005; 
Agarwal et al, 2008) which may not always be 
the case as blogs linking to each other are not 
necessarily part of communications or threads. 
With the advent of micro blogging tools such 
as Twitter (Honeycutt and Herring, 2009) the 
role of blogs as a conversational medium has 
diminished and bloggers link to each other as a 
socially networked cluster by linking to their 
most favorite blogs on their home page as is 
shown in the snapshot of a blog in Figure 1:     
Normally those bloggers link to each other 
that have similar interests and importantly talk 
about same topics. Hence the idea of topic cluster 
is used to extract those clusters from the blogos-
phere that have  strong interest in some specific 
topics which they mention frequently in their 
blog posts and additionally they form a linked 
cluster of blogs. As pointed out by Hassan et al 
the ?task of providing users with a list of particu-
larly important blogs with a recurring interest in 
a specific topic is a problem that is very signifi-
cant in the Information Retrieval domain? 
(2009). For the purpose of solving this problem 
we propose the notion of ?topic clusters.? The 
task is much different from traditional communi-
Figure 1: Blog Showing the List of Blogs it Follows 
57
ty detection in the blogosphere as it utilizes both 
content and link based analysis. The process of 
finding topic clusters is carried out by calculating 
a metric ?Topic Discussion Isolation Rank? 
which we explain in detail in section 3.3. 
3.2 Rank Assignment to Topic Clusters 
As we explained in section 1, due to the unique 
nature of the blogosphere, traditional link-based 
methods such as PageRank (Page et al, 1998) 
may not be appropriate for the ranking task in 
blogs. This is the main reason that we use the 
content of blog posts and lexical similarity in 
blog posts along with links for the rank assign-
ment function that we propose. Furthermore we 
take a blog as aggregate of all its posts for the 
retrieval task.  
3.3 Topic Discussion Isolation Rank 
Topic Discussion Isolation Rank is a metric that 
is used to find the cluster of blogs for a specific 
topic. It takes each blog in isolation and analyses 
the contents of its posts to discover its interest in 
a queried topic. We consider a blog along three 
dimensions as Figure 2 shows:                
As mentioned in section 1 of this paper we 
utilize some natural language processing tech-
niques to ensure better coverage of our cluster-
finding and ranking methodology: those tech-
niques are applied along the part of speech di-
mension shown in Figure 1, for a given topic we 
analyze blog post contents not only for that par-
ticular topic but also for its associated adjectives 
and adverbs i.e. the topic itself is treated as a 
noun and its adjectives and adverbs are also used. 
For example if the topic of interest is ?democra-
cy? we will also analyze the blog post contents 
for adjective ?democratic? and adverb ?demo-
cratically.? Furthermore, a weight in descending 
order is assigned to the noun (denoted as w
n
), 
adjective (denoted as w
adj
) and adverb (denoted 
as w
adv
) of the queried topic where w
n
>w
adj
>w
adv
.  
This approach guarantees better coverage of the 
blogosphere and the chances of missing out blogs 
that have interest in the queried topic are minim-
al. The blog post number denotes the number of 
the post in which the word is found and occur-
rence is a true/false parameter denoting whether 
or not the word exists in the blog post. Based on 
these three dimensions we formulated the TDIR 
metric as follows:  
1+ (n
noun
x w
n
)+(n
adjective 
x w
adj
)+(n
adverb
x w
adv
) 
Number of total posts  
   Here w
n, 
w
adj and 
w
adv 
are as explained pre-
viously in this section and n
noun 
denotes the num-
ber of times nouns are found in all the blog posts, 
n
adjective 
denotes the number of times adjectives 
are found in all the blog posts and n
adverb 
denotes 
the number of times adverbs are found in all the 
blog posts. This metric is calculated for each 
blog in isolation and the blogs that have TDIR 
value of greater than 1 are considered part of the 
topic cluster. 
Additionally we also use various forms of the 
queried topic in the calculation of TDIR as this 
also ensures better coverage during the cluster 
detection process. In the world of the blogos-
phere, bloggers have all the freedom to use what-
ever terms they want to use for a particular topic 
and it is this freedom which adds to the difficulty 
of the Information Retrieval community. Within 
the TDIR metric we propose use of alternate 
terms/spellings/phrases for a given topic ? an 
example being the use of ?Obama? by some 
bloggers and ?United States first Black Presi-
dent? or ?United States? Black President? by oth-
ers. Such ambiguity with respect to posts talking 
about same topic but using different phras-
es/spellings/terms can be resolved by using a 
corpus-based approach with listing of alternate 
phrases and terms for the broad topics. Moreover 
the weights used for each of the part of speech 
?noun?, ?adjective? and ?adverb? in the TDIR 
metric can be adjusted differently for different 
topics with some topics having a stronger indica-
Figure 2: Blog TDIR Dimensions 
58
tion of discussion of that topic through occur-
rence of noun and some through occurrence of 
adjective or adverb. Some examples of these var-
ious measures are shown in our experimental 
evaluations that are explained in section 4. 
3.4 Topic Discussion Rank 
After the cluster-finding phase we perform the 
ranking step by means of Topic Discussion Rank. 
It is in this phase that the socially networked and 
linked blogs play a role in boosting each other?s 
ranks. It is reasonable to assign a higher topic 
rank to a blog that has interest in the specific top-
ic and is also a follower of many blogs with simi-
lar topic discussions than one that mentions the 
topic under consideration but does not link to 
other similar blogs: Topic Discussion Rank does 
that by taking into account both link structure 
and TDIR explained in previous section. This has 
the advantage of taking into account both factors: 
the content of the blog posts and the link struc-
ture of its neighborhood. 
The following piecewise function shows how 
the metric Topic Discussion Rank is calculated:       
Explanation of notations used:  
b - blog  
o : (o,b) ? outlinks from blog b  
The TDR is same as the TDIR in case of the 
blog having zero outlinks as such a blog exists in 
isolation and does not have a strong participation 
within the social network of the blogosphere. In 
the case of a blog having one or more outlinks to 
other blogs we add its own TDIR to the factor  
.   
Here matching links represent blogs that are 
part of topic cluster for a given topic (i.e. those 
having TDIR greater than 1 as explained in sec-
tion 3.3) and each matching link?s TDIR is 
summed up and multiplied by a factor called 
damp. Note that summation of TDIR is used in 
the first iteration only, in the other iterations it is 
replaced by TDR of the blogs.  
Furthermore it is important to note that the 
process of TDR computation is an iterative one 
similar to PageRank (Page et al, 1998) computa-
tion, however the termination condition is unlike 
PageRank in that PageRank terminates when 
rank values are normalized whereas our approach 
uses the blog depth as a termination condition 
which is an adjustable parameter. Due to the 
changed termination condition the role of spam 
blogs is minimized. 
The damping factor damp is introduced to mi-
nimize biasness as is explained below. Consider 
the two blogs as shown with the link structure 
represented by arrows:      
In this case let?s assume the TDIR of blog A is 
2 and the TDIR of blog B is 1. Using the formu-
lation for TDR without the damping factor we 
would have 2+(1/1x1)=3 for blog A and 
1+(1/1x2)=3 for blog B which is not the true ref-
lection of their topic discussion ranks. However 
when we use the damping factor the resultant 
TDR?s are 2+(1/1x1x0.9)=2.9 for blog A and 
1+(1/1x2x0.9)=2.8 for blog B and this more cor-
rectly represents the topic discussion ranks of 
both the blogs. 
4 Experimental Evaluations 
This section presents details of our experiments 
on real blog data. We use precision and recall to 
measure the effectiveness of our approach of 
cluster-finding. The experimental data is released 
as an XML corpus which can be downloaded 
from: 
http://unhp.com.pk/blogosphereResearch/data.tar
.gz. 
Figure 3: Example for Damping Factor Explanation  
 
59
4.1 Data and Methodology 
The data used in the experiments was gathered 
from 102 blog sites which comprised of 50,471 
blog posts. Currently we have restricted the data 
set to only the blogspot domain (blogger.com 
service by Google).We used four blog sites as 
seeds and from them the link structure of the 
blogs was extracted after which the crawl (Qure-
shi et al, 2010) was performed using the XML 
feeds of the blogs to retrieve all the posts in each 
blog. Each blog had an average of 494 posts. 
The topics for which we perform the experi-
ments of finding TDIR and TDR were taken to 
be ?compute?, ?democracy?, ?secularism?, 
?bioinformatics?, ?haiti? and ?obama.?  
The measures that we use to assess the accura-
cy of our method are precision and recall which 
are widely used statistical classification measures 
for the Information Retrieval domain. The two 
measures are calculated using equations 4.1 and 
4.2:  
Precision =    |Ct nCa|            (4.1) 
  |Ca|   
Recall  =      |Ct nCa|              (4.2)  
|Ct| 
Here Ca represents the topic cluster set found 
using our algorithm i.e. the set of blogs that have 
interest in the queried topic, in other words it is 
the set of the blogs that have TDIR greater than 1. 
Ct represents the true topic cluster set meaning 
the set of those blogs that not just mention the 
topic but are really interested in it. The reason for 
distinguishing between true cluster set Ct and 
algorithmic cluster set Ca is that our method just 
searches for the given keyword i.e. topic in all 
the posts and since natural language is so rich 
that just mentioning the topic does not represent 
the fact that the blog is a part of that topic cluster. 
Hence we use a human annotator/labeler for 
identification of the true cluster set from the set 
of the 102 blogs for each of the 6 topics that we 
used in our experiments.  
4.2 Results 
We plot the precision and recall graphs for the 
topics chosen. Figure 4 shows the graph for pre-
cision: 
       
The average precision was found to be 0.87 
which reflects the accurate relevance of our me-
thod. As can be seen from the graph in figure 4 
the precision falls below the 0.8 mark only for 
the topics compute and secularism ? the reason 
for this is that for these two topics a higher pro-
portion of false positives were discovered. Not 
all the posts having the word ?compute? were 
actually related to computing as found by human 
annotator. Same was the case for the word secu-
larism ? since our method searches for adjective 
secular and adverb secularly in case of secular-
ism not being found hence there were some blogs 
in which secular was used but the blog?s focus 
was not in secularism as an idea. On the other 
hand precision measures for the topics ?democ-
racy?, ?obama?, ?haiti? and ?bioinformatics? 
were quite good because these words are likely 
to be found in the blogs that actually focus on 
them as a topic hence reducing the chances of 
false positives. 
Figure 5 shows the graph for recall:  
    The average recall was found to be 0.971 
which reflects the high coverage of our method. 
As the graph in figure 5 shows the recall value is 
Figure 4: Precision Graph for Chosen Topics  
 
Figure 5: Recall Graph for Chosen Topics  
60
mostly close to 1 for the chosen topics. This high 
coverage is attributed to the part of speech di-
mension as discussed in section 3.3; this tech-
nique rules out the chances of false negatives and 
hence we obtain a high recall for our method. 
4.3 Additional Experiments 
In addition to experiments on the six coarse-
grained topics mentioned above we performed 
some additional experiments on two fine-grained 
topics and also repeated the experiment per-
formed on topic ?Obama? with an additional 
term ?Democrats.? On formulating the cluster 
with these two terms the precision increased 
from 0.907 to 0.95 which clearly shows that in-
corporation of extra linguistic features into the 
TDIR formulation ensures better results. Moreo-
ver the ranks of some blogs were found to be 
higher than the ranks obtained previously and 
this increase in rank was due to the fact that 
many posts had subject theme ?Obama? but they 
used the term ?Democrats? ? when we used this 
alternate term the ranks i.e. TDR more correctly 
represented the role of the blogs in the cluster. 
The two fine grained topics for which we re-
peated our experiments were: healthcare bill and 
avatar. Additional terms were also included in 
the TDIR and TDR computation process which 
were as follows:  
healthcare bill ? obamacare 
avatar- sky people,  jake sully  
These alternate terms were chosen as these are 
the commonly associated terms when these top-
ics are discussed. At this point we provided them 
as query topics but for future work our plan is to 
use a machine learning approach for learning 
these alternate phrases for each topic, and know-
ledge bases such as Wikipedia may also be used 
to gather the alternate terms for different topics. 
The precision for the topic healthcare bill was 
found to be 0.857 which had a negligible effect 
on excluding ?obamacare?; however recall suf-
fered more on exclusion of alternate term ?ob-
amacare? as it fell from 1 to 0.667. Results for 
the topic ?avatar? however were quite different 
with a precision of 0.47 and a recall of 1; this 
was due to the large number of false positives 
that were retrieved for the term avatar and we 
found reason for this to be that our approach does 
not take into consideration case-sensitivity at this 
point hence it failed to distinguish between the 
term ?avatar? and movie ?Avatar?. Also in the 
case of topic ?avatar? the alternate phrases did 
not have any effect and hence there is a need to 
refine the approach for fine-grained topics such 
as this one ? we present future directions for re-
finement of our approach in section 5. 
5 Conclusions and Future Work 
In this paper we proposed the concept of ?topic 
clusters? to solve the blog categorization task for 
the Information Retrieval domain. The proposed 
method offers a new dimension in blog commu-
nity detection and blog ranking by taking into 
account both link structure and contents of blog 
posts. Furthermore the natural language 
processing techniques we use provide a higher 
coverage thereby leading to a high average recall 
value of 0.971 in the experiments we performed. 
At the same time we achieved a good accuracy as 
was reflected by an average precision of 0.87. 
For future work we aim to combine our pro-
posed solution into a framework for auto genera-
tion of useful content on a variety of topics such 
as ?blogopedia?; the content can be obtained au-
tomatically from the blog posts and in this way 
manual effort may be saved. We also plan to re-
fine our approach by taking into account the 
temporal aspects of blog posts such as time in-
terval between blog posts, start post date and 
time, end post data and time into our formulation 
for ?Topic Discussion Isolation Rank? and ?Top-
ic Discussion Rank?. Moreover as future direc-
tions of this work we plan to incorporate a ma-
chine learning framework for the assignment of 
the weights corresponding to each topic and for 
the additional phrases to use for each of the top-
ics that we wish to cluster.         
61
References 
Agarwal, Nitin, Huan Liu, Lei Tang, and Philip S. Yu, 
2008. Identifying the influential bloggers in a 
community. In Proceedings of the international 
Conference on Web Search and Web Data Mining 
(Palo Alto, California, USA, February 11 - 12, 
2008). WSDM '08. ACM. 
Anderson, Paul, 2007. What is Web 2.0? Ideas, tech-
nologies and implications for education. Technical 
report, JISC. 
Chi, Yun, Shenghuo Zhu, Xiaodan Song, Junichi Ta-
temura, and Belle L. Tseng,  2007. Structural and 
temporal analysis of the blogosphere through 
community factorization. In Proceedings of the 
13th ACM SIGKDD international Conference on 
Knowledge Discovery and Data Mining (San Jose, 
California, USA, August 12 - 15, 2007). KDD '07. 
ACM. 
Fujimura,Ko, Takafumi Inoue, and Masayuki Sugiza-
ki, 2005. The EigenRumor Algorithm for Ranking 
Blogs. In Proceedings of the WWW 2005 Work-
shop on the Weblogging Ecosystem: Aggregation, 
Analysis and Dynamics. 
Gruhl, Daniel, R. Guha, David Liben-Nowell, and 
Andrew Tomkins, 2004. Information diffusion 
through blogspace. In Proceedings of the 13th in-
ternational Conference on World Wide Web (New 
York, NY, USA, May 17 - 20, 2004). WWW '04. 
ACM. 
Hassan, Ahmed, Dragomir Radev, Junghoo Cho and 
Amruta Joshi, 2009. Content Based Recommenda-
tion and Summarization in the Blogosphere. Third 
International AAAI Conference on Weblogs and 
Social Media, AAAI Publications. 
Honeycutt, Courtenay, and Susan C. Herring, 2009. 
Beyond microblogging: Conversation and collabo-
ration via Twitter. In Proceedings Hawaii Interna-
tional Conference on System Sciences, IEEE Press 
Kumar, Ravi, Jasmine Novak, Prabhakar Raghavan, 
and Andrew Tomkins, 2003. On the bursty evolu-
tion of blogspace. In Proceedings of the 12th inter-
national Conference on World Wide Web (Budap-
est, Hungary, May 20 - 24, 2003). WWW '03. 
ACM. 
Leskovec, Jure, Andreas Krause, Carlos Guestrin, 
Christos Faloutsos, Jeanne Van-Briesen, and Nata-
lie  Glance, 2007. Costeffective outbreak detection 
in networks. In The 13th International Conference 
on Knowledge Discovery and Data Mining (KDD) 
2007. ACM.   
Li, Hui,  Sourav S. Bhowmick,  and Aixin Sun, 2009. 
Blog cascade affinity: analysis and prediction. In 
Proceeding of the 18th ACM Conference on infor-
mation and Knowledge Management (Hong Kong, 
China, November 02 - 06, 2009). CIKM '09. ACM. 
Mishne, G. and Maarten de Rijke, 2006. A Study of 
Blog Search. In Proceedings of ECIR-2006. LNCS 
vol 3936. Springer. 
Nakajima,Shinsuke, Junichi Tatemura, Yoichiroara 
Hino, Yoshinori Hara and Katsumi Tanaka, 2005. 
Discovering Important Bloggers based on Analyz-
ing Blog Threads. In Proceedings of the 14th inter-
national Conference on World Wide Web (Chiba, 
Japan, May 10 - 14, 2005). WWW '05. ACM. 
O'Reilly, Tim, 2005. What is Web 2.0: Design Pat-
terns and Business Models for the next generation 
of software.  
Page, Larry, Sergey Brin, Rajeev Motwani and Terry 
Winograd, 1999. The PageRank citation ranking: 
Bringing order to the Web, Technical Report, Stan-
ford University. 
Qureshi, M. Atif, Arjumand Younus and Francisco 
Rojas, 2010. Analyzing Web Crawler as Feed For-
ward Engine for Efficient Solution to Search Prob-
lem in the Minimum Amount of Time through a 
Distributed Framework. In Proceedings of 1
st 
In-
ternational Conference on Information Science and 
Applications, IEEE Publications. 
Sifry, David, 2009 Sifry?s Alerts. 
http://www.sifry.com/alerts/
 
Sorapure, Madeleine. 2003. Screening moments, 
scrolling lives: Diary writing on the web. Biogra-
phy: An Interdisciplinary Quarterly, 26(1), 1-23.  
62
Proceedings of the Fifth Law Workshop (LAW V), pages 143?151,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Annotating Events, Temporal Expressions and Relations in Italian: 
the It-TimeML Experience for the Ita-TimeBank 
 
 
Tommaso Caselli Valentina Bartalesi Lenzi Rachele Sprugnoli 
ILC ?A.Zampolli? - CNR  
Via G. Moruzzi, 1  
56124 Pisa 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
caselli@ilc.cnt.it bartalesi@celct.it sprugnoli@celct.it 
Emanuele Pianta Irina Prodanof 
CELCT 
Via della Cascata 56/c  
38123 Povo (TN) 
ILC ?A.Zampolli? - CNR  
Via G. Moruzzi, 1  
56124 Pisa 
pianta@fbk.eu prodanof@ilc.cnr.it 
 
Abstract 
This paper presents the annotation 
guidelines and specifications which have 
been developed for the creation of the 
Italian TimeBank, a language resource 
composed of two corpora manually 
annotated with temporal and event 
information. In particular, the adaptation 
of the TimeML scheme to Italian is 
described, and a special attention is 
given to the methodology used for the 
realization of the annotation 
specifications, which are strategic in 
order to create good quality annotated 
resources and to justify the annotated 
items. The reliability of the It-TimeML 
guidelines and specifications is 
evaluated on the basis of the results of 
the inter-coder agreement performed 
during the annotation of the two corpora. 
? Introduction 
In recent years a renewed interest in temporal 
processing has spread in the NLP community, 
thanks to the success of the TimeML annotation 
scheme (Pustejovsky et al, 2003a) and to the 
availability of annotated resources, such as the 
English and French TimeBanks (Pustejovsky et 
al., 2003b; Bittar, 2010) and the TempEval 
corpora (Verhagen et al, 2010). 
The ISO TC 37 / SC 4 initiative 
(?Terminology and other language and content 
resources?) and the TempEval-2 contest have 
contributed to the development of TimeML-
compliant annotation schemes in languages 
other than English, namely Spanish, Korean, 
Chinese, French and Italian. Once the 
corresponding corpora will be completed and 
made available, the NLP community will benefit 
from having access to different language 
resources with a common layer of annotation 
which could boost studies in multilingual 
temporal processing and improve the 
performance of complex multilingual NLP 
systems, such as Question-Answering and 
Textual Entailment. 
This paper focuses on the annotation 
guidelines and specifications which have been 
developed for the creation of the Italian 
TimeBank (hereafter, Ita-TimeBank). The 
distinction between annotation guidelines and 
annotation specifications is of utmost 
importance in order to distinguish between the 
abstract, formal definition of an annotation 
scheme and the actual realization of the 
annotated language resource. In addition to this, 
documenting the annotation specification 
facilitates the reduplication of annotations and 
justify the annotated items. 
The paper is organized as follows: Section 2 
will describe in detail specific issues related to 
the temporal annotation of Italian for the two 
main tags of the TimeML annotation scheme, 
143
namely <EVENT> and <TIMEX3>. Section 3 
will present the realization of the annotation 
specifications and will document them. Section 
4 focuses on the evaluation of the annotation 
scheme on the Ita-TimeBank, formed by two 
corpora independently realized by applying the 
annotation specifications. Finally, in Section 5 
conclusions and extensions to the current 
annotation effort will be reported. 
Notice that, for clarity's sake, in this paper the 
examples will focus only on the tag (or attribute 
or link) under discussion. 
? It-TimeML: Extensions and 
Language Specific Issues 
Applying an annotation scheme to a language 
other than the one for which it was initially 
developed, requires a careful study of the 
language specific issues related to the linguistic 
phenomena taken into account (Im et al, 2009; 
Bittar, 2008). 
TimeML focuses on Events (i.e. actions, 
states, and processes - <EVENT> tag), 
Temporal Expressions (i.e. durations, calendar 
dates, times of day and sets of time - 
<TIMEX3> tag), Signals (e.g. temporal 
prepositions and subordinators - <SIGNAL> 
tag) and various kind of dependencies between 
Events and/or Temporal Expressions (i.e. 
temporal, aspectual and subordination relations - 
<TLINK>, <ALINK> and <SLINK> tags 
respectively). 
An ISO language-independent specification 
of TimeML is under development but it is still 
in the enquiry stage1. For this reason, in the 
following subsections we will mostly compare 
the Italian annotation guidelines with the latest 
version of the English annotation guidelines 
(TimeML Working group, 2010), focusing on 
the two main tags, i.e <EVENT> and 
<TIMEX3>, in Italian. 
2.1 The <EVENT> tag 
The <EVENT> tag is used to mark-up instances 
of eventualities (Bach, 1986). This category 
comprises all types of actions (punctual or 
durative) and states as well. With respect to 
                                                          
1
http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalog
ue_detail.htm?csnumber=37331 
previous annotations schemes (Katz and Arosio, 
2001, Filatova and Hovy, 2001, Setzer and 
Gaizauskas, 2001 among other), TimeML 
allows for annotating as Events not only verbs 
but also nouns, adjectives and prepositional 
phrases. 
In the adaptation to Italian, two annotation 
principles adopted for English, that is an 
orientation towards surface linguistic 
phenomena and the notion of minimal chunk for 
the tag extent, have been preserved without 
major modifications. The main differences with 
respect to the English version rely i.) in the 
attribute list; and ii.) in the attributes values. 
In Italian 12 core attributes apply with respect 
to the 10 attributes in English. The newly 
introduced attributes are MOOD and VFORM 
which capture key distinctions of the Tense-
Mood-Aspect (TMA) system of the Italian 
language. These two attributes are common to 
other languages, such as Spanish, Catalan, 
French and Korean. 
The MOOD attribute captures the contrastive 
grammatical expression of different modalities 
of presentation of an Event when realized by a 
verb. Annotating this attribute is important since 
grammatical modality has an impact on the 
identification of temporal and subordinating 
relations, and on the assessment of 
veridicity/factivity values. Mood in Italian is 
expressed as part of the verb morphology and 
not by means of modal auxiliary verbs as in 
English (e.g. through the auxiliary ?would?),. 
Thus, the solution to deal with this phenomenon 
adopted for English TimeML (where the main 
verb is annotated with the attribute 
MODALITY=?would?, see below) is not 
applicable in Italian unless relevant information 
is lost. The values of the MOOD attribute, as 
listed below, have been adapted to Italian and 
extended with respect to those proposed in the 
ISO-TimeML specification: 
 
? NONE: it is used as the default value and 
corresponds to the Indicative mood: 
(1.) Le forze dell?ordine hanno <EVENT 
? mood="NONE"> schierato </EVENT> 
3.000 agenti. [The police has deployed 
3,000 agents.] 
 
144
? CONDITIONAL: it signals the conditional 
mood which is used to speak of an Event 
whose realization is dependent on a certain 
condition, or to signal the future-in-the-
past: 
(2.) <EVENT ... mood="COND"> 
Mangerei </EVENT> del pesce. [I would 
eat fish.] 
  
? SUBJUNCTIVE: it has several uses in 
independent clauses and is required for 
certain types of dependent clauses. 
(3.) Voglio che tu te ne <EVENT ? 
mood="SUBJUNCTIVE">vada</EVENT> 
[I want you to go.] 
  
? IMPERATIVE: it is used to express direct 
commands or requests, to signal a 
prohibition, permission or any other kind of 
exhortation. 
 
The attribute VFORM is responsible for 
distinguishing between non-finite and finite 
forms of verbal Events. Its values are: 
 
? NONE: it is the default value and signals 
finite verb forms: 
(4.) Le forze dell?ordine hanno <EVENT 
? vForm="NONE">schierato</EVENT> 
3.000 agenti. [The police has deployed 
3,000 agents.] 
 
? INFINITIVE: for infinitive verb forms: 
(5.) Non ? possibile <EVENT ? 
vForm=''INFINITIVE''>viaggiare</EVEN
T>. [It?s not possible to travel.] 
 
? GERUND: for gerundive verb forms: 
(6.) Ha evitato l'incidente <EVENT ? 
vForm=''GERUND''> andando </EVENT> 
piano. [Driving slowly, he avoided the 
incident.] 
 
? PARTICIPLE: for participle verb forms: 
(7.) <EVENT ? vForm=?PARTICIPLE?> 
Vista </EVENT> Maria, se ne and?. 
[Having seen Maria, he left.] 
 
As for attribute values, the most important 
changes introduced for Italian in comparison 
with the English TimeML, are related to the 
ASPECT and MODALITY attributes. 
The ASPECT attribute captures standard 
distinctions in the grammatical category of 
aspect or Event viewpoint (Smith, 1991). In 
English TimeML it has the following values: i.) 
PROGRESSIVE; ii.) PERFECTIVE; iii.) 
PERFECTIVE_PROGRESSIVE, or iv.) NONE. 
The main differences with respect to the English 
guidelines concern the following points:  
i.) the absence of the value 
PERFECTIVE_PROGRESSIVE and  
ii.) the presence of the value 
IMPERFECTIVE, which is part of the ISO 
TimeML current definition.  
These differences are due to language specific 
phenomena related to the expression of the 
grammatical aspect in Italian and English and to 
the application of the TimeML surface oriented 
annotation philosophy. In particular, the 
assignment of the aspectual values is strictly 
determined by the verb surface forms. For 
instance, in English the verb form ?is teaching? 
requires the PROGRESSIVE value. On the 
other hand, the Italian counterpart of ?is 
teaching? can be realized in two ways: either by 
means of the simple present (insegna [s/he 
teaches]) or by means of a specific verbal 
periphrasis (sta insegnando [s/he is teaching]). 
In order to distinguish between these two verb 
forms, and to account also for other typical 
Romance languages tense forms, such as the 
Italian Imperfetto, the use of the additional 
IMPERFECTIVE value is necessary. Thus, 
insegna [s/he teaches], as well as the Imperfetto 
insegnava [s/he was teaching] are annotated as 
IMPERFECTIVE, whereas sta insegnando [s/he 
is teaching] is annotated as PROGRESSIVE. On 
the other hand, the absence of the 
PERFECTIVE_PROGRESSIVE value, used for 
English tense forms of the kind ?he has been 
teaching?, is due to the lack of Italian verb 
surface forms which may require its use. 
In English, modal verbs are not annotated as 
Events and the MODALITY attribute is 
associated to the main verb (the value of the 
attribute is the token corresponding to the modal 
verb). Unlike English modals, Italian modal 
verbs, such as potere [can/could; may/might], 
volere [want; will/would] and dovere 
[must/have to; ought to; shall/should], are to be 
145
considered similar to other lexical verbs in that 
it is possible to assign them values for tense and 
aspect. Consequently, each instance of Italian 
modal verbs will be annotated with the tag 
<EVENT>. The value of the MODALITY 
attribute is the lemma of the verb (e.g. dovere). 
A further language specific aspect concerns 
the annotation of verbal periphrases, that is 
special constructions with at least two verbs 
(and sometimes other words) that behave as a 
group like a single verb would. In Italian, it is 
possible to identify different instances of verbal 
periphrases, namely: 
 
? aspectual periphrases (example 8 below), 
which encode progressive or habitual 
aspect; 
? modal periphrases (example 9), which 
encode modality not realized by proper 
modal verbs;  
? phasal periphrases (example 10), which 
encode information on a particular phase in 
the description of an Event. 
 
Following Bertinetto (1991), in the last two 
cases, i.e. modal periphrases and phasal 
periphrases, both verbal elements involved 
should be annotated, while in the case of the 
aspectual periphrasis only the main verb (verb 
head) has to be marked; e.g.: 
(8.) Maria stava <EVENT ? 
ASPECT=?PROGRESSIVE?> mangiando. 
[Maria was eating] 
(9.) Il compito di matematica <EVENT ... 
MODALITY=?ANDARE?> va </EVENT> 
<EVENT ... > svolto </EVENT> per domani. 
[Maths exercises must be done for tomorrow]  
(10.) I contestatori hanno <EVENT ... 
CLASS=?ASPECTUAL?> iniziato </EVENT> 
a <EVENT> lanciare </EVENT> pietre. 
[Demonstrators started to throw stones.] 
Similarly to what proposed for English, in 
presence of multi-tokens realization of Events, 
two main annotation strategies have been 
followed: 
 
? in case the multi-token Event expression 
corresponds to an instance of a collocation 
or of an idiomatic expression, then only the 
head (verbal, nominal or other) of the 
expression is marked up;  
? in case the multi-token Event is realized by 
light verb expressions, then two separate 
<EVENT> tags are to be created both for 
the verb and the nominal/prepositional 
complement.  
2.2 The <TIMEX3> tag  
The TIMEX3 tag relies on and is as much 
compliant as possible with the TIDES TIMEX2 
annotation. The Italian adaptation of this 
annotation scheme is presented in Magnini et al 
(2006). The only difference concerns the 
annotation of articulated prepositions which are 
annotated as signals, while in the TIMEX2 
specifications they are considered as part of the 
textual realization of Temporal Expressions: 
(11a.) <TIMEX2 ?> nel 2011 </TIMEX2> 
[in 2011] 
(11b.) <SIGNAL ?> nel </SIGNAL> 
<TIMEX3?>2011</TIMEX3> [in 2011] 
On the other hand, with respect to the 
TIMEX3 annotation of other languages such as 
English, we decided to follow the TIMEX2 
specification by annotating many adjectives as 
Temporal Expressions (e.g. recente [recent], ex 
[former]) and including modifiers like che 
rimane in l?anno che rimane [the remaining 
year] into the extent of the TIMEX3 tag since it 
is essential for the normalization of temporal 
expressions. 
3 From Annotation Guidelines to 
Specifications ?
As already stated, the annotation guidelines 
represent an abstract, formal level of description 
which, in this case, is mainly based on a detailed 
study of the relevant linguistic levels. Once the 
guidelines are applied to real language data, 
further issues arise and need to be tackled. This 
section focuses on a method for developing 
annotation specifications. Annotation 
specifications are to be seen as the actual 
realization of the annotation guidelines. The 
identification and distinction of annotation 
guidelines from annotation specification is of 
major importance as it is to be conceived as a 
new level of Best Practice for the creation of 
146
semantically annotated Language Resources 
(Calzolari and Caselli, 2009). 
The process of realization of the annotation 
specifications is strategic both to realize good 
quality annotated resources and to justify why 
certain textual items have to be annotated. As 
for the It-TimeML experience we will illustrate 
this process by making reference and reporting 
examples for two tags, namely for the 
<EVENT> and the <TLINK> tags. 
As a general procedure for the development 
of the annotation specifications, we have taken 
inspiration from the DAMSL Manual (Core and 
Allen, 1997). Different decision trees have been 
created for each task. For instance, for the 
annotation of the <EVENT> tag, four different 
decision trees have been designed for each POS 
(i.e. nouns, verbs, adjectives and prepositional 
phrases) which could be involved in the 
realization of an Event. In particular, the most 
complex decision tree is that developed for noun 
annotation. The identification of the eventive 
reading of nouns has been formalized into a 
discrimination process of different properties: 
firstly superficial properties are taken into 
consideration, i.e. whether a morphologically 
related verb exists or not, and whether the noun 
co-occurs with special verb predicates (for 
instance aspectual verbs such as iniziare [to 
start] or light verbs such as fare [to do]); then, 
deeper semantic properties are analyzed, which 
involve other levels such as word sense 
disambiguation and noun classification (e.g. 
whether the noun is a functional or an 
incremental one). 
Other decision trees have been improved to 
avoid inconsistencies in Event classification. 
For instance, the identification of Reporting 
Events showed to be problematic because of the 
vague definition adopted in the guidelines. A 
Reporting Event is a giving information speech 
act in which a communicator conveys a message 
to an addressee. To help annotators in deciding 
whether an event is a Reporting one, the 
annotation specifications suggest to rely on 
FrameNet as a starting point (Baker, et al 
1998). More specifically, an Italian lexical unit 
has been classified as Reporting if it is the 
translation equivalent of one of the lexical units 
assigned to the Communication frame, which 
has Message as a core element. Among the 
frames using and inherited from the 
Communication frame, only the ones having the 
Message as a core element and conveying a 
giving information speech act have been 
selected and the lexical units belonging to them 
have been classified as Reporting Events: e.g. 
urlare [to scream] from the 
Communication_noise frame, sottolineare [to 
stress] from the Convey_importance frame, 
dichiarare [to declare] from the Statement 
frame. 
Similarly, for the identification of TLINKs, a 
set of decision trees has been developed to 
identify the conditions under which a temporal 
relation is to be annotated and a method to 
decide the value of the reltype attribute. For 
instance, the annotation of temporal relations 
between nominal Events and Temporal 
Expressions in the same sentence is allowed 
only when the Temporal Expression is realized 
either by an adjective or a prepositional phrase 
of the form ''di (of) + TEMPORAL 
EXPRESSION'' e.g.: 
(12.) La <EVENT eid=''e1'' ... > riunione 
</EVENT> <SIGNAL sid=''s1'' ... > di 
</SIGNAL> <TIMEX3 tid=''t1'' ... > ieri 
</TIMEX3> [yesterday meeting] 
<TLINK lid=''l1'' eventInstanceID=''e01'' 
relatedToTime=''t01'' signalID="s1" 
relType=''IS_INCLUDED''/> 
In addition, decision trees based on the idea 
that signals provide useful information to 
TLINK classification have been used to assign 
the reltype value to TLINKs holding between a 
duration and an Event. For example, the pattern 
?EVENT + tra (in) + DURATION? identifies 
the value AFTER, while the pattern ?EVENT + 
per (for) + DURATION? is associated with the 
value MEASURE. 
(13.) Il pacco <EVENT eid=''e1'' ... >arriver? 
</EVENT> <SIGNAL sid=''s1'' ... > tra 
</SIGNAL> <TIMEX3 tid=''t1'' ... > due giorni 
</TIMEX3> [the package will arrive in two 
days] 
<TLINK lid=''l1'' eventInstanceID=''e1'' 
relatedToTime=''t1'' signalID="s1" 
relType=''AFTER?/> 
(14.) Sono stati <EVENT eid=''e1'' ... > 
sposati </EVENT> <SIGNAL sid=''s1'' ... > per 
</SIGNAL> <TIMEX3 tid=''t1'' ... > dieci anni 
147
</TIMEX3> [they have been married for ten 
years] 
<TLINK lid=''l1'' eventInstanceID=''e1'' 
relatedToTime=''t1'' signalID="s1" 
relType=''MEASURE?/> 
The advantages of this formalization are 
many. The impact of the annotators' subjectivity 
is limited, thus reducing the risk of 
disagreement. Moreover, trees can then be 
easily used either as features for the 
development of a automatic learner or as 
instructions in a rule-based automatic annotation 
system. 
? Evaluating Annotations 
Two corpora have been developed in parallel 
following the It-TimeML annotation scheme, 
namely the CELCT corpus and the ILC corpus. 
Once these two corpora will be completed and 
released, they will form the Italian TimeBank 
providing the NLP community with the largest 
resource annotated with temporal and event 
information (more than 150K tokens). 
In this section, the two corpora are briefly 
described and the results of the inter-coder 
agreement (Artstein and Poesio, 2008) achieved 
during their annotation are compared in order to 
evaluate the quality of the guidelines and of the 
resources. 
The CELCT corpus has been created within 
the LiveMemories project2 and it consists of 
news stories taken from the Italian Content 
Annotation Bank (I-CAB, Magnini et al, 
2006). More than 180,000 tokens have been 
annotated with Temporal Expressions and 
more than 90,000 tokens have been annotated 
also with Events, Signals and Links. The 
Brandeis Annotation Tool3 (BAT) has been 
used for the pilot annotation and for the 
automatic computation of the inter-coder 
agreement on the extent and the attributes of 
Temporal Expressions, Events and Signals. 
After the pilot annotation, the first prototype of 
the CELCT Annotation Tool (CAT) has been 
used to perform the annotation and to compute 
the inter-coder agreement on Links. For what 
concern the annotation effort, the work on 
                                                          
2
 http://www.livememories.org 
3
 http://www.timeml.org/site/bat/ 
Temporal Expressions, Events and Signals 
involved 2 annotators while 3 annotators have 
been engaged in the annotation of Links. The 
annotation started in January 2010 and required 
a total of 1.3 person/years. Table 1 shows the 
total number of annotated markables together 
with the results of the inter-coder agreement on 
tag extent performed by two annotators on a 
subset of the corpus of about four thousand 
tokens. For the annotation of Event and Signal 
extents, statistics include average precision and 
recall and Cohen? kappa, while the Dice 
Coefficient has been computed for the extent of 
Links and Temporal Expressions. 
 
Markable # Agreement 
TIMEX3 4,852 Dice=0.94 
EVENT 17,554 K=0.93 P&R=0.94 
SIGNAL 2,045 K=0.88 P&R=0.88  
TLINK 3,373 Dice=0.86 
SLINK 3,985 Dice=0.93 
ALINK 238 Dice=0.90 
Table 1: Annotated markables and results of 
the inter-coder agreement on tag extent4 
 
Table 2 provides the value of Fleiss? kappa 
computed for the annotation of Temporal 
Expression, Event and Link attributes. 
 
Tag and attribute Agreement-Kappa 
TIMEX3.type  1.00 
TIMEX3.value 0.92 
TIMEX3.mod 0.89 
EVENT.aspect  0.96  
EVENT.class  0.87  
EVENT.modality  1.00  
EVENT.mood  0.90  
EVENT.polarity  1.00  
EVENT.pos  1.00  
EVENT.tense  0.94  
EVENT.vform  0.98  
TLINK.relType 0.88 
SLINK.relType 0.93 
ALINK.relType 1.00 
Table 2: Inter-coder agreement on 
attributes 
                                                          
4 Please note that the number of annotated Temporal 
Expressions is calculated on a total of 180,000 tokens, 
while the number of Events, Signals and Links is 
calculated on more than 90,000 tokens. 
148
The ILC corpus is composed of 171 
newspaper stories collected from the Italian 
Syntactic-Semantic Treebank, the PAROLE 
corpus and the web for a total of 68,000 
tokens (40,398 tokens are freely available, the 
remaining are available with restrictions). The 
news reports were selected to be comparable 
in content and size to the English TimeBank 
and they are mainly about international and 
national affairs, political and financial subject. 
The annotation of Temporal Expressions, 
Event extents and Signals has been completed 
while the annotation of Event attributes and 
LINKs is a work in progress. A subset of the 
corpus has been used as data set in the 
TempEval-2 evaluation campaign organized 
within SemEval-2 in 2010. So far the 
annotation has been performed thanks to eight 
voluntary students under the supervision of 
two judges using BAT. The annotation started 
in March 2009 and is requiring a total of 3 
person/years. Table 3 reports the total number 
of Temporal Expressions, Events, Signals and 
TLINKs together with the results of the inter-
coder agreement on tag extent performed on 
about 30,000 tokens. To measure the 
agreement on tag extents, average precision 
and recall and Cohen? kappa have been 
calculated. The annotation of Temporal Links 
has been divided into three subtasks: the first 
subtask is the relation between two Temporal 
Expressions, the second is the relation 
between an Event and a Temporal Expression, 
the third regards the relation between two 
Events. 
 
Markable # Agreement 
TIMEX3 2,314 K=0.95 P&R= 0.95 
EVENT 10,633 K=0.87 P&R= 0.86 
SIGNAL 1,704 K=0.83 P&R= 0.84 
 
T
L
I
N
K 
TIMEX3?
TIMEX3 
353 K=0.95 
EVENT?
TIMEX3 
512 K=0.87 
EVENT?
EVENT 
1,014 in progress 
Table 3: Annotated markables and results of 
the inter-coder agreement on tag extent 
 
The values of Fleiss? kappa computed for 
the assignment of attribute values are 
illustrated in Table 4. 
 
Tag and attribute Agreement ? Kappa 
TIMEX3.type  0.96 
TIMEX3.value 0.96 
TIMEX3.mod 0.97 
EVENT.aspect  0.93  
EVENT.class  0.82  
EVENT.modality  0.92  
EVENT.mood  0.89  
EVENT.polarity  0.75  
EVENT.pos  0.95  
EVENT.tense  0.97  
EVENT.vform  0.94  
TLINK.relType in progress 
Table 4: Annotated TLINKs and results of the 
inter-coder agreement 
 
Given the data reported in the above tables, 
it is possible to claim that the results of the 
inter-coder agreement are good and 
comparable beyond the different annotation 
method used to develop the two corpora. So 
far, the ILC corpus has been annotated 
without time constraints by several annotators 
with varying backgrounds in linguistics using 
BAT. With this web-based tool, each file has 
been assigned to many annotators and an 
adjudication phase on discrepancies has been 
performed by an expert judge. As required by 
BAT, the annotation has been divided into 
many annotation layers so each annotator 
focused only on a specific set of It-TimeML 
tags. On the other hand, few expert annotators 
have been involved in the development of the 
CELCT corpus interacting and negotiating 
common solutions to controversial 
annotations. With respect to BAT, the CELCT 
Annotation Tool is stand-alone and it does not 
require neither the parallel annotation of the 
same text, nor the decomposition of 
annotation tasks allowing to have flexibility in 
the annotation process and a unitary view of 
all annotation layers. These features are 
helpful when working with strict project 
deadlines. 
A comparison with the inter-coder agreement 
achieved during the annotation of the English 
TimeBank 1.2 (Pustejovsky et al, 2006a), 
shows that the scores obtained for the CELCT 
149
and the ILC corpora are substantially higher in 
the following results: (i) average precision and 
recall on the identification of tag extent (e.g. 
0.83 vs. 0.95 of ILC Corpus and 0.94 of CELCT 
Corpus for TIMEX3; 0.78 vs. 0.87 of ILC 
Corpus and 0.93 of CECLT Corpus); (ii) kappa 
score on Event classification (0.67 vs. 0.82 of 
ILC Corpus and 0.87 of the CELCT Corpus); 
(iii) kappa score on TLINK classification (0.77 
vs. 0.86 of CELCT Corpus). 
The similarity of the agreement results among 
the three resources and the improvement of the 
scores obtained on the CELCT and the ILC 
corpora with respect to the English TimeBank 
1.2, can be taken as an indication of the quality 
and coverage of the It-TimeML annotation 
guidelines and specifications. Annotators 
showed to perform consistently demonstrating 
the reliability of the annotation scheme. 
? Conclusions and Future Works 
This paper reports on the creation of a new 
semantic resource for Italian which has been 
developed independently but with a joint effort 
between two different research institutions. The 
Ita-TimeBank will represent a large corpus 
annotated with information for temporal 
processing which can boost the multilingual 
research in this field and represent a case study 
for the creation of semantic annotated resources. 
One of the most interesting point of this work 
is represented by the methodology followed for 
the development of the corpora: in addition to 
the guidelines, annotation specifications have 
been created in order to report in detail the 
actual choices done during the annotation. This 
element should be pushed forward in the 
community as a new best practice for the 
creation of good quality semantically annotated 
resources. 
The results obtained show the reliability of 
the adaptation of the annotation guidelines to 
Italian and of the methodology used for the 
creation of the resources. 
Future works will concentrate in different 
directions, mainly due to the research interests 
of the two groups which have taken part to this 
effort but they will be coordinated. 
An interesting aspect which could be 
investigated is the annotation of the anaphoric 
relations between Events. This effort could be 
done in a more reliable way since the primary 
linguistic items have been already annotated. 
Moreover, this should boost research in the 
development of annotation schemes which could 
be easily integrated with each other without 
losing descriptive and representational 
information for other language phenomena. 
Another topic to deepen regards the definition 
of the appropriate argument structure in It-
TimeML in order to annotate relations between 
entities (e.g. persons and organizations) and 
Events in which they are involved (Pustejovsky 
et al, 2006b). 
As regards the distribution of the Ita-
TimeBank, the resource will soon be available 
in an in-line format. In order to integrate the 
temporal annotation with other linguistic 
annotations, a standoff version of the Ita-
TimeBank needs to be developed. When this is 
made available, we plan to merge the manual 
annotation of temporal and event information 
with other types of linguistic stand-off 
annotations (i.e. tokenization, lemma, PoS, 
multi-words, various kinds of named entities) 
which are already available for the I-CAB 
corpus.  
In order to encourage research on systems 
capable of temporal inference and event-based 
reasoning, the Ita-TimeBank could be used as 
gold standard within specific evaluation 
campaigns as the next TempEval initiative. 
Finally, the use of crowdsourcing will be 
explored to reduce annotation effort in terms of 
financial cost and time. The most difficult 
challenge to face will be the splitting of a 
complicated annotation scheme as It-TimeML 
into simple tasks which can be effectively 
performed by not expert contributors. 
Acknowledgments 
The development of the CELCT corpus has 
been supported by the LiveMemories project 
(Active Digital Memories of Collective Life), 
funded by the Autonomous Province of Trento 
under the Major Projects 2006 research 
program. We would like to thank Alessandro 
Marchetti, Giovanni Moretti and Marc 
Verhagen who collaborated with us in 
processing and annotating the CELCT corpus. 
150
References 
Andr? Bittar. 2008. Annotation des informations 
temporelles dans des textes en fran?ais,. In 
Proceedings of RECITAL 2008, Avignon, France. 
Andr? Bittar. 2010. Building a TimeBank for French: 
A Reference Corpus Annotated According to the 
ISO-TimeML Standard. PhD Thesis. 
Andrea Setzer and Robert Gaizauskas.2001. A Pilot 
Study On Annotating Temporal Relations In Text. 
In: Proceedings of the ACL 2001 Workshop on 
Temporal and Spatial Information Processing. 
Bernardo Magnini, Emanuele Pianta, Christian 
Girardi, Matteo Negri, Lorenza Romano, Manuela 
Speranza, Valentina Bartalesi Lenzi and Rachele 
Sprugnoli. 2006. I-CAB: the Italian Content 
Annotation Bank. In Proceedings of LREC 2006, 
Genova, Italy. 
Bernardo Magnini, Matteo Negri, Emanuele Pianta, 
Manuela Speranza, Valentina Bartalesi Lenzi, and 
Rachele Sprugnoli. 2006. Italian Content 
Annotation Bank (I-CAB): Temporal Expressions 
(V.2.0). Technical Report, FBK-irst. 
Carlota S. Smith. 1991. The Parameter of Aspect. 
Kluwer, Dordrecht. 
Collin F., Baker, Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet project. In: 
Proceedings of the COLING-ACL, pages 86-90. 
Montreal, Canada. 
Elena Filatova and Eduard Hovy. 2001. Assigning 
Time-Stamps To Event-Clauses. In: Proceedings 
of the ACL 2001 Workshop on Temporal and 
Spatial Information Processing. 
Emmon Bach. 1986. The algebra of events. 
Linguistics and Philosophy, 9, 5?16. 
Graham Katz and Fabrizio Arosio. 2001. The 
Annotation Of Temporal Information In Natural 
Language Sentences. In: Proceedings of the ACL 
2001 Workshop on Temporal and Spatial 
Information Processing. 
ISO: Language Resource Management ? Semantic 
Annotation Framework (SemAF) - Part 1: Time 
and Events. Secretariat KATS, August 2007. ISO 
Report ISO/TC37/SC4 N269 version 19 (ISO/WD 
24617-1). 
James Pustejovsky, Jessica Littman and Roser Saur?. 
2006b. Argument Structure in TimeML. In: 
Graham Katz, James Pustejovsky and Frank 
Schilder (eds.) Dagstuhl Seminar Proceedings. 
Internationales Begegnungs- und 
Forschungszentrum (IB-FI), Schloss Dagstuhl, 
Germany. 
James Pustejovsky, Jessica Littman, Roser Saur?, and 
Marc Verhagen. 2006a. TimeBank 1.2 
Documentation. 
http://timeml.org/site/timebank/documentation-
1.2.html 
James Pustejovsky, Jos? Casta?o, Robert Ingria, 
Roser Saur?, Robert Gaizauskas, Andrea Setzer 
and Graham Katz. 2003a. TimeML: Robust 
Specification of Event and Temporal Expressions 
in Text. In: Proceedings of IWCS-5, Fifth 
International Workshop on Computational 
Semantics. 
James Pustejovsky, Patrick Hanks, Roser, Saur?, 
Andrew See, Robert Gaizauskas, Andrea Setzer, 
Dragomir Radev, Beth Sundheim, David Day,Lisa 
Ferro, and Marcia Lazo. 2003b. The TIMEBANK 
corpus. In: Proceedings of Corpus Linguistics 
2003, pages 647-656. 
Marc Verhagen, Roser Saur?, Tommaso Caselli and 
James Pustejovsky. 2010. SemEval-2010 Task 13: 
TempEval-2. In: Proceedings of the 5th 
International Workshop on Semantic Evaluation. 
Mark G. Core and James F. Allen. 1997. Coding 
Dialogs with the DAMSL Annotation Scheme. In: 
Working Notes of AAAI Fall Symposium on 
Communicative Action in Humans and Machines. 
Nicoletta Calzolari, and Tommaso Caselli 2009. 
Short Report on the FLaReNet / SILT Workshop 
and Panel on Semantic Annotation, TR-ILC-CNR. 
Pier Marco Bertinetto. 1991. Il verbo. In: R. L. and 
G. Salvi (eds.) Grande Grammatica Italiana di 
Consultazione, volume II, pages 13-161. Il 
Mulino. 
Ron Artstein and Massimo Poesio. Inter-coder 
agreement for computational linguistics. 
Computational Linguistics, pages 555?596, 2008. 
Seohyun Im, Hyunjo You, Hayun Jang, Seungho 
Nam, and Hyopil Shin. 2009. KTimeML: 
Specification of Temporal and Event Expressions 
in Korean Text. In: Proceedings of the 7th 
workshop on Asian Language Resources in 
conjunction with ACL-IJCNLP 2009, Suntec City, 
Singapore. 
TimeML Working Group. 2010. TimeML 
Annotation Guidelines version 1.3.Manuscript, 
Brandeis University. 
151
JEP-TALN-RECITAL 2012, Atelier DEFT 2012: D?fi Fouille de Textes, pages 15?24,
Grenoble, 4 au 8 juin 2012. c?2012 ATALA & AFCP
Key-concept extraction from French articles with KX
Sara Tonelli1 Elena Cabrio2 Emanuele Pianta1
(1) FBK, via Sommarive 18, Povo (Trento), Italy
(2) INRIA, 2004 Route des Lucioles BP93, Sophia Antipolis cedex, France
satonelli@fbk.eu, elena.cabrio@inria.fr, pianta@fbk.eu
R?SUM?
Nous pr?sentons une adaptation du syst?me KX qui accomplit l?extraction non supervis?e et
multilingue des mots-cl?s, pour l?atelier d??valuation francophone en fouille de textes (DEFT
2012). KX s?lectionne une liste de mots-cl?s (avec leur poids) dans un document, en combinant
des annotations linguistiques de base avec des mesures statistiques. Pour l?adapter ? la langue
fran?aise, un analyseur morphologique pour le Fran?ais a ?t? ajout? au syst?me pour d?river les
patrons lexicaux. De plus, des param?tres comme les seuils de fr?quence pour l?extraction de
collocations, et les index de relevance des concepts-cl?s ont ?t? calcul?s et fix?s sur le corpus
d?apprentissage. En concernant les pistes de DEFT 2012, KX a obtenu de bons r?sultats (Piste 1 -
avec terminologie : 0.27 F1 ; Piste 2 : 0.19 F1) en demandant un effort r?duit pour l?adaptation
du domaine et du langage.
ABSTRACT
We present an adaptation for the French text mining challenge (DEFT 2012) of the KX system
for multilingual unsupervised key-concept extraction. KX carries out the selection of a list of
weighted keywords from a document by combining basic linguistic annotations with simple
statistical measures. In order to adapt it to the French language, a French morphological
analyzer (PoS-Tagger) has been added into the extraction pipeline, to derive lexical patterns.
Moreover, parameters such as frequency thresholds for collocation extraction and indicators for
key-concepts relevance have been calculated and set on the training documents. In the DEFT
2012 tasks, KX achieved good results (i.e. 0.27 F1 for Task 1 - with terminological list, and 0.19
F1 for Task 2) with a limited additional effort for domain and language adaptation.
MOTS-CL?S : Extraction de mots-cl?s, patrons linguistiques, terminologie.
KEYWORDS: Key-concept extraction, linguistic patterns, terminology.
1 Introduction
Key-concepts are simple words or phrases that provide an approximate but useful characterization
of the content of a document, and offer a good basis for applying content-based similarity
functions. In general, key-concepts can be used in a number of interesting ways both for human
and automatic processing. For instance, a quick topic search can be carried out over a number
15
of documents indexed according to their key-concepts, which is more precise and efficient
than full-text search. Also, key-concepts can be used to calculate semantic similarity between
documents and to cluster the texts according to such similarity (Ricca et al, 2004). Furthermore,
key-concepts provide a sort of quick summary of a document, thus they can be used as an
intermediate step in extractive summarization to identify the text segments reflecting the content
of a document. (Jones et al, 2002), for example, exploit key-concepts to rank the sentences in a
document by relevance, counting the number of key-concept stems occurring in each sentence.
In the light of the increasing importance of key-concepts in several applications, from search
engines to digital libraries, a recent task for the evaluation of key-concept extraction was also
proposed at SemEval-2010 campaign (Kim et al, 2010)
In this work, we present an adaptation of the KX system for multilingual key-concept extraction
(Pianta et Tonelli, 2010) for the French text mining challenge (DEFT 2012) task. A preliminary
version of KX for French took part in the DEFT 2011 campaign on ?Abstract ? article matching?
(Tonelli et Pianta, 2011), and achieved good performances in both tracks (0.990 and 0.964 F1
respectively).
Compared to the previous version of KX, we have now integrated into the extraction pipeline a
French morphological analyzer (Chrupala et al, 2008). This allows us to exploit morphological
information while selecting candidate key-concepts, while in the version used at DEFT 2011 the
selection was made using regular expressions and black lists.
The paper is structured as follows : in Section 2 we detail the architecture of KX (i.e. our key-
concepts extraction tool), providing an insight into its parameters configuration. In Section 3 we
present the setting defined and adopted for the DEFT 2012 task, while in Section 4 we report the
system performances on the training and on the test sets. Finally, we draw some conclusions, and
discuss future improvements of our approach in Section 5.
2 Key-concept extraction with KX
This section describes in details the basic KX architecture for unsupervised key-concept extraction.
KX can handle texts in several languages (i.e. English, Italian, French, Finnish and Swedish), and
it is distributed with the TextPro NLP Suite1 (Pianta et al, 2008). KX architecture is the same
across all languages, except for the module selecting multiword expressions, that is based on PoS
tags (this is the only language-dependent part of the system). In order to perform this selection,
a morphological analyzer/PoS tagger has been integrated for each of the five languages, and
some selection rules have been manually defined. More details on the French rules are reported
in Section 2.2 and in Section 3.
2.1 Pre-processing of the reference corpus
If a domain corpus is available, the extraction of key-concepts from a single document can be
preceded by a pre-processing step, during which key-concepts are extracted from the corpus and
their inverse document frequency (IDF) at corpus level is computed by applying the standard
formula :
1
http://textpro.fbk.eu/
16
I DFk = log NDFk
where N is the number of documents in the corpus, and DFk is the number of documents in thecorpus that contain the key-concepts k. The I DF of a rare term tends to be high, while the I DF
of a frequent one is likely to be low. Therefore, I DF may be a good indicator for distinguishing
between common, generic words and specific ones, which are good candidates for being a
key-concept. For DEFT 2012, we have used as a reference corpus all the documents contained in
the training and in the test sets (468 documents in total).
2.2 Key-concept extraction
Figure 1 shows KX work-flow for the key-concept extraction process : starting from a document,
a list of key-concepts ranked by relevance is provided as the output of the system. The same
work-flow applies both to i) the extraction of key-concepts from a single document, and to ii) the
extraction of different statistics including IDF from a reference corpus, which can be optionally
used as additional information when processing a single document. For more information, see
above and Section 2.3.
FIG. 1 ? Key-concept extraction workflow with KX
As a first step, the system takes a document in input and tokenize the text. Then, all possible
n-grams composed by any token sequence are extracted, for instance ??clipse de soleil?, ?tous les?,
?ou chacun?. The user can set the max length of the selected n-grams : for DEFT 2012 we set such
length to six.
Then, from the n-gram list a sublist of multiword expressions (MWE) is derived, i.e. combinations
of words expressing a unitary concept, for example ?proc?s de travail? or ??conomie politique?.
17
In the selection step, the user can choose to rely only on local (document) evidence or to make
use also of global (corpus) evidence. As for the first case, a frequency threshold called MinDoc
can be set, which corresponds to the minimum number of occurrences of n-grams in the current
document. If a reference corpus is also available, another threshold can be added, MinCorpus,
which corresponds to the minimum number of occurrences of an n-gram in the corpus. KX marks
an n-gram in a document as a multiword term if it occurs at least MinCorpus times in the
corpus or at least MinDoc times in the document. The two parameters depend on the size of the
reference corpus and the document respectively. In our case, the corpus was the set of documents
used in the training and in the test set (see Section 2.1).
A similar, frequency-based, strategy is used to solve ambiguities in how sequences of contiguous
multiwords should be segmented. For instance, given the sequence ?retour des bonnes mani?res?
we need to decide whether we recognize ?retour des bonnes? or ?bonnes mani?res?. To this
purpose, the strength of each alternative MWE is calculated as follows, and then the stronger
one is selected.
St reng thcol loc = docF requenc y ? corpusF requenc y
In the next step, the single words and the MWEs are ranked by frequency to obtain a first list
of key-concepts. Thus, frequency is the baseline ranking parameter, based on the assumption
that important concepts are mentioned more frequently than less important ones. Frequency is
normalized by dividing the number of key-concept occurrences by the total number of tokens in
the current document.
As shown in Figure 1, the first key-concepts list is obtained by applying black and white lists
almost at every step of the process. A black list is applied to discard n-grams containing one of
the language-specific stopwords defined by the user, for example ?avons?, ?peut?, ?puis?, ?parce?.
Also single words corresponding to stopwords are discarded when the most frequent tokens
are included into the first key-concept list. For example, in French we may want to exclude all
key-concepts containing the words ?toi?, ?tr?s?, ?finalement?, etc.
When deriving multiword expressions (MWEs) from the n-gram list, KX applies another selection
strategy. This selection is crucial because only MWEs are selected as candidate key-concepts, since
they correspond to combinations of words expressing a unitary concept, for example ?r?gime de
despotisme familial? or ?reproduction mat?rielle?. The n-grams are analyzed with the Morfette
morphological analyzer (Chrupala et al, 2008) in order to select as multiword expressions only
the n-grams that match certain lexical patterns (i.e. part-of-speech). This is the so-called linguistic
filter. For example, one of the patterns admitted for 3-grams is the following :
[SP]? [O]? [SP]
This means that a 3-gram is a candidate multiword term if it is composed by a single or plural
noun (S and P respectively), followed by a preposition (defined as O), followed by another noun.
This is matched for example by the 3-gram ?proc?s [S] de [O] travail [S]?.
Finally, black and white lists can be manually compiled also for key-concepts, to define expressions
that should never be selected as relevant key-concepts, as well as terms that should always be
included in the key-concept rank. For example, the preposition ?de? is very frequent in documents,
so it can happen that it is selected as single-word key-concept. In order to avoid this, ?de? can be
included in the key-concept black list.
18
2.3 First key-concept ranking
Different techniques are used to re-rank the frequency-based list of key-concepts obtained in
the previous step according to their relevance. If a reference corpus is available, as in our case,
additional information can be used to understand which key-concepts are more specific to a
document, and therefore are more likely to be relevant for such document.
In order to find the best ranking mechanism, and to tailor it to the type of key-concepts we want
to extract, the following parameters can be set :
Key-concept IDF : This parameter takes into account the fact that, given a data collection, a
concept that is mentioned in many documents is less relevant to our task than a concept oc-
curring in few documents. To activate it, a reference corpus must undergo a pre-processing
step in which the key-concepts are extracted from each document in the corpus, and the
corresponding inverse document frequency (IDF) is computed, as described in Section 2.1.
When this parameter is activated, for each key-concept found in the current document, its
I DF computed over the reference corpus is retrieved and multiplied by the key-concept
frequency at document level.
Key-concept length : Number of tokens in a key-concept. Concepts expressed by longer phrases
are expected to be more specific, and thus more informative. When this parameter is
activated, the frequency is multiplied by the key-concept length. For example, if ?expression
verbale? has frequency 6 and ?expression verbale des ?motions? has frequency 5, the
activation of the key-concept length parameter gives ?expression verbale? = 6 * 2 = 12 and
?expression verbale des ?motions? = 5 * 4 = 20. In this way, the 4-gram is assigned a higher
ranking than the 2-gram.
Position of first occurrence : Important concepts are expected to be mentioned before less
relevant ones. If the parameter is activated, the frequency score will be multiplied by the
PosFact factor computed as :
PosFact =
DistF romEnd
Max Index
2
where Max Index is the length of the current document, and DistF romEnd is Max Index
minus the position of the first key-concept occurrence in the text.
A configuration file allows the user to independently activate such parameters. The key-concept
relevance is then calculated by multiplying the normalized frequency of a key-concept by the
score obtained by each active parameter. We eventually obtain a ranking of key-concepts ordered
by relevance. The user can also set the number of top ranked key-concepts to consider as best
candidates.
2.4 Final key-concept ranking
Section 2.3 described the first set of ranking strategies, that can be optionally followed by another
set of operations to adjust the preliminary ranking. Again, such operations can be independently
activated through a separate configuration file. The parameters have been introduced to deal
19
with the so-called nested key-concepts (Frantzi et al, 2000), i.e. those that appear within other
longer candidate key-concepts. After the first ranking, which is still influenced by the key-concept
frequency, nested (shorter) key-concepts tend to have a higher ranking than the containing
(longer) ones, because the former are usually more frequent than the latter. However, in some
settings, for example in scientific articles, longer key-concepts are generally preferred over
shorter ones because they are more informative and specific. In such cases, the user may want
to adjust the ranking in order to give preference to longer key-concepts and to reduce or set to
zero the score of nested key-concepts. These operations are allowed by activating the following
parameters :
Shorter concept subsumption : It happens that two concepts can occur in the key-concept list,
such that one is a specification of the other. Concept subsumption and boosting (see below)
are used to merge or rerank such couples of concepts. If a key-concept is (stringwise)
included in a longer key-concept with a higher frequency-based score, the score of the
shorter key-concept is transferred to the count of the longer one. For example, if ?expression
verbale? has frequency 4 and ?expression verbale des ?motions? has frequency 6, by activa-
ting this parameter the relevance of ?expression verbale des ?motions? is 6 + 4 = 10, while
the relevance of ?expression verbale? is set to zero. The idea behind this strategy is that
nested key-concepts can be deleted from the final key-concept list without losing relevant
information, since their meaning is nevertheless contained in the longer key-concepts.
Longer concept boosting : This parameter applies in case a key-concept is (stringwise) included
in a longer key-concept with a lower relevance. Its activation should better balance the
ranking in order to take into account that longer n-grams are generally less frequent, but
not less relevant, than shorter ones. The parameter is available in two different versions,
having different criteria for computing such boosting. With the first option, the average
score between the two key-concepts relevance is computed. Such score is assigned to the
less frequent key-concepts and subtracted from the frequency score of the higher ranked
one. With the second option, the longer key-concepts is assigned the frequency of the shorter
one. In none of the two variants key-concepts are deleted from the relevance list, as it
happens by activating the Shorter concept subsumption parameter.
For example, if ?expression verbale? has score 6 and ?expression verbale des ?motions? has
score 4, by activating the first option of this parameter the relevance of ?expression verbale?
becomes 6 - ((6 + 4) / 2) = 1, while the relevance of ?expression verbale des ?motions? is
set to 5, i.e. (6 + 4) / 2 .
With the second option, both the relevance of ?expression verbale des ?motions? and of
?expression verbale? is set to 6.
The examples above show that these parameters set by the user can change the output of the
ranking by deleting some entries and boosting some others. After applying one cycle of subsump-
tion/boosting, the order of the concepts can dramatically change, producing the conditions for
further subsumption/boosting of concepts. The user can set the number of iterations for the
application of this re-ranking mechanism, and each cycle increases the impact of the re-ranking
on the key-concept list. The parameters can be activated together and in different combinations.
If all parameters are set, the short concept subsumption procedure is applied first, then the longer
concept boosting is run on the output of the first re-ranking, so that the initial relevance-based
list goes through two reordering steps.
20
3 KX configuration for the DEFT 2012 task
As introduced before (Section 2.2), to port KX to the French language and, in particular, to adapt
it to the DEFT 2012 task, the Morfette morphological analyzer (Chrupala et al, 2008) has been
integrated into the system, to select as multiword expressions only the n-grams matching certain
lexical patterns (i.e. part-of-speech). Such lexical patterns are learned on the gold standard, and
manually formalized and added into the system as a linguistic filter. In order to speed up this
process, we took advantage of the set of lexical patterns defined for Italian, and we checked
if they could be applied also for French. Moreover, new patterns were added in compliance
with DEFT training data requirements. For example, the following n-grams have been added as
allowed patterns (i.e. candidate multiword terms) :
? 6-grams : [SP]-[O]-[SP]-[O]-[S]-[JK], where S and P correspond to singular or plural nouns,
O to the prepositions (also in combination with the article), and J and K to singular or plural
adjectives (e.g. ?soul?vement [S] des [O] M?tis [S] dans l? [O] Ouest [S] canadien [J]?) ;
? 5-grams : [SP]-[O]-[SP]-[O]-[P], (e.g. ?gestion [S] des [O] troupeaux [P] de [O] rennes [P]?) ;
? 4-grams : [SP]-[JK]-[0]-[S], (e.g. ?histoire [S] canonique [J] de la [0] traduction [S]?) ;
? 3-grams : [S]-[SP]-[JK], (e.g. ?fran?ais [S] langue [S] premi?re [J]?).
We compiled black lists both for common French stopwords (containing e.g. articles, prepositions,
a few numbers, and functional verbs) and stopphrases (prepositional structures such as ?au sujet
de?, ?en dehors de?, ?en face de?), since we do not want them to be selected as key-concepts.
As for the IDF value mentioned in Section 2.1, it has been computed for 86,419 key-concepts
extracted from DEFT 2012 training and test set. Among the key-concepts with the highest
IDF (i.e. best candidates for final selection), we find ?inversions culturelles?, ?hypertextualit??,
?am?nagement terminologique?. These are key-concepts that occur only in one document of the
reference corpus. Among the key-concepts with a low IDF, instead, we find very common terms
and expressions such as ?rapport?, ?partie? and ?exemple?, which are likely to be discarded as
key-concepts.
The standard KX architecture has also been adapted to one of the two tracks of DEFT 2012,
namely the one in which a terminological list was provided. For that track, the set of documents
to be processed was accompanied by a list of domain terminology. By comparing the gold key-
concepts in the training set with this list, we observed that all terms in the terminology were
also gold key-concepts. Therefore, we modified KX so that, in the final re-ranking, the candidate
key-concepts being present in the terminology list were significantly boosted. This adaptation
lead to an improvement of almost 0.8 P/R/F1 on the test set (see Section 4).
4 Evaluation
Since KX does not require supervision, we used the training set to identify the best parameter
setting, which was then applied in the test phase. The results obtained on the training and on the
test set are discussed in the following subsections.
21
4.1 System evaluation on the training set
We report in Table 1 the best parameter setting on the training documents. Note that the reported
evaluation measures have been computed using our own scorer, which counts as correct each
key-concept exactly matching with the gold standard (case-insensitive). The results reported for
the test set, instead, have been computed by the task organizers with another scorer, which may
apply a slightly different strategy.
We extracted for each document the top k key-concepts, with k being the number of key-
concepts assigned to each document in the training set (this number may vary from document to
document). For this reason, Precision and Recall are the same.
Task 1 : Task 2 :
with terminology w/o terminology
KX Parameters
1. MinCorpus 8 8
2. MinDoc 3 3
3. Use corpusId f Yes Yes
4. Multiply relevance by key-concept length Yes Yes
5. Consider position of first occurrence No No
6. Shorter concept subsumption No No
7. Longer concept boosting No No
8. Boost key-concepts in terminology list Yes No
P/R/F1 on training set F1 0.18 F1 0.15
TAB. 1 ? Best parameter combination for training set
The results obtained on the training set suggest that the key-concepts required in this task should
not be too specific, since the parameters aimed at preferring specific (i.e. longer) key-concepts
are not activated in the best performing setting (we refer to parameters n. 6 and 7 in the above
Table). Also the position of the first key-concept occurrence is not relevant, since the parameter
n. 5 is not part of the best setting. This is in contrast with KX setting used for Semeval 2010
(Pianta et Tonelli, 2010). In that case, boosting the relevance of specific key-concepts, and of
those occurring in the article abstract had a positive effect on the final performance. Note also
that the performance measured on French documents in DEFT is around 0.10 points lower
than that achieved at Semeval on English scientific articles. We believe that this is not due to
a different system performance on the two languages, but rather on the evaluation strategy,
because Semeval scorer required the key-concepts to be stemmed and took into account some
syntactic variations of the same key-concept (Kim et al, 2010).
4.2 System evaluation on the test set
For each task, we submitted three system runs, testing different parameter combinations. Specifi-
cally, for Task 1 (with terminological list), the three runs had the following configurations :
1. Parameter setting reported in Section 4.1 (with boosting of key-concepts in terminology
22
list) ;
2. Parameter setting as in Section 4.1 but Consider position of first occurrence activated (with
boosting of key-concepts in terminology list) ;
3. Parameter setting as in Section 4.1 but terminology list is not taken into account.
As for Task 2 (without terminological list), the three runs had the following configurations :
1. Parameter setting reported in Section 4.1 ;
2. Parameter setting as in Section 4.1 but Consider position of first occurrence activated ;
3. Parameter setting reported in Section 4.1 but system run only on article abstracts.
Task 1 : Task 2 :
with terminology w/o terminology
KX Run 1 0.2682 0.1880
KX Run 2 0.2737 0.1901
KX Run 3 0.1976 0.1149
TAB. 2 ? KX performance on test set
We decided to activate the parameter Consider position of first occurrence, even if it was not
part of the best performing setting in the training phase, because it achieved good results in
the Semeval 2010 challenge on English. The results confirm that, in both tasks, this yielded a
(limited) improvement.
In both tasks, the third run was used to exploit configurations that were not tested in the training
phase. In Task 1, the third run was obtained without taking into account the terminology list.
The difference in performance between Run 1 and Run 3 confirms that this information is indeed
very relevant. In Task 2, the third run concerned the extraction of key-concepts only from the
abstracts, and not from the whole articles. Also in this case, the initial hypothesis that the abstract
may contain all relevant key-concepts proved to be wrong.
At DEFT 2012, 10 teams submitted at least one run in Task 1, and 9 teams in Task 2. The best
performing run of KX was ranked 6th out of 10 in Task 1 and 5th out of 9 in Task 2. In Task 1
the mean F1 for the best submission of each team was 0.3575, the median was 0.3321 and the
standard deviation 0.2985, with system performances ranging from 0.0428 (lowest performance)
to 0.9488 (best run). In Task 2 the mean F1 for the best submission of each team was 0.2045,
the median was 0.1901 and the standard deviation 0.1522, with system performances ranging
from 0.0785 (lowest performance) to 0.5874 (best run).
These results show that the use of terminology significantly improves the overall system perfor-
mance, as confirmed in Table 2. However, KX seems to be more competitive in the second task
compared to other systems. This confirms that KX strength lies in its domain-independence and
in the fact that is does not require any additional information to achieve a good performance.
Furthermore, we believe that the second task is more realistic than the first one : in a real
application scenario, it is unlikely that a terminological list, containing only the key-concepts to
be identified, is actually available.
23
5 Conclusions
In this paper, we presented the French version of the KX system, and we described the experiments
we carried out for our participation at DEFT 2012. KX achieved good results with few adjustments
of the parameter setting and a limited additional effort for domain and language adaptation. Our
system requires no supervision and its English and Italian versions are distributed as a standalone
key-concept extractor. Its extension, which takes into account a reference terminological list,
proved to be effective and achieved a moderate improvement in the first task of the evaluation
challenge.
A limitation of our system is that it is not able to identify key-concepts that are not present
in the document. This kind of concepts amounted to around 20% of the gold key-concepts in
the training set, and this feature strongly affected the outcome of our evaluation. A strategy to
exploit external knowledge sources to extract common subsumers of the given key-concepts may
be investigated in the future.
Acknowledgements
The development of KX has been partially funded by the European Commission under the contract
number FP7-248594, PESCaDO project.
R?f?rences
CHRUPALA, G., DINU, G. et van GENABITH, J. (2008). Learning Morphology with Morfette. In
Proceedings of the 6th International Conference on Languages Resources and Evaluations (LREC
2008), Marrakech, Morocco.
FRANTZI, K., ANANIADOU, S. et MIMA, H. (2000). Automatic recognition of multi-word terms :
the C-value/NC-value. Journal of Digital Libraries, 3(2):115?130.
JONES, S., LUNDY, S. et PAYNTER, G. (2002). Interactive Document Summarisation Using Auto-
matically Extracted Keyphrases. In Proceedings of the 35th Hawaii International Conference on
System Sciences, Hawaii.
KIM, S. N., MEDELYAN, O., KAN, M.-Y. et BALDWIN, T. (2010). SemEval-2010 Task 5 : Automatic
keyphrase extraction from scientific articles. In Proceedings of SemEval 2010, Task 5 : Keyword
extraction from Scientific Articles, Uppsala, Sweden.
PIANTA, E., GIRARDI, C. et ZANOLI, R. (2008). The TextPro tool suite. In Proceedings of the 6th
Language Resources and Evaluation Conference (LREC), Marrakech, Morocco.
PIANTA, E. et TONELLI, S. (2010). KX : A flexible system for Keyphrase eXtraction. In Proceedings
of SemEval 2010, Task 5 : Keyword extraction from Scientific Articles, Uppsala, Sweden.
RICCA, F., TONELLA, P., GIRARDI, C. et PIANTA, E. (2004). An empirical study on keyword-based
web site clustering. In Proceedings of the 12th IWPC, Bari, Italy.
TONELLI, S. et PIANTA, E. (2011). Matching documents and summaries using key-concepts. In
Proceedings of DEFT 2011, Montpellier, France.
24
NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 40?48,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Making Readability Indices Readable
Sara Tonelli
FBK, Trento, Italy
satonelli@fbk.eu
Ke Tran Manh
Charles University, Prague, CR
ketranmanh@gmail.com
Emanuele Pianta
FBK, Trento, Italy
pianta@fbk.eu
Abstract
Although many approaches have been pre-
sented to compute and predict readability of
documents in different languages, the infor-
mation provided by readability systems often
fail to show in a clear and understandable way
how difficult a document is and which aspects
contribute to content readability. We address
this issue by presenting a system that, for a
given document in Italian, provides not only
a list of readability indices inspired by Coh-
Metrix, but also a graphical representation
of the difficulty of the text compared to the
three levels of Italian compulsory education,
namely elementary, middle and high-school
level. We believe that this kind of represen-
tation makes readability assessment more in-
tuitive, especially for educators who may not
be familiar with readability predictions via su-
pervised classification. In addition, we present
the first available system for readability as-
sessment of Italian inspired by Coh-Metrix.
1 Introduction
The task of readability assessment consists in quan-
tifying how difficult a text is for a reader. This kind
of assessment has been widely used for several pur-
poses, such as evaluating the reading level of chil-
dren and impaired persons and improving Web con-
tent accessibility for users with low literacy level.
While indices and methodologies for readabil-
ity assessment of English have been widely investi-
gated, and research on English readability has been
continuously progressing thanks to advances in psy-
cholinguistic research and in natural language pro-
cessing, only limited efforts have been made to ex-
tend current approaches to other languages. An
adaptation of the basic Flesch Index (Flesch, 1946)
exists for many languages, but only in few cases
more sophisticated approaches have been adopted,
taking into account recent studies on text cohesion,
readability and simplification.
With this work, we aim at bridging the gap be-
tween the standard approach to Italian readability
based on the Gulpease index (following the same
criteria of the Flesch Index) and the more advanced
approaches to readability currently available for En-
glish and based on psycholinguistic principles. In
particular, we present a set of indices for Ital-
ian readability covering different linguistics aspects,
from the lexical to the discourse level, which are in-
spired by Coh-Metrix (Graesser et al, 2004). We
make this analysis available online, but we differ-
entiate our service from that of Coh-Metrix1 in that
we provide a graphical representation of the aspects
affecting readability, comparing a document with
the average indices of elementary, middle and high-
school level texts. This makes readability analysis
really intuitive, so that a user can straightforwardly
understand how difficult a document is, and see if
some aspects (e.g. lexicon, syntax, discourse) affect
readability more than others.
Our research goals are: i) to analyze the adequacy
of the Gulpease index for discriminating between
the readability levels of texts used for teaching and
testing in the Italian school practice, ii) to implement
an adaptation of Coh-Metrix indices for Italian, iii)
to make the readability analysis available online and
1http://cohmetrix.memphis.edu
40
understandable to naive users.
2 Related work
The first formulas to automatically compute the dif-
ficulty of a text were devised for English, starting
from the Flesch Index (Flesch, 1946), followed by
the Gunning Fog (Gunning, 1952), the SMOG index
(McLaughlin, 1969) and the Fleisch-Kincaid (Kin-
caid et al, 1975). These metrics combine factors,
such as word and sentence length, that are easy to
compute and that should approximate the linguistic
elements that impact on readability. Similar indexes
have been proposed also for other languages such
as German (Bamberger and Vanecek, 1984), French
(Kandel and Moles, 1958) and Spanish (Huerta,
1959).
The first readability formula for Italian, the
Flesch-Vacca (Franchina and Vacca, 1986), was in-
troduced in the early seventies and was based on an
adaptation of the Flesch index (Flesch, 1946). How-
ever, it has been widely replaced by the Gulpease
index (Lucisano and Piemontese, 1988), which was
introduced in the eighties by the Gruppo Universi-
tario Linguistico Pedagogico (GULP) of the Univer-
sity of Rome. The Gulpease index takes into account
the length of a word in characters rather than in syl-
lables, which proved to be more reliable for assess-
ing the readability of Italian texts. The index ranges
from 0 (lowest readability) to 100 (maximum read-
ability).
In recent years, research on English readability
has progressed toward more sophisticated models
that take into account difficulty at syntactic, seman-
tic and discourse level thanks to advances in psy-
cholinguistic accounts of text processing (Graesser
et al, 2004) and to the availability of a wide range
of NPL tools (e.g. dependency and constituency
parsers, anaphora resolution systems, etc.) and re-
sources (e.g. WordNet). However, for many other
languages current approaches for readability assess-
ment still rely on few basic factors. A notable ex-
ception is the Coh-Metrix-PORT tool (Scarton et al,
2009; Aluisio et al, 2010), which includes 60 read-
ability measures for Brazilian Portuguese inspired
by the Coh-Metrix (Graesser et al, 2004).
A different approach has been followed by the de-
velopers of the DeLite system for German (Glo?ckner
et al, 2006; von der Bru?ck et al, 2008): the tool
computes a set of indices measuring the linguistic
complexity of a document through deep parsing and
outputs a final readability score obtained by apply-
ing the k-nearest neighbor algorithm based on 3,000
ratings from 300 users.
As for Italian, the only work aimed at improving
on the performance of standard readability indices
has been proposed by Dell?Orletta et al (2011), who
implement a set of lexical and morpho-syntactic fea-
tures to distinguish between normal and simplified
newspaper articles in a binary classification task.
Our work differs from their approach in that we
choose a different type of corpus for a different au-
dience (i.e. children with different proficiency levels
vs. adults with low literacy skills or mild cognitive
impairment). We also enrich their feature set in that
our indices capture also semantic and discourse as-
pects of a text. In this respect, we take advantage
of cognitive and psycholinguistic evidence support-
ing the idea behind Coh-Metrix that high textual co-
herence and cohesion result in improved readability
with any type of readers (Beck et al, 1984s; Cataldo
and Oakhill, 2000; Linderholm et al, 2000), and that
discourse connectives and spatio-temporal informa-
tion in a text strongly contribute to cohesion.
3 The corpus
Our goal is to develop a system that can be used in
real scenarios, for instance by teachers who want to
assess if a text is understandable by children in a
certain class. Therefore, we avoid collecting a cor-
pus with documents showing different degrees of
simplification according to a ?controlled? scenario.
This strategy was adopted for instance by Crossley
et al (2011), who compared different readability in-
dices using news texts manually simplified into ad-
vanced, intermediate and beginning difficulty level.
Also the experiments on readability assessment of
Portuguese texts by Scarton et al (2009) were con-
ducted on a corpus of news articles manually simpli-
fied by a linguist according to a natural and a strong
simplification level.
Our approach is different in that we take texts
used for teaching and comprehension exercises in
Italian schools and divide them into three classes,
according to the class level in which they are em-
41
Class 1 Class 2 Class 3
(63 docs) (55 docs) (62 docs)
Doc. length 530 776 1085
in tokens (? 273) (? 758) (? 1152)
Gulpease 55.92 53.88 50.54
(? 6.35) (? 6.13) (? 6.98)
Table 1: Corpus statistics. All values are averaged. StDev
is reported between parenthesis.
ployed. This means that in Class 1 we collect
all documents written for children in elementary
schools (aged 6-10), in Class 2 we collect texts
for children in middle schools (aged 11-13), and in
Class 3 we gather documents written for teenagers
in high schools (aged 14-18). The classes contain
respectively 63, 55 and 62 documents.
As shown in Table 1, the average length of the
documents increases with the school level. How-
ever, the single documents show high variability,
especially those in Class 3. Texts have been se-
lected so as to represent the most common genres
and knowledge domains in school texts. Thus, the
corpus contains a balanced selection of both narra-
tive and expository texts. The latter belong mostly to
the following domains: history, literature, biology,
physics, chemistry, geography and philosophy. The
corpus includes also all official text comprehension
tests used in Italy in the INVALSI school proficiency
evaluation campaign2.
4 Readability assessment based on
Gulpease
We first analyze the behaviour of the Gulpease in-
dex in our corpus, in order to assess if this measure
is adequate for capturing the readability of the doc-
uments. We compute the index by applying to each
document the standard formula:
Gulpdoc = 89 +
(300 ? #sentsdoc) ? (10 ? #charsdoc)
#tokensdoc
Average Gulpease and standard deviation for each
class are reported in Table 1.
2National Institute for the Evaluation of the Educational
System by the Ministry of Research and University, http:
//www.invalsi.it/invalsi/index.php
Fig. 1 shows the distribution of the Gulpease in-
dex in the corpus. On the x axis the document id is
reported, with document 1?63 belonging to Class 1
(elementary), document 64?118 to Class 2 (middle)
and 119?180 to Class 3 (high school). On the y axis,
the Gulpease index is reported, ranging from 41 (i.e.
the lowest readability level in the corpus) to 87 (i.e.
highest readability).
Although the highest readability score is obtained
by a document of Class 1, and the lowest scores
concern documents in Class 3, the three classes do
not seem to be separable based solely on Gulpease.
In particular, documents in Class 2, written for stu-
dents in middle school, show scores partly overlap-
ping with Class 1 and partly with Class 3. Further-
more, the great majority of the documents in the cor-
pus have a Gulpease index included between 50 and
60 and the average Gulpease does not differ consis-
tently across the three classes (Table 1).
Figure 1: Distribution of Gulpease index in the corpus.
Document id on x axis, and Gulpease on y axis
For children in the elementary school, a text with
a Gulpease index between 0 and 55 usually corre-
sponds to the frustration level. For children in the
middle school, the frustration level is reached with a
Gulpease index between 0 and 35. For high-school
students, this level is reached with Gulpease being
between 0 and 10.3
3More information on how to interpret Gulpease for each
of the three classes is reported at http://www.eulogos.
net/ActionPagina_1045.do
42
4.1 Coh-Metrix for English
Coh-Metrix is a computational tool available on-
line at http://cohmetrix.memphis.edu that
can analyze an English document and produce a list
of indices expressing the cohesion of the text. These
indices have been devised based on psycholinguistic
studies on the mental representation of textual con-
tent (McNamara et al, 1996) and address various
characteristics of explicit text, from lexicon to syn-
tax, semantics and discourse, that contribute to the
creation of this representation. Although the tool re-
lies on widely used NLP techniques such as PoS tag-
ging and parsing, there have been limited attempts to
employ it in studies on automatic assessment of text
cohesion. Nevertheless, recent works in the NLP
community investigating the impact of entity grids
(Barzilay and Lapata, 2008) or of discourse relations
(Pitler and Nenkova, 2008) on text coherence and
readability go in the same direction as research on
Coh-Metrix, in that they aim at identifying the lin-
guistic features that best express readability at syn-
tactic, semantic and discourse level.
The indices belonging to Coh-Metrix are divided
into five main classes:
? General Word and Text Information: The in-
dices in this class capture the correlation be-
tween brain?s processing time and word-level
information. For example, many syllables in a
word or many words in a sentence are likely to
make a document more difficult for the brain to
process it. Also, if the type/token ratio is high,
the text should be more difficult because there
are many unique words to be decoded.
? Syntactic Indices: The indices in this class as-
sess syntactic complexity and the frequency of
particular syntactic constituents in a text. The
intuition behind this class is that high syntactic
complexity makes a text more difficult to pro-
cess, lowering its readability, because it usually
implies syntactic ambiguity, structural density,
high number of embedded constituents.
? Referential and Semantic Indices: These in-
dices assess the negative impact on readability
of cohesion gaps, which occur when the words
in a sentence do not connect to other sentences
in the text. They are based on coreference and
anaphoric chains as well as on semantic simi-
larity between segments of the same document
exploiting Latent Semantic Analysis (LSA).
? Indices for Situation Model Dimensions: The
indices in this class express the degree of com-
plexity of the mental model evoked by a doc-
ument (Dijk and Kintsch, 1983) and involves
four main dimensions: causality, intentionality,
time and space.
? Standard readability indices: They comprise
traditional indices for readability assessment
including Flesch Reading Ease and Flesch Kin-
caid Grade Level.
Although the developers of Coh-Metrix claim that
the internal version of the tool includes hundreds of
measures, the online demo shows only 60 of them.
This is partly due to the fact that some metrics are
computed using resources protected by copyright,
and partly because the whole framework is still un-
der development. We refer to these 60 metrics in or-
der to implement the Coh-Metrix version for Italian,
that we call Coease.
4.2 Coease: Coh-Metrix for Italian
In the Coh-Metrix adaptation for Italian, we follow
as much as possible the description of the single in-
dices reported on the official Coh-Metrix documen-
tation. However, in some cases, not all implementa-
tion details are given, so that we may have slightly
different versions of single indices. Besides, one
set of indices is based on the MRC Psycholinguis-
tic Database (Wilson, 2003), a resource including
around 150,000 words with concreteness ratings col-
lected through psycholinguistic experiments, which
is not available for Italian. In general terms, how-
ever, we try to have some indices for each of the
classes described in Section 4.1, in order to repre-
sent all relevant aspects of text cohesion.
The list of all indices is reported in Table 2. In-
dices from 1 to 6 capture some information about the
length of the documents in terms of syllables, words,
sentences and paragraphs. Syllables are computed
using the Perl module Lingua::IT::Hyphenate4.
4http://search.cpan.org/?acalpini/
Lingua-IT-Hyphenate-0.14/
43
Indices from 7 to 10 focus on familiarity of con-
tent words (verbs, nouns, adjectives and adverbs)
measured as their frequency in a reference corpus.
While in English the frequency list was the CELEX
database (Baayen et al, 1995), for Italian we ex-
tracted it from the dump of Italian Wikipedia5. The
idea behind these indices is that unfamiliar words or
technical terminology should have a low frequency
in the reference corpus, which is supposed to be
a general corpus representing many domains. In-
dex 8 is the logarithm of raw frequency of content
words, because logarithm proved to be compatible
with reading time (Haberlandt and Graesser, 1985).
Index 9 is obtained by computing first the lowest fre-
quency score among all the content words in each
sentence, and then calculating the mean. Index 10 is
obtained by computing first the lowest log frequency
score among all content words in each sentence, and
then calculating the mean. Content words were ex-
tracted by running the TextPro NLP suite for Italian
(Pianta et al, 2008)6 and keeping only words tagged
with one of WordNet PoS, namely v, a, n and r.
Indices 11 and 12 compute the abstractness of
nouns and verbs by measuring the distance between
the WordNet synset containing the lemma (most fre-
quent sense) and the root. Then, the mean distance
of all nouns and verbs in the text is computed. We
obtain this index using MultiWordNet (Pianta et al,
2002), the Italian version of WordNet, aligned at
synset level with the English one.
Indices from 13 to 17 measure the syntactic com-
plexity of sentences based on parsing output. Indices
13-15 are computed after parsing each sentence with
the Italian version of Berkeley constituency-based
parser (Lavelli and Corazza, 2009)7. NP incidence
is the incidence of atomic NPs (i.e. not containing
any other NPs) per 1000 words. Higher-level con-
stituents index is the mean distance between each
terminal word in the text and the parse tree root.
Main verb information needed for computing index
16 is obtained by parsing each sentence with Malt
parser for Italian (Lavelli et al, 2009) and taking the
sentence root as main verb. The index accounts for
5http://it.wikipedia.org
6TextPro achieved 95% PoS tagging accuracy at Evalita
2009 evaluation campaign for Italian tools.
7The parser achieved 84% F1 at Evalita 2011 evaluation
campaign for Italian tools.
the memory load needed by a reader to understand a
sentence. Index 17 is calculated by comparing each
token to a manual list of negations and computing
the total number of negations per 1000 words.
Indices 18 and 19 are computed again using
TextPro and the output of Berkeley parser. Index 18
is the ratio of words labelled as pronouns to the in-
cidence of all NPs in the text. High pronoun density
implies low readability, because it makes referential
cohesion less explicit.
Indices from 20 to 29 capture the cohesion of
sentences by taking into account different types of
connectives. In order to compute them, we manu-
ally create lists of connectives divided into additive,
causal, logical and temporal. Then, for each list, we
identify positive (i.e. extending events) and negative
(i.e. ceasing to extend expected events) connectives.
For instance, ?inoltre? (?moreover?) is a positive ad-
ditive connective, while ?ma? (?but?) is a negative ad-
ditive connective. We further compute the incidence
of conditional operators by comparing each token to
a manual list. In order to create such lists, we stick
to their English version by first translating them into
Italian and then manually adding some missing con-
nectives. However, this does not avoid ambiguity,
since some connectives with high frequency can ap-
pear in more than one list, for instance ?e? (?and?),
which can be both temporal and additive.
Indices 30 and 31 capture syntactic similarity of
sentences and are based on the assumption that a
document showing high syntactic variability is more
difficult to understand. This index computes the pro-
portion of intersecting nodes between two syntactic
trees by looking for the largest common subtree, so
that every node except terminal node has the same
production rule in both trees. Index 32 calculates
the proportion of adjacent sentences that share at
least one argument expressed by a noun or a pro-
noun, while indices 33 and 34 compute this propor-
tion based on stems and content words. Stems are
obtained by applying the Snowball stemmer8 to the
lemmatized documents.
Indices 35?40 capture the situation model dimen-
sions of the text. Causal and intentional cohesion
corresponds to the ratio between causal or inten-
tional particles (i.e. connectives and adverbs) and
8http://snowball.tartarus.org/
44
causal or intentional verbs. The rationale behind
this is that a text with many causal verbs and few
causal particles is less readable because the con-
nections between events is not explicitly expressed.
Since no details where given on how these particles
and verbs were extracted for English, we devise our
own methodology. First, we produce manual lists
of causal and intentional particles in Italian. As for
causal verbs, we first select all synsets in the En-
glish WordNet containing ?cause to? in their glosses,
and then obtain the corresponding version in Ital-
ian through MultiWordNet. Intentional verbs are
obtained by first extracting all verbs from English
WordNet that belong to the following categories:
cognition, communication, competition, consump-
tion, contact, creation, emotion, motion and percep-
tion, and then mapping them to the Italian corre-
sponding verbs in MultiWordNet. Temporal cohe-
sion is computed as the average of repetitions of
tense and aspect in the document. Repetitions are
calculated by mapping the output of TextPro mor-
phological analysis of verbs to the labels considered
for tense, i.e. past, present and future, and for as-
pect, i.e. static, completed and in progress. Spa-
tial cohesion reflects the extent to which the sen-
tences are related by spatial particles or relations,
and corresponds to the mean of location and mo-
tion ratio score. Location score is the incidence of
locative prepositions (LSP) divided by LPS plus the
incidence of location nouns. Location nouns are ob-
tained from WordNet and from the Entity Recog-
nizer of TextPro. Motion score is the incidence of
motion particles (MSP) divided by MSP plus the in-
cidence of motion verbs. Motion verbs information
is extracted from WordNet as well. As for motion
and locative particles, we first create a manual list,
which however contains particles that can express
both location and motion (for instance ?in?). The dis-
tinction between the two types of particles is based
on the dependency structure of each sentence: if the
particle is headed by a motion verb and dominates
a location noun, then we assume that it is a motion
particle. Instead, if it heads a location noun but is
not dominated by a motion verb, then it is a locative
particle. We are aware of the fact that this selection
process is quite coarse-grained and can be biased by
wrong dependency structures, ambiguity of nouns
and verbs and limited extension of Italian WordNet.
However, it is a viable solution to approximate the
information conveyed by the corresponding indices
in English, given that no clear explanation for their
implementation is given.
4.3 Additional indices
We implement also three additional indices that are
not part of Coh-Metrix for English. They are re-
ported in Table 2 with the ID 41?46.
Indices 41 and 42 are based on the Basic Ital-
ian Vocabulary (de Mauro, 2000). This resource
includes a list of 7,000 words, which were manu-
ally classified as highly familiar to native speakers of
Italian. We introduce these indices because past ex-
periments on Italian readability by Dell?Orletta et al
(2011) showed that, by combining this information
with some basic features such as word and sentence
length, it was possible to achieve 0.95 accuracy in
a binary classification task aimed at distinguishing
standard newspaper articles from simplified articles
for L2 readers. Index 41 corresponds to the percent-
age of tokens whose base form is listed in the Basic
Italian Vocabulary, while index 42 is the percentage
of (unique) lemmas. The latter is the same feature
implemented by Dell?Orletta et al (2011).
Index 43 is Gulpease, computed following the for-
mula reported in Section 4. We add it to our in-
dex list in line with Coh-Metrix, which includes also
standard readability metrics such as Flesch-Reading
Ease and Flesch-Kincaid.
5 The Online System
The Coease indices have been made available
online through a Web interface at http://
readability.fbk.eu. This allows users to
copy and paste a document in the text field and to
compute all available indices, similar to the func-
tionalities of the English Coh-Metrix tool. We have
normalized each index so that it is comprised be-
tween -1 and +1 using the scaling function available
in LIBSVM (Chang and Lin, 2011). Low scores ex-
press low readability for the given index while high
scores correspond to highly readable texts.
In order to identify the indices that are most cor-
related with the readability levels, we computed
Pearson correlation coefficients between each index
and the three classes, similar to Pitler and Nenkova
45
(2008). The ten most correlated indices are marked
with (*) in Table 2. It is interesting to note that 6
out of 10 indices are not part of the standard Coh-
Metrix framework, and account for lexical informa-
tion. In all cases, correlation is moderate, being
0.3 ? r ? 0.6.
Figure 2: Graphical representation of readability as plot-
ted by the Coease web interface. Index id on x axis, and
normalized value on y axis
Coease is designed in order to enable users to
compute readability of a given document and com-
pare it with the average values for the three classes in
our reference corpus (Section 3). Therefore, the av-
erage normalized score of each index for each class
has been computed based on the corpus. Then, every
time a new document is analyzed, the output scores
are plotted together with the average scores for each
of the three classes. This allows a user to compare
different aspects of the current document, such as
the lexicon or the syntax, with the averages of the
three classes. For example, a user may discover that
a document is highly complex from the lexical point
of view, since its lexical indices are in line with those
of high-school texts. However, its syntax may be
rather simple, having syntax-based indices similar to
those of elementary textbooks. This kind of compar-
ison provides information that are generally not cap-
tured via supervised classification. If we trained a
classifier using the indices as features, we would be
able to assign a new document to elementary, mid-
dle or high-school level, but a naive user would not
be able to understand how the single indices affect
classification. Besides, this graphical representation
allows a user to identify documents that should not
be classified into a specific class, because its indices
fall into different classes. Furthermore, we can de-
tect documents with different degrees of readability
within each class.
As an example, we report in Fig. 2 the graphical
representation returned by the system after analyz-
ing an article taken from ?Due Parole?9 (labeled as
?current?), an online newspaper for adult L2 learn-
ers. The scores are compared with the average val-
ues of the 10 most correlated indices, which are re-
ported on the x axis in the same order as they are
described in Table 2. According to the plot, the ar-
ticle has a degree of readability similar to the ?high-
school? class, although some indices show that its
readability is higher (see for instance the index n. 9,
i.e. lexical overlap with Class 3 documents).
The current system version returns only the 10
most correlated indices for the sake of clarity. How-
ever, it easy configurable in order to plot all indices,
or just a subset selected by the user.
6 Conclusions and Future Work
We present Coease, a system for readability assess-
ment of Italian inspired by Coh-Metrix principles.
This set of indices improves on Gulpease index in
that it takes into account discourse coherence, syn-
tactic parsing and semantic complexity in order to
account for the psycholinguistic and cognitive rep-
resentations involved in reading comprehension.
We make Coease available through an online in-
terface. A user can easily analyze a document and
compare its readability to three difficulty levels, cor-
responding to average elementary, middle and high-
school readability level. The graphical representa-
tion returned by the system makes this comparison
straightforward, in that the indices computed for the
current document are plotted together with the 10
most correlated indices in Coease.
In the future, we will analyze the reason why lex-
ical indices are among the most correlated ones with
the three classes. The lower impact of syntactic in-
formation, for instance, could be affected by parsing
performance. However, this could depend also on
how syntactic indices are computed in Coh-Metrix:
9http://www.dueparole.it/
46
we will investigate whether alternative ways to cal-
culate the indices may be more appropriate for Ital-
ian texts.
In addition, we plan to use the indices as features
for predicting the readability of unseen texts. In a
classification setting, it will be interesting to see if
the 10 best indices mentioned in the previous sec-
tions are also the most predictive features, given that
some information may become redundant (for in-
stance, the Gulpease index).
Acknowledgments
The work described in this paper has been partially
funded by the European Commission under the con-
tract number FP7-ICT-2009-5, Terence project.
References
Sandra Aluisio, Lucia Specia, Caroline Gasperin, and
Carolina Scarton. 2010. Readability assessment for
text simplification. In Proceedings of the NAACL
HLT 2010 Fifth Workshop on Innovative Use of NLP
for Building Educational Applications, pages 1?9,
Stroudsburg, PA, USA.
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (release 2). CD-ROM.
Richard Bamberger and Erich Vanecek. 1984. Lesen-
Verstehen-Lernen-Schreiben. Jugend un Volk Verlags-
gesellschaft.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Computa-
tional Linguistics, 34:1?34, March.
I. L. Beck, M. G. McKeown, G. M. Sinatra, and J. A.
Loxterman. 1984s. Revisiting social studies text
from a text-processing perspective: Evidence of im-
proved comprehensibility. Reading Research Quar-
terly, 26:251?276.
M. G. Cataldo and J. Oakhill. 2000. Why are poor com-
prehenders inefficient searchers? An investigation into
the effects of text representation and spatial memory
on the ability to locate information in text. Journal of
Educational Psychology, 92:791?799.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1?
27:27. Software available at http://www.csie.
ntu.edu.tw/?cjlin/libsvm.
Scott A. Crossley, David B. Allen, and Danielle S. Mc-
Namara. 2011. Text readability and intuitive simplifi-
cation: A comparison of readability formula. Reading
in a Foreign Language, 23(1):84?101.
Tullio de Mauro. 2000. Il Dizionario della Lingua Ital-
iana. Paravia, Torino, Italy.
Felice Dell?Orletta, Simonetta Montemagni, and Giu-
lia Venturi. 2011. READ?IT: Assessing Readability
of Italian Texts with a View to Text Simplification.
In Proceedings of the Second Workshop on Speech
and Language Processing for Assistive Technologies,
pages 73?83, Edinburgh, Scotland, UK, July. Associa-
tion for Computational Linguistics.
T. A. Van Dijk and W. Kintsch. 1983. Strategies of dis-
course comprehension. Academic Press, New York,
US.
Rudolf Flesch. 1946. The Art of plain talk. Harper.
V. Franchina and R. Vacca. 1986. Adaptation of Flesch
readability index on a bilingual text written by the
same author both in Italian and English languages.
Linguaggi, 3:47?49.
Ingo Glo?ckner, Sven Hartrumpf, Hermann Helbig, Jo-
hannes Leveling, and Rainer Osswald. 2006. An ar-
chitecture for rating and controlling text readability. In
Proceedings of KONVENS 2006, pages 32?35, Kon-
stanz, Germany, October.
A. Graesser, D. McNamara, M. Louwerse, and Z. Cai.
2004. Coh-Metrix: Analysis of text on cohesion and
language. Behavioral Research Methods, Instruments,
and Computers, 36:193?202.
Robert Gunning. 1952. The technique of clear writing.
McGraw-Hill.
Karl F. Haberlandt and Arthur C. Graesser. 1985. Com-
ponent processes in text comprehension and some of
their interactions. Journal of Experimental Psychol-
ogy, 114(3):357?374.
F. Huerta. 1959. Medida sencillas de lecturabilidad.
Consigna, 214:29?32.
L. Kandel and A. Moles. 1958. Application de l?Indice
de Flesch a` la langue franc?aise. Cahiers d?Etudes de
Radio-Television, pages 253?274.
J.P. Kincaid, R.P. Fishburne, R.L. Rogers, and B.S.
Chissom. 1975. Derivation of New Readability For-
mulas for Navy Enlisted Personnel. Research Branch
Report.
Alberto Lavelli and Anna Corazza. 2009. The Berkeley
Parser at EVALITA 2009 Constituency Parsing Task.
In Proceedings of EVALITA Evaluation Campaign.
A. Lavelli, J. Hall, J. Nilsson, and J. Nivre. 2009.
MaltParser at the EVALITA 2009 Dependency Parsing
Task. In Proceedings of EVALITA Evaluation Cam-
paign.
T. Linderholm, M. G. Everson, P. van den Broek,
M. Mischinski, A. Crittenden, and J. Samuels. 2000.
Effects of Causal Text Revisions on More- and Less-
Skilled Readers? Comprehension of Easy and Difficult
Texts. Cognition and Instruction, 18:525?556.
47
Pietro Lucisano and Maria Emanuela Piemontese. 1988.
Gulpease. Una formula per la predizione della diffi-
colta` dei testi in lingua italiana. Scuola e Citta`, 3:57?
68.
G. H. McLaughlin. 1969. SMOG grading: A new read-
ability formula. Journal of Reading, 12(8):639?646.
D.S. McNamara, E. Kintsch, N.B. Songer, and
W. Kintsch. 1996. Are good texts always better? Text
coherence, background knowledge, and levels of un-
derstanding in learning from text. Cognition and In-
struction, pages 1?43.
Emanuele Pianta, Luisa Bentivogli, and Christian Gi-
rardi. 2002. MultiWordNet: developing an aligned
multilingual database. In First International Confer-
ence on Global WordNet, pages 292?302, Mysore, In-
dia.
Emanuele Pianta, Christian Girardi, and Roberto Zanoli.
2008. The TextPro tool suite. In Proc. of the 6th Lan-
guage Resources and Evaluation Conference (LREC),
Marrakech, Morocco.
Emily Pitler and Ani Nenkova. 2008. Revisiting Read-
ability: A Unified Framework for Predicting Text
Quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 186?195, Honolulu.
Caroline E. Scarton, Daniel M. Almeida, and Sandra M.
Alu??sio. 2009. Ana?lise da Inteligibilidade de tex-
tos via ferramentas de Processamento de L??ngua Natu-
ral: adaptando as me?tricas do Coh-Metrix para o Por-
tugue?s. In Proceedings of STIL-2009, Sa?o Carlos,
Brazil.
Tim von der Bru?ck, Sven Hartrumpf, and Hermann Hel-
big. 2008. A Readability Checker with Super-
vised Learning using Deep Architecture. Informatica,
32:429?435.
Michael Wilson. 2003. MRC Psycholinguistic
Database: Machine Usable Dictionary, Version 2.00.
Rutherford Appleton Laboratory, Oxfordshire, Eng-
land.
ID Feature list
General word and text information
Basic Count
1-3 N. of words, sents and parag. in text
4 Mean n. of syllables per content word*
5 Mean n. of words per sentence
6 Mean n. of sentences per paragraph
Frequencies
7 Raw frequency of content words
8 Log of raw frequency of content words
9 Min raw frequency of content words
10 Log min raw frequency of content words
Hypernymy
11 Mean hypernym value of nouns
12 Mean hypernym value of verbs
Syntactic indices
Constituents information
13 Noun phrase incidence
14 Mean n. of modifiers per NP
15 Higher level constituents
16 Mean n. of words before main verb
17 Negation incidence
Pronouns, Types, Tokens
18 Pronoun ratio
19 Type-token ratio
Connectives
20 Incidence of all connectives
21-22 Incidence of pos./neg. additive conn.
23-24 Incidence of pos./neg. temporal conn.
25-26 Incidence of pos./neg. causal conn.
27-28 Incidence of pos./neg.* logical conn.
29 Incidence of conditional operators
Syntactic similarity
30 Tree intersection between adj. sentences
31 Tree intersection between all sentences
Referential and Semantic Indices
Coreference
32 Adjacent argument overlap*
33 Stem overlap between adjacent sentences
34 Content word overlap between adj. sents.
Situation model dimensions
35-36 Causal content and cohesion
37-38 Intentional content and cohesion*
39-40 Temporal and spatial cohesion
Features not included in Coh-Metrix
41 Lemma overlap with VBI (token-based)*
42 Lemma overlap with VBI (type-based)*
43 Gulpease index*
44 Lexical overlap with Class 1*
45 Lexical overlap with Class 2*
46 Lexical overlap with Class 3*
Table 2: Coease indices for readability assessment. (*)
shows the indices with highest Pearson correlation.48
