Proceedings of the 6th Workshop on Statistical Machine Translation, pages 426?432,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Two-step translation with grammatical post-processing?
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova? and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{marecek,rosa,galuscakova,bojar}@ufal.mff.cuni.cz
Abstract
This paper describes an experiment in which
we try to automatically correct mistakes in
grammatical agreement in English to Czech
MT outputs. We perform several rule-based
corrections on sentences parsed to dependency
trees. We prove that it is possible to improve
the MT quality of majority of the systems par-
ticipating in WMT shared task. We made both
automatic (BLEU) and manual evaluations.
1 Introduction
This paper is a joint report on two English-to-Czech
submissions to the WMT11 shared translation task.
The main contribution is however the proposal and
evaluation of a rule-based post-processing system
DEPFIX aimed at correcting errors in Czech gram-
mar applicable to any MT system. This is somewhat
the converse of other approaches (e.g. Simard et al
(2007)) where a statistical system was applied for
the post-processing of a rule-based one.
2 Our phrase-based systems
This section briefly describes our underlying phrase-
based systems. One of them (CU-BOJAR) was sub-
mitted directly to the WMT11 manual evaluation,
the other one (CU-TWOSTEP) was first corrected by
the proposed method (Section 3 below) and then
submitted under the name CU-MARECEK.
?This research has been supported by the European Union
Seventh Framework Programme (FP7) under grant agreement
n? 247762 (Faust), n? 231720 (EuroMatrix Plus), and by the
grants GAUK 116310 and GA201/09/H057.
2.1 Data for statistical systems
Our training parallel data consists of CzEng 0.9
(Bojar and Z?abokrtsky?, 2009), the News Commen-
tary corpus v.6 as released by the WMT11 orga-
nizers, the EMEA corpus, a corpus collected from
the transcripts of TED talks (http://www.ted.com),
the parallel news and separately some of the par-
allel web pages of the European Commission
(http://ec.europa.eu), and the Official Journal of the
European Union as released by the Apertium con-
sortium (http://apertium.eu/data).
A custom web crawler was used for the European
Commission website. English and Czech websites
were matched according to their URLs. Unfortu-
nately, Czech websites very often contain untrans-
lated parts of English texts. Because of this, we
aimed especially at the news articles, which are very
often translated correctly and also more relevant for
the shared task. Texts were segmented using train-
able tokenizer (Klyueva and Bojar, 2008) and dedu-
plicated. Processed texts were automatically aligned
by Hunalign (Varga and others, 2005).
The data from the Official Journal were first con-
verted from XML to plain text. The documents were
paired according to their filenames. To better han-
dle the nature of these data, we decided to divide
the documents into two classes based on the aver-
age number of words per sentence: ?lists? are docu-
ments with less than 2.8 words per sentence, other
documents are called ?texts?. The corresponding
?lists? were aligned line by line. The corresponding
?texts? were automatically segmented by trainable
tokenizer and aligned automatically by Hunalign.
We use the following two Czech language mod-
426
els, their weights are optimized in MERT:
? 5-gram LM from the Czech side of CzEng (ex-
cluding the Navajo section). The LM was con-
structed by interpolating LMs of the individual do-
mains (news, EU legislation, technical documenta-
tion, etc.) to achieve the lowest perplexity on the
WMT08 news test set.
? 6-gram LM from the monolingual data supplied by
WMT11 organizers (news of the individual years
and News Commentary), the Czech National Cor-
pus and a web collection of Czech texts. Again, the
final LM is constructed by interpolating the smaller
LMs1 for the WMT08 news test set.
2.2 Baseline Moses (CU-BOJAR)
The system denoted CU-BOJAR for English-to-
Czech is simple phrase-based translation, i.e. Moses
without factors. We tokenized, lemmatized and
tagged all texts using the tools wrapped in TectoMT
(Popel and Z?abokrtsky?, 2010). We further tokenize
e.g. dashed words (?23-year?) after all the process-
ing is finished. Phrase-based MT is then able to
handle such expressions both at once, or decompose
them as needed to cover unseen variations. We use
lexicalized reordering (orientation-bidirectional-fe).
The translation runs in ?supervised truecase?, which
means that we use the output of our lemmatizers
to decide whether the word should be lowercased
or should preserve uppercasing. After the transla-
tion, the first letter in the output is simply upper-
cased. The model is optimized using Moses? stan-
dard MERT on the WMT09 test set.
The organizers of WMT11 encouraged partici-
pants to apply simple normalization to their data
(both for training and testing).2 The main purpose
of the normalization is to improve the consistency of
typographical rules. Unfortunately, some of the au-
tomatic changes may accidentally damage the mean-
ing of the expression.3 We therefore opted to submit
1The interpolated LM file (gzipped ARPA format) is 5.1 GB
so we applied LM pruning as implemented in SRI toolkit with
the threshold 10?14 to reduce the file size to 2.3 GB.
2http://www.statmt.org/wmt11/normalize-punctuation.perl
3Fixing the ordering of the full stop and the quote is wrong
because the order (at least in Czech typesetting) depends on
whether it is the full sentence or a final phrase that is captured
in the quotes. Even riskier are rules handling decimal and thou-
sand separators in numbers. While there are language-specific
conventions, they are not always followed and the normaliza-
tion can in such cases confuse the order of magnitude by 3.
the output based on non-normalized test sets as our
primary English-to-Czech submission.
We invested much less effort into the submission
called CU-BOJAR for Czech-to-English. The only
interesting feature there is the use of alternative de-
coding paths to translate either from the Czech form
or from the Czech lemma equipped with meaning-
bearing morphological properties, e.g. the number
of nouns. Bojar and Kos (2010) used the same setup
with simple lemmas in the fallback decoding path.
The enriched lemmas perform marginally better.
2.3 Two-step translation
Our two-step translation is essentially the same
setup as detailed by Bojar and Kos (2010): (1)
the English source is translated to simplified Czech,
and (2) the simplified Czech is monotonically trans-
lated to fully inflected Czech. Both steps are sim-
ple phrase-based models. Instead of word forms, the
simplified Czech uses lemmas enriched by a sub-
set of morphological features selected manually to
encode only properties overt both in English and
Czech such as the tense of verbs or number of nouns.
Czech-specific morphological properties indicating
various agreements (e.g. number and gender of ad-
jectives, gender of verbs) are imposed in the second
step solely on the basis of the language model.
The first step uses the same parallel and mono-
lingual corpora as CU-BOJAR, except the LMs being
trained on the enriched lemmas, not on word forms.
The second step uses exactly the same LM as CU-
BOJAR but the phrase-table is extracted from all our
Czech monolingual data (phrase length limit of 1.)
3 Grammatical post-processing
Phrase-based machine translation systems often
have problems with grammatical agreement, espe-
cially on longer dependencies. Sometimes, there is
a mistake in agreement even between adjacent words
because each one belongs to a different phrase. The
goal of our post-processing is to correct forms of
some words so that they do not violate grammatical
rules (eg. grammatical agreement).
The problem is how to find the correct syntactic
relations in the output of an MT system. Parsers
trained on correct sentences can rely on grammat-
ical agreement, according to which they determine
427
the dependencies between words. Unfortunately, the
agreement in MT outputs is often wrong and the
parser fails to produce a correct parse tree. There-
fore, we would need a parser trained on a manually
annotated treebank consisting of specific outputs of
machine translation systems. Such a treebank does
not exist and we do not even want to create one, be-
cause the MT systems are changing constantly and
also because manual annotation of texts that are of-
ten not even understandable would be almost a su-
perhuman task.
The DEPFIX system was implemented in TectoMT
framework (Popel and Z?abokrtsky?, 2010). MT out-
puts were tagged by Morc?e tagger (Spoustova? et al,
2007) and then parsed with MST parser (McDon-
ald et al, 2005) that was trained on the Prague De-
pendency Treebank (Hajic? and others, 2006), i.e.
on correct Czech sentences. We used an improved
implementation with some additional features es-
pecially tuned for Czech (Nova?k and Z?abokrtsky?,
2007). The parser accuracy is much lower on the
?noisy? MT output sentences, but a lot of dependen-
cies in which we are to correct grammatical agree-
ment are determined correctly. Adapting the parser
for outputs of MT systems will be addressed in the
coming months.
A typical example of a correction is the agreement
between the subject and the predicate: they should
share the morphological number and gender. If they
do not, we simply change the number and gender
of the predicate in agreement with the subject.4 An
example of such a changed predicate is in Figure 1.
Apart from the dependency tree of the target sen-
tence, we can also use the dependency tree of the
source sentence. Source sentences are grammat-
ically correct and the accuracy of the tagger and
the parser is accordingly higher there. Words in
the source and target sentences are aligned using
GIZA++5 (Och and Ney, 2003) but verbose outputs
of the original MT systems would be possibly a bet-
ter option. The rules for fixing grammatical agree-
ment between words can thus consider also the de-
pendency relations and morphological caregories of
their English counterparts in the input sentence.
4In this case, we suppose that the number of the subject has
a much higher chance to be correct.
5GIZA++ was run on lemmatized texts in both directions
and intersection symmetrization was used.
Some
people
came
later
Atr
Sb
Pred
Advplpl
.AuxK
p?i?liPredpl
N?kte??
lid?
p?i?el
pozd?ji
Atr
Sb
Pred
Advsg, mpl
.AuxK
Figure 1: Example of fixing subject-predicate agreement.
The Czech word pr?is?el [he came] has a wrong morpho-
logical number and gender.
3.1 Grammatical rules
We have manually devised a set of the following
rules. Their input is the dependency tree of a Czech
sentence (MT output) and its English source sen-
tence (MT input) with the nodes aligned where pos-
sible. Each of the rules fires if the specified con-
ditions (?IF?) are matched, executes the command
(?DO?) , usually changing one or more morphologi-
cal categories of the word, and generates a new word
form for any word which was changed.
The rules make use of several morphological cat-
egories of the word (node:number, node:gender...),
its syntactic relation to its parent in the dependency
tree (node:afun) and the same information for its
English counterpart (node:en) and other nodes in
the dependency trees.
The order of the rules in this paper follows the
order in which they are applied; this is important, as
often a rule changes a morphological category of a
word which is then used by a subsequent rule.
3.1.1 Noun number (NounNum)
In Czech, a word in singular sometimes has the
same form as in plural. Because the tagger often
fails to tag the word correctly, we try to correct the
tag of a noun tagged as singular if its English coun-
terpart is in plural, so that the subsequent rules can
work correctly.
We trust the form of the word but changing the
number may also require to change the morphologi-
cal case (i.e. the tagger was wrong with both number
and case). In such cases we choose the first (linearly
428
from nominative to instrumentative) case matching
the form. The rule is:
IF: node:pos = noun &
node:number = singular &
node:en:number = plural
DO: node:number := plural;
node:case := find case(node:form, plural);
3.1.2 Subject case (SubjCase)
The subject of a Czech sentence must be in the
nominative case. Since the parser often fails in
marking the correct word as a subject, we use the
English source sentence and presuppose that the
Czech counterpart of the English subject is also a
subject in the Czech sentence.
IF: node:en:afun = subject
DO: node:case := nominative;
3.1.3 Subject-predicate agreement (SubjPred)
Subject and predicate in Czech agree in their mor-
phological number. To identify a Czech Subject, we
trust the subject in the English sentence. Then we
copy the number from the (Czech) Subject to the
Czech Predicate.
IF: node:en:afun = subject &
parent:afun = predicate
DO: parent:number := node:number;
3.1.4 Subject-past participle agreement (SubjPP)
Czech past participles agree with subject in
morphological gender.
IF: node:pos = noun|pronoun &
node:en:afun = subject &
parent:pos = verb past participle
DO: parent:number := node:number;
parent:gender := node:gender;
3.1.5 Preposition without children (PrepNoCh)
In our dependency trees, the preposition is the
parent of the words it belongs to (usually a noun). A
preposition without children is incorrect so we find
nodes aligned to its English counterpart?s children
and rehang them under the preposition.
IF: node:afun = preposition &
!node:has children &
node:en:has children
DO: foreach node:en:child;
node:en:child:cs:parent := node;
3.1.6 Preposition-noun agreement (PrepNoun)
Every prepositions gets a morphological case as-
signed to it by the tagger, with which the dependent
noun should agree.
IF: parent:pos = preposition &
node:pos = noun
DO: node:case := parent:case;
3.1.7 Noun-adjective agreement (NounAdj)
Czech adjectives and nouns agree in morpholog-
ical gender, number and case. We assume that the
noun is correct and change the adjective accordingly.
IF: node:pos = adjective &
parent:pos = noun
DO: node:gender := parent:gender;
node:number := parent:number;
node:case := parent:case;
3.1.8 Reflexive particle deletion (ReflTant)
Czech reflexive verbs are accompanied by reflex-
ive particles (?se? and ?si?). We delete particles not
beloning to any verb (or adjective derived from a
verb).
IF: node:form = ?se?|?si? &
node:pos = pronoun &
parent:pos != verb|verbal adjective
DO: remove node;
4 Experiments and results
We tested our CU-TWOSTEP system with DEPFIX
post-processing on both WMT10 and WMT11 test-
ing data. This combined system was submitted to
shared translation task as CU-MARECEK. We also
ran the DEPFIX post-processing on all other partici-
pating systems.
4.1 Automatic evaluation
The achieved BLEU scores are shown in Tables 1
and 2. They show the scores before and after the
DEPFIX post-processing. It is interesting that the
improvements are quite different between the years
2010 and 2011 in terms of their BLEU score. While
the average improvement on WMT10 test set was
0.21 BLEU points, it was only 0.05 BLEU points on
the WMT11 test set. Even the results of the same
TWOSTEP system differ in a similar way, so it must
have been caused by the different data.
429
system before after improvement
cu-twostep 15.98 16.13 0.15 (0.05 - 0.26)
cmu-heaf. 16.95 17.04 0.09 (-0.01 - 0.20)
cu-bojar 15.85 16.09 0.24 (0.14 - 0.36)
cu-zeman 12.33 12.55 0.22 (0.12 - 0.32)
dcu 13.36 13.59 0.23 (0.13 - 0.37)
dcu-combo 18.79 18.90 0.11 (0.02 - 0.23)
eurotrans 10.10 10.11 0.01 (-0.04 - 0.07)
koc 11.74 11.91 0.17 (0.08 - 0.26)
koc-combo 16.60 16.86 0.26 (0.16 - 0.37)
onlineA 11.81 12.08 0.27 (0.17 - 0.38)
onlineB 16.57 16.79 0.22 (0.11 - 0.33)
potsdam 12.34 12.57 0.23 (0.14 - 0.35)
rwth-combo 17.54 17.79 0.25 (0.15 - 0.35)
sfu 11.43 11.83 0.40 (0.29 - 0.52)
uedin 15.91 16.19 0.28 (0.18 - 0.40)
upv-combo 17.51 17.73 0.22 (0.10 - 0.34)
Table 1: Depfix improvements on the WMT10 systems
in BLEU score. Confidence intervals, which were com-
puted on 1000 bootstrap samples, are in brackets.
system before after improvement
cu-twostep 16.57 16.60 0.03 (-0.07 - 0.13)
cmu-heaf. 20.24 20.32 0.08 (-0.03 - 0.19)
commerc2 09.32 09.32 0.00 (-0.04 - 0.04)
cu-bojar 16.88 16.85 -0.03 (-0.12 - 0.07)
cu-popel 14.12 14.11 -0.01 (-0.06 - 0.03)
cu-tamch. 16.32 16.28 -0.04 (-0.14 - 0.06)
cu-zeman 14.61 14.80 0.19 (0.09 - 0.29)
jhu 17.36 17.42 0.06 (-0.03 - 0.16)
online-B 20.26 20.31 0.05 (-0.06 - 0.16)
udein 17.80 17.88 0.08 (-0.02 - 0.17)
upv-prhlt. 20.68 20.69 0.01 (-0.08 - 0.11)
Table 2: Depfix improvements on the WMT11 systems
in BLEU score. Confidence intervals are in brackets.
4.2 Manual evaluation
Two independent annotators evaluated DEPFIX man-
ually on the outputs of CU-TWOSTEP and ONLINE-
B. We randomly selected 1000 sentences from the
newssyscombtest2011 data set and the appropri-
ate translations made by these two systems. The
annotators got the outputs before and after DEPFIX
post-processing and their task was to decide which
translation6 from these two is better and label it by
the letter ?a?. If it was not possible to determine
6They were also provided with the source English sentence
and the reference translation. The options were shuffled and
indentical candidate sentences were collapsed.
A / B improved worsened indefinite total
improved 273 20 15 308
worsened 12 59 7 78
indefinite 53 35 42 130
total 338 114 64 516
Table 5: Matrix of the inter-annotator agreement
rule fired impr. wors. % impr.
SubjCase 51 46 5 90.2
SubjPP 193 165 28 85.5
NounAdj 434 354 80 81.6
NounNum 156 122 34 78.2
PrepNoun 135 99 36 73.3
SubjPred 68 48 20 70.6
ReflTant 15 10 5 66.7
PrepNoCh 45 29 16 64.4
Table 6: Rules and their utility.
which is better, they labeled both by ?n?.
Table 3 below shows that about 60% of sentences
fixed by DEPFIX were improved and only about 20%
were worsened. DEPFIX worked a little better on the
ONLINE-B, making fewer changes but also fewer
wrong changes. It is probably connected with the
fact that overall better translations by ONLINE-B are
easier to parse.
The matrix of inter-annotator agreement is in Ta-
ble 5. Our two annotators agreed in 374 sentences
(out of 516), that is 72.5%. On the other hand, if
we consider only cases where both annotators chose
different translation as better (no indefinite marks),
we get only 8.8% disagreement (32 out of 364).
Using the manual evaluation, we can also measure
performance of the individual rules. Table 6 shows
the number of all, improved or worsened sentences
where a particular rule was applied. Definitely, the
most useful rule (used often and quite reliable) was
the one correcting noun-adjective agreement, fol-
lowed by the subject-pastparticiple agreement rule.
In each changed sentence, two rules (not neces-
sarily related ones) were applied on average.
4.3 Manual evaluation across data sets
The fact that the improvements in BLEU scores on
WMT10 test set are much higher has led us to one
more experiment: we compare manual annotations
of 330 sentences from each of the WMT10 and
430
system annotator changed improved worsened indefinite
count % count % count %
cu-bojar-twostep A 269 152 56.5 39 14.5 78 29.0
cu-bojar-twostep B 269 173 64.3 50 18.6 46 17.1
online-B A 247 156 63.1 39 15.9 52 21.1
online-B B 247 165 66.8 64 25.9 18 7.3
Table 3: Manual evaluation of the DEPFIX post-processing on 1000 randomly chosen sentences from WMT11 test set.
test set changed improved worsened indefinite BLEU
count % count % count % before after diff
newssyscombtest2010 104 52 50.0 20 19.2 32 30.8 16.99 17.38 0.39
newssyscombtest2011 101 66 65.3 19 18.8 16 15.8 13.99 13.87 -0.12
Table 4: Manual and automatic evaluation of the DEPFIX post-processing on CU-TWOSTEP system across different
datasets. 330 sentences were randomly selected from each of the WMT10 and WMT11 test sets. Both manual scores
and BLEU are computed only on the sentences that were changed by the DEPFIX post-processing.
WMT11 sets as translated by CU-TWOSTEP and cor-
rected by DEPFIX. Table 4 shows that WMT10 and
WMT11 are comparable in manually estimated im-
provement (50?65%). BLEU does not indicate that
and even estimates a drop in quality on this subset
WMT11. (The absolute BLEU scores differ from
BLEUs on the whole test sets but we are interested
only in the change of the scores.) BLEU is thus not
very suitable for the evaluation of DEPFIX.
5 Conclusions and future work
Manual evaluation shows that our DEPFIX approach
to improving MT output quality is sensible. Al-
though it is unable to correct many serious MT er-
rors, such as wrong lexical choices, it can improve
the grammaticality of the output in a way that the
language model often cannot, which leads to out-
put that is considered to be better by humans. We
also suggest that BLEU is not appropriate metric
for measuring changes in grammatical correctness
of sentences, especially with inflective languages.
An advantage of our method is that it is possible
to apply it on output of any MT system (although it
works better for phrase-based MT systems). While
DEPFIX has been developed using the output of CU-
BOJAR, the rules we devised are not specific to any
MT system. They simply describe several grammat-
ical rules of Czech language that can be machine-
checked and if errors are found, the output can be
corrected. Moreover, our method only requires the
source sentence and the translation output for its op-
eration ? i.e. it is not necessary to modify the MT
system itself.
We are now considering modifications of the
parser so that it is able to parse the incorrect sen-
tences produced by MT. Theoretically it would be
possible to train the parser on annotated ungrammat-
ical sentences, but we do not want to invest such an-
notation labour. Instead, when parsing the Czech
sentence we will make the parser utilize the infor-
mation contained in the parse tree of the English
sentence, which is usually correct. We will proba-
bly also have to make the parser put less weight to
the often incorrect tagger output. An alternative is
to avoid parsing of the target and project the source
parse to the target side using word alignments, if
provided by the MT system.
Because some of our rules are able to work using
only the tagger output, we will also try to apply them
before the parsing as they might help the parser by
correcting some of the tags.
We will also try several modifications of the tag-
ger, but the English sentence does not help us so
much here, because it does not contain any infor-
mation regarding the most common errors ? in-
correct assignment of morphological gender and
case. However, it could help with part of speech
and morphological number disambiguation. More-
over, it would be probably helpful for us if the tag-
ger included several most probable hypotheses, as
the single-output-only disambiguation is often erro-
neous on ungrammatical sentences.
431
References
Ondrej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T0 1, Philadelphia.
Natalia Klyueva and Ondr?ej Bojar. 2008. UMC 0.1:
Czech-Russian-English Multilingual Corpus. In Pro-
ceedings of International Conference Corpus Linguis-
tics, pages 188?195, Saint-Petersburg.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, edi-
tors, Lecture Notes in Artificial Intelligence, Proceed-
ings of the 10th I nternational Conference on Text,
Speech and Dialogue, Lecture Notes in Computer Sci-
ence, pages 92?98, Pilsen, Czech Republic. Springer
Science+Business Media Deutschland GmbH.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007.
Statistical phrase-based post-editing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 508?515, Rochester, New York, April.
Association for Computational Linguistics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Da?niel Varga et al 2005. Parallel corpora for medium
density languages. In Proceedings of the Recent Ad-
vances in Natural Language Processing, pages 590?
596, Borovets, Bulgaria.
432
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 374?381,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Selecting Data for English-to-Czech Machine Translation ?
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos? Stanojevic?, Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?m. 25, Praha 1, CZ-118 00, Czech Republic
{tamchyna,galuscakova,kamran,bojar}@ufal.mff.cuni.cz,
milosh.stanojevic@gmail.com
Abstract
We provide a few insights on data selection for
machine translation. We evaluate the quality
of the new CzEng 1.0, a parallel data source
used in WMT12. We describe a simple tech-
nique for reducing out-of-vocabulary rate af-
ter phrase extraction. We discuss the bene-
fits of tuning towards multiple reference trans-
lations for English-Czech language pair. We
introduce a novel approach to data selection
by full-text indexing and search: we select
sentences similar to the test set from a large
monolingual corpus and explore several op-
tions of incorporating them in a machine trans-
lation system. We show that this method can
improve translation quality. Finally, we de-
scribe our submitted system CU-TAMCH-BOJ.
1 Introduction
Selecting suitable data is important in all stages of
creating an SMT system. For training, the data size
plays an essential role, but the data should also be as
clean as possible. The new CzEng 1.0 was prepared
with the emphasis on data quality and we evaluate
it against the previous version to show whether the
effect for MT is positive.
Out-of-vocabulary rate is another problem related
to data selection. We present a simple technique to
reduce it by including words that became spurious
OOVs during phrase extraction.
? This work was supported by the project EuroMatrixPlus
(FP7-ICT-2007-3-231720 of the EU and 7E09003+7E11051 of
the Czech Republic) and the Czech Science Foundation grants
P406/11/1499 and P406/10/P259.
Another topic we explore is to use multiple refer-
ences for tuning to make the procedure more robust
as suggested by Dyer et al (2011). We evaluate this
approach for translating from English into Czech.
The main focus of our paper however lies in pre-
senting a method for data selection using full-text
search. We index a large monolingual corpus and
then extract sentences from it that are similar to the
input sentences. We use these sentences in several
ways: to create a new language model, a new phrase
table and a tuning set. The method can be seen as
a kind of domain adaptation. We show that it con-
tributes positively to translation quality and we pro-
vide a thorough evaluation.
2 Data and Tools
2.1 Comparison of CzEng 1.0 and 0.9
As this year?s WMT is the first to include the new
version of CzEng (Bojar et al, 2012b), we carried
out a few experiments to compare its suitability for
MT with its predecessor, CzEng 0.9. Apart from
size (which has almost doubled), there are impor-
tant differences between the two versions. In CzEng
0.9, the largest portion by far came from movie sub-
titles (a data source of varying quality), followed by
EU legislation and technical manuals. On the other
hand, CzEng 1.0 has over 4 million sentence pairs
from fiction and nearly the same amount of data
from EU legislation. Roughly 3 million sentence
pairs come from movie subtitles. This proportion
of domains suggests a higher quality of data. More-
over, sentences in CzEng 1.0 were automatically fil-
tered using a maximum entropy classifier that uti-
374
Vocab. [k]
Corpus and Domain Sents BLEU En Cs
CzEng 0.9
all 1M
14.77?0.12 187 360
CzEng 1.0 15.23?0.18 221 396
CzEng 0.9
news 100k
14.34?0.05 53 125
CzEng 1.0 14.01?0.13 47 113
Table 1: Comparison of CzEng 0.9 and 1.0.
lized a variety of features.
We trained contrastive phrase-based Moses SMT
systems?the first one on 1 million randomly se-
lected sentence pairs from CzEng 0.9, the other on
the same amount of data from CzEng 1.0. Another
contrastive pair of MT systems was based on small
in-domain data only: 100k sentences from the news
sections of CzEng 0.9 and 1.0. For each experiment,
the random selection was done 5 times. In both
experiments, identical data were used for the LM
(News Crawl corpus from 2011), tuning (WMT10
test set) and evaluation (WMT11 test set).
Table 1 shows the results. The ? sign in this case
denotes the standard deviation over the 5 experi-
ments (each with a different random sample of train-
ing data). The results indicate that overall, CzEng
1.0 is a more suitable source of parallel data?most
likely thanks to the more favorable distribution of
domains. However in the small in-domain setting,
using CzEng 0.9 data resulted in significantly higher
BLEU scores.
The vocabulary size of the news section seems to
have dropped since 0.9. We attribute this to the filter-
ing: sentences with obscure words are hard to align
so they are likely to be filtered out (the word align-
ment score as output by Giza++ received a large
weight in the classifier training). These unusual
words then do not appear in the vocabulary.
2.2 Lucene
Apache Lucene1 is a high performance open-source
search engine library written in Java. We use Lucene
to take advantage of the information retrieval (IR)
technique for domain adaptation. Each sentence of
a large corpus is indexed as a separate document; a
document is the unit of indexing and searching in
Lucene. The sentences (documents) can then be re-
1http://lucene.apache.org
trieved based on Lucene similarity formula2, given
a ?query corpus?. Lucene uses Boolean model for
initial filtering of documents. Vector Space Model
with a refined version of Tf-idf statistic is then used
to score the remaining candidates.
In the normal IR scenario, the query is usually
small. However, for domain adaptation a query can
be a whole corpus. Lucene does not allow such
big queries. This problem is resolved by taking
the query corpus sentence by sentence and search-
ing many times. The final score of a sentence in the
index is calculated as the average of the scores from
the sentence-level queries. Methods that make use
of this functionality are discussed in Section 5.
3 Reducing OOV by Relaxing Alignments
Out-of-vocabulary (OOV) rate has been shown to
increase during phrase extraction (Bojar and Kos,
2010). This is due to unfortunate alignment of some
words?no consistent phrase pair that includes them
can be extracted. This issue can be partially over-
come by adding translations of these ?lost? words
(according to Giza++ word alignment) to the ex-
tracted phrase table. This is not our original tech-
nique, it was suggested by Mermer and Saraclar
(2011), though it is not included in the published ab-
stract.
The extraction of phrases in the (hierarchical) de-
coder Jane (Stein et al, 2011) offers a range of sim-
ilar heuristics. Tinsley et al (2009) also observes
gains when extending the set of phrases consistent
with the word alignment by phrases consistent with
aligned parses.
We evaluated this technique on two sets of train-
ing data?the news section of CzEng 1.0 and the
whole CzEng 1.0. The OOV rate of the phrase table
was reduced nearly to the corpus OOV rate in both
cases, however the improvement was negligible?
only a handful of the newly added words occurred
in the test set. Table 2 shows the results. Trans-
lation performance using the improved phrase table
was identical to the baseline.
2http://tiny.cc/ca2ccw
375
Test Set OOV % New
CzEng Sections Baseline Reduced Phrases
news (197k sents) 3.69 3.66 12034
all (14.8M sents) 1.09 1.09 154204
Table 2: Source-side phrase table OOV.
Sections 1 reference 3 references
news 11.37?0.47 11.62?0.50
all 16.07?0.55 15.90?0.57
Table 3: BLEU scores on WMT12 test set when tuning
on WMT11 test set towards one or more references.
4 Tuning to Multiple Reference
Translations
Tuning towards multiple reference translations has
been shown to help translation quality, see Dyer et
al. (2011) and the cited works. Thanks to the other
references, more possible translations of each word
are considered correct, as well as various orderings
of words.
We tried two approaches: tuning to one true refer-
ence and one pseudo-reference, and tuning to multi-
ple human-translated references.
For the first method, which resembles computer-
generated references via paraphrasing as used in
(Dyer et al, 2011), we created the pseudo-reference
by translating the development set using TectoMT,
a deep syntactic MT with rich linguistic processing
implemented in the Treex platform3. We hoped that
the very different output of this decoder would be
beneficial for tuning, however we achieved no im-
provement at all.
For the second experiment we used 3 translations
of WMT11 test set. One is the true reference dis-
tributed for the shared task and two were translated
manually from the German version of the data into
Czech. We achieved a small improvement in final
BLEU score when training on a small data set. On
the complete constrained training data for WMT12,
there was no improvement?in fact, the BLEU score
as evaluated on the WMT12 test set was negligibly
lower. Table 3 summarizes our results. The ? sign
denotes the confidence bounds estimated via boot-
strap resampling (Koehn, 2004).
3http://ufal.ms.mff.cuni.cz/treex/
Used Selected Sel. Sents Avg
Models per Trans. Total BLEU?std
None ? 0 12.39?0.06
LM ? 16k ? rand. sel. 12.18?0.06
LM 3 16k 12.73?0.04
LM 100 502k 14.21?0.11
LM 1000 3.8M 15.12?0.08
LM All Sents 18.3M 15.55?0.11
Table 4: Results of experiments with Lucene, language
model adapted.
5 Experiments with Domain Adaptation
Domain adaptation is widely recognized as a tech-
nique which can significantly improve translation
quality (Wu et al, 2008; Bertoldi and Federico,
2009; Daume? and Jagarlamudi, 2011). In our ex-
periments we tried to select sentences close to the
source side of the test set and use them to improve
the final translation.
The parallel data used in this section are only
small: the news section of CzEng 1.0 (197k sentence
pairs, 4.2M Czech words, 4.8M English words). We
tuned the models on WMT09 test set and evaluated
on WMT11 test set. The techniques examined here
rely on a large monolingual corpus to select data
from. We used all the monolingual data provided by
the organizers of WMT11 (18.3M sentences, 316M
words).
5.1 Tailoring the Language Model
Our first attempt was to tailor the language model
to the test set. Our approach is similar to Zhao et
al. (2004). In Moore and Lewis (2010), the authors
compare several approaches to selecting data for LM
and Axelrod et al (2011) extend their ideas and ap-
ply them to MT.
Naturally, we only used the source side of the test
set. First we translated the test set using a baseline
translation system. Lucene indexer was then used
to select sentences similar to the translated ones in
the large target-side monolingual corpus. Finally, a
new language model was created from the selected
sentences.
The weight of the new LM has to reflect the im-
portance of the language model during both MERT
tuning as well as final application on (a different)
test set. If the new LM were based only on the final
376
test set, MERT would underestimate its value and
vice versa. Therefore, we actually translated both
our development (WMT09) as well as final test set
(WMT11) using the baseline model and created a
LM relevant to their union.
The results of performed experiments with do-
main adaptation are in Table 4. To compensate for
low stability of MERT, we ran the optimization five
times and report the average BLEU achieved. The
? value indicates the standard deviation of the five
runs.
The first row provides the scores for the baseline
experiment with no tailored language model. We
have run the experiment for three values of selected
sentences per one sentence of the test corpus: 3,
100 and 1000 closest-matching sentences were ex-
tracted. With more and more data in the LM, the
scores increase. The second line in Table 4 confirms
the usefulness of the sentence selection. Picking the
same amount of 16k sentences randomly performs
worse. As the last row indicates, taking all available
data leads to the best score.
Note that when selecting the sentences, we used
lemmas instead of word forms to reduce data sparse-
ness. So Lucene was actually indexing the lemma-
tized version of the monolingual data and the base-
line translation translated English lemmas to Czech
lemmas when creating the ?query corpus?. The final
models were created from the original sentences, not
their lemmatized versions.
5.2 Tailoring the Translation Model
Reverse self-training is a trick that allows to improve
the translation model using (target-side) monolin-
gual data and can lead to a performance improve-
ment (Bojar and Tamchyna, 2011; Lambert et al,
2011).
In our scenario, we translated the selected sen-
tences (in the opposite direction, i.e. from the target
into the source language). Then we created a new
translation model (in the original direction) based on
the alignment of selected sentences and their reverse
translation. This new model is finally combined with
the baseline model and weighted by MERT. The
whole scenario is shown in Figure 1.
The results of our experiments are in Table 5. We
ran the experiment with translation model adaptation
for 100 most similar sentences selected by Lucene.
Each experiment was again performed five times.
Due to the low stability of tuning, we also tried in-
creasing the size of n-best lists used by MERT.
Experiments with tailored translation model are
significantly better than the baseline but the im-
provement against the experiment with only the lan-
guage model adapted (with the corresponding 100
sentences selected) is very small.
5.3 Discussion of Domain Adaptation
Experiments
According to the results, using Lucene improves
translation performance already in the case when
only three sentences are selected for each translated
sentence. Our results are further supported by the
contrastive setup that used a language model cre-
ated from a random selection of the same number of
sentences?the translation quality even slightly de-
graded.
On the other hand, adding more sentences to lan-
guage model further improves results and the best
result is achieved when the language model is cre-
ated using the whole monolingual corpus. This
could have two reasons:
Too good domain match. The domain of the
whole monolingual corpus is too close to the test
corpus. Adding the whole monolingual corpus is
thus the best option. For more diverse monolingual
data, some domain-aware subsampling like our ap-
proach is likely to actually help.
Our style of retrieval. Our queries to Lucene
represent sentences as simple bags of words. Lucene
prefers less frequent words and the structure of the
sentence is therefore often ignored. For example it
prefers to retrieve sentences with the same proper
name rather than sentences with similar phrases or
longer expressions. This may not be the best option
for language modelling.
Our method can thus be useful mainly in the case
when the data available are too large to be processed
as a whole. It can also highly reduce the compu-
tation power and time necessary to achieve good
translation quality: the result achieved using the lan-
guage model created via Lucene for 1000 selected
sentences is not significantly worse than the result
achieved using the whole monolingual corpus but
the required data are 5 times smaller.
377
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
S [C
S]
Sen
ten
ces
 Sim
ilar
 to 
Tra
nsl
ate
d T
S [C
S]
Re
ver
se 
Tra
nsl
ate
d S
ent
enc
es 
Sim
ilar
 to 
Tra
nsl
ate
d T
S [E
N]
Luc
ene
Ba
sel
ine
 Tr
ans
lati
on 
[EN
->C
S]
Do
ma
in A
dap
ted
 LM
Re
ver
se 
Tra
nsl
atio
n T
M
Re
ver
se 
Tra
nsl
atio
n [C
S->
EN]
Ori
gin
al L
M
Ori
gin
al T
M
Tes
t Se
t [EN
]
Tra
nsl
ate
d T
est
 Se
t [C
S]
Fin
al T
ran
sla
tion
 [EN
->C
S]
Figure 1: Scenario of reverse self-training.
Used N-Best Sel. Sents Sel. Sents Avg
Models per Trans. Sent. Total BLEU?std
None 100 ? 0 12.39?0.06
None 200 ? 0 12.4?0.03
LM + TM 100 100 502k 14.32?0.13
LM + TM 200 100 502k 14.36?0.07
Table 5: Results of experiments with Lucene, translation model applied.
5.4 Tuning Towards Selected Data
Domain adaptation can also be done by selecting a
suitable development corpus (Zheng et al, 2010; Li
et al, 2004). The final model parameters depend on
the domain of the development corpus. By choos-
ing a development corpus that is close to our test
set we might tune in the right direction. We imple-
mented this adaptation by querying the source side
of our large parallel corpus using the source side of
the test corpus. After that, the development corpus
is constructed from the selected sentences and their
corresponding reference translations.
This experiment uses a fixed model based on the
news section of CzEng 1.0. We only use different
tuning sets and run the MERT optimization. All the
resulting systems are tested on the WMT11 test set:
Baseline system is tuned on 2489 sentence pairs
selected randomly from whole CzEng 1.0 parallel
corpus. Lucene system uses 2489 sentence pairs se-
lected from CzEng 1.0 using Lucene. The selection
is done by choosing the most similar sentences to the
source side of the final test set. WMT10 system is
System avg BLEU?std
Baseline 11.41?0.25
Lucene 12.31?0.01
WMT10 12.37?0.02
Perfect selection 12.64?0.02
Bad selection 6.37?0.64
Table 6: Results of tuning with different corpora
tuned on 2489 sentence pairs of WMT10 test set. To
identify an upper bound, we also include a Perfect
selection system which is tuned on the final WMT11
test set. Naturally, this is not a fair competitor.
In order to make the results more reliable, it is
necessary to repeat the experiment several times
(Clark et al, 2011). Lucene and the WMT10 system
were tuned 3 times while baseline system was tuned
9 times because of randomness in selection of tun-
ing corpora (3 different tuning corpora each tuned 3
times). The results are shown in Table 6.
Even though the variance of the baseline system
is high (because we randomly selected corpora 3
378
times), the difference in scores between baseline
and Lucene system is high enough to conclude that
tuning on Lucene-selected corpus helps translation
quality. Still it does not give better BLEU score
than system tuned on WMT10 corpus. One possi-
ble reason is that the whole CzEng 1.0 is of some-
what lower quality than the news section. Given that
our final test set (WMT11) is also from the news
domain, tuning towards WMT10 corpus probably
leads to a better domain adaptation that tuning to-
wards all the domains in CzEng.
The tuning set must not overlap with the training
set. To illustrate the problem, we did a small exper-
iment with the same settings as above and randomly
selected 2489 sentences from training corpora. We
again ran the random selection 3 times and tuned 3
times with each of the extracted tuning sets, see the
?Bad selection? in Table 6.
In all the experiments with badly selected sen-
tences, the distortion and language model get an
extremely low weight compared to the weights of
translation model. This is because they are not use-
ful in translation of tuning data which was already
seen during training. Instead of reordering two short
phrases A and B, system already knows the transla-
tion of the phrase A B so no distortion is needed. On
unseen sentences, such weights lead to poor results.
This amplifies a drawback of our approach:
source texts have to be known prior to system tuning
or even before phrase extraction.
There are methods available that could tackle this
problem. Wuebker et al (2010) store phrase pair
counts per sentence when extracting phrases and
thus they can reestimate the probabilities when a
sentence has to be excluded from the phrase tables.
For large parallel corpora, suffix arrays (Callison-
Burch et al, 2005) have been used. Suffix arrays
allow for a quick retrieval of relevant sentence pairs,
the phrase extraction is postponed and performed on
the fly for each input sentence. It is trivial to fil-
ter out sentences belonging to the tuning set during
this delayed extraction. With dynamic suffix arrays
(Levenberg et al, 2010), one could even simply re-
move the tuning sentences from the suffix array.
6 Submitted Systems
This paper covers the submissions CU-TAMCH-BOJ.
We translated from English into Czech. Our setup
was very similar to CU-BOJAR (Bojar et al, 2012a),
but our primary submission is tuned on multiple ref-
erence translations as described in Section 4.
Apart from the additional references, this is a con-
strained setup. CzEng 1.0 were the only parallel data
used in training. We used a factored model to trans-
late the combination of English surface form and
part-of-speech tag into Czech form+POS. We used
separate 6-gram language models trained on CzEng
1.0 (interpolated by domain) and all News Crawl
corpora (18.3M setences, interpolated by years).
Additionaly, we created an 8-gram language model
on target POS tags. For reordering, we employed a
lexicalized model trained on CzEng 1.0.
Table 7 summarizes the official result of the pri-
mary submission and a contrastive baseline (tuned to
just one reference translation). There is a slight de-
crease in BLEU, but the translation error rate (TER)
is slightly better when more references were used.
The differences are however very small, suggesting
that tuning to more references did not have any sig-
nificant effect.
System BLEU TER
multiple references 14.5 0.765
contrastive baseline 14.6 0.774
Table 7: Scores of the submitted systems.
7 Conclusion
We showed that CzEng 1.0 is of better overall qual-
ity than its predecessor. We described a technique
for reducing phrase-table OOV rate, but achieved no
improvement for WMT12. Similarly, tuning to mul-
tiple references did not prove very beneficial.
We introduced a couple of techniques that exploit
full-text search in large corpora. We showed that
adding selected sentences as an additional LM im-
proves translations. Adding a new phrase table ac-
quired via reverse self-training resulted only in small
gains. Tuning to selected sentences resulted in a
better system than tuning to a random set. How-
ever the Lucene-selected corpus fails to outperform
good-quality in-domain tuning data.
379
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain adaptation via pseudo in-domain data selec-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 355?362, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, StatMT
?09, pages 182?189, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran. 2012a.
Probes in a Taxonomy of Factored Phrase-Based Mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics. Submit-
ted.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The Joy of Parallelism with CzEng
1.0. In Proceedings of LREC2012, Istanbul, Turkey,
May. ELRA, European Language Resources Associa-
tion. In print.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the ACL, pages 255?262.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011. Better Hypothesis Testing for Statistical Ma-
chine Translation: Controlling for Optimizer Instabil-
ity. In Proceedings of the Association for Computa-
tional Lingustics. Association for Computational Lin-
guistics.
Hal Daume?, III and Jagadeesh Jagarlamudi. 2011. Do-
main adaptation for machine translation by mining un-
seen words. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies: short papers - Vol-
ume 2, HLT ?11, pages 407?412, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English Translation System. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 337?343, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
EMNLP 2004, Barcelona, Spain.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on trans-
lation model adaptation using monolingual data. In
Proceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 284?293, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Abby Levenberg, Chris Callison-Burch, and Miles Os-
borne. 2010. Stream-based translation models for
statistical machine translation. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the ACL, pages 394?402.
Mu Li, Yinggong Zhao, Dongdong Zhang, and Ming
Zhou. 2004. Adaptive development data selection for
log-linear model in statistical machine translation. In
In Proceedings of COLING 2004.
Coskun Mermer and Murat Saraclar. 2011. Un-
supervised Turkish Morphological Segmentation for
Statistical Machine Translation. Abstract at Ma-
chine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Daniel Stein, David Vilar, Stephan Peitz, Markus Freitag,
Matthias Huck, and Hermann Ney. 2011. A Guide to
Jane, an Open Source Hierarchical Translation Toolkit.
Prague Bulletin of Mathematical Linguistics, 95:5?18,
March.
John Tinsley, Mary Hearne, and Andy Way. 2009. Ex-
ploiting parallel treebanks to improve phrase-based
statistical machine translation. In Alexander F. Gel-
bukh, editor, CICLing, volume 5449 of Lecture Notes
in Computer Science, pages 318?331. Springer.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proceedings of the 22nd International Conference on
Computational Linguistics - Volume 1, COLING ?08,
pages 993?1000, Stroudsburg, PA, USA. Association
for Computational Linguistics.
380
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
475?484.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Proceed-
ings of the 20th international conference on Compu-
tational Linguistics, COLING ?04, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Zhongguang Zheng, Zhongjun He, Yao Meng, and Hao
Yu. 2010. Domain adaptation for statistical machine
translation in development corpus selection. In Uni-
versal Communication Symposium (IUCS), 2010 4th
International, pages 2 ?7, oct.
381
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 141?147,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
PhraseFix: Statistical Post-Editing of TectoMT
Petra Galu?c??kov?, Martin Popel, and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk? n?me?st? 25, Prague, Czech Republic
{galuscakova,popel,bojar}@ufal.mff.cuni.cz
Abstract
We present two English-to-Czech systems
that took part in the WMT 2013 shared
task: TECTOMT and PHRASEFIX. The
former is a deep-syntactic transfer-based
system, the latter is a more-or-less stan-
dard statistical post-editing (SPE) applied
on top of TECTOMT. In a brief survey, we
put SPE in context with other system com-
bination techniques and evaluate SPE vs.
another simple system combination tech-
nique: using synthetic parallel data from
TECTOMT to train a statistical MT sys-
tem (SMT). We confirm that PHRASEFIX
(SPE) improves the output of TECTOMT,
and we use this to analyze errors in TEC-
TOMT. However, we also show that ex-
tending data for SMT is more effective.
1 Introduction
This paper describes two submissions to the
WMT 2013 shared task:1 TECTOMT ? a deep-
syntactic tree-to-tree system and PHRASEFIX ?
statistical post-editing of TECTOMT using Moses
(Koehn et al, 2007). We also report on exper-
iments with another hybrid method where TEC-
TOMT is used to produce additional (so-called
synthetic) parallel training data for Moses. This
method was used in CU-BOJAR and CU-DEPFIX
submissions, see Bojar et al (2013).
2 Overview of Related Work
The number of approaches to system combination
is enormous. We very briefly survey those that
form the basis of our work reported in this paper.
2.1 Statistical Post-Editing
Statistical post-editing (SPE, see e.g. Simard et al
(2007), Dugast et al (2009)) is a popular method
1http://www.statmt.org/wmt13
for improving outputs of a rule-based MT sys-
tem. In principle, SPE could be applied to any
type of first-stage system including a statistical
one (Oflazer and El-Kahlout, 2007; B?chara et al,
2011), but most benefit could be expected from
post-editing rule-based MT because of the com-
plementary nature of weaknesses and advantages
of rule-based and statistical approaches.
SPE is usually done with an off-the-shelf SMT
system (e.g. Moses) which is trained on output of
the first-stage system aligned with reference trans-
lations of the original source text. The goal of SPE
is to produce translations that are better than both
the first-stage system alone and the second-stage
SMT trained on the original training data.
Most SPE approaches use the reference trans-
lations from the original training parallel corpus
to train the second-stage system. In contrast,
Simard et al (2007) use human-post-edited first-
stage system outputs instead. Intuitively, the lat-
ter approach achieves better results because the
human-post-edited translations are closer to the
first-stage output than the original reference trans-
lations. Therefore, SPE learns to perform the
changes which are needed the most. However, cre-
ating human-post-edited translations is laborious
and must be done again for each new (version of
the) first-stage system in order to preserve its full
advantage over using the original references.2
Rosa et al (2013) have applied SPE on
English?Czech SMT outputs. They have used
the approach introduced by B?chara et al (2011),
but no improvement was achieved. However, their
rule-based post-editing were found helpful.
Our SPE setting (called PHRASEFIX) uses
TECTOMT as the first-stage system and Moses as
the second-stage system. Ideally, TECTOMT pre-
2If more reference translations are available, it would be
beneficial to choose such references for training SPE which
are most similar to the first-stage outputs. However, in our
experiments only one reference is available.
141
serves well-formed syntactic sentence structures,
and the SPE (Moses) fixes low fluency wordings.
2.2 MT Output Combination
An SPE system is trained to improve the output
of a single first-stage system. Sometimes, more
(first-stage) systems are available, and we would
like to combine them. In MT output selection,
for each sentence one system?s translation is se-
lected as the final output. In MT output combi-
nation, the final translation of each sentence is a
combination of phrases from several systems. In
both approaches, the systems are treated as black
boxes, so only their outputs are needed. In the
simplest setting, all systems are supposed to be
equally good/reliable, and the final output is se-
lected by voting, based on the number of shared n-
grams or language model scores. The number and
the identity of the systems to be combined there-
fore do not need to be known in advance. More so-
phisticated methods learn parameters/weights spe-
cific for the individual systems. These methods
are based e.g. on confusion networks (Rosti et al,
2007; Matusov et al, 2008) and joint optimization
of word alignment, word order and lexical choice
(He and Toutanova, 2009).
2.3 Synthetic Data Combination
Another way to combine several first-stage sys-
tems is to employ a standard SMT toolkit, e.g.
Moses. The core of the idea is to use the n first-
stage systems to prepare synthetic parallel data
and include them in the training data for the SMT.
Corpus Combination (CComb) The easiest
method is to use these n newly created paral-
lel corpora as additional training data, i.e. train
Moses on a concatenation of the original paral-
lel sentences (with human-translated references)
and the new parallel sentences (with machine-
translated pseudo-references).
Phrase Table Combination (PTComb) An-
other method is to extract n phrase tables in
addition to the original phrase table and ex-
ploit the Moses option of multiple phrase tables
(Koehn and Schroeder, 2007). This means that
given the usual five features (forward/backward
phrase/lexical log probability and phrase penalty),
we need to tune 5 ? (n+1) features. Because such
MERT (Och, 2003) tuning may be unstable for
higher n, several methods were proposed where
the n+1 phrase tables are merged into a single one
(Eisele et al, 2008; Chen et al, 2009). Another is-
sue of phrase table combination is that the same
output can be achieved with phrases from several
phrase tables, leading to spurious ambiguity and
thus less diversity in n-best lists of a given size
(see Chen et al (2009) for one possible solution).
CComb does not suffer from the spurious ambi-
guity issue, but it does not allow to tune special
features for the individual first-stage systems.
In our experiments, we use both CComb and
PTComb approaches. In PTComb, we use TEC-
TOMT as the only first-stage system and Moses as
the second-stage system. We use the two phrase
tables separately (the merging is not needed; 5 ? 2
is still a reasonable number of features in MERT).
In CComb, we concatenate English?Czech par-
allel corpus with English??synthetic Czech? cor-
pus translated from English using TECTOMT. A
single phrase table is created from the concate-
nated corpus.
3 TECTOMT
TECTOMT is a linguistically-motivated tree-to-
tree deep-syntactic translation system with trans-
fer based on Maximum Entropy context-sensitive
translation models (Marec?ek et al, 2010) and
Hidden Tree Markov Models (?abokrtsk? and
Popel, 2009). It employs some rule-based compo-
nents, but the most important tasks in the analysis-
transfer-synthesis pipeline are based on statistics
and machine learning. There are three main rea-
sons why it is a suitable candidate for SPE and
other hybrid methods.
? TECTOMT has quite different distribution
and characteristics of errors compared to
standard SMT (Bojar et al, 2011).
? TECTOMT is not tuned for BLEU using
MERT (its development is rather driven by hu-
man inspection of the errors although different
setups are regularly evaluated with BLEU as an
additional guidance).
? TECTOMT uses deep-syntactic dependency
language models in the transfer phase, but it
does not use standard n-gram language mod-
els on the surface forms because the current syn-
thesis phase supports only 1-best output.
The version of TECTOMT submitted to WMT
2013 is almost identical to the WMT 2012 version.
Only a few rule-based components (e.g. detection
of surface tense of English verbs) were refined.
142
Corpus Sents TokensCzech English
CzEng 15M 205M 236M
tmt(CzEng) 15M 197M 236M
Czech Web Corpus 37M 627M ?
WMT News Crawl 25M 445M ?
Table 1: Statistics of used data.
4 Common Experimental Setup
All our systems (including TECTOMT) were
trained on the CzEng (Bojar et al, 2012) par-
allel corpus (development and evaluation sub-
sets were omitted), see Table 1 for statistics.
We translated the English side of CzEng with
TECTOMT to obtain ?synthetic Czech?. This
way we obtained a new parallel corpus, denoted
tmt(CzEng), with English? synthetic Czech sen-
tences. Analogically, we translated the WMT
2013 test set (newstest2013) with TECTOMT and
obtained tmt(newstest2013). Our baseline SMT
system (Moses) trained on CzEng corpus only was
then also used for WMT 2013 test set transla-
tion, and we obtained smt(newstest2013). For all
MERT tuning, newstest2011 was used.
4.1 Alignment
All our parallel data were aligned with GIZA++
(Och and Ney, 2003) and symmetrized with
the ?grow-diag-final-and? heuristics. This ap-
plies also to the synthetic corpora tmt(CzEng),
tmt(newstest2013),3 and smt(newstest2013).
For the SPE experiments, we decided to base
alignment on (genuine and synthetic Czech) lem-
mas, which could be acquired directly from the
TECTOMT output. For the rest of the experiments,
we approximated lemmas with just the first four
lowercase characters of each (English and Czech)
token.
4.2 Language Models
In all our experiments, we used three language
models on truecased forms: News Crawl as pro-
vided by WMT organizers,4 the Czech side of
CzEng and the Articles section of the Czech Web
3Another possibility was to adapt TECTOMT to output
source-to-target word alignment, but GIZA++ was simpler to
use also due to different internal tokenization in TECTOMT
and our Moses pipeline.
4The deep-syntactic LM of TECTOMT was trained only
on this News Crawl data ? http://www.statmt.org/
wmt13/translation-task.html (sets 2007?2012).
BLEU 1-TER
TECTOMT 14.71?0.53 35.61?0.60
PHRASEFIX 17.73?0.54 35.63?0.65
Filtering 14.68?0.50 35.47?0.57
Mark Reliable Phr. 17.87?0.55 35.57?0.66
Mark Identities 17.87?0.57 35.85?0.68
Table 2: Comparison of several strategies of SPE.
Best results are in bold.
Corpus (Spoustov? and Spousta, 2012).
We used SRILM (Stolcke, 2002) with modified
Kneser-Ney smoothing. We trained 5-grams on
CzEng; on the other two corpora, we trained 7-
grams and pruned them if the (training set) per-
plexity increased by less than 10?14 relative. The
domain of the pruned corpora is similar to the test
set domain, therefore we trained 7-grams on these
corpora. Adding CzEng corpus can then increase
the results only very slightly ? training 5-grams on
CzEng is therefore sufficient and more efficient.
Each of the three LMs got its weight as-
signed by MERT. Across the experiments, Czech
Web Corpus usually gained the largest portion of
weights (40?17% of the total weight assigned to
language models), WMT News Crawl was the sec-
ond (32?15%), and CzEng was the least useful
(15?7%), perhaps due to its wide domain mixture.
5 SPE Experiments
We trained a base SPE system as described in Sec-
tion 2.1 and dubbed it PHRASEFIX.
First two rows of Table 2 show that the first-
stage TECTOMT system (serving here as the base-
line) was significantly improved in terms of BLEU
(Papineni et al, 2002) by PHRASEFIX (p < 0.001
according to the paired bootstrap test (Koehn,
2004)), but the difference in TER (Snover et
al., 2006) is not significant.5 The preliminary
results of WMT 2013 manual evaluation show
only a minor improvement: TECTOMT=0.476
vs. PHRASEFIX=0.484 (higher means better, for
details on the ranking see Callison-Burch et al
(2012)).
5The BLEU and TER results reported here slightly differ
from the results shown at http://matrix.statmt.
org/matrix/systems_list/1720 because of differ-
ent tokenization and normalization. It seems that statmt.org
disables the --international-tokenization
switch, so e.g. the correct Czech quotes (?word?) are not
tokenized, hence the neighboring tokens are never counted
as matching the reference (which is tokenized as " word ").
143
Despite of the improvement, PHRASEFIX?s
phrase table (synthetic Czech ? genuine Czech)
still contains many wrong phrase pairs that worsen
the TECTOMT output instead of improving it.
They naturally arise in cases where the genuine
Czech is a too loose translation (or when the
English-Czech sentence pair is simply misaligned
in CzEng), and the word alignment between gen-
uine and synthetic Czech struggles.
Apart from removing such garbage phrase pairs,
it would also be beneficial to have some control
over the SPE. For instance, we would like to gen-
erally prefer the original output of TECTOMT ex-
cept for clear errors, so only reliable phrase pairs
should be used. We examine several strategies:
Phrase table filtering. We filter out all phrase
pairs with forward probability ? 0.7 and all sin-
gleton phrase pairs. These thresholds were set
based on our early experiments. Similar filtering
was used by Dugast et al (2009).
Marking of reliable phrases. This strategy is
similar to the previous one, but the low-frequency
phrase pairs are not filtered-out. Instead, a special
feature marking these pairs is added. The subse-
quent MERT of the SPE system selects the best
weight for this indicator feature. The frequency
and probability thresholds for marking a phrase
pair are the same as in the previous case.
Marking of identities A special feature indicat-
ing the equality of the source and target phrase in
a phrase pair is added. In general, if the output
of TECTOMT matched the reference, then such
output was probably good and does not need any
post-editing. These phrase pairs should be perhaps
slightly preferred by the SPE.
As apparent from Table 2, marking either reli-
able phrases or identities is useful in our SPE set-
ting in terms of BLEU score. In terms of TER
measure, marking the identities slightly improves
PHRASEFIX. However, none of the improvements
is statistically significant.
6 Data Combination Experiments
We now describe experiments with phrase table
and corpus combination. In the training step, the
source-language monolingual corpus that serves
as the basis of the synthetic parallel data can
be:
? the source side of the original parallel training
corpus (resulting in tmt(CzEng)),
? a huge source-language monolingual corpus for
which no human translations are available (we
have not finished this experiment yet),
? the source side of the test set (resulting in
tmt(newstest2013) if translated by TECTOMT
or smt(newstest2013) if translated by baseline
configuration of Moses trained on CzEng), or
? a combination of the above.
There is a trade-off in the choice: the source
side of the test set is obviously most useful for
the given input, but it restricts the applicability (all
systems must be installed or available online in the
testing time) and speed (we must wait for the slow-
est system and the combination).
So far, in PTComb we tried adding the full
synthetic CzEng (?CzEng + tmt(CzEng)?), adding
the test set (?CzEng + tmt(newstest2013)? and
?CzEng + smt(newstest2013)?), and adding both
(?CzEng + tmt(CzEng) + tmt(newstest2013)?). In
CComb, we concatenated CzEng and full syn-
thetic CzEng (?CzEng + tmt(CzEng)?).
There are two flavors of PTComb: either the
two phrase tables are used both at once as alter-
native decoding paths (?Alternative?), where each
source span is equipped with translation options
from any of the tables, or the synthetic Czech
phrase table is used only as a back-off method if a
source phrase is not available in the primary table
(?Back-off?). The back-off model was applied to
source phrases of up to 5 tokens.
Table 3 summarizes our results with phrase ta-
ble and corpus combination. We see that adding
synthetic data unrelated to the test set does bring
only a small benefit in terms of BLEU in the case
of CComb, and we see a small improvement in
TER in two cases. Adding the (synthetic) transla-
tion of the test set helps. However, adding trans-
lated source side of the test set is helpful only if
it is translated by the TECTOMT system. If our
baseline system is used for this translation, the re-
sults even slightly drop.
Somewhat related experiments for pivot lan-
guages by Galu?c??kov? and Bojar (2012) showed
a significant gain when the outputs of a rule-based
system were added to the training data of Moses.
In their case however, the genuine parallel corpus
was much smaller than the synthetic data. The
benefit of unrelated synthetic data seems to van-
ish with larger parallel data available.
144
Training Data for Moses Decoding Type BLEU 1-TER
baseline: CzEng ? 18.52?0.57 36.41?0.66
tmt(CzEng) ? 15.96?0.53 33.67?0.63
CzEng + tmt(CzEng) CComb 18.57?0.57 36.47?0.64
CzEng + tmt(CzEng) PTComb Alternative 18.42?0.58 36.47?0.65
CzEng + tmt(CzEng) PTComb Back-off 18.38?0.57 36.25?0.65
CzEng + tmt(newstest2013) PTComb Alternative 18.68?0.57 37.00?0.65
CzEng + smt(newstest2013) PTComb Alternative 18.46?0.54 36.59?0.65
CzEng + tmt(CzEng) + tmt(newstest2013) PTComb Alternative 18.85?0.58 37.03?0.66
Table 3: Comparison of several strategies used for Synthetic Data Combination (PTComb ? phrase table
combination and CComb ? corpus combination).
BLEU Judged better
SPE 17.73?0.54 123
PTComb 18.68?0.57 152
Table 4: Automatic (BLEU) and manual (number
of sentences judged better than the other system)
evaluation of SPE vs. PTComb.
7 Discussion
7.1 Comparison of SPE and PTComb
Assuming that our first-stage system, TECTOMT,
guarantees the grammaticality of the output (sadly
often not quite true), we see SPE and PTComb
as two complementary methods that bring in the
goods of SMT but risk breaking the grammati-
cality. Intuitively, SPE feels less risky, because
one would hope that the post-edits affect short se-
quences of words and not e.g. the clause structure.
With PTComb, one relies purely on the phrase-
based model and its well-known limitations with
respect to grammatical constraints.
Table 4 compares the two approaches empir-
ically. For SPE, we use the default PHRASE-
FIX; for PTComb, we use the option ?CzEng +
tmt(newstest2013)?. The BLEU scores are re-
peated.
We ran a small manual evaluation where three
annotators judged which of the two outputs was
better. The identity of the systems was hidden,
but the annotators had access to both the source
and the reference translation. Overall, we col-
lected 333 judgments over 120 source sentences.
Of the 333 judgments, 17 marked the two systems
as equally correct, and 44 marked the systems as
incomparably wrong. Across the remaining 275
non-tying comparisons, PTComb won ? 152 vs.
123.
We attribute the better performance of PTComb
to the fact that, unlike SPE, it has direct access to
the source text. Also, the risk of flawed sentence
structure in PTComb is probably not too bad, but
this can very much depend on the language pair.
English?Czech translation does not need much
reordering in general.
Based on the analysis of the better marked re-
sults of the PTComb system, the biggest problem
is the wrong selection of the word and word form,
especially for verbs. PTComb also outperforms
SPE in processing of frequent phrases and sub-
ordinate clauses. This problem could be solved
by enhancing fluency in SPE or by incorporat-
ing more training data. Another possibility would
be to modify TECTOMT system to produce more
than one-best translation as the correct word or
word form may be preserved in sequel transla-
tions.
7.2 Error Analysis of TECTOMT
While SPE seems to perform worse, it has a
unique advantage: it can be used as a feedback
for improving the first stage system. We can either
inspect the filtered SPE phrase table or differences
in translated sentences.
After submitting our WMT 2013 systems, this
comparison allowed us to spot a systematic error
in TECTOMT tagging of latin-origin words:
source pancreas
TECTOMT slinivek [plural]
PHRASEFIX slinivky [singular] br?i?n?
The part-of-speech tagger used in TECTOMT in-
correctly detects pancreas as plural, and the wrong
morphological number is used in the synthesis.
PHRASEFIX correctly learns that the plural form
slinivek should be changed to singular slinivky,
which has also a higher language model score.
Moreover, PHRASEFIX also learns that the trans-
145
lation of pancreas should be two words (br?i?n?
means abdominal). TECTOMT currently uses a
simplifying assumption of 1-to-1 correspondence
between content words, so it is not able to produce
the correct translation in this case.
Another example shows where PHRASEFIX
recovered from a lexical gap in TECTOMT:
source people who are strong-willed
TECTOMT lid? , kter?? jsou siln? willed
PHRASEFIX lid? , kter?? maj? silnou vu?li
TECTOMT?s primary translation model considers
strong-willed an OOV word, so a back-off dictio-
nary specialized for hyphen compounds is used.
However, this dictionary is not able to translate
willed. PHRASEFIX corrects this and also the
verb jsou = are (the correct Czech translation is
maj? silnou vu?li = have a strong will).
Finally, PHRASEFIX can also break things:
source You won?t be happy here
TECTOMT Nebudete ?t?astn? tady
PHRASEFIX Vy tady ?t?astn? [you here happy]
Here, PHRASEFIX damaged the translation by
omitting the negative verb nebudete = you won?t.
8 Conclusion
Statistical post-editing (SPE) and phrase table
combination (PTComb) can be seen as two com-
plementary approaches to exploiting the mutual
benefits of our deep-transfer system TECTOMT
and SMT.
We have shown that SPE improves the results of
TECTOMT. Several variations of SPE have been
examined, and we have further improved SPE re-
sults by marking identical and reliable phrases us-
ing a special feature. However, SMT still out-
performs SPE according to BLEU and TER mea-
sures. Finally, employing PTComb, we have im-
proved the baseline SMT system by utilizing ad-
ditional data translated by the TECTOMT system.
A small manual evaluation suggests that PTComb
is on average better than SPE, though in about one
third of sentences SPE was judged better. In our
future experiments, we plan to improve SPE by
applying techniques suited for monolingual align-
ment, e.g. feature-based aligner considering word
similarity (Rosa et al, 2012) or extending the par-
allel data with vocabulary identities to promote
alignment of the same word form (Dugast et al,
2009). Marking and filtering methods for SPE also
deserve a deeper study. As for PTComb, we plan
to combine several sources of synthetic data (in-
cluding a huge source-language monolingual cor-
pus).
Acknowledgements
This research is supported by the grants
GAUK 9209/2013, FP7-ICT-2011-7-288487
(MosesCore) of the European Union and SVV
project number 267 314. We thank the two
anonymous reviewers for their comments.
References
Hanna B?chara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. MT Summit XIII, pages 308?315.
Ondr?ej Bojar, Milo? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proc. of WMT, pages 1?11,
Edinburgh, Scotland. ACL.
Ondr?ej Bojar, Zdene?k ?abokrtsk?, Ondr?ej Du?ek, Pe-
tra Galu?c??kov?, Martin Majli?, David Marec?ek, Jir??
Mar??k, Michal Nov?k, Martin Popel, and Ale? Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928, Istanbul,
Turkey. ELRA.
Ondr?ej Bojar, Rudolf Rosa, and Ale? Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proc. of WMT.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proc. of WMT, Montreal,
Canada. ACL.
Yu Chen, Michael Jellinghaus, Andreas Eisele,
Yi Zhang, Sabine Hunsicker, Silke Theison, Chris-
tian Federmann, and Hans Uszkoreit. 2009. Com-
bining Multi-Engine Translations with Moses. In
Proc. of WMT, pages 42?46, Athens, Greece. ACL.
Lo?c Dugast, Jean Senellart, and Philipp Koehn.
2009. Statistical Post Editing and Dictionary Ex-
traction: Systran/Edinburgh Submissions for ACL-
WMT2009. In Proc. of WMT, pages 110?114,
Athens, Greece. ACL.
Andreas Eisele, Christian Federmann, Herv? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to Integrate Multi-
ple Rule-Based Machine Translation Engines into a
Hybrid System. In Proc. of WMT, pages 179?182,
Columbus, Ohio. ACL.
Petra Galu?c??kov? and Ondr?ej Bojar. 2012. Improving
SMT by Using Parallel Data of a Closely Related
Language. In Proc. of HLT, pages 58?65, Amster-
dam, Netherlands. IOS Press.
146
Xiaodong He and Kristina Toutanova. 2009. Joint Op-
timization for Machine Translation System Combi-
nation. In Proc. of EMNLP, pages 1202?1211, Sin-
gapore. ACL.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Trans-
lation. In Proc. of WMT, pages 224?227, Prague,
Czech Republic. ACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proc. of ACL, pages 177?180, Prague, Czech Re-
public. ACL.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of
EMNLP, Barcelona, Spain.
David Marec?ek, Martin Popel, and Zdene?k ?abokrt-
sk?. 2010. Maximum entropy translation model
in dependency-based MT framework. In Proc. of
MATR, pages 201?206. ACL.
Evgeny Matusov, Gregor Leusch, Rafael E. Banchs,
Nicola Bertoldi, Daniel Dechelotte, Marcello Fed-
erico, Muntsin Kolss, Young-Suk Lee, Jose B.
Marino, Matthias Paulik, Salim Roukos, Holger
Schwenk, and Hermann Ney. 2008. System Combi-
nation for Machine Translation of Spoken and Writ-
ten Language. IEEE, 16(7):1222?1237.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29(1):19?51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proc. of ACL,
Sapporo, Japan.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proc. of WMT, pages 25?32. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Stroudsburg, PA, USA. ACL.
Rudolf Rosa, Ondr?ej Du?ek, David Marec?ek, and Mar-
tin Popel. 2012. Using Parallel Features in Parsing
of Machine-Translated Sentences for Correction of
Grammatical Errors. In Proc. of SSST, pages 39?48,
Jeju, Republic of Korea. ACL.
Rudolf Rosa, David Marec?ek, and Ale? Tamchyna.
2013. Deepfix: Statistical Post-editing of Statistical
Machine Translation Using Deep Syntactic Analy-
sis. Sofia, Bulgaria. ACL.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple
Machine Translation Systems. In Proc. of NAACL,
pages 228?235, Rochester, New York. ACL.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical Phrase-Based Post-Editing. In
Proc. of NAACL, pages 508?515, Rochester, New
York. ACL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of Association for Machine Translation in
the Americas, pages 223?231.
Johanka Spoustov? and Miroslav Spousta. 2012. A
High-Quality Web Corpus of Czech. In Proc. of
LREC, Istanbul, Turkey. ELRA.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of ICSLP, pages
257?286.
Zdene?k ?abokrtsk? and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proc. of IJCNLP, pages 145?148,
Suntec, Singapore.
147
