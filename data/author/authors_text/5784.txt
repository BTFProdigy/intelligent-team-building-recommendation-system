Using a Mixture of N-Best Lists from Multiple MT Systems
in Rank-Sum-Based Confidence Measure for MT Outputs ?
Yasuhiro Akiba?,?, Eiichiro Sumita?, Hiromi Nakaiwa?,
Seiichi Yamamoto?, and Hiroshi G. Okuno?
? ATR Spoken Language Translation Research Laboratories
2-2-2 Hikaridai, Keihana Science City, Kyoto 619-0288, Japan
? Graduate School of Informatics, Kyoto University
Yoshida-Honmachi, Sakyo-ku, Kyoto 606-8501, Japan
{yasuhiro.akiba, eiichiro.sumita, hiromi.nakaiwa seiichi.yamamoto}@atr.jp, and okuno@i.kyoto-u.ac.jp
Abstract
This paper addressees the problem of eliminat-
ing unsatisfactory outputs from machine trans-
lation (MT) systems. The authors intend to
eliminate unsatisfactory MT outputs by using
confidence measures. Confidence measures for
MT outputs include the rank-sum-based confi-
dence measure (RSCM) for statistical machine
translation (SMT) systems. RSCM can be ap-
plied to non-SMT systems but does not always
work well on them. This paper proposes an
alternative RSCM that adopts a mixture of the
N-best lists from multiple MT systems instead
of a single-system?s N-best list in the exist-
ing RSCM. In most cases, the proposed RSCM
proved to work better than the existing RSCM
on two non-SMT systems and to work as well
as the existing RSCM on an SMT system.
1 Introduction
This paper addresses the challenging problem of
eliminating unsatisfactory outputs from machine
translation (MT) systems, which are subsystems of
a speech-to-speech machine translation (S2SMT)
system. The permissible range of translation quality
by MT/S2SMT systems depends on the user. Some
users permit only perfect translations, while other
users permit even translations with flawed grammar.
Unsatisfactory MT outputs are those whose transla-
tion quality is worse than the level the user can per-
mit.
In this paper, the authors intend to eliminate un-
satisfactory outputs by using confidence measures
for MT outputs. The confidence measures1 indicate
how perfect/satisfactory the MT outputs are. In the
? This research was supported in part by the Ministry of Public
Management, Home Affairs, Posts and Telecommunications,
Japan.
1These confidence measures are a kind of automatic evalu-
ator such as mWER (Niessen et al, 2000) and BLEU (Papineni
et al, 2001). While mWER and BLEU cannot be used online,
these confidence measures can. This is because the former are
based on reference translations, while the latter is not.
discipline of MT, confidence measures for MT out-
puts have rarely been investigated.
The few existing confidence measures include
the rank-sum-based confidence measure (RSCM)
for statistical machine translation (SMT) systems,
Crank in (Ueffing et al, 2003). The basic idea
of this confidence measure is to roughly calculate
the word posterior probability by using ranks of
MT outputs in an N-best list from an SMT system.
In the discipline of non-parametric statistical test,
ranks of numerical values are commonly used in-
stead of the numerical values themselves for statis-
tical tests. In the case of the existing RSCM, the
ranks of probabilities of MT outputs in the N-best
list were used instead of the probabilities of the out-
puts themselves. The existing RSCM scores each
word in an MT output by summing the comple-
mented ranks of candidates in the N-best list that
contain the same word in a Levenshtein-aligned po-
sition (Levenshtein, 1966). When the confidence
values of all words in the MT output are larger than
a fixed threshold, the MT output is judged as cor-
rect/perfect. Otherwise, the output is judged as in-
correct/imperfect.
The existing RSCM does not always work well
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
Performance of existing method (A|BCD)
J2E SAT + Existing method
J2E HPAT + Existing method
J2E D3 + Existing method
Figure 1: Performance of the existing RSCM on three
different types of Japanese-to-English (J2E) MT sys-
tems: D3, HPAT, and SAT. The existing RSCM tried to
accept perfect MT outputs (grade A in Section 4) and to
reject imperfect MT outputs (grades B, C, and D in Sec-
tion 4).
on types of MT systems other than SMT systems.
Figure 1 shows the differences among the perfor-
mances, indicated by the Receiver Operating Char-
acteristics (ROC) curve (Section 4.1), of the exist-
ing RSCM on each of three MT systems (Section
4.2.1): D3, HPAT, and SAT (Doi and Sumita, 2003;
Imamura et al, 2003; Watanabe et al, 2003). Only
SAT is an SMT system; the others are not. The ideal
ROC curve is a square (0,1), (1,1), (1,0); thus, the
closer the curve is to a square, the better the perfor-
mance of the RSCM is. The performances of the
existing RSCM on the non-SMT systems, D3 and
HPAT, are much worse than that on the SMT sys-
tem, SAT.
The performance of the existing RSCM depends
on the goodness/density of MT outputs in the N-
best list from the system. However, the system?s
N-best list does not always give a good approxi-
mation of the total summation of the probability
of all candidate translations given the source sen-
tence/utterance. The N-best list is expected to ap-
proximate the total summation as closely as possi-
ble.
This paper proposes a method that eliminates
unsatisfactory top output by using an alternative
RSCM based on a mixture of N-best lists from mul-
tiple MT systems (Figure 2). The elimination sys-
tem is intended to be used in the selector architec-
ture, as in (Akiba et al, 2002). The total transla-
tion quality of the selector architecture proved to be
better than the translation quality of each element
MT system. The final output from the selection sys-
tem is the best among the satisfactory top2 outputs
from the elimination system. In the case of Fig-
ure 2, the selection system can receive zero to three
top MT outputs. When the selection system receive
fewer than two top MT outputs, the selection sys-
tem merely passes a null output or the one top MT
output.
The proposed RSCM differs from the existing
RSCM in its N-best list. The proposed RSCM re-
2To distinguish the best output from the selection system,
the MT output in the first place in each N-best list (e.g., N-best
lista in Figure 2 ) refers to the top MT output.
The best output
Elimination System
Selection System
Satisfactory top outputs
MTa MTb MTc
The top outputa
?
The M-th best outputa
Input
M-best lista
The top outputb
?
The M-th best outputb
M-best listb
The top outputc
?
The M-th best outputc
M-best listc
Figure 2: Image of our eliminator
ceives an M-best list from each element MT sys-
tem. Next, it sorts the mixture of the MT outputs in
all M-best lists in the order of the average product
(Section 3.2) of the scores of a language model and
a translation model (Akiba et al, 2002). This sorted
mixture is used instead of the system?s N-best list in
the existing RSCM.
To experimentally evaluate the proposed RSCM,
the authors applied the proposed RSCM and the ex-
isting RSCM to a test set of the Basic Travel Ex-
pression Corpus (Takezawa et al, 2002). The pro-
posed RSCM proved to work better than the exist-
ing RSCM on the non-SMT systems and to work as
well as the existing RSCM on the SMT system.
The next section outlines the existing RSCM.
Section 3 proposes our RSCM. Experimental results
are shown and discussed in Section 4. Finally, our
conclusions are presented in Section 5.
2 The Existing RSCM
The existing confidence measures include the rank-
sum-based confidence measure (RSCM) for SMT
systems (Ueffing et al, 2003). The basic idea of
this RSCM is to roughly calculate the word poste-
rior probability by using ranks of MT outputs in the
N-best list of an SMT system. That is, the ranks of
probabilities of MT outputs in the N-best list were
used instead of the probabilities of the outputs them-
selves, as in the non-parametric statistical test.
Hereafter, e?I1 and wIn1 denote the top output2
and the n-th best output in the N-best list, respec-
tively. e?i denotes the i-th word in the top MT output
e?I1. Li(e?I1, wIn1 ) denote the Levenshtein alignment3
(Levenshtein, 1966) of e?i on the n-th best output
wIn1 according to the top output e?I1. The existing
RSCM of the word e?i is the sum of the ranks of MT
outputs in an N-best list containing the word e?i in a
position that is aligned to i in the Levenshtein align-
ment, which is normalized by the total rank sum:
Crank(e?i) =
?N
n=1(N ? n) ? ?(e?i, Li(e?I1, wIn1 ))
N(N + 1)/2 ,
where ?(?, ?) is the Kronecker function, that is, if
words/morphemes x and y are the same, ?(x, y) =
1; otherwise, ?(x, y) = 0. Thus, only in the case
where e?i and Li(e?I1, wIn1 ) are the same, the rank of
the MT output wIn1 , N ? n, is summed. In the
calculation of Crank, N ? n is summed instead of
the rank n because ranks near the top of the N-best
list contribute more to the score Crank.
3This is the word on the n-th best output wIn1 , aligned with
the i-th word e?i, in the calculation of edit distance from the top
MT output e?I1 to the n-th best output wIn1 .
In this paper, the calculation of Crank is slightly
modified to sum N ? n + 1 so that the total sum-
mation is equal to N(N + 1)/2. Moreover, when
there are MT outputs that have the same score, such
MT outputs are assigned the average rank as in the
discipline of non-parametric statistical test.
As shown in Section 1, the existing RSCM does
not always work well on types of MT systems other
than SMT systems. This is because the system?s
N-best list does not always give a good approxi-
mation of the total summation of the probability
of all candidate translations given the source sen-
tence/utterance. The N-best list is expected to ap-
proximate the total summation as closely as possi-
ble.
3 Proposed Method
In this section, the authors propose a method that
eliminates unsatisfactory top output by using an al-
ternative RSCM based on a mixture of N-best lists
from multiple MT systems. The judgment that
the top output is satisfactory is based on the same
threshold comparison as the judgment that the top
output is perfect, as mentioned in Section 1. The
elimination system and the alternative RSCM are
explained in Sections 3.1 and 3.2, respectively.
3.1 Elimination system
This section proposes a method that eliminates
unsatisfactory top outputs by using an alternative
RSCM based on a mixture of N-best lists from mul-
tiple MT systems (Figure 3). This elimination sys-
tem is intended to be used in the selector architec-
ture (Figure 2). The elimination system receives
an M-best list from each element MT system and
outputs only top2 outputs whose translation quality
is better than or as good as that which the user can
permit. In the case of Figure 3, the number of MT
systems is three; thus, the elimination system can
output zero to three top MT outputs, which depends
on the number of the eliminated top outputs.
MTa MTb MTc
The top outputa
?
The M-th best outputa
Input
Satisfactory top outputs
M-best lista
The top outputb
?
The M-th best outputb
M-best listb
The top outputc
?
The M-th best outputc
M-best listc
3M outputs sorted in the higher order
Sorter based on SMT?s scoring system
Checker based on rank sum
Elimination System
Figure 3: Proposed RSCM
The proposed elimination system judges whether
a top output is satisfactory by using a threshold
comparison, as in (Ueffing et al, 2003). When
the confidence values of all words in the top out-
put, which are calculated by using the alternative
RSCM explained in Section 3.2, are larger than a
fixed threshold, the top output is judged as satisfac-
tory. Otherwise, the top output is judged as unsatis-
factory. The threshold was optimized on a develop-
ment corpus.
3.2 The proposed RSCM
The proposed RSCM is an extension of the existing
RSCM outlined in Section 2. The proposed RSCM
differs from the existing RSCM in the adopted N-
best list (Figure 3). The proposed RSCM receives
an M-best list from each element MT system. Next
the proposed RSCM sorts the mixture of all the MT
outputs in the order of the average product of the
scores of a language model and a translation model
(Akiba et al, 2002). This sorted mixture is alter-
natively used instead of the system?s N-best list in
the existing RSCM. That is, the proposed RSCM
checks whether it accepts/rejects each top MT out-
put in the original M-best lists by using the sorted
mixture; on the other hand, the existing RSCM
checks whether it accepts/rejects the top MT out-
put in the system?s N-best list by using the system?s
N-best.
For scoring MT outputs, the proposed RSCM
uses a score based on a translation model called
IBM4 (Brown et al, 1993) (TM-score) and a score
based on a language model for the translation tar-
get language (LM-score). As Akiba et al (2002)
reported, the products of TM-scores and LM-scores
are statistical variables. Even in the case where the
translation model (TM) and the language model for
the translation target language (LM) are trained on
a sub-corpus of the same size, changing the training
corpus also changes the TM-score, the LM-score,
and their product. Each pair of TM-score and LM-
score differently order the MT outputs.
For robust scoring, the authors adopt the multi-
ple scoring technique presented in (Akiba et al,
2002). The multiple scoring technique prepares
C1 Ck
C
k-fold Cross Validation
?..
TM1LM1 TMkLMk?..
C0
TM0LM0
Parallel corpus
Figure 4: Method for training multiple pairs of Lan-
guage Models (LMs) and Translation Models (TMs)
(Akiba et al, 2002).
multiple subsets of the full parallel corpus accord-
ing to k-fold cross validation (Mitchell, 1997) and
trains both TM and LM on each subset. Each
MT output is scored in k ways. For example, the
full parallel corpus C is divided into three subsets
Vi (i = 0, 1, 2). For each i, the proposed method
trains a translation model TMi on Ci (= C ? Vi)
and a language model LMi on the target-language
part of Ci (Figure 4). MT outputs in the mixture are
sorted by using the average of the product scores
by TMi and LMi for each i. In (Akiba et al, 2002),
this multiple scoring technique was shown to select
the best translation better than a single scoring tech-
nique that uses TM and LM trained from a full cor-
pus.
4 Experimental Comparison
The authors conducted an experimental compari-
son between the proposed RSCM and the existing
RSCM in the framework of the elimination system.
The task of both RSCMs was to judge whether each
top2 MT output from an MT system is satisfactory,
that is, whether the translation quality of the top MT
output is better than or as good as that which the
user can permit.
In this experiment, the translation quality of MT
outputs was assigned one of four grades: A, B,
C, or D as follows: (A) Perfect: no problems in
either information or grammar; (B) Fair: easy-to-
understand, with either some unimportant informa-
tion missing or flawed grammar; (C) Acceptable:
broken, but understandable with effort; (D) Non-
sense: important information has been translated in-
correctly. This evaluation standard was introduced
by Sumita et al (1999) to evaluate S2SMT systems.
In advance, each top MT output was evaluated by
nine native speakers of the target language, who
were also familiar with the source language, and
then assigned the median grade of the nine grades.
To conduct a fair comparison, the number of MT
outputs in the system?s N-best list and the number
of MT outputs in the mixture are expected to be
the same. Thus, the authors used either a three-
best list from each of three MT systems or a five-
best list from each of two non-SMT MT systems
for the proposed RSCM and a ten-best list for the
existing RSCM. Naturally, this setting4 is not disad-
vantageous for the existing RSCM.
4In the future, we will conduct a large-scale experiment to
investigate how both RSCMs work while increasing the size of
the system?s N-best list and the mixture of M-best lists.
Table 1: Confusion matrix
Accept Reject Subtotal
Satisfactory Vs,a Vs,r Vs (= Vs,a + Vs,r)
Unsatisfactory Vu,a Vu,r Vu (= Vu,a + Vu,r)
4.1 Evaluation metrics
The performances of both RSCMs were evaluated
by using three different metrics: ROC Curve, H-
mean, and Accuracy. For each MT system, these
metrics were separately calculated by using a con-
fusion matrix (Table 1). For example, for J2E
D3 (Section 4.2.1), the proposed RSCM checked
each top MT output from J2E D3 by using the input
mixture of three-best lists from the three J2E MT
systems (Section 4.2.1); on the other hand, the ex-
isting RSCM checked each top MT output from J2E
D3 by using the input ten-best list from J2E D3. For
J2E D3, the results were counted up into the con-
fusion matrix of each RSCM, and the metrics were
calculated as follows:
ROC Curve plots the correct acceptance rate ver-
sus the correct rejection rate for different values of
the threshold. Correct acceptance rate (CAR) is
defined as the number of satisfactory outputs that
have been accepted, divided by the total number of
satisfactory outputs, that is, Vs,a/Vs (Table 1). Cor-
rect rejection rate (CRR) is defined as the number
of unsatisfactory outputs that have been rejected, di-
vided by the total number of unsatisfactory outputs,
that is, Vu,r/Vu (Table 1).
H-mean is defined as a harmonic mean5 of
the CAR and the CRR (Table 1), 2 ? CAR ?
CRR/(CAR + CRR).
Accuracy is defined as a weighted mean6 of the
CAR and the CRR (Table 1), (Vs ? CAR + Vu ?
CRR)/(Vs + Vu) = (Vs,a + Vu,r)/(Vs + Vu).
For each performance of H-mean and Accuracy,
10-fold cross validation was conducted. The thresh-
old was fixed such that the performance was maxi-
mized on each non-held-out subset, and the perfor-
mance was calculated on the corresponding held-out
subset. To statistically test the differences in per-
formance (H-mean or Accuracy) between the confi-
dence measures, the authors conducted a pairwise t-
test (Mitchell, 1997), which was based on the results
of 10-fold cross validation. When the difference in
performance meets the following condition, the dif-
ference is statistically different at a confidence level
5This harmonic mean is used for summarizing two mea-
sures, each of which has a trade-off relationship with each
other. For example, F-measure is the harmonic mean of pre-
cision and recall, which is well used in the discipline of Infor-
mation Retrieval.
6This weighted mean is used for evaluating classification
tasks in the discipline of Machine Learning.
00.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-D3 (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Proposed method (D3+HPAT)
Existing method + reordering
Contours by H-mean
Figure 5: ROC Curves of both
RSCMs for J2E-D3
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-HPAT (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Proposed method (D3+HPAT)
Existing method + reordering
Contours by H-mean
Figure 6: ROC Curves of both
RSCMs for J2E-HPAT
0
0.2
0.4
0.6
0.8
1
0 0.2 0.4 0.6 0.8 1
Co
rre
ct
 re
jec
tio
n r
ate
: y
Correct acceptance rate: x
J2E-SAT (A|BCD)
0.80.7
y=x
Existing method
Proposed method (D3+HPAT+SAT)
Contours by H-mean
Figure 7: ROC Curves of both
RSCMs for J2E-SAT
Table 2: Performance of MT systems: Each number
in the AB row indicates the ratio of A-or-B-graded
translation by each MT system. Each number in the
other rows similarly indicates corresponding ratios.
J2E MT systems E2J MT systems
D3 HPAT SAT D3 HPAT SAT
A 63.7 42.5 67.2 58.4 59.6 69.8
AB 72.1 63.7 74.7 72.9 75.4 81.1
ABC 78.8 79.0 82.5 83.3 86.8 88.0
of 1-?%.
|ppro ? pext| > t(?,10?1) ? S/
?
10,
where ppro and pext, respectively, denote the aver-
age performance of the proposed RSCM and the ex-
isting RSCM, t(?,10?1) denotes the upper ? point of
the Student?s t-distribution with (10 ? 1) degrees of
freedom, and S denotes the estimated standard de-
viation of the average difference in performance.
4.2 Experimental conditions
4.2.1 MT systems
Three English-to-Japanese (E2J) MT systems and
three Japanese-to-English (J2E) MT systems of the
three types described below were used. Table 2
shows the performances of these MT systems.
D3 (DP-match Driven transDucer) is an
example-based MT system using online-
generated translation patterns (Doi and Sumita,
2003).
HPAT (Hierarchical Phrase Alignment based
Translation) is a pattern-based system using au-
tomatically generated syntactic transfer (Imamura
et al, 2003).
SAT (Statistical ATR Translator) is an SMT
system using a retrieved seed translation as the
start point for decoding/translation (Watanabe et al,
2003).
4.2.2 Test set
The test set used consists of five hundred and ten
pairs of English and Japanese sentences, which
Table 3: Corpora for training TMs and LMs: Basic
Travel Expression Corpus Nos. 1-3 (Takezawa et al,
2002), Travel Reservation Corpus (Takezawa, 1999), and
MT-Aided Dialogue Corpus No. 1 (Kikui et al, 2003)
.
Japanese English
# of sentences 449,357
# of words 3,471,996 2,978,517
Vocabulary size 43,812 28,217
Ave. sent. length 7.7 6.6
were randomly selected from the Basic Travel Ex-
pression Corpus (BTEC) (Takezawa et al, 2002).
BTEC contains a variety of expressions used in a
number of situations related to overseas travel.
4.2.3 Training TMs and LMs
The corpora used for training TMs and LMs de-
scribed in Section 3.2 were merged corpora (Table
3). The number of trained TMs/LMs was three.
The translation models and language models were
learned by using GIZA++ (Och and Ney, 2000) and
the CMU-Cambridge Toolkit (Clarkson and Rosen-
feld, 1997), respectively.
4.3 Experimental results and discussion
4.3.1 ROC Curve
In order to plot the ROC Curve, the authors con-
ducted the same experiment as shown in Figure 1.
That is, in the case where the grade of satisfactory
translations is only grade A, each of the proposed
and existing RSCMs tried to accept grade A MT
outputs and to reject grade B, C, or D MT outputs.
Figures 5 to 7 show the ROC Curves for each of the
three J2E MT systems (D3, HPAT, and SAT).
The curves with diamond marks, cross marks,
triangle marks, and circle marks show the ROC
Curves for the existing RSCM, the proposed RSCM
by using the mixture of three-best lists from D3,
HPAT and SAT, the proposed RSCM by using the
mixture of five-best lists from D3 and HPAT, and
the existing RSCM with reordering, respectively. In
the existing RSCM with reordering, the system?s
Table 4: Ten-fold cross-validated pairwise t-test of H-mean: Each set of three columns corresponds to the experimen-
tal results of each of the three MT systems: D3, HPAT, and SAT. Each floating number in the first to third column of
each MT system indicates the average performance of the proposed RSCM, the average difference of the performance
of the proposed RSCM from that of the existing RSCM, and the t-value of the left-next difference, respectively. The
bold floating numbers indicate that the left-next difference is significant at a confidence level of 95%. The floating
numbers on the three rows for each MT system, whose row heads are ?A | BCD?, ?AB | CD?, or ?ABC | D?, corre-
spond to the three types of experiments in which each RSCM tried to accept/reject the MT output assigned one of the
grades left/right of ?|?, respectively.
E2J-D3 E2J-HPAT E2J-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 76.2 15.7 4.424 73.2 14.1 5.099 65.5 0.3 0.108
AB | CD 77.3 16.5 5.154 72.6 14.3 3.865 66.9 2.8e-5 0.002
ABC | D 74.9 11.4 5.963 74.7 16.6 4.906 73.2 5.5 2.281
J2E-D3 J2E-HPAT J2E-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 76.8 16.1 4.928 75.5 25.8 9.218 70.2 -3.3 1.618
AB | CD 79.6 15.9 4.985 70.8 28.9 6.885 66.0 -5.9 2.545
ABC | D 77.7 14.4 4.177 71.0 22.6 4.598 72.1 1.7 0.588
Table 5: Ten-fold cross-validated pairwise t-test of Accuracy: The description of this figure is the same as that of
Table 4 except that Accuracy is used instead of H-mean.
E2J-D3 E2J-HPAT E2J-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 77.4 10.5 4.354 71.1 15.4 5.667 76.4 1.1 1.000
AB | CD 78.2 4.9 2.953 78.2 2.5 2.176 81.1 0.0 0.000
ABC | D 85.0 1.3 1.172 84.1 -2.9 2.182 88.0 0.0 0.000
J2E-D3 J2E-HPAT J2E-SAT
Separating point Ave. Diff. T-val. Ave. Diff. T-val. Ave. Diff. T-val.
A | BCD 78.8 15.8 8.243 76.2 18.2 8.118 76.4 3.1 1.041
AB | CD 77.8 4.1 3.279 72.7 8.8 3.288 77.6 -1.5 0.537
ABC | D 83.3 2.9 1.771 77.4 -1.7 1.646 82.7 0.1 0.428
original N-best list was sorted by using the aver-
age of the product scores from the multiple scor-
ing technique described in Section 3.2, and the ex-
isting RSCM with reordering used this sorted sys-
tem?s N-best instead of the system?s original N-best.
The dotted lines indicate the contours by H-mean
from 0.7 to 0.8. The ideal ROC curve is a square
(0, 1), (1, 1), (1, 0); thus, the closer the curve is to a
square, the better the performance of the RSCM is.
In Figures 5 and 6, the curves of the proposed
RSCM by using the mixture of three-best lists from
the three MT systems are much closer to a square
than that of the existing RSCM; moreover, the
curves of the proposed RSCM by using the mixture
of five-best lists from the two MT systems are much
closer to a square than that of the existing RSCM.
Note that the superiority of the proposed RSCM to
the existing RSCM is maintained even in the case
where an M-best list from the SMT system was not
used. The curves of the existing RSCM with re-
ordering are closer to a square than those of the ex-
isting RSCM. Thus the performance of the proposed
RSCM on the non-SMT systems, D3 and HPAT, are
much better than that of the existing RSCM. The
difference between the performance of the proposed
and existing RSCMs is due to both resorting the MT
outputs and using a mixture of N-best lists.
In Figure 7, the curve of the proposed RSCM is a
little closer when CRR is larger than CAR; and the
curve of the existing RSCM is a little closer when
CAR is larger than CRR. Thus, the performance
of the proposed RSCM on the SMT system, SAT,
is a little better than that of the existing RSCM in
the case where CRR is regarded as important; sim-
ilarly, the performance of the proposed RSCM on
the SMT system is a little worse than that of the ex-
isting RSCM in the case where CAR is regarded as
important.
4.3.2 H-mean and Accuracy
Tables 4 and 5 show the experimental results of ten-
fold cross-validated pairwise t-tests of the perfor-
mance of H-mean and Accuracy, respectively.
On the non-SMT systems, Table 4 shows that at
every level of translation quality that the user would
permit, the H-mean of the proposed RSCM is sig-
nificantly better than that of the existing RSCM. On
the SMT MT system, Table 4 shows that at every
permitted level of translation quality, there is no sig-
nificant difference between the H-mean of the pro-
posed RSCM and that of the existing RSCM except
for two cases: ?ABC | D? for E2J- SAT and ?AB |
CD? for J2E- SAT.
Table 5 shows almost the same tendency as Table
4. As for difference, in the case where the transla-
tion quality that the user would permit is better than
D, there is no significant difference between the Ac-
curacy of the proposed RSCM and that of the exist-
ing RSCM except in the one case of ?ABC | D? for
E2J-HPAT.
As defined in Section 4.1, Accuracy is an eval-
uation metric whose value is sensitive/inclined to
the ratio of the number of satisfactory translations
and unsatisfactory translations. H-mean is an eval-
uation metric whose value is independent/natural to
this ratio. We need to use these different evaluation
metrics according to the situations encountered. For
general purposes, the natural evaluation metric, H-
mean, is better. In the case where the test set reflects
special situations encountered, Accuracy is useful.
Regardless of whether we encounter any special
situation, in most cases on a non-SMT system, the
proposed RSCM proved to be significantly better
than the existing RSCM. In most cases on an SMT
system, the proposed RSCM proved to be as good
in performance as the existing RSCM.
This paper reports a case study in which a mixture
of N-best lists from multiple MT systems boosted
the performance of the RSCM for MT outputs. The
authors believe the proposed RSCM will work well
only when each of the element MT systems comple-
ments the others, but the authors leave the question
of the best combination of complementary MT sys-
tems open for future study.
5 Conclusions
This paper addressed the problem of eliminating un-
satisfactory outputs from MT systems. It proposed
a method that eliminates unsatisfactory outputs by
using an alternative RSCM based on a mixture of
N-best lists from multiple MT systems. The au-
thors compared the proposed and existing RSCMs
in the framework of an elimination system. When
the number of MT outputs both in the N-best list for
the existing RSCM and in the mixture of N-best lists
for the proposed RSCM is almost the same number,
i.e. ten, in most cases, the proposed RSCM proved
to work better than the existing RSCM on two non-
SMT systems and to work as well as the existing
RSCM on an SMT system.
In the future, the authors will conduct the follow-
ing experiments: (1) investigating how the proposed
RSCM works when the size of the M-best lists is
increased, and (2) seeing how the proposed RSCM
influences the performance of the selection system.
References
Yasuhiro Akiba, Taro Watanabe, and Eiichiro Sumita. 2002.
Using language and translation models to select the best
among outputs from multiple MT systems. In Proc.
COLING-2002, pages 8?14.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19(2):263?311.
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical lan-
guage modeling using the CMU-Cambridge toolkit. In
Proc. EUROSPEECH-1997, pages 2707?2710.
Takao Doi and Eiichiro Sumita. 2003. Input sentence splitting
and translating. In Proc. the HLT-NAACL 2003 Workshop
on DDMT, pages 104?110.
Kenji Imamura, Eiichiro Sumita, and Yuji Matsumoto. 2003.
Feedback cleaning of machine translation rules using auto-
matic evaluation. In Proc. ACL-2003, pages 447?454.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki Takezawa, and
Seiichi Yamamoto. 2003. Creating corpora for speech-
to-speech translation. In Proc. EUROSPEECH-2003, vol-
ume 1, pages 381?384.
Vladimir I. Levenshtein. 1966. Binary codes capable of cor-
recting deletions, insertions and reversals. Soviet Physics
Doklady, 10(8):707?710.
Tom M. Mitchell. 1997. Machine Learning. The McGraw-Hill
Companies Inc., New York, USA.
Sonja Niessen, Franz J. Och, G. Leusch, and Hermann Ney.
2000. An evaluation tool for machine translation: Fast eval-
uation for machine translation research. In Proc. LREC-
2000, pages 39?45.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proc. ACL-2000, pages 440?447.
Kishore A. Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a method for automatic evaluation of ma-
chine translation. In Technical Report RC22176 (W0109-
022), IBM Research Division, Thomas J. Watson Research
Center, Yorktown Heights, NY, pages 257?258.
Eiichiro Sumita, Setsuo Yamada, Kazuhiro Yamamoto,
Michael Paul, Hideki Kashioka, Kai Ishikawa, and Satoshi
Shirai. 1999. Solutions to problems inherent in spoken-
language translation: The ATR-MATRIX approach. In
Proc. MT Summit VII, pages 229?235.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hiro-
fumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a
broad-coverage bilingual corpus for speech translation of
travel conversations in the real world. In Proc. LREC-2002,
pages 147?152.
Toshiyuki Takezawa. 1999. Building a bilingual travel conver-
sation database for speech translation research. In Proc. the
Oriental COCOSDA Workshop-1999, pages 17?20.
Nicola Ueffing, Klaus Macherey, and Hermann Ney. 2003.
Confidence measures for statistical machine translation. In
Proc. MT Summit IX, pages 394?401.
Taro Watanabe, Eiichiro Sumita, and Hiroshi G. Okuno. 2003.
Chunk-based statistical translation. In Proc. MT Summit IX,
pages 410?417.
Automatic Construction of a Transfer Dictionary
Considering Directionality
Kyonghee Paik, Satoshi Shirai? and Hiromi Nakaiwa
{kyonghee.paik,hiromi.nakaiwa}@atr.jp * sat@fw.ipsj.or.jp
ATR Spoken Language Translation Laboratories
2-2-2, Keihanna Science City Kyoto, Japan 619-0288
?NTT Advanced Technology Corporation
12-1, Ekimaehoncho, Kawasaki-ku, Kawasaki-shi, Japan 210-0007
Abstract
In this paper, we show how to construct a
transfer dictionary automatically. Dictionary
construction, one of the most difficult tasks
in developing a machine translation system, is
expensive. To avoid this problem, we investi-
gate how we build a dictionary using existing
linguistic resources. Our algorithm can be ap-
plied to any language pairs, but for the present
we focus on building a Korean-to-Japanese
dictionary using English as a pivot. We
attempt three ways of automatic construction
to corroborate the effect of the directionality
of dictionaries. First, we introduce ?one-time
look up?method using a Korean-to-English and
a Japanese-to-English dictionary. Second, we
show a method using ?overlapping constraint?
with a Korean-to-English dictionary and an
English-to-Japanese dictionary. Third, we con-
sider another alternative method rarely used
for building a dictionary: an English-to-Korean
dictionary and English-to-Japanese dictionary.
We found that the first method is the most
effective and the best result can be obtained
from combining the three methods.
1 Introduction
There are many ways of dictionary building.
For machine translation, a bilingual transfer
dictionary is a most important resource. An in-
teresting approach is the Papillon Project that
focuses on building a multilingual lexical data
base to construct large, detailed and principled
dictionaries (Boitet et al, 2002). The main
source of multilingual dictionaries is monolin-
gual dictionaries. Each monolingual dictionary
is connected to interlingual links. To make
this possible, we need many contributors, ex-
? Some of this research was done while at ATR.
perts and the donated data. One of the stud-
ies related to the Papillon Project tried to link
the words using definitions between English and
French, but the method can be extended to
other language pairs (Lafourcade, 2002). Other
research that focuses on the automatic build-
ing of bilingual dictionaries include Tanaka and
Umemura (1994), Shirai and Yamamoto (2001),
Shirai et al (2001), Bond et al (2001), and
Paik et al (2001).
Our main concern is automatically building
a bilingual dictionary, especially with different
combinations of dictionaries. None of the re-
search on building dictionaries seriously consid-
ers the characteristics of dictionaries. A dic-
tionary has a peculiar characteristic according
to its directionality. For example, we use a
Japanese-to-English (henceforth, J?E) dictio-
nary mainly used by Japanese often when they
write or speak in English. Naturally, in this sit-
uation, a Japanese person knows the meaning
of the Japanese word that s/he wants to trans-
late into English. Therefore, an explanation for
the word is not necessary, except for the words
whose concept is hard to translate with a single
word. Part-of-speech (henceforth POS) infor-
mation is also secondary for a Japanese person
when looking up the meaning of the correspond-
ing equivalent to the Japanese word.
On the other hand, an English-to-Japanese
(henceforth E?J) dictionary is basically used
from a Japanese point of view to discover the
meaning of an English word, how it is used and
so on. Therefore, explanatory descriptions, ex-
ample sentences, and such grammatical infor-
mation as POS are all important. As shown in
(2), a long explanation is used to describe the
meaning of tango, its POS and such grammat-
ical information as singular or plural. Also, an
E?J dictionary includes the word in plenty of
examples, comparing to a J?E dictionary. The
following examples clearly show the difference.
(1) J?E:   :  dance  the tango   s 
(2) E?J: tan 	 go /(n. pl    s)
 

 :a. 
Annotating Honorifics Denoting Social Ranking of Referents
Shigeko Nariyama*!, Hiromi Nakaiwa*, Melanie Siegel"
Communication Science Lab, NTT Kyoto, Japan
*{shigekon, hiromi}@cslab.kecl.ntt.co.jp
! The University of Melbourne, Australia shigeko@unimelb.edu.au
"DFKI GmbH Saarbr?cken, Germany siegel@dfki.de
Abstract
This paper proposes an annotating
scheme that encodes honorifics
(respectful words). Honorifics are used
extensively in Japanese, reflecting the
social relationship (e.g. social ranks and
age) of the referents. This referential
information is vital for resolving zero
pronouns and improving machine
translation outputs. Annotating honorifics
is a complex task that involves
identifying a predicate with honorifics,
assigning ranks to referents of the
predicate, calibrating the ranks, and
connecting referents with their predicates.
1 Introduction
To varying extents, languages have ways to
reflect the speaker?s deference towards the
addressee and people being referred to in
utterances (c.f. Brown and Levinson 1987): by
adopting a more polite air or tone of voice,
avoiding coarse language, and modifying one?s
choice of specific vocabulary. This is prominent
in Asian languages, Japanese and Korean in
particular, which exhibit an extensive use of
honorifics (respectful words).
Morphologically for example, French has a
choice of the familiar tu and the formal vous (a
third person plural) for the second person
referent. Similarly Greek has the same choice:
esei and eseis respectively. European languages
commonly project one?s deference by the use of
different personal pronouns and titles (e.g. Mr.,
Dr., and Hon.).
Japanese and Korean, on the other hand, have
numerous ways to say ?I? or ?you? calibrated by
social position, age, gender and other factors.
The projection of honorifics extends over the
vocabulary of verbs, adjectives, and nouns as
well as sentence structures, to elevate a person
or humble oneself. (1) and (2) below from
Japanese are such examples, which use honorific
verbs instead of neutral forms kuru ?come?, iku
?go/accompany?, and motomeru ?seek?:
(1)??????????????
Irasshat-tara, otomosuru.
come-when accompany
?When (an honouree) comes, (an honourer)
accompanies (the honoree).?
(2)???????
Enjo-o aoida.
help-OB sought
?(A lower ranked person) turns to (a higher
ranked person) for help.?
Examples (1) and (2) also reveal the
notorious problem of zero pronoun resolution in
Japanese, where the subject and the object of a
sentence are frequently left unexpressed
(Nakaiwa 2002, Nariyama 2003, inter alia). It
is clear from the examples that coding the
honorific relations of referents provides vital
information for identifying what zero pronouns
refer to; namely, to know whether or not a
predicate denotes disparity of social rank
between referents and to identify the rank of the
referents. This is what this paper proposes to
do. Siegel (2000) reported that 23.9% of
Japanese zero pronouns in task-oriented
dialogues can be resolved using information
gleaned from honorification.
Coding of honorifics also improves machine
translation outputs into Japanese in choosing
the correct predicate depending on the
relationship of the referents. Inappropriate use
of honorifics, in particular the use of the plain
form where an honorific form should be used, is
rude and can be offensive.
91
Section 2 reviews some earlier work on this
topic in NLP; Section 3 elaborates on honorifics;
Section 4 formulates the ranking factors; Section
5 proposes a way to assign ranks to referents;
Section 6 discusses a way to calibrate rankings
of referents, as ranks are relative to the ranks of
other referents in the sentence; Section 7
describes our annotation scheme, and finally
conclusion in Section 8.
2 Earlier studies
The Japanese honorification system has been
studied extensively in linguistics, particularly in
sociolinguistics. Because of its importance and
frequent use in the Japanese language, there has
been some related work in NLP; within the
framework of grammar formalism, GPSG by
Ikeya (1983), JPSG by Gunji (1987), and more
recently HPSG by Siegel (2000); work from a
view point of resolving zero pronouns in
dialogues by Dohsaka (1990).
Of these, the most thorough work on
Japanese honorification is seen in JACY, a
Japanese HPSG grammar (Siegel 2000, Siegel
and Bender 2002). It extends the BACKGR
(owe ? honour) relation (Pollard and Sag 1994),
which accounts only for subject honorifics, to
accommodate the other types of honorification
used in Japanese (see Section 3.1 for the types).
The full account of the Japanese
honorification system requires syntactic and
pragmatic information in many dimensions, with
more input from the latter, the gathering of
which is an extremely convoluted task. This
paper builds on the basics from JACY and
complements it in two ways to extend the JACY
annotation presented in Section 7.
1. Ranking referents in social hierarchy
2. Calibrating the ranks
Regarding 1, honorifics tell which referent is
higher in rank, so each referent must be assigned
a rank to make use of honorific information.
This is crucial when generating sentences to
assign appropriate forms of honorific nouns and
predicates in machine translation output into
Japanese. In processing, ranking referents is not
usually of importance when referents are overt,
but it is when referents are zero pronouns. The
identification of zero pronouns relies heavily on
the honorific information conveyed in the
predicates.
Regarding 2, social rank is not absolute, but
relative, so that the same referent may be higher
or lower depending on which referent it appears
with in a sentence. For example, the president of
a company is socially regarded as ranked higher
than the managers, who are in turn higher than
clerks, but this rank is outweighed when their
clients come in the sentence, in which case the
president is ranked lower than their clients.
3 Honorifics
Honorifics is a term used to represent words that
convey esteem or respect. Extensive studies on
Japanese honorification revealed many forms of
honorifics in use. The use of honorification is
mandatory in many social situation. 1 Hence,
every sentence can be viewed as coded for
honorification if we consider the lack of an
honorific marking as a sign that there is no
hierarchical difference between referents.
Types of honorifics that indicate who is
shown respect are described in Subsection 3.1,
and forms of honorifics in Subsection 3.2.
3.1 Types of honorifics
Honorifics in modern (post-war) Japanese are
generally classified into the following three
categories, depending on who is shown respect
(Martin 1964, Matsumoto 1997, Nariyama 2003,
inter alia). The first two types are often referred
to as ?propositional (referential) honorifics?.
i. Subject honorifics (called Sonkeigo in
Japanese): to elevate or show respect
towards the subject of a sentence
ii. Non-subject honorifics (called Humility,
or Kenjogo): to humble oneself by showing
respect to the non-subject referent,
generally the object
iii. Addressee honorifics (?polite?,
Teineigo): to show respect towards the
listener
Note that the expressions of deference are by
nature made essentially with human referents,
i.e. between an honouree and an honourer.
1 However, sometimes honorification is uncoded even for
respected referents, especially when the respected person is
not present at the site of an utterance.
92
However, paying respect often extends to things
and events related to the honouree in Japanese.
This is often expressed with an honorific prefix
o- or go- to the nouns. For example, the passage
(3) is used by train conductors for ticket
inspection. The use of non-subject honorific
form means that the unexpressed subject (i.e. the
train conductor) is showing respect towards the
tickets, which belong to or have some relation to
his honourees (i.e. the passengers).
(3)?????????
Kippu-o haikenshi-masu.
ticket-OB look[NsubH]-Polite
'(An honourer) is going to inspect (his honourees?)
tickets.' ! 'Let me inspect your tickets, please.'
3.2 Forms of honorifics
Honorifics in Japanese take various forms that
are reflected in the word forms, either in lexical
choice or in inflections ? verbs in Subsection
3.2.1, adjectives and nouns in Subsection 3.2.2,
and also sentence structures in Subsection 3.2.3.
3.2.1 Verbs
There are five ways of expressing referent
honorification in verbs, depending on the type of
verb and the level of respect that is intended.2
Types 3 displays the highest deference, and
3>2>1>4 in descending order. Type 5 displays a
formality rather than deference towards the
referent. The larger the gap in the hierarchy, the
more disparity of referents in rank we expect.
3.2.1.1 Type 1: Alternation of verb forms
Verbs can be transformed into subject honorific
(SubH) and non-subject honorific (NsubH)
structures as follows:
SubH: o + verb stem + ni naru (?become?).
NsubH: o + verb stem + suru (?do?).
2 According to Wenger (1983:283-292), 70% of verbs have
Subject honorific forms, while only 36% of verbs have
non-subject honorific forms. He explains why not all verbs
have forms of honorification, although he does not explain
why there are fewer non-subject honorific forms.
Honorification cannot occur, 1) unless the subject is
human; this explains why there are no honorific forms for
verbs such as kooru ?freeze? and hoeru ?bark?; and 2) on
verbs that have negative connotations, such as kuiarasu ?eat
greedily?.
Accordingly, machi ?wait?, for example, can be
turned into two different forms of honorifics:
(4a) O-machi-ni naru. [SubH]
'(An honouree) waits (for someone/something).'
(4b) O-machi-suru. [NsubH]
'(An honourer) waits (for an honouree).?
The honorific prefix o- can be go-, as shown
below. Basically, o- is used for Japanese native
verbs and nouns and go- for Sino-Japanese
(Chinese originated) words.
(5a) Go-shichaku-ni naru. [SubH]
'(An honouree) tries on (clothes).'
(5b) Go-hookoku-suru [NsubH]
'(An honourer) reports (to an honouree).'
3.2.1.2 Type 2: Suppletive forms
Different lexical items are used for some (more
frequently used) verbs. For example, the
following examples all mean '? eat':
(6a) Taberu. [non-honorific: neutral]
'(Someone) eats.'
(6b) Meshiagaru. [SubH]
'(An honouree) eats.'
(6c) Itadaku. [NsubH]
'(An honourer) eats.'
Table 1 shows some examples of other
suppletive forms of honorification.
Neutral SubH NsubH
do suru nasaru itasu
exist/stay iru irassharu/
o-ide-ni-naru oru
go iku irassharu/ mairu/
o-ide-ni-naru ukagau
come kuru irassharu/
o-ide-ni-naru mairu
say iu ossharu moosu
eat/drink taberu/nomu meshiagaru itadaku
Table 1: Suppletive forms of honorification
Notice that some honorific forms are shared
by very different meanings of verbs. For
instance, irassharu can mean either ?come?,
?go?, or ?stay?. The nature of honorification is
said to be indirect in expression. This semantic
neutralization poses problems in machine
translation outputs from Japanese.
93
3.2.1.3 Type 3: Combination of Types 1 & 2
This usage is restricted to some verbs, for
example:
(7) O-meshiagari-ni naru.
'(Someone highly respected) eats.'
3.2.1.4 Type 4: Use of passive form -rare
The passive -rare is suffixed to the verb stem to
display subject honorifics instead of the passive
interpretation; for example:
(8) Tabe-rare-ru.
'(An honouree) eats.'
c.f. (6a) Taberu. [non-honorific: neutral]
'(Someone) eats.'
Note that there are no corresponding
constructions of Types 3 and 4 for non-subject
honorific forms.
3.2.1.5 Type 5: Lexical semantics
The semantics of some verbs give rise to
referential restrictions, in that the subject must
be higher or lower than the non-subject referent.
This has been neglected in previous studies of
honorification. Analogous to the example in (1),
insotsu ?? ?take? has a restricted usage as ?(a
higher ranked person) leads (a group of lower
ranked people).?
We used Lexeed (Bond et al 2004) - a
manually built self-contained lexicon, to extract
verbs and verbal nouns with such referential
restrictions. It consists of words and their
definitions for the most familiar 28,000 words in
Japanese, as measured by native speakers. This
set is formulated to cover the most basic words,
which cover 72.2% of the words in a typical
Japanese newspaper. Since honorification tends
to be found more in sophisticated words than in
basic words, we used those extracted verbs as
seeds to expand the list using the Goi-Taikei
thesaurus (Ikehara et al 1997).
For example, the semantic class meirei ??
?command? (Class Number 1824) lists
synonyms, such as iitsukeru ????? ?tell?,
and shiji ?? ?instruct?, all of which exhibit the
same referential restriction: a high ranked person
as the subject and a low ranked person as the
object. However, this is not always the case. For
instance, kyoka ?? ?permit? (Class Number
1735) includes as its synonyms dooi ?? ?agree?
and sansei ?? ?agree/approve? that do not
exhibit the same referential restriction as kyoka.
We manually extracted from Lexeed 698
such verbs (397 of these are ?a higher ranked
person does to a lower person? and the rest 301
are the reverse), and from Goi-Taikei further 429
(228, 201 respectively), 1127 in total.
3.2.2 Nouns and adjectives
Honorification is also expressed on nouns
(including verbal nouns) and adjectives by the
honorific prefix o- with variants on-, go-, and mi.
Honorific prefixes have four functions:
[1] An entity/action belongs to the honouree.
[2] An entity/action has an implication to the
honouree, even when it belongs to the
speaker.3
[3] Addressee honorifics to show formality of
speech/politeness to the addressee.4
[4] Conventional usage5
The use of the honorific particles in [1]
provides important information on the type of
referents. Possessors are seldom expressed and
there are no definite/indefinite articles in
Japanese (Bond 2005), but honorific particles
can take on these functions. For example, o-
nimotsu (honorable luggage) means 'your/his/...
luggage', and go-ryokoo means 'your/his/... trip'.
Although the exact identity of the honoree-
possessors is context dependent, as the following
minimal pair of sentences show, in (9a) the
possessors can never be the speaker or the
speaker's in-group member (see Subsection 4.2
for ?in-group?), as indicted by *. In contrast, the
identity of the subject, as in (9b) without an
honorific particle, is generally the speaker or his
in-group member.
3 For example, o-tegami (literally, ?honourable letter?) is
used when the letter is something to do with the honouree;
it could be the letter that the honouree wrote, a letter sent
by someone else to the honouree, or a letter written by the
speaker to the honouree.
4 For example, o-hana (flowers) and o-shokuji (meal) are
such cases where possession is not a concern.
5 The standard example of this type is go-han ?honourable-
rice? meaning 'rice/meal'. Such honorific particles do not
convey honorifics, but are seen as part of set phrases.
94
(9a)???????
O-genki de iru.
Hon.-good health be stay
'(The honouree/*I/*In-group) is in good health.'
(9b) ??????
?-Genki de iru
good health be stay
'(*The honouree /I/ In-group) is well.'
3.2.3 Sentence structures
Honorification is manifested also in the choice
of sentence structure. The causative construction
can be used only when the causer is superior in
social hierarchy to the causee, as shown in (10).
If the causee is equal to or superior over the
causer, the benefactive construction is used,
conveying the same proposition with the
connotation that the causee has accepted the
causer's request instead of command, as in (11).
Thus, the sentence structure reveals the
referential disparity in rank.
(10) ???????????
Watashi-wa otooto-ni hon-o yom-ase-ta.
I younger brother-IO book-OB read-Caus-Past
'I made my younger brother read the book.'
(11) ???????????????
Watashi-wa sensei-ni hon-o yon-de morat-ta.
I teacher-IO book-OB read-and receive-Past
'(I requested my teacher to read the book for me, and)
my teacher read the book for me.'
4 Ranking factors
Section 3 explained the various forms that
indicate disparity of referents in rank. This
section describes three factors that induce such
disparity in rank: Social hierarchy, in-group and
out-group distinction, and unfamiliarity of the
addressee.
4.1 Social hierarchy
Social hierarchy is the core rank-inducing
factor, which can be overridden by the other
two factors. It refers to social ranks in such
social settings as company, school, family, as
well as general age/generational rank. For
example, an employer is perceived as ranked
higher than his employees, and a teacher is
higher than his students, and the older a person
is, the higher he is ranked.
Social hierarchy functions similar to the
Subject-Verb agreement in terms of person,
number and gender seen in many European
languages. Although Japanese has no syntactic
coding of such a S-V agreement, verbs agree
with the referential relation of the subject and
other referents in terms of social hierarchy (the
same view is held by Pollard and Sag 1994).
4.2 In-group and out-group distinction
Referents are also classified according to the in-
group and out-group distinction, depending on
the social relation among three parties: the
speaker, the addressee, and the people being
referred to.6 For example, in (12) an officer of
a company (the speaker) talks about the
president of his company (referent) to his boss
(addressee). The officer is ranked lower than
the president and his boss, and accordingly the
subject honorific and addressee honorific
(?Polite?) are used. However in (13), when he
reports the same proposition to people outside
the company, the president is regarded as a ?in-
group? member to the speaker, and therefore the
description of him uses the non-subject
honorific form of verb, the same as the speaker
would use to describe himself. In other words,
the rank assigned from social hierarchy is
overridden by the in-group and out-group
distinction.
(12) ??????????????
Shachoo-ga irrashai-mashi-ta.
president-SB come[SubH]-Polite-Past
?The president has arrived.'
(13) ?????????
Shachoo-ga mairi-mashi-ta.
president-SB come[NsubH]-Polite-Past
?The president has arrived.'
Thus, the dichotomy of in-group/out-group
distinction is relative. This is prominently seen
in the use of family terms, as shown in Table 2.
When someone talks to her/his mother or about
her with her/his family, 'mother' is referred to as
6 Generally, the type of honorific use is also determined by
the three parties. To be more precise, setting and bystander
also play a part in determining the type of honorifics to be
used (Brown and Levinson 1987).
95
okaasan, using the out-group (OG) form, while
when talking about her to outsiders, she is
referred to as haha, using the in-group (IG)
form.
There are three lexical types that reflect the
in-group and out-group distinction.
1) the deictic prefixes:
too- hon-, hei-, setu, etc. for the in-group
use, translated into English as 'my/our', and
ki-, o-, on-, etc. for the out-group use,
translated as 'your/his/her/their'. For
example, too-koo (my/our school) versus ki-
koo (your/their school).
2) the suffixes -san/-sama/-dono:
for instance, gakusei 'a student' is referred to
as gakusei-san out of deference to a
respected out-group person (e.g. ?your
students?, ?student of your school?).
3) suppletive forms:
some examples are shown in Table 2.
IG referent OG referent
mother haha o-kaasan
father chichi o-toosan
wife tsuma, kanai o-kusan
son segare, musuko go-shisoku,
musuko-san
daughter musume o-joo-san
Table 2: Referential forms by in-group and
out-group
4.3 Unfamiliarity of the addressee
In apparent absence of disparity in social
ranking and age, honorifics can still be used
subjectively in formal settings, when
communicating with unfamiliar people,
particularly by female speakers.
5 Assigning referents with ranks using
Goi-Taikei thesaurus
In order to make use of honorific information,
each referent must be assigned with a rank to
determine which referent in a sentence is
ranked the highest. We use Goi-Taikei for this
assignment (Ikehara et al 1997). It has a
semantic feature tree with over 3,000 nodes of
semantic classes organised with a maximum of
12 levels (see Figure 1). It includes in its
semantic classes information on occupational
status, generation, family composition; the sort
of information needed for this assignment.
Noun Level 1
Concrete Abstract
Agent Place Concrete
Person Organization
Human Person (Occupation/Status/Role)
Human Occupation Status Role Level 6
Gender Seniority Specialised king minister director ?
Male Fem Infant ?Adult Elderly
Teacher Student
? ? ? ? ? ? Level 12
Figure 1: Excerpt from Goi-Taikei thesaurus
In addition, the following two tasks are
required:
1) Group some semantic classes together from
different nodes.
For the honorific use, some semantic classes
that are scattered over different nodes in the tree
should be grouped together. For instance, the
information relevant to Social hierarchy is found
not only under Occupation (status) but also
under Organization, Family, and so forth.
2) Rank the semantic classes where relevant.
Figure 2 is a preliminary result showing
ranks of referents in selected semantic classes,
noted as class names followed by their semantic
class numbers in Goi-Taikei listed in ascending
order.
96
Social hierarchy
- senior 142 > junior 143
- experienced 145 > less experienced 146
- master 139 > apprentice 140
- teacher 237 > student 238
- king/emperor 320 > aristocrats 321
- minister 322> clerk 326
- directors 323 > deputy director 324 >
executive 325 >
Age/Generation
- elderly 63 > adult 60 > youth 57 >
boy/girl 54 > infant 51
- ancestors 84 > grandparents 81 > parents
78 > children 86 > grandchildren 89 >
descendants 92
- older sibling 94 > younger sibling 97
- uncle/aunt 101 > nephew/niece 104
Figure 2: Strings of ranks
This list needs to be expanded. As the list is
taken exhaustively from Goi-Taikei, these
entries must be augmented with other thesauri,
organisation charts, genealogical trees, and other
ways as well as by hand.
6 Calibration of ranks
Ranks of referents are not absolute, but relative
to the other referents in the sentence. For
example, an adult referent is ranked higher than
a youth, but the same referent is ranked lower
when appearing with an elderly in the same
sentence. Similarly, manager in a company is
higher than workers with no title, but the same
manager is lower than the company president.
Thus, the calibration of rankings is necessary.
However, calibrating ranks while capturing
relative ranks is an extremely complicated task,
as any combination of referents and ranking
categories can appear in a sentence as well as
the fact that one referent may belong to multiple
categories. For example, a measure has to be
taken in case one referent is ranked in one
string (e.g. ?minister?) than the other referent
(e.g. ?clerk?) in the same sentence, but he is
lower in another string (e.g. ?less experienced?
in the profession or younger in age) (see Figure
2); or when the in-group and out-group
distinction takes the precedence in the form of
honorifics, for instance a referent is senior than
the other referent, but he is an in-group member
to the speaker.
More complicated still, within the same
class, there may exist a disparity in rank. For
example, the age difference, even by one year,
can determine the use of honorifics, so that
honorifics is used between two referents under
the same class adult.
Considering the above, we propose the
following calibration scheme as an initial step
of dealing with the complex phenomena of
honorifics.
[1] create referential links, example modules
of which are suggested in Figure 3. Each
string of ranks in Figure 2 constitutes a
module, which is connected to another
module. Figure 3 shows that a referent
?JOHN? is a student as well as a child of his
parents that is depicted. JOHN belongs to
other modules of strings; he may be an elder
bother at home, and may be a senior student
at school, each of which is a member of a
module and is connected to other modules.
Connections between two modules may be
more than one, for example, ?grandparent?
may be a teacher of ?teacher? of JOHN.
It is necessary to identify as many
modules as identifiable and to link them in
order to accurately determine the ranks of each
referent for a sentence.
Out-group members
Prime Minister
Ministry of Education In-group members
Principal ancestor
Head teacher grandparent
teacher PTA/parent
student ?JOHN? child
grandchild
descendant
Figure 3: Two modules of referential links
97
[2] a diagram for calibrating ranks
Figure 4 is proposed to capture the
mechanisms of honorifics that determine the
ranks of referents for a sentence.
The referential links are the first (core)
rank determining factor. When one referent
belongs to multiple strings, for instance, a
string from Social hierarchy and another
from Age, then the former takes the higher
rank, which is noted as ?Social > Age?. The
case where two referents belong to the same
class but still appear with honorifics is due to
the subtle difference in rank, noted as ?The
same class?.
These ranks assigned by the referential
links can be overridden by ?in/out group
precedence?, which is determined by the type
of modules, as shown in Figure 3.
The use of honorifics in absence of
disparity in social rank is interpreted as lack
of familiarity of the addressee.
Referential links
NB: Multiple links (Social > Age)
NB: The same class
Unfamiliarity (no disparity in rank)
Figure 4: Diagram of calibrating ranks
7 Annotation
Our annotation method is an extension of the
framework of JACY, a Japanese HPSG grammar
(Siegel 2000), as discussed in Section 2.
Subsection 7.1 describes the JACY annotation
and Subsection 7.2 is the extension we made
from this research.
7.1 JACY annotation
The JACY annotation scheme for honorification
can be seen in Figure 5 with examples on the
bottom of the tree. It annotates honorification
concerning referential nouns (honorific entity),
predicative honorifics (subject honorifics) that
are triggered by honorific entities, and predicates
of the addressee honorifics. The notion of
polarity is used to denote the three types of
value; a polarity value ?+? means a subject
honorific form, ?-? denotes a non-subject
honorific form, and ?bool? is indeterminate. It is
capable of accounting for the basic types of
honorification, as being expressed by verb
forms, suppletive forms, passive, nouns and
adjectives.
Figure 5: JACY annotation for honorifics
(with examples)
7.2 Extended JACY annotation
Based on our findings, we extend the JACY
annotation Figure 5 to Figure 6 by adding two
relations in the honorification, Social ranking
and In-group relation.
As for Social ranking, Subsection 3.2.1.5
introduced those verbs with referential
restrictions, such as insotsu ?? ?take? has a
restricted usage as ?(a higher ranked person)
leads (a group of lower ranked people).? These
lexical items are added to honorific information
in JACY, as being part of the lexical type
hierarchy. In addition, the use of causative that
imposes the interpretation ?a high ranked person
acts on the lower? is accounted for under Social
ranking (see Subsection 3.2.3).
We notate the relation deriving from social
ranking as social_ranking_rel. It has two
arguments, which show the semantic indices of
the verbal arguments, the first (or left) argument
being ranked higher. The relation is triggered by
the lexical types and the causative usage.
Example 10, 'I made my younger brother read
the book', is annotated with social_ranking-rel
(watashi, otooto), while example 11 'my teacher
read the book for me' is annotated with
social_ranking-rel (sensei, watashi).
In/out group precedence
Honorifics
Propositional
honorifics
Subject
honorific
Addressee
honorifics
(Polarity -)(Polarity +)
o-hanasi ni nar-u, o-hanasi su-ru,
hanasi-mas-u, hanas-u,(poliitiy -)
Honorifics
entity
sensei watashi
(Polarity +) (Polarity -)(Polarity +)
98
Figure 6: Extended JACY annotation for
honorifics (with examples)
The distinction between in-group and out-
group makes it necessary to add a further
relation, called in_group_rel. It has two
arguments, relating the speaker with the
predicate's subject. As in the other honorific
relations, it gets a POLARITY feature, showing
an in-group relation with [POLARITY +] and an
out-group relation with [POLARITY -], and
?bool? for indeterminate. The nominal
expressions that trigger in-group relations (such
as okaasan and haha in Table 2) add this
relation to the CONTEXT.
For a predicate, such as Example 13, with
subject honorific information [POLARITY -]
and a subject with honorific entity information
[POLARITY +], an in_group_rel is added to
relate the speaker and the subject, annotated as
in_group_rel (speaker, shachoo).
To better understand the interaction of Social
ranking and In-group relation, we refer to
examples 12 and 13. In processing, predicative
honorifics is identified not by the referential
nouns, but by the predicates. So, if the predicate
is minus_shon (- SubH) and the subject is
plus_ohon (+ entity honorifics), i.e. (13), then
there is an in-group relation. On the other hand,
with an out-group relation as in (12), the
predicate is plus_shon (+ SubH) and the subject
is plus_ohon (+ entity honorifics).
(12) ??????????????
Shachoo-ga irrashai-mashi-ta.
president-SB come[SubH]-Polite-Past
?The president has arrived.'
(13) ?????????
Shachoo-ga mairi-mashi-ta.
president-SB come[NsubH]-Polite-Past
?The president has arrived.'
(14) is an example that combines different
types of honorific information. Its CONTEXT
annotation is described in Figure 7.7 The usage
of the noun haha triggers an in_group_rel
(speaker, haha) with [POLARITY +], while the
usage of the noun okaasan will trigger an
in_group_rel (speaker, okaasan) with
[POLARITY -]. The extraction of social ranking
information from Goi-Taikei shown in Figure 2
makes use of this relation social_ranking_rel
(arg1, arg2) between the entities in the sentence,
for example 14 social_ranking_rel (sensei,
haha).
(14) ?????????????????
Haha-wa senseo-ni denwa-o
Monther-Top teacher-Dat call-Acc
site-morai-mashi-ta.
do-receive-Polite-past
?My mother got the teacher to call.?
CONTEXT C-INDS SPEAKER #1
ADDRESSEE #2
entity-honor-rel
HONORER #1
HONORED sensei
POLARITY +
addr-honor-rel
HONORER #1
HONORED #2
POLARITY +
empathy-rel
BACKGR  EMPER #1
EMPEE haha
in-group-rel
ARG1 #1
AEG2 haha
POLARITY +
Social-ranking-rel
ARG1 sensei
ARG2 haha
Figure 7: Context annotation of complex
honorification
7 The values of HONORED or ARGx are actually pointers
to the indices of the entities, which are written here as the
orthographic realization for readability.
Honorifics
Propositional
honorifics
Subject
honorifics
Addressee
honorifics
(Polarity -)(Polarity +)
o-hanasi ni nar-u,
etc.
o-hanasi su-ru, etc.
hanasi-mas-u, etc.
hanas-u, etc.
(Polarity -)
Entity
honorifics
sensei watashi
(Polarity +) (Polarity -)(Polarity +)
Social
ranking
In-group
relation
(Polarity +)
(Polarity -)Yomaseru
(Causative),
Insotsu, etc
haha okaasan
99
8 Conclusion
This paper has proposed a scheme to realise the
complex linguistic phenomena of the Japanese
honorifics in tangible forms for auto-processing.
Ranking referents is an extremely complex task
that requires a combined understanding of
syntax, semantics and pragmatics in many
dimensions.
In future work, the referential links and their
calibration need to be expanded to make an
annotation more meaningful. This will be an
incremental process and takes a substantial
amount of work, perhaps comparable to that
required in creating a thesaurus or knowledge
base.
The annotated data will be a valuable
resource for research on zero pronoun resolution
and Machine Translation of generating Japanese
sentences. As the Korean honorification system
is quite similar to the Japanese, it will be
feasible to make use of the approach also for
Korean. Furthermore, a part of the approach can
be extended as well for Chinese, since Japanese
makes use of the Chinese characters.
Acknowledgement
We are grateful to Kyonghee Paik for her
invaluable advice.
References
F. Bond. 2005. Translating the untranslatable:
A solution to the problem of generating
English determiners. CSLI Publications.
California
F. Bond et al 2004. The Hinoki Treebank: A
Treebank for Text Understanding, In Proc. of
the First IJCNLP, Lecture Notes in Computer
Science. Springer Verlag
P. Brown and S.C. Levinson. 1987. Politeness:
Some universals in language usage.
Cambridge University Press
K. Dohsaka. 1990. Identifying the referents of
zero-pronouns in Japanese dialogues. In Proc.
Of the 9th European Conference on Artificial
Intelligence. 240-245
T. Gunji. 1987. Japanese phrase structure
grammar. Dordrecht: Reidel
S.I. Harada. 1976. Honorifics. In Shibatani (ed).
Japanese Generative Grammar. Syntax and
Semantics Series Vol.5. New York: Academic
Press. 499-561
M. Hoelter. 1999. Lexical-semantic information
in Head-Driven Phrase Structure grammar
and natural language processing. Lincom
Theoretical Linguistics
S. Ikehara et al (eds). 1997. Japanese Lexicon.
Iwanami Publishing
A. Ikeya. 1983. Japanese honorific systems. In
Proc. of the 3rd Korean-Japanese Joint
Workshop. Seoul
S. Martin. 1964. Speech levels in Japanese and
Korean. In Hymes. Language in culture and
society. Harper and Row. 407-415
Y. Matsumoto. 1997. The rise and fall of
Japanese nonsubject honorifics. Journal of
Pragmatics. 28:719-740
H. Nakaiwa. 2002. Studies on zero pronoun
resolution for the Japanese-to-English
Machine translation (in Japanese).
S. Nariyama. 2003. Ellipsis and Reference-
tracking in Japanese. SLCS 66, Amsterdam:
John Benjamins
S. Nariyama et al 2005. Extracting
Representative Arguments from Dictionaries
for Resolving Zero Pronouns. In Proc. of the
MT Summit X, Phuket
C. Pollard and I.A. Sag. 1994. Head-Driven
Phrase Structure Grammar. Chicago:
University of Chicago Press
M. Siegel. 2000. Japanese Honorification in an
HPSG Framework. In Proc. of the 14th
Pacific Asia Conference on Language,
Information and Computation. 289-300.
Waseda University, Tokyo
M. Siegel and E M. Bender. 2002. Efficient
deep processing of Japanese. In Proc. of the
3rd Workshop on Asian Language Resources
and International Standardization at
COLING.Taipei
J. Wenger. 1983. Variation and change in
Japanese honorific forms. In Miyagawa and
Kitagawa (eds). Studies in Japanese language
use. vol.16. Edmonton: Linguistic Research
Inc. 283-292
100
Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 65?68,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Implemented Description of Japanese:
The Lexeed Dictionary and the Hinoki Treebank
Sanae Fujita, Takaaki Tanaka, Francis Bond, Hiromi Nakaiwa
NTT Communication Science Laboratories,
Nippon Telegraph and Telephone Corporation
{sanae, takaaki, bond, nakaiwa}@cslab.kecl.ntt.co.jp
Abstract
In this paper we describe the current state
of a new Japanese lexical resource: the
Hinoki treebank. The treebank is built
from dictionary definition sentences, and
uses an HPSG based Japanese grammar to
encode both syntactic and semantic infor-
mation. It is combined with an ontology
based on the definition sentences to give a
detailed sense level description of the most
familiar 28,000 words of Japanese.
1 Introduction
In this paper we describe the current state of a
new lexical resource: the Hinoki treebank. The
ultimate goal of our research is natural language
understanding ? we aim to create a system that
can parse text into some useful semantic represen-
tation. This is an ambitious goal, and this pre-
sentation does not present a complete solution,
but rather a road-map to the solution, with some
progress along the way.
The first phase of the project, which we present
here, is to construct a syntactically and semanti-
cally annotated corpus based on the machine read-
able dictionary Lexeed (Kasahara et al, 2004).
This is a hand built self-contained lexicon: it con-
sists of headwords and their definitions for the
most familiar 28,000 words of Japanese. Each
definition and example sentence has been parsed,
and the most appropriate analysis selected. Each
content word in the sentences has been marked
with the appropriate Lexeed sense. The syntac-
tic model is embodied in a grammar, while the se-
mantic model is linked by an ontology. This makes
it possible to test the use of similarity and/or se-
mantic class based back-offs for parsing and gen-
eration with both symbolic grammars and statisti-
cal models.
In order to make the system self sustaining we
base the first growth of our treebank on the dic-
tionary definition sentences themselves. We then
train a statistical model on the treebank and parse
the entire lexicon. From this we induce a the-
saurus. We are currently tagging other genres with
the same information. We will then use this infor-
mation and the thesaurus to build a parsing model
that combines syntactic and semantic information.
We will also produce a richer ontology ? for ex-
ample extracting selectional preferences. In the
last phase, we will look at ways of extending our
lexicon and ontology to less familiar words.
2 The Lexeed Semantic Database of
Japanese
The Lexeed Semantic Database of Japanese con-
sists of all Japanese words with a familiarity
greater than or equal to five on a seven point
scale (Kasahara et al, 2004). This gives 28,000
words in all, with 46,000 different senses. Defini-
tion sentences for these sentences were rewritten
to use only the 28,000 familiar words (and some
function words). The defining vocabulary is ac-
tually 16,900 different words (60% of all possi-
ble words). A simplified example entry for the
last two senses of the word doraibfla
?driver? is given in Figure 1, with English glosses
added, but omitting the example sentences. Lex-
eed itself consists of just the definitions, familiar-
ity and part of speech, all the underlined features
are those added by the Hinoki project.
3 The Hinoki Treebank
The structure of our treebank is inspired by the
Redwoods treebank of English (Oepen et al,
2002) in which utterances are parsed and the anno-
tator selects the best parse from the full analyses
derived by the grammar. We had four main rea-
sons for selecting this approach. The first was that
we wanted to develop a precise broad-coverage
65
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
INDEX doraiba?
POS noun Lexical-Type noun-lex
FAMILIARITY 6.5 [1?7] (? 5) Frequency 37 Entropy 0.79
SENSE 1 . . .
SENSE 2
P(S2) = 0.84
?
?
?
?
?
?
?
DEFINITION 1/ / 1/ / 1/
Someone who drives a car.
HYPERNYM 1 hito ?person?
SEM. CLASS ?292:chauffeur/driver? (? ?5:person?)
WORDNET driver1
?
?
?
?
?
?
?
SENSE 3
P(S2) = 0.05
?
?
?
?
?
?
?
?
?
?
DEFINITION 1/ / / 1/ / / 3/ / /
In golf, a long-distance club. A number one wood.
HYPERNYM 3 kurabu ?club?
SEM. CLASS ?921:leisure equipment? (? 921)
WORDNET driver5
DOMAIN 1 gorufu ?golf?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 1: Entry for the Word doraibfla ?driver? (with English glosses)
grammar in tandem with the treebank, as part of
our research into natural language understanding.
Treebanking the output of the parser allows us
to immediately identify problems in the grammar,
and improving the grammar directly improves the
quality of the treebank in a mutually beneficial
feedback loop.
The second reason is that we wanted to annotate
to a high level of detail, marking not only depen-
dency and constituent structure but also detailed
semantic relations. By using a Japanese gram-
mar (JACY: Siegel (2000)) based on a monostratal
theory of grammar (Head Driven Phrase Structure
Grammar) we could simultaneously annotate syn-
tactic and semantic structure without overburden-
ing the annotator. The treebank records the com-
plete syntacto-semantic analysis provided by the
HPSG grammar, along with an annotator?s choice
of the most appropriate parse. From this record,
all kinds of information can be extracted at various
levels of granularity: A simplified example of the
labeled tree, minimal recursion semantics repre-
sentation (MRS) and semantic dependency views
for the definition of 2 doraibfla ?driver?
is given in Figure 2.
The third reason was that use of the grammar as
a base enforces consistency ? all sentences anno-
tated are guaranteed to have well-formed parses.
The last reason was the availability of a reason-
ably robust existing HPSG of Japanese (JACY),
and a wide range of open source tools for de-
veloping the grammars. We made extensive use
of tools from the the Deep Linguistic Process-
ing with HPSG Initiative (DELPH-IN: http://
www.delph-in.net/) These existing resources
enabled us to rapidly develop and test our ap-
proach.
3.1 Syntactic Annotation
The construction of the treebank is a two stage
process. First, the corpus is parsed (in our case
using JACY), and then the annotator selects the
correct analysis (or occasionally rejects all anal-
yses). Selection is done through a choice of dis-
criminants. The system selects features that distin-
guish between different parses, and the annotator
selects or rejects the features until only one parse
is left. The number of decisions for each sentence
is proportional to log2 in the length of the sentence
(Tanaka et al, 2005). Because the disambiguat-
ing choices made by the annotators are saved, it
is possible to semi-automatically update the tree-
bank when the grammar changes. Re-annotation
is only necessary in cases where the parse has be-
come more ambiguous or, more rarely, existing
rules or lexical items have changed so much that
the system cannot reconstruct the parse.
The Lexeed definition sentences were already
POS tagged. We experimented with using the POS
tags to mark trees as good or bad (Tanaka et al,
2005). This enabled us to reduce the number of
annotator decisions by 20%.
One concern with Redwoods style treebanking
is that it is only possible to annotate those trees
that the grammar can parse. Sentences for which
no analysis had been implemented in the grammar
or which fail to parse due to processing constraints
are left unannotated. This makes grammar cov-
66
UTTERANCE
NP
VP N
PP V
N CASE-P V V
jidflosha o unten suru hito
car ACC drive do person
Parse Tree
?h0,x1{h0 :proposition m(h1)
h1 :hito n(x1) ?person?
h2 :ude f q(x1,h1,h6)
h3 : jidosha n(x2) ?car?
h4 :ude f q(x2,h3,h7)
h5 :unten s(e1,x1,x2)}??drive?
MRS
{x1 :
e1 :unten s(ARG1 x1 : hito n,ARG2 x2 : jidosha n)
r1 : proposition m(MARG e1 : unten s)}
Semantic Dependency
Figure 2: Parse Tree, Simplified MRS and Dependency Views for 2 doraibfla ?driver?
erage a significant issue. We extended JACY by
adding the defining vocabulary, and added some
new rules and lexical-types (more detail is given
in Bond et al (2004)). None of the rules are spe-
cific to the dictionary domain. The grammatical
coverage over all sentences is now 86%. Around
12% of the parsed sentences were rejected by the
treebankers due to an incomplete semantic repre-
sentation. The total size of the treebank is cur-
rently 53,600 definition sentences and 36,000 ex-
ample sentences: 89,600 sentences in total.
3.2 Sense Annotation
All open class words were annotated with their
sense by five annotators. Inter-annotator agree-
ment ranges from 0.79 to 0.83. For example, the
word kurabu ?club? is tagged as sense 3 in
the definition sentence for driver3, with the mean-
ing ?golf-club?. For each sense, we calculate the
entropy and per sense probabilities over four cor-
pora: the Lexeed definition and example sentences
and Newspaper text from the Kyoto University and
Senseval 2 corpora (Tanaka et al, 2006).
4 Applications
4.1 Stochastic Parse Ranking
Using the treebanked data, we built a stochastic
parse ranking model. The ranker uses a maximum
entropy learner to train a PCFG over the parse
derivation trees, with the current node, two grand-
parents and several other conditioning features. A
preliminary experiment showed the correct parse
is ranked first 69% of the time (10-fold cross val-
idation on 13,000 sentences; evaluated per sen-
tence). We are now experimenting with extensions
based on constituent weight, hypernym, semantic
class and selectional preferences.
4.2 Ontology Acquisition
To extract hypernyms, we parse the first defini-
tion sentence for each sense (Nichols et al, 2005).
The parser uses the stochastic parse ranking model
learned from the Hinoki treebank, and returns the
semantic representation (MRS) of the first ranked
parse. In cases where JACY fails to return a parse,
we use a dependency parser instead. The highest
scoping real predicate is generally the hypernym.
For example, for doraibfla2 the hypernym is hito
?person? and for doraib fla3 the hypernym is
kurabu ?club? (see Figure 1). We also extract
other relationships, such as synonym and domain.
Because the words are sense tags, we can special-
ize the relations to relations between senses, rather
than just words: ?hypernym: doraiba?3, kurabu3?.
Once we have synonym/hypernym relations, we
can link the lexicon to other lexical resources. For
example, for the manually constructed Japanese
ontology Goi-Taikei (Ikehara et al, 1997) we link
to its semantic classes by the following heuristic:
look up the semantic classes C for both the head-
word (wi) and hypernym(s) (wg). If at least one of
the index word?s classes is subsumed by at least
one of the genus? classes, then we consider the re-
lationship confirmed. To link cross-linguistically,
we look up the headwords and hypernym(s) in a
translation lexicon and compare the set of trans-
lations ci ? C(T (wi)) with WordNet (Fellbaum,
1998)). Although looking up the translation adds
noise, the additional filter of the relationship triple
effectively filters it out again.
Adding the ontology to the dictionary interface
makes a far more flexible resource. For example,
by clicking on the ?hypernym: doraiba?3, goru f u1?
link, it is possible to see a list of all the senses re-
67
lated to golf, a link that is inaccessible in the paper
dictionary.
4.3 Semi-Automatic Grammar
Documentation
A detailed grammar is a fundamental component
for precise natural language processing. It pro-
vides not only detailed syntactic and morphologi-
cal information on linguistic expressions but also
precise and usually language-independent seman-
tic structures of them. To simplify grammar de-
velopment, we take a snapshot of the grammar
used to treebank in each development cycle. From
this we extract information about lexical items
and their types from both the grammar and tree-
bank and convert it into an electronically accesi-
ble structured database (the lexical-type database:
Hashimoto et al, 2005). This allows grammar de-
velopers and treebankers to see comprehensive up-
to-date information about lexical types, including
documentation, syntactic properties (super types,
valence, category and so on), usage examples from
the treebank and links to other dictionaries.
5 Further Work
We are currently concentrating on three tasks. The
first is improving the coverage of the grammar,
so that we can parse more sentences to a cor-
rect parse. The second is improving the knowl-
edge acquisition, in particular learning other in-
formation from the parsed defining sentences ?
such as lexical-types, semantic association scores,
meronyms, and antonyms. The third task is adding
the knowledge of hypernyms into the stochastic
model.
The Hinoki project is being extended in several
ways. For Japanese, we are treebanking other gen-
res, starting with Newspaper text, and increasing
the vocabulary, initially by parsing other machine
readable dictionaries. We are also extending the
approach multilingually with other grammars in
the DELPH-IN group. We have started with the
English Resource Grammar and the Gnu Contem-
porary International Dictionary of English and are
investigating Korean and Norwegian through co-
operation with the Korean Research Grammar and
NorSource.
6 Conclusion
In this paper we have described the current state of
the Hinoki treebank. We have further showed how
it is being used to develop a language-independent
system for acquiring thesauruses from machine-
readable dictionaries.
With the improved the grammar and ontology,
we will use the knowledge learned to extend our
model to words not in Lexeed, using definition
sentences from machine-readable dictionaries or
where they appear within normal text. In this way,
we can grow an extensible lexicon and thesaurus
from Lexeed.
Acknowledgements
We thank the treebankers, Takayuki Kurib-
ayashi, Tomoko Hirata and Koji Yamashita, for
their hard work and attention to detail.
References
Francis Bond, Sanae Fujita, Chikara Hashimoto, Kaname
Kasahara, Shigeko Nariyama, Eric Nichols, Akira Ohtani,
Takaaki Tanaka, and Shigeaki Amano. 2004. The Hinoki
treebank: A treebank for text understanding. In Proceed-
ings of the First International Joint Conference on Natural
Language Processing (IJCNLP-04). Springer Verlag. (in
press).
Christine Fellbaum, editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Chikara Hashimoto, Francis Bond, Takaaki Tanaka, and
Melanie Siegel. 2005. Integration of a lexical type
database with a linguistically interpreted corpus. In 6th
International Workshop on Linguistically Integrated Cor-
pora (LINC-2005), pages 31?40. Cheju, Korea.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei ?
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lex-
icon: Lexeed. SIG NLC-159, IPSJ, Tokyo. (in Japanese).
Eric Nichols, Francis Bond, and Daniel Flickinger. 2005. Ro-
bust ontology acquisition from machine-readable dictio-
naries. In Proceedings of the International Joint Confer-
ence on Artificial Intelligence IJCAI-2005, pages 1111?
1116. Edinburgh.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christoper D. Manning, Dan Flickinger, and Thorsten
Brant. 2002. The LinGO redwoods treebank: Motivation
and preliminary applications. In 19th International Con-
ference on Computational Linguistics: COLING-2002,
pages 1253?7. Taipei, Taiwan.
Melanie Siegel. 2000. HPSG analysis of Japanese. In Wolf-
gang Wahlster, editor, Verbmobil: Foundations of Speech-
to-Speech Translation, pages 265? 280. Springer, Berlin,
Germany.
Takaaki Tanaka, Francis Bond, and Sanae Fujita. 2006. The
Hinoki sensebank ? a large-scale word sense tagged cor-
pus of Japanese ?. In Frontiers in Linguistically Anno-
tated Corpora 2006. Sydney. (ACL Workshop).
Takaaki Tanaka, Francis Bond, Stephan Oepen, and Sanae
Fujita. 2005. High precision treebanking ? blazing useful
trees using POS information. In ACL-2005, pages 330?
337.
68
