Coling 2008: Companion volume ? Posters and Demonstrations, pages 75?78
Manchester, August 2008
Using very simple statistics for review search: An exploration
Bo Pang
Yahoo! Research
bopang@yahoo-inc.com
Lillian Lee
Computer Science Department, Cornell University
llee@cs.cornell.edu
Abstract
We report on work in progress on using
very simple statistics in an unsupervised
fashion to re-rank search engine results
when review-oriented queries are issued;
the goal is to bring opinionated or subjec-
tive results to the top of the results list. We
find that our proposed technique performs
comparably to methods that rely on sophis-
ticated pre-encoded linguistic knowledge,
and that both substantially improve the ini-
tial results produced by the Yahoo! search
engine.
1 Introduction
One important information need shared by many
people is to find out about opinions and perspec-
tives on a particular topic (Mishne and de Rijke,
2006; Pang and Lee, 2008). In fact, locating rel-
evant subjective texts was a core task in the 2006
and 2007 TREC Blog tracks (Ounis et al, 2006;
Ounis et al, 2008). Most participants considered a
two-phase re-ranking approach, where first topic-
based relevancy search was employed, and then
some sort of filtering for subjectivity was applied;
these filters were based on trained classifiers or
subjectivity lexicons.
We propose an alternative approach to review
search, one that is unsupervised and that does
not rely on pre-existing dictionaries. Rather, it
in essence simply re-ranks the top k topic-based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
search results by placing those that have the least
idiosyncratic term distributions, with respect to the
statistics of the top k results, at the head of the list.
The fact that it is the least, not the most, rare terms
with respect to the search results that are most in-
dicative of subjectivity may at first seem rather
counterintuitive; indeed, previous work has found
rare terms to be important subjectivity cues (Wiebe
et al, 2004). However, reviews within a given set
of search results may tend to resemble each other
because they tend to all discuss salient attributes of
the topic in question.
2 Algorithm
Define a search set as the top n webpages returned
in response to a review- or opinion-oriented query
by a high-quality initial search engine, in our case,
the top 20 returned by Yahoo!. As a question of
both pragmatic and scientific value, we consider
how much information can be gleaned simply from
the items in the search set itself; in particular, we
ask whether the subjective texts in the search set
can be ranked above the objective ones solely from
examination of the patterns of term occurrences
across the search-set documents.
The idea we pursue is based in part on the as-
sumption that the initial search engine is of rel-
atively high quality, so that many of the search-
set documents probably are, in fact, subjective.
Therefore, re-ordering the top-ranked documents
by how much they resemble the other search-set
documents in aggregate may be a good way to
identify the reviews. Indeed, perhaps the reviews
will be similar to one another because they all tend
to discuss salient features of the topic in question.
75
Suppose we have defined a search-set rarity
function Rarity
ss
(t) (see Section 2.1 below) that
varies inversely with the number of documents in
the search set that contain the term t. Then, we
define the idiosyncrasy score of a document d as
the average search-set rarity of the most common
terms it contains:
I(d, k) =
1
k
?
t?k-commonest-terms(d)
Rarity
ss
(t) ,
(1)
where k-commonest-terms(d) is the k common-
est terms in the search set that also occur in d. For
example, when we set k to be the size of the vo-
cabulary of d, the idiosyncrasy score is the aver-
age search-set rarity of all the terms d contains.
Then, to instantiate the similarity intuition outlined
above, we simply rank by decreasing idiosyncrasy.
The reason we look at just the top most com-
mon terms is that the rarer terms might be noise.
For example, terms that occur in just a few of
the search-set documents might represent page- or
site-specific information that is irrelevant to the
query; but the presence of such terms does not nec-
essarily indicate that the document in question is
objective.
One potential problem with the approach out-
lined above is the presence of stopwords, since
all documents, subjective or objective, can be ex-
pected to contain many of them. Therefore, stop-
word removal is indicated as an important pre-
processing step. As it turns out, the commonly-
used InQuery stopword list (Allan et al, 2000)
contains terms like ?less? that, while uninforma-
tive for topic-based information retrieval, may be
important for subjectivity detection. Therefore, we
used a 102-item list1 based solely on frequencies in
the British National Corpus.
2.1 Defining search-set rarity
There are various ways to define a search-set rar-
ity function on terms. Inspired by the efficacy of
the inverse document frequency (IDF) in informa-
tion retrieval, we consider several definitions for
Rarity
ss
(t). Let n
ss
(t) be the number of docu-
ments in the search set (not the entire corpus) that
contain the term t. Due to space constraints, we
1www.eecs.umich.edu/?qstout/586/bncfreq.html
only report results for:
Rarity
ss
(t)
def
=
1
n
ss
(t)
,
which is linearly increasing in 1/n
ss
(t), (as befits
a measure of ?idiosyncrasy?). The other defini-
tions we considered were logarithmic or polyno-
mial in 1/n
ss
(t), and performed similarly to the
linear function.
2.2 Comparison algorithms
OpinionFinder is a state-of-the-art publicly avail-
able software package for sentiment analysis that
can be applied to determining sentence-level sub-
jectivity (Riloff and Wiebe, 2003; Wiebe and
Riloff, 2005). It employs a number of
pre-processing steps, including sentence splitting,
part-of-speech tagging, stemming, and shallow
parsing. Shallow parsing is needed to identify the
extraction patterns that the sentence classifiers in-
corporate.
We used OpinionFinder?s sentence-level output2
to perform document-level subjectivity re-ranking
as follows. The result of running OpinionFinder?s
sentence classifier is that each valid sentence3 is
annotated with one of three labels: ?subj?, ?obj?,
or ?unknown?. First, discard the sentences labeled
?unknown?. Then, rank the documents by de-
creasing percentage of subjective sentences among
those sentences that are left. In the case of ties, we
use the ranking produced by the initial search en-
gine.
We also considered a more lightweight way
to incorporate linguistic knowledge: score each
document according the percentage of adjectives
within the set of tokens it contains. The motiva-
tion is previous work suggesting that the presence
of adjectives is a strong indicator of the subjectiv-
ity of the enclosing sentence (Hatzivassiloglou and
Wiebe, 2000; Wiebe et al, 2004).
2There are actually two versions. We used the accuracy-
optimized version, as it outperformed the precision-optimized
version.
3OpinionFinder will only process documents in which all
strings identified as sentences by the system contain fewer
than 1000 words. For the 31 documents in our dataset that
failed this criterion, we set their score to 0.
76
p@1 p@2 p@3 p@4 p@5 p@10 p@S MAP
Search-engine baseline .536 .543 .541 .554 .554 .528 .538 .612
OpinionFinder (accuracy version) .754 .717 .729 .725 .733 .675 .690 .768
% of adjectives (type-based) .710 .703 .696 .681 .678 .625 .633 .715
idiosyncrasy(linear), k = 50 .797 .783 .739 .717 .696 .613 .640 .729
idiosyncrasy(linear), k = 100 .754 .783 .768 .739 .716 .630 .665 .743
idiosyncrasy(linear), k = 200 .768 .761 .744 .746 .716 .623 .653 .731
idiosyncrasy(linear), k = 300 .754 .761 .749 .736 .704 .614 .641 .724
Table 1: Average search-set subjective-document precision results. ?S?: number of subjective docu-
ments. Bold and underlining: best and second-best performance per column, respectively.
3 Evaluation
Our focus is on the quality of the documents placed
at the very top ranks, since users often look only
at the first page or first half of the first page of
results (Joachims et al, 2005). Hence, we report
the precision of the top 1-5 and 10 documents, as
well as precision at the number of subjective doc-
uments and mean average precision (MAP) for the
subjective documents. All performance numbers
are averages over the 69 search sets in our data,
described next.
Data Here, we sketch the data acquisition and
labeling process. In order to get real user queries
targeted at reviews, we began with a randomly se-
lected set of queries containing the word ?review?
or ?reviews?4 from the the query log available at
http://www.sigkdd.org/kdd2005/kddcup/
KDDCUPData.zip . We created a search set for
each query by taking the top 20 webpages returned
by the Yahoo! search engine and applying some
postprocessing. Over a dozen volunteer annotators
then labeled the documents as to whether they
were subjective or objective according to a set
of detailed instructions. The end result was
over 1300 hand-labeled documents distributed
across 69 search sets, varying widely with re-
spect to query topic. Our dataset download site
is http://www.cs.cornell.edu/home/llee/
data/search-subj.html .
For almost every annotator, at least two of his
or her search sets were labeled by another person
as well, so that we could measure pair-wise agree-
4Subsequent manual filtering discarded some non-
opinion-oriented queries, such as ?alternative medicine re-
view volume5 numer1 pages 28 38 2000?.
ment with respect to multiple queries. On average,
there was agreement on 88.2% of the documents
per search set, with the average Kappa coefficient
(?) being an acceptable 0.73, reflecting in part the
difficulty of the judgment.5 The lowest ? occurs
on a search set with a 75% agreement rate.
Results A natural and key baseline is the ranking
provided by the Yahoo! search engine, which is a
high-quality, industrial-strength system. We con-
sider this to be a crucial point of comparison. The
results are shown in the top line in Table 1.
OpinionFinder clearly outperforms the initial
search engine by a substantial margin, indicating
that there are ample textual cues that can help
achieve better subjectivity re-ranking.
The adjective-percentage baseline is also far su-
perior to that of the search-engine baseline at all
ranks, but does not quite match OpinionFinder.
(Note that to achieve these results, we first dis-
carded all terms contained in three or fewer of the
search-set documents, since including such terms
decreased performance.) Still, it is interesting to
see that it appears that a good proportion of the
improvements provided by OpinionFinder can be
achieved using just adjective counts alone.
We now turn to subjectivity re-ranking based on
term-distribution (idiosyncrasy) information. For
5One source of disagreement that stems from the specifics
of our design is that we instructed annotators to mark ?sales
pitch? documents as non-reviews, on the premise that al-
though such texts are subjective, they are not valuable to a
user searching for unbiased reviews. (Note that this pol-
icy presumably makes the dataset more challenging for au-
tomated algorithms.) There are several cases where only one
annotator identified this type of bias, which is not surprising
since the authors of sales pitches may actively try to fool read-
ers into believing the text to be unbiased.
77
consistency with the adjective-based method just
described, we first discarded all terms contained in
three or fewer of the search-set documents.
As shown in Table 1, the idiosyncrasy-based al-
gorithm posts results that are overall strongly su-
perior to those of the initial, high-quality search
engine algorithm and also generally better than the
adjective-percentage algorithm. Note that these
phenomena hold for a range of values of k. The
overall performance is also on par with Opin-
ionFinder; for instance, according to the paired
t-test, the only statistically significant perfor-
mance difference (.05 level) between the accuracy-
emphasizing version of OpinionFinder and the
idiosyncrasy-based algorithm for k = 100 is for
precision at 10. In some sense, this is a striking re-
sult: just looking at within-search-set frequencies
yields performance comparable to that of a method
that utilizes rich linguistic knowledge and external
resources regarding subjectivity indicators.
Another interesting observation is that term-
distribution information seems to be more effective
for achieving high precision at the very top ranks
(precision at 1, 2, 3, and 4), whereas in contrast,
relatively deep NLP seems to be more effective at
achieving high precision at the ?lower? top ranks,
as demonstrated by the results for precision at 5,
10, and the number of subjective documents, and
for MAP. These results suggest that a combination
of the two methods could produce even greater im-
provements.
4 Concluding remarks
We considered the task of document-level sub-
jectivity re-ranking of search sets, a task mod-
eling a scenario in which a search engine is
queried to find reviews. We found that our pro-
posed term-distributional, idiosyncrasy-based al-
gorithm yielded the best precision for the very top
ranks, whereas the more linguistically-oriented,
knowledge-rich approach exemplified by Opinion-
Finder gave the best results for precision at lower
ranks. It therefore seems that both types of infor-
mation can be very valuable for the subjectivity
re-ranking task, since they have somewhat com-
plementary performance behaviors and both out-
perform the initial search engine and an adjective-
based approach.
Our motivation that within a search set, reviews
tend to resemble one another rather than differ
is reminiscent of intuitions underlying the use of
pseudo relevance feedback (PF) in IR (Ruthven
and Lalmas, 2003, Section 3.5). Future work in-
cludes comparison against PF methods and inves-
tigation of ways to select the value of k.
Acknowledgments We thank Eli Barzilay, Rich Caru-
ana, Thorsten Joachims, Jon Kleinberg, Ravi Kumar, and the
reviewers for their very useful help. We are also very grateful
to our annotators, Mohit Bansal, Eric Breck, Yejin Choi, Matt
Connelly, Tom Finley, Effi Georgala, Asif-ul Haque, Kersing
Huang, Evie Kleinberg, Art Munson, Ben Pu, Ari Rabkin,
Benyah Shaparenko, Ves Stoyanov, and Yisong Yue. This
paper is based upon work supported in part by the NSF un-
der grant no. IIS-0329064, a Yahoo! Research Alliance gift,
Google Anita Borg Memorial Scholarship funds, a Cornell
Provost?s Award for Distinguished Research, and an Alfred
P. Sloan Research Fellowship. Any opinions, findings, and
conclusions or recommendations expressed are those of the
authors and do not necessarily reflect the views or official
policies, either expressed or implied, of any sponsoring in-
stitutions, the U.S. government, or any other entity.
References
Allan, James, Margaret E. Connell, W. Bruce Croft, Fang-
Fang Feng, David Fisher, and Xiaoyan Li. 2000. IN-
QUERY and TREC-9. In Proceedings of TREC, pages
551?562. NIST Special Publication 500-249.
Hatzivassiloglou, Vasileios and Janyce Wiebe. 2000. Effects
of adjective orientation and gradability on sentence subjec-
tivity. In Proceedings of COLING.
Joachims, Thorsten, Laura Granka, Bing Pan, Helene Hem-
brooke, and Geri Gay. 2005. Accurately interpreting
clickthrough data as implicit feedback. In Proceedings of
SIGIR, pages 154?161.
Mishne, Gilad and Maarten de Rijke. 2006. A study of blog
search. In Proceedings of ECIR.
Ounis, Iadh, Maarten de Rijke, Craig Macdonald, Gilad
Mishne, and Ian Soboroff. 2006. Overview of the TREC-
2006 Blog Track. In Proceedings of TREC.
Ounis, Iadh, Craig Macdonald, and Ian Soboroff. 2008. On
the TREC Blog Track. In Proceedings of ICWSM.
Pang, Bo and Lillian Lee. 2008. Opinion Mining and Sen-
timent Analysis. Foundations and Trends in Information
Retrieval series. Now publishers.
Riloff, Ellen and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceedings
of EMNLP.
Ruthven, Ian and Mounia Lalmas. 2003. A survey on the
use of relevance feedback for information access systems.
Knowledge Engineering Review, 18(2):95?145.
Wiebe, Janyce M. and Ellen Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated texts.
In Proceedings of CICLing, number 3406 in LNCS, pages
486?497.
Wiebe, Janyce M., Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjec-
tive language. Computational Linguistics, 30(3):277?308,
September.
78
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 609?618,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Matching Reviews to Objects using a Language Model
Nilesh Dalvi Ravi Kumar Bo Pang Andrew Tomkins
Yahoo! Research
701 First Ave
Sunnyvale, CA 94089
{ndalvi,ravikumar,bopang,atomkins}@yahoo-inc.com
Abstract
We develop a general method to match un-
structured text reviews to a structured list
of objects. For this, we propose a lan-
guage model for generating reviews that
incorporates a description of objects and a
generic review language model. This mix-
ture model gives us a principled method to
find, given a review, the object most likely
to be the topic of the review. Extensive
experiments and analysis on reviews from
Yelp show that our language model-based
method vastly outperforms traditional tf-
idf-based methods.
1 Introduction
Consider a user searching for reviews of
?Casablanca Moroccan Restaurant.? The search
engine would like to obtain as many reviews of
this restaurant as possible, both to offer a high-
quality result set for even obscure restaurants, and
to enable advanced applications such as aggrega-
tion/summarization/categorization of reviews and
recommendation of alternate restaurants. To solve
this problem, it faces two high-level challenges:
first, identify the restaurant review pages on the
Web; and second, given a review, identify the
restaurant that is being reviewed. There has been
previous work addressing the first challenge (Sec-
tion 2). We focus in this paper on the second.
The Web is replete with restaurant reviews
available on top restaurant verticals such as Yelp
and CitySearch, as well as newspaper articles,
newsgroup discussions, blog posts, small local re-
view aggregators and so forth. Ideally, the search
engine would like to obtain reviews from all pos-
sible sources. While identifying the subject mat-
ter of a given review on the large sites may be
amenable to structured extraction through wrapper
induction or related techniques, it is typically not
cost-effective to apply such techniques to smaller
?tail? sites, and purely unstructured sources re-
quire alternate approaches altogether. In this pa-
per, we explore the setting of matching reviews to
objects using only their textual content. Note that
matching reviews to objects is a pervasive prob-
lem beyond the restaurant domain. Shopping ver-
ticals like to aggregate camera reviews, entertain-
ment verticals wish to collect movie reviews, and
so on. We use restaurant reviews as a running ex-
ample, but the techniques are general.
More specifically, the problem we consider in
this paper is the following. Given a list of struc-
tured objects (restaurants/cameras/movies) and a
text review, identify the object from the list that
is the topic of the review. Our focus on tex-
tual content allows us to expand the universe of
sources from which we can extract reviews to in-
clude sources that are purely textual, such as fo-
rum posts, blog posts, newsgroup postings, and
the like. In fact, even among collections of ?struc-
tured? sources like review aggregators, there are
no highly accurate unsupervised techniques to
match a known review page to an object. Struc-
tured (e.g., HTML) cues provide valuable lever-
age in attacking this problem, but the types of tex-
tual cues we focus on are also a key part of the
puzzle; in such a context, our techniques can still
contribute to the overall matching problem.
It is important to contrast our problem against
two settings of related flavor ? entity matching,
whose goal is to find the correspondence between
two structured objects and information retrieval
(IR), whose goal is to match unstructured short
text (query) against unstructured text (document).
Our problem is considerably harder than entity
matching for the following reasons. In matching
two structured objects there is often a natural cor-
respondence between their attributes, whereas no
such correspondence exists between an object and
609
its review. For instance, while trying to match a
review to a restaurant object, it is unclear if a spe-
cific portion of the review refers to the name of the
restaurant, or to its location, or is a statement not
concerning specifics of the restaurant. Moreover,
even if we wish to use entity matching, we must
first recognize the entities from a review. There
are two methods to do this, namely, wrapper in-
duction and information extraction. Wrapper in-
duction methods have serious limitations: they are
applicable only to highly-structured websites and
involve human labeling effort that is expensive and
error-prone and entails constant maintenance to
keep wrappers up-to-date. Information extraction
methods (Cardie, 1997; Sarawagi, 2008), on the
other hand, often have limited accuracy.
Our problem is also not amenable to classical
IR methods such as tf-idf. For example, suppose
we want to find the relevant restaurant for a given
review. The standard tf-idf will treat the review as
the query, the set of restaurant as documents and
compute the tf-idf scores. Now consider a restau-
rant called ?Food.?
1
Since the term ?food? is rare
as a restaurant name, it will get a very high idf
score and hence will likely be the top match for all
reviews containing the word ?food.? In fact, unlike
in traditional IR, a ?query? (i.e., review) is long
and a ?document? (i.e., restaurant) is short ? this
demands adapting established IR concepts such as
inverse document frequency and document length
normalization to our setting. If we take the op-
posite view by considering reviews as documents
and restaurants as queries, we still deviate from the
IR setting, since now we need to rank and find the
best ?query? for a given ?document.? In Section
3.4, we illustrate the shortcomings of both these
approaches.
In fact, the nature of the object database we con-
sider provides several unique opportunities over
traditional IR. First the ?document?, i.e., the ob-
ject to be matched, has more semantics, since
each document is associated with one or more se-
mantic attribute, such as the name/location of the
restaurant. Second, the ?query?, i.e., the text we
are matching is known to be a review of the ob-
ject, and hence is rendered in a language that is
?review-like? ? this can be modeled by a genera-
tive process that produces reviews from objects.
Third, the set of objects we are interested in is
1
1569 Lexington Ave., New York, NY 10029. (212) 348-
0200.
given a priori, and we only seek to match reviews
with one of these objects; this makes our problem
more tractable than open-ended entity recognition.
Our contributions. We propose a general
method to match reviews to objects. To this end,
we postulate a language model for generating re-
views. The intuition behind our model is simple
and natural: when a review is written about an ob-
ject, each word in the review is drawn either from a
description of the object or from a generic review
language that is independent of the object. This
mixture model leads to a method to find, given a
review, the object most likely to be the topic of the
review.
Our method is light-weight and scalable and
can be viewed as obviating the need for highly-
expensive information extraction. Since the
method is text-based and does not rely on any
HTML structural clues, it is especially applicable
to reviews present in blogs and the so-called tail
web sites ? web sites for which it is not feasible
to maintain wrappers to automatically extract the
object of a review.
We then report results on over 11K restaurant
reviews from Yelp. The experiments and our ex-
tensive analysis show that our language model-
based method significantly outperforms traditional
tf-idf based methods, which fail to take full ad-
vantage of the properties that are specific to our
setting.
2 Related work
Opinion topic identification is the work closest
to ours. In a recent paper, Stoyanov and Cardie
(2008) approach this problem by treating it as an
exercise in topic coreference resolution. Though
they have to deal with topic ambiguities and a lack
of explicit topic mentions as in our case, their no-
tion of a topic is not driven by a structured list-
ing. There has been some work on fine-grained
opinion extraction from reviews (Kobayashi et al,
2004; Yi et al, 2003; Popescu and Etzioni, 2005;
Hu and Liu, 2004); see (Pang and Lee, 2008) for a
comprehensive survey. Most of this body of work
focused on identifying product features of the ob-
ject under review, rather than identifying the prod-
uct itself. Note that while a dictionary of prod-
ucts is often more readily available than a dictio-
nary of product features, identifying objects of re-
views is non-trivial even with the help of the for-
mer. Indeed, it has been reported that lexicon-
610
lookup methods have limited success on general
non-product review texts (Stoyanov and Cardie,
2008). In general, this line of work is more rooted
in the information extraction literature, where text
spans covering the object (or features of the ob-
ject) were extracted as the first step; in contrast,
we do not have an explicit extraction phase. Since
the (very extensive) list of candidate objects are
given as input, our task is to rank all matching ob-
jects, and in this sense is closer in nature to infor-
mation retrieval tasks. There has been some work
on detecting reviews in large-scale collections (Ng
et al, 2006; Barbosa et al, 2009); this is a logical
step that precedes the review matching step, the
topic of our paper.
Language modeling is becoming a powerful
paradigm in the realm of information retrieval ap-
plications (Ponte and Croft, 1998; Hiemstra, 1998;
Song and Croft, 1999; Lafferty and Zhai, 2003;
Zhai, 2008). The basic theme behind language
modeling is to first postulate a model for each doc-
ument and for a given query select the document
that is most likely to have generated the query;
smoothing is an important means to manage data
sparsity in language models (Zhai and Lafferty,
2004). As noted earlier, language models devel-
oped for IR are unsuitable for our setting. Further-
more, there are opportunities, such as the presence
of structure in our data, which we use in this work
(Section 3.2). In fact, in a subsequent paper, we
show how a language model specific to each at-
tribute can further improve the accuracy of review
matching (Dalvi et al, 2009).
Entity matching is a well-studied topic in
databases. There are several approaches to entity
matching: non-relational approaches, which con-
sider pairwise attribute similarities between enti-
ties (Newcombe et al, 1959; Fellegi and Sunter,
1969), relational approaches, which exploit the re-
lationships that exist between entities (Ananthakr-
ishna et al, 2002; Kalashnikov et al, 2005), and
collective approaches, which exploit the relation-
ship between various matching decisions, (Bhat-
tacharya and Getoor, 2007; McCallum and Well-
ner, 2004). The EROCS system (Chakaravarthy et
al., 2006), which uses information extraction and
entity matching, is closest in spirit to our problem;
they, however, employ tf-idf to match, which we
show to be significantly sub-optimal in our set-
ting.
3 Model and method
In this section we present the problem formula-
tion, the basic generative model for reviews, a
method based on this model to associate an object
with a review, and the techniques to estimate the
parameters of this model.
Problem formulation. Let E denote a set of ob-
jects. Each object e ? E has a set of attributes
and let text(e) denote the union of the textual con-
tent of all its attributes. Suppose we have a col-
lection of reviews R, where each review is writ-
ten (mainly) about one of the objects in the listing
E . The problem now is to correctly associate each
r ? R with exactly one of e ? E .
We model each review as a bag of words.
Therefore, notation such as ?w ? r? for a word
w and a review r makes sense. For a review r and
an object e, let r
e
= r ? text(e).
As a running example, we use E to denote the
set of all restaurants and R to denote the set of all
restaurant reviews.
3.1 A generative model for reviews
We first state the intuition behind our generative
model: when a review r is written about an object
e, some words in r (e.g., the name and the address
of the restaurant) are drawn from text(e) to refer
to the object under discussion, while some other
words are drawn from a generic review language
independent of e.
Formally, let ? ? (0, 1) be a parameter.
Let P
e
(?) denote a distribution whose support is
text(e); this corresponds to the distribution of
words specific to the object e, taken from the de-
scription text(e). We use P
e
(w) to denote the
probability the word w is chosen according to this
distribution. Let P (?) be an object-independent
distribution whose support is the review language,
i.e., all the words that can be used to write a re-
view; we use P (w) to denote the probability the
word w is chosen according to this distribution.
Now, for a given object e, a review r is gener-
ated as follows. Each word in r is generated in-
dependently: with probability ?, a word w is cho-
sen with probability P
e
(w) and with probability
1 ? ?, a word w is chosen with probability P (w).
Thus, the review generation process is a multino-
mial, where the underlying process is a mixture of
object-specific language and a generic review lan-
guage.
611
Given a review r and an object e, by our inde-
pendence assumption,
Pr[r | e] = Z(r)
?
w?r
Pr[w | e]
= Z(r)
?
w?r
((1 ? ?)P (w) + ?P
e
(w)), (1)
where Z(r) is a normalizing term that only de-
pends on the length of r and the counts of the
words in it. Recalling r
e
= r ? text(e), we note
that P
e
(w) assigns zero probability to w 6? r
e
.
From (1), we get
Pr[r | e] = Z(r)
?
w?r\r
e
(1 ? ?)P (w)?
?
w?r
e
((1 ? ?)P (w) + ?P
e
(w))
= Z(r)
?
w?r
(1 ? ?)P (w) ?
?
w?r
e
(
1 +
?
1 ? ?
P
e
(w)
P (w)
)
. (2)
Note that Eq. (2) appears similar to the formula
obtained in the language model approach for IR
(Hiemstra and Kraaij, 1998); the interpretation of
terms, however, is very different. For instance,
P (w) in our case is computed over the ?query?
corpus whereas the analogous term (collection fre-
quency) in (Hiemstra and Kraaij, 1998) is com-
puted over the ?document? corpus. As the ?Food?
restaurant example in Section 1 suggests, using
the ?document? frequency is undesirable. The use
of ?query? corpus frequency arises naturally from
our generative story and also guides us to a differ-
ent way to estimate P (w); see Section 3.3.
3.2 Matching a review to an object
Given the above review language model (RLM),
we now state how to match a given review to an
object. According to our model, the most likely
object e
?
to have generated a review r is given by
e
?
= argmax
e
Pr[e | r] = argmax
e
Pr[e]
Pr[r]
?Pr[r | e].
In the absence of any information, we assume
a uniform distribution for Pr[e]. (Additional
information about objects, such as their rat-
ing/popularity, can be used to model Pr[e] more
accurately.) From this, we get
e
?
= argmax
e
Pr[r | e],
or equivalently,
e
?
= argmax
e
log Pr[r | e].
Since Z(r)
?
w?r
((1??)P (w)) is independent of
e, using (2), we have
e
?
= argmax
e
?
w?r
e
log
(
1 +
?
1 ? ?
P
e
(w)
P (w)
)
.
(3)
3.3 Estimating the parameters
We now describe how to estimate the parameters
of the model, namely, P (?), P
e
(?), and ?.
Recall that P (?) is the distribution of generic re-
view language. Ideally, for each review r, if we
know the component r
(e)
that came from the dis-
tribution P
e
(?) and the component r
(g)
that came
from P (?), then we can collect the r
(g)
compo-
nents of all the reviews in R, denoted as R
(g)
, and
estimate P (?) by the fraction of occurrences of w
in R
(g)
. More specifically, let c(w,R
(g)
) denote
the number of times w occurs in R
(g)
. With add-
one smoothing, we estimate
P (w) =
c(w,R
(g)
) + 1
?
w
?
c(w
?
,R
(g)
) + |V |
,
where |V | is the vocabulary size.
In reality, we only have access to r and not to the
components r
(e)
and r
(g)
. If we have an aligned
review corpus R
?
, where for each review r, we
know the true object e that generated it, we can
closely approximate r
(e)
with r
e
.
2
Let no-obj(R
?
)
be the set of processed reviews where for each
review-object pair (r, e), words in text(e) are re-
moved from r. By treating no-obj(R
?
) as an ap-
proximation of R
(g)
, we can compute P (w) in the
aforementioned manner. If we only have access
to a review collection R
?
with no object align-
ment, there are other ways to effectively approx-
imate R
(g)
; see Section 5.3 for more details.
Unlike P (?), we cannot learn an individual lan-
guage model P
e
(?) for each e, since we cannot ex-
pect to have training examples of reviews for each
possible object e in the dataset. Thus, we need
a simpler way to model P
e
(w). The most naive
way would be to assume a uniform distribution,
i.e., P
e
(w) = 1/|text(e)|. However, each word
2
There can be exceptions to this, e.g., review of a restau-
rant called ?Tasty Bites? might use the word ?tasty? from the
review language, but not to refer to the restaurant. Nonethe-
less, we believe these will be rare exceptions and will not
have significant effect in the estimation of P (?).
612
in text(e) may not be generated with equal prob-
ability. In our running example, consider the case
when text(e) contains the full name of the restau-
rant, i.e., ?Casablanca Moroccan Restaurant.? A
review for this restaurant is more likely to choose
the word ?Casablanca? than any other word to re-
fer to this restaurant since this is arguably more in-
formative than ?Moroccan? or ?Restaurant.? This
can be captured by using the frequency f
w
of the
word w in R or in {text(e) | e ? E}. For a suit-
able function g(w) that is inversely growing as f
w
(say, g(w) = log(1/f
w
)), we let
P
e
(w) =
g(w)
?
w
?
?text(e)
g(w
?
)
.
Alternatively, it is possible to construct models
where P
e
(w) is more directly estimated from the
data; in fact, one can also use suitable transla-
tion models to estimate P
e
(w) for w that may not
even occur in text(e) ? this will help in cases
where reviews use an abbreviation such as ?Casa?
or ?CMR? to refer to our running example. Such
models require either fine-grained labeled exam-
ples or, as we show in (Dalvi et al, 2009), more
sophisticated estimation techniques.
It is tempting to assume that common words
such as ?Restaurant? may not contribute towards
matching a review to an object and hence one can
conveniently set P
e
(w) = 0 for such words w.
(Such a list of words can easily be compiled using
a domain-specific stopword list.) This may hurt ?
in our example, the presence of the word ?Restau-
rant? in a review might help to disambiguate the
object of reference, if the listing were also to con-
tain a ?Casablanca Moroccan Cafe?.
3.4 Properties of the model
Eq. (3) indicates that our method (denoted as
RLM) gives less importance to common words
with high P (w). This corresponds to the intuition
behind the standard tf-idf scheme. Why, then, do
we expect RLM to be more effective? Here, we
discuss the salient features of our method, con-
trasting it with tf-idf in particular.
First, we take a closer look at different ways to
apply tf-idf techniques to our setting. Since the
task is to find the most relevant object given a re-
view, a naive way to apply the standard tf-idf (de-
noted TFIDF) will treat each review to be the query
and each object to be a document and score docu-
ments using the standard tf-idf scoring. This, how-
ever, leads to severe problems since this computes
the inverse document scores over the object corpus
? recall the ?Food? example in Section 1.
A more reasonable way to apply tf-idf is to
instead treat objects as queries and reviews as
documents for computing tf-idf scores (denoted
TFIDF
+
). For a word w, let Q(w) =
df(w)
N
,
where N is the number of reviews in the corpus
and df(w) is the number of reviews containing w.
Given a review r and an object e, the score of the
object is given by
?
w?r
e
log (1/Q(w)), and we
want to pick the object with the maximum score.
As we will discuss later, document-length nor-
malization (i.e., normalizing by object description
length so that a restaurant with a long name does
not get an unfair disadvantage) is still non-trivial
here.
As noted earlier, Eq. (3), used by RLM for
matching reviews with objects, has a striking re-
semblance to the TFIDF
+
scoring function. Both
have the form
e
?
= argmax
e
?
w?r
e
log f(w),
where for RLM,
f(w) = f
R
(w) = 1 +
?
1 ? ?
P
e
(w)
P (w)
,
and for TFIDF
+
,
f(w) = f
B
(w) =
1
Q(w)
.
In both cases, f(w) is monotonically decreas-
ing in the frequency of w in the corpus. How-
ever, there are several differences between the two
cases. We highlight some of them here, with the
aim of illustrating the power of our review lan-
guage model (RLM).
Object length normalization. First note that the
P
e
(w) term in f
R
(w) acts as an object length nor-
malizing term, i.e., it adds up to one for each
e and weighs down P (w) for objects with long
text(e). This also has the effect of penalizing re-
views that are missing critical words in the object
description. In contrast, f
B
(w) is unnormalized
with respect to the object length. The standard
document normalization techniques in IR do not
apply well to our setting since our ?documents?
(i.e., object descriptions) are short. E.g., if the ob-
ject description contains only one token, the stan-
dard cosine-normalization technique (Salton et al,
613
1975) will yield a normalized score of 1 irre-
spective of the token. Thus for a review contain-
ing the words ?Food? and ?Casablanca?, the stan-
dard normalization will yield the same score for a
restaurant named ?Food? and a restaurant named
?Casablanca?, ignoring the fact that ?Food? is
much more likely to be an object-independent
term. Note that this only becomes a problem when
the entire ?document? is part of the match, which
rarely happens in an IR setting where the docu-
ments are typically much longer than the queries.
Indeed, in our experiments, we observe lower per-
formance when we apply cosine-normalization to
the tf-idf scores. On the other hand, in f
R
(w), the
P (w) term can still distinguish the two aforemen-
tioned objects even when P
e
(w) are equal.
Dampening. With ? < 1, f
R
(w) is effectively
a dampened version of
P
e
(w)
P (w)
. In other words, dif-
ferences between very frequent words and very in-
frequent words are somewhat smoothed out. In-
deed, if we modify TFIDF
+
by introducing a sim-
ilar dampening factor into f
B
(w), we observe im-
provement in its performance (Section 5.4).
Removingmentions of an object. Another differ-
ence is that in RLM, P (w) is estimated on reviews
with object mentions removed, since the model in-
dicate that P (w) accounts for object-independent
review language. In contrast, TFIDF
+
computes
Q(w) on full reviews. We illustrate the differ-
ence on the following example. Consider a review
that reads ?. . .Maggiano?s has great Fondue.? If
?Maggiano?s? and ?Fondue? both occur the same
number of times in the corpus, then they get the
same idf (i.e., Q(w)) score. In RLM, however,
?Maggiano?s? will get much smaller probability
in the generic review distribution P (?) than ?Fon-
due?, since ?Maggiano?s? almost always occurs in
reviews as restaurant name mentions, thus is re-
moved from the estimation of its P (?) probabil-
ity. On the other hand, the word ?Fondue? is more
likely to retain higher probability in P (?) since it
tends to appear as dish names. As a result, our
model will assign higher weight to ?Maggiano?s
Restaurant? than ?Fondue Restaurant?. As we can
see, RLM evaluates the ability of a word to identify
the review object rather than rely on the absolute
rarity of the word, which is done by tf-idf.
Using term counts. One last difference is that
f
R
(w) uses term counts of words rather than the
standard document counts used by f
B
(w). Our
evaluation suggests that at least in practice, this
does not have a big impact on the overall accuracy.
In the experiments we show that these factors
together account for the performance difference
between RLM and tf-idf. Our model gives a prin-
cipled way to introduce these factors, however.
4 Data
In this section we describe the dataset constructed
for the task of matching restaurant reviews to the
corresponding restaurant objects. Our goal is to
obtain a large collection of reviews on which to
estimate the generic language model, with a sig-
nificant portion of them aligned with the objects
for which the reviews were written; this portion
will serve as the gold-standard test set.
To this end, we obtained a set of reviews from
the Yelp website, yelp.com. This website con-
tains a collection of reviews about various busi-
nesses and for each business, has a webpage con-
taining the business information and a list of re-
views. We crawled all restaurant pages from Yelp.
For each restaurant, we extracted its name and
city location from the business information sec-
tion via HTML cues, and a list of no more than
40 reviews. We obtained the textual content of
299,762 reviews, each aligned with one of a set
of 12,408 unique restaurants hosted on Yelp. Note
that while our technique is not targeted for head
sites like Yelp (where wrapper induction might
be a more accurate approach), this provides a
large-scale dataset, conveniently labeled with ob-
ject information, and simulates the tail-site sce-
nario where we rely heavily on the textual content
of reviews to identify objects.
Many of the reviews in Yelp do not contain any
identifying information. In fact, some of them are
as short as ?Great place. Awesome food!!?. We
processed the dataset to retain only reviews that
mention the name of the restaurant, even if par-
tially, and, when the restaurant name is a common
word, also the city of the restaurant. Each of the
remaining reviews is expected to have enough in-
formation for a human to identify the restaurant
corresponding to the review.
To further increase the difficulty of the match-
ing task, we obtained a much more extensive list
of restaurant objects in the Yahoo! Local database,
which contains 681,320 restaurants. Our task
is to match a given Yelp review, using only its
free-form textual content, with its corresponding
614
restaurant in the Yahoo! Local database. We then
proceeded to generate the gold standard that con-
tains the correct restaurant in the Yahoo! Local
database for each review. We employed geocoding
to match addresses across the two databases along
with approximate name matches. Note that in the
final dataset, only half of the restaurants have the
exact same name listed in both Yelp and Yahoo!
Local; this limits the success of naive dictionary-
based methods.
The final aligned dataset contained 24,910 Yelp
reviews (R), covering 6,010 restaurants. We set
aside half of the reviews (R
?
) to estimate the mod-
els and the other half (R
test
) to evaluate our tech-
nique. We also set aside 1,000 reviews as devel-
opment set, on which we conducted initial exper-
iments. The total size of the test corpus, R
test
was 11,217. The splitting of R into R
?
, R
test
,
and the development set was done in such a way
that there are no overlapping restaurants between
them. Also, the reviews that were filtered out
because of lack of identifying information were
added back to R
?
for learning the review language
model, expandingR
?
to a total of 205,447 reviews.
5 Evaluation
In this section we evaluate our proposed review-
language based matching algorithm RLM.
5.1 Experimental considerations
Baseline system. We use the TFIDF and TFIDF
+
algorithms described in Section 3.4 as baseline
algorithms. Since we are comparing objects
that can have varying lengths, we tried the stan-
dard cosine-normalization techniques for docu-
ment length normalization. For reasons described
in Section 3.4, however, the normalization signif-
icantly lowered the accuracy. All the numbers re-
ported here are using tf-idf scores without normal-
ization.
Efficiency. For both RLM and the baseline algo-
rithms, it is impractical to compute the similar-
ity of a review with each object in the database.
Since all objects that do not intersect with the re-
view have a zero score, we built an inverted in-
dex to retrieve all objects containing a given word.
Even a simple inverted index can be very ineffi-
cient since for each review, words such as ?Restau-
rant? or ?Cafe? retrieve a substantial fraction of
the whole database. Hence, we further optimized
the index by looking at the document frequencies
of the words and considering word bigrams in ob-
ject descriptions. The index only retrieves ob-
jects that have a non-trivial overlap with the re-
view; e.g., an overlap of ?Casablanca? is consid-
ered non-trivial while an overlap of ?Restaurant?
is considered trivial. Once these candidates are re-
trieved, our scoring function takes into account all
overlapping tokens.
For the YELP dataset, the index returns an av-
erage of 200 restaurants for each review. This
points to the general difficulty of review match-
ing over a large corpus of objects, since a simple
dictionary-based named-entity recognition will hit
at least 200 objects for many reviews.
Experiment settings. For RLM, we conducted
initial experiments and performed parameter esti-
mation on the development data. The experimen-
tal settings we used for RLM are as follows: we
set g(w) = log(1/f
w
) for P
e
, where f
w
is esti-
mated on the review collection. P (w) is estimated
on all reviews in R
?
, where for each review, all to-
kens of its corresponding text(e), if present, are
removed, in order to approximate the generic re-
view language independent of e, as required by
our generative model. We estimate ? to be 0.002,
tuned on the development set; in our experiments,
we observe that the performance is not very sensi-
tive to ?.
5.2 Main results
In this section we present the main comparisons
between RLM and the baseline in details.
Performance measure. Our task resembles a
standard IR task in that our algorithm ranks can-
didate objects for a given review by their ?about-
ness? level. Unlike a standard IR task, however,
we are not interested in retrieving multiple ?rel-
evant? objects, as each review in our dataset has
only one single correct match from E . A review
match is correct if the top-1 prediction (i.e., e
?
) is
accurate. In what follows, we report the average
accuracy for various experimental settings. Note
that we can take the average accuracy over all re-
views (reported as micro-average), regardless of
which restaurants they are about; or we can first
compute the average for reviews about the same
restaurant, and report the average over all restau-
rants (macro-average). When not specified, we re-
port the micro-average.
Main comparisons. Table 1(a) summarizes the
main comparison. Our proposed algorithm RLM
615
Method Micro-avg. Macro-avg.
RLM 0.647 0.576
TFIDF
+
0.518 0.481
TFIDF 0.314 0.317
(a) Main comparison.
Method Micro-avg. Macro-avg.
RLM-UNIFORM 0.634 0.562
RLM-UNCUT 0.627 0.546
RLM-DECAP 0.640 0.573
(b) RLM variants.
Method Micro-avg. Macro-avg.
TFIDF
+
-N 0.586 0.523
TFIDF
+
-D 0.593 0.533
TFIDF
+
-O 0.522 0.488
TFIDF
+
-ND 0.628 0.549
TFIDF
+
-NDO 0.647 0.576
(c) TFIDF
+
variants.
Table 1: Average accuracy of the top-1 prediction
for various techniques. Micro-average computed
over 11,217 reviews inR
test
; macro-average com-
puted over 2,810 unique restaurants in R
test
.
clearly outperforms the TFIDF
+
baseline mea-
sured by either micro- or macro-average accuracy.
The standard TFIDF, as predicted, performs the
worst.
Some reviews can be particularly difficult to
match, which can be reflected in a low matching
score. Nonetheless, we predict the most likely ob-
ject. Suppose we impose a threshold and return
the most likely object only when its score is above
threshold, we can then compute precision and re-
call at different thresholds. Figure 1 presents the
precision?recall curve (using micro-average) for
both RLM and TFIDF
+
. Again, RLM clearly out-
performs TFIDF
+
across the board.
We then generalize the definition of accuracy
into accuracy@k: a review is considered as cor-
rectly matched if one of the top-k objects returned
is the correct match. We plot accuracy@k as a
function of k. While the gap between RLM and
TFIDF
+
is smaller as k increases, RLM clearly
outperforms TFIDF
+
for all k ? {1, . . . , 10}.
One final comparison is accuracy@1 as a func-
tion of the review length. Given our current set-
ting, longer reviews might be more difficult to
match since they may include more proper nouns
such as dish names and related restaurants, and
Figure 1: Precision?recall curve (of top one pre-
diction): RLM vs. TFIDF
+
baseline.
Figure 2: Accuracy@k (percentage of reviews
whose correct match is returned in one of its top-k
predictions): RLM vs. TFIDF
+
baseline.
Figure 3: Average accuracy of the top-1 prediction
for reviews with different length (on test set): RLM
vs. TFIDF
+
baseline.
616
yield a longer list of highly competitive candi-
date objects. Interestingly, the gap between RLM
and TFIDF
+
is much smaller for shorter reviews.
As reviews get longer, the performance of RLM
is relatively stable, whereas the performance of
TFIDF
+
drops down significantly.
5.3 Experimental choices for RLM
We now examine the experimental choices we
made for different components of RLM by defin-
ing the following variations of RLM.
RLM-UNIFORM: rather than setting g(w) =
log(1/f
w
) for P
e
, we use the uniform distribution
P
e
(w) = 1/|text(e)|. From the third line of Table
1 (b), there is a slight accuracy drop of ? 1.3%.
RLM-UNCUT: suppose we only have access to
a review corpus with no alignment to text(e), and
thus have to approximate P (w) by estimating it
on the set of original ?un-cut? reviews, how much
does that affect our performance? As indicated in
the fourth row of Table 1 (b), this reduces accuracy
by about 2% on our test data.
RLM-DECAP: as an alternative way to deal with
lack of aligned data, we consider a variation of
the above algorithm by removing all the capital-
ized words from un-annotated reviews. Clearly,
this can result in both ?over-cutting? and ?under-
cutting? of true restaurant name mentions. How-
ever, as indicated in the fourth row of Table 1 (b),
this is very close to the best accuracy achieved.
Thus, an effective model can be learned even with-
out aligned data.
5.4 Revisiting TFIDF
+
: what?s amiss?
In this section we revisit the main differences be-
tween our model and the TFIDF
+
outlined in Sec-
tion 3.4, and investigate their empirical impor-
tance by introducing these features into TFIDF
+
and examine their effectiveness in that framework.
Object length normalization. We con-
sider a modified TFIDF
+
measure f
M
(w) =
P
e
(w)/Q(w), which we call TFIDF
+
-N (normal-
ized). As shown in Table 1 (c), this change alone
can increase the average accuracy by nearly 7%.
Dampening. We consider a modified TFIDF
+
measure f
M
(w) = 1 + ? ?
N
df(w)
, which we call
TFIDF
+
-D. Table 1 (c) reports the performance of
using this measure, with ? = 0.1 (set on develop-
ment data). Again, this measure alone can induce
over 7% increase in accuracy. Indeed, combin-
ing normalization and dampening, (i.e., f
M
(w) =
1+? ?P
e
(w) ?
N
df(w)
), denoted as TFIDF
+
-ND, we
get comparable performance to RLM-UNCUT.
Removing mentions of objects. Again, we can
incorporate this in a heuristic way in TFIDF
+
,
which we denote by TFIDF
+
-O. Interestingly,
while using the original f
B
(w) function with
df(w) computed on the object-removed review
collection does not yield a big improvement, this
does bring the performance of the fully modified
TFIDF
+
to the same level of the standard RLM
(see line marked TFIDF
+
-NDO.)
Using term counts. Our investigation suggests
that at least in practice, using Q(w) vs. P (w) is
not a critical decision, as a fully modified TFIDF
+
can achieve the same performance using df(w) to
quantify frequency of the word. Our experiments
on this dataset show that each of the other model-
ing decisions incorporated in RLM is important.
6 Conclusions
We proposed a generative model for reviews
where reviews are generated from the mixture of
a distribution involving object terms and a generic
review language model. The model provides us
a principled way to match reviews to objects.
Our evaluation on a real-world dataset shows that
our techniques vastly outperforms standard tf-idf
based techniques.
Acknowledgments
We thank Don Metzler for many discussions and
the anonymous reviewers for their comments.
References
R. Ananthakrishna, S. Chaudhuri, and V. Ganti. 2002.
Eliminating fuzzy duplicates in data warehouses. In
Proc. 28th VLDB, pages 586?596.
L. Barbosa, R. Kumar, B. Pang, and A. Tomkins. 2009.
For a few dollars less: Identifying review pages sans
human labels. In Proc. NAACL.
I. Bhattacharya and L. Getoor. 2007. Collective entity
resolution in relational data. ACM TKDD, 1(1).
C. Cardie. 1997. Empirical methods in information
extraction. AI Magazine, 18(4):65?80.
V. T. Chakaravarthy, H. Gupta, P. Roy, and M. Mo-
hania. 2006. Efficiently linking text documents
with relevant structured information. In Proc. 32nd
VLDB, pages 667?678.
617
N. Dalvi, R. Kumar, B. Pang, and A. Tomkins. 2009.
A translation model for matching reviews to objects.
Manuscript.
I. P. Fellegi and A. B. Sunter. 1969. A theory for record
linkage. JASIS, 64:1183?1210.
D. Hiemstra and W. Kraaij. 1998. Twenty-one at
TREC7: Ad-hoc and cross-language track. In Proc.
7th TREC, pages 174?185.
D. Hiemstra. 1998. A linguistically motivated prob-
abilistic model of information retrieval. In Proc.
ECDL, pages 569?584.
M. Hu and B. Liu. 2004. Mining opinion features in
customer reviews. In Proc. AAAI, pages 755?760.
D. V. Kalashnikov, S. Mehrotra, and Z. Chen. 2005.
Exploiting relationships for domain-independent
data cleaning. In Proc. 5th SDM.
N. Kobayashi, K. Inui, Y. Matsumoto, K. Tateishi, and
T. Fukushima. 2004. Collecting evaluative expres-
sions for opinion extraction. In Proc. 1st IJCNLP,
pages 596?605.
J. Lafferty and C. Zhai. 2003. Probabilistic relevance
models based on document and query generation. In
W. B. Croft and J. Lafferty, editors, Language Mod-
eling and Information Retrieval. Academic Publish-
ers.
A. McCallum and B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proc. 17th NIPS.
H. B. Newcombe, J. M. Kennedy, S. J. Axford, and
A. P. James. 1959. Automatic linkage of vital
records. Science, 130:954?959.
V. Ng, S. Dasgupta, and S. M. Niaz Arifin. 2006. Ex-
amining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proc. 21st COLING/44th ACL, pages 611?
618.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
J. M. Ponte and W. B. Croft. 1998. A language model-
ing approach to information retrieval. In Proc. 21st
SIGIR, pages 275?281.
A.-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc.
HLT/EMNLP.
G. Salton, A. Wong, and C. S. Yang. 1975. A vec-
tor space model for automatic indexing. Commun.
ACM, 18(11):613?620.
S. Sarawagi. 2008. Information extraction. Founda-
tions and Trends in Databases, 1(3):261?377.
F. Song and W. B. Croft. 1999. A general language
model for information retrieval. In Proc. 22nd SI-
GIR, pages 279?280.
V. Stoyanov and C. Cardie. 2008. Topic identification
for fine-grained opinion analysis. In Proc. COLING.
J. Yi, T. Nasukawa, R. Bunescu, and W. Niblack. 2003.
Sentiment analyzer: Extrating sentiments about a
given topic. In Proc. 3rd ICDM, pages 427?434.
C. Zhai and J. Lafferty. 2004. A study of smoothing
methods for language models applied to information
retrieval. ACM TOIS, 22(2):179?214.
C. Zhai. 2008. Statistical language models for infor-
mation retrieval a critical review. Foundations and
Trends in Information Retrieval, 2(3):137?213.
618
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 494?502,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
For a few dollars less: Identifying review pages sans human labels
Luciano Barbosa
Dept. of Computer Science
University of Utah
Salt Lake City, UT 84112, USA.
lbarbosa@cs.utah.edu
Ravi Kumar Bo Pang Andrew Tomkins
Yahoo! Research
701 First Ave
Sunnyvale, CA 94089, USA.
{ravikumar,bopang,atomkins}@yahoo-inc.com
Abstract
We address the problem of large-scale auto-
matic detection of online reviews without us-
ing any human labels. We propose an efficient
method that combines two basic ideas: Build-
ing a classifier from a large number of noisy
examples and using the structure of the web-
site to enhance the performance of this classi-
fier. Experiments suggest that our method is
competitive against supervised learning meth-
ods that mandate expensive human effort.
1 Introduction
Shoppers are migrating to the web and online re-
views are playing a critical role in affecting their
shopping decisions, online and offline. According
to two surveys published by comScore (2007) and
Horrigan (2008), 81% of web users have done on-
line research on a product at least once. Among
readers of online reviews, more than 70% reported
that the reviews had a significant influence on their
purchases. Realizing this economic potential, search
engines have been scrambling to cater to such user
needs in innovative ways. For example, in response
to a product-related query, a search engine might
want to surface only review pages, perhaps via a ?fil-
ter by? option, to the user. More ambitiously, they
might want to dissect the reviews, segregate them
into novice and expert judgments, distill sentiments,
and present an aggregated ?wisdom of the crowds?
opinion to the user. Identifying review pages is the
indispensable enabler to fulfill any such ambition;
nonetheless, this problem does not seem to have
been addressed at web scale before.
Detecting review webpages in a few, review-only
websites is an easy, manually-doable task. A large
fraction of the interesting review content, however,
is present on pages outside such websites. This is
where the task becomes challenging. Review pages
might constitute a minority and can be buried in
a multitude of ways among non-review pages ?
for instance, the movie review pages in nytimes.
com, which are scattered among all news articles, or
the product review pages in amazon.com, which
are accessible from the product description page. An
automatic and scalable method to identify reviews
is thus a practical necessity for the next-generation
search engines. The problem is actually more gen-
eral than detecting reviews: it applies to detecting
any ?horizontal? category such as buying guides, fo-
rums, discussion boards, FAQs, etc.
Given the nature of these problems, it is tempt-
ing to use supervised classification. A formidable
barrier is the labeling task itself since human la-
bels need time and money. On the other hand, it
is easier to generate an enormous number of low-
quality labeled examples through purely automatic
methods. This prompts the question: Can we do re-
view detection by focusing just on the textual con-
tent of a large number of automatically obtained but
low-quality labeled examples, perhaps also utilizing
the site structure specific to each website? And how
will it compare to the best supervised classification
method? We address these questions in this paper.
Main contributions. We propose the first end-to-
end method that can operate at web scale to effi-
ciently detect review pages. Our method is based
on using simple URL-based clues to automatically
494
partition a large collection of webpages into two
noisy classes: One that consists mostly of review
webpages and another that consists of a mixture
of some review but predominantly non-review web-
pages (more details in Section 4.2).
We analyze the use of a naive Bayes classifier in
this noisy setting and present a simple algorithm for
review page classification. We further enhance the
performance of this classifier by incorporating infor-
mation about the structure of the website that is man-
ifested through the URLs of the webpages. We do
this by partitioning the website into clusters of web-
pages, where the clustering delicately balances the
information in the site-unaware labels provided by
the classifier in the previous step and the site struc-
ture encoded in the URL tokens; a decision tree is
used to accomplish this. Our classification method
for noisily-labeled examples and the use of site-
specific cues to improve upon a site-independent
classifier are general techniques that may be appli-
cable in other large-scale web analyses.
Experiments on 2000 hand-labeled webpages
from 40 websites of varying sizes show that besides
being computationally efficient, our human-label-
free method not only outperforms those based on
off-the-shelf subjectivity detection but also remains
competitive against the state-of-the-art supervised
text classification that relies on editorial labels.
2 Related work
The related work falls into roughly four categories:
Document- and sentence-level subjectivity detec-
tion, sentiment analysis in the context of reviews,
learning from noisy labeled examples, and exploit-
ing site structure for classification.
Given the subjective nature of reviews, document-
level subjectivity classification is closely related to
our work. There have been a number of approaches
proposed to address document-level subjectivity in
news articles, weblogs, etc. (Yu and Hatzivas-
siloglou, 2003; Wiebe et al, 2004; Finn and Kush-
merick, 2006; Ni et al, 2007; Stepinski and Mit-
tal, 2007). Ng et al (2006) experiment with review
identification for known domains using datasets with
clean labels (e.g., movie reviews vs. movie-related
non-reviews), a setting different from that of ours.
Pang and Lee (2008b) present a method on re-
ranking documents that are web search results for a
specific query (containing the word review) based
on the subjective/objective distinction. Given the na-
ture of the query, they implicitly detect reviews from
unknown sources. But their re-ranking algorithm
only applies to webpages known to be (roughly) re-
lated to the same narrow subject. Since the web-
pages in our datasets cover not only a diverse range
of websites but also a diverse range of topics, their
approach does not apply. To the best of our knowl-
edge, there has been no work on identifying review
pages at the scale and diversity we consider.
Subjectivity classification of within-document
items, such as terms, has been an active line of re-
search (Wiebe et al (2004) present a survey). Iden-
tifying subjective sentences in a document via off-
the-shelf packages is an alternative way of detect-
ing reviews without (additional) human annotations.
In particular, the OpinionFinder system (Riloff and
Wiebe, 2003; Wiebe and Riloff, 2005) is a state-of-
the-art knowledge-rich sentiment-analysis system.
We will use it as one of our baselines and compare
its performance with our methods.
There has been a great deal of previous work in
sentiment analysis that worked with reviews, but
they were typically restricted to using reviews ex-
tracted from one or two well-known sources, by-
passing automatic review detection. Examples of
such early work include (Turney, 2002; Pang et al,
2002; Dave et al, 2003; Hu and Liu, 2004; Popescu
and Etzioni, 2005). See Pang and Lee (2008a) for
a more comprehensive survey. Building a collection
of diverse review webpages, not limited to one or
two hosts, can better facilitate such research.
Learning from noisy examples has been studied
for a long time in the learning theory community
(Angluin and Laird, 1988). Learning naive Bayes
classifiers from noisy data (either features or labels
or both) was studied by Yang et al (2003). Their
focus, however, is to reconstruct the underlying con-
ditional probability distributions from the observed
noisy dataset. We, on the other hand, rely on the vol-
ume of labels to drown the noise. Along this spirit,
Snow et al (2008) show that obtaining multiple low-
quality labels (through Mechanical Turk) can ap-
proach high-quality editorial labels. Unlike in their
setting, we do not have multiple low-quality labels
for the same URL. The extensive body of work in
495
semi-supervised learning or learning from one class
is also somewhat relevant to our work. A major dif-
ference is that they tend to work with small amount
of clean, labeled data. In addition, many semi-
supervised/transductive learning algorithms are not
efficient for web-scale data.
Using site structure for web analysis tasks has
been addressed in a variety of contexts. For ex-
ample, Kening et al (2005) exploit the structure
of a website to improve classification. On a re-
lated note, co-training has also been used to utilize
inter-page link information in addition to intra-page
textual content: Blum and Mitchell (1998) use an-
chor texts pointing to a webpage as the alternative
?view? of the page in the context of webpage clas-
sification. Their algorithm is largely site-unaware
in that it does not explicitly exploit site structures.
Utilizing site structures also has remote connections
to wrapper induction, and there is extensive litera-
ture on this topic. Unfortunately, the methods in all
of these work require human labeling, which is pre-
cisely what our work is trying to circumvent.
3 Methodology
In this section we describe our basic methodology
for identifying review pages. Our method consists
of two main steps. The first is to use a large amount
of noisy training examples to learn a basic classifier
for review webpages; we adapt a simple naive Bayes
classifier for this purpose. The second is to improve
the performance of this basic classifier by exploiting
the website structure; we use a decision tree for this.
Let P be the set of all webpages. Let C+ denote
the positive class, i.e., the set of all review pages and
let C? denote the negative class, i.e., the set of all
non-review pages. Each webpage p is exactly in one
of C+ or C?, and is labeled +1 or ?1 respectively.
3.1 Learning from large amounts of noisy data
Previous work using supervised or semi-supervised
learning approaches for sentiment analysis assumes
relatively high-quality labels that are produced ei-
ther via human annotation or automatically gener-
ated through highly accurate rules (e.g., assigning
positive or negative label to a review according to
automatically extracted star ratings).
We examine a different scenario where we can au-
tomatically generate large amount of relatively low-
quality labels. Section 4.2 describes the process
in more detail, but briefly, in a collection of pages
crawled from sites that are very likely to host re-
views, those with the word review in their URLs
are very likely to contain reviews (the noisy posi-
tive set C?+) and the rest of the pages on those sites
are less likely to contain reviews (the more noisy
negative set C??). More formally, for a webpage
p, suppose Pr[p ? C+ | p ? C?+] = ? and
Pr[p ? C+ | p ? C??] = ?, where 1 > ?  ? > 0.
Can we still learn something useful from C?+ and C??
despite the labels being highly noisy?
The following analysis is based on a naive Bayes
classifier. We chose naive Bayes classifier since the
learning phase can easily be parallelized.
Given a webpage (or a document) p represented
as a bag of features {fi}, we wish to assign a class
argmaxc?{C+,C?} Pr[c | p] to this webpage. Naive
Bayes classifiers assume fi?s to be conditionally in-
dependent and we have Pr[p | c] = ?Pr[fi | c].
Let ri = Pr[fi | C+]/Pr[fi | C?] denote the con-
tribution of each feature towards classification, and
rc = Pr[C+]/Pr[C?] denote the ratio of class pri-
ors. First note that
log Pr[C+|p]Pr[C?|p] = log
(
Pr[C+]
Pr[C?] ?
Pr[p|C+]
Pr[p|C?]
)
= log
(
Pr[C+]
Pr[C?] ?
?
ri
)
= log rc +? log ri.
A webpage p receives label +1 iff Pr[C+ | p] >
Pr[C? | p], and by above, if and only if ? log ri >
? log rc.
When we do not have a reasonable estimate of
Pr[C+] and Pr[C?], as in our setting, the best we
can do is to assume rc = 1. In this case, p receives
label +1 if and only if? log ri > 0. Thus, a feature
fi with log ri > 0 has a positive contribution to-
wards p being labeled +1; call fi to be a ?positive?
feature. Typically we use relative-frequency estima-
tion of Pr[c] and Pr[fi | c] for c ? {C+, C?}. Now,
how does the estimation from a dataset with noisy
labels compare with the estimation from a dataset
with clean labels?
To examine this, we calculate the following:
Pr[fi | C?+] = ?Pr[fi | C+] + (1? ?) Pr[fi | C?],
Pr[fi | C??] = ? Pr[fi | C+] + (1? ?) Pr[fi | C?].
Let r?i = Pr[fi| eC+]Pr[fi| eC?] =
?ri+(1??)
?ri+(1??) . Clearly r?i is mono-
tonic but not linear in ri. Furthermore, it is bounded:
496
(1? ?)/(1? ?) ? r?i ? ?/?. However,
r?i > 1 ?? ?ri + (1? ?) > ?ri + (1? ?)
?? (???)ri > (???) ?? ri > 1,where
the last step used ? > ?. Thus, the sign of log r?i is
the same as that of log ri, i.e., a feature contribut-
ing positively to? log ri will continue to contribute
positively to? log r?i (although its magnitude is dis-
torted) and vice versa.
The above analysis motivates an alternative model
to naive Bayes. Instead of each feature fi placing
a weighted vote log r?i in the final decision, we trust
only the sign of log r?i, and let each feature fi place a
vote for the class C+ (respectively, C?) if log r?i > 0
(respectively, log r?i < 0). Intuitively, this model
just compares the number of ?positive? features and
the number of ?negative? features, ignoring the mag-
nitude (since it is distorted anyway). This is pre-
cisely our algorithm: For a given threshold ?, the
final label nbu?(p) of a webpage p is given by
nbu?(p) = sgn (? sgn(log r?i)? ?) ,
where sgn is the sign function. For comparison
purposes, we also indicate the ?weighted? version:
nbw?(p) = sgn (? log r?i ? ?) .
If ? = 0, we omit ? and use nb to denote a generic
label assigned by any of the above algorithms.
Note that even though our discussions were for
two-class and in particular, review classification,
they are equally applicable to a wide range of clas-
sification tasks in large-scale web-content analysis.
Our analysis of learning from automatically gener-
ated noisy examples is thus of independent interest.
3.2 Utilizing site structure
Can the structure of a website be exploited to im-
prove the classification of webpages given by nb(?)?
While not all websites are well-organized, quite a
number of them exhibit certain structure that makes
it possible to identify large subsites that contain only
review pages. Typically but not always this structure
is manifested through the tokens in the URL corre-
sponding to the webpage. For instance, the pattern
http://www.zagat.com/verticals/
PropertyDetails.aspx?VID=a&R=b,
where a,b are numbers, is indicative of all
webpages in zagat.com that are reviews of
restaurants. In fact, we can think of this as a
generalization of having the keyword review in
the URL. Now, suppose we have an initial labeling
nb(p) ? {?1} for each webpage p produced by a
classifier (as in the previous section, or one that is
trained on a small set of human annotated pages),
can we further improve the labeling using the
pattern in the URL structure?
It is not immediate how to best use the URL
structure to identify the review subsites. First,
URLs contain irrelevant information (e.g., the to-
ken verticals in the above example), thus clus-
tering by simple cosine similarity may not dis-
cover the review subsites. Second, the subsite
may not correspond to a subtree in the URL hi-
erarchy, i.e., it is not reasonable to expect all
the review URLs to share a common prefix.
Third, the URLs contain a mixture of path com-
ponents (e.g., www.zagat.com/verticals/
PropertyDetails.aspx) and key-value pairs
(e.g., VID=a and R=b) and hence each token (re-
gardless of its position) in the URL could play a
role in determining the review subsite. Furthermore,
conjunction of presence/absence of certain tokens in
the URL may best correspond to subsite member-
ship. In light of these, we represent each URL (and
hence the corresponding webpage) by a bag {gi} of
tokens obtained from the URL. We perform a crude
form of feature selection by dropping tokens that
are either ubiquitous (occurring in more than 99%
of URLs) or infrequent (occurring in fewer than 1%
of URLs) in a website; neither yields useful infor-
mation.
Our overall approach will be to use gi?s to par-
tition P into clusters {Ci} of webpages such that
each cluster Ci is predominantly labeled as either
review or non-review by nb(?). This automati-
cally yields a new label cls(p) for each page p,
which is the majority label of the cluster of p:
cls(p) = sgn
(?
q?C(p) nb(q)
)
,
where C(p) is the cluster of p. To this end, we use
a decision tree classifier to build the clusters. This
classifier will use the features {gi} and the target la-
bels nb(?). The classifier is trained on all the web-
pages in the website and in the obtained decision
tree, each leaf, consisting of pages with the same
set of feature values leading down the path, corre-
sponds to a cluster of webpages. Note that the clus-
ters delicately balance the information in the site-
unaware labels nb(?) and the site structure encoded
497
in the URLs (given by gi?s). Thus the label cls(p)
can be thought of as a smoothed version of nb(p).
Even though we can expect most clusters to be ho-
mogeneous (i.e., pure reviews or non-reviews), the
above method can produce clusters that are inher-
ently heterogeneous. This can happen if the web-
site URLs are organized such that many subsites
contain both review and non-review webpages. To
take this into account, we propose the following
hybrid approach that interpolates between the un-
smoothed labels given by nb(?) and the smoothed
labels given by cls(?). For a cluster Ci, the dis-
crepancy disc(Ci) = ?p?Ci [cls(p) 6= nb(p)]; thisquantity measures the number of disagreements be-
tween the majority label cls(p) and the original label
nb(p) for each page p in the cluster. The decision
tree guarantees disc(Ci) ? |Ci|/2. We call a cluster
Ci to be ?-homogeneous if disc(Ci) ? ?|Ci|, where
? ? [0, 1/2]. For a fixed ?, the hybrid label of a web-
page p is given by
hyb?(p) =
{ cls(p) if C(p) is ?-homogeneous,
nb(p) otherwise.
Note that hyb1/2(p) = cls(p) and hyb0(p) = nb(p).
Note that in the above discussions, any clustering
method that can incorporate the site-unaware labels
nb(?) and the site-specific tokens in gi?s could have
been used; off-the-shelf decision tree was merely a
specific way to realize this.
4 Data
It is crucial for this study to create a dataset that
is representative of a diverse range of websites that
host reviews over different topics in different styles.
We are not aware of any extensive index of online
review websites and we do not want to restrict our
study to a few well-known review aggregation web-
sites (such as yelp.com or zagat.com) since
this will not represent the less popular and more spe-
cialized ones. Instead, we utilized user-generated
tags for webpages, available on social bookmarking
websites such as del.icio.us.
We obtained (a sample of) a snapshot of URL?tag
pairs from del.icio.us. We took the top one
thousand sites with review* tags; these websites
hopefully represent a broad coverage. We were able
to crawl over nine hundred of these sites and the re-
sulting collection of webpages served as the basis
of the experiments in this paper. We refer to these
websites (or the webpages from these sites, when it
is clear from the context) as Sall.
4.1 Gold-standard test set
When the websites are as diverse as represented in
Sall, there is no perfect automatic way to generate
the ground truth labels. Thus we sampled a number
of pages for human labeling as follows.
First, we set aside 40 sites as the test sites (S40).
In order to represent different types of websites (to
the best we can), we sampled the 40 sites so that S40
covers different size ranges, since large-scale web-
sites and small-scale websites are often quite dif-
ferent in style, topic, and content. We uniformly
sampled 10 sites from each of the four size cate-
gories (roughly, sites with 100?5K, 5K?25K, 25K?
100K, and 100K+ webpages)1. Indeed, S40 (as did
Sall) covered a wide range of topics (e.g., games,
books, restaurants, movies, music, and electronics)
and styles (e.g., dedicated review sites, product sites
that include user reviews, newspapers with movie re-
view sections, religious sites hosting book reviews,
and non-English review sites).
We then sampled 50 pages to be labeled from each
site in S40. Since there are some fairly large sites
that have only a small number of review pages, a
uniform sampling may yield no review webpages
from those sites. To reflect the natural distribu-
tion on a website and to represent pages from both
classes, the webpages were sampled in the follow-
ing way. For each website in S40, 25 pages were
uniformly sampled (representing the natural distri-
bution) and 25 pages were sampled from among
?equivalence classes? based on URLs so that pages
from each major URL pattern were represented.
Here, each webpage in the site is represented by a
URL signature containing the most frequent tokens
that occur in the URLs in that site and all pages with
the same signature form an equivalence class.
For our purposes, a webpage is considered a re-
view if it contains significant amount of textual in-
formation expressing subjective opinions on or per-
sonal experiences with a given product / service.
When in doubt, the guiding principle is whether
1As we do not want to waste human annotation on sites with
no reviews at all, a quick pre-screening process eliminated can-
didate sites that did not seem to host any reviews.
498
a page can be a satisfactory result page for users
searching for reviews. More specifically, the human
annotation labeled each webpage, after thoroughly
examining the content, with one of the following
seven intuitive labels: ?single? (contains exactly one
review), ?multiple? (concatenation of more than one
review), ?no? (clearly not a review page), ?empty?
(looks like a page that could contain reviews but had
none), ?login? (a valid user login needed to look at
the content), ?hub? (a pointer to one or more review
pages), and ?ambiguous? (border-line case, e.g., a
webpage with a one line review). The first two labels
were treated as +1 (i.e., reviews) and the last five la-
bels were treated as ?1 (i.e., non-reviews). Out of
the 2000 pages, we obtained 578 pages labeled +1
and the 1422 pages labeled ?1. On a pilot study us-
ing two human judges, we obtained 78% inter-judge
agreement for the seven labels and 92% inter-judge
agreement if we collapse the labels to ?1. Percent-
ages of reviews in our samples from different sites
range from 14.6% to 93.9%.
Preprocessing for text-based analysis. We pro-
cessed the crawled webpages using lynx to ex-
tract the text content. To discard templated content,
which is an annoying issue in large-scale web pro-
cessing, and HTML artifacts, we used the following
preprocessing. First, the HTML tags <p>, <br>,
</tr>, and </td> were interpreted as paragraph
breaks, the ?.? inside a paragraph was interpreted as
a sentence break, and whitespace was used to tok-
enize words in a sentence. A sentence is considered
?good? if it has at least seven alphabetic words and
a paragraph is considered ?good? if it has at least
two good sentences. After extracting the text us-
ing lynx, only the good paragraphs were retained.
This effectively removes most of the templated con-
tent (e.g., navigational phrases) and retains most of
the ?natural language? texts. Because of this pre-
processing, 485 pages out of 2000 turned out to be
empty and these were discarded (human labels on
97% of these empty pages were ?1).
4.2 Dataset with noisy labels
As discussed in Section 3.1, our goal is to obtain a
large noisy set of positive and negative labeled ex-
amples. We obtained these labels for the webpages
in the training sites, Srest, which is essentially Sall \
S40. First, the URLs in Srest were tokenized using a
unigram model based on an English dictionary; this
is so that strings such as reviewoftheday are
properly interpreted.
C?+: To be labeled +1, the path-component of
the URL of the webpage has to contain the token
review. Our assumption is that such pages are
highly likely to be review pages. On a uniform sam-
ple of 100 such pages in Sall, 90% were found to be
genuine reviews. Thus, we obtained a collection of
webpages with slightly noisy positive labels.
C??: The rest of the pages in Srest were labeled
?1. Clearly this is a noisy negative set since not all
pages containing reviews have review as part of
their URLs (recall the example from zagat.com);
thus many pages in C?? can still be reviews.
While the negative labels in Srest are more noisy
than the positive labels, we believe most of the non-
review pages are in C??, and as most websites con-
tain a significant number of non-review pages, the
percentage of reviews in C?? is smaller than that in
C?+ (the assumption ?  ? in Section 3.1).
We collected all the paragraphs (as defined ear-
lier) from both C?+ and C?? separately. We elim-
inated duplicate paragraphs (this further mitigates
the templates issue, especially for sites generated
by content-management software), and trained a un-
igram language model as in Section 3.1.
5 Evaluations
The evaluations were conducted on the 1515 labeled
(non-empty) pages in S40 described in Section 4.1.
We report the accuracy (acc.) as well as precision
(prec.), recall (rec.), and f-measure (fmeas.) for C+.
Trivial baselines. Out of the 1515 labeled pages,
565 were labeled +1 and 950 were labeled ?1. Ta-
ble 1 summarizes the performance of baselines that
always predict one of the classes and a baseline that
randomly select a class according to the class dis-
tribution S40. As we can see, the best accuracy
is .63, the best f-measure is .54, and they cannot
be achieved by the same baseline. Before present-
acc. prec. rec. fmeas.
always C? .63 - 0 -
always C+ .37 .37 1 .54
random .53 .37 .37 .37
Table 1: Trivial baseline performances.
499
ing the main results of our methods, we introduce
a much stronger baseline that utilizes a knowledge-
rich subjectivity detection package.
5.1 Using subjectivity detectors
This baseline is motivated by the fact that reviews
often contain extensive subjective content. There are
many existing techniques that detect subjectivity in
text. OpinionFinder (http://www.cs.pitt.
edu/mpqa/opinionfinderrelease/) is a
well-known system that processes documents and
automatically identifies subjective sentences in
them. OpinionFinder uses two subjective sentence
classifiers (Riloff and Wiebe, 2003; Wiebe and
Riloff, 2005). The first (denoted opfA) focuses on
yielding the highest accuracy; the second (denoted
opfB) optimizes precision at the expense of recall.
The methods underlying OpinionFinder incorporate
extensive tools from linguistics (including, speech
activity verbs, psychological verbs, FrameNet verbs
and adjectives with frame ?experiencer?, among oth-
ers) and machine learning. In terms of performance,
previous work has shown that OpinionFinder is a
challenging system to improve upon for review re-
trieval (Pang and Lee, 2008b). Computationally,
OpinionFinder is very expensive and hence unattrac-
tive for large-scale webpage analysis (running Opin-
ionFinder on 1515 pages took about five hours).
Therefore, we also propose a light-weight subjectiv-
ity detection mechanism called lwd, which counts
the number of opinion words in each sentence in the
text. The opinion words (5403 of them) were ob-
tained from an existing subjectivity lexicon (http:
//www.cs.pitt.edu/mpqa).
We ran both opfA and opfB on the tokenized text
(running them on raw HTML produced worse re-
sults). Each sentence in the text was labeled subjec-
tive or objective. We experimented with two ways
to label a document using sentence-level subjectiv-
ity labels. We labeled a document +1 if it contained
at least k subjective sentences (denoted as opf?(k),
where k > 0 is the absolute threshold), or at least
f fraction of its sentences were labeled subjective
(denoted as opf?(f), where f ? (0, 1] is the rela-
tive threshold). We conducted exhaustive parameter
search with both opfA and opfB. For instance, the
performances of opfA as a function of the thresh-
olds, both absolute and relative, is shown in Fig-
ure 1. Table 2 summarizes the best performances
of opf?(k) (first two rows) and opf?(f) (next two
rows), in terms of accuracy and f-measure (bold-
faced). Similarly, for lwd, we labeled a document
+1 if at least k sentences have at least ` opin-
ion words (denoted lwd(k, `).) Table 2 once again
shows the best performing parameters for both accu-
racy and f-measure for lwd. Our results indicate that
a simple method such as lwd can come very close to
a sophisticated system such as opf?.
acc. prec. rec. fmeas.
opfA(2) .704 .597 .634 .615
opfB(2) .659 .526 .857 .652
opfA(.17) .652 .529 .614 .568
opfB(.36) .636 .523 .797 .632
lwd(1, 4) .716 .631 .572 .600
lwd(1, 1) .666 .538 .740 .623
Table 2: Best performances of opf? and lwd methods.
Figure 1: Performance of opfA as a function of thresh-
olds: Absolute and relative.
5.2 Main results
As stated earlier, we do not have any prior knowl-
edge about the value of ? and hence have to work
with ? = 0. To investigate the implications of
this assumption, we study the performance of nbu?
and nbw? as a function of ?. The accuracy and f-
measures are plotted in Figure 2. There are three
500
acc. prec. rec. fmeas.
nbu .753 .652 .726 .687
cls .756 .696 .616 .654
hyb1/3 .777 .712 .674 .693
Table 3: Performance of our methods.
conclusions that can be drawn from this study: (i)
The peak values of accuracy and f-measure are com-
parable for both nbu? and nbw? , (ii) at ? = 0, nbu is
much better than nbw, in terms of both accuracy and
f-measure, and (iii) the best performance of nbu? oc-
curs at ? ? 0. Given the difficulty of obtaining ? if
one were to use nbw? , the above conclusions vali-
date our intuition and the algorithm in Section 3.1.
Figure 2: Performance as threshold changes: Comparing
nbu? (marked as (u)) with nbw? (marked as (w)).
Table 3 shows the performance of the site-specific
method outlined in Section 3.2. The clusters
were generated using the unpruned J48 decision
tree in Weka (www.cs.waikato.ac.nz/ml/
weka). In our experiments, we set ? = 1/3 as a
natural choice for the hybrid method. As we see
the performance of nbu is about 7% better than the
best performance using a subjectivity-based method
(in terms of accuracy). The performance of the
smoothed labels (decision tree-based clustering) is
comparable to that of nbu. However, the hybrid
method hyb1/3 yields an additional 3% relative im-
provement over nbu. Paired t-test over the accura-
cies for these 40 sites shows both hyb1/3 and nbu
to be statistically significantly better than the opf?
with best accuracy (with p < 0.05, p < 0.005,
respectively), and hyb1/3 to be statistically signifi-
cantly better than nbu (with p < 0.05).
5.3 Cross-validation on S40
While the main focus of our paper is to study
how to detect reviews without human labels, we
present cross validation results on S40 as a compar-
ison point. The goal of this experiment is to get a
sense of the best possible accuracy and f-measure
numbers using labeled data and the state-of-the-
art method for text classification, namely, SVMs.
In other words, the performance numbers obtained
through SVMs and cross-validation can be thought
of as realistic ?upper bounds? on the performance of
content-based review detection. We used SVMlight
(svmlight.joachims.org) for this purpose.
The cross-validation experiment was conducted
as follows. We split the data by site to simulate the
more realistic setting where pages in the test set do
not necessarily come from a known site. Each fold
consisted of one site from each size category; thus,
36 of the 40 sites in S40 were used for training and
the remainder for testing. Over ten folds, the aver-
age performance was: accuracy .795, precision .759,
recall .658, and f-measure .705.
Thus our methods in Section 3 come reason-
ably close to the ?upper bound? given by SVMs
and human-labeled data. In fact, while the su-
pervised SVMs statistically significantly outperform
nbu, they are statistically indistinguishable from
hyb1/3 via paired t-test over site-level accuracies.
6 Conclusions
In this paper we proposed an automatic method to
perform efficient and large-scale detection of re-
views. Our method is based on two principles:
Building a classifier from a large number of noisy
labeled examples and using the site structure to im-
prove the performance of this classifier. Extensive
experiments suggest that our method is competitive
against supervised learning methods that depend on
expensive human labels. There are several interest-
ing avenues for future research, including improv-
ing the current method for exploiting the site struc-
ture. On a separate note, previous research has ex-
plicitly studied sentiment analysis as an application
of transfer learning (Blitzer et al, 2007). Given the
diverse range of topics present in our dataset, ad-
dressing topic-dependency is also an interesting fu-
ture research direction.
501
References
Dana Angluin and Philip D. Laird. 1988. Learning from
noisy examples. Machine Learning, 2(4):343?370.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of 45th ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of 11th COLT, pages 92?100.
comScore. 2007. Online consumer-generated re-
views have significant impact on offline pur-
chase behavior. Press Release, November.
http://www.comscore.com/press/
release.asp?press=1928.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: Opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of 12th WWW, pages 519?528.
Aidan Finn and Nicholas Kushmerick. 2006. Learn-
ing to classify documents according to genre. JASIST,
7(5):1506?1518.
John A. Horrigan. 2008. Online shopping. Pew Internet
& American Life Project Report.
Minqing Hu and Bing Liu. 2004. Mining opinion fea-
tures in customer reviews. In Proceedings of 19th
AAAI, pages 755?760.
Gao Kening, Yang Leiming, Zhang Bin, Chai Qiaozi, and
Ma Anxiang. 2005. Automatic classification of web
information based on site structure. In Cyberworlds,
pages 552?558.
Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin. 2006.
Examining the role of linguistic knowledge sources in
the automatic identification and classification of re-
views. In Proceedings of 21st COLING/44th ACL
Poster, pages 611?618.
Xiaochuan Ni, Gui-Rong Xue, Xiao Ling, Yong Yu, and
Qiang Yang. 2007. Exploring in the weblog space
by detecting informative and affective articles. In Pro-
ceedings of 16th WWW, pages 281?290.
Bo Pang and Lillian Lee. 2008a. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Bo Pang and Lillian Lee. 2008b. Using very simple
statistics for review search: An exploration. In Pro-
ceedings of 22nd COLING. Poster.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proceedings of EMNLP,
pages 79?86.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of HLT/EMNLP, pages 339?346.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of EMNLP, pages 105?112.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Adam Stepinski and Vibhu Mittal. 2007. A fact/opinion
classifier for news articles. In Proceedings of 30th SI-
GIR, pages 807?808.
Peter Turney. 2002. Thumbs up or thumbs down? Se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of 40th ACL, pages
417?424.
Janyce M. Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unanno-
tated texts. In Proceedings of CICLing, pages 486?
497.
Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce,
Matthew Bell, and Melanie Martin. 2004. Learn-
ing subjective language. Computational Linguistics,
30(3):277?308.
Yirong Yang, Yi Xia, Yun Chi, and Richard R. Muntz.
2003. Learning naive Bayes classifier from noisy data.
Technical Report 56, UCLA.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of EMNLP, pages 129?136.
502
Tutorial Abstracts of ACL-08: HLT, page 1,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Introduction to Computational Advertising
Evgeniy Gabrilovich Vanja Josifovski Bo Pang
Yahoo! Research
701 First Avenue
Sunnyvale, CA 94085, USA
{gabr,vanjaj,bopang}@yahoo-inc.com
1 Introduction
Web advertising is the primary driving force behind
many Web activities, including Internet search as
well as publishing of online content by third-party
providers. Even though the notion of online ad-
vertising barely existed a decade ago, the topic is
so complex that it attracts attention of a variety of
established scientific disciplines, including compu-
tational linguistics, computer science, economics,
psychology, and sociology, to name but a few. Con-
sequently, a new discipline ? Computational Ad-
vertising ? has emerged, which studies the process
of advertising on the Internet from a variety of an-
gles. A successful advertising campaign should be
relevant to the immediate user?s information need as
well as more generally to user?s background and per-
sonalized interest profile, be economically worth-
while to the advertiser and the intermediaries (e.g.,
the search engine), as well as be aesthetically pleas-
ant and not detrimental to user experience.
2 Content Overview
In this tutorial, we focus on one important aspect of
online advertising that is relevant to the ACL and
HLT communities, namely, contextual relevance.
There are two main scenarios for online advertis-
ing, as advertisers might request to display their ads
for a query submitted to a Web search engine, or
for a Web page that the user reads online.The for-
mer scenario is called sponsored search, since ads
are matched to the Web search results, and the lat-
ter ? content match, as ads are matched to a larger
amount of content. It is essential to emphasize that
in both cases the context of user actions is defined
by a body of text, which could be quite short in the
case of sponsored search or fairly long in the case
of content match. Consequently, the ad matching
problem lends itself to many NLP methods, but also
poses numerous challenges and open research prob-
lems in text summarization, natural language gener-
ation, named entity extraction, computer-human in-
teraction, and others.
At first approximation, the process of obtaining
relevant ads can be reduced to conventional infor-
mation retrieval, where we construct a query that
describes the user?s context, and then execute this
query against a large inverted index of ads. We show
how to augment the standard information retrieval
approach using query expansion and text classifica-
tion techniques. First, we demonstrate how to em-
ploy a relevance feedback assumption and use Web
search results produced by the query. We also go
beyond the conventional bag of words indexing, and
construct additional features by classifying both the
input context and the ad descriptions with respect to
a large external taxonomy. A third type of features
is constructed from a lexicon of named entities ob-
tained by analyzing the entire Web as a corpus.
We present a unified approach to Web advertis-
ing, which uses the same underlying infrastructure
to handle both sponsored search and content match
scenarios. The last part of the tutorial will be de-
voted to recent research results as well as open prob-
lems, such as automatically classifying cases when
no ads should be shown, handling geographic names
(and more generally, location awareness), and con-
text modeling for vertical portals.
1
Syntax-based Alignment of Multiple Translations: Extracting Paraphrases
and Generating New Sentences
Bo Pang
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
pabo@cs.cornell.edu
Kevin Knight and Daniel Marcu
Information Sciences Institute
University of Southern California
Marina Del Rey, CA 90292 USA
{knight,marcu}@isi.edu
Abstract
We describe a syntax-based algorithm that au-
tomatically builds Finite State Automata (word
lattices) from semantically equivalent transla-
tion sets. These FSAs are good representa-
tions of paraphrases. They can be used to ex-
tract lexical and syntactic paraphrase pairs and
to generate new, unseen sentences that express
the same meaning as the sentences in the input
sets. Our FSAs can also predict the correctness
of alternative semantic renderings, which may
be used to evaluate the quality of translations.
1 Introduction
In the past, paraphrases have come under the scrutiny
of many research communities. Information retrieval re-
searchers have used paraphrasing techniques for query re-
formulation in order to increase the recall of information
retrieval engines (Sparck Jones and Tait, 1984). Natural
language generation researchers have used paraphrasing
to increase the expressive power of generation systems
(Iordanskaja et al, 1991; Lenke, 1994; Stede, 1999).
And researchers in multi-document text summarization
(Barzilay et al, 1999), information extraction (Shinyama
et al, 2002), and question answering (Lin and Pantel,
2001; Hermjakob et al, 2002) have focused on identi-
fying and exploiting paraphrases in the context of recog-
nizing redundancies, alternative formulations of the same
meaning, and improving the performance of question an-
swering systems.
In previous work (Barzilay and McKeown, 2001; Lin
and Pantel, 2001; Shinyama et al, 2002), paraphrases
are represented as sets or pairs of semantically equiva-
lent words, phrases, and patterns. Although this is ade-
quate in the context of some applications, it is clearly too
weak from a generative perspective. Assume, for exam-
ple, that we know that text pairs (stock market rose, stock
market gained) and (stock market rose, stock prices rose)
have the same meaning. If we memorized only these two
pairs, it would be impossible to infer that, in fact, con-
sistent with our intuition, any of the following sets of
phrases are also semantically equivalent: {stock market
rose, stock market gained, stock prices rose, stock prices
gained } and {stock market, stock prices } in the con-
text of rose or gained; {market rose }, {market gained
}, {prices rose } and {prices gained } in the context of
stock; and so on.
In this paper, we propose solutions for two problems:
the problem of paraphrase representation and the problem
of paraphrase induction. We propose a new, finite-state-
based representation of paraphrases that enables one to
encode compactly large numbers of paraphrases. We also
propose algorithms that automatically derive such repre-
sentations from inputs that are now routinely released in
conjunction with large scale machine translation evalu-
ations (DARPA, 2002): multiple English translations of
many foreign language texts. For instance, when given
as input the 11 semantically equivalent English transla-
tions in Figure 1, our algorithm automatically induces the
FSA in Figure 2, which represents compactly 49 distinct
renderings of the same semantic meaning. Our FSAs
capture both lexical paraphrases, such as {fighting, bat-
tle}, {died, were killed} and structural paraphrases such
as {last week?s fighting, the battle of last week}. The
contexts in which these are correct paraphrases are also
conveniently captured in the representation.
In previous work, Langkilde and Knight (1998) used
word lattices for language generation, but their method
involved hand-crafted rules. Bangalore et al (2001) and
Barzilay and Lee (2002) both applied the technique of
multi-sequence alignment (MSA) to align parallel cor-
pora and produced similar FSAs. For their purposes,
they mainly need to ensure the correctness of consensus
among different translations, so that different constituent
orderings in input sentences do not pose a serious prob-
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 102-109
                                                         Proceedings of HLT-NAACL 2003
1. At least 12 people were killed in the battle last week. 2. At least 12 people lost their lives in last week?s fighting.
3. Last week?s fight took at least 12 lives. 4. The fighting last week killed at least 12.
5. The battle of last week killed at least 12 persons. 6. At least 12 persons died in the fighting last week.
7. At least 12 died in the battle last week. 8. At least 12 people were killed in the fighting last week.
9. During last week?s fighting, at least 12 people died. 10. Last week at least twelve people died in the fighting.
11. Last week?s fighting took the lives of twelve people.
Figure 1: Sample Sentence Group from the Chinese-English DARPA Evaluation Corpus: 11 English translations of
the same Chinese sentence.
 
 
at
 
during
 last
 
the
 
least
 
last
 
week
 
battle
 
fighting
 
 were
 
died
 lost
killed
 
in
 
their
 12
persons
*e*
people
 
the
  
last
 
weekbattle
fighting
 
lives
 
in
 
last
 
week
 
fighting
?s
 
at
 
?s
 
least
 
fighting
fight
 
 
died
 
in
 
peopletwelve
the
 
 
at
 
least
 
died
 
week
 
fighting?s
 
people12
 
killed
took
 
the
 
at
of
 
last
week  
lives
 
least
 
of
 
twelve people
 
12
lives
persons
*e*
Figure 2: FSA produced by our syntax-based alignment algorithm from the input in Figure 1.
  
*e*
 the
 
during
*e*
fighting
 
battle
 last
 
*e*
week
weeks
 
fight
fighting
*e*
 
killed
of took
*e*
 
the
 
at
 
lives
 
least
of
 
twelve
 
12
 
people
persons
*e*
 
lives
died
*e*
 in
 
*e*
the  
*e*
battle
fighting
 
*e*
 
last
weeks
week
 
fighting
*e*
*e*
 
people
 
lost
 
were
their
killed
Figure 3: FSA produced by a Multi-Sequence Alignment algorithm from the input in Figure 1.
lem. In contrast, we want to ensure the correctness of
all paths represented by the FSAs, and direct application
of MSA in the presence of different constituent orderings
can be problematic. For example, when given as input the
same sentences in Figure 1, one instantiation of the MSA
algorithm produces the FSA in Figure 3, which contains
many ?bad? paths such as the battle of last week?s fight-
ing took at least 12 people lost their people died in the
fighting last week?s fighting (See Section 4.2.2 for a more
quantitative analysis.). It?s still possible to use MSA if,
for example, the input is pre-clustered to have the same
constituent ordering (Barzilay and Lee (2003)). But we
chose to approach this problem from another direction.
As a result, we propose a new syntax-based algorithm to
produce FSAs.
In this paper, we first introduce the multiple transla-
tion corpus that we use in our experiments (see Section
2). We then present the algorithms that we developed to
induce finite-state paraphrase representations from such
data (see Section 3). An important part of the paper is
dedicated to evaluating the quality of the finite-state rep-
resentations that we derive (see Section 4). Since our rep-
resentations encode thousands and sometimes millions of
equivalent verbalizations of the same meaning, we use
both manual and automatic evaluation techniques. Some
of the automatic evaluations we perform are novel as
well.
2 Data
The data we use in this work is the LDC-available
Multiple-Translation Chinese (MTC) Corpus1 developed
for machine translation evaluation, which contains 105
news stories (993 sentences) from three sources of jour-
nalistic Mandarin Chinese text. These stories were inde-
pendently translated into English by 11 translation agen-
cies. Each sentence group, which consists of 11 semanti-
cally equivalent translations, is a rich source for learning
lexical and structural paraphrases. In our experiments,
we use 899 of the sentence groups ? the sentence groups
with sentences longer than 45 words were dropped.
3 A Syntax-Based Alignment Algorithm
Our syntax-based alignment algorithm, whose pseu-
docode is shown in Figure 4, works in three steps. In the
first step (lines 1-5 in Figure 4), we parse every sentence
in a sentence group and merge all resulting parse trees
into a parse forest. In the second step (line 6), we extract
1Linguistic Data Consortium (LDC) Catalog Number
LDC2002T01, ISBN 1-58563-217-1.
1. ParseForest = 
2. foreach s ? SentenceGroup
3. t = parseTree(s);
4. ParseForest = Merge(ParseForest, t);
5. endfor
6. Extract FSA from ParseForest;
7. Squeeze FSA;
Figure 4: The Syntax-Based Alignment Algorithm.
an FSA from the parse forest and then we compact it fur-
ther using a limited form of bottom-up alignment, which
we call squeezing (line 7). In what follows, we describe
each step in turn.
Top-down merging. Given a sentence group, we pass
each of the 11 sentences to Charniak?s (2000) parser to
get 11 parse trees. The first step in the algorithm is to
merge these parse trees into one parse-forest-like struc-
ture using a top-down process.
Let?s consider a simple case in which the parse for-
est contains one single tree, Tree 1 in Figure 5, and we
are adding Tree 2 to it. Since the two trees correspond
to sentences that have the same meaning and since both
trees expand an S node into an NP and a V P , it is rea-
sonable to assume that NP1 is a paraphrase of NP2 and
V P1 is a paraphrase of V P2. We merge NP1 with NP2
and V P1 with V P2 and continue the merging process on
each of the subtrees recursively, until we either reach the
leaves of the trees or the two nodes that we examine are
expanded using different syntactic rules.
When we apply this process to the trees in Figure 5,
the NP nodes are merged all the way down to the leaves,
and we get ?12? as a paraphrase of ?twelve? and ?people?
as a paraphrase of ?persons?; in contrast, the two V P s
are expanded in different ways, so no merging is done
beyond this level, and we are left with the information
that ?were killed? is a paraphrase of ?died?.
We repeat this top-down merging procedure with each
of the 11 parse trees in a sentence group. So far, only
constituents with same syntactic type are treated as para-
phrases. However, later we shall see that we can match
word spans whose syntactic types differ.
Keyword checking. The matching process described
above appears quite strict ? the expansions must match
exactly for two nodes to be merged. But consider the fol-
lowing parse trees:
1.(S (NP1 people)(V P1 were killed in this battle))
2.(S (NP2 this battle)(V P2 killed people))
If we applied the algorithm described above, we would
mistakenly align NP1 with NP2 and V P1 with V P2 ?
the algorithm described so far makes no use of lexical
12
twelve
people
persons were killed
died
Merge
Linearization
Tree 1 Tree 2
Parse Forest
FSA / Word Lattice
BEG END
+
S
NP VP
CD12 NNpersons AUXwere VP
VBkilled
S
NP VP
CD
twelve NNpeople VBdied
NP VP
CD NN AUX VPVB
12
twelve
people
persons
...
were
...killed...died
Figure 5: Top-down merging of parse trees and FSA ex-
traction.
information.
To prevent such erroneous alignments, we also imple-
ment a simple keyword checking procedure. We note
that since the word ?battle? appears in both V P1 and
NP2, this can serve as an evidence against the merging of
(NP1, NP2) and (V P1, V P2). A similar argument can
be constructed for the word ?people?. So in this exam-
ple we actually have double evidence against merging; in
general, one such clue suffices to stop the merging.
Our keyword checking procedure acts as a filter. A list
of keywords is maintained for each node in a syntactic
tree. This list contains all the nouns, verbs, and adjectives
that are spanned by a syntactic node. Before merging two
nodes, we check to see whether the keyword lists asso-
ciated with them share words with other nodes. That is,
supposed we just merged nodes A and B, and they are ex-
panded with the same syntactic rule into A1A2...An and
B1B2...Bn respectively; before we merge each Ai with
Bi, we check for each Bi if its keyword list shares com-
mon words with any Aj (j 6= i). If they do not, we con-
tinue the top-down merging process; otherwise we stop.
  
detroit
 a
 
building
 
detroit
 
detroit
 
a
 building
building
 
in
 
?s
 building
 
building
 
reduced
 
to
 
rubble
flattened
razed
 
was
 
blasted
leveled
razed
 
razed
 
leveled
 into
 
to
detroitbuilding
 
to down  
the ground
ashes
ground
 
the ground
  
levelled
 
to
 
in detroit ground
a. Before squeezing
 
 
detroit
 a
*e*  
?s
*e*
 
building  
building
 
reduced
 
*e*
was
 
flattened
 
blasted
leveled
 levelled
 
to
razed
 
leveled
*e*
 
into
 
to  
to
rubble
 
in detroit
down ashes
the
*e*
ground
b. After squeezing
Figure 6: Squeezing effect
In our current implementation, a pair of synonyms can
not stop an otherwise legitimate merging, but it?s possi-
ble to extend our keyword checking process with the help
of lexical resources such as WordNet in future work.
Mapping Parse Forests into Finite State Automata.
The process of mapping Parse Forests into Finite State
Automata is simple. We simply traverse the parse forest
top-down and create alternative paths for every merged
node. For example, the parse forest in Figure 5 is mapped
into the FSA shown at the bottom of the same figure. In
the FSA, there is a word associated with each edge. Dif-
ferent paths between any two nodes are assumed to be
paraphrases of each other. Each path that starts from the
BEGIN node and ends at the END node corresponds
to either an original input sentence or a paraphrase sen-
tence.
Squeezing. Since we adopted a very strict matching
criterion in top-down merging, a small difference in the
syntactic structure of two trees prevents some legitimate
mergings from taking place. This behavior is also exacer-
bated by errors in syntactic parsing. Hence, for instance,
three edges labeled detroit at the leftmost of the top FSA
in Figure 6 were kept apart. To compensate for this ef-
fect, our algorithm implements an additional step, which
we call squeezing. If two different edges that go into (or
out of) the same node in an FSA are labeled with the same
word, the nodes on the other end of the edges are merged.
We apply this operation exhaustively over the FSAs pro-
duced by the top-down merging procedure. Figure 6 il-
lustrates the effect of this operation: the FSA at the top
of this figure is compressed into the more compact FSA
shown at the bottom of it. Note that in addition to reduc-
ing the redundant edges, this also gives us paraphrases
not available in the FSA before squeezing (e.g. {reduced
to rubble, blasted to ground}). Therefore, the squeezing
operation, which implements a limited form of lexically
driven alignment similar to that exploited by MSA algo-
rithms, leads to FSAs that have a larger number of paths
and paraphrases.
4 Evaluation
The evaluation for our finite state representations and al-
gorithm requires careful examination. Obviously, what
counts as a good result largely depends on the applica-
tion one has in mind. If we are extracting paraphrases for
question-reformulation, it doesn?t really matter if we out-
put a few syntactically incorrect paraphrases, as long as
we produce a large number of semantically correct ones.
If we want to use the FSA for MT evaluation (for exam-
ple, comparing a sentence to be evaluated with the pos-
sible paths in FSA), we would want all paths to be rela-
tively good (which we will focus on in this paper), while
in some other applications, we may only care about the
quality of the best path (not addressed in this paper). Sec-
tion 4.1 concentrates on evaluating the paraphrase pairs
that can be extracted from the FSAs built by our system,
while Section 4.2 is dedicated to evaluating the FSAs di-
rectly.
4.1 Evaluating paraphrase pairs
4.1.1 Human-based evaluation of paraphrases
By construction, different paths between any two
nodes in the FSA representations that we derive are para-
phrases (in the context in which the nodes occur). To
evaluate our algorithm, we extract paraphrases from our
FSAs and ask human judges to evaluate their correctness.
We compare the paraphrases we collect with paraphrases
that are derivable from the same corpus using a co-
training-based paraphrase extraction algorithm (Barzilay
and McKeown, 2001). To the best of our knowledge, this
is the most relevant work to compare against since it aims
at extracting paraphrase pairs from parallel corpus. Un-
like our syntax-based algorithm which treats a sentence
as a tree structure and uses this hierarchical structural in-
formation to guide the merging process, their algorithm
treats a sentence as a sequence of phrases with surround-
ing contexts (no hierarchical structure involved) and co-
trains classifiers to detect paraphrases and contexts for
paraphrases. It would be interesting to compare the re-
sults from two algorithms so different from each other.
For the purpose of this experiment, we randomly se-
lected 300 paraphrase pairs (Ssyn) from the FSAs pro-
duced by our system. Since the co-training-based al-
gorithm of Barzilay and McKeown (2001) takes paral-
lel corpus as input, we created out of the MTC corpus
55 ? 993 sentence pairs (Each equivalent translation set
of cardinality 11 was mapped into
(11
2
)
equivalent trans-
lation pairs.). Regina Barzilay kindly provided us the list
of paraphrases extracted by their algorithm from this par-
allel corpus, from which we randomly selected another
set of 300 paraphrases (Scotr).
Correct Partial Incorrect
Ssyn 85% 12% 3%
Judge 1 Scotr 68% 13% 19%
Ssyn 80% 13% 7%
Judge 2 Scotr 63% 13% 24%
Ssyn 81% 5% 13%
Judge 3 Scotr 68% 3% 29%
Ssyn 77% 17% 5%
Judge 4 Scotr 68% 16% 16%
Average of Ssyn 81% 12% 7%
All Judges Scotr 66% 11% 22%
Table 1: A comparison of the correctness of the para-
phrases produced by the syntax-based alignment (Ssyn)
and co-training-based (Scotr) algorithms.
The resulting 600 paraphrase pairs were mixed and
presented in random order to four human judges. Each
judge was asked to assess the correctness of 150 para-
phrase pairs (75 pairs from each system) based on the
context, i.e., the sentence group, from which the para-
phrase pair was extracted. Judges were given three
choices: ?Correct?, for perfect paraphrases, ?Partially
correct?, for paraphrases in which there is only a par-
tial overlap between the meaning of two paraphrases (e.g.
while {saving set, aid package} is a correct paraphrase
pair in the given context, {set, aide package} is consid-
ered partially correct), and ?Incorrect?. The results of the
evaluation are presented in Table 1.
Although the four evaluators were judging four differ-
ent sets, each clearly rated a higher percentage of the out-
puts produced by the syntax-based alignment algorithm
as ?Correct?. We should note that there are parameters
specific to the co-training algorithm that we did not tune
to work for this particular corpus. In addition, the co-
training algorithm recovered more paraphrase pairs: the
syntax-based algorithm extracted 8666 pairs in total with
1051 of them extracted at least twice (i.e. more or less
reliable), while the numbers for the co-training algorithm
is 2934 out of a total of 16993 pairs. This means we are
not comparing the accuracy on the same recall level.
Aside from evaluating the correctness of the para-
phrases, we are also interested in the degree of overlap
between the paraphrase pairs discovered by the two algo-
rithms so different from each other. We find that out of
the 1051 paraphrase pairs that were extracted from more
than one sentence group by the syntax-based algorithm,
62.3% were also extracted by the co-training algorithm;
and out of the 2934 paraphrase pairs from the results of
co-training algorithm, 33.4% were also extracted by the
syntax-based algorithm. This shows that in spite of the
very different cues the two different algorithms rely on,
range of ASL 1-10 10-20 20-30 30-45
recall 30.7% 16.3% 7.8% 3.8%
Table 2: Recall of WordNet-consistent synonyms.
they do discover a lot of common pairs.
4.1.2 WordNet-based analysis of paraphrases
In order to (roughly) estimate the recall (of lexical syn-
onyms) of our algorithm, we use the synonymy relation
in WordNet to extract all the synonym pairs present in
our corpus. This extraction process yields the list of all
WordNet-consistent synonym pairs that are present in our
data. (Note that some of the pairs identified as synonyms
by WordNet, like ?follow/be?, are not really synonyms in
the contexts defined in our data set, which may lead to
artificial deflation of our recall estimate.) Once we have
the list of WordNet-consistent paraphrases, we can check
how many of them are recovered by our method. Table 2
gives the percentage of pairs recovered for each range of
average sentence length (ASL) in the group.
Not surprisingly, we get higher recall with shorter sen-
tences, since long sentences tend to differ in their syn-
tactic structures fairly high up in the parse trees, which
leads to fewer mergings at the lexical level. The recall
on the task of extracting lexical synonyms, as defined
by WordNet, is not high. But after all, this is not what
our algorithm has been designed for. It?s worth notic-
ing that the syntax-based algorithm also picks up many
paraphrases that are not identified as synonyms in Word-
Net. Out of 3217 lexical paraphrases that are learned by
our system, only 493 (15.3%) are WordNet synonyms,
which suggests that paraphrasing is a much richer and
looser relation than synonymy. However, the WordNet-
based recall figures suggest that WordNet can be used as
an additional source of information to be exploited by our
algorithm.
4.2 Evaluating the FSA directly
We noted before that apart from being a natural represen-
tation of paraphrases, the FSAs that we build have their
own merit and deserve to be evaluated directly. Since our
FSAs contain large numbers of paths, we design auto-
matic evaluation metrics to assess their qualities.
4.2.1 Language Model-based evaluation
If we take our claims seriously, each path in our FSAs
that connects the start and end nodes should correspond to
a well-formed sentence. We are interested in both quan-
tity (how many sentences our automata are able to pro-
duce) and quality (how good these sentences are). To an-
swer the first question, we simply count the number of
paths produced by our FSAs.
average N (# of paths) logN
length max ave max ave
1 - 10 22749 775 10.0 5.2
10 - 20 172386 4468 12.1 6.2
20 - 30 3479544 29202 15.1 5.8
30 - 45 684589 4135 13.4 4.5
Table 3: Statistics on Number of Paths in FSAs
random variable mean std. dev
ent(FSA)? ent(SG) ?0.11586 1.25162
ent(MTS)? ent(SG) 1.74259 1.05749
Table 4: Quality judged by LM
Table 3 gives the statistics on the number of paths pro-
duced by our FSAs, reported by the average length of
sentences in the input sentence groups. For example, the
sentence groups that have between 10 and 20 words pro-
duce, on average, automata that can yield 4468 alterna-
tive, semantically equivalent formulations.
Note that if we always get the same degree of merging
per word across all sentence groups, the number of paths
would tend to increase with the sentence length. This is
not the case here. Apparently we are getting less merg-
ing with longer sentences. But still, given 11 sentences,
we are capable of generating hundreds, thousands, and in
some cases even millions of sentences.
Obviously, we should not get too happy with our abil-
ity to boost the number of equivalent meanings if they are
incorrect. To assess the quality of the FSAs generated by
our algorithm, we use a language model-based metric.
We train a 4-gram model over one year of the Wall
Street Journal using the CMU-Cambridge Statistical Lan-
guage Modeling toolkit (v2). For each sentence group
SG, we use this language model to estimate the aver-
age entropy of the 11 original sentences in that group
(ent(SG)). We also compute the average entropy of
all the sentences in the corresponding FSA built by our
syntax-based algorithm (ent(FSA)). As the statistics in
Table 4 show, there is little difference between the av-
erage entropy of the original sentences and the average
entropy of the paraphrase sentences we produce. To bet-
ter calibrate this result, we compare it with the average
entropy of 6 corresponding machine translation outputs
(ent(MTS)), which were also made available by LDC
in conjunction with the same corpus. As one can see, the
difference between the average entropy of the machine
produced output and the average entropy of the origi-
nal 11 sentences is much higher than the difference be-
tween the average entropy of the FSA-produced outputs
and the average entropy of the original 11 sentences. Ob-
viously, this does not mean that our FSAs only produce
well-formed sentences. But it does mean that our FSAs
produce sentences that look more like human produced
sentences than machine produced ones according to a lan-
guage model.
4.2.2 Word repetition analysis
Not surprisingly, the language model we used in Sec-
tion 4.2.1 is far from being a perfect judge of sentence
quality. Recall the example of ?bad? path we gave in Sec-
tion 1: the battle of last week?s fighting took at least 12
people lost their people died in the fighting last week?s
fighting. Our 4-gram based language model will not find
any fault with this sentence. Notice, however, that some
words (such as ?fighting? and ?people?) appear at least
twice in this path, although they are not repeated in any
of the source sentences. These erroneous repetitions in-
dicate mis-alignment. By measuring the frequency of
words that are mistakenly repeated, we can now examine
quantitatively whether a direct application of the MSA
algorithm suffers from different constituent orderings as
we expected.
For each sentence group, we get a list of words that
never appear more than once in any sentence in this
group. Given a word from this list and the FSA built
from this group, we count the total number of paths that
contain this word (C) and the number of paths in which
this word appears at least twice (Cr, i.e. number of er-
roneous repetitions). We define the repetition ratio to
be Cr/C, which is the proportion of ?bad? paths in this
FSA according to this word. If we compute this ra-
tio for all the words in the lists of the first 499 groups2
and the corresponding FSAs produced by an instantia-
tion of the MSA algorithm3, the average repetition ra-
tio is 0.0304992 (14.76% of the words have a non-zero
repetition ratio, and the average ratio for these words is
0.206671). In comparison, the average repetition ratio for
our algorithm is 0.0035074 (2.16% of the words have a
non-zero repetition ratio4, and the average ratio for these
words is 0.162309). The presence of different constituent
orderings does pose a more serious problem to the MSA
algorithm.
4.2.3 MT-based evaluation
Recently, Papineni et al (2002) have proposed an au-
tomatic MT system evaluation technique (the BLEU
score). Given an MT system output and a set of refer-
2MSA runs very slow for longer sentences, and we believe
using the first 499 groups should be enough to make our point.
3We thank Regina Barzilay for providing us this set of re-
sults
4Note that FSAs produced right after keyword checking will
not yield any non-zero repetition ratio. However, if there are
mis-alignment not prevented by keyword checking in an FSA,
it may contain paths with erroneous repetition of words after
squeezing.
range 0-1 1-2 2-3 3-4 4-5
count 546 256 80 15 2
Table 5: Statistics for edgain
ence translations, one can estimate the ?goodness? of the
MT output by measuring the n-gram overlap between the
output and the reference set. The higher the overlap, i.e.,
the closer an output string is to a set of reference transla-
tions, the better a translation it is.
We hypothesize that our FSAs provide a better repre-
sentation against which the outputs of MT systems can
be evaluated because they encode not just a few but thou-
sands of equivalent semantic formulations of the desired
meaning. Ideally, if the FSAs we build accept all and
only the correct renderings of a given meaning, we can
just give a test sentence to the reference FSA and see if
it is accepted by it. Since this is not a realistic expecta-
tion, we measure the edit distance between a string and
an FSA instead: the smaller this distance is, the closer it
is to the meaning represented by the FSA.
To assess whether our FSAs are more appropriate rep-
resentations for evaluating the output of MT systems, we
perform the following experiment. For each sentence
group, we hold out one sentence as test sentence, and try
to evaluate how much of it can be predicted from the other
10 sentences. We compare two different ways of estimat-
ing the predictive power. (a) we compute the edit distance
between the test sentence and the other 10 sentences in
the set. The minimum of this distance is ed(input). (b)
we use dynamic programming to efficiently compute the
minimum distance (ed(FSA)) between the test sentence
and all the paths in the FSA built from the other 10 sen-
tences. The smaller the edit distance is, the better we
are predicting a test sentence. Mathematically, the differ-
ence between these two measures ed(input)? ed(FSA)
characterizes how much is gained in predictive power by
building the FSA.
We carry out the experiment described above in a
?leave-one-out? fashion (i.e. each sentence serves as
a test sentence once). Now let edgain be the average
of ed(input) ? ed(FSA) over the 11 runs for a given
group. We compute this for all 899 groups and find the
mean for edgain to be 0.91 (std. dev = 0.78). Table 5
gives the count for groups whose edgain falls into the
specified range. We can see that the majority of edgain
falls under 2.
We are also interested in the relation between the pre-
dictive power of the FSAs and the number of reference
translations they are derived from. For a given group, we
randomly order the sentences in it, set the last one as the
test sentence, and try to predict it with the first 1, 2, 3,
... 10 sentences. We investigate whether more sentences
ed(FSAn) ed(inputn)
?ed(FSA10) ?ed(FSAn)
n mean std. dev mean std. dev
1 5.65 3.86 0 0
2 3.66 3.02 0.19 0.60
3 2.71 2.55 0.33 0.76
4 2.10 2.33 0.46 0.90
5 1.56 2.01 0.56 0.95
6 1.18 1.79 0.65 1.02
7 0.79 1.48 0.75 1.09
8 0.49 1.10 0.81 1.11
9 0.21 0.74 0.89 1.16
10 0 0 0.93 1.21
Table 6: Effect of monotonically increasing the number
of reference sentences
yield an increase in the predictive power.
Let ed(FSAn) be the edit distance from the test sen-
tence to the FSA built on the first n sentences; similarly,
let ed(inputn) be the minimum edit distance from the
test sentence to an input set that consists of only the first
n sentences. Table 6 reports the effect of using differ-
ent number of reference translations. The first column
shows that each translation is contributing to the predic-
tive power of our FSA. Even when we add the tenth trans-
lation to our FSA, we still improve its predictive power.
The second column shows that the more sentences we add
to the FSA the larger the difference between its predic-
tive power and that of a simple set. The results in Table 6
suggest that our FSA may be used in order to refine the
BLEU metric (Papineni et al, 2002).
5 Conclusion & Future Work
In this paper, we presented a new syntax-based algorithm
that learns paraphrases from a newly available dataset.
The multiple translation corpus that we use in this paper
is the first instance in a series of similar corpora that are
built and made publicly available by LDC in the context
of a series of DARPA-sponsored MT evaluations. The
algorithm we proposed constructs finite state represen-
tations of paraphrases that are useful in many contexts:
to induce large lists of lexical and structural paraphrases;
to generate semantically equivalent renderings of a given
meaning; and to estimate the quality of machine transla-
tion systems. More experiments need to be carried out
in order to assess extrinsically whether the FSAs we pro-
duce can be used to yield higher agreement scores be-
tween human and automatic assessments of translation
quality.
In our future work, we wish to experiment with more
flexible merging algorithms and to integrate better the
top-down and bottom-up processes that are used to in-
duce FSAs. We also wish to extract more abstract para-
phrase patterns from the current representation. Such pat-
terns are more likely to get reused ? which would help us
get reliable statistics for them in the extraction phase, and
also have a better chance of being applicable to unseen
data.
Acknowledgments
We thank Hal Daume? III, Ulrich Germann, and Ulf Herm-
jakob for help and discussions; Eric Breck, Hubert Chen,
Stephen Chong, Dan Kifer, and Kevin O?Neill for par-
ticipating in the human evaluation; and the Cornell NLP
group and the reviewers for their comments on this pa-
per. We especially want to thank Regina Barzilay and
Lillian Lee for many valuable suggestions and help at var-
ious stages of this work. Portions of this work were done
while the first author was visiting Information Sciences
Institute. This work was supported by the Advanced
Research and Development Activity (ARDA)?s Advance
Question Answering for Intelligence (AQUAINT) Pro-
gram under contract number MDA908-02-C-0007, the
National Science Foundation under ITR/IM grant IIS-
0081334 and a Sloan Research Fellowship to Lillian Lee.
Any opinions, findings, and conclusions or recommen-
dations expressed above are those of the authors and do
not necessarily reflect the views of the National Science
Foundation or the Sloan Foundation.
References
Srinivas Bangalore, German Bordel, and Giuseppe Ric-
cardi. 2001. Computing consensus translation from
multiple machine translation systems. In Workshop on
Automatic Speech Recognition and Understanding.
Regina Barzilay and Lillian Lee. 2002. Bootstrap-
ping lexical choice via multiple-sequence alignment.
In Proceedings of the 2002 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 164?171.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proceedings of HLT/NAACL.
Regina Barzilay and Kathleen McKeown. 2001. Extract-
ing paraphrases from a parallel corpus. In Proceedings
of the ACL/EACL, pages 50?57.
Regina Barzilay, Kathleen McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In Proceedings of the
ACL, pages 550?557.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the NAACL.
DARPA. 2002. In DARPA IAO Machine Translation
Workshop, Santa Monica, CA, July 22-23.
Ulf Hermjakob, Abdessamad Echihabi, and Daniel
Marcu. 2002. Natural language based reformulation
resource and web exploitation for question answer-
ing. In Proceedings of the Text Retrieval Conference
(TREC?2002). November.
Lidija Iordanskaja, Richard Kittredge, and Alain Polge?re.
1991. Lexical selection and paraphrase in a meaning-
text generation model. In Ce?cile L. Paris, William R.
Swartout, and William C. Mann, editors, Natural Lan-
guage Generation in Artificial Intelligence and Com-
putational Linguistics, pages 293?312. Kluwer Aca-
demic Publisher.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of of ACL/COLING.
Nils Lenke. 1994. Anticipating the reader?s problems
and the automatic generation of paraphrases. In Pro-
ceedings of the 15th International Conference on Com-
putational Linguistics, volume 1, pages 319?323, Ky-
oto, Japan, August 5?9.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question answering. In Proceedings
of ACM SIGKDD Conference on Knowledge Discov-
ery and Data Mining 2001, pages 323?328.
Kishore Papineni, Salim Roukos, Todd Ward, John Hen-
derson, and Florence Reeder. 2002. Corpus-based
comprehensive and diagnostic MT evaluation: Initial
Arabic, Chinese, French, and Spanish results. In Pro-
ceedings of the Human Language Technology Confer-
ence, pages 124?127, San Diego, CA, March 24-27.
Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, and
Ralph Grishman. 2002. Automatic paraphrase acqui-
sition from news articles. In Proceedings of the Hu-
man Language Technology Conference (HLT?02), San
Diego, CA, March 24-27. Poster presentation.
Karen Sparck Jones and John I. Tait. 1984. Automatic
search term variant generation. Journal of Documen-
tation, 40(1):50?66.
Manfred Stede. 1999. Lexical Semantics and
Knowledge Representation in Multilingual Text
Generation. Kluwer Academic Publishers,
Boston/Dordrecht/London.
Thumbs up? Sentiment Classification using Machine Learning
Techniques
Bo Pang and Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY 14853 USA
{pabo,llee}@cs.cornell.edu
Shivakumar Vaithyanathan
IBM Almaden Research Center
650 Harry Rd.
San Jose, CA 95120 USA
shiv@almaden.ibm.com
Abstract
We consider the problem of classifying doc-
uments not by topic, but by overall senti-
ment, e.g., determining whether a review
is positive or negative. Using movie re-
views as data, we find that standard ma-
chine learning techniques definitively out-
perform human-produced baselines. How-
ever, the three machine learning methods
we employed (Naive Bayes, maximum en-
tropy classification, and support vector ma-
chines) do not perform as well on sentiment
classification as on traditional topic-based
categorization. We conclude by examining
factors that make the sentiment classifica-
tion problem more challenging.
1 Introduction
Today, very large amounts of information are avail-
able in on-line documents. As part of the effort to
better organize this information for users, researchers
have been actively investigating the problem of au-
tomatic text categorization.
The bulk of such work has focused on topical cat-
egorization, attempting to sort documents accord-
ing to their subject matter (e.g., sports vs. poli-
tics). However, recent years have seen rapid growth
in on-line discussion groups and review sites (e.g.,
the New York Times? Books web page) where a cru-
cial characteristic of the posted articles is their senti-
ment, or overall opinion towards the subject matter
? for example, whether a product review is pos-
itive or negative. Labeling these articles with their
sentiment would provide succinct summaries to read-
ers; indeed, these labels are part of the appeal and
value-add of such sites as www.rottentomatoes.com,
which both labels movie reviews that do not con-
tain explicit rating indicators and normalizes the
different rating schemes that individual reviewers
use. Sentiment classification would also be helpful in
business intelligence applications (e.g. MindfulEye?s
Lexant system1) and recommender systems (e.g.,
Terveen et al (1997), Tatemura (2000)), where user
input and feedback could be quickly summarized; in-
deed, in general, free-form survey responses given in
natural language format could be processed using
sentiment categorization. Moreover, there are also
potential applications to message filtering; for exam-
ple, one might be able to use sentiment information
to recognize and discard ?flames?(Spertus, 1997).
In this paper, we examine the effectiveness of ap-
plying machine learning techniques to the sentiment
classification problem. A challenging aspect of this
problem that seems to distinguish it from traditional
topic-based classification is that while topics are of-
ten identifiable by keywords alone, sentiment can be
expressed in a more subtle manner. For example, the
sentence ?How could anyone sit through this movie??
contains no single word that is obviously negative.
(See Section 7 for more examples). Thus, sentiment
seems to require more understanding than the usual
topic-based classification. So, apart from presenting
our results obtained via machine learning techniques,
we also analyze the problem to gain a better under-
standing of how difficult it is.
2 Previous Work
This section briefly surveys previous work on non-
topic-based text categorization.
One area of research concentrates on classifying
documents according to their source or source style,
with statistically-detected stylistic variation (Biber,
1988) serving as an important cue. Examples in-
clude author, publisher (e.g., the New York Times vs.
The Daily News), native-language background, and
?brow? (e.g., high-brow vs. ?popular?, or low-brow)
(Mosteller and Wallace, 1984; Argamon-Engelson et
1http://www.mindfuleye.com/about/lexant.htm
                                            Association for Computational Linguistics.
                      Language Processing (EMNLP), Philadelphia, July 2002, pp. 79-86.
                         Proceedings of the Conference on Empirical Methods in Natural
al., 1998; Tomokiyo and Jones, 2001; Kessler et al,
1997).
Another, more related area of research is that of
determining the genre of texts; subjective genres,
such as ?editorial?, are often one of the possible
categories (Karlgren and Cutting, 1994; Kessler et
al., 1997; Finn et al, 2002). Other work explicitly
attempts to find features indicating that subjective
language is being used (Hatzivassiloglou and Wiebe,
2000; Wiebe et al, 2001). But, while techniques for
genre categorization and subjectivity detection can
help us recognize documents that express an opin-
ion, they do not address our specific classification
task of determining what that opinion actually is.
Most previous research on sentiment-based classi-
fication has been at least partially knowledge-based.
Some of this work focuses on classifying the semantic
orientation of individual words or phrases, using lin-
guistic heuristics or a pre-selected set of seed words
(Hatzivassiloglou and McKeown, 1997; Turney and
Littman, 2002). Past work on sentiment-based cat-
egorization of entire documents has often involved
either the use of models inspired by cognitive lin-
guistics (Hearst, 1992; Sack, 1994) or the manual or
semi-manual construction of discriminant-word lex-
icons (Huettner and Subasic, 2000; Das and Chen,
2001; Tong, 2001). Interestingly, our baseline exper-
iments, described in Section 4, show that humans
may not always have the best intuition for choosing
discriminating words.
Turney?s (2002) work on classification of reviews
is perhaps the closest to ours.2 He applied a spe-
cific unsupervised learning technique based on the
mutual information between document phrases and
the words ?excellent? and ?poor?, where the mu-
tual information is computed using statistics gath-
ered by a search engine. In contrast, we utilize sev-
eral completely prior-knowledge-free supervised ma-
chine learning methods, with the goal of understand-
ing the inherent difficulty of the task.
3 The Movie-Review Domain
For our experiments, we chose to work with movie
reviews. This domain is experimentally convenient
because there are large on-line collections of such re-
views, and because reviewers often summarize their
overall sentiment with a machine-extractable rat-
ing indicator, such as a number of stars; hence, we
did not need to hand-label the data for supervised
learning or evaluation purposes. We also note that
Turney (2002) found movie reviews to be the most
2Indeed, although our choice of title was completely
independent of his, our selections were eerily similar.
difficult of several domains for sentiment classifica-
tion, reporting an accuracy of 65.83% on a 120-
document set (random-choice performance: 50%).
But we stress that the machine learning methods and
features we use are not specific to movie reviews, and
should be easily applicable to other domains as long
as sufficient training data exists.
Our data source was the Internet Movie Database
(IMDb) archive of the rec.arts.movies.reviews
newsgroup.3 We selected only reviews where the au-
thor rating was expressed either with stars or some
numerical value (other conventions varied too widely
to allow for automatic processing). Ratings were
automatically extracted and converted into one of
three categories: positive, negative, or neutral. For
the work described in this paper, we concentrated
only on discriminating between positive and nega-
tive sentiment. To avoid domination of the corpus
by a small number of prolific reviewers, we imposed
a limit of fewer than 20 reviews per author per sen-
timent category, yielding a corpus of 752 negative
and 1301 positive reviews, with a total of 144 re-
viewers represented. This dataset will be available
on-line at http://www.cs.cornell.edu/people/pabo/-
movie-review-data/ (the URL contains hyphens only
around the word ?review?).
4 A Closer Look At the Problem
Intuitions seem to differ as to the difficulty of the sen-
timent detection problem. An expert on using ma-
chine learning for text categorization predicted rela-
tively low performance for automatic methods. On
the other hand, it seems that distinguishing positive
from negative reviews is relatively easy for humans,
especially in comparison to the standard text catego-
rization problem, where topics can be closely related.
One might also suspect that there are certain words
people tend to use to express strong sentiments, so
that it might suffice to simply produce a list of such
words by introspection and rely on them alone to
classify the texts.
To test this latter hypothesis, we asked two gradu-
ate students in computer science to (independently)
choose good indicator words for positive and nega-
tive sentiments in movie reviews. Their selections,
shown in Figure 1, seem intuitively plausible. We
then converted their responses into simple decision
procedures that essentially count the number of the
proposed positive and negative words in a given doc-
ument. We applied these procedures to uniformly-
distributed data, so that the random-choice baseline
result would be 50%. As shown in Figure 1, the
3http://reviews.imdb.com/Reviews/
Proposed word lists Accuracy Ties
Human 1 positive: dazzling, brilliant, phenomenal, excellent, fantastic 58% 75%
negative: suck, terrible, awful, unwatchable, hideous
Human 2 positive: gripping, mesmerizing, riveting, spectacular, cool, 64% 39%
awesome, thrilling, badass, excellent, moving, exciting
negative: bad, cliched, sucks, boring, stupid, slow
Figure 1: Baseline results for human word lists. Data: 700 positive and 700 negative reviews.
Proposed word lists Accuracy Ties
Human 3 + stats positive: love, wonderful, best, great, superb, still, beautiful 69% 16%
negative: bad, worst, stupid, waste, boring, ?, !
Figure 2: Results for baseline using introspection and simple statistics of the data (including test data).
accuracy ? percentage of documents classified cor-
rectly ? for the human-based classifiers were 58%
and 64%, respectively.4 Note that the tie rates ?
percentage of documents where the two sentiments
were rated equally likely ? are quite high5 (we chose
a tie breaking policy that maximized the accuracy of
the baselines).
While the tie rates suggest that the brevity of
the human-produced lists is a factor in the relatively
poor performance results, it is not the case that size
alone necessarily limits accuracy. Based on a very
preliminary examination of frequency counts in the
entire corpus (including test data) plus introspection,
we created a list of seven positive and seven negative
words (including punctuation), shown in Figure 2.
As that figure indicates, using these words raised the
accuracy to 69%. Also, although this third list is of
comparable length to the other two, it has a much
lower tie rate of 16%. We further observe that some
of the items in this third list, such as ??? or ?still?,
would probably not have been proposed as possible
candidates merely through introspection, although
upon reflection one sees their merit (the question
mark tends to occur in sentences like ?What was the
director thinking??; ?still? appears in sentences like
?Still, though, it was worth seeing?).
We conclude from these preliminary experiments
that it is worthwhile to explore corpus-based tech-
niques, rather than relying on prior intuitions, to se-
lect good indicator features and to perform sentiment
classification in general. These experiments also pro-
vide us with baselines for experimental comparison;
in particular, the third baseline of 69% might actu-
ally be considered somewhat difficult to beat, since
it was achieved by examination of the test data (al-
though our examination was rather cursory; we do
4Later experiments using these words as features for
machine learning methods did not yield better results.
5This is largely due to 0-0 ties.
not claim that our list was the optimal set of four-
teen words).
5 Machine Learning Methods
Our aim in this work was to examine whether it suf-
fices to treat sentiment classification simply as a spe-
cial case of topic-based categorization (with the two
?topics? being positive sentiment and negative sen-
timent), or whether special sentiment-categorization
methods need to be developed. We experimented
with three standard algorithms: Naive Bayes clas-
sification, maximum entropy classification, and sup-
port vector machines. The philosophies behind these
three algorithms are quite different, but each has
been shown to be effective in previous text catego-
rization studies.
To implement these machine learning algorithms
on our document data, we used the following stan-
dard bag-of-features framework. Let {f1, . . . , fm} be
a predefined set of m features that can appear in
a document; examples include the word ?still? or
the bigram ?really stinks?. Let ni(d) be the num-
ber of times fi occurs in document d. Then, each
document d is represented by the document vector
~d := (n1(d), n2(d), . . . , nm(d)).
5.1 Naive Bayes
One approach to text classification is to assign to a
given document d the class c? = argmaxc P (c | d).
We derive the Naive Bayes (NB) classifier by first
observing that by Bayes? rule,
P (c | d) = P (c)P (d | c)P (d) ,
where P (d) plays no role in selecting c?. To estimate
the term P (d | c), Naive Bayes decomposes it by as-
suming the fi?s are conditionally independent given
d?s class:
PNB(c | d) :=
P (c)
(
?m
i=1 P (fi | c)ni(d)
)
P (d) .
Our training method consists of relative-frequency
estimation of P (c) and P (fi | c), using add-one
smoothing.
Despite its simplicity and the fact that its con-
ditional independence assumption clearly does not
hold in real-world situations, Naive Bayes-based text
categorization still tends to perform surprisingly well
(Lewis, 1998); indeed, Domingos and Pazzani (1997)
show that Naive Bayes is optimal for certain problem
classes with highly dependent features. On the other
hand, more sophisticated algorithms might (and of-
ten do) yield better results; we examine two such
algorithms next.
5.2 Maximum Entropy
Maximum entropy classification (MaxEnt, or ME,
for short) is an alternative technique which has
proven effective in a number of natural lan-
guage processing applications (Berger et al, 1996).
Nigam et al (1999) show that it sometimes, but not
always, outperforms Naive Bayes at standard text
classification. Its estimate of P (c | d) takes the fol-
lowing exponential form:
PME(c | d) :=
1
Z(d) exp
(
?
i
?i,cFi,c(d, c)
)
,
where Z(d) is a normalization function. Fi,c is a fea-
ture/class function for feature fi and class c, defined
as follows:6
Fi,c(d, c?) :=
{ 1, ni(d) > 0 and c? = c
0 otherwise .
For instance, a particular feature/class function
might fire if and only if the bigram ?still hate? ap-
pears and the document?s sentiment is hypothesized
to be negative.7 Importantly, unlike Naive Bayes,
MaxEnt makes no assumptions about the relation-
ships between features, and so might potentially per-
form better when conditional independence assump-
tions are not met.
The ?i,c?s are feature-weight parameters; inspec-
tion of the definition of PME shows that a large ?i,c
means that fi is considered a strong indicator for
6We use a restricted definition of feature/class func-
tions so that MaxEnt relies on the same sort of feature
information as Naive Bayes.
7The dependence on class is necessary for parameter
induction. See Nigam et al (1999) for additional moti-
vation.
class c. The parameter values are set so as to max-
imize the entropy of the induced distribution (hence
the classifier?s name) subject to the constraint that
the expected values of the feature/class functions
with respect to the model are equal to their expected
values with respect to the training data: the under-
lying philosophy is that we should choose the model
making the fewest assumptions about the data while
still remaining consistent with it, which makes intu-
itive sense. We use ten iterations of the improved
iterative scaling algorithm (Della Pietra et al, 1997)
for parameter training (this was a sufficient num-
ber of iterations for convergence of training-data ac-
curacy), together with a Gaussian prior to prevent
overfitting (Chen and Rosenfeld, 2000).
5.3 Support Vector Machines
Support vector machines (SVMs) have been shown to
be highly effective at traditional text categorization,
generally outperforming Naive Bayes (Joachims,
1998). They are large-margin, rather than proba-
bilistic, classifiers, in contrast to Naive Bayes and
MaxEnt. In the two-category case, the basic idea be-
hind the training procedure is to find a hyperplane,
represented by vector ~w, that not only separates
the document vectors in one class from those in the
other, but for which the separation, or margin, is as
large as possible. This search corresponds to a con-
strained optimization problem; letting cj ? {1,?1}
(corresponding to positive and negative) be the cor-
rect class of document dj , the solution can be written
as
~w :=
?
j
?jcj ~dj , ?j ? 0,
where the ?j ?s are obtained by solving a dual opti-
mization problem. Those ~dj such that ?j is greater
than zero are called support vectors, since they are
the only document vectors contributing to ~w. Clas-
sification of test instances consists simply of deter-
mining which side of ~w?s hyperplane they fall on.
We used Joachim?s (1999) SVM light package8 for
training and testing, with all parameters set to their
default values, after first length-normalizing the doc-
ument vectors, as is standard (neglecting to normal-
ize generally hurt performance slightly).
6 Evaluation
6.1 Experimental Set-up
We used documents from the movie-review corpus
described in Section 3. To create a data set with uni-
form class distribution (studying the effect of skewed
8http://svmlight.joachims.org
Features # of frequency or NB ME SVM
features presence?
(1) unigrams 16165 freq. 78.7 N/A 72.8
(2) unigrams ? pres. 81.0 80.4 82.9
(3) unigrams+bigrams 32330 pres. 80.6 80.8 82.7
(4) bigrams 16165 pres. 77.3 77.4 77.1
(5) unigrams+POS 16695 pres. 81.5 80.4 81.9
(6) adjectives 2633 pres. 77.0 77.7 75.1
(7) top 2633 unigrams 2633 pres. 80.3 81.0 81.4
(8) unigrams+position 22430 pres. 81.0 80.1 81.6
Figure 3: Average three-fold cross-validation accuracies, in percent. Boldface: best performance for a given
setting (row). Recall that our baseline results ranged from 50% to 69%.
class distributions was out of the scope of this study),
we randomly selected 700 positive-sentiment and 700
negative-sentiment documents. We then divided this
data into three equal-sized folds, maintaining bal-
anced class distributions in each fold. (We did not
use a larger number of folds due to the slowness of
the MaxEnt training procedure.) All results reported
below, as well as the baseline results from Section 4,
are the average three-fold cross-validation results on
this data (of course, the baseline algorithms had no
parameters to tune).
To prepare the documents, we automatically re-
moved the rating indicators and extracted the tex-
tual information from the original HTML docu-
ment format, treating punctuation as separate lex-
ical items. No stemming or stoplists were used.
One unconventional step we took was to attempt
to model the potentially important contextual effect
of negation: clearly ?good? and ?not very good? in-
dicate opposite sentiment orientations. Adapting a
technique of Das and Chen (2001), we added the tag
NOT to every word between a negation word (?not?,
?isn?t?, ?didn?t?, etc.) and the first punctuation
mark following the negation word. (Preliminary ex-
periments indicate that removing the negation tag
had a negligible, but on average slightly harmful, ef-
fect on performance.)
For this study, we focused on features based on
unigrams (with negation tagging) and bigrams. Be-
cause training MaxEnt is expensive in the number of
features, we limited consideration to (1) the 16165
unigrams appearing at least four times in our 1400-
document corpus (lower count cutoffs did not yield
significantly different results), and (2) the 16165 bi-
grams occurring most often in the same data (the
selected bigrams all occurred at least seven times).
Note that we did not add negation tags to the bi-
grams, since we consider bigrams (and n-grams in
general) to be an orthogonal way to incorporate con-
text.
6.2 Results
Initial unigram results The classification accu-
racies resulting from using only unigrams as fea-
tures are shown in line (1) of Figure 3. As a whole,
the machine learning algorithms clearly surpass the
random-choice baseline of 50%. They also hand-
ily beat our two human-selected-unigram baselines
of 58% and 64%, and, furthermore, perform well in
comparison to the 69% baseline achieved via limited
access to the test-data statistics, although the im-
provement in the case of SVMs is not so large.
On the other hand, in topic-based classification,
all three classifiers have been reported to use bag-
of-unigram features to achieve accuracies of 90%
and above for particular categories (Joachims, 1998;
Nigam et al, 1999)9 ? and such results are for set-
tings with more than two classes. This provides
suggestive evidence that sentiment categorization is
more difficult than topic classification, which cor-
responds to the intuitions of the text categoriza-
tion expert mentioned above.10 Nonetheless, we still
wanted to investigate ways to improve our senti-
ment categorization results; these experiments are
reported below.
Feature frequency vs. presence Recall that we
represent each document d by a feature-count vector
(n1(d), . . . , nm(d)). However, the definition of the
9Joachims (1998) used stemming and stoplists; in
some of their experiments, Nigam et al (1999), like us,
did not.
10We could not perform the natural experiment of at-
tempting topic-based categorization on our data because
the only obvious topics would be the film being reviewed;
unfortunately, in our data, the maximum number of re-
views per movie is 27, too small for meaningful results.
MaxEnt feature/class functions Fi,c only reflects the
presence or absence of a feature, rather than directly
incorporating feature frequency. In order to investi-
gate whether reliance on frequency information could
account for the higher accuracies of Naive Bayes and
SVMs, we binarized the document vectors, setting
ni(d) to 1 if and only feature fi appears in d, and
reran Naive Bayes and SVM light on these new vec-
tors.11
As can be seen from line (2) of Figure 3,
better performance (much better performance for
SVMs) is achieved by accounting only for fea-
ture presence, not feature frequency. Interestingly,
this is in direct opposition to the observations of
McCallum and Nigam (1998) with respect to Naive
Bayes topic classification. We speculate that this in-
dicates a difference between sentiment and topic cat-
egorization ? perhaps due to topic being conveyed
mostly by particular content words that tend to be
repeated ? but this remains to be verified. In any
event, as a result of this finding, we did not incor-
porate frequency information into Naive Bayes and
SVMs in any of the following experiments.
Bigrams In addition to looking specifically for
negation words in the context of a word, we also
studied the use of bigrams to capture more context
in general. Note that bigrams and unigrams are
surely not conditionally independent, meaning that
the feature set they comprise violates Naive Bayes?
conditional-independence assumptions; on the other
hand, recall that this does not imply that Naive
Bayes will necessarily do poorly (Domingos and Paz-
zani, 1997).
Line (3) of the results table shows that bigram
information does not improve performance beyond
that of unigram presence, although adding in the bi-
grams does not seriously impact the results, even for
Naive Bayes. This would not rule out the possibility
that bigram presence is as equally useful a feature
as unigram presence; in fact, Pedersen (2001) found
that bigrams alone can be effective features for word
sense disambiguation. However, comparing line (4)
to line (2) shows that relying just on bigrams causes
accuracy to decline by as much as 5.8 percentage
points. Hence, if context is in fact important, as our
intuitions suggest, bigrams are not effective at cap-
turing it in our setting.
11Alternatively, we could have tried integrating fre-
quency information into MaxEnt. However, feature/class
functions are traditionally defined as binary (Berger et
al., 1996); hence, explicitly incorporating frequencies
would require different functions for each count (or count
bin), making training impractical. But cf. (Nigam et al,
1999).
Parts of speech We also experimented with ap-
pending POS tags to every word via Oliver Mason?s
Qtag program.12 This serves as a crude form of word
sense disambiguation (Wilks and Stevenson, 1998):
for example, it would distinguish the different usages
of ?love? in ?I love this movie? (indicating sentiment
orientation) versus ?This is a love story? (neutral
with respect to sentiment). However, the effect of
this information seems to be a wash: as depicted in
line (5) of Figure 3, the accuracy improves slightly
for Naive Bayes but declines for SVMs, and the per-
formance of MaxEnt is unchanged.
Since adjectives have been a focus of previous work
in sentiment detection (Hatzivassiloglou and Wiebe,
2000; Turney, 2002)13, we looked at the performance
of using adjectives alone. Intuitively, we might ex-
pect that adjectives carry a great deal of informa-
tion regarding a document?s sentiment; indeed, the
human-produced lists from Section 4 contain almost
no other parts of speech. Yet, the results, shown in
line (6) of Figure 3, are relatively poor: the 2633
adjectives provide less useful information than uni-
gram presence. Indeed, line (7) shows that simply
using the 2633 most frequent unigrams is a better
choice, yielding performance comparable to that of
using (the presence of) all 16165 (line (2)). This may
imply that applying explicit feature-selection algo-
rithms on unigrams could improve performance.
Position An additional intuition we had was that
the position of a word in the text might make a dif-
ference: movie reviews, in particular, might begin
with an overall sentiment statement, proceed with
a plot discussion, and conclude by summarizing the
author?s views. As a rough approximation to deter-
mining this kind of structure, we tagged each word
according to whether it appeared in the first quar-
ter, last quarter, or middle half of the document14.
The results (line (8)) didn?t differ greatly from using
unigrams alone, but more refined notions of position
might be more successful.
7 Discussion
The results produced via machine learning tech-
niques are quite good in comparison to the human-
generated baselines discussed in Section 4. In terms
of relative performance, Naive Bayes tends to do the
worst and SVMs tend to do the best, although the
12http://www.english.bham.ac.uk/staff/oliver/soft-
ware/tagger/index.htm
13Turney?s (2002) unsupervised algorithm uses bi-
grams containing an adjective or an adverb.
14We tried a few other settings, e.g., first third vs. last
third vs middle third, and found them to be less effective.
differences aren?t very large.
On the other hand, we were not able to achieve ac-
curacies on the sentiment classification problem com-
parable to those reported for standard topic-based
categorization, despite the several different types of
features we tried. Unigram presence information
turned out to be the most effective; in fact, none of
the alternative features we employed provided consis-
tently better performance once unigram presence was
incorporated. Interestingly, though, the superiority
of presence information in comparison to frequency
information in our setting contradicts previous obser-
vations made in topic-classification work (McCallum
and Nigam, 1998).
What accounts for these two differences ? dif-
ficulty and types of information proving useful ?
between topic and sentiment classification, and how
might we improve the latter? To answer these ques-
tions, we examined the data further. (All examples
below are drawn from the full 2053-document cor-
pus.)
As it turns out, a common phenomenon in the doc-
uments was a kind of ?thwarted expectations? narra-
tive, where the author sets up a deliberate contrast
to earlier discussion: for example, ?This film should
be brilliant. It sounds like a great plot, the actors are
first grade, and the supporting cast is good as well, and
Stallone is attempting to deliver a good performance.
However, it can?t hold up? or ?I hate the Spice Girls.
...[3 things the author hates about them]... Why I saw
this movie is a really, really, really long story, but I
did, and one would think I?d despise every minute of
it. But... Okay, I?m really ashamed of it, but I enjoyed
it. I mean, I admit it?s a really awful movie ...the ninth
floor of hell...The plot is such a mess that it?s terrible.
But I loved it.? 15
In these examples, a human would easily detect
the true sentiment of the review, but bag-of-features
classifiers would presumably find these instances dif-
ficult, since there are many words indicative of the
opposite sentiment to that of the entire review. Fun-
damentally, it seems that some form of discourse
analysis is necessary (using more sophisticated tech-
15This phenomenon is related to another common
theme, that of ?a good actor trapped in a bad movie?:
?AN AMERICAN WEREWOLF IN PARIS is a failed at-
tempt... Julie Delpy is far too good for this movie. She im-
bues Serafine with spirit, spunk, and humanity. This isn?t
necessarily a good thing, since it prevents us from relax-
ing and enjoying AN AMERICAN WEREWOLF IN PARIS
as a completely mindless, campy entertainment experience.
Delpy?s injection of class into an otherwise classless produc-
tion raises the specter of what this film could have been
with a better script and a better cast ... She was radiant,
charismatic, and effective ....?
niques than our positional feature mentioned above),
or at least some way of determining the focus of each
sentence, so that one can decide when the author is
talking about the film itself. (Turney (2002) makes
a similar point, noting that for reviews, ?the whole
is not necessarily the sum of the parts?.) Further-
more, it seems likely that this thwarted-expectations
rhetorical device will appear in many types of texts
(e.g., editorials) devoted to expressing an overall
opinion about some topic. Hence, we believe that an
important next step is the identification of features
indicating whether sentences are on-topic (which is
a kind of co-reference problem); we look forward to
addressing this challenge in future work.
Acknowledgments
We thank Joshua Goodman, Thorsten Joachims, Jon
Kleinberg, Vikas Krishna, John Lafferty, Jussi Myl-
lymaki, Phoebe Sengers, Richard Tong, Peter Tur-
ney, and the anonymous reviewers for many valuable
comments and helpful suggestions, and Hubie Chen
and Tony Faradjian for participating in our baseline
experiments. Portions of this work were done while
the first author was visiting IBM Almaden. This pa-
per is based upon work supported in part by the Na-
tional Science Foundation under ITR/IM grant IIS-
0081334. Any opinions, findings, and conclusions or
recommendations expressed above are those of the
authors and do not necessarily reflect the views of
the National Science Foundation.
References
Shlomo Argamon-Engelson, Moshe Koppel, and
Galit Avneri. 1998. Style-based text categoriza-
tion: What newspaper am I reading? In Proc. of
the AAAI Workshop on Text Categorization, pages
1?4.
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39?71.
Douglas Biber. 1988. Variation across Speech and
Writing. Cambridge University Press.
Stanley Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for ME models. IEEE
Trans. Speech and Audio Processing, 8(1):37?50.
Sanjiv Das and Mike Chen. 2001. Yahoo! for
Amazon: Extracting market sentiment from stock
message boards. In Proc. of the 8th Asia Pacific
Finance Association Annual Conference (APFA
2001).
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 19(4):380?393.
Pedro Domingos and Michael J. Pazzani. 1997. On
the optimality of the simple Bayesian classifier un-
der zero-one loss. Machine Learning, 29(2-3):103?
130.
Aidan Finn, Nicholas Kushmerick, and Barry Smyth.
2002. Genre classification and domain transfer
for information filtering. In Proc. of the Eu-
ropean Colloquium on Information Retrieval Re-
search, pages 353?362, Glasgow.
Vasileios Hatzivassiloglou and Kathleen McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proc. of the 35th ACL/8th EACL, pages
174?181.
Vasileios Hatzivassiloglou and Janyce Wiebe. 2000.
Effects of adjective orientation and gradability on
sentence subjectivity. In Proc. of COLING.
Marti Hearst. 1992. Direction-based text interpre-
tation as an information access refinement. In
Paul Jacobs, editor, Text-Based Intelligent Sys-
tems. Lawrence Erlbaum Associates.
Alison Huettner and Pero Subasic. 2000. Fuzzy
typing for document management. In ACL
2000 Companion Volume: Tutorial Abstracts and
Demonstration Notes, pages 26?27.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many rel-
evant features. In Proc. of the European Confer-
ence on Machine Learning (ECML), pages 137?
142.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Scho?lkopf and
Alexander Smola, editors, Advances in Kernel
Methods - Support Vector Learning, pages 44?56.
MIT Press.
Jussi Karlgren and Douglass Cutting. 1994. Recog-
nizing text genres with simple metrics using dis-
criminant analysis. In Proc. of COLING.
Brett Kessler, Geoffrey Nunberg, and Hinrich
Schu?tze. 1997. Automatic detection of text genre.
In Proc. of the 35th ACL/8th EACL, pages 32?38.
David D. Lewis. 1998. Naive (Bayes) at forty: The
independence assumption in information retrieval.
In Proc. of the European Conference on Machine
Learning (ECML), pages 4?15. Invited talk.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proc. of the AAAI-98 Workshop on
Learning for Text Categorization, pages 41?48.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case
of the Federalist Papers. Springer-Verlag.
Kamal Nigam, John Lafferty, and Andrew McCal-
lum. 1999. Using maximum entropy for text clas-
sification. In Proc. of the IJCAI-99 Workshop on
Machine Learning for Information Filtering, pages
61?67.
Ted Pedersen. 2001. A decision tree of bigrams is an
accurate predictor of word sense. In Proc. of the
Second NAACL, pages 79?86.
Warren Sack. 1994. On the computation of point of
view. In Proc. of the Twelfth AAAI, page 1488.
Student abstract.
Ellen Spertus. 1997. Smokey: Automatic recog-
nition of hostile messages. In Proc. of Innova-
tive Applications of Artificial Intelligence (IAAI),
pages 1058?1065.
Junichi Tatemura. 2000. Virtual reviewers for col-
laborative exploration of movie reviews. In Proc.
of the 5th International Conference on Intelligent
User Interfaces, pages 272?275.
Loren Terveen, Will Hill, Brian Amento, David Mc-
Donald, and Josh Creter. 1997. PHOAKS: A sys-
tem for sharing recommendations. Communica-
tions of the ACM, 40(3):59?62.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from round here, are you? Naive Bayes
detection of non-native utterance text. In Proc. of
the Second NAACL, pages 239?246.
Richard M. Tong. 2001. An operational system for
detecting and tracking opinions in on-line discus-
sion. Workshop note, SIGIR 2001 Workshop on
Operational Text Classification.
Peter D. Turney and Michael L. Littman. 2002. Un-
supervised learning of semantic orientation from
a hundred-billion-word corpus. Technical Report
EGB-1094, National Research Council Canada.
Peter Turney. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. In Proc. of the ACL.
Janyce M. Wiebe, Theresa Wilson, and Matthew
Bell. 2001. Identifying collocations for recognizing
opinions. In Proc. of the ACL/EACL Workshop
on Collocation.
Yorick Wilks and Mark Stevenson. 1998. The gram-
mar of sense: Using part-of-speech tags as a first
step in semantic disambiguation. Journal of Nat-
ural Language Engineering, 4(2):135?144.
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 327?335,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Get out the vote: Determining support or opposition from Congressional
floor-debate transcripts
Matt Thomas, Bo Pang, and Lillian Lee
Department of Computer Science, Cornell University
Ithaca, NY 14853-7501
mattthomas84@gmail.com, pabo@cs.cornell.edu, llee@cs.cornell.edu
Abstract
We investigate whether one can determine
from the transcripts of U.S. Congressional
floor debates whether the speeches repre-
sent support of or opposition to proposed
legislation. To address this problem, we
exploit the fact that these speeches occur
as part of a discussion; this allows us to
use sources of information regarding re-
lationships between discourse segments,
such as whether a given utterance indicates
agreement with the opinion expressed by
another. We find that the incorporation
of such information yields substantial im-
provements over classifying speeches in
isolation.
1 Introduction
One ought to recognize that the present
political chaos is connected with the de-
cay of language, and that one can prob-
ably bring about some improvement by
starting at the verbal end. ? Orwell,
?Politics and the English language?
We have entered an era where very large
amounts of politically oriented text are now avail-
able online. This includes both official documents,
such as the full text of laws and the proceedings of
legislative bodies, and unofficial documents, such
as postings on weblogs (blogs) devoted to politics.
In some sense, the availability of such data is sim-
ply a manifestation of a general trend of ?every-
body putting their records on the Internet?.1 The
1It is worth pointing out that the United States? Library of
Congress was an extremely early adopter of Web technology:
the THOMAS database (http://thomas.loc.gov) of congres-
online accessibility of politically oriented texts in
particular, however, is a phenomenon that some
have gone so far as to say will have a potentially
society-changing effect.
In the United States, for example, governmen-
tal bodies are providing and soliciting political
documents via the Internet, with lofty goals in
mind: electronic rulemaking (eRulemaking) ini-
tiatives involving the ?electronic collection, dis-
tribution, synthesis, and analysis of public com-
mentary in the regulatory rulemaking process?,
may ?[alter] the citizen-government relationship?
(Shulman and Schlosberg, 2002). Additionally,
much media attention has been focused recently
on the potential impact that Internet sites may have
on politics2, or at least on political journalism3.
Regardless of whether one views such claims as
clear-sighted prophecy or mere hype, it is obvi-
ously important to help people understand and an-
alyze politically oriented text, given the impor-
tance of enabling informed participation in the po-
litical process.
Evaluative and persuasive documents, such as
a politician?s speech regarding a bill or a blog-
ger?s commentary on a legislative proposal, form a
particularly interesting type of politically oriented
text. People are much more likely to consult such
evaluative statements than the actual text of a bill
or law under discussion, given the dense nature of
legislative language and the fact that (U.S.) bills
often reach several hundred pages in length (Smith
et al, 2005). Moreover, political opinions are ex-
sional bills and related data was launched in January 1995,
when Mosaic was not quite two years old and Altavista did
not yet exist.
2E.g., ?Internet injects sweeping change into U.S. poli-
tics?, Adam Nagourney, The New York Times, April 2, 2006.
3E.g., ?The End of News??, Michael Massing, The New
York Review of Books, December 1, 2005.
327
plicitly solicited in the eRulemaking scenario.
In the analysis of evaluative language, it is fun-
damentally necessary to determine whether the au-
thor/speaker supports or disapproves of the topic
of discussion. In this paper, we investigate the
following specific instantiation of this problem:
we seek to determine from the transcripts of
U.S. Congressional floor debates whether each
?speech? (continuous single-speaker segment of
text) represents support for or opposition to a pro-
posed piece of legislation. Note that from an ex-
perimental point of view, this is a very convenient
problem to work with because we can automati-
cally determine ground truth (and thus avoid the
need for manual annotation) simply by consulting
publicly available voting records.
Task properties Determining whether or not a
speaker supports a proposal falls within the realm
of sentiment analysis, an extremely active re-
search area devoted to the computational treatment
of subjective or opinion-oriented language (early
work includes Wiebe and Rapaport (1988), Hearst
(1992), Sack (1994), and Wiebe (1994); see Esuli
(2006) for an active bibliography). In particu-
lar, since we treat each individual speech within
a debate as a single ?document?, we are consider-
ing a version of document-level sentiment-polarity
classification, namely, automatically distinguish-
ing between positive and negative documents (Das
and Chen, 2001; Pang et al, 2002; Turney, 2002;
Dave et al, 2003).
Most sentiment-polarity classifiers proposed in
the recent literature categorize each document in-
dependently. A few others incorporate various
measures of inter-document similarity between the
texts to be labeled (Agarwal and Bhattacharyya,
2005; Pang and Lee, 2005; Goldberg and Zhu,
2006). Many interesting opinion-oriented docu-
ments, however, can be linked through certain re-
lationships that occur in the context of evaluative
discussions. For example, we may find textual4
evidence of a high likelihood of agreement be-
4Because we are most interested in techniques applicable
across domains, we restrict consideration to NLP aspects of
the problem, ignoring external problem-specific information.
For example, although most votes in our corpus were almost
completely along party lines (and despite the fact that same-
party information is easily incorporated via the methods we
propose), we did not use party-affiliation data. Indeed, in
other settings (e.g., a movie-discussion listserv) one may not
be able to determine the participants? political leanings, and
such information may not lead to significantly improved re-
sults even if it were available.
tween two speakers, such as explicit assertions (?I
second that!?) or quotation of messages in emails
or postings (see Mullen and Malouf (2006) but cf.
Agrawal et al (2003)). Agreement evidence can
be a powerful aid in our classification task: for ex-
ample, we can easily categorize a complicated (or
overly terse) document if we find within it indica-
tions of agreement with a clearly positive text.
Obviously, incorporating agreement informa-
tion provides additional benefit only when the in-
put documents are relatively difficult to classify
individually. Intuition suggests that this is true
of the data with which we experiment, for several
reasons. First, U.S. congressional debates contain
very rich language and cover an extremely wide
variety of topics, ranging from flag burning to in-
ternational policy to the federal budget. Debates
are also subject to digressions, some fairly natural
and others less so (e.g., ?Why are we discussing
this bill when the plight of my constituents regard-
ing this other issue is being ignored??)
Second, an important characteristic of persua-
sive language is that speakers may spend more
time presenting evidence in support of their po-
sitions (or attacking the evidence presented by
others) than directly stating their attitudes. An
extreme example will illustrate the problems in-
volved. Consider a speech that describes the U.S.
flag as deeply inspirational, and thus contains only
positive language. If the bill under discussion is a
proposed flag-burning ban, then the speech is sup-
portive; but if the bill under discussion is aimed at
rescinding an existing flag-burning ban, the speech
may represent opposition to the legislation. Given
the current state of the art in sentiment analysis,
it is doubtful that one could determine the (proba-
bly topic-specific) relationship between presented
evidence and speaker opinion.
Qualitative summary of results The above dif-
ficulties underscore the importance of enhancing
standard classification techniques with new infor-
mation sources that promise to improve accuracy,
such as inter-document relationships between the
documents to be labeled. In this paper, we demon-
strate that the incorporation of agreement model-
ing can provide substantial improvements over the
application of support vector machines (SVMs) in
isolation, which represents the state of the art in
the individual classification of documents. The en-
hanced accuracies are obtained via a fairly primi-
tive automatically-acquired ?agreement detector?
328
total train test development
speech segments 3857 2740 860 257
debates 53 38 10 5
average number of speech segments per debate 72.8 72.1 86.0 51.4
average number of speakers per debate 32.1 30.9 41.1 22.6
Table 1: Corpus statistics.
and a conceptually simple method for integrat-
ing isolated-document and agreement-based in-
formation. We thus view our results as demon-
strating the potentially large benefits of exploiting
sentiment-related discourse-segment relationships
in sentiment-analysis tasks.
2 Corpus
This section outlines the main steps of the process
by which we created our corpus (download site:
www.cs.cornell.edu/home/llee/data/convote.html).
GovTrack (http://govtrack.us) is an independent
website run by Joshua Tauberer that collects pub-
licly available data on the legislative and fund-
raising activities of U.S. congresspeople. Due to
its extensive cross-referencing and collating of in-
formation, it was nominated for a 2006 ?Webby?
award. A crucial characteristic of GovTrack from
our point of view is that the information is pro-
vided in a very convenient format; for instance,
the floor-debate transcripts are broken into sepa-
rate HTML files according to the subject of the
debate, so we can trivially derive long sequences
of speeches guaranteed to cover the same topic.
We extracted from GovTrack all available tran-
scripts of U.S. floor debates in the House of Rep-
resentatives for the year 2005 (3268 pages of tran-
scripts in total), together with voting records for all
roll-call votes during that year. We concentrated
on debates regarding ?controversial? bills (ones in
which the losing side generated at least 20% of the
speeches) because these debates should presum-
ably exhibit more interesting discourse structure.
Each debate consists of a series of speech seg-
ments, where each segment is a sequence of un-
interrupted utterances by a single speaker. Since
speech segments represent natural discourse units,
we treat them as the basic unit to be classified.
Each speech segment was labeled by the vote
(?yea? or ?nay?) cast for the proposed bill by the
person who uttered the speech segment.
We automatically discarded those speech seg-
ments belonging to a class of formulaic, generally
one-sentence utterances focused on the yielding
of time on the house floor (for example, ?Madam
Speaker, I am pleased to yield 5 minutes to the
gentleman from Massachusetts?), as such speech
segments are clearly off-topic. We also removed
speech segments containing the term ?amend-
ment?, since we found during initial inspection
that these speeches generally reflect a speaker?s
opinion on an amendment, and this opinion may
differ from the speaker?s opinion on the underly-
ing bill under discussion.
We randomly split the data into training, test,
and development (parameter-tuning) sets repre-
senting roughly 70%, 20%, and 10% of our data,
respectively (see Table 1). The speech segments
remained grouped by debate, with 38 debates as-
signed to the training set, 10 to the test set, and 5
to the development set; we require that the speech
segments from an individual debate all appear in
the same set because our goal is to examine clas-
sification of speech segments in the context of the
surrounding discussion.
3 Method
The support/oppose classification problem can be
approached through the use of standard classifiers
such as support vector machines (SVMs), which
consider each text unit in isolation. As discussed
in Section 1, however, the conversational nature
of our data implies the existence of various rela-
tionships that can be exploited to improve cumu-
lative classification accuracy for speech segments
belonging to the same debate. Our classification
framework, directly inspired by Blum and Chawla
(2001), integrates both perspectives, optimizing
its labeling of speech segments based on both in-
dividual speech-segment classification scores and
preferences for groups of speech segments to re-
ceive the same label. In this section, we discuss
the specific classification framework that we adopt
and the set of mechanisms that we propose for
modeling specific types of relationships.
329
3.1 Classification framework
Let s1, s2, . . . , sn be the sequence of speech seg-
ments within a given debate, and let Y and
N stand for the ?yea? and ?nay? class, respec-
tively. Assume we have a non-negative func-
tion ind(s, C) indicating the degree of preference
that an individual-document classifier, such as an
SVM, has for placing speech-segment s in class
C. Also, assume that some pairs of speech seg-
ments have weighted links between them, where
the non-negative strength (weight) str(`) for a
link ` indicates the degree to which it is prefer-
able that the linked speech segments receive the
same label. Then, any class assignment c =
c(s1), c(s2), . . . , c(sn) can be assigned a cost
?
s
ind(s, c(s))+
?
s,s?: c(s) 6=c(s?)
?
` between s,s?
str(`),
where c(s) is the ?opposite? class from c(s). A
minimum-cost assignment thus represents an opti-
mum way to classify the speech segments so that
each one tends not to be put into the class that
the individual-document classifier disprefers, but
at the same time, highly associated speech seg-
ments tend not to be put in different classes.
As has been previously observed and exploited
in the NLP literature (Pang and Lee, 2004; Agar-
wal and Bhattacharyya, 2005; Barzilay and Lap-
ata, 2005), the above optimization function, unlike
many others that have been proposed for graph or
set partitioning, can be solved exactly in an prov-
ably efficient manner via methods for finding min-
imum cuts in graphs. In our view, the contribution
of our work is the examination of new types of
relationships, not the method by which such re-
lationships are incorporated into the classification
decision.
3.2 Classifying speech segments in isolation
In our experiments, we employed the well-known
classifier SVMlight to obtain individual-document
classification scores, treating Y as the positive
class and using plain unigrams as features.5 Fol-
lowing standard practice in sentiment analysis
(Pang et al, 2002), the input to SVMlight con-
sisted of normalized presence-of-feature (rather
than frequency-of-feature) vectors. The ind value
5SVMlight is available at svmlight.joachims.org. Default
parameters were used, although experimentation with differ-
ent parameter settings is an important direction for future
work (Daelemans and Hoste, 2002; Munson et al, 2005).
for each speech segment s was based on the signed
distance d(s) from the vector representing s to the
trained SVM decision plane:
ind(s,Y) def=
?
???
???
1 d(s) > 2?s;(
1 + d(s)2?s
)
/2 |d(s)| ? 2?s;
0 d(s) < ?2?s
where ?s is the standard deviation of d(s) over all
speech segments s in the debate in question, and
ind(s,N ) def= 1? ind(s,Y).
We now turn to the more interesting problem of
representing the preferences that speech segments
may have for being assigned to the same class.
3.3 Relationships between speech segments
A wide range of relationships between text seg-
ments can be modeled as positive-strength links.
Here we discuss two types of constraints that are
considered in this work.
Same-speaker constraints: In Congressional
debates and in general social-discourse contexts,
a single speaker may make a number of comments
regarding a topic. It is reasonable to expect that in
many settings, the participants in a discussion may
be convinced to change their opinions midway
through a debate. Hence, in the general case we
wish to be able to express ?soft? preferences for all
of an author?s statements to receive the same label,
where the strengths of such constraints could, for
instance, vary according to the time elapsed be-
tween the statements. Weighted links are an ap-
propriate means to express such variation.
However, if we assume that most speakers do
not change their positions in the course of a dis-
cussion, we can conclude that all comments made
by the same speaker must receive the same label.
This assumption holds by fiat for the ground-truth
labels in our dataset because these labels were
derived from the single vote cast by the speaker
on the bill being discussed.6 We can implement
this assumption via links whose weights are essen-
tially infinite. Although one can also implement
this assumption via concatenation of same-speaker
speech segments (see Section 4.3), we view the
fact that our graph-based framework incorporates
6We are attempting to determine whether a speech seg-
ment represents support or not. This differs from the problem
of determining what the speaker?s actual opinion is, a prob-
lem that, as an anonymous reviewer put it, is complicated by
?grandstanding, backroom deals, or, more innocently, plain
change of mind (?I voted for it before I voted against it?)?.
330
both hard and soft constraints in a principled fash-
ion as an advantage of our approach.
Different-speaker agreements In House dis-
course, it is common for one speaker to make ref-
erence to another in the context of an agreement
or disagreement over the topic of discussion. The
systematic identification of instances of agreement
can, as we have discussed, be a powerful tool for
the development of intelligently selected weights
for links between speech segments.
The problem of agreement identification can be
decomposed into two sub-problems: identifying
references and their targets, and deciding whether
each reference represents an instance of agree-
ment. In our case, the first task is straightfor-
ward because we focused solely on by-name ref-
erences.7 Hence, we will now concentrate on the
second, more interesting task.
We approach the problem of classifying refer-
ences by representing each reference with a word-
presence vector derived from a window of text
surrounding the reference.8 In the training set,
we classify each reference connecting two speak-
ers with a positive or negative label depending on
whether the two voted the same way on the bill un-
der discussion9. These labels are then used to train
an SVM classifier, the output of which is subse-
quently used to create weights on agreement links
in the test set as follows.
Let d(r) denote the distance from the vector
representing reference r to the agreement-detector
SVM?s decision plane, and let ?r be the standard
deviation of d(r) over all references in the debate
in question. We then define the strength agr of the
agreement link corresponding to the reference as:
agr(r) def=
?
??
??
0 d(r) < ?agr;
? ? d(r)/4?r ?agr ? d(r) ? 4?r;
? d(r) > 4?r.
The free parameter ? specifies the relative impor-
7One subtlety is that for the purposes of mining agree-
ment cues (but not for evaluating overall support/oppose
classification accuracy), we temporarily re-inserted into our
dataset previously filtered speech segments containing the
term ?yield?, since the yielding of time on the House floor
typically indicates agreement even though the yield state-
ments contain little relevant text on their own.
8We found good development-set performance using the
30 tokens before, 20 tokens after, and the name itself.
9Since we are concerned with references that potentially
represent relationships between speech segments, we ignore
references for which the target of the reference did not speak
in the debate in which the reference was made.
Agreement classifier
(?reference?agreement??)
Devel.
set
Test
set
majority baseline 81.51 80.26
Train: no amdmts; ?agr = 0 84.25 81.07
Train: with amdmts; ?agr = 0 86.99 80.10
Table 2: Agreement-classifier accuracy, in per-
cent. ?Amdmts?=?speech segments containing the
word ?amendment??. Recall that boldface indi-
cates results for development-set-optimal settings.
tance of the agr scores. The threshold ?agr con-
trols the precision of the agreement links, in that
values of ?agr greater than zero mean that greater
confidence is required before an agreement link
can be added.10
4 Evaluation
This section presents experiments testing the util-
ity of using speech-segment relationships, evalu-
ating against a number of baselines. All reported
results use values for the free parameter ? derived
via tuning on the development set. In the tables,
boldface indicates the development- and test-set
results for the development-set-optimal parameter
settings, as one would make algorithmic choices
based on development-set performance.
4.1 Preliminaries: Reference classification
Recall that to gather inter-speaker agreement in-
formation, the strategy employed in this paper is
to classify by-name references to other speakers
as to whether they indicate agreement or not.
To train our agreement classifier, we experi-
mented with undoing the deletion of amendment-
related speech segments in the training set. Note
that such speech segments were never included in
the development or test set, since, as discussed in
Section 2, their labels are probably noisy; how-
ever, including them in the training set alows the
classifier to examine more instances even though
some of them are labeled incorrectly. As Table
2 shows, using more, if noisy, data yields bet-
ter agreement-classification results on the devel-
opment set, and so we use that policy in all subse-
quent experiments.11
10Our implementation puts a link between just one arbi-
trary pair of speech segments among all those uttered by a
given pair of apparently agreeing speakers. The ?infinite-
weight? same-speaker links propagate the agreement infor-
mation to all other such pairs.
11Unfortunately, this policy leads to inferior test-set agree-
331
Agreement classifier Precision (in percent):
Devel. set Test set
?agr = 0 86.23 82.55
?agr = ? 89.41 88.47
Table 3: Agreement-classifier precision.
An important observation is that precision may
be more important than accuracy in deciding
which agreement links to add: false positives with
respect to agreement can cause speech segments
to be incorrectly assigned the same label, whereas
false negatives mean only that agreement-based
information about other speech segments is not
employed. As described above, we can raise
agreement precision by increasing the threshold
?agr, which specifies the required confidence for
the addition of an agreement link. Indeed, Table
3 shows that we can improve agreement precision
by setting ?agr to the (positive) mean agreement
score ? assigned by the SVM agreement-classifier
over all references in the given debate12. How-
ever, this comes at the cost of greatly reducing
agreement accuracy (development: 64.38%; test:
66.18%) due to lowered recall levels. Whether
or not better speech-segment classification is ulti-
mately achieved is discussed in the next sections.
4.2 Segment-based speech-segment
classification
Baselines The first two data rows of Table
4 depict baseline performance results. The
#(?support?) ? #(?oppos?) baseline is meant
to explore whether the speech-segment classifica-
tion task can be reduced to simple lexical checks.
Specifically, this method uses the signed differ-
ence between the number of words containing the
stem ?support? and the number of words contain-
ing the stem ?oppos? (returning the majority class
if the difference is 0). No better than 62.67% test-
set accuracy is obtained by either baseline.
Using relationship information Applying an
SVM to classify each speech segment in isolation
leads to clear improvements over the two base-
line methods, as demonstrated in Table 4. When
we impose the constraint that all speech segments
uttered by the same speaker receive the same la-
bel via ?same-speaker links?, both test-set and
ment classification. Section 4.5 contains further discussion.
12We elected not to explicitly tune the value of ?agr in or-
der to minimize the number of free parameters to deal with.
Support/oppose classifer
(?speech segment?yea??)
Devel.
set
Test
set
majority baseline 54.09 58.37
#(?support?)?#(?oppos?) 59.14 62.67
SVM [speech segment] 70.04 66.05
SVM + same-speaker links 79.77 67.21
SVM + same-speaker links . . .
+ agreement links, ?agr = 0 89.11 70.81
+ agreement links, ?agr = ? 87.94 71.16
Table 4: Segment-based speech-segment classifi-
cation accuracy, in percent.
Support/oppose classifer
(?speech segment?yea??)
Devel.
set
Test
set
SVM [speaker] 71.60 70.00
SVM + agreement links . . .
with ?agr = 0 88.72 71.28
with ?agr = ? 84.44 76.05
Table 5: Speaker-based speech-segment classifica-
tion accuracy, in percent. Here, the initial SVM is
run on the concatenation of all of a given speaker?s
speech segments, but the results are computed
over speech segments (not speakers), so that they
can be compared to those in Table 4.
development-set accuracy increase even more, in
the latter case quite substantially so.
The last two lines of Table 4 show that the
best results are obtained by incorporating agree-
ment information as well. The highest test-set re-
sult, 71.16%, is obtained by using a high-precision
threshold to determine which agreement links to
add. While the development-set results would in-
duce us to utilize the standard threshold value of 0,
which is sub-optimal on the test set, the ?agr = 0
agreement-link policy still achieves noticeable im-
provement over not using agreement links (test set:
70.81% vs. 67.21%).
4.3 Speaker-based speech-segment
classification
We use speech segments as the unit of classifica-
tion because they represent natural discourse units.
As a consequence, we are able to exploit relation-
ships at the speech-segment level. However, it is
interesting to consider whether we really need to
consider relationships specifically between speech
segments themselves, or whether it suffices to sim-
ply consider relationships between the speakers
332
of the speech segments. In particular, as an al-
ternative to using same-speaker links, we tried a
speaker-based approach wherein the way we de-
termine the initial individual-document classifica-
tion score for each speech segment uttered by a
person p in a given debate is to run an SVM on the
concatenation of all of p?s speech segments within
that debate. (We also ensure that agreement-link
information is propagated from speech-segment to
speaker pairs.)
How does the use of same-speaker links com-
pare to the concatenation of each speaker?s speech
segments? Tables 4 and 5 show that, not sur-
prisingly, the SVM individual-document classifier
works better on the concatenated speech segments
than on the speech segments in isolation. How-
ever, the effect on overall classification accuracy
is less clear: the development set favors same-
speaker links over concatenation, while the test set
does not.
But we stress that the most important obser-
vation we can make from Table 5 is that once
again, the addition of agreement information leads
to substantial improvements in accuracy.
4.4 ?Hard? agreement constraints
Recall that in in our experiments, we created
finite-weight agreement links, so that speech seg-
ments appearing in pairs flagged by our (imper-
fect) agreement detector can potentially receive
different labels. We also experimented with forc-
ing such speech segments to receive the same la-
bel, either through infinite-weight agreement links
or through a speech-segment concatenation strat-
egy similar to that described in the previous sub-
section. Both strategies resulted in clear degrada-
tion in performance on both the development and
test sets, a finding that validates our encoding of
agreement information as ?soft? preferences.
4.5 On the development/test set split
We have seen several cases in which the method
that performs best on the development set does
not yield the best test-set performance. However,
we felt that it would be illegitimate to change the
train/development/test sets in a post hoc fashion,
that is, after seeing the experimental results.
Moreover, and crucially, it is very clear that
using agreement information, encoded as prefer-
ences within our graph-based approach rather than
as hard constraints, yields substantial improve-
ments on both the development and test set; this,
we believe, is our most important finding.
5 Related work
Politically-oriented text Sentiment analysis has
specifically been proposed as a key enabling tech-
nology in eRulemaking, allowing the automatic
analysis of the opinions that people submit (Shul-
man et al, 2005; Cardie et al, 2006; Kwon et al,
2006). There has also been work focused upon de-
termining the political leaning (e.g., ?liberal? vs.
?conservative?) of a document or author, where
most previously-proposed methods make no di-
rect use of relationships between the documents to
be classified (the ?unlabeled? texts) (Laver et al,
2003; Efron, 2004; Mullen and Malouf, 2006). An
exception is Grefenstette et al (2004), who exper-
imented with determining the political orientation
of websites essentially by classifying the concate-
nation of all the documents found on that site.
Others have applied the NLP technologies of
near-duplicate detection and topic-based text cat-
egorization to politically oriented text (Yang and
Callan, 2005; Purpura and Hillard, 2006).
Detecting agreement We used a simple method
to learn to identify cross-speaker references indi-
cating agreement. More sophisticated approaches
have been proposed (Hillard et al, 2003), in-
cluding an extension that, in an interesting re-
versal of our problem, makes use of sentiment-
polarity indicators within speech segments (Gal-
ley et al, 2004). Also relevant is work on the gen-
eral problems of dialog-act tagging (Stolcke et al,
2000), citation analysis (Lehnert et al, 1990), and
computational rhetorical analysis (Marcu, 2000;
Teufel and Moens, 2002).
We currently do not have an efficient means
to encode disagreement information as hard con-
straints; we plan to investigate incorporating such
information in future work.
Relationships between the unlabeled items
Carvalho and Cohen (2005) consider sequential
relations between different types of emails (e.g.,
between requests and satisfactions thereof) to clas-
sify messages, and thus also explicitly exploit the
structure of conversations.
Previous sentiment-analysis work in different
domains has considered inter-document similar-
ity (Agarwal and Bhattacharyya, 2005; Pang and
Lee, 2005; Goldberg and Zhu, 2006) or explicit
333
inter-document references in the form of hyper-
links (Agrawal et al, 2003).
Notable early papers on graph-based semi-
supervised learning include Blum and Chawla
(2001), Bansal et al (2002), Kondor and Lafferty
(2002), and Joachims (2003). Zhu (2005) main-
tains a survey of this area.
Recently, several alternative, often quite sophis-
ticated approaches to collective classification have
been proposed (Neville and Jensen, 2000; Laf-
ferty et al, 2001; Getoor et al, 2002; Taskar et
al., 2002; Taskar et al, 2003; Taskar et al, 2004;
McCallum and Wellner, 2004). It would be inter-
esting to investigate the application of such meth-
ods to our problem. However, we also believe
that our approach has important advantages, in-
cluding conceptual simplicity and the fact that it is
based on an underlying optimization problem that
is provably and in practice easy to solve.
6 Conclusion and future work
In this study, we focused on very general types
of cross-document classification preferences, uti-
lizing constraints based only on speaker identity
and on direct textual references between state-
ments. We showed that the integration of even
very limited information regarding inter-document
relationships can significantly increase the accu-
racy of support/opposition classification.
The simple constraints modeled in our study,
however, represent just a small portion of the
rich network of relationships that connect state-
ments and speakers across the political universe
and in the wider realm of opinionated social dis-
course. One intriguing possibility is to take ad-
vantage of (readily identifiable) information re-
garding interpersonal relationships, making use of
speaker/author affiliations, positions within a so-
cial hierarchy, and so on. Or, we could even at-
tempt to model relationships between topics or
concepts, in a kind of extension of collaborative
filtering. For example, perhaps we could infer that
two speakers sharing a common opinion on evo-
lutionary biologist Richard Dawkins (a.k.a. ?Dar-
win?s rottweiler?) will be likely to agree in a de-
bate centered on Intelligent Design. While such
functionality is well beyond the scope of our cur-
rent study, we are optimistic that we can develop
methods to exploit additional types of relation-
ships in future work.
Acknowledgments We thank Claire Cardie, Jon
Kleinberg, Michael Macy, Andrew Myers, and the
six anonymous EMNLP referees for valuable dis-
cussions and comments. We also thank Reviewer
1 for generously providing additional post hoc
feedback, and the EMNLP chairs Eric Gaussier
and Dan Jurafsky for facilitating the process (as
well as for allowing authors an extra proceedings
page. . .). This paper is based upon work sup-
ported in part by the National Science Founda-
tion under grant no. IIS-0329064. Any opinions,
findings, and conclusions or recommendations ex-
pressed are those of the authors and do not neces-
sarily reflect the views or official policies, either
expressed or implied, of any sponsoring institu-
tions, the U.S. government, or any other entity.
References
A. Agarwal, P. Bhattacharyya. 2005. Sentiment anal-
ysis: A new approach for effective use of linguis-
tic knowledge and exploiting similarities in a set of
documents to be classified. In Proceedings of the
International Conference on Natural Language Pro-
cessing (ICON).
R. Agrawal, S. Rajagopalan, R. Srikant, Y. Xu. 2003.
Mining newsgroups using networks arising from so-
cial behavior. In Proceedings of WWW, 529?535.
N. Bansal, A. Blum, S. Chawla. 2002. Correla-
tion clustering. In Proceedings of the Symposium
on Foundations of Computer Science (FOCS), 238?
247. Journal version in Machine Learning Journal,
special issue on theoretical advances in data cluster-
ing, 56(1-3):89?113 (2004).
R. Barzilay, M. Lapata. 2005. Collective content selec-
tion for concept-to-text generation. In Proceedings
of HLT/EMNLP, 331?338.
A. Blum, S. Chawla. 2001. Learning from labeled and
unlabeled data using graph mincuts. In Proceedings
of ICML, 19?26.
C. Cardie, C. Farina, T. Bruce, E. Wagner. 2006. Us-
ing natural language processing to improve eRule-
making. In Proceedings of Digital Government Re-
search (dg.o).
V. Carvalho, W. W. Cohen. 2005. On the collective
classification of email ?speech acts?. In Proceedings
of SIGIR, 345?352.
W. Daelemans, V. Hoste. 2002. Evaluation of ma-
chine learning methods for natural language pro-
cessing tasks. In Proceedings of the Third Interna-
tional Conference on Language Resources and Eval-
uation (LREC), 755?760.
S. Das, M. Chen. 2001. Yahoo! for Amazon: Extract-
ing market sentiment from stock message boards. In
Proceedings of the Asia Pacific Finance Association
Annual Conference (APFA).
K. Dave, S. Lawrence, D. M. Pennock. 2003. Mining
the peanut gallery: Opinion extraction and semantic
classification of product reviews. In Proceedings of
WWW, 519?528.
334
M. Efron. 2004. Cultural orientation: Classifying sub-
jective documents by cociation [sic] analysis. In
Proceedings of the AAAI Fall Symposium on Style
and Meaning in Language, Art, Music, and Design,
41?48.
A. Esuli. 2006. Sentiment classification bibliography.
liinwww.ira.uka.de/bibliography/Misc/Sentiment.html.
M. Galley, K. McKeown, J. Hirschberg, E. Shriberg.
2004. Identifying agreement and disagreement in
conversational speech: Use of Bayesian networks to
model pragmatic dependencies. In Proceedings of
the 42nd ACL, 669?676.
L. Getoor, N. Friedman, D. Koller, B. Taskar. 2002.
Learning probabilistic models of relational structure.
Journal of Machine Learning Research, 3:679?707.
Special issue on the Eighteenth ICML.
A. B. Goldberg, J. Zhu. 2006. Seeing stars
when there aren?t many stars: Graph-based semi-
supervised learning for sentiment categorization.
In TextGraphs: HLT/NAACL Workshop on Graph-
based Algorithms for Natural Language Processing.
G. Grefenstette, Y. Qu, J. G. Shanahan, D. A. Evans.
2004. Coupling niche browsers and affect analysis
for an opinion mining application. In Proceedings
of RIAO.
M. Hearst. 1992. Direction-based text interpretation as
an information access refinement. In P. Jacobs, ed.,
Text-Based Intelligent Systems, 257?274. Lawrence
Erlbaum Associates.
D. Hillard, M. Ostendorf, E. Shriberg. 2003. Detection
of agreement vs. disagreement in meetings: Train-
ing with unlabeled data. In Proceedings of HLT-
NAACL.
T. Joachims. 2003. Transductive learning via spectral
graph partitioning. In Proceedings of ICML, 290?
297.
R. I. Kondor, J. D. Lafferty. 2002. Diffusion kernels
on graphs and other discrete input spaces. In Pro-
ceedings of ICML, 315?322.
N. Kwon, S. Shulman, E. Hovy. 2006. Multidimen-
sional text analysis for eRulemaking. In Proceed-
ings of Digital Government Research (dg.o).
J. Lafferty, A. McCallum, F. Pereira. 2001. Condi-
tional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, 282?289.
M. Laver, K. Benoit, J. Garry. 2003. Extracting policy
positions from political texts using words as data.
American Political Science Review.
W. Lehnert, C. Cardie, E. Riloff. 1990. Analyzing re-
search papers using citation sentences. In Program
of the Twelfth Annual Conference of the Cognitive
Science Society, 511?18.
D. Marcu. 2000. The theory and practice of discourse
parsing and summarization. MIT Press.
A. McCallum, B. Wellner. 2004. Conditional mod-
els of identity uncertainty with application to noun
coreference. In Proceedings of NIPS.
T. Mullen, R. Malouf. 2006. A preliminary investiga-
tion into sentiment analysis of informal political dis-
course. In Proceedings of the AAAI Symposium on
Computational Approaches to Analyzing Weblogs,
159?162.
A. Munson, C. Cardie, R. Caruana. 2005. Optimizing
to arbitrary NLP metrics using ensemble selection.
In Proceedings of HLT-EMNLP, 539?546.
J. Neville, D. Jensen. 2000. Iterative classification in
relational data. In Proceedings of the AAAI Work-
shop on Learning Statistical Models from Relational
Data, 13?20.
B. Pang, L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
271?278.
B. Pang, L. Lee. 2005. Seeing stars: Exploiting class
relationships for sentiment categorization with re-
spect to rating scales. In Proceedings of the ACL.
B. Pang, L. Lee, S. Vaithyanathan. 2002. Thumbs
up? Sentiment classification using machine learning
techniques. In Proceedings of EMNLP, 79?86.
S. Purpura, D. Hillard. 2006. Automated classifica-
tion of congressional legislation. In Proceedings of
Digital Government Research (dg.o).
W. Sack. 1994. On the computation of point of view.
In Proceedings of AAAI, pg. 1488. Student abstract.
S. Shulman, D. Schlosberg. 2002. Electronic rulemak-
ing: New frontiers in public participation. Prepared
for the Annual Meeting of the American Political
Science Association.
S. Shulman, J. Callan, E. Hovy, S. Zavestoski. 2005.
Language processing technologies for electronic
rulemaking: A project highlight. In Proceedings of
Digital Government Research (dg.o), 87?88.
S. S. Smith, J. M. Roberts, R. J. Vander Wielen. 2005.
The American Congress. Cambridge University
Press, fourth edition.
A. Stolcke, N. Coccaro, R. Bates, P. Taylor, C. Van Ess-
Dykema, K. Ries, E. Shriberg, D. Jurafsky, R. Mar-
tin, M. Meteer. 2000. Dialogue act modeling for
automatic tagging and recognition of conversational
speech. Computational Linguistics, 26(3):339?373.
B. Taskar, P. Abbeel, D. Koller. 2002. Discriminative
probabilistic models for relational data. In Proceed-
ings of UAI, Edmonton, Canada.
B. Taskar, C. Guestrin, D. Koller. 2003. Max-margin
Markov networks. In Proceedings of NIPS.
B. Taskar, V. Chatalbashev, D. Koller. 2004. Learn-
ing associative Markov networks. In Proceedings of
ICML.
S. Teufel, M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4):409?445.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proceedings of the ACL, 417?424.
J. M. Wiebe, W. J. Rapaport. 1988. A computational
theory of perspective and reference in narrative. In
Proceedings of the ACL, 131?138.
J. M. Wiebe. 1994. Tracking point of view in narrative.
Computational Linguistics, 20(2):233?287.
H. Yang, J. Callan. 2005. Near-duplicate detection
for eRulemaking. In Proceedings of Digital Gov-
ernment Research (dg.o).
J. Zhu. 2005. Semi-supervised learning literature
survey. Computer Sciences Technical Report TR
1530, University of Wisconsin-Madison. Available
at http://www.cs.wisc.edu/?jerryzhu/pub/ssl survey.pdf;
has been updated since the initial 2005 version.
335
Proceedings of the 43rd Annual Meeting of the ACL, pages 115?124,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Seeing stars: Exploiting class relationships for sentiment categorization with
respect to rating scales
Bo Pang    and Lillian Lee    
(1) Department of Computer Science, Cornell University
(2) Language Technologies Institute, Carnegie Mellon University
(3) Computer Science Department, Carnegie Mellon University
Abstract
We address the rating-inference problem,
wherein rather than simply decide whether
a review is ?thumbs up? or ?thumbs
down?, as in previous sentiment analy-
sis work, one must determine an author?s
evaluation with respect to a multi-point
scale (e.g., one to five ?stars?). This task
represents an interesting twist on stan-
dard multi-class text categorization be-
cause there are several different degrees
of similarity between class labels; for ex-
ample, ?three stars? is intuitively closer to
?four stars? than to ?one star?.
We first evaluate human performance at
the task. Then, we apply a meta-
algorithm, based on a metric labeling for-
mulation of the problem, that alters a
given  -ary classifier?s output in an ex-
plicit attempt to ensure that similar items
receive similar labels. We show that
the meta-algorithm can provide signifi-
cant improvements over both multi-class
and regression versions of SVMs when we
employ a novel similarity measure appro-
priate to the problem.
1 Introduction
There has recently been a dramatic surge of inter-
est in sentiment analysis, as more and more people
become aware of the scientific challenges posed and
the scope of new applications enabled by the pro-
cessing of subjective language. (The papers col-
lected by Qu, Shanahan, and Wiebe (2004) form a
representative sample of research in the area.) Most
prior work on the specific problem of categorizing
expressly opinionated text has focused on the bi-
nary distinction of positive vs. negative (Turney,
2002; Pang, Lee, and Vaithyanathan, 2002; Dave,
Lawrence, and Pennock, 2003; Yu and Hatzivas-
siloglou, 2003). But it is often helpful to have more
information than this binary distinction provides, es-
pecially if one is ranking items by recommendation
or comparing several reviewers? opinions: example
applications include collaborative filtering and de-
ciding which conference submissions to accept.
Therefore, in this paper we consider generalizing
to finer-grained scales: rather than just determine
whether a review is ?thumbs up? or not, we attempt
to infer the author?s implied numerical rating, such
as ?three stars? or ?four stars?. Note that this differs
from identifying opinion strength (Wilson, Wiebe,
and Hwa, 2004): rants and raves have the same
strength but represent opposite evaluations, and ref-
eree forms often allow one to indicate that one is
very confident (high strength) that a conference sub-
mission is mediocre (middling rating). Also, our
task differs from ranking not only because one can
be given a single item to classify (as opposed to a
set of items to be ordered relative to one another),
but because there are settings in which classification
is harder than ranking, and vice versa.
One can apply standard  -ary classifiers or regres-
sion to this rating-inference problem; independent
work by Koppel and Schler (2005) considers such
115
methods. But an alternative approach that explic-
itly incorporates information about item similarities
together with label similarity information (for in-
stance, ?one star? is closer to ?two stars? than to
?four stars?) is to think of the task as one of met-
ric labeling (Kleinberg and Tardos, 2002), where
label relations are encoded via a distance metric.
This observation yields a meta-algorithm, applicable
to both semi-supervised (via graph-theoretic tech-
niques) and supervised settings, that alters a given
 -ary classifier?s output so that similar items tend to
be assigned similar labels.
In what follows, we first demonstrate that hu-
mans can discern relatively small differences in (hid-
den) evaluation scores, indicating that rating infer-
ence is indeed a meaningful task. We then present
three types of algorithms ? one-vs-all, regression,
and metric labeling ? that can be distinguished by
how explicitly they attempt to leverage similarity
between items and between labels. Next, we con-
sider what item similarity measure to apply, propos-
ing one based on the positive-sentence percentage.
Incorporating this new measure within the metric-
labeling framework is shown to often provide sig-
nificant improvements over the other algorithms.
We hope that some of the insights derived here
might apply to other scales for text classifcation that
have been considered, such as clause-level opin-
ion strength (Wilson, Wiebe, and Hwa, 2004); af-
fect types like disgust (Subasic and Huettner, 2001;
Liu, Lieberman, and Selker, 2003); reading level
(Collins-Thompson and Callan, 2004); and urgency
or criticality (Horvitz, Jacobs, and Hovel, 1999).
2 Problem validation and formulation
We first ran a small pilot study on human subjects
in order to establish a rough idea of what a reason-
able classification granularity is: if even people can-
not accurately infer labels with respect to a five-star
scheme with half stars, say, then we cannot expect a
learning algorithm to do so. Indeed, some potential
obstacles to accurate rating inference include lack
of calibration (e.g., what an understated author in-
tends as high praise may seem lukewarm), author
inconsistency at assigning fine-grained ratings, and
Rating diff. Pooled Subject 1 Subject 2
 or more 100% 100% (35) 100% (15)
2 (e.g., 1 star) 83% 77% (30) 100% (11)
1 (e.g.,   star) 69% 65% (57) 90% (10)
0 55% 47% (15) 80% ( 5)
Table 1: Human accuracy at determining relative
positivity. Rating differences are given in ?notches?.
Parentheses enclose the number of pairs attempted.
ratings not entirely supported by the text1.
For data, we first collected Internet movie reviews
in English from four authors, removing explicit rat-
ing indicators from each document?s text automati-
cally. Now, while the obvious experiment would be
to ask subjects to guess the rating that a review rep-
resents, doing so would force us to specify a fixed
rating-scale granularity in advance. Instead, we ex-
amined people?s ability to discern relative differ-
ences, because by varying the rating differences rep-
resented by the test instances, we can evaluate mul-
tiple granularities in a single experiment. Specifi-
cally, at intervals over a number of weeks, we au-
thors (a non-native and a native speaker of English)
examined pairs of reviews, attemping to determine
whether the first review in each pair was (1) more
positive than, (2) less positive than, or (3) as posi-
tive as the second. The texts in any particular review
pair were taken from the same author to factor out
the effects of cross-author divergence.
As Table 1 shows, both subjects performed per-
fectly when the rating separation was at least 3
?notches? in the original scale (we define a notch
as a half star in a four- or five-star scheme and 10
points in a 100-point scheme). Interestingly, al-
though human performance drops as rating differ-
ence decreases, even at a one-notch separation, both
subjects handily outperformed the random-choice
baseline of 33%. However, there was large variation
in accuracy between subjects.2
1For example, the critic Dennis Schwartz writes that ?some-
times the review itself [indicates] the letter grade should have
been higher or lower, as the review might fail to take into con-
sideration my overall impression of the film ? which I hope to
capture in the grade? (http://www.sover.net/?ozus/cinema.htm).
2One contributing factor may be that the subjects viewed
disjoint document sets, since we wanted to maximize experi-
mental coverage of the types of document pairs within each dif-
ference class. We thus cannot report inter-annotator agreement,
116
Because of this variation, we defined two differ-
ent classification regimes. From the evidence above,
a three-class task (categories 0, 1, and 2 ? es-
sentially ?negative?, ?middling?, and ?positive?, re-
spectively) seems like one that most people would
do quite well at (but we should not assume 100%
human accuracy: according to our one-notch re-
sults, people may misclassify borderline cases like
2.5 stars). Our study also suggests that people could
do at least fairly well at distinguishing full stars in
a zero- to four-star scheme. However, when we
began to construct five-category datasets for each
of our four authors (see below), we found that in
each case, either the most negative or the most pos-
itive class (but not both) contained only about 5%
of the documents. To make the classes more bal-
anced, we folded these minority classes into the ad-
jacent class, thus arriving at a four-class problem
(categories 0-3, increasing in positivity). Note that
the four-class problem seems to offer more possi-
bilities for leveraging class relationship information
than the three-class setting, since it involves more
class pairs. Also, even the two-category version of
the rating-inference problem for movie reviews has
proven quite challenging for many automated clas-
sification techniques (Pang, Lee, and Vaithyanathan,
2002; Turney, 2002).
We applied the above two labeling schemes to
a scale dataset3 containing four corpora of movie
reviews. All reviews were automatically pre-
processed to remove both explicit rating indicators
and objective sentences; the motivation for the latter
step is that it has previously aided positive vs. neg-
ative classification (Pang and Lee, 2004). All of the
1770, 902, 1307, or 1027 documents in a given cor-
pus were written by the same author. This decision
facilitates interpretation of the results, since it fac-
tors out the effects of different choices of methods
for calibrating authors? scales.4 We point out that
but since our goal is to recover a reviewer?s ?true? recommen-
dation, reader-author agreement is more relevant.
While another factor might be degree of English fluency, in
an informal experiment (six subjects viewing the same three
pairs), native English speakers made the only two errors.
3Available at http://www.cs.cornell.edu/People/pabo/movie-
review-data as scale dataset v1.0.
4From the Rotten Tomatoes website?s FAQ: ?star systems
are not consistent between critics. For critics like Roger Ebert
and James Berardinelli, 2.5 stars or lower out of 4 stars is al-
ways negative. For other critics, 2.5 stars can either be positive
it is possible to gather author-specific information
in some practical applications: for instance, systems
that use selected authors (e.g., the Rotten Tomatoes
movie-review website ? where, we note, not all
authors provide explicit ratings) could require that
someone submit rating-labeled samples of newly-
admitted authors? work. Moreover, our results at
least partially generalize to mixed-author situations
(see Section 5.2).
3 Algorithms
Recall that the problem we are considering is multi-
category classification in which the labels can be
naturally mapped to a metric space (e.g., points on a
line); for simplicity, we assume the distance metric

	 A Sentimental Education: Sentiment Analysis Using Subjectivity
Summarization Based on Minimum Cuts
Bo Pang and Lillian Lee
Department of Computer Science
Cornell University
Ithaca, NY 14853-7501
{pabo,llee}@cs.cornell.edu
Abstract
Sentiment analysis seeks to identify the view-
point(s) underlying a text span; an example appli-
cation is classifying a movie review as ?thumbs up?
or ?thumbs down?. To determine this sentiment po-
larity, we propose a novel machine-learning method
that applies text-categorization techniques to just
the subjective portions of the document. Extracting
these portions can be implemented using efficient
techniques for finding minimum cuts in graphs; this
greatly facilitates incorporation of cross-sentence
contextual constraints.
1 Introduction
The computational treatment of opinion, sentiment,
and subjectivity has recently attracted a great deal
of attention (see references), in part because of its
potential applications. For instance, information-
extraction and question-answering systems could
flag statements and queries regarding opinions
rather than facts (Cardie et al, 2003). Also, it
has proven useful for companies, recommender sys-
tems, and editorial sites to create summaries of peo-
ple?s experiences and opinions that consist of sub-
jective expressions extracted from reviews (as is
commonly done in movie ads) or even just a re-
view?s polarity ? positive (?thumbs up?) or neg-
ative (?thumbs down?).
Document polarity classification poses a signifi-
cant challenge to data-driven methods, resisting tra-
ditional text-categorization techniques (Pang, Lee,
and Vaithyanathan, 2002). Previous approaches fo-
cused on selecting indicative lexical features (e.g.,
the word ?good?), classifying a document accord-
ing to the number of such features that occur any-
where within it. In contrast, we propose the follow-
ing process: (1) label the sentences in the document
as either subjective or objective, discarding the lat-
ter; and then (2) apply a standard machine-learning
classifier to the resulting extract. This can prevent
the polarity classifier from considering irrelevant or
even potentially misleading text: for example, al-
though the sentence ?The protagonist tries to pro-
tect her good name? contains the word ?good?, it
tells us nothing about the author?s opinion and in
fact could well be embedded in a negative movie
review. Also, as mentioned above, subjectivity ex-
tracts can be provided to users as a summary of the
sentiment-oriented content of the document.
Our results show that the subjectivity extracts
we create accurately represent the sentiment in-
formation of the originating documents in a much
more compact form: depending on choice of down-
stream polarity classifier, we can achieve highly sta-
tistically significant improvement (from 82.8% to
86.4%) or maintain the same level of performance
for the polarity classification task while retaining
only 60% of the reviews? words. Also, we ex-
plore extraction methods based on a minimum cut
formulation, which provides an efficient, intuitive,
and effective means for integrating inter-sentence-
level contextual information with traditional bag-of-
words features.
2 Method
2.1 Architecture
One can consider document-level polarity classi-
fication to be just a special (more difficult) case
of text categorization with sentiment- rather than
topic-based categories. Hence, standard machine-
learning classification techniques, such as support
vector machines (SVMs), can be applied to the en-
tire documents themselves, as was done by Pang,
Lee, and Vaithyanathan (2002). We refer to such
classification techniques as default polarity classi-
fiers.
However, as noted above, we may be able to im-
prove polarity classification by removing objective
sentences (such as plot summaries in a movie re-
view). We therefore propose, as depicted in Figure
1, to first employ a subjectivity detector that deter-
mines whether each sentence is subjective or not:
discarding the objective ones creates an extract that
should better represent a review?s subjective content
to a default polarity classifier.
s1
s2
s3
s4
s_n
+/?s4
s1
subje
ctivity
dete
ctor
yes
no
no
yes
n?sentence review subjectivesentence? m?sentence extract(m<=n) review?positive or negative
defa
ult
class
ifier
pola
rity
subjectivity extraction
Figure 1: Polarity classification via subjectivity detec-
tion.
To our knowledge, previous work has not in-
tegrated sentence-level subjectivity detection with
document-level sentiment polarity. Yu and Hatzi-
vassiloglou (2003) provide methods for sentence-
level analysis and for determining whether a doc-
ument is subjective or not, but do not combine these
two types of algorithms or consider document polar-
ity classification. The motivation behind the single-
sentence selection method of Beineke et al (2004)
is to reveal a document?s sentiment polarity, but they
do not evaluate the polarity-classification accuracy
that results.
2.2 Context and Subjectivity Detection
As with document-level polarity classification, we
could perform subjectivity detection on individual
sentences by applying a standard classification algo-
rithm on each sentence in isolation. However, mod-
eling proximity relationships between sentences
would enable us to leverage coherence: text spans
occurring near each other (within discourse bound-
aries) may share the same subjectivity status, other
things being equal (Wiebe, 1994).
We would therefore like to supply our algorithms
with pair-wise interaction information, e.g., to spec-
ify that two particular sentences should ideally re-
ceive the same subjectivity label but not state which
label this should be. Incorporating such informa-
tion is somewhat unnatural for classifiers whose in-
put consists simply of individual feature vectors,
such as Naive Bayes or SVMs, precisely because
such classifiers label each test item in isolation.
One could define synthetic features or feature vec-
tors to attempt to overcome this obstacle. However,
we propose an alternative that avoids the need for
such feature engineering: we use an efficient and
intuitive graph-based formulation relying on find-
ing minimum cuts. Our approach is inspired by
Blum and Chawla (2001), although they focused on
similarity between items (the motivation being to
combine labeled and unlabeled data), whereas we
are concerned with physical proximity between the
items to be classified; indeed, in computer vision,
modeling proximity information via graph cuts has
led to very effective classification (Boykov, Veksler,
and Zabih, 1999).
2.3 Cut-based classification
Figure 2 shows a worked example of the concepts
in this section.
Suppose we have n items x1, . . . , xn to divide
into two classes C1 and C2, and we have access to
two types of information:
? Individual scores indj(xi): non-negative esti-
mates of each xi?s preference for being in Cj based
on just the features of xi alone; and
? Association scores assoc(xi, xk): non-negative
estimates of how important it is that xi and xk be in
the same class.1
We would like to maximize each item?s ?net hap-
piness?: its individual score for the class it is as-
signed to, minus its individual score for the other
class. But, we also want to penalize putting tightly-
associated items into different classes. Thus, after
some algebra, we arrive at the following optimiza-
tion problem: assign the xis to C1 and C2 so as to
minimize the partition cost
?
x?C1
ind2(x)+
?
x?C2
ind1(x)+
?
xi?C1,
xk?C2
assoc(xi, xk).
The problem appears intractable, since there are
2n possible binary partitions of the xi?s. How-
ever, suppose we represent the situation in the fol-
lowing manner. Build an undirected graph G with
vertices {v1, . . . , vn, s, t}; the last two are, respec-
tively, the source and sink. Add n edges (s, vi), each
with weight ind1(xi), and n edges (vi, t), each with
weight ind2(xi). Finally, add
(n
2
)
edges (vi, vk),
each with weight assoc(xi, xk). Then, cuts in G
are defined as follows:
Definition 1 A cut (S, T ) of G is a partition of its
nodes into sets S = {s} ? S? and T = {t} ? T ?,
where s 6? S?, t 6? T ?. Its cost cost(S, T ) is the sum
of the weights of all edges crossing from S to T . A
minimum cut of G is one of minimum cost.
1Asymmetry is allowed, but we used symmetric scores.
[]
s t
Y
M
N
2ind (Y) [.2]1ind (Y) [.8]
2ind (M) [.5]1ind (M) [.5]
[.1]assoc(Y,N)
2ind (N) [.9]1ind (N)
assoc(M,N)
assoc(Y,M)
[.2]
[1.0]
[.1]
C1 Individual Association Cost
penalties penalties
{Y,M} .2 + .5 + .1 .1 + .2 1.1
(none) .8 + .5 + .1 0 1.4
{Y,M,N} .2 + .5 + .9 0 1.6
{Y} .2 + .5 + .1 1.0 + .1 1.9
{N} .8 + .5 + .9 .1 + .2 2.5
{M} .8 + .5 + .1 1.0 + .2 2.6
{Y,N} .2 + .5 + .9 1.0 + .2 2.8
{M,N} .8 + .5 + .9 1.0 + .1 3.3
Figure 2: Graph for classifying three items. Brackets enclose example values; here, the individual scores happen to
be probabilities. Based on individual scores alone, we would put Y (?yes?) in C1, N (?no?) in C2, and be undecided
about M (?maybe?). But the association scores favor cuts that put Y and M in the same class, as shown in the table.
Thus, the minimum cut, indicated by the dashed line, places M together with Y in C1.
Observe that every cut corresponds to a partition of
the items and has cost equal to the partition cost.
Thus, our optimization problem reduces to finding
minimum cuts.
Practical advantages As we have noted, formulat-
ing our subjectivity-detection problem in terms of
graphs allows us to model item-specific and pair-
wise information independently. Note that this is
a very flexible paradigm. For instance, it is per-
fectly legitimate to use knowledge-rich algorithms
employing deep linguistic knowledge about sen-
timent indicators to derive the individual scores.
And we could also simultaneously use knowledge-
lean methods to assign the association scores. In-
terestingly, Yu and Hatzivassiloglou (2003) com-
pared an individual-preference classifier against a
relationship-based method, but didn?t combine the
two; the ability to coordinate such algorithms is
precisely one of the strengths of our approach.
But a crucial advantage specific to the utilization
of a minimum-cut-based approach is that we can use
maximum-flow algorithms with polynomial asymp-
totic running times ? and near-linear running times
in practice ? to exactly compute the minimum-
cost cut(s), despite the apparent intractability of
the optimization problem (Cormen, Leiserson, and
Rivest, 1990; Ahuja, Magnanti, and Orlin, 1993).2
In contrast, other graph-partitioning problems that
have been previously used to formulate NLP clas-
sification problems3 are NP-complete (Hatzivassi-
loglou and McKeown, 1997; Agrawal et al, 2003;
Joachims, 2003).
2Code available at http://www.avglab.com/andrew/soft.html.
3Graph-based approaches to general clustering problems
are too numerous to mention here.
3 Evaluation Framework
Our experiments involve classifying movie reviews
as either positive or negative, an appealing task for
several reasons. First, as mentioned in the intro-
duction, providing polarity information about re-
views is a useful service: witness the popularity of
www.rottentomatoes.com. Second, movie reviews
are apparently harder to classify than reviews of
other products (Turney, 2002; Dave, Lawrence, and
Pennock, 2003). Third, the correct label can be ex-
tracted automatically from rating information (e.g.,
number of stars). Our data4 contains 1000 positive
and 1000 negative reviews all written before 2002,
with a cap of 20 reviews per author (312 authors
total) per category. We refer to this corpus as the
polarity dataset.
Default polarity classifiers We tested support vec-
tor machines (SVMs) and Naive Bayes (NB). Fol-
lowing Pang et al (2002), we use unigram-presence
features: the ith coordinate of a feature vector is
1 if the corresponding unigram occurs in the input
text, 0 otherwise. (For SVMs, the feature vectors
are length-normalized). Each default document-
level polarity classifier is trained and tested on the
extracts formed by applying one of the sentence-
level subjectivity detectors to reviews in the polarity
dataset.
Subjectivity dataset To train our detectors, we
need a collection of labeled sentences. Riloff and
Wiebe (2003) state that ?It is [very hard] to ob-
tain collections of individual sentences that can be
easily identified as subjective or objective?; the
polarity-dataset sentences, for example, have not
4Available at www.cs.cornell.edu/people/pabo/movie-
review-data/ (review corpus version 2.0).
been so annotated.5 Fortunately, we were able
to mine the Web to create a large, automatically-
labeled sentence corpus6. To gather subjective
sentences (or phrases), we collected 5000 movie-
review snippets (e.g., ?bold, imaginative, and im-
possible to resist?) from www.rottentomatoes.com.
To obtain (mostly) objective data, we took 5000 sen-
tences from plot summaries available from the In-
ternet Movie Database (www.imdb.com). We only
selected sentences or snippets at least ten words
long and drawn from reviews or plot summaries of
movies released post-2001, which prevents overlap
with the polarity dataset.
Subjectivity detectors As noted above, we can use
our default polarity classifiers as ?basic? sentence-
level subjectivity detectors (after retraining on the
subjectivity dataset) to produce extracts of the orig-
inal reviews. We also create a family of cut-based
subjectivity detectors; these take as input the set of
sentences appearing in a single document and de-
termine the subjectivity status of all the sentences
simultaneously using per-item and pairwise rela-
tionship information. Specifically, for a given doc-
ument, we use the construction in Section 2.2 to
build a graph wherein the source s and sink t cor-
respond to the class of subjective and objective sen-
tences, respectively, and each internal node vi cor-
responds to the document?s ith sentence si. We can
set the individual scores ind1(si) to PrNBsub (si) and
ind2(si) to 1 ? PrNBsub (si), as shown in Figure 3,
where PrNBsub (s) denotes Naive Bayes? estimate of
the probability that sentence s is subjective; or, we
can use the weights produced by the SVM classi-
fier instead.7 If we set al the association scores
to zero, then the minimum-cut classification of the
sentences is the same as that of the basic subjectiv-
ity detector. Alternatively, we incorporate the de-
gree of proximity between pairs of sentences, con-
trolled by three parameters. The threshold T spec-
ifies the maximum distance two sentences can be
separated by and still be considered proximal. The
5We therefore could not directly evaluate sentence-
classification accuracy on the polarity dataset.
6Available at www.cs.cornell.edu/people/pabo/movie-
review-data/ , sentence corpus version 1.0.
7We converted SVM output di, which is a signed distance
(negative=objective) from the separating hyperplane, to non-
negative numbers by
ind1(si)
def
=
{
1 di > 2;
(2 + di)/4 ?2 ? di ? 2;
0 di < ?2.
and ind2(si) = 1 ? ind1(si). Note that scaling is employed
only for consistency; the algorithm itself does not require prob-
abilities for individual scores.
non-increasing function f(d) specifies how the in-
fluence of proximal sentences decays with respect to
distance d; in our experiments, we tried f(d) = 1,
e1?d, and 1/d2. The constant c controls the relative
influence of the association scores: a larger c makes
the minimum-cut algorithm more loath to put prox-
imal sentences in different classes. With these in
hand8, we set (for j > i)
assoc(si, sj)
def
=
{
f(j ? i) ? c if (j ? i) ? T ;
0 otherwise.
4 Experimental Results
Below, we report average accuracies computed by
ten-fold cross-validation over the polarity dataset.
Section 4.1 examines our basic subjectivity extrac-
tion algorithms, which are based on individual-
sentence predictions alone. Section 4.2 evaluates
the more sophisticated form of subjectivity extrac-
tion that incorporates context information via the
minimum-cut paradigm.
As we will see, the use of subjectivity extracts
can in the best case provide satisfying improve-
ment in polarity classification, and otherwise can
at least yield polarity-classification accuracies indis-
tinguishable from employing the full review. At the
same time, the extracts we create are both smaller
on average than the original document and more
effective as input to a default polarity classifier
than the same-length counterparts produced by stan-
dard summarization tactics (e.g., first- or last-N sen-
tences). We therefore conclude that subjectivity ex-
traction produces effective summaries of document
sentiment.
4.1 Basic subjectivity extraction
As noted in Section 3, both Naive Bayes and SVMs
can be trained on our subjectivity dataset and then
used as a basic subjectivity detector. The former has
somewhat better average ten-fold cross-validation
performance on the subjectivity dataset (92% vs.
90%), and so for space reasons, our initial discus-
sions will focus on the results attained via NB sub-
jectivity detection.
Employing Naive Bayes as a subjectivity detec-
tor (ExtractNB) in conjunction with a Naive Bayes
document-level polarity classifier achieves 86.4%
accuracy.9 This is a clear improvement over the
82.8% that results when no extraction is applied
8Parameter training is driven by optimizing the performance
of the downstream polarity classifier rather than the detector
itself because the subjectivity dataset?s sentences come from
different reviews, and so are never proximal.
9This result and others are depicted in Figure 5; for now,
consider only the y-axis in those plots.
.
.
.
.
.
.
subsub
NB NBs1
s2
s3
s4
s_n
     
    
constructgraph computemin. cut extractcreate s1
s4
m?sentence extract(m<=n)


 
 


 
 
			
			

 


 

n?sentence review
v1
v2
s
v3
edge crossing the cut
v2
v3
v1
ts
v n
t
v n
proximity link
individual subjectivity?probability linkPr
1?Pr   (s1)Pr   (s1)

    
Figure 3: Graph-cut-based creation of subjective extracts.
(Full review); indeed, the difference is highly sta-
tistically significant (p < 0.01, paired t-test). With
SVMs as the polarity classifier instead, the Full re-
view performance rises to 87.15%, but comparison
via the paired t-test reveals that this is statistically
indistinguishable from the 86.4% that is achieved by
running the SVM polarity classifier on ExtractNB
input. (More improvements to extraction perfor-
mance are reported later in this section.)
These findings indicate10 that the extracts pre-
serve (and, in the NB polarity-classifier case, appar-
ently clarify) the sentiment information in the orig-
inating documents, and thus are good summaries
from the polarity-classification point of view. Fur-
ther support comes from a ?flipping? experiment:
if we give as input to the default polarity classifier
an extract consisting of the sentences labeled ob-
jective, accuracy drops dramatically to 71% for NB
and 67% for SVMs. This confirms our hypothesis
that sentences discarded by the subjectivity extrac-
tion process are indeed much less indicative of sen-
timent polarity.
Moreover, the subjectivity extracts are much
more compact than the original documents (an im-
portant feature for a summary to have): they contain
on average only about 60% of the source reviews?
words. (This word preservation rate is plotted along
the x-axis in the graphs in Figure 5.) This prompts
us to study how much reduction of the original doc-
uments subjectivity detectors can perform and still
accurately represent the texts? sentiment informa-
tion.
We can create subjectivity extracts of varying
lengths by taking just the N most subjective sen-
tences11 from the originating review. As one base-
10Recall that direct evidence is not available because the po-
larity dataset?s sentences lack subjectivity labels.
11These are the N sentences assigned the highest probability
by the basic NB detector, regardless of whether their probabil-
line to compare against, we take the canonical sum-
marization standard of extracting the first N sen-
tences ? in general settings, authors often be-
gin documents with an overview. We also con-
sider the last N sentences: in many documents,
concluding material may be a good summary, and
www.rottentomatoes.com tends to select ?snippets?
from the end of movie reviews (Beineke et al,
2004). Finally, as a sanity check, we include results
from the N least subjective sentences according to
Naive Bayes.
Figure 4 shows the polarity classifier results as
N ranges between 1 and 40. Our first observation
is that the NB detector provides very good ?bang
for the buck?: with subjectivity extracts containing
as few as 15 sentences, accuracy is quite close to
what one gets if the entire review is used. In fact,
for the NB polarity classifier, just using the 5 most
subjective sentences is almost as informative as the
Full review while containing on average only about
22% of the source reviews? words.
Also, it so happens that at N = 30, performance
is actually slightly better than (but statistically in-
distinguishable from) Full review even when the
SVM default polarity classifier is used (87.2% vs.
87.15%).12 This suggests potentially effective ex-
traction alternatives other than using a fixed proba-
bility threshold (which resulted in the lower accu-
racy of 86.4% reported above).
Furthermore, we see in Figure 4 that the N most-
subjective-sentences method generally outperforms
the other baseline summarization methods (which
perhaps suggests that sentiment summarization can-
not be treated the same as topic-based summariza-
ities exceed 50% and so would actually be classified as subjec-
tive by Naive Bayes. For reviews with fewer than N sentences,
the entire review will be returned.
12Note that roughly half of the documents in the polarity
dataset contain more than 30 sentences (average=32.3, standard
deviation 15).
 55
 60
 65
 70
 75
 80
 85
 90
 1  5  10  15  20  25  30  35  40
Av
era
ge
 ac
cur
acy
N
Accuracy for N-sentence abstracts (def =  NB)
most subjective N sentenceslast N sentencesfirst N sentencesleast subjective N sentencesFull review
 55
 60
 65
 70
 75
 80
 85
 90
 1  5  10  15  20  25  30  35  40
Av
era
ge
 ac
cur
acy
N
Accuracy for N-sentence abstracts (def = SVM)
most subjective N sentenceslast N sentencesfirst N sentencesleast subjective N sentencesFull review
Figure 4: Accuracies using N-sentence extracts for NB (left) and SVM (right) default polarity classifiers.
 83
 83.5
 84
 84.5
 85
 85.5
 86
 86.5
 87
 0.6  0.7  0.8  0.9  1  1.1
Av
era
ge
 ac
cur
acy
% of words extracted
Accuracy for subjective abstracts (def = NB)
difference in accuracy 
ExtractSVM+Prox
ExtractNB+ProxExtractNB
ExtractSVM
not statistically significant
Full Review
indicates statistically significant
improvement in accuracy
 83
 83.5
 84
 84.5
 85
 85.5
 86
 86.5
 87
 0.6  0.7  0.8  0.9  1  1.1
Av
era
ge
 ac
cur
acy
% of words extracted
Accuracy for subjective abstracts (def = SVM)
difference in accuracy ExtractNB+Prox
ExtractSVM+Prox
ExtractSVM
ExtractNB not statistically significant
Full Review
improvement in accuracy
indicates statistically significant
Figure 5: Word preservation rate vs. accuracy, NB (left) and SVMs (right) as default polarity classifiers.
Also indicated are results for some statistical significance tests.
tion, although this conjecture would need to be veri-
fied on other domains and data). It?s also interesting
to observe how much better the last N sentences are
than the first N sentences; this may reflect a (hardly
surprising) tendency for movie-review authors to
place plot descriptions at the beginning rather than
the end of the text and conclude with overtly opin-
ionated statements.
4.2 Incorporating context information
The previous section demonstrated the value of
subjectivity detection. We now examine whether
context information, particularly regarding sentence
proximity, can further improve subjectivity extrac-
tion. As discussed in Section 2.2 and 3, con-
textual constraints are easily incorporated via the
minimum-cut formalism but are not natural inputs
for standard Naive Bayes and SVMs.
Figure 5 shows the effect of adding in
proximity information. ExtractNB+Prox and
ExtractSVM+Prox are the graph-based subjectivity
detectors using Naive Bayes and SVMs, respec-
tively, for the individual scores; we depict the
best performance achieved by a single setting of
the three proximity-related edge-weight parameters
over all ten data folds13 (parameter selection was
not a focus of the current work). The two compar-
isons we are most interested in are ExtractNB+Prox
versus ExtractNB and ExtractSVM+Prox versus
ExtractSVM.
We see that the context-aware graph-based sub-
jectivity detectors tend to create extracts that are
more informative (statistically significant so (paired
t-test) for SVM subjectivity detectors only), al-
though these extracts are longer than their context-
blind counterparts. We note that the performance
13Parameters are chosen from T ? {1, 2, 3}, f(d) ?
{1, e1?d, 1/d2}, and c ? [0, 1] at intervals of 0.1.
enhancements cannot be attributed entirely to the
mere inclusion of more sentences regardless of
whether they are subjective or not ? one counter-
argument is that Full review yielded substantially
worse results for the NB default polarity classifier?
and at any rate, the graph-derived extracts are still
substantially more concise than the full texts.
Now, while incorporating a bias for assigning
nearby sentences to the same category into NB and
SVM subjectivity detectors seems to require some
non-obvious feature engineering, we also wish
to investigate whether our graph-based paradigm
makes better use of contextual constraints that can
be (more or less) easily encoded into the input of
standard classifiers. For illustrative purposes, we
consider paragraph-boundary information, looking
only at SVM subjectivity detection for simplicity?s
sake.
It seems intuitively plausible that paragraph
boundaries (an approximation to discourse bound-
aries) loosen coherence constraints between nearby
sentences. To capture this notion for minimum-cut-
based classification, we can simply reduce the as-
sociation scores for all pairs of sentences that oc-
cur in different paragraphs by multiplying them by
a cross-paragraph-boundary weight w ? [0, 1]. For
standard classifiers, we can employ the trick of hav-
ing the detector treat paragraphs, rather than sen-
tences, as the basic unit to be labeled. This en-
ables the standard classifier to utilize coherence be-
tween sentences in the same paragraph; on the other
hand, it also (probably unavoidably) poses a hard
constraint that all of a paragraph?s sentences get the
same label, which increases noise sensitivity.14 Our
experiments reveal the graph-cut formulation to be
the better approach: for both default polarity clas-
sifiers (NB and SVM), some choice of parameters
(including w) for ExtractSVM+Prox yields statisti-
cally significant improvement over its paragraph-
unit non-graph counterpart (NB: 86.4% vs. 85.2%;
SVM: 86.15% vs. 85.45%).
5 Conclusions
We examined the relation between subjectivity de-
tection and polarity classification, showing that sub-
jectivity detection can compress reviews into much
shorter extracts that still retain polarity information
at a level comparable to that of the full review. In
fact, for the Naive Bayes polarity classifier, the sub-
jectivity extracts are shown to be more effective in-
put than the originating document, which suggests
14For example, in the data we used, boundaries may have
been missed due to malformed html.
that they are not only shorter, but also ?cleaner? rep-
resentations of the intended polarity.
We have also shown that employing the
minimum-cut framework results in the develop-
ment of efficient algorithms for sentiment analy-
sis. Utilizing contextual information via this frame-
work can lead to statistically significant improve-
ment in polarity-classification accuracy. Directions
for future research include developing parameter-
selection techniques, incorporating other sources of
contextual cues besides sentence proximity, and in-
vestigating other means for modeling such informa-
tion.
Acknowledgments
We thank Eric Breck, Claire Cardie, Rich Caruana,
Yejin Choi, Shimon Edelman, Thorsten Joachims,
Jon Kleinberg, Oren Kurland, Art Munson, Vincent
Ng, Fernando Pereira, Ves Stoyanov, Ramin Zabih,
and the anonymous reviewers for helpful comments.
This paper is based upon work supported in part
by the National Science Foundation under grants
ITR/IM IIS-0081334 and IIS-0329064, a Cornell
Graduate Fellowship in Cognitive Studies, and by
an Alfred P. Sloan Research Fellowship. Any opin-
ions, findings, and conclusions or recommendations
expressed above are those of the authors and do not
necessarily reflect the views of the National Science
Foundation or Sloan Foundation.
References
Agrawal, Rakesh, Sridhar Rajagopalan, Ramakrish-
nan Srikant, and Yirong Xu. 2003. Mining news-
groups using networks arising from social behav-
ior. In WWW, pages 529?535.
Ahuja, Ravindra, Thomas L. Magnanti, and
James B. Orlin. 1993. Network Flows: Theory,
Algorithms, and Applications. Prentice Hall.
Beineke, Philip, Trevor Hastie, Christopher Man-
ning, and Shivakumar Vaithyanathan. 2004.
Exploring sentiment summarization. In AAAI
Spring Symposium on Exploring Attitude and Af-
fect in Text: Theories and Applications (AAAI
tech report SS-04-07).
Blum, Avrim and Shuchi Chawla. 2001. Learning
from labeled and unlabeled data using graph min-
cuts. In Intl. Conf. on Machine Learning (ICML),
pages 19?26.
Boykov, Yuri, Olga Veksler, and Ramin Zabih.
1999. Fast approximate energy minimization via
graph cuts. In Intl. Conf. on Computer Vision
(ICCV), pages 377?384. Journal version in IEEE
Trans. Pattern Analysis and Machine Intelligence
(PAMI) 23(11):1222?1239, 2001.
Cardie, Claire, Janyce Wiebe, Theresa Wilson, and
Diane Litman. 2003. Combining low-level and
summary representations of opinions for multi-
perspective question answering. In AAAI Spring
Symposium on New Directions in Question An-
swering, pages 20?27.
Cormen, Thomas H., Charles E. Leiserson, and
Ronald L. Rivest. 1990. Introduction to Algo-
rithms. MIT Press.
Das, Sanjiv and Mike Chen. 2001. Yahoo! for
Amazon: Extracting market sentiment from stock
message boards. In Asia Pacific Finance Associ-
ation Annual Conf. (APFA).
Dave, Kushal, Steve Lawrence, and David M. Pen-
nock. 2003. Mining the peanut gallery: Opinion
extraction and semantic classification of product
reviews. In WWW, pages 519?528.
Dini, Luca and Giampaolo Mazzini. 2002. Opin-
ion classification through information extraction.
In Intl. Conf. on Data Mining Methods and
Databases for Engineering, Finance and Other
Fields, pages 299?310.
Durbin, Stephen D., J. Neal Richter, and Doug
Warner. 2003. A system for affective rating of
texts. In KDD Wksp. on Operational Text Classi-
fication Systems (OTC-3).
Hatzivassiloglou, Vasileios and Kathleen Mc-
Keown. 1997. Predicting the semantic orienta-
tion of adjectives. In 35th ACL/8th EACL, pages
174?181.
Joachims, Thorsten. 2003. Transductive learning
via spectral graph partitioning. In Intl. Conf. on
Machine Learning (ICML).
Liu, Hugo, Henry Lieberman, and Ted Selker.
2003. A model of textual affect sensing using
real-world knowledge. In Intelligent User Inter-
faces (IUI), pages 125?132.
Montes-y-Go?mez, Manuel, Aurelio Lo?pez-Lo?pez,
and Alexander Gelbukh. 1999. Text mining as a
social thermometer. In IJCAI Wksp. on Text Min-
ing, pages 103?107.
Morinaga, Satoshi, Kenji Yamanishi, Kenji Tateishi,
and Toshikazu Fukushima. 2002. Mining prod-
uct reputations on the web. In KDD, pages 341?
349. Industry track.
Pang, Bo, Lillian Lee, and Shivakumar
Vaithyanathan. 2002. Thumbs up? Senti-
ment classification using machine learning
techniques. In EMNLP, pages 79?86.
Qu, Yan, James Shanahan, and Janyce Wiebe, edi-
tors. 2004. AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Ap-
plications. AAAI technical report SS-04-07.
Riloff, Ellen and Janyce Wiebe. 2003. Learning
extraction patterns for subjective expressions. In
EMNLP.
Riloff, Ellen, Janyce Wiebe, and Theresa Wilson.
2003. Learning subjective nouns using extraction
pattern bootstrapping. In Conf. on Natural Lan-
guage Learning (CoNLL), pages 25?32.
Subasic, Pero and Alison Huettner. 2001. Af-
fect analysis of text using fuzzy semantic typing.
IEEE Trans. Fuzzy Systems, 9(4):483?496.
Tong, Richard M. 2001. An operational system for
detecting and tracking opinions in on-line discus-
sion. SIGIR Wksp. on Operational Text Classifi-
cation.
Turney, Peter. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised
classification of reviews. In ACL, pages 417?424.
Wiebe, Janyce M. 1994. Tracking point of view in
narrative. Computational Linguistics, 20(2):233?
287.
Yi, Jeonghee, Tetsuya Nasukawa, Razvan Bunescu,
and Wayne Niblack. 2003. Sentiment analyzer:
Extracting sentiments about a given topic using
natural language processing techniques. In IEEE
Intl. Conf. on Data Mining (ICDM).
Yu, Hong and Vasileios Hatzivassiloglou. 2003.
Towards answering opinion questions: Separat-
ing facts from opinions and identifying the polar-
ity of opinion sentences. In EMNLP.
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 571?582,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Personalized Recommendation of User Comments via Factor Models
Deepak Agarwal Bee-Chung Chen Bo Pang
Yahoo! Research
701 First Ave
Sunnyvale, CA 94089
{dagarwal,beechun,bopang}@yahoo-inc.com
Abstract
In recent years, the amount of user-generated
opinionated texts (e.g., reviews, user com-
ments) continues to grow at a rapid speed: fea-
tured news stories on a major event easily at-
tract thousands of user comments on a popular
online News service. How to consume subjec-
tive information of this volume becomes an in-
teresting and important research question. In
contrast to previous work on review analysis
that tried to filter or summarize information
for a generic average user, we explore a dif-
ferent direction of enabling personalized rec-
ommendation of such information.
For each user, our task is to rank the comments
associated with a given article according to
personalized user preference (i.e., whether the
user is likely to like or dislike the comment).
To this end, we propose a factor model that
incorporates rater-comment and rater-author
interactions simultaneously in a principled
way. Our full model significantly outperforms
strong baselines as well as related models that
have been considered in previous work.
1 Introduction
Recent years have seen rapid growth in user-
generated opinions online. Many of them are user
reviews: a best-seller or a popular restaurant can
get over 1000 reviews on top review sites like Ama-
zon or Yelp. A large quantity of them also come in
the form of user comments on blogs or news arti-
cles. Most notably, during the short period of time
for which a major event is active, news stories on
one single event can easily attract over ten thousand
comments on a popular online news site like Yahoo!
News. One question becomes immediate: how can
we help people consume such gigantic amount of
opinionated information?
One possibility is to take the summarization route.
Briefly speaking (see Section 2 for a more detailed
discussion), previous work has largely formulated
review summarization as automatically or manually
identify ratable aspects, and present overall senti-
ment polarity for each aspect (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Snyder and Barzilay,
2007; Titov and McDonald, 2008). A related line
of research looked into predicting helpfulness of re-
views in the hope of promoting those with better
quality, where helpfulness is usually defined as some
function over the percentage of users who found the
review to be helpful (Kim et al, 2006; Liu et al,
2007; Danescu-Niculescu-Mizil et al, 2009). In
short, the focus of previous work has been on dis-
tilling subjective information for an average user.
Whether opinion consumers are looking for qual-
ity information or just wondering what other people
think, each may have different purposes or prefer-
ences that is not well represented by a generic av-
erage user. If we think about how we deal with in-
formation content overflow on the Web, there have
been two main frameworks to identify relevant infor-
mation for each person. One is search. Indeed many
top review sites allow users to search within reviews
for a given entity. But this is only useful when users
have explicit information needs that can be formu-
lated as queries. The other paradigm is recommen-
dation: based on what users have liked or disliked in
the past, the system will automatically recommend
571
new items.
Can we provide similar recommendation mech-
anisms to help users consume large quantities of
subjective information? Many commenting environ-
ments allow users to mark ?like? or ?dislike? over
existing comments (e.g., Yahoo! News comments,
Facebook posts, or review sites that allow helpful-
ness votes). Can we learn from users? past prefer-
ences, so that when a user is reading a new article,
we have a system that automatically ranks its com-
ments according to their likelihood of being liked by
the user? This can be used directly to create person-
alized presentation of comments (e.g., into a ?like?
column and a ?dislike? column), as well as enabling
down-stream applications such as personalized sum-
marization.
Recommending textual information has recently
attracted more attention. So far, the focus has been
mainly on recommending news articles (Ahn et al,
2007; Das et al, 2007). Our task differs in several
aspects. Intuitively, recommending news articles is
largely about identifying the topics of interest to a
given user, and it is conceivable that unigram repre-
sentation of full-length articles can reasonably cap-
ture that information. In our case, most comments
for an article a user is reading are already of interest
to that user topically. Which ones the user ends up
liking may depend on several non-topical aspects of
the text: whether the user agrees with the viewpoint
expressed in the comment, whether the comment is
convincing and well-written, etc. Previous work has
shown that such analysis can be more difficult than
topic-based analysis (Pang and Lee, 2008), and we
have the additional challenge that comments are typ-
ically much shorter than full-length articles. How-
ever, the difficulty in analyzing the textual infor-
mation in comments can be alleviated by additional
contextual information such as author identities. If
between a pair of users one consistently likes or dis-
likes the other, then at least for the heavy users, this
authorship information alone could be highly infor-
mative. Indeed, previous work in collaborative filter-
ing has usually found no additional gain from lever-
aging content information when entity-level prefer-
ence information is abundant.
In this paper, we present a principled way of uti-
lizing multiple sources of information for the task of
recommending user comments, which significantly
outperforms strong baseline methods, as well as pre-
vious methods proposed for text recommendation.
While using authorship information alone tends to
provide stronger signal than using textual informa-
tion alone, to our surprise, even for heavy users,
adding textual information to the authorship infor-
mation yields additional improvements.
2 Related Work
There are two main bodies of related work: our
problem formulation is closer to collaborative filter-
ing, while the nature of the text we are dealing with
has more in common with opinion mining and sen-
timent analysis.
Our approach is related to a large body of work
in collaborative filtering. While a proper survey
is not possible here, we describe some of the ap-
proaches that are germane. Classical approaches in
collaborative filtering are based on item-item/user-
user similarity, these are nearest-neighbor methods
where the response for a user-item pair is predicted
based on a local neighborhood mean (Sarwar et al,
2001; Wang et al, 2006). In general, neighbor-
hoods are defined by measuring similarities between
users/items through correlation measures like Pear-
son, cosine similarities, etc. Better approaches to
estimate similarities have also been proposed in Ko-
ren (2010). However, modern methods based on
matrix factorization have been shown to outperform
nearest neighbor methods (Salakhutdinov and Mnih,
2008a,b; Bell et al, 2007). Generalizations of ma-
trix factorization to include both features and past
ratings have been proposed (Agarwal and Chen,
2009; Stern et al, 2009). The approach in this pa-
per is an extension where in addition to interactions
among users and items (comments in our case), we
also consider the authorship information. Three-way
interactions were recently studied for personalized
tag recommendation (Rendle and Lars, 2010). Their
model was based on the sum of two-way interac-
tions, and was trained by using pairwise tag pref-
erences for each (user, item) pair. However, no fea-
tures were considered, which is an important consid-
eration for us. We show using both text and author-
ship provides the best performance.
Our work is also related to news personalization
that has received increasing attention in the last few
572
years. For instance, Billsus and Pazanni (2007) de-
scribes an approach to build user profile models for
adaptive personalization in the context of mobile
content access. Their approach is based on a hybrid
model that combines content-based approaches with
similarity methods used in recommender systems.
This is further exemplified in the work by Ahn et al
(2007) where text processing techniques are used to
build content profiles for users to recommend per-
sonalized news. In our experiments, we show that
such approaches are inferior to our method. A con-
tent agnostic approach based on collaborative filter-
ing techniques was proposed by Das et al (2007);
cold-start for new items/users was not their focus,
but is important for our task ? candidate comments
for recommendation are often not in training data.
As discussed in Section 1, previous work in opin-
ion mining and sentiment analysis has addressed the
information consumption challenge via review sum-
marization. Discussion of early work in that di-
rection can be found in Pang and Lee (2008). In
this line of work, opinions for each given aspect are
usually summarized as the average sentiment po-
larity associated with that aspect. Related to that,
people have looked into predicting review helpful-
ness given the textual information in reviews, where
helpfulness is either defined as the percentage of
users who have voted the review to be helpful (Kim
et al, 2006), or labeled by annotators according to
a set of criteria (Liu et al, 2007). Our goal dif-
fers in that we look for personalized ranking (what
a specific user might like) rather than generic qual-
ity (what an average user might like). Subsequently,
there has been work that tried to predict similarly
defined helpfulness scores using meta-information
over the reviewer. For instance, whether the au-
thor has used his/her true name or where the user
is from (Danescu-Niculescu-Mizil et al, 2009), as
well as graph structure in the social network be-
tween reviewers (Lu et al, 2010). In this work, we
simply use author identity to provide more context
to the short text; in future work, additional meta-
information over users can easily be incorporated
via our model.
As discussed in Section 1, whether a rater likes a
comment or not may depend on whether they agree
with the viewpoint expressed in the text and quality
of the text. While previous work has not looked into
the reader-comments relationship, there has been re-
lated work on identifying political orientations or
viewpoints (Lin and Hauptmann, 2006; Lin et al,
2006; Mullen and Malouf, 2006, 2008; Laver et al,
2003); whether a piece of text expresses support or
opposition in congressional debates (Thomas et al,
2006) or online debates (Somasundaran and Wiebe,
2009, 2010); as well as identifying contrastive re-
lationship (Kawahara et al, 2010). Note that it is
not trivial to use previous work along this line to di-
rectly serve as sub-components in our setting. For
instance, for work on identifying political orienta-
tions or viewpoints, the training data consists of text
with the desired labels. In our setting, our labels
come in the form of whether users liked or disliked
a previous comment. In the simplest case, we might
have pair-wise constraints on whether two pieces of
text have the same viewpoints (i.e., liked or dis-
liked by the same rater), which would yield a dif-
ferent learning problem akin to the metric learning
problem; note, however, the complication that two
pieces of text receiving different labels from a given
user might not necessarily contain contrasting view-
points. Consequently, rather than trying to reduce
this problem to a set of known text classification
tasks, we address this task via a collaborative filter-
ing framework that incorporates textual features.
3 Method
In this section, we describe our model that predicts
rater affinity to comments. A key strength of our
model is the ability to incorporate rater-comment
and rater-author interactions simultaneously in a
principled fashion. Our model also provides a seam-
less mechanism to transition from cold-start (where
recommendations need to be made for users or items
with no or few past ratings) to warm-start scenarios
? with a large amount of data, it fits a per-rater (au-
thor) model; with increase in data sparsity, the model
applies a small sample size correction through fea-
tures (in our case, textual features). The exact for-
mula for such corrections in the presence of sparsity
is based on parameter estimates that are obtained by
applying an EM algorithm to the training data.
573
3.1 Model
Notation: Let yij denote the rating that user i, called
the rater, gives to comment j. Since throughout, we
use suffix i to denote a rater and suffix j to denote a
comment, we slightly abuse notation and let xi (of
dimension pu) and xj (of dimension pc) denote fea-
ture vectors of user i and comment j respectively.
For example, xi can be the bag of words represen-
tation (a sparse vector) inferred through text anal-
ysis on comments voted positively by user i in the
past, and xj can be the bag of words representation
for comment j. We use a(j) to denote the author of
comment j, and use ?ij to denote the mean rating by
rater i on comment j, i.e., ?ij = E(yij). Of course it
is impossible to estimate ?ij empirically since each
user i usually rates a comment j at most once.
Model specification: We work in a generalized
linear model framework (McCullagh and Nelder,
1989) that assumes ?ij (or some monotone function
h of ?ij) is an additive function of (1) the rater bias
?i of user i since some users may have a tendency
of rating comments more positively or negatively
than others, (2) popularity ?j of comment j, which
could reflect the quality of the comment in this set-
ting, and (3) the author reputation ?a(j) of user a(j)
since comments by a reputed author may in general
get more positive ratings. Thus, the overall bias is
?i + ?j + ?a(j).
In addition to the bias, we include terms that
capture interactions among entities (raters, authors,
comments). Indeed, capturing such interactions is a
non-trivial part of our modeling procedure. In our
approach, we take recourse to factor models that
have been widely used in collaborative filtering ap-
plications in recent times. The basic idea is to at-
tach latent factors to each rater, author and comment.
These latent factors are finite dimensional Euclidean
vectors that are unknown and estimated from the
data. They provide a succinct representation of vari-
ous aspects that are important to explain interaction
among entities. In our case, we use the following
factors ? (a) user factor vi of dimension rv(? 1)
to model rater-author affinity, (b) user factor ui and
comment factor cj of dimension ru(? 1) to model
rater-comment affinity. Intuitively, each could repre-
sent viewpoints of users or comments along different
i index for raters
j index for comments
a(j) author of comment j
yij rating given by rater i to comment j
?ij mean rating given by rater i to comment j
xj feature vector of comment j
(e.g., textual features in comment j)
xi feature vector of user i
(e.g., comments voted positively by user i)
bias terms:
?i rater bias of user i
?j popularity of comment j
(e.g., quality of the comment)
?a(j) reputation of the author of comment j
interaction terms:
vi user factor for rater-author affinity
ui, cj factors for rater-comment affinity
Table 1: Table of Notations.
dimensions.
Affinity of rater i to comment j by author a(j)
is captured by (1) similarity between viewpoints of
users i and a(j), measured by v?iva(j); and (2) simi-
larity between the preferences of user i and the per-
spectives reflected in comment j, measured by u?icj .
The overall interaction is v?iva(j) + u?icj . Then, the
mean rating ?ij , or more precisely h(?ij), is mod-
eled as the sum of bias and interaction terms. Math-
ematically, we assume:
yij ? N(?ij , ?2y) or Bernoulli(?ij)
h(?ij) = ?i + ?j + ?a(j) + v?iva(j) + u?icj
(1)
For numeric ratings, we use the Gaussian distri-
bution denoted by N (mean,var); for binary rat-
ings, we use the Bernoulli distribution. For Gaus-
sian, h(?ij) = ?ij , and for Bernoulli, we assume
h(?ij) = log ?ij1??ij , which is the commonly usedlogistic transformation.
Table 1 summarizes the notations for easy refer-
ences. We denote the full model specified above as
vv+uc since both user-user interaction v?iva(j) and
user-comment interaction u?icj are modeled at the
same time.
Latent factors: A natural approach to estimate la-
tent factors in Equation 1 is through a maximum
likelihood estimation (MLE) approach. This does
574
not work in our scenario since a large fraction of
entities have small sample size. For instance, if a
comment is rated only by one user and ru > 1,
the model is clearly overparametrized and the MLE
of the comment factor would tend to learn idiosyn-
crasies in the training data. Hence, it is imperative
to impose constraints on the factors to obtain esti-
mates that generalize well on unseen data. We work
in a Bayesian framework where such constraints are
imposed through prior distributions. The crucial is-
sue is the selection of appropriate priors. In our sce-
nario, we need priors that provide a good backoff
estimate when interacting entities have small sam-
ple sizes. For instance, to estimate latent factors of
a user with little data, we provide a backoff estimate
that is obtained by pooling data across users with
the same user features. We perform such a pooling
through regression, the mathematical specification is
given below.
?i ? N(g?xi, ?2?), ui ? N(Gxi, ?2u),
?j ? N(d?xj , ?2?), cj ? N(Dxj , ?2c ),
?a(j) ? N(0, ?2?), vi ? N(0, ?2v),
where gpu?1 and dpc?1 are regression weight vec-
tors, and Gru?pu and Dru?pc are regression weight
matrices. These regression weights are learnt from
data and provide the backoff estimate. Take the prior
distribution of ui for example. We can rewrite the
prior as ui = Gxi + ?i, where ?i ? N(0, ?2u).
If user i has no rating in the training data, ui will
be predicted as the prior mean (backoff) Gxi, a lin-
ear projection from the feature vector xi through
matrix G learnt from data. This projection can be
thought of as a multivariate linear regression prob-
lem with weight matrix G, one weight vector per
dimension of ui. However, if user i has many rat-
ings in the training data, we will precisely estimate
the per-user residual ?i that is not captured by the re-
gressionGxi. For sample sizes in between these two
extremes, the per user residual estimate is ?shrunk?
toward zero ? amount of shrinkage depends on the
sample size, past user ratings, variability in ratings
on comments rated by the user, and the value of vari-
ance components ?2? s.
3.2 Special Cases of Our Model
Our full model (vv+uc) includes several existing
models explored in collaborative filtering and social
networks as special cases.
The matrix factorization model: This model as-
sumes the mean rating of user i on item j is given
by h(?ij) = ?i + ?j + u?icj , and the mean of
the prior distributions on ?i, ?j ,ui, cj are zero, i.e.,
g = d = G = D = 0. Recent work clearly illus-
trates that this method obtains better predictive accu-
racy than classical collaborative filtering techniques
based on item-item similarity (Bell et al (2007)).
The uc model: This is also a matrix factorization
model but with priors based on regressions (i.e.,
non-zero g, d,G,D). It provides a mechanism to
deal with both cold and warm-start scenarios in rec-
ommender applications (Agarwal and Chen (2009)).
The vv model: This model assumes h(?ij) = ?i +
?a(j) + v?iva(j). It was first proposed by Hoff (2005)
to model interactions in social networks. The model
was fitted to small datasets (at most a few hundred
nodes) and the goal was to test certain hypotheses
on social behavior, out-of-sample prediction was not
considered.
The low-rank bilinear regression model: Here,
h(?ij) = g?xi + d?xj + x?iG?Dxj . This is a re-
gression model purely based on features with no per-
user or per-comment latent factors. In a more gen-
eral form, x?iG?Dxj can be written as x?iAxj , where
Apu?pc is the matrix of regression weights (Chu and
Park, 2009). However, since xi and xj are typically
high dimensional, A can be a large matrix that needs
to be learnt from data. To reduce dimensionality, one
can decompose A as A = G?D, where the number
of rows inD andG are small. Thus, instead of learn-
ingA, we learn a low-rank approximation ofA. This
ensures scalability and provides an attractive method
to avoid over-fitting.
3.3 Model Fitting
Model fitting for our model is based on the
expectation-maximization (EM) algorithm (Demp-
ster et al, 1977). For ease of exposition and space
constraints, we only provide a sketch of the algo-
rithm for the Gaussian case, the logistic model can
be fitted along the same lines by using a variational
approximation (see Agarwal and Chen (2009)).
Let Y = {yij} denote the set of the observed
ratings. In the EM parlance, this is ?incomplete?
575
data that gets augmented with the latent factors
? = {ui,vi, cj} to obtain the ?complete? data.
The goal of the EM algorithm is to find the param-
eter ? = (g, d,G,D, ?2?, ?2?, ?2u, ?2v , ?2y) that maxi-
mizes the ?incomplete? data likelihood Pr(Y |?) =?
Pr(Y ,?|?)d? that is obtained after marginaliza-
tion (taking expectation) over the distribution of ?.
Since such marginalization is not available in closed
form for our model, we use the EM algorithm.
EM algorithm: The complete data log-likelihood
l(?;Y ,?) for the full model in the Gaussian case
(where h(?ij) = ?ij) is given by l(?;Y ,?) =
? 12
?
ij
(
(yij ? ?ij)2/?2y + log ?2y
)
? 12
?
i
(
(?i ? g?xi)2/?2? + log ?2?
)
? 12
?
j
(
(?j ? d?xj)2/?2? + log ?2?
)
? 12
?
i
(
?ui ?Gxi?2/?2u + ru log ?2u
)
? 12
?
j
(
?cj ?Dxj?2/?2c + ru log ?2c
)
,
? 12
?
i
(
v?ivi/?2v + rv log ?2v + ?2i /?2? + log ?2?
)
,
where ru is the dimension of factors ui and cj , and
rv is the dimension of vi. Let ?(t) denote the esti-
mated parameter setting at the tth iteration. The EM
algorithm iterates through the following two steps
until convergence.
? E-step: Compute ft(?) = E?[l(?;Y ,?) |?(t)]
as a function of ?, where the expectation is taken
over the posterior distribution of (? |?(t),Y ).
Note that here ? is the input variable of function
ft, but ?(t) consists of known quantities (deter-
mined in the previous iteration).
? M-step: Find the ? that maximizes the expecta-
tion computed in the E-step.
?(t+1) = argmax
?
ft(?)
Since the expectation in the E-step is not available in
a closed form, we use a Gibbs sampler to compute
the Monte Carlo expectation (Booth and Hobert,
1999). The Gibbs sampler repeats the following
procedure L times. It samples ?i, ?i, ?j , ui, vj ,
and cj sequentially one at a time by sampling from
the corresponding full conditional distributions. The
full conditional distributions are all Gaussian, hence
they are easy to sample. Once a Monte Carlo ex-
pectation is calculated from the samples, an updated
estimate of ? is obtained in the M-step. The opti-
mization of variance components ?2? s in the M-step
is available in closed form, the regression param-
eters are estimated through off-the-shelf linear re-
gression routines. We note that the posterior distri-
bution of latent factors for known ? is multi-modal,
we have found the Monte Carlo based EM method to
outperform other optimization methods like gradient
descent in terms of predictive accuracy.
4 Experiments
4.1 Data
We obtained comment rating data between March
and May, 2010 from Yahoo! News, with all user IDs
anonymized. On this site, users can post comments
on news article pages and rate the comments made
by others through thumb-up (positive) or thumb-
down (negative) votes. Clearly, for articles with very
few comments, there is no need to recommend com-
ments. Also, we do not expect deep personalized
recommendations for users who have rated very few
comments in the past. To focus on instances of in-
terest to us, we restricted ourselves to a subset of the
rating data associated with relatively heavy raters.
In particular, we formed the experimental dataset
by randomly selecting 9,003 raters who provided at
least 200 ratings (of which at least 10 were posi-
tive and 10 were negative), 189,291 authors who re-
ceived at least 20 ratings, and 5,088 news articles
that received at least 40 comments in the raw dataset
during the three-month period. Note that the per en-
tity sample size in the experimental dataset can be
smaller than the thresholds specified above. For in-
stance, a rater with more than 200 ratings in the raw
dataset can have fewer than 200 in the experimental
dataset due to the removal of certain authors or news
articles. (See Figure 2 for a distribution of users with
different activity levels.) In total, we have 4,440,222
ratings on 1,197,098 comments.
The 5,088 news articles were split into training
articles (the earliest 50%), tuning articles (next 5%),
and test articles (the last 45%) based on their pub-
lication time. The ratings and comments were split
into training, tuning, and test sets according to the
article they were associated with. All tuning param-
eters are determined using the tuning set, and per-
formances are reported over the test set. Note that
576
this training-test split ensures that performance on
the test data best simulates our application scenar-
ios. It also creates a completely cold-start situation
for comments ? no comment in the test set has any
past rating in the training set.
4.2 Experimental Setup
Features: All comments were tokenized, lower-
cased, with stopwords and punctuations removed.
We limited the vocabulary to the 10K most frequent
tokens in all comments associated with the training
articles. (See Section 4.3.3 for a discussion on the
effect of the vocabulary size.) For a given comment
j, xj is its bag of words representation, L2 normal-
ized. For term weighting, we experimented with
both presence value and tf-idf weighting. The latter
gives slight better performance. Rater feature vector
xi is created by summing over the feature vectors
of all comments rated positively by rater i, which is
then L2 normalized.
Methods: We compare the following methods
based on our model: The full model vv+uc, as well
as the three main special cases, vv, uc, and bilin-
ear, as defined in Section 3. The dimensions of vi,
ui and cj (i.e., rv and ru), and the rank of bilin-
ear are selected to obtain the best AUC on the tun-
ing set. In our experiments, rv = 2, ru = 3 and
rank of bilinear is 3. In addition, we also evaluate
the following baseline methods that predict per-user
preferences in isolation, primarily based on textual
information.
? Cosine similarity (cos): x?ixj . This is simply
based on how similar a new comment j is to the
comments rater i has liked in the past.
? Per-user SVM (svm): For each rater, train a sup-
port vector machine (SVM) classifier using only
comments (xj) rated by that user.
? Per-user Naive Bayes (nb): For each rater, train
a Naive Bayes classifier using only comments
(xj) rated by that user.1
Note that SVMs typically yield the best performance
on text classification tasks; a Naive Bayes classifier
1As we mentioned in Section 4.1, not all users have training
data of both classes in the experimental dataset. For svm and
nb, we use the following backoff: for users with training data
from only ci, we predict ci; for users with no training data at
all, we predict the majority class, in this case, the positive class.
can be more robust over shorter text spans common
in user comments given the high variance. For fair
comparisons, for the three baseline methods, we use
a simple way of utilizing author information: the
feature space is augmented with author IDs and each
xj is augmented with a(j)2. In Section 4.3, we only
report results using the augmented feature vectors
since they yield better performance (though the dif-
ference is fairly small).
Performance metrics: We use two types of met-
rics to measure the performance of a method: (1)
A global metric based on Receiver Operating Char-
acteristic (ROC) and (2) Precision at rank k (P@k).
The former measures the overall correlation of pre-
dicted scores for a method with the observed rat-
ings in the test set, while the latter measures the
performance of a hypothetical top-k recommenda-
tion scenario using the method. To summarize an
ROC curve into a single number, we use the Area
Under the ROC Curve (AUC). Since random guess
yields AUC score of 0.5, regardless of the class dis-
tribution, using this measure makes it convenient for
us to compare the performance over different sub-
sets of the data (where class distributions could be
different). The P@k of a method is computed as
follows: (1) For each rater, rank comments that the
rater rated in the test set according to the scores pre-
dicted by the method, and compute the precision at
rank k for that rater; and then (2) average the per-
rater precision numbers over all raters. To report
P@k, for k = 5, 10, 20, we only use raters who have
at least 50 ratings in the test set. Statistical signif-
icance based on a two-sample t-test across raters is
also reported.
4.3 Results
4.3.1 Main comparisons
We first show the ROC curves of different meth-
ods on the test set in Figure 1, and the AUCs and
precisions in Table 2. Results from significance tests
are in Table 3.
First, note that while svm significantly outper-
forms random guesses and nb, it is worse than bi-
linear, which is also using (mostly) textual infor-
mation, but learns the model for all users together,
2We assign weight 1 to a(j), so that the author information
have the same impact as the textual features.
577
False positive rate
T
r
u
e
 
p
o
s
i
t
i
v
e
 
r
a
t
e
0.0 0.2 0.4 0.6 0.8 1.0
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
T
r
u
e
 
p
o
s
i
t
i
v
e
 
r
a
t
e
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
T
r
u
e
 
p
o
s
i
t
i
v
e
 
r
a
t
e
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
T
r
u
e
 
p
o
s
i
t
i
v
e
 
r
a
t
e
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
T
r
u
e
 
p
o
s
i
t
i
v
e
 
r
a
t
e
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
T
r
u
e
 
p
o
s
i
t
i
v
e
 
r
a
t
e
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
T
r
u
e
 
p
o
s
i
t
i
v
e
 
r
a
t
e
0
.
0
0
.
2
0
.
4
0
.
6
0
.
8
1
.
0
vv+uc
vv
uc
bilinear
svm
nb
cos
random
Figure 1: ROC curves of different models
Method AUC P@5 P@10 P@20
vv+uc 0.8360 0.9152 0.9079 0.8942
vv 0.8090 0.8810 0.8807 0.8727
uc 0.7857 0.9046 0.8921 0.8694
bilinear 0.7701 0.9028 0.8894 0.8668
svm 0.6768 0.7814 0.7678 0.7497
nb 0.6465 0.7660 0.7486 0.7309
cos 0.5382 0.6834 0.6813 0.6754
Table 2: AUCs and precisions of different models.
rather than in isolation. Next, uc outperforms bilin-
ear (significantly in AUC, P@10 and P@20), show-
ing per-user and per-comment latent factors help.
Note that vv outperforms uc in ROC, AUC and
P@20, but is worse than uc in P@5 and P@10; we
will take a closer look at this later. Finally, the full
model vv+uc significantly outperforms both vv and
uc, achieving 0.83 in AUC, and close to 90% in pre-
cision at rank 20.
4.3.2 Break-down by user activity level
Next, we investigate model performance in differ-
ent subsets of the test set. For succinctness, we use
AUC as our performance metric. In Figure 2(a), we
breakdown model performance by different author
activity levels. In Figure 2(b), we breakdown model
performance by different voter activity levels. We
also generated similar plots with the y-axis replaced
by P@5, P@10 and P@20, and observed the same
trend except that vv starts to outperform uc at differ-
ent user activity thresholds for different metrics.
Comparison Metrics p-value
vv+uc > vv All < 10?7
vv+uc > uc All < 10?20
uc > bilinear All except P@5 < 0.006
bilinear > svm All < 10?20
vv > svm All < 10?20
svm > nb All < 10?8
nb > cos All < 10?20
Table 3: Paired t-test results. Note that uc is better than
bilinear in P@5, but not significant. The orders of uc
and vv are not consistent across different metrics.
Not surprisingly, vv performs poorly for raters or
authors with no ratings observed in the training data.
However, once we have a small amount of ratings, it
starts to outperform uc, even though intuitively, the
textual information in the comment should be more
informative than the authorship information alone.
Using paired t-tests with significance level 0.05, we
report when vv starts to significantly outperform uc
in the following table, which is interpreted as fol-
lows ? vv is not significantly worse than uc in met-
ric M if the author of a test comment received at
least Neq ratings in the training set, and vv signifi-
cantly outperforms uc in metric M if the author re-
ceived at least Ngt ratings in the training set.
Metric M P@5 P@10 P@20 AUC
# Ratings Neq 50 5 5 5
Ngt 1000 50 5 5
Recall that our training/test split is by article. Since
we have never observed a rater?s preference over the
test articles before, it is rather surprising that author
information alone can yield 0.8 in AUC score, even
for very light authors who have received between 3
and 5 votes in total in the training data. This suggests
that users? viewpoints are quite consistent: a large
portion of the ratings can be adequately explained
by the pair of user identities. One interesting obser-
vation is that the number of ratings required for vv to
outperform uc in P@5 is quite high. This suggests
that to obtain high precision at the top of a recom-
mended list, comment features are important.
Nonetheless, modeling textual information in ad-
dition to author information provides additional im-
provements. Based on paired t-tests with signifi-
578
0.60
0.65
0.70
0.75
0.80
0.85
0.90
A
U
C
0
?
0
1
?
2
3
?
5
6
?
1
0
1
1
?
2
0
2
1
?
5
0
5
1
?
1
0
0
1
0
1
?
2
0
0
2
0
1
?
5
0
0
5
0
1
?
2
0
0
0
4
0
9
K
1
6
8
K
1
4
7
K
1
4
3
K
1
7
0
K
2
3
1
K
1
8
4
K
1
3
1
K
1
2
0
K
8
2
K
vv+uc
vv
uc
bilinear
svm
nb
(a) AUC by author activity levels
0.5
0.6
0.7
0.8
A
U
C
0
?
0
1
?
5
6
?
1
0
1
1
?
2
0
2
1
?
5
0
5
1
?
1
0
0
1
0
1
?
2
0
0
2
0
1
?
5
0
0
5
0
1
?
2
0
0
0
2
5
K
2
2
K
1
5
K
3
1
K
9
6
K
2
0
2
K
4
0
4
K
5
9
7
K
4
0
3
K
vv+uc
vv
uc
bilinear
svm
nb
(b) AUC by rater activity levels
0.70
0.75
0.80
0.85
0.90
A
U
C
5
0
?
1
0
0
1
0
1
?
2
0
0
2
0
1
?
5
0
0
5
0
1
?
2
0
0
0
2
1
K
2
9
K
4
1
K
1
9
K
vv+uc
vv
uc
bilinear
svm
nb
(c) AUC by user activity levels
Figure 2: AUC of different models as a function of the activity level of authors or raters. The x-axis (bottom) has the
form m-n, meaning the subset of the test data in which the number of ratings that each author received (as in (a)) or
each rater gave (as in (b)) in the training set is between m and n. In (c), we select both authors and raters based on the
m-n criterion. The x-axis (top) denotes the number of ratings in the subset
cance level 0.05, vv+uc significantly outperforms vv
in all metrics if the author received < 500 ratings
in the training set. Except for the very heavy au-
thors, even for cases where both raters and authors
are heavy users (Figure 2(c)), adding the comment
feature information still yields additional improve-
ment over the already impressive performance of us-
ing vv alone. In spite of the simple representation we
adopted for the textual information, the full model is
still capable of accounting for part of the residual
errors from vv model (that uses authorship informa-
tion alone) by using comment features ? what was
actually written does matter.
Finally, if we breakdown the comparison be-
tween vv+uc and uc for different user activity lev-
els, vv+uc significantly outperforms uc (with level
0.05) in all metrics if the author received at least 5
ratings in the training set.
4.3.3 Analysis of textual features
Recall that we limited the vocabulary size to the
10K most frequent terms for efficiency reasons. Is
this limitation likely to affect our model perfor-
mance significantly? We examined the effect of dif-
ferent numbers of features. In the following table,
#features = n means that both xi and xj are bags
of n words3. Since the vv model does not utilize
rater or comment features, we examine AUC of the
uc model.
#features 1K 3K 5K 10K
AUC 0.7713 0.7855 0.7872 0.7876
As can be seen, the performance improvement is in
the 4th decimal place when we increase from 5K
features to 10K features. Thus, we do not further
increase the number of features in our experiments.
Note that our full model does not require rater fea-
tures and comment features to be in the same feature
space. Each is projected into the hidden ?viewpoint?
space, via G and D, separately. For simplicity and
easy comparison to other methods, we used all com-
ments liked by a rater in the past to build the feature
vector of the rater. But since the full model already
has information of the textual content of comments
from the comment features, and which comments
were liked by the users from the ratings, rater fea-
tures constructed this way do not provide any new
information. Indeed, if we model ui ? N(1, ?2u),
instead of ui ? N(Gxi, ?2u), this omission of xi
does not hurt the performance of the model. In fu-
ture work, other meta-information about the rater
3Note that we used n most useful features in each case.
579
can easily be incorporated into xi to enrich rater rep-
resentation.
Recall that comment featuresxj were projected to
comment factors cj via D. We envisioned that the
comment factors could be representing viewpoints.
Does our model conform to this intuition? Let?s con-
sider the simplest case, where we restrict ui and cj
to be one-dimensional vectors. In this case, each can
be represented by scalars ui and cj . If ui and cj
are of the same sign, then the rater is likely to like
the comment. Words assigned high positive weights
or low negative weights via D will have significant
contributions to the overall sign of cj . Now if we ex-
amine such words, will we see any meaningful dif-
ferences in the underlying viewpoints of these two
groups of words?
To address this question qualitatively, we manu-
ally sampled words with heavy weights, focusing on
politics-related ones (so that viewpoints are likely
to be polarized and easier to interpret). At one ex-
treme, we observe words like repukes, repugs, which
seemed to be derogatory mentions of Republica-
tions, and likely to represent an anti-Republication
point of view. At the other end, we observe terms
like libtards, nobama, obummer. While terms like
nobama may appear to be typos at first sight, a
quick search online reveals that these are at least
intentional typos expressing anti-Obama sentiments,
which clearly represents an opposite underlying per-
spective from terms like repukes.
These examples also illustrate the importance to
learn directly from the data of interest to us. Such
indicative words would never have appeared in more
formal writings. While we do not have direct labels
for perspectives, our model seems to be capturing
the underlying perspectives (as much as a unigram-
based model could) by learning from user preference
labels across different users. This allows us to learn
the text features most relevant to our dataset, which
is particularly important given the time-sensitive and
ever-evolving nature of news-related comments.
5 Conclusions
In this paper, we promote personalized recommen-
dation as a novel way of helping users to consume
large quantities of subjective information. We pro-
pose using a principled way of incorporating both
rater-comment and rater-author interactions simul-
taneously. Our full model significantly outperforms
strong baseline methods, as well as previous meth-
ods proposed for text recommendation. In particu-
lar, learning weights over textual features across all
users outperforms learning for each user individu-
ally, which holds true even for heavy raters. Further-
more, while using authorship information alone pro-
vides stronger signal than using textual information
alone, to our surprise, even for heavy users, adding
textual information yields additional improvements.
It is difficult to comprehensively capture user
affinity to comments using a finite number of rat-
ings observed during a certain time interval. News
and comments on news articles are dynamic in na-
ture, novel aspects may emerge over time. To cap-
ture such dynamic behavior, comment factors have
to be allowed to evolve over time and such an evolu-
tion would also necessitate the re-estimation of user
factors. Incorporating such temporal dynamics into
our modeling framework is a challenging research
problem and requires significant elaboration of our
current approach.
Acknowledgments
We thank the anonymous reviewers for useful sug-
gestions.
References
D. Agarwal and B.C. Chen. Regression-based latent
factor models. In Proceedings of the 15th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 19?28. ACM,
2009.
Jae-Wook Ahn, Peter Brusilovsky, Jonathan Grady,
Daqing He, and Sue Y. Syn. Open user profiles
for adaptive news systems: help or harm? In Pro-
ceedings of the 16th international conference on
World Wide Web (WWW), 2007.
Robert Bell, Yehuda Koren, and Chris Volinsky.
Modeling relationships at multiple scales to im-
prove accuracy of large recommender systems. In
KDD, 2007.
D. Billsus and M. Pazanni. Adaptive news access.
Springer, Berlin, 2007.
J.G. Booth and J.P Hobert. Maximizing generalized
linear mixed model likelihoods with an automated
580
monte carlo EM algorithm. J.R.Statist. Soc. B,
1999.
Wei Chu and Seung T. Park. Personalized recom-
mendation on dynamic content using predictive
bilinear models. In Proceedings of the 18th inter-
national conference on World Wide Web (WWW),
2009.
Cristian Danescu-Niculescu-Mizil, Gueorgi
Kossinets, Jon Kleinberg, and Lillian Lee.
How opinions are received by online communi-
ties: A case study on Amazon.com helpfulness
votes. In Proceedings of WWW, pages 141?150,
2009.
Abhinandan S. Das, Mayur Datar, Ashutosh Garg,
and Shyam Rajaram. Google news personaliza-
tion: scalable online collaborative filtering. In
Proceedings of the 16th international conference
on World Wide Web (WWW), 2007.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Soci-
ety, Series B, 39:1?38, 1977.
Peter D. Hoff. Bilinear mixed-effects models for
dyadic data. Journal of the American Statistical
Association, 100(469):286?295, 2005.
Minqing Hu and Bing Liu. Mining and summa-
rizing customer reviews. In Proceedings of the
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 168?177,
2004.
Daisuke Kawahara, Kentaro Inui, and Sadao Kuro-
hashi. Identifying contradictory and contrastive
relations between statements to outline web infor-
mation on a given topic. In Proceedings of the
23rd International Conference on Computational
Linguistics (COLING): Posters, 2010.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. Automatically assessing re-
view helpfulness. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 423?430, Sydney,
Australia, July 2006. Association for Computa-
tional Linguistics.
Y. Koren. Factor in the neighbors: Scalable and ac-
curate collaborative filtering. ACM Transactions
on Knowledge Discovery from Data (TKDD), 4
(1):1?24, 2010. ISSN 1556-4681.
Michael Laver, Kenneth Benoit, and John Garry. Ex-
tracting policy positions from political texts using
words as data. American Political Science Review,
97(2):311?331, 2003.
Wei-Hao Lin and Alexander Hauptmann. Are these
documents written from different perspectives? A
test of different perspectives based on sta tisti-
cal distribution divergence. In Proceedings of the
International Conference on Computational Lin-
guistics (COLING)/Proceedings of the Associa-
tion for Computational Linguistics (ACL), pages
1057?1064, Sydney, Australia, July 2006. Asso-
ciation for Computational Linguistics.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. Which side are you on?
identifying perspectives at the document and sen-
tence levels. In Proceedings of the Conference on
Natural Language Learning (CoNLL), 2006.
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou
Huang, and Ming Zhou. Low-quality product
review detection in opinion summarization. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL), pages 334?342, 2007. Poster
paper.
Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas,
and Livia Polanyi. Exploiting social context for
review quality prediction. In Proceedings of the
19th International World Wide Web Conference
(WWW), 2010.
P. McCullagh and J. A. Nelder. Generalized Linear
Models. Chapman & Hall/CRC, 1989.
Tony Mullen and Robert Malouf. A preliminary in-
vestigation into sentiment analysis of informal po-
litical discourse. In AAAI Symposium on Compu-
tational Approaches to Analysing Weblogs (AAAI-
CAAW), pages 159?162, 2006.
Tony Mullen and Robert Malouf. Taking sides:
User classification for informal online political
discourse. Internet Research, 18:177?190, 2008.
Bo Pang and Lillian Lee. Opinion mining and sen-
timent analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, 2008.581
Ana-Maria Popescu and Oren Etzioni. Extracting
product features and opinions from reviews. In
Proceedings of the Human Language Technol-
ogy Conference and the Conference on Empir-
ical Methods in Natural Language Pro cessing
(HLT/EMNLP), 2005.
Steffen Rendle and Schmidt-Thie Lars. Pairwise
interaction tensor factorization for personalized
tag recommendation. In Proceedings of the third
ACM international conference on Web search and
data mining, WSDM ?10, pages 81?90, New
York, NY, USA, 2010. ACM.
R. Salakhutdinov and A. Mnih. Bayesian proba-
bilistic matrix factorization using Markov chain
Monte Carlo. In Proceedings of the 25th inter-
national conference on Machine learning, pages
880?887. ACM, 2008a.
R. Salakhutdinov and A. Mnih. Probabilistic ma-
trix factorization. Advances in neural information
processing systems, 20:1257?1264, 2008b.
Badrul Sarwar, George Karypis, Joseph Konstan,
and John Reidl. Item-based collaborative filter-
ing recommendation algorithms. In WWW ?01:
Proceedings of the 10th international conference
on World Wide Web, pages 285?295. ACM, 2001.
Benjamin Snyder and Regina Barzilay. Multiple as-
pect ranking using the Good Grief algorithm. In
Proceedings of the Joint Human Language Tech-
nology/North American Chapter of the ACL Con-
ference (HLT- NAACL), pages 300?307, 2007.
Swapna Somasundaran and Janyce Wiebe. Recog-
nizing stances in online debates. In Proceedings
of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint
Conference on Natural Language Processing of
the AFNLP, 2009.
Swapna Somasundaran and Janyce Wiebe. Recog-
nizing stances in ideological on-line debates. In
Proceedings of the NAACL HLT 2010 Workshop
on Computational Approaches to Analysis and
Generation of Emotion in Text, 2010.
David H. Stern, Ralf Herbrich, and Thore Grae-
pel. Matchbox: large scale online bayesian rec-
ommendations. In Proceedings of the 18th inter-
national conference on World Wide Web (WWW),
2009.
Matt Thomas, Bo Pang, and Lillian Lee. Get out
the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
327?335, 2006.
Ivan Titov and Ryan McDonald. A joint model of
text and aspect ratings for sentiment summariza-
tion. In Proceedings of the Association for Com-
putational Linguistics (ACL), 2008.
Jun Wang, Arjen P. de Vries, and Marcel J. T. Rein-
ders. Unifying user-based and item-based collab-
orative filtering approaches by similarity fusion.
In SIGIR ?06: Proceedings of the 29th annual in-
ternational ACM SIGIR conference on Research
and development in information retrieval, pages
501?508, New York, NY, USA, 2006. ACM.
582
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1489?1499, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Revisiting the Predictability of Language:
Response Completion in Social Media
Bo Pang Sujith Ravi
Yahoo! Research
4401 Great America Parkway
Santa Clara, CA 95054, USA
bopang42@gmail.com sujith ravi@yahoo.com
Abstract
The question ?how predictable is English??
has long fascinated researchers. While prior
work has focused on formal English typically
used in news articles, we turn to texts gener-
ated by users in online settings that are more
informal in nature. We are motivated by a
novel application scenario: given the difficulty
of typing on mobile devices, can we help re-
duce typing effort with message completion,
especially in conversational settings? We pro-
pose a method for automatic response comple-
tion. Our approach models both the language
used in responses and the specific context pro-
vided by the original message. Our experi-
mental results on a large-scale dataset show
that both components help reduce typing ef-
fort. We also perform an information-theoretic
study in this setting and examine the entropy
of user-generated content, especially in con-
versational scenarios, to better understand pre-
dictability of user generated English.
1 Introduction
How predictable is language? As early as 1951, long
before large quantities of texts (or the means to pro-
cess them) were easily available, Shannon had raised
this question and proceeded to answer it with a set
of clever analytical estimations. He studied the pre-
dictability of printed English, or ?how well can the
next letter of a text be predicted when the preced-
ing N letters are known? (Shannon, 1951). This
was quantified as the conditional entropy, which
measures the amount of information conveyed from
statistics over the preceding context. In this paper,
we discuss a novel application setting which mirrors
the predictability study as defined by Shannon.
(a) Google (b) Amazon (c) Netflix
Figure 1: Query completion as users type into the ?Search
using Google? box on a browser, as well as the search box
in Amazon and Netflix.
Text completion for user-generated texts: Con-
sider a user who is chatting with her contact or post-
ing to a social media site using a mobile device. If
we can predict the next word given the preceding
words that were already typed in, we can help reduce
the typing cost by offering users suggestions of pos-
sible completions of their partially typed messages
(e.g., in a drop-down list). If the intended word is
ranked reasonably high, the user can select the word
instead of typing it. Assuming a lower cost associ-
ated with selections, this could lead to less typing
effort for the user.
An interface like this would be quite familiar to
Web users today. Providing suggestions of possi-
ble completions to partially typed queries, which we
will refer to as query completion,1 is a common fea-
ture of search boxes (Figure 1). In spite of the
similarity in the interface, the underlying technical
challenge can be quite different. Query completion
does not necessarily rely on language models: can-
1Note that this feature is often tagged as ?query suggestion?
in the user interface; we avoid that terminology since it is often
used to refer to query re-formulation (of a completely entered
query) in the literature, which is a very different task.
1489
didate completions can be limited to popular queries
that were previously submitted to the site or entries
in a closed database of available objects, and rank-
ing can be done by overall popularity. In contrast,
our scenario requires generation of unseen texts.
Given the difficulty of generating full-length text,
we consider a more realistic setting, where we per-
form completion on a word-by-word basis. Each
time, we propose candidate completions at the word-
level when the user is about to start a new word,
or has partially entered the first few letters; once
this word is successfully completed, we move on
to the next one. This predict-verify-predict process
exactly mirrors the human experiment described by
Shannon (1951), except we do this at the word-level
rather than the letter-level: having the user examine
and verify predictions at the letter level would not be
a practical solution for the intended application.
The response completion task: In addition, our
task has another interesting difference from Shan-
non?s human experiment. Consider the mobile-
device user mentioned previously. If the user is re-
plying to a piece of text (e.g., an instant message
sent by a contact), we have an additional source of
contextual information in the stimulus, or the text
which triggered the response that the user is try-
ing to type. Can we learn from previously observed
stimulus-response pairs (which we will refer to as
exchanges)? That is, can we take advantage of this
conversational setting and effectively use the infor-
mation provided by stimulus to better predict the
next word in the response? We refer to this task as
the response completion task.
Our task is different from ?chatter-bots? (Weizen-
baum, 1966), where the goal is to generate a re-
sponse to an input that would resemble a human con-
versation partner. Instead, we want to complete a
response as the replier intends to. Recently, Ritter
et. al (2011) experimented with automatic response
generation in social media. They had a similar con-
versational setting, but instead of completion based
on partial input, they attempted to generate a re-
sponse in its entirety given only the stimulus. While
many of the generated responses are deemed possi-
ble replies to the stimulus, they have a low chance
of actually matching the real response given by the
user: they reported BLEU scores between 0 and 2
for various systems. This clearly shows the diffi-
culty of the task. While we are addressing a more
modest setting, would the problem prove to be too
difficult even in this case?
In this paper, we propose a method for auto-
matic response completion. Our approach models
the generic language used in responses, as well as
the contextual information provided by the stim-
ulus. We construct a large-scale dataset of user-
generated textual exchanges, and our experimental
results show that both components help reduce typ-
ing effort. In addition, to better understand pre-
dictability of user generated English, we perform
an information-theoretic study in this conversational
setting to investigate the entropy of user-generated
content.
2 Related work
There has been previous work in the area of human-
computer interaction that examined text entry for
mobile devices (MacKenzie and Soukoreff, 2002).
In particular, one line of work looked into predic-
tive text input, which examined input effort reduc-
tion by language prediction. Previous work in pre-
dictive text input had very different focus from our
study. Oftentimes, the focus was to model actual
typing efforts using mobile device keypads, examine
the speed and cognitive load of different input meth-
ods, and evaluate with emprical user studies in lab
settings (James and Reischel, 2001; How and Kan,
2005), where the underlying technique for language
prediction can be as simple as unigram frequency
(James and Reischel, 2001), or restricted to narrow
domains such as grocery shopping lists (Nurmi et
al., 2009). In addition, to the best of our knowledge,
no previous work in predictive text input addressed
the conversationl setting.
As discussed in Section 1, the response genera-
tion task (Ritter et al 2011) also considered the con-
verstational setting, but the MT-based technique was
not well-suited to produce responses as intended by
the user. There has been extensive previous research
in language modeling (Rosenfeld, 2000). While pre-
vious work has explored Web text sources that are
?better matched to a conversational speaking style?
(Bulyko et al 2003), we are not aware of much pre-
vious work that has taken advantage of information
in the stimulus for word predictions in responses.
1490
Previous work on entropy of language stems from
the field of information theory (Shannon, 1948),
starting with Shannon (1951). An extensive bibliog-
raphy covering early related work (e.g., insights into
the structure of language via information theory, en-
tropy estimates via other techniques and/or for dif-
ferent languages, as well as a broad range of applica-
tions of such estimates) can be found in (Cover and
King, 1978). More recently, Brown et. al (1992)
computed an upper bound for the entropy of En-
glish with a trigram model, using the Brown corpus.
Some other related works on this topic include (Tea-
han and Cleary, 1996; Moradi et al 1998). There
was also a recent study using entropy in the context
of Web search (Mei and Church, 2008). In other set-
tings, entropy has also been employed as a tool for
studying the linguistic properties of ancient scripts
(e.g., Indus Script) (Rao et al 2009). While this
seems like an interesting application of information
theory for linguistic studies, it has also generated
some controversies (Farmer et al 2004).
In contrast, our work departs from traditional sce-
narios significantly. We perform entropy studies
over texts generated in online settings which are
more informal in nature. Additionally, we utilize the
properties of language predictability within a novel
application for automatically completing responses
in conversational settings. Also, in our case we do
not have to worry about issues like ?is this a lan-
guage or not?? because we work with real English
news data which include articles written by pro-
fessional editors and comments generated by users
reading those articles.
3 Model
In this section, we first state our problem more for-
mally, followed by descriptions of the basicN -gram
language model we use, as well as two approaches
that model both stimulus and preceding words in
response as the context for the next-word genera-
tion. Given the intended application, we hope to
achieve better prediction without incurring signifi-
cant increase in model size.
3.1 Problem definition
Consider a stimulus-response pair, where the stim-
ulus is a sequence of tokens s = (s1, s2, ..., sm),
and the response is a sequence of tokens r =
(r1, r2, ..., rn). Let r1..i = (r1, r2, ..., ri), our task
is to generate and rank candidates for ri+1 given s
and r1..i.
Note the models described in this section do not
assume any knowledge of partial input for ri+1. For
the setting where the first c characters of ri+1 were
also entered, we can restrict the candidate list to the
subset with the matching prefix, and use the same
ranking function.
3.2 Generic Response Language Model
First, we consider an N -gram language model
trained on all responses in the training data as our
generic response language model. Here we consider
N = 3. Normally, trigram models use back-off to
both bigrams and unigrams; in order to compare the
effectiveness of trigram models vs. bigram models
under comparable model size, we use back-off only
to unigrams in both cases:
trigram: P (ri+1 | r1..i) = ?1 ? P3(ri+1 | ri, ri?1)
+(1? ?1) ? P1(ri+1)
bigram: P (ri+1 | r1..i) = ?1 ? P2(ri+1 | ri)
+(1? ?1) ? P1(ri+1)
If we ignore the context provided by texts in the
stimulus, we can simply generate and rank candidate
words from the dictionary according to the generic
response LM: P (ri+1 | r1..i).
As we will discuss in more detail in Section 6.2,
modeling s and r1..i jointly in the prediction of ri+1
would be rather expensive. In the following sec-
tions, we follow two main approaches to break this
down into separate components: P (ri+1 | r1..i) and
P (ri+1 | s), and model each one separately.
3.3 Translating Stimuli to Responses
As mentioned in Section 1, Ritter et. al (2011) have
considered a related task of generating a response in
its entirety given only the text in the stimulus. They
cast the problem as a translation task, where the
stimulus is considered as the source language and
the response is considered as the target language.
We can adapt this approach for our response com-
pletion task.
Consider the noisy channel model used in statisti-
cal machine translation: P (r|s) ? P (r)?P (s|r). In
1491
order to predict ri+1 given r1..i and s, for each can-
didate ri+1, in principle one can marginalize over
all possible completions of r1..i+1, and rank candi-
date ri+1 by that. That is, let P(n) be the distribution
of response length, let r? be a possible completion
of r1..i+1 (i.e., a response whose first i + 1 tokens
match r1..i+1). For each possible n > i, we need to
marginalize over all possible r? of length n, and rank
ri+1 according to
P (r1..i+1 | s) =
?
n>i
P (n) ?
?
|r?|=n
P (r? | s)
Clearly this will be computationally expensive. In-
stead, we take a greedy approach, and choose ri+1
which yields the optimal partial response (without
looking ahead):
P (r1..i+1 | s) ? P (r1..i+1) ? P (s | r1..i+1)
which is equivalent to ranking candidate ri+1 by
P (ri+1 | r1..i) ? P (s | r1..i+1) (1)
Since the first component is our LM model, and the
second component is a translation model, we denote
this as the LM+TM model. We use IBM Model-1 to
learn the translation table on the training data. At
test time, equal number of candidates are generated
by each component, and combined to be ranked by
Eq. 1.
3.4 Mixture Model
One potential concern over applying the translation
model is that the response can often contain novel in-
formation not implied by the stimulus. While tech-
nically this could be generated from the so-called
null token used in machine translation (added to the
source text to account for target text with no clear
alignment in the source text), significant amount of
text corresponding to new information not in the
source text is not what null tokens are meant to be
capturing. In general, our problem here is a lack of
clear word-to-word or phrase-to-phrase mapping in
a stimulus-response pair, at least not what one would
expect in clean parallel data.
Alternatively, one can model the response genera-
tion process with a mixture model: with probability
?s, we generate a word according to a distribution
over s (P (w | s)), and with probability 1 ? ?s, we
generate a word using the response language model:
P (ri+1 | s, r1..i) = ?s ? P (ri+1 | s)
+ (1? ?s) ? P (ri+1 | r1..i)
(2)
We examine two concrete ways of exploiting the
context provided by s.
Model 1 ? LM + Selection model First, we ex-
amine a very simple instantiation of P (w | s) where
we select a token in s uniformly at random. This
is based on the intuition that to be semantically co-
herent, a reply often needs to repeat certain content
words in the stimulus. (Similar intuition has been
explored in the context of text coherency (Barzilay
and Lapata, 2005).) This is particularly useful for
words that are less frequently used: they may not
be able to receive enough statistics to be promoted
otherwise. More specifically,
P (ri+1 | s) =
1ri+1?s
|s|
We can take ?s to be a constant ?select, which can be
estimated in the training data as the probability of a
response token being a repetition of a token in the
corresponding stimulus.
Model 2 ? LM + Topic model Another way to
incorporate information provided in s is to use it to
constrict the topic in r. We can learn a topic model
over conversations in the training data using Latent
Dirchlet Allocation (LDA) (Blei et al 2003). At test
time, we identify the most likely topic of the conver-
sation based on s, and expect ri+1 to be generated
from this topic. That is,
P (ri+1 | s) = P (ri+1 | t?)
where t? = argmaxtP (topic = t | s)
More specifically, we first train a topic model on (s,
r) pairs from the training data. Given a new stim-
ulus s, we then select the highest ranked topic as
being representative of s. Note that alternatively we
could consider all possible topic assignments; in that
case we would have had to sum probabilities over
all topics, and that could also introduce noise. A
similar strategy has been previously employed for
1492
other topic modeling applications in information re-
trieval, where documents are smoothed with their
highest ranked topic (Yi and Allan, 2009). We lower
the weight ?s if P (t? | s) is low. That is, we use
?s = ?topic ? P (t? | s) in Eq. 2.
4 Data
In order to investigate text completion in a conver-
sational setting, we need to construct a large-scale
dataset with textual exchanges among users. An
ideal dataset would have been a collection of in-
stant messages, but these type of datasets are diffi-
cult to obtain given privacy concerns. To the best
of our knowledge, existing SMS (short message ser-
vice) datasets only contain isolated text spans and do
not provide enough information to reconstruct the
conversations. There are, however, a high volume
of textual exchanges taking places in public forums.
Many sites with a user comment environment allow
other users to reply to existing comments, where the
original comment and its reply can form a (stimulus,
response) pair for our purposes.
To this end, we extracted (comment, reply) pairs
from Yahoo! News2, where under each news article,
a user can post a new comment or reply to an exist-
ing comment. In fact, a popular comment can have a
long thread of replies where multi-party discussions
take place. To ensure the reply is a direct response to
the original comment, we took only the first reply to
a comment, and consider the resulting pair as a tex-
tual exchange in the form of a (stimulus, response)
pair. We gathered data from a period of 14 weeks
between March and May, 2011. A random sample
yielded a total of 1,487,995 exchanges, representing
237,040 unique users posting responses to stimuli
comments authored by 357,811 users. In the raw
dataset (i.e., before tokenization), stimuli average at
59 tokens (332 characters), and responses average at
26 tokens (144 characters).
We took the first 12 weeks of data as training data
(1,269,732 exchanges) and the rest 2 weeks of data
as test data (218,263 exchanges).
2Note that previous work has used a dataset with 1 million
Twitter conversations extracted from a scraping of the site (Rit-
ter et al 2011), where a status update and its replies in Twitter
form ?conversations?. This dataset is no longer publicly avail-
able. At the time of this writing, we were not able to identify a
data source to re-construct a dataset like that.
5 Experiments
5.1 Evaluation measures
Recall@k : Here, we follow a standard evaluation
strategy used to assess ranking quality in informa-
tion retrieval applications. For each word, we check
if the correct answer is one of the top-k tokens being
suggested. We then compute the recall at different
values of k. While this is a straight-forward mea-
sure to assess the overall quality of different top-k
lists, it is not tailored to suit our specific task of re-
sponse completion. In particular, this measure (a)
does not distinguish between (typing) savings for a
short word versus a long one, and (b) does not dis-
tinguish between the correct answer being higher up
in the list versus lower as long as the word is present
in the top-k list.
TypRed : Our main evaluation measure is based on
?reduction in typing effort for a user of the system?,
which is a more informative measure for our task.
We estimate the typing reduction via a hypothetical
typing model3 in the following manner:
Suppose we show top k predictions for a given
setting. Now, there are two possible scenarios:
1. if the user does not find the correct answer in
the top-k list, he/she gives up on this word and
will have to type the entire word. The typing
cost is then estimated to be the number of char-
acters in the word lw;
2. if the user spots the correct answer in the list,
the cost for choosing the word is proportional
to the rank of the word rankw, with a fixed cost
ratio c0. Suppose the user scrolls down the list
using the down-arrow (?) to reach the intended
word (instead of typing), then rankw ? c0 re-
flects the scrolling effort required, where c0 is
the relative cost of scrolling down versus typing
a character.
In general, pressing a fixed key can have a lower
cost than typing a new one, in addition, we can
imagine a virtual keyboard where navigational keys
occupy bigger real-estate, and thus incur less cost
to press. As a result, it?s reasonable to assume c0
value that is smaller than 1. In all our experiments,
c0 = 0.5 unless otherwise noted. Note that if the
3More accurate measures can be be developed by observing
user behavior in a lab setting. We leave this as future work.
1493
System TypRed (c = 0) TypRed (c = 1) TypRed (c = 2)
1. Generic Response LM (trigram) 15.10 22.57 14.29
2. Generic Response LM (trigram) + TM 9.03 17.53 11.56
3. Mixture Model 1: 15.18? 23.43? 15.13?
Generic Response LM (trigram) + Selection
4. Mixture Model 2: 15.10 22.57 14.33
Generic Response LM (trigram) + Topic Model
Table 1: Comparison of various prediction models (in terms of TypRed score @ rank 5) on all (stimulus,response)
pairs from a large test collection (218,263 exchanges) when the first c characters of each word are typed in by the user.
A higher score indicates better performance. ? indicates statistical significance (p < 0.05) over the baseline score.
typing model assumes a user selects the intended
word using an interface that is similar to a mousing
device, the cost may increase with rankw at a sub-
linear rate; in that case, our measure will be over-
estimating the cost.
In order to have a consistent measure that al-
ways improves as the ranking improves, we assume
a clever user who will choose to finish the word by
typing or by selecting, depending on which cost is
lower. Combining these two cases under the clever-
user model, we estimate the reduction in typing cost
for every word as follows:
TypRed(w, rankw) = 100?[1?
min(lw, rankw ? c0)
lw
]
where w is the correct word, lw is the length of
w, and rankw is the rank of w in the top-k list.
A higher value of TypRed implies higher savings
achieved in typing cost and thereby better prediction
performance.
5.2 Experimental setup
We run experiments using the models described in
Section 3 under two different settings: (1) previous
words from the response are provided, and (2) pre-
vious words from response + first c characters of the
current word are provided.
During the candidate generation phase, for every
position in the response message we present the top
1,000 candidates (as scored by the generic response
language model or mixture models). We reserve a
small subset of (?1,000) exchanges as development
data for tuning parameters from our models.
For the generic response language models, we
set the interpolation weight ?1 = 0.9. For the
selection-based mixture model, we estimate the mix-
ture weights on the training data and set ?select
(0.09). For the topic-based mixture model, we ran
a grid search with different parameter settings for
?topic on the held-out development set and chose the
value (0.01) that gave the best performance (in terms
of TypRed).
5.3 Results
Previous words from response observed: We first
present results for the setting where only previous
words from the response are provided as context.
We use TypRed scores as our evaluation measure
here (higher TypRed implies more savings in typ-
ing effort). Even with a unigram LM we achieve
a small but non-negligible reduction (TypRed=2.15)
in the typing cost. But a bigram LM significantly
improves performance (TypRed=11.91), and with
trigram LM we observe even better performance
(TypRed=15.10). Since the trigram LM yields a
high performance, we set this as our default LM for
all other models.
Recall that in all experiments, we set c0, the cost
ratio of selecting a candidate from the ranked top-
k list (via scrolling) versus typing a character to a
value of 0.5. But we also experimented with a hy-
pothetical setting where c0 = 1 and noticed that the
trigram LM achieves a slightly lower but still signif-
icant typing reduction (TypRed score of 9.58 versus
15.10 for the earlier case).
The first column of Table 1 (c = 0) compares the
performance of other models for this setting. We
find that adding a translation model (LM+TM) does
not help for this task; in fact, it results in lower
scores than using the LM alone. This suggests that
a translation-based generative approach may not be
suitable, if the goal is to predict text as intended by
the user. This is consistent with previous observa-
tions on a related task (Ritter et al 2011), as we
1494
discussed in Section 1.
In contrast, the mixture models do much better.
In fact, LM+Selection model produces better results
than trigram LM alone. We also note that estimat-
ing the mixture parameter on the training data rather
than using a fixed value increases TypRed scores:
14.02 with a fixed ?select = 0.5 versus 15.18 with
??select = 0.09. This comparison also holds for
c > 0 ? that is, a naive version of LM+Selection
that selects a word from the stimulus whenever the
prefix allows would not have worked well.
In principle the LM+Topic model is potentially
more powerful in that P (w | s) is not limited to
the words in s. However, in our experiments it
does not yield any considerable improvement over
the original LM. We postulate that this could be due
to the following reason: once the context provided
by s is reduced to the topic level, it is not specific
enough to provide additional information over pro-
ceding words in the response.
Previous words from response + first c characters
of current word observed: Table 1 also compares
the TypRed performance of all the models under set-
tings where c > 0. We notice striking improve-
ments in performance for c = 1 which is consistent
across all models. Our best model is able to save
the user approximately 23% in terms of typing ef-
fort (according to TypRed scores). Interestingly a lot
less reduction was observed for c = 2: the second
character, on average, does not improve the ranking
enough to justify the cost of typing this extra char-
acter.
Next, we pick our best model (Mixture Model 1)
and perform some further analysis. We examine the
effect of providing longer list (shown in Table 2) and
notice little further improvement beyond k = 10.
We also note that the TypRed improvement achieved
over the baseline (LM) model at rank 10 is more than
twice the gain achieved at rank 5.
We also evaluated its performance using a stan-
dard measure (Recall@k). Figure 2 plots the recall
achieved by the system at different ranks k. An in-
creasing recall at even high ranks (k = 100) sug-
gests that the quality of the candidate list retrieved
by this model is good. This also suggests that there
is still room for improvements, and we leave that as
interesting future work.
Rank (k) TypRed score
1 9.02
5 15.18
10 16.14
15 16.28
20 16.29
25 16.29
Table 2: Comparison of typing reductions achieved over
the entire test data when top k list is provided to the user.
 0 10 20 30 40 50 60 70  0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
Recall @ k
Rank 
(k)
Figure 2: Recall @ rank k for Mixture Model 1 on the
entire test data.
 
0
 
5
 
10
 
15
 
20
 
25
 
30
 
35  0
 
2
 
4
 
6
 
8
 
10
 
12
Average TypRed
Word 
length
 (# of ch
aracters
)
no lett
er prov
ided (c=
0)
first le
tter pro
vided 
(c=1)
first 2 
letters
 provid
ed (c=2
)
Figure 3: Average TypRed score versus Word length (#
of characters) for Mixture Model 1 when the first c char-
acters of the word is typed in by the user.
Finally, in Figure 3, we plot the average TypRed
scores against individual token (word) length. Fig-
ure 3 indicates that the model is able to achieve a
higher reduction for shorter words compared to very
long ones. This demonstrates the utility of such a re-
sponse completion system, especially since shorter
words are predominant in conversational settings.
We also compared the average reduction achieved
on messages of different lengths (number of words).
Overall we observe consistent reduction for differ-
ent message lengths. This suggests our system can
1495
Source Top translations
:) :) ! you ? :D
lmao lol lmao u ... i
feeling feeling feel better ! you
question . question the , to
Are I . , are yes
Table 3: Examples of a few stimulus/response transla-
tions learned using IBM Model-1.
OBAMA, USA, Fact, Meghan, GIVE, PRESIDENT, Canadian,
Mitch, Jon, Kerry, TODAY, Justice, Liberalism, ...
President, Notice, Tax, LMAO, Hmmm, Trump, people,
OBAMA, common, Aren, WAIT, Bachman, mon, McCain, ...
Great, Cut, Release, Ummm, Rest, Mark, isnt, YAHOO, Sad,
END, RON, jesus, Ugh, TRUMP, ...
Nice, Navy, Make, Interesting, Remember, Excuse, WAKE,
Hooray, Birth, mon, Yeah, Dumb, Michael, geronimo, ...
Table 4: Examples of top representative words for a few
topics generated by the LDA topic model trained on news
comment data.
be useful for both Tweet-like short messages as well
as more lengthy exchanges in detailed discussions.
5.4 Discussion
Table 3 displays some sample translations learned
using the TM model described in Section 3.3. In-
terestingly, emoticons and informal expressions like
:) or lmao in the stimulus tend to evoke similar
type of expressions in the response (as seen in Ta-
ble 3). Some translations (e.g., feeling? better) are
indicative of question/answer type of exchanges in
our data. But most of the other translations are noisy
or uninformative (e.g., Are? .). This provides fur-
ther evidence as to why a translation-based approach
is not well suited for this particular task and hence
does not perform as well as other methods.
Finally, in Table 4, we provide a few sample top-
ics generated by the LDA topic model (which is used
by Mixture Model 2 described in Section 3.4). We
find that while a few topics display some seman-
tic coherence (e.g., political figures), many of them
are noisy (or too generic) which further supports our
earlier observation that they are not useful enough to
help in the prediction task.
6 Entropy of user comments
We adapt the notion of predictability of English as
examined by Shannon (1951) from letter-prediction
to token-prediction, and define the predictability of
English as how well can the next token be predicted
when the precedingN tokens are known. How much
does the immediate context in the response help re-
duce the uncertainty? How does user-generated con-
tent compare with more formal English in this re-
spect? And how about the corresponding stimuli ?
given the preceding N tokens, does the knowledge
of stimulus further reduce the uncertainty? These
questions motivated a series of studies over entropy
in different datasets.4
6.1 Comparison of N -gram entropy
Following Shannon (1951), we consider the follow-
ing function FN , which can be called the N -gram
entropy, as the measure of predictability:
FN = ?
?
i,j
p(bi, j) log2 p(j | bi)
where bi is a block of N ? 1 tokens, j is an arbitrary
token following bi, and p(j | bi) is the conditional
probability of j given bi. This conditional entropy
reflects how much is the uncertainty of the next to-
ken reduced by knowing the precedingN?1 tokens.
Under this measure, is user-generated content
more predictable or less predictable than the more
formal ?printed? English examined by Shannon?
Maybe it is more predictable, since most users in
informal settings use simpler English, which may
contain fewer variations than the complex structures
observed in more formal English. Or perhaps it is
less predictable ? variations among different users
(who may not follow proper grammar) may lead
to more uncertainty in the prediction of ?the next
word?. Which would be the case?
To answer this question empirically, we construct
a reference dataset written in more formal English
(Df ) to be compared against the user comments
dataset described in Section 4 (Du). If Df covers
very different topics fromDu, then even if we do ob-
serve differences in entropy, it could be due to topi-
cal differences. A standard mixed-topic dataset like
4Note that our findings are not to be interpreted as prediction
performance over unseen texts. For that, one needs to compute
cross-entropy between training and test corpora. Since Section
5 is already addressing this question with proper training / test
split, in this section, we focus on the variability of language
usage in a corpus. This also avoids having to control for ?com-
parable? training/test splits in different types of datasets.
1496
the Brown Corpus (Kucera and Francis, 1967) may
not be ideal in this sense (e.g., it contains fiction cat-
egories such as ?Romance and Love Story?, which
may not be represented in our Du). Instead, we ob-
tained a sample of news articles on Yahoo! News
during March - May, 2011, and extracted unique
sentences from these articles. This yields a Df with
more comparable subject matters to Du.5
Next, we compare both the entropy over unigrams
and N -gram entropy in three datasets: the news ar-
ticle dataset described above, and comments data
(Section 4) separated into stimuli and responses. We
also report corresponding numbers computed on the
Brown Corpus as references. Note that datasets with
different vocabulary size can lead to different en-
tropy: the entropy of picking a word from the vo-
cabulary uniformly at random would have been dif-
ferent. Thus, we sample each dataset at different
rates, and plot the (conditional) entropy in the sam-
ple against the corresponding vocabulary size.
As shown in Figure 4(a), the entropy of unigrams
in Du (both stimuli and responses) is consistently
lower than in Df .6 On the other hand, both stimuli
and responses exhibit higher uncertainty in bigram
entropy (Figure 4(b)) and trigram entropy (Figure
4(c)). That is, when no contexts are provided, word
choices (from similarly-sized vocabularies) in Df is
more evenly distributed than in Du; but once the
proceeding words are given, the next word is more
predictable in Df than in Du. We postulate that
the difference in unigram entropy could be due to
(a) more balanced topic coverage in Df vs. more
skewed topic coverage in Du, or (b) professional re-
porters mastering a more balanced use of the vocab-
ulary. If (b) is the main reason, however, the lower
trigram entropy in Df would seem unexpected ?
shouldn?t professional journalists also have a more
balanced use of different phrases? Upon further
contemplation, what we hypothesized earlier could
be true: professional writers use the ?proper? En-
glish expected in news coverage, which could limit
5We note that this does not guarantee the exact same topic
distribution as in the comment data.
6For reference, Shannon (1951) estimated the entropy of En-
glish to be 11.82 bits per word, due to an incorrect calculation
of a 8727-word vocabulary given Zipf distribution. The correct
number should be 9.27 bits per word for a vocabulary size of
12,366 (Yavuz, 1978).
 9
 9.5
 10
 10.5
 11
 10000  100000  1e+06
en
tro
py
vocabulary size
brown corpusnews articlesstimulusresponse
(a) Entropy of unigrams
 4
 4.5
 5
 5.5
 6
 6.5
 7
 7.5
 10000  100000  1e+06
con
ditio
na
l en
tro
py
vocabulary size
brown corpusnews articlestimulusresponse
(b) Bigram entropy (F2)
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
 5
 10000  100000  1e+06
con
ditio
na
l en
tro
py
vocabulary size
brown corpusnews articlestimulusresponse
(c) Trigram entropy (F3)
Figure 4: Entropy of unigrams and N -gram entropy.
their trigram uncertainties; on the other hand, users
are not bound by conventions (or even grammars),
which could lead to higher variations.
Interestingly, distributions in the stimulus dataset
are closer to news articles: they have a higher uni-
gram entropy than responses, but a lower trigram en-
tropy at comparable vocabulary sizes. In particular,
recall from Section 4 that our comments dataset con-
tains roughly 237K repliers and 357K original com-
1497
 0
 1
 2
 3
 4
 5
 6
 7
 8
 10000  100000  1e+06
con
ditio
nal
 en
trop
y
vocabulary size
bigram (F 2 )bigram+stimulus (G 2 )trigram (F 3 )
Figure 5: Predicting the next word in responses: bigram
entropy vs. bigram+stimulus entropy vs. trigram entropy.
menters. If higher trigram entropy is due to variance
among different users, the stimulus dataset should
have had a higher trigram entropy. We leave an ex-
planation of this interesting behavior as future work.
6.2 Information in stimuli
We now examine the next question: does knowing
words in the stimulus further reduce the uncertainty
of the next word in the response? For simplicity,
we model the stimulus as a collection of unigrams.
Consider the following conditional entropy:
GN = ?
?
i,k,j
p(bi, j, sk) log2 p(j | bi, sk)
where bi is a block of N ? 1 tokens in a response
r, j is an arbitrary token following bi, and sk is an
arbitrary token in the corresponding stimulus s for r.
Note that for each bi, we consider every token in the
corresponding s. That is, a (stimulus, response) pair
withm and n tokens respectively generatesm?(n?
N +1) observations of (bi, j, sk) tuples. We refer to
this as the N -gram+stimulus entropy. If knowing sk
in addition to bi does not provide extra information,
then p(j | bi, sk) = p(j | bi), and GN = FN .
Figure 5 plots GN for N = 2. Interestingly, we
observe F2 > G2 > F3 (this trend holds for larger
values ofN , omitted here for clarity). That is, know-
ing both the preceding N ? 1 tokens and tokens in
the stimulus results lowers the uncertainty over the
next token in response (bigram+stimulus entropy <
bigram entropy); on the other hand, this is not as ef-
fective as knowing one more token in the preceding
block (trigram entropy< bigram+stimulus entropy).
Note that from the model size perspective, mod-
eling p(j | bi, sk) as in GN would have been much
more expensive than p(j | bi) in FN+1. Take the
case of G2 vs. F3. Let V be the vocabulary of
user comments (ignore for now differences in re-
sponses and stimuli). While both seem to require
computations over V ? V ? V , the number of
unique observed (bi, j, sk) tuples for G2 (i.e., num-
ber of unique bigrams in responses paired up with
unigrams in corresponding stimuli) is 725,458,892,
whereas the number of unique observed (bi, j) pairs
for F3 (i.e., number of unique trigrams) is only
14,692,952. This means modeling trigrams would
result in a model 2% the size of bigram+stimulus,
yet it could achieve better reduction in uncertainty.
Note that in order to reduce model complex-
ity, the models proposed in Section 3 all broke
down P (ri+1 | s, r1..i) into independent components
P (ri+1 | s) and P (ri+1 | r1..i), rather than model-
ing the effect of s and r1..i jointly as the underlying
model corresponding to GN . Indeed, it would have
been impractical to model p(j | bi, sk) directly. Our
studies confirmed the validity of this choice: even if
we look at the performance on the training data itself
(i.e., igoring data sparseness issues), the smaller tri-
gram model would have yielded better results than
the significantly more expensive bigram+stimulus
model. Still, since GN shows a consistent improve-
ment over FN , there could be more information in
the stimulus that we are not yet fully utilizing, which
can be interesting future work.
7 Conclusions
In this paper, we examined a novel application: au-
tomatic response completion in conversational set-
tings. We investigated the effectiveness of several
models that incorporate contextual information pro-
vided by the partially typed response as well as the
stimulus. We found that the partially typed response
provides strong signals. In addition, using a mix-
ture model which also incorporates stimulus content
yielded the best overall result. We also performed
empirical studies to examine the predictability of
user-generated content. Our analysis (entropy es-
timates along with upper-bound numbers observed
from experiments) suggest that there can be interest-
ing future work to explore the contextual informa-
tion provided by the stimulus more effectively and
further improve the response completion task.
1498
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL?05.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alcation. Journal of Machine
Learning Research, 3:993?1022.
Peter F. Brown, Vincent J. Della Pietra, Robert L. Mercer,
Stephen A. Della Pietra, and Jennifer C. Lai. 1992. An
estimate of an upper bound for the entropy of English.
Comput. Linguist., 18:31?40.
Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.
2003. Getting more mileage from web text sources for
conversational speech language modeling using class-
dependent mixtures. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology: companion volume of the Proceed-
ings of HLT-NAACL 2003?short papers - Volume 2,
NAACL-Short ?03, pages 7?9.
Thomas M. Cover and Roger C. King. 1978. A con-
vergent gambling estimate of the entropy of English.
IEEE Transactions on Information Theory, 24:413?
421.
Steve Farmer, Richard Sproat, and Michael Witzel. 2004.
The collapse of the Indus-script thesis: The myth of a
literate Harappan civilization. Electronic Journal of
Vedic Studies, 11:379?423 and 623?656.
Yijue How and Min-Yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
phones. In Proceedings of the Human Computer In-
terfaces International (HCII).
Christina L. James and Kelly M. Reischel. 2001. Text
input for mobile devices: comparing model prediction
to actual performance. In Proceedings of the SIGCHI
conference on Human factors in computing systems,
CHI ?01, pages 365?371, New York, NY, USA. ACM.
Henry Kucera and W. Nelson Francis. 1967. Com-
putational analysis of present-day American English.
Brown University Press.
I. Scott MacKenzie and R. William Soukoreff. 2002.
Text entry for mobile computing: Models and meth-
ods,theory and practice. Human-Computer Interac-
tion, 17(2-3):147?198.
Qiaozhu Mei and Kenneth Church. 2008. Entropy of
search logs: how hard is search? with personalization?
with backoff? In Proceedings of the international con-
ference on Web search and web data mining, WSDM
?08, pages 45?54.
Hamid Moradi, Jerzy W. Grzymala-busse, and James A.
Roberts. 1998. Entropy of english text: experiments
with humans and a machine learning system based on
rough sets. Information Sciences, 104:31?47.
Petteri Nurmi, Andreas Forsblom, Patrik Flore?en, Peter
Peltonen, and Petri Saarikko. 2009. Predictive text
input in a mobile shopping assistant: methods and in-
terface design. In Proceedings of the 14th interna-
tional conference on Intelligent user interfaces, IUI
?09, pages 435?438, New York, NY, USA. ACM.
Rajesh P. N. Rao, Nisha Yadav, Mayank N. Vahia,
Hrishikesh Joglekar, R. Adhikari, and Iravatham Ma-
hadevan. 2009. Entropic evidence for linguistic struc-
ture in the Indus script. Science.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 583?593.
Ronald Rosenfeld. 2000. Two decades of statistical lan-
guage modeling: Where do we go from here? Pro-
ceedings of the IEEE, 88.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379?423 and 623?656.
Claude E. Shannon. 1951. Prediction and entropy
of printed English. Bell System Technical Journal,
30:50?64.
W. J. Teahan and John G. Cleary. 1996. The entropy of
English using PPM-based models. In In Data Com-
pression Conference, pages 53?62. IEEE Computer
Society Press.
Joseph Weizenbaum. 1966. Eliza: a computer program
for the study of natural language communication be-
tween man and machine. Commun. ACM, 9:36?45.
D. Yavuz. 1978. Zipf?s law and entropy (Corresp.).
IEEE Transactions on Information Theory, 20:650.
Xing Yi and James Allan. 2009. A comparative study
of utilizing topic models for information retrieval. In
Proceedings of the European Conference on IR Re-
search on Advances in Information Retrieval, pages
29?41.
1499
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 365?368,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
For the sake of simplicity:
Unsupervised extraction of lexical simplifications from Wikipedia
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil and Lillian Lee
my89@cornell.edu, bopang@yahoo-inc.com, cristian@cs.cornell.edu, llee@cs.cornell.edu
Abstract
We report on work in progress on extract-
ing lexical simplifications (e.g., ?collaborate?
? ?work together?), focusing on utilizing
edit histories in Simple English Wikipedia for
this task. We consider two main approaches:
(1) deriving simplification probabilities via an
edit model that accounts for a mixture of dif-
ferent operations, and (2) using metadata to
focus on edits that are more likely to be sim-
plification operations. We find our methods
to outperform a reasonable baseline and yield
many high-quality lexical simplifications not
included in an independently-created manu-
ally prepared list.
1 Introduction
Nothing is more simple than greatness; indeed, to be
simple is to be great. ?Emerson, Literary Ethics
Style is an important aspect of information pre-
sentation; indeed, different contexts call for differ-
ent styles. Here, we consider an important dimen-
sion of style, namely, simplicity. Systems that can
rewrite text into simpler versions promise to make
information available to a broader audience, such as
non-native speakers, children, laypeople, and so on.
One major effort to produce such text is the
Simple English Wikipedia (henceforth SimpleEW)1,
a sort of spin-off of the well-known English
Wikipedia (henceforth ComplexEW) where hu-
man editors enforce simplicity of language through
rewriting. The crux of our proposal is to learn lexical
simplifications from SimpleEW edit histories, thus
leveraging the efforts of the 18K pseudonymous in-
dividuals who work on SimpleEW. Importantly, not
all the changes on SimpleEW are simplifications; we
thus also make use of ComplexEW edits to filter out
non-simplifications.
Related work and related problems Previous
work usually involves general syntactic-level trans-
1http://simple.wikipedia.org
formation rules [1, 9, 10].2 In contrast, we explore
data-driven methods to learn lexical simplifications
(e.g., ?collaborate? ? ?work together?), which are
highly specific to the lexical items involved and thus
cannot be captured by a few general rules.
Simplification is strongly related to but distinct
from paraphrasing and machine translation (MT).
While it can be considered a directional form of
the former, it differs in spirit because simplification
must trade off meaning preservation (central to para-
phrasing) against complexity reduction (not a con-
sideration in paraphrasing). Simplification can also
be considered to be a form of MT in which the two
?languages? in question are highly related. How-
ever, note that ComplexEW and SimpleEW do not
together constitute a clean parallel corpus, but rather
an extremely noisy comparable corpus. For ex-
ample, Complex/Simple same-topic document pairs
are often written completely independently of each
other, and even when it is possible to get good
sentence alignments between them, the sentence
pairs may reflect operations other than simplifica-
tion, such as corrections, additions, or edit spam.
Our work joins others in using Wikipedia revi-
sions to learn interesting types of directional lexical
relations, e.g, ?eggcorns?3 [7] and entailments [8].
2 Method
As mentioned above, a key idea in our work is to
utilize SimpleEW edits. The primary difficulty in
working with these modifications is that they include
not only simplifications but also edits that serve
other functions, such as spam removal or correction
of grammar or factual content (?fixes?). We describe
two main approaches to this problem: a probabilis-
tic model that captures this mixture of different edit
operations (?2.1), and the use of metadata to filter
out undesirable revisions (?2.2).
2One exception [5] changes verb tense and replaces pro-
nouns. Other lexical-level work focuses on medical text [4, 2],
or uses frequency-filtered WordNet synonyms [3].
3A type of lexical corruption, e.g., ?acorn???eggcorn?.
365
2.1 Edit model
We say that the kth article in a Wikipedia corre-
sponds to (among other things) a title or topic (e.g.,
?Cat?) and a sequence ~dk of article versions caused
by successive edits. For a given lexical item or
phrase A, we write A ? ~dk if there is any version
in ~dk that contains A. From each ~dk we extract a
collection ek = (ek,1, ek,2, . . . , ek,nk) of lexical edit
instances, repeats allowed, where ek,i = A ? a
means that phrase A in one version was changed to
a in the next, A 6= a; e.g., ?stands for? ? ?is the
same as?. (We defer detailed description of how we
extract lexical edit instances from data to ?3.1.) We
denote the collection of ~dk in ComplexEW and Sim-
pleEW as C and S, respectively.
There are at least four possible edit operations: fix
(o1), simplify (o2), no-op (o3), or spam (o4). How-
ever, for this initial work we assume P (o4) = 0.4
Let P (oi | A) be the probability that oi is applied
to A, and P (a | A, oi) be the probability of A ? a
given that the operation is oi. The key quantities of
interest are P (o2 | A) in S, which is the probability
thatA should be simplified, and P (a | A, o2), which
yields proper simplifications of A. We start with an
equation that models the probability that a phrase A
is edited into a:
P (a | A) =
?
oi??
P (oi | A)P (a | A, oi), (1)
where ? is the set of edit operations. This involves
the desired parameters, which we solve for by esti-
mating the others from data, as described next.
Estimation Note that P (a | A, o3) = 0 if A 6= a.
Thus, if we have estimates for o1-related probabili-
ties, we can derive o2-related probabilities via Equa-
tion 1. To begin with, we make the working as-
sumption that occurrences of simplification in Com-
plexEW are negligible in comparison to fixes. Since
we are also currently ignoring edit spam, we thus
assume that only o1 edits occur in ComplexEW.5
Let fC(A) be the fraction of ~dk in C
containing A in which A is modified:
fC(A) =
|{~dk?C|?a,i such that ek,i=A?a}|
|{~dk?C|A?~dk}|
.
4Spam/vandalism detection is a direction for future work.
5This assumption also provides useful constraints to EM,
which we plan to apply in the future, by reducing the number of
parameter settings yielding the same likelihood.
We similarly define fS(A) on ~dk in S. Note that we
count topics (version sequences), not individual ver-
sions: if A appears at some point and is not edited
until 50 revisions later, we should not conclude
that A is unlikely to be rewritten; for example, the
intervening revisions could all be minor additions,
or part of an edit war.
If we assume that the probability of any particular
fix operation being applied in SimpleEW is propor-
tional to that in ComplexEW? e.g., the SimpleEW
fix rate might be dampened because already-edited
ComplexEW articles are copied over ? we have6
P? (o1 | A) = ?fC(A)
where 0 ? ? ? 1. Note that in SimpleEW,
P (o1 ? o2 | A) = P (o1 | A) + P (o2 | A),
where P (o1 ? o2 | A) is the probability that A is
changed to a different word in SimpleEW, which we
estimate as P? (o1 ? o2 | A) = fS(A). We then set
P?(o2 | A) = max (0, fS(A)? ?fC(A)).
Next, under our working assumption, we estimate
the probability of A being changed to a as a fix
by the proportion of ComplexEW edit instances that
rewrite A to a:
P? (a | A, o1) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? C}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? C}|
.
A natural estimate for the conditional probability
of A being rewritten to a under any operation type
is based on observations of A ? a in SimpleEW,
since that is the corpus wherein both operations are
assumed to occur:
P? (a | A) =
|{(k, i) pairs | ek,i = A? a ? ~dk ? S}|
?
a? |{(k, i) pairs | ek,i = A? a
? ? ~dk ? S}|
.
Thus, from (1) we get that for A 6= a:
P?(a | A,o2) =
P?(a | A)? P?(o1 | A)P?(a | A,o1)
P?(o2 | A)
.
2.2 Metadata-based methods
Wiki editors have the option of associating a com-
ment with each revision, and such comments some-
times indicate the intent of the revision. We there-
fore sought to use comments to identify ?trusted?
6Throughout, ?hats? denote estimates.
366
revisions wherein the extracted lexical edit instances
(see ?3.1) would be likely to be simplifications.
Let ~rk = (r1k, . . . , r
i
k, . . .) be the sequence of revi-
sions for the kth article in SimpleEW, where rik is the
set of lexical edit instances (A ? a) extracted from
the ith modification of the document. Let cik be the
comment that accompanies rik, and conversely, let
R(Set) = {rik|c
i
k ? Set}.
We start with a seed set of trusted comments,
Seed. To initialize it, we manually inspected a small
sample of the 700K+ SimpleEW revisions that bear
comments, and found that comments containing a
word matching the regular expression *simpl* (e.g,
?simplify?) seem promising. We thus set Seed :=
{ ? simpl?} (abusing notation).
The SIMPL method Given a set of trusted revi-
sions TRev (in our case TRev = R(Seed)), we
score each A ? a ? TRev by the point-wise mu-
tual information (PMI) between A and a.7 We write
RANK(TRev) to denote the PMI-based ranking of
A? a ? TRev, and use SIMPL to denote our most
basic ranking method, RANK(R(Seed)).
Two ideas for bootstrapping We also considered
bootstrapping as a way to be able to utilize revisions
whose comments are not in the initial Seed set.
Our first idea was to iteratively expand the set
of trusted comments to include those that most of-
ten accompany already highly ranked simplifica-
tions. Unfortunately, our initial implementations in-
volved many parameters (upper and lower comment-
frequency thresholds, number of highly ranked sim-
plifications to consider, number of comments to add
per iteration), making it relatively difficult to tune;
we thus omit its results.
Our second idea was to iteratively expand the
set of trusted revisions, adding those that contain
already highly ranked simplifications. While our
initial implementation had fewer parameters than
the method sketched above, it tended to terminate
quickly, so that not many new simplifications were
found; so, again, we do not report results here.
An important direction for future work is to differ-
entially weight the edit instances within a revision,
as opposed to placing equal trust in all of them; this
7PMI seemed to outperform raw frequency and conditional
probability.
could prevent our bootstrapping methods from giv-
ing common fixes (e.g., ?a?? ?the?) high scores.
3 Evaluation8
3.1 Data
We obtained the revision histories of both Sim-
pleEW (November 2009 snapshot) and ComplexEW
(January 2008 snapshot). In total, ?1.5M revisions
for 81733 SimpleEW articles were processed (only
30% involved textual changes). For ComplexEW,
we processed ?16M revisions for 19407 articles.
Extracting lexical edit instances. For each ar-
ticle, we aligned sentences in each pair of adja-
cent versions using tf-idf scores in a way simi-
lar to Nelken and Shieber [6] (this produced sat-
isfying results because revisions tended to repre-
sent small changes). From the aligned sentence
pairs, we obtained the aforementioned lexical edit
instances A ? a. Since the focus of our study
was not word alignment, we used a simple method
that identified the longest differing segments (based
on word boundaries) between each sentence, except
that to prevent the extraction of entire (highly non-
matching) sentences, we filtered out A ? a pairs if
either A or a contained more than five words.
3.2 Comparison points
Baselines RANDOM returns lexical edit instances
drawn uniformly at random from among those ex-
tracted from SimpleEW. FREQUENT returns the
most frequent lexical edit instances extracted from
SimpleEW.
Dictionary of simplifications The SimpleEW ed-
itor ?Spencerk? (Spencer Kelly) has assembled a list
of simple words and simplifications using a combi-
nation of dictionaries and manual effort9. He pro-
vides a list of 17,900 simple words ? words that do
not need further simplification ? and a list of 2000
transformation pairs. We did not use Spencerk?s set
as the gold standard because many transformations
we found to be reasonable were not on his list. In-
stead, we measured our agreement with the list of
transformations he assembled (SPLIST).
8Results at http://www.cs.cornell.edu/home/llee/data/simple
9http://www.spencerwaterbed.com/soft/simple/about.html
367
3.3 Preliminary results
The top 100 pairs from each system (edit model10
and SIMPL and the two baselines) plus 100 ran-
domly selected pairs from SPLIST were mixed and
all presented in random order to three native English
speakers and three non-native English speakers (all
non-authors). Each pair was presented in random
orientation (i.e., either as A ? a or as a ? A),
and the labels included ?simpler?, ?more complex?,
?equal?, ?unrelated?, and ??? (?hard to judge?). The
first two labels correspond to simplifications for the
orientations A ? a and a ? A, respectively. Col-
lapsing the 5 labels into ?simplification?, ?not a sim-
plification?, and ??? yields reasonable agreement
among the 3 native speakers (? = 0.69; 75.3% of the
time all three agreed on the same label). While we
postulated that non-native speakers11 might be more
sensitive to what was simpler, we note that they dis-
agreed more than the native speakers (? = 0.49) and
reported having to consult a dictionary. The native-
speaker majority label was used in our evaluations.
Here are the results; ?-x-y? means that x and y are
the number of instances discarded from the precision
calculation for having no majority label or majority
label ???, respectively:
Method Prec@100 # of pairs
SPLIST 86% (-0-0) 2000
Edit model 77% (-0-1) 1079
SIMPL 66% (-0-0) 2970
FREQUENT 17% (-1-7) -
RANDOM 17% (-1-4) -
Both baselines yielded very low precisions ?
clearly not all (frequent) edits in SimpleEW were
simplifications. Furthermore, the edit model yielded
higher precision than SIMPL for the top 100 pairs.
(Note that we only examined one simplification per
A for those A where P? (o2 | A) was well-defined;
thus ?# of pairs? does not directly reflect the full
potential recall that either method can achieve.)
Both, however, produced many high-quality pairs
(62% and 71% of the correct pairs) not included in
SPLIST. We also found the pairs produced by these
two systems to be complementary to each other. We
10We only considered those A such that freq(A ? ?) >
1 ? freq(A) > 100 on both SimpleEW and ComplexEW. The
final top 100 A ? a pairs were those with As with the highest
P (o2 | A). We set ? = 1.
11Native languages: Russian; Russian; Russian and Kazakh.
believe that these two approaches provide a good
starting point for further explorations.
Finally, some examples of simplifications found
by our methods: ?stands for? ? ?is the same
as?, ?indigenous? ? ?native?, ?permitted? ? ?al-
lowed?, ?concealed? ? ?hidden?, ?collapsed? ?
?fell down?, ?annually?? ?every year?.
3.4 Future work
Further evaluation could include comparison with
machine-translation and paraphrasing algorithms. It
would be interesting to use our proposed estimates
as initialization for EM-style iterative re-estimation.
Another idea would be to estimate simplification pri-
ors based on a model of inherent lexical complexity;
some possible starting points are number of sylla-
bles (which is used in various readability formulae)
or word length.
Acknowledgments We first wish to thank Ainur Yessenalina
for initial investigations and helpful comments. We are
also thankful to R. Barzilay, T. Bruce, C. Callison-Burch, J.
Cantwell, M. Dredze, C. Napoles, E. Gabrilovich, & the review-
ers for helpful comments; W. Arms and L. Walle for access to
the Cornell Hadoop cluster; J. Cantwell for access to computa-
tional resources; R. Hwa & A. Owens for annotation software;
M. Ulinski for preliminary explorations; J. Cantwell, M. Ott, J.
Silverstein, J. Yatskar, Y. Yatskar, & A. Yessenalina for annota-
tions. Supported by NSF grant IIS-0910664.
References
[1] R. Chandrasekar, B. Srinivas. Automatic induction of rules
for text simplification. Knowledge-Based Systems, 1997.
[2] L. Dele?ger, P. Zweigenbaum. Extracting lay paraphrases
of specialized expressions from monolingual comparable
medical corpora. Workshop on Building and Using Com-
parable Corpora, 2009.
[3] S. Devlin, J. Tait. The use of a psycholinguistic database in
the simplification of text for aphasic readers. In Linguistic
Databases, 1998.
[4] N. Elhadad, K. Sutaria. Mining a lexicon of technical terms
and lay equivalents. Workshop on BioNLP, 2007.
[5] B. Beigman Klebanov, K. Knight, D. Marcu. Text simplifi-
cation for information-seeking applications. OTM Confer-
ences, 2004.
[6] R. Nelken, S. M. Shieber. Towards robust context-sensitive
sentence alignment for monolingual corpora. EACL, 2006.
[7] R. Nelken, E. Yamangil. Mining Wikipedia?s article re-
vision history for training computational linguistics algo-
rithms. WikiAI, 2008.
[8] E. Shnarch, L. Barak, I. Dagan. Extracting lexical reference
rules from Wikipedia. ACL, 2009.
[9] A. Siddharthan, A. Nenkova, K. McKeown. Syntactic
simplification for improving content selection in multi-
document summarization. COLING, 2004.
[10] D. Vickrey, D. Koller. Sentence simplification for seman-
tic role labeling/ ACL, 2008.
368
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 135?140,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Search in the Lost Sense of ?Query?: Question Formulation in Web Search
Queries and its Temporal Changes
Bo Pang Ravi Kumar
Yahoo! Research
701 First Ave
Sunnyvale, CA 94089
{bopang,ravikumar}@yahoo-inc.com
Abstract
Web search is an information-seeking activ-
ity. Often times, this amounts to a user seek-
ing answers to a question. However, queries,
which encode user?s information need, are
typically not expressed as full-length natural
language sentences ? in particular, as ques-
tions. Rather, they consist of one or more text
fragments. As humans become more search-
engine-savvy, do natural-language questions
still have a role to play in web search?
Through a systematic, large-scale study, we
find to our surprise that as time goes by, web
users are more likely to use questions to ex-
press their search intent.
1 Introduction
A web search query is the text users enter into the
search box of a search engine to describe their infor-
mation need. By dictionary definition, a ?query? is
a question. Indeed, a natural way to seek informa-
tion is to pose questions in a natural-language form
(?how many calories in a banana?). Present day web
search queries, however, have largely lost the orig-
inal semantics of the word query: they tend to be
fragmented phrases (?banana calories?) instead of
questions. This could be a result of users learning
to express their information need in search-engine-
friendly forms: shorter queries fetch more results
and content words determine relevance.
We ask a simple question: as users become
more familiar with the nuances of web search,
are question-queries ? natural-language questions
posed as queries ? gradually disappearing from the
search vernacular? If true, then the need for search
engines to understand question-queries is moot.
Anecdotal evidence from Google trends suggests
it could be the opposite. For specific phrases, one
can observe how the fraction of query traffic con-
taining the phrase1 changes over time. For instance,
as shown next, the fraction of query traffic contain-
ing ?how to? has in fact been going up since 2007.
However, such anecdotal evidence cannot fully
support claims about general behavior in query for-
mulation. In particular, this upward trend could
be due to changes in the kind of information users
are now seeking from the Web, e.g., as a result of
growing popularity of Q&A sites or as people en-
trust search engines with more complex information
needs; supporting the latter, in a very recent study,
Aula et al (2010) noted that users tend to formu-
late more question-queries when faced with difficult
search tasks. We, on the other hand, are interested in
a more subtle trend: for content that could easily be
reached via non-question-queries, are people more
likely to use question-queries over time?
We perform a systematic study of question-
queries in web search. We find that question-queries
account for ? 2% of all the query traffic and ? 6%
of all unique queries. Even when averaged over in-
tents, the fraction of question-queries to reach the
1www.google.com/intl/en/trends/about.html
135
same content is growing over the course of one year.
The growth is measured but statistically significant.
The study of long-term temporal behavior of
question-queries, we believe, is novel. Previous
work has explored building question-answering sys-
tems using web knowledge and Wikipedia (see Du-
mais et al (2002) and the references therein). Our
findings call for a greater synergy between QA and
IR in the web search context and an improved un-
derstanding of question-queries by search engines.
2 Related work
There has been some work on studying and exploit-
ing linguistic structure in web queries. Spink and
Ozmultu (2002) investigate the difference in user
behavior between a search engine that encouraged
questions and one that did not; they did not explore
intent aspects. Barr et al (2008) analyze the occur-
rence of POS tags in queries.
Query log analysis is an active research area.
While we also analyze queries, our goal is very dif-
ferent: we are interested in certain linguistic aspects
of queries, which are usually secondary in log anal-
ysis. For a comprehensive survey on this topic, see
the monograph of Silvestri (2010). There has been
some work on short-term (hourly) temporal analysis
of query logs, e.g., Beitzel et al (2004) and on long
queries, e.g., Bendersky and Croft (2009).
Using co-clicking to infer query-query relation-
ships was proposed by Baeza-Yates and Tiberi
(2007). Their work, however, is more about the
query-click graph and its properties. There has also
been a lot of work on query clustering by common
intent using this graph, e.g., Yi and Maghoul (2009)
and Wen et al (2002). We focus not on clustering
but on understanding the expression of intent.
3 Method
We address the main thesis of the work by retrospec-
tively studying queries issued to a search engine over
the course of 12 consecutive months.
Q-queries. First we define a notion of question
queries based on the standard definition of questions
in English. A query is a Q-query if it contains at
least two tokens and satisfies one of the following
criteria.
(i) Starts with one of the interrogative words, or
Q-words (?how, what, which, why, where, when,
who, whose?).
(ii) Starts with ?do, does, did, can, could, has,
have, is, was, are, were, should?. While this ensures
a legitimate question in well-formed English texts,
in queries, we may get ?do not call list?. Thus, we
insist that the second token cannot be ?not?.
(iii) Ends with a question mark (???).
Otherwise it is a Q-query. The list of key-
words (Q-words) is chosen using an English lexi-
con. Words such as ?shall? and ?will?, even though
interrogative in nature, introduce more ambiguity
(e.g., ?shall we dance lyrics? or ?will smith?) and
do not account for much traffic in general; discard-
ing such words will not impact the findings.
Co-click data on ?stable? URLs. We work with the
set of queries collected between Dec 2009 and Nov
2010 from the Yahoo! querylog. We gradually refine
this raw data to study changes in query formulation
over comparable and consistent search intents.
1. Sall consists of all incoming search queries af-
ter preprocessing: browser cookies2 that correspond
to possible robots/automated queries and queries
with non-alphanumeric characters are discarded; all
punctuations, with the exception of ???, are re-
moved; all remaining tokens are lower-cased, with
the original word ordering preserved.
2. Call consists of queries formulated for similar
search intent, where intent was approximated by the
result URL clicked in response to the query. That is,
we assume queries that lead to a click on the same
URL are issued with similar information need. To
reduce the noise introduced by this approximation
when users explore beyond their original intent, we
focus on (query, URL) pairs where the URL u was
clicked from top-10 search results3 for query q.
3. Uc50Q is our final dataset with queries grouped
over ?stable? intents. First, for each month m, we
collect the multiset Ci of all (q, ui) pairs for each
clicked URL ui, where the size of Ci is the to-
tal number of clicks received by ui during m. Let
2We approximate user identity via the browser cookie
(which are anonymized for privacy). While browser cookies
can be unreliable (e.g, they can be cleared), in practice, they are
the best proxy for unique users.
3In any case, clicks beyond top-10 results (i.e., the first result
page) only account for a small fraction of click traffic.
136
U (m) be all URLs for month m. We restrict to
U =
?
m U
(m). This set represents intents and con-
tents that persist over the 12-month period, allowing
us to examine query formulation changes over time.
We then extract a subset UQ of U consisting of
the URLs associated with at least one Q-query in
one of the months. Interestingly, we observe that
|UQ|
|U | = 0.55: roughly half of the ?stable? URLs are
associated with at least one Q-query!
Finally, we restrict to URLs with at least 50
clicks in each month to obtain reliable statistics later
on. U c50Q consists of a random sample of such
URLs, with 423,672 unique URLs and 231M unique
queries (of which 21M (9%) are Q-queries).
Q-level. For each search intent (i.e., a click on u), to
capture the degree to which people express that in-
tent via Q-queries, we define its Q-level as the frac-
tion of clicks on u from Q-queries. Since we are
interested in general query formulation behavior, we
do not want our analysis to be dominated by trends
in popular intents. Thus, we take macro-average
of Q-level over different URLs in a given month,
and our main aim is to explore long-term temporal
changes in this value.
4 Results
4.1 Characteristics of Q-queries
Are Q-queries really questions? We examine 100
random queries from the least frequent Q-queries
in our dataset. Only two are false-positives: ?who
wants to be a millionaire game? (TV show-based
game) and ?can tho nail florida? (a local business).
The rest are indeed question-like: while they are not
necessarily grammatical, the desire to express the in-
tent by posing it as a question is unmistakable.
Still, are they mostly ostensible questions like
?how find network key?, or well-formed full-length
questions like ?where can i watch one tree hill sea-
son 7 episode 2?? (Both are present in our dataset.)
Given the lack of syntactic parsers that are ap-
propriate for search queries, we address this ques-
tion using a more robust measure: the probability
mass of function words. In contrast to content words
(open class words), function words (closed class
words) have little lexical meaning ? they mainly
provide grammatical information and are defined by
their syntactic behavior. As a result, most function
words are treated as stopwords in IR systems, and
web users often exclude them from queries. A high
fraction of function words is a signal of queries be-
having more like normal texts in terms of the amount
of tokens ?spent? to be structurally complete.
We use the list of function words from Sequence
Publishing4, and augment the auxiliary verbs with
a list from Wikipedia5. Since most of the Q-words
used to identifyQ-queries are function words them-
selves, a higher fraction of function words in Q-
queries is immediate. We remove the word used for
Q-query identification from the input string to avoid
trivial observations. That is, ?how find network key?
becomes ?find network key?, with zero contribution
to the probability mass of function words.
The following table summarizes the probabil-
ity mass of function words in all unique Q-
queries and Q-queries in U c50Q , compared to two
natural-language corpora: a sample of 6.6M ques-
tions posted by web users on a community-based
question-answering site, Yahoo! Answers (QY!A),
and the Brown corpus6 (Br). All datasets went
through the same query preprocessing steps, as well
as the Q-word-removal step described above.
Type Q-q Q-q QY!A Br
Auxiliary verbs 0.4 8.5 8.1 5.8
Conjunctions 1.2 1.4 3.4 4.5
Determiners 2.0 8.7 8.2 10.1
Prepositions 6.5 13.7 10.1 13.3
Pronouns 0.7 3.4 9.1 5.9
Quantifiers 0.1 0.7 0.4 0.6
Ambiguous 2.1 2.7 4.6 7.0
Total 12.9 39.0 43.9 47.1
Clearly, Q-queries are more similar to the two
natural-language corpora in terms of this shallow
measure of structural completeness. Notably, they
contain a much higher fraction of function words
compared to Q-queries, even though they express
similar search intent.
This trend is consistent when we break down by
type, except that Q-queries contain fewer conjunc-
tions and pronouns compared to QY!A and Br. This
happens since Q-queries do not tend to have com-
plex sentence or discourse structures. Our results
4www.sequencepublishing.com/academic.html.
5en.wikipedia.org/wiki/List_of_English_
auxiliary_verbs
6khnt.aksis.uib.no/icame/manuals/brown/
137
suggest that if users express their information need
in a question form, they are more likely to express it
in a structurally complete fashion.
Lastly, we examine the length of Q-queries and
Q-queries in each multiset Ci. If Q-queries con-
tain other content words in place of Q-words to ex-
press similar intent (e.g., ?steps to publish a book?
vs. ?how to publish a book?), we should observe a
similar length distribution. Instead, we find that on
average Q-queries tend to be longer than Q-queries
by 3.58 tokens. Even if we remove theQ-word and a
companion function word, Q-queries would still be
one to two words longer. In web search, where the
overall query traffic averages at shorter than 3 to-
kens, this is a significant difference in length ? ap-
parently people are more generous with words when
they write in the question mode.
4.2 Trend of Q-level
We have just confirmed that Q-queries resemble
natural-language questions to a certain degree. Next
we turn to our central question: how does Q-level
(macro-averaged over different intents) change over
time? To this end, we compute a linear regression
of Q-level across 12 months, conduct a hypothesis
test (with the null hypothesis being the slope of the
regression equal to zero), and report the P -value for
two-tailed t-test.
As shown in Figure 1(a), there is a mid-range cor-
relation between Q-level and time in U c50Q (corre-
lation coefficient r = 0.78). While the trend is
measured with slope = 0.000678 (it would be sur-
prising if the slope for the average behavior of this
many users were any steeper!), it is statistically sig-
nificant that Q-level is growing over time: the null
hypothesis is rejected with P < 0.001. That is, over
a large collection of intents and contents, users are
becoming more likely to formulate queries in ques-
tion forms, even though such content could easily be
reached via non-question-queries.
One may question if this is an artifact of using
?stable? clicked URLs. Could it be that search en-
gines learn from user behavior data and gradually
present such URLs in lower ranks (i.e., shown ear-
lier in the page; e.g., first result returned), which in-
creases the chance of them being seen and clicked?
This is indeed true, but it holds for both Q-queries
andQ-queries. More specifically, if we consider the
 0.039
 0.041
 0.045
 2  4  6  8  10  12
Q-le
vel
month
slope = 0.000678
(a) Q-level
 0.013
 0.015
 0.017
 0.019
 0.021
 1  10  100  1000
ave
rage
 Q-ra
te
user activity level in a month
(b) Q-rate
Figure 1: Q-level for different months in U c50Q ; Q-rate
for users with different activity levels in Sall.
rank of the clicked URL as a measure of search re-
sult quality (the lower the better), we observe im-
provements for both Q-queries and Q-queries over
time (and the gap is shortening). However, the av-
erage click position for Q-queries is consistently
higher in rank throughout the time. Thus, it is
not because the search engine is answering the Q-
queries better than Q-queries that users start to use
Q-queries more. While we might still postulate that
the decreasing gap in search quality (as measured
by click positions) might have contributed to the in-
crease in Q-level, if we examine the co-click data
without the stability constraint, we observe the fol-
lowing: an increasing click traffic from Q-queries
and an increasing gap in click positions between Q-
queries and Q-queries.
In addition, we also observe an upward trend for
the overall incoming query traffic accounted for by
Q-queries in Sall (slope = 0.000142, r = 0.618,
P < 0.05). The upward trend in the fraction of
unique queries coming fromQ-queries is even more
pronounced (slope = 0.000626, r = 0.888, P <
0.001). While this trend could be partly due to dif-
138
ferences in search intent, it nonetheless reinforces
the general message of increases inQ-queries usage.
This is also consistent with the anecdotal evidence
from Google trends (Section 1) suggesting that the
trends we observe are not search-engine specific and
have been in existence for over a year.7
4.3 Observations in the overall query traffic
Note that in U c50Q , Q-level averages ? 4%; recall
also for a rather significant portion of the web con-
tent, at least one user chose to formulate his/her in-
tent in Q-queries ( |UQ||U | = 0.55). Both reflect the
prevalence of Q-queries. Is that specific to well-
constrained datasets like U c50Q ? We examine the
overall incoming queries represented in Sall. On av-
erage, Q-queries account for 1.8% of query traffic.
5.7% of all unique queries are Q-queries, indicating
greater diversity in Q-queries.
What types of questions do users ask? The table
below shows the top Q-words in the query traffic;
?how? and ?what? lead the chart.
word % word % word %
how 0.7444 what 0.4360 where 0.0928
? 0.0715 who 0.0684 is 0.0676
can 0.0658 why 0.0648 when 0.0549
do 0.0295 does 0.0294 are 0.0193
which 0.0172 did 0.0075 should 0.0072
How does the query traffic associated with differ-
ent Q-words change over time? We observe that all
slopes are positive (though not all are statistically
significant), indicating that the increase inQ-queries
happens for different types of questions.
Is it only a small number of amateur users who
persist withQ-queries? We defineQ-rate for a given
user (approximated by browser cookie b) as the frac-
tion of query traffic accounted for byQ-queries. We
plot this against b?s activity level, measured by the
number of queries issued by b in a month. We binned
users by their activity levels on the log2-scale and
compute the average Q-rate for that bin. As shown
in Figure 1(b), relatively light users who issue up
to 30 queries per month do not differ much in Q-
rate on an aggregate level. Interestingly, mid-range
users (around 300 queries per month) exhibit higher
7An explanation of why the upward trend starts at the end
of 2007 is beyond the scope of this work; we postulate that this
coincides with the rise in popularity of community-based Q&A
sites.
Q-rate than the light users. And for the most heavy
users, the Q-rate tapers down.
Furthermore, taking the data from the last month
in Sall, we observe that for users who issued at least
258 queries, more than half of them have issued at
least one Q-query in that month ? using Q-queries
is rather prevalent among non-amateur users.
5 Concluding remarks
In this paper we study the prevalence and charac-
teristics of natural-language questions in web search
queries. To the best of our knowledge, this is the
first study of such kind. Our study shows that ques-
tions in web search queries are both prevalent and
temporally increasing. Our central observation is
that this trend holds in terms of how people formu-
late queries for the same search intent (in the care-
fully constructed dataset U c50Q ). The message is re-
inforced as we observe a similar trend in the per-
centage of overall incoming query traffic being Q-
queries; in addition, anectodal evidence can be ob-
tained from Google trends.
We recall the following two findings from our
study. (a) Given the construction of U c50Q , the up-
ward trend we observe is not a direct result of users
looking for different types of information, although
it is possible that the rise of Q&A sites and users
entrusting search engines with more complex infor-
mation needs could have indirect influences. (b) The
results in Section 4.2 suggest that in U c50Q ,Q-queries
receive inferior results than Q-queries (i.e., higher
average rank for clicked results for Q-queries for
similar search intents), thus the rise in the use of
Q-queries is not a direct result of users learning the
most effective query formulation for the search en-
gine. These suggest an interesting research question:
what is causing the rise in question-query usage?
Irrespective of the cause, given that there is an
increased use of Q-queries in spite of the seem-
ingly inferior search results, there is a strong need
for the search engines to improve their handling of
question-queries.
Acknowledgments
We thank Evgeniy Gabrilovich, Lillian Lee, D.
Sivakumar, and the anonymous reviewers for many
useful suggestions.
139
References
Anne Aula, Rehan M. Khan, and Zhiwei Guan. 2010.
How does search behavior change as search becomes
more difficult? In Proc. 28th CHI, pages 35?44.
Ricardo Baeza-Yates and Alessandro Tiberi. 2007. Ex-
tracting semantic relations from query logs. In Proc.
13th KDD, pages 76?85.
Cory Barr, Rosie Jones, and Moira Regelson. 2008. The
linguistic structure of English web-search queries. In
Proc. EMNLP, pages 1021?1030.
Steven M. Beitzel, Eric C. Jensen, Abdur Chowdhury,
David Grossman, and Ophir Frieder. 2004. Hourly
analysis of a very large topically categorized web
query log. In Proc. 27th SIGIR, pages 321?328.
M. Bendersky and W. B. Croft. 2009. Analysis of long
queries in a large scale search log. In Proc. WSDM
Workshop on Web Search Click Data.
Susan Dumais, Michele Banko, Eric Brill, Jimmy Lin,
and Andrew Ng. 2002. Web question answering: Is
more always better? In Proc. 25th SIGIR, pages 291?
298.
Mark Kro?ll and Markus Strohmaier. 2009. Analyzing
human intentions in natural language text. In Proc.
5th K-CAP, pages 197?198.
Cody Kwok, Oren Etzioni, and Daniel S. Weld. 2001.
Scaling question answering to the web. ACM TOIS,
19:242?262.
Josiane Mothe and Ludovic Tanguy. 2005. Linguistic
features to predict query difficulty. In Proc. SIGIR
Workshop on Predicting Query Difficulty - Methods
and Applications.
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proc. 16th
CIKM, pages 683?690.
Fabrizio Silvestri. 2010. Mining Query Logs: Turning
Search Usage Data into Knowledge. Foundations and
Trends in Information Retrieval, 4(1):1?174.
Amanda Spink and H. Cenk Ozmultu. 2002. Char-
acteristics of question format web queries: An ex-
ploratory study. Information Processing and Manage-
ment, 38(4):453?471.
Markus Strohmaier and Mark Kro?ll. 2009. Studying
databases of intentions: do search query logs capture
knowledge about common human goals? In Proc. 5th
K-CAP, pages 89?96.
Ji-Rong Wen, Jian-Yun Nie, and Hong-Jiang Zhang.
2002. Query clustering using user logs. ACM TOIS,
20:59?81.
Jeonghee Yi and Farzin Maghoul. 2009. Query cluster-
ing using click-through graph. In Proc. 18th WWW,
pages 1055?1056.
140
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 545?553,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Spice it Up? Mining Refinements to Online
Instructions from User Generated Content
Gregory Druck
Yahoo! Research
gdruck@gmail.com
Bo Pang
Yahoo! Research
bopang42@gmail.com
Abstract
There are a growing number of popular web
sites where users submit and review instruc-
tions for completing tasks as varied as build-
ing a table and baking a pie. In addition to pro-
viding their subjective evaluation, reviewers
often provide actionable refinements. These
refinements clarify, correct, improve, or pro-
vide alternatives to the original instructions.
However, identifying and reading all relevant
reviews is a daunting task for a user. In this
paper, we propose a generative model that
jointly identifies user-proposed refinements in
instruction reviews at multiple granularities,
and aligns them to the appropriate steps in the
original instructions. Labeled data is not read-
ily available for these tasks, so we focus on
the unsupervised setting. In experiments in the
recipe domain, our model provides 90.1% F1
for predicting refinements at the review level,
and 77.0% F1 for predicting refinement seg-
ments within reviews.
1 Introduction
People turn to the web to seek advice on a wide
variety of subjects. An analysis of web search
queries posed as questions revealed that ?how to?
questions are the most popular (Pang and Kumar,
2011). People consult online resources to answer
technical questions like ?how to put music on my
ipod,? and to find instructions for tasks like tying
a tie and cooking Thanksgiving dinner. Not sur-
prisingly, there are many Web sites dedicated to
providing instructions. For instance, on the pop-
ular DIY site instructables.com (?share what you
make?), users post instructions for making a wide
variety of objects ranging from bed frames to ?The
Stirling Engine, absorb energy from candles, coffee,
and more!1? There are also sites like allrecipes.com
that are dedicated to a specific domain. On these
community-based instruction sites, instructions are
posted and reviewed by users. For instance, the
aforementioned ?Stirling engine? has received over
350 reviews on instructables.com.
While user-generated instructions greatly increase
the variety of instructions available online, they
are not necessarily foolproof, or appropriate for all
users. For instance, in the case of recipes, a user
missing a certain ingredient at home might wonder
whether it can be safely omitted; a user who wants
to get a slightly different flavor might want to find
out what substitutions can be used to achieve that ef-
fect. Reviews posted by other users provide a great
resource for mining such information. In recipe re-
views, users often offer their customized version of
the recipe by describing changes they made: e.g., ?I
halved the salt? or ?I used honey instead of sugar.?
In addition, they may clarify portions of the instruc-
tions that are too concise for a novice to follow, or
describe changes to the cooking method that result
in a better dish. We refer to such actionable infor-
mation as a refinement.
Refinements can be quite prevalent in instruction
reviews. In a random sample of recipe reviews
from allrecipes.com, we found that 57.8% contain
refinements of the original recipe. However, sift-
ing through all reviews for refinements is a daunting
1http://www.instructables.com/id/
The-Sterling-Engine-absorb-energy-from-candles-c
545
task for a user. Instead, we would like to automat-
ically identify refinements in reviews, summarize
them, and either create an annotated version of the
instructions that reflects the collective experience of
the community, or, more ambitiously, revise the in-
structions directly.
In this paper, we take first steps toward these goals
by addressing the following tasks: (1) identifying re-
views that contain refinements, (2) identifying text
segments within reviews that describe refinements,
and (3) aligning these refinement segments to steps
in the instructions being reviewed (Figure 1 provides
an example). Solving these tasks provides a foun-
dation for downstream summarization and seman-
tic analysis, and also suggests intermediate applica-
tions. For example, we can use review classifica-
tion to filter or rank reviews as they are presented to
future users, since reviews that contain refinements
are more informative than a review which only says
?Great recipe, thanks for posting!?
To the best of our knowledge, no previous work
has explored this aspect of user-generated text.
While review mining has been studied extensively,
we differ from previous work in that instead of fo-
cusing on evaluative information, we focus action-
able information in the reviews. (See Section 2 for a
more detailed discussion.)
There is no existing labeled data for the tasks of
interest, and we would like the methods we develop
to be easily applied in multiple domains. Motivated
by this, we propose a generative model for solving
these tasks jointly without labeled data. Interest-
ingly, we find that jointly modeling refinements at
both the review and segment level is beneficial. We
created a new recipe data set, and manually labeled
a random sample to evaluate our model and several
baselines. We obtain 90.1% F1 for predicting refine-
ments at the review level, and 77.0% F1 for predict-
ing refinement segments within reviews.
2 Related Work
At first glance, the task of identifying refinements
appears similar to subjectivity detection (see (Pang
and Lee, 2008) for a survey). However, note that an
objective sentence is not necessarily a refinement:
e.g., ?I took the cake to work?; and a subjective sen-
tence can still contain a refinement: e.g., ?I reduced
the sugar and it came out perfectly.?
Our end goal is similar to review summarization.
However, previous work on review summarization
(Hu and Liu, 2004; Popescu and Etzioni, 2005; Titov
and McDonald, 2008) in product or service domains
focused on summarizing evaluative information ?
more specifically, identifying ratable aspects (e.g.,
?food? and ?service? for restaurants) and summariz-
ing the overall sentiment polarity for each aspect. In
contrast, we are interested in extracting a subset of
the non-evaluative information. Rather than ratable
aspects that are common across the entire domain
(e.g., ?ingredient?, ?cooking method?), we are in-
terested in actionable information that is related and
specific to the subject of the review.
Note that while our end goal is to summa-
rize objective information, it is still very differ-
ent from standard multi-document summarization
(Radev et al, 2002) of news articles. Apart from
differences in the quantity and the nature of the in-
put, we aim to summarize a distribution over what
should or can be changed, rather than produce a con-
sensus using different accounts of an event. In terms
of modeling approaches, in the context of extractive
summarization, Barzilay and Lee (2004) model con-
tent structure (i.e., the order in which topics appear)
in documents. We also model document structure,
but we do so to help identify refinement segments.
We share with previous work on predicting re-
view quality or helpfulness an interest in identify-
ing ?informative? text. Early work tried to exploit
the intuition that a helpful review is one that com-
ments on product details. However, incorporating
product-aspect-mention count (Kim et al, 2006) or
similarity between the review and product specifi-
cation (Zhang and Varadarajan, 2006) as features
did not seem to improve the performance when the
task was predicting the percentage of helpfulness
votes. Instead of using the helpfulness votes, Liu
et al (2007) manually annotated reviews with qual-
ity judgements, where a best review was defined as
one that contains complete and detailed comments.
Our notion of informativeness differs from previ-
ous work. We do not seek reviews that contain de-
tailed evaluative information; instead, we seek re-
views that contain detailed actionable information.
Furthermore, we are not expecting any single review
to be comprehensive; rather, we seek to extract a
546
collection of refinements representing the collective
wisdom of the community.
To the best of our knowledge, there is little pre-
vious work on mining user-generated data for ac-
tionable information. However, there has been in-
creasing interest in language grounding. In partic-
ular, recent work has studied learning to act in an
external environment by following textual instruc-
tions (Branavan et al, 2009, 2010, 2011; Vogel and
Jurafsky, 2010). This line of research is complemen-
tary to our work. While we do not utilize extensive
linguistic knowledge to analyze actionable informa-
tion, we view this is an interesting future direction.
We propose a generative model that makes pre-
dictions at both the review and review segment level.
Recent work uses a discriminative model with a sim-
ilar structure to perform sentence-level sentiment
analysis with review-level supervision (Ta?ckstro?m
and McDonald, 2011). However, sentiment polarity
labels at the review level are easily obtained. In con-
trast, refinement labels are not naturally available,
motivating the use of unsupervised learning. Note
that the model of Ta?ckstro?m and McDonald (2011)
cannot be used in a fully unsupervised setting.
3 Refinements
In this section, we define refinements more pre-
cisely. We use recipes as our running example, but
our problem formulation and models are not specific
to this domain.
A refinement is a piece of text containing action-
able information that is not entailed by the original
instructions, but can be used to modify or expand the
original instructions. A refinement could propose an
alternative method or an improvement (e.g., ?I re-
placed half of the shortening with butter?, ?Let the
shrimp sit in 1/2 marinade for 3 hours?), as well as
provide clarification (?definitely use THIN cut pork
chops, otherwise your panko will burn before your
chops are cooked?).
Furthermore, we distinguish between a verified
refinement (what the user actually did) and a hy-
pothetical refinement (?next time I think I will try
evaporated milk?). In domains similar to recipes,
where instructions may be carried out repeatedly,
there exist refinements in both forms. Since instruc-
tions should, in principle, contain information that
has been well tested, in this work, we consider only
the former as our target class. In a small percent-
age of reviews we observed ?failed attempts? where
a user did not follow a certain step and regretted the
diversion. In this work, we do not consider them to
be refinements. We refer to text that does not contain
refinements as background.
Finally, we note that the presence of a past tense
verb does not imply a refinement (e.g., ?Everyone
loved this dish?, ?I got many compliments?). In fact,
not all text segments that describe an action are re-
finements (e.g., ?I took the cake to work?, ?I fol-
lowed the instructions to a T?).
4 Models
In this section we describe our models. To iden-
tify refinements without labeled data, we propose
a generative model of reviews (or more gener-
ally documents) with latent variables. We assume
that each review x is divided into segments, x =
(x1, . . . ,xT ). Each segment is a sub-sentence-level
text span. We assume that the segmentation is ob-
served, and hence it is not modeled. The segmenta-
tion procedure we use is described in Section 5.1.
While we focus on the unsupervised setting, note
that the model can also be used in a semi-supervised
setting. In particular, coarse (review-level) labels
can be used to guide the induction of fine-grained
latent structure (segment labels, alignments).
4.1 Identifying Refinements
We start by directly modeling refinements at the seg-
ment level. Our first intuition is that refinement and
background segments can often be identified by lex-
ical differences. Based on this intuition, we can ig-
nore document structure and generate the segments
with a segment-level mixture of multinomials (S-
Mix). In general we could use n multinomials to
represent refinements and m multinomials to repre-
sent background text, but in this paper we simply use
n=m= 1. Therefore, unsupervised learning in S-
Mix can be viewed as clustering the segments with
two latent states. As is standard practice in unsu-
pervised learning, we subsequently map these latent
states onto the labels of interest: r and b, for refine-
ment and background, respectively. Note, however,
that this model ignores potential sequential depen-
547
dencies among segments. A segment following a re-
finement segment in a review may be more likely to
be a refinement than background, for example.
To incorporate this intuition, we could instead
generate reviews with a HMM (Rabiner, 1989) over
segments (S-HMM) with two latent states. Let zi
be the latent label variable for the ith segment. The
joint probability of a review and segment labeling is
p(x, z;?) =
T?
j=1
p(zj |zj?1;?)p(xj |zj ;?), (1)
where p(zj |zj?1;?) are multinomial transition dis-
tributions, allowing the model to learn that p(zj =
r|zj?1 = r;?) > p(zj = b|zj?1 = r;?) as moti-
vated above, and p(xj |zj ;?) are multinomial emis-
sion distributions. Note that all words in a segment
are generated independently conditioned on zj .
While S-HMM models sequential dependencies,
note that it imposes the same transition probabili-
ties on each review. In a manually labeled random
sample of recipe reviews, we find that refinement
segments tend to be clustered together in certain re-
views (?bursty?), rather than uniformly distributed
across all reviews. Specifically, while we estimate
that 23% of all segments are refinements, 42% of
reviews do not contain any refinements. In reviews
that contain a refinement, 34% of segments are re-
finements. S-HMM cannot model this phenomenon.
Consequently, we extend S-HMM to include a la-
tent label variable y for each review that takes val-
ues yes (contains refinement) and no (does not con-
tain refinement). The extended model is a mixture
of HMMs (RS-MixHMM) where y is the mixture
component.
p(x, y, z;?) = p(y;?)p(x, z|y;?) (2)
The two HMMs p(x, z | y=yes;?) and p(x, z | y=
no;?) can learn different transition multinomials
and consequently different distributions over z for
different y. On the other hand, we do not believe
the textual content of the background segments in a
y = yes review should be different from those in
a y = no review. Thus, the emission distributions
are shared between the two HMMs, p(xj |zj , y;?) =
p(xj |zj ;?).
Note that the definition of y imposes additional
constraints on RS-MixHMM: 1) reviews with y=no
cannot contain refinement segments, and 2) reviews
with y = yes must contain at least one refinement
segment. We enforce constraint (1) by disallow-
ing refinement segments zj = r when y = no:
p(zj = r|zj?1, y = no;?) = 0. Therefore, with
one background label, only the all background la-
bel sequence has non-zero probability when y=no.
Enforcing constraint (2) is more challenging, as the
y = yes HMM must assign zero probability when
all segments are background, but permit background
segments when refinement segments are present.
To enforce constraint (2), we ?rewire? the HMM
structure for y = yes so that a path that does not
go through the refinement state r is impossible. We
first expand the state representation by replacing b
with two states that encode whether or not the first
r has been encountered yet: bnot?yet encodes that
all previous states in the path have also been back-
ground; bok encodes that at least one refinement state
has been encountered2. We prohibit paths from end-
ing with bnot?yet by augmenting RS-MixHMM with
a special final state f , and fixing p(zT+1 = f |zT =
bnot?yet, y = yes;?) = 0. Furthermore, to enforce
the correct semantics of each state, paths cannot start
with bok, p(z1 = bok|y = yes;?) = 0, and transi-
tions from bnot?yet to bok, bok to bnot?yet, and r to
bnot?yet are prohibited.
Note that RS-MixHMM also generalizes to the
case where there are multiple refinement (n>1) and
background (m > 1) labels. Let Zr be the set of
refinement labels, and Zb be the set of background
labels. The transition structure is analogous to the
n= m= 1 case, but statements involving r are ap-
plied for each z ? Zr, and statements involving b are
applied for each z ? Zb. For example, the y = yes
HMM contains 2|Zb| background states.
In summary, the generative process of RS-
MixHMM involves first selecting whether the re-
view will contain a refinement. If the answer is yes,
a sequence of background segments and at least one
refinement segment are generated using the y= yes
HMM. If the answer is no, only background seg-
ments are generated. Interestingly, by enforcing
constraints (1) and (2), we break the label symme-
try that necessitates mapping latent states onto labels
2In this paper, the two background states share emission
multinomials, p(xj |zj = bnot?yet;?) = p(xj |zj = bok;?),
though this is not required.
548
when using S-Mix and S-HMM. Indeed, in the ex-
periments we present in Section 5.3, mapping is not
necessary for RS-MixHMM.
Note that the relationship between document-
level labels and segment-level labels that we model
is related to the multiple-instance setting (Dietterich
et al, 1997) in the machine learning literature. In
multiple-instance learning (MIL), rather than having
explicit labels at the instance (e.g., segment) level,
labels are given for bags of instances (e.g., docu-
ments). In the binary case, a bag is negative only
if all of its instances are negative. While we share
this problem formulation, work on MIL has mostly
focussed on supervised learning settings, and thus
it is not directly applicable to our unsupervised set-
ting. Foulds and Smyth (2011) propose a generative
model for MIL in which the generation of the bag
label y is conditioned on the instance labels z. As a
result of this setup, their model reduces to our S-Mix
baseline in a fully unsupervised setting.
Finally, although we motivated including the
review-level latent variable y as a way to improve
segment-level prediction of z, note that predictions
of y are useful in and of themselves. They provide
some notion of review usefulness and can be used to
filter reviews for search and browsing. They addi-
tionally give us a way to measure whether a set of
instructions is often modified or performed as speci-
fied. Finally, if we want to provide supervision, it is
much easier to annotate whether a review contains a
refinement than to annotate each segment.
4.2 Alignment with the Instructions
In addition to the review x, we also observe the set of
instructions s being discussed. Often a review will
reference specific parts of the instructions. We as-
sume that each set of instructions is segmented into
steps, s = (s1, . . . , sS). We augment our model
with latent alignment variables a = (a1, . . . , aT ),
where aj = ` denotes that the jth review segment is
referring to the `th step of s. We also define a special
NULL instruction step. An alignment to NULL sig-
nifies that the segment does not refer to a specific in-
struction step. Note that this encoding assumes that
each review segment refers to at most one instruction
step. Alignment predictions could facilitate further
analysis of how refinements affect the instructions,
as well as aid in summarization and visualization of
refinements.
The joint probability under the augmented model,
which we refer to as RSA-MixHMM, is
p(a,x, y, z|s;?) = p(y;?)p(a,x, z|y, s;?) (3)
p(a,x, z|y, s;?) =
T?
j=1
p(aj , zj |aj?1, zj?1, y, s;?)
? p(xj |aj , zj , s;?).
Note that the instructions s are assumed to be ob-
served and hence are not generated by the model.
RSA-MixHMM can be viewed as a mixture of
HMMs where each state encodes both a segment la-
bel zj and an alignment variable aj . Encoding an
alignment problem as a sequence labeling problem
was first proposed by Vogel et al (1996). Note that
RSA-MixHMM uses a similar expanded state rep-
resentation and transition structure as RS-MixHMM
to encode the semantics of y.
In our current model, the transition probability de-
composes into the product of independent label tran-
sition and alignment transition probabilities
p(aj , zj |aj?1, zj?1, y, s;?) =p(aj |aj?1, y, s;?)
? p(zj |zj?1, y, s;?),
and p(aj |aj?1, y, s;?) = p(aj |y, s;?) simply en-
codes the probability that segments align to a (non-
NULL) instruction step given y. This allows the
model to learn, for example, that reviews that con-
tain refinements refer to the instructions more often.
Intuitively, a segment and the step it refers to
should be lexically similar. Consequently, RSA-
MixHMM generates segments using a mixture of the
multinomial distribution for the segment label zj and
the (fixed) multinomial distribution3 for the step saj .
In this paper, we do not model the mixture proba-
bility and simply assume that all overlapping words
are generated by the instruction step. When aj =
NULL, only the segment label multinomial is used.
Finally, we disallow an alignment to a non-NULL
step if no words overlap: p(xj |aj , zj , s;?) = 0.
4.3 Inference and Parameter Estimation
Because our model is tree-structured, we can
efficiently compute exact marginal distributions
3Stopwords are removed from the instruction step.
549
over latent variables using the sum-product algo-
rithm (Koller and Friedman, 2009). Similarly, to
find maximum probability assignments, we use the
max-product algorithm.
At training time we observe a set of re-
views and corresponding instructions, D =
{(x1, s1), . . . , (xN , sN )}. The other variables, y, z,
and a, are latent. For all models, we estimate param-
eters to maximize the marginal likelihood of the ob-
served reviews. For example, for RSA-MixHMM,
we estimate parameters using
argmax
?
N?
i=1
log
?
a,z,y
p(a,xi, y, z|si;?).
This problem cannot be solved analytically, so we
use the Expectation Maximization (EM) algorithm.
5 Experiments
5.1 Data
In this paper, we use recipes and reviews from
allrecipes.com, an active community where we es-
timate that the mean number of reviews per recipe is
54.2. We randomly selected 22,437 reviews for our
data set. Of these, we randomly selected a subset
of 550 reviews and determined whether or not each
contains a refinement, using the definition provided
in Section 3. In total, 318 of the 550 (57.8%) con-
tain a refinement. We then randomly selected 119 of
the 550 and labeled the individual segments. Of the
712 segments in the selected reviews, 165 (23.2%)
are refinements and 547 are background.
We now define our review segmentation scheme.
Most prior work on modeling latent document sub-
structure uses sentence-level labels (Barzilay and
Lee, 2004; Ta?ckstro?m and McDonald, 2011). In
the recipe data, we find that sentences often con-
tain both refinement and background segments: ?[I
used a slow cooker with this recipe and] [it turned
out great!]? Additionally, we find that sentences of-
ten contain several distinct refinements: ?[I set them
on top and around the pork and] [tossed in a can
of undrained french cut green beans and] [cooked
everything on high for about 3 hours].? To make re-
finements easier to identify, and to facilitate down-
stream processing, we allow sub-sentence segments.
Our segmentation procedure leverages a phrase
structure parser. In this paper we use the Stanford
Parser4. Based on a quick manual inspection, do-
main shift and ungrammatical sentences do cause
a significant degradation in parsing accuracy when
compared to in-domain data. However, this is ac-
ceptable because we only use the parser for segmen-
tation. We first parse the entire review, and subse-
quently iterate through the tokens, adding a segment
break when any of the following conditions is met:
? sentence break (determined by the parser)
? token is a coordinating conjunction (CC) with
parent other than NP, PP, ADJP
? token is a comma (,) with parent other than NP,
PP, ADJP
? token is a colon (:)
The resulting segmentations are fixed during learn-
ing. In future work we could extend our model to
additionally identify segment boundaries.
5.2 Experimental Setup
We first describe the methods we evaluate. For com-
parison, we provide results with a baseline that ran-
domly guesses according to the class distribution for
each task. We also evaluate a Review-level model:
? R-Mix: A review-level mixture of multinomi-
als with two latent states.
Note that this is similar to clustering at the review
level, except that class priors are estimated. R-Mix
does not provide segment labels, though they can be
obtained by labeling all segments with the review
label.
We also evaluate the two Segment-level models
described in Section 4.1 (with two latent states):
? S-Mix: A segment-level mixture model.
? S-HMM: A segment-level HMM (Eq. 1).
These models do not provide review labels. To ob-
tain them, we assign y = yes if any segment is la-
beled as a refinement, and y=no otherwise.
Finally, we evaluate three versions of our model
(Review + Segment and Review + Segment +
4http://nlp.stanford.edu/software/lex-parser.shtml
550
Alignment) with one refinement segment label and
one background segment label5:
? RS-MixHMM: A mixture of HMMs (Eq. 2)
with constraints (1) and (2) (see Section 4).
? RS-MixMix: A variant of RS-MixHMM with-
out sequential dependencies.
? RSA-MixHMM: The full model that also in-
corporates alignment (Eq. 3).
Segment multinomials are initialized with a small
amount of random noise to break the initial symme-
try. RSA-MixHMM segment multinomials are in-
stead initialized to the RS-MixHMM solution. We
apply add-0.01 smoothing to the emission multino-
mials and add-1 smoothing to the transition multi-
nomials in the M-step. We estimate parameters with
21,887 unlabeled reviews by running EM until the
relative percentage decrease in the marginal likeli-
hood is ? 10?4 (typically 10-20 iterations).
The models are evaluated on refinement F1 and
accuracy for both review and segment predictions
using the annotated data described in Section 5.1.
For R-Mix and the segment (S-) models, we select
the 1:1 mapping of latent states to labels that maxi-
mizes F1. For RSA-MixHMM and the RS- models
this was not necessary (see Section 4.1).
5.3 Results
Table 1 displays the results. R-Mix fails to ac-
curately distinguish refinement and background re-
views. The words that best discriminate the two
discovered review classes are ?savory ingredients?
(chicken, pepper, meat, garlic, soup) and ?bak-
ing/dessert ingredients? (chocolate, cake, pie, these,
flour). In other words, reviews naturally cluster by
topics rather than whether they contain refinements.
The segment models (S-) substantially outper-
form R-Mix on all metrics, demonstrating the ben-
efit of segment-level modeling and our segmenta-
tion scheme. However, S-HMM fails to model
the ?burstiness? of refinement segments (see Sec-
tion 4.1). It predicts that 76.2% of reviews con-
tain refinements, and additionally that 40.9% of seg-
ments contain refinements, whereas the true values
5Attempts at modeling refinement and background sub-
types by increasing the number of latent states failed to sub-
stantially improve the results.
are 57.8% and 23.2%, respectively. As a result, these
models provide high recall but low precision.
In comparison, our models, which model the re-
view labels6 y, yield more accurate refinement pre-
dictions. They provide statistically significant im-
provements in review and segment F1, as well as
accuracy, over the baseline models. RS-MixHMM
predicts that 62.9% of reviews contain refinements
and 28.2% of segments contain refinements, values
that are much closer to the ground truth. The re-
finement emission distributions for S-HMM and RS-
MixHMM are fairly similar, but the probabilities of
several key terms like added, used, and instead are
higher with RS-MixHMM.
The review F1 results demonstrate that our mod-
els are able to very accurately distinguish refinement
reviews from background reviews. As motivated in
Section 4.1, there are several applications that can
benefit from review-level predictions directly. Addi-
tionally, note that review labeling is not a trivial task.
We trained a supervised logistic regression model
with bag-of-words and length features (for both the
number of segments and the number of words) using
10-fold cross validation on the labeled dataset. This
supervised model yields mean review F1 of 78.4,
11.7 F1 points below the best unsupervised result7.
Augmenting RS-MixMix with sequential depen-
dencies, yielding RS-MixHMM, provides a mod-
erate (though not statistically significant) improve-
ment in segment F1. RS-MixHMM learns that re-
finement reviews typically begin and end with back-
ground segments, and that refinement segments tend
to appear in succession.
RSA-MixHMM additionally learns that segments
in refinement reviews are more likely to align to non-
NULL recipe steps. It also encourages the segment
multinomials to focus modeling effort on words that
appear only in the reviews. As a result, in addition to
yielding alignments, RSA-MixHMM provides small
improvements over RS-MixHMM (though they are
not statistically significant).
6We note that enforcing the constraint that a refinement re-
view must contain at least one refinement segment using the
method in Section 4.1 provides a statistically significant signif-
icant improvement in review F1 of 4.0 for RS-MixHMM.
7Note that we do not consider this performance to be the
upper-bound of supervised approaches; clearly, supervised ap-
proaches could benefit from additional labeled data. However,
labeled data is relatively expensive to obtain for this task.
551
Model
review (57.8% refinement) segment (23.2% refinement)
acc prec rec F1 acc prec rec F1
random baseline 51.2? 57.8 57.8 57.8? 64.4? 23.2 23.2 23.2?
R-Mix 61.5? 69.1 60.4 64.4? 55.8? 27.9 57.6 37.6?
S-Mix 77.5? 72.4 98.7 83.5? 80.6? 54.7 95.2 69.5?
S-HMM 79.8? 74.7 98.4 84.9? 80.3? 54.3 95.8 69.3?
RS-MixMix 87.1 85.4 93.7 89.4 86.4 65.6 86.7 74.7
RS-MixHMM 87.3 85.6 93.7 89.5 87.9 69.7 84.8 76.5
RSA-MixHMM 88.2 87.1 93.4 90.1 88.5 71.7 83.0 77.0
Table 1: Unsupervised experiments comparing models for review and segment refinement identification on the recipe
data. Bold indicates the best result, and a ? next to an accuracy or F1 value indicates that the improvements obtained
by RS-MixMix, RS-MixHMM, and RSA-MixHMM are significant (p = 0.05 according to a bootstrap test).
[ I loved these muffins! ] [ I used walnuts inside 
the batter and ] [ used whole wheat flour only 
as well as flaxseed instead of wheat germ. ] 
[ They turned out great! ] [ I couldn't stop eating 
them. ] [ I've made several batches of these 
muffins and all have been great. ] [ I make tiny 
alterations each time usually. ] [ These muffins 
are great with pears as well. ] [ I think golden 
raisins are much better than regular also! ]
1. Preheat oven to 375 degrees F (190 degrees C).
2. Lightly oil 18 muffin cups, or coat with nonstick 
cooking spray.
3. In a medium bowl, whisk together eggs, egg whites, 
apple butter, oil and vanilla.
4. In a large bowl, stir together flours, sugar, cinnamon, 
baking powder, baking soda and salt.
5. Stir in carrots, apples and raisins.
6. Stir in apple butter mixture until just moistened.
7. Spoon the batter into the prepared muffin cups, filling 
them about 3/4 full.
8. In a small bowl, combine walnuts and wheat germ; 
sprinkle over the muffin tops.
9. Bake at 375 degrees F (190 degrees C) for 15 to 20 
minutes, or until the tops are golden and spring back 
when lightly pressed.
Figure 1: Example output (best viewed in color). Bold segments in the review (left) are those predicted to be refine-
ments. Red indicates an incorrect segment label, according to our gold labels. Alignments to recipe steps (right) are
indicated with colors and arrows. Segments without colors and arrows align to the NULL recipe step (see Section 4.2).
We provide an example alignment in Figure 1.
Annotating ground truth alignments is challenging
and time-consuming due to ambiguity, and we feel
that the alignments are best evaluated via a down-
stream task. Therefore, we leave thorough evalua-
tion of the quality of the alignments to future work.
6 Conclusion and Future Work
In this paper, we developed unsupervised meth-
ods based on generative models for mining refine-
ments to online instructions from reviews. The pro-
posed models leverage lexical differences in refine-
ment and background segments. By augmenting the
base models with additional structure (review labels,
alignments), we obtained more accurate predictions.
However, to further improve accuracy, more lin-
guistic knowledge and structure will need to be in-
corporated. The current models provide many false
positives in the more subtle cases, when some words
that typically indicate a refinement are present, but
the text does not describe a refinement according to
the definition in Section 3. Examples include hypo-
thetical refinements (?next time I will substitute...?)
and discussion of the recipe without modification (?I
found it strange to... but it worked ...?, ?I love bal-
samic vinegar and herbs?, ?they baked up nicely?).
Other future directions include improving the
alignment model, for example by allowing words in
the instruction step to be ?translated? into words in
the review segment. Though we focussed on recipes,
the models we proposed are general, and could be
applied to other domains. We also plan to consider
this task in other settings such as online forums, and
develop methods for summarizing refinements.
Acknowledgments
We thank Andrei Broder and the anonymous reviewers
for helpful discussions and comments.
552
References
Regina Barzilay and Lillian Lee. Catching the drift:
Probabilistic content models, with applications to
generation and summarization. In HLT-NAACL
2004: Proceedings of the Main Conference, pages
113?120, 2004.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. Reinforcement learning for
mapping instructions to actions. In Proceedings
of the Association for Computational Linguistics
(ACL), 2009.
S.R.K Branavan, Luke Zettlemoyer, and Regina
Barzilay. Reading between the lines: Learning
to map high-level instructions to commands. In
Proceedings of the Association for Computational
Linguistics (ACL), 2010.
S.R.K. Branavan, David Silver, and Regina Barzilay.
Learning to win by reading manuals in a monte-
carlo framework. In Proceedings of the Associa-
tion for Computational Linguistics (ACL), 2011.
Thomas G. Dietterich, Richard H. Lathrop, and
Toma?s Lozano-Pe?rez. Solving the multiple in-
stance problem with axis-parallel rectangles. Ar-
tificial Intelligence, 89(1 - 2):31 ? 71, 1997.
J. R. Foulds and P. Smyth. Multi-instance mixture
models and semi-supervised learning. In SIAM
International Conference on Data Mining, 2011.
Minqing Hu and Bing Liu. Mining and summa-
rizing customer reviews. In Proceedings of the
ACM SIGKDD Conference on Knowledge Dis-
covery and Data Mining (KDD), pages 168?177,
2004.
Soo-Min Kim, Patrick Pantel, Tim Chklovski, and
Marco Pennacchiotti. Automatically assessing re-
view helpfulness. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP), pages 423?430, 2006.
D. Koller and N. Friedman. Probabilistic Graphical
Models: Principles and Techniques. MIT Press,
2009.
Jingjing Liu, Yunbo Cao, Chin-Yew Lin, Yalou
Huang, and Ming Zhou. Low-quality product
review detection in opinion summarization. In
Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP-CoNLL), pages 334?342, 2007.
Bo Pang and Ravi Kumar. Search in the lost sense
of query: Question formulation in web search
queries and its temporal changes. In Proceedings
of the Association for Computational Linguistics
(ACL), 2011.
Bo Pang and Lillian Lee. Opinion mining and sen-
timent analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, 2008.
Ana-Maria Popescu and Oren Etzioni. Extract-
ing product features and opinions from reviews.
In Proceedings of the Human Language Tech-
nology Conference and the Conference on Em-
pirical Methods in Natural Language Processing
(HLT/EMNLP), 2005.
Lawrence Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. Proceedings of the IEEE, 77(2):257?286,
1989.
Dragomir R. Radev, Eduard Hovy, and Kathleen
McKeown. Introduction to the special issue on
summarization. Computational Linguistics, 28
(4):399?408, 2002. ISSN 0891-2017.
Oscar Ta?ckstro?m and Ryan McDonald. Discovering
fine-grained sentiment with latent variable struc-
tured prediction models. In Proceedings of the
33rd European conference on Advances in infor-
mation retrieval, ECIR?11, pages 368?374, 2011.
Ivan Titov and Ryan McDonald. A joint model of
text and aspect ratings for sentiment summariza-
tion. In Proceedings of the Association for Com-
putational Linguistics (ACL), 2008.
Adam Vogel and Daniel Jurafsky. Learning to fol-
low navigational directions. In Proceedings of the
Association for Computational Linguistics (ACL),
2010.
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics - Volume 2, COL-
ING ?96, pages 836?841, 1996.
Zhu Zhang and Balaji Varadarajan. Utility scoring
of product reviews. In Proceedings of the ACM
SIGIR Conference on Information and Knowledge
Management (CIKM), pages 51?57, 2006.
553
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 175?185,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
The effect of wording on message propagation:
Topic- and author-controlled natural experiments on Twitter
Chenhao Tan
Dept. of Computer Science
Cornell University
chenhao@cs.cornell.edu
Lillian Lee
Dept. of Computer Science
Cornell University
llee@cs.cornell.edu
Bo Pang
Google Inc.
bopang42@gmail.com
Abstract
Consider a person trying to spread an
important message on a social network.
He/she can spend hours trying to craft the
message. Does it actually matter? While
there has been extensive prior work look-
ing into predicting popularity of social-
media content, the effect of wording per
se has rarely been studied since it is of-
ten confounded with the popularity of the
author and the topic. To control for these
confounding factors, we take advantage
of the surprising fact that there are many
pairs of tweets containing the same url and
written by the same user but employing
different wording. Given such pairs, we
ask: which version attracts more retweets?
This turns out to be a more difficult task
than predicting popular topics. Still, hu-
mans can answer this question better than
chance (but far from perfectly), and the
computational methods we develop can do
better than both an average human and a
strong competing method trained on non-
controlled data.
1 Introduction
How does one make a message ?successful?? This
question is of interest to many entities, including
political parties trying to frame an issue (Chong
and Druckman, 2007), and individuals attempting
to make a point in a group meeting. In the first
case, an important type of success is achieved if
the national conversation adopts the rhetoric of the
party; in the latter case, if other group members
repeat the originating individual?s point.
The massive availability of online messages,
such as posts to social media, now affords re-
searchers new means to investigate at a very large
scale the factors affecting message propagation,
also known as adoption, sharing, spread, or vi-
rality. According to prior research, important fea-
tures include characteristics of the originating au-
thor (e.g., verified Twitter user or not, author?s
messages? past success rate), the author?s social
network (e.g., number of followers), message tim-
ing, and message content or topic (Artzi et al,
2012; Bakshy et al, 2011; Borghol et al, 2012;
Guerini et al, 2011; Guerini et al, 2012; Hansen
et al, 2011; Hong et al, 2011; Lakkaraju et al,
2013; Milkman and Berger, 2012; Ma et al, 2012;
Petrovi?c et al, 2011; Romero et al, 2013; Suh et
al., 2010; Sun et al, 2013; Tsur and Rappoport,
2012). Indeed, it?s not surprising that one of the
most retweeted tweets of all time was from user
BarackObama, with 40M followers, on November
6, 2012: ?Four more years. [link to photo]?.
Our interest in this paper is the effect of alterna-
tive message wording, meaning how the message
is said, rather than what the message is about. In
contrast to the identity/social/timing/topic features
mentioned above, wording is one of the few fac-
tors directly under an author?s control when he or
she seeks to convey a fixed piece of content. For
example, consider a speaker at the ACL business
meeting who has been tasked with proposing that
Paris be the next ACL location. This person can-
not on the spot become ACL president, change the
shape of his/her social network, wait until the next
morning to speak, or campaign for Rome instead;
but he/she can craft the message to be more hu-
morous, more informative, emphasize certain as-
pects instead of others, and so on. In other words,
we investigate whether a different choice of words
affects message propagation, controlling for user
and topic: would user BarackObama have gotten
significantly more (or fewer) retweets if he had
used some alternate wording to announce his re-
election?
Although we cannot create a parallel universe
175
Table 1: Topic- and author-controlled (TAC) pairs. Topic control = inclusion of the same URL.
author tweets #retweets
natlsecuritycnn t
1
: FIRST ON CNN: After Petraeus scandal, Paula Broadwell looks to recapture ?normal life.? http://t.co/qy7GGuYW n
1
= 5
t
2
: First on CNN: Broadwell photos shared with Security Clearance as she and her family fight media portrayal of her [same URL] n
2
= 29
ABC t
1
: Workers, families take stand against Thanksgiving hours: http://t.co/J9mQHiIEqv n
1
= 46
t
2
: Staples, Medieval Times Workers Say Opening Thanksgiving Day Crosses the Line [same URL] n
2
= 27
cactus music t
1
: I know at some point you?ve have been saved from hunger by our rolling food trucks friends. Let?s help support them!
http://t.co/zg9jwA5j
n
1
= 2
t
2
: Food trucks are the epitome of small independently owned LOCAL businesses! Help keep them going! Sign the petition [same
URL]
n
2
= 13
in which BarackObama tweeted something else
1
,
fortunately, a surprising characteristic of Twitter
allows us to run a fairly analogous natural exper-
iment: external forces serendipitously provide an
environment that resembles the desired controlled
setting (DiNardo, 2008). Specifically, it turns out
to be unexpectedly common for the same user to
post different tweets regarding the same URL ?
a good proxy for fine-grained topic
2
? within a
relatively short period of time.
3
Some example
pairs are shown in Table 1; we see that the paired
tweets may differ dramatically, going far beyond
word-for-word substitutions, so that quite interest-
ing changes can be studied.
Looking at these examples, can one in fact tell
from the wording which tweet in a topic- and
author-controlled pair will be more successful?
The answer may not be a priori clear. For example,
for the first pair in the table, one person we asked
found t
1
?s invocation of a ?scandal? to be more
attention-grabbing; but another person preferred
t
2
because it is more informative about the URL?s
content and includes ?fight media portrayal?. In
an Amazon Mechanical Turk (AMT) experiment
(?4), we found that humans achieved an average
accuracy of 61.3%: not that high, but better than
chance, indicating that it is somewhat possible for
humans to predict greater message spread from
different deliveries of the same information.
Buoyed by the evidence of our AMT study that
wording effects exist, we then performed a battery
of experiments to seek generally-applicable, non-
1
Cf. the Music Lab ?multiple universes? experiment to
test the randomness of popularity (Salganik et al, 2006).
2
Although hashtags have been used as coarse-grained
topic labels in prior work, for our purposes, we have no assur-
ance that two tweets both using, say, ?#Tahrir? would be at-
tempting to express the same message but in different words.
In contrast, see the same-URL examples in Table 1.
3
Moreover, Twitter presents tweets to a reader in strict
chronological order, so that there are no algorithmic-ranking
effects to compensate for in determining whether readers saw
a tweet. And, Twitter accumulates retweet counts for the en-
tire retweet cascade and displays them for the original tweet
at the root of the propagation tree, so we can directly use
Twitter?s retweet counts to compare the entire reach of the
different versions.
Twitter-specific features of more successful phras-
ings. ?5.1 applies hypothesis testing (with Bonfer-
roni correction to ameliorate issues with multiple
comparisons) to investigate the utility of features
like informativeness, resemblance to headlines,
and conformity to the community norm in lan-
guage use. ?5.2 further validates our findings via
prediction experiments, including on completely
fresh held-out data, used only once and after an
array of standard cross-validation experiments.
4
We achieved 66.5% cross-validation accuracy and
65.6% held-out accuracy with a combination of
our custom features and bag-of-words. Our clas-
sifier fared significantly better than a number of
baselines, including a strong classifier trained on
the most- and least-retweeted tweets that was even
granted access to author and timing metadata.
2 Related work
The idea of using carefully controlled experiments
to study effective communication strategies dates
back at least to Hovland et al (1953). Recent
studies range from examining what characteris-
tics of New York Times articles correlate with high
re-sharing rates (Milkman and Berger, 2012) to
looking at how differences in description affect
the spread of content-controlled videos or images
(Borghol et al, 2012; Lakkaraju et al, 2013).
Simmons et al (2011) examined the variation of
quotes from different sources to examine how tex-
tual memes mutate as people pass them along, but
did not control for author. Predicting the ?success?
of various texts such as novels and movie quotes
has been the aim of additional prior work not al-
ready mentioned in ?1 (Ashok et al, 2013; Louis
and Nenkova, 2013; Danescu-Niculescu-Mizil et
al., 2012; Pitler and Nenkova, 2008; McIntyre and
Lapata, 2009). To our knowledge, there have been
no large-scale studies exploring wording effects in
a both topic- and author-controlled setting. Em-
ploying such controls, we find that predicting the
more effective alternative wording is much harder
than the previously well-studied problem of pre-
4
And after crossing our fingers.
176
dicting popular content when author or topic can
freely vary.
Related work regarding the features we consid-
ered is deferred to ?5.1 (features description).
3 Data
Our main dataset was constructed by first gath-
ering 1.77M topic- and author-controlled (hence-
forth TAC) tweet pairs
5
differing in more than just
spacing.
6
We accomplished this by crawling time-
lines of 236K user ids that appear in prior work
(Kwak et al, 2010; Yang and Leskovec, 2011)
via the Twitter API. This crawling process also
yielded 632K TAC pairs whose only difference
was spacing, and an additional 558M ?unpaired?
tweets; as shown later in this paper, we used these
extra corpora for computing language models and
other auxiliary information. We applied non-
obvious but important filtering ? described later
in this section ? to control for other external fac-
tors and to reduce ambiguous cases. This brought
us to a set of 11,404 pairs, with the gold-standard
labels determined by which tweet in each pair was
the one that received more retweets according to
the Twitter API. We then did a second crawl to
get an additional 1,770 pairs to serve as a held-out
dataset. The corresponding tweet IDs are available
online at http://chenhaot.com/pages/
wording-for-propagation.html. (Twit-
ter?s terms of service prohibit sharing the actual
tweets.)
Throughout, we refer to the textual content of
the earlier tweet within a TAC pair as t
1
, and of the
later one as t
2
. We denote the number of retweets
received by each tweet by n
1
and n
2
, respectively.
We refer to the tweet with higher (lower) n
i
as the
?better (worse)? tweet.
Using ?identical? pairs to determine how to
compensate for follower-count and timing ef-
fects. In an ideal setting, differences between
n
1
and n
2
would be determined solely by dif-
ferences in wording. But even with a TAC pair,
retweets might exhibit a temporal bias because of
the chronological order of tweet presentation (t
1
might enjoy a first-mover advantage (Borghol et
al., 2012) because it is the ?original?; alternatively,
5
No data collection/processing was conducted at Google.
6
The total excludes: tweets containing multiple URLs;
tweets from users posting about the same URL more than five
times (since such users might be spammers); the third, fourth,
or fifth version for users posting between three and five tweets
for the same URL; retweets (as identified by Twitter?s API or
by beginning with ?RT @?); non-English tweets.
3 6 12 18 24 36 48time lag (hours)2468
10121416D >1K f?ers>2.5K f?ers>5K f?ers>10K f?ers
(a) For identical TAC pairs,
retweet-count deviation vs.
time lag between t
1
and
t
2
, for the author follower-
counts given in the legend.
0 2 4 6 8n102
468
10
E?(n 2|n 1)
>5K f?ers,<12hrsotherwise
(b) Avg. n
2
vs. n
1
for iden-
tical TAC pairs, highlighting
our chosen time-lag and fol-
lower thresholds. Bars: stan-
dard error. Diagonal line:
p
Epn
2
|n
1
q ? n
1
.
Figure 1: (a): The ideal case where n
2
? n
1
when t
1
? t
2
is best approximated when t
2
oc-
curs within 12 hours of t
1
and the author has at
least 10,000 or 5,000 followers. (b): in our chosen
setting (blue circles), n
2
indeed tends to track n
1
,
whereas otherwise (black squares), there?s a bias
towards retweeting t
1
.
t
2
might be preferred because retweeters consider
t
1
to be ?stale?). Also, the number of followers an
author has can have complicated indirect effects
on which tweets are read (space limits preclude
discussion).
We use the 632K TAC pairs wherein t
1
and
t
2
are identical
7
to check for such confounding
effects: we see how much n
2
deviates from n
1
in such settings, since if wording were the only
explanatory factor, the retweet rates for identical
tweets ought to be equal. Figure 1(a) plots how
the time lag between t
1
and t
2
and the author?s
follower-count affect the following deviation esti-
mate:
D ?
?
0?n
1
?10
| pEpn
2
|n
1
q ? n
1
|,
where
p
Epn
2
|n
1
q is the average value of n
2
over
pairs whose t
1
is retweeted n
1
times. (Note that
the number of pairs whose t
1
is retweeted n
1
times
decays exponentially with n
1
; hence, we condi-
tion on n
1
to keep the estimate from being domi-
nated by pairs with n
1
? 0, and do not consider
n
1
? 10 because there are too few such pairs to es-
timate
p
Epn
2
|n
1
q reliably.) Figure 1(a) shows that
the setting where we (i) minimize the confound-
ing effects of time lag and author?s follower-count
and (ii) maximize the amount of data to work with
7
Identical up to spacing: Twitter prevents exact copies by
the same author appearing within a short amount of time, but
some authors work around this by inserting spaces.
177
is: when t
2
occurs within 12 hours after t
1
and
the author has more than 5,000 followers. Figure
1(b) confirms that for identical TAC pairs, our cho-
sen setting indeed results in n
2
being on average
close to n
1
, which corresponds to the desired set-
ting where wording is the dominant differentiating
factor.
8
Focus on meaningful and general changes.
Even after follower-count and time-lapse filtering,
we still want to focus on TAC pairs that (i) ex-
hibit significant/interesting textual changes (as ex-
emplified in Table 1, and as opposed to typo cor-
rections and the like), and (ii) have n
2
and n
1
suf-
ficiently different so that we are confident in which
t
i
is better at attracting retweets. To take care of
(i), we discarded the 50% of pairs whose similar-
ity was above the median, where similarity was
tf-based cosine.
9
For (ii), we sorted the remain-
ing pairs by n
2
? n
1
and retained only the top and
bottom 5%.
10
Moreover, to ensure that we do not
overfit to the idiosyncrasies of particular authors,
we cap the number of pairs contributed by each
author to 50 before we deal with (ii).
4 Human accuracy on TAC pairs
We first ran a pilot study on Amazon Mechan-
ical Turk (AMT) to determine whether humans
can identify, based on wording differences alone,
which of two topic- and author- controlled tweets
is spread more widely. Each of our 5 AMT tasks
involved a disjoint set of 20 randomly-sampled
TAC pairs (with t
1
and t
2
randomly reordered);
subjects indicated ?which tweet would other peo-
ple be more likely to retweet??, provided a short
justification for their binary response, and clicked
a checkbox if they found that their choice was a
?close call?. We received 39 judgments per pair in
aggregate from 106 subjects total (9 people com-
pleted all 5 tasks). The subjects? justifications
were of very high quality, convincing us that they
all did the task in good faith
11
. Two examples for
8
We also computed the Pearson correlation between n
1
and n
2
, even though it can be dominated by pairs with smaller
n
1
. The correlation is 0.853 for ?? 5K f?ers, ?12hrs?,
clearly higher than the 0.305 correlation for ?otherwise?.
9
Idf weighting was not employed because changes to fre-
quent words are of potential interest. Urls, hashtags, @-
mentions and numbers were normalized to [url], [hashtag],
[at], and [num] before computing similarity.
10
For our data, this meant n
2
? n
1
? 10 or ? ?15. Cf.
our median number of retweets: 30.
11
We also note that the feedback we got was quite pos-
itive, including: ?...It?s fun to make choices between close
tweets and use our subjective opinion. Thanks and best of
the third TAC pair in Table 1 were: ?[t
1
makes] the
cause relate-able to some people, therefore show-
ing more of an appeal as to why should they click
the link and support? and, expressing the opposite
view, ?I like [t
2
] more because [t
1
] starts out with
a generalization that doesn?t affect me and try to
make me look like I had that experience before?.
If we view the set of 3900 binary judgments
for our 100-TAC-pair sample as constituting in-
dependent responses, then the accuracy for this
set is 62.4% (rising to 63.8% if we exclude the
587 judgments deemed ?close calls?). However, if
we evaluate the accuracy of the majority response
among the 39 judgments per pair, the number rises
to 73%. The accuracy of the majority response
generally increases with the dominance of the ma-
jority, going above 90% when at least 80% of the
judgments agree (although less than a third of the
pairs satisfied this criterion).
Alternatively, we can consider the average ac-
curacy of the 106 subjects: 61.3%, which is bet-
ter than chance but far from 100%. (Variance was
high: one subject achieved 85% accuracy out of
20 pairs, but eight scored below 50%.) This re-
sult is noticeably lower than the 73.8%-81.2% re-
ported by Petrovi?c et al (2011), who ran a sim-
ilar experiment involving two subjects and 202
tweet pairs, but where the pairs were not topic- or
author-controlled.
12
We conclude that even though propagation pre-
diction becomes more challenging when topic
and author controls are applied, humans can
still to some degree tell which wording attracts
more retweets. Interested readers can try this
out themselves at http://chenhaot.com/
retweetedmore/quiz.
5 Experiments
We now investigate computationally what word-
ing features correspond to messages achieving a
broader reach. We start (?5.1) by introducing a set
of generally-applicable and (mostly) non-Twitter-
specific features to capture our intuitions about
what might be better ways to phrase a message.
We then use hypothesis testing (?5.1) to evaluate
the importance of each feature for message prop-
luck with your research? and ?This was very interesting and
really made me think about how I word my own tweets. Great
job on this survey!?. We only had to exclude one person (not
counted among the 106 subjects), doing so because he or she
gave the same uninformative justification for all pairs.
12
The accuracy range stems from whether author?s social
features were supplied and which subject was considered.
178
Table 2: Notational conventions for tables in ?5.1.
One-sided paired t-test for feature efficacy
????: p?1e-20 ????: p?1-1e-20
??? : p?0.001 ??? : p?0.999
?? : p?0.01 ?? : p?0.99
? : p?0.05 ? : p?0.95
?: passes our Bonferroni correction
One-sided binomial test for feature increase
(Do authors prefer to ?raise? the feature in t
2
?)
YES : t
2
has a higher feature score than t
1
, ? ? .05
NO : t
2
has a lower feature score than t
1
, ? ? .05
(x%): %pf
2
? f
1
q, if sig. larger or smaller than 50%
agation and the extent to which authors employ
it, followed by experiments on a prediction task
(?5.2) to further examine the utility of these fea-
tures.
5.1 Features: efficacy and author preference
What kind of phrasing helps message propaga-
tion? Does it work to explicitly ask people to share
the message? Is it better to be short and concise or
long and informative? We define an array of fea-
tures to capture these and other messaging aspects.
We then examine (i) how effective each feature is
for attracting more retweets; and (ii) whether au-
thors prefer applying a given feature when issuing
a second version of a tweet.
First, for each feature, we use a one-sided paired
t-test to test whether, on our 11K TAC pairs, our
score function for that feature is larger in the bet-
ter tweet versions than in the worse tweet versions,
for significance levels ? ? .05, .01, .001, 1e-20.
Given that we did 39 tests in total, there is a risk
of obtaining false positives due to multiple test-
ing (Dunn, 1961; Benjamini and Hochberg, 1995).
To account for this, we also report significance re-
sults for the conservatively Bonferroni-corrected
(?BC?) significance level ? = 0.05/39=1.28e-3.
Second, we examine author preference for ap-
plying a feature. We do so because one (but by no
means the only) reason authors post t
2
after having
already advertised the same URL in t
1
is that these
authors were dissatisfied with the amount of atten-
tion t
1
got; in such cases, the changes may have
been specifically intended to attract more retweets.
We measure author preference for a feature by the
percentage of our TAC pairs
13
where t
2
has more
?occurrences? of the feature than t
1
, which we de-
note by ?%pf
2
? f
1
q?. We use the one-sided bi-
nomial test to see whether %pf
2
? f
1
q is signifi-
cantly larger (or smaller) than 50%.
13
For our preference experiments, we added in pairs where
n
2
? n
1
was not in the top or bottom 5% (cf. ?3, meaningful
changes), since to measure author preference it?s not neces-
sary that the retweet counts differ significantly.
Table 3: Explicit requests for sharing (where only
occurrences POS-tagged as verbs count, according
to the Gimpel et al (2011) tagger).
effective? author-preferred?
rt ???? * ??
retweet ???? * YES (59%)
spread ??? ? * YES (56%)
please ??? ? * ??
pls ? ??? ??
plz ?? ?? ??
Table 4: Informativeness.
effective? author-preferred?
length (chars) ???? * YES (54%)
verb ???? * YES (56%)
noun ???? * ??
adjective ??? ? * YES (51%)
adverb ??? ? * YES (55%)
proper noun ??? ? * NO? (45%)
number ???? * NO? (48%)
hashtag ? ??? ??
@-mention ??? ? * YES (53%)
Not surprisingly, it helps to ask people to share.
(See Table 3; the notation for all tables is ex-
plained in Table 2.) The basic sanity check we
performed here was to take as features the number
of occurrences of the verbs ?rt?, ?retweet?, ?please?,
?spread?, ?pls?, and ?plz? to capture explicit re-
quests (e.g. ?please retweet?).
Informativeness helps. (Table 4) Messages that
are more informative have increased social ex-
change value (Homans, 1958), and so may be
more worth propagating. One crude approxima-
tion of informativeness is length, and we see that
length helps.
14
In contrast, Simmons et al (2011)
found that shorter versions of memes are more
likely to be popular. The difference may result
from TAC-pair changes being more drastic than
the variations that memes undergo.
A more refined informativeness measure is
counts of the parts of speech that correspond
to content. Our POS results, gathered using a
Twitter-specific tagger (Gimpel et al, 2011), echo
those of Ashok et al (2013) who looked at predict-
14
Of course, simply inserting garbage isn?t going to lead
to more retweets, but adding more information generally in-
volves longer text.
179
Table 5: Conformity to the community and one?s
own past, measured via scores assigned by various
language models.
effective? author-preferred?
twitter unigram ??? ? * YES (54%)
twitter bigram ??? ? * YES (52%)
personal unigram ??? ? * YES (52%)
personal bigram ??? NO? (48%)
ing the success of books. The diminished effect of
hashtag inclusion with respect to what has been re-
ported previously (Suh et al, 2010; Petrovi?c et al,
2011) presumably stems from our topic and author
controls.
Be like the community, and be true to yourself
(in the words you pick, but not necessarily in
how you combine them). (Table 5) Although dis-
tinctive messages may attract attention, messages
that conform to expectations might be more eas-
ily accepted and therefore shared. Prior work has
explored this tension: Lakkaraju et al (2013), in a
content-controlled study, found that the more up-
voted Reddit image titles balance novelty and fa-
miliarity; Danescu-Niculescu-Mizil et al (2012)
(henceforth DCKL?12) showed that the memora-
bility of movie quotes corresponds to higher lexi-
cal distinctiveness but lower POS distinctiveness;
and Sun et al (2013) observed that deviating from
one?s own past language patterns correlates with
more retweets.
Keeping in mind that the authors in our data
have at least 5000 followers
15
, we consider two
types of language-conformity constraints an au-
thor might try to satisfy: to be similar to what
is normal in the Twitter community, and to be
similar to what his or her followers expect. We
measure a tweet?s similarity to expectations by its
score according to the relevant language model,
1
|T |
?
xPT logpppxqq, where T refers to either all
the unigrams (unigram model) or all and only bi-
grams (bigram model).
16
We trained a Twitter-
community language model from our 558M un-
paired tweets, and personal language models from
each author?s tweet history.
Imitate headlines. (Table 6) News headlines are
often intentionally written to be both informative
and attention-getting, so we introduce the idea of
15
This is not an artificial restriction on our set of authors; a
large follower count means (in principle) that our results draw
on a large sample of decisions whether to retweet or not.
16
The tokens [at], [hashtag], [url] were ignored in the
unigram-model case to prevent their undue influence, but re-
tained in the bigram model to capture longer-range usage
(?combination?) patterns.
Table 6: LM-based resemblance to headlines.
effective? author-preferred?
headline unigram ?? ?? YES (53%)
headline bigram ???? * YES (52%)
Table 7: Retweet score.
effective? author-preferred?
rt score ?? ?? * NO? (49%)
verb rt score ???? * ??
noun rt score ??? ? * ??
adjective rt score ? ??? YES (50%)
adverb rt score ? ??? YES (51%)
proper noun rt score ??? NO? (48%)
scoring by a language model built from New York
Times headlines.
17
Use words associated with (non-paired)
retweeted tweets. (Table 7) We expect that
provocative or sensationalistic tweets are likely
to make people react. We found it difficult to
model provocativeness directly. As a rough
approximation, we check whether the changes in
t
2
with respect to t
1
(which share the same topic
and author) involve words or parts-of-speech that
are associated with high retweet rate in a very
large separate sample of unpaired tweets (retweets
and replies discarded). Specifically, for each word
w that appears more than 10 times, we compute
the probability that tweets containing w are
retweeted more than once, denoted by rspwq. We
define the rt score of a tweet as max
wPT rspwq,
where T is all the words in the tweet, and the
rt score of a particular POS tag z in a tweet as
max
wPT&tagpwq?zrspwq.
Include positive and/or negative words. (Ta-
ble 8) Prior work has found that including posi-
tive or negative sentiment increases message prop-
agation (Milkman and Berger, 2012; Godes et al,
2005; Heath et al, 2001; Hansen et al, 2011). We
measured the occurrence of positive and negative
words as determined by the connotation lexicon
of Feng et al (2013) (better coverage than LIWC).
Measuring the occurrence of both simultaneously
was inspired by Riloff et al (2013).
Refer to other people (but not your audience).
(Table 9) First-person has been found useful for
success before, but in the different domains of sci-
entific abstracts (Guerini et al, 2012) and books
(Ashok et al, 2013).
17
To test whether the results stem from similarity to news
rather than headlines per se, we constructed a NYT-text LM,
which proved less effective. We also tried using Gawker
headlines (often said to be attention-getting) but pilot studies
revealed insufficient vocabulary overlap with our TAC pairs.
180
Table 8: Sentiment (contrast is measured by pres-
ence of both positive and negative sentiments).
effective? author-preferred?
positive ??? ? * ??
negative ??? ? * ??
contrast ??? ? * ??
Table 9: Pronouns.
effective? author-preferred?
1st person singular ??? YES (51%)
1st person plural ??? YES (52%)
2nd person ??? YES (57%)
3rd person singular ?? ?? YES (55%)
3rd person plural ? ??? YES (58%)
Generality helps. (Table 10) DCKL?12 posited
that movie quotes are more shared in the culture
when they are general enough to be used in multi-
ple contexts. We hence measured the presence of
indefinite articles vs. definite articles.
The easier to read, the better. (Table 11) We
measure readability by using Flesch reading ease
(Flesch, 1948) and Flesch-Kincaid grade level
(Kincaid et al, 1975), though they are not de-
signed for short texts. We use negative grade level
so that a larger value indicates easier texts to read.
Final question: Do authors prefer to do what
is effective? Recall that we use binomial tests to
determine author preference for applying a feature
more in t
2
. Our preference statistics show that au-
thor preferences in many cases are aligned with
feature efficacy. But there are several notable ex-
ceptions: for example, authors tend to increase the
use of @-mentions and 2nd person pronouns even
though they are ineffective. On the other hand,
they did not increase the use of effective ones
like proper nouns and numbers; nor did they tend
to increase their rate of sentiment-bearing words.
Bearing in mind that changes in t
2
may not always
be intended as an effort to improve t
1
, it is still in-
teresting to observe that there are some contrasts
between feature efficacy and author preferences.
5.2 Predicting the ?better? wording
Here, we further examine the collective efficacy
of the features introduced in ?5.1 via their perfor-
mance on a binary prediction task: given a TAC
pair (t
1
, t
2
), did t
2
receive more retweets?
Our approach. We group the features introduced
in ?5.1 into 16 lexicon-based features (Table 3,
8, 9, 10), 9 informativeness features (Table 4), 6
language model features (Table 5, 6), 6 rt score
features (Table 7), and 2 readability features (Ta-
ble 11). We refer to all 39 of them together as
Table 10: Generality.
effective? author-preferred?
indefinite articles (a,an) ??? ? * ??
definite articles (the) ??? YES (52%)
Table 11: Readability.
effective? author-preferred?
reading ease ?? ?? YES (52%)
negative grade level ? ??? YES (52%)
custom features. We also consider tagged bag-of-
words (?BOW?) features, which includes all the
unigram (word:POS pair) and bigram features that
appear more than 10 times in the cross-validation
data. This yields 3,568 unigram features and 4,095
bigram features, for a total of 7,663 so-called
1,2-gram features. Values for each feature are nor-
malized by linear transformation across all tweets
in the training data to lie in the range r0, 1s.18
For a given TAC pair, we construct its feature
vector as follows. For each feature being consid-
ered, we compute its normalized value for each
tweet in the pair and take the difference as the fea-
ture value for this pair. We use L2-regularized lo-
gistic regression as our classifier, with parameters
chosen by cross validation on the training data.
(We also experimented with SVMs. The perfor-
mance was very close, but mostly slightly lower.)
A strong non-TAC alternative, with social infor-
mation and timing thrown in. One baseline re-
sult we would like to establish is whether the topic
and author controls we have argued for, while
intuitively compelling for the purposes of trying
to determine the best way for a given author to
present some fixed content, are really necessary
in practice. To test this, we consider an alterna-
tive binary L2-regularized logistic-regression clas-
sifier that is trained on unpaired data, specifically,
on the collection of 10,000 most retweeted tweets
(gold-standard label: positive) plus the 10,000
least retweeted tweets (gold-standard label: neg-
ative) that are neither retweets nor replies. Note
that this alternative thus is granted, by design,
roughly twice the training instances that our clas-
sifiers have, as a result of having roughly the same
number of tweets, since our instances are pairs.
Moreover, we additionally include the tweet au-
thor?s follower count, and the day and hour of
posting, as features. We refer to this alternative
classifier as  TAC+ff+time. (Mnemonic: ?ff? is
used in bibliographic contexts as an abbreviation
18
We also tried normalization by whitening, but it did not
lead to further improvements.
181
(a) Cross-validation and heldout accuracy for various feature sets. Blue lines inside
bars: performance when custom features are restricted to those that pass our Bon-
ferroni correction (no line for readability because no readability features passed).
Dashed vertical line:  TAC+ff+time performance.
1000 3000 5000 7000 900058%60%
62%64%66%
68%70% custom+1,2-gramcustom1,2-gramhuman
(b) Cross-validation accuracy vs data size.
Human performance was estimated from a
disjoint set of 100 pairs (see ?4).
Figure 2: Accuracy results. Pertinent significance results are as follows. In cross-validation, custom+1,2-
gram is significantly better than  TAC+ff+time (p=0) and 1,2-gram (p=3.8e-7). In heldout validation,
custom+1,2-gram is significantly better than  TAC+ff+time (p=3.4e-12) and 1,2-gram (p=0.01) but not
unigram (p=0.08), perhaps due to the small size of the heldout set.
for ?and the following?.) We apply it to a tweet
pair by computing whether it gives a higher score
to t
2
or not.
Baselines. To sanity-check whether our classifier
provides any improvement over the simplest meth-
ods one could try, we also report the performance
of the majority baseline, our request-for-sharing
features, and our character-length feature.
Performance comparison. We compare the ac-
curacy (percentage of pairs whose labels were cor-
rectly predicted) of our approach against the com-
peting methods. We report 5-fold cross validation
results on our balanced set of 11,404 TAC pairs
and on our completely disjoint heldout data
19
of
1,770 TAC pairs; this set was never examined dur-
ing development, and there are no authors in com-
mon between the two testing sets.
Figure 2(a) summarizes the main results. While
 TAC+ff+time outperforms the majority base-
line, using all the features we proposed beats
 TAC+ff+time by more than 10% in both cross-
validation (66.5% vs 55.9%) and heldout valida-
tion (65.6% vs 55.3%). We outperform the aver-
age human accuracy of 61% reported in our Ama-
zon Mechanical Turk experiments (for a different
data sample);  TAC+ff+time fails to do so.
The importance of topic and author con-
trol can be seen by further investigation of
 TAC+ff+time?s performance. First, note that
19
To construct this data, we used the same criteria as in
?3: written by authors with more than 5000 followers, posted
within 12 hours, n
2
? n
1
? 10 or ? ?15, and cosine simi-
larity threshold value the same as in ?3, cap of 50 on number
of pairs from any individual author.
it yields an accuracy of around 55% on our
alternate-version-selection task,
20
even though its
cross-validation accuracy on the larger most- and
least-retweeted unpaired tweets averages out to a
high 98.8%. Furthermore, note the superior per-
formance of unigrams trained on TAC data vs
 TAC+ff+time ? which is similar to our uni-
grams but trained on a larger but non-TAC dataset
that included metadata. Thus, TAC pairs are a use-
ful data source even for non-custom features. (We
also include individual feature comparisons later.)
Informativeness is the best-performing custom
feature group when run in isolation, and outper-
forms all baselines, as well as  TAC+ff+time;
and we can see from Figure 2(a) that this is not
due just to length. The combination of all our 39
custom features yields approximately 63% accu-
racy in both testing settings, significantly outper-
forming informativeness alone (p?0.001 in both
cases). Again, this is higher than our estimate of
average human performance.
Not surprisingly, the TAC-trained BOW fea-
tures (unigram and 1,2-gram) show impressive
predictive power in this task: many of our custom
features can be captured by bag-of-word features,
in a way. Still, the best performance is achieved
20
One might suspect that the problem is that
 TAC+ff+time learns from its training data to over-
rely on follower-count, since that is presumably a good
feature for non-TAC tweets, and for this reason suffers when
run on TAC data where follower-counts are by construction
non-informative. But in fact, we found that removing the
follower-count feature from  TAC+ff+time and re-training
did not lead to improved performance. Hence, it seems that
it is the non-controlled nature of the alternate training data
that explains the drop in performance.
182
by combining our custom and 1,2-gram features
together, to a degree statistically significantly bet-
ter than using 1,2-gram features alone.
Finally, we remark on our Bonferroni correc-
tion. Recall that the intent of applying it is to
avoid false positives. However, in our case, Fig-
ure 2(a) shows that our potentially ?false? posi-
tives ? features whose effectiveness did not pass
the Bonferroni correction test ? actually do raise
performance in our prediction tests.
Size of training data. Another interesting obser-
vation is how performance varies with data size.
For n ? 1000, 2000, . . . , 10000, we randomly
sampled n pairs from our 11,404 pairs, and com-
puted the average cross-validation accuracy on the
sampled data. Figure 2(b) shows the averages over
50 runs of the aforementioned procedure. Our cus-
tom features can achieve good performance with
little data, in the sense that for sample size 1000,
they outperform BOW features; on the other hand,
BOW features quickly surpass them. Across the
board, the custom+1,2-gram features are consis-
tently better than the 1,2-gram features alone.
Top features. Finally, we examine some of
the top-weighted individual features from our ap-
proach and from the competing  TAC+ff+time
classifier. The top three rows of Table 12 show the
best custom and best and worst unigram features
for our method; the bottom two rows show the best
and worst unigrams for  TAC+ff+time. Among
custom features, we see that community and per-
sonal language models, informativeness, retweet
scores, sentiment, and generality are represented.
As for unigram features, not surprisingly, ?rt? and
?retweet? are top features for both our approach
and  TAC+ff+time. However, the other unigrams
for the two methods seem to be a bit different in
spirit. Some of the unigrams determined to be
most poor only by our method appear to be both
surprising and yet plausible in retrospect: ?icymi?
(abbreviation for ?in case you missed it?) tends to
indicate a direct repetition of older information,
so people might prefer to retweet the earlier ver-
sion; ?thanks? and ?sorry? could correspond to
personal thank-yous and apologies not meant to
be shared with a broader audience, and similarly
@-mentioning another user may indicate a tweet
intended only for that person. The appearance of
[hashtag] in the best  TAC+ff+time unigrams is
consistent with prior research in non-TAC settings
(Suh et al, 2010; Petrovi?c et al, 2011).
Table 12: Features with largest coefficients, de-
limited by commas. POS tags omitted for clarity.
Our approach
best 15 custom twitter bigram, length (chars), rt
(the word), retweet (the word), verb, verb retweet score,
personal unigram, proper noun, number, noun, positive
words, please (the word), proper noun retweet score,
indefinite articles (a,an), adjective
best 20 unigrams rt, retweet, [num], breaking,
is, win, never, ., people, need, official, officially, are,
please, november, world, girl, !!!, god, new
worst 20 unigrams :, [at], icymi, also, comments,
half, ?, earlier, thanks, sorry, highlights, bit, point, up-
date, last, helping, peek, what, haven?t, debate
 TAC+ff+time
best 20 unigrams [hashtag], teen, fans, retweet,
sale, usa, women, butt, caught, visit, background, up-
coming, rt, this, bieber, these, each, chat, houston, book
worst 20 unigrams :, ..., boss, foundation, ?, ?,
others, john, roll, ride, appreciate, page, drive, correct,
full, ?, looks, @ (not as [at]), sales, hurts
6 Conclusion
In this work, we conducted the first large-scale
topic- and author-controlled experiment to study
the effects of wording on information propagation.
The features we developed to choose the bet-
ter of two alternative wordings posted better per-
formance than that of all our comparison algo-
rithms, including one given access to author and
timing features but trained on non-TAC data, and
also bested our estimate of average human perfor-
mance. According to our hypothesis tests, help-
ful wording heuristics include adding more infor-
mation, making one?s language align with both
community norms and with one?s prior messages,
and mimicking news headlines. Readers may
try out their own alternate phrasings at http:
//chenhaot.com/retweetedmore/ to see
what a simplified version of our classifier predicts.
In future work, it will be interesting to examine
how these features generalize to longer and more
extensive arguments. Moreover, understanding
the underlying psychological and cultural mecha-
nisms that establish the effectiveness of these fea-
tures is a fundamental problem of interest.
Acknowledgments. We thank C. Callison-Burch,
C. Danescu-Niculescu-Mizil, J. Kleinberg, P.
Mahdabi, S. Mullainathan, F. Pereira, K. Raman,
A. Swaminathan, the Cornell NLP seminar par-
ticipants and the reviewers for their comments; J.
Leskovec for providing some initial data; and the
anonymous annotators for all their labeling help.
This work was supported in part by NSF grant IIS-
0910664 and a Google Research Grant.
183
References
Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012.
Predicting responses to microblog posts. In Pro-
ceedings of NAACL (short paper).
Vikas Ganjigunte Ashok, Song Feng, and Yejin Choi.
2013. Success with style: Using writing style to
predict the success of novels. In Proceedings of
EMNLP.
Eitan Bakshy, Jake M. Hofman, Winter A. Mason, and
Duncan J. Watts. 2011. Everyone?s an influencer:
Quantifying influence on twitter. In Proceedings of
WSDM.
Yoav Benjamini and Yosef Hochberg. 1995. Control-
ling the false discovery rate: A practical and pow-
erful approach to multiple testing. Journal of the
Royal Statistical Society. Series B (Methodological),
pages 289?300.
Youmna Borghol, Sebastien Ardon, Niklas Carlsson,
Derek Eager, and Anirban Mahanti. 2012. The
untold story of the clones: Content-agnostic factors
that impact YouTube video popularity. In Proceed-
ings of KDD.
Dennis Chong and James N. Druckman. 2007. Fram-
ing theory. Annual Review of Political Science,
10:103?126.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Pro-
ceedings of ACL.
John DiNardo. 2008. Natural experiments and quasi-
natural experiments. In The New Palgrave Dictio-
nary of Economics. Palgrave Macmillan.
Olive Jean Dunn. 1961. Multiple comparisons among
means. Journal of the American Statistical Associa-
tion, 56(293):52?64.
Song Feng, Jun Seok Kang, Polina Kuznetsova, and
Yejin Choi. 2013. Connotation lexicon: A dash of
sentiment beneath the surface meaning. In Proceed-
ings of ACL.
Rudolph Flesch. 1948. A new readability yardstick.
Journal of applied psychology, 32(3):221.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan,
and Noah A. Smith. 2011. Part-of-speech Tagging
for Twitter: Annotation, Features, and Experiments.
In Proceedings of NAACL (short paper).
David Godes, Dina Mayzlin, Yubo Chen, Sanjiv
Das, Chrysanthos Dellarocas, Bruce Pfeiffer, Barak
Libai, Subrata Sen, Mengze Shi, and Peeter Verlegh.
2005. The firm?s management of social interactions.
Marketing Letters, 16(3-4):415?428.
Marco Guerini, Carlo Strapparava, and G?ozde
?
Ozbal.
2011. Exploring text virality in social networks. In
Proceedings of ICWSM (poster).
Marco Guerini, Alberto Pepe, and Bruno Lepri. 2012.
Do linguistic style and readability of scientific ab-
stracts affect their virality? In Proceedings of
ICWSM (poster).
Lars Kai Hansen, Adam Arvidsson, Finn
?
Arup Nielsen,
Elanor Colleoni, and Michael Etter. 2011. Good
friends, bad news-affect and virality in Twitter.
Communications in Computer and Information Sci-
ence, 185:34?43.
Chip Heath, Chris Bell, and Emily Sternberg. 2001.
Emotional selection in memes: The case of urban
legends. Journal of personality and social psychol-
ogy, 81(6):1028.
George C. Homans. 1958. Social Behavior as Ex-
change. American Journal of Sociology, 63(6):597?
606.
Liangjie Hong, Ovidiu Dan, and Brian D. Davison.
2011. Predicting popular messages in Twitter. In
Proceedings of WWW.
Carl I. Hovland, Irving L. Janis, and Harold H. Kelley.
1953. Communication and Persuasion: Psycholog-
ical Studies of Opinion Change, volume 19. Yale
University Press.
J. Peter Kincaid, Robert P. Fishburne Jr., Richard L.
Rogers, and Brad S. Chissom. 1975. Derivation
of new readability formulas (automated readability
index, fog count and flesch reading ease formula)
for navy enlisted personnel. Technical report, DTIC
Document.
Haewoon Kwak, Changhyun Lee, Hosung Park, and
Sue Moon. 2010. What is Twitter, a social network
or a news media? In Proceedings of WWW.
Himabindu Lakkaraju, Julian McAuley, and Jure
Leskovec. 2013. What?s in a name? Understanding
the interplay between titles, content, and communi-
ties in social media. In Proceedings of ICWSM.
Annie Louis and Ani Nenkova. 2013. What makes
writing great? First experiments on article quality
prediction in the science journalism domain. Trans-
actions of ACL.
Zongyang Ma, Aixin Sun, and Gao Cong. 2012. Will
this #hashtag be popular tomorrow? In Proceedings
of SIGIR.
Neil McIntyre and Mirella Lapata. 2009. Learning to
tell tales: A data-driven approach to story genera-
tion. In Proceedings of ACL-IJCNLP.
Katherine L Milkman and Jonah Berger. 2012. What
makes online content viral? Journal of Marketing
Research, 49(2):192?205.
184
Sa?sa Petrovi?c, Miles Osborne, and Victor Lavrenko.
2011. RT to win! Predicting message propagation
in Twitter. In Proceedings of ICWSM.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of EMNLP.
Ellen Riloff, Ashequl Qadir, Prafulla Surve, Lalin-
dra De Silva, Nathan Gilbert, and Ruihong Huang.
2013. Sarcasm as contrast between a positive sen-
timent and negative situation. In Proceedings of
EMNLP.
Daniel M. Romero, Chenhao Tan, and Johan Ugander.
2013. On the interplay between social and topical
structure. In Proceedings of ICWSM.
Matthew J. Salganik, Peter Sheridan Dodds, and Dun-
can J. Watts. 2006. Experimental study of inequal-
ity and unpredictability in an artificial cultural mar-
ket. Science, 311(5762):854?856.
Matthew P. Simmons, Lada A Adamic, and Eytan Adar.
2011. Memes online: Extracted, subtracted, in-
jected, and recollected. In Proceedings of ICWSM.
Bongwon Suh, Lichan Hong, Peter Pirolli, and Ed H.
Chi. 2010. Want to be retweeted? Large scale an-
alytics on factors impacting retweet in Twitter net-
work. In Proceedings of SocialCom.
Tao Sun, Ming Zhang, and Qiaozhu Mei. 2013. Unex-
pected relevance: An empirical study of serendipity
in retweets. In Proceedings of ICWSM.
Oren Tsur and Ari Rappoport. 2012. What?s in a hash-
tag?: Content based prediction of the spread of ideas
in microblogging communities. In Proceedings of
WSDM.
Jaewon Yang and Jure Leskovec. 2011. Patterns of
temporal variation in online media. In Proceedings
of WSDM.
185
