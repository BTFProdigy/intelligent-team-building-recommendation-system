Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 984?992,
Beijing, August 2010
Multi-Document Summarization via
the Minimum Dominating Set
Chao Shen and Tao Li
School of Computing and Information Sciences
Florida Internation University
{cshen001|taoli}@cs.fiu.edu
Abstract
Multi-document summarization has
been an important problem in infor-
mation retrieval. It aims to dis-
till the most important information
from a set of documents to gener-
ate a compressed summary. Given
a sentence graph generated from a
set of documents where vertices repre-
sent sentences and edges indicate that
the corresponding vertices are simi-
lar, the extracted summary can be de-
scribed using the idea of graph dom-
ination. In this paper, we propose
a new principled and versatile frame-
work for multi-document summariza-
tion using the minimum dominating
set. We show that four well-known
summarization tasks including generic,
query-focused, update, and compara-
tive summarization can be modeled as
different variations derived from the
proposed framework. Approximation
algorithms for performing summariza-
tion are also proposed and empirical
experiments are conducted to demon-
strate the effectiveness of our proposed
framework.
1 Introduction
As a fundamental and effective tool for docu-
ment understanding and organization, multi-
document summarization enables better infor-
mation services by creating concise and infor-
mative reports for a large collection of doc-
uments. Specifically, in multi-document sum-
marization, given a set of documents as input,
the goal is to produce a condensation (i.e.,
a generated summary) of the content of the
entire input set (Jurafsky and Martin, 2008).
The generated summary can be generic where
it simply gives the important information con-
tained in the input documents without any
particular information needs or query/topic-
focused where it is produced in response to a
user query or related to a topic or concern the
development of an event (Jurafsky and Mar-
tin, 2008; Mani, 2001).
Recently, new summarization tasks such as
update summarization (Dang and Owczarzak,
2008) and comparative summarization (Wang
et al, 2009a) have also been proposed. Up-
date summarization aims to generate short
summaries of recent documents to capture
new information different from earlier docu-
ments and comparative summarization aims
to summarize the differences between compa-
rable document groups.
In this paper, we propose a new principled
and versatile framework for multi-document
summarization using the minimum dominat-
ing set. Many known summarization tasks in-
cluding generic, query-focused, update, and
comparative summarization can be modeled
as different variations derived from the pro-
posed framework. The framework provides an
elegant basis to establish the connections be-
tween various summarization tasks while high-
lighting their differences.
In our framework, a sentence graph is first
generated from the input documents where
vertices represent sentences and edges indicate
that the corresponding vertices are similar. A
natural method for describing the extracted
summary is based on the idea of graph dom-
ination (Wu and Li, 2001). A dominating set
of a graph is a subset of vertices such that
every vertex in the graph is either in the sub-
set or adjacent to a vertex in the subset; and
984
a minimum dominating set is a dominating
set with the minimum size. The minimum
dominating set of the sentence graph can be
naturally used to describe the summary: it
is representative since each sentence is either
in the minimum dominating set or connected
to one sentence in the set; and it is with
minimal redundancy since the set is of mini-
mum size. Approximation algorithms are pro-
posed for performing summarization and em-
pirical experiments are conducted to demon-
strate the effectiveness of our proposed frame-
work. Though the dominating set problem has
been widely used in wireless networks, this pa-
per is the first work on using it for modeling
sentence extraction in document summariza-
tion.
The rest of the paper is organized as fol-
lows. In Section 2, we review the related work
about multi-document summarization and the
dominating set. After introducing the min-
imum dominating set problem in graph the-
ory in Section 3, we propose the minimum
dominating set based framework for multi-
document summarization and model the four
summarization tasks including generic, query-
focused, update, and comparative summariza-
tion in Section 4. Section 5 presents the exper-
imental results and analysis, and finally Sec-
tion 6 concludes the paper.
2 Related Work
Generic Summarization For generic sum-
marization, a saliency score is usually as-
signed to each sentence and then the sen-
tences are ranked according to the saliency
score. The scores are usually computed based
on a combination of statistical and linguistic
features. MEAD (Radev et al, 2004) is an
implementation of the centroid-based method
where the sentence scores are computed based
on sentence-level and inter-sentence features.
SumBasic (Nenkova and Vanderwende, 2005)
shows that the frequency of content words
alone can also lead good summarization re-
sults. Graph-based methods (Erkan and
Radev, 2004; Wan et al, 2007b) have also
been proposed to rank sentences or passages
based on the PageRank algorithm or its vari-
ants.
Query-Focused Summarization In
query-focused summarization, the informa-
tion of the given topic or query should be
incorporated into summarizers, and sentences
suiting the user?s declared information need
should be extracted. Many methods for
generic summarization can be extended to
incorporate the query information (Saggion
et al, 2003; Wei et al, 2008). Wan et al
(Wan et al, 2007a) make full use of both
the relationships among all the sentences in
the documents and relationship between the
given query and the sentences by manifold
ranking. Probability models have also been
proposed with different assumptions on the
generation process of the documents and
the queries (Daume? III and Marcu, 2006;
Haghighi and Vanderwende, 2009; Tang et
al., 2009).
Update Summarization and Compara-
tive Summarization Update summariza-
tion was introduced in Document Understand-
ing Conference (DUC) 2007 (Dang, 2007) and
was a main task of the summarization track in
Text Analysis Conference (TAC) 2008 (Dang
and Owczarzak, 2008). It is required to sum-
marize a set of documents under the assump-
tion that the reader has already read and
summarized the first set of documents as the
main summary. To produce the update sum-
mary, some strategies are required to avoid re-
dundant information which has already been
covered by the main summary. One of the
most frequently used methods for remov-
ing redundancy is Maximal Marginal Rele-
vance(MMR) (Goldstein et al, 2000). Com-
parative document summarization is proposed
by Wang et. al. (Wang et al, 2009a) to
summarize the differences between compara-
ble document groups. A sentence selection
approach is proposed in (Wang et al, 2009a)
to accurately discriminate the documents in
different groups modeled by the conditional
entropy.
985
The Dominating Set Many approxima-
tion algorithms have been developed for find-
ing minimum dominating set for a given
graph (Guha and Khuller, 1998; Thai et al,
2007). Kann (Kann, 1992) shows that the
minimum dominating set problem is equiv-
alent to set cover problem, which is a well-
known NP-hard problem. Dominating set has
been widely used for clustering in wireless net-
works (Chen and Liestman, 2002; Han and
Jia, 2007). It has been used to find topic
words for hierarchical summarization (Lawrie
et al, 2001), where a set of topic words is ex-
tracted as a dominating set of word graph. In
our work, we use the minimum dominating set
to formalize the sentence extraction for docu-
ment summarization.
3 The Minimum Dominating Set
Problem
Given a graph G =< V,E >, a dominating
set of G is a subset S of vertices with the
following property: each vertex of G is either
in the dominating set S, or is adjacent to some
vertices in S.
Problem 3.1. Given a graph G, the mini-
mum dominating set problem (MDS) is to find
a minimum size subset S of vertices, such that
S forms a dominating set.
MDS is closely related to the set cover prob-
lem (SC), a well-known NP-hard problem.
Problem 3.2. Given F , a finite collection
{S1, S2, . . . , Sn} of finite sets, the set cover
problem (SC) is to find the optimal solution
F ? = arg min
F ??F
|F ?| s.t.
?
S??F ?
S? =
?
S?F
S.
Theorem 3.3. There exists a pair of polyno-
mial time reduction between MDS and SC.
So, MDS is also NP-hard and it has been
shown that there are no approximate solutions
within c log |V |, for some c > 0 (Feige, 1998;
Raz and Safra, 1997).
3.1 An Approximation Algorithm
A greedy approximation algorithm for the SC
problem is described in (Johnson, 1973). Ba-
sically, at each stage, the greedy algorithm
chooses the set which contains the largest
number of uncovered elements.
Based on Theorem 3.3, we can obtain a
greedy approximation algorithm for MDS.
Starting from an empty set, if the current sub-
set of vertices is not the dominating set, a new
vertex which has the most number of the ad-
jacent vertices that are not adjacent to any
vertex in the current set will be added.
Proposition 3.4. The greedy algorithm ap-
proximates SC within 1 + ln s where s is the
size of the largest set.
It was shown in (Johnson, 1973) that the
approximation factor for the greedy algorithm
is no more thanH(s) , the s-th harmonic num-
ber:
H(s) =
s?
k=1
1
k ? ln s+ 1
Corollary 3.5. MDS has a approximation al-
gorithm within 1 + ln? where ? is the maxi-
mum degree of the graph.
Corollary 3.5 follows directly from Theo-
rem 3.3 and Proposition 3.4.
4 The Summarization Framework
4.1 Sentence Graph Generation
To perform multi-document summarization
via minimum dominating set, we need to first
construct a sentence graph in which each node
is a sentence in the document collection. In
our work, we represent the sentences as vec-
tors based on tf-isf, and then obtain the cosine
similarity for each pair of sentences. If the
similarity between a pair of sentences si and
sj is above a given threshold ?, then there is
an edge between si and sj .
For generic summarization, we use all sen-
tences for building the sentence graph. For
query-focused summarization, we only use the
sentences containing at least one term in the
query. In addition, when a query q is involved,
we assign each node si a weight, w(si) =
d(si, q) = 1 ? cos(si, q), to indicate the dis-
tance between the sentence and the query q.
After building the sentence graph, we can
formulate the summarization problem using
986
Generic Summary
(a)
Query-focused Summary
query
(b)
Updated Summary
C
1
C
2
(c)
Comparative Summary
Comparative Summary
Comparative Summary
C
2
C
1
C
3
(d)
Figure 1: Graphical illustrations of multi-document summarization via the minimum domi-
nating set. (a): The minimum dominating set is extracted as the generic summary. (b):The
minimum weighted dominating set is extracted as the query-based summary. (c):Vertices in
the right rectangle represent the first document set C1, and ones in the left represent the sec-
ond document set where update summary is generated. (d):Each rectangle represents a group
of documents. The vertices with rings are the dominating set for each group, while the solid
vertices are the complementary dominating set, which is extracted as comparative summaries.
the minimum dominating set. A graphical il-
lustration of the proposed framework is shown
in Figure 1.
4.2 Generic Summarization
Generic summarization is to extract the most
representative sentences to capture the impor-
tant content of the input documents. Without
taking into account the length limitation of
the summary, we can assume that the sum-
mary should represent all the sentences in the
document set (i.e., every sentence in the docu-
ment set should either be extracted or be sim-
ilar with one extracted sentence). Meanwhile,
a summary should also be as short as possi-
ble. Such summary of the input documents
under the assumption is exactly the minimum
dominating set of the sentence graph we con-
structed from the input documents in Section
4.1. Therefore the summarization problem
can be formulated as the minimum dominat-
ing set problem.
However, usually there is a length restric-
tion for generating the summary. Moreover,
the MDS is NP-hard as shown in Section 3.
Therefore, it is straightforward to use a greedy
approximation algorithm to construct a subset
of the dominating set as the final summary. In
the greedy approach, at each stage, a sentence
which is optimal according to the local crite-
ria will be extracted. Algorithm 1 describes
Algorithm 1 Algorithm for Generic Summariza-
tion
INPUT: G, W
OUTPUT: S
1: S = ?
2: T = ?
3: while L(S) < W and V (G)! = S do
4: for v ? V (G)? S do
5: s(v) = |{ADJ(v) ? T}|
6: v? = argmaxv s(v)
7: S = S ? {v?}
8: T = T ?ADJ(v?)
an approximation algorithm for generic sum-
marization. In Algorithm 1, G is the sen-
tence graph, L(S) is the length of the sum-
mary, W is the maximal length of the sum-
mary, and ADJ(v) = {v?|(v?, v) ? E(G)} is
the set of vertices which are adjacent to the
vertex v. A graphical illustration of generic
summarization using the minimum dominat-
ing set is shown in Figure 1(a).
4.3 Query-Focused Summarization
Letting G be the sentence graph constructed
in Section 4.1 and q be the query, the query-
focused summarization can be modeled as
D? = argminD?G
?
s?D d(s, q) (1)
s.t. D is a dominating set of G.
Note that d(s, q) can be viewed as the weight
of vertex in G. Here the summary length is
minimized implicitly, since if D? ? D, then
987
?
s?D? d(s, q) ?
?
s?D d(s, q). The problem
in Eq.(1) is exactly a variant of the minimum
dominating set problem, i.e., the minimum
weighted dominating set problem (MWDS).
Similar to MDS, MWDS can be reduced
from the weighted version of the SC problem.
In the weighted version of SC, each set has a
weight and the sum of weights of selected sets
needs to be minimized. To generate an ap-
proximate solution for the weighted SC prob-
lem, instead of choosing a set i maximizing
|SET (i)|, a set i minimizing w(i)|SET (i)| is cho-
sen, where SET (i) is composed of uncovered
elements in set i, and w(i) is the weight of set
i. The approximate solution has the same ap-
proximation ratio as that for MDS, as stated
by the following theorem (Chvatal, 1979).
Theorem 4.1. An approximate weighted
dominating set can be generated with a size at
most 1+log??|OPT |, where ? is the maximal
degree of the graph and OPT is the optimal
weighted dominating set.
Accordingly, from generic summarization to
query-focused summarization, we just need to
modify line 6 in Algorithm 1 to
v? = argmin
v
w(v)
s(v) , (2)
where w(v) is the weight of vertex v. A graph-
ical illustration of query-focused summariza-
tion using the minimum dominating set is
shown in Figure 1(b).
4.4 Update Summarization
Give a query q and two sets of documents C1
and C2, update summarization is to generate
a summary of C2 based on q, given C1. Firstly,
summary of C1, referred as D1 can be gener-
ated. Then, to generate the update summary
of C2, referred as D2, we assume D1 and D2
should represent all query related sentences in
C2, and length of D2 should be minimized.
Let G1 be the sentence graph for C1. First
we use the method described in Section 4.3 to
extract sentences from G1 to form D1. Then
we expand G1 to the whole graph G using the
second set of documents C2. G is then the
graph presentation of the document set in-
cluding C1 and C2. We can model the update
summary of C2 as
D? = argminD2
?
s?D2 w(s) (3)
s.t. D2 ?D1 is a dominating set of G.
Intuitively, we extract the smallest set of sen-
tences that are closely related to the query
from C2 to complete the partial dominating
set of G generated from D1. A graphical il-
lustration of update summarization using the
minimum dominating set is shown in Fig-
ure 1(c).
4.5 Comparative Summarization
Comparative document summarization aims
to summarize the differences among compara-
ble document groups. The summary produced
for each group should emphasize its difference
from other groups (Wang et al, 2009a).
We extend our method for update sum-
marization to generate the discriminant sum-
mary for each group of documents. Given N
groups of documents C1, C2, . . . , CN , we first
generate the sentence graphs G1, G2, . . . , GN ,
respectively. To generate the summary for
Ci, 1 ? i ? N , we view Ci as the update
of all other groups. To extract a new sen-
tence, only the one connected with the largest
number of sentences which have no represen-
tatives in any groups will be extracted. We
denote the extracted set as the complemen-
tary dominating set, since for each group we
obtain a subset of vertices dominating those
are not dominated by the dominating sets of
other groups. To perform comparative sum-
marization, we first extract the standard dom-
inating sets for G1, . . . , GN , respectively, de-
noted as D1, . . . , DN . Then we extract the
so-called complementary dominating set CDi
for Gi by continuing adding vertices in Gi to
find the dominating set of ?1?j?NGj given
D1, . . . ,Di?1,Di+1, . . . ,DN . A graphical il-
lustration of comparative summarization is
shown in Figure 1(d).
988
DUC04 DUC05 DUC06 TAC08 A TAC08 B
Type of Summarization Generic Topic-focused Topic-focused Topic-focused Update
#topics NA 50 50 48 48
#documents per topic 10 25-50 25 10 10
Summary length 665 bytes 250 words 250 words 100 words 100 words
Table 1: Brief description of the data set
5 Experiments
We have conducted experiments on all four
summarization tasks and our proposed meth-
ods based on the minimum dominating set
have outperformed many existing methods.
For the generic, topic-focused and update
summarization tasks, the experiments are per-
form the DUC data sets using ROUGE-2 and
ROUGE-SU (Lin and Hovy, 2003) as evalua-
tion measures. For comparative summariza-
tion, a case study as in (Wang et al, 2009a) is
performed. Table 1 shows the characteristics
of the data sets. We use DUC04 data set to
evaluate our method for generic summariza-
tion task and DUC05 and DUC06 data sets
for query-focused summarization task. The
data set for update summarization, (i.e. the
main task of TAC 2008 summarization track)
consists of 48 topics and 20 newswire articles
for each topic. The 20 articles are grouped
into two clusters. The task requires to pro-
duce 2 summaries, including the initial sum-
mary (TAC08 A) which is standard query-
focused summarization and the update sum-
mary (TAC08 B) under the assumption that
the reader has already read the first 10 docu-
ments.
We apply a 5-fold cross-validation proce-
dure to choose the threshold ? used for gener-
ating the sentence graph in our method.
5.1 Generic Summarization
We implement the following widely used or
recent published methods for generic summa-
rization as the baseline systems to compare
with our proposed method (denoted as MDS).
(1) Centroid: The method applies MEAD al-
gorithm (Radev et al, 2004) to extract sen-
tences according to the following three pa-
rameters: centroid value, positional value,
and first-sentence overlap. (2) LexPageR-
ank: The method first constructs a sentence
connectivity graph based on cosine similarity
and then selects important sentences based on
the concept of eigenvector centrality (Erkan
and Radev, 2004). (3) BSTM: A Bayesian
sentence-based topic model making use of
both the term-document and term-sentence
associations (Wang et al, 2009b).
Our method outperforms the simple Cen-
troid method and another graph-based Lex-
PageRank, and its performance is close to the
results of the Bayesian sentence-based topic
model and those of the best team in the DUC
competition. Note however that, like clus-
tering or topic based methods, BSTM needs
the topic number as the input, which usually
varies by different summarization tasks and is
hard to estimate.
5.2 Query-Focused Summarization
We compare our method (denoted as MWDS)
described in Section 4.3 with some recently
published systems. (1) TMR (Tang et al,
2009): incorporates the query information
into the topic model, and uses topic based
score and term frequency to estimate the im-
portance of the sentences. (2) SNMF (Wang
et al, 2008): calculates sentence-sentence
similarities by sentence-level semantic analy-
sis, clusters the sentences via symmetric non-
negative matrix factorization, and extracts
the sentences based on the clustering result.
(3) Wiki (Nastase, 2008): uses Wikipedia
as external knowledge to expand query and
builds the connection between the query and
the sentences in documents.
Table 3 presents the experimental compar-
ison of query-focused summarization on the
two datasets. From Table 3, we observe that
our method is comparable with these systems.
This is due to the good interpretation of the
summary extracted by our method, an ap-
989
DUC04
ROUGE-2 ROUGE-SU
DUC Best 0.09216 0.13233
Centroid 0.07379 0.12511
LexPageRank 0.08572 0.13097
BSTM 0.09010 0.13218
MDS 0.08934 0.13137
Table 2: Results on generic summariza-
tion.
DUC05 DUC06
ROUGE-2 ROUGE-SU ROUGE-2 ROUGE-SU
DUC Best 0.0725 0.1316 0.09510 0.15470
SNMF 0.06043 0.12298 0.08549 0.13981
TMR 0.07147 0.13038 0.09132 0.15037
Wiki 0.07074 0.13002 0.08091 0.14022
MWDS 0.07311 0.13061 0.09296 0.14797
Table 3: Results on query-focused summariza-
tion.
proximate minimal dominating set of the sen-
tence graph. On DUC05, our method achieves
the best result; and on DUC06, our method
outperforms all other systems except the best
team in DUC. Note that our method based
on the minimum dominating set is much sim-
pler than other systems. Our method only
depends on the distance to the query and has
only one parameter (i.e., the threshold ? in
generating the sentence graph).
 0.065
 0.07
 0.075
 0.08
 0.085
 0.09
 0.095
 0.05  0.1  0.15  0.2  0.25
R
O
UG
E-
2
Similarity threshold ?
DUC 06
DUC 05
Figure 2: ROUGE-2 vs. threshold ?
We also conduct experiments to empirically
evaluate the sensitivity of the threshold ?.
Figure 2 shows the ROUGE-2 curve of our
MWDS method on the two datasets when ?
varies from 0.04 to 0.26. When ? is small,
edges fail to represent the similarity of the sen-
tences, while if ? is too large, the graph will
be sparse. As ? is approximately in the range
of 0.1? 0.17, ROUGE-2 value becomes stable
and relatively high.
5.3 Update Summarization
Table 5 presents the experimental results on
update summarization. In Table 5, ?TAC
Best? and ?TAC Median? represent the best
and median results from the participants of
TAC 2008 summarization track in the two
tasks respectively according to the TAC 2008
report (Dang and Owczarzak, 2008). As seen
from the results, the ROUGE scores of our
methods are higher than the median results.
The good results of the best team typically
come from the fact that they utilize advanced
natural language processing (NLP) techniques
to resolve pronouns and other anaphoric ex-
pressions. Although we can spend more efforts
on the preprocessing or language processing
step, our goal here is to demonstrate the ef-
fectiveness of formalizing the update summa-
rization problem using the minimum dominat-
ing set and hence we do not utilize advanced
NLP techniques for preprocessing. The exper-
imental results demonstrate that our simple
update summarization method based on the
minimum dominating set can lead to compet-
itive performance for update summarization.
TAC08 A TAC08 B
ROUGE-2 ROUGE-
SU
ROUGE-2 ROUGE-
SU
TAC Best 0.1114 0.14298 0.10108 0.13669
TAC Median 0.08123 0.11975 0.06927 0.11046
MWDS 0.09012 0.12094 0.08117 0.11728
Table 5: Results on update summarization.
5.4 Comparative Summarization
We use the top six largest clusters of doc-
uments from TDT2 corpora to compare the
summary generated by different comparative
summarization methods. The topics of the six
document clusters are as follows: topic 1: Iraq
Issues; topic 2: Asia?s economic crisis; topic 3:
Lewinsky scandal; topic 4: Nagano Olympic
Games; topic 5: Nuclear Issues in Indian and
Pakistan; and topic 6: Jakarta Riot. From
each of the topics, 30 documents are extracted
990
Topic Complementary Dominating Set Discriminative Sentence Selection Dominating Set
1 ? ? ? U.S. Secretary of State
Madeleine Albright arrives to
consult on the stand-off between
the United Nations and Iraq.
the U.S. envoy to the United
Nations, Bill Richardson, ? ? ?
play down China?s refusal to sup-
port threats of military force
against Iraq
The United States and Britain
do not trust President Sad-
dam and wants cdotswarning
of serious consequences if Iraq
violates the accord.
2 Thailand?s currency, the
baht, dropped through a
key psychological level of ? ? ?
amid a regional sell-off sparked
by escalating social unrest in
Indonesia.
Earlier, driven largely by the de-
clining yen, South Korea?s
stock market fell by ? ? ? , while
the Nikkei 225 benchmark in-
dex dipped below 15,000 in the
morning ? ? ?
In the fourth quarter, IBM
Corp. earned $2.1 billion, up
3.4 percent from $2 billion a
year earlier.
3 ? ? ? attorneys representing Pres-
ident Clinton and Monica
Lewinsky.
The following night Isikoff ? ? ? ,
where he directly followed the
recitation of the top-10 list: ?Top
10 White House Jobs That
Sound Dirty.?
In Washington, Ken Starr?s
grand jury continued its inves-
tigation of theMonica Lewin-
sky matter.
4 Eight women and six men were
named Saturday night as the
first U.S. Olympic Snow-
board Team as their sport
gets set to make its debut in
Nagano, Japan.
this tunnel is finland?s cross coun-
try version of tokyo?s alpine ski
dome, and olympic skiers flock
from russia, ? ? ? , france and aus-
tria this past summer to work out
the kinks ? ? ?
If the skiers the men?s super-
G and the women?s downhill
on Saturday, they will be back
on schedule.
5 U.S. officials have announced
sanctions Washington will im-
pose on India and Pakistan
for conducting nuclear tests.
The sanctions would stop all for-
eign aid except for humanitarian
purposes, ban military sales to
India ? ? ?
And Pakistan?s prime min-
ister says his country will sign
the U.N.?s comprehensive
ban on nuclear tests if In-
dia does, too.
6 ? ? ? remain in force around
Jakarta, and at the Parliament
building where thousands of
students staged a sit-in Tues-
day ? ? ? .
?President Suharto has given
much to his country over the
past 30 years, raising Indone-
sia?s standing in the world ? ? ?
What were the students doing
at the time you were there, and
what was the reaction of the
students to the troops?
Table 4: A case study on comparative document summarization. Some unimportant words are skipped due to
the space limit. The bold font is used to annotate the phrases that are highly related with the topics, and italic
font is used to highlight the sentences that are not proper to be used in the summary.
randomly to produce a one-sentence summary.
For comparison purpose, we extract the sen-
tence with the maximal degree as the base-
line. Note that the baseline can be thought
as an approximation of the dominating set
using only one sentence. Table 4 shows the
summaries generated by our method (comple-
mentary dominating set (CDS)), discrimina-
tive sentence selection (DSS) (Wang et al,
2009a) and the baseline method. Our CDS
method can extract discriminative sentences
for all the topics. DSS can extract discrimina-
tive sentences for all the topics except topic 4.
Note that the sentence extracted by DSS for
topic 4 may be discriminative from other top-
ics, but it is deviated from the topic Nagano
Olympic Games. In addition, DSS tends to
select long sentences which should not be pre-
ferred for summarization purpose. The base-
line method may extract some general sen-
tences, such as the sentence for topic 2 and
topic 6 in Table 4.
6 Conclusion
In this paper, we propose a framework to
model the multi-document summarization us-
ing the minimum dominating set and show
that many well-known summarization tasks
can be formulated using the proposed frame-
work. The proposed framework leads to sim-
ple yet effective summarization methods. Ex-
perimental results show that our proposed
methods achieve good performance on several
multi-document document tasks.
7 Acknowledgements
This work is supported by NSF grants IIS-
0549280 and HRD-0833093.
991
References
Chen, Y.P. and A.L. Liestman. 2002. Approximating
minimum size weakly-connected dominating sets for
clustering mobile ad hoc networks. In Proceedings
of International Symposium on Mobile Ad hoc Net-
working & Computing. ACM.
Chvatal, V. 1979. A greedy heuristic for the set-
covering problem. Mathematics of operations re-
search, 4(3):233?235.
Dang, H.T. and K Owczarzak. 2008. Overview of the
TAC 2008 Update Summarization Task. In Pro-
ceedings of the Text Analysis Conference (TAC).
Dang, H.T. 2007. Overview of DUC 2007. In Docu-
ment Understanding Conference.
Daume? III, H. and D. Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of the ACL-
COLING.
Erkan, G. and D.R. Radev. 2004. Lexpagerank: Pres-
tige in multi-document text summarization. In Pro-
ceedings of EMNLP.
Feige, U. 1998. A threshold of lnn for approximating
set cover. Journal of the ACM (JACM), 45(4):634?
652.
Goldstein, J., V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document summariza-
tion by sentence extraction. In NAACL-ANLP 2000
Workshop on Automatic summarization.
Guha, S. and S. Khuller. 1998. Approximation algo-
rithms for connected dominating sets. Algorithmica,
20(4):374?387.
Haghighi, A. and L. Vanderwende. 2009. Exploring
content models for multi-document summarization.
In Proceedings of HLT-NAACL.
Han, B. and W. Jia. 2007. Clustering wireless ad
hoc networks with weakly connected dominating
set. Journal of Parallel and Distributed Computing,
67(6):727?737.
Johnson, D.S. 1973. Approximation algorithms for
combinatorial problems. In Proceedings of STOC.
Jurafsky, D. and J.H. Martin. 2008. Speech and lan-
guage processing. Prentice Hall New York.
Kann, V. 1992. On the approximability of NP-
complete optimization problems. PhD thesis, De-
partment of Numerical Analysis and Computing
Science, Royal Institute of Technology, Stockholm.
Lawrie, D., W.B. Croft, and A. Rosenberg. 2001.
Finding topic words for hierarchical summarization.
In Proceedings of SIGIR.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation
of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL.
Mani, I. 2001. Automatic summarization. Computa-
tional Linguistics, 28(2).
Nastase, V. 2008. Topic-driven multi-document
summarization with encyclopedic knowledge and
spreading activation. In Proceedings of EMNLP.
Nenkova, A. and L. Vanderwende. 2005. The impact
of frequency on summarization. Microsoft Research,
Redmond, Washington, Tech. Rep. MSR-TR-2005-
101.
Radev, D.R., H. Jing, M. Stys?, and D. Tam. 2004.
Centroid-based summarization of multiple docu-
ments. Information Processing and Management,
40(6):919?938.
Raz, R. and S. Safra. 1997. A sub-constant error-
probability low-degree test, and a sub-constant
error-probability PCP characterization of NP. In
Proceedings of STOC.
Saggion, H., K. Bontcheva, and H. Cunningham. 2003.
Robust generic and query-based summarisation. In
Proceedings of EACL.
Tang, J., L. Yao, and D. Chen. 2009. Multi-topic
based Query-oriented Summarization. In Proceed-
ings of SDM.
Thai, M.T., N. Zhang, R. Tiwari, and X. Xu. 2007.
On approximation algorithms of k-connected m-
dominating sets in disk graphs. Theoretical Com-
puter Science, 385(1-3):49?59.
Wan, X., J. Yang, and J. Xiao. 2007a. Manifold-
ranking based topic-focused multi-document sum-
marization. In Proceedings of IJCAI.
Wan, X., J. Yang, and J. Xiao. 2007b. Towards an
iterative reinforcement approach for simultaneous
document summarization and keyword extraction.
In Proceedings of ACL.
Wang, D., T. Li, S. Zhu, and C. Ding. 2008. Multi-
document summarization via sentence-level seman-
tic analysis and symmetric matrix factorization. In
Proceedings of SIGIR.
Wang, D., S. Zhu, T. Li, and Y. Gong. 2009a. Com-
parative document summarization via discrimina-
tive sentence selection. In Proceeding of CIKM.
Wang, D., S. Zhu, T. Li, and Y. Gong. 2009b.
Multi-document summarization using sentence-
based topic models. In Proceedings of the ACL-
IJCNLP.
Wei, F., W. Li, Q. Lu, and Y. He. 2008. Query-
sensitive mutual reinforcement chain and its ap-
plication in query-oriented multi-document summa-
rization. In Proceedings of SIGIR.
Wu, J. and H. Li. 2001. A dominating-set-based rout-
ing scheme in ad hoc wireless networks. Telecom-
munication Systems, 18(1):13?36.
992
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 949?958,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Non-negative Matrix Factorization Based Approach for Active Dual
Supervision from Document and Word Labels
Chao Shen and Tao Li
School of Computing and Information Sciences
Florida International University
Miami, FL 33199 USA
{cshen001,taoli}@cs.fiu.edu
Abstract
In active dual supervision, not only informa-
tive examples but also features are selected for
labeling to build a high quality classifier with
low cost. However, how to measure the infor-
mativeness for both examples and feature on
the same scale has not been well solved. In
this paper, we propose a non-negative matrix
factorization based approach to address this is-
sue. We first extend the matrix factorization
framework to explicitly model the correspond-
ing relationships between feature classes and
examples classes. Then by making use of
the reconstruction error, we propose a unified
scheme to determine which feature or exam-
ple a classifier is most likely to benefit from
having labeled. Empirical results demonstrate
the effectiveness of our proposed methods.
1 Introduction
Active learning, as an effective paradigm to optimize
the learning benefit from domain experts? feedback
and to reduce the cost of acquiring labeled examples
for supervised learning, has been intensively stud-
ied in recent years (McCallum and Nigam, 1998;
Tong and Koller, 2002; Settles, 2009). Traditional
approaches for active learning query the human ex-
perts to obtain the labels for intelligently chosen
data samples. However, in text classification where
the input data is generally represented as document-
word matrices, human supervision can be obtained
on both documents and words. For example, in sen-
timent analysis of product reviews, human labelers
can label reviews as positive or negative, they can
also label the words that elicit positive sentiment
(such as ?sensational? and ?electrifying?) as posi-
tive and words that evoke negative sentiment (such
as ?depressed? and ?unfulfilling?) as negative. Re-
cent work has demonstrated that labeled words (or
feature supervision) can greatly reduce the number
of labeled samples for building high-quality classi-
fiers (Druck et al, 2008; Zaidan and Eisner, 2008).
In fact, different kinds of supervision generally have
different acquisition costs, different degrees of util-
ity and are not mutually redundant (Sindhwani et
al., 2009). Ideally, effective active learning schemes
should be able to utilize different forms of supervi-
sion.
To incorporate the supervision on words and doc-
uments at same time into the active learning scheme,
recently an active dual supervision (or dual active
learning) has been proposed (Melville and Sind-
hwani, 2009; Sindhwani et al, 2009). Comparing
with traditional active learning which aims to select
the most ?informative? examples (e.g., documents)
for domain experts to label, active dual supervi-
sion selects both the ?informative? examples (e.g.,
documents) and features (e.g., words) for labeling.
For active dual supervision to be effective, there
are three important components: a) an underlying
learning mechanism that is able to learn from both
the labeled examples and features (i.e., incorporat-
ing supervision on both examples and features); b)
methods for estimating the value of information for
example and feature labels; and c) a scheme that
should be able to trade-off the costs and benefits of
the different forms of supervision since they have
different labeling costs and different benefits.
949
In Sindhwani et al?s initial work on active dual
supervision (Sindhwani et al, 2009), a transductive
bipartite graph regularization approach is used for
learning from both labeled examples and features.
In addition, uncertainty sampling and experimental
design are used for selecting informative examples
and features for labeling. To trade-off between dif-
ferent types of supervision, a simple probabilistic
interleaving scheme where the active learner prob-
abilistically queries the example oracle and the fea-
ture oracle is used. One problem in their method is
that the values of acquiring the feature labels and
the example labels are not on the same scale.
Recently, Li et al (Li et al, 2009) proposed a
dual supervision method based on constrained non-
negative tri-factorization of the document-term ma-
trix where the labeled features and examples are
naturally incorporated as sets of constraints. Hav-
ing a framework for incorporating dual-supervision
based on matrix factorization, gives rise to the nat-
ural question of how to perform active dual super-
vision in this setting. Since rows and columns are
treated equally in estimating the errors of matrix fac-
torization, another question is can we address the
scaling issue in comparing the value of feature la-
bels and example labels.
In this paper, we study the problem of ac-
tive dual supervision using non-negative matrix tri-
factorization. Our work is based on the dual supervi-
sion framework using constrained non-negative tri-
factorization proposed in (Li et al, 2009). We first
extend the framework to explicitly model the corre-
sponding relationships between feature classes and
example classes. Then by making use of the recon-
struction error criterion in matrix factorization, we
propose a unified scheme to evaluate the value of
feature and example labels. Instead of comparing
the estimated performance increase of new feature
labels or example labels, our proposed scheme as-
sumes that a better supervision (a feature label or a
example label) should lead to a more accurate re-
construction of the original data matrix. In our pro-
posed scheme, the value of feature labels and ex-
ample labels is computed on the same scale. The
experiments show that our proposed unified scheme
to query selection (i.e., feature/example selection for
labeling) outperforms the interleaving schemes and
the scheme based on expected log gain.
The rest of this paper is organized as follows: the
related work is discussed in Section 2, and the dual
supervision framework based on non-negative ma-
trix tri-factorization is introduced in Section 3. We
extend non-negative matrix tri-factorization to active
learning settings in Section 4, and propose a unified
scheme for query selection in Section 5. Experi-
ments are presented in Section 6, and finally Section
7 concludes the paper.
2 Related Work
We point the reader to a recent report (Settles, 2009)
for an in-depth survey on active learning. In this
section, we briefly cover related work to position our
contributions appropriately.
Active Learning/Active Dual Supervision Most
prior work in active learning has focused on pooled-
based techniques, where examples from an unla-
beled pool are selected for labeling (Cohn et al,
1994). With the study of learning from labeled fea-
tures, many research efforts on active learning with
feature supervision are also reported (Melville et al,
2005; Raghavan et al, 2006). (Godbole et al, 2004)
proposed the notion of feature uncertainty and in-
corporated the acquired feature labels into learning
by creating one-term mini-documents. (Druck et al,
2009) performed active learning via feature labeling
using several uncertainty reduction heuristics using
the learning model developed in (Druck et al, 2008).
(Sindhwani et al, 2009) studied the problem of ac-
tive dual supervision from examples and features
using a graph-based dual supervision method with
a simple probabilistic method for interleaving fea-
ture labels and example labels. In our work, we de-
velop our active dual supervision framework using
constrained non-negative tri-factorization and also
propose a unified scheme to evaluate the value of
feature and example labels. We note the very re-
cent work of (Attenberg et al, 2010), which pro-
poses a unified approach for the dual active learn-
ing problem using expected utility where the utility
is defined as the log gain of the classification model
with a new labeled document or word. Conceptu-
ally, our proposed unified scheme is a special case
of the expected utility framework where the utility
is computed using the matrix reconstruction error.
The utility based on the log gain of the classification
950
model may not be reliable as small model changes
resulted from a single additional example label or
feature label may not be reflected in the classifica-
tion performance (Attenberg et al, 2010). The em-
pirical comparisons show that our proposed unified
scheme based on reconstruction error outperforms
the expected log gain.
Dual Supervision Note that a learning method
that is capable of performing dual supervision (i.e.,
learning from both labeled examples and features)
is the basis for active dual supervision. Dual su-
pervision is a relatively new area of research and
few methods have been developed for dual super-
vision. In (Sindhwani and Melville, 2008; Sind-
hwani et al, 2008), a bipartite graph regularization
model (GRADS) is used to diffuse label informa-
tion along both sides of the document-term matrix
and to perform dual supervision for semi-supervised
sentiment analysis. Conceptually, their model im-
plements a co-clustering assumption closely related
to Singular Value Decomposition (see also (Dhillon,
2001; Zha et al, 2001) for more on this perspec-
tive). In (Sandler et al, 2008), standard regulariza-
tion models are constrained using graphs of word co-
occurrences. In (Melville et al, 2009), Naive Bayes
classifier is extended, where the parameters, the con-
ditional word distributions given the classes, are es-
timated by combining multiple sources, e.g. docu-
ment labels and word labels. Our work is based on
the dual supervision framework using constrained
non-negative tri-factorization.
3 Learning with Dual Supervision via
Tri-NMF
Our dual supervision model is based on non-
negative matrix tri-factorization (Tri-NMF), where
the non-negative input document-word matrix is ap-
proximated by 3 factor matrices as X ? GSF T , in
which,X is an n?m document-term matrix,G is an
n ? k non-negative orthogonal matrix representing
the probability of generating a document from a doc-
ument cluster, F is an m? k non-negative orthogo-
nal matrix representing the probability of generating
a word from a word cluster, and S is a k ? k non-
negative matrix providing the relationship between
document cluster space and word cluster space.
While Tri-NMF is first applied in co-clustering, Li
et al (Li et al, 2009) extended it to incorporate la-
beled words and documents as dual supervision via
two loss terms in the objective function of Tri-NMF
as following:
minF,G,S ?X ?GSF T ?2
+? trace[(F ? F0)TC1(F ? F0)]
+? trace[(G?G0)TC2(G?G0)].
(1)
Here, ? > 0 is a parameter which determines the
extent to which we enforce F ? F0 to its labeled
rows. C1 is a m ? m diagonal matrix whose en-
try (C1)ii = 1 if the row of F0 is labeled, that is,
the class of the i-th word is known and (C1)ii = 0
otherwise. ? > 0 is a parameter which determines
the extent to which we enforce G ? G0 to its la-
beled rows. C2 is a n ? n diagonal matrix whose
entry (C2)ii = 1 if the row of G0 is labeled, that
is, the category of the i-th document is known and
(C2)ii = 0 otherwise. The squared loss terms ensure
that the solution for G,F in the otherwise unsuper-
vised learning problem be close to the prior knowl-
edge G0, F0. So the partial labels on documents and
words can be described using G0 and F0, respec-
tively.
4 Dual Supervision with Explicit Class
Alignment
4.1 Modeling the Relationships between Word
Classes and Document Classes
In the solution to Equation 1, we have S = GTXF ,
or
Slk = gTl Xfk =
1
|Rl|1/2|Ck|1/2
?
i?Rl
?
j?Ck
Xij ,
(2)
where |Rl| is the size of the l-th document class, and
|Ck| is the size of the k-th word class (Ding et al,
2006). Note that Slk represents properly normalized
within-class sum of weights (l = k) and between-
class sum of weights (l 6= k). So, S represents the
relationship between the classes over documents and
the classes over words. Under the assumption that
the i-th document class should correspond to the i-
th word class, S should be an approximate diago-
nal matrix, since the documents of i-th class is more
likely to contain the words of the i-th class. Note
951
that S is not an exact diagonal matrix, since a doc-
ument of one class apparently can use words from
other classes (especially G and F are required to be
approximately orthogonal, which means the classi-
fication is rigorous). However, in Equation 1, there
are no explicit constraints on the relationship be-
tween word classes and document classes. Instead,
the relationship is established and enforced implic-
itly using existing labeled documents and words.
In active learning, the set of starting labeled doc-
uments or words is small, and this may generate an
ill-formed S, leading to an incorrect alignment of
word classes and document classes. To explicitly
model the relationships between word classes and
document classes, we constrain the shape of S via
an extra loss term in the objective function as fol-
lows:
minF,G,S ?X ?GSF T ?2
+? trace[(F ? F0)TC1(F ? F0)]
+? trace[(G?G0)TC2(G?G0)]
+? trace[(S ? S0)T (S ? S0)]
(3)
where S0 is a diagonal matrix.
How to Choose S0 If there is no orthogonal con-
straint on F,G and I-divergence is used as the ob-
jective function, it can been shown that the factors
of Tri-NMF have probabilistic interpretation (Ding
et al, 2008; Shen et al, 2011):
Fil = P (w = wi|zw = l),
Gjk = P (d = dj |zd = k),
Skl = P (zd = k, zw = l),
(4)
where w is word variable, d is document variable,
and zw, zd are random variables indicating word
class and document class respectively. F and G
represent posterior distributions for words and docu-
ments, and S represents the joint distribution of doc-
ument class and word class. With such an interpre-
tation, S0 can be easily decided in balanced classifi-
cation problems with each diagonal entry equals to
one over the number of classes.
However, in our setting of Tri-NMF, orthogonal
constraints are enforced on F,G and Euclidean dis-
tance is used as the objective function. To pre-
compute S0, one way is to first solve the optimiza-
tion problem Equation 1 with another constraint that
S should be diagonal. Alternatively, to keep it sim-
ple, we ignore the known label information and just
assume there exists a diagonal matrix S0 and two
orthogonal matrices G,F , that
GS0F T ? X.
Then
trace[XXT ] ? trace[GS0F TFST0 GT ],
= trace[S0ST0 F TFGTG],
= trace[S0ST0 ],
= ?k(S0)2kk.
(5)
So if a classification problem is balanced with K
classes, S0 can be estimated as following:
(S0)kl =
{ ?
trace[XXT ]
K l = k,
0 otherwise. (6)
4.2 Computing Algorithm
This optimization problem can be solved using the
following update rules
Gjk ? Gjk XFS+?C2G0(GGTXFS+?GGTC2G)jk ,
Sjk ? Sjk F
TXTG+?S0
(FTFSGTG+?S)jk ,
Fjk ? Fjk X
TGST+?C1F0
(FFTXTGST+?C1F )jk .
(7)
The algorithm consists of an iterative procedure us-
ing the above three rules until convergence.
Theorem 4.1 The above iterative algorithm con-
verges.
Theorem 4.2 At convergence, the solution satisfies
the Karuch-Kuhn-Tucker (KKT) optimality condi-
tion, i.e., the algorithm converges correctly to a lo-
cal optima.
Theorem 4.1 can be proved using the standard aux-
iliary function approach (Lee and Seung, 2001).
Proof of Theorem 4.2: Proof for the update rules
of G,F is the same as in (Li et al, 2009). Here we
focus on the update rule of S. We want to minimize
L(S) = ?X ?GSF T ?2
+? trace[(F ? F0)TC1(F ? F0)]
+? trace[(G?G0)TC2(G?G0)]
+? trace[(S ? S0)T (S ? S0)].
(8)
952
The gradient of L is
?L
?S = 2F
TFSGTG? 2F TXTG+ 2?(S ? S0)
The KKT complementarity condition for the non-
negativity of Sjk gives
[2F TFSGTG?2F TXTG+2?(S?S0)]jkSjk = 0.
This is the fixed point relation that local minima for
S must satisfy, which is equivalent with the update
rule of S in Equation 7.
5 A Unified Scheme for Query Selection
Using the Reconstruction Error
5.1 Introduction
An ideal active dual supervision scheme should be
able to evaluate the value of acquiring labels for doc-
uments and words on the same scale. In the initial
study of dual active supervision, different scores are
used for documents and words (e.g. uncertainty for
documents and certainty for words), and thus they
are not on the same scale (Sindhwani et al, 2009).
Recently, the framework of Expected Utility (Esti-
mated Risk Minimization) is proposed in (Attenberg
et al, 2010). At each step of the framework, the next
word or document selected for labeling is the one
that will result in the highest estimated improvement
in classifier performance as defined as:
EU(qj) =
K?
k=1
P (qj = ck)U(qj = ck), (9)
where K is the class number, P (qj = ck) indicates
the probability that qj , j-th query (a word or docu-
ment), belongs to the k-th class, and the U(qj = ck)
indicates the utility that qj belongs to the k-th class.
However, the choice of the utility measure is still a
challenge.
5.2 Reconstruction Error
In our matrix factorization framework, rows and
columns are treated equally in estimating the errors
of matrix factorization, and the reconstruction error
is thus a natural measure of utility. Let the current
supervision knowledge be G0, F0. To select a new
unlabeled document/word for labeling, we assume
that a good supervision should lead to a good con-
strained factorization for the document-term matrix,
X ? GSF T . If the new query qj is a word and its
label is k, then the new factorization is
G?j=k, S?j=k, F ?j=k
= argminG,S,F ?X ?GSF T ?2
+ ? trace[(G?G0)TC2(G?G0)]
+ ? trace[(F ? F0,j=k)TC1(F ? F0,j=k)]
+ ? trace[(S ? S0)T (S ? S0)],
(10)
where F0,j=k is same as F0 except that
F0,j=k(j, k) = 1. In other words, we obtained
a new factorization using the labeled words. Sim-
ilarly, if the new query qj is a document, then the
new factorization is
G?j=k, S?j=k, F ?j=k
= argminG,S,F ?X ?GSF T ?2
+ ? trace[(G?G0,j=k)TC2(G?G0,j=k)]
+ ? trace[(F ? F0)TC1(F ? F0)]
+ ? trace[(S ? S0)T (S ? S0)],
(11)
where G0,j=k is same as G0 except that
G0,j=k(j, k) = 1. In other words, we obtained
a new factorization using the labeled documents.
Then the new reconstruction error is
RE(qj = k) = ?X ?G?j=kS?j=kF ?j=k?2. (12)
So the expected utility of a document or word label
query, qj , can be computed as
EU(qj) =
K?
k=1
P (qj = k)? (?RE(qj = k)). (13)
To calculate the P (qj = k), which is the posterior
distribution for words or documents, probabilistic
interpretation of Tri-NMF is abused. When a query
qj is a word, P (qj = k) is
P (zw = k|w = wi)
? P (w = wi|zw = k)
?K
j=1 P (zw = k, zd = j)
= Fik ?
?K
j=1 Skj , (14)
otherwise,
P (zd = k|d = di)
? P (d = di|zd = k)
?K
j=1 P (zw = j, zd = k)
= Gik ?
?K
j=1 Sjk. (15)
953
5.3 Algorithm Description
Computational Improvement: It can be computa-
tionally intensive if the reconstruction error is com-
puted for all unknown documents and words. In-
spired by (Attenberg et al, 2010), we first select the
top 100 unknown words that the current model is
most certain about, and the top 100 unknown docu-
ments that the current model is most uncertain about.
Then we identify the words or documents in this
pool with the highest expected utility (reconstruc-
tion error). Equations 14 and 15 are used to perform
the initial selection of top 100 unknown words and
top 100 unknown documents.
Algorithm 1 Active Dual Supervision Algorithm
Based on Matrix Factorization
INPUT: X , document-word matrix; F0, current la-
beled words; G0, current labeled documents; O, the
oracle
OUTPUT: G, classification result for all documents
in X
1. Get base factorization of X: G,S, F .
2. Active dual supervision
repeat
D is the set of top 100 unlabeled documents
with most uncertainty;
W is the set of top 100 unlabeled words with
most certainty;
Q = D ?W ;
for all q ? Q do
for k = 1 to K do
Get G?q=k, F ?q=k, S?q=k by Equation 10 or
Equation 11 according to whether the
query q is a document or a word;
Calculate EU(q) by Equation 13;
q? = argmaxq EU(q);
Acquire new label of q?, l from O;
G,F, S = G?q?=l, F ?q?=l, S?q?=l;until stop criterion is met.
The overall algorithm procedure is described in
Algorithm 1. First we iteratively use the updat-
ing rules of Equation 7 to obtain the factoriza-
tion G,F, S based on initial labeled documents and
words. Then to select a new query, for each unla-
beled document or word in the pool and for each
possible class, we compute the reconstruction error
with new supervision (using the current factoriza-
tion results as initialization values). It is efficient to
compute a new factorization due to the sparsity of
the matrices. The document-term matrix is typically
very sparse with z  nm non-zero entries while k is
typically also much smaller than document number
n, and word numberm. By using sparse matrix mul-
tiplications and avoiding dense intermediate matri-
ces, updating F, S,G each takesO(k2(m+n)+kz)
time per iteration which scales linearly with the di-
mensions and density of the data matrix (Li et al,
2009). Empirically, the number of iterations that is
needed to compute the new factorization is usually
very small (less than 10).
6 Experiments
6.1 Experiments Settings
Three popular binary text classification datasets are
used in the experiments: ibm-mac (1937 examples),
baseball-hockey (1988 examples) and med-space
(1972 examples) datasets. All of them are drawn
from the 20-newsgroups text collection1 where the
task is to assign messages into the newsgroup in
which they appeared. Top 1500 frequent words in
each dataset are used as features in the binary vec-
tor representation. These datasets have labels for all
the documents. For a document query, the oracle re-
turns its label. We construct the word oracle in the
same manner as in (Sindhwani et al, 2009): first
compute the information gain of words with respect
to the known true class labels in the training splits of
a dataset, and then the top 100 words as ranked by
information gain are assigned the label which is the
class in which the word appears more frequently. To
those words with labels, the word oracle returns its
label; otherwise, the oracle returns a ?don?t know?
response (no word label is obtained for learning, but
the word is excluded from the following query se-
lection).
Results are averaged over 10 random training-
test splits. For each split, 30% examples are used
for testing. All methods are initialized by a ran-
dom choice of 10 document labels and 10 word la-
bels. For simplicity, we follow the widely used cost
model (Raghavan and Allan, 2007; Druck et al,
1http://www.ai.mit.edu/people/jrennie/
20_newsgroups/
954
2008; Sindhwani et al, 2009) where features are
roughly 5 times cheaper to label than examples, so
we assume the cost is 1 for a word query and is 5 for
a document query. We set ? = ? = 5, ? = 1 for all
the following experiments2.
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
10-10
20-15
30-20
40-25
50-30
400-50
500-60
600-70
700-80
800-90
Ac
cu
ra
cy
#labeled documents-#labeled words
w/o. constraint on S
w/. constraint on S
(a) baseball-hockey
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
10-10
20-15
30-20
40-25
50-30
400-50
500-60
600-70
700-80
800-90
Ac
cu
ra
cy
#labeled documents-#labeled words
w/o. constraint on S
w/. constraint on S
(b) ibm-mac
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
10-10
20-15
30-20
40-25
50-30
400-50
500-60
600-70
700-80
800-90
Ac
cu
ra
cy
#labeled documents-#labeled words
w/o. constraint on S
w/. constraint on S
(c) med-space
Figure 1: Comparing the performance of dual supervision
via Tri-NMF w/ and w/o the constraint on S.
2We do not perform fine tuning on the parameters since the
main objective of the paper is to demonstrate the effectiveness
of matrix factorization based methods for dual active supervi-
sion. A vigorous investigation on the parameter choices is our
further work.
6.2 Experimental Results
Effect of Constraints on S in Constrained Tri-
NMF Figure 1 demonstrates the effectiveness of
dual supervision with explicit class alignment via
Tri-NMF as described in Section 4. When there
are enough labeled documents and words, the con-
straints on S have a relative small impact on the per-
formance of dual supervision. However, in the be-
ginning phase of active learning, the labeled dataset
can be small (such as 10 labeled documents and 10
labeled words). In this case, without the constraint
of S, the matrix factorization may generate incorrect
class alignment, thus lead to almost random classi-
fication results (around 50% accuracy), as shown in
Figure 1, and further make unreasonable the follow-
ing evaluation of queries.
Comparing Query Selection Approaches Figure
2 compares our proposed unified scheme (denoted as
Expected-reconstruction-error) with the following
baselines using Tri-NMF as the classifier for dual
supervision: (1). Interleaved-uncertainty which
first selects feature query by certainty and sample
query by uncertainty and then combines the two
types of queries using an interleaving scheme. The
interleaving probability (probability to select the
query as a document) is set as 0.2, 0.4, 0.6 and
0.8. (2). Expected-log-gain which selects feature
and sample query by maximizing the expected log
gain. Expected-reconstruction-error outperforms
interleaving schemes with all the different interleav-
ing probability values with which we experimented.
It also has a better performance than Expected-log-
gain. Although log gain is a finer-grained utility
measure of classifier performance than accuracy and
has a good performance in the setting with a large set
of starting labeled documents (e.g., 100 documents),
it is not reliable especially in the setting with a small
set of labeled data. Different from the Expected-log-
gain, Expected-reconstruction-error estimates the
utility using the matrix reconstruction error, making
use of information of all documents and words, in-
cluding those unlabeled.
Comparing Interleaving Scheme vs. the Uni-
fied Scheme To further demonstrate the benefit
of the proposed unified scheme , we compare it
with its interleaved version: Interleaved-expected-
955
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Expected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error
(a) baseball-hockey
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Expected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error
(b) ibm-mac
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Expected-log-gainInterleaved-uncertainty-0.2Interleaved-uncertainty-0.4Interleaved-uncertainty-0.6Interleaved-uncertainty-0.8Expected-reconstruction-error
(c) med-space
Figure 2: Comparing the different query selection approaches in active learning via Tri-NMF with dual supervision.
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Interleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error
(a) baseball-hockey
 0.68
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Interleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error
(b) ibm-mac
 0.56
 0.58
 0.6
 0.62
 0.64
 0.66
 0.68
 0.7
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
Interleaved-expected-reconstruction-error-0.2Interleaved-expected-reconstruction-error-0.4Interleaved-expected-reconstruction-error-0.6Interleaved-expected-reconstruction-error-0.8Expected-reconstruction-error
(c) med-space
Figure 3: Comparing the unified and interleaving scheme based on reconstruction error.
construction-error which computes the utility of a
query using the reconstruction error, but uses inter-
leaving scheme to decide which type of query to
select. We experiment with different interleaving
probability values ranging from 0.2 to 0.8, which
lead to quite different performance results. From
Figure 3, the optimal interleaving probability value
varies on different datasets. For example, the proba-
bility value of 0.8 is among the optimal interleaving
probability values on baseball-hockey dataset but
performs poorly on ibm-mac dataset. This obser-
vation also illustrates the need for a unified scheme,
because of the difficulty in choosing the optimal in-
terleaving probability value. Although the proposed
unified scheme is not significantly better than its in-
terleaving counterparts for all interleaving probabil-
ity values on all datasets, it avoids the bad choices.
Figure 5 presents the sequence of different query
types selected by our unified scheme and it clearly
demonstrates the distribution patterns of different
query types. At the beginning phase of active learn-
ing, word queries have much higher probabilities to
be selected, which is consistent with the result of
previous work: feature labels can be more effec-
tive than examples in text classification (Druck et
 50  100  150  200  250  300
Qu
ery
 Ty
pe
Query Sequence
Word
Document
(a) baseball-hockey
 50  100  150  200  250  300
Qu
ery
 Ty
pe
Query Sequence
Word
Document
(b) ibm-mac
Figure 5: Example of query sequence.
al., 2008). And in the later learning phase, docu-
ments are more likely to be selected, since the num-
ber of words that can benefit the classification is
much smaller than the effective documents.
Reconstruction Error vs. Interleaving uncer-
tainty using GRADS It should be pointed out that
our unified scheme for query selection based on re-
construction error does not rely on the estimation
of model performance on training data and can be
easily integrated with other dual supervision mod-
956
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0.94
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
GRADS-Interleaving-0.5GRADS-Reconstruction-Error
(a) baseball-hockey
 0.62
 0.64
 0.66
 0.68
 0.7
 0.72
 0.74
 0.76
 0.78
 0.8
 0.82
 0.84
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
GRADS-Interleaving-0.5GRADS-Reconstruction-Error
(b) ibm-mac
 0.86
 0.87
 0.88
 0.89
 0.9
 0.91
 0.92
 0.93
 0  100  200  300  400  500  600  700  800
Acc
ura
cy
Labeling Cost
GRADS-Interleaving-0.5GRADS-Reconstruction-Error
(c) med-space
Figure 4: GRADS with reconstruction error and interleaving uncertainty.
els such as GRADS (Sindhwani et al, 2008). Fig-
ure 4 shows the comparison of GRADS using the
interleaved scheme with an interleaving probability
of 0.5, and using our unified scheme based on recon-
struction error. Among the 3 datasets we used, the
reconstruction error based approach outperforms the
interleaving scheme on baseball-hockey and ibm-
mac, and has similar performance with the interleav-
ing scheme on med-space.
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0  100  200  300  400  500  600  700  800
Ac
cu
ra
cy
Labeling Cost
GRADS-Interleaving-0.2
GRADS-Interleaving-0.4
GRADS-Interleaving-0.6
GRADS-Interleaving-0.8
Tri-NMF-Reconstruction-Error
Figure 6: Comparing active dual supervision using ma-
trix factorization with GRADS on sentiment analysis.
Comparing Active Dual Supervision Using Ma-
trix Factorization with GRADS on Sentiment
Analysis The sentiment analysis experiment is
conducted on the movies review dataset (Pang et al,
2002), containing 1000 positive and 1000 negative
movie reviews. The results are shown in Figure 6.
The experimental results clearly demonstrate the ef-
fectiveness of our approach, denoted as Tri-NMF-
Reconstruction-Error.
7 Conclusions
In this paper, we study the problem of active dual
supervision, and propose a matrix tri-factorization
based approach to address the issue, how to evaluate
labeling benifit of different types of queries (exam-
ples or features) in the same scale. Following ex-
tending the nonnegative matrix tri-factorization to
the active dual supervision setting, we use the recon-
struction error to evaluate the value of feature and
example labels. Experimental results show that our
proposed approach outperforms existing methods.
Acknowledgement
The work is partially supported by NSF grants
DMS-0915110, CCF-0830659, and HRD-0833093.
We would like to thank Dr. Vikas Sindhwani for
his insightful discussions and for sharing us with his
GRADS code.
References
J. Attenberg, P. Melville, and F. Provost. 2010. A Uni-
fied Approach to Active Dual Supervision for Label-
ing Features and Examples. Machine Learning and
Knowledge Discovery in Databases, pages 40?55.
D. Cohn, L. Atlas, and R. Ladner. 1994. Improving gen-
eralization with active learning. Machine Learning,
15(2):201?221.
I.S. Dhillon. 2001. Co-clustering documents and words
using bipartite spectral graph partitioning. In Pro-
ceedings of the seventh ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 269?274. ACM.
C. Ding, T. Li, W. Peng, and H. Park. 2006. Orthogonal
nonnegative matrix t-factorizations for clustering. In
Proceedings of the 12th ACM SIGKDD international
957
conference on Knowledge discovery and data mining,
pages 126?135. ACM.
C. Ding, T. Li, and W. Peng. 2008. On the equiva-
lence between non-negative matrix factorization and
probabilistic latent semantic indexing. Computational
Statistics & Data Analysis, 52(8):3913?3927.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of the 31st annual in-
ternational ACM SIGIR conference on Research and
development in information retrieval, pages 595?602.
ACM.
G. Druck, B. Settles, and A. McCallum. 2009. Active
learning by labeling features. In Proceedings of the
2009 conference on Empirical methods in natural lan-
guage processing, pages 81?90. Association for Com-
putational Linguistics.
S. Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.
2004. Document classification through interactive su-
pervision of document and term labels. Knowledge
Discovery in Databases: PKDD 2004, pages 185?196.
D.D. Lee and H.S. Seung. 2001. Algorithms for non-
negative matrix factorization. Advances in neural in-
formation processing systems, 13.
T. Li, Y. Zhang, and V. Sindhwani. 2009. A non-negative
matrix tri-factorization approach to sentiment classifi-
cation with lexical prior knowledge. In Proceedings of
the Joint Conference of the 47th Annual Meeting of the
ACL, pages 244?252. Association for Computational
Linguistics.
A.K. McCallum and K. Nigam. 1998. Employing EM
and pool-based active learning for text classification.
In Proceedings of the Fifteenth International Confer-
ence on Machine Learning. Citeseer.
P. Melville and V. Sindhwani. 2009. Active dual su-
pervision: Reducing the cost of annotating examples
and features. In Proceedings of the NAACL HLT 2009
Workshop on Active Learning for Natural Language
Processing, pages 49?57. Association for Computa-
tional Linguistics.
P. Melville, M. Saar-Tsechansky, F. Provost, and
R. Mooney. 2005. An expected utility approach to
active feature-value acquisition. In Proceedings of
Fifth IEEE International Conference on Data Mining.
IEEE.
P. Melville, W. Gryc, and R.D. Lawrence. 2009. Senti-
ment analysis of blogs by combining lexical knowl-
edge with text classification. In Proceedings of
the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 1275?
1284. ACM.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learning
techniques. In Proceedings of the 2002 conference
on Empirical methods in natural language processing,
pages 79?86. Association for Computational Linguis-
tics.
H. Raghavan and J. Allan. 2007. An interactive algo-
rithm for asking and incorporating feature feedback
into support vector machines. In Proceedings of the
30th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 79?86. ACM.
H. Raghavan, O. Madani, and R. Jones. 2006. Active
learning with feedback on features and instances. The
Journal of Machine Learning Research, 7:1655?1686.
T. Sandler, P.P. Talukdar, L.H. Ungar, and J. Blitzer.
2008. Regularized learning with networks of features.
Advances in Neural Information Processing Systems,
pages 1401?1408.
B. Settles. 2009. Active Learning Literature Survey.
Technical Report 1648.
C. Shen, T. Li, and C. Ding. 2011. Integrating Clustering
and Multi-Document Summarization by Bi-mixture
Probabilistic Latent Semantic Analysis (PLSA) with
Sentence Bases. In Proceedings of the national con-
ference on Artificial intelligence. AAAI Press.
V. Sindhwani and P. Melville. 2008. Document-word
co-regularization for semi-supervised sentiment anal-
ysis. In Data Mining, Eighth IEEE International Con-
ference on, pages 1025?1030. IEEE.
V. Sindhwani, J. Hu, and A. Mojsilovic. 2008. Regular-
ized co-clustering with dual supervision. Advances in
Neural Information Processing Systems, 21.
V. Sindhwani, P. Melville, and R.D. Lawrence. 2009.
Uncertainty sampling and transductive experimental
design for active dual supervision. In Proceedings of
the 26th Annual International Conference on Machine
Learning, pages 953?960. ACM.
S. Tong and D. Koller. 2002. Support vector machine
active learning with applications to text classification.
The Journal of Machine Learning Research, 2:45?66.
Omar F. Zaidan and Jason Eisner. 2008. Modeling anno-
tators: A generative approach to learning from annota-
tor rationales. In Proceedings of the 2008 conference
on Empirical methods in natural language processing,
pages 31?40. Association for Computational Linguis-
tics, October.
H. Zha, X. He, C. Ding, H. Simon, and M. Gu. 2001. Bi-
partite graph partitioning and data clustering. In Pro-
ceedings of the tenth international conference on In-
formation and knowledge management, pages 25?32.
ACM.
958
Proceedings of NAACL-HLT 2013, pages 1152?1162,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
A Participant-based Approach for Event Summarization Using
Twitter Streams
Chao Shen1, Fei Liu2, Fuliang Weng2, Tao Li1
1School of Computing and Information Sciences, Florida International University
Miami, Florida 33199, USA
2Research and Technology Center, Robert Bosch LLC
Palo Alto, California 94304, USA
{cshen001, taoli}@cs.fiu.edu
{fei.liu, fuliang.weng}@us.bosch.com
Abstract
Twitter offers an unprecedented advantage on
live reporting of the events happening around
the world. However, summarizing the Twit-
ter event has been a challenging task that was
not fully explored in the past. In this paper,
we propose a participant-based event summa-
rization approach that ?zooms-in? the Twit-
ter event streams to the participant level, de-
tects the important sub-events associated with
each participant using a novel mixture model
that combines the ?burstiness? and ?cohesive-
ness? properties of the event tweets, and gen-
erates the event summaries progressively. We
evaluate the proposed approach on different
event types. Results show that the participant-
based approach can effectively capture the
sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events,
yielding summaries with considerably better
coverage than the state-of-the-art.
1 Introduction
Twitter has increasingly become a critical source of
information. People report the events they are ex-
periencing or publish comments on a wide variety
of events happening around the world, ranging from
the unexpected natural disasters, regional riots, to
many scheduled events, such as sports games, po-
litical debates, local festivals, and even academic
conferences. The Twitter data streams thus cover
a broad range of events and broadcast these in-
formation in a live manner. Event summarization
in this paper aims to generate a representative and
concise textual description of the scheduled events
that are being lively reported on Twitter, providing
people with an alternative means of observing the
world beyond the traditional journalism. Specifi-
cally, we investigate scheduled events of different
types, including six of the NBA (National Basket-
ball Association) sports games and a representative
conference event, namely the Apple CEO?s keynote
speech in the Apple Worldwide Developers Confer-
ence (WWDC 2012)1. All these events have excited
great discussion among the Twitter community.
Summarizing the Twitter event is a challenging
task that has yet been fully explored in the past.
Most previous summarization studies focus on the
well-formatted news documents, as driven by the
annual DUC2 and TAC3 evaluations. In contrast,
the Twitter messages (a.k.a., tweets) are very short
and noisy, containing nonstandard terms such as ab-
breviations, acronyms, emoticons, etc. (Liu et al,
2011b; Liu et al, 2012; Eisenstein, 2013). The
noisy contents also cause great difficulties to the tra-
ditional NLP tools such as NER and dependency
parser (Ritter et al, 2011; Foster et al, 2011), lim-
iting the possibility of applying finer-grained event
analysis tools. In nature, the event tweets are closely
associated with the timeline and are drastically dif-
ferent from a static collection of news documents.
The tweets converge into text streams that pulse
along the timeline and cluster around the important
moments or sub-events. These ?sub-events? are of
crucial importance since they represent a surge of in-
terest from the Twitter audience and the correspond-
1https://developer.apple.com/wwdc/
2http://duc.nist.gov/
3http://www.nist.gov/tac/
1152
Figure 1: Example Twitter event stream (upper) and par-
ticipant stream (lower). Event stream contains tweets
related to an NBA basketball game (Spurs vs Thunder)
scheduled on May 31, 2012; participant stream contains
tweets corresponding to the player Russell Westbrook in
team Thunder. X-axis denotes the timeline and y-axis
represents the number of tweets per 10-second interval.
ing key information must be reflected in the event
summary. As such, event summarization research
has been focusing on developing accurate sub-event
detection systems and generating text descriptions
that can best summarize the sub-events in a progres-
sive manner (Chakrabarti and Punera, 2011; Nichols
et al, 2012; Zubiaga et al, 2012).
In Figure 1, we show an example Twitter event
stream and one of its ?participant? streams. The
event stream contains all the tweets related to an
NBA basketball game Spurs vs Thunder; while
the participant stream contains only tweets corre-
sponding to the player Russell Westbrook in this
game. Previous research on event summarization
focuses on identifying the important moments from
the coarse-level event stream. This may yield sev-
eral side effects: first, the spike patterns are not
clearly identifiable from the overall event stream,
though they are more clearly seen if we ?zoom-in? to
the participant level; second, it is arguable whether
the important sub-events can be accurately detected
based solely on the tweet volume change; third, a
popular participant or sub-event can elicit huge vol-
ume of tweets which dominant the event discussion
and shield less prominent sub-events. For example,
in the NBA games, discussions about the key players
(e.g., ?LeBron James?, ?Kobe Bryant?) can heavily
shadow other important participants or sub-events,
resulting in an event summary with repetitive de-
scriptions about the dominant players.
In this work, we propose a novel participant-
based event summarization approach, which dynam-
ically identifies the participants from data streams,
then ?zooms-in? the event stream to participant
level, detects the important sub-events related to
each participant using a novel time-content mixture
model, and generates the event summary progres-
sively by concatenating the descriptions of the im-
portant sub-events. Results show that the mixture
model-based sub-event detection approach can effi-
ciently incorporate the ?burstiness? and ?cohesive-
ness? of the participant streams, and the participant-
based event summarization can effectively capture
the sub-events that have otherwise been shadowed
by the long-tail of other dominant sub-events, yield-
ing summaries with considerably better coverage
than the state-of-the-art approach.
2 Related Work
Mining Twitter for event information has received
increasing attention in recent years. Many research
studies focus on identifying the trending events from
Twitter and providing a concise and dynamic visual-
ization of the information. The identified events are
often represented using a set of keywords. (Petro-
vic et al, 2010) proposed an algorithm based on
locality-sensitive hashing for detecting new events
from a stream of Twitter posts. (O?Connor et al,
2010; Becker et al, 2011b; Becker et al, 2011a;
Weng et al, 2011) proposed demo systems to dis-
play the event-related themes and popular tweets,
allowing the users to navigate through their topic
of interest. (Zhao et al, 2011) described an effort
to perform data collection and event recognition de-
spite various limits to the free access of Twitter data.
(Diao et al, 2012) integrated both temporal infor-
mation and users? personal interests for bursty topic
detection from the microblogs. (Ritter et al, 2012)
described an open-domain event-extraction and cat-
egorization system, which extracts an open-domain
calendar of significant events from Twitter.
With the identified events of interest, there is an
ever-increasing demand for event summarization,
which distills the huge volume of Twitter discus-
sions into a concise and representative textual de-
scription of the events. Many studies start with
the text summarization approaches that have been
shown to perform well on the news documents and
1153
develop adaptations to fit these methods to a col-
lection of event tweets. (Sharifi et al, 2010b) pro-
posed a graph-based phrase reinforcement algorithm
to build a one-sentence summary from a collection
of topic tweets. (Sharifi et al, 2010a; Inouye and
Kalita, 2011) presented a hybrid TF-IDF approach
to extract one- or multiple-sentence summary for
each topic. (Liu et al, 2011a) proposed to use
the concept-based ILP framework for summarizing
the Twitter trending topics, using both tweets and
the webpages linked from the tweets as input text
sources. (Harabagiu and Hickl, 2011) introduced a
generative framework that incorporates event struc-
ture and user behavior information in summarizing
multiple microblog posts related to the same topic.
Regarding summarizing the data streams, (Mar-
cus et al, 2011) introduced a ?TwitInfo? system to
visually summarize and track the events on Twit-
ter. They proposed an automatic peak detection and
labeling algorithm for the social streams. (Taka-
mura et al, 2011) proposed a summarization model
based on the facility location problem, which gener-
ates summary for a stream of short documents along
the timeline. (Chakrabarti and Punera, 2011) pro-
posed an event summarization algorithm based on
learning an underlying hidden state representation
of the event via hidden Markov models. (Louis and
Newman, 2012) presented a method for summariz-
ing a collection of tweets related to a business. The
proposed procedure aggregates tweets into subtopic
clusters which are then ranked and summarized
by a few representative tweets from each cluster.
(Nichols et al, 2012; Zubiaga et al, 2012) focused
on real-time event summarization, which detects the
sub-events by identifying those moments where the
tweet volume has increases sharply, then uses var-
ious weighting schemes to perform tweet selection
and finally generates the event summary.
Our work is different from the above research
studies in three folds: first, we propose to ?zoom-
in? the Twitter event streams to the participant
level, which allows us to clearly identify the im-
portant sub-events associated with each participant
and generate a balanced event summary with com-
prehensive coverage of all the important sub-events;
second, we propose a novel time-content mixture
model approach for sub-event detection, which ef-
fectively leverages the ?burstiness? and ?cohesive-
ness? of the event tweets and accurately detects
the participant-level sub-events. Third, we evalu-
ate the participant-based event summarization sys-
tem on different event types and demonstrate that the
proposed approach outperforms the state-of-the-art
method by a considerable margin.
3 Participant-based Event Summarization
We propose a novel participant-centered event sum-
marization approach that consists of three key com-
ponents: (1) ?Participant Detection? dynamically
identifies the event participants and divides the
entire event stream into a number of participant
streams (Section 3.1); (2) ?Sub-event Detection? in-
troduces a novel time-content mixture model ap-
proach to identify the important sub-events associ-
ated with each participant; these ?participant-level
sub-events? are then merged along the timeline to
form a set of ?global sub-events?4, which capture
all the important moments in the event stream (Sec-
tion 3.2); (3) ?Summary Tweet Extraction? extracts
the representative tweets from the global sub-events
and forms a comprehensive coverage of the event
progress (Section 3.3).
3.1 Participant Detection
We define event participants as the entities that play
a significant role in shaping the event progress. ?Par-
ticipant? is a general concept to denote the event
participating persons, organizations, product lines,
etc., each of which can be captured by a set of
correlated proper nouns. For example, the NBA
player ?LeBron Raymone James? can be represented
by {LeBron James, LeBron, LBJ, King James, L.
James}, where each proper noun represents a unique
mention of the participant. In this work, we automat-
ically identify the proper nouns from tweet streams,
filter out the infrequent ones using a threshold ?,
and cluster them into individual event participants.
This process allows us to dynamically identify the
key participating entities and provide a full-coverage
for these participants in the event summary.
4We use ?participant sub-events? and ?global sub-events?
respectively to represent the important moments happened on
the participant-level and on the entire event-level. A ?global
sub-event? may consist of one or more ?participant sub-events?.
For example., the ?steal? action in the basketball game typically
involves both the defensive and offensive players, and can be
generated by merging the two participant-level sub-events.
1154
We formulate the participant detection in a hier-
archical agglomerative clustering framework. The
CMU TweetNLP tool (Gimpel et al, 2011) was used
for proper noun tagging. The proper nouns (a.k.a.,
mentions) are grouped into clusters in a bottom-up
fashion. Two mentions are considered similar if they
share (1) lexical resemblance, and (2) contextual
similarity. For example, in the following two tweets
?Gotta respect Anthony Davis, still rocking the uni-
brow?, ?Anthony gotta do something about that uni-
brow?, the two mentions Anthony Davis and An-
thony are referring to the same participant and they
share both character overlap (?anthony?) and con-
text words (?unibrow?, ?gotta?). We use sim(ci, cj)
to represent the similarity between two mentions ci
and cj , defined as:
sim(ci, cj) = lex sim(ci, cj)?cont sim(ci, cj)
where the lexical similarity (lex sim(?)) is defined
as a binary function representing whether a mention
ci is an abbreviation, acronym, or part of another
mention cj , or if the character edit distance between
the two mentions is less than a threshold ?5:
lex sim(ci, cj)=
?
?
?
1 ci(cj) is part of cj(ci)
1 EditDist(ci, cj) < ?
0 Otherwise
We define the context similarity (cont sim(?)) of
two mentions as the cosine similarity between their
context vectors ~vi and ~vj . Note that on the tweet
stream, two temporally distant tweets can be very
different even though they are lexically similar, e.g.,
two slam dunk shots performed by the same player
at different time points are different. We there-
fore restrain the context to a segment of the tweet
stream |Sk| and then take the weighted average of
the segment-based similarity as the final context
similarity. To build the context vector, we use term
frequency (TF) as the term weight and remove all the
stopwords. We use |D| to represent the total tweets
in the event stream.
cont sim|Sk|(ci, cj) = cos(~vi, ~vj)
cont sim(ci, cj) =
?
k
|Sk|
|D|
? cont sim|Sk|(ci, cj)
5? was empirically set as 0.2?min{|ci|, |cj |}
t w
Wz? |D|
? ? ? ?'K B
Figure 2: Plate notation of the mixture model.
Similarity between two clusters of mentions are de-
fined as the maximum possible similarity between a
pair of mentions, each from one cluster:
sim(Ci, Cj) = max
ci?Ci,cj?Cj
sim(ci, cj)
We perform bottom-up agglomerative clustering on
the mentions until a stopping threshold ? has been
reached for sim(Ci, Cj). The clustering approach
naturally groups the frequent proper nouns into par-
ticipants. The participant streams are then formed
by gathering the tweets that contain one or more
mentions in the participant cluster.
3.2 Mixture Model-based Sub-event Detection
A sub-event corresponds to a topic that emerges
from the data stream, being intensively discussed
during a short period, and then gradually fades away.
The tweets corresponding to a sub-event thus de-
mand not only ?temporal burstiness? but also a cer-
tain degree of ?lexical cohesiveness?. To incorporate
both the time and content aspects of the sub-events,
we propose a mixture model approach for sub-event
detection. Figure 2 shows the plate notation.
In the proposed model, each tweet d in the data
stream D is generated from a topic z, weighted by
piz . Each topic is characterized by both its content
and time aspects. The content aspect is captured by
a multinomial distribution over the words, param-
eterized by ?; while the time aspect is character-
ized by a Gaussian distribution, parameterized by ?
and ?, with ? represents the average time point that
the sub-event emerges and ? determines the duration
of the sub-event. These distributions bear similari-
ties with the previous work (Hofmann, 1999; Allan,
2002; Haghighi and Vanderwende, 2009). In addi-
tion, there are often background or ?noise? topics
that are being constantly discussed over the entire
1155
event evolvement process and do not present the de-
sired ?burstiness? property. We use a uniform dis-
tribution U(tb, te) to model the time aspect of these
?background? topics, with tb and te being the event
beginning and end time points. The content aspect
of a background topic is modeled by similar multi-
nomial distribution, parameterized by ??. We use the
maximum likelihood parameter estimation. The data
likelihood can be represented as:
L(D) =
?
d?D
?
z
{pizpz(td)
?
w?d
pz(w)}
where pz(td) models the timestamp of tweet d under
the topic z; pz(w) corresponds to the word distribu-
tion in topic z. They are defined as:
pz(td) =
{
N(td;?z, ?z) if z is a sub-event topic
U(tb, te) if z is background topic
pz(w) =
{
p(w; ?z) if z is a sub-event topic
p(w; ??z) if z is background topic
where both p(w; ?z) and p(w; ??z) are multinomial
distributions over the words. Initially, we assume
there are K sub-event topics and B background top-
ics and use the EM algorithm for model fitting. The
EM equations are listed below:
E-step:
p(zd = j) ?
?
?
?
pijN(d;?j , ?j)
?
w?d
p(w; ?j) if j <= K
pijU(tb, te)
?
w?d
p(w; ??j) else
M-step:
pij ?
?
d
p(zd = j)
p(w; ?j) ?
?
d
p(zd = j)? c(w, d)
p(w; ??j) ?
?
d
p(zd = j)? c(w, d)
?j =
?
d p(zd = j)? td
?K
j=1
?
d p(zd = j)
?2j =
?
d p(zd = j)? (td ? ?j)
2
?K
j=1
?
d p(zd = j)
To process the data stream D, we divide the data
into 10-second bins and process each bin at a time.
The peak time of a sub-event was determined as
the bin that has the most tweets related to this sub-
event. During EM initialization, the number of sub-
event topics K was empirically decided by scanning
through the data stream and examine tweets in ev-
ery 3-minute stream segment. If there was a spike6,
we add a new sub-event to the model and use the
tweets in this segment to initialize the value of ?,
?, and ?. Initially, we use a fixed number of back-
ground topics with B = 4. A topic re-adjustment
was performed after the EM process. We merge two
sub-events in a data stream if they (1) locate closely
in the timeline, with peaks times within a 2-minute
window; and (2) share similar word distributions:
among the top-10 words with highest probability in
the word distributions, there are over 5 words over-
lap. We also convert the sub-event topics to back-
ground topics if their ? values are greater than a
threshold ?7. We then re-run the EM to obtain the
updated parameters. The topic re-adjustment pro-
cess continues until the number of sub-events and
background topics do not change further.
We obtain the ?participant sub-events? by ap-
plying this sub-event detection approach to each of
the participant streams. The ?global sub-events?
are obtained by merging the participant sub-events
along the timeline. We merge two participant sub-
events into a global sub-event if (1) their peaks are
within a 2-minute window, and (2) the Jaccard simi-
larity (Lee, 1999) between their associated tweets is
greater than a threshold (set to 0.1 empirically). The
tweets associated with each global sub-event are the
ones with p(z|d) greater than a threshold ?, where z
is one of the participant sub-events and ? was set to
0.7 empirically. After the sub-event detection pro-
cess, we obtain a set of global sub-events and their
associated event tweets.8
3.3 Summary Tweet Extraction
We extract a representative tweet from each of the
global sub-events and concatenate them to form an
informative event summary. Note that our goal in
this work is to identify all the important moments
6We use the algorithm described in (Marcus et al, 2011) as
a baseline and ad hoc spike detection algorithm.
7? was set to 5 minutes in our experiments.
8We empirically set some threshold values in the topic re-
adjustment and sub-event merging process. In future, we would
like to explore more principled way of parameter selection.
1156
Event Date Duration #Tweets
Lakers vs Okc 05/19/2012 3h10m 218,313
N Celtics vs 76ers 05/23/2012 3h30m 245,734
B Celtics vs Heat 05/30/2012 3h30m 345,335
A Spurs vs Okc 05/31/2012 3h 254,670
Heat vs Okc (1) 06/12/2012 3h30m 331,498
Heat vs Okc (2) 06/21/2012 3h30m 332,223
Apple?s WWDC?12 Conf. 06/11/2012 3h30m 163,775
Table 1: Statistics of the data set, including six NBA bas-
ketball games and the WWDC 2012 conference event.
for event summarization, but not on proposing new
methods for tweet selection. We thus use the Hybrid
TF-IDF approach (Sharifi et al, 2010a; Liu et al,
2011a) to extract the representative sentences from
a collection of tweets. In this approach, each tweet
was considered as a sentence. The sentences were
ranked according to the average TF-IDF score of the
consisting words; top weighted sentences were it-
eratively extracted, while excluding those that have
high cosine similarity with the existing summary
sentences. (Inouye and Kalita, 2011) showed the
Hybrid TF-IDF approach performs constantly better
than the phrase reinforcement algorithm and other
traditional summarization systems.
4 Data Corpus
We evaluate the proposed event summarization ap-
proach on six NBA basketball games and a repre-
sentative conference event, namely the Apple CEO?s
keynote speech in the Apple Worldwide Develop-
ers Conference (WWDC 2012)9. We use the het-
erogeneous event types to verify that the proposed
approach can robustly and efficiently produce sum-
maries on different event streams. The tweet streams
corresponding to these events are collected using
the Twitter Streaming API10 with pre-defined key-
word set. For NBA games, we use the team names,
first name and last name of the players and head
coaches as keywords for retrieving the event tweets;
for the WWDC conference, the keyword set contains
about 20 terms related to the Apple event, such as
?wwdc?, ?apple?, ?mac?, etc. We crawl the tweets
in real-time when these scheduled events are taking
place; nevertheless, certain non-event tweets could
be mis-included due to the broad coverage of the
used keywords. During preprocessing, we filter out
9https://developer.apple.com/wwdc/
10https://dev.twitter.com/docs/streaming-apis
Time Action (Sub-event) Score
9:22 Chris Bosh misses 10-foot two point shot 7-2
9:22 Serge Ibaka defensive rebound 7-2
9:11 Kevin Durant makes 15-foot two point shot 9-2
8:55 Serge Ibaka shooting foul (Shane Battier draws 9-2
the foul)
8:55 Shane Battier misses free throw 1 of 2 9-2
8:55 Miami offensive team rebound 9-2
8:55 Shane Battier makes free throw 2 of 2 9-3
Table 2: An example clip of the play-by-play live cov-
erage of an NBA game (Heat vs Okc). ?Time? corre-
sponds to the minutes left in the current quarter of the
game; ?Score? shows the score between the two teams.
the tweets containing URLs, non-English tweets,
and retweets since they are less likely containing
new information regarding the event progress. Ta-
ble 1 shows statistics of the event tweets after the
filtering process. In total, there are over 1.8 million
tweets used in the event summarization experiments.
We use the play-by-play live coverage collected
from the ESPN11 and MacRumors12 websites as ref-
erence, which provide detailed descriptions of the
NBA and WWDC events as they unfold. Table 2
shows an example clip of the play-by-play descrip-
tions of an NBA game. Ideally, each item in the live
coverage descriptions may correspond to a sub-event
in the tweet streams, but in reality, not all actions
would attract enough attention from the Twitter au-
dience. We use a human annotator to manually filter
out the actions that did not lead to any spike in the
corresponding participant stream. The rest items are
projected to the participant and event streams as the
goldstandard sub-events. The projection was man-
ually performed since the ?game clock? associated
with the goldstandard (first column in Table 2) does
not align well with the ?wall clock? due to the game
rules such as timeout and halftime rest. To evalu-
ate the participant detection performance, we ask the
annotator to manually group the proper noun men-
tions into clusters, each cluster corresponds to a par-
ticipant. The mentions that do not correspond to any
participant are discarded. The goldstandard event
summaries are generated by manually selecting one
representative tweet from each of the groundtruth
global sub-events. We choose not to use the play-
by-play descriptions as reference summaries since
their vocabulary is rather limited and do not overlap
with the tweet language.
11http://espn.go.com/nba/scoreboard
12http://www.macrumorslive.com/archive/wwdc12/
1157
Example Participants - NBA game
westbrook, russell westbrook
stephen jackson, steven jackson, jackson
james, james harden, harden
ibaka, serge ibaka
oklahoma city thunder, oklahoma
gregg popovich, greg popovich, popovich
kevin durant, kd, durant
thunder, okc, #okc, okc thunder, #thunder
Example Participants - WWDC Conference
macbooks, mbp, macbook pro, macbook air,...
google maps, google, apple maps
wwdc, apple wwdc, #wwdc
os, mountain, os x mountain, os x
iphone 4s, iphone 3gs, iphone
Table 3: Example participants automatically detected
from the NBA game Spurs vs Okc (2012-5-31) and the
WWDC?12 conference.
5 Experimental Results
We evaluate the participant-based event summariza-
tion in a cascaded fashion and present results for
each of the three components, including the par-
ticipant detection (Section 5.1), sub-event detection
(Section 5.2), and quantitative and qualitative evalu-
ation of example event summaries (Section 5.3).
5.1 Participant Detection Results
In Table 3, we show example participants that were
automatically detected by the proposed hierarchical
agglomerative clustering approach. We note that the
clusters include various mentions of the same event
participant, e.g., ?gregg popovich?, ?greg popovich?,
and ?popovich? are both referring to the head coach
of the team Spurs; ?macbooks?, ?macbook pro?,
?mbp? are referring to a line of products from Apple.
Quantitatively, we evaluate the participant detection
results on both participant- and mention-level. As-
sume the system-detected and the goldstandard par-
ticipant clusters are Ts and Tg respectively. We de-
fine a correct participant as a system detected par-
ticipant with more than half of its associated men-
tions are included in a goldstandard participant (re-
ferred to as the hit participant). As a result, we
can define the participant-level precision and recall
as below:
participant-prec = #correct-participants/|Ts|
participant-recall = #hit-participants/|Tg|
Note that a correct participant may include incor-
rect mentions, and that more than one correct par-
Figure 3: Participant detection performance. The upper
figures represent the participant-level precision and re-
call scores; while the lower figures represent the mention-
level precision and recall. X-axis corresponds to the six
NBA games and the WWDC conference.
ticipants may correspond to the same hit participant,
both of which are undesired. In the latter case, we
use representative participant to refer to the cor-
rect participant which contains the most mentions
in the hit participant. In this way, we build a 1-
to-1 mapping from the detected participants to the
groundtruth participants. Next, we define correct
mentions as the union of the overlapping mentions
between all pairs of representative and hit partici-
pants. Then we calculate the mention-level precision
and recall as the number of correct mentions divided
by the total mentions in the system or goldstandard
participant clusters.
Figure 3 shows the participant- and mention-level
precision and recall scores. We experimented with
different similarity measures for the agglomerative
clustering approach13. The ?global context? means
that the context vectors are created from the entire
data stream; this may not perform well since dif-
ferent participants can share similar global context.
E.g., the terms ?shot?, ?dunk?, ?rebound? can ap-
pear in the context of any NBA players and are not
13The stopping threshold ? was set to 0.15, local context
length is 3 minutes, and frequency threshold ? was set to 200.
1158
Participant-level Sub-event Detection Global Sub-event Detection
Event
#P #S
Spike MM
#S
Spike Participant + Spike Participant + MM
R P F R P F R P F R P F R P F
Lakers vs Okc 9 65 0.75 0.31 0.44 0.71 0.39 0.50 48 0.67 0.38 0.48 0.94 0.19 0.32 0.88 0.40 0.55
Celtics vs 76ers 10 88 0.52 0.39 0.45 0.53 0.43 0.47 60 0.65 0.51 0.57 0.72 0.18 0.29 0.78 0.39 0.52
Celtics vs Heat 14 152 0.53 0.29 0.37 0.50 0.38 0.43 67 0.57 0.41 0.48 0.97 0.21 0.35 0.91 0.28 0.43
Spurs vs Okc 12 98 0.78 0.46 0.58 0.84 0.57 0.68 81 0.41 0.42 0.41 0.88 0.35 0.50 0.91 0.54 0.68
Heat vs Okc (1) 15 123 0.75 0.27 0.40 0.72 0.35 0.47 85 0.41 0.47 0.44 0.94 0.20 0.33 0.96 0.34 0.50
Heat vs okc (2) 13 153 0.74 0.36 0.48 0.76 0.43 0.55 92 0.41 0.33 0.37 0.88 0.21 0.34 0.87 0.38 0.53
WWDC?12 10 56 0.64 0.14 0.23 0.59 0.33 0.42 43 0.53 0.26 0.35 0.77 0.14 0.24 0.70 0.31 0.43
Average 12 105 0.67 0.32 0.42 0.66 0.41 0.50 68 0.52 0.40 0.44 0.87 0.21 0.34 0.86 0.38 0.52
Table 4: Sub-event detection results on both participant and the event streams. ?Spike? corresponds to the spike
detection algorithm proposed in (Marcus et al, 2011); ?MM? represents our proposed time-content mixture model
approach. ?#P? and ?#S? list the number of participants and sub-events in each event stream.
discriminative enough. We found that adding the
lexical similarity measure greatly boosted the clus-
tering performance, especially on the mention-level,
and that combining the lexical similarity with the lo-
cal context is even more helpful for some events.
We notice that two events (celtics vs 76ers and
celtics vs heat) yield relatively low precision on both
participant- and mention-level. Taking a close look
at the data, we found that these two events acciden-
tally co-occurred with other popular events, namely
the TV program ?American Idol? finale and the NBA
Draft. The keyword based data crawler thus includes
many noisy tweets in the event streams, leading to
some false participants being detected.
5.2 Sub-event Detection Results
We compare our proposed time-content mixture
model (noted as ?MM?) against the spike detection
algorithm proposed in (Marcus et al, 2011) (noted
as ?Spike?) . The spike algorithm is based on the
tweet volume change. It uses 10 seconds as a time
unit, calculates the tweet arrival rate in each unit,
and identifies the rates that are significantly higher
than the mean tweet rate. For these rate spikes, the
algorithm finds the local maximum of tweet rate and
identify a window surrounding the local maximum.
We tune the parameter of the ?Spike? approach (set
? = 4) so that it yields similar recall values as the
mixture model approach. We then apply the ?MM?
and ?Spike? approaches to both the participant and
event streams and evaluate the sub-event detection
performance. Results are shown in Table 4. A sys-
tem detected sub-event is considered to match the
goldstandard sub-event if its peak time is within a
2-minute window of the goldstandard.
We first apply the ?Spike? and ?MM? approach to
the participant streams. The participant streams on
which we cannot detect any meaningful sub-events
have been excluded, the resulting number of partic-
ipants are listed in Table 4 and denoted as ?#P?.
In general, we found the ?MM? approach can per-
form better since it inherently incorporates both the
?burstiness? and ?lexical cohesiveness? of the event
tweets, while the ?Spike? approach relies solely on
the ?burstiness? property. Note that although we di-
vide the entire event stream into participant streams,
some key participants still own huge amount of dis-
cussion and the spike patterns are not always clearly
identifiable. The time-content mixture model gains
advantages in these cases.
We apply three settings to detect global sub-
events on the data streams. ?Spike? directly ap-
plies the spike algorithm on the entire event stream;
the ?Participant + Spike? and ?Participant + MM?
approaches first perform sub-event detection on the
participant streams and then merge the detected sub-
events along the timeline to generate global sub-
events. Note that there are fewer goldstandard
sub-events (?#S?) on the global streams since each
global sub-event may correspond to one or multiple
participant-level sub-events. Because of the averag-
ing effect, spike patterns on the entire event stream
is less obvious than those on the participant streams.
As a result, few spikes have been detected on the
event stream using the ?Spike? algorithm, which
leads to low recall as compared to other participant-
based approaches. It also indicates that, by dividing
the entire event stream into participant streams, we
have a better chance of identifying the sub-events
that have otherwise been shadowed by the domi-
nant sub-events or participants. The two participant-
based methods yield similar recall but ?Participant
1159
+ Spike? yields slightly worse precision, since it is
very sensitive to the spikes on the participant-level,
leading to the rise of false alarms. The ?Participant +
MM? approach is much better in precision, which is
consistent to our findings on the participant streams.
5.3 Summarization Results
Summarization evaluation has been a longstanding
issue in the literature (Nenkova and Mckeown, 2011;
Liu and Liu, 2010). There are even less studies fo-
cusing on evaluating the event summaries generated
from data streams. Since the summary annotation
takes quite some effort, we sample a 10-minute seg-
ment from each of the seven event streams and ask
a human annotator to select representative tweets
for each segment. We then compare the system
summaries against the manual summaries using the
ROUGE-1 (Lin, 2004) metric. The quantitative re-
sults and qualitative analysis are presented in Table 5
and Table 6 respectively. Note that the ROUGE
scores are based solely on the n-gram overlap be-
tween the system and reference summaries, which
may not be the most appropriate measure for eval-
uating the Twitter event summaries. However, we
do notice that the accurate sub-event detection per-
formance can successfully translate into a gain of
the ROUGE scores. Qualitatively, the participant-
based event summarization approach focus more on
extracting tweets associated with the targeted partic-
ipants, which could lead to better text coherence.
6 Conclusion and Future Work
In this work, we made an initial attempt to gen-
erate event summaries using Twitter data streams.
We proposed a participant-based event summariza-
tion approach which ?zooms-in? the Twitter event
streams to the participant level, detects the impor-
tant sub-events associated with each participant us-
ing a novel mixture model that incorporates both the
?burstiness? and ?cohesiveness? of tweets, and gen-
erates the event summaries progressively. Results
show that the proposed approach can effectively cap-
ture the sub-events that have otherwise been shad-
owed by the long-tail of other dominant sub-events,
yielding summaries with considerably better cover-
age. Without loss of generality, we report results
on the entire event streams, though the proposed ap-
proach can well be applied in an online fashion.
Event Method R(%) P(%) F(%)
NBA Average
Spike 14.73 23.24 16.87
Participant + Spike 54.60 14.65 22.40
Participant + MM 54.36 23.06 31.53
WWDC Conf.
Spike 26.58 39.62 31.82
Participant + Spike 49.37 25.16 33.33
Participant + MM 42.77 31.73 36.07
Table 5: ROUGE-1 scores of summarization
Method Summary
Manual
Good drive for durant
Pretty shot by Duncan
Good 3 point tony parker
Nice move westbrook
Good shot Westbrook
Spike
Game 3. Spurs vs. OKC
Okc and spurs game.
Participant
+ Spike
OKLAHOMA CITY THUNDER vs san antonio
spurs!! YA
I hope okc win the series. Ill hate too see the heat
play San Antonio
we aint in San Antonio anymore.
NBA: SA 0 OKC 8, 9:11 1st.#TeamOkc
San antonio spurs for 21 consecutive win? #nba
Somebody Should Stop Tim Duncan.
Pass the damn ball Westbrook
Good 3 pointer tony parker!
Participant
+ MM
Tim Duncan shot is so precise
Tim Duncan is gettin started
Good 3 pointer tony parker!
Sefalosa guarding tony parker. Good fucking move
coach brooks
Westbrook = 2 Fast 2 Furious
Niggas steady letting Tim Duncan shoot
Westbrook mid range shot is automatic
Table 6: Example summaries for an event segment. Par-
ticipants are marked using italicized text.
There are many challenges left in this line of re-
search. Having a standardized evaluation metric for
event summaries is one of them. In the current work,
we employed ROUGE-1 for summary evaluation,
since it has been shown to correlate well with the hu-
man judgements on noisy text genres (Liu and Liu,
2010). We would like to explore other evaluation
metrics (e.g., ROUGE-2, -SU4, Pyramid (Nenkova
et al, 2007)) and the human evaluation in future.
We will also explore better ways of integrating the
sub-event detection and summarization approaches.
Acknowledgments
Part of this work was done during the first author?s
internship in Bosch Research and Technology Cen-
ter. The work is also partially supported by NSF
grants DMS-0915110 and HRD-0833093.
1160
References
James Allan. 2002. Topic detection and tracking: Event-
based information organization. Kluwer Academic
Publishers Norwell, MA, USA.
Hila Becker, Feiyang Chen, Dan Iter, Mor Naaman, and
Luis Gravano. 2011a. Automatic identification and
presentation of twitter content for planned events. In
Proceedings of the Fifth International AAAI Confer-
ence on Weblogs and Social Media (ICWSM), pages
655?656.
Hila Becker, Mor Naaman, and Luis Gravano. 2011b.
Beyond trending topics: Real-world event identifica-
tion on twitter. In Proceedings of the Fifth Interna-
tional AAAI Conference on Weblogs and Social Media
(ICWSM), pages 438?441.
Deepayan Chakrabarti and Kunal Punera. 2011. Event
summarization using tweets. In Proceedings of the
Fifth International AAAI Conference on Weblogs and
Social Media (ICWSM), pages 66?73.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng Lim.
2012. Finding bursty topics from microblogs. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 536?544.
Jacob Eisenstein. 2013. What to do about bad language
on the internet. In Proceedings of the 2013 Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL/HLT).
Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,
Joseph Le Roux, Stephen Hogan, Joakim Nivre,
Deirdre Hogan, and Josef van Genabith. 2011. #hard-
toparse: POS tagging and parsing the twitterverse. In
Proceedings of the AAAI Workshop on Analyzing Mi-
crotext, pages 20?25.
Kevin Gimpel, Nathan Schneider, Brendan O?Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies (ACL-HLT), pages 42?47.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-Document summariza-
tion. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics (NAACL), pages 362?370.
Sanda Harabagiu and Andrew Hickl. 2011. Relevance
modeling for microblog summarization. In Proceed-
ings of the Fifth International AAAI Conference on We-
blogs and Social Media (ICWSM), pages 514?517.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence (UAI).
David Inouye and Jugal K. Kalita. 2011. Compar-
ing twitter summarization algorithms for multiple post
summaries. In Proceedings of 2011 IEEE Third Inter-
national Conference on Social Computing, pages 290?
306.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
25?32.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Workshop on Text Sum-
marization Branches Out.
Feifan Liu and Yang Liu. 2010. Exploring correlation
between ROUGE and human evaluation on meeting
summaries. IEEE Transactions on Audio, Speech, and
Language Processing, 18(1):187?196.
Fei Liu, Yang Liu, and Fuliang Weng. 2011a. Why is
?SXSW? trending? Exploring multiple text sources
for twitter topic summarization. In Proceedings of the
ACL Workshop on Language in Social Media (LSM),
pages 66?75.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011b. Insertion, deletion, or substitution? Normal-
izing text messages without pre-categorization nor su-
pervision. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A broad-
coverage normalization system for social media lan-
guage. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 1035?1044.
Annie Louis and Todd Newman. 2012. Summarization
of business-related tweets: A concept-based approach.
In Proceedings of the 24th International Conference
on Computational Linguistics (COLING).
Adam Marcus, Michael S. Bernstein, Osama Badar,
David R. Karger, Samuel Madden, and Robert C.
Miller. 2011. Twitinfo: Aggregating and visualizing
microblogs for event exploration. In Proceedings of
the SIGCHI Conference on Human Factors in Com-
puting Systems, pages 227?236.
Ani Nenkova and Kathleen Mckeown. 2011. Automatic
summarization. Foundations and Trends in Informa-
tion Retrieval, 5(2?3):103?233.
Ani Nenkova, Rebecca Passonneau, and Kathleen Mcke-
own. 2007. The pyramid method: Incorporating hu-
man content selection variation in summarization eval-
uation. ACM Transactions on Speech and Language
Processing, 4(2).
1161
Jeffrey Nichols, Jalal Mahmud, and Clemens Drews.
2012. Summarizing sporting events using twitter. In
Proceedings of the 2012 ACM Interntional Conference
on Intelligent User Interfaces (IUI), pages 189?198.
Brendan O?Connor, Michel Krieger, and David Ahn.
2010. TweetMotif: Exploratory search and topic sum-
marization for twitter. In Proceedings of the Fourth
International AAAI Conference on Weblogs and Social
Media (ICWSM), pages 384?385.
Sasa Petrovic, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with application
to twitter. In Proceedings of the 2010 Annual Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics (NAACL), pages
181?189.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An exper-
imental study. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1524?1534.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 1104?1112.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010a. Experiments in microblog summariza-
tion. In Proceedings of the 2010 IEEE Second Interna-
tional Conference on Social Computing, pages 49?56.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010b. Summarizing microblogs automati-
cally. In Proceedings of the 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics (NAACL), pages 685?688.
Hiroya Takamura, Hikaru Yokono, and Manabu Oku-
mura. 2011. Summarizing a document stream. In
Proceedings of the 33rd European Conference on Ad-
vances in Information Retrieval (ECIR), pages 177?
188.
Jui-Yu Weng, Cheng-Lun Yang, Bo-Nian Chen, Yen-Kai
Wang, and Shou-De Lin. 2011. Imass: An intelli-
gent microblog analysis and summarization system. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL-HLT), pages 133?138.
Siqi Zhao, Lin Zhong, Jehan Wickramasuriya, and Venu
Vasudevan. 2011. Human as real-time sensors of so-
cial and physical events: A case study of twitter and
sports games. Technical Report TR0620-2011, Rice
University and Motorola Labs.
Arkaitz Zubiaga, Damiano Spina, Enrique Amigo?, and
Julio Gonzalo. 2012. Towards real-time summariza-
tion of scheduled events from twitter streams. In Pro-
ceedings of the 23rd ACM Conference on Hypertext
and Social Media, pages 319?320.
1162
