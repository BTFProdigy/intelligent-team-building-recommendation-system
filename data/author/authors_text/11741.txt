Proceedings of the 12th Conference of the European Chapter of the ACL, pages 398?405,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Generating a Non-English Subjectivity Lexicon:
Relations That Matter
Valentin Jijkoun and Katja Hofmann
ISLA, University of Amsterdam
Amsterdam, The Netherlands
{jijkoun,k.hofmann}@uva.nl
Abstract
We describe a method for creating a non-
English subjectivity lexicon based on an
English lexicon, an online translation ser-
vice and a general purpose thesaurus:
Wordnet. We use a PageRank-like algo-
rithm to bootstrap from the translation of
the English lexicon and rank the words
in the thesaurus by polarity using the net-
work of lexical relations in Wordnet. We
apply our method to the Dutch language.
The best results are achieved when using
synonymy and antonymy relations only,
and ranking positive and negative words
simultaneously. Our method achieves an
accuracy of 0.82 at the top 3,000 negative
words, and 0.62 at the top 3,000 positive
words.
1 Introduction
One of the key tasks in subjectivity analysis is
the automatic detection of subjective (as opposed
to objective, factual) statements in written doc-
uments (Mihalcea and Liu, 2006). This task is
essential for applications such as online market-
ing research, where companies want to know what
customers say about the companies, their prod-
ucts, specific products? features, and whether com-
ments made are positive or negative. Another
application is in political research, where pub-
lic opinion could be assessed by analyzing user-
generated online data (blogs, discussion forums,
etc.).
Most current methods for subjectivity identi-
fication rely on subjectivity lexicons, which list
words that are usually associated with positive or
negative sentiments or opinions (i.e., words with
polarity). Such a lexicon can be used, e.g., to clas-
sify individual sentences or phrases as subjective
or not, and as bearing positive or negative senti-
ments (Pang et al, 2002; Kim and Hovy, 2004;
Wilson et al, 2005a). For English, manually cre-
ated subjectivity lexicons have been available for
a while, but for many other languages such re-
sources are still missing.
We describe a language-independent method
for automatically bootstrapping a subjectivity lex-
icon, and apply and evaluate it for the Dutch lan-
guage. The method starts with an English lexi-
con of positive and negative words, automatically
translated into the target language (Dutch in our
case). A PageRank-like algorithm is applied to the
Dutch wordnet in order to filter and expand the set
of words obtained through translation. The Dutch
lexicon is then created from the resulting ranking
of the wordnet nodes. Our method has several ben-
efits:
? It is applicable to any language for which a
wordnet and an automatic translation service
or a machine-readable dictionary (from En-
glish) are available. For example, the Eu-
roWordnet project (Vossen, 1998), e.g., pro-
vides wordnets for 7 languages, and free on-
line translation services such as the one we
have used in this paper are available for many
other languages as well.
? The method ranks all (or almost all) entries of
a wordnet by polarity (positive or negative),
which makes it possible to experiment with
different settings of the precision/coverage
threshold in applications that use the lexicon.
We apply our method to the most recent version
of Cornetto (Vossen et al, 2007), an extension of
the Dutch WordNet, and we experiment with vari-
ous parameters of the algorithm, in order to arrive
at a good setting for porting the method to other
languages. Specifically, we evaluate the quality of
the resulting Dutch subjectivity lexicon using dif-
ferent subsets of wordnet relations and informa-
tion in the glosses (definitions). We also examine
398
the effect of the number of iterations on the per-
formance of our method. We find that best perfor-
mance is achieved when using only synonymy and
antonymy relations and, moreover, the algorithm
converges after about 10 iterations.
The remainder of the paper is organized as fol-
lows. We summarize related work in section 2,
present our method in section 3 and describe the
manual assessment of the lexicon in section 4. We
discuss experimental results in section 5 and con-
clude in section 6.
2 Related work
Creating subjectivity lexicons for languages other
than English has only recently attracted attention
of the research community. (Mihalcea et al, 2007)
describes experiments with subjectivity classifica-
tion for Romanian. The authors start with an En-
glish subjectivity lexicon with 6,856 entries, Opin-
ionFinder (Wiebe and Riloff, 2005), and automat-
ically translate it into Romanian using two bilin-
gual dictionaries, obtaining a Romanian lexicon
with 4,983 entries. A manual evaluation of a sam-
ple of 123 entries of this lexicon showed that 50%
of the entries do indicate subjectivity.
In (Banea et al, 2008) a different approach
based on boostrapping was explored for Roma-
nian. The method starts with a small seed set of
60 words, which is iteratively (1) expanded by
adding synonyms from an online Romanian dic-
tionary, and (2) filtered by removing words which
are not similar (at a preset threshold) to the orig-
inal seed, according to an LSA-based similarity
measure computed on a half-million word cor-
pus of Romanian. The lexicon obtained after 5
iterations of the method was used for sentence-
level sentiment classification, indicating an 18%
improvement over the lexicon of (Mihalcea et al,
2007).
Both these approaches produce unordered sets
of positive and negative words. Our method,
on the other hand, assigns polarity scores to
words and produces a ranking of words by polar-
ity, which provides a more flexible experimental
framework for applications that will use the lexi-
con.
Esuli and Sebastiani (Esuli and Sebastiani,
2007) apply an algorithm based on PageRank to
rank synsets in EnglishWordNet according to pos-
itive and negativite sentiments. The authors view
WordNet as a graph where nodes are synsets and
synsets are linked with the synsets of terms used
in their glosses (definitions). The algorithm is ini-
tialized with positivity/negativity scores provided
in SentiWordNet (Esuli and Sebastiani, 2006), an
English sentiment lexicon. The weights are then
distributed through the graph using an the algo-
rithm similar to PageRank. Authors conclude that
larger initial seed sets result in a better ranking
produced by the method. The algorithm is always
run twice, once for positivity scores, and once for
negativity scores; this is different in our approach,
which ranks words from negative to positive in
one run. See section 5.4 for a more detailed com-
parison between the existing approaches outlined
above and our approach.
3 Approach
Our approach extends the techniques used in
(Esuli and Sebastiani, 2007; Banea et al, 2008)
for mining English and Romanian subjectivity lex-
icons.
3.1 Boostrapping algorithm
We hypothesize that concepts (synsets) that are
closely related in a wordnet have similar meaning
and thus similar polarity. To determine relatedness
between concepts, we view a wordnet as a graph
of lexical relations between words and synsets:
? nodes correspond to lexical units (words) and
synsets; and
? directed arcs correspond to relations between
synsets (hyponymy, meronymy, etc.) and be-
tween synsets and words they contain; in one
of our experiments, following (Esuli and Se-
bastiani, 2007), we also include relations be-
tween synsets and all words that occur in their
glosses (definitions).
Nodes and arcs of such a graph are assigned
weights, which are then propagated through the
graph by iteratively applying a PageRank-like al-
gorithm.
Initially, weights are assigned to nodes and arcs
in the graph using translations from an English po-
larity lexicon as follows:
? words that are translations of the positive
words from the English lexicon are assigned
a weight of 1, words that are translations of
the negative words are initialized to -1; in
general, weight of a word indicates its polar-
ity;
399
? All arcs are assigned a weight of 1, except
for antonymy relations which are assigned
a weight of -1; the intuition behind the arc
weights is simple: arcs with weight 1 would
usually connect synsets of the same (or simi-
lar) polarity, while arcs with weight -1 would
connect synsets with opposite polarities.
We use the following notation. Our algorithm
is iterative and k = 0, 1, . . . denotes an iteration.
Let aki be the weight of the node i at the k-th iter-
ation. Let wjm be the weight of the arc that con-
nects node j with nodem; we assume the weight is
0 if the arc does not exist. Finally, ? is a damping
factor of the PageRank algorithm, set to 0.8. This
factor balances the impact of the initial weight of
a node with the impact of weight received through
connections to other nodes.
The algorithm proceeds by updating the weights
of nodes iteratively as follows:
ak+1i = ? ?
?
j
akj ? wji
?
m |wjm|
+ (1? ?) ? a0i
Furthermore, at each iterarion, all weights ak+1i
are normalized by maxj |a
k+1
j |.
The equation above is a straightforward exten-
sion of the PageRank method for the case when
arcs of the graph are weighted. Nodes propagate
their polarity mass to neighbours through outgoing
arcs. The mass transferred depends on the weight
of the arcs. Note that for arcs with negative weight
(in our case, antonymy relation), the polarity of
transferred mass is inverted: i.e., synsets with neg-
ative polarity will enforce positive polarity in their
antonyms.
We iterate the algorithm and read off the result-
ing weight of the word nodes. We assume words
with the lowest resulting weight to have negative
polarity, and word nodes with the highest weight
positive polarity. The output of the algorithm is a
list of words ordered by polarity score.
3.2 Resources used
We use an English subjectivity lexicon of Opinion-
Finder (Wilson et al, 2005b) as the starting point
of our method. The lexicon contains 2,718 English
words with positive polarity and 4,910 words with
negative polarity. We use a free online translation
service1 to translate positive and negative polar-
ity words into Dutch, resulting in 974 and 1,523
1http://translate.google.com
Dutch words, respectively. We assumed that a
word was translated into Dutch successfully if the
translation occurred in the Dutch wordnet (there-
fore, the result of the translation is smaller than the
original English lexicon).
The Dutch wordnet we used in our experiments
is the most recent version of Cornetto (Vossen et
al., 2007). This wordnet contains 103,734 lexical
units (words), 70,192 synsets, and 157,679 rela-
tions between synsets.
4 Manual assessments
To assess the quality of our method we re-used
assessments made for earlier work on comparing
two resources in terms of their usefulness for au-
tomatically generating subjectivity lexicons (Jij-
koun and Hofmann, 2008). In this setting, the
goal was to compare two versions of the Dutch
Wordnet: the first from 2001 and the other from
2008. We applied the method described in sec-
tion 3 to both resources and generated two subjec-
tivity rankings. From each ranking, we selected
the 2000 words ranked as most negative and the
1500 words ranked as most positive, respectively.
More negative than positive words were chosen to
reflect the original distribution of positive vs. neg-
ative words. In addition, we selected words for
assessment from the remaining parts of the ranked
lists, randomly sampling chunks of 3000 words at
intervals of 10000 words with a sampling rate of
10%. The selection was made in this way because
we were mostly interested in negative and positive
words, i.e., the words near either end of the rank-
ings.
4.1 Assessment procedure
Human annotators were presented with a list of
words in random order, for each word its part-of-
speech tag was indicated. Annotators were asked
to identify positive and negative words in this list,
i.e., words that indicate positive (negative) emo-
tions, evaluations, or positions.
Annotators were asked to classify each word on
the list into one of five classes:
++ the word is positive in most contexts (strongly
positive)
+ the word is positive in some contexts (weakly
positive)
0 the word is hardly ever positive or negative
(neutral)
400
? the a word is negative in some contexts
(weakly negative)
?? the word is negative in most contexts
(strongly negative)
Cases where assessors were unable to assign a
word to one of the classes, were separately marked
as such.
For the purpose of this study we were only inter-
ested in identifying subjective words without con-
sidering subjectivity strength. Furthermore, a pi-
lot study showed assessments of the strength of
subjectivity to be a much harder task (54% inter-
annotator agreement) than distinguishing between
positive, neutral and negative words only (72%
agreement). We therefore collapsed the classes of
strongly and weakly subjective words for evalua-
tion. These results for three classes are reported
and used in the remainder of this paper.
4.2 Annotators
The data were annotated by two undergraduate
university students, both native speakers of Dutch.
Annotators were recruited through a university
mailing list. Assessment took a total of 32 work-
ing hours (annotating at approximately 450-500
words per hour) which were distributed over a to-
tal of 8 annotation sessions.
4.3 Inter-annotator Agreement
In total, 9,089 unique words were assessed, of
which 6,680 words were assessed by both anno-
tators. For 205 words, one or both assessors could
not assign an appropriate class; these words were
excluded from the subsequent study, leaving us
with 6,475 words with double assessments.
Table 1 shows the number of assessed words
and inter-annotator agreement overall and per
part-of-speech. Overall agreement is 69% (Co-
hen?s ?=0.52). The highest agreement is for ad-
jectives, at 76% (?=0.62) . This is the same
level of agreement as reported in (Kim and Hovy,
2004) for English. Agreement is lowest for verbs
(55%, ?=0.29) and adverbs (56%, ?=0.18), which
is slightly less than the 62% agreement on verbs
reported by Kim and Hovy. Overall we judge
agreement to be reasonable.
Table 2 shows the confusion matrix between the
two assessors. We see that one assessor judged
more words as subjective overall, and that more
words are judged as negative than positive (this
POS Count % agreement ?
noun 3670 70% 0.51
adjective 1697 76% 0.62
adverb 25 56% 0.18
verb 1083 55% 0.29
overall 6475 69% 0.52
Table 1: Inter-annotator agreement per part-of-
speech.
can be explained by our sampling method de-
scribed above).
? 0 + Total
? 1803 137 39 1979
0 1011 1857 649 3517
+ 81 108 790 979
Total 2895 2102 1478 6475
Table 2: Contingency table for all words assessed
by two annotators.
5 Experiments and results
We evaluated several versions of the method of
section 3 in order to find the best setting.
Our baseline is a ranking of all words in the
wordnet with the weight -1 assigned to the trans-
lations of English negative polarity words, 1 as-
signed to the translations of positive words, and
0 assigned to the remaining words. This corre-
sponds to simply translating the English subjec-
tivity lexicon.
In the run all.100 we applied our method to all
words, synsets and relations from the DutchWord-
net to create a graph with 153,386 nodes (70,192
synsets, 83,194 words) and 362,868 directed arcs
(103,734 word-to-synset, 103,734 synset-to-word,
155,400 synset-to-synset relations). We used 100
iterations of the PageRank algorihm for this run
(and all runs below, unless indicated otherwise).
In the run syn.100 we only used synset-to-
word, word-to-synset relations and 2,850 near-
synonymy relations between synsets. We added
1,459 near-antonym relations to the graph to
produce the run syn+ant.100. In the run
syn+hyp.100 we added 66,993 hyponymy and
66,993 hyperonymy relations to those used in run
syn.100.
We also experimented with the information pro-
vided in the definitions (glosses) of synset. The
glosses were available for 68,122 of the 70,192
401
synsets. Following (Esuli and Sebastiani, 2007),
we assumed that there is a semantic relationship
between a synset and each word used in its gloss.
Thus, the run gloss.100 uses a graph with 70,192
synsets, 83,194 words and 350,855 directed arcs
from synsets to lemmas of all words in their
glosses. To create these arcs, glosses were lemma-
tized and lemmas not found in the wordnet were
ignored.
To see if the information in the glosses can com-
plement the wordnet relations, we also generated
a hybrid run syn+ant+gloss.100 that used arcs de-
rived from word-to-synset, synset-to-word, syn-
onymy, antonymy relations and glosses.
Finally, we experimented with the number of
iterations of PageRank in two setting: using all
wordnet relations and using only synonyms and
antonyms.
5.1 Evaluation measures
We used several measures to evaluate the quality
of the word rankings produced by our method.
We consider the evaluation of a ranking parallel
to the evaluation for a binary classification prob-
lem, where words are classified as positive (resp.
negative) if the assigned score exceeds a certain
threshold value. We can select a specific thresh-
old and classify all words exceeding this score as
positive. There will be a certain amount of cor-
rectly classified words (true positives), and some
incorrectly classified words (false positives). As
we move the threshold to include a larger portion
of the ranking, both the number of true positives
and the number of false positives increase.
We can visualize the quality of rankings by plot-
ting their ROC curves, which show the relation be-
tween true positive rate (portion of the data cor-
rectly labeled as positive instances) and false pos-
itive rate (portion of the data incorrectly labeled
as positive instances) at all possible threshold set-
tings.
To compare rankings, we compute the area un-
der the ROC curve (AUC), a measure frequently
used to evaluate the performance of ranking clas-
sifiers. The AUC value corresponds to the proba-
bility that a randomly drawn positive instance will
be ranked higher than a randomly drawn negative
instance. Thus, an AUC of 0.5 corresponds to ran-
dom performance, a value of 1.0 corresponds to
perfect performance. When evaluating word rank-
ings, we compute AUC? and AUC+ as evalua-
Run ?k Dk AUC? AUC+
baseline 0.395 0.303 0.701 0.733
syn.10 0.641 0.180 0.829 0.837
gloss.100 0.637 0.181 0.829 0.835
all.100 0.565 0.218 0.792 0.787
syn.100 0.645 0.177 0.831 0.839
syn+ant.100 0.650 0.175 0.833 0.841
syn+ant+gloss.100 0.643 0.178 0.831 0.838
syn+hyp.100 0.594 0.203 0.807 0.810
Table 3: Evaluation results
tion measures for the tasks of identifying words
with negative (resp., positive) polarity.
Other measures commonly used to evalu-
ate rankings are Kendall?s rank correlation, or
Kendall?s tau coefficient, and Kendall?s dis-
tance (Fagin et al, 2004; Esuli and Sebastiani,
2007). When comparing rankings, Kendall?s mea-
sures look at the number of pairs of ranked items
that agree or disagree with the ordering in the gold
standard. The measures can deal with partially
ordered sets (i.e., rankings with ties): only pairs
that are ordered in the gold standard are used.
Let T = {(ai, bi)}i denote the set of pairs or-
dered in the gold standard, i.e., ai ?g bi. Let
C = {(a, b) ? T | a ?r b} be the set of con-
cordant pairs, i.e., pairs ordered the same way in
the gold standard and in the ranking. Let D =
{(a, b) ? T | b ?r a} be the set of discordant
pairs and U = T \ (C ? D) the set of pairs or-
dered in the gold standard, but tied in the rank-
ing. Kendall?s rank correlation coefficient ?k and
Kendall?s distance Dk are defined as follows:
?k =
|C| ? |D|
|T |
Dk =
|D|+ p ? |U |
|T |
where p is a penalization factor for ties, which we
set to 0.5, following (Esuli and Sebastiani, 2007).
The value of ?k ranges from -1 (perfect dis-
agreement) to 1 (perfect agreement), with 0 indi-
cating an almost random ranking. The value of
Dk ranges from 0 (perfect agreement) to 1 (per-
fect disagreement).
When applying Kendall?s measures we assume
that the gold standard defines a partial order: for
two words a and b, a ?g b holds when a ? Ng, b ?
Ug ? Pg or when a ? Ug, b ? Pg; here Ng, Ug, Pg
are sets of words judged as negative, neutral and
positive, respectively, by human assessors.
5.2 Types of wordnet relations
The results in Table 3 indicate that the method per-
forms best when only synonymy and antonymy
402
Negative polarity
False positive rate
Tru
e p
osit
ive 
rate
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
baseline
all.100
gloss.100
syn+ant.100
syn+hyp.100
Positive polarity
False positive rate
Tru
e p
osit
ive 
rate
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
baseline
all.100
gloss.100
syn+ant.100
syn+hyp.100
Figure 1: ROC curves showing the impact of using different sets of relations for negative and positive
polarity. Graphs were generated using ROCR (Sing et al, 2005).
relations are considered for ranking. Adding hy-
ponyms and hyperonyms, or adding relations be-
tween synsets and words in their glosses substan-
tially decrease the performance, according to all
four evaluation measures. With all relations, the
performance degrades even further. Our hypothe-
sis is that with many relations the polarity mass of
the seed words is distributed too broadly. This is
supported by the drop in the performance early in
the ranking at the ?negative? side of runs with all
relations and with hyponyms (Figure 1, left). An-
other possible explanation can be that words with
many incoming arcs (but without strong connec-
tions to the seed words) get substantial weights,
thereby decreasing the quality of the ranking.
Antonymy relations also prove useful, as using
them in addition to synonyms results in a small
improvement. This justifies our modification of
the PageRank algorithm, when we allow negative
node and arc weights.
In the best setting (syn+ant.100), our method
achieves an accuracy of 0.82 at top 3,000 negative
words, and 0.62 at top 3,000 positive words (esti-
mated from manual assessments of a sample, see
section 4). Moreover, Figure 1 indicates that the
accuracy of the seed set (i.e., the baseline transla-
tions of the English lexicon) is maintained at the
positive and negative ends of the ranking for most
variants of the method.
5.3 The number of iterations
In Figure 2 we plot how the AUC? measure
changes when the number of PageRank iterations
increases (for positive polarity; the plots are al-
most identical for negative polarity). Although the
absolute maximum of AUC is achieved at 110 iter-
ation (60 iterations for positive polarity), the AUC
clearly converges after 20 iterations. We conclude
that after 20 iterations all useful information has
been propagated through the graph. Moreover, our
version of PageRank reaches a stable weight dis-
tribution and, at the same time, produces the best
ranking.
5.4 Comparison to previous work
Although the values in the evaluation results are,
obviously, language-dependent, we tried to repli-
cate the methods used in the literature for Roma-
nian and English (section 2), to the degree possi-
ble.
Our baseline replicates the method of (Mihal-
cea et al, 2007): i.e., a simple translation of the
English lexicon into the target language. The
run syn.10 is similar to the iterative method used
in (Banea et al, 2008), except that we do not per-
form a corpus-based filtering. We run PageRank
for 10 iterations, so that polarity is propagated
from the seed words to all their 5-step-synonymy
neighbours. Table 3 indicates that increasing the
number of iterations in the method of (Banea et
403
0 50 100 150 200
0.7
0
0.7
5
0.8
0
0.8
5
0.9
0
Number of iterations
AU
C
all relations
synsets+antonyms
Figure 2: The number of iterations and the ranking
quality (AUC), for positive polarity. Rankings for
negative polarity behave similarly.
al., 2008) might help to generate a better subjec-
tivity lexicon.
The run gloss.100 is similar to the PageRank-
based method of (Esuli and Sebastiani, 2007).
The main difference is that Esuli and Sebastiani
used the extended English WordNet, where words
in all glosses are manually assigned to their cor-
rect synsets: the PageRank method then uses re-
lations between synsets and synsets of words in
their glosses. Since such a resource is not avail-
able for our target language (Dutch), we used rela-
tions between synsets and words in their glosses,
instead. With this simplification, the PageRank
method using glosses produces worse results than
the method using synonyms. Further experiments
with the extended English WordNet are neces-
sary to investigate whether this decrease can be at-
tributed to the lack of disambiguation for glosses.
An important difference between our method
and (Esuli and Sebastiani, 2007) is that the lat-
ter produces two independent rankings: one for
positive and one for negative words. To evalu-
ate the effect of this choice, we generated runs
gloss.100.N and gloss.100.P that used only nega-
tive (resp., only positive) seed words. We compare
these runs with the run gloss.100 (that starts with
both positive and negative seeds) in Table 4. To
allow a fair comparison of the generated rankings,
the evaluation measures in this case are calculated
separately for two binary classification problems:
words with negative polarity versus all words, and
words with positive polarity versus all.
The results in Table 4 clearly indicate that in-
Run ??k D
?
k AUC
?
gloss.100 0.669 0.166 0.829
gloss.100.N 0.562 0.219 0.782
?+k D
+
k AUC
+
gloss.100 0.665 0.167 0.835
gloss.100.P 0.580 0.210 0.795
Table 4: Comparison of separate and simultaneous
rankings of negative and positive words.
formation about words of one polarity class helps
to identify words of the other polarity: negative
words are unlikely to be also positive, and vice
versa. This supports our design choice: ranking
words from negative to positive in one run of the
method.
6 Conclusion
We have presented a PageRank-like algorithm that
bootstraps a subjectivity lexicon from a list of
initial seed examples (automatic translations of
words in an English subjectivity lexicon). The al-
gorithm views a wordnet as a graph where words
and concepts are connected by relations such as
synonymy, hyponymy, meronymy etc. We initial-
ize the algorithm by assigning high weights to pos-
itive seed examples and low weights to negative
seed examples. These weights are then propagated
through the wordnet graph via the relations. After
a number of iterations words are ranked according
to their weight. We assume that words with lower
weights are likely negative and words with high
weights are likely positive.
We evaluated several variants of the method for
the Dutch language, using the most recent version
of Cornetto, an extension of Dutch WordNet. The
evaluation was based on the manual assessment
of 9,089 words (with inter-annotator agreement
69%, ?=0.52). Best results were achieved when
the method used only synonymy and antonymy
relations, and was ranking positive and negative
words simultaneously. In this setting, the method
achieves an accuracy of 0.82 at the top 3,000 neg-
ative words, and 0.62 at the top 3,000 positive
words.
Our method is language-independent and can
easily be applied to other languages for which
wordnets exist. We plan to make the implemen-
tation of the method publicly available.
An additional important outcome of our experi-
ments is the first (to our knowledge) manually an-
notated sentiment lexicon for the Dutch language.
404
The lexicon contains 2,836 negative polarity and
1,628 positive polarity words. The lexicon will be
made publicly available as well. Our future work
will focus on using the lexicon for sentence- and
phrase-level sentiment extraction for Dutch.
Acknowledgments
This work was supported by projects DuO-
MAn and Cornetto, carried out within the
STEVIN programme which is funded by the
Dutch and Flemish Governments (http://
www.stevin-tst.org), and by the Nether-
lands Organization for Scientific Research (NWO)
under project number 612.061.814.
References
Carmen Banea, Rada Mihalcea, and Janyce Wiebe.
2008. A bootstrapping method for building subjec-
tivity lexicons for languages with scarce resources.
In LREC.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In Proceedings of LREC 2006,
pages 417?422.
Andrea Esuli and Fabrizio Sebastiani. 2007. Pager-
anking wordnet synsets: An application to opinion
mining. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 424?431.
Ronald Fagin, Ravi Kumar, Mohammad Mahdian,
D. Sivakumar, and Erik Vee. 2004. Com-
paring and aggregating rankings with ties. In
PODS ?04: Proceedings of the twenty-third ACM
SIGMOD-SIGACT-SIGART symposium on Princi-
ples of database systems, pages 47?58, New York,
NY, USA. ACM.
Valentin Jijkoun and Katja Hofmann. 2008.
Task-based Evaluation Report: Building a
Dutch Subjectivity Lexicon. Technical report.
Technical report, University of Amsterdam.
http://ilps.science.uva.nl/biblio/
cornetto-subjectivity-lexicon.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th International Conference on Computational
Linguistics (COLING).
R. Mihalcea and H. Liu. 2006. A corpus-based ap-
proach to finding happiness. In Proceedings of
the AAAI Spring Symposium on Computational Ap-
proaches to Weblogs.
Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007. Learning multilingual subjective language via
cross-lingual projections. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics, pages 976?983, Prague, Czech Repub-
lic, June. Association for Computational Linguis-
tics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification using
machine learning techniques. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2002), pages 79?86.
T. Sing, O. Sander, N. Beerenwinkel, and T. Lengauer.
2005. ROCR: visualizing classifier performance in
R. Bioinformatics, 21(20):3940?3941.
P. Vossen, K. Hofman, M. De Rijke, E. Tjong
Kim Sang, and K. Deschacht. 2007. The cornetto
database: Architecture and user-scenarios. In Pro-
ceedings of 7th Dutch-Belgian Information Retrieval
Workshop DIR2007.
Piek Vossen, editor. 1998. EuroWordNet: a mul-
tilingual database with lexical semantic networks.
Kluwer Academic Publishers, Norwell, MA, USA.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceeding of CICLing-05, In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, volume 3406 of
Lecture Notes in Computer Science, pages 475?486.
Springer-Verlag.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005a. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing (HLT/EMNLP 2005), pages 347?354.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005b. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings of
HLTEMNLP 2005.
405
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 174?182,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Lexical Patterns or Dependency Patterns:
Which Is Better for Hypernym Extraction?
Erik Tjong Kim Sang
Alfa-informatica
University of Groningen
e.f.tjong.kim.sang@rug.nl
Katja Hofmann
ISLA, Informatics Institute
University of Amsterdam
khofmann@science.uva.nl
Abstract
We compare two different types of extraction
patterns for automatically deriving semantic
information from text: lexical patterns, built
from words and word class information, and
dependency patterns with syntactic informa-
tion obtained from a full parser. We are partic-
ularly interested in whether the richer linguis-
tic information provided by a parser allows for
a better performance of subsequent informa-
tion extraction work. We evaluate automatic
extraction of hypernym information from text
and conclude that the application of depen-
dency patterns does not lead to substantially
higher precision and recall scores than using
lexical patterns.
1 Introduction
For almost a decade, automatic sentence parsing
systems with a reasonable performance (90+% con-
stituent precision/recall) have been available for En-
glish (Charniak, 1999). In recent years there has
been an increase in linguistic applications which use
parsing as a preprocessing step, e.g. Snow et al
(2006) and Surdeanu et al (2008). One of the boosts
for these new applications was the increasing power
of desktop computers, which allows for an easier ac-
cess to the computing-intensive parsing results. An-
other is the increased popularity of dependency pars-
ing of which the results can easily be incorporated
into followup systems.
Although there is a consensus about the fact that
the richness of the dependency structures should, in
principle, enable better performance than lexical in-
formation or shallow parsing results, it is not clear if
these better results can also be obtained in practice.
A performance of 90% precision and recall at con-
stituent level still leaves an average of one error in
a medium-length sentence of ten words. These er-
rors could degrade the performance of any approach
which relies heavily on parser output.
The question of whether to include a full parser as
a preprocessor for natural language processing task,
has led to a heated discussion between the two au-
thors of the paper. One of us argues that full parsers
are slow and make too many errors, and relies on
shallow techniques like part-of-speech tagging for
preprocessing. The other points at the decreasing
costs of computing and improvements in the reliabil-
ity of parsers, and recommends dependency parsers
as preprocessing tools.
While no automatic text preprocessing method is
free of errors, it is indeed true that approaches other
than full parsing, like for example shallow parsing,
offer useful information at a considerably cheaper
processing cost. The choice between using a heavy
full parser or a light shallow language analyzer is
one that developers of language processing systems
frequently have to make. The expected performance
boost of parsed data could be an important motiva-
tion for choosing for full syntactic analysis. How-
ever, we do not know how big the difference be-
tween the two methods will be. In order to find this
out, we designed an experiment in which we com-
pared the effects of preprocessing with and without
using information generated by a full parser.
In this paper, we compare two text preprocess-
ing approaches for a single language processing
task. The first of the two methods is shallow lin-
174
guistic processing, a robust and fast text analysis
method which only uses information from words,
like lemma information and part-of-speech classes.
The second method is dependency parsing which in-
cludes information about the syntactic relations be-
tween words. The natural language processing task
which we will use for assessing the usability of the
two processing methods is automatic extraction of
hypernym information from text. The language of
the text documents is Dutch. We expect that the find-
ings of this study would have been similar if any
other Germanic language (including English) was
used.
The contribution of this paper is a thorough and
fair comparison of the involved preprocessing tech-
niques. There have been earlier studies of hyper-
nym extraction with either lexical or dependency ex-
traction patterns. However, these studies applied the
techniques to a variety of different data sets and used
different evaluation techniques. We will apply the
two methods to the same data, evaluate the results in
a consistent manner and examine the differences.
After this introduction, we will describe the task,
the preprocessing methods and the evaluation setting
in more detail. In the third section, we will show
how our experiments were set up and present the re-
sults. Section four contains a detailed discussion of
the two methods and their effect on the extraction
task. In the final section of the paper, we will present
some concluding remarks.
2 Task and methods
We will apply two different preprocessing methods
to the task of extracting lexical information from
text. In the next sections we describe this task, dis-
cuss different methods for preprocessing the data
and outline the method used for evaluating the re-
sults.
2.1 Extracting hypernym relations
We will concentrate on extracting a single type of
lexical relation: hypernymy. Word A is a hypernym
of word B if the meaning of A both covers the mean-
ing of B and is broader. For example, color is a hy-
pernym of red which in turn is a hypernym of scar-
let. If A is a hypernym of B than B is a hyponym of
A.
There has been quite a lot of work on extracting
hypernymy pairs from text. The pioneering work
of Hearst (1992) applied fixed patterns like NP1 ,
especially NP2 to derive that NP1 is a hypernym
of NP2. Lately there has been a lot of interest in
acquiring such text patterns using a set of hyper-
nymy examples, e.g. Pantel et al (2004) and Snow
(2006). Application of such techniques has not been
restricted to English but also involved other lan-
guages such as Dutch (Tjong Kim Sang and Hof-
mann, 2007). Recent work has also examined ex-
tracting hypernym information from structured data,
like Wikipedia (Sumida and Torisawa, 2008).
For our extraction work, we will closely follow
the approach described in Snow et al (2006):
1. Collect from a text corpus phrases (consecutive
word sequences from a single sentence) that
contain a pair of nouns
2. Mark each phrase as containing a hypernym
pair or a non-hypernym pair according to a lex-
ical resource
3. Remove the noun pair from the phrases and
register how often each phrase is associated by
hypernym pairs and by non-hypernym pairs
4. Use this information for training a machine
learning system to predict whether two nouns
are a hypernym-hyponym pair based on the
phrases in which they occur in a text corpus
For example, we find two phrases: colors such as
cyan and colors such as birds, both of which contain
the basic phrase such as. We mark the first phrase
as a hypernym phrase (color is a hypernym of cyan)
while the second is marked as non-hypernym (color
is not a hypernym of bird). Thus the pattern such as
will receive a positive point and a negative point. A
machine learning algorithm can deduce from these
numbers that two other nouns occurring in the same
pattern will have an estimated probability of 50% of
being related according to hypernymy. The learner
can use information from other patterns to obtain a
better estimation of this probability.
2.2 Lexical patterns
We use two different text preprocessing methods
which automatically assign linguistic information to
sentences. The first preprocessing method has the
175
advantage of offering a fast analysis of the data but
its results are less elaborate than those of the second
method. The first method consists of three steps:
? Tokenization: sentence boundaries are detected
and punctuation signs are separated from words
? Part-of-speech tagging: part-of-speech classes
like noun and verb are assigned to words
? Lemmatization: words are reduced to their ba-
sic form (lemma)
The analysis process would convert a phrase like
Large cities in northern England such as Liverpool
are beyond revival. to lemmas and their associated
part-of-speech tags: large/JJ city/NN in/IN north/JJ
England/NNP such/DT as/IN Liverpool/NNP be/VB
beyond/IN revival/NN ./.
Like in the work of Snow et al (2005), the tar-
get phrases for hypernym extraction are two noun
phrases, with a maximum of three tokens in be-
tween and one or two optional extra tokens (a non-
head token of the first noun phrase and/or one of
the second noun phrase). The lexical preprocessing
method uses two basic regular expressions for find-
ing noun phrases: Determiner? Adjective* Noun+
and ProperNoun+. It assumes that the final token
of the matched phrase is the head. Here is one set of
four patterns which can be derived from the example
sentence:
1. NP in NP
2. large NP in NP
3. NP in north NP
4. large NP in north NP
The patterns contain the lemmas rather than the
words of the sentence in order to allow for general
patterns. For the same reason, the noun phrases have
been replaced by the token NP. Each of the four pat-
terns will be used as evidence for a possible hyper-
nymy relation between the two noun phrase heads
city and England. As a novel extension to the work
of Snow et al, we included two additional variants
of each pattern in which either the first NP or the
second NP was replaced by its head:
5. city in NP
6. NP in England
This enabled us to identify among others appositions
as patterns: president NP.
2.3 Dependency patterns
A dependency analysis contains the same three steps
used for finding lexical patterns: tokenization, part-
of-speech tagging and lemmatization. Additionally,
it includes a fourth step:
? Dependency parsing: find the syntactic depen-
dency relations between the words in each sen-
tence
The syntactic analysis is head-based which means
that for each word in the sentence it finds another
word that dominates it. Here is a possible analysis
of the previous example sentence:
large:JJ:MOD:NN:city
city:NN:SUBJ:VBD:be
in:IN:MOD:NN:city
north:JJ:MOD:NNP:England
England:NNP:OBJ1:IN:in
such:DT:MOD:IN:as
as:IN:MOD:NN:city
Liverpool:NNP:OBJ1:IN:as
be:VB:?:?:?
beyond:IN:MOD:VB:be
revival:NN::OBJ1:IN:beyond
Each line contains a lemma, its part-of-speech tag,
the relation between the word and its head, the part-
of-speech tag of its head and the lemma of the head
word. Our work with dependency patterns closely
follows the work of Snow et al (2005). Patterns are
defined as dependency paths with at most three in-
termediate nodes between the two focus nouns. Ad-
ditional satellite nodes can be present next to the two
nouns. The dependency patterns contain more infor-
mation than the lexical patterns. Here is one of the
patterns that can be derived for the two noun phrases
large cities and northern England in the example
sentence:
NP1:NN:SUBJ:VBD:
in:IN:MOD:NN:NP1
NP2:NNP:OBJ1:IN:in
The pattern defines a path from the head lemma city
via in, to England. Note that lemma information
linking outside this pattern (be at the end of the first
line) has been removed and that lemma information
176
from the target noun phrases has been replaced by
the name of the noun phrase (NP1 at the end of the
second line). For each dependency pattern, we build
six variants similar to the six variants of the lexical
patterns: four with additional information from the
two noun phrases and two more with head informa-
tion of one of the two target NPs.
Both preprocessing methods can identify phrases
like N such as N1 , N2 and N3 as well. Such phrases
produce evidence for each of the pairs (N,N1),
(N,N2) and (N,N3). These three noun pairs will be
included in the data collected for the patterns that
can be derived from the phrase.
We expect that an important advantage of using
dependency patterns over lexical patterns will be
that the former offer a wider coverage. In the ex-
ample sentence, no lexical pattern will associate city
with Liverpool because there are too many words in
between. However, a dependency pattern will cre-
ate a link between these two words, via the word
as. This will enable the dependency patterns to find
out that city is a hypernym of Liverpool, where the
lexical patterns are not able to do this based on the
available information.
The two preprocessing methods generate a large
number of noun pairs associated by patterns. Like
Snow et al (2005), we keep only noun pairs which
are associated by at least five different patterns. The
same constraint is enforced on the extraction pat-
terns: we keep only the patterns which are associ-
ated by at least five different noun pairs. The data
is converted to binary feature vectors representing
noun pairs. These are training data for a Bayesian
Logistic Regression system, BBRtrain (Genkin et
al., 2004). We use the default settings of the learn-
ing system and test its prediction capability in a bi-
nary classification task: whether two nouns are re-
lated according to hypernymy or not. Evaluation is
performed by 10-fold cross validation.
2.4 Evaluation
For parameter optimization we need an automatic
evaluation procedure, since repeated manual checks
of results generated by different versions of the
learner require too much time. We have adopted the
evaluation method of Snow et al(2006): compare
the generated hypernyms with hypernyms present in
a lexical resource, in our case the Dutch part of Eu-
roWordNet (1998).
This choice results in two restrictions. First, we
will only consider pairs of known words (words that
are present in the lexical resource) for evaluation.
We have no information about other words so we
make no assumptions about them. Second, if two
words appear in the lexical resource but not in the
hypernym relation of that same resource then we
will assume that they are unrelated. In other words,
we assume the hypernymy relation specified in the
lexical resource as complete (like in the work of
Snow et al (2006)).
We use standard evaluation scores. We will com-
pute precision and recall for the candidate hyper-
nyms, as well as the related F?=1 rate, the harmonic
mean between precision and recall. Precision will be
computed against all chosen candidate hypernyms.
However, recall will only be computed against the
positive noun pairs which occur in the phrases se-
lected by the examined method. The different pre-
processing methods may cause different numbers of
positive pairs to be selected. Only these pairs will
be used for computing recall scores. Others will be
ignored. For this reason we will report the selected
number of positive target pairs in the result tables as
well1.
3 Experiments and results
We have applied the extraction techniques to two
different Dutch corpora. The first is a collection of
texts from the news domain. It consists of texts from
five different Dutch news papers from the Twente
News Corpus collection. Two versions of this cor-
pus exist. We have worked with the version which
contains the years 1997-2005 (26 million sentences
and 450 million tokens). The second corpus is the
Dutch Wikipedia. Here we used a version of Octo-
ber 2006 (5 million sentences and 58 million words).
Syntactic preprocessing of the material was done
with the Alpino parser, the best available parser for
Dutch with a labeled dependency accuracy of 89%
(Van Noord, 2006). Rather than performing the
parsing task ourselves, we have relied on an avail-
able parsed treebank which included the text corpora
1In a seperate study we have shown that the observed differ-
ences between the two methods remain the same when recall is
computed over sets of similar sizes (Tjong Kim Sang, 2009).
177
that we wanted to use (Van Noord, 2009).
The parser also performs part-of-speech tagging
and lemmatization, tasks which are useful for the
lexical preprocessing methods. However, taking fu-
ture real-time applications in mind, we did not want
the lexical processing to be dependent on the parser.
Therefore we have developed an in-house part-of-
speech tagger and lemmatizer based on the material
created in the Corpus Spoken Dutch project (Eynde,
2005). The tagger achieved an accuracy of 96% on
test data from the same project while the lemmatizer
achieved 98%.
We used the Dutch part of EuroWordNet (Vossen,
1998) as the gold standard lexical resource, both for
training and testing. In the lexicon, many nouns have
different senses. This can cause problems for the
pattern extraction process. For example, if a noun
N1 with sense X is related to another noun N2 then
the appearance of N1 with sense Y with N2 in the
text may be completely accidental and say nothing
about the relation between the two words. In that
case it would be wrong to regard the context of the
two words as an interesting extraction pattern.
There are several ways to deal with this prob-
lem. One is to automatically assign senses to words.
However we do not have a reliable sense tagger for
Dutch at our disposal. Another method was pro-
posed by Snow et al(2005): assume that every word
bears its most frequent sense. But this is also in-
formation which we lack for Dutch: our lexical re-
source does not contain frequency information for
word senses. We have chosen the approach sug-
gested by Hofmann and Tjong Kim Sang (2007):
remove all nouns with multiple senses from the data
set and use only the monosemous words for find-
ing good extraction patterns. This restriction is only
imposed in the training phase. We consider both
monosemous words and polysemous words in the
evaluation process.
We imposed two additional restrictions on the lex-
ical resource. First, we removed the top noun of
the hypernymy hierarchy (iets) from the list of valid
hypernyms. This word is a valid hypernym of any
other noun. It is not an interesting suggestion for
the extraction procedure to put forward. Second, we
restricted the extraction procedure to propose only
known hypernyms as candidate hypernyms. Nouns
that appeared in the lexical resources only as hy-
lexical patterns
Data source Targ. Prec. Recall F?=1
AD 620 55.8% 27.9% 37.2
NRC 882 50.4% 23.8% 32.3
Parool 462 51.8% 21.9% 30.8
Trouw 607 54.1% 25.9% 35.0
Volkskrant 970 49.7% 24.1% 32.5
Newspapers 3307 43.1% 26.7% 33.0
Wikipedia 1288 63.4% 44.3% 52.1
dependency patterns
Data source Targ. Prec. Recall F?=1
AD 706 42.9% 30.2% 35.4
NRC 1224 26.2% 25.3% 25.7
Parool 584 31.2% 23.8% 27.0
Trouw 760 35.3% 29.0% 31.8
Volkskrant 1204 29.2% 25.5% 27.2
Newspapers 3806 20.7% 29.1% 24.2
Wikipedia 1580 61.9% 47.0% 53.4
Table 1: Hypernym extraction scores for the five news-
papers in the Twente News Corpus (AD, NRC, Parool,
Trouw and Volkskrant) and for the Dutch Wikipedia.
The Targets column shows the number of unique posi-
tive word pairs in each data set. The Dutch Wikipedia
contains about as much data as one of the newspaper sec-
tions.
ponyms (leaf nodes of the hypernymy tree) were
never proposed as candidate hypernyms. This made
sense for our evaluation procedure which is only
aimed at finding known hypernym-hyponym pairs.
We performed two hypernym extraction experi-
ments, one which used lexical extraction patterns
and one which used dependency patterns2. The re-
sults from the experiments can be found in Table
1. The newspaper F-scores obtained with lexical
patterns are similar to those reported for English
(Snow et al, 2005, 32.0) but the dependency pat-
terns perform worse. Both approaches perform well
on Wikipedia data, most likely because of the more
repeated sentence structures and the presence of
many definition sentences. For newspaper data, lex-
ical patterns outperform dependency patterns both
for precision and F?=1. For Wikipedia data the dif-
ferences are smaller and in fact the dependency pat-
2The software used in these experiment has been made avail-
able at http://www.let.rug.nl/erikt/cornetto/D08.zip
178
terns obtain the best F-score. For all data sets, the
dependency patterns suggest more related pairs than
the lexical patterns (column Targets). The differ-
ences between the two pattern types are significant
(p < 0.05) for all evaluation measures for Newspa-
pers and for positive targets and recall for Wikipedia.
4 Result analysis
In this section, we take a closer look at the results de-
scribed in the previous section. We start with look-
ing for an explanation for the differences between
the scores obtained with lexical patterns and depen-
dency patterns. First we examine the results for
Wikipedia data and then the results for newspaper
data. Finally, we perform an error analysis to find
out the strengths and weaknesses of each of the two
methods.
4.1 Wikipedia data
The most important difference between the two pat-
tern types for Wikipedia data is the number of posi-
tive targets (Table 1). Dependency patterns find 23%
more related pairs in the Wikipedia data than lexi-
cal patterns (1580 vs. 1288). This effect can also
be simulated by changing the size of the corpus. If
we restrict the data set of the dependency patterns
to 70% of its current size then the patterns retrieve a
similar number of positive targets as the lexical pat-
terns, 1289, with comparable precision, recall and
F?=1 scores (62.5%, 46.6% and 53.4). So we expect
that the effect of applying the dependency patterns
is the same as applying the lexical patterns to 43%
more data.
4.2 Newspaper data
Performance-wise there seems to be only a small
difference between the two preprocessing methods
when applied to the Wikipedia data set. However,
when we examine the scores obtained on the news-
paper data (Table 1) then we find larger differences.
Dependency patterns remain finding more positive
targets and obtaining a larger recall score but their
precision score is disappointing. However, when we
examine the precision-recall plots of the two meth-
ods (Figure 1, obtained by varying the acceptance
threshold of the machine learner), they are almost
indistinguishable. The performance line for lexical
patterns extends further to the left than the one of
Figure 1: Performance of individual hypernym extraction
patterns applied to the combination of five newspapers
and Wikipedia. Each + in the graphs represent a differ-
ent extraction pattern. The precision-recall graphs for the
machine learner (lines) are identical for each data source
except for the extended part of the performance line for
lexical patterns.
179
lexical patterns applied to Newspapers
Key Phrase Targ. Prec. Recall F?=1
N and other N 376 22.0% 11.4% 15.0
N such as N 222 25.1% 6.7% 10.6
N like N 579 7.6% 17.5% 10.6
N , such as N 263 15.6% 8.0% 10.5
N ( N 323 7.5% 9.8% 8.5
dependency patterns applied to Newspapers
Key Phrase Targ. Prec. Recall F?=1
N and other N 420 21.1% 11.0% 14.5
N be a N 451 8.2% 11.8% 9.7
N like N 205 27.3% 5.4% 9.0
N be N 766 5.7% 20.1% 8.8
N such as N 199 22.4% 5.2% 8.5
lexical patterns applied to Wikipedia
Key Phrase Targ. Prec. Recall F?=1
N be a N 294 40.8% 22.8% 29.3
N be N 418 22.9% 32.5% 26.9
a N be N 185 53.3% 14.4% 22.6
N such as N 161 57.5% 12.5% 20.5
N ( N 188 21.2% 14.6% 17.3
dependency patterns applied to Wikipedia
Key Phrase Targ. Prec. Recall F?=1
N be N 609 33.6% 38.5% 35.9
N be a N 452 44.3% 28.6% 34.8
the N be N 258 34.0% 16.3% 22.1
a N be N 184 44.7% 11.6% 18.5
N N 234 16.6% 14.8% 15.6
Table 2: Best performing extraction patterns according to
F-scores.
the dependency patterns but the remainder of the two
graphs overlap. The measured performances in Ta-
ble 1 are different because the machine learner put
the acceptance level for extracted pairs at different
points of the graph: the performance lines in both
newspaper graphs contain (recall,precision) points
(26.7%,43.1%) and (29.1%,20.7%).
We are unable to find major differences in the re-
sults of the two approaches. We conclude that, apart
from an effect which can be simulated with some
extra data, there is no difference between prepro-
cessing text with shallow methods and with a full
56 ? covered by other patterns
12 48% required full parsing
6 24% lemmatization errors
3 12% omitted for lack of support
3 12% pos tagging errors
1 4% extraction pattern error
81 100%
45 ? covered by other patterns
38 64% parsing errors
10 17% lemmatization errors
7 12% extraction pattern errors
3 5% omitted for lack of support
1 2% pos tagging error
104 100%
Table 3: Primary causes of recall errors made by the lex-
ical pattern N such as N (top) and the best performing
corresponding dependency pattern (bottom).
dependency parser.
4.3 Error analysis
Despite the lack of performance differences between
the two preprocessing methods, there are still inter-
nal differences which cause one method to generate
different related word pairs than the other. We will
now examine in detail two extraction patterns and
specify their distinct effects on the output results.
We hope that by carefully examining their output we
can learn about the strengths and weaknesses of the
two approaches.
We take a closer look at extraction pattern N such
as N for Newspaper data (second best for lexical pat-
terns and fifth best for dependency patterns, see Ta-
ble 2). The lexical pattern found 222 related word
pairs while the dependency pattern discovered 199.
118 of these pairs were found by both patterns which
means that the lexical pattern missed 81 of the pairs
while the dependency pattern missed 104.
An overview of the cause of the recall errors can
be found in Table 3. The two extraction patterns
do not overlap completely. The dependency parser
ignored punctuation signs and therefore the depen-
dency pattern covers both phrases with and without
punctuation. However, these phrase variants result
in different lexical patterns. This is the cause for
56 hypernyms being missed by the lexical pattern.
180
Meanwhile there is a difference between a depen-
dency pattern without the conjunction and and one
with the conjunction, while there is a unified lexi-
cal pattern processing both phrases with and without
conjunctions. This caused the dependency pattern to
miss 45 hypernyms. However, all of these ?missed?
hypernyms are handled by other patterns.
The main cause of the recall differences between
the two extraction patterns was the parser. The de-
pendency pattern found twelve hypernyms which
the lexical pattern missed because they required an
analysis which was beyond part-of-speech tagging
and the basic noun phrase identifier used by the lex-
ical preprocessor. Six hypernyms required extend-
ing a noun phrase with a prepositional phrase, five
needed noun phrase extension with a relative clause
and one involved appositions. An example of such
a phrase is illnesses caused by vitamin deficits, like
scurvy and beriberi.
However, the syntactic information that was avail-
able to the dependency pattern did also have a neg-
ative effect on its recall. 38 of the hypernyms de-
tected by the lexical pattern were missed by the de-
pendency pattern because there was a parsing error
in the relevant phrase. In more than half of the cases,
this involved attaching the phrase starting with such
as at an incorrect position. We found that a phrase
like N1 such as N2 , N3 and N4 could have been split
at any position. We even found some cases of prepo-
sitional phrases and relative clauses incorrectly be-
ing moved from other positions in the sentence into
the target phrase.
Other recall error causes appear less frequently.
The two preprocessing methods used different
lemmatization algorithms which also made different
errors. The effects of this were visible in the errors
made by the two patterns. Some hypernyms that
were found by both patterns but were not present
in both results because of insufficient support from
other patterns (candidate hypernyms should be sup-
ported by at least five different patterns). The ef-
fect of errors in part-of-speech tags was small. Our
data analysis also revealed some inconsistencies in
the extraction patterns which should be examined.
5 Concluding remarks
We have evaluated the effects of two different pre-
processing methods for a natural language process-
ing task: automatically identifying hypernymy in-
formation. The first method used lexical patterns
and relied on shallow processing techniques like
part-of-speech tagging and lemmatization. The sec-
ond method used dependency patterns which re-
lied on additional information obtained from depen-
dency parsing.
In earlier work, McCarthy et al (2007) found
that for word sense disambiguation using the-
sauri generated from dependency relations perform
only slightly better than thesauri generated from
proximity-based relations. Jijkoun et al (2004)
showed that information obtained from dependency
patterns significantly improved the performance of a
question answering system. Li and Roth (2001) re-
port that preprocessing by shallow parsing allows for
a more accurate post-processing of ill-formed sen-
tences than preprocessing with full parsing.
Our study supports the findings of McCarthy et
al. (2007). We found only minor differences in per-
formances between the two preprocessing methods.
The most important difference: about 20% extra
positive cases that were identified by the dependency
patterns applied to Wikipedia data, can be overcome
by increasing the data set of the lexical patterns by
half. We believe that obtaining more data may often
be easier than dealing with the extra computing time
required for parsing the data. For example, in the
course of writing this paper, we had to refrain from
using a recent version of Wikipedia because pars-
ing the data would have taken 296 days on a single
processor machine compared with a single hour for
tagging the data.
References
Eugene Charniak. 1999. A maximum-entropy inspired
parser. Technical Report CS-99-12, Brown University.
Frank Van Eynde. 2005. Part of Speech Tagging en Lem-
matisering van het Corpus Gesproken Nederlands.
K.U. Leuven. (in Dutch).
Alexander Genkin, David D. Lewis, and David Madigan.
2004. Large-Scale Bayesian Logistic Regression for
Text Categorization. Technical report, Rutgers Uni-
versity, New Jersey.
181
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
ACL-92. Newark, Delaware, USA.
Katja Hofmann and Erik Tjong Kim Sang. 2007. Au-
tomatic extension of non-english wordnets. In Pro-
ceedings of SIGIR?07. Amsterdam, The Netherlands
(poster).
Valentin Jijkoun, Maarten de Rijke, and Jori Mur. 2004.
Information extraction for question answering: Im-
proving recall through syntactic patterns. In Proceed-
ings of Coling?04. Geneva, Switzerland.
Xin Li and Dan Roth. 2001. Exploring evidence for shal-
low parsing.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Caroll. 2007. Unsupervised acquisition of predomi-
nant word senses. Computational Linguistics, 33(4).
Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.
2004. Towards terascale knowledge acquisition.
In Proceedings of COLING 2004, pages 771?777.
Geneva, Switzerland.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In NIPS 2005. Vancouver, Canada.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous evi-
dence. In Proceedings of COLING/ACL 2006. Sydney,
Australia.
Asuka Sumida and Kentaro Torisawa. 2008. Hacking
wikipedia for hyponymy relation acquisition. In Pro-
ceedings of IJCNLP 2008. Hyderabad, India.
Mihai Surdeanu, Richard Johansson, Llu??s Ma`rquez,
Adam Meyers, and Joakim Nivre. 2008. The conll-
2008 shared task on joint learning of syntactic and se-
mantic dependencies. In Proceedings of CoNLL-2008.
Manchester, UK.
Erik Tjong Kim Sang and Katja Hofmann. 2007. Au-
tomatic extraction of dutch hypernym-hyponym pairs.
In Proceedings of CLIN-2006. Leuven, Belgium.
Erik Tjong Kim Sang. 2009. To use a treebank or not ?
which is better for hypernym extraction. In Proceed-
ings of the Seventh International Workshop on Tree-
banks and Linguistic Theories (TLT 7). Groningen,
The Netherlands.
Gertjan Van Noord. 2006. At last parsing is now oper-
ational. In Piet Mertens, Cedrick Fairon, Anne Dis-
ter, and Patrick Watrin, editors, TALN06. Verbum Ex
Machina. Actes de la 13e conference sur le traitement
automatique des langues naturelles.
Gertjan Van Noord. 2009. Huge parsed corpora in lassy.
In Proceedings of TLT7. LOT, Groningen, The Nether-
lands.
Piek Vossen. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publisher.
182
