








From Informat ion Retr ieval  to Informat ion Extract ion 
Dav id  M i lward  and  James  Thomas  
SR I  In ternat iona l  
Su i te  23 Mi l lers Yard  
Mil l  Lane  
Cambr idge  
CB2 1RQ 
milward@cam, sri. com, j rt lO03@cam, sri. com 
Abst ract  
This paper describes a system which en- 
ables users to create on-the-fly queries 
which involve not just keywords, but 
also sortal constraints and linguistic on- 
straints. The user can specify how the 
results should be presented e.g. in terms 
of links to documents, or as table en- 
tries. The aim is to bridge the gap be- 
tween keyword based Information Re- 
trieval and pattern based Information 
Extraction. 
1 In t roduct ion  
The amount of information available lectronically 
in a free-text format is ever increasing. People re- 
quire tools that give them the best possible answer 
to their queries even when a full answer may not 
be available. 
Current web Information Retrieval (IR) engines 
standardly retrieve URLs to whole documents, 
and typical user queries are just an unordered 
set of keywords. This is robust and allows un- 
restricted queries, but precision can be poor, and 
the output is not particularly convenient for many 
queries. For example, ff we are interested in e.g. 
companies associated with Mr. Jones, we are likely 
to prefer an output in terms of an alphabetically 
ordered list of companies (with links to the sen- 
tences in which they appear) rather than a list of 
URLs. 
Information Extraction (IE) systems pro- 
vide better presentation of results (e.g. tables, 
database records etc.), and tend to include more 
precise searches which depend upon not just find- 
ing keywords but finding them in particular posi- 
tions or in particular grammatical relationships. 
However, current IE systems typically require 
queries to be programmed beforehand, so are suit- 
able only for cases where the same query is to be 
used time and again e.g. in the processing of a 
newsfeed. 
Customisation ofan IE system involves two ma- 
jor tasks: 
? adapting the information extraction system 
to a new domain, e.g. adding vocabulary, new 
grammatical constructions and domain ontol- 
og:y. 
? creating new patterns for extraction in the 
new domain. 
Customising current IE systems is thus expensive. 
A domain expert and a linguist with knowledge of 
the IE system will generally be required to specify 
the task, manually mark-up or categorise domain 
data, perhaps build a domain ontology, write ex- 
traction patterns and fine-tune them for the best 
compromise of precision and recall. 
There are thus pressing reasons for automating 
the customisation task and reducing the involve- 
ment of linguists as much as possible. Several 
methods of automatically inferring patterns and 
semantic dictionaries from annotated texts have 
been suggested, e.g. (Riloff, 1993; Riloff, 1996; 
Sonderland et al, 1995; Yangarber and Grishman, 
1997). Pierce and Cardie (Cardie and Pierce, 
1998) suggest hat the use of annotated texts can 
be replaced by an interactive approach in which 
an end-user assists the system in bootstrapping 
patterns from a set of example results. However 
even this method is ill suited to one-off queries, 
where the user is unlikely to start from example 
results, but needs to be able to interactively home 
in on a set of answers. 
In this paper we describe a system which en- 
ables users to build advanced on-the-fly queries. 
The interface is intended to be intuitive for users 
already familiar with keyword IR but adds the ex- 
tra functionality of sortal, linguistic and positional 
constraints that are more common in IE. Adding 
extra linguistic onstraints may improve precision, 
and cause a drop in recall, in exactly the same way 
as adding an extra non-optional keyword. 
The approach is less ambitious than provid- 
ing full Natural Language querying, but allows 
85 
professional users of search technology (e.g. an- 
alysts or research scientists) to get most of the 
advantages of IE without having to program pat- 
terns. We assume that retraining of linguistic 
components ( uch as the tokeniser and tagger) for 
a new domain will still be performed by a lin- 
guist/programmer, and that end-users will be sup- 
plied with libraries of useful sorts such as person 
names, companies, locations, protein names etc. 
which are appropriate for their domain. 
2 IR  and  NLP  
Our key interest in this work was to provide a sys- 
tem which allowed users to get answers: not just 
documents or sub-documents. We have not ad- 
dressed the question of whether or not these tech- 
niques would also be useful for more traditional IR 
in the sense of finding the most relevant document 
for a particular query. There is some potential 
since there are extra options to refine or expand 
a query e.g. using sortal constraints such as com- 
pany and location, and restrictive constraints such 
as subject_of or same sentence. Since the linguis- 
tic constraints are under user control the query is 
more likely to be accurate than in systems where 
linguistic constraints are derived from a natural 
language query (though at the expense of usabil- 
ity). 
The system was designed to deal with multiple 
answer queries such as "which protein interacts 
with TAF-2?'.  This differs somewhat from the 
TREC question answering track ((TREC), 2000), 
where the emphasis is on questions which have 
a single answer, and systems attempt o provide 
the most relevant sub-document. To attempt he 
TREC task we would need to extend the sys- 
tem with a relevance weighting mechanism, and 
provide further techniques for query expansion. 
We would then expect the system to do well, 
since (Srihari and Li, 1999) show that the use of 
sortal constraints such as company, location and 
time plus constraints uch as same sentence give 
good results, even with a relatively simple ranking 
mechanism. 
How much extra cost is involved in using lin- 
guistic information? There is obviously some ini- 
tial cost in parsing and analysing the texts, how- 
ever this can largely be hidden by preprocessing 
the documents. In addition there is a space cost 
in having a much larger index: this is necessary 
since we are keeping more information about the 
document's structure. Finally, there is the cost of 
evaluating more complex constraints. The cost of 
using sortal constraints i negligible: we can index 
them in exactly the same way as words. However, 
i l :  P 
i2 :  a 
i3 :  b 
i4 : i1 ( i2 , i3 )  
Figure 1: Distributed representation f P(a, b) 
relational constraints such as same sentence do in- 
troduce extra processing. The figures we present 
at the end of this paper show that this first im- 
plementation of the system is fast enough to be 
usable for some real applications (and very fast 
by Information Extraction standards), but is not 
yet in the same league as standard IR engines. 
3 Highlight 
Highlight (Thomas et al, 2000) is a general- 
purpose IE engine for use in commercial appli- 
cations. The work described in the current pa- 
per extends Highlight by building an interface on 
top of it and replacing the internal representation 
of linguistically analysed texts with a representa- 
tion based on distributed representations c.f. (Mil- 
ward, 2000). Demos of the Highlight System are 
accessible from the SRI Cambridge web site at 
http://www.cam.sri.com 
3.1 Distributed Representation 
The inspiration for the approach taken here was 
the work in (Milward, 2000) on interpretation of 
dialogue utterances. This uses distributed seman- 
tic representations (based on indexing individual 
parts of a semantic structure) to encode both full 
and partial semantic analyses. For example, the 
logical form P(a,b) is represented by the set of 
constraints given in Figure 1. 
This approach aims to combine the advantages 
of shallow pattern-matching approaches (which 
are fast and robust) with those of deeper analysis 
which tends to do better for phenomena which in- 
volve scope. For example, consider the utterance 
"The 2.10 flight to Boston will not stop at New 
York". A simple pattern matching approach look- 
ing for \[flight 1 to \[location/via \[location\] would ex- 
tract the wrong information since it has no notion 
of the scope of the negation. 
In this work we take the same idea, but apply 
it to Information Extraction. Sets of indexed con- 
straints (which are themselves partial descriptions 
of semantic structure) are used directly as search 
expressions. Since the indexed constraints can en- 
code full semantic structures, the search terms can 
be more or less specific: we can start with just con- 
straints on lexical items, and then build up to a 
full semantic structure by adding structural con- 
86 
obligatory 
I:J(K,L), 
J:at, 
M:leave, 
L:T, 
time(T) 
optional 
(I) K>H, H:M(N) 
(2) K:M(N) => departure_time(T) 
Figure 2: Extracting a departure time 
straints. 
Search is now a question of expressing the ap- 
propriate constraints on the information to be ex- 
tracted. For example, Figure 2 is a sketch rule 
to extract departure time from a sentence such 
as "I leave at 3pm." The rule subsumes a range 
of rules which vary in the specificity with which 
they apply. For example, applying only the oblig- 
atory conditions will result in the extraction of 
3pro as departure time from sentences such as "I 
leave Cambridge to arrive at 3pm" since in the 
obligatory component there is no restriction on 
M with respect o T, i.e. the leave event and the 
time. Adding the optional constraints in (1) gives 
us the restriction that the leaving must be domi- 
nated by the preposition at, e.g. "I leave from the 
station at 3pm," and (2) requires immediate domi- 
nance: "I leave at 3pm." This ranges from pattern 
matching to almost full semantics. Any particu- 
lar application of a rule can be scored according to 
the number of optional conditions which are ful- 
filled, thereby using linguistic constraints if they 
are available. 
In practice, the system described here departs 
from this approach in several respects. Firstly, we 
were working with the pre-existing Highlight sys- 
tem which uses shallow syntactic processing based 
on cascaded patterns (similar to (Hobbs et al, 
1996)). Deriving semantic relationships from this 
is not particularly reliable, so it is preferable to use 
search terms which rely more on positional clues. 
We therefore used a distributed syntactic repre- 
sentation to provide more reliable syntactic on- 
straints (e.g. head word of, subject of, same sen- 
tence) augmented by positional constraints (e.g. 
precedes, immediately precedes). Secondly, we 
wanted an intuitive user interface so constraints 
such as 'dominates' were not appropriate. 
3.2 User  In ter face 
Given the underlying representation described 
above, the user's task in building a query is to 
propose a set of constraints which can be matched 
against he representation f a text or set of texts. 
The interface attempts to preserve the conve- 
nience of keyword based IR but also enable more 
refined searches, and control over the presenta- 
tion of results. Keyword based search is a special 
case where the user specifies one or more keywords 
which they want to find in a document. However, 
users can also specify that they want to find a class 
of item (e.g. companies) and refine the search for 
items within the same sentence, not just the same 
document. 
For example, to find 'Esso' and a location in the 
same sentence we need the set of constraints given 
below: 
J : Esso  
location (T) 
same_sentence (J,T) 
The interface emphasises the items the user wants 
to search for. Consider Figure 3. The user is 
looking for two items, the first including the word 
Esso (this could be e.g. "Esso Corp.", "Esso Hold- 
ings" etc.), and the second item of sort 'location'. 
The effect of pressing ~Add' is to include the posi- 
tional constraint that the two items must appear 
in the same sentence. In later parts of this paper 
we provide examples of more sophisticated queries 
which imitate what is currently achieved by pat- 
tern matching in more standard IE systems (e.g. 
Fastus (Hobbs et al, 1996)). 
In our approach IE is a seamless extension of 
IR. This can be contrasted with some more typi- 
cal ways of tying together IE and IR by performing 
an IR search followed by IE. In that approach, l i t  
is used first to select a limited set of documents, 
then IE is applied. This is fine if the queries and 
the keywords are fixed (and appropriate for each 
other). It is not ideal otherwise. For example, 
suppose you are interested in Esso's profits. To 
achieve your query you might use IR to find doc- 
uments containing Esso, then use an IE system 
which has been customised to look for company 
profits. However, some of the results are likely 
to be unexpected, for example, you would obtain 
Shell's profits if there were a document describing 
Shell's profits which just happens to mention Esso 
in passing. 
Items can be constrained to have a particular 
head word, to include a particular word or to be of 
87 
Intelligent Search and Extraction of Information 
Figure 3: User interface 
88 
a particular sort. Multiple constraints on a single 
item are possible, as shown in Figure 5. Positional 
constraints can include any kind of inter-item con- 
straints e.g. precedes, same sentence. There are 
further option buttons which concern the files to 
be queried and the layout of the output. The de- 
fault is to provide a table which includes each item 
found plus the sentence in which it appears in the 
document. 
Two levels of user expertise (novice and expert) 
allow more or less control over the technical details 
of the query. Expert mode (accessed by clicking 
the button at the top right) looks much the same 
but has facilities such as altering the template out- 
put, naming the items and more options on the 
pull-down menus. For instance, in expert mode 
the user may specify which items are optional in 
the query, and what their syntactic lass might be. 
There are also additional extra parameters for the 
expert user for the output templates. 
A typical user query is given in Figure 4. Here 
the user is looking for appositives in the pat- 
tern Person Noun o.f Company, for example John 
Smith, chairman of X-Corp. 1 Note that the query 
is not particularly linguistically sophisticated: the 
Position item is glossed as a noun group and the 
single preposition of is specified when a looser re- 
striction (perhaps to be any preposition) would 
certainly turn up more results. However, this 
query is quick and simple to construct and can 
be used as a diagnostic for a more detailed query. 
A more complex query is shown in Figure 5 for 
a protein interaction task. The sort interaction 
in Figure 5 could be defined as a disjunction of 
the constraints head word = interact, head word 
= bind, head word = associate). 
3.3 One-o f f  Query  vs Pat tern  Base  
Highlight can operate in two distinct modes: in- 
teractive and batch. The interactive mode suits 
one-off queries as seen but can also be used to 
prototype queries on a subset of a corpus. Once 
a user is satisfied with the accuracy of a query, 
this user-defined 'pattern'  can be stored for later 
use. Batch mode is used when a query is to be 
run over a large amount of text and requires no 
intervention from the user other than the initial 
set-up. 
3.4 P reprocess ing  Fi les and  ScalabiHty 
If a user makes two queries over the same set of 
documents it does not make sense to do all the lin- 
guistic processing twice. To avoid this, documents 
1We currently do not index commas but a posi- 
tional constraint representing separation by a comma 
could easily be added. 
can be preprocessed. Preprocessing involves tag- 
ging, chunking, recognition of sorts, and conver- 
sion of the results into a set of constraints. At 
query time, there is no further linguistic process- 
ing to be done, just constraint satisfaction. 
The system has a relatively simple outer loop 
which considers a query for each document in turn 
(rather than e.g. using a single cross-document i -
dex). If a document has not been preprocessed, 
it is processed, then tested against he query. If 
a document has been preprocessed, the results of 
preprocessing are loaded, and tested against the 
query. Preprocessing produces a worthwhile in- 
crease in speed. Loading the preprocessed files 
ready for constraint satisfaction takes (on aver- 
age) less than a tenth of the time it takes to pro- 
cess the files to get to the same stage. However, 
preprocessed files do take around 60 times more 
filespace than the source files from which they are 
derived 2. 
Although loading a preprocessed file is much 
faster than processing from scratch, the loading 
is still a significant factor in the total processing 
time. In fact for simple queries the loading ac- 
counts for over 90% of processing time. We were 
therefore keen to load only those files where there 
was a chance of success for a query. One way to 
do this is to split the query into an IR and an 
IE component and to use IR to pre filter the set 
of documents. If we only have to load one tenth 
of the documents then again we can expect a 10 
times speed up (assuming the time to do IR is 
relatively trivial relative to the time as a whole). 
A simple way to achieve IR filtering is to extract 
out any non-optional keywords from the query, 
and then only process those documents that con- 
tain the keywords. However, many of the queries 
which we use do not contain keywords at all, only 
sorts. In these cases, we cannot run IR over the 
source documents, since these do not contain the 
sortal information. Instead we search over a sum- 
mary file which is created during preprocessing. 
This contains a set of all the sorts and words found 
in the file. The II:t stage consists of selecting just 
those files which match the sorts and keywords 
in the query. This set is then passed to the IE 
component which deals with relational constraints 
such as same sentence, and interactions between 
constraints which have to be calculated on the fly 
such as precedes which are not indexed during pre- 
processing. 
The IrE component satifies the constraints in the 
2This is worse than it need be: we have not yet 
attempted to rationalise the preprocessed files, or use 
encoding schemes to reduce redundancy. 
89 
Intelligent Search and Extraction of Information 
Figure 4: A typical one-off query for case: John Smith, chairman of X-Corp 
90 
Intelligent Search and Extraction of Information l~ ....................................... ! . 
Figure 5: Query for protein interactions e.g. observe that Tar-1 binds TBP. 
91 
query against he constraints in the preprocessed 
file. The constraint solver tries to satisfy the most 
specific constraints in the query first. Constraints 
are indexed and reverse indexed for efficiency. 
The processing times we currently achieve are 
very good by the standards of Information Ex- 
traction, and are adequate for the applications for 
which we have been using the system. The cur- 
rent approach is similar to that of e.g. Molla and 
Hess (Aliod and Hess, 1999), who first partition 
the index space into separate documents, and use 
the IR component of queries as a filter. 
Table 1 shows the difference in processing times 
on two queries for two different datasets. Times 
for processing each query on each dataset are la- 
belled Old for Highlight with no IR  and no prepro- 
cessed files, New for Highlight with preprocessed 
files and NewIR for Highlight with both an IR  
stage and preprocessed files. 3 
New% and NewIR% give New/Old and 
NewIR/Old respectively. From the table we 
can see that (for these queries) adding the pre- 
processed files reduces total processing time by 
around 75%. Adding the IR stage reduces it by 
a further couple of percent in the case of the FT 
files and by 10% for the WSJ  files. The perfor- 
mance increase on FT  data is less dramatic be- 
cause the data provides more hits (i.e. we extract 
more templates per input file) but note that for 
both datasets these improvements are at the worse 
end of the spectrum: both the WSJ and FT  files 
stand a very good chance of containing both per- 
son and company (the sorts in our test queries) 
and so the IR component will propose a large num- 
ber of each set for IE treatment, and we can also 
expect several hits per file, which tends to slow 
query processing. In other queries, e.g. a search 
for a company name, we would expect things to 
be much quicker as borne out by the results in 
Table 2. 
4 Per fo rmance  
Figures 6, 7 correspond to the queries in Figures 3 
and 4 respectively. Figure 8 is the result of search- 
ing for protein interactions (a more general version 
of the query in Figure 5.) 4 Table 2 contrasts these 
results. FilesIR and FilesIE refer to the number 
3The WSJ dataset consists of 100 Wall Street Jour- 
nal articles, the FT set comes from 60 Financial Times 
documents. Query 1 looks for a person and company 
in the same sentence while Query 2 is for Person, X 
of/at/in Company. All tests were carried out on a 
Sun Ultra 2200 (200MHz, 256Mb RAM) r~mning Sic- 
stus Prolog 3.7.1 under Solaris 7. 
4For space reasons we have only presented a sample 
of our query results. 
of files input to each stage of the system, i.e. the 
query in Figure 6 was run over 500 news articles, 
only one of them was selected by our IR  compo- 
nent and was thus input to the IE component. 
Hits denotes how many of the files passed to IE 
actually had at least one template in them and 
Templates hows how many templates were ex- 
tracted as a result of the query. Time is the total 
time for the query in seconds. 
The query in Figure 6 looks for documents con- 
taining Esso and a location, s Because Esso is 
such a good discriminator in this document set, 
appearing in only one out of 500 documents, the 
IE query only considers one document and the 
whole query succeeds in 0.5sec. 
Figure 7 takes longer than this because 218 of 
500 files contain the common sorts person and 
company. Figure 8 is similar but has the addi- 
tional overhead of around 2/3 of all files having 
hits where Figure 7 has only 1/20. 
Factors affect the total amount of t ime that  a 
query takes include: 
? specificity of the query 
? complexity of the query 
? size of the source file 
? whether or not there is a hit in the file 
With the addition of the IR  stage, the specificity 
of the query has the potential to impact greatly on 
total query time as already seen. Having a hit in 
the file can also significantly affect timings since 
there is no chance of an early failure of constraint 
satisfaction. For example, taking a file from each 
of the queries in Figures 7 and 8 which each take 
270msec to load (i.e. are of the same size) we find 
that the one in which there is no hit takes 10msec 
to process but the one is which there is a hit takes 
6.5sec (of which just l l0msec is spent writ.rag re- 
sults to file.) 
The lessons from these results are not partic- 
ularly surprising: queries should use the most 
specific words and sorts possible to get good 
value from the IR  component. If there are many 
solutions--and remember that this system aims 
to extract specific information rather than a rele- 
vant document or document passagc then there 
will be a time penalty. 
5Because there is no sentential restriction on these 
two items, they do not appear on the same row in 
the output. This is an option available to the user 
which means that in queries of this kind, the output 
is a summary of the "true" output (a cross product of 
each item with each other item) which can obviously 
result in very large output tables. 
92 
Dataset 
WSJ 
WSJ 
FT 
FT 
Query 
1 
2 
Old (msec) New (msec) \[ New% NewIR (msec) NewIR 
133880 32890 25 19120 14 
135840 33610 25 19540 14 
94830 21300 22 18850 20 
92240 21590 23 18960 21 
Table 1: Comparison of IR and non-lR preprocessing 
% 
Figure 
6 
7 
8 
FflesIR FilesIE 
500 1 
500 218 
100 59 
Hits 
1 
11 
42 
Templates Time (s) 
19 0.5 
11 27 
44 66 
Table 2: Comparison of query times 
Our expectation is that complex queries which 
also involve a lot of hits are more likely for batch 
mode operation, so the time penalty will not be 
so crucial. One-off queries will tend to be simpler, 
involving searches for less frequent information. 
However, we are also investigating further ways 
to improve processing speed, in particular during 
constraint satisfaction. 
5 Eva luat ion  
We have not yet performed a large-scale valua- 
tion of the user-customisable version of Highlight. 
The earlier version of Highlight was evaluated for 
the task of extracting protein interactions, obtain- 
ing a recall of 55-58% and precision of 77%. For 
this particular task we would not expect he re- 
sults for the new system to differ significantly: the 
underlying engine is the same and we can use the 
interface to generate sets of constraints which are 
equivalent to the previous patterns. For example, 
Figures 5 (above) and 9 show how an old pattern 
can be generated in the new interface. Both look 
for phrases uch as . . .  observed that TAF binds 
TBP, the old pattern places an in teract ion  tag 
around the relevant ext while in the new inter- 
face the user selects which items will be output 
(in this case, the ones which have names rather 
than numbers.) 
For other tasks, there are some factors which 
might affect the relative recall and precision fig- 
ures between the original Highlight system and 
the customisable version. Firstly, the original 
Highlight system can use cascades of pattern 
matching rules to perform 'blocking tactics' i.e. 
one rule fires for the sole purpose of preventing 
another from firing. To achieve a similar ability 
in customisable Highlight we need to introduce a
way of choosing between alternative queries when 
more than one can apply e.g. by always choos- 
ing the most specific. Secondly, the original High- 
light sometimes resorted to external program calls 
which is no longer possible. 
6 Applications 
The system described in this paper is still under 
development but is already being used in pilot 
projects with commercial clients. Areas in which 
it has been deployed include: 
? expertise database: extracting relationships 
between employees and the projects in which 
they are involved from internal company doc- 
uments. The relationships degrade from 
those which represent certain involvement 
(e.g. writing a report) to those that repre- 
sent possible involvement (e.g. mentioned in 
the same sentence). 
? competitor database: extracting information 
about companies from Financial Times doc- 
uments. Here, we were after in particular the 
chairman, chief executive and so on of any 
companies in the texts. Again, sets of rela- 
tionships howed the degree of certainty with 
which a person was related to a company. 
? conference database: extracting information 
such as conference title, location, date and 
cost from conference calls downloaded from 
the web. This task was particularly interest- 
ing because it required the gathering of in- 
formation from a variety of locations in each 
document rather than the more usual single 
sentence. 
In order to test the flexibility of the new system, 
we have also been recoding previous applications 
of the Highlight system. 
93 
Query rcm\]  
I Sentence iUnki 
tThepr0clueers include Shell Canada Ltd., aunit of 
iRoya\] Dutch t Shell Group; Emo Resources Canada i 
ILtd., a unit of Imperial Oil Ltd., which is 71 
l%-owned by Exxon Corp.; and Gulf Canada i 
Resources Ltd., a unit of Olympia & York 
,Developments Ltd. I 
Source 1 I t 
I 1 i 
i- Canada Ltd. i I \[ 
i 
!wsj 0473- 
i - 
I I . . . .  
f 
iwsj_0473!- l ' l l  
, J i ! 
i~s j J )473\ [ -  
i 
J 
I i- 
i l 
i ' i 
i- 
i ! 
i 
1 
i 
t 
l 
I 
I 
I the Arctic 
l 
iCanada 
Houston 
the Mackenzie 
\[River delta 
Alberta 
ithe Prudhoe Bay 
area  
*Alaska 
"Foothills wants to make it clear to other pipeline 
icompanies that it's on first insofar as transporting 
igas from the Arctic to southern markets ," Mr. 
\[Hillary said. 
At least wo rival applications are expected to 
iemerge in coming months, including one from 
TransCCanada PipeLines Ltd., Canada's largest 
\[natural gas pipeline operator. 
? The Toronto-based company, together with 
.Tenneco Inc. of Houston, has had an incomplete 
iproposal filed with Canadian regulators since 1984 
!that it is now updating. 
Like Foothills, TransCanada's Polar Gas 
!consortium plans to build a pipeline directly south 
from the Mackenzie River delta in Canada's 
~?estorn Arctic with an initial capacity to transport 
i I. 2 billion cubic feet of gas daily. 
ilndustry sources aid they expect afierce battle to 
\[emerge b tween TransCanada, which has a 
imonopoly on Canadian gas transportation east of 
!Alberta, and Nova and Westcoast, which control 
Ithe pipelines within and running west of Alberta, 
irespectiv . 
i 
\[link 
ilink 
ilink 
\[link 
link 
link 
E 
i U- S. gas buyers must also decide whether they Want i i 
to enter firm contracts for Mackenzie delta gas or \[ 
develop Alaskan reserves in the Pmdhoe Bay area llink t 
ifn'st, a project hat has been on hold for more than , i 
I a decade- I i 
i But Foothills aid it plans to seek regulatory t 8 \[approval to build an alternativeline, the Alaska ll'mk I 
iNatural Gas Transportation System further north \[ 
itoward Alaska. 
Figure 6: Sample of results from query for keyword Esso and sort location 
94 
Source \[ Person \[ Position 
! 
? \[Robert L r~s3_O111IBe~ 
i 
I \[ 
I 
I i ~,,~ 0209 iMurray 
.... ' -  \[Robinson 
chairman and 
vresident 
Ipresident 
I 
i 
i 
i 
Iforrner president 
executive officer 
Query results 
\[ Company i Sentence il'Ink 
, .4 START Robert U Bernstein,' .....
IR?dom House iof chairman and P resident ?f Rand?m 
House Inc., announced his ilin__X 
i " i resignation from the publishing \[ 
\] \] house he has mn for 23 years. I 
\[ " ) )  { I The devdopmemt could have a \[ 
I i dramatic effect on farm \[ 
! i iproduction, especiailycotton ,)' said i I ! . 
iDdta & Pine !of\[Murray Robinson, president of \[ ...-link \[Land Co. i \[Ddta & Pine Land Co., a 
\[ \[Southwide Inc. subsidiary that is i 
\[ \]one of the largest cotton seed I 
i \[producers in the U.S. I 
i iSTART Michael Blair, former i : 
) ipresident and chief executive ! 
i \[ FEnfield Corp. iof officer of Enfield Corp., failed to i 
' win election to the company's i I~ 
i i " ~ board at a special shareholder I 
! i imee~ng,  i 
I !" David Rockefeller, chairman of " I i 
Rockefeller Group, said the i i 
IRockefeller Group !of company talked with many ilink i 
potential investors In Japan, the 
i i United States and Europe. . ..... i ' 
i . . . . . .  i i E Among the other alumni are 
I i 
i " i t 
\[wsj_0276\[Michael Blair and chief 
L i 
! i 
"d , . jDavl 
!wsJ_0391 {Rockefeller chairman 
: i 
I ! t i ..... .I .............. j
i iwsj 0509 Stephen Wolf \[chairman 
l I ~chairman i . . . . .  !Joseph L and o>~J !Dionne chief executive 
i i ' o~cer  
L .. . . . . . . . .  ! i 
I 
i 
UAL Inc. lof 
i ) 
i McGraw-Hill Inc. !of 
Stephen Wolf, now chairman of 
UAL Inc., and Thomas Haskett, 
president of Pan Am Corp 
| t 
!link , 
I 
t ! 
START Joseph L Dionne, ! 
chairman and chief executive I 
\[officer of McGraw-Hill Inc.,was llink \[ 
\[dected to the board of directors of 
this eleclzonics manufacturer, i t 
Figure 7: Sample of results from query for Person, X of Company 
95 
Query results 
domain Rhe E2F 1 tramactivation domain. ~-~ 
l i i "' lln yeast, Cdilinteracts with 
i t cyelin-deptmdeat kinases, including \[ 
human CDC2 (I16940), CDK2 Ili~k \[bio_1014 iiCdil \[interacts, cyclin-.-dependent kinases i(116953), and CDK3 (123828), but n?t 
I I ........... i .... ~'with CI)K4 (12~29) .  i 
I i \ [~sa~t~a. (~s)d~omt~.~that  ! 
! iPtrf interacts with both TTF1 and Pol I, i , -~ 
- \[ interacts and binds to transcripts containing the I l~J-K bio 101871Ptrf both TTF1 and Pol I 
}t i( \[3-prime end of pre-rRNA in vitro. { 
\[CNK physically interacts with RAF and * 
bio_10261iCNK RAF  ia.pl~ears tolocalize to cell-cell contact 
! 
re~om. i 
bio 10~R iThe SGT itheparvovil'm irecord (603419) The SGT protein i 
. _ .__iprotein imerac~ inol~rucmralprotein !mteractswith eparvovirm ilink 
? , iNS1 inomtmcmral protein NS1. i 
I i ............... Vlnohara et al (1998) demonstrated that i f i " 
bio 10444\[RICK interacts \[CLARP iRICKphysieallyinteraetswith CLARP' i lh~ 
- . - \[a caspase-like molecule known to bind , 
\] iSangoram et al (1998) demomtxated i 
! ? iunR bio_10876 an iin acts Drosophila that human TIM interacts with ~. . .  ? , Drosophila per, mouse PER1, and mouse i i 
i i ~ i PER2 (see 603426) in vitro. ~ ' i I,, \[ i . . . . .  : 
i \[MIZ1 interacts peeificallywith MYC ilink i bio_l1073iMIZl lintel'acts \[MYC and NMYC land NMYC (164840). 
Figure 8: Sample of results from query for X interacts with Y 
bio_relation :sp: 
\[ VG1/ tag(vg ,VGType,Word l ) ,  
tha~/ThatTag ,  
NP1/ tag(np ,N1,N1Sor t ,UL l , Id l ) ,  
VG/ tag(vg ,headed_vg ,Word) ,  
NP2/~ag(np,N2,N2Sort,UL2,Id2) 
\] 
==> 
\[ VG1/tag(vg,VGType,Wordl), 
that/ThatTag, 
\[ NP1/~ag(np,N1,NISort,OLl,Idl), 
VG/tag(vg,headed_vg,Word)\], 
NP2/tag(np,N2,N2Sort,UL2,Id2) 
\]/interaction(Id) 
\] 
if 
bio_transitive_relation(Word), 
observation_vord(Wordl), 
malte_new_id(Id) 
Figure 9: Pattern from basic Highlight for case: observe that Tar-1 binds TBP. 
96 
7 Summary  
This paper has given a brief outline of a sys- 
tem which may be used for one-off queries in 
the style of current Information Retrieval systems 
but with the advantages of Information Extrac- 
tion technology. The same interface can be used 
to build queries" appropriate for more traditional 
batch mode Information Extraction tasks. 
Text Retrieval Conferences (TREC). 2000. 
http: //trec. nist. gov/. 
Roman Yangarber and Ralph Grishman. 1997. 
Customization of Information Extraction sys- 
tems. In Proceedings of the International Work- 
shop on Lexically Driven Information Extrac- 
tion. 
References  
Diego Molla Aliod and Micheal Hess. 1999. On 
the scalability of the answer extraction system 
"extrans". In Applications of Natural Language 
to Information Systems (NLDB'99), pages 219-- 
224. 
Claire Cardie and David Pierce. 1998. Proposal 
for an interactive nvironment for Information 
Extraction. Technical Report TR98-1702, De- 
partment of Computer Science, Cornell Univer- 
sity. 
Jerry R. Hobbs, Douglas Appelt, David Israel 
John Bear, Megumi Kameyama, Mark Stickel, 
and Mabry Tyson. 1996. Fastus: A cascaded 
finite-state transducer for extracting informa- 
tion from natural-language text. In E. Roche 
and Y. Schabes, editors, Finite State Devices 
for Natural Language Processing. MIT Press. 
David Milward. 2000. Distributing representation 
for robust interpretation f dialogue utterances. 
In Proceedings of the 38th A CL Conference. 
Ellen Riloff. 1993. Automatically constructing 
a dictionary for Information Extraction tasks. 
In Proceedings of the 11th National Confer- 
ence on Artificial Intelligence , pages 811-816. 
AAAI/MIT Press. 
Ellen Riloff. 1996. Automatically generating ex- 
traction patterns from untagged text. In Pro- 
ceedings of the 13th National Conference on Ar- 
tificial Intelligence, pages 1044-1049. 
Stephen Sonderland, David Fisher, Jonathan 
Aseltine, and Wendy Lehnert. 1995. CRYS- 
TAL:inducing a conceptual dictionary. In Pro- 
ceedings off the 14th International Joinct Con- 
ference on Artificial Intelligence. 
Rohini Srihari and Wei Li. 1999. Information ex- 
traction supported question answering. In Pro- 
ceedings of the Eighth Text Retrieval Conference 
(TREC-8). 
James Thomas, David Milward, Christos Ouzou- 
his, Stephen Pulman, and Mark Carroll. 2000. 
Automatic extraction of protein interactions 
from scientific abstracts. In Pacific Symposium 
on Biocomputing. 
97 
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 75?79,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Precise Medication Extraction using Agile Text Mining 
Chaitanya Shivade*,  James Cormack?, David Milward? 
*The Ohio State University, Columbus, Ohio, USA 
?Linguamatics Ltd, Cambridge, UK 
shivade@cse.ohio-state.edu, 
{james.cormack,david.milward}@linguamatics.com 
  
Abstract 
Agile text mining is widely used for commercial 
text mining in the pharmaceutical industry. It can 
be applied without building an annotated training 
corpus, so is well-suited to novel or one-off 
extraction tasks. In this work we wanted to see how 
efficiently it could be adapted for healthcare 
extraction tasks such as medication extraction. The 
aim was to identify medication names, associated 
dosage, route of administration, frequency, 
duration and reason, as specified in the 2009 i2b2 
medication challenge.  
 
Queries were constructed based on 696 discharge 
summaries available as training data. Performance 
was measured on a test dataset of 251 unseen 
documents. F1-scores were calculated by 
comparing system annotations against ground truth 
provided for the test data.  
 
Despite the short amount of time spent in adapting 
the system to this task, it achieved high precision 
and reasonable recall (precision of 0.92, recall of 
0.715). It would have ranked fourth in comparison 
to the original challenge participants on the basis of 
its F-score of 0.805 for phrase level horizontal 
evaluation. This shows that agile text mining is an 
effective approach towards information extraction 
that can yield highly accurate results.  
1 Introduction 
Medication information occupies a sizeable 
portion of clinical notes, especially discharge 
summaries. This includes medications on 
admission, during hospital course, and at 
discharge. This information is useful for clinical 
tasks such as inferring adverse drug reactions, 
clinical trial recruitment, etc. The i2b2 Natural 
Language Processing (NLP) challenges 
encourage the development of systems for 
clinical applications, using a shared task, 
publicly available clinical data, and comparison 
of performance with the other participating 
systems, subject to rigid evaluation metrics. The 
2009 challenge (Uzuner, Solti, & Cadag, 2010) 
aimed to extract mentions of medication names, 
associated dosage, route of administration, 
frequency, duration and the reason for 
medication.  
The project used the Linguamatics Interactive 
Information Extraction (I2E) platform. This 
combines NLP,  terminologies and search 
technology to provide a unique ?agile? text 
mining approach (Milward et al., 2005) that can 
yield highly precise results in a small amount of 
time. The approach involves semantic annotation 
and indexing of data followed by interactive 
design of queries that capture typical syntactic 
and semantic features of the desired information. 
While the system uses machine learning 
approaches within its core linguistic processing, 
the final set of queries are essentially 
syntactic/semantic rules identifying specific 
information in the text. 
2 Section Identification 
Although discharge summaries are considered to 
be unstructured data, there are typical 
characteristics associated with them. There is a 
specific flow of information within every 
discharge summary, starting with details of 
patient?s admission, followed by the hospital 
course and ending with discharge instructions. 
Other common sections include chief complaint, 
physical examination, etc. There were more than 
twenty headings to express discharge 
medications in the training data (?Medications on 
discharge,? ?Discharge meds,? etc.). The training 
data was processed to identify section headings 
and multiple forms of the same heading were 
normalized to a single heading. The plain text 
75
was converted into XML with tags representing 
section names. 
3 Offset Information 
To allow evaluation of results in the i2b2 format, 
the text was preprocessed to include line 
numbers and word numbers as further XML 
annotations.  
4 Natural Language Processing 
Indexing documents with I2E uses a standard 
NLP pipeline involving tokenization of the text, 
part-of-speech tagging, and linguistic chunking. 
The output of the pipeline provides useful 
linguistic information, particularly about the 
location of noun phrases and verb phrases, for 
use in entity extraction and querying.   
5 Terminologies 
The I2E platform uses hierarchical terminologies 
to extract entities from the text. These can 
include freely available terminologies such as 
MeSH, and the NCI thesaurus, as well as 
proprietary terminologies such as MedDRA. A 
series of regular expressions allow for the 
indexing of numeric terms (integers, fractions, 
decimal numbers) and measurement units (length, 
time, weight, etc.). In addition, custom 
terminologies can be created for specific tasks by 
combining or merging existing terminologies, or 
by using the system itself to help discover 
terminology from the data. 
6 Querying 
The I2E framework provides an interactive 
querying experience that is similar to a web 
search. While users can enter text queries just as 
one might in an internet search engine,  the query 
interface also allows specification of linguistic 
and non-linguistic units as ?containers? for other 
units. For example, it is possible to search for a 
noun phrase within a sentence and to specify 
words, regular expressions and concepts from 
terminologies. Non-linguistic units can be 
customized to regulate the ordering of items 
within the container, the number of items that 
may occur between two items and whether they 
are constrained by linguistic boundaries, such as 
the sentence.  The output of the query can also be 
customized so as to provide structured 
representation of the query results. 
 As an example, one of the typical ways a 
medication is prescribed follows the construct: 
?Aspirin 625 mg p.o. b.i.d.? This means Aspirin 
with a dosage of 625 milligrams is to be 
consumed orally (p.o.), twice a day (b.i.d.). A 
query to capture this construct can be constructed 
as a non-linguistic phrase, starting with (a) a 
pharmacological substance (a concept from the 
appropriate branch of the NCI-thesaurus), 
followed by (b) a numerical term, (c) a unit for 
measuring weight, (d) a dosage abbreviation and 
finally, (e) an abbreviation for the frequency of 
medication.  
A query containing items only for (a), (b) and (c) 
will give results for all phrases containing a 
pharmacological substance followed by its 
dosage (Aspirin 625 mg, Tylenol 350 mg, etc.). 
The graphical query interface is sufficiently 
flexible to allow many different orderings of 
these constructs and to negate false positive 
results. 
 User defined terminologies can be 
systematically constructed to allow consistent 
matching of lists of terms and to generate concise 
queries. For example, candidates for 
abbreviations corresponding to the route of 
administration were found by constructing a 
query with items for (a), (b), (c) and (e) and an 
empty word container for (d). This gave all 
phrases containing (a), (b), (c), and any word in 
the discharge summary that was followed by (e). 
The results of this query were candidates for 
route of administration. The efficiency of 
querying in I2E provides an opportunity to 
interactively refine parts of the final query and 
discover terms in the training data that might be 
missed by regular expressions and thesauri.   
Queries can also be limited to specific sections 
of the document. The pre-processing step 
described above identified sections in discharge 
summaries of the i2b2 medications challenge 
76
corpus. The queries can thus be limited to only a 
few specific sections such as ?Medications on 
Admission? and ?Medications on Discharge? by 
embedding the query in a section container. The 
challenge specified not to include medications 
mentioned as allergies for a patient. Results 
obtained in the allergies section of discharge 
summaries were therefore ignored using this 
approach. 
7 Post-processing 
I2E?s default output is an HTML table with 
columns corresponding to different containers 
used in the query. Output can also be limited to 
predefined columns of interest. Multiple queries 
are often required to capture different pieces of 
information spread across the corpus. In the i2b2 
challenge, there are multiple fields associated 
with every mention of medication. A single 
structured record corresponding to every mention 
of medication is expected as an output. Spasic et 
al. (2010) view the challenge as a template filling 
task where the participating system is expected 
to fill slots in a template. Thus, the output can be 
configured to be 6 columns representing each of 
the templates. Following their terminology, 
different semantic queries filled different slots of 
the same template. These slots were aggregated 
into a single template using post-processing. 
Multiple issues had to be taken care of in this 
step. Different queries captured parts of the text 
corresponding to the same slot. For example, a 
query aimed at capturing a particular linguistic 
construct may extract frequency as ?daily after 
dinner,? while another query may capture its 
substring ?daily.? In this case, the former 
extraction, which is the longer string, received 
priority as per the challenge specifications. 
Another important problem encountered was that 
of multiple matches for the same field. For 
example, Insulin and Aspart were identified as 
separate pharmacological substances during the 
indexing process. However, ?Insulin aspart? is 
considered as a single medication name as per 
the challenge specifications. Two separate 
templates are thus created. The results of the post 
processing collapse them into one. Certain terms 
from the terminologies did not match the 
definition of a medication, since terminology 
branches are often generic. For example, the 
Chemicals and Drugs branch of MeSH 
constitutes terms such as coffee. Therefore, a list 
of false positives for medication names 
corresponding to these matches was generated 
from the training data.  
8 Experiments 
The i2b2 website offers downloading of the NLP 
dataset for the 2009 challenge after signing a 
Data Usage Agreement. The training data 
consists of 696 discharge summaries. A subset of 
ten documents with gold standard annotations 
has been made available by the organizers. The 
test dataset consists of 251 documents which 
were annotated by the participants under a 
community annotation experiment conducted by 
the organizers (Uzuner, Solti, Xia, et al. 2010). 
These 251 documents and their corresponding 
gold standard annotations are also available. The 
performance was calculated using phrase level 
and token level metrics for horizontal and 
vertical evaluations as defined in (Uzuner, Solti, 
& Cadag, 2010). The phrase level horizontal 
evaluation measures the performance of a system 
across all six fields. This was used as a primary 
metric to rank the results in the challenge. 
Terminology P R F1 
NCI  0.953 0.657 0.777 
MeSH  0.923 0.563 0.699 
NCI + MeSH  0.932 0.688 0.792 
NCI + FDA  0.947 0.678 0.790 
MeSH + FDA  0.921 0.571 0.705 
NCI + MeSH + 
FDA 
0.931 0.698 0.798 
NCI + MeSH + 
FDA + RxNorm  
0.92 0.715 0.805 
Table 1: Comparison of Different Terminologies. 
In order to assess the utility of different 
terminologies, the same set of queries were 
modified by replacing the concept from one with 
the corresponding concept in another. For 
example: Pharmacological substance from NCI 
77
was replaced with Chemicals and Drugs from 
MeSH. This offered an objective way to compare 
the coverage of MeSH and NCI with respect to 
medication names. Coverage of multiple 
terminologies can be leveraged by aggregating 
the results of queries resulting from different 
terminologies. NCI thesaurus, MeSH, a list of 
FDA drug labels, and RxNorm were used. In 
addition a custom terminology was prepared by 
capturing medication names in the training data 
that were missed by the terminologies. The best 
F-score was obtained when query results for all 
sources were aggregated. Addition of sources 
resulted in a drop in precision but increased 
recall. Table 1 summarizes these results, where 
columns P and R denote precision and recall 
respectively.     
9 Results 
Twenty teams representing 23 organizations and 
nine countries participated in the medication 
challenge. The other systems used a variety of 
rule-based, machine-learning and hybrid systems, 
with the most popular being rule-based systems 
(Uzuner et al., 2010). The best ranked system, 
detailed in Patrick & Li (2009), was an example 
of a hybrid system, using both rule-based and 
statistical classifiers.  
No. Group P R F1 
1 USyd  0.896 0.82 0.857 
2 Vanderbilt  0.840 0.803 0.821 
3 Manchester  0.864 0.766 0.812 
*  I2E  0.920 0.715 0.805 
4 NLM  0.784 0.823 0.803 
5 BME - 
Humboldt   
0.841 0.758 0.797 
6 OpenU  0.850 0.748 0.796 
7 UParis  0.799 0.761 780 
8 LIMSI  0.827 0.725 0.773 
9 UofUtah  0.832 0.715 0.769 
10 U 
Wisconsin 
Madison  
0.904 0.661 0.764 
Table 2: Phrase level horizontal evaluation 
Phrase level horizontal evaluation was used as a 
metric to rank the performance of participants in 
the challenge. Table 2 compares the performance 
of I2E with the top ten participants in the 
challenge using this metric. It achieves highly 
precise results as compared to other participants 
of the challenge. The vertical evaluation which 
measures the performance along individual fields 
showed that the system performed poorly on 
duration and reason, in common with other 
systems. As reported by the organizers of the 
challenge (Uzuner et al., 2010), capturing 
duration and reason is a hard task. They report 
that this is primarily due to the variation in 
length and content of these fields in the training 
and testing data. 
10 Conclusion 
Extracting information through interactive design 
of queries can achieve highly precise results in a 
short amount of time. Much of the time in this 
project was spent on pre-processing documents 
to allow the results to conform to the i2b2 format. 
The time taken on query development was of the 
order of a few weeks, including a couple of days 
training in the system at the start of the project. 
This process requires far less specialist 
knowledge of Artificial Intelligence than other 
solutions to this challenge and the easy to use 
interface means refinement is straightforward. 
Clearly, recall still needs to be improved: our 
best system would have been ranked 4th out of 
21 systems in the phrase level horizontal 
evaluation. Examination of the training material 
suggests this is due to gaps in the drug coverage 
provided by the terminologies rather than gaps in 
the query patterns. We will therefore concentrate 
on extending drug coverage in our future work. 
Acknowledgments 
The authors would like to thank Tracy Gregory, 
Himanshu Agarwal and Matthijs Vakar for their 
help in this project. 
References  
Milward, D. et al., 2005. Ontology-based interactive 
information extraction from scientific abstracts. 
78
Comparative and functional genomics, 6(1-2), pp.67?
71. 
Spasic, I. et al., 2010. Medication information 
extraction with linguistic pattern matching and 
semantic rules. Journal of the American Medical 
Informatics Association?: JAMIA, 17(5), pp.532?5. 
Uzuner, O., Solti, I., Xia, F., et al., 2010. Community 
annotation experiment for ground truth generation for 
the i2b2 medication challenge. Journal of the 
American Medical Informatics Association?: JAMIA, 
17(5), pp.519?23. 
Uzuner, O. Solti, I. & Cadag, E., 2010. Extracting 
medication information from clinical text. Journal of 
the American Medical Informatics Association?: 
JAMIA, 17(5), pp.514?8 
Patrick J & Li M. A Cascade Approach to Extracting 
Medication Events. Proceedings of the Third i2b2 
Workshop on Challenges in Natural Language 
Processing for Clinical Data, 2009.  
 
79
