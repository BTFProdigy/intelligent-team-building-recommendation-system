Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 430?438, Prague, June 2007. c?2007 Association for Computational Linguistics
Smooth Bilingual N-gram Translation
Holger Schwenk Marta R. Costa-jussa` and Jose? A.R. Fonollosa
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
schwenk@lismi.fr
UPC - TALP
Barcelona 08034, Spain
{mruiz,adrian}@gps.tsc.upc.edu
Abstract
We address the problem of smoothing trans-
lation probabilities in a bilingual N-gram-
based statistical machine translation system.
It is proposed to project the bilingual tuples
onto a continuous space and to estimate the
translation probabilities in this representa-
tion. A neural network is used to perform the
projection and the probability estimation.
Smoothing probabilities is most important
for tasks with a limited amount of training
material. We consider here the BTEC task
of the 2006 IWSLT evaluation. Improve-
ments in all official automatic measures are
reported when translating from Italian to En-
glish. Using a continuous space model for
the translation model and the target language
model, an improvement of 1.5 BLEU on the
test data is observed.
1 Introduction
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Among all possible target language sen-
tences the one with the highest probability is chosen:
e? = arg max
e
Pr(e|f) = arg max
e
Pr(f |e) Pr(e)
where Pr(f |e) is the translation model and Pr(e)
is the target language model. This approach is
usually referred to as the noisy source-channel ap-
proach in statistical machine translation (Brown et
al., 1993).
During the last few years, the use of context
in SMT systems has provided great improvements
in translation. SMT has evolved from the origi-
nal word-based approach to phrase-based translation
systems (Och et al, 1999; Koehn et al, 2003). A
phrase is defined as a group of source words f? that
should be translated together into a group of target
words e?. The translation model in phrase-based sys-
tems includes the phrase translation probabilities in
both directions, i.e. P (e?|f?) and P (f? |e?).
The use of a maximum entropy approach simpli-
fies the introduction of several additional models ex-
plaining the translation process :
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models and
the ?i weights are typically optimized to maximize
a scoring function on a development set (Och and
Ney, 2002).
The phrase translation probabilities P (e?|f? ) and
P (f? |e?) are usually obtained using relative frequency
estimates. Statistical learning theory, however, tells
us that relative frequency estimates have several
drawbacks, in particular high variance and low bias.
Phrase tables may contain several millions of en-
tries, most of which appear only once or twice,
which means that we are confronted with a data
sparseness problem. Surprisingly, there seems to be
little work addressing the issue of smoothing of the
phrase table probabilities.
On the other hand, smoothing of relative fre-
quency estimates was extensively investigated in the
430
area of language modeling. A systematic compari-
son can be for instance found in (Chen and Good-
man, 1999). Language models and phrase tables
have in common that the probabilities of rare events
may be overestimated. However, in language mod-
eling probability mass must be redistributed in order
to account for the unseen n-grams. Generalization
to unseen events is less important in phrase-based
SMT systems since the system searches only for the
best segmentation and the best matching phrase pair
among the existing ones.
We are only aware of one work that performs a
systematic comparison of smoothing techniques in
phrase-based machine translation systems (Foster et
al., 2006). Two types of phrase-table smoothing
were compared: black-box and glass-box methods.
Black-methods do not look inside phrases but in-
stead treat them as atomic objects. By these means,
all the methods developed for language modeling
can be used. Glass-box methods decompose P (e?|f?)
into a set of lexical distributions P (e|f? ). For in-
stance, it was suggested to use IBM-1 probabili-
ties (Och et al, 2004), or other lexical translation
probabilities (Koehn et al, 2003; Zens and Ney,
2004). Some form of glass-box smoothing is now
used in all state-of-the-art statistical machine trans-
lation systems.
Another approach related to phrase table smooth-
ing is the so-called N-gram translation model
(Marin?o et al, 2006). In this model, bilingual tu-
ples are used instead of the phrase pairs and n-gram
probabilities are considered rather than relative fre-
quencies. Therefore, smoothing is obtained us-
ing the standard techniques developed for language
modeling. In addition, a context dependence of the
phrases is introduced. On the other hand, some
restrictions on the segmentation of the source sen-
tence must be used. N-gram-based translation mod-
els were extensively compared to phrase-based sys-
tems on several tasks and typically achieve compa-
rable performance.
In this paper we propose to investigate improved
smoothing techniques in the framework of the N-
gram translation model. Despite the undeniable suc-
cess of n-graam back-off models, these techniques
have several drawbacks from a theoretical point of
view: the words are represented in a discrete space,
the vocabulary. This prevents ?true interpolation? of
the probabilities of unseen n-grams since a change
in this word space can result in an arbitrary change
of the n-gram probability. An alternative approach
is based on a continuous representation of the words
(Bengio et al, 2003). The basic idea is to convert
the word indices to a continuous representation and
to use a probability estimator operating in this space.
Since the resulting distributions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. Prob-
ability estimation and interpolation in a continuous
space is mathematically well understood and numer-
ous powerful algorithms are available that can per-
form meaningful interpolations even when only a
limited amount of training material is available. This
approach was successfully applied to language mod-
eling in large vocabulary continuous speech recogni-
tion (Schwenk, 2007) and to language modeling in
phrase-based SMT systems (Schwenk et al, 2006).
In this paper, we investigate whether this ap-
proach is useful to smooth the probabilities involved
in the bilingual tuple translation model. Reliable es-
timation of unseen n-grams is very important in this
translation model. Most of the trigram tuples en-
countered in the development or test data were never
seen in the training data. N-gram hit rates are re-
ported in the results section of this paper. We report
experimental results for the BTEC corpus as used
in the 2006 evaluations of the international work-
shop on spoken language translation IWSLT (Paul,
2006). This task provides a very limited amount
of resources in comparison to other tasks like the
translation of journal texts (NIST evaluations) or of
parliament speeches (TC-STAR evaluations). There-
fore, new techniques must be deployed to take the
best advantage of the limited resources. Among the
language pairs tested in this years evaluation, Ital-
ian to English gave the best BLEU results in this
year evaluation. The better the translation quality is,
the more it is challenging to outperform it without
adding more data. We show that a new smoothing
technique for the translation model achieves a sig-
nificant improvement in the BLEU score for a state-
of-the-art statistical translation system.
This paper is organized as follows. In the next
section we first describe the baseline statistical ma-
chine translation systems. Section 3 presents the ar-
chitecture and training algorithms of the continuous
431
space translation model and section 4 summarizes
the experimental evaluation. The paper concludes
with a discussion of future research directions.
2 N-gram-based Translation Model
The N -gram-based translation model has been de-
rived from the finite-state perspective; more specif-
ically, from the work of Casacuberta (2001). How-
ever, different from it, where the translation model
is implemented by using a finite-state transducer,
the N -gram-based system implements a bilingual
N -gram model. It actually constitutes a language
model of bilingual units, referred to as tuples, which
approximates the joint probability between source
and target languages by using N -grams, such as de-
scribed by the following equation:
p(e, f) ?
K
?
k=1
p((e, f)k|(e, f)k?1, . . . , (e, f)k?4)
(2)
where e refers to target, f to source and (e, f)k to
the kth tuple of a given bilingual sentence pair.
Bilingual units (tuples) are extracted from any
word-to-word alignment according to the following
constraints:
? a monotonic segmentation of each bilingual
sentence pairs is produced,
? no word inside the tuple is aligned to words
outside the tuple, and
? no smaller tuples can be extracted without vio-
lating the previous constraints.
As a consequence of these constraints, only one
segmentation is possible for a given sentence pair.
Two important issues regarding this translation
model must be considered. First, it often occurs that
a large number of single-word translation probabil-
ities are left out of the model. This happens for all
words that are always embedded in tuples contain-
ing two or more words, then no translation probabil-
ity for an independent occurrence of these embed-
ded words will exist. To overcome this problem, the
tuple trigram model is enhanced by incorporating
1-gram translation probabilities for all the embed-
ded words detected during the tuple extraction step.
These 1-gram translation probabilities are computed
from the intersection of both the source-to-target and
the target-to-source alignments.
The second issue has to do with the fact that some
words linked to NULL end up producing tuples with
NULL source sides. Since no NULL is actually ex-
pected to occur in translation inputs, this type of tu-
ple is not allowed. Any target word that is linked to
NULL is attached either to the word that precedes
or the word that follows it. To determine this, an ap-
proach based on the IBM1 probabilities was used, as
described in (Marin?o et al, 2006).
2.1 Additional features
The following feature functions were used in the N-
gram-based translation system:
? A target language model. In the baseline sys-
tem, this feature consists of a 4-gram back-off
model of words, which is trained from the tar-
get side of the bilingual corpus.
? A source-to-target lexicon model and a
target-to-source lexicon model. These fea-
ture, which are based on the lexical parameters
of the IBM Model 1, provide a complementary
probability for each tuple in the translation ta-
ble.
? A word bonus function. This feature intro-
duces a bonus based on the number of target
words contained in the partial-translation hy-
pothesis. It is used to compensate for the sys-
tem?s preference for short output sentences.
All these models are combined in the de-
coder. Additionally, the decoder allows for a
non-monotonic search with the following distorsion
model.
? A word distance-based distorsion model.
P (tK1 ) = exp(?
K
?
k=1
dk)
where dk is the distance between the first word
of the kth tuple (unit), and the last word+1 of
the (k ? 1)th tuple.
432
Figure 1: Comparing regular and unfolded tuples.
Distance are measured in words referring to the units
source side.
To reduce the computational cost we place lim-
its on the search using two parameters: the distor-
tion limit (the maximum distance measured in words
that a tuple is allowed to be reordered, m) and the
reordering limit (the maximum number of reorder-
ing jumps in a sentence, j). Tuples need to be ex-
tracted by an unfolding technique (Marin?o et al,
2006). This means that the tuples are broken into
smaller tuples, and these are sequenced in the order
of the target words. In order not to lose the infor-
mation on the correct order, the decoder performs a
non-monotonic search. Figure 1 shows an example
of tuple unfolding compared to the monotonic ex-
traction. The unfolding technique produces a differ-
ent bilingual n-gram language model with reordered
source words.
In order to combine the models in the decoder
suitably, an optimization tool based on the Simplex
algorithm is used to compute log-linear weights for
each model.
3 Continuous Space N-gram Models
The architecture of the neural network n-gram
model is shown in Figure 2. A standard
fully-connected multi-layer perceptron is
used. The inputs to the neural network are
the indices of the n?1 previous units (words
or tuples) in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the poste-
rior probabilities of all units of the vocabulary:
projection
layer hidden
layer
output
layerinput
projections
shared
LM probabilities
for all words
probability estimation
Neural Network
discrete
representation:
indices in wordlist
continuous
representation:
P dimensional vectors
N
wj?1 P
H
N
P (wj =1|hj)
wj?n+1
wj?n+2
P (wj =i|hj)
P (wj =N|hj)
cl
oiM
Vdj
p1 =
pN =
pi =
Figure 2: Architecture of the continuous space LM.
hj denotes the context wj?n+1, . . . , wj?1. P is the
size of one projection and H ,N is the size of the
hidden and output layer respectively. When short-
lists are used the size of the output layer is much
smaller than the size of the vocabulary.
P (wj = i|hj) ?i ? [1,N ] (3)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith unit of
the vocabulary is coded by setting the ith element of
the vector to 1 and all the other elements to 0. The
ith line of the N ?P dimensional projection matrix
corresponds to the continuous representation of the
ith unit. Let us denote cl these projections, dj the
hidden layer activities, oi the outputs, pi their soft-
max normalization, and mjl, bj , vij and ki the hid-
den and output layer weights and the corresponding
biases. Using these notations, the neural network
performs the following operations:
dj = tanh
(
?
l
mjl cl + bj
)
(4)
oi =
?
j
vij dj + ki (5)
pi = eoi /
N
?
r=1
eor (6)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj).
433
Training is performed with the standard back-
propagation algorithm minimizing the following er-
ror function:
E =
N
?
i=1
ti log pi + ?
?
?
?
jl
m2jl +
?
ij
v2ij
?
? (7)
where ti denotes the desired output, i.e., the proba-
bility should be 1.0 for the next unit in the training
sentence and 0.0 for all the other ones. The first part
of this equation is the cross-entropy between the out-
put and the target probability distributions, and the
second part is a regularization term that aims to pre-
vent the neural network from over-fitting the train-
ing data (weight decay). The parameter ? has to be
determined experimentally. Training is done using
a re-sampling algorithm as described in (Schwenk,
2007).
It can be shown that the outputs of a neural net-
work trained in this manner converge to the posterior
probabilities. Therefore, the neural network directly
minimizes the perplexity on the training data. Note
also that the gradient is back-propagated through the
projection-layer, which means that the neural net-
work learns the projection of the units onto the con-
tinuous space that is best for the probability estima-
tion task.
In general, the complexity to calculate one prob-
ability with this basic version of the neural network
n-gram model is dominated by the dimension of the
output layer since the size of the vocabulary (10k
to 64k) is usually much larger than the dimension of
the hidden layer (200 to 500). Therefore, in previous
applications of the continuous space n-gram model,
the output was limited to the s most frequent units, s
ranging between 2k and 12k (Schwenk, 2007). This
is called a short-list.
Sents Words
Train (bitexts) 20k 155.4/166.3k
Dev 489 5.2k
Eval 500 6k
Table 1: Available data in the supplied resources of
the 2006 IWSLT evaluation.
4 Experimental Evaluation
In this work we report results on the Basic Travel-
ing Expression Corpus (BTEC) as used in the 2006
evaluations of the international workshop on spoken
language translation (IWSLT). This corpus consists
of typical sentences from phrase books for tourists in
several languages (Takezawa et al, 2002). We report
results on the supplied development corpus of 489
sentences and the official test set of the IWSLT?06
evaluation. The main measure is the BLEU score,
using seven reference translations. The scoring is
case insensitive and punctuations are ignored. De-
tails on the available data are summarized in Table 1.
We concentrated first on the translation from Ital-
ian to English. All participants in the IWSLT evalua-
tion achieved much better performances for this lan-
guage pair than for the other considered translation
directions. This makes it more difficult to achieve
additional improvements.
A non-monotonic search was performed follow-
ing a local reordering named in Section 2, setting
m = 5 and j = 3. Also we used histogram prun-
ing in the decoder, i.e. the maximum number of hy-
potheses in a stack is limited to 50.
4.1 Language-dependent preprocessing
Italian contracted prepositions have been separated
into preposition + article, such as ?alla???a la?,
?degli???di gli? or ?dallo???da lo?, among others.
4.2 Model training
The training and development data for the bilingual
back-off and neural network translation model were
created as follows. Given the alignment of the train-
ing parallel corpus, we perform a unique segmenta-
tion of each parallel sentence following the criterion
of unfolded segmentation seen in Section 2. This
segmentation is used in a sequence as training text
for building the language model. As an example,
given the alignment and the unfold extraction of Fig-
ure 1, we obtain the following training sentence:
<s> how long#cua?nto does#NULL last#dura
the#el flight#vuelo </s>
The reference bilingual trigram back-off transla-
tion model was trained on these bilingual tuples us-
434
ing the SRI LM toolkit (Stolcke, 2002). Different
smoothing techniques were tried, and best results
were obtained using Good-Turing discounting.
The neural network approach was trained on ex-
actly the same data. A context of two tuples was
used (trigram model). The training corpus contains
about 21,500 different bilingual tuples. We decided
to limit the output of the neural network to the 8k
most frequent tuples (short-list). This covers about
90% of the requested tuple n-grams in the training
data.
Similar to previous applications, the neural net-
work is not used alone but interpolation is performed
to combine several n-gram models. First of all, the
neural network and the reference back-off model are
interpolated together - this always improved perfor-
mance since both seem to be complementary. Sec-
ond, four neural networks with different sizes of the
continuous representation were trained and interpo-
lated together. This usually achieves better general-
ization behavior than training one larger neural net-
work. The interpolation coefficients were calculated
by optimizing perplexity on the development data,
using an EM procedure. The obtained values are
0.33 for the back-off translation model and about
0.16 for each neural network model respectively.
This interpolation is used in all our experiments. For
the sake of simplicity we will still call this the con-
tinuous space translation model.
Each network was trained independently using
early stopping on the development data. Conver-
gence was achieved after about 10 iterations through
the training data (less than 20 minutes of processing
on a standard Linux machine). The other parameters
are as follows:
? Context of two tuples (trigram)
? The dimension of the continuous representation
of the tuples were c =120,140,150 and 200,
? The dimension of the hidden layer was set to
P = 200,
? The initial learning rate was 0.005 with an ex-
ponential decay,
? The weight decay coefficient was set to ? =
0.00005.
N-gram models are usually evaluated using per-
plexity on some development data. In our case, i.e.
using bilingual tuples as basic units (?words?), it is
less obvious if perplexity is a useful measure. Nev-
ertheless, we provide these numbers for complete-
ness. The perplexity on the development data of the
trigram back-off translation model is 227.0. This
could be reduced to 170.4 using the neural network.
It is also very informative to analyze the n-gram
hit-rates of the back-off model on the development
data: 10% of the probability requests are actually a
true trigram, 40% a bigram and about 49% are fi-
nally estimated using unigram probabilities. This
means that only a limited amount of phrase con-
text is used in the standard N-gram-based translation
model. This makes this an ideal candidate to ap-
ply the continuous space model since probabilities
are interpolated for all possible contexts and never
backed-up to shorter contexts.
4.3 Results and analysis
The incorporation of the neural translation model
is done using n-best list. Each hypothesis is com-
posed of a sequence of bilingual tuples and the cor-
responding scores of all the feature functions. Fig-
ure 3 shows an example of such an n-best list. The
neural trigram translation model is used to replace
the scores of the trigram back-off translation model.
This is followed by a re-optimization of the coef-
ficients of all feature functions, i.e. maximization
of the BLEU score on the development data using
the numerical optimization tool CONDOR (Berghen
and Bersini, 2005). An alternative would be to add
a feature function and to combine both translation
models under the log-linear model framework, us-
ing maximum BLEU training.
Another open question is whether it might by
better to already use the continuous space transla-
tion model during decoding. The continuous space
model has a much higher complexity than a back-
off n-gram. However, this can be heavily optimized
when rescoring n-best lists, i.e. by grouping to-
gether all calls in the whole n-best list with the same
context, resulting in only one forward pass through
the neural network. This is more difficult to per-
form when the continuous space translation model
is used during decoding. Therefore, this was not in-
vestigated in this work.
435
spiacente#sorry tutto occupato#it ?s full
spiacente#i ?m sorry tutto occupato#it ?s full
spiacente#i ?m afraid tutto occupato#it ?s full
spiacente#sorry tutto#all occupato#busy
spiacente#sorry tutto#all occupato#taken
Figure 3: Example of sentences in the n-best list of
bilingual tuples. The special character ?#? is used to
separate the source and target sentence words. Sev-
eral words in one tuple a grouped together using ? .?
In all our experiments 1000-best lists were used.
In order to evaluate the quality of these n-best lists,
an oracle trigram back-off translation model was
build on the development data. Rescoring the n-
best lists with this translation model resulted in an
increase of the BLEU score of about 10 points (see
Table 2). While there is an decrease of about 6%
for the position dependent word error rate (mWER),
a smaller change in the position independent word
error rate was observed (mPER). This suggests that
most of the alternative translation hypothesis re-
sult in word reorderings and not in many alternative
word choices. This is one of the major drawbacks
of phrase- and N-gram-based translation systems:
only translations observed in the training data can
be used. There is no generalization to new phrase
pairs.
Back-off Oracle Neural
BLEU 42.34 52.45 43.87
mWER 41.6% 35.6% 40.3%
mPER 31.5% 28.2% 30.7%
Table 2: Comparison of different N-gram-
translation models on the development data.
When the 1000-best lists are rescored with the
neural network translation model the BLEU score
increases by 1.5 points (42.34 to 43.87). Similar im-
provements were observed in the word error rates
(see Table 2). For comparison, a 4-gram back-off
translation model was also built, but no change of
the BLEU score was observed. This suggests that
careful smoothing is more important than increasing
the context when estimating the translation probabil-
ities in an N-gram-based statistical machine transla-
tion system.
In previous work, we have investigated the use of
the neural network approach to modeling the target
language for the IWSLT task (Schwenk et al, 2006).
We also applied this technique to this improved N-
gram-based translation system. In our implemen-
tation, the neural network target 4-gram language
model gives an improvement of 1.3 points BLEU
on the development data (42.34 to 43.66), in com-
parison to 1.5 points for the neural translation model
(see Table 3).
Back-off neural neural neural
TM+LM TM LM TM+LM
BLEU 42.34 43.87 43.66 44.83
Table 3: Combination of a neural translation model
(TM) and a neural language model (LM). BLEU
scores on the development data.
The neural translation and target language model
were also applied to the test data, using of course the
same feature function coefficients as for the devel-
opment data. The results are given in Table 4 for all
the official measures of the IWSLT evaluation. The
new smoothing method of the translation probabili-
ties achieves improvement in all measures. It gives
also an additional gain (again in all measures) when
used together with a neural target language model.
Surprisingly, neural TM and neural LM improve-
ments almost add up: when both techniques are used
together, the BLEU scores increases by 1.5 points
(36.97 ? 38.50). Remember that the reference N-
gram-based translation system already uses a local
reordering approach.
Back-off neural neural neural
TM+LM TM LM TM+LM
BLEU 36.97 37.21 38.04 38.50
mWER 48.10 47.42 47.83 47.61
mPER 38.21 38.07 37.26 37.12
NIST 8.3 8.3 8.6 8.7
Meteor 63.16 63.40 64.70 65.20
Table 4: Test set scores for the combination of a
neural translation model (TM) and a neural language
model (LM).
436
5 Discussion
Phrase-based approaches are the de-facto standard
in statistical machine translation. The phrases are
extracted automatically from the word alignments
of parallel texts, and the different possible transla-
tions of a phrase are weighted using relative fre-
quency. This can be problematic when the data is
sparse. However, there seems to be little work on
possible improvements of the relative frequency es-
timates by some smoothing techniques. It is today
common practice to use additional feature functions
like IBM-1 scores to obtain some kind of smoothing
(Och et al, 2004; Koehn et al, 2003; Zens and Ney,
2004), but better estimation of the phrase probabili-
ties is usually not addressed.
An alternative way to represent phrases is to de-
fine bilingual tuples. Smoothing, and context de-
pendency, is obtained by using an n-gram model on
these tuples. In this work, we have extended this
approach by using a new smoothing technique that
operates on a continuous representation of the tu-
ples. Our method is distinguished by two charac-
teristics: better estimation of the numerous unseen
n-grams, and a discriminative estimation of the tu-
ple probabilities. Results are provided on the BTEC
task of the 2006 IWSLT evaluation for the translation
direction Italian to English. This task provides very
limited amount of resources in comparison to other
tasks. Therefore, new techniques must be deployed
to take the best advantage of the limited resources.
We have chosen the Italian to English task because it
is challenging to enhance a good quality translation
task (over 40 BLEU percentage). Using the continu-
ous space model for the translation and target lan-
guage model, an improvement of 2.5 BLEU on the
development data and 1.5 BLEU on the test data was
observed.
Despite these encouraging results, we believe that
additional research on improved estimation of prob-
abilities in N-gram- or phrase-based statistical ma-
chine translation systems is needed. In particu-
lar, the problem of generalization to new trans-
lations seems to be promising to us. This could
be addressed by the so-called factored phrase-based
model as implemented in the Moses decoder (Koehn
et al, 2007). In this approach words are decom-
posed into several factors. These factors are trans-
lated and a target phrase is generated. This model
could be complemented by a factored continuous
tuple N-gram. Factored word language models
were already successfully used in speech recogni-
tion (Bilmes and Kirchhoff, 2003; Alexandrescu and
Kirchhoff, 2006) and an extension to machine trans-
lation seems to be promising.
The described smoothing method was explicitly
developed to tackle the data sparseness problem in
tasks like the BTEC corpus. It is well known from
language modeling that careful smoothing is less im-
portant when large amounts of data are available.
We plan to investigate whether this also holds for
smoothing of the probabilities in phrase- or tuple-
based statistical machine translation systems.
6 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR (IST-
2002-FP6-506738), by the French Government un-
der the project INSTAR (ANR JCJC06 143038) and
the the Spanish government under a FPU grant and
the project AVIVAVOZ (TEC2006-13964-C03).
References
A. Alexandrescu and K. Kirchhoff. 2006. Factored neu-
ral language models. In HLT-NAACL.
Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3(2):1137?1155.
F. Vanden Berghen and H. Bersini. 2005. CON-
DOR, a new parallel, constrained extension of pow-
ell?s UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of Com-
putational and Applied Mathematics, 181:157?175.
J. A. Bilmes and K. Kirchhoff. 2003. Factored language
models and generalized backoff. In HLT-NAACL.
P. Brown, S. Della Pietra, V. J. Della Pietra, and R: Mer-
cer. 1993. The mathematics of statistical machine
translation. Computational Linguistics, 19(2):263?
311.
F. Casacuberta, D. Llorens, C. Mart??nez, S. Molau,
F. Nevado, H. Ney, M. Pastor, D. Pico?, A. Sanchis,
E. Vidal, and J.M. Vilar. 2001. Speech-to-speech
translation based on finite-state transducers. Interna-
tional Conference on Acoustic, Speech and Signal Pro-
cessing, 1.
437
S. F. Chen and J. T. Goodman. 1999. An empirical study
of smoothing techniques for language modeling. CSL,
13(4):359?394.
G. Foster, R. Kuhn, and H. Johnson. 2006. Phrasetable
smoothing for statistical machine translation. In
EMNLP06, pages 53?61.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrased-based machine translation. In Human Lan-
guage Technology Conference (HLT-NAACL), pages
127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, demonstration session.
J.B. Marin?o, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M. R. Costa-jussa`.
2006. Bilingual n-gram statistical machine transla-
tion. Computational Linguistics, 32(4):527?549, De-
cember.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL, pages 295?302.
F. J. Och, C. Tillmann, and H. Ney. 1999. Improved
alignment models for statistical machine translation.
In Joint SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Copora,
pages 20?28.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In HLT-
NAACL, pages 161?168.
M. Paul. 2006. Overview of the IWSLT 2006 campaign.
In IWSLT, pages 1?15.
H. Schwenk, M. R. Costa-jussa`, and J. A. R. Fonollosa.
2006. Continuous space language models for the iwslt
2006 task. IWSLT, pages 166?173.
H. Schwenk. 2007. Continuous space language models.
Computer Speech and Language, 21:492?518.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In ICSLP, pages II: 901?904.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a borad-coverage bilin-
gual corpus for speech translation of travel conversa-
tions in the real world. In LREC, pages 147?152.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In HLT/NACL,
pages 257?264.
438
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 201?208, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Training Neural Network Language Models
On Very Large Corpora ?
Holger Schwenk and Jean-Luc Gauvain
LIMSI-CNRS
BP 133, 91436 Orsay cedex, FRANCE
schwenk,gauvain@limsi.fr
Abstract
During the last years there has been grow-
ing interest in using neural networks for
language modeling. In contrast to the well
known back-off n-gram language models,
the neural network approach attempts to
overcome the data sparseness problem by
performing the estimation in a continuous
space. This type of language model was
mostly used for tasks for which only a
very limited amount of in-domain training
data is available.
In this paper we present new algorithms to
train a neural network language model on
very large text corpora. This makes pos-
sible the use of the approach in domains
where several hundreds of millions words
of texts are available. The neural network
language model is evaluated in a state-of-
the-art real-time continuous speech recog-
nizer for French Broadcast News. Word
error reductions of 0.5% absolute are re-
ported using only a very limited amount
of additional processing time.
1 Introduction
Language models play an important role in many
applications like character and speech recognition,
machine translation and information retrieval. Sev-
eral approaches have been developed during the last
?This work was partially financed by the European Commis-
sion under the FP6 Integrated Project TC-STAR.
decades like n-gram back-off word models (Katz,
1987), class models (Brown et al, 1992), structured
language models (Chelba and Jelinek, 2000) or max-
imum entropy language models (Rosenfeld, 1996).
To the best of our knowledge word and class n-gram
back-off language models are still the dominant ap-
proach, at least in applications like large vocabulary
continuous speech recognition or statistical machine
translation. In many publications it has been re-
ported that modified Kneser-Ney smoothing (Chen
and Goodman, 1999) achieves the best results. All
the reference back-off language models (LM) de-
scribed in this paper are build with this technique,
using the SRI LM toolkit (Stolcke, 2002).
The field of natural language processing has re-
cently seen some changes by the introduction of new
statistical techniques that are motivated by success-
ful approaches from the machine learning commu-
nity, in particular continuous space LMs using neu-
ral networks (Bengio and Ducharme, 2001; Bengio
et al, 2003; Schwenk and Gauvain, 2002; Schwenk
and Gauvain, 2004; Emami and Jelinek, 2004), Ran-
dom Forest LMs (Xu and Jelinek, 2004) and Ran-
dom cluster LMs (Emami and Jelinek, 2005). Usu-
ally new approaches are first verified on small tasks
using a limited amount of LM training data. For
instance, experiments have been performed using
the Brown corpus (1.1M words), parts of the Wall-
street journal corpus (19M words) or transcriptions
of acoustic training data (up to 22M words). It is
much more challenging to compare the new statis-
tical techniques to carefully optimized back-off LM
trained on large amounts of data (several hundred
millions words). Training may be difficult and very
201
time consuming and the algorithms used with sev-
eral tens of millions examples may be impracticable
for larger amounts. Training back-off LMs on large
amounts of data is not a problem, as long as power-
ful machines with enough memory are available in
order to calculate the word statistics. Practice has
also shown that back-off LMs seem to perform very
well when large amounts of training data are avail-
able and it is not clear that the above mentioned new
approaches are still of benefit in this situation.
In this paper we compare the neural network
language model to n-gram model with modified
Kneser-Ney smoothing using LM training corpora
of up to 600M words. New algorithms are pre-
sented to effectively train the neural network on such
amounts of data and the necessary capacity is ana-
lyzed. The LMs are evaluated in a real-time state-
of-the-art speech recognizer for French Broadcast
News. Word error reductions of up to 0.5% abso-
lute are reported.
2 Architecture of the neural network LM
The basic idea of the neural network LM is to project
the word indices onto a continuous space and to use
a probability estimator operating on this space (Ben-
gio and Ducharme, 2001; Bengio et al, 2003). Since
the resulting probability functions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. A neural
network can be used to simultaneously learn the pro-
jection of the words onto the continuous space and
to estimate the n-gram probabilities. This is still a
n-gram approach, but the LM posterior probabilities
are ?interpolated? for any possible context of length
n-1 instead of backing-off to shorter contexts.
The architecture of the neural network n-gram
LM is shown in Figure 1. A standard fully-
connected multi-layer perceptron is used. The
inputs to the neural network are the indices of
the n?1 previous words in the vocabulary hj =
wj?n+1, ..., wj?2, wj?1 and the outputs are the pos-
terior probabilities of all words of the vocabulary:
P (wj = i|hj) ?i ? [1, N ] (1)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the i-th word
of the vocabulary is coded by setting the i-th ele-
ment of the vector to 1 and all the other elements to
projection
layer hidden
layer
output
layerinput
projections
shared
continuousrepresentation: representation:
indices in wordlist
LM probabilitiesdiscrete for all words
probability estimation
Neural Network
N
wj?1 P
H
N
P (wj=1|hj)
wj?n+1
wj?n+2
P (wj=i|hj)
P (wj=N|hj)
P dimensional vectors
ck
oiM
Vdj
p1 =
pN =
pi =
Figure 1: Architecture of the neural network
language model. hj denotes the context
wj?n+1, ..., wj?1. P is the size of one projec-
tion and H and N is the size of the hidden and
output layer respectively. When shortlists are used
the size of the output layer is much smaller then the
size of the vocabulary.
0. The i-th line of the N ?P dimensional projection
matrix corresponds to the continuous representation
of the i-th word. Let us denote ck these projections,
dj the hidden layer activities, oi the outputs, pi their
softmax normalization, and mjl, bj , vij and ki the
hidden and output layer weights and the correspond-
ing biases. Using these notations the neural network
performs the following operations:
dj = tanh
(
?
l
mjl cl + bj
)
(2)
oi =
?
j
vij dj + ki (3)
pi = e
oi /
N?
k=1
eok (4)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj). Training is
performed with the standard back-propagation algo-
rithm minimizing the following error function:
E =
N?
i=1
ti log pi + ?(
?
jl
m2jl +
?
ij
v2ij) (5)
where ti denotes the desired output, i.e., the proba-
bility should be 1.0 for the next word in the training
202
sentence and 0.0 for all the other ones. The first part
of this equation is the cross-entropy between the out-
put and the target probability distributions, and the
second part is a regularization term that aims to pre-
vent the neural network from overfitting the training
data (weight decay). The parameter ? has to be de-
termined experimentally.
It can be shown that the outputs of a neural net-
work trained in this manner converge to the posterior
probabilities. Therefore, the neural network directly
minimizes the perplexity on the training data. Note
also that the gradient is back-propagated through the
projection-layer, which means that the neural net-
work learns the projection of the words onto the con-
tinuous space that is best for the probability estima-
tion task. The complexity to calculate one probabil-
ity with this basic version of the neural network LM
is quite high:
O = (n? 1)? P ?H + H + H ?N + N (6)
where P is the size of one projection and H and N is
the size of the hidden and output layer respectively.
Usual values are n=4, P=50 to 200, H=400 to 1000
and N=40k to 200k. The complexity is dominated
by the large size of the output layer. In this paper the
improvements described in (Schwenk, 2004) have
been used:
1. Lattice rescoring: speech recognition is done
with a standard back-off LM and a word lattice
is generated. The neural network LM is then
used to rescore the lattice.
2. Shortlists: the neural network is only used to
predict the LM probabilities of a subset of the
whole vocabulary.
3. Regrouping: all LM probabilities needed for
one lattice are collected and sorted. By these
means all LM probability requests with the
same context ht lead to only one forward pass
through the neural network.
4. Block mode: several examples are propagated
at once through the neural network, allowing
the use of faster matrix/matrix operations.
5. CPU optimization: machine specific BLAS
libraries are used for fast matrix and vector op-
erations.
The idea behind shortlists is to use the neural
network only to predict the s most frequent words,
s ? |V |, reducing by these means drastically the
complexity. All words of the word list are still con-
sidered at the input of the neural network. The LM
probabilities of words in the shortlist (P?N ) are cal-
culated by the neural network and the LM probabil-
ities of the remaining words (P?B) are obtained from
a standard 4-gram back-off LM:
P? (wt|ht) =
{
P?N (wt|ht)PS(ht) if wt ? shortlist
P?B(wt|ht) else
(7)
PS(ht) =
?
w?shortlist(ht)
P?B(w|ht) (8)
It can be considered that the neural network redis-
tributes the probability mass of all the words in the
shortlist. This probability mass is precalculated and
stored in the data structures of the back-off LM. A
back-off technique is used if the probability mass for
a requested input context is not directly available.
Normally, the output of a speech recognition sys-
tem is the most likely word sequence given the
acoustic signal, but it is often advantageous to pre-
serve more information for subsequent processing
steps. This is usually done by generating a lattice,
a graph of possible solutions where each arc cor-
responds to a hypothesized word with its acoustic
and language model scores. In the context of this
work LIMSI?s standard large vocabulary continuous
speech recognition decoder is used to generate lat-
tices using a n-gram back-off LM. These lattices are
then processed by a separate tool and all the LM
probabilities on the arcs are replaced by those calcu-
lated by the neural network LM. During this lattice
rescoring LM probabilities with the same context ht
are often requested several times on potentially dif-
ferent nodes in the lattice. Collecting and regrouping
all these calls prevents multiple forward passes since
all LM predictions for the same context are immedi-
ately available at the output.
Further improvements can be obtained by prop-
agating several examples at once though the net-
work, also known as bunch mode (Bilmes et al,
1997; Schwenk, 2004). In comparison to equation 2
and 3, this results in using matrix/matrix instead of
matrix/vector operations which can be aggressively
optimized on current CPU architectures. The Intel
203
Math Kernel Library was used.1 Bunch mode is also
used for training the neural network. Training of a
typical network with a hidden layer with 500 nodes
and a shortlist of length 2000 (about 1M parameters)
take less than one hour for one epoch through four
million examples on a standard PC.
3 Application to Speech Recognition
In this paper the neural network LM is evaluated
in a real-time speech recognizer for French Broad-
cast News. This is a very challenging task since
the incorporation of the neural network LM into
the speech recognizer must be very effective due
to the time constraints. The speech recognizer it-
self runs in 0.95xRT2 and the neural network in less
than 0.05xRT. The compute platform is an Intel Pen-
tium 4 extreme (3.2GHz, 4GB RAM) running Fe-
dora Core 2 with hyper-threading.
The acoustic model uses tied-state position-
dependent triphones trained on about 190 hours of
Broadcast News data. The speech features consist
of 39 cepstral parameters derived from a Mel fre-
quency spectrum estimated on the 0-8kHz band (or
0-3.8kHz for telephone data) every 10ms. These
cepstral coefficients are normalized on a segment
cluster basis using cepstral mean removal and vari-
ance normalization. The feature vectors are linearly
transformed (MLLT) to better fit the diagonal co-
variance Gaussians used for acoustic modeling.
Decoding is performed in two passes. The first
fast pass generates an initial hypothesis, followed
by acoustic model adaptation (CMLLR and MLLR)
and a second decode pass using the adapted mod-
els. Each pass generates a word lattice which is ex-
panded with a 4-gram LM. The best solution is then
extracted using pronunciation probabilities and con-
sensus decoding. Both passes use very tight prun-
ing thresholds, especially for the first pass, and fast
Gaussian computation based on Gaussian short lists.
For the final decoding pass, the acoustic models
include 23k position-dependent triphones with 12k
tied states, obtained using a divisive decision tree
based clustering algorithm with a 35 base phone set.
1http://www.intel.com/software/products/mkl/
2In speech recognition, processing time is measured in mul-
tiples of the length of the speech signal, the real time factor
xRT. For a speech signal of 2h, a processing time of 0.5xRT
corresponds to 1h of calculation.
The system is described in more detail in (Gauvain
et al, 2005).
The neural network LM is used in the last pass
to rescore the lattices. A short-list of length 8192
was used in order to fulfill the constraints on the pro-
cessing time (the complexity of the neural network
to calculate a LM probability is almost linear with
the length of the short-list). This gives a coverage of
about 85% when rescoring the lattices, i.e. the per-
centage of LM requests that are actually performed
by the neural network.
3.1 Language model training data
The following resources have been used for lan-
guage modeling:
? Transcriptions of the acoustic training data
(4.0M words)
? Commercial transcriptions (88.5M words)
? Newspaper texts (508M words)
? WEB data (13.6M words)
First a language model was built for each cor-
pus using modified Kneser-Ney smoothing as imple-
mented in the SRI LM toolkit (Stolcke, 2002). The
individual LMs were then interpolated and merged
together. An EM procedure was used to determine
the coefficients that minimize the perplexity on the
development data. Table 1 summarizes the charac-
teristics of the individual text corpora.
corpus #words Perpl. Coeffs.
Acoustic transcr. 4M 107.4 0.43
Commercial transcr. 88.5M 137.8 0.14
Newspaper texts 508M 103.0 0.35
WEB texts 13.6M 136.7 0.08
All interpolated 614M 70.2 -
Table 1: Characteristics of the text corpora (number
of words, perplexity on the development corpus and
interpolation coefficients)
Although the detailed transcriptions of the audio
data represent only a small fraction of the available
data, they get an interpolation coefficient of 0.43.
This shows clearly that they are the most appropriate
text source for the task. The commercial transcripts,
204
the newspaper and WEB texts reflect less well the
speaking style of broadcast news, but this is to some
extent counterbalanced by the large amount of data.
One could say that these texts are helpful to learn
the general grammar of the language. The word list
includes 65301 words and the OOV rate is 0.95% on
a development set of 158k words.
3.2 Training on in-domain data only
Following the above discussion, it seems natural to
first train a neural network LM on the transcrip-
tions of the acoustic data only. The architecture
of the neural network is as follows: a continuous
word representation of dimension 50, one hidden
layer with 500 neurons and an output layer limited
to the 8192 most frequent words. This results in
3.2M parameters for the continuous representation
of the words and about 4.2M parameters for the sec-
ond part of the neural network that estimates the
probabilities. The network is trained using standard
stochastic back-propagation.3 The learning rate was
set to 0.005 with an exponential decay and the regu-
larization term is weighted with 0.00003. Note that
fast training of neural networks with more than 4M
parameters on 4M examples is already a challenge.
The same fast algorithms as described in (Schwenk,
2004) were used. Apparent convergence is obtained
after about 40 epochs though the training data, each
one taking 2h40 on standard PC equipped with two
Intel Xeon 2.8GHz CPUs.
The neural network LM alone achieves a perplex-
ity of 103.0 which is only a 4% relative reduction
with respect to the back-off LM (107.4, see Table 1).
If this neural network LM is interpolated with the
back-off LM trained on the whole training set the
perplexity decreases from 70.2 to 67.6. Despite this
small improvements in perplexity a notable word er-
ror reduction was obtained from 14.24% to 14.02%,
with the lattice rescoring taking less than 0.05xRT.
In the following sections, it is shown that larger im-
provements can be obtained by training the neural
network on more data.
3.3 Adding selected data
Training the neural network LM with stochastic
back-propagation on all the available text corpora
3The weights are updated after each example.
would take quite a long time. The estimated time
for one training epoch with the 88M words of com-
mercial transcriptions is 58h, and more than 12 days
if all the 508M words of newspaper texts were used.
This is of course not very practicable. One solution
to this problem is to select a subset of the data that
seems to be most useful for the task. This was done
by selecting six month of the commercial transcrip-
tions that minimize the perplexity on the develop-
ment set. This gives a total of 22M words and the
training time is about 14h per epoch.
One can ask if the capacity of the neural network
should be augmented in order to deal with the in-
creased number of examples. Experiments with hid-
den layer sizes from 400 to 1000 neurons have been
performed (see Table 2).
size 400 500 600 1000?
Tr. time 11h20 13h50 16h15 11+16h
Px alone 100.5 100.1 99.5 94.5
interpol. 68.3 68.3 68.2 68.0
Werr 13.99% 13.97% 13.96% 13.92%
? Interpolation of networks with 400 and 600
hidden units.
Table 2: Performance for a neural network LM and
training time per epoch as a function of the size of
the hidden layer (fixed 6 months subset of commer-
cial transcripts).
Although there is a small decrease in perplexity
and word error when increasing the dimension of the
hidden layer, this is at the expense of a higher pro-
cessing time. The training and recognition time are
in fact almost linear to the size of the hidden layer.
An alternative approach to augment the capacity of
the neural network is to modify the dimension of the
continuous representation of the words (in the range
50 to 150). The idea behind this is that the proba-
bility estimation may be easier in a higher dimen-
sional space (instead of augmenting the capacity of
the non-linear probability estimator itself). This is
similar in spirit to the theory behind support vector
machines (Vapnik, 1998).
Increasing the dimension of the projection layer
has several advantages as can be seen from the Fig-
ure 2. First, the perplexity and word error rates
are lower than those obtained when the size of the
205
 90
 95
 100
 105
 110
 115
 120
 0  10  20  30  40  50
Pe
rp
le
xi
ty
Epochs
dim 50
dim 60
dim 70
dim 100
dim 120
dim 150
Figure 2: Perplexity in function of the size of the
continuous word representation (500 hidden units,
fixed 6 months subset of commercial transcripts).
hidden layer is increased. Second, convergence is
faster: the best result is obtained after about 15
epochs while up to 40 are needed with large hidden
layers. Finally, increasing the size of the continu-
ous word representation has only a small effect on
the training and recognition complexity of the neu-
ral network4 since most of the calculation is done
to propagate and learn the connections between the
hidden and the output layer (see equation 6). The
best result was obtained with a 120 dimensional
continuous word representation. The perplexity is
67.9 after interpolation with the back-off LM and
the word error rate is 13.88%.
3.4 Training on all available data
In this section an algorithm is proposed for training
the neural network on arbitrary large training cor-
pora. The basic idea is quite simple: instead of
performing several epochs over the whole training
data, a different small random subset is used at each
epoch. This procedure has several advantages:
? There is no limit on the amount of training data,
? After some epochs, it is likely that all the train-
ing examples have been seen at least once,
? Changing the examples after each epoch adds
noise to the training procedure. This potentially
increases the generalization performance.
This algorithm is summarized in figure 4. The
parameters of this algorithm are the size of the ran-
dom subsets that are used at each epoch. We chose
414h20 for P=120 and H=500.
 80
 85
 90
 95
 100
 105
 110
 115
 120
 0  5  10  15  20  25  30  35  40  45  50
Pe
rp
le
xi
ty
Epochs
6 month fix
1% resampled
5% resampled
10% resampled
20% resampled
Figure 3: Perplexity when resampling different ran-
dom subsets of the commercial transcriptions. (word
representation of dimension 120, 500 hidden units)
to always use the full corpus of transcriptions of the
acoustic data since this is the most appropriate data
for the task. Experiments with different random sub-
sets of the commercial transcriptions and the news-
paper texts have been performed (see Figure 3 and
5). In all cases the same neural network architecture
was used, i.e a 120 dimensional continuous word
representation and 500 hidden units. Some experi-
ments with larger hidden units showed basically the
same convergence behavior. The learning rate was
again set to 0.005, but with a slower exponential de-
cay.
First of all it can be seen from Figure 3 that the
results are better when using random subsets instead
of a fixed selection of 6 months, although each ran-
dom subset is actually smaller (for instance a total of
12.5M examples for a subset of 10%). Best results
were obtained when taking 10% of the commercial
+ Train network for one epoch
Repeat
Select training data:
? Use all acoustic transcriptions (4M words)
? Extract random subset of examples  from the large corpora
? Shuffle data
   (performing weight updates after each example)
+ Test performance on development data
Until convergence
Figure 4: Training algorithm for large corpora
206
Back-off LM Neural Network LM
Training data [#words] 600M 4M 22M 92.5M? 600M?
Training time [h/epoch] - 2h40 14h 9h40 12h 3 ? 12h
Perplexity (NN LM alone) - 103.0 97.5 84.0 80.0 76.5
Perplexity (interpolated LMs) 70.2 67.6 67.9 66.7 66.5 65.9
Word error rate (interpolated LMs) 14.24% 14.02% 13.88% 13.81% 13.75% 13.61%
? By resampling different random parts at the beginning of each epoch.
Table 3: Comparison of the back-off and the neural network LM using different amounts of training data.
The perplexities are given for the neural network LM alone and interpolated with the back-off LM trained
on all the data. The last column corresponds to three interpolated neural network LMs.
transcriptions. The perplexity is 66.7 after interpo-
lation with the back-off LM and the word error rate
is 13.81% (see summary in Table 3). Larger sub-
sets of the commercial transcriptions lead to slower
training, but don?t give better results.
Encouraged by these results, we also included the
508M words of newspaper texts in the training data.
The size of the random subsets were chosen in order
to use between 4 and 9M words of each corpus. Fig-
ure 5 summarizes the results. There seems to be no
obvious benefit from resampling large subsets of the
individual corpora. We choose to resample 10% of
the commercial transcriptions and 1% of the news-
paper texts.
 80
 85
 90
 95
 100
 105
 110
 0  5  10  15  20  25  30  35  40  45  50
Pe
rp
le
xi
ty
Epochs
6 month transcription fix
10% transcriptions
5% transcr + 1% journal
5% transcr + 2% journal
10% transcr + 1% journal
10% transcr + 2% journal
Figure 5: Perplexity when resampling different ran-
dom subsets of the commercial transcriptions and
the newspaper texts.
Table 3 summarizes the results of the different
neural network LMs. It can be clearly seen that the
perplexity of the neural network LM alone decreases
significantly with the amount of training data used.
The perplexity after interpolation with the back-off
LM changes only by a small amount, but there is a
notable improvement in word error rate. This is an-
other experimental evidence that the perplexity of a
LM is not directly related to the word error rate.
The best neural network LM achieves a word er-
ror reduction of 0.5% absolute with respect to the
carefully tuned back-off LM (14.24% ? 13.75%).
The additional processing time needed to rescore the
lattices is less than 0.05xRT. This is a significant im-
provement, in particular for a fast real-time continu-
ous speech recognition system. When more process-
ing time is available a word error rate of 13.61% can
be achieved by interpolating three neural networks
together (in 0.14xRT).
3.5 Using a better speech recognizer
The experimental results have also been validated
using a second speech recognizer running in about
7xRT. This systems differs from the real-time recog-
nizer by a larger 200k word-list, additional acoustic
model adaptation passes and less pruning. Details
are described in (Gauvain et al, 2005). The word er-
ror rate of the reference system using a back-off LM
is 10.74%. This can be reduced to 10.51% using a
neural network LM trained on the fine transcriptions
only and to 10.20% when the neural network LM
is trained on all data using the described resampling
approach. Lattice rescoring takes about 0.2xRT.
4 Conclusions and future work
Neural network language models are becoming a
serious alternative to the widely used back-off lan-
guage models. Consistent improvements in perplex-
ity and word error rate have been reported (Bengio
et al, 2003; Schwenk and Gauvain, 2004; Schwenk
and Gauvain, 2005; Emami and Jelinek, 2004). In
these works, the amount of training data was how-
207
ever limited to a maximum of 20M words due to the
high complexity of the training algorithm.
In this paper new techniques have been described
to train neural network language models on large
amounts of text corpora (up to 600M words). The
evaluation with a state-of-the-art speech recognition
system for French Broadcast News showed a signif-
icant word error reduction of 0.5% absolute. The
neural network LMs is incorporated into the speech
recognizer by rescoring lattices. This is done in less
than 0.05xRT.
Several extensions of the learning algorithm it-
self are promising. We are in particular interested
in smarter ways to select different subsets from the
large corpus at each epoch (instead of a random
choice). One possibility would be to use active
learning, i.e. focusing on examples that are most
useful to decrease the perplexity. One could also
imagine to associate a probability to each training
example and to use these probabilities to weight the
random sampling. These probabilities would be up-
dated after each epoch. This is similar to boosting
techniques (Freund, 1995) which build sequentially
classifiers that focus on examples wrongly classified
by the preceding one.
5 Acknowledgment
The authors would like to thank Yoshua Bengio for
fruitful discussions and helpful comments. The au-
thors would like to recognize the contributions of
G. Adda, M. Adda and L. Lamel for their involve-
ment in the development of the speech recognition
systems on top of which this work is based.
References
Yoshua Bengio and Rejean Ducharme. 2001. A neural
probabilistic language model. In NIPS, volume 13.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3(2):1137?1155.
Jeff Bilmes, Krste Asanovic, Chee whye Chin, and Jim
Demmel. 1997. Using phipac to speed error back-
propagation learning. In ICASSP, pages V:4153?
4156.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?470.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech & Language,
13(4):283?332.
Stanley F. Chen and Joshua T. Goodman. 1999. An
empirical study of smoothing techniques for language
modeling. Computer Speech & Language, 13(4):359?
394.
Ahmad Emami and Frederick Jelinek. 2004. Exact train-
ing of a neural syntactic language model. In ICASSP,
pages I:245?248.
Ahmad Emami and Frederick Jelinek. 2005. Random
clusterings for language modeling. In ICASSP, pages
I:581?584.
Yoav Freund. 1995. Boosting a weak learning al-
gorithm by majority. Information and Computation,
121(2):256?285.
Jean-Luc Gauvain, Gilles Adda, Martine Adda-Decker,
Alexandre Allauzen, Veronique Gendner, Lori Lamel,
and Holger Schwenk. 2005. Where are we in tran-
scribing BN french? In Eurospeech.
Slava M. Katz. 1987. Estimation of probabilities from
sparse data for the language model component of
a speech recognizer. IEEE Transactions on ASSP,
35(3):400?401.
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer
Speech & Language, 10(3):187?228.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In ICASSP, pages I: 765?
768.
Holger Schwenk and Jean-Luc Gauvain. 2004. Neu-
ral network language models for conversational speech
recognition. In ICSLP, pages 1215?1218.
Holger Schwenk and Jean-Luc Gauvain. 2005. Build-
ing continuous space language models for transcribing
european languages. In Eurospeech.
Holger Schwenk. 2004. Efficient training of large neu-
ral networks for language modeling. In IJCNN, pages
3059?3062.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP, pages II: 901?904.
Vladimir Vapnik. 1998. Statistical Learning Theory.
Wiley, New York.
Peng Xu and Frederick Jelinek. 2004. Random forest in
language modeling. In EMNLP, pages 325?332.
208
Large and Diverse Language Models for Statistical Machine Translation
Holger Schwenk?
LIMSI - CNRS
France
schwenk@limsi.fr
Philipp Koehn
School of Informatics
University of Edinburgh
Scotland
pkoehn@inf.ed.ac.uk
Abstract
This paper presents methods to combine
large language models trained from diverse
text sources and applies them to a state-of-
art French?English and Arabic?English ma-
chine translation system. We show gains of
over 2 BLEU points over a strong baseline
by using continuous space language models
in re-ranking.
1 Introduction
Often more data is better data, and so it should come
as no surprise that recently statistical machine trans-
lation (SMT) systems have been improved by the
use of large language models (LM). However, train-
ing data for LMs often comes from diverse sources,
some of them are quite different from the target do-
main of the MT application. Hence, we need to
weight and combine these corpora appropriately. In
addition, the vast amount of training data available
for LM purposes and the desire to use high-order
n-grams quickly exceeds the conventional comput-
ing resources that are typically available. If we are
not able to accommodate large LMs integrated into
the decoder, using them in re-ranking is an option.
In this paper, we present and compare methods to
build LMs from diverse training corpora. We also
show that complex LMs can be used in re-ranking
to improve performance given a strong baseline. In
particular, we use high-order n-grams continuous
space LMs to obtain MT of the well-known NIST
2006 test set that compares very favorably with the
results reported in the official evaluation.
?new address: LIUM, University du Maine, France,
Holger.Schwenk@lium.univ-lemans.fr
2 Related Work
The utility of ever increasingly large LMs for MT
has been recognized in recent years. The effect
of doubling LM size has been powerfully demon-
strated by Google?s submissions to the NIST eval-
uation campaigns. The use of billions of words of
LM training data has become standard in large-scale
SMT systems, and even trillion word LMs have been
demonstrated. Since lookup of LM scores is one of
the fundamental functions in SMT decoding, effi-
cient storage and access of the model becomes in-
creasingly difficult.
A recent trend is to store the LM in a distributed
cluster of machines, which are queried via network
requests (Brants et al, 2007; Emami et al, 2007).
It is easier, however, to use such large LMs in re-
ranking (Zhang et al, 2006). Since the use of clus-
ters of machines is not always practical (or afford-
able) for SMT applications, an alternative strategy
is to find more efficient ways to store the LM in the
working memory of a single machine, for instance
by using efficient prefix trees and fewer bits to store
the LM probability (Federico and Bertoldi, 2006).
Also the use of lossy data structures based on Bloom
filters has been demonstrated to be effective for LMs
(Talbot and Osborne, 2007a; Talbot and Osborne,
2007b). This allows the use of much larger LMs,
but increases the risk of errors.
3 Combination of Language Models
LM training data may be any text in the output
language. Typically, however, we are interested in
building a MT system for a particular domain. If text
resources come from a diversity of domains, some
may be more suitable than others. A common strat-
661
projection
layer hidden
layer
output
layerinput
projections
shared
LM probabilities
for all words
probability estimation
Neural Network
discrete
representation:
indices in wordlist
continuous
representation:
P dimensional vectors
N
wj?1 P
H
N
P (wj =1|hj)
wj?n+1
wj?n+2
P (wj =i|hj)
P (wj =N|hj)
cl
oiM
Vdj
p1 =
pN =
pi =
Figure 1: Architecture of the continuous space LM.
egy is to divide up the LM training texts into smaller
parts, train a LM for each of them and combine these
in the SMT system. Two strategies may be em-
ployed to combine LMs: One is the use of interpola-
tion. LMs are combined into one by weighting each
based on their relevance to the focus domain. The
weighting is carried out by optimizing perplexity of
a representative tuning set that is taken from the do-
main. Standard LM toolkits like SRILM (Stolcke,
2002) provide tools to estimate optimal weights us-
ing the EM algorithm.
The second strategy exploits the log-linear model
that is the basis of modern SMT systems. In this
framework, a linear combination of feature func-
tions is used, which include the log of the LM prob-
ability. It is straight-forward to use multiple LMs in
this framework and treat each as a feature function
in the log-linear model. Combining several LMs in
the log domain corresponds to multiplying the cor-
responding probabilities. Strictly speaking, this sup-
poses an independence assumption that is rarely sat-
isfied in practice. The combination coefficients are
optimized on a criterion directly related to the trans-
lation performance, for instance the BLEU score.
In summary, these strategies differ in two points:
linear versus log-linear combination, and optimizing
perplexity versus optimizing BLEU scores.
4 Continuous Space Language Models
This LM approach is based a continuous represen-
tation of the words (Bengio et al, 2003). The ba-
sic idea is to convert the word indices to a continu-
ous representation and to use a probability estima-
tor operating in this space. Since the resulting dis-
tributions are smooth functions of the word repre-
sentation, better generalization to unknown n-grams
can be expected. This approach was successfully ap-
plied to language modeling in small (Schwenk et al,
2006) an medium-sized phrase-based SMT systems
(De?chelotte et al, 2007).
The architecture of the continuous space language
model (CSLM) is shown in Figure 1. A standard
fully-connected multi-layer perceptron is used. The
inputs to the neural network are the indices of the
n?1 previous words in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the posterior
probabilities of all words of the vocabulary:
P (wj = i|hj) ?i ? [1,N ] (1)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith word of
the vocabulary is coded by setting the ith element of
the vector to 1 and all the other elements to 0. The
ith line of the N ?P dimensional projection matrix
corresponds to the continuous representation of the
ith word.1 Let us denote cl these projections, dj the
hidden layer activities, oi the outputs, pi their soft-
max normalization, and mjl, bj , vij and ki the hid-
den and output layer weights and the corresponding
biases. Using these notations, the neural network
performs the following operations:
dj = tanh
(
?
l
mjl cl + bj
)
(2)
oi =
?
j
vij dj + ki (3)
pi = eoi /
N
?
r=1
eor (4)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj).
Training is performed with the standard back-
propagation algorithm minimizing the following er-
ror function:
E =
N
?
i=1
ti log pi + ?
?
?
?
jl
m2jl +
?
ij
v2ij
?
? (5)
1Typical values are P = 200 . . . 300
662
where ti denotes the desired output. The parame-
ter ? has to be determined experimentally. Train-
ing is done using a resampling algorithm (Schwenk,
2007). It can be shown that the outputs of a neural
network trained in this manner converge to the pos-
terior probabilities. Therefore, the neural network
directly minimizes the perplexity on the training
data. Note also that the gradient is back-propagated
through the projection-layer, which means that the
neural network learns the projection of the words
that is best for the probability estimation task.
In general, the complexity to calculate one prob-
ability is dominated by the output layer dimension
since the size of the vocabulary (here N=273k) is
usually much larger than the dimension of the hid-
den layer (here H=500). Therefore, the CSLM is
only used when the to be predicted word falls into
the 8k most frequent ones. While this substantially
decreases the dimension of the output layer, it still
covers more than 90% of the LM requests. The
other requests are obtained from a standard back-off
LM. Note that the full vocabulary is still used for the
words in the context (input of the neural network).
The incorporation of the CSLM into the SMT
system is done by using n-best lists. In all our
experiments, the LM probabilities provided by the
CSLM are added as an additional feature function.
It is also possible to use only one feature function
for the modeling of the target language (interpola-
tion between the back-off and the CSLM), but this
would need more memory since the huge back-off
LM must be loaded during n-best list rescoring.
We did not try to use the CSLM directly during
decoding since this would result in increased decod-
ing times. Calculating a LM probability with a back-
off model corresponds basically to a table look-up,
while a forward pass through the neural network is
necessary for the CSLM. Very efficient optimiza-
tions are possible, in particular when n-grams with
the same context can be grouped together, but a re-
organization of the decoder may be necessary.
5 Language Models in Decoding and
Re-Ranking
LM lookups are one of the most time-consuming
steps in the decoding process, which makes time-
efficient implementations essential. Consequently,
the LMs have to be held in the working memory of
the machine, since disk lookups are simply too slow.
Filtering LMs to the n-grams which are needed for
the decoding a particular sentence may be an option,
but is useful only to a degree. Since the order of out-
put words is unknown before decoding, all n-grams
that contain any of output words that may be gener-
ated during decoding need to be preserved.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0  20  40  60  80  100  120
ra
tio
 o
f 5
-g
ra
m
s 
re
qu
ire
d 
(ba
g-o
f-w
ord
s)
sentence length
Figure 2: Ratio of 5-grams required to translate one
sentence. The graph plots the ratio against sentence
length. For a 40-word sentence, typically 5% of
the LM is needed (numbers from German?English
model trained on Europarl).
See Figure 2 for an illustration that highlights
what ratio of the LM is needed to translate a sin-
gle sentence. The ratio increases roughly linear
with sentence length. For a typical 30-word sen-
tence, about 4% of the LM 5-grams may be po-
tentially generated during decoding. For large 100-
word sentences, the ratio is about 15%.2 These num-
bers suggest that we may be able to use 5?10 times
larger LMs, if we filter the LM prior to the decod-
ing of each sentence. SMT decoders such as Moses
(Koehn et al, 2007) may store the translation model
in an efficient on-disk data structure (Zens and Ney,
2007), leaving almost the entire working memory
for LM storage. However, this means for 32-bit ma-
chines a limit of 3 GB for the LM.
On the other hand, we can limit the use of very
large LMs to a re-ranking stage. In two-pass de-
2The numbers were obtained using a 5-gram LM trained
on the English side of the Europarl corpus (Koehn, 2005), a
German?English translation model trained on Europarl, and the
WMT 2006 test set (Koehn and Monz, 2006).
663
French English
News Commentary 1.2M 1.0M
Europarl 37.5M 33.8M
Table 1: Combination of a small in-domain (News
Commentary) and large out-of-domain (Europarl)
training corpus (number of words).
coding, the initial decoder produces an n-best list
of translation candidates (say, n=1000), and a sec-
ond pass exploits additional features, for instance
very large LMs. Since the order of English words
is fixed, the number of different n-grams that need
to be looked up is dramatically reduced. However,
since the n-best list is only the tip of the iceberg
of possible translations, we may miss the translation
that we would have found with a LM integrated into
the decoding process.
6 Experiments
In our experiments we are looking for answers to the
open questions on the use of LMs for SMT: Do per-
plexity and BLEU score performance correlate when
interpolating LMs? Should LMs be combined by in-
terpolation or be used as separate feature functions
in the log-linear machine translation model? Is the
use of LMs in re-ranking sufficient to increase ma-
chine translation performance?
6.1 Interpolation
In the WMT 2007 shared task evaluation campaign
(Callison-Burch et al, 2007) domain adaptation was
a special challenge. Two training corpora were pro-
vided: a small in-domain corpus (News Commen-
tary) and the about 30 times bigger out-of-domain
Europarl corpus (see Table 1). One method for do-
main adaptation is to bias the LM towards the in-
domain data. We train two LMs and interpolate them
to optimize performance on in-domain data. In our
experiments, the translation model is first trained on
the combined corpus without weighting. We use the
Moses decoder (Koehn et al, 2007) with default set-
tings. The 5-gram LM was trained using the SRILM
toolkit. We only run minimum error rate training
once, using the in-domain LM. Using different LMs
for tuning may change our findings reported here.
When interpolating the LMs, different weights
 180
 200
 220
 240
 260
 280
 300
 0  0.2  0.4  0.6  0.8  1
 24
 25
 26
 27
 28
 29
Pe
rp
le
xi
ty
BL
EU
 sc
or
e
Interpolation coefficient
Bleu scores
Dev perplexity
Dev
Test
Figure 3: Different weight given to the out-of-
domain data and effect on perplexity of a develop-
ment set (nc-dev2007) and on the BLEU score of the
test set (nc-devtest2007).
TM LM BLEU (test)
combined 2 features 27.30
combined interpolated 0.42 27.23
2 features 2 features 27.64
2 features interpolated 0.42 27.63
Table 2: Combination of the translation models
(TM) by simple concatenation of the training data
vs. use of two feature functions, and combination
of the LM (LM) by interpolation or the use of two
feature functions.
may be given to the out-of-domain versus the in-
domain LM. One way to tune the weight is to opti-
mize perplexity on a development set (nc-dev2007).
We examine values between 0 and 1, the EM proce-
dure gives the lowest perplexity of 193.9 at a value
of 0.42. Does this setting correspond with good
BLEU scores on the development and test set (nc-
devtest2007) ? See Figure 3 for a comparison. The
BLEU score on the development data is 28.55 when
the interpolation coefficient is used that was ob-
tained by optimizing the perplexity. A slightly better
value of 28.78 good be obtained when using an in-
terpolation coefficient of 0.15. The test data seems
to be closer to the out-of-domain Europarl corpus
since the best BLEU scores would be obtained for
smaller values of the interpolation coefficient.
The second question we raised was: Is interpola-
tion of LMs preferable to the use of multiple LMs
664
as separate feature functions. See Table 2 for num-
bers in the same experimental setting for two dif-
ferent comparisons. First, we compare the perfor-
mance of the interpolated LM with the use of two
feature functions. The resulting BLEU scores are
very similar (27.23 vs. 27.30). In a second experi-
ment, we build two translation models, one for each
corpus, and use separate feature functions for them.
This gives a slightly better performance, but again it
gives almost identical results for the use of interpo-
lated LMs vs. two LMs as separate feature functions
(27.63 vs. 27.64).
These experiments suggest that interpolated LMs
give similar performance to the use of multiple LMs.
In terms of memory efficiency, this is good news,
since an interpolated LM uses less memory.
6.2 Re-Ranking
Let us now turn our attention to the use of very large
LMs in decoding and re-ranking. The largest freely
available training sets for MT are the corpora pro-
vided by the LDC for the NIST and GALE evalu-
ation campaigns for Arabic?English and Chinese?
English. In this paper, we concentrate on the first
language pair. Our starting point is a system us-
ing Moses trained on a training corpus of about 200
million words that was made available through the
GALE program. Training such a large system pushes
the limits of the freely available standard tools.
For instance, GIZA++, the standard tool for word
alignment keeps a word translation table in memory.
The only way to get it to process the 200 million
word parallel corpus is to stem all words to their first
five letters (hence reducing vocabulary size). Still,
GIZA++ training takes more than a week of com-
pute time on our 3 GHz machines. Training uses
default settings of Moses. Tuning is carried out us-
ing the 2004 NIST evaluation set. The resulting sys-
tem is competitive with the state of the art. The best
Corpus Words
Parallel training data (train) 216M
AFP part of Gigaword (afp) 390M
Xinhua part of Gigaword (xin) 228M
Full Gigaword (giga) 2,894M
Table 3: Size of the training corpora for LMs in
number of words (including punctuation)
Px Bleu score
Decode LM eval04 eval04 eval06
3-gram train+xin+afp 86.9 50.57 43.69
3-gram train+giga 85.9 50.53 43.99
4-gram train+xin+afp 74.9 50.99 43.90
Reranking with continuous space LM:
5-gram train+xin+afp 62.5 52.88 46.02
6-gram train+xin+afp 60.9 53.25 45.96
7-gram train+xin+afp 60.5 52.95 45.96
Table 4: Improving MT performance with larger
LMs trained on more training data and using higher
order of n-grams (Px denotes perplexity).
performance we obtained is a BLEU score of 46.02
(case insensitive) on the most recent eval06 test set.
This compares favorably to the best score of 42.81
(case sensitive), obtained in 2006 by Google. Case-
sensitive scoring would drop our score by about 2-3
BLEU points.
To assess the utility of re-ranking with large LMs,
we carried out a number of experiments, summa-
rized in Table 4. We used the English side of the par-
allel training corpus and the Gigaword corpus dis-
tributed by the LDC for language modeling. See
Table 3 for the size of these corpora. While this
puts us into the moderate billion word range of large
LMs, it nevertheless stresses our resources to the
limit. The largest LMs that we are able to support
within 3 GB of memory are a 3-gram model trained
on all the data, or a 4-gram model trained only on
train+afp+xin. On disk, these models take up 1.7 GB
compressed (gzip) in the standard ARPA format. All
these LMs are interpolated by optimizing perplexity
on the tuning set (eval04).
The baseline result is a BLEU score of 43.69 us-
ing a 3-gram trained on train+afp+xin. This can be
slightly improved by using either a 3-gram trained
on all data (BLEU score of 43.99) or by using a
4-gram trained on train+afp+xin (BLEU score of
43.90). We were not able to use a 4-gram trained on
all data during the search. Such a model would take
more than 6GB on disk. An option would be to train
the model on all the data and to prune or quantize
it in order to fit in the available memory. This may
give better results than limiting the training data.
Next, we examine if we can get significantly bet-
ter performance using different LMs in re-ranking.
665
To this end, we train continuous space 5-gram to 7-
gram LMs and re-rank a 1000-best list (without du-
plicate translations) provided by the decoder using
the 4-gram LM. The CSLM was trained on the same
data as the back-off LMs. It yields an improvement
in perplexity of about 17% relative.
With various higher order n-grams models, we
obtain significant gains, up to just over 46 BLEU
on the 2006 NIST evaluation set. A gain of over
2 BLEU points underscores the potential for re-
ranking with large LM, even when the baseline LM
was already trained on a large corpus. Note also the
good generalization behavior of this approach : the
gain obtained on the test data matches or exceeds in
most cases the improvements obtained on the devel-
opment data. The CSLM is also very memory ef-
ficient since it uses a distributed representation that
does not increase with the size of training material
used. Overall, about 1GB of main memory is used.
7 Discussion
In this paper we examined a number of issues re-
garding the role of LMs in large-scale SMT sys-
tems. We compared methods to combine training
data from diverse corpora and showed that interpo-
lation of LMs by optimizing perplexity yields simi-
lar results to combining them as feature functions in
the log-linear model.
We applied for the first time continuous space
LMs to the large-scale Arabic?English NIST eval-
uation task. We obtained large improvements (over
2 BLEU points) over a strong baseline, thus validat-
ing both continuous space LMs and re-ranking as a
method to exploit large LMs.
Acknowledgments
This work has been partially funded by the French
Government under the project INSTAR (ANR
JCJC06 143038) and the DARPA Gale program,
Contrat No. HR0011-06-C-0022 and the Euro-
Matrix funded by the European Commission (6th
Framework Programme).
References
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3(2):1137?1155.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In EMNLP, pages 858?867.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Second Work-
shop on SMT, pages 136?158.
Daniel De?chelotte, Holger Schwenk, Hlne Bonneau-
Maynard, Alexandre Allauzen, and Gilles Adda.
2007. A state-of-the-art statistical machine translation
system based on Moses. In MT Summit.
Ahmad Emami, Kishore Papineni, and Jeffrey Sorensen.
2007. Large-scale distributed language modeling. In
ICASSP.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In First Workshop on SMT, pages
94?101.
Philipp Koehn and Christof Monz. 2006. Manual and au-
tomatic evaluation of machine translation between Eu-
ropean languages. In First Workshop on SMT, pages
102?121.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL Demo and
Poster Sessions, pages 177?180, June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit.
Holger Schwenk, Marta R. Costa-jussa`, and Jose? A. R.
Fonollosa. 2006. Continuous space language models
for the IWSLT 2006 task. In IWSLT, pages 166?173.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP, pages II: 901?904.
David Talbot and Miles Osborne. 2007a. Randomised
language modelling for statistical machine translation.
In ACL, pages 512?519.
David Talbot and Miles Osborne. 2007b. Smoothed
Bloom filter language models: Tera-scale LMs on the
cheap. In EMNLP, pages 468?476.
Richard Zens and Hermann Ney. 2007. Efficient phrase-
table representation for machine translation with appli-
cations to online MT and speech translation. In NACL,
pages 492?499.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vogel.
2006. Distributed language modeling for n-best list
re-ranking. In EMNLP, pages 216?223.
666
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 723?730,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Continuous Space Language Models for Statistical Machine Translation
Holger Schwenk and Daniel Dchelotte and Jean-Luc Gauvain
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
{schwenk,dechelot,gauvain}@limsi.fr
Abstract
Statistical machine translation systems are
based on one or more translation mod-
els and a language model of the target
language. While many different trans-
lation models and phrase extraction al-
gorithms have been proposed, a standard
word n-gram back-off language model is
used in most systems.
In this work, we propose to use a new sta-
tistical language model that is based on a
continuous representation of the words in
the vocabulary. A neural network is used
to perform the projection and the proba-
bility estimation. We consider the trans-
lation of European Parliament Speeches.
This task is part of an international evalua-
tion organized by the TC-STAR project in
2006. The proposed method achieves con-
sistent improvements in the BLEU score
on the development and test data.
We also present algorithms to improve the
estimation of the language model proba-
bilities when splitting long sentences into
shorter chunks.
1 Introduction
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source sen-
tence f . Among all possible target sentences the
one with maximal probability is chosen. The clas-
sical Bayes relation is used to introduce a target
language model (Brown et al, 1993):
e? = argmaxe Pr(e|f) = argmaxe Pr(f |e) Pr(e)
where Pr(f |e) is the translation model and Pr(e)
is the target language model. This approach is
usually referred to as the noisy source-channel ap-
proach in statistical machine translation.
Since the introduction of this basic model, many
improvements have been made, but it seems that
research is mainly focused on better translation
and alignment models or phrase extraction algo-
rithms as demonstrated by numerous publications
on these topics. On the other hand, we are aware
of only a small amount of papers investigating
new approaches to language modeling for statis-
tical machine translation. Traditionally, statistical
machine translation systems use a simple 3-gram
back-off language model (LM) during decoding to
generate n-best lists. These n-best lists are then
rescored using a log-linear combination of feature
functions (Och and Ney, 2002):
e? ? argmaxe Pr(e)
?1 Pr(f |e)?2 (1)
where the coefficients ?i are optimized on a devel-
opment set, usually maximizing the BLEU score.
In addition to the standard feature functions, many
others have been proposed, in particular several
ones that aim at improving the modeling of the tar-
get language. In most SMT systems the use of a
4-gram back-off language model usually achieves
improvements in the BLEU score in comparison
to the 3-gram LM used during decoding. It seems
however difficult to improve upon the 4-gram LM.
Many different feature functions were explored in
(Och et al, 2004). In that work, the incorporation
of part-of-speech (POS) information gave only a
small improvement compared to a 3-gram back-
off LM. In another study, a factored LM using
POS information achieved the same results as the
4-gram LM (Kirchhoff and Yang, 2005). Syntax-
based LMs were investigated in (Charniak et al,
723
2003), and reranking of translation hypothesis us-
ing structural properties in (Hasan et al, 2006).
An interesting experiment was reported at the
NIST 2005 MT evaluation workshop (Och, 2005):
starting with a 5-gram LM trained on 75 million
words of Broadcast News data, a gain of about
0.5 point BLEU was observed each time when the
amount of LM training data was doubled, using at
the end 237 billion words of texts. Most of this
additional data was collected by Google on the In-
ternet. We believe that this kind of approach is dif-
ficult to apply to other tasks than Broadcast News
and other target languages than English. There are
many areas where automatic machine translation
could be deployed and for which considerably less
appropriate in-domain training data is available.
We could for instance mention automatic trans-
lation of medical records, translation systems for
tourism related tasks or even any task for which
Broadcast news and Web texts is of limited help.
In this work, we consider the translation of Eu-
ropean Parliament Speeches from Spanish to En-
glish, in the framework of an international evalua-
tion organized by the European TC-STAR project
in February 2006. The training data consists of
about 35M words of aligned texts that are also
used to train the target LM. In our experiments,
adding more than 580M words of Broadcast News
data had no impact on the BLEU score, despite
a notable decrease of the perplexity of the target
LM. Therefore, we suggest to use more complex
statistical LMs that are expected to take better ad-
vantage of the limited amount of appropriate train-
ing data. Promising candidates are random forest
LMs (Xu and Jelinek, 2004), random cluster LMs
(Emami and Jelinek, 2005) and the neural network
LM (Bengio et al, 2003). In this paper, we inves-
tigate whether the latter approach can be used in a
statistical machine translation system.
The basic idea of the neural network LM, also
called continuous space LM, is to project the word
indices onto a continuous space and to use a prob-
ability estimator operating on this space. Since the
resulting probability functions are smooth func-
tions of the word representation, better generaliza-
tion to unknown n-grams can be expected. A neu-
ral network can be used to simultaneously learn
the projection of the words onto the continuous
space and to estimate the n-gram probabilities.
This is still a n-gram approach, but the LM pos-
terior probabilities are ?interpolated? for any pos-
sible context of length n-1 instead of backing-off
to shorter contexts. This approach was success-
fully used in large vocabulary speech recognition
(Schwenk and Gauvain, 2005), and we are inter-
ested here if similar ideas can be applied to statis-
tical machine translation.
This paper is organized as follows. In the next
section we first describe the baseline statistical
machine translation system. Section 3 presents
the architecture of the continuous space LM and
section 4 summarizes the experimental evaluation.
The paper concludes with a discussion of future
research directions.
2 Statistical Translation Engine
A word-based translation engine is used based on
the so-called IBM-4 model (Brown et al, 1993).
A brief description of this model is given below
along with the decoding algorithm.
The search algorithm aims at finding what tar-
get sentence e is most likely to have produced the
observed source sentence f . The translation model
Pr(f |e) is decomposed into four components:
1. a fertility model;
2. a lexical model of the form t(f |e), which
gives the probability that the target word e
translates into the source word f ;
3. a distortion model, that characterizes how
words are reordered when translated;
4. and probabilities to model the insertion of
source words that are not aligned to any tar-
get words.
An A* search was implemented to find the best
translation as predicted by the model, when given
enough time and memory, i.e., provided pruning
did not eliminate it. The decoder manages par-
tial hypotheses, each of which translates a subset
of source words into a sequence of target words.
Expanding a partial hypothesis consists of cover-
ing one extra source position (in random order)
and, by doing so, appending one, several or possi-
bly zero target words to its target word sequence.
For details about the implemented algorithm, the
reader is referred to (De?chelotte et al, 2006).
Decoding uses a 3-gram back-off target lan-
guage model. Equivalent hypotheses are merged,
and only the best scoring one is further expanded.
The decoder generates a lattice representing the
724
we
I
we
should
should
must
remember
remind
remember
that
,
that
that
,
that
you
,
,
,
becausebecause
because
it
I
they
that
can
can
can be
say
be
, because
can
itthey
we
that
can
can
can
be
be
have
be
have
be
have
it
it
has
forgotten
has forgotten
has
has
forgotten
forgotten
been
forgotten
been
forgotten
forgotten
.
.
forgotten
.
.
.
.
.
.
Figure 1: Example of a translation lattice. Source
sentence: ?conviene recordarlo , porque puede
que se haya olvidado .?, Reference 1: ?it is ap-
propriate to remember this , because it may have
been forgotten .? Reference 2: ?it is good to re-
member this , because maybe we forgot it .?
explored search space. Figure 1 shows an example
of such a search space, here heavily pruned for the
sake of clarity.
2.1 Sentence Splitting
The execution complexity of our SMT decoder in-
creases non-linearly with the length of the sen-
tence to be translated. Therefore, the source text
is split into smaller chunks, each one being trans-
lated separately. The chunks are then concatenated
together. Several algorithms have been proposed
in the literature that try to find the best splits, see
for instance (Berger et al, 1996). In this work, we
first split long sentences at punctuation marks, the
remaining segments that still exceed the allowed
length being split linearly. In a second pass, ad-
joining very short chunks are merged together.
During decoding, target LM probabilities of the
type Pr(w1|<s>) and Pr(</s>|wn?1wn) will be
requested at the beginning and at the end of the
hypothesized target sentence respectively.1 This is
correct when a whole sentence is translated, but
leads to wrong LM probabilities when processing
smaller chunks. Therefore, we define a sentence
break symbol, <b>, that is used at the beginning
and at the end of a chunk. During decoding a 3-
gram back-off LM is used that was trained on text
where sentence break symbols have been added.
Each chunk is translated and a lattice is gen-
1The symbols <s> and </s> denote the begin and end of
sentence marker respectively.
erated. The individual lattices are then joined,
omitting the sentence break symbols. Finally, the
resulting lattice is rescored with a LM that was
trained on text without sentence breaks. In that
way we find the best junction of the chunks. Sec-
tion 4.1 provides comparative results of the differ-
ent algorithms to split and join sentences.
2.2 Parameter Tuning
It is nowadays common practice to optimize the
coefficients of the log-linear combination of fea-
ture functions by maximizing the BLEU score on
the development data (Och and Ney, 2002). This
is usually done by first creating n-best lists that
are then reranked using an iterative optimization
algorithm.
In this work, a slightly different procedure was
used that operates directly on the translation lat-
tices. We believe that this is more efficient than
reranking n-best lists since it guarantees that al-
ways all possible hypotheses are considered. The
decoder first generates large lattices using the cur-
rent set of parameters. These lattices are then
processed by a separate tool that extracts the best
path, given the coefficients of six feature functions
(translations, distortion, fertility, spontaneous in-
sertion, target language model probability, and a
sentence length penalty). Then, the BLEU score
of the extracted solution is calculated. This tool is
called in a loop by the public numerical optimiza-
tion tool Condor (Berghen and Bersini, 2005). The
solution vector was usually found after about 100
iterations. In our experiments, only two cycles
of lattice generation and parameter optimization
were necessary (with a very small difference in the
BLEU score).
In all our experiments, the 4-gram back-off and
the neural network LM are used to calculate lan-
guage model probabilities that replace those of the
default 3-gram LM. An alternative would be to de-
fine each LM as a feature function and to combine
them under the log-linear model framework, us-
ing maximum BLEU training. We believe that this
would not make a notable difference in our experi-
ments since we do interpolate the individual LMs,
the coefficients being optimized to minimize per-
plexity on the development data. However, this
raises the interesting question whether the two cri-
teria lead to equivalent performance. The result
section provides some experimental evidence on
this topic.
725
3 Continuous Space Language Models
The architecture of the neural network LM is
shown in Figure 2. A standard fully-connected
multi-layer perceptron is used. The inputs to
the neural network are the indices of the n?1
previous words in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the posterior
probabilities of all words of the vocabulary:
P (wj = i|hj) ?i ? [1, N ] (2)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith word
of the vocabulary is coded by setting the ith ele-
ment of the vector to 1 and all the other elements
to 0. The ith line of the N ? P dimensional pro-
jection matrix corresponds to the continuous rep-
resentation of the ith word. Let us denote cl these
projections, dj the hidden layer activities, oi the
outputs, pi their softmax normalization, and mjl,
bj , vij and ki the hidden and output layer weights
and the corresponding biases. Using these nota-
tions, the neural network performs the following
operations:
dj = tanh
(?
l
mjl cl + bj
)
(3)
oi =
?
j
vij dj + ki (4)
pi = eoi /
N?
r=1
eor (5)
The value of the output neuron pi corresponds di-
rectly to the probability P (wj = i|hj). Training is
performed with the standard back-propagation al-
gorithm minimizing the following error function:
E =
N?
i=1
ti log pi + ?
?
??
jl
m2jl +
?
ij
v2ij
?
? (6)
where ti denotes the desired output, i.e., the prob-
ability should be 1.0 for the next word in the train-
ing sentence and 0.0 for all the other ones. The
first part of this equation is the cross-entropy be-
tween the output and the target probability dis-
tributions, and the second part is a regulariza-
tion term that aims to prevent the neural network
from overfitting the training data (weight decay).
The parameter ? has to be determined experimen-
tally. Training is done using a resampling algo-
rithm (Schwenk and Gauvain, 2005).
projection
layer hidden
layer
output
layerinput
projections
shared
LM probabilities
for all words
probability estimation
Neural Network
discrete
representation:
indices in wordlist
continuous
representation:
P dimensional vectors
N
wj?1 P
H
N
P (wj=1|hj)
wj?n+1
wj?n+2
P (wj=i|hj)
P (wj=N|hj)
cl
oiM
Vdj
p1 =
pN =
pi =
Figure 2: Architecture of the continuous space
LM. hj denotes the context wj?n+1, . . . , wj?1. P
is the size of one projection and H ,N is the size
of the hidden and output layer respectively. When
short-lists are used the size of the output layer is
much smaller then the size of the vocabulary.
It can be shown that the outputs of a neural net-
work trained in this manner converge to the poste-
rior probabilities. Therefore, the neural network
directly minimizes the perplexity on the train-
ing data. Note also that the gradient is back-
propagated through the projection-layer, which
means that the neural network learns the projec-
tion of the words onto the continuous space that is
best for the probability estimation task.
The complexity to calculate one probability
with this basic version of the neural network LM is
quite high due to the large output layer. To speed
up the processing several improvements were used
(Schwenk, 2004):
1. Lattice rescoring: the statistical machine
translation decoder generates a lattice using
a 3-gram back-off LM. The neural network
LM is then used to rescore the lattice.
2. Shortlists: the neural network is only used to
predict the LM probabilities of a subset of the
whole vocabulary.
3. Efficient implementation: collection of all
LM probability requests with the same con-
text ht in one lattice, propagation of several
examples at once through the neural network
and utilization of libraries with CPU opti-
mized matrix-operations.
The idea behind short-lists is to use the neural
726
network only to predict the s most frequent words,
s being much smaller than the size of the vocab-
ulary. All words in the vocabulary are still con-
sidered at the input of the neural network. The
LM probabilities of words in the short-list (P?N )
are calculated by the neural network and the LM
probabilities of the remaining words (P?B) are ob-
tained from a standard 4-gram back-off LM:
P? (wt|ht) =
{
P?N (wt|ht)PS(ht) if wt ? short-list
P?B(wt|ht) else
(7)
PS(ht) =
?
w?short?list(ht)
P?B(w|ht) (8)
It can be considered that the neural network redis-
tributes the probability mass of all the words in the
short-list. This probability mass is precalculated
and stored in the data structures of the back-off
LM. A back-off technique is used if the probability
mass for a input context is not directly available.
It was not envisaged to use the neural network
LM directly during decoding. First, this would
probably lead to slow translation times due to the
higher complexity of the proposed LM. Second, it
is quite difficult to incorporate n-gram language
models into decoding, for n>3. Finally, we be-
lieve that the lattice framework can give the same
performances than direct decoding, under the con-
dition that the alternative hypotheses in the lattices
are rich enough. Estimates of the lattice oracle
BLEU score are given in the result section.
4 Experimental Evaluation
The experimental results provided here were ob-
tained in the framework of an international evalua-
tion organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
long-term effort to advance research in all core
technologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the min-
utes edited by the European Parliament in sev-
eral languages, also known as the Final Text Edi-
tions (Gollan et al, 2005). These texts were
aligned at the sentence level and they are used
to train the statistical translation models (see Ta-
ble 1 for some statistics). In addition, about 100h
of Parliament plenary sessions were recorded and
transcribed. This data is mainly used to train
2http://www.tc-star.org/
Spanish English
Sentence Pairs 1.2M
Total # Words 37.7M 33.8M
Vocabulary size 129k 74k
Table 1: Statistics of the parallel texts used to train
the statistical machine translation system.
the speech recognizers, but the transcriptions were
also used for the target LM of the translation sys-
tem (about 740k words).
Three different conditions are considered in
the TC-STAR evaluation: translation of the Fi-
nal Text Edition (text), translation of the tran-
scriptions of the acoustic development data (ver-
batim) and translation of speech recognizer output
(ASR). Here we only consider the verbatim condi-
tion, translating from Spanish to English. For this
task, the development data consists of 792 sen-
tences (25k words) and the evaluation data of 1597
sentences (61k words). Parts of the test data ori-
gins from the Spanish parliament which results in
a (small) mismatch between the development and
test data. Two reference translations are provided.
The scoring is case sensitive and includes punctu-
ation symbols.
The translation model was trained on 1.2M sen-
tences of parallel text using the Giza++ tool. All
back-off LMs were built using modified Kneser-
Ney smoothing and the SRI LM-toolkit (Stolcke,
2002). Separate LMs were first trained on the
English EPPS texts (33.8M words) and the tran-
scriptions of the acoustic training material (740k
words) respectively. These two LMs were then in-
terpolated together. Interpolation usually results in
lower perplexities than training directly one LM
on the pooled data, in particular if the corpora
come from different sources. An EM procedure
was used to find the interpolation coefficients that
minimize the perplexity on the development data.
The optimal coefficients are 0.78 for the Final Text
edition and 0.22 for the transcriptions.
4.1 Performance of the sentence splitting
algorithm
In this section, we first analyze the performance of
the sentence split algorithm. Table 2 compares the
results for different ways to translate the individ-
ual chunks (using a standard 3-gram LM versus
an LM trained on texts with sentence breaks in-
serted), and to extracted the global solution (con-
727
LM used Concatenate Lattice
during decoding 1-best join
Without
sentence breaks 40.20 41.63
With
sentence breaks 41.45 42.35
Table 2: BLEU scores for different ways to trans-
late sentence chunks and to extract the global so-
lution (see text for details).
catenating the 1-best solutions versus joining the
lattices followed by LM rescoring). It can be
clearly seen that joining the lattices and recalculat-
ing the LM probabilities gives better results than
just concatenating the 1-best solutions of the in-
dividual chunks (first line: BLEU score of 41.63
compared to 40.20). Using a LM trained on texts
with sentence breaks during decoding gives an ad-
ditional improvement of about 0.7 points BLEU
(42.35 compared to 41.63).
In our current implementation, the selection of
the sentence splits is based on punctuation marks
in the source text, but our procedure is compatible
with other methods. We just need to apply the sen-
tence splitting algorithm on the training data used
to build the LM during decoding.
4.2 Using the continuous space language
model
The continuous space language model was trained
on exactly the same data than the back-off refer-
ence language model, using the resampling algo-
rithm described in (Schwenk and Gauvain, 2005).
In this work, we use only 4-gram LMs, but the
complexity of the neural network LM increases
only slightly with the order of the LM. For each
experiment, the parameters of the log-linear com-
bination were optimized on the development data.
Perplexity on the development data set is a pop-
ular and easy to calculate measure to evaluate the
quality of a language model. However, it is not
clear if perplexity is a good criterion to predict
the improvements when the language model will
be used in a SMT system. For information, and
comparison with the back-off LM, Figure 3 shows
the perplexities for different configurations of the
continuous space LM. The perplexity clearly de-
creases with increasing size of the short-list and a
value of 8192 was used. In this case, 99% of the
requested LM probabilities are calculated by the
neural network when rescoring a lattice.
 72
 73
 74
 75
 76
 77
 78
 79
 80
 81
 82
 0  5  10  15  20  25  30  35
Pe
rp
le
xit
y
Number of training epochs
4-gram back-off LM
short-list of 2k
short-list of 4k
short-list of 8k
Figure 3: Perplexity of different configurations of
the continuous space LM.
Although the neural network LM could be used
alone, better results are obtained when interpolat-
ing it with the 4-gram back-off LM. It has even
turned out that it was advantageous to train several
neural network LMs with different context sizes3
and to interpolate them altogether. In that way,
a perplexity decrease from 79.6 to 65.0 was ob-
tained. For the sake of simplicity we will still call
this interpolation the neural network LM.
Back-off LM Neural LM
3-gram 4-gram 4-gram
Perplexity 85.5 79.6 65.0
Dev data:
BLEU 42.35 43.36 44.42
WER 45.9% 45.1% 44.4%
PER 31.8% 31.3% 30.8%
Eval data:
BLEU 39.77 40.62 41.45
WER 48.2% 47.4% 46.7%
PER 33.6% 33.1% 32.8%
Table 3: Result comparison for the different LMs.
BLEU uses 2 reference translations. WER=word
error rate, PER=position independent WER.
Table 3 summarizes the results on the devel-
opment and evaluation data. The coefficients of
the feature functions are always those optimized
on the development data. The joined translation
lattices were rescored with a 4-gram back-off and
the neural network LM. Using a 4-gram back-
off LM gives an improvement of 1 point BLEU
3The values are in the range 150. . .400. The other param-
eters are: H=500, ?=0.00003 and the initial learning rate was
0.005 with an exponential decay. The networks were trained
for 20 epochs through the training data.
728
Spanish: es el nico premio Sajarov que no ha podido recibir su premio despus de ms de tres
mil quinientos das de cautiverio .
Backoff LM: it is only the Sakharov Prize has not been able to receive the prize after three thousand
, five days of detention .
CSLM : it is the only Sakharov Prize has not been able to receive the prize after three thousand
five days of detention .
Reference 1: she is the only Sakharov laureate who has not been able to receive her prize after
more than three thousand five hundred days in captivity .
Reference 2: she is the only Sacharov prizewinner who couldn?t yet pick up her prize after more
than three thousand five hundred days of imprisonment .
Figure 4: Example translation using the back-off and the continuous space language model (CSLM).
on the Dev data (+0.8 on Test set) compared to
the 3-gram back-off LM. The neural network LM
achieves an additional improvement of 1 point
BLEU (+0.8 on Test data), on top of the 4-gram
back-off LM. Small improvements of the word er-
ror rate (WER) and the position independent word
error rate (PER) were also observed.
As usually observed in SMT, the improvements
on the test data are smaller than those on the de-
velopment data which was used to tune the param-
eters. As a rule of thumb, the gain on the test data
is often half as large as on the Dev-data. The 4-
gram back-off and neural network LM show both
a good generalization behavior.
 42.8
 43
 43.2
 43.4
 43.6
 43.8
 44
 44.2
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
 64
 66
 68
 70
 72
 74
 76
 78
BL
EU
 s
co
re
Pe
rp
le
xit
y
Interpolation coefficient
4-gram back-off LM
BLEU score
Perplexity
Figure 5: BLEU score and perplexity in function
of the interpolation coefficient of the back-off 4-
gram LM.
Figure 5 shows the perplexity and the BLEU
score for different interpolation coefficients of the
4-gram back-off LM. For a value of 1.0 the back-
off LM is used alone, while only the neural net-
work LMs are used for a value of 0.0. Using an
EM procedure to minimize perplexity of the inter-
polated model gives a value of 0.189. This value
also seems to correspond to the best BLEU score.
This is a surprising result, and has the advan-
tage that we do not need to tune the interpola-
tion coefficient in the framework of the log-linear
feature function combination. The weights of the
other feature functions were optimized separately
for each experiment. We noticed a tendency to
a slightly higher weight for the continuous space
LM and a lower sentence length penalty.
In a contrastive experiment, the LM training
data was substantially increased by adding 352M
words of commercial Broadcast News data and
232M words of CNN news collected on the Inter-
net. Although the perplexity of the 4-gram back-
off LM decreased by 5 points to 74.1, we observed
no change in the BLEU score. In order to estimate
the oracle BLEU score of the lattices we build a 4-
gram back-off LM on the development data. Lat-
tice rescoring achieved a BLEU score of 59.10.
There are many discussions about the BLEU
score being or not a meaningful measure to as-
sess the quality of an automatic translation sys-
tem. It would be interesting to verify if the contin-
uous space LM has an impact when human judg-
ments of the translation quality are used, in partic-
ular with respect to fluency. Unfortunately, this is
not planed in the TC-STAR evaluation campaign,
and we give instead an example translation (see
Figure 4). In this case, two errors were corrected
(insertion of the word ?the? and deletion of the
comma).
5 Conclusion and Future Work
Some SMT decoders have an execution complex-
ity that increases rapidly with the length of the
sentences to be translated, which are usually split
729
into smaller chunks and translated separately. This
can lead to translation errors and bad modeling
of the LM probabilities of the words at both ends
of the chunks. We have presented a lattice join-
ing and rescoring approach that obtained signifi-
cant improvements in the BLEU score compared
to simply concatenating the 1-best solutions of
the individual chunks. The task considered is the
translation of European Parliament Speeches in
the framework of the TC-STAR project.
We have also presented a neural network LM
that performs probability estimation in a contin-
uous space. Since the resulting probability func-
tions are smooth functions of the word represen-
tation, better generalization to unknown n-grams
can be expected. This is particularly interesting
for tasks where only limited amounts of appropri-
ate LM training material are available, but the pro-
posed LM can be also trained on several hundred
millions words. The continuous space LM is used
to rescore the translation lattices. We obtained
an improvement of 0.8 points BLEU on the test
data compared to a 4-gram back-off LM, which it-
self had already achieved the same improvement
in comparison to a 3-gram LM.
The results reported in this paper have been ob-
tained with a word based SMT system, but the
continuous space LM can also be used with a
phrase-based system. One could expect that the
target language model plays a different role in
a phrase-based system since the phrases induce
some local coherency on the target sentence. This
will be studied in the future. Another promis-
ing direction that we have not yet explored, is to
build long-span LM, i.e. with n much greater than
4. The complexity of our approach increases only
slightly with n. Long-span LM could possibly im-
prove the word-ordering of the generated sentence
if the translation lattices include the correct paths.
References
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(2):1137?1155.
A. Berger, S. Della Pietra, and Vincent J. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22:39?71.
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
P. Brown, S. Della Pietra, Vincent J. Della Pietra, and
R. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003.
Syntax-based language models for machine transla-
tion. In Machine Translation Summit.
Daniel De?chelotte, Holger Schwenk, and Jean-Luc
Gauvain. 2006. The 2006 LIMSI statistical ma-
chine translation system for TC-STAR. In TC-STAR
Speech to Speech Translation Workshop, Barcelona.
Ahmad Emami and Frederick Jelinek. 2005. Ran-
dom clusterings for language modeling. In ICASSP,
pages I:581?584.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and
H. Ney. 2005. Cross domain automatic transcrip-
tion on the TC-STAR EPPS corpus. In ICASSP.
Sasa Hasan, Olivier Bender, and Hermann Ney. 2006.
Reranking translation hypothesis using structural
properties. In LREC.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation.
In ACL?05 workshop on Building and Using Paral-
lel Text, pages 125?128.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302,
University of Pennsylvania.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith,
K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smor-
gasbord of features for statistical machine transla-
tion. In NAACL, pages 161?168.
Franz-Joseph Och. 2005. The Google statistical ma-
chine translation system for the 2005 Nist MT eval-
uation, Oral presentation at the 2005 Nist MT Eval-
uation workshop, June 20.
Holger Schwenk and Jean-Luc Gauvain. 2005. Train-
ing neural network language models on very large
corpora. In EMNLP, pages 201?208.
Holger Schwenk. 2004. Efficient training of large
neural networks for language modeling. In IJCNN,
pages 3059?3062.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP, pages II: 901?
904.
Peng Xu and Frederick Jelinek. 2004. Random forest
in language modeling. In EMNLP, pages 325?332.
730
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 65?71,
Rochester, New York, April 2007. c?2007 Association for Computational Linguistics
Combining Morphosyntactic Enriched Representation with
n-best Reranking in Statistical Translation
H. Bonneau-Maynard, A. Allauzen, D. De?chelotte and H. Schwenk
Spoken Language Processing Group
LIMSI-CNRS, BP 133
91403 Orsay cedex, FRANCE
{maynard,allauzen,dechelot,schwenk}@limsi.fr
Abstract
The purpose of this work is to explore
the integration of morphosyntactic infor-
mation into the translation model itself, by
enriching words with their morphosyntac-
tic categories. We investigate word dis-
ambiguation using morphosyntactic cate-
gories, n-best hypotheses reranking, and
the combination of both methods with
word or morphosyntactic n-gram lan-
guage model reranking. Experiments
are carried out on the English-to-Spanish
translation task. Using the morphosyn-
tactic language model alone does not
results in any improvement in perfor-
mance. However, combining morphosyn-
tactic word disambiguation with a word
based 4-gram language model results in a
relative improvement in the BLEU score
of 2.3% on the development set and 1.9%
on the test set.
1 Introduction
Recent works in statistical machine translation
(SMT) shows how phrase-based modeling (Och and
Ney, 2000a; Koehn et al, 2003) significantly out-
perform the historical word-based modeling (Brown
et al, 1993). Using phrases, i.e. sequences of
words, as translation units allows the system to pre-
serve local word order constraints and to improve
the consistency of phrases during the translation pro-
cess. Phrase-based models provide some sort of
context information as opposed to word-based mod-
els. Training a phrase-based model typically re-
quires aligning a parallel corpus, extracting phrases
and scoring them using word and phrase counts. The
derived statistics capture the structure of natural lan-
guage to some extent, including implicit syntactic
and semantic relations.
The output of a SMT system may be difficult to
understand by humans, requiring re-ordering words
to recover its syntactic structure. Modeling language
generation as a word-based Markovian source (an n-
gram language model) discards linguistic properties
such as long term word dependency and word-order
or phrase-order syntactic constraints. Therefore, ex-
plicit introduction of structure in the language mod-
els becomes a major and promising focus of atten-
tion.
However, as of today, it seems difficult to outper-
form a 4-gram word language model. Several stud-
ies have attempted to use morphosyntactic informa-
tion (also known as part-of-speech or POS informa-
tion) to improve translation. (Och et al, 2004) have
explored many different feature functions. Rerank-
ing n-best lists using POS has also been explored by
(Hasan et al, 2006). In (Kirchhoff and Yang, 2005),
a factored language model using POS information
showed similar performance to a 4-gram word lan-
guage model. Syntax-based language models have
also been investigated in (Charniak et al, 2003). All
these studies use word phrases as translation units
and POS information in just a post-processing step.
This paper explores the integration of morphosyn-
tactic information into the translation model itself
by enriching words with their morphosyntactic cat-
65
egories. The same idea has already been applied
in (Hwang et al, 2007) to the Basic Travel Ex-
pression Corpus (BTEC). To our knowledge, this
approach has not been evaluated on a large real-
word translation problem. We report results on
the TC-STAR task (public European Parliament Ple-
nary Sessions translation). Furthermore, we pro-
pose to combine this approach with classical n-best
list reranking. Experiments are carried out on the
English-to-Spanish task using a system based on the
publicly available Moses decoder.
This paper is organized as follows: In Section
2 we first describe the baseline statistical machine
translation systems. Section 3 presents the consid-
ered task and the processing of the corpora. The
experimental evaluation is summarized in section 4.
The paper concludes with a discussion of future re-
search directions.
2 System Description
The goal of statistical machine translation is to pro-
duce a target sentence e from a source sentence f .
Among all possible target language sentences the
one with the highest probability is chosen. The use
of a maximum entropy approach simplifies the intro-
duction of several additional models explaining the
translation process:
e? = argmaxPr(e|f)
= argmaxe {exp(
?
i
?ihi(e, f))} (1)
where the feature functions hi are the system
models characterizing the translation process, and
the coefficients ?i act as weights.
2.1 Moses decoder
Moses1 is an open-source, state-of-the-art phrase-
based decoder. It implements an efficient beam-
search algorithm. Scripts are also provided to train a
phrase-based model. The popular Giza++ (Och and
Ney, 2000b) tool is used to align the parallel corpora.
The baseline system uses 8 feature functions hi,
namely phrase translation probabilities in both di-
rections, lexical translation probabilities in both di-
rections, a distortion feature, a word and a phrase
1http://www.statmt.org/moses/
penalty and a trigram target language model. Ad-
ditional features can be added, as described in the
following sections. The weights ?i are typically op-
timized so as to maximize a scoring function on a
development set (Och and Ney, 2002).
The moses decoder can output n-best lists, pro-
ducing either distinct target sentences or not (as
different segmentations may lead to the same sen-
tence). In this work, distinct sentences were always
used.
These n-best lists can be rescored using higher
order language models (word- or syntactic-based).
There are two ways to carry out the rescoring: one,
by replacing the language model score or by adding
a new feature function; two, by performing a log-
linear interpolation of the language model used for
decoding and the new language model. This latter
approach was used in all the experiments described
in this paper. The set of weights is systematically
re-optimized using the algorithm presented below.
2.2 Weight optimization
A common criterion to optimize the coefficients of
the log-linear combination of feature functions is to
maximize the BLEU score (Papineni et al, 2002)
on a development set (Och and Ney, 2002). For
this purpose, the public numerical optimization tool
Condor (Berghen and Bersini, 2005) is integrated in
the following iterative algorithm:
0. Using good general purpose weights, the
Moses decoder is used to generate 1000-best
lists.
1. The 1000-best lists are reranked using the cur-
rent set of weights.
2. The current hypothesis is extracted and scored.
3. This BLEU score is passed to Condor, which
either computes a new set of weights (the al-
gorithm then proceeds to step 1) or detects that
a local maxima has been reached and the algo-
rithm stops iterating.
The solution is usually found after about 100 itera-
tions. It is stressed that the n-best lists are generated
only once and that the whole tuning operates only
on the n-best lists.
66
English: IPP declareV V P resumedV V D theDT sessionNN ofIN theDT EuropeanNP ParliamentNP
Spanish: declaroV Lfin reanudadoV Ladj elART perodoNC dePREP sesionesNC
delPDEL ParlamentoNC EuropeoADJ
Figure 1: Example of POS-tag enriched bi-text used to train the translation models
2.3 POS disambiguation
It is well-known that syntactic structures vary
greatly across languages. Spanish, for example,
can be considered as a highly inflectional language,
whereas inflection plays only a marginal role in En-
glish.
POS language models can be used to rerank the
translation hypothesis, but this requires tagging the
n-best lists generated by the SMT system. This can
be difficult since POS taggers are not well suited for
ill-formed or incorrect sentences. Finding a method
in which morphosyntactic information is used di-
rectly in the translation model could help overcome
this drawback but also takes account for the syntac-
tic specificities of both source and target languages.
It seems likely that the morphosyntactic informa-
tion of each word will be useful to encode linguis-
tic characteristics, resulting in a sort of word disam-
biguation by considering its morphosyntactic cate-
gory. Therefore, in this work we investigate a trans-
lation model which enriches every word with its syn-
tactic category. The enriched translation units are a
combination of the original word and the POS tag, as
shown in Figure 1. The translation system takes a se-
quence of enriched units as inputs and outputs. This
implies that the test data must be POS tagged before
translation. Likewise, the POS tags in the enriched
output are removed at the end of the process to pro-
vide the final translation hypothesis which contain
only a word sequence. This approach also allows
to carry out a n-best reranking step using either a
word-based or a POS-based language model.
3 Task, corpus and tools
The experimental results reported in this article were
obtained in the framework of an international evalu-
ation organized by the European TC-STAR project2
in February 2006. This project is envisaged as a
2http://www.tc-star.org/
long-term effort to advance research in all core tech-
nologies for speech-to-speech translation.
The main goal of this evaluation is to trans-
late public European Parliament Plenary Sessions
(EPPS). The training material consists of the sum-
mary edited by the European Parliament in several
languages, which is also known as the Final Text
Editions (Gollan et al, 2005). These texts were
aligned at the sentence level and they are used to
train the statistical translation models (see Table 1
for some statistics).
Spanish English
Whole parallel corpus
Sentence Pairs 1.2M
Total # Words 34.1M 32.7M
Vocabulary size 129k 74k
Sentence length ? 40
Sentence Pairs 0.91M
Total # Words 18.5M 18.0M
Word vocabulary 104k 71k
POS vocabulary 69 59
Enriched units vocab. 115k 77.6k
Table 1: Statistics of the parallel texts used to train
the statistical machine translation system.
Three different conditions are considered in the
TC-STAR evaluation: translation of the Final Text
Edition (text), translation of the transcriptions of the
acoustic development data (verbatim) and transla-
tion of speech recognizer output (ASR). Here we
only consider the verbatim condition, translating
from English to Spanish. For this task, the develop-
ment and test data consists of about 30k words. The
test data is partially collected in the Spanish parlia-
ment. This results in a small mismatch between de-
velopment and test data. Two reference translations
are provided. The scoring is case sensitive and in-
cludes punctuation symbols.
67
3.1 Text normalization
The training data used for normalization differs sig-
nificantly from the development and test data. The
Final Text Edition corpus follows common ortho-
graphic rules (for instance, the first letter of the word
following a full stop or a column is capitalized) and
represents most of the dates, quantities, article refer-
ences and other numbers in digits. Thus the text had
to be ?true-cased? and all numbers were verbalized
using in-house language-specific tools. Numbers are
not tagged as such at this stage; this is entirely left
to the POS tagger.
3.2 Translation model training corpus
Long sentences (more than 40 words) greatly slow
down the training process, especially at the align-
ment step with Giza++. As shown in Figure 2, the
histogram of the length of Spanish sentences in the
training corpus decreases steadily after a length of
20 to 25 words, and English sentences exhibit a sim-
ilar behavior. Suppressing long sentences from the
corpus reduces the number of aligned sentences by
roughly 25% (see Table 1) but speeds the whole
training procedure by a factor of 3. The impact on
performance is discussed in the next section.
 0
 5000
 10000
 15000
 20000
 25000
 30000
 35000
 0  10  20  30  40  50  60  70  80  90  100
Histogram of Spanish sentences? lengths (training set)
Figure 2: Histogram of the sentence length (Spanish
part of the parallel corpus).
3.3 Language model training corpus
In the experiments reported below, a trigram word
language model is used during decoding. This
model is trained on the Spanish part of the parallel
corpus using only sentences shorter than 40 words
(total of 18.5M of language model training data).
Second pass language models were trained on all
available monolingual data (34.1M words).
3.4 Tools
POS tagging was performed with the TreeTagger
(Schmid, 1994). This software provides resources
for both of the considered languages and it is freely
available. TreeTagger is a Markovian tagger that
uses decision trees to estimate trigram transition
probabilities. The English version is trained on the
PENN treebank corpus3 and the Spanish version on
the CRATER corpus.4
Language models are built using the SRI-LM
toolkit (Stolcke, 2002). Modified Knesser-Ney dis-
counting was used for all models. In (Goodman,
2001), a systematic description and comparison of
the usual smoothing methods is reported. Modified
Knesser-Ney discounting appears to be the most ef-
ficient method.
4 Experiments and Results
Two baseline English-to-Spanish translation mod-
els were created with Moses. The first model was
trained on the whole parallel text ? note that sen-
tences with more than 100 words are excluded by
Giza++. The second model was trained on the cor-
pus using only sentences with at most 40 words. The
BLEU score on the development set using good gen-
eral purpose weights is 48.0 for the first model and
47.0 for the second. Because training on the whole
bi-text is much slower, we decided to perform our
experiments on the bi-texts restricted to the ?short?
sentences.
4.1 Language model generation
The reranking experiments presented below use the
following language models trained on the Spanish
part of the whole training corpus:
? word language models,
? POS language model,
? POS language model, with a stop list used to
remove the 100 most frequent words (POS-
stop100 LM),
? language model of enriched units.
3http://www.cis.upenn.edu/ treebank
4http://www.comp.lancs.ac.uk/linguistics/crater/corpus.html
68
English : you will be aware President that over the last few sessions in Strasbourg. ..
Baseline: usted sabe que el Presidente durante los u?ltimos sesiones en Estrasburgo ...
Enriched units: usted sabe que el Presidente en los u?ltimos per??odos de sesiones en Estrasburgo ...
English : ... in this house there might be some recognition ...
Baseline: ... en esta asamblea no puede ser un cierto reconocimiento ...
Enriched units: ... en esta asamblea existe un cierto reconocimiento ...
Figure 3: Comparative translations using the baseline word system and the enriched unit system.
For each of these four models, various orders
were tested (n = 3, 4, 5), but in this paper we only
report those orders that yielded the greatest improve-
ments. POS language models were obtained by first
extracting POS sequences from the previously POS-
tagged training corpus and then by estimating stan-
dard back-off language models.
As shown in Table 1, the vocabulary size of the
word language model is 104k for Spanish and 74k
for English. The number of POS is small: 69 for
Spanish and 59 for English. We emphasize that
the tagset provided by TreeTagger does include nei-
ther gender nor number distinction. The vocabulary
size of the enriched-unit language model is 115k for
Spanish and 77.6k for English. The syntactical am-
biguity of words is low: the mean ambiguity ratio is
1.14 for Spanish and 1.12 for English.
4.2 Reranking the word n-best lists
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on words as units are summarized in Table 2. The
baseline result, with trigram word LM reranking,
gives a BLEU score of 47.0 (1rst row). From the
n-best lists provided by this translation model, we
compared reranking performances with different tar-
get language models. As observed in the literature,
an improvement can be obtained by reranking with
a 4-gram word language model (47.0 ? 47.5, 2d
row). By post-tagging this n-best list, a POS lan-
guage model reranking can be performed. However,
reranking with a 5-gram POS language model alone
does not give any improvement from the baseline
(BLEU score of 46.9, 3rd row). This result corre-
sponds to known work in the literature (Kirchhoff
and Yang, 2005; Hasan et al, 2006), when using
POS only as a post-processing step during rerank-
ing. As suggested in section 2.3, this lack of per-
formance can be due to the fact that the tagger is
not able to provide a usefull tagging of sentences
included in the n-best lists. This observation is
also available when reranking of the word n-best is
done with a language model based on enriched units
(BLEU score of 47.6, not reported in Table 2).
4.3 POS disambiguation and reranking
The results concerning reranking experiments of the
n-best lists provided by the translation model based
on enriched units are summarized in Table 3. Us-
ing a trigram language model of enriched transla-
tion units leads to a BLEU score of 47.4, a 0.4 in-
crease over the baseline presented in section 4.2.
Figure 3 shows comparative translation examples
from the baseline and the enriched translation sys-
tems. In the first example, the baseline system out-
puts ?durante los u?ltimos sesiones? where the en-
riched translation system produces ?en los u?ltimos
per??odos de sesiones?, a better translation that may
be attributed to the introduction of the masculine
word ?per??odos?, allowing the system to build a
syntactically correct sentence. In the second exam-
ple, the syntactical error ?no puede ser un cierto re-
conocimiento? produced by the baseline system in-
duces an incorrect meaning of the sentence, whereas
the enriched translation system hypothesis ?existe un
cierto reconocimiento? is both syntactically and se-
mantically correct.
Reranking the enriched n-best with POS language
models (either with or without a stop list) does not
seem to be efficient (0.3 BLEU increasing with the
POS-stop100 language model).
A better improvement is obtained when reranking
is performed with the 4-gram word language model.
This results in a BLEU score of 47.9, correspond-
ing to a 0.9 improvement over the word baseline. It
is interesting to observe that reranking a n-best list
69
Dev. Test
3g word LM baseline 47.0 46.0
4g word LM reranking 47.5 46.5
5g POS reranking 46.9 46.1
Table 2: BLEU scores using words as translation
units.
obtained with a translation model based on enriched
units with a word LM results in better performances
than a enriched units LM reranking of a n-best list
obtained with a translation model based on words.
The last two rows of Table 3 give results when
combining word and POS language models to rerank
the enriched n-best lists. In both cases, 10 features
are used for reranking (8 Moses features + word
language model probability + POS language model
probability). The best result is obtained by com-
bining the 5-gram word language model with the 5-
gram POS-stop100 language model. In that case,
the best BLEU score is observed (48.1), with a 2.3%
relative increase over the trigram word baseline.
4.4 Results on the test set
The results on the test set are given in the second
column of Tables 2 and 3. Although the enriched
translation system is only 0.1 BLEU over the base-
line system (46.0 ? 46.1) when using a trigram lan-
guage model, the best condition observed on the de-
velopment set (word and POS-stop100 LMs rerank-
ing) results in a 46.8 BLEU score, corresponding to
a 0.8 increasing.
It can be observed that rescoring with a 4-gram
word language model leads to same score resulting
in a 1.9% relative increase over the trigram word
baseline.
5 Conclusion and future work
Combining word language model reranking of n-
best lists based on syntactically enriched units seems
to produce more consistent hypotheses. Using en-
riched translation units results in a relative 2.3%
improvement in BLEU on the development set and
1.9% on the test over the trigram baseline. Over a
standard translation model with 4-gram rescoring,
the enriched unit translation model leads to an abso-
lute increase in BLEU score of 0.4 both on the devel-
opment and the test sets. These first results are en-
Dev. Test
3g enriched units LM baseline 47.4 46.1
4g enriched units LM reranking 47.8 46.8
4g word LM reranking 47.9 46.9
5g POS LM reranking 47.5 46.2
5g POS-stop100 LM reranking 47.7 46.3
word + POS LMs reranking 47.9 46.9
word + POS-stop100 LMs rerank. 48.1 46.8
Table 3: BLEU scores using enriched translation
units.
couraging enough to further investigate the integra-
tion of syntactic information in the translation model
itself, rather than to restrict it to the post-processing
pass. As follow-up experiments, it is planned to in-
clude gender and number information in the tagset,
as well as the word stems to the enriched units.
This work should be considered as preliminary
experiments for the investigation of factored trans-
lation models, which Moses is able to handle. POS
factorization is indeed a way to add some general-
ization capability to the enriched translation models.
6 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR (IST-
2002-FP6-506738), and by the French Government
under the project INSTAR (ANR JCJC06 143038).
We would like to thanks Marc Ferras for his help
concerning the Spanish language.
References
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of powell?s
UOBYQA algorithm: Experimental results and com-
parison with the DFO algorithm. Journal of Computa-
tional and Applied Mathematics, 181:157?175.
Peter F Brown, Stephen A Della Pietra, Vincent J Della
Pietra, and Robert L Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
E. Charniak, K. Knight, and K. Yamada. 2003. Syntax-
based language models for machine translation. In
Proceedings of MT Summit IX.
C. Gollan, M. Bisani, S. Kanthak, R. Schlueter, and ?H.
Ney. 2005. Cross domain automatic transcription on
70
the TC-STAR epps corpus. In Proceedings of ICASSP
2005.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15(4):403?434, October.
S. Hasan, O. Bender, and H. Ney. 2006. Reranking trans-
lation hypothesis using structural properties. In Pro-
ceedings of EACL 2006.
Y.S. Hwang, A. Finch, and Y. Sasaki. 2007. Improving
statistical machine translation using shallow linguistic
knoledge. to be published in Computer, Speech and
Language.
Katrin Kirchhoff and Mei Yang. 2005. Improved lan-
guage modeling for statistical machine translation. In
Proceedings of ACL ?05 workshop on Building and Us-
ing Parallel Text, pages 125?128.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the Human Language Technology Conference
2003 (HLT-NAACL 2003), Edmonton, Canada, May.
Franz Josef Och and Hermann Ney. 2000a. Improved
statistical alignment models. In Proc. of the 38th An-
nual Meeting of the Association for Computational
Linguistics, pages 440?447, Hongkong, China, Octo-
ber.
Franz Josef Och and Hermann Ney. 2000b. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Computa-
tional Linguistics, pages 440?447, Hong Kong, China,
October.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
F.-J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In NAACL,
pages 161?168.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, University of Pennsylva-
nia.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing, September.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In Proceedings of ICSLP, pages II:
901?904.
71
Proceedings of the Second Workshop on Statistical Machine Translation, pages 189?192,
Prague, June 2007. c?2007 Association for Computational Linguistics
Building a Statistical Machine Translation System
for French using the Europarl Corpus
Holger Schwenk
LIMSI-CNRS, bat 508, BP 133
91403 Orsay cedex, FRANCE
schwenk@limsi.fr
Abstract
This paper describes the development of a
statistical machine translation system based
on the Moses decoder for the 2007 WMT
shared tasks. Several different translation
strategies were explored. We also use a sta-
tistical language model that is based on a
continuous representation of the words in
the vocabulary. By these means we expect to
take better advantage of the limited amount
of training data. Finally, we have investi-
gated the usefulness of a second reference
translation of the development data.
1 Introduction
This paper describes the development of a statistical
machine translation system based on the Moses de-
coder (Koehn et al, 2007) for the 2007 WMT shared
tasks. Due to time constraints, we only considered
the translation between French and English. A sys-
tem with a similar architecture was successfully ap-
plied to the translation between Spanish and En-
glish in the framework of the 2007 TC-STAR eval-
uation.1 For the 2007 WMT shared task a recipe is
provided to build a baseline translation system using
the Moses decoder. Our system differs in several as-
pects from this base-line: 1) the training data is not
lower-cased; 2) Giza alignments are calculated on
sentences of up to 90 words; 3) a two pass-decoding
was used; and 4) a so called continuous space lan-
guage model is used in order to take better advantage
of the limited amount of training data.
1A paper on this work is submitted to MT Sumit 2007.
This architecture is motivated and detailed in the
following sections.
2 Architecture of the system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . It is today common practice to use phrases
as translation units (Koehn et al, 2003; Och and
Ney, 2003) and a log linear framework in order to
introduce several models explaining the translation
process:
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models and
the ?i weights are typically optimized to maximize
a scoring function on a development set (Och and
Ney, 2002). In our system fourteen features func-
tions were used, namely phrase and lexical transla-
tion probabilities in both directions, seven features
for the lexicalized distortion model, a word and a
phrase penalty and a target language model (LM).
The system is constructed as follows. First,
Giza++ is used to perform word alignments in both
directions. Second, phrases and lexical reorderings
are extracted using the default settings of the Moses
SMT toolkit. A target LM is then constructed as
detailed in section 2.1. The translation itself is per-
formed in two passes: first, Moses in run and a 1000-
best list is generated for each sentence. When gen-
erating n-best lists it may happen that the same tar-
get sentence is generated multiple times, for instance
using different segmentations of the source sentence
189
or a different set of phrases. We enforced all the
hypothesis in an n-best list to be lexically different
since our purpose was to rescore them with a LM.
The parameters of Moses are tuned on devtest2006
for the Europarl task and nc-dev2007 for the news
commentary task, using the cmert tool.
These 1000-best lists are then rescored with dif-
ferent language models, either using a longer con-
text or performing the probability estimation in the
continuous space. After rescoring, the weights of the
feature functions are optimized again using the nu-
merical optimization toolkit Condor (Berghen and
Bersini, 2005). Note that this step operates only on
the 1000-best lists, no re-decoding is performed. In
general, this results in an increased weight for the
LM. Comparative results are provided in the result
section whether it seems to be better to use higher
order language models already during decoding, or
to generate first rich n-best lists and to use the im-
proved LMs during rescoring.
2.1 Language modeling
The monolingual part of the Europarl (38.3M En-
glish and 43.1 French words) and the news commen-
tary corpus (1.8M/1.2M words) were used. Separate
LMs were build on each data source and then lin-
early interpolated, optimizing the coefficients with
an EM procedure. This usually gives better re-
sults than building an LM on the pooled data. Note
that we build two sets of LMs: a first set tuned on
devtest2006, and a second one on nc-dev2007. It
is not surprising to see that the interpolation coeffi-
cients differ significantly: 0.97/0.03 for devtest2006
and 0.42/0.58 for nc-dev2007. The perplexities of
the interpolated LMs are given in Table 1.
2.2 Continuous space language model
Overall, there are roughly 40 million words of texts
available to train the target language models. This
is a quite limited amount in comparison to tasks like
the NIST machine translation evaluations for which
several billion words of newspaper texts are avail-
able. Therefore, new techniques must be deployed
to take the best advantage of the limited resources.
Here, we propose to use the so-called continu-
ous space LM. The basic idea of this approach is to
project the word indices onto a continuous space and
to use a probability estimator operating on this space
French English
Eparl News Eparl News
Back-off LM:
3-gram 47.0 91.6 57.2 160.1
4-gram 41.5 85.2 51.6 152.4
Continuous space LM:
4-gram 35.8 73.9 44.5 133.4
5-gram 33.9 71.2 - -
6-gram 33.1 70.1 41.2 127.0
Table 1: Perplexities on devtest2006 (Europarl) and
nc-dev2007 (news commentary) for various LMs.
(Bengio et al, 2003). Since the resulting probability
functions are smooth functions of the word repre-
sentation, better generalization to unknown n-grams
can be expected. A neural network can be used to si-
multaneously learn the projection of the words onto
the continuous space and to estimate the n-gram
probabilities. This is still a n-gram approach, but
the LM probabilities are ?interpolated? for any pos-
sible context of length n-1 instead of backing-off to
shorter contexts.
This approach was successfully used in large vo-
cabulary continuous speech recognition (Schwenk,
2007) and in a phrase-based system for a small task
(Schwenk et al, 2006). Here, it is the first time
applied in conjunction with a lexicalized reordering
model. A 4-gram continuous space LM achieves a
perplexity reduction of about 13% relative with re-
spect to a 4-gram back-off LM (see Table 1). Ad-
ditional improvements can be obtained by using a
longer context. Note that this is difficult for back-
off LMs due to insufficient training data.
3 Experimental Evaluation
The system was trained on the Europarl parallel texts
only (approx. 1.3M words). The news commentary
parallel texts were not used. We applied the tok-
enization proposed by the Moses SMT toolkit and
the case was preserved. While case sensitivity may
hurt the alignment process, we believe that true case
is beneficial for language modeling, in particular in
future versions of our system in which we plan to
use POS information. Experiences with alternative
tokenizations are undergoing.
The parameters of the system were tuned on
190
DevTest2006 Test2006
Decode: 3-gram 4-gram 3-gram 4-gram
Back-off LM:
decode 30.88 - 30.82 -
4-gram 31.65 31.43 31.35 30.86
Continuous space LM:
4-gram 31.96 31.75 32.03 31.59
5-gram 31.97 31.86 31.90 31.50
6-gram 32.00 31.93 31.89 31.64
Lex. diff. 904.2 797.6 900.6 795.8
Oracle 37.82 37.64 - -
Table 2: Comparison of different translation strate-
gies (BLEU scores for English to French): 3- or 4-
gram decoding (columns) and n-best list rescoring
with various language models (lines).
devtest2006 and nc-dev2007 respectively. The
generalization performance was estimated on the
test2006 and nc-devtest2007 corpora respectively.
3.1 Comparison of decoding strategies
Two different decoding strategies were compared in
order to find out whether it is necessary to already
use higher-order LMs during decoding or whether
the incorporation of this knowledge can be post-
poned to the n-best list rescoring. Tri- or 4-gram
back-off language models were used during decod-
ing. In both cases the generated n-best lists were
rescored with higher order back-off or the continu-
ous space language model. A beam of 0.6 was used
in all our experiments.
The oracle BLEU scores of the generated n-best
lists were estimated by rescoring the n-best lists with
a cheating LM trained on the development data. We
also provide the average number of lexically differ-
ent hypothesis in the n-best lists. The results are
summarized in Table 2 and 3. The numbers in bold
indicate the systems that were used in the evaluation.
These results are somehow contradictory : while
running Moses with a trigram LM seems to be better
when translating from English to French, a 4-gram
LM achieves better results when translating to En-
glish. An analysis after the evaluation seems to indi-
cate that the pruning was too aggressive for a 4-gram
LM, at least for a morphologically rich language like
French. Using a beam of 0.4 and a faster implemen-
DevTest2006 Test2006
Decode: 3-gram 4-gram 3-gram 4-gram
Back-off LM:
decode 32.21 - 31.50 -
4-gram 32.46 32.34 32.07 32.12
Continuous space LM:
4-gram 32.87 32.90 30.51 32.47
6-gram 32.85 32.98 32.46 32.50
Lex. diff. 791.3 822.7 802.5 827.8
Oracle 38.80 39.69 - -
Table 3: Comparison of different translation strate-
gies (BLEU scores for French to English).
tation of lexical reordering in the Moses decoder, it
is apparently better to use a 4-gram LM during de-
coding. The oracle scores of the n-best lists and
the average number of lexically different hypothe-
sis seem to correlate well with the BLEU scores: in
all cases it is better to use the system that produced
n-best lists with more variety and a higher oracle
BLEU score.
The continuous space language model achieved
improvements in the BLEU by about 0.4 on the de-
velopment data. It is interesting to note that this ap-
proach showed a very good generalization behavior:
the improvements obtained on the test data are as
good or even exceed those observed on the Dev data.
3.2 Multiple reference translations
Only one reference translation is provided for all
tasks in the WMT?07 evaluation. This may be prob-
lematic since systems that do not use the official jar-
gon or different word order may get ?incorrectly? a
low BLEU score. We have also noticed that the ref-
erence translations are not always real translations
of the input, but they rely on document wide context
information. Therefore, we have produced a second
set of sentence based reference translations.2
The improvements brought by the continuous
space LM are much higher using the new reference
translations. Using both reference translations to-
gether leads to an important increase of the BLEU
score and confirms the improvements obtained by
the continuous space LM. These results are in line
2The second reference translations can be downloaded from
http://instar.limsi.fr/en/data.html
191
Ref. transl.: official addtl. both retuned
Back-off 31.64 32.91 47.62 47.95
CSLM 32.00 33.81 48.66 49.02
Table 4: Impact of additional human reference trans-
lations (devtest2006, English to French)
with our experiences when translating from English
to Spanish in the framework of the TC-STAR project
(gain of about 1 point BLEU). The BLEU scores can
be further improved by rerunning the whole tuning
process using two reference translations (last col-
umn of Table 4).
Second reference translations for the test data are
not yet available. Therefore the devtest data was
split into two parts: the back-off and the CSLM
achieve BLEU scores of 47.98 and 48.66 respec-
tively on the first half used for tuning, and of 47.95
and 49.02 on the second half used for testing.
3.3 Adaptation to the news commentary task
We only performed a limited domain adaptation: the
LMs and the coefficients of the log-linear combi-
nation of the feature functions were optimized on
nc-dev2007. We had no time to add the news com-
mentary parallel texts which may result in miss-
ing translations for some news specific words. The
BLEU scores on the development and development
test data are summarized in Table 5. A trigram
was used to generate 1000-best lists that were then
rescored with various language models.
Language modeling seems to be difficult when
translating from English to French: the use of a 4-
gram has only a minor impact. The continuous space
LM achieves an improvement of 0.3 on nc-dev and
0.5 BLEU on nc-devtest. There is no benefit for us-
English/French French/English
dev devtest dev devtest
Back-off LM:
decode 27.11 25.31 27.57 26.21
4-gram 27.35 25.53 27.56 26.55
Continuous space LM:
4-gram 27.63 26.01 28.25 26.87
6-gram 27.60 25.64 28.38 27.26
Table 5: BLEU scores for news commentary task.
ing longer span LMs. The BLEU score is even 0.5
worse on nc-devtest due to a brevity penalty of 0.95.
The continuous space LM also achieves interesting
improvements in the BLEU score when translating
from French to English.
4 Acknowledgments
This work has been partially funded by the European
Union under the integrated project TC-STAR and by
the French Government under the project INSTAR
(ANR JCJC06 143038). The author would like to
recognize the contributions of A. Allauzen for his
help with the creation of the second reference trans-
lations.
References
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3(2):1137?1155.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of powell?s
UOBYQA algorithm: Experimental results and com-
parison with the DFO algorithm. Journal of Computa-
tional and Applied Mathematics, 181:157?175.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL, demonstation session.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Holger Schwenk, Marta R. Costa-jussa`, and Jose? A. R.
Fonollosa. 2006. Continuous space language models
for the IWSLT 2006 task. In IWSLT, pages 166?173,
November.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
192
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 16?23,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
On the use of Comparable Corpora to improve SMT performance
Sadaf Abdul-Rauf and Holger Schwenk
LIUM, University of Le Mans, FRANCE
Sadaf.Abdul-Rauf@lium.univ-lemans.fr
Abstract
We present a simple and effective method
for extracting parallel sentences from
comparable corpora. We employ a sta-
tistical machine translation (SMT) system
built from small amounts of parallel texts
to translate the source side of the non-
parallel corpus. The target side texts are
used, along with other corpora, in the lan-
guage model of this SMT system. We
then use information retrieval techniques
and simple filters to create French/English
parallel data from a comparable news cor-
pora. We evaluate the quality of the ex-
tracted data by showing that it signifi-
cantly improves the performance of an
SMT systems.
1 Introduction
Parallel corpora have proved be an indispens-
able resource in Statistical Machine Translation
(SMT). A parallel corpus, also called bitext, con-
sists in bilingual texts aligned at the sentence level.
They have also proved to be useful in a range of
natural language processing applications like au-
tomatic lexical acquisition, cross language infor-
mation retrieval and annotation projection.
Unfortunately, parallel corpora are a limited re-
source, with insufficient coverage of many lan-
guage pairs and application domains of inter-
est. The performance of an SMT system heav-
ily depends on the parallel corpus used for train-
ing. Generally, more bitexts lead to better per-
formance. Current resources of parallel corpora
cover few language pairs and mostly come from
one domain (proceedings of the Canadian or Eu-
ropean Parliament, or of the United Nations). This
becomes specifically problematic when SMT sys-
tems trained on such corpora are used for general
translations, as the language jargon heavily used in
these corpora is not appropriate for everyday life
translations or translations in some other domain.
One option to increase this scarce resource
could be to produce more human translations, but
this is a very expensive option, in terms of both
time and money. In recent work less expensive but
very productive methods of creating such sentence
aligned bilingual corpora were proposed. These
are based on generating ?parallel? texts from al-
ready available ?almost parallel? or ?not much
parallel? texts. The term ?comparable corpus? is
often used to define such texts.
A comparable corpus is a collection of texts
composed independently in the respective lan-
guages and combined on the basis of similarity
of content (Yang and Li, 2003). The raw mate-
rial for comparable documents is often easy to ob-
tain but the alignment of individual documents is a
challenging task (Oard, 1997). Multilingual news
reporting agencies like AFP, Xinghua, Reuters,
CNN, BBC etc. serve to be reliable producers
of huge collections of such comparable corpora.
Such texts are widely available from LDC, in par-
ticular the Gigaword corpora, or over the WEB
for many languages and domains, e.g. Wikipedia.
They often contain many sentences that are rea-
sonable translations of each other, thus potential
parallel sentences to be identified and extracted.
There has been considerable amount of work on
bilingual comparable corpora to learn word trans-
lations as well as discovering parallel sentences.
Yang and Lee (2003) use an approach based on
dynamic programming to identify potential paral-
lel sentences in title pairs. Longest common sub
sequence, edit operations and match-based score
functions are subsequently used to determine con-
fidence scores. Resnik and Smith (2003) pro-
pose their STRAND web-mining based system
and show that their approach is able to find large
numbers of similar document pairs.
Works aimed at discovering parallel sentences
16
French: Au total, 1,634 million d?e?lecteurs doivent de?signer les 90 de?pute?s de la prochaine le?gislature
parmi 1.390 candidats pre?sente?s par 17 partis, dont huit sont repre?sente?s au parlement.
Query: In total, 1,634 million voters will designate the 90 members of the next parliament among 1.390
candidates presented by 17 parties, eight of which are represented in parliament.
Result: Some 1.6 million voters were registered to elect the 90 members of the legislature from 1,390
candidates from 17 parties, eight of which are represented in parliament, several civilian organisations
and independent lists.
French: ?Notre implication en Irak rend possible que d?autres pays membres de l?Otan, comme
l?Allemagne par exemple, envoient un plus gros contingent? en Afghanistan, a estime? M.Belka au cours
d?une confe?rence de presse.
Query: ?Our involvement in Iraq makes it possible that other countries members of NATO, such
as Germany, for example, send a larger contingent in Afghanistan, ?said Mr.Belka during a press
conference.
Result: ?Our involvement in Iraq makes it possible for other NATO members, like Germany for
example, to send troops, to send a bigger contingent to your country, ?Belka said at a press conference,
with Afghan President Hamid Karzai.
French: De son co?te?, Mme Nicola Duckworth, directrice d?Amnesty International pour l?Europe et
l?Asie centrale, a de?clare? que les ONG demanderaient a` M.Poutine de mettre fin aux violations des
droits de l?Homme dans le Caucase du nord.
Query: For its part, Mrs Nicole Duckworth, director of Amnesty International for Europe and Central
Asia, said that NGOs were asking Mr Putin to put an end to human rights violations in the northern
Caucasus.
Result: Nicola Duckworth, head of Amnesty International?s Europe and Central Asia department, said
the non-governmental organisations (NGOs) would call on Putin to put an end to human rights abuses
in the North Caucasus, including the war-torn province of Chechnya.
Figure 1: Some examples of a French source sentence, the SMT translation used as query and the poten-
tial parallel sentence as determined by information retrieval. Bold parts are the extra tails at the end of
the sentences which we automatically removed.
include (Utiyama and Isahara, 2003), who use
cross-language information retrieval techniques
and dynamic programming to extract sentences
from an English-Japanese comparable corpus.
They identify similar article pairs, and then, treat-
ing these pairs as parallel texts, align their sen-
tences on a sentence pair similarity score and use
DP to find the least-cost alignment over the doc-
ument pair. Fung and Cheung (2004) approach
the problem by using a cosine similarity measure
to match foreign and English documents. They
work on ?very non-parallel corpora?. They then
generate all possible sentence pairs and select the
best ones based on a threshold on cosine simi-
larity scores. Using the extracted sentences they
learn a dictionary and iterate over with more sen-
tence pairs. Recent work by Munteanu and Marcu
(2005) uses a bilingual lexicon to translate some
of the words of the source sentence. These trans-
lations are then used to query the database to find
matching translations using information retrieval
(IR) techniques. Candidate sentences are deter-
mined based on word overlap and the decision
whether a sentence pair is parallel or not is per-
formed by a maximum entropy classifier trained
on parallel sentences. Bootstrapping is used and
the size of the learned bilingual dictionary is in-
creased over iterations to get better results.
Our technique is similar to that of (Munteanu
and Marcu, 2005) but we bypass the need of the
bilingual dictionary by using proper SMT transla-
tions and instead of a maximum entropy classifier
we use simple measures like the word error rate
(WER) and the translation error rate (TER) to de-
cide whether sentences are parallel or not. Using
the full SMT sentences, we get an added advan-
tage of being able to detect one of the major errors
of this technique, also identified by (Munteanu and
Marcu, 2005), i.e, the cases where the initial sen-
tences are identical but the retrieved sentence has
17
a tail of extra words at sentence end. We try to
counter this problem as detailed in 4.1.
We apply this technique to create a parallel cor-
pus for the French/English language pair using the
LDC Gigaword comparable corpus. We show that
we achieve significant improvements in the BLEU
score by adding our extracted corpus to the already
available human-translated corpora.
This paper is organized as follows. In the next
section we first describe the baseline SMT system
trained on human-provided translations only. We
then proceed by explaining our parallel sentence
selection scheme and the post-processing. Sec-
tion 4 summarizes our experimental results and
the paper concludes with a discussion and perspec-
tives of this work.
2 Baseline SMT system
The goal of SMT is to produce a target sentence
e from a source sentence f . Among all possible
target language sentences the one with the highest
probability is chosen:
e? = arg max
e
Pr(e|f) (1)
= arg max
e
Pr(f |e) Pr(e) (2)
where Pr(f |e) is the translation model and
Pr(e) is the target language model (LM). This ap-
proach is usually referred to as the noisy source-
channel approach in SMT (Brown et al, 1993).
Bilingual corpora are needed to train the transla-
tion model and monolingual texts to train the tar-
get language model.
It is today common practice to use phrases as
translation units (Koehn et al, 2003; Och and
Ney, 2003) instead of the original word-based ap-
proach. A phrase is defined as a group of source
words f? that should be translated together into a
group of target words e?. The translation model in
phrase-based systems includes the phrase transla-
tion probabilities in both directions, i.e. P (e?|f?)
and P (f? |e?). The use of a maximum entropy ap-
proach simplifies the introduction of several addi-
tional models explaining the translation process :
e? = arg maxPr(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (3)
The feature functions hi are the system mod-
els and the ?i weights are typically optimized to
maximize a scoring function on a development
SMT baseline
system
phrase
table
3.3G
4?gram
LM
Fr En
automatic
translations
En
words
words
275M
up to
Fr En
human translations
words
116M
up to
Figure 2: Using an SMT system used to translate
large amounts of monolingual data.
set (Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty, and a target language
model.
The system is based on the Moses SMT
toolkit (Koehn et al, 2007) and constructed as fol-
lows. First, Giza++ is used to perform word align-
ments in both directions. Second, phrases and
lexical reorderings are extracted using the default
settings of the Moses SMT toolkit. The 4-gram
back-off target LM is trained on the English part
of the bitexts and the Gigaword corpus of about
3.2 billion words. Therefore, it is likely that the
target language model includes at least some of
the translations of the French Gigaword corpus.
We argue that this is a key factor to obtain good
quality translations. The translation model was
trained on the news-commentary corpus (1.56M
words)1 and a bilingual dictionary of about 500k
entries.2 This system uses only a limited amount
of human-translated parallel texts, in comparison
to the bitexts that are available in NIST evalua-
tions. In a different versions of this system, the
Europarl (40M words) and the Canadian Hansard
corpus (72M words) were added.
In the framework of the EuroMatrix project, a
test set of general news data was provided for the
shared translation task of the third workshop on
1Available at http://www.statmt.org/wmt08/
shared-task.html
2The different conjugations of a verb and the singular and
plural form of adjectives and nouns are counted as multiple
entries.
18
EN
SMT
FR
used as queries
per day articles
candidate sentence pairs parallel 
sentences
+?5 day articles
from English Gigaword
English
translations Gigaword
French
174M words
133M words
tail
removal
sentences with
extra words at ends
+
24.3M words
parallel 
number / table
comparison
      length  
removing
WER/TER
26.8M words
Figure 3: Architecture of the parallel sentence extraction system.
SMT (Callison-Burch et al, 2008), called new-
stest2008 in the following. The size of this cor-
pus amounts to 2051 lines and about 44 thousand
words. This data was randomly split into two parts
for development and testing. Note that only one
reference translation is available. We also noticed
several spelling errors in the French source texts,
mainly missing accents. These were mostly auto-
matically corrected using the Linux spell checker.
This increased the BLEU score by about 1 BLEU
point in comparison to the results reported in the
official evaluation (Callison-Burch et al, 2008).
The system tuned on this development data is used
translate large amounts of text of French Gigaword
corpus (see Figure 2). These translations will be
then used to detect potential parallel sentences in
the English Gigaword corpus.
3 System Architecture
The general architecture of our parallel sentence
extraction system is shown in figure 3. Start-
ing from comparable corpora for the two lan-
guages, French and English, we propose to trans-
late French to English using an SMT system as de-
scribed above. These translated texts are then used
to perform information retrieval from the English
corpus, followed by simple metrics like WER and
TER to filter out good sentence pairs and even-
tually generate a parallel corpus. We show that a
parallel corpus obtained using this technique helps
considerably to improve an SMT system.
We shall also be trying to answer the following
question over the course of this study: do we need
to use the best possible SMT systems to be able to
retrieve the correct parallel sentences or any ordi-
nary SMT system will serve the purpose ?
3.1 System for Extracting Parallel Sentences
from Comparable Corpora
LDC provides large collections of texts from mul-
tilingual news reporting agencies. We identified
agencies that provided news feeds for the lan-
guages of our interest and chose AFP for our
study.3
We start by translating the French AFP texts to
English using the SMT systems discussed in sec-
tion 2. In our experiments we considered only
the most recent texts (2002-2006, 5.5M sentences;
about 217M French words). These translations are
then treated as queries for the IR process. The de-
sign of our sentence extraction process is based on
the heuristic that considering the corpus at hand,
we can safely say that a news item reported on
day X in the French corpus will be most proba-
bly found in the day X-5 and day X+5 time pe-
riod. We experimented with several window sizes
and found the window size of ?5 days to be the
most accurate in terms of time and the quality of
the retrieved sentences.
Using the ID and date information for each sen-
tence of both corpora, we first collect all sentences
from the SMT translations corresponding to the
same day (query sentences) and then the corre-
sponding articles from the English Gigaword cor-
3LDC corpora LDC2007T07 (English) and LDC2006T17
(French).
19
pus (search space for IR). These day-specific files
are then used for information retrieval using a ro-
bust information retrieval system. The Lemur IR
toolkit (Ogilvie and Callan, 2001) was used for
sentence extraction. The top 5 scoring sentences
are returned by the IR process. We found no evi-
dence that retrieving more than 5 top scoring sen-
tences helped get better sentences. At the end of
this step, we have for each query sentence 5 po-
tentially matching sentences as per the IR score.
The information retrieval step is the most time
consuming task in the whole system. The time
taken depends upon various factors like size of the
index to search in, length of the query sentence
etc. To give a time estimate, using a ?5 day win-
dow required 9 seconds per query vs 15 seconds
per query when a ?7 day window was used. The
number of results retrieved per sentence also had
an impact on retrieval time with 20 results tak-
ing 19 seconds per query, whereas 5 results taking
9 seconds per query. Query length also affected
the speed of the sentence extraction process. But
with the problem at we could differentiate among
important and unimportant words as nouns, verbs
and sometimes even numbers (year, date) could be
the keywords. We, however did place a limit of
approximately 90 words on the queries and the in-
dexed sentences. This choice was motivated by the
fact that the word alignment toolkit Giza++ does
not process longer sentences.
A Krovetz stemmer was used while building the
index as provided by the toolkit. English stop
words, i.e. frequently used words, such as ?a? or
?the?, are normally not indexed because they are
so common that they are not useful to query on.
The stop word list provided by the IR Group of
University of Glasgow4 was used.
The resources required by our system are min-
imal : translations of one side of the comparable
corpus. We will be showing later in section 4.2
of this paper that with an SMT system trained on
small amounts of human-translated data we can
?retrieve? potentially good parallel sentences.
3.2 Candidate Sentence Pair Selection
Once we have the results from information re-
trieval, we proceed on to decide whether sentences
are parallel or not. At this stage we choose the
best scoring sentence as determined by the toolkit
4http://ir.dcs.gla.ac.uk/resources/
linguistic utils/stop words
and pass the sentence pair through further filters.
Gale and Church (1993) based their align program
on the fact that longer sentences in one language
tend to be translated into longer sentences in the
other language, and that shorter sentences tend to
be translated into shorter sentences. We also use
the same logic in our initial selection of the sen-
tence pairs. A sentence pair is selected for fur-
ther processing if the length ratio is not more than
1.6. A relaxed factor of 1.6 was chosen keeping
in consideration the fact that French sentences are
longer than their respective English translations.
Finally, we discarded all sentences that contain a
large fraction of numbers. Typically, those are ta-
bles of sport results that do not carry useful infor-
mation to train an SMT.
Sentences pairs conforming to the previous cri-
teria are then judged based on WER (Levenshtein
distance) and translation error rate (TER). WER
measures the number of operations required to
transform one sentence into the other (insertions,
deletions and substitutions). A zero WER would
mean the two sentences are identical, subsequently
lower WER sentence pairs would be sharing most
of the common words. However two correct trans-
lations may differ in the order in which the words
appear, something that WER is incapable of tak-
ing into account as it works on word to word ba-
sis. This shortcoming is addressed by TER which
allows block movements of words and thus takes
into account the reorderings of words and phrases
in translation (Snover et al, 2006). We used both
WER and TER to choose the most suitable sen-
tence pairs.
4 Experimental evaluation
Our main goal was to be able to create an addi-
tional parallel corpus to improve machine transla-
tion quality, especially for the domains where we
have less or no parallel data available. In this sec-
tion we report the results of adding these extracted
parallel sentences to the already available human-
translated parallel sentences.
We conducted a range of experiments by adding
our extracted corpus to various combinations of al-
ready available human-translated parallel corpora.
We experimented with WER and TER as filters to
select the best scoring sentences. Generally, sen-
tences selected based on TER filter showed better
BLEU and TER scores than their WER counter
parts. So we chose TER filter as standard for
20
 18.5
 19
 19.5
 20
 20.5
 21
 21.5
 22
 0  2  4  6  8  10  12  14  16
B
LE
U
 sc
or
e
French words for training [M]
newsbitexts only
TER filter
 WER 
Figure 4: BLEU scores on the Test data using an
WER or TER filter.
our experiments with limited amounts of human
translated corpus. Figure 4 shows this WER vs
TER comparison based on BLEU and TER scores
on the test data in function of the size of train-
ing data. These experiments were performed with
only 1.56M words of human-provided translations
(news-commentary corpus).
4.1 Improvement by sentence tail removal
Two main classes of errors common in such
tasks: firstly, cases where the two sentences share
many common words but actually convey differ-
ent meaning, and secondly, cases where the two
sentences are (exactly) parallel except at sentence
ends where one sentence has more information
than the other. This second case of errors can be
detected using WER as we have both the sentences
in English. We detected the extra insertions at the
end of the IR result sentence and removed them.
Some examples of such sentences along with tails
detected and removed are shown in figure 1. This
resulted in an improvement in the SMT scores as
shown in table 1.
This technique worked perfectly for sentences
having TER greater than 30%. Evidently these
are the sentences which have longer tails which
result in a lower TER score and removing them
improves performance significantly. Removing
sentence tails evidently improved the scores espe-
cially for larger data, for example for the data size
of 12.5M we see an improvement of 0.65 and 0.98
BLEU points on dev and test data respectively and
1.00 TER points on test data (last line table 1).
The best BLEU score on the development data
is obtained when adding 9.4M words of automat-
ically aligned bitexts (11M in total). This corre-
Limit Word BLEU BLEU TER
TER tail Words Dev Test Test
filter removal (M) data data data
0 1.56 19.41 19.53 63.17
10 no 1.58 19.62 19.59 63.11yes 19.56 19.51 63.24
20 no 1.7 19.76 19.89 62.49yes 19.81 19.75 62.80
30 no 2.1 20.29 20.32 62.16yes 20.16 20.22 62.02
40 no 3.5 20.93 20.81 61.80yes 21.23 21.04 61.49
45 no 4.9 20.98 20.90 62.18yes 21.39 21.49 60.90
50 no 6.4 21.12 21.07 61.31yes 21.70 21.70 60.69
55 no 7.8 21.30 21.15 61.23yes 21.90 21.78 60.41
60 no 9.8 21.42 20.97 61.46yes 21.96 21.79 60.33
65 no 11 21.34 21.20 61.02yes 22.29 21.99 60.10
70 no 12.2 21.21 20.84 61.24yes 21.86 21.82 60.24
Table 1: Effect on BLEU score of removing extra
sentence tails from otherwise parallel sentences.
sponds to an increase of about 2.88 points BLEU
on the development set and an increase of 2.46
BLEU points on the test set (19.53 ? 21.99) as
shown in table 2, first two lines. The TER de-
creased by 3.07%.
Adding the dictionary improves the baseline
system (second line in Table 2), but it is not nec-
essary any more once we have the automatically
extracted data.
Having had very promising results with our pre-
vious experiments, we proceeded onto experimen-
tation with larger human-translated data sets. We
added our extracted corpus to the collection of
News-commentary (1.56M) and Europarl (40.1M)
bitexts. The corresponding SMT experiments
yield an improvement of about 0.2 BLEU points
on the Dev and Test set respectively (see table 2).
4.2 Effect of SMT quality
Our motivation for this approach was to be able
to improve SMT performance by ?creating? paral-
lel texts for domains which do not have enough
or any parallel corpora. Therefore only the news-
21
total BLEU score TER
Bitexts words Dev Test Test
News 1.56M 19.41 19.53 63.17
News+Extracted 11M 22.29 21.99 60.10
News+dict 2.4M 20.44 20.18 61.16
News+dict+Extracted 13.9M 22.40 21.98 60.11
News+Eparl+dict 43.3M 22.27 22.35 59.81
News+Eparl+dict+Extracted 51.3M 22.47 22.56 59.83
Table 2: Summary of BLEU scores for the best systems on the Dev-data with the news-commentary
corpus and the bilingual dictionary.
 19
 19.5
 20
 20.5
 21
 21.5
 22
 22.5
 2  4  6  8  10  12  14
B
LE
U
 sc
or
e
French words for training [M]
news + extractedbitexts only
dev
 test
Figure 5: BLEU scores when using news-
commentary bitexts and our extracted bitexts fil-
tered using TER.
commentary bitext and the bilingual dictionary
were used to train an SMT system that produced
the queries for information retrieval. To investi-
gate the impact of the SMT quality on our sys-
tem, we built another SMT system trained on large
amounts of human-translated corpora (116M), as
detailed in section 2. Parallel sentence extrac-
tion was done using the translations performed by
this big SMT system as IR queries. We found
no experimental evidence that the improved au-
tomatic translations yielded better alignments of
the comaprable corpus. It is however interesting to
note that we achieve almost the same performance
when we add 9.4M words of autoamticallly ex-
tracted sentence as with 40M of human-provided
(out-of domain) translations (second versus fifth
line in Table 2).
5 Conclusion and discussion
Sentence aligned parallel corpora are essential for
any SMT system. The amount of in-domain paral-
lel corpus available accounts for the quality of the
translations. Not having enough or having no in-
domain corpus usually results in bad translations
for that domain. This need for parallel corpora,
has made the researchers employ new techniques
and methods in an attempt to reduce the dire need
of this crucial resource of the SMT systems. Our
study also contributes in this regard by employing
an SMT itself and information retrieval techniques
to produce additional parallel corpora from easily
available comparable corpora.
We use automatic translations of comparable
corpus of one language (source) to find the cor-
responding parallel sentence from the comparable
corpus in the other language (target). We only
used a limited amount of human-provided bilin-
gual resources. Starting with about a total 2.6M
words of sentence aligned bilingual data and a
bilingual dictionary, large amounts of monolin-
gual data are translated. These translations are
then employed to find the corresponding match-
ing sentences in the target side corpus, using infor-
mation retrieval methods. Simple filters are used
to determine whether the retrieved sentences are
parallel or not. By adding these retrieved par-
allel sentences to already available human trans-
lated parallel corpora we were able to improve the
BLEU score on the test set by almost 2.5 points.
Almost one point BLEU of this improvement was
obtained by removing additional words at the end
of the aligned sentences in the target language.
Contrary to the previous approaches as in
(Munteanu and Marcu, 2005) which used small
amounts of in-domain parallel corpus as an initial
resource, our system exploits the target language
side of the comparable corpus to attain the same
goal, thus the comparable corpus itself helps to
better extract possible parallel sentences. The Gi-
gaword comparable corpora were used in this pa-
per, but the same approach can be extended to ex-
22
tract parallel sentences from huge amounts of cor-
pora available on the web by identifying compara-
ble articles using techniques such as (Yang and Li,
2003) and (Resnik and Y, 2003).
This technique is particularly useful for lan-
guage pairs for which very little parallel corpora
exist. Other probable sources of comparable cor-
pora to be exploited include multilingual ency-
clopedias like Wikipedia, encyclopedia Encarta
etc. There also exist domain specific compara-
ble corpora (which are probably potentially par-
allel), like the documentations that are done in the
national/regional language as well as English, or
the translations of many English research papers in
French or some other language used for academic
proposes.
We are currently working on several extensions
of the procedure described in this paper. We will
investigate whether the same findings hold for
other tasks and language pairs, in particular trans-
lating from Arabic to English, and we will try to
compare our approach with the work of Munteanu
and Marcu (2005). The simple filters that we are
currently using seem to be effective, but we will
also test other criteria than the WER and TER. Fi-
nally, another interesting direction is to iterate the
process. The extracted additional bitexts could be
used to build an SMT system that is better opti-
mized on the Gigaword corpus, to translate again
all the sentence from French to English, to per-
form IR and the filtering and to extract new, po-
tentially improved, parallel texts. Starting with
some million words of bitexts, this process may
allow to build at the end an SMT system that
achieves the same performance than we obtained
using about 40M words of human-translated bi-
texts (news-commentary + Europarl).
6 Acknowledgments
This work was partially supported by the Higher
Education Commission, Pakistan through the
HEC Overseas Scholarship 2005 and the French
Government under the project INSTAR (ANR
JCJC06 143038). Some of the baseline SMT sys-
tems used in this work were developed in a coop-
eration between the University of Le Mans and the
company SYSTRAN.
References
P. Brown, S. Della Pietra, Vincent J. Della Pietra, and
R. Mercer. 1993. The mathematics of statisti-
cal machine translation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. 2008.
Further meta-evaluation of machine translation. In
Third Workshop on SMT, pages 70?106.
Pascale Fung and Percy Cheung. 2004. Mining very-
non-parallel corpora: Parallel sentence and lexicon
extraction via bootstrapping and em. In Dekang
Lin and Dekai Wu, editors, EMNLP, pages 57?63,
Barcelona, Spain, July. Association for Computa-
tional Linguistics.
William A. Gale and Kenneth W. Church. 1993. A
program for aligning sentences in bilingual corpora.
Computational Linguistics, 19(1):75?102.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Dragos Stefan Munteanu and Daniel Marcu. 2005. Im-
proving machine translation performance by exploit-
ing non-parallel corpora. Computational Linguis-
tics, 31(4):477?504.
Douglas W. Oard. 1997. Alternative approaches for
cross-language text retrieval. In In AAAI Sympo-
sium on Cross-Language Text and Speech Retrieval.
American Association for Artificial Intelligence.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Philip Resnik and Noah A. Smith Y. 2003. The web
as a parallel corpus. Computational Linguistics,
29:349?380.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In ACL.
Masao Utiyama and Hitoshi Isahara. 2003. Reliable
measures for aligning Japanese-English news arti-
cles and sentences. In Erhard Hinrichs and Dan
Roth, editors, ACL, pages 72?79.
Christopher C. Yang and Kar Wing Li. 2003. Auto-
matic construction of English/Chinese parallel cor-
pora. J. Am. Soc. Inf. Sci. Technol., 54(8):730?742.
23
Proceedings of the Third Workshop on Statistical Machine Translation, pages 119?122,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
First Steps towards a general purpose French/English
Statistical Machine Translation System
Holger Schwenk
LIUM, University of Le Mans
72085 Le Mans cedex 9
FRANCE
schwenk@lium.univ-lemans.fr
Jean-Baptiste Fouet Jean Senellart
SYSTRAN SA
92044 Paris La De?fense cedex
FRANCE
fouet,senellart@systran.fr
Abstract
This paper describes an initial version of a
general purpose French/English statistical ma-
chine translation system. The main features
of this system are the open-source Moses de-
coder, the integration of a bilingual dictionary
and a continuous space target language model.
We analyze the performance of this system on
the test data of the WMT?08 evaluation.
1 Introduction
Statistical machine translation (SMT) is today con-
sidered as a serious alternative to rule-based ma-
chine translation (RBMT). While RBMT systems
rely on rules and linguistic resources built for that
purpose, SMT systems can be developed without
the need of any language knowledge and are only
based on bilingual sentence-aligned and large mono-
lingual data. However, while the monolingual data
is usually available in large amounts, bilingual texts
are a sparse resource for most of the language pairs.
The largest SMT systems are currently build for the
translation of Mandarin and Arabic to English, us-
ing more than 170M words of bitexts that are eas-
ily available from the LDC. Recent human evalua-
tions of these systems seem to indicate that they have
reached a level of performance allowing a human be-
ing to understand the automatic translations and to
answer complicated questions on its content (Jones,
2008).
In a joint project between the University of Le
Mans and the company SYSTRAN, we try to build
similar general purpose SMT systems for Euro-
pean languages. In the final version, these systems
will not only be trained on all available mono- and
bilingual data, but also will include additional re-
sources from SYSTRAN like high quality dictio-
naries, named entity transliteration and rule-based
translation of expressions like numbers and dates.
Our ultimate goal is to combine the power of data-
driven approaches and the concentrated knowledge
present in RBMT resources. In this paper, we de-
scribe an initial version of an French/English sys-
tem and analyze its performance on the test corpora
of the WMT?08 workshop.
2 Architecture of the system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . It is today common practice to use phrases
as translation units (Koehn et al, 2003; Och and
Ney, 2003) and a log linear framework in order to
introduce several models explaining the translation
process:
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models and
the ?i weights are typically optimized to maximize
a scoring function on a development set (Och and
Ney, 2002). In our system fourteen features func-
tions were used, namely phrase and lexical transla-
tion probabilities in both directions, seven features
for the lexicalized distortion model, a word and a
phrase penalty and a target language model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
119
First, Giza++ is used to perform word alignments
in both directions. Second, phrases and lexical re-
orderings are extracted using the default settings of
the Moses SMT toolkit. A 4-gram target LM is
then constructed as detailed in section 2.2. The
translation itself is performed in two passes: first,
Moses is run and a 1000-best list is generated
for each sentence. The parameters of Moses are
tuned on devtest2006 for the Europarl task and
nc-devtest2007 for the news task, using the cmert
tool. These 1000-best lists are then rescored with a
continuous space 5-gram LM and the weights of the
feature functions are optimized again using the nu-
merical optimization toolkit Condor (Berghen and
Bersini, 2005). Note that this step operates only
on the 1000-best lists, no re-decoding is performed.
This basic architecture of the system is identical to
the one used in the 2007 WMT evaluation(Schwenk,
2007a).
2.1 Translation model
In the frame work of the 2008 WMT shared
task, two parallel corpora were provided: the Eu-
roparl corpus (about 33M words) and the news-
commentary corpus (about 1.2M words). It is known
that the minutes of the debates of the European
parliament use a particular jargon and these texts
alone do not seem to be the appropriate to build a
French/English SMT system for other texts. The
more general news-commentary corpus is unfortu-
nately rather small in size. Therefore, with the
goal to build a general purpose system, we inves-
tigated whether more bilingual resources are avail-
able. Two corpora were identified: the proceedings
of the Canadian parliament, also known as Hansard
corpus (about 61M words), and data from the United
nations (105M French and 89M English words). In
the current version of our system only the Hansard
bitexts are used.
In addition to these human generated bitexts, we
investigated whether the translations of a high qual-
ity bilingual dictionary could be integrated into a
SMT system. SYSTRAN provided this resource
with more than 200 thousand entries, different forms
of a verb or genres of an noun or adjective being
counted as one entry. It is still an open research
question how to best integrate a bilingual dictionary
into a SMT system. At least two possibilities come
to mind: add the entries directly to the phrase ta-
ble or add the words and their translations to the bi-
texts. With the first solution one can be sure that the
entries are added like there are and that they won?t
suffer any deformation due to imperfect alignment
of multi-word expressions. However, it is not obvi-
ous how to obtain the phrase translation and lexical
probabilities for each new phrase. The second solu-
tion has the potential advantage that the dictionary
words could improve the alignments of these words
when they also appear in the other bitexts. The cal-
culation of the various scores of the phrase table is
simplified too, since we can use the standard phrase
extraction procedure. However, one has to be aware
that all the translations that appear only in the dictio-
nary will be equally likely which certainly does not
correspond to the reality. In future work will try to
improve these estimates using monolingual data.
For now, we used about ten thousand verbs and
hundred thousand nouns from the dictionary. For
each verb, we generated all the conjugations in the
past, present, future and conditional tense; and for
each noun the singular and plural form were gener-
ated. In total this resulted in 512k ?new sentences?
in the bitexts.
2.2 Language model
In comparison to bilingual texts which are needed
for the translation model, it is much easier to find
large quantities of monolingual data, in English as
well as in French. In this work, the following re-
sources were used for the language model:
? the monolingual parts of the Europarl, Hansard,
UN and the news commentary corpus,
? the Gigaword corpus in French and English as
provided by LDC (770M and 3261M words re-
spectively),
? about 33M words of newspaper texts crawled
from the WEB (French only)
Separate LMs were build on each data source with
the SRI LM toolkit (Stolcke, 2002) and then linearly
interpolated, optimizing the coefficients with an EM
procedure. Note that we build two sets of LMs: a
first set tuned on devtest2006, and a second one on
nc-devtest2007. The perplexities of these LMs are
120
French English
Data Eparl News Eparl News
Back-off 4-gram LM:
Eparl+news 52.6 184.0 42.0 105.8
All 50.0 136.1 39.7 85.4
Continuous space 5-gram LM:
All 42.0 118.9 34.1 75.0
Table 1: Perplexities on devtest2006 (Europarl) and
nc-devtest2007 (news commentary) for various LMs.
given in Table 1. We were not able to obtain signifi-
cantly better results with 5-gram back-off LMs.
It can be clearly seen that the additional LM data,
despite its considerable size, achieves only a small
decrease in perplexity for the Europarl data. This
task is so particular that other out-of-domain data
does not seem to be very useful. The system opti-
mized on the more general news-commentary task,
however, seems to benefit from the additional mono-
lingual resources. Note however, that the test data
newstest2008 is not of the same type and we may
have a mismatch between development and test data.
We also used a so-called continuous space lan-
guage model (CSLM). The basic idea of this ap-
proach is to project the word indices onto a contin-
uous space and to use a probability estimator oper-
ating on this space (Bengio et al, 2003). Since the
resulting probability functions are smooth functions
of the word representation, better generalization to
unknown n-grams can be expected. A neural net-
work can be used to simultaneously learn the pro-
jection of the words onto the continuous space and
to estimate the n-gram probabilities. This is still a
n-gram approach, but the LM probabilities are ?in-
terpolated? for any possible context of length n-1
instead of backing-off to shorter contexts. This ap-
proach was successfully used in large vocabulary
continuous speech recognition (Schwenk, 2007b)
and in a phrase-based SMT systems (Schwenk et al,
2006; De?chelotte et al, 2007). Here, it is the first
time trained on large amounts of data, more than 3G
words for the English LM. This approach achieves
an average perplexity reduction of almost 14% rela-
tive (see Table 1).
3 Experimental Evaluation
The shared evaluation task of the third workshop
on statistical machine translation features two dif-
ferent test sets: test2008 and newstest2008. The
first one contains data from the European parlia-
ment of the same type than the provided training and
development data. Therefore good generalization
performance can be expected. The second test set,
however, is news type data from unknown sources.
Scanning some of the sentences after the evaluation
seems to indicate that this data is more general than
the provided news-commentary training and devel-
opment data ? it contains for instance financial and
public health news.
Given the particular jargon of the European par-
liament, we decided to build two different systems,
one rather general system tuned in nc-devtest2007
and an Europarl system tuned on devtest2006. Both
systems use the tokenization proposed by the Moses
SMT toolkit and the case was preserved in the trans-
lation and language model. Therefore, in contrast to
the official BLEU scores, we report here case sensi-
tive BLEU scores as calculated by the NIST tool.
3.1 Europarl system
The results of the Europarl system are summarized
in Table 2. The translation model was trained on
the Europarl and the news-commentary data, aug-
mented by parts of the dictionary. The LM was
trained on all the data, but the additional out-of-
domain data has probably little impact given the
small improvements in perplexity (see Table 1).
French/English English/French
Model 2007 2008 2007 2008
baseline 32.64 32.61 31.15 31.80
base+CSLM 32.98 33.08 31.63 32.37
base+dict 32.69 32.75 30.97 31.59
+CSLM 33.11 33.13 31.54 32.34
Table 2: Case sensitive BLEU scores for the Europarl
system (test data)
When translating from French to English the
CSLM achieves a improvement of about 0.4 points
BLEU. Adding the dictionary had no significant im-
pact, probably due to the jargon of the parliament
proceedings. For the opposite translation direction,
121
the dictionary even seems to worsen the perfor-
mance. One reason for this observation could be the
fact that the dictionary adds many French transla-
tions for one English word. These translation are
not correctly weighted and we have to rely com-
pletely on the target LM to select the correct one.
This may explain the large improvement achieved
by the CSLM in this case (+0.75 BLEU).
3.2 News system
The results of the more generic news system are
summarized in Table 3. The translation model
was trained on the news-commentary, Europarl and
Hansard bitexts as well as parts of the dictionary.
The LM was again trained on all data.
French/English English/French
Model/bitexts 2007 2008 2007 2008
news 29.31 17.98 28.60 17.51
news+dict 30.09 18.78 28.92 18.01
news+eparl 30.53 20.39 28.55 19.70
+dict 30.94 20.63 28.46 19.96
+Hansard 31.48 21.10 28.97 20.21
+CSLM 31.98 21.02 29.64 20.51
Table 3: Case sensitive BLEU scores of the news system
(nc-test2007 and newstest2008)
First of all, we realize that the BLEU scores on
the out-of-domain generic 2008 news data are much
lower than on the nc-test2007 data. Adding more
than 60M words of the Hansard bitexts gives an im-
provement of the BLEU score of about 0.5 for most
of the test sets and translation directions. The dictio-
nary is very interesting when only a limited amount
of resources is available ? a gain of up to 0.8 BLEU
when only the news-commentary bitexts are used ?
but still useful when more data is available. As far
as we know, this is the first time that adding a dic-
tionary improved the translation quality of a very
strong baseline. In previous works, results were only
reported in a setting with limited resources (Vogel et
al., 2003; Popovic? and Ney, 2006). However, we be-
lieve that he integration of the dictionary is not yet
optimal, in particular with respect to the estimation
of the translation probabilities. The only surprising
result is the bad performance of the CSLM on the
newstest2008 data for the translation from French to
English. We are currently investigating this.
This work has been partially funded by the
French Government under the project INSTAR (ANR
JCJC06 143038).
References
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3(2):1137?1155.
Frank Vanden Berghen and Hugues Bersini. 2005. CON-
DOR, a new parallel, constrained extension of powell?s
UOBYQA algorithm: Experimental results and com-
parison with the DFO algorithm. Journal of Computa-
tional and Applied Mathematics, 181:157?175.
Daniel De?chelotte, Holger Schwenk, He?le`ne Bonneau-
Maynard, Alexandre Allauzen, and Gilles Adda.
2007. A state-of-the-art statistical machine translation
system based on moses. In MT Summit, pages 127?
133.
D. Jones. 2008. DLPT* MT comprehension test re-
sults, Oral presentation at the 2008 Nist MT Evalua-
tion workshop, March 27.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demonstra-
tion session.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Maja Popovic? and Hermann Ney. 2006. Statistical
machine translation with a small amount of bilingual
training data. In LREC workshop on Minority Lan-
guage, pages 25?29.
Holger Schwenk, Marta R. Costa-jussa`, and Jose? A. R.
Fonollosa. 2006. Continuous space language models
for the IWSLT 2006 task. In IWSLT, pages 166?173,
November.
Holger Schwenk. 2007a. Building a statistical machine
translation system for French using the Europarl cor-
pus. In Second Workshop on SMT, pages 189?192.
Holger Schwenk. 2007b. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Andreas Stolcke. 2002. SRILM - an extensible language
modeling toolkit. In ICSLP, pages II: 901?904.
Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-
ble, Ashish Venugopal, Bing Zhao, and Alex Waibel.
2003. The CMU statistical machine translation sys-
tem. In MT Summit, pages 402?409.
122
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 130?134,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
SMT and SPE Machine Translation Systems for WMT?09
Holger Schwenk and Sadaf Abdul-Rauf and Lo??c Barrault
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
schwenk,abdul,barrault@lium.univ-lemans.fr
Jean Senellart
SYSTRAN SA
92044 Paris La De?fense cedex, FRANCE
senellart@systran.fr
Abstract
This paper describes the development of
several machine translation systems for
the 2009 WMT shared task evaluation.
We only consider the translation between
French and English. We describe a sta-
tistical system based on the Moses de-
coder and a statistical post-editing sys-
tem using SYSTRAN?s rule-based system.
We also investigated techniques to auto-
matically extract additional bilingual texts
from comparable corpora.
1 Introduction
This paper describes the machine translation sys-
tems developed by the Computer Science labo-
ratory at the University of Le Mans (LIUM) for
the 2009 WMT shared task evaluation. This work
was performed in cooperation with the company
SYSTRAN. We only consider the translation be-
tween French and English (in both directions).
The main differences to the previous year?s system
(Schwenk et al, 2008) are as follows: better us-
age of SYSTRAN?s bilingual dictionary in the sta-
tistical system, less bilingual training data, addi-
tional language model training data (news-train08
as distributed by the organizers), usage of com-
parable corpora to improve the translation model,
and development of a statistical post-editing sys-
tem (SPE). These different components are de-
scribed in the following.
2 Used Resources
In the frame work of the 2009 WMT shared trans-
lation task many resources were made available.
The following sections describe how they were
used to train the translation and language models
of the systems.
2.1 Bilingual data
The latest version of the French/English Europarl
and news-commentary corpus were used. We re-
alized that the first corpus contains parts with for-
eign languages. About 1200 such lines were ex-
cluded.1 Additional bilingual corpora were avail-
able, namely the Canadian Hansard corpus (about
68M English words) and an UN corpus (about
198M English words). In several initial exper-
iments, we found no evidence that adding this
data improves the overall system and they were
not used in the final system, in order to keep
the phrase-table small. We also performed ex-
periments with the provided so-called bilingual
French/English Gigaword corpus (575M English
words in release 3). Again, we were not able
to achieve any improvement by adding this data
to the training material of the translation model.
These findings are somehow surprising since it
was eventually believed by the community that
adding large amounts of bitexts should improve
the translation model, as it is usually observed for
the language model (Brants et al, 2007).
In addition to these human generated bitexts,
we also integrated a high quality bilingual dictio-
nary from SYSTRAN. The entries of the dictio-
nary were directly added to the bitexts. This tech-
nique has the potential advantage that the dictio-
nary words could improve the alignments of these
words when they also appear in the other bitexts.
However, it is not guaranteed that multi-word ex-
pressions will be correctly aligned by GIZA++
and that only meaningful translations will actually
appear in the phrase-table. A typical example is
fire engine ? camion de pompiers, for which the
individual constituent words are not good trans-
lations of each other. The use of a dictionary to
improve an SMT system was also investigated by
1Lines 580934?581316 and 599839?600662.
130
EN
SMT
FR
used as queries
per day articles
candidate sentence pairs parallel 
sentences
+?5 day articles
from English Gigaword
English
translations Gigaword
French
174M words
133M words
tail
removal
sentences with
extra words at ends
+
9.3M words
parallel 
number / table
comparison
      length  
removing
WER
10.3M words
Figure 1: Architecture of the parallel sentence extraction system (Rauf and Schwenk, 2009).
(Brown et al, 1993).
In comparison to our previous work (Schwenk
et al, 2008), we also included all verbs in the
French subjonctif and passe? simple tense. In fact,
those tenses seem to be frequently used in news
material. In total about 10,000 verbs, 1,500 adjec-
tives/adverbs and more than 100,000 noun forms
were added.
2.2 Use of Comparable corpora
Available human translated bitexts such as the UN
and the Hansard corpus seem to be out-of domain
for this task, as mentioned above. Therefore, we
investigated a new method to automatically extract
and align parallel sentences from comparable in-
domain corpora. In this work we used the AFP
news texts since there are available in the French
and English LDC Gigaword corpora.
The general architecture of our parallel sentence
extraction system is shown in figure 1. We first
translate 174M words from French into English
using an SMT system. These English sentences
are then used to search for translations in the En-
glish AFP texts of the Gigaword corpus using in-
formation retrieval techniques. The Lemur toolkit
(Ogilvie and Callan, 2001) was used for this pur-
pose. Search was limited to a window of ?5 days
of the date of the French news text. The retrieved
candidate sentences were then filtered using the
word error rate with respect to the automatic trans-
lations. In this study, sentences with an error rate
below 32% were kept. Sentences with a large
length difference (French versus English) or con-
taining a large fraction of numbers were also dis-
carded. By these means, about 9M words of ad-
ditional bitexts were obtained. An improved ver-
sion of this algorithm using TER instead of the
word error rate is described in detail in (Rauf and
Schwenk, 2009).
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. We
realized that the news-train08 corpora contained
some foreign texts, in particular in German. We
tried to filter those lines using simple regular ex-
pressions. We also discarded lines with a large
fraction of numerical expressions. In addition,
LDC?s Gigaword collection, the Hansard corpus
and the UN corpus were used for both languages.
Finally, about 30M words crawled from the WEB
were used for the French LM. All this data pre-
dated the evaluation period.
2.4 Development data
All development was done on news-dev2009a and
news-dev2009b was used as internal test set. The
default Moses tokenization was used. All our
models are case sensitive and include punctuation.
The BLEU scores reported in this paper were cal-
culated with the NIST tool and are case sensitive.
3 Language Modeling
Language modeling plays an important role in
SMT systems. 4-gram back-off language models
(LM) were used in all our systems. The word list
contains all the words of the bitext used to train
the translation model and all words that appear at
least ten times in the news-train08 corpus. Sep-
arate LMs were build on each data source with
the SRI LM toolkit (Stolcke, 2002) and then lin-
early interpolated, optimizing the coefficients with
an EM procedure. The perplexities of these LMs
131
Corpus # Fr words Dev09a Dev09b Test09
SMT system
Eparl+NC 46.5M 22.44 22.38 25.60
Eparl+NC+dict 48.5M 22.60 22.55 26.01
Eparl+NC+dict+AFP 57.8M 22.82 22.63? 26.18
SPE system
SYSTRAN - 17.76 18.13 19.98
Eparl+NC 45.5M 22.84 22.59# 25.59
Eparl+NC+AFP 54.4M 22.72 21.96 25.40
Table 1: Case sensitive NIST BLEU scores for the French-English systems. ?NC? denotes the news-
commentary bitexts, ?dict? SYSTRAN?s bilingual dictionary and ?AFP? the automatically aligned news
texts (?=primary, #=contrastive system)
are given in Table 2. Adding the new news-train08
monolingual data had an important impact on the
quality of the LM, even when the Gigaword data
is already included.
Data French English
Vocabulary size 407k 299k
Eparl+news 248.8 416.7
+ LDC Gigaword 142.2 194.9
+ Hansard and UN 137.5 187.5
news-train08 alone 165.0 245.9
all 120.6 174.8
Table 2: Perplexities on the development data of
various language models.
4 Architecture of the SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source
sentence f . It is today common practice to use
phrases as translation units (Koehn et al, 2003;
Och and Ney, 2003) and a log linear framework in
order to introduce several models explaining the
translation process:
e? = arg max p(e|f)
= arg max
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to max-
imize a scoring function on a development set
(Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).2 This speeds
up the process and corrects an error of GIZA++
that can appear with rare words. This previously
caused problems when adding the entries of the
bilingual dictionary to the bitexts.
Phrases and lexical reorderings are extracted us-
ing the default settings of the Moses toolkit. The
parameters of Moses are tuned on news-dev2009a,
using the cmert tool. The basic architecture of
the system is identical to the one used in the
2008 WMT evaluation (Schwenk et al, 2008),
but we did not use two pass decoding and n-best
list rescoring with a continuous space language
model.
The results of the SMT systems are summarized
in the upper part of Table 1 and 3. The dictionary
and the additional automatically produced AFP bi-
texts achieved small improvements when translat-
ing from French to English. In the opposite trans-
lation direction, the systems that include the addi-
tional AFP texts exhibit a bad generalisation be-
havior. We provide also the performance of the
different systems on the official test set, calculated
after the evaluation. In most of the cases, the ob-
served improvements carry over on the test set.
5 Architecture of the SPE system
During the last years statistical post-editing sys-
tems have shown to achieve very competitive per-
formance (Simard et al, 2007; Dugast et al,
2007). The main idea of this techniques is to use
2The source is available at http://www.cs.cmu.
edu/
?
qing/
132
Corpus # En words Dev09a Dev09b Test09
SMT system
Eparl+NC 41.6M 21.89 21.78 23.80
Eparl+NC+dict 44.0M 22.28 22.35# 24.13
Eparl+NC+dict+AFP 51.7M 22.21 21.43 23.88
SPE system
SYSTRAN - 18.68 18.84 20.29
Eparl+NC 44.2M 23.03 23.15 24.36
Eparl+NC+AFP 53.3M 22.95 23.15? 24.62
Table 3: Case sensitive NIST BLEU scores for the English-French systems. ?NC? denotes the news-
commentary bitexts, ?dict? denotes SYSTRAN?s bilingual dictionary and ?AFP? the automatically
aligned news texts (?=primary, #=contrastive system)
an SMT system to correct the errors of a rule-
based translation system. In this work, SYSTRAN
server version 6, followed by an SMT system
based on Moses were used. The post-editing sys-
tems uses exactly the same language models than
the above described stand-alone SMT systems.
The translation model was trained on the Europarl,
the news-commentary and the extracted AFP bi-
texts. The results of these SPE systems are sum-
marized in the lower part of Table 1 and 3. SYS-
TRAN?s rule-based system alone already achieves
remarkable BLEU scores although it was not op-
timized or adapted to this task. This could be sig-
nificantly improved using statistical post-editing.
The additional AFP texts were not useful when
translating form French to English, but helped to
improve the generalisation behavior for the En-
glish/French systems.
When translating from English to French (Ta-
ble 3), the SPE system is clearly better than the
carefully optimized SMT system. Consequently,
it was submitted as primary system and the SMT
system as contrastive one.
6 Conclusion and discussion
We described the development of two comple-
mentary machine translation systems for the 2009
WMT shared translation task: an SMT and an SPE
system. The last one is based on SYSTRAN?s
rule-based system. Interesting findings of this re-
search include the fact that the SPE system out-
performs the SMT system when translating into
French. This system has also obtained the best
scores in the human evaluation.
With respect to the SMT system, we were
not able to improve the translation model by
adding large amounts of bitexts, although different
sources were available (Canadian Hansard, UN
or WEB data). Eventually these corpora are too
noisy or out-of-domain. On the other hand, the
integration of a high quality bilingual dictionary
was helpful, as well as the automatic alignment of
news texts from comparable corpora.
Future work will concentrate on the integration
of previously successful techniques, in particu-
lar continuous space language models and lightly-
supervised training (Schwenk, 2008). We also be-
lieve that the tokenization could be improved, in
particular for the French sources texts. Numbers,
dates and other numerical expressions could be
translated by a rule-based system.
System combination has recently shown to pro-
vide important improvements of translation qual-
ity. We are currently working on a combination of
the SMT and SPE system. It may be also interest-
ing to add a third (hierarchical) MT system.
7 Acknowledgments
This work has been partially funded by the French
Government under the project INSTAR (ANR
JCJC06 143038) and the by the Higher Education
Commission, Pakistan through the HEC Overseas
Scholarship 2005.
133
References
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In EMNLP, pages 858?
867.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, Meredith J. Goldsmith, Jan Hajic,
Robert L. Mercer, and Surya Mohanty. 1993. But
dictionaries are data too. In Proceedings of the
workshop on Human Language Technology, pages
202?205, Princeton, New Jersey.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical post-editing on SYSTRAN?s rule-based
translation system. In Second Workshop on SMT,
pages 179?182.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In ACL, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Sadaf Abdul Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In EACL, page to be published.
Holger Schwenk, Jean-Baptiste Fouet, and Jean Senel-
lart. 2008. First steps towards a general purpose
French/English statistical machine translation sys-
tem. In Third Workshop on SMT, pages 119?122.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT, pages 182?189.
Michel Simard, Nicola Ueffing, Pierre Isabelle, and
Roland Kuhn. 2007. Rule-based translation with
statistical phrase-based post-editing. In Second
Workshop on SMT, pages 203?206.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In ICSLP, pages II: 901?
904.
134
Proceedings of the 2nd Workshop on Building and Using Comparable Corpora, ACL-IJCNLP 2009, pages 46?54,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
Exploiting Comparable Corpora with TER and TERp
Sadaf Abdul-Rauf and Holger Schwenk
LIUM, University of Le Mans, FRANCE
Sadaf.Abdul-Rauf@lium.univ-lemans.fr
Abstract
In this paper we present an extension of a
successful simple and effective method for
extracting parallel sentences from com-
parable corpora and we apply it to an
Arabic/English NIST system. We exper-
iment with a new TERp filter, along with
WER and TER filters. We also report a
comparison of our approach with that of
(Munteanu and Marcu, 2005) using ex-
actly the same corpora and show perfor-
mance gain by using much lesser data.
Our approach employs an SMT system
built from small amounts of parallel texts
to translate the source side of the non-
parallel corpus. The target side texts are
used, along with other corpora, in the lan-
guage model of this SMT system. We then
use information retrieval techniques and
simple filters to create parallel data from
a comparable news corpora. We evaluate
the quality of the extracted data by show-
ing that it significantly improves the per-
formance of an SMT systems.
1 Introduction
Parallel corpora, a requisite resource for Statistical
Machine Translation (SMT) as well as many other
natural language processing applications, remain
a sparse resource due to the huge expense (human
as well as monetary) required for their creation.
A parallel corpus, also called bitext, consists in
bilingual texts aligned at the sentence level. SMT
systems use parallel texts as training material and
monolingual corpora for target language model-
ing. Though enough monolingual data is available
for most language pairs, it is the parallel corpus
that is a sparse resource.
The performance of an SMT system heavily
depends on the parallel corpus used for train-
ing. Generally, more bitexts lead to better perfor-
mance. The existing resources of parallel corpora
cover a few language pairs and mostly come from
one domain (proceedings of the Canadian or Eu-
ropean Parliament, or of the United Nations). The
language jargon used in such corpora is not very
well suited for everyday life translations or transla-
tions of some other domain, thus a dire need arises
for more parallel corpora well suited for everyday
life and domain adapted translations.
One option to increase this scarce resource
could be to produce more human translations, but
this is a very expensive option, in terms of both
time and money. Crowd sourcing could be an-
other option, but this has its own costs and thus
is not very practical for all cases. The world
wide web can also be crawled for potential ?par-
allel sentences?, but most of the found bilingual
texts are not direct translations of each other and
not very easy to align. In recent works less ex-
pensive but very productive methods of creating
such sentence aligned bilingual corpora were pro-
posed. These are based on generating ?parallel?
texts from already available ?almost parallel? or
?not much parallel? texts. The term ?comparable
corpus? is often used to define such texts.
A comparable corpus is a collection of texts
composed independently in the respective lan-
guages and combined on the basis of similarity of
content (Yang and Li, 2003). The raw material for
comparable documents is often easy to obtain but
the alignment of individual documents is a chal-
lenging task (Oard, 1997). Potential sources of
comparable corpora are multilingual news report-
ing agencies like AFP, Xinhua, Al-Jazeera, BBC
etc, or multilingual encyclopedias like Wikipedia,
Encarta etc. Such comparable corpora are widely
available from LDC, in particular the Gigaword
corpora, or over the WEB for many languages
and domains, e.g. Wikipedia. They often contain
many sentences that are reasonable translations of
46
each other. Reliable identification of these pairs
would enable the automatic creation of large and
diverse parallel corpora.
The ease of availability of these comparable
corpora and the potential for parallel corpus as
well as dictionary creation has sparked an interest
in trying to make maximum use of these compa-
rable resources, some of these works include dic-
tionary learning and identifying word translations
(Rapp, 1995), named entity recognition (Sproat
et al, 2006), word sense disambiguation (Kaji,
2003), improving SMT performance using ex-
tracted parallel sentences (Munteanu and Marcu,
2005), (Rauf and Schwenk, 2009). There has been
considerable amount of work on bilingual compa-
rable corpora to learn word translations as well
as discovering parallel sentences. Yang and Lee
(2003) use an approach based on dynamic pro-
gramming to identify potential parallel sentences
in title pairs. Longest common sub sequence, edit
operations and match-based score functions are
subsequently used to determine confidence scores.
Resnik and Smith (2003) propose their STRAND
web-mining based system and show that their ap-
proach is able to find large numbers of similar doc-
ument pairs.
Works aimed at discovering parallel sentences
include (Utiyama and Isahara, 2003), who use
cross-language information retrieval techniques
and dynamic programming to extract sentences
from an English-Japanese comparable corpus.
They identify similar article pairs, and then, treat-
ing these pairs as parallel texts, align their sen-
tences on a sentence pair similarity score and use
DP to find the least-cost alignment over the doc-
ument pair. Fung and Cheung (2004) approach
the problem by using a cosine similarity measure
to match foreign and English documents. They
work on ?very non-parallel corpora?. They then
generate all possible sentence pairs and select the
best ones based on a threshold on cosine simi-
larity scores. Using the extracted sentences they
learn a dictionary and iterate over with more sen-
tence pairs. Recent work by Munteanu and Marcu
(2005) uses a bilingual lexicon to translate some
of the words of the source sentence. These trans-
lations are then used to query the database to find
matching translations using information retrieval
(IR) techniques. Candidate sentences are deter-
mined based on word overlap and the decision
whether a sentence pair is parallel or not is per-
formed by a maximum entropy classifier trained
on parallel sentences. Bootstrapping is used and
the size of the learned bilingual dictionary is in-
creased over iterations to get better results.
Our technique is similar to that of (Munteanu
and Marcu, 2005) but we bypass the need of the
bilingual dictionary by using proper SMT transla-
tions and instead of a maximum entropy classifier
we use simple measures like the word error rate
(WER) and the translation edit rate (TER) to de-
cide whether sentences are parallel or not. We
also report an extension of our work (Rauf and
Schwenk, 2009) by experimenting with an addi-
tional filter TERp, and building a named entity
noun dictionary using the unknown words from
the SMT (section 5.2). TERp has been tried en-
couraged by the outperformance of TER in our
previous study on French-English. We have ap-
plied our technique on a different language pair
Arabic-English, versus French-English that we re-
ported the technique earlier on. Our use of full
SMT sentences, gives us an added advantage of
being able to detect one of the major errors of
these approaches, also identified by (Munteanu
and Marcu, 2005), i.e, the cases where the initial
sentences are identical but the retrieved sentence
has a tail of extra words at sentence end. We dis-
cuss this problem as detailed in section 5.1.
We apply our technique to create a parallel cor-
pus for the Arabic/English language pair. We
show that we achieve significant improvements
in the BLEU score by adding our extracted cor-
pus to the already available human-translated cor-
pora. We also perform a comparison of the data
extracted by our approach and that by (Munteanu
and Marcu, 2005) and report the results in Sec-
tion 5.3.
This paper is organized as follows. In the next
section we first describe the baseline SMT system
trained on human-provided translations only. We
then proceed by explaining our parallel sentence
selection scheme and the post-processing. Sec-
tion 5 summarizes our experimental results and
the paper concludes with a discussion and perspec-
tives of this work.
2 Task Description
In this paper, we consider the translation from
Arabic into English, under the same conditions as
the official NIST 2008 evaluation. The used bi-
47
texts include various news wire translations1 as
well as some texts from the GALE project.2 We
also added the 2002 to 2005 test data to the paral-
lel training data (using all reference translations).
This corresponds to a total of about 8M Arabic
words. Our baseline system is trained on these bi-
texts only.
We use the 2006 NIST test data as development
data and the official NIST 2008 test data as in-
ternal test set. All case sensitive BLEU scores
are calculated with the NIST scoring tool with re-
spect to four reference translations. Both data sets
include texts from news wires as well as news-
groups.
LDC provides large collections of monolingual
data, namely the LDC Arabic and English Giga-
word corpora. There are two text sources that do
exist in Arabic and English: the AFP and XIN col-
lection. It is likely that each corpora contains sen-
tences which are translations of the other. We aim
to extract those. We have used the XIN corpus
for all of our reported results and the collection
of the AFP and XIN for comparison with ISI. Ta-
ble 1 summarizes the characteristics of the corpora
used. Note that the English part is much larger
than the Arabic one (we found the same to be the
case for French-English AFP comparable corpora
that we used in our previous study). The number
of words are given after tokenization.
Source Arabic English
AFP 138M 527M
XIN 51M 140M
Table 1: Characteristics of the available compara-
ble Gigaword corpora for the Arabic-English task
(number of words).
3 Baseline SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source sen-
tence f . It is today common practice to use phrases
as translation units (Koehn et al, 2003; Och and
Ney, 2003) and a log linear framework in order
to introduce several models explaining the transla-
tion process:
e? = argmax p(e|f)
1LDC2003T07, 2004E72, T17, T18, 2005E46 and
2006E25.
2LDC2005E83, 2006E24, E34, E85 and E92.
= argmax
e
{exp(?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to max-
imize a scoring function on a development set
(Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, Giza++ is used to perform word alignments
in both directions. Second, phrases and lexical re-
orderings are extracted using the default settings
of the Moses SMT toolkit. The target 4-gram
back-off language model is trained on the English
part of all bitexts as well as the whole English Gi-
gaword corpus.
4 System Architecture
The general architecture of our parallel sentence
extraction system is shown in figure 1. Starting
from comparable corpora for the two languages,
Arabic and English, we first translate Arabic to
English using an SMT system as described in the
above sections. These translated texts are then
used to perform information retrieval from the
English corpus, followed by simple metrics like
WER, TER or TERp to filter out good sentence
pairs and eventually generate a parallel corpus.
We show that a parallel corpus obtained using this
technique helps considerably to improve an SMT
system.
4.1 System for Extracting Parallel Sentences
from Comparable Corpora
We start by translating the Arabic XIN and AFP
texts to English using the SMT systems discussed
in section 2. In our experiments we considered
only the most recent texts (2001-2006, 1.7M sen-
tences; about 65.M Arabic words for XIN ). For
our experiments on effect on SMT quality we use
only the XIN corpus. We use the combination
of AFP and XIN for comparison of sentences ex-
tracted by our approach with that of (Munteanu
and Marcu, 2005). These translations are then
treated as queries for the IR process. The design
of our sentence extraction process is based on the
heuristic that considering the corpus at hand, we
48
ENSMT
used as queries per day articles candidate sentence pairs parallel sentences
+?5 day articlesfrom English Gigaword
English
translations 
tail
removal
sentences with
extra words at ends
+
parallel 
number / table
comparison      length  
removing
Arabic
comparable
corpus
AR
WER/TER/TERp
Figure 1: Architecture of the parallel sentence extraction system.
can safely say that a news item reported on day X
in the Arabic corpus will be most probably found
in the day X-5 and day X+5 time period. We ex-
perimented with several window sizes and found
the window size of is to be the most accurate in
terms of time and the quality of the retrieved sen-
tences. (Munteanu and Marcu, 2005) have also
worked with a ?5 day window.
Using the ID and date information for each sen-
tence of both corpora, we first collect all sentences
from the SMT translations corresponding to the
same day (query sentences) and then the corre-
sponding articles from the English Gigaword cor-
pus (search space for IR). These day-specific files
are then used for information retrieval using a ro-
bust information retrieval system. The Lemur IR
toolkit (Ogilvie and Callan, 2001) was used for
sentence extraction.
The information retrieval step is the most time
consuming task in the whole system. The time
taken depends upon various factors like size of the
index to search in, length of the query sentence
etc. To give a time estimate, using a ?5 day win-
dow required 9 seconds per query vs 15 seconds
per query when a ?7 day window was used. We
placed a limit of approximately 90 words on the
queries and the indexed sentences. This choice
was motivated by the fact that the word alignment
toolkit Giza++ does not process longer sentences.
A Krovetz stemmer was used while building the
index as provided by the toolkit. English stop
words, i.e. frequently used words, such as ?a? or
?the?, are normally not indexed because they are
so common that they are not useful to query on.
The stop word list provided by the IR Group of
University of Glasgow3 was used.
The resources required by our system are min-
imal : translations of one side of the comparable
corpus. It has already been demonstrated in (Rauf
and Schwenk, 2009) that when using translations
as queries, the quality of the initial SMT is not
a factor for better sentence retrieval and that an
SMT system trained on small amounts of human-
translated data can ?retrieve? potentially good par-
allel sentences.
4.2 Candidate Sentence Pair Selection
The information retrieval process gives us the po-
tential parallel sentences per query sentence, the
decision of their being parallel or not needs to be
made about them. At this stage we choose the
best scoring sentence as determined by the toolkit
and pass the sentence pair through further filters.
Gale and Church (1993) based their align program
on the fact that longer sentences in one language
tend to be translated into longer sentences in the
other language, and that shorter sentences tend to
be translated into shorter sentences. We initially
used the same logic in our selection of the candi-
date sentence pairs. However our observation was
that the filters that we use, WER, TER and TERp
implicitly place a penalty when the length differ-
3http://ir.dcs.gla.ac.uk/resources/
linguistic_utils/stop_words
49
ence between two sentences is too large. Thus us-
ing this inherent property, we did not apply any
explicit sentence length filtering.
The candidate sentences pairs are then judged
based on simple filters. Our choice of filters
in accordance to the task in consideration were
the WER (Levenshtein distance), Translation Edit
Rate (TER) and the relatively new Translation Edit
Rate plus (TERp). WER measures the number
of operations required to transform one sentence
into the other (insertions, deletions and substitu-
tions). A zero WER would mean the two sen-
tences are identical, subsequently lower WER sen-
tence pairs would be sharing most of the common
words. However two correct translations may dif-
fer in the order in which the words appear, some-
thing that WER is incapable of taking into ac-
count. This shortcoming is addressed by TER
which allows block movements of words and thus
takes into account the reorderings of words and
phrases in translation (Snover et al, 2006). TERp
is an extension of Translation Edit Rate and was
one of the top performing metrics at the NIST
Metric MATR workshop 4. It had the highest ab-
solute correlation, as measured by the Pearson cor-
relation coefficient, with human judgments in 9
of the 45 test conditions. TERp tries to address
the weaknesses of TER through the use of para-
phrases, morphological stemming, and synonyms,
as well as edit costs that are optimized to corre-
late better with various types of human judgments
(Snover et al, 2009). The TER filter allows shifts
if the two strings (the word sequence in the trans-
lated and the IR retrieved sentence) match exactly,
however TERp allows shifts if the words being
shifted are exactly the same, are synonyms, stems
or paraphrases of each other, or any such combi-
nation. This allows better sentence comparison
by incorporation of sort of linguistic information
about words.
5 Experimental evaluation
Our main goal was to be able to create an addi-
tional parallel corpus to improve machine transla-
tion quality, especially for the domains where we
have less or no parallel data available. In this sec-
tion we report the results of adding these extracted
parallel sentences to the already available human-
translated parallel sentences.
4http://www.itl.nist.gov/iad/mig/
/tests/metricsmatr/2008/
#words BLEU
Bitexts Arabic Eval06 Eval08
Baseline 5.8M 42.64 39.35
+WER-10 5.8M 42.73 39.70
+WER-40 7.2M 43.34 40.59
+WER-60 14.5M 43.95 41.20
+WER-70 20.4M 43.58 41.18
+TER-30 6.5M 43.41 40.08
+TER-50 12.5M 43.90 41.45
+TER-60 17.3M 44.30 41.73
+TER-75 24.1M 43.79 41.21
+TERp-10 5.8M 42.69 39.80
+TERp-40 10.2M 43.89 41.44
+TERp-60 20.8M 43.94 41.25
+TERp-80 27.7M 43.90 41.58
Table 2: Summary of BLEU scores for the best
systems selected based on various thresholds of
WER, TER and TERp filters
We conducted a range of experiments by adding
our extracted corpus to various combinations of
already available human-translated parallel cor-
pora. For our experiments on effect on SMT qual-
ity we use only the XIN extracted corpus. We
experimented with WER, TER and TERp as fil-
ters to select the best scoring sentences. Table 2
shows some of the scores obtained based on BLEU
scores on the Dev and test data as a function of
the size of the added extracted corpus. The name
of the bitext indicates the filter threshold used, for
example, TER-50 means sentences selected based
on TER filter threshold of 50. Generally, sen-
tences selected based on TER filter showed bet-
ter BLEU scores on NIST06 than their WER and
TERp counter parts up to almost 21M words. Also
for the same filter threshold TERp selected longer
sentences, followed by TER and then WER, this
fact is evident from table 2, where for the fil-
ter threshold of 60, TERp and TER select 20.8M
and 17.3 words respectively, whereas WER selects
14.5M words.
Figure 2 shows the trend obtained in function
of the number of words added. These experiments
were performed by adding our extracted sentences
to only 5.8M words of human-provided transla-
tions. Our best results are obtained when 11.5M
of our extracted parallel sentences based on TER
filter are added to 5.8M of News wire and gale par-
allel corpora. We gain an improvement of 1.66
BLEU points on NIST06 and 2.38 BLEU points
50
 41.5
 42
 42.5
 43
 43.5
 44
 44.5
 45
 5  10  15  20  25  30
BL
EU
 sc
ore
 on
 nis
t06
Arabic words for training [M]
baseline
TERTERpWER
 38.5
 39
 39.5
 40
 40.5
 41
 41.5
 42
 5  10  15  20  25  30
BL
 sc
ore
 on
 nis
t08
Arabic words for training [M]
baseline
TERTERpWER
Figure 2: BLEU scores on the NIST06 (Dev,
top) and NIST08 (test, bottom) data using an
WER,TER or TERp filter as a function of the num-
ber of extracted Arabic words added.
on NIST08 (TER-60 in table 2 ).
An interesting thing to notice in figure 2 is that
no filter was able to clearly outperform the others,
which is contradictory to our experiments with the
French-English language pair (Rauf and Schwenk,
2009), where the TER filter clearly outperformed
the WER filter. WER is worse than TER but less
evident here than for our previous experiments for
the French-English language pair. This perfor-
mance gain by using the TER filter for French-
English was our main motivation for trying TERp.
We expected TERp to get better results compared
to WER and TER, but TER filter seems the better
one among the three filters. Note that all condi-
tions in all the experiments were identical. This
gives a strong hint of language pair dependency,
making the decision of suitability of a particular
filter dependent on the language pair in considera-
tion.
5.1 Sentence tail removal
Two main classes of errors are known when ex-
tracting parallel sentences from comparable cor-
pora: firstly, cases where the two sentences share
many common words but actually convey differ-
ent meaning, and secondly, cases where the two
sentences are (exactly) parallel except at sentence
ends where one sentence has more information
than the other. This second case of errors can
be detected using WER as we have the advan-
tage of having both the sentences in English. We
detected the extra insertions at the end of the IR
result sentence and removed them. Some exam-
ples of such sentences along with tails detected
and removed are shown in figure 3. Since this
gives significant improvement in the SMT scores
we used it for all our extracted sentences (Rauf
and Schwenk, 2009). However, similar to our ob-
servations in the last section, the tails were much
shorter as compared to our previous experiments
with French-English, also most of the tails in this
Arabic-English data were of type as shown in last
line figure 3. This is a factor dependent on re-
porting agency and its scheme for reporting, i.e,
whether it reports an event independently in each
language or uses the translation from one language
to the other .
5.2 Dictionary Creation
In our translations, we keep the unknown words as
they are, i.e. in Arabic (normally a flag is used so
that Moses skips them). This enables us to build a
dictionary. Consider the case with translation with
one unknown word in Arabic, if all the other words
around align well with the English sentence that
we found with IR, we could conclude the trans-
lation of the unknown Arabic word, see figure 3
line 5. We were able to make a dictionary us-
ing this scheme which was comprised mostly of
proper nouns often not found in Arabic-English
dictionaries. Our proper noun dictionary com-
prises of about 244K words, some sample words
are shown in figure 4. Adding the proper nouns
found by this technique to the initial SMT sys-
tem should help improve translations for new sen-
tences, as these words were before unknown to the
system. However, the impact of addition of these
words on translation quality is to be evaluated at
the moment.
51
Arabic:                   	
 
      Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,
pages 129?132, Dublin, Ireland, August 23-29 2014.
THE MATECAT TOOL
M. Federico and N. Bertoldi and M. Cettolo and M. Negri and M. Turchi
Fondazione Bruno Kessler, Trento (Italy)
M. Trombetti and A. Cattelan and A. Farina and
D. Lupinetti and A. Martines and A. Massidda
Translated Srl, Roma (Italy)
H. Schwenk and L. Barrault and F. Blain
Universit?e du Maine, Le Mans (France)
P. Koehn and C. Buck and U. Germann
The University of Edinburgh (United Kingdom)
www.matecat.com
Abstract
We present a new web-based CAT tool providing translators with a professional work environ-
ment, integrating translation memories, terminology bases, concordancers, and machine transla-
tion. The tool is completely developed as open source software and has been already successfully
deployed for business, research and education. The MateCat Tool represents today probably the
best available open source platform for investigating, integrating, and evaluating under realistic
conditions the impact of new machine translation technology on human post-editing.
1 Introduction
The objective of MateCat
1
is to improve the integration of machine translation (MT) and human transla-
tion within the so-called computer aided translation (CAT) framework. CAT tools represent nowadays the
dominant technology in the translation industry. They provide translators with text editors that can man-
age several document formats and suitably arrange their content into text segments ready to be translated.
Most importantly, CAT tools provide access to translation memories (TMs), terminology databases, con-
cordance tools and, more recently, to machine translation (MT) engines. A TM is basically a repository
of translated segments. During translation, the CAT tool queries the TM to search for exact or fuzzy
matches of the current source segment. These matches are proposed to the user as translation sugges-
tions. Once a segment is translated, its source and target texts are added to the TM for future queries. The
integration of suggestions from an MT engine as a complement to TM matches is motivated by recent
studies (Federico et al., 2012; Green et al., 2013; L?aubli et al., 2013), which have shown that post-editing
MT suggestions can substantially improve the productivity of professional translators. MateCat lever-
ages the growing interest and expectations in statistical MT by advancing the state-of-the-art along three
directions:
? Self-tuning MT, i.e. methods to train statistical MT for specific domains or translation projects;
? User adaptive MT, i.e. methods to quickly adapt statistical MT from user corrections and feedback;
? Informative MT, i.e. supply more information to enhance users? productivity and work experience.
Research along these three directions has converged into a new generation CAT software, which is
both an enterprise level translation workbench (currently used by several hundreds of professional trans-
lators) as well as an advanced research platform for integrating new MT functions, running post-editing
This work is licensed under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:// creativecommons.org/licenses/by/4.0/.
1
MateCat, acronym of Machine Translation Enhanced Computer Assisted Translation, is a 3-year research project (11/2011-
10/2014) funded by the European Commission under FP7 (grant agreement no 287688). The project consortium is led by FBK
(Trento, Italy) and includes the University of Edinburgh (United Kingdom), Universit?e du Maine (Le Mans, France), and
Translated Srl (Rome, Italy).
129
Figure 1: The MateCat Tool editing page.
experiments and measuring user productivity. The MateCat Tool, which is distributed under the LGPL
open source license, combines features of the most advanced systems (either commercial, like the pop-
ular SDL Trados Workbench,
2
or free like OmegaT
3
) with new functionalities. These include: i) an
advanced API for the Moses Toolkit,
4
customizable to languages and domains, ii) ease of use through a
clean and intuitive web interface that enables the collaboration of multiple users on the same project, iii)
concordancers, terminology databases and support for customizable quality estimation components and
iv) advanced logging functionalities.
2 The MateCat Tool in a Nutshell
Overview. The MateCat Tool runs as a web-server accessible through Chrome, Firefox and Safari. The
CAT web-server connects with other services via open APIs: the TM server MyMemory
5
, the commer-
cial Google Translate (GT) MT server, and a list of Moses-based servers specified in a configuration file.
While MyMemory?s and GT?s servers are always running and available, customized Moses servers have
to be first installed and set-up. Communication with the Moses servers extends the GT API in order to
support self-tuning, user-adaptive and informative MT functions. The natively supported document for-
mat of MateCat Tool is XLIFF,
6
although its configuration file makes it possible to specify external file
converters. The tool supports Unicode (UTF-8) encoding, including non latin alphabets and right-to-left
languages, and handles texts embedding mark-up tags.
How it works. The tool is intended both for individual translators or managers of translation projects
involving one or more translators. A translation project starts by uploading one or more documents and
specifying the desired translation direction. Then the user can optionally select a MT engine from an
available list and/or a new or existing private TM in MyMemory, by specifying its private key. Notice
that the public MyMemory TM and the GT MT services are assumed by default. The following step is
the volume analysis of the document, which reports statistics about the words to be actually translated
based on the coverage provided by the TM. At this stage, long documents can be also split into smaller
portions to be for instance assigned to different translators or translated at different times. The following
step starts the actual translation process by opening the editing window. All source segments of the
2
http://www.translationzone.com/
3
http://www.omegat.org/
4
http://www.statmt.org/moses/
5
http://mymemory.translated.net
6
http://docs.oasis-open.org/xliff/v1.2/os/xliff-core.html
130
document and their corresponding target segments are arranged side-by-side on the screen. By selecting
one segment, an editing pane opens (Figure 1) including an editable field that is initialized with the best
available suggestion or with the last post-edit. Translation hints are shown right below together with
their origin (MT or TM). Their ranking is based on the TM match score or the MT confidence score. MT
hints with no confidence score are assigned a default score. Tag consistency is automatically checked
during translation and warnings are possibly shown in the editing window. An interesting feature of the
MateCat Tool is that each translation project is uniquely identified by its URL page which also includes
the currently edited segment. This permits for instance more users to simultaneously access and work on
the same project. Moreover, to support simultaneous team work on the same project, translators can mark
the status (draft, translated, approved, rejected) of each segment with a corresponding color (see Figure
1, right blue bar). The user interface is enriched with search and replace functions, a progress report at
the bottom of the page, and several shortcut commands for the skilled users. Finally, the tool embeds a
concordance tool to search for terms in the TM, and a glossary where each user can upload, query and
update her terminology base. Users with a Google account can access a project management page which
permits then to manage all their projects, including storage, deletion, and access to the editing page.
MT support. The tool supports Moses-based servers able to provide an enhanced CAT-MT commu-
nication. In particular, the GT API is augmented with feedback information provided to the MT engine
every time a segment is post-edited as well as enriched MT output, including confidence scores, word
lattices, etc. The developed MT server supports multi-threading to serve multiple translators, properly
handles text segments including tags, and instantly adapts from the post-edits performed by each user
(Bertoldi et al., 2013).
Edit Log. During post-editing the tool collects timing information for each segment, which is updated
every time the segment is opened and closed. Moreover, for each segment, information is collected about
the generated suggestions and the one that has actually been post-edited. This information is accessible at
any time through a link in the Editing Page, named Editing Log. The Editing Log page (Figure 2) shows
a summary of the overall editing performed so far on the project, such as the average translation speed
and post-editing effort and the percentage of top suggestions coming from MT or the TM. Moreover,
for each segment, sorted from the slowest to the fastest in terms of translation speed, detailed statistics
about the performed edit operations are reported. This information, with even more details, can be also
downloaded as a CSV file to perform a more detailed post-editing analysis. While the information shown
in the Edit Log page is very useful to monitor progress of a translation project in real time, the CSV file
is a fundamental source of information for detailed productivity analyses once the project is ended.
3 Applications.
The MateCat Tool has been exploited by the MateCat project to investigate new MT functions (Bertoldi
et al., 2013; Cettolo et al., 2013; Turchi et al., 2013; Turchi et al., 2014) and to evaluate them in a real
professional setting, in which translators have at disposal all the sources of information they are used
to work with. Moreover, taking advantage of its flexibility and ease of use, the tool has been recently
exploited for data collection and education purposes (a course on CAT technology for students in trans-
lation studies). An initial version of the tool has also been leveraged by the Casmacat project
7
to create
a workbench (Alabau et al., 2013), particularly suitable for investigating advanced interaction modalities
such as interactive MT, eye tracking, and handwritten input. Currently the tool is employed by Trans-
lated for their internal translation projects and is being tested by several international companies, both
language service providers and IT companies. This has made possible to collect continuous feedback
from hundreds of translators, which besides helping us to improve the robustness of the tool is also
influencing the way new MT functions will be integrated to supply the best help to the final user.
7
http://www.casmacat.eu
131
Figure 2: The MateCat Tool edit log page.
References
Vicent Alabau, Ragnar Bonk, Christian Buck, Michael Carl, Francisco Casacuberta, Mercedes Garca-Mart??nez,
Jes?us Gonz?alez, Philipp Koehn, Luis Leiva, Bartolom?e Mesa-Lao, Daniel Oriz, Herv?e Saint-Amand, Germ?an
Sanchis, and Chara Tsiukala. 2013. Advanced computer aided translation with a web-based workbench. In
Proceedings of Workshop on Post-editing Technology and Practice, pages 55?62.
Nicola Bertoldi, Mauro Cettolo, and Marcello Federico. 2013. Cache-based Online Adaptation for Machine
Translation Enhanced Computer Assisted Translation. In Proceedings of the MT Summit XIV, pages 35?42,
Nice, France, September.
Mauro Cettolo, Christophe Servan, Nicola Bertoldi, Marcello Federico, Lo??c Barrault, and Holger Schwenk. 2013.
Issues in Incremental Adaptation of Statistical MT from Human Post-edits. In Proceedings of the MT Summit
XIV Workshop on Post-editing Technology and Practice (WPTP-2), pages 111?118, Nice, France, September.
Marcello Federico, Alessandro Cattelan, and Marco Trombetti. 2012. Measuring user productivity in machine
translation enhanced computer assisted translation. In Proceedings of the Tenth Conference of the Association
for Machine Translation in the Americas (AMTA).
Spence Green, Jeffrey Heer, and Christopher D Manning. 2013. The efficacy of human post-editing for language
translation. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 439?
448. ACM.
Samuel L?aubli, Mark Fishel, Gary Massey, Maureen Ehrensberger-Dow, and Martin Volk. 2013. Assessing Post-
Editing Efficiency in a Realistic Translation Environment. In Michel Simard Sharon O?Brien and Lucia Specia
(eds.), editors, Proceedings of MT Summit XIVWorkshop on Post-editing Technology and Practice, pages 83?91,
Nice, France.
Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the subjectivity of human judgements
in MT quality estimation. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
240?251, Sofia, Bulgaria, August. Association for Computational Linguistics.
Marco Turchi, Antonios Anastasopoulos, Jos?e G.C. de Souza, and Matteo Negri. 2014. Adaptive Quality Estima-
tion for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational
Linguistics (ACL ?14). Association for Computational Linguistics.
132
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724?1734,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Learning Phrase Representations using RNN Encoder?Decoder
for Statistical Machine Translation
Kyunghyun Cho
Bart van Merri
?
enboer Caglar Gulcehre
Universit?e de Montr?eal
firstname.lastname@umontreal.ca
Dzmitry Bahdanau
Jacobs University, Germany
d.bahdanau@jacobs-university.de
Fethi Bougares Holger Schwenk
Universit?e du Maine, France
firstname.lastname@lium.univ-lemans.fr
Yoshua Bengio
Universit?e de Montr?eal, CIFAR Senior Fellow
find.me@on.the.web
Abstract
In this paper, we propose a novel neu-
ral network model called RNN Encoder?
Decoder that consists of two recurrent
neural networks (RNN). One RNN en-
codes a sequence of symbols into a fixed-
length vector representation, and the other
decodes the representation into another se-
quence of symbols. The encoder and de-
coder of the proposed model are jointly
trained to maximize the conditional prob-
ability of a target sequence given a source
sequence. The performance of a statisti-
cal machine translation system is empiri-
cally found to improve by using the con-
ditional probabilities of phrase pairs com-
puted by the RNN Encoder?Decoder as an
additional feature in the existing log-linear
model. Qualitatively, we show that the
proposed model learns a semantically and
syntactically meaningful representation of
linguistic phrases.
1 Introduction
Deep neural networks have shown great success in
various applications such as objection recognition
(see, e.g., (Krizhevsky et al., 2012)) and speech
recognition (see, e.g., (Dahl et al., 2012)). Fur-
thermore, many recent works showed that neu-
ral networks can be successfully used in a num-
ber of tasks in natural language processing (NLP).
These include, but are not limited to, language
modeling (Bengio et al., 2003), paraphrase detec-
tion (Socher et al., 2011) and word embedding ex-
traction (Mikolov et al., 2013). In the field of sta-
tistical machine translation (SMT), deep neural
networks have begun to show promising results.
(Schwenk, 2012) summarizes a successful usage
of feedforward neural networks in the framework
of phrase-based SMT system.
Along this line of research on using neural net-
works for SMT, this paper focuses on a novel neu-
ral network architecture that can be used as a part
of the conventional phrase-based SMT system.
The proposed neural network architecture, which
we will refer to as an RNN Encoder?Decoder, con-
sists of two recurrent neural networks (RNN) that
act as an encoder and a decoder pair. The en-
coder maps a variable-length source sequence to a
fixed-length vector, and the decoder maps the vec-
tor representation back to a variable-length target
sequence. The two networks are trained jointly to
maximize the conditional probability of the target
sequence given a source sequence. Additionally,
we propose to use a rather sophisticated hidden
unit in order to improve both the memory capacity
and the ease of training.
The proposed RNN Encoder?Decoder with a
novel hidden unit is empirically evaluated on the
task of translating from English to French. We
train the model to learn the translation probabil-
ity of an English phrase to a corresponding French
phrase. The model is then used as a part of a stan-
dard phrase-based SMT system by scoring each
phrase pair in the phrase table. The empirical eval-
uation reveals that this approach of scoring phrase
pairs with an RNN Encoder?Decoder improves
the translation performance.
We qualitatively analyze the trained RNN
Encoder?Decoder by comparing its phrase scores
with those given by the existing translation model.
The qualitative analysis shows that the RNN
Encoder?Decoder is better at capturing the lin-
guistic regularities in the phrase table, indirectly
explaining the quantitative improvements in the
overall translation performance. The further anal-
ysis of the model reveals that the RNN Encoder?
Decoder learns a continuous space representation
of a phrase that preserves both the semantic and
syntactic structure of the phrase.
1724
2 RNN Encoder?Decoder
2.1 Preliminary: Recurrent Neural Networks
A recurrent neural network (RNN) is a neural net-
work that consists of a hidden state h and an
optional output y which operates on a variable-
length sequence x = (x
1
, . . . , x
T
). At each time
step t, the hidden state h
?t?
of the RNN is updated
by
h
?t?
= f
(
h
?t?1?
, x
t
)
, (1)
where f is a non-linear activation func-
tion. f may be as simple as an element-
wise logistic sigmoid function and as com-
plex as a long short-term memory (LSTM)
unit (Hochreiter and Schmidhuber, 1997).
An RNN can learn a probability distribution
over a sequence by being trained to predict the
next symbol in a sequence. In that case, the output
at each timestep t is the conditional distribution
p(x
t
| x
t?1
, . . . , x
1
). For example, a multinomial
distribution (1-of-K coding) can be output using a
softmax activation function
p(x
t,j
= 1 | x
t?1
, . . . , x
1
) =
exp
(
w
j
h
?t?
)
?
K
j
?
=1
exp
(
w
j
?
h
?t?
)
,
(2)
for all possible symbols j = 1, . . . ,K, where w
j
are the rows of a weight matrix W. By combining
these probabilities, we can compute the probabil-
ity of the sequence x using
p(x) =
T
?
t=1
p(x
t
| x
t?1
, . . . , x
1
). (3)
From this learned distribution, it is straightfor-
ward to sample a new sequence by iteratively sam-
pling a symbol at each time step.
2.2 RNN Encoder?Decoder
In this paper, we propose a novel neural network
architecture that learns to encode a variable-length
sequence into a fixed-length vector representation
and to decode a given fixed-length vector rep-
resentation back into a variable-length sequence.
From a probabilistic perspective, this new model
is a general method to learn the conditional dis-
tribution over a variable-length sequence condi-
tioned on yet another variable-length sequence,
e.g. p(y
1
, . . . , y
T
?
| x
1
, . . . , x
T
), where one
x1 x2 xT
yT' y2 y1
c
Decoder
Encoder
Figure 1: An illustration of the proposed RNN
Encoder?Decoder.
should note that the input and output sequence
lengths T and T
?
may differ.
The encoder is an RNN that reads each symbol
of an input sequence x sequentially. As it reads
each symbol, the hidden state of the RNN changes
according to Eq. (1). After reading the end of
the sequence (marked by an end-of-sequence sym-
bol), the hidden state of the RNN is a summary c
of the whole input sequence.
The decoder of the proposed model is another
RNN which is trained to generate the output se-
quence by predicting the next symbol y
t
given the
hidden state h
?t?
. However, unlike the RNN de-
scribed in Sec. 2.1, both y
t
and h
?t?
are also con-
ditioned on y
t?1
and on the summary c of the input
sequence. Hence, the hidden state of the decoder
at time t is computed by,
h
?t?
= f
(
h
?t?1?
, y
t?1
, c
)
,
and similarly, the conditional distribution of the
next symbol is
P (y
t
|y
t?1
, y
t?2
, . . . , y
1
, c) = g
(
h
?t?
, y
t?1
, c
)
.
for given activation functions f and g (the latter
must produce valid probabilities, e.g. with a soft-
max).
See Fig. 1 for a graphical depiction of the pro-
posed model architecture.
The two components of the proposed RNN
Encoder?Decoder are jointly trained to maximize
the conditional log-likelihood
max
?
1
N
N
?
n=1
log p
?
(y
n
| x
n
), (4)
1725
where ? is the set of the model parameters and
each (x
n
,y
n
) is an (input sequence, output se-
quence) pair from the training set. In our case,
as the output of the decoder, starting from the in-
put, is differentiable, we can use a gradient-based
algorithm to estimate the model parameters.
Once the RNN Encoder?Decoder is trained, the
model can be used in two ways. One way is to use
the model to generate a target sequence given an
input sequence. On the other hand, the model can
be used to score a given pair of input and output
sequences, where the score is simply a probability
p
?
(y | x) from Eqs. (3) and (4).
2.3 Hidden Unit that Adaptively Remembers
and Forgets
In addition to a novel model architecture, we also
propose a new type of hidden unit (f in Eq. (1))
that has been motivated by the LSTM unit but is
much simpler to compute and implement.
1
Fig. 2
shows the graphical depiction of the proposed hid-
den unit.
Let us describe how the activation of the j-th
hidden unit is computed. First, the reset gate r
j
is
computed by
r
j
= ?
(
[W
r
x]
j
+
[
U
r
h
?t?1?
]
j
)
, (5)
where ? is the logistic sigmoid function, and [.]
j
denotes the j-th element of a vector. x and h
t?1
are the input and the previous hidden state, respec-
tively. W
r
and U
r
are weight matrices which are
learned.
Similarly, the update gate z
j
is computed by
z
j
= ?
(
[W
z
x]
j
+
[
U
z
h
?t?1?
]
j
)
. (6)
The actual activation of the proposed unit h
j
is
then computed by
h
?t?
j
= z
j
h
?t?1?
j
+ (1? z
j
)
?
h
?t?
j
, (7)
where
?
h
?t?
j
= ?
(
[Wx]
j
+
[
U
(
r h
?t?1?
)]
j
)
. (8)
In this formulation, when the reset gate is close
to 0, the hidden state is forced to ignore the pre-
vious hidden state and reset with the current input
1
The LSTM unit, which has shown impressive results in
several applications such as speech recognition, has a mem-
ory cell and four gating units that adaptively control the in-
formation flow inside the unit, compared to only two gating
units in the proposed hidden unit. For details on LSTM net-
works, see, e.g., (Graves, 2012).
z
r
h h
~ x
Figure 2: An illustration of the proposed hidden
activation function. The update gate z selects
whether the hidden state is to be updated with
a new hidden state
?
h. The reset gate r decides
whether the previous hidden state is ignored. See
Eqs. (5)?(8) for the detailed equations of r, z, h
and
?
h.
only. This effectively allows the hidden state to
drop any information that is found to be irrelevant
later in the future, thus, allowing a more compact
representation.
On the other hand, the update gate controls how
much information from the previous hidden state
will carry over to the current hidden state. This
acts similarly to the memory cell in the LSTM
network and helps the RNN to remember long-
term information. Furthermore, this may be con-
sidered an adaptive variant of a leaky-integration
unit (Bengio et al., 2013).
As each hidden unit has separate reset and up-
date gates, each hidden unit will learn to capture
dependencies over different time scales. Those
units that learn to capture short-term dependencies
will tend to have reset gates that are frequently ac-
tive, but those that capture longer-term dependen-
cies will have update gates that are mostly active.
In our preliminary experiments, we found that
it is crucial to use this new unit with gating units.
We were not able to get meaningful result with an
oft-used tanh unit without any gating.
3 Statistical Machine Translation
In a commonly used statistical machine translation
system (SMT), the goal of the system (decoder,
specifically) is to find a translation f given a source
sentence e, which maximizes
p(f | e) ? p(e | f)p(f),
where the first term at the right hand side is called
translation model and the latter language model
(see, e.g., (Koehn, 2005)). In practice, however,
most SMT systems model log p(f | e) as a log-
linear model with additional features and corre-
1726
sponding weights:
log p(f | e) =
N
?
n=1
w
n
f
n
(f , e) + logZ(e), (9)
where f
n
and w
n
are the n-th feature and weight,
respectively. Z(e) is a normalization constant that
does not depend on the weights. The weights are
often optimized to maximize the BLEU score on a
development set.
In the phrase-based SMT framework
introduced in (Koehn et al., 2003) and
(Marcu and Wong, 2002), the translation model
log p(e | f) is factorized into the translation
probabilities of matching phrases in the source
and target sentences.
2
These probabilities are
once again considered additional features in the
log-linear model (see Eq. (9)) and are weighted
accordingly to maximize the BLEU score.
Since the neural net language model was pro-
posed in (Bengio et al., 2003), neural networks
have been used widely in SMT systems. In
many cases, neural networks have been used to
rescore translation hypotheses (n-best lists) (see,
e.g., (Schwenk et al., 2006)). Recently, however,
there has been interest in training neural networks
to score the translated sentence (or phrase pairs)
using a representation of the source sentence as
an additional input. See, e.g., (Schwenk, 2012),
(Son et al., 2012) and (Zou et al., 2013).
3.1 Scoring Phrase Pairs with RNN
Encoder?Decoder
Here we propose to train the RNN Encoder?
Decoder (see Sec. 2.2) on a table of phrase pairs
and use its scores as additional features in the log-
linear model in Eq. (9) when tuning the SMT de-
coder.
When we train the RNN Encoder?Decoder, we
ignore the (normalized) frequencies of each phrase
pair in the original corpora. This measure was
taken in order (1) to reduce the computational ex-
pense of randomly selecting phrase pairs from a
large phrase table according to the normalized fre-
quencies and (2) to ensure that the RNN Encoder?
Decoder does not simply learn to rank the phrase
pairs according to their numbers of occurrences.
One underlying reason for this choice was that the
existing translation probability in the phrase ta-
ble already reflects the frequencies of the phrase
2
Without loss of generality, from here on, we refer to
p(e | f) for each phrase pair as a translation model as well
pairs in the original corpus. With a fixed capacity
of the RNN Encoder?Decoder, we try to ensure
that most of the capacity of the model is focused
toward learning linguistic regularities, i.e., distin-
guishing between plausible and implausible trans-
lations, or learning the ?manifold? (region of prob-
ability concentration) of plausible translations.
Once the RNN Encoder?Decoder is trained, we
add a new score for each phrase pair to the exist-
ing phrase table. This allows the new scores to en-
ter into the existing tuning algorithm with minimal
additional overhead in computation.
As Schwenk pointed out in (Schwenk, 2012),
it is possible to completely replace the existing
phrase table with the proposed RNN Encoder?
Decoder. In that case, for a given source phrase,
the RNN Encoder?Decoder will need to generate
a list of (good) target phrases. This requires, how-
ever, an expensive sampling procedure to be per-
formed repeatedly. In this paper, thus, we only
consider rescoring the phrase pairs in the phrase
table.
3.2 Related Approaches: Neural Networks in
Machine Translation
Before presenting the empirical results, we discuss
a number of recent works that have proposed to
use neural networks in the context of SMT.
Schwenk in (Schwenk, 2012) proposed a simi-
lar approach of scoring phrase pairs. Instead of the
RNN-based neural network, he used a feedforward
neural network that has fixed-size inputs (7 words
in his case, with zero-padding for shorter phrases)
and fixed-size outputs (7 words in the target lan-
guage). When it is used specifically for scoring
phrases for the SMT system, the maximum phrase
length is often chosen to be small. However, as the
length of phrases increases or as we apply neural
networks to other variable-length sequence data,
it is important that the neural network can han-
dle variable-length input and output. The pro-
posed RNN Encoder?Decoder is well-suited for
these applications.
Similar to (Schwenk, 2012), Devlin et al.
(Devlin et al., 2014) proposed to use a feedfor-
ward neural network to model a translation model,
however, by predicting one word in a target phrase
at a time. They reported an impressive improve-
ment, but their approach still requires the maxi-
mum length of the input phrase (or context words)
to be fixed a priori.
1727
Although it is not exactly a neural network they
train, the authors of (Zou et al., 2013) proposed
to learn a bilingual embedding of words/phrases.
They use the learned embedding to compute the
distance between a pair of phrases which is used
as an additional score of the phrase pair in an SMT
system.
In (Chandar et al., 2014), a feedforward neural
network was trained to learn a mapping from a
bag-of-words representation of an input phrase to
an output phrase. This is closely related to both the
proposed RNN Encoder?Decoder and the model
proposed in (Schwenk, 2012), except that their in-
put representation of a phrase is a bag-of-words.
A similar approach of using bag-of-words repre-
sentations was proposed in (Gao et al., 2013) as
well. Earlier, a similar encoder?decoder model us-
ing two recursive neural networks was proposed
in (Socher et al., 2011), but their model was re-
stricted to a monolingual setting, i.e. the model
reconstructs an input sentence. More recently, an-
other encoder?decoder model using an RNN was
proposed in (Auli et al., 2013), where the de-
coder is conditioned on a representation of either
a source sentence or a source context.
One important difference between the pro-
posed RNN Encoder?Decoder and the approaches
in (Zou et al., 2013) and (Chandar et al., 2014) is
that the order of the words in source and tar-
get phrases is taken into account. The RNN
Encoder?Decoder naturally distinguishes between
sequences that have the same words but in a differ-
ent order, whereas the aforementioned approaches
effectively ignore order information.
The closest approach related to the proposed
RNN Encoder?Decoder is the Recurrent Contin-
uous Translation Model (Model 2) proposed in
(Kalchbrenner and Blunsom, 2013). In their pa-
per, they proposed a similar model that consists
of an encoder and decoder. The difference with
our model is that they used a convolutional n-gram
model (CGM) for the encoder and the hybrid of
an inverse CGM and a recurrent neural network
for the decoder. They, however, evaluated their
model on rescoring the n-best list proposed by the
conventional SMT system and computing the per-
plexity of the gold standard translations.
4 Experiments
We evaluate our approach on the English/French
translation task of the WMT?14 workshop.
4.1 Data and Baseline System
Large amounts of resources are available to build
an English/French SMT system in the framework
of the WMT?14 translation task. The bilingual
corpora include Europarl (61M words), news com-
mentary (5.5M), UN (421M), and two crawled
corpora of 90M and 780M words respectively.
The last two corpora are quite noisy. To train
the French language model, about 712M words of
crawled newspaper material is available in addi-
tion to the target side of the bitexts. All the word
counts refer to French words after tokenization.
It is commonly acknowledged that training sta-
tistical models on the concatenation of all this
data does not necessarily lead to optimal per-
formance, and results in extremely large mod-
els which are difficult to handle. Instead, one
should focus on the most relevant subset of the
data for a given task. We have done so by
applying the data selection method proposed in
(Moore and Lewis, 2010), and its extension to bi-
texts (Axelrod et al., 2011). By these means we
selected a subset of 418M words out of more
than 2G words for language modeling and a
subset of 348M out of 850M words for train-
ing the RNN Encoder?Decoder. We used the
test set newstest2012 and 2013 for data
selection and weight tuning with MERT, and
newstest2014 as our test set. Each set has
more than 70 thousand words and a single refer-
ence translation.
For training the neural networks, including the
proposed RNN Encoder?Decoder, we limited the
source and target vocabulary to the most frequent
15,000 words for both English and French. This
covers approximately 93% of the dataset. All the
out-of-vocabulary words were mapped to a special
token ([UNK]).
The baseline phrase-based SMT system was
built using Moses with default settings. This sys-
tem achieves a BLEU score of 30.64 and 33.3 on
the development and test sets, respectively (see Ta-
ble 1).
4.1.1 RNN Encoder?Decoder
The RNN Encoder?Decoder used in the experi-
ment had 1000 hidden units with the proposed
gates at the encoder and at the decoder. The in-
put matrix between each input symbol x
?t?
and the
hidden unit is approximated with two lower-rank
matrices, and the output matrix is approximated
1728
Models
BLEU
dev test
Baseline 30.64 33.30
RNN 31.20 33.87
CSLM + RNN 31.48 34.64
CSLM + RNN + WP 31.50 34.54
Table 1: BLEU scores computed on the develop-
ment and test sets using different combinations of
approaches. WP denotes a word penalty, where
we penalizes the number of unknown words to
neural networks.
similarly. We used rank-100 matrices, equivalent
to learning an embedding of dimension 100 for
each word. The activation function used for
?
h in
Eq. (8) is a hyperbolic tangent function. The com-
putation from the hidden state in the decoder to
the output is implemented as a deep neural net-
work (Pascanu et al., 2014) with a single interme-
diate layer having 500 maxout units each pooling
2 inputs (Goodfellow et al., 2013).
All the weight parameters in the RNN Encoder?
Decoder were initialized by sampling from an
isotropic zero-mean (white) Gaussian distribution
with its standard deviation fixed to 0.01, except
for the recurrent weight parameters. For the re-
current weight matrices, we first sampled from a
white Gaussian distribution and used its left singu-
lar vectors matrix, following (Saxe et al., 2014).
We used Adadelta and stochastic gradient
descent to train the RNN Encoder?Decoder
with hyperparameters  = 10
?6
and ? =
0.95 (Zeiler, 2012). At each update, we used 64
randomly selected phrase pairs from a phrase ta-
ble (which was created from 348M words). The
model was trained for approximately three days.
Details of the architecture used in the experi-
ments are explained in more depth in the supple-
mentary material.
4.1.2 Neural Language Model
In order to assess the effectiveness of scoring
phrase pairs with the proposed RNN Encoder?
Decoder, we also tried a more traditional approach
of using a neural network for learning a target
language model (CSLM) (Schwenk, 2007). Espe-
cially, the comparison between the SMT system
using CSLM and that using the proposed approach
of phrase scoring by RNN Encoder?Decoder will
clarify whether the contributions from multiple
neural networks in different parts of the SMT sys-
tem add up or are redundant.
We trained the CSLM model on 7-grams
from the target corpus. Each input word
was projected into the embedding space R
512
,
and they were concatenated to form a 3072-
dimensional vector. The concatenated vector was
fed through two rectified layers (of size 1536 and
1024) (Glorot et al., 2011). The output layer was
a simple softmax layer (see Eq. (2)). All the
weight parameters were initialized uniformly be-
tween ?0.01 and 0.01, and the model was trained
until the validation perplexity did not improve for
10 epochs. After training, the language model
achieved a perplexity of 45.80. The validation set
was a random selection of 0.1% of the corpus. The
model was used to score partial translations dur-
ing the decoding process, which generally leads to
higher gains in BLEU score than n-best list rescor-
ing (Vaswani et al., 2013).
To address the computational complexity of
using a CSLM in the decoder a buffer was
used to aggregate n-grams during the stack-
search performed by the decoder. Only when
the buffer is full, or a stack is about to
be pruned, the n-grams are scored by the
CSLM. This allows us to perform fast matrix-
matrix multiplication on GPU using Theano
(Bergstra et al., 2010; Bastien et al., 2012).
?60 ?50 ?40 ?30 ?20 ?10 0?14
?12
?10
?8
?6
?4
?2
0
RNN Scores (log)
TM 
Scor
es (lo
g)
Figure 3: The visualization of phrase pairs ac-
cording to their scores (log-probabilities) by the
RNN Encoder?Decoder and the translation model.
4.2 Quantitative Analysis
We tried the following combinations:
1. Baseline configuration
2. Baseline + RNN
3. Baseline + CSLM + RNN
4. Baseline + CSLM + RNN + Word penalty
1729
Source Translation Model RNN Encoder?Decoder
at the end of the [a la fin de la] [?r la fin des ann?ees] [?etre sup-
prim?es `a la fin de la]
[`a la fin du] [`a la fin des] [`a la fin de la]
for the first time [r
c
? pour la premir?ere fois] [?et?e donn?es pour
la premi`ere fois] [?et?e comm?emor?ee pour la
premi`ere fois]
[pour la premi`ere fois] [pour la premi`ere fois ,]
[pour la premi`ere fois que]
in the United States
and
[? aux ?tats-Unis et] [?et?e ouvertes aux
?
Etats-
Unis et] [?et?e constat?ees aux
?
Etats-Unis et]
[aux Etats-Unis et] [des Etats-Unis et] [des
?
Etats-Unis et]
, as well as [?s , qu?] [?s , ainsi que] [?re aussi bien que] [, ainsi qu?] [, ainsi que] [, ainsi que les]
one of the most [?t ?l? un des plus] [?l? un des plus] [?etre retenue
comme un de ses plus]
[l? un des] [le] [un des]
(a) Long, frequent source phrases
Source Translation Model RNN Encoder?Decoder
, Minister of Commu-
nications and Trans-
port
[Secr?etaire aux communications et aux trans-
ports :] [Secr?etaire aux communications et aux
transports]
[Secr?etaire aux communications et aux trans-
ports] [Secr?etaire aux communications et aux
transports :]
did not comply with
the
[vestimentaire , ne correspondaient pas `a des]
[susmentionn?ee n? ?etait pas conforme aux]
[pr?esent?ees n? ?etaient pas conformes `a la]
[n? ont pas respect?e les] [n? ?etait pas conforme
aux] [n? ont pas respect?e la]
parts of the world . [
c
? gions du monde .] [r?egions du monde con-
sid?er?ees .] [r?egion du monde consid?er?ee .]
[parties du monde .] [les parties du monde .]
[des parties du monde .]
the past few days . [le petit texte .] [cours des tout derniers jours .]
[les tout derniers jours .]
[ces derniers jours .] [les derniers jours .] [cours
des derniers jours .]
on Friday and Satur-
day
[vendredi et samedi `a la] [vendredi et samedi `a]
[se d?eroulera vendredi et samedi ,]
[le vendredi et le samedi] [le vendredi et samedi]
[vendredi et samedi]
(b) Long, rare source phrases
Table 2: The top scoring target phrases for a small set of source phrases according to the translation
model (direct translation probability) and by the RNN Encoder?Decoder. Source phrases were randomly
selected from phrases with 4 or more words. ? denotes an incomplete (partial) character. r is a Cyrillic
letter ghe.
The results are presented in Table 1. As ex-
pected, adding features computed by neural net-
works consistently improves the performance over
the baseline performance.
The best performance was achieved when we
used both CSLM and the phrase scores from the
RNN Encoder?Decoder. This suggests that the
contributions of the CSLM and the RNN Encoder?
Decoder are not too correlated and that one can
expect better results by improving each method in-
dependently. Furthermore, we tried penalizing the
number of words that are unknown to the neural
networks (i.e. words which are not in the short-
list). We do so by simply adding the number of
unknown words as an additional feature the log-
linear model in Eq. (9).
3
However, in this case we
3
To understand the effect of the penalty, consider the set
of all words in the 15,000 large shortlist, SL. All words x
i
/?
SL are replaced by a special token [UNK] before being scored
by the neural networks. Hence, the conditional probability of
any x
i
t
/? SL is actually given by the model as
p (x
t
= [UNK] | x
<t
) = p (x
t
/? SL | x
<t
)
=
X
x
j
t
/?SL
p
?
x
j
t
| x
<t
?
? p
?
x
i
t
| x
<t
?
,
where x
<t
is a shorthand notation for x
t?1
, . . . , x
1
.
were not able to achieve better performance on the
test set, but only on the development set.
4.3 Qualitative Analysis
In order to understand where the performance im-
provement comes from, we analyze the phrase pair
scores computed by the RNN Encoder?Decoder
against the corresponding p(f | e) from the trans-
lation model. Since the existing translation model
relies solely on the statistics of the phrase pairs in
the corpus, we expect its scores to be better esti-
mated for the frequent phrases but badly estimated
for rare phrases. Also, as we mentioned earlier
in Sec. 3.1, we further expect the RNN Encoder?
Decoder which was trained without any frequency
information to score the phrase pairs based rather
on the linguistic regularities than on the statistics
of their occurrences in the corpus.
We focus on those pairs whose source phrase is
long (more than 3 words per source phrase) and
As a result, the probability of words not in the shortlist is
always overestimated. It is possible to address this issue by
backing off to an existing model that contain non-shortlisted
words (see (Schwenk, 2007)) In this paper, however, we opt
for introducing a word penalty instead, which counteracts the
word probability overestimation.
1730
Source Samples from RNN Encoder?Decoder
at the end of the [`a la fin de la] (?11)
for the first time [pour la premi`ere fois] (?24) [pour la premi`ere fois que] (?2)
in the United States and [aux
?
Etats-Unis et] (?6) [dans les
?
Etats-Unis et] (?4)
, as well as [, ainsi que] [,] [ainsi que] [, ainsi qu?] [et UNK]
one of the most [l? un des plus] (?9) [l? un des] (?5) [l? une des plus] (?2)
(a) Long, frequent source phrases
Source Samples from RNN Encoder?Decoder
, Minister of Communica-
tions and Transport
[ , ministre des communications et le transport] (?13)
did not comply with the [n? tait pas conforme aux] [n? a pas respect l?] (?2) [n? a pas respect la] (?3)
parts of the world . [arts du monde .] (?11) [des arts du monde .] (?7)
the past few days . [quelques jours .] (?5) [les derniers jours .] (?5) [ces derniers jours .] (?2)
on Friday and Saturday [vendredi et samedi] (?5) [le vendredi et samedi] (?7) [le vendredi et le samedi] (?4)
(b) Long, rare source phrases
Table 3: Samples generated from the RNN Encoder?Decoder for each source phrase used in Table 2. We
show the top-5 target phrases out of 50 samples. They are sorted by the RNN Encoder?Decoder scores.
Figure 4: 2?D embedding of the learned word representation. The left one shows the full embedding
space, while the right one shows a zoomed-in view of one region (color?coded). For more plots, see the
supplementary material.
frequent. For each such source phrase, we look
at the target phrases that have been scored high
either by the translation probability p(f | e) or
by the RNN Encoder?Decoder. Similarly, we per-
form the same procedure with those pairs whose
source phrase is long but rare in the corpus.
Table 2 lists the top-3 target phrases per source
phrase favored either by the translation model
or by the RNN Encoder?Decoder. The source
phrases were randomly chosen among long ones
having more than 4 or 5 words.
In most cases, the choices of the target phrases
by the RNN Encoder?Decoder are closer to ac-
tual or literal translations. We can observe that the
RNN Encoder?Decoder prefers shorter phrases in
general.
Interestingly, many phrase pairs were scored
similarly by both the translation model and the
RNN Encoder?Decoder, but there were as many
other phrase pairs that were scored radically dif-
ferent (see Fig. 3). This could arise from the
proposed approach of training the RNN Encoder?
Decoder on a set of unique phrase pairs, discour-
aging the RNN Encoder?Decoder from learning
simply the frequencies of the phrase pairs from the
corpus, as explained earlier.
Furthermore, in Table 3, we show for each of
the source phrases in Table 2, the generated sam-
ples from the RNN Encoder?Decoder. For each
source phrase, we generated 50 samples and show
the top-five phrases accordingly to their scores.
We can see that the RNN Encoder?Decoder is
able to propose well-formed target phrases with-
out looking at the actual phrase table. Importantly,
the generated phrases do not overlap completely
with the target phrases from the phrase table. This
encourages us to further investigate the possibility
of replacing the whole or a part of the phrase table
1731
Figure 5: 2?D embedding of the learned phrase representation. The top left one shows the full represen-
tation space (5000 randomly selected points), while the other three figures show the zoomed-in view of
specific regions (color?coded).
with the proposed RNN Encoder?Decoder in the
future.
4.4 Word and Phrase Representations
Since the proposed RNN Encoder?Decoder is not
specifically designed only for the task of machine
translation, here we briefly look at the properties
of the trained model.
It has been known for some time that
continuous space language models using
neural networks are able to learn seman-
tically meaningful embeddings (See, e.g.,
(Bengio et al., 2003; Mikolov et al., 2013)). Since
the proposed RNN Encoder?Decoder also projects
to and maps back from a sequence of words into
a continuous space vector, we expect to see a
similar property with the proposed model as well.
The left plot in Fig. 4 shows the 2?D embedding
of the words using the word embedding matrix
learned by the RNN Encoder?Decoder. The pro-
jection was done by the recently proposed Barnes-
Hut-SNE (van der Maaten, 2013). We can clearly
see that semantically similar words are clustered
with each other (see the zoomed-in plots in Fig. 4).
The proposed RNN Encoder?Decoder naturally
generates a continuous-space representation of a
phrase. The representation (c in Fig. 1) in this
case is a 1000-dimensional vector. Similarly to the
word representations, we visualize the representa-
tions of the phrases that consists of four or more
words using the Barnes-Hut-SNE in Fig. 5.
From the visualization, it is clear that the RNN
Encoder?Decoder captures both semantic and syn-
tactic structures of the phrases. For instance, in
the bottom-left plot, most of the phrases are about
the duration of time, while those phrases that are
syntactically similar are clustered together. The
bottom-right plot shows the cluster of phrases that
are semantically similar (countries or regions). On
the other hand, the top-right plot shows the phrases
that are syntactically similar.
5 Conclusion
In this paper, we proposed a new neural network
architecture, called an RNN Encoder?Decoder
that is able to learn the mapping from a sequence
1732
of an arbitrary length to another sequence, possi-
bly from a different set, of an arbitrary length. The
proposed RNN Encoder?Decoder is able to either
score a pair of sequences (in terms of a conditional
probability) or generate a target sequence given a
source sequence. Along with the new architecture,
we proposed a novel hidden unit that includes a re-
set gate and an update gate that adaptively control
how much each hidden unit remembers or forgets
while reading/generating a sequence.
We evaluated the proposed model with the task
of statistical machine translation, where we used
the RNN Encoder?Decoder to score each phrase
pair in the phrase table. Qualitatively, we were
able to show that the new model is able to cap-
ture linguistic regularities in the phrase pairs well
and also that the RNN Encoder?Decoder is able to
propose well-formed target phrases.
The scores by the RNN Encoder?Decoder were
found to improve the overall translation perfor-
mance in terms of BLEU scores. Also, we
found that the contribution by the RNN Encoder?
Decoder is rather orthogonal to the existing ap-
proach of using neural networks in the SMT sys-
tem, so that we can improve further the perfor-
mance by using, for instance, the RNN Encoder?
Decoder and the neural net language model to-
gether.
Our qualitative analysis of the trained model
shows that it indeed captures the linguistic regu-
larities in multiple levels i.e. at the word level as
well as phrase level. This suggests that there may
be more natural language related applications that
may benefit from the proposed RNN Encoder?
Decoder.
The proposed architecture has large potential
for further improvement and analysis. One ap-
proach that was not investigated here is to re-
place the whole, or a part of the phrase table by
letting the RNN Encoder?Decoder propose target
phrases. Also, noting that the proposed model is
not limited to being used with written language,
it will be an important future research to apply the
proposed architecture to other applications such as
speech transcription.
Acknowledgments
KC, BM, CG, DB and YB would like to thank
NSERC, Calcul Qu?ebec, Compute Canada, the
Canada Research Chairs and CIFAR. FB and HS
were partially funded by the European Commis-
sion under the project MateCat, and by DARPA
under the BOLT project.
References
[Auli et al.2013] Michael Auli, Michel Galley, Chris
Quirk, and Geoffrey Zweig. 2013. Joint language
and translation modeling with recurrent neural net-
works. In Proceedings of the ACL Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1044?1054.
[Axelrod et al.2011] Amittai Axelrod, Xiaodong He,
and Jianfeng Gao. 2011. Domain adaptation via
pseudo in-domain data selection. In Proceedings of
the ACL Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 355?362.
[Bastien et al.2012] Fr?ed?eric Bastien, Pascal Lamblin,
Razvan Pascanu, James Bergstra, Ian J. Goodfellow,
Arnaud Bergeron, Nicolas Bouchard, and Yoshua
Bengio. 2012. Theano: new features and speed im-
provements. Deep Learning and Unsupervised Fea-
ture Learning NIPS 2012 Workshop.
[Bengio et al.2003] Yoshua Bengio, R?ejean Ducharme,
Pascal Vincent, and Christian Janvin. 2003. A neu-
ral probabilistic language model. J. Mach. Learn.
Res., 3:1137?1155, March.
[Bengio et al.2013] Y. Bengio, N. Boulanger-
Lewandowski, and R. Pascanu. 2013. Advances
in optimizing recurrent networks. In Proceedings
of the 38th International Conference on Acoustics,
Speech, and Signal Processing (ICASSP 2013),
May.
[Bergstra et al.2010] James Bergstra, Olivier Breuleux,
Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,
Guillaume Desjardins, Joseph Turian, David Warde-
Farley, and Yoshua Bengio. 2010. Theano: a CPU
and GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
[Chandar et al.2014] Sarath Chandar, Stanislas Lauly,
Hugo Larochelle, Mitesh Khapra, Balaraman Ravin-
dran, Vikas Raykar, and Amrita Saha. 2014. An au-
toencoder approach to learning bilingual word repre-
sentations. arXiv:1402.1454 [cs.CL], Febru-
ary.
[Dahl et al.2012] George E. Dahl, Dong Yu, Li Deng,
and Alex Acero. 2012. Context-dependent pre-
trained deep neural networks for large vocabulary
speech recognition. IEEE Transactions on Audio,
Speech, and Language Processing, 20(1):33?42.
[Devlin et al.2014] Jacob Devlin, Rabih Zbib,
Zhongqiang Huang, Thomas Lamar, Richard
Schwartz, , and John Makhoul. 2014. Fast and
robust neural network joint models for statistical
machine translation. In Proceedings of the ACL
2014 Conference, ACL ?14, pages 1370?1380.
1733
[Gao et al.2013] Jianfeng Gao, Xiaodong He, Wen tau
Yih, and Li Deng. 2013. Learning semantic repre-
sentations for the phrase translation model. Techni-
cal report, Microsoft Research.
[Glorot et al.2011] X. Glorot, A. Bordes, and Y. Ben-
gio. 2011. Deep sparse rectifier neural networks. In
AISTATS?2011.
[Goodfellow et al.2013] Ian J. Goodfellow, David
Warde-Farley, Mehdi Mirza, Aaron Courville, and
Yoshua Bengio. 2013. Maxout networks. In
ICML?2013.
[Graves2012] Alex Graves. 2012. Supervised Se-
quence Labelling with Recurrent Neural Networks.
Studies in Computational Intelligence. Springer.
[Hochreiter and Schmidhuber1997] S. Hochreiter and
J. Schmidhuber. 1997. Long short-term memory.
Neural Computation, 9(8):1735?1780.
[Kalchbrenner and Blunsom2013] Nal Kalchbrenner
and Phil Blunsom. 2013. Two recurrent continuous
translation models. In Proceedings of the ACL Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1700?1709.
[Koehn et al.2003] Philipp Koehn, Franz Josef Och,
and Daniel Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology - Volume 1, NAACL ?03, pages 48?54.
[Koehn2005] P. Koehn. 2005. Europarl: A parallel cor-
pus for statistical machine translation. In Machine
Translation Summit X, pages 79?86, Phuket, Thai-
land.
[Krizhevsky et al.2012] Alex Krizhevsky, Ilya
Sutskever, and Geoffrey Hinton. 2012. Ima-
geNet classification with deep convolutional neural
networks. In Advances in Neural Information
Processing Systems 25 (NIPS?2012).
[Marcu and Wong2002] Daniel Marcu and William
Wong. 2002. A phrase-based, joint probability
model for statistical machine translation. In Pro-
ceedings of the ACL-02 Conference on Empirical
Methods in Natural Language Processing - Volume
10, EMNLP ?02, pages 133?139.
[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever,
Kai Chen, Greg Corrado, and Jeff Dean. 2013. Dis-
tributed representations of words and phrases and
their compositionality. In Advances in Neural Infor-
mation Processing Systems 26, pages 3111?3119.
[Moore and Lewis2010] Robert C. Moore and William
Lewis. 2010. Intelligent selection of language
model training data. In Proceedings of the ACL
2010 Conference Short Papers, ACLShort ?10,
pages 220?224, Stroudsburg, PA, USA.
[Pascanu et al.2014] R. Pascanu, C. Gulcehre, K. Cho,
and Y. Bengio. 2014. How to construct deep recur-
rent neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.
[Saxe et al.2014] Andrew M. Saxe, James L. McClel-
land, and Surya Ganguli. 2014. Exact solutions
to the nonlinear dynamics of learning in deep lin-
ear neural networks. In Proceedings of the Second
International Conference on Learning Representa-
tions (ICLR 2014), April.
[Schwenk et al.2006] Holger Schwenk, Marta R. Costa-
Juss`a, and Jos?e A. R. Fonollosa. 2006. Continuous
space language models for the iwslt 2006 task. In
IWSLT, pages 166?173.
[Schwenk2007] Holger Schwenk. 2007. Continuous
space language models. Comput. Speech Lang.,
21(3):492?518, July.
[Schwenk2012] Holger Schwenk. 2012. Continuous
space translation models for phrase-based statisti-
cal machine translation. In Martin Kay and Chris-
tian Boitet, editors, Proceedings of the 24th Inter-
national Conference on Computational Linguistics
(COLIN), pages 1071?1080.
[Socher et al.2011] Richard Socher, Eric H. Huang, Jef-
frey Pennington, Andrew Y. Ng, and Christopher D.
Manning. 2011. Dynamic pooling and unfolding
recursive autoencoders for paraphrase detection. In
Advances in Neural Information Processing Systems
24.
[Son et al.2012] Le Hai Son, Alexandre Allauzen, and
Franc?ois Yvon. 2012. Continuous space transla-
tion models with neural networks. In Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, NAACL HLT ?12,
pages 39?48, Stroudsburg, PA, USA.
[van der Maaten2013] Laurens van der Maaten. 2013.
Barnes-hut-sne. In Proceedings of the First Inter-
national Conference on Learning Representations
(ICLR 2013), May.
[Vaswani et al.2013] Ashish Vaswani, Yinggong Zhao,
Victoria Fossum, and David Chiang. 2013. De-
coding with large-scale neural language models im-
proves translation. Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1387?1392.
[Zeiler2012] Matthew D. Zeiler. 2012. ADADELTA:
an adaptive learning rate method. Technical report,
arXiv 1212.5701.
[Zou et al.2013] Will Y. Zou, Richard Socher,
Daniel M. Cer, and Christopher D. Manning.
2013. Bilingual word embeddings for phrase-based
machine translation. In Proceedings of the ACL
Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1393?1398.
1734
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 11?15,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Collaborative Machine Translation Service for Scientific texts
Patrik Lambert
University of Le Mans
patrik.lambert@lium.univ-lemans.fr
Jean Senellart
Systran SA
senellart@systran.fr
Laurent Romary
Humboldt Universita?t Berlin /
INRIA Saclay - Ile de France
laurent.romary@inria.fr
Holger Schwenk
University of Le Mans
holger.schwenk@lium.univ-lemans.fr
Florian Zipser
Humboldt Universita?t Berlin
f.zipser@gmx.de
Patrice Lopez
Humboldt Universita?t Berlin /
INRIA Saclay - Ile de France
patrice.lopez@inria.fr
Fre?de?ric Blain
Systran SA /
University of Le Mans
frederic.blain@
lium.univ-lemans.fr
Abstract
French researchers are required to fre-
quently translate into French the descrip-
tion of their work published in English. At
the same time, the need for French people
to access articles in English, or to interna-
tional researchers to access theses or pa-
pers in French, is incorrectly resolved via
the use of generic translation tools. We
propose the demonstration of an end-to-end
tool integrated in the HAL open archive for
enabling efficient translation for scientific
texts. This tool can give translation sugges-
tions adapted to the scientific domain, im-
proving by more than 10 points the BLEU
score of a generic system. It also provides
a post-edition service which captures user
post-editing data that can be used to incre-
mentally improve the translations engines.
Thus it is helpful for users which need to
translate or to access scientific texts.
1 Introduction
Due to the globalisation of research, the English
language is today the universal language of sci-
entific communication. In France, regulations re-
quire the use of the French language in progress
reports, academic dissertations, manuscripts, and
French is the official educational language of the
country. This situation forces researchers to fre-
quently translate their own articles, lectures, pre-
sentations, reports, and abstracts between English
and French. In addition, students and the general
public are also challenged by language, when it
comes to find published articles in English or to
understand these articles. Finally, international
scientists not even consider to look for French
publications (for instance PhD theses) because
they are not available in their native languages.
This problem, incorrectly resolved through the
use of generic translation tools, actually reveals
an interesting generic problem where a commu-
nity of specialists are regularly performing trans-
lations tasks on a very limited domain. At the
same time, other communities of users seek trans-
lations for the same type of documents. Without
appropriate tools, the expertise and time spent for
translation activity by the first community is lost
and do not benefit to translation requests of the
other communities.
We propose the demonstration of an end-to-end
tool for enabling efficient translation for scientific
texts. This system, developed for the COSMAT
ANR project,1 is closely integrated into the HAL
open archive,2 a multidisciplinary open-access
archive which was created in 2006 to archive pub-
lications from all the French scientific commu-
nity. The tool deals with handling of source doc-
ument format, generally a pdf file, specialised
translation of the content, and user-friendly user-
interface allowing to post-edit the output. Behind
1http://www.cosmat.fr/
2http://hal.archives-ouvertes.fr/?langue=en
11
the scene, the post-editing tool captures user post-
editing data which are used to incrementally im-
prove the translations engines. The only equip-
ment required by this demonstration is a computer
with an Internet browser installed and an Internet
connection.
In this paper, we first describe the complete
work-flow from data acquisition to final post-
editing. Then we focus on the text extraction pro-
cedure. In Section 4, we give details about the
translation system. Then in section 5, we present
the translation and post-editing interface. We fi-
nally give some concluding remarks.
The system will be demonstrated at EACL in
his tight integration with the HAL paper deposit
system. If the organizers agree, we would like to
offer the use of our system during the EACL con-
ference. It would automatically translate all the
abstracts of the accepted papers and also offers
the possibility to correct the outputs. This result-
ing data would be made freely available.
2 Complete Processing Work-flow
The entry point for the system are ?ready to pub-
lish? scientific papers. The goal of our system
was to extract content keeping as many meta-
information as possible from the document, to
translate the content, to allow the user to perform
post-editing, and to render the result in a format as
close as possible to the source format. To train our
system, we collected from the HAL archive more
than 40 000 documents in physics and computer
science, including articles, PhD theses or research
reports (see Section 4). This material was used to
train the translation engines and to extract domain
bilingual terminology.
The user scenario is the following:
? A user uploads an article in PDF format3 on
the system.
? The document is processed by the open-
source Grobid tool (see section 3) to extract
3The commonly used publishing format is PDF files
while authoring format is principally a mix of Microsoft
Word file and LaTeX documents using a variety of styles.
The originality of our approach is to work on the PDF file
and not on these source formats. The rationale being that 1/
the source format is almost never available, 2/ even if we had
access to the source format, we would need to implement a
filter specific to each individual template required by such or
such conference for a good quality content extraction
the content. The extracted paper is structured
in the TEI format where title, authors, refer-
ences, footnotes, figure captions are identi-
fied with a very high accuracy.
? An entity recognition process is performed
for markup of domain entities such as:
chemical compounds for chemical papers,
mathematical formulas, pseudo-code and ob-
ject references in computer science papers,
but also miscellaneous acronyms commonly
used in scientific communication.
? Specialised terminology is then recognised
using the Termsciences4 reference termi-
nology database, completed with terminol-
ogy automatically extracted from the train-
ing corpus. The actual translation of the pa-
per is performed using adapted translation as
described in Section 4.
? The translation process generates a bilingual
TEI format preserving the source structure
and integrating the entity annotation, multi-
ple terminology choices when available, and
the token alignment between source and tar-
get sentences.
? The translation is proposed to the user for
post-editing through a rich interactive inter-
face described in Section 5.
? The final version of the document is then
archived in TEI format and available for dis-
play in HTML using dedicated XSLT style
sheets.
3 The Grobid System
Based on state-of-the-art machine learning tech-
niques, Grobid (Lopez, 2009) performs reliable
bibliographic data extraction from scholar articles
combined with multi-level term extraction. These
two types of extraction present synergies and cor-
respond to complementary descriptions of an arti-
cle.
This tool parses and converts scientific arti-
cles in PDF format into a structured TEI docu-
ment5 compliant with the good practices devel-
oped within the European PEER project (Bretel et
al., 2010). Grobid is trained on a set of annotated
4http://www.termsciences.fr
5http://www.tei-c.org
12
scientific article and can be re-trained to fit tem-
plates used for a specific conference or to extract
additional fields.
4 Translation of Scientific Texts
The translation system used is a Hybrid Machine
Translation (HMT) system from French to En-
glish and from English to French, adapted to
translate scientific texts in several domains (so
far physics and computer science). This sys-
tem is composed of a statistical engine, cou-
pled with rule-based modules to translate spe-
cial parts of the text such as mathematical for-
mulas, chemical compounds, pseudo-code, and
enriched with domain bilingual terminology (see
Section 2). Large amounts of monolingual and
parallel data are available to train a SMT system
between French and English, but not in the scien-
tific domain. In order to improve the performance
of our translation system in this task, we extracted
in-domain monolingual and parallel data from the
HAL archive. All the PDF files deposited in HAL
in computer science and physics were made avail-
able to us. These files were then converted to
plain text using the Grobid tool, as described in
the previous section. We extracted text from all
the documents from HAL that were made avail-
able to us to train our language model. We built
a small parallel corpus from the abstracts of the
PhD theses from French universities, which must
include both an abstract in French and in English.
Table 1 presents statistics of these in-domain data.
The data extracted from HAL were used to
adapt a generic system to the scientific litera-
ture domain. The generic system was mostly
trained on data provided for the shared task of
Sixth Workshop on Statistical Machine Transla-
tion6 (WMT 2011), described in Table 2.
Table 3 presents results showing, in the
English?French direction, the impact on the sta-
tistical engine of introducing the resources ex-
tracted from HAL, as well as the impact of do-
main adaptation techniques. The baseline statis-
tical engine is a standard PBSMT system based
on Moses (Koehn et al 2007) and the SRILM
tookit (Stolcke, 2002). Is was trained and tuned
only on WMT11 data (out-of-domain). Incorpo-
rating the HAL data into the language model and
tuning the system on the HAL development set,
6http://www.statmt.org/wmt11/translation-task.html
Set Domain Lg Sent. Words Vocab.
Parallel data
Train cs+phys En 55.9 k 1.41 M 43.3 k
Fr 55.9 k 1.63 M 47.9 k
Dev cs En 1100 25.8 k 4.6 k
Fr 1100 28.7 k 5.1 k
phys En 1000 26.1 k 5.1 k
Fr 1000 29.1 k 5.6 k
Test cs En 1100 26.1 k 4.6 k
Fr 1100 29.2 k 5.2 k
phys En 1000 25.9 k 5.1 k
Fr 1000 28.8 k 5.5 k
Monolingual data
Train cs En 2.5 M 54 M 457 k
Fr 761 k 19 M 274 k
phys En 2.1 M 50 M 646 k
Fr 662 k 17 M 292 k
Table 1: Statistics for the parallel training, develop-
ment, and test data sets extracted from thesis abstracts
contained in HAL, as well as monolingual data ex-
tracted from all documents in HAL, in computer sci-
ence (cs) and physics (phys). The following statistics
are given for the English (En) and French (Fr) sides
(Lg) of the corpus: the number of sentences, the num-
ber of running words (after tokenisation) and the num-
ber of words in the vocabulary (M and k stand for mil-
lions and thousands, respectively).
yielded a gain of more than 7 BLEU points, in
both domains (computer science and physics). In-
cluding the theses abstracts in the parallel training
corpus, a further gain of 2.3 BLEU points is ob-
served for computer science, and 3.1 points for
physics. The last experiment performed aims at
increasing the amount of in-domain parallel texts
by translating automatically in-domain monolin-
gual data, as suggested by Schwenk (2008). The
synthesised bitext does not bring new words into
the system, but increases the probability of in-
domain bilingual phrases. By adding a synthetic
bitext of 12 million words to the parallel training
data, we observed a gain of 0.5 BLEU point for
computer science, and 0.7 points for physics.
Although not shown here, similar results were
obtained in the French?English direction. The
French?English system is actually slightly bet-
ter than the English?French one as it is an easier
translation direction.
13
Translation Model Language Model Tuning Domain CS PHYS
words (M) Bleu words (M) Bleu
wmt11 wmt11 wmt11 371 27.3 371 27.1
wmt11 wmt11+hal hal 371 36.0 371 36.2
wmt11+hal wmt11+hal hal 287 38.3 287 39.3
wmt11+hal+adapted wmt11+hal hal 299 38.8 307 40.0
Table 3: Results (BLEU score) for the English?French systems. The type of parallel data used to train the
translation model or language model are indicated, as well as the set (in-domain or out-of-domain) used to tune
the models. Finally, the number of words in the parallel corpus and the BLEU score on the in-domain test set are
indicated for each domain: computer science and physics.
Figure 1: Translation and post-editing interface.
Corpus English French
Bitexts:
Europarl 50.5M 54.4M
News Commentary 2.9M 3.3M
Crawled (109 bitexts) 667M 794M
Development data:
newstest2009 65k 73k
newstest2010 62k 71k
Monolingual data:
LDC Gigaword 4.1G 920M
Crawled news 2.6G 612M
Table 2: Out-of-domain development and training data
used (number of words after tokenisation).
5 Post-editing Interface
The collaborative aspect of the demonstrated ma-
chine translation service is based on a post-editing
tool, whose interface is shown in Figure 1. This
tool provides the following features:
? WYSIWYG display of the source and target
texts (Zones 1+2)
? Alignment at the sentence level (Zone 3)
? Zone to review the translation with align-
ment of source and target terms (Zone 4) and
terminology reference (Zone 5)
? Alternative translations (Zone 6)
The tool allows the user to perform sentence
level post-editing and records details of post-
editing activity, such as keystrokes, terminology
selection, actual edits and time log for the com-
plete action.
6 Conclusions and Perspectives
We proposed the demonstration of an end-to-end
tool integrated into the HAL archive and enabling
14
efficient translation for scientific texts. This tool
consists of a high-accuracy PDF extractor, a hy-
brid machine translation engine adapted to the sci-
entific domain and a post-edition tool. Thanks to
in-domain data collected from HAL, the statisti-
cal engine was improved by more than 10 BLEU
points with respect to a generic system trained on
WMT11 data.
Our system was deployed for a physic confer-
ence organised in Paris in Sept 2011. All accepted
abstracts were translated into author?s native lan-
guages (around 70% of them) and proposed for
post-editing. The experience was promoted by
the organisation committee and 50 scientists vol-
unteered (34 finally performed their post-editing).
The same experience will be proposed for authors
of the LREC conference. We would like to offer
a complete demonstration of the system at EACL.
The goal of these experiences is to collect and dis-
tribute detailed ?post-editing? data for enabling
research on this activity.
Acknowledgements
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004).
References
Foudil Bretel, Patrice Lopez, Maud Medves, Alain
Monteil, and Laurent Romary. 2010. Back to
meaning ? information structuring in the PEER
project. In TEI Conference, Zadar, Croatie.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proc. of the 45th Annual
Meeting of the Association for Computational Lin-
guistics (Demo and Poster Sessions), pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Patrice Lopez. 2009. GROBID: Combining auto-
matic bibliographic data recognition and term ex-
traction for scholarship publications. In Proceed-
ings of ECDL 2009, 13th European Conference on
Digital Library, Corfu, Greece.
Holger Schwenk. 2008. Investigations on large-scale
lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
15
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 832?840,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Multi-Domain Translation Model Framework
for Statistical Machine Translation
Rico Sennrich
Institute of Computational Linguistics
University of Zurich
Binzmu?hlestr. 14
CH-8050 Zu?rich
sennrich@cl.uzh.ch
Holger Schwenk and Walid Aransa
LIUM, University of Le Mans
72085 Le Mans cedex 9, France
lastname@lium.univ-lemans.fr
Abstract
While domain adaptation techniques for
SMT have proven to be effective at im-
proving translation quality, their practical-
ity for a multi-domain environment is of-
ten limited because of the computational
and human costs of developing and main-
taining multiple systems adapted to differ-
ent domains. We present an architecture
that delays the computation of translation
model features until decoding, allowing
for the application of mixture-modeling
techniques at decoding time. We also de-
scribe a method for unsupervised adapta-
tion with development and test data from
multiple domains. Experimental results on
two language pairs demonstrate the effec-
tiveness of both our translation model ar-
chitecture and automatic clustering, with
gains of up to 1 BLEU over unadapted sys-
tems and single-domain adaptation.
1 Introduction
The effectiveness of domain adaptation ap-
proaches such as mixture-modeling (Foster and
Kuhn, 2007) has been established, and has led to
research on a wide array of adaptation techniques
in SMT, for instance (Matsoukas et al, 2009; Shah
et al, 2012). In all these approaches, adaptation is
performed during model training, with respect to a
representative development corpus, and the mod-
els are kept unchanged when then system is de-
ployed. Therefore, when working with multiple
and/or unlabelled domains, domain adaptation is
often impractical for a number of reasons. Firstly,
maintaining multiple systems for each language
pair, each adapted to a different domain, is costly
in terms of computational and human resources:
the full system development pipeline needs to be
performed for all identified domains, all the mod-
els are separately stored and need to be switched at
runtime. This is impractical in many real applica-
tions, in particular a web translation service which
is faced with texts coming from many different do-
mains. Secondly, domain adaptation bears a risk
of performance loss. If there is a mismatch be-
tween the domain of the development set and the
test set, domain adaptation can potentially harm
performance compared to an unadapted baseline.
We introduce a translation model architecture
that delays the computation of features to the de-
coding phase. The calculation is based on a vec-
tor of component models, with each component
providing the sufficient statistics necessary for the
computation of the features. With this framework,
adaptation to a new domain simply consists of up-
dating a weight vector, and multiple domains can
be supported by the same system.
We also present a clustering approach for un-
supervised adaptation in a multi-domain environ-
ment. In the development phase, a set of develop-
ment data is clustered, and the models are adapted
to each cluster. For each sentence that is being
decoded, we choose the weight vector that is op-
timized on the closest cluster, allowing for adap-
tation even with unlabelled and heterogeneous test
data.
2 Related Work
(Ortiz-Mart??nez et al, 2010) delay the compu-
tation of translation model features for the pur-
pose of interactive machine translation with online
training. The main difference to our approach is
that we store sufficient statistics not for a single
model, but a vector of models, which allows us to
832
weight the contribution of each component model
to the feature calculation. The similarity suggests
that our framework could also be used for inter-
active learning, with the ability to learn a model
incrementally from user feedback, and weight it
differently than the static models, opening new re-
search opportunities.
(Sennrich, 2012b) perform instance weighting
of translation models, based on the sufficient
statistics. Our framework implements this idea,
with the main difference that the actual combina-
tion is delayed until decoding, to support adapta-
tion to multiple domains in a single system.
(Razmara et al, 2012) describe an ensemble de-
coding framework which combines several trans-
lation models in the decoding step. Our work is
similar to theirs in that the combination is done
at runtime, but we also delay the computation of
translation model probabilities, and thus have ac-
cess to richer sufficient statistics. In principle,
our architecture can support all mixture operations
that (Razmara et al, 2012) describe, plus addi-
tional ones such as forms of instance weighting,
which are not possible after the translation proba-
bilities have been computed.
(Banerjee et al, 2010) focus on the problem of
domain identification in a multi-domain setting.
They use separate translation systems for each do-
main, and a supervised setting, whereas we aim
for a system that integrates support for multiple
domains, with or without supervision.
(Yamamoto and Sumita, 2007) propose unsu-
pervised clustering at both training and decoding
time. The training text is divided into a number
of clusters, a model is trained on each, and during
decoding, each sentence is assigned to the clos-
est cluster-specific model. Our approach bears re-
semblance to this clustering, but is different in that
Yamamoto and Sumita assign each sentence to the
closest model, and use this model for decoding,
whereas in our approach, each cluster is associ-
ated with a mixture of models that is optimized to
the cluster, and the number of clusters need not be
equal to the number of component models.
3 Translation Model Architecture
This section covers the architecture of the multi-
domain translation model framework. Our transla-
tion model is embedded in a log-linear model as is
common for SMT, and treated as a single transla-
tion model in this log-linear combination. We im-
plemented this architecture for phrase-based mod-
els, and will use this terminology to describe it,
but in principle, it can be extended to hierarchical
or syntactic models.
The architecture has two goals: move the calcu-
lation of translation model features to the decoding
phase, and allow for multiple knowledge sources
(e.g. bitexts or user-provided data) to contribute to
their calculation. Our immediate purpose for this
paper is domain adaptation in a multi-domain en-
vironment, but the delay of the feature computa-
tion has other potential applications, e.g. in inter-
active MT.
We are concerned with calculating four features
during decoding, henceforth just referred to as the
translation model features: p(s|t), lex(s|t), p(t|s)
and lex(t|s). s and t denote the source and target
phrase. We follow the definitions in (Koehn et al,
2003).
Traditionally, the phrase translation probabili-
ties p(s|t) and p(t|s) are estimated through un-
smoothed maximum likelihood estimation (MLE).
p(x|y) = c(x, y)c(y) =
c(x, y)?
x? c(x?, y)
(1)
where c denotes the count of an observation, and
p the model probability.
The lexical weights lex(s|t) and lex(t|s) are
calculated as follows, using a set of word align-
ments a between s and t:1
lex(s|t, a) =
n?
i=1
1
|{j|(i, j) ? a}|
?
?(i,j)?a
w(si|tj)
(2)
A special NULL token is added to t and aligned to
each unaligned word in s. w(si|tj) is calculated
through MLE, as in equation 1, but based on the
word (pair) frequencies.
To combine statistics from a vector of n com-
ponent corpora, we can use a weighted version of
equation 1, which adds a weight vector ? of length
n (Sennrich, 2012b):
p(x|y;?) =
?n
i=1 ?ici(x, y)?n
i=1
?
x? ?ici(x?, y)
(3)
The word translation probabilities w(ti|sj) are de-
fined analogously, and used in equation 2 for a
weighted version.
1The equation shows lex(s|t); lex(t|s) is computed anal-
ogously.
833
In order to compute the translation model fea-
tures online, a number of sufficient statistics need
to be accessible at decoding time. For p(s|t)
and p(t|s), we require the statistics c(s), c(t) and
c(s, t). For accessing them during decoding, we
simply store them in the decoder?s data struc-
ture, rather than storing pre-computed translation
model features. This means that we can use exist-
ing, compact data formats for storing and access-
ing them.2
The statistics are accessed when the decoder
collects all translation options for a phrase s in the
source sentence. We then access all translation op-
tions for each component table, obtaining a vector
of statistics c(s) for the source phrase, and c(t) and
c(s, t) for each potential target phrase. For phrase
pairs which are not found, c(s, t) and c(t) are ini-
tially set to 0.
Note that c(t) is potentially incorrect at this
point, since a phrase pair not being found does
not entail that c(t) is 0. After all tables have been
accessed, and we thus know the full set of possi-
ble translation options (s, t), we perform a second
round of lookups for all c(t) in the vector which
are still set to 0. We introduce a second table for
accessing c(t) efficiently, again storing it in the de-
coder?s data structure. We can easily create such a
table by inverting the source and target phrases,
deduplicating it for compactness (we only need
one entry per target phrase), and storing c(t) as
only feature.
For lex(s|t), we require an alignment a, plus
c(tj) and c(si, tj) for all pairs (i, j) in a. lex(t|s)
can be based on the same alignment a (with the ex-
ception of NULL alignments, which can be added
online), but uses statistics c(sj) and c(ti, sj). For
estimating the lexical probabilities, we load the
frequencies into a vector of four hash tables.3
Both space and time complexity of the lookup
is linear to the number of component tables. We
deem it is still practical because the collection of
translation options is typically only a small frac-
tion of total decoding time, with search making
up the largest part. For storing and accessing the
sufficient statistics (except for the word (pair) fre-
quencies), we use an on-disk data structure pro-
2We have released an implementation of the architecture
as part of the Moses decoder.
3c(s, t) and c(t, s) are not identical since the lexical
probabilities are based on the unsymmetrized word align-
ment frequencies (in the Moses implementation which we re-
implement).
phrase (pair) c1(x) c2(x)
row 300 80
(row, Zeile) 240 20
(row, Reihe) 60 60
? p(Zeile|row) p(Reihe|row)
(1, 1) 0.68 0.32
(1, 10) 0.40 0.60
(10, 1) 0.79 0.21
Table 1: Illustration of instance weighting with
weight vectors for two corpora.
vided by Moses, which reduces the memory re-
quirements. Still, the number of components may
need to be reduced, for instance through clustering
of training data (Sennrich, 2012a).
With a small modification, our framework could
be changed to use a single table that stores a vec-
tor of n statistics instead of a vector of n tables.
While this would be more compact in terms of
memory, and keep the number of table lookups in-
dependent of the number of components, we chose
a vector of n tables for its flexibility. With a vec-
tor of tables, tables can be quickly added to or re-
moved from the system (conceivable even at run-
time), and can be polymorph. One applications
where this could be desirable is interactive ma-
chine translation, where one could work with a
mix of compact, static tables, and tables designed
to be incrementally trainable.
In the unweighted variant, the resulting fea-
tures are equivalent to training on the concatena-
tion of all training data, excepting differences in
word alignment, pruning4 and rounding. The ar-
chitecture can thus be used as a drop-in replace-
ment for a baseline system that is trained on con-
catenated training data, with non-uniform weights
only being used for texts for which better weights
have been established. This can be done either us-
ing domain labels or unsupervised methods as de-
scribed in the next section.
As a weighted combination method, we imple-
mented instance weighting as described in equa-
tion 3. Table 1 shows the effect of weighting two
corpora on the probability estimates for the trans-
lation of row. German Zeile (row in a table) is pre-
dominant in a bitext from the domain IT, whereas
4We prune the tables to the most frequent 50 phrase pairs
per source phrase before combining them, since calculat-
ing the features for all phrase pairs of very common source
phrases causes a significant slow-down. We found that this
had no significant effects on BLEU.
834
0 1 2 3 4 5 60
1
2
3
4
5
6
entropy with KDE LM (IT)
entr
opy
with
Acq
uisL
M(L
EGA
L)
gold clusters
0 1 2 3 4 5 60
1
2
3
4
5
6
entropy with KDE LM (IT)
entr
opy
with
Acq
uisL
M(L
EGA
L)
clustering with Euclidean distance
0 1 2 3 4 5 60
1
2
3
4
5
6
entropy with KDE LM (IT)
entr
opy
with
Acq
uisL
M(L
EGA
L)
clustering with cosine similarity
Figure 1: Clustering of data set which contains sentences from two domains: LEGAL and IT. Compari-
son between gold segmentation, and clustering with two alternative distance/similarity measures. Black:
IT; grey: LEGAL.
Reihe (line of objects) occurs more often in a legal
corpus. Note that the larger corpus (or more pre-
cisely, the one in which row occurs more often)
has a stronger impact on the probability distribu-
tion with uniform weights (or in a concatenation of
data sets). Instance weighting allows us to modify
the contribution of each corpus. In our implemen-
tation, the weight vector is set globally, but can be
overridden on a per-sentence basis. In principle,
using different weight vectors for different phrase
pairs in a sentence is conceivable. The framework
can also be extended to support other combination
methods, such as a linear interpolation of models.
4 Unsupervised Clustering for Online
Translation Model Adaptation
The framework supports decoding each sentence
with a separate weight vector of size 4n, 4 being
the number of translation model features whose
computation can be weighted, and n the number
of model components. We now address the ques-
tion of how to automatically select good weights in
a multi-domain task. As a way of optimizing in-
stance weights, (Sennrich, 2012b) minimize trans-
lation model perplexity on a set of phrase pairs,
automatically extracted from a parallel develop-
ment set. We follow this technique, but want to
have multiple weight vectors, adapted to different
texts, between which the system switches at de-
coding time. The goal is to perform domain adap-
tation without requiring domain labels or user in-
put, neither for development nor decoding.
The basic idea consists of three steps:
1. Cluster a development set into k clusters.
2. Optimize translation model weights for each
cluster.
3. For each sentence in the test set, assign it
to the nearest cluster and use the translation
model weights associated with the cluster.
For step 2, we use the algorithm by (Sennrich,
2012b), implemented in the decoder to allow for a
quick optimization of a running system. We will
here discuss steps 1 and 3 in more detail.
4.1 Clustering the Development Set
We use k-means clustering to cluster the sentences
of the development set. We train a language model
on the source language side of each of the n
component bitexts, and compute an n-dimensional
vector for each sentence by computing its entropy
with each language model. Our aim is not to dis-
criminate between sentences that are more likely
and unlikely in general, but to cluster on the ba-
sis of relative differences between the language
model entropies. For this purpose, we choose
the cosine as our similarity measure. Figure 1
illustrates clustering in a two-dimensional vector
space, and demonstrates that Euclidean distance is
unsuitable because it may perform a clustering that
is irrelevant to our purposes.
As a result of development set clustering, we
obtain a bitext for each cluster, which we use to
optimize the model weights, and a centroid per
cluster. At decoding time, we need only perform
an assignment step. Each test set sentence is as-
signed to the centroid that is closest to it in the
vector space.
4.2 Scalability Considerations
Our theoretical expectation is that domain adapta-
tion will fail to perform well if the test data is from
835
a different domain than the development data, or
if the development data is a heterogeneous mix
of domains. A multi-domain setup can mitigate
this risk, but only if the relevant domain is repre-
sented in the development data, and if the devel-
opment data is adequately segmented for the op-
timization. We thus suggest that the development
data should contain enough data from all domains
that one wants to adapt to, and a high number of
clusters.
While the resource requirements increase with
the number of component models, increasing the
number of clusters is computationally cheap at
runtime. Only the clustering of the develop-
ment set and optimization of the translation model
weights for each clusters is affected by k. This
means that the approach can in principle be scaled
to a high number of clusters, and support a high
number of domains.5
The biggest risk of increasing the number of
clusters is that if the clusters become too small,
perplexity minimization may overfit these small
clusters. We will experiment with different num-
bers of clusters, but since we expect the optimal
number of clusters to depend on the amount of
development data, and the number of domains,
we cannot make generalized statements about the
ideal number of k.
While it is not the focus of this paper, we also
evaluate language model adaptation. We perform
a linear interpolation of models for each clus-
ter, with interpolation coefficients optimized us-
ing perplexity minimization on the development
set. The cost of moving language model interpo-
lation into the decoding phase is far greater than
for translation models, since the number of hy-
potheses that need to be evaluated by the language
model is several orders of magnitudes higher than
the number of phrase pairs used during the trans-
lation. For the experiments with language model
adaptation, we have chosen to perform linear in-
terpolation offline, and perform language model
switching during decoding. While model switch-
ing is a fast operation, it also makes the space com-
plexity of storing the language models linear to the
number of clusters. For scaling the approach to a
high number of clusters, we envision that multi-
5If the development set is labelled, one can also use a gold
segmentation of development sets instead of k-means cluster-
ing. At decoding time, cluster assignment can be performed
by automatically assigning each sentence to the closest cen-
troid, or again through gold labels, if available.
data set sentences words (de)
kde 216 000 1 990 000
kdedoc 2880 41 000
kdegb 51 300 450 000
oo 41 000 434 000
oo3 56 800 432 000
php 38 500 301 000
tm 146 000 2 740 000
acquis 2 660 000 58 900 000
dgt 372 000 8 770 000
ecb 110 000 2 850 000
ep7 1 920 000 50 500 000
nc7 159 000 3 950 000
total (train) 5 780 000 131 000 000
dev (IT) 3500 47 000
dev (LEGAL) 2000 46 800
test (IT) 5520 51 800
test (LEGAL) 9780 250 000
Table 2: Parallel data sets English?German.
data set sentences words (en)
eu 1 270 000 25 600 000
fiction 830 000 13 700 000
navajo 30 000 490 000
news 110 000 2 550 000
paraweb 370 000 3 930 000
subtitles 2 840 000 21 200 000
techdoc 970 000 7 270 000
total (train) 6 420 000 74 700 000
dev 3500 50 700
test 3500 49 600
Table 3: Parallel data sets Czech?English.
pass decoding, with an unadapted language model
in the first phase, and rescoring with a language
model adapted online, could perform adequately,
and keep the complexity independent of the num-
ber of clusters.
5 Evaluation
5.1 Data and Methods
We conduct all experiments with Moses (Koehn et
al., 2007), SRILM (Stolcke, 2002), and GIZA++
(Och and Ney, 2003). Log-linear weights are op-
timized using MERT (Och and Ney, 2003). We
keep the word alignment and lexical reordering
models constant through the experiments to min-
imize the number of confounding factors. We re-
port translation quality using BLEU (Papineni et
836
system TM adaptation LM adaptation TM+LM adaptationIT LEGAL IT LEGAL IT LEGAL
baseline 21.1 49.9 21.1 49.9 21.1 49.9
1 cluster (no split) 21.3* 49.9 21.8* 49.7 21.8* 49.8
2 clusters 21.6* 49.9 22.2* 50.4* 22.8* 50.2*
4 clusters 21.7* 49.9 23.1* 50.2* 22.6* 50.2*
8 clusters 22.1* 49.9 23.1* 50.1* 22.7* 50.3*
16 clusters 21.1 49.9 22.6* 50.3* 21.9* 50.1*
gold clusters 21.8* 50.1* 22.4* 50.1* 23.2* 49.9
Table 4: Translation experiments EN?DE. BLEU scores reported.
al., 2002). We account for optimizer instability
by running 3 independent MERT runs per system,
and performing significance testing with MultEval
(Clark et al, 2011). Systems significantly better
than the baseline with p < 0.01 are marked with
(*).
We conduct experiments on two data sets. The
first is an English?German translation task with
two domains, texts related to information technol-
ogy (IT) and legal documents (LEGAL). We use
data sets from both domains, plus out-of-domain
corpora, as shown in table 2. 7 data sets come from
the domain IT: 6 from OPUS (Tiedemann, 2009)
and a translation memory (tm) provided by our in-
dustry partner. 3 data sets are from the legal do-
main: the ECB corpus from OPUS, plus the JRC-
Acquis (Steinberger et al, 2006) and DGT-TM
(Steinberger et al, 2012). 2 data sets are out-of-
domain, made available by the 2012 Workshop on
Statistical Machine Translation (Callison-Burch et
al., 2012). The development sets are random sam-
ples from the respective in-domain bitexts (held-
out from training). The test sets have been pro-
vided by Translated, our industry partner in the
MATECAT project.
Our second data set is CzEng 0.9, a Czech?
English parallel corpus (Bojar and Zabokrtsky?,
2009). It contains text from 7 different sources, on
which we train separate component models. The
size of the corpora is shown in table 3. As de-
velopment and test sets, we use 500 sentences of
held-out data per source.
For both data sets, language models are trained
on the target side of the bitexts. In all experiments,
we keep the number of component models con-
stant: 12 for EN?DE, 7 for CZ?EN. We vary the
number of clusters k from 1, which corresponds to
adapting the models to the full development set, to
16. The baseline is the concatenation of all train-
Data set ?IT ?LEGAL ?cluster 1 ?cluster 2
kde 1.0 1.0 1.0 1.0
kdedoc 0.64 12.0 86.0 6.4
kdegb 1.6 2.3 1.7 2.7
oo 0.76 1.6 0.73 1.7
oo3 1.8 4.7 2.4 2.7
php 0.79 6.3 0.69 3.5
tm 1.3 1.3 1.5 1.1
acquis 0.024 3.5 0.018 1.9
dgt 0.053 4.5 0.033 2.4
ecb 0.071 2.3 0.039 1.2
ep7 0.037 0.53 0.024 0.29
nc7 0.1 1.1 0.063 0.62
Table 5: Weight vectors for feature p(t|s) opti-
mized on four development sets (from gold split
and clustering with k = 2).
ing data, with no adaptation performed. We also
evaluate the labelled setting, where instead of un-
supervised clustering, we use gold labels to split
the development and test sets, and adapt the mod-
els to each labelled domain.
5.2 Results
Table 4 shows results for the EN?DE data set. For
our clustering experiments, the development set is
the concatenation of the LEGAL and IT develop-
ment sets. However, we always use the gold seg-
mentation between LEGAL and IT for MERT and
testing. This allows for a detailed analysis of the
effect of development data clustering for the pur-
pose of model adaptation. In an unlabelled setting,
one would have to run MERT either on the full de-
velopment set (as we will do for the CZ?EN task)
or separately on each cluster, or use an alternative
approach to optimize log-linear weights in a multi-
domain setting, such as feature augmentation as
described by (Clark et al, 2012).
837
system TM adaptation LM adaptation TM+LM adaptation
baseline 34.4 34.4 34.4
1 cluster (no split) 34.5 33.7 34.1
2 clusters 34.6 34.0 34.4
4 clusters 34.7* 34.3 34.6
8 clusters 34.7* 34.5 34.9*
16 clusters 34.7* 34.7* 35.0*
gold clusters 35.0* 35.0* 35.4*
Table 6: Translation experiments CZ?EN. BLEU scores reported.
We find that an adaptation of the TM and LM
to the full development set (system ?1 cluster?)
yields the smallest improvements over the un-
adapted baseline. The reason for this is that the
mixed-domain development set is not representa-
tive for the respective test sets. Using multiple
adapted systems yields better performance. For
the IT test set, the system with gold labels and TM
adaptation yields an improvement of 0.7 BLEU
(21.1 ? 21.8), LM adaptation yields 1.3 BLEU
(21.1 ? 22.4), and adapting both models outper-
forms the baseline by 2.1 BLEU (21.1 ? 23.2).
The systems that use unsupervised clusters reach
a similar level of performance than those with
gold clusters, with best results being achieved
by the systems with 2?8 clusters. Some sys-
tems outperform both the baseline and the gold
clusters, e.g. TM adaptation with 8 clusters
(21.1 ? 21.8 ? 22.1), or LM adaptation with 4
or 8 clusters (21.1 ? 22.4 ? 23.1).
Results with 16 clusters are slightly worse than
those with 2?8 clusters due to two effects. Firstly,
for the system with adapted TM, one of the three
MERT runs is an outlier, and the reported BLEU
score of 21.1 is averaged from the three MERT
runs achieving 22.1, 21.6, and 19.6 BLEU, respec-
tively. Secondly, about one third of the IT test
set is assigned to a cluster that is not IT-specific,
which weakens the effect of domain adaptation for
the systems with 16 clusters.
For the LEGAL subset, gains are smaller. This
can be explained by the fact that the majority of
training data is already from the legal domain,
which makes it unnecessary to boost its impact on
the probability distribution even further.
Table 5 shows the automatically obtained trans-
lation model weight vectors for two systems,
?gold clusters? and ?2 clusters?, for the feature
p(t|s). It illustrates that all the corpora that we
consider out-of-domain for IT are penalized by
a factor of 10?50 (relative to the in-domain kde
corpus) for the computation of this feature. For
the LEGAL domain, the weights are more uni-
form, which is congruent with our observation that
BLEU changes little.
Table 6 shows results for the CZ?EN data set.
For each system, MERT is performed on the full
development set. As in the first task, adaptation to
the full development set is least effective. The sys-
tems with unsupervised clusters significantly out-
perform the baseline. For the system with 16 clus-
ters, we observe an improvement of 0.3 BLEU for
TM adaptation, and 0.6 BLEU for adapting both
models (34.4 ? 34.7 ? 35.0). The labelled sys-
tem, i.e. the system with 7 clusters corresponding
to the 7 data sources, both for the development and
test set, performs best. We observe gains of 0.6
BLEU (34.4 ? 35.0) for TM or LM adaptation,
and 1 BLEU (34.4 ? 35.4) when both models are
adapted.
We conclude that the translation model archi-
tecture is effective in a multi-domain setting, both
with unsupervised clusters and labelled domains.
The fact that language model adaptation yields an
additional improvement in our experiments sug-
gests that it it would be worthwhile to also inves-
tigate a language model data structure that effi-
ciently supports multiple domains.
6 Conclusion
We have presented a novel translation model ar-
chitecture that delays the computation of trans-
lation model features to the decoding phase, and
uses a vector of component models for this com-
putation. We have also described a usage scenario
for this architecture, namely its ability to quickly
switch between weight vectors in order to serve as
an adapted model for multiple domains. A sim-
ple, unsupervised clustering of development data
is sufficient to make use of this ability and imple-
838
ment a multi-domain translation system. If avail-
able, one can also use the architecture in a labelled
setting.
Future work could involve merging our trans-
lation model framework with the online adapta-
tion of other models, or the log-linear weights.
Our approach is orthogonal to that of (Clark et
al., 2012), who perform feature augmentation to
obtain multiple sets of adapted log-linear weights.
While (Clark et al, 2012) use labelled data, their
approach could in principle also be applied after
unsupervised clustering.
The translation model framework could also
serve as the basis of real-time adaptation of trans-
lation systems, e.g. by using incremental means to
update the weight vector, or having an incremen-
tally trainable component model that learns from
the post-edits by the user, and is assigned a suit-
able weight.
Acknowledgments
This research was partially funded by the
Swiss National Science Foundation under grant
105215 126999, the European Commission
(MATECAT, ICT-2011.4.2 287688) and the
DARPA BOLT project.
References
Pratyush Banerjee, Jinhua Du, Baoli Li, Sudip Kumar
Naskar, Andy Way, and Josef Van Genabith. 2010.
Combining multi-domain statistical machine trans-
lation models using automatic classifiers. In 9th
Conference of the Association for Machine Trans-
lation in the Americas (AMTA 2010), Denver, Col-
orado, USA.
Ondrej Bojar and Zdenek Zabokrtsky?. 2009. Czeng
0.9: Large parallel treebank with rich annotation.
Prague Bull. Math. Linguistics, 92:63?84.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
176?181, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Jonathan H. Clark, Alon Lavie, and Chris Dyer. 2012.
One system, many domains: Open-domain statisti-
cal machine translation via feature augmentation. In
Conference of the Association for Machine Transla-
tion in the Americas 2012 (AMTA 2012), San Diego,
California, USA.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, StatMT ?07, pages 128?135, Prague, Czech
Republic. Association for Computational Linguis-
tics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
NAACL ?03: Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology, pages 48?54, Edmonton, Canada. As-
sociation for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and Bing
Zhang. 2009. Discriminative corpus weight estima-
tion for machine translation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 2 - Volume 2, pages
708?717, Singapore. Association for Computational
Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2010. Online learning for in-
teractive statistical machine translation. In HLT-
NAACL, pages 546?554. The Association for Com-
putational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, Pennsylvania, USA. Association for
Computational Linguistics.
Majid Razmara, George Foster, Baskaran Sankaran,
and Anoop Sarkar. 2012. Mixing multiple trans-
lation models in statistical machine translation. In
839
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics, Jeju, Re-
public of Korea. Association for Computational Lin-
guistics.
Rico Sennrich. 2012a. Mixture-modeling with unsu-
pervised clusters for domain adaptation in statistical
machine translation. In 16th Annual Conference of
the European Association for Machine Translation
(EAMT 2012), pages 185?192, Trento, Italy.
Rico Sennrich. 2012b. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 539?
549, Avignon, France. Association for Computa-
tional Linguistics.
Kashif Shah, Loc Barrault, and Holger Schwenk.
2012. A general framework to weight heteroge-
neous parallel data for model adaptation in statistical
machine translation. In Conference of the Associa-
tion for Machine Translation in the Americas 2012
(AMTA 2012), San Diego, California, USA.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
Daniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC?2006),
Genoa, Italy.
Ralf Steinberger, Andreas Eisele, Szymon Klocek,
Spyridon Pilos, and Patrick Schlu?ter. 2012. DGT-
TM: A freely available translation memory in 22 lan-
guages. In Proceedings of the Eight International
Conference on Language Resources and Evaluation
(LREC?12), Istanbul, Turkey. European Language
Resources Association (ELRA).
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Seventh International
Conference on Spoken Language Processing, pages
901?904, Denver, CO, USA.
Jo?rg Tiedemann. 2009. News from OPUS - a col-
lection of multilingual parallel corpora with tools
and interfaces. In N. Nicolov, K. Bontcheva,
G. Angelova, and R. Mitkov, editors, Recent
Advances in Natural Language Processing, vol-
ume V, pages 237?248. John Benjamins, Amster-
dam/Philadelphia, Borovets, Bulgaria.
Hirofumi Yamamoto and Eiichiro Sumita. 2007. Bilin-
gual cluster based models for statistical machine
translation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 514?523, Prague, Czech Republic.
840
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 121?126,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
LIUM SMT Machine Translation System for WMT 2010
Patrik Lambert, Sadaf Abdul-Rauf and Holger Schwenk
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French ma-
chine translation systems for the 2010
WMT shared task evaluation. These sys-
tems were standard phrase-based statisti-
cal systems based on the Moses decoder,
trained on the provided data only. Most
of our efforts were devoted to the choice
and extraction of bilingual data used for
training. We filtered out some bilingual
corpora and pruned the phrase table. We
also investigated the impact of adding two
types of additional bilingual texts, ex-
tracted automatically from the available
monolingual data. We first collected bilin-
gual data by performing automatic trans-
lations of monolingual texts. The second
type of bilingual text was harvested from
comparable corpora with Information Re-
trieval techniques.
1 Introduction
This paper describes the machine translation sys-
tems developed by the Computer Science labora-
tory at the University of Le Mans (LIUM) for the
2010 WMT shared task evaluation. We only con-
sidered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Schwenk
et al, 2009) are as follows: restriction to the data
recommended for the workshop, usage of the (fil-
tered) French?English gigaword bitext, pruning of
the phrase table, and usage of automatic trans-
lations of the monolingual news corpus to im-
prove the translation model. We also used a larger
amount of bilingual data extracted from compara-
ble corpora than was done in 2009. These different
points are described in the rest of the paper, to-
gether with a summary of the experimental results
showing the impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used
to train the translation and language models of the
system.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations may be used
directly with the source texts to build additional
bitexts, or as queries of an Information Retrieval
(IR) system to extract new bitexts from compara-
ble corpora. In a second stage, these additional
bilingual data were incorporated to the system (see
Section 4 and Tables 1 and 2).
The latest version of the News-Commentary
(NC) corpus, of the Europarl (Eparl) corpus (ver-
sion 5), and of the United Nations (UN) corpus
were used. We also took as training data a sub-
set of the French?English Gigaword (109) cor-
pus. Since a significant part of the data was
crawled from the web, we thought that many sen-
tence pairs may be only approximate translations
of each other. We applied a lexical filter to dis-
card them. Furthermore, some sentences of this
corpus were extracted from web page menus and
are not grammatical. Although we could have
used a part of the menu items as a dictionary, for
simplicity we applied an n-gram language model
(LM) filter to remove all non-grammatical sen-
tences. Thanks to this filter, sentences out of the
language model domain (in this case, mainly the
news domain), may also have been discarded be-
cause they contain many unknown or unfrequent
n-grams. The lexical filter was based on the IBM
model 1 cost (Brown et al, 1993) of each side of
a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter
121
was trained on a corpus composed of Eparl, NC,
and UN data. The language model filter was an
n-gram LM cost of the target sentence (see Sec-
tion 3), normalised with respect to its length. This
filter was trained with all monolingual resources
available except the 109 data. We generated a first
subset, 1091, selecting sentence pairs with a lexi-
cal cost inferior to 4 and an LM cost inferior to
2.3. The corpus selected in this way contains 115
million words in the English side (out of 580 mil-
lion in the original corpus). Close to the evaluation
deadline we decided to generate a second corpus
(1092) by raising the LM cost threshold to 2.6. The
1092 corpus contains 232 million words on the En-
glish side (twice as much as in the 1091 corpus).
In the French side of the bilingual corpora, for
the French?English direction only, the contrac-
tions ?du? (?of the?), ?au? and ?aux? (?to the? singu-
lar and plural) were substituted by their expanded
forms (?de le?, ?a` le? and ?a` les?).
2.2 Use of Automatic Translations and
Comparable corpora
Available human translated bitexts such as the UN
corpus seem to be out-of domain for this task.
We used two types of automatically extracted re-
sources to adapt our system to the task domain.
First, we generated automatic translations of the
French News corpus provided (231M words), and
selected the sentences with a normalised transla-
tion cost (returned by the decoder) inferior to a
threshold. The resulting bitext has no new words
in the English side, since all words of the transla-
tion output come from the translation model, but
it contains new combinations (phrases) of known
words, and reinforces the probability of some
phrase pairs (Schwenk, 2008).
Second, as in last year?s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. This year
we used the AFP and APW news texts since there
are available in the French and English LDC Gi-
gaword corpora. The general architecture of our
parallel sentence extraction system is described in
detail by Abdul-Rauf and Schwenk (2009). We
first translated 91M words from French into En-
glish using our first stage SMT system. These En-
glish sentences were then used to search for trans-
lations in the English AFP and APW texts of the
Gigaword corpus using information retrieval tech-
niques. The Lemur toolkit (Ogilvie and Callan,
2001) was used for this purpose. Search was lim-
ited to a window of ?5 days of the date of the
French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER be-
low 65% for the French?English system and 75%
for the English?French system were kept. Sen-
tences with a large length difference (French ver-
sus English) or containing a large fraction of num-
bers were also discarded. By these means, about
15M words of additional bitexts were obtained to
include in the French?English system, and 21M
words to include in the English?French system.
Note that these additional bitexts do not depend
on the translation direction. The most suitable
amount of additional data was just different in
the French?English and English?French transla-
tion directions.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the devel-
opment and test periods were removed from the
Gigaword collections.
2.4 Development data
All development was done on news-test2008, and
newstest2009 was used as internal test set. For all
corpora except the French side of the bitexts used
to train the French?English system (see above),
the default Moses tokenization was used. How-
ever, we added abbreviations for the French tok-
enizer. All our models are case sensitive and in-
clude punctuation. The BLEU scores reported in
this paper were calculated with the multi-bleu.perl
tool and are case sensitive. The BLEU score
was one of metrics with the best correlation with
human ratings in last year evaluation (Callison-
Burch et al, 2009) for the French?English and
English?French directions.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT)
is to produce a target sentence e from a source
sentence f . It is today common practice to use
phrases as translation units (Koehn et al, 2003;
Och and Ney, 2003) and a log linear framework in
order to introduce several models explaining the
122
translation process:
e? = argmax
e
p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system mod-
els and the ?i weights are typically optimized to
maximize a scoring function on a development
set (Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++
that can appear with rare words.
Phrases and lexical reorderings are extracted
using the default settings of the Moses toolkit.
The parameters of Moses were tuned on news-
test2008, using the ?new? MERT tool. We repeated
the training process three times, each with a differ-
ent seed value for the optimisation algorithm. In
this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word
list contains all the words of the bitext used to
train the translation model and all words that ap-
pear at least ten times in the monolingual corpora.
Words of the monolingual corpora containing spe-
cial characters or sequences of uppercase charac-
ters were not included in the word list. Separate
LMs were build on each data source with the SRI
LM toolkit (Stolcke, 2002) and then linearly in-
terpolated, optimizing the coefficients with an EM
procedure. The perplexities of these LMs were
103.4 for French and 149.2 for English.
4 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 1 and 2, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
1The source is available at http://www.cs.cmu.
edu/?qing/
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is
not significant. The reverse is not true. Note that
most of the improvements shown in the tables are
small and not significant. However many of the
gains are cumulative and the sum of several small
gains makes a significant difference.
Phrase-table Pruning
We tried to prune the phrase-table as proposed by
Johnson et. al. (2007), and available in moses
(?sigtest-filter?). We used the ? ?  filter2. As
lines 3 and 4 of Table 1, and lines 3 and 4 of Ta-
ble 2 reveal, in addition to the reduction 43% of
the phrase-table, a small gain in BLEU score (0.15
and 0.11 respectively) was obtained with the prun-
ing.
Baseline French?English System
The first section of Table 1 (lines 1 to 5) shows re-
sults of the development of the baseline SMT sys-
tem, used to generate automatic translations. Al-
though being out-of-domain data, the introduction
of the UN corpus yields an improvement of one
BLEU point with respect to Eparl+NC. Adding the
1091 corpus, we gain 0.7 BLEU point more. Ac-
tually, we obtained the same score with the 1091
added directly to Eparl+NC (line 5). However, we
choose to include the UN corpus to generate trans-
lations to have a larger vocabulary. The system
highlighted in bold (line 4) is the one we choose
to generate our English translations.
Although no French translations were gener-
ated, we did similar experiments in the English?
French direction (lines 1 to 4 of Table 2). In this
direction, the 1091 corpus is still more valuable than
the UN corpus when added to Eparl+NC, but with
less difference in terms of BLEU score. In this di-
2The p-value of two-by-two contingency tables (describ-
ing the degree of association between a source and a target
phrase) is calculated with Fisher exact test. This probability
is interpreted as the probability of observing by chance an as-
sociation that is at least as strong as the given one, and hence
as its significance. An important special case of a table oc-
curs when a phrase pair occurs exactly once in the corpus,
and each of the component phrases occurs exactly once in its
side of the parallel corpus (1-1-1 phrase pairs). In this case
the negative log of the p-value is ? = logN (N is number of
sentence pairs in the corpus). ? ?  is the largest threshold
that results in all of the 1-1-1 phrase pairs being included.
123
rection, we obtain a gain by adding the UN corpus
to Eparl+NC+1091.
Filtering the 109 Corpus
Lines 5 to 7 of Table 1 show the impact of filtering
the 109 corpus. The system trained on the full 109
corpus added to Eparl+NC achieves a BLEU score
of 26.83. Substituting the full 109 corpus by 1091 (5
times smaller), i.e. using the first filtering settings,
we gain 0.13 BLEU point. Using 1092 instead of
1091, we gain another 0.16 BLEU point, that is 0.3
in total. With respect to not using the 109 data at
all (as we did last year), we gain 0.8 BLEU point.
Impact of the Additional Bitexts
With the baseline French?English SMT system
(see above), we translated the French News cor-
pus to generated an additional bitext (News). We
also translated some parts of the French LDC Gi-
gaword corpus, to serve as queries to our IR sys-
tem (see section 2.2). The resulting additional bi-
text is referred to as IR. Lines 8 to 13 of Table 1
and lines 6 to 12 of Table 2 summarize the system
development including the additional bitexts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate
the automatic translations, but with less than 30%
of the data. This holds in both translation direc-
tions. Adding the News corpus to a larger corpus,
such as Eparl+NC+1091, has less impact but still
yields some improvement: 0.15 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. Note
that the number of additional phrase-table entries
per additional running word is twice as high for
the News bitext than for the other corpora. For
example, with respect to Eparl+NC+UN+1091 (Ta-
ble 2), Eparl+NC+UN+1091+News has 56M more
words and 116M more entries in the phrase-table,
thus the ratio is more than 2. For all other cor-
pora, the ratio is equal to 1 or less. This is un-
expected, particularly in this case where the News
bitext has no new English vocabulary with respect
to the Eparl+NC+UN+1091 corpus, from which its
English side was generated.
With the IR additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the system trained on Eparl+NC+UN,
while the IR bitext is 10 times smaller than the
UN corpus. Added to Eparl+NC+1091+News, the
IR bitext allows gains of 0.13 and 0.2 BLEU point
respectively in the French?English and English?
French directions.
Comparing the systems trained on
Eparl+NC+1091 or Eparl+NC+10
9
2 to the sys-
tems trained on the same corpora plus News+IR,
we can estimate the cumulative impact of the
additional bitexts. The gain is around 0.3 BLEU
point for French?English and around 0.5 BLEU
point for English?French.
Final System
In both translation directions our best system was
the one trained on Eparl+NC+1092+News+IR. We
further achieved small improvements (0.3 BLEU
point) by pruning the phrase-table (as above) and
by using a language model with no cut-off together
with increasing the beam size and/or the maxi-
mum number of translation table entries per input
phrase. Note that the English LM with cut-off had
a size of 6G, and the one with no cut-off had a
size of 29G. It was too much to fit in our 72G
machines so we pruned it with the SRILM prun-
ing tool down to a size of 19G. The French LM
with cut-off had a size of 2G and the one with
no cut-off had a size of 9G. These sizes corre-
spond to the binary format. Taking as example the
French?English direction, the running time went
from 8600 seconds for the system of line 14 (with
a threshold pruning coefficient of 0.4 and a LM
with cut-off) to 28200 seconds for the system sub-
mitted (with the LM without cut-off pruned by the
SRILM tool and a threshold pruning coefficient of
0.00001).
5 Conclusions and Further Work
We presented the development of our machine
translation system for the French?English and
English?French 2010 WMT shared task. Our sys-
tem was actually a standard phrase-based SMT
system based on the Moses decoder. Its original-
ity mostly lied in the choice and extraction of the
training data used.
We decided to use a part of the 109 French?
English corpus. We found this resource useful,
even without filtering. We nevertheless gained 0.3
BLEU point by selecting sentences based on an
IBM Model 1 filter and a language model filter.
We pruned the phrase table with the ?sigtest-
filter? distributed in Moses, yielding improve-
124
Bitext #Fr Words P-table Mem news-test2008 newstest2009
(M) size (M) (G) BLEU BLEU
1 Eparl+NC 52 66 19.3 22.80 (0.03) 25.31 (0.2)
2 Eparl+NC+UN 275 250 22.8 23.38 (0.1) 26.30 (0.2)
3 Eparl+NC+UN+1091 406 376 25.1 23.81 (0.05) 27.0 (0.2)
4 Eparl+NC+UN+1091 pruned 406 215 21.4 23.96 (0.1) 27.15 (0.18)
5 Eparl+NC+1091 183 198 22.1 23.83 (0.07) 26.96 (0.04)
6 Eparl+NC+1092 320 319 24.1 23.95 (0.03) 27.12 (0.1)
7 Eparl+NC+109 733 580 29.5 23.65 (0.09) 26.83 (0.2)
8 Eparl+NC+News 111 188 19.5 23.46 (0.1) 26.95 (0.2)
9 Eparl+NC+1091+News 242 317 22.5 23.77 (0.04) 27.11 (0.04)
10 Eparl+NC+IR 68 78 19.5 22.97 (0.03) 26.20 (0.1)
11 Eparl+NC+News+IR 127 198 20.1 23.62 (0.01) 27.04 (0.06)
12 Eparl+NC+1091+News+IR 258 327 22.8 23.75 (0.05) 27.24 (0.05)
13 Eparl+NC+1092+News+IR 395 441 24.4 23.87 (0.03) 27.43 (0.08)
14 Eparl+NC+1092+News+IR pruned 395 285 62.5 24.04 27.72
(+larger beam, +no-cutoff LM)
Table 1: French?English results: number of French words (in million), number of entries in the phrase-
table (in million), memory needed during decoding (in gigabytes) and BLEU scores in the development
(news-test2008) and internal test (newstest2009) sets for the different systems developped. The BLEU
scores and the number in parentheses are the average and standard deviation over 3 values (see Section 3.)
ments of 0.1 to 0.2 BLEU point for a 43% reduc-
tion of the phrase-table size.
We used additional bitexts extracted automati-
cally from the available monolingual corpora. The
first type of additional bitext is generated with au-
tomatic translations of the monolingual data with
a baseline SMT system. The second one is ex-
tracted from comparable corpora, with Informa-
tion Retrieval techniques. With the additional bi-
texts we gained 0.3 and 0.5 BLEU point for the
French?English and English?French systems, re-
spectively.
Next year we want to perform an improved se-
lection of parallel training data with re-sampling
techniques. We also want to use a continuous
space language model (Schwenk, 2007) in an n-
best list rescoring step after decoding. Finally, we
plan to train different types of systems (such as
a hierarchical SMT system and a Statistical Post-
Editing system) and combine their outputs with
the MANY open source system combination soft-
ware (Barrault, 2010).
Acknowledgments
This work has been partially funded by
the European Union under the EuroMatrix
Plus project ? Bringing Machine Transla-
tion for European Languages to the User ?
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720).
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16?23, Athens, Greece.
Lo??c Barrault. 2010. MANY : Open source machine
translation system combination. Prague Bulletin
of Mathematical Linguistics, Special Issue on Open
Source Tools for Machine Translation, 93:147?155.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the ACL Fourth Workshop on Sta-
tistical Machine Translation, pages 1?28, Athens,
Greece.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49?57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
125
Bitext #En Words Phrase-table news-test2008 newstest2009
(M) size (M) BLEU BLEU
1 Eparl+NC+UN 242 258 24.21 (0.01) 25.29 (0.12)
2 Eparl+NC+1091 163 203 24.24 (0.06) 25.51 (0.13)
3 Eparl+NC+UN+1091 357 385 24.46 (0.08) 25.73 (0.20)
4 Eparl+NC+UN+1091 pruned 357 221 24.42 (0.1) 25.84 (0.05)
5 Eparl+NC+1092 280 330 24.43 (0.04) 25.68 (0.12)
6 Eparl+NC+News 103 188 24.27 (0.2) 25.70 (0.15)
7 Eparl+NC+1091+News 218 321 24.51 (0.05) 25.83 (0.05)
8 Eparl+NC+UN+1091+News 413 501 24.70 (0.1) 25.86 (0.14)
9 Eparl+NC+IR 69 81 24.14 (0.05) 25.17 (0.2)
10 Eparl+NC+News+IR 124 201 24.32 (0.12) 25.84 (0.17)
11 Eparl+NC+1091+News+IR 239 333 24.54 (0.1) 26.03 (0.15)
12 Eparl+NC+1092+News+IR 356 453 24.68 (0.04) 26.19 (0.05)
13 Eparl+NC+1092+News+IR pruned 356 293 25.06 26.53
(+larger beam, +no-cutoff LM)
Table 2: English?French results: number of English words (in million), number of entries in the phrase-
table (in million) and BLEU scores in the development (news-test2008) and internal test (newstest2009)
sets for the different systems developped. The BLEU scores and the number in parentheses are the
average and standard deviation over 3 values (see Section 3.)
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967?975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn et al 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103?108.
Holger Schwenk, Sadaf Abdul Rauf, Lo??c Barrault,
and Jean Senellart. 2009. SMT and SPE machine
translation systems for WMT?09. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 130?134, Athens, Greece. Association
for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901?904, Denver,
CO.
126
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 392?399,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Translation Model Adaptation by Resampling
Kashif Shah, Lo??c Barrault, Holger Schwenk
LIUM, University of Le Mans
Le Mans, France.
FirstName.LastName@lium.univ-lemans.fr
Abstract
The translation model of statistical ma-
chine translation systems is trained on par-
allel data coming from various sources and
domains. These corpora are usually con-
catenated, word alignments are calculated
and phrases are extracted. This means
that the corpora are not weighted accord-
ing to their importance to the domain of
the translation task. This is in contrast
to the training of the language model for
which well known techniques are used to
weight the various sources of texts. On
a smaller granularity, the automatic cal-
culated word alignments differ in quality.
This is usually not considered when ex-
tracting phrases either.
In this paper we propose a method to auto-
matically weight the different corpora and
alignments. This is achieved with a resam-
pling technique. We report experimen-
tal results for a small (IWSLT) and large
(NIST) Arabic/English translation tasks.
In both cases, significant improvements in
the BLEU score were observed.
1 Introduction
Two types of resources are needed to train statis-
tical machine translation (SMT) systems: parallel
corpora to train the translation model and mono-
lingual texts in the target language to build the
language model. The performance of both mod-
els depends of course on the quality and quantity
of the available resources.
Today, most SMT systems are generic, i.e. the
same system is used to translate texts of all kinds.
Therefore, it is the domain of the training re-
sources that influences the translations that are se-
lected among several choices. While monolingual
texts are in general easily available in many do-
mains, the freely available parallel texts mainly
come from international organisations, like the
European Union or the United Nations. These
texts, written in particular jargon, are usually
much larger than in-domain bitexts. As an exam-
ple we can cite the development of an NIST Ara-
bic/English phrase-based translation system. The
current NIST test sets are composed of a news
wire part and a second part of web-style texts.
For both domains, there is only a small number
of in-domain bitexts available, in comparison to
almost 200 millions words of out-of-domain UN
texts. The later corpus is therefore likely to domi-
nate the estimation of the probability distributions
of the translation model.
It is common practice to use a mixture language
model with coefficients that are optimized on the
development data, i.e. by these means on the do-
main of the translation task. Domain adaptation
seems to be more tricky for the translation model
and it seems that very little research has been done
that seeks to apply similar ideas to the translation
model. To the best of our knowledge, there is no
commonly accepted method to weight the bitexts
coming from different sources so that the transla-
tion model is best optimized to the domain of the
task. Mixture models are possible when only two
different bitexts are available, but are rarely used
for more corpora (see discussion in the next sec-
tion).
In this work we propose a new method to adapt
the translation model of an SMT system. We only
perform experiments with phrase-based systems,
but the method is generic and could be easily ap-
plied to an hierarchical or syntax-based system.
We first associate a weighting coefficient to each
bitext. The main idea is to use resampling to pro-
duce a new collection of weighted alignment files,
followed by the standard procedure to extract the
phrases. In a second step, we also consider the
392
alignment score of each parallel sentence pair, em-
phasizing by these means good alignments and
down-weighting less reliable ones. All the param-
eters of our procedure are automatically tuned by
optimizing the BLEU score on the development
data.
The paper is organized as follows. The next
section describes related work on weighting the
corpora and model adaptation. Section 3 de-
scribes the architecture allowing to resample and
to weight the bitexts. Experimental results are pre-
sented in section 4 and the paper concludes with a
discussion.
2 Related Work
Adaptation of SMT systems is a topic of in-
creasing interest since few years. In previous
work, adaptation is done by using mixture mod-
els, by exploiting comparable corpora and by self-
enhancement of translation models.
Mixture models were used to optimize the co-
efficients to the adaptation domain. (Civera and
Juan, 2007) proposed a model that can be used
to generate topic-dependent alignments by exten-
sion of the HMM alignment model and derivation
of Viterbi alignments. (Zhao et al, 2004) con-
structed specific language models by using ma-
chine translation output as queries to extract sim-
ilar sentences from large monolingual corpora.
(Foster and Kuhn, 2007) applied a mixture model
approach to adapt the system to a new domain by
using weights that depend on text distances to mix-
ture components. The training corpus was divided
into different components, a model was trained on
each part and then weighted appropriately for the
given context. (Koehn and Schroeder, 2007) used
two language models and two translation models:
one in-domain and other out-of-domain to adapt
the system. Two decoding paths were used to
translate the text.
Comparable corpora are exploited to find addi-
tional parallel texts. Information retrieval tech-
niques are used to identify candidate sentences
(Hildebrand et al, 2005). (Snover et al, 2008)
used cross-lingual information retrieval to find
texts in the target language that are related to the
domain of the source texts.
A self-enhancing approach was applied by
(Ueffing, 2006) to filter the translations of the
test set with the help of a confidence score and
to use reliable alignments to train an additional
phrase table. This additional table was used with
the existing generic phrase table. (Ueffing, 2007)
further refined this approach by using transduc-
tive semi-supervised methods for effective use of
monolingual data from the source text. (Chen et
al., 2008) performed domain adaptation simulta-
neously for the translation, language and reorder-
ing model by learning posterior knowledge from
N-best hypothesis. A related approach was in-
vestigated in (Schwenk, 2008) and (Schwenk and
Senellart, 2009) in which lightly supervised train-
ing was used. An SMT system was used to trans-
late large collections of monolingual texts, which
were then filtered and added to the training data.
(Matsoukas et al, 2009) propose to weight each
sentence in the training bitext by optimizing a dis-
criminative function on a given tuning set. Sen-
tence level features were extracted to estimate the
weights that are relevant to the given task. Then
certain parts of the training bitexts were down-
weighted to optimize an objective function on the
development data. This can lead to parameter
over-fitting if the function that maps sentence fea-
tures to weights is complex.
The technique proposed in this paper is some-
how related to the above approach of weighting
the texts. Our method does not require an ex-
plicit specification of the in-domain and out-of-
domain training data. The weights of the corpora
are directly optimized on the development data us-
ing a numerical method, similar to the techniques
used in the standard minimum error training of the
weights of the feature functions in the log-linear
criterion. All the alignments of the bitexts are re-
sampled and given equal chance to be selected and
therefore, influence the translation model in a dif-
ferent way. Our proposed technique does not re-
quire the calculation of extra sentence level fea-
tures, however, it may use the alignments score as-
sociated with each aligned sentence pair as a con-
fidence score.
3 Description of the algorithm
The architecture of the algorithm is summarized in
figure 1. The starting point is an (arbitrary) num-
ber of parallel corpora. We first concatenate these
bitexts and perform word alignments in both direc-
tions using GIZA++. This is done on the concate-
nated bitexts since GIZA++ may perform badly
if some of the individual bitexts are rather small.
Next, the alignments are separated in parts corre-
393
Figure 1: Architecture of SMT Weighting System
sponding to the individual bitexts and a weighting
coefficient is associated to each one. We are not
aware of a procedure to calculate these coefficients
in an easy and fast way without building an actual
SMT system. Note that there is an EM procedure
to do this for language modeling.
In the next section, we will experimentally com-
pare equal coefficients, coefficients set to the same
values than those obtained when building an inter-
polated language model on the source language,
and a new method to determine the coefficients by
optimizing the BLEU score on the development
data.
One could imagine to directly use these coef-
ficients when calculating the various probabilities
of the extracted phrases. In this work, we propose
a different procedure that makes no assumptions
on how the phrases are extracted and probabilities
are calculated. The idea is to resample alignments
from the alignment file corresponding to the indi-
vidual bitexts according to their weighting coeffi-
cients. By these means, we create a new, poten-
tially larger alignment file, which then in turn will
be used by the standard phrase extraction proce-
dure.
3.1 Resampling the alignments
In statistics, resampling is based upon repeated
sampling within the same sample until a sample
is obtained which better represents a given data
set (Yu, 2003). Resampling is used for validating
models on given data set by using random subsets.
It overcomes the limitations to make assumptions
about the distribution of the data. Usually resam-
pling is done several times to better estimate and
select the samples which better represents the tar-
get data set. The more often we resample, the
closer we get to the true probability distribution.
In our case we performed resampling with re-
placement according to the following algorithm:
Algorithm 1 Resampling
1: for i = 0 to required size do
2: Select any alignment randomly
3: Alscore ? normalized alignment score
4: Threshold? rand[0, 1]
5: if Alscore > Threshold then
6: keep it
7: end if
8: end for
Let us call resampling factor, the number of
times resampling should be done. An interesting
question is to determine the optimal value of this
resampling factor.
It actually depends upon the task or data we are
experimenting on. We may start with one time
resampling and could stop when results becomes
stable. Figure 2 plots a typical curve of the BLEU
score as a function of the number of times we re-
sample. It can be observed that the curve is grow-
ing proportionally to the resampling factor until it
becomes stable after a certain point.
3.2 Weighting Schemes
We concentrated on translation model adaptation
when the bitexts are heterogeneous, e.g. in-
domain and out-of-domain or of different sizes. In
this case, weighting these bitexts seems interest-
ing and can be used in order to select data which
better represent the target domain. Secondly when
sentences are aligned, some alignments are reli-
able and some are less. Using unreliable align-
ments can put negative effect on the translation
quality. So we need to exclude or down-weight
394
 52
 52.5
 53
 53.5
 54
 54.5
 55
 55.5
 56
 0  5  10  15  20
BL
EU
Resampling factor
dev
test
baseline(test)
Figure 2: The curve shows that by increasing the
resampling factor we get better and stable results
on Dev and Test.
unreliable alignments and keep or up-weight the
good ones. We conceptually divided the weight-
ing in two parts that is (i) weighting the corpora
and (ii) weighting the alignments
3.2.1 Weighting Corpora
We started to resample the bitexts with equal
weights to see the effect of resampling. This gives
equal importance to each bitext without taking into
account the domain of the text to be translated.
However, it should be better to give appropriate
weights according to a given domain as shown in
equation 1
?1bitext1 + ?2bitext2 + ..+ ?nbitextn (1)
where the ?n are the coefficients to optimize.
One important question is how to find out the ap-
propriate coefficient for each corpus. We investi-
gated a technique similar to the algorithm used to
minimize the perplexity of an interpolated target
LM. Alternatively, it is also possible to construct a
interpolated language model on the source side of
bitexts. This approach was implemented and these
coefficients were used as the weights for each bi-
text. One can certainly ask the question whether
the perplexity is a good criterion for weighting bi-
texts. Therefore, we worked on direct optimiza-
tion of these coefficients by CONDOR (Berghen
and Bersini, 2005). This freely available tool is a
numerical optimizer based on Powell?s UOBYQA
algorithm (Powell, 1994). The aim of CONDOR
is to minimize a objective function using the least
number of function evaluations. Formally, it is
used to find x? ? Rn with given constraints which
satisfies
F (x?) = min
x
F (x) (2)
where n is the dimension of search space and x?
is the optimum of x. The following algorithm was
used to weight the bitexts.
Algorithm 2 WeightingCorpora
1: Determine word to word alignment with
GIZA++ on concatenated bitext.
2: while Not converged do
3: Run Condor initialized with LM weights.
4: Create new alignment file by resampling
according to weights given by Condor.
5: Use the alignment file to extract phrases
and build the translation table (phrase table)
6: Tune the system with MERT (this step can
be skipped until weights are optimized to
save time)
7: Calculate the BLEU score
8: end while
3.2.2 Weighting Alignments
Alignments produced by GIZA++ have alignment
scores associated with each sentence pair in both
direction, i.e. source to target and target to source.
We used these alignment scores as confidence
measurement for each sentence pair. Alignment
scores depend upon the length of each sentence,
therefore, they must be normalized regarding the
size of the sentence. Alignment scores have a very
large dynamic range and we have applied a loga-
rithmic mapping in order to flatten the probability
distribution :
log(? ?
( ntrg
?
asrc trg + nsrc
?
atrg src)
2
) (3)
where a is the alignment score, n the size of a
sentence and ? a coefficient to optimize. This is
also done by Condor.
Of course, some alignments will appear several
times, but this will increase the probability of cer-
tain phrase-pairs which are supposed to be more
related to the target domain. We have observed
that the weights of an interpolated LM build on
the source side of the bitext are good initial val-
ues for CONDOR. Moreover, weights optimized
by Condor are in the same order than these ?LM
weights?. Therefore, we do not perform MERT
of the SMT systems build at each step of the op-
timization of the weights ?i and ? by CONDOR,
395
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) Dev (NIST06) Test (NIST08)
Baseline 53.98 53.37 43.16 42.21
With equal weights 53.71 53.20 43.10 42.11
With LM weights 54.20 53.71 43.42 42.22
Condor weights 54.80 53.98 43.49 42.28
Table 1: BLEU scores when weighting corpora (one time resampling)
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) Dev (NIST06) Test (NIST08)
Baseline 53.98 53.37 43.16 42.21
With equal weights 53.80 53.30 43.13 42.15
With LM weights 54.32 53.91 43.54 42.37
Condor weights 55.10 54.13 43.80 42.40
Table 2: BLEU scores when weighting corpora (optimum number of resampling)
IWSLT Task NIST Task
Dev (Dev6) Test (Dev7) TER(Test) Dev (NIST06) Test (NIST08) TER(Test)
Baseline 53.98 53.37 32.75 43.16 42.21 51.69
With equal weights 53.85 53.33 32.80 43.28 42.21 51.72
With LM weights 54.80 54.10 31.50 43.42 42.41 51.50
Condor weights 55.48 54.58 31.31 43.95 42.54 51.35
Table 3: BLEU and TER scores when weighting corpora and alignments (optimum number of resam-
pling)
but use the values obtained by running MERT on
a system obtained by using the ?LM weights? to
weight the alignments. Once CONDOR has con-
verged to optimal weights, we can then tune our
system by MERT. This saves lot of time taken by
the tuning process and it had no impact on the re-
sults.
4 Experimental evaluation
The baseline system is a standard phrase-based
SMT system based on the Moses SMT toolkit
(Koehn and et al, 2007). In our system we
used fourteen features functions. These features
functions include phrase and lexical translation
probabilities in both directions, seven features for
lexicalized distortion model, a word and phrase
penalty, and a target language model. The MERT
tool is used to tune the coefficients of these fea-
ture functions. We considered Arabic to English
translation. Tokenization of the Arabic source
texts is done by a tool provided by SYSTRAN
which also performs a morphological decompo-
sition. We considered two well known official
evaluation tasks to evaluate our approach, namely
NIST and IWSLT.
For IWSLT, we used the BTEC bitexts (194M
words), Dev1, Dev2, Dev3 (60M words each) as
training data, Dev6 as development set and Dev7
as test set. From previous experiments, we have
evidence that the various development corpora are
not equally important and weighting them cor-
rectly should improve the SMT system. We an-
alyze the translation quality as measured by the
BLEU score for the three methods: equal weights,
LM weights and Condor weights and considering
one time resampling. Further experiments were
performed using the optimized number of resam-
pling with and without weighting the alignments.
We have realized that it is beneficial to always in-
clude the original alignments. Even if we resample
many times there is a chance that some alignments
might never be selected but we do not want to
loose any information. By keeping original align-
ments, all alignments are given a chance to be se-
396
lected at least once. All these results are summa-
rized in tables 1, 2 and 3.
One time resampling along with equal weights
gave worse results than the baseline system while
improvements in the BLEU score were observed
with LM and Condor weights for the IWSLT task,
as shown in table 1. Resampling many times al-
ways gave more stable results, as already shown
in figure 2 and as theoretically expected. For this
task, we resampled 15 times. The improvements
in the BLEU score are shown in table 2. Fur-
thermore, using the alignment scores resulted in
additional improvements in the BLEU score. For
the IWSLT task, we achieved and overall improve-
ment of 1.5 BLEU points on the development set
and 1.2 BLEU points on the test set as shown in
table 3
To validate our approach we further experi-
mented with the NIST evaluation task. Most of
the training data used in our experiments for the
NIST task is made available through the LDC. The
bitexts consist of texts from the GALE project1
(1.6M words), various news wire translations2
(8.0M words) on development data from pre-
vious years (1.6M words), LDC treebank data
(0.4M words) and the ISI extracted bitexts (43.7M
words). The official NIST06 evaluation data was
used as development set and the NIST08 evalua-
tion data was used as test set. The same procedure
was adapted for the NIST task as for the IWSLT
task. Results are shown in table 1 by using differ-
ent weights and one time resampling. Further im-
provements in the results are shown in table 2 with
the optimum number of resampling which is 10
for this task. Finally, results by weighting align-
ments along with weighting corpora are shown in
table 3. Our final system achieved an improve-
ment of 0.79 BLEU points on the development set
and 0.33 BLEU points on the test set. TER scores
are also shown on test set of our final system in
table 3. Note that these results are state-of-the-art
when compared to the official results of the 2008
NIST evaluation3.
The weights of the different corpora are shown
in table 4 for the IWSLT and NIST task. In both
cases, the weights optimized by CONDOR are
substantially different form those obtained when
1LDC2005E83, 2006E24, E34, E85 and E92
2LDC2003T07, 2004E72, T17, T18, 2005E46 and
2006E25.
3http://www.nist.gov/speech/tests/mt/
2008/
creating an interpolated LM on the source side of
the bitexts. In any case, the weights are clearly
non uniform, showing that our algorithm has fo-
cused on in-domain data. This can be nicely seen
for the NIST task. The Gale texts were explictely
created to contain in-domain news wire and WEB
texts and actually get a high weight despite their
small size, in comparison to the more general news
wire collection from LDC.
5 Conclusion and future work
We have proposed a new technique to adapt the
translation model by resampling the alignments,
giving a weight to each corpus and using the
alignment score as confidence measurement of
each aligned phrase pair. Our technique does not
change the phrase pairs that are extracted,4 but
only the corresponding probability distributions.
By these means we hope to adapt the translation
model in order to increase the weight of transla-
tions that are important to the task, and to down-
weight the phrase pairs which result from unreli-
able alignments.
We experimentally verified the new method on
the low-resource IWSLT and the resource-rich
NIST?08 tasks. We observed significant improve-
ment on both tasks over state-of-the-art baseline
systems. This weighting scheme is generic and
it can be applied to any language pair and target
domain. We made no assumptions on how the
phrases are extracted and it should be possible to
apply the same technique to other SMT systems
which rely on word-to-word alignments.
On the other hand, our method is computation-
ally expensive since the optimisation of the coef-
ficients requires the creation of a new phrase table
and the evaluation of the resulting system in the
tuning loop. Note however, that we run GIZA++
only once.
In future work, we will try to directly use the
weights of the corpora and the alignments in the
algorithm that extracts the phrase pairs and cal-
culates their probabilities. This would answer
the interesting question whether resampling itself
is needed or whether weighting the corpora and
alignments is the key to the observed improve-
ments in the BLEU score.
Finally, it is straight forward to consider more
feature functions when resampling the alignments.
This may be a way to integrate linguistic knowl-
4when also including the original alignments
397
IWSLT Task BTEC Dev1 Dev2 Dev3
# of Words 194K 60K 60K 60K
LM Coeffs 0.7233 0.1030 0.0743 0.0994
Condor Coeffs 0.6572 0.1058 0.1118 0.1253
NIST TASK Gale NewsWire TreeBank Dev ISI
# of words 1.6M 8.1M 0.4M 1.7M 43.7M
LM Coeffs 0.3215 0.1634 0.0323 0.1102 0.3726
Condor Coeffs 0.4278 0.1053 0.0489 0.1763 0.2417
Table 4: Weights of the different bitexts.
edge into the SMT system, e.g. giving low scores
to word alignments that are ?grammatically not
reasonable?.
Acknowledgments
This work has been partially funded by the Eu-
ropean Commission under the project Euromatrix
and by the Higher Education Commission(HEC)
Pakistan as Overseas scholarship. We are very
thankful to SYSTRAN who provided support for
the Arabic tokenization.
References
Frank Vanden Berghen and Hugues Bersini.
2005. CONDOR, a new parallel, constrained
extension of Powell?s UOBYQA algorithm:
Experimental results and comparison with the
DFO algorithm. Journal of Computational and
Applied Mathematics, 181:157?175, Septem-
ber.
Boxing Chen, Min Zhang, Aiti Aw, and
Haizhou Li. 2008. Exploiting n-best hypothe-
ses for SMT self- enhancement. In Association
for Computational Linguistics, pages 157?160.
Jorge Civera and Alfons Juan. 2007. Do-
main adaptation in statistical machine transla-
tion with mixture modelling. In Second Work-
shop on SMT, pages 177?180.
George Foster and Roland Kuhn. 2007.
Mixture-model adaptation for SMT. In Pro-
ceedings of the Second Workshop on Statistical
Machine Translation, pages 128?135. Associa-
tion for Computational Linguistics.
Almut Silja Hildebrand, Matthias Eck, Stephan
Vogel, and Alex Waibel. 2005. Adaptation
of the translation model for statistical machine
translation based on information retrieval. In
EAMT, pages 133?142.
Philipp Koehn and et al 2007. Moses: Open
source toolkit for statistical machine transla-
tion. In Association for Computational Linguis-
tics, demonstration session., pages 224?227.
Philipp Koehn and Josh Schroeder. 2007. Ex-
periments in domain adaptation for statistical
machine translation. In Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, pages 224?227. Association for Computa-
tional Linguistics.
Spyros Matsoukas, Antti-Veikko I. Rosti, and
Bing Zhang. 2009. Discriminative corpus
weight estimation for machine translation. In
Proceedings of the 2009 Conference on Empir-
ical Methods in Natural Language Processing,
pages 708?717.
M.J.D. Powell. 1994. A direct search opti-
mization method that models the objective and
constraint functions by linar interpolation. In
In Advances in Optimization and Numerical
Analysis, Proceedings of the sixth Workshop
on Optimization and Numerical Analysis, Oax-
aca, Mexico, volume 275, pages 51?67. Kluwer
Academic Publishers.
Holger Schwenk and Jean Senellart. 2009.
Translation model adaptation for an Ara-
bic/French news translation system by lightly-
supervised training. In MT Summit.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical
machine translation. In IWSLT, pages 182?189.
Matthew Snover, Bonnie Dorr, and Richard
Schwartz. 2008. Language and translation
398
model adaptation using comparalble corpora.
In Proceedings of the 2008 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 857?866.
Nicola Ueffing. 2006. Using monolingual sour-
cce language data to improve MT performance.
In IWSLT, pages 174?181.
Nicola Ueffing. 2007. Transductive learning for
statistical machine translation. In Association
for Computational Linguistics, pages 25?32.
Chong Ho Yu. 2003. Resampling methods:
Concepts, applications, and justification. In
Practical Assessment Research and Evaluation.
Bing Zhao, Matthias Ech, and Stephen Vogal.
2004. Language model adaptation for statistical
machine translation with structured query mod-
els. In Proceedings of the 20th international
conference on Computational Linguistics. As-
sociation for Computational Linguistics.
399
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 284?293,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Investigations on Translation Model Adaptation Using Monolingual Data
Patrik Lambert, Holger Schwenk, Christophe Servan and Sadaf Abdul-Rauf
LIUM, University of Le Mans
72085 Le Mans, France
FirstName.LastName@lium.univ-lemans.fr
Abstract
Most of the freely available parallel data to
train the translation model of a statistical ma-
chine translation system comes from very spe-
cific sources (European parliament, United
Nations, etc). Therefore, there is increasing
interest in methods to perform an adaptation
of the translation model. A popular approach
is based on unsupervised training, also called
self-enhancing. Both only use monolingual
data to adapt the translation model. In this pa-
per we extend the previous work and provide
new insight in the existing methods. We report
results on the translation between French and
English. Improvements of up to 0.5 BLEU
were observed with respect to a very com-
petitive baseline trained on more than 280M
words of human translated parallel data.
1 Introduction
Adaptation of a statistical machine translation sys-
tem (SMT) is a topic of increasing interest during
the last years. Statistical (n-gram) language models
are used in many domains and several approaches to
adapt such models were proposed in the literature,
for instance in the framework of automatic speech
recognition. Many of these approaches were suc-
cessfully used to adapt the language model of an
SMT system. On the other hand, it seems more chal-
lenging to adapt the other components of an SMT
system, namely the translation and reordering mod-
els. In this work we consider the adaptation of the
translation model of a phrase-based SMT system.
While rule-based machine translation rely on
rules and linguistic resources built for that purpose,
SMT systems can be developed without the need of
any language-specific expertise and are only based
on bilingual sentence-aligned data (?bitexts?) and
large monolingual texts. However, while monolin-
gual data are usually available in large amounts and
for a variety of tasks, bilingual texts are a sparse re-
source for most language pairs.
Current parallel corpora mostly come from one
domain (proceedings of the Canadian or European
Parliament, or of the United Nations). This is prob-
lematic when SMT systems trained on such corpora
are used for general translations, as the language jar-
gon heavily used in these corpora is not appropriate
for everyday life translations or translations in some
other domain. This problem could be attacked by ei-
ther searching for more in-domain training data, e.g.
by exploring comparable corpora or the WEB, or by
adapting the translation model to the task. In this
work we consider translation model adaptation with-
out using additional bilingual data. One can dis-
tinguish two types of translation model adaptation:
first, adding new source words or/and new transla-
tions to the model; and second, modifying the prob-
abilities of the existing model to better fit the topic
of the task. These two directions are complementary
and could be simultaneously applied. In this work
we focus on the second type of adaptation.
In this work, we focus on statistical phrase-
based machine translations systems (PBSMT), but
the methods could be also applied to hierarchical
systems. In PBSMT, the translation model is rep-
resented by a large list of all known source phrases
and their translations. Each entry is weighted us-
ing several probabilities, e.g. the popular Moses
284
system uses phrase translation probabilities in the
forward and backward direction, as well as lexical
probabilities in both directions. The entries of the
phrase-table are automatically extracted from sen-
tence aligned parallel data and they are usually quite
noisy. It is not uncommon to encounter several hun-
dreds, or even thousands of possible translations of
frequent source phrases. Many of these automati-
cally extracted translations are probably wrong and
are never used since their probabilities are (fortu-
nately) small in comparison to better translations.
Therefore, several approaches were proposed to fil-
ter these phrase-tables, reducing considerably their
size without any loss of the quality, or even achiev-
ing improved performance (Johnson et al, 2007).
Given these observations, adaptation of the trans-
lation model of PBSMT systems could be performed
by modifying the probability distribution of the ex-
isting phrases without necessarily modifying the en-
tries. The idea is of course to increase the prob-
abilities of translations that are appropriate to the
task and to decrease the probabilities of the other
ones. Ideally, we should also add new translations or
source phrase, but this seems to be more challenging
without any additional parallel data.
A common way to modify a statistical model is to
use a mixture model and to optimize the coefficients
to the adaptation domain. This was investigated in
the framework of SMT by several authors, for in-
stance for word alignment (Civera and Juan, 2007),
for language modeling (Zhao et al, 2004; Koehn
and Schroeder, 2007) and to a lesser extent for the
translation model (Foster and Kuhn, 2007; Chen et
al., 2008). This mixture approach has the advan-
tage that only few parameters need to be modified,
the mixture coefficients. On the other hand, many
translation probabilities are modified at once and it
is not possible to selectively modify the probabilities
of particular phrases.
Another direction of research is self-enhancing of
the translation model. This was first proposed by
Ueffing (2006). The idea is to translate the test data,
to filter the translations with help of a confidence
score and to use the most reliable ones to train an
additional small phrase table that is jointly used with
the generic phrase table. This could be also seen as a
mixture model with the in-domain component being
build on-the-fly for each test set. In practice, such
an approach is probably only feasible when large
amounts of test data are collected and processed at
once, e.g. a typical evaluation set up with a test set of
about 50k words. This method of self-enhancing the
translation model seems to be more difficult to apply
for on-line SMT, e.g. a WEB service, since often the
translation of some sentences only is requested. In
follow up work, this approach was refined (Ueffing
et al, 2007). Domain adaptation was also performed
simultaneously for the translation, language and re-
ordering model (Chen et al, 2008).
A somehow related approach was named lightly-
supervised training (Schwenk, 2008). In that work
an SMT system is used to translate large amounts of
monolingual texts, to filter them and to add them to
the translation model training data. This approach
was reported to obtain interesting improvements
in the translations quality (Schwenk and Senellart,
2009; Bertoldi and Federico, 2009). In comparison
to self enhancing as proposed by Ueffing (2006),
lightly-supervised training does not adapt itself to
the test data, but large amounts of monolingual train-
ing data are translated and a completely new model
is built. This model can be applied to any test data,
including a WEB service.
In this paper we propose to extend this approach
in several ways. First, we argue that the automatic
translations should not be performed from the source
to the target language, but in the opposite direction.
Second, we propose to use the segmentation ob-
tained during translation instead of performing word
alignments with GIZA++ (Och and Ney, 2003) of
the automatic translations. Finally, we propose to
enrich the vocabulary of the adapted system by de-
tecting untranslated words and automatically infer-
ring possible translations from the stemmed form
and the existing translations in the phrase table.
This paper is organized as follows. In the next
section we first describe our approach in detail. Sec-
tion 3 describes the considered task, the available
resources and the baseline PBSMT system. Results
are summarized in section 4 and the paper concludes
with a discussion and perspectives of this work.
2 Architecture of the approach
In this paper we propose to extend in several ways
the translation model adaptation by unsupervised
285
training as proposed by Schwenk (2008). In that
paper the authors propose to first build a PBSMT
system using all available human translated bi-
texts. This system is then used to translate large
amounts of monolingual data in the source language.
These automatic translations are filtered using the
sentence-length normalized log score of Moses, i.e.
the sum of the log-scores of all feature functions.
Putting a threshold on this score, only the most re-
liable translations are kept. This threshold was de-
termined experimentally. The automatic translations
were added to the parallel training data and a new
PBSMT model was build, performing the complete
pipeline of word alignment with GIZA++, phrase
extraction and scoring and tuning the system on
development data with MERT. In Schwenk (2009)
significant improvement were obtained by this ap-
proach when translating from Arabic to French.
2.1 Choice of the translation direction
First, we argue that it should be better to translate
monolingual data in the opposite translation direc-
tion of the system that we want to improve, i.e. from
the target into the source language. When translat-
ing large amounts of monolingual data, the system
will of course produce some wrong translations with
respect to choice of the vocabulary, to word order,
to morphology, etc. If we translate from the source
to the target language, these wrong translations are
added to the phrase table and may be used in future
translations performed by the adapted system. When
we add the automatic translations performed in the
opposite direction to the training data, the possibly
wrong translations will appear on the source side of
the entries in the adapted phrase table. PBSMT sys-
tems segment the source sentence according to the
available entries in the phrase table. Since the source
sentence is usually grammatically and semantically
correct, with the eventual exception of speech trans-
lation, it is unlikely that the wrong entries in the
phrase table will be ever used, e.g. phrases with bad
word choice or wrong morphology.
The question of the choice of the translation di-
rection was already raised by Bertoldi and Fed-
erico (2009). However, when data in the source
language is available they adapt only the translation
model (TM), while they adapt the TM and the lan-
guage model (LM) when data in the target language
is given. Of course the system with adapted LM is
much better, but this doesn?t prove that target mono-
lingual data are better than source monolingual data
for TM adaptation. In our paper, we use the same,
best, LM for all systems and we adapt the baseline
system with bitexts synthesized from source or tar-
get monolingual data.
2.2 Word alignment
In the work of Schwenk (2008), the filtered auto-
matic translation were added to the parallel training
data and the full pipeline to build a PBSMT sys-
tem was performed again, including word alignment
with GIZA++. Word alignment of bitexts of several
hundreds of millions of words is a very time con-
suming step. Therefore we propose to use the seg-
mentation into phrases and words obtained implic-
itly during the translation of the monolingual data
with the moses toolkit. These alignments are simply
added to the previously calculated alignments of the
human translated bitexts and a new phrase table is
built.
This new procedure does not only speed-up the
overall processing, but there are also investigations
that these alignments obtained by decoding are more
suitable to extract phrases than the symmetrized
word alignments produced by GIZA++. For in-
stance, Wuebker et al (2010) proposed to trans-
late the training data, using forced alignment and
a leave-one-out technique, and to use the induced
alignments to extract phrases. They have observed
improvements with respect to word alignment ob-
tained by GIZA++. On the other hand, Bertoldi and
Federico (2009) adapted an SMT system with au-
tomatic translations and trained the translation and
reordering models on the word alignment used by
moses. They reported a very small drop in per-
formance with respect to training word alignments
with GIZA++. Similar ideas were also used in pivot
translation. Bertoldi et al (2008) translated from the
pivot language to the source language to create par-
allel training data for the direct translation.
2.3 Treatment of unknown words
Statistical machine translation systems have some
trouble dealing with morphologically rich lan-
guages. It can happen, in function of the avail-
able training data, that translations of words are only
286
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
known in some forms and not in others. For in-
stance, for a user of MT technology it is quite dif-
ficult to understand why the system can translate
the French word ?je pense?1, but not ?tu penses?2.
There have been attempts in the literature to address
this problem, for instance by Habash (2008) to deal
with the Arabic language. It is actually possible to
automatically infer possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
stemmed les travaux sont fini
segment les travaux sont <n translation=?finished||ended?
proposed prob=?0.008||0.0001?>finis</n>
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
1I think
2you think
comparing the n-grams contained in the phrase ta-
ble and the source segment in order to detect iden-
tical words. Once the unknown word is selected,
we are looking for its stemmed form in the dictio-
nary and propose some translations for the unknown
word based on lexical score of the phrase table (see
Table 2 for some examples). The stemmer used is
the snowball stemmer3. Then the different hypothe-
sis are evaluated with the target language model.
This kind of processing could be done either be-
fore running the Moses decoder, i.e. using the
XML mark-up of Moses, or after decoding by post-
processing the untranslated words. In both cases, we
are unable to differentiate the possible translations
of the same source phrase with meaningful transla-
tion probabilities, and they won?t be added to the
phrase-table, nor put into a context with other words
that may trigger their use.
Therefore, we propose to use this technique to re-
place unknown words during the translation of the
monolingual data that we use to adapt the transla-
tion model. By these means, the automatically in-
duced translations of previously unknown morpho-
logical forms will be put into a context and actually
appear in the new adapted phrase-table. The corre-
sponding translation probabilities will be those cor-
responding to their frequency in the monolingual in-
domain data.
This procedure has been implemented, but we
were not able to obtain improvements in the BLEU
score. However, one can ask if automatic metrics,
evaluated on a test corpus of limited size, are the best
choice to judge this technique. In fact, in our setting
we have observed that less than 0.2% of the words
in the test set are unknown. We argue that the ability
to complement the phrase-table with many morpho-
logical forms of other wise known words, can only
improve the usability of SMT systems.
3 Task Description and resources
In this paper, we consider the translation of news
texts between French and English, in both direc-
tions. In order to allow comparisons, we used ex-
actly the same data as those allowed for the inter-
national evaluation organized in the framework of
the sixth workshop on SMT, to be held in Edinburgh
3http://snowball.tartarus.org/
287
Parallel data Size English/French French/English
[M words] Dev Test Dev Test
Eparl + nc 54 26.20 (0.06) 28.06 (0.2) 26.70 (0.06) 27.41 (0.2)
Eparl + nc + crawled1 168 26.84 (0.09) 29.08 (0.1) 27.96 (0.09) 28.20 (0.04)
Eparl + nc + crawled2 286 26.95 (0.04) 29.29 (0.03) 28.20 (0.03) 28.57 (0.1)
Eparl + nc + un 379 26.57 28.52 - -
Eparl + nc + crawled1 + un 514 26.87 28.99 - -
Eparl + nc + crawled2 + un 631 26.99 29.26 - -
Table 4: Case sensitive BLEU scores as a function of the amount of parallel training data. (Eparl=Europarl, nc=News
Commentary, crawled1/2=sub-sampled crawled bitexts, un=sub-sampled United Nations bitexts).
Corpus English French
Bitexts:
Europarl 50.5M 54.4M
News Commentary 2.9M 3.3M
United Nations 344M 393M
Crawled (109 bitexts) 667M 794M
Development data:
newstest2009 65k 73k
newstest2010 62k 71k
Monolingual data:
LDC Gigaword 4.1G 920M
Crawled news 2.6G 612M
Table 3: Available training data for the translation be-
tween French and English for the translation evaluation
at WMT?11 (number of words after tokenisation).
in July 2011. Preliminary results of this evaluation
are available on the Internet.4 Table 3 summarizes
the available training and development data. We op-
timized our systems on newstest2009 and used
newstest2010 as internal test set. For both cor-
pora, only one reference translations is available.
Scoring was performed with NIST?s implementation
of the BLEU score (?mt-eval? version 13).
3.1 Baseline system
The baseline system is a standard phrase-based SMT
system based on the the Moses SMT toolkit (Koehn
et al, 2007). It uses fourteen features functions
for translation, namely phrase and lexical translation
probabilities in both directions, seven features for
the lexicalized distortion model, a word and a phrase
penalty, and a target language model. It is con-
4http://matrix.statmt.org
structed as follows. First, word alignments in both
directions are calculated. We used a multi-threaded
version of the GIZA++ tool (Gao and Vogel, 2008).
Phrases and lexical reorderings are extracted using
the default settings of the Moses toolkit. All the bi-
texts were concatenated. The parameters of Moses
are tuned on the development data using the MERT
tool. For most of the runs, we performed three op-
timizations using different starting points and report
average results. English and French texts were to-
kenised using a modified version of the tools of the
Moses suite. Punctuation and case were preserved.
The language models were trained on all the avail-
able data, i.e. the target side of the bitexts, the whole
Gigaword corpus and the crawled monolingual data.
We build 4-gram back-off LMs with the SRI LM
toolkit using Modified Kneser-Ney and no cut-off
on all the n-grams. Past experience has shown that
keeping all n-grams slightly improves the perfor-
mance although this produces quite huge models
(10G and 30G of disk space for French and English
respectively).
Table 4 gives the baseline results using various
amounts of bitexts. Starting with the Europarl and
the News Commentary corpora, various amounts of
human translated data were added. The organizers
of the evaluation provide the so called 109 French-
English parallel corpus which contains almost 800
million words of data crawled from Canadian and
European Internet pages. Following works from the
2010 WMT evaluation (Lambert et al, 2010), we
filtered this data using IBM-1 probabilities and lan-
guage model scores to keep only the most reliable
translations. Two subsets were built with 115M and
232M English words respectively (using two differ-
288
alignment Dev Test
BLEU BLEU TER
giza 27.34 (0.01) 29.80 (0.06) 55.34 (0.06)
reused giza 27.40 (0.05) 29.82 (0.10) 55.30 (0.02)
reused moses 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
Table 5: Results for systems trained via different word alignment configurations. The values are the average over
3 MERT runs performed with different seeds. The numbers in parentheses are the standard deviation of these three
values. Translation was performed from English to French, adding 45M words of automatic translations (translated
from French to English) to the baseline system ?eparl+nc+crawled2?.
ent settings of the filter thresholds). They are re-
ferred to as ?crawled1? and ?crawled2? respectively.
Adding this data improved the BLEU score of al-
most 1 BLEU point (28.30 ? 29.27). This is our
baseline system to be improved by translation model
adaptation. Using the UN data gave no significant
improvement despite its huge size. This is probably
a typical example that it is not necessarily useful to
use all available parallel training data, in particular
when a very specific (out-of domain) jargon is used.
Consequently, the UN data was not used in the sub-
sequent experiments.
We were mainly working on the translation from
English to French. Therefore only one baseline sys-
tem was build for the reverse translation direction.
4 Experimental Evaluation
The system trained on Europarl, News Commen-
tary and the sub-sampled version of the 109 bitexts
(?eparl+nc+crawled2?, in the third line of Table 3),
was used to translate parts of the crawled news in
French and English. Statistics on the translated data
are given in Table 6.
We focused on the most recent data since the
time period of our development and test data was
end of 2008 and 2009 respectively. In the future
we will translate all the available monolingual data
and make it available to the community in order to
ease the widespread use of this kind of translation
model adaptation methods. These automatic trans-
lations were filtered using the sentence normalized
log-score of the decoder, as proposed by (Schwenk,
2008). However, we did not perform systematic ex-
periments to find the optimal threshold on this score,
but simply used a value which seems to be a good
compromise of quality and quantity of the transla-
tions. This gave us about 45M English words of
Corpus French (fe) English (ef)
available filtered available filtered
2009 92 31 121 45
2010 43 12 112 49
2011 8 2 15 6
total 219 45 177 100
Table 6: Monolingual data used to adapt the systems,
given in millions of English words. Under ?French (fe)?,
we indicated the number of translated English words
from French, and under ?English (ef)? we reported the
number of source English words translated into French.
Thus ?fe? and ?ef? refer respectively to French?English
and English?French translation direction of monolingual
data. In the experiments we used the 100M English?
French (ef) filtered monolingual data, as well as a 45M-
word subset (in order to have the same amount of data as
for French?English) and a 65M-word subset.
automatic translations from French, as well as the
translations into French of 100M English words, to
be used to adapt the baseline systems.
4.1 Word alignment
In order to build a phrase table with the translated
data, we re-used the word alignment obtained dur-
ing the translation with the moses toolkit. We com-
pared the system trained via these alignments to
the systems built by running GIZA++ on all the
data. When word alignments of the baseline corpus
(not adapted) are trained together with the translated
data, they could be affected by phrase pairs com-
ing from incorrect translations. To measure this ef-
fect, we trained an additional system, for which the
alignments of the baseline corpus are those trained
without the translated data. For the translated data,
we re-use the GIZA++ alignments trained on all the
data. Results for these three alignment configura-
289
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.20 (0.06) 28.06 (0.22) 56.85 (0.09)
news fe 45M 27.18 (0.09) 29.03 (0.07) 55.97 (0.07)
news ef 45M 26.15 (0.04) 28.44 (0.09) 56.56 (0.11)
Eparl + nc + crawled2 - 26.95 (0.04) 29.29 (0.03) 55.77 (0.19)
news fe 45M 27.42 (0.02) 29.77 (0.06) 55.27 (0.03)
news ef 45M 26.75 (0.04) 28.88 (0.10) 56.06 (0.05)
Table 7: Translation results of the English?French systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M refers to the number of English running words.
baseline translated bitexts Dev Test
BLEU BLEU TER
Eparl + nc - 26.70 (0.06) 27.41 (0.24) 55.07 (0.17)
news fe 45M 27.47 (0.08) 27.77 (0.23) 54.84 (0.13)
news ef 45M 27.55 (0.05) 28.51 (0.10) 54.12 (0.09)
news ef 65M 27.58 (0.03) 28.70 (0.09) 54.06 (0.17)
news ef 100M 27.63 (0.06) 28.68 (0.06) 54.02 (0.06)
Eparl + nc + crawled2 - 28.20 (0.03) 28.54 (0.12) 54.17 (0.15)
news fe 45M 28.02 (0.11) 28.40 (0.10) 54.45 (0.06)
news ef 45M 28.24 (0.06) 28.93 (0.22) 53.90 (0.08)
news ef 65M 28.16 (0.19) 28.75 (0.06) 54.03 (0.14)
news ef 100M 28.28 (0.09) 28.96 (0.03) 53.79 (0.09)
Table 8: Translation results of the French?English systems augmented with a bitext obtained by translating news data
from English to French (ef) and French to English (fe). 45M/65M/100M refers to the number of English running
words.
tions are presented in Table 5. In these systems
French sources and English translations (45 mil-
lion words) were added to the ?eparl+nc+crawled2?
baseline corpus. According to BLEU and TER met-
rics, reusing Moses alignments to build the adapted
phrase table has no significant impact on the system
performance. We repeated the experiment without
the 109 corpus and with the smaller selection of 109
(crawled1) and arrived to the same conclusion.
However, the re-use of Moses alignments saves time
and resources. On the larger baseline corpus, the
mGiza process lasted 46 hours with two jobs of 4
thread running and a machine with two Intel X5650
quad-core processors.
4.2 Choice of the translation direction
A second point under study in this work is the effect
of the translation direction of the monolingual data
used to adapt the translation model. Tables 7 and
8 present results for, respectively, English?French
and French?English systems adapted with news data
translated from English to French (ef) and French
to English (fe). The experiment was repeated with
two baseline corpora. The results show clearly
that target to source translated data are more use-
ful than source to target translated data. The im-
provement in terms of BLEU score due to the use of
target-to-source translated data instead of source-to-
target translated data ranges from 0.5 to 0.9 for the
French?English and English?French systems. For
instance, when translating from English to French
(Table 7), the baseline system ?eparl+nc? achieves
a BLEU score of 28.06 on the test set. This could
be improved to 29.03 using automatic translations
in the reverse direction (French to English), while
we only achieve a BLEU score of 28.44 when us-
ing automatic translation performed in the same di-
rection as the system to be adapted. The effect is
even clearer when we try to adapt the large system
290
?eparl+nc+crawled2?. Adding automatic transla-
tions translated from English-to-French did actually
lead to a lower BLEU score (29.29 ? 28.88) while
we observe an improvement of nearly 0.5 BLEU in
the other case.
With target-to-source translated news data,
the gain with respect to the baseline corpus
for English-French systems (Table 7) is nearly
1 BLEU for ?Eparl+nc? and 0.5 BLEU for
?Eparl+nc+crawled2?. With the same amount
of translated data (45 million English words),
approximately the same gains are observed in
French?English systems. Due to the larger avail-
ability of English news data, we were able to use
larger sets of target-to-source translated data for
French-English systems, as can be seen in Table 8.
With a bitext containing additionally 20 million
English words, we get a further improvement of
0.2 BLEU for ?Eparl+nc? (28.51 ? 28.70), but no
improvement for ?Eparl+nc+crawled2? (the BLEU
score is even lower, but the scores lie within the
error interval). No further gain on the test data is
achieved if we add again 35 million English words
(total of 100M words) to the system ?Eparl+nc?.
With the ?Eparl+nc+crawle2? baseline, no sig-
nificant improvement is observed if we adapt the
system with 100M words instead of only 45M.
4.3 Result analysis
To get more insight into what happens to the model
when we add the automatic translations, we cal-
culated some statistics of the phrase table, pre-
sented in Table 9. Namely, we calculated the
number of entries in the phrase table, the aver-
age number of translation options of each source
phrase, the average entropy for each source phrase,
the average source phrase length (in words) and
the average target phrase length. The entropy is
calculated over the probabilities of all translation
options for each source phrase. Comparing the
baseline with ?Eparl+nc? and the baseline with
?Eparl+nc+crawl2?, we can observe that the aver-
age number of translation options was nearly mul-
tiplied by 3 with the addition of 230 million words
of human translated bitexts. As a consequence the
average entropy was increased from 1.84 to 2.08.
On the contrary, adding 100 million words of in-
domain automatic translations, the average num-
ber of translation options increased by only 5%
for the ?Eparl+nc? baseline, and decreased for the
?Eparl+nc+crawl2? baseline. A decrease may occur
if new source phrases with less translation options
than the average are added. Furthermore, with the
addition of 45 million words of in-domain data, the
average entropy dropped from 1.84 to 1.33 or 1.60
for the ?Eparl+nc? baseline, and from 2.08 to 1.81 or
1.96 for the ?Eparl+nc+crawl2? baseline. With both
baselines, the more translations are added to the sys-
tem, the lower the entropy, although in some case
the number of translation options increases (this is
the case when we pass from 65M to 100M words
of synthetic data). These results illustrate the fact
that the automatic translations only reinforce some
probabilities in the model, with the subsequent de-
crease in entropy, while human translations add new
vocabulary. Note also that in the corpus using au-
tomatic translations, new words can only occur in
the source side. Thus when translating from French
to English, automatic translations from English to
French are expected to yield more translation op-
tions and a higher entropy than the automatic trans-
lations from French to English. This is what is ef-
fectively observed in Table 9.
5 Conclusion
Unsupervised training is widely used in other ar-
eas, in particular large vocabulary speech recogni-
tion. The statistical models in speech recognition
use a generative approach based on small units, usu-
ally triphones. Each triphone is modeled by a hid-
den Markov model and Gaussian mixture probabil-
ity distributions (plus many improvements like pa-
rameter tying etc). Many methods were developed
to adapt such models. The corresponding model
in statistical machine translation is the phrase table,
a long list of known words with their translations
and probabilities. It seems much more challenging
to adapt this kind of statistical model with unsuper-
vised training, i.e. monolingual data. Nevertheless,
we believe that unsupervised training can be also
very useful in SMT. To the best of our knowledge,
work in this area is very recent and only in its begin-
nings. This paper tries to give additional insights in
this promising method.
Our work is based on the approach initially pro-
291
baseline translated bitexts entries (M) translations entropy src size trg size
Eparl + nc - 7.16 83.83 1.84 1.80 2.81
news fe 45M 7.42 70.00 1.33 1.83 2.80
news ef 45M 8.24 81.58 1.60 1.86 2.79
news ef 65M 8.42 81.58 1.55 1.88 2.79
news ef 100M 9.21 85.93 1.54 1.90 2.79
Eparl + nc + crawl2 - 25.42 235.16 2.08 1.76 2.93
news fe 45M 25.54 217.21 1.81 1.77 2.93
news ef 45M 26.09 228.07 1.96 1.78 2.93
news ef 65M 26.21 226.45 1.91 1.78 2.93
news ef 100M 26.79 227.08 1.89 1.79 2.93
Table 9: Phrase table statistics for French?English systems augmented with bitexts built via automatic translations.
Only the entries useful to translate the development set were present in the considered phrase table.
posed in (Schwenk, 2008): build a first SMT sys-
tem, use it to translate large amounts of monolingual
data, filter the obtained translations, add them to the
bitexts and build a new system from scratch.
We proposed several extensions to this technique
which seem to improve the translations quality in
our experiments. First of all, we have observed that
it is clearly better to add automatically translated
texts to the translations model training data which
were translated from the target to the source lan-
guage. This seems to ensure that potentially wrong
translations are not used in the new model.
Second, we were able to skip the process of per-
forming word alignment of this additional parallel
data without any significant loss in the BLEU score.
Performing word alignments with GIZA++ can eas-
ily take several days when several hundred millions
of bitexts are available. Instead, we directly used the
word alignments produced by Moses when translat-
ing the monolingual data. This resulted in an appre-
ciable speed-up of the procedure, but has also inter-
esting theoretical aspects. Reusing the word align-
ment from the translation process is expected to re-
sult in a phrase extraction process that is more con-
sistent with the use of the phrases.
Finally, we outlined a method to automatically
add new translations without any additional parallel
training data. In fact, when translating from a mor-
phologically rich language to an easier one, in our
case from French to English, it is often possible to
infer the translations of unobserved morphological
forms of nouns, verbs or adjectives. This is obtained
by looking up the stemmed form in an automati-
cally constructed dictionary. This kind of approach
could be also applied to a classical PBSMT system,
by adding various forms to the phrase table, but it
is not obvious to come up with reasonable transla-
tions probabilities for these new entries. In our ap-
proach, the unknown word forms are processed in
large amounts of monolingual data and the induced
translations will appear in the context of complete
sentences. Wrong translations can be blocked by the
language model and the new translations can appear
in phrases of various lengths.
This paper provided a detailed experimental eval-
uation of these methods. We considered the trans-
lation between French and English using the same
data than was made available for the 2011 WMT
evaluation. Improvement of up to 0.5 BLEU were
observed with respect to an already competitive sys-
tem trained on more than 280M words of human
translated parallel data.
Acknowledgments
This work has been partially funded by the French
Government under the project COSMAT (ANR
ANR-09-CORD-004.) and the European Commis-
sion under the project EUROMATRIXPLUS (ICT-
2007.2.2-FP7-231720).
292
References
Nicola Bertoldi and Marcello Federico. 2009. Domain
adaptation for statistical machine translation. In Forth
Workshop on SMT, pages 182?189.
Nicola Bertoldi, Madalina Barbaiani, Marcello Federico,
and Roldano Cattoni. 2008. Phrase-based statistical
machine translation with pivot languages. In IWSLT,
pages 143?149.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008. Exploiting n-best hypotheses for SMT self-
enhancement. In ACL, pages 157?160.
Jorge Civera and Alfons Juan. 2007. Domain adaptation
in statistical machine translation with mixture mod-
elling. In Second Workshop on SMT, pages 177?180,
June.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In EMNLP, pages 128?135.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In EMNLP, pages
967?975, Prague, Czech Republic.
Philipp Koehn and Josh Schroeder. 2007. Experiments in
domain adaptation for statistical machine translation.
In Second Workshop on SMT, pages 224?227, June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Workshop on SMT, pages 121?126.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Holger Schwenk and Jean Senellart. 2009. Translation
model adaptation for an Arabic/French news trans-
lation system by lightly-supervised training. In MT
Summit.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
Nicola Ueffing, Gholamreza Haffari, and Anoop Sarkar.
2007. Transductive learning for statistical machine
translation. In ACL, pages 25?32.
Nicola Ueffing. 2006. Using monolingual source-
language data to improve MT performance. In IWSLT,
pages 174?181.
Joern Wuebker, Arne Mauser, and Hermann Ney. 2010.
Training phrase translation models with leaving-one-
out. In ACL, pages 475?484, Uppsala, Sweden, July.
Association for Computational Linguistics.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In Coling.
293
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 464?469,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2011
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2011
WMT shared task evaluation. Our main sys-
tems were standard phrase-based statistical
systems based on the Moses decoder, trained
on the provided data only, but we also per-
formed initial experiments with hierarchical
systems. Additional, new features this year in-
clude improved translation model adaptation
using monolingual data, a continuous space
language model and the treatment of unknown
words.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2011 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year?s system (Lambert et
al., 2010) are as follows: use of more training data
as provided by the organizers, improved translation
model adaptation by unsupervised training, a con-
tinuous space language model for the translation
into French, some attempts to automatically induce
translations of unknown words and first experiments
with hierarchical systems. These different points are
described in the rest of the paper, together with a
summary of the experimental results showing the
impact of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations were then used
directly with the source texts to create additional bi-
texts. In a second stage, these additional bilingual
data were incorporated into the system (see Sec-
tion 5 and Tables 4 and 5).
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
6) were used. We also took as training data a sub-
set of the French?English Gigaword (109) corpus.
We applied the same filters as last year to select this
subset. The first one is a lexical filter based on the
IBM model 1 cost (Brown et al, 1993) of each side
of a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter was
trained on a corpus composed of Eparl, NC, and UN
data. The other filter is an n-gram language model
(LM) cost of the target sentence (see Section 3), nor-
malised with respect to its length. This filter was
trained with all monolingual resources available ex-
cept the 109 data. We generated two subsets, both
by selecting sentence pairs with a lexical cost infe-
rior to 4, and an LM cost respectively inferior to 2.3
(1091, 115 million English words) and 2.6 (10
9
2, 232
million English words).
464
2.2 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the task do-
main.
First, we generated automatic translations of the
provided monolingual News corpus and selected the
sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitext contain no new translations, since
all words of the translation output come from the
translation model, but it contains new combinations
(phrases) of known words, and reinforces the prob-
ability of some phrase pairs (Schwenk, 2008). This
year, we improved this method in the following way.
In the original approach, the automatic translations
are added to the human translated bitexts and a com-
plete new system is build, including time consuming
word alignment with GIZA++. For WMT?11, we
directly used the word-to-word alignments produced
by the decoder at the output instead of GIZA?s align-
ments. This speeds-up the procedure and yields the
same results in our experiments. A detailed compar-
ison is given in (Lambert et al, 2011).
Second, as in last year?s evaluation, we automat-
ically extracted and aligned parallel sentences from
comparable in-domain corpora. We used the AFP
and APW news texts since there are available in the
French and English LDC Gigaword corpora. The
general architecture of our parallel sentence extrac-
tion system is described in detail by Abdul-Rauf and
Schwenk (2009). We first translated 91M words
from French into English using our first stage SMT
system. These English sentences were then used to
search for translations in the English AFP and APW
texts of the Gigaword corpus using information re-
trieval techniques. The Lemur toolkit (Ogilvie and
Callan, 2001) was used for this purpose. Search
was limited to a window of ?5 days of the date of
the French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER below
75% were kept. Sentences with a large length differ-
ence (French versus English) or containing a large
fraction of numbers were also discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.3 Monolingual data
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC?s Gigaword collection was used for
both languages. Data corresponding to the develop-
ment and test periods were removed from the Giga-
word collections.
2.4 Development data
All development was done on newstest2009, and
newstest2010 was used as internal test set. The de-
fault Moses tokenization was used. However, we
added abbreviations for the French tokenizer. All
our models are case sensitive and include punctua-
tion. The BLEU scores reported in this paper were
calculated with the tool multi-bleu.perl and are case
sensitive.
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sen-
tence f . Our main system is a phrase-based system
(Koehn et al, 2003; Och and Ney, 2003), but we
have also performed some experiments with a hier-
archical system (Chiang, 2007). Both use a log lin-
ear framework in order to introduce several models
explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och
and Ney, 2002). The phrase-based system uses four-
teen features functions, namely phrase and lexical
translation probabilities in both directions, seven
features for the lexicalized distortion model, a word
and a phrase penalty and a target language model
(LM). The hierarchical system uses only 8 features:
a LM weight, a word penalty and six weights for the
translation model.
Both systems are based on the Moses SMT toolkit
(Koehn et al, 2007) and constructed as follows.
465
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases, lexical reorderings or hierarchical rules
are extracted using the default settings of the Moses
toolkit. The parameters of Moses were tuned on
newstest2009, using the ?new? MERT tool. We re-
peated the training process three times, each with a
different seed value for the optimisation algorithm.
In this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build on
each data source with the SRI LM toolkit (Stolcke,
2002) and then linearly interpolated, optimizing the
coefficients with an EM procedure. The perplexities
of these LMs were 99.4 for French and 129.7 for
English. In addition, we build a 5-gram continuous
space language model for French (Schwenk, 2007).
This model was trained on all the available French
texts using a resampling technique. The continu-
ous space language model is interpolated with the
4-gram back-off model and used to rescore n-best
lists. This reduces the perplexity by about 8% rela-
tive.
4 Treatment of unknown words
Finally, we propose a method to actually add new
translations to the system inspired from (Habash,
2008). For this, we propose to identity unknown
words and propose possible translations.
Moses has two options when encountering an un-
known word in the source language: keep it as it is
or drop it. The first option may be a good choice
for languages that use the same writing system since
the unknown word may be a proper name. The sec-
ond option is usually used when translating between
language based on different scripts, e.g. translating
1The source is available at http://www.cs.cmu.edu/
?qing/
Source language Source language Target language
French stemmed form English
finies fini finished
efface?s efface? erased
hawaienne hawaien Hawaiian
... ... ...
Table 1: Example of translations from French to English
which are automatically extracted from the phrase-table
with the stemmed form.
from Arabic to English. Alternatively, we propose to
infer automatically possible translations when trans-
lating from a morphologically rich language, to a
simpler language. In our case, we use this approach
to translate from French to English.
Several of the unknown words are actually adjec-
tives, nouns or verbs in a particular form that itself
is not known, but the phrase table would contain the
translation of a different form. As an example we
can mention the French adjective finies which is in
the female plural form. After stemming we may be
able to find the translation in a dictionary which is
automatically extracted from the phrase-table (see
Table 1). This idea was already outlined by (Bo-
jar and Tamchyna, 2011) to translate from Czech to
English.
First, we automatically extract a dictionary from
the phrase table. This is done, be detecting all 1-to-1
entries in the phrase table. When there are multi-
ple entries, all are kept with their lexical translations
probabilities. Our dictionary has about 680k unique
source words with a total of almost 1M translations.
source segment les travaux sont finis
target segment works are finis
stemmed word found fini
translations found finished, ended
segment proposed works are finished
works are ended
segment kept works are finished
Table 2: Example of the treatment of an unknown French
word and its automatically inferred translation.
The detection of unknown words is performed by
comparing the source and the target segment in order
to detect identical words. Once the unknown word
is selected, we are looking for its stemmed form in
the dictionary and propose some translations for the
unknown word based on lexical score of the phrase
table (see Table 2 for some examples). The snowball
466
Bitext #Fr Words PT size newstest2009 newstest2010
(M) (M) BLEU BLEU TER METEOR
Eparl+NC 56 7.1 26.74 27.36 (0.19) 55.11 (0.14) 60.13 (0.05)
Eparl+NC+1091 186 16.3 27.96 28.20 (0.04) 54.46 (0.10) 60.88 (0.05)
Eparl+NC+1092 323 25.4 28.20 28.57 (0.10) 54.12 (0.13) 61.20 (0.05)
Eparl+NC+news 140 8.4 27.31 28.41 (0.13) 54.15 (0.14) 61.13 (0.04)
Eparl+NC+1092+news 406 25.5 27.93 28.70 (0.24) 54.12 (0.16) 61.30 (0.20)
Eparl+NC+1092+IR 351 25.3 28.07 28.51 (0.18) 54.07 (0.06) 61.18 (0.07)
Eparl+NC+1092+news+IR 435 26.1 27.99 28.93 (0.02) 53.84 (0.07) 61.46 (0.07)
+larger beam+pruned PT 435 8.2 28.44 29.05 (0.14) 53.74 (0.16) 61.68 (0.09)
Table 4: French?English results: number of French words (in million), number of entries in the filtered phrase-table
(in million) and BLEU scores in the development (newstest2009) and internal test (newstest2010) sets for the different
systems developed. The BLEU scores and the number in parentheses are the average and standard deviation over 3
values (see Section 3)
corpus newstest2010 subtest2010
number of sentences 2489 109
number of words 70522 3586
number of UNK detected 118 118
nbr of sentences containing UNK 109 109
BLEU Score without UNK process 29.43 24.31
BLEU Score with UNK process 29.43 24.33
TER Score without UNK process 53.08 58.54
TER Score with UNK process 53.08 58.59
Table 3: Statistics of the unknown word (UNK) process-
ing algorithm on our internal test (newstest2010) and its
sub-part containing only the processed sentences (sub-
test2010).
stemmer2 was used. Then the different hypothesis
are evaluated with the target language model.
We processed the produced translations with this
method. It can happen that some words are transla-
tions of themselves, e.g. the French word ?duel? can
be translated by the English word ?duel?. If theses
words are present into the extracted dictionary, we
keep them. If we do not find any translation in our
dictionary, we keep the translation. By these means
we hope to keep named entities.
Several statistics made on our internal test (new-
stest2010) are shown in Table 3. Its shows that the
influence of the detected unknown words is minimal.
Only 0.16% of the words in the corpus are actually
unknown. However, the main goal of this process
is to increase the human readability and usefulness
without degrading automatic metrics. We also ex-
pect a larger impact in other tasks for which we have
2http://snowball.tartarus.org/
smaller amounts of parallel training data. In future
versions of this detection process, we will try to de-
tect unknown words before the translation process
and propose alternatives hypothesis to the Moses de-
coder.
5 Results and Discussion
The results of our SMT system for the French?
English and English?French tasks are summarized
in Tables 4 and 5, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between two
average scores is less than the sum of the standard
deviations, we can say that this difference is not sig-
nificant. The reverse is not true. Note that most of
the improvements shown in the tables are small and
not significant. However many of the gains are cu-
mulative and the sum of several small gains makes a
significant difference.
Baseline French?English System
The first section of Table 4 shows results of the de-
velopment of the baseline SMT system, used to gen-
erate automatic translations.
Although no French translations were generated,
we did similar experiments in the English?French
direction (first section of Table 5).
467
Bitext #En Words newstest2009 newstest2010
(M) BLEU BLEU TER
Eparl+NC 52 26.20 28.06 (0.22) 56.85 (0.08)
Eparl+NC+1091 167 26.84 29.08 (0.12) 55.83 (0.14)
Eparl+NC+1092 284 26.95 29.29 (0.03) 55.77 (0.19)
Eparl+NC+1092+news 299 27.34 29.56 (0.14) 55.44 (0.18)
Eparl+NC+1092+IR 311 27.14 29.43 (0.12) 55.48 (0.06)
Eparl+NC+1092+news+IR 371 27.32 29.73 (0.21) 55.16 (0.20)
+rescoring with CSLM 371 27.46 30.04 54.79
Table 5: English?French results: number of English words (in million) and BLEU scores in the development (new-
stest2009) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and the number
in parentheses are the average and standard deviation over 3 values (see Section 3.)
In both cases the best system is the one trained
on the Europarl, News-commentary and 1092 cor-
pora. This system was used to generate the auto-
matic translations. We did not observe any gain
when adding the United Nations data, so we dis-
carded this data.
Impact of the Additional Bitexts
With the baseline French?English SMT system (see
above), we translated the French News corpus to
generate an additional bitext (News). We also trans-
lated some parts of the French LDC Gigaword cor-
pus, to serve as queries to our IR system (see section
2.2). The resulting additional bitext is referred to as
IR. The second section of Tables 4 and 5 summarize
the system development including the additional bi-
texts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate the
automatic translations, but with less than half of
the data. Adding the News corpus to a larger cor-
pus, such as Eparl+NC+1092, has less impact but
still yields some improvement: 0.1 BLEU point in
French?English and 0.3 in English?French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. This
effect is studied in detail in a separate paper (Lam-
bert et al, 2011). With the IR additional bitext added
to Eparl+NC+1092, we observe no improvement in
French to English, and a very small improvement
in English to French. However, added to the base-
line system (Eparl+NC+1092) adapted with the News
data, the IR additional bitexts yield a small (0.2
BLEU) improvement in both translation directions.
Final System
In both translation directions our best system was the
one trained on Eparl+NC+1092+News+IR. We fur-
ther achieved small improvements by pruning the
phrase-table and by increasing the beam size. To
prune the phrase-table, we used the ?sigtest-filter?
available in Moses (Johnson et al, 2007), more pre-
cisely the ??  filter3.
We also build hierarchical systems on the various
human translated corpora, using up to 323M words
(corpora Eparl+NC+1092). The systems yielded sim-
ilar results than the phrase-based approach, but re-
quired much more computational resources, in par-
ticular large amounts of main memory to perform
the translations. Running the decoder was actually
only possible with binarized rule-tables. Therefore,
the hierarchical system was not used in the evalua-
tion system.
3The p-value of two-by-two contingency tables (describing
the degree of association between a source and a target phrase)
is calculated with Fisher exact test. This probability is inter-
preted as the probability of observing by chance an association
that is at least as strong as the given one, and hence as its sig-
nificance. An important special case of a table occurs when a
phrase pair occurs exactly once in the corpus, and each of the
component phrases occurs exactly once in its side of the paral-
lel corpus (1-1-1 phrase pairs). In this case the negative log of
the p-value is ? = logN (N is number of sentence pairs in the
corpus). ? ?  is the largest threshold that results in all of the
1-1-1 phrase pairs being included.
468
6 Conclusions and Further Work
We presented the development of our statistical ma-
chine translation systems for the French?English
and English?French 2011 WMT shared task. In the
official evaluation the English?French system was
ranked first according to the BLEU score and the
French?English system second.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Forms Wanted:
Training SMT on Monolingual Data. Abstract at
Machine Translation and Morphologically-Rich Lan-
guages. Research Workshop of the Israel Science
Foundation University of Haifa, Israel, January.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. Computational Linguistics, 19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In ACL 08.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Sadaf Abdul-Rauf, and Holger Schwenk.
2010. LIUM SMT machine translation system for
WMT 2010. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 121?126, Uppsala, Sweden, July.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
model adaptation using monolingual data. In Sixth
Workshop on SMT, page this volume.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proc. of the Annual Meet-
ing of the Association for Computational Linguistics,
pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
469
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 11?19,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Large, Pruned or Continuous Space Language Models on a GPU
for Statistical Machine Translation
Holger Schwenk, Anthony Rousseau and Mohammed Attik
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
Holger.Schwenk@lium.univ-lemans.fr
Abstract
Language models play an important role in
large vocabulary speech recognition and sta-
tistical machine translation systems. The
dominant approach since several decades are
back-off language models. Some years ago,
there was a clear tendency to build huge lan-
guage models trained on hundreds of billions
of words. Lately, this tendency has changed
and recent works concentrate on data selec-
tion. Continuous space methods are a very
competitive approach, but they have a high
computational complexity and are not yet in
widespread use. This paper presents an ex-
perimental comparison of all these approaches
on a large statistical machine translation task.
We also describe an open-source implemen-
tation to train and use continuous space lan-
guage models (CSLM) for such large tasks.
We describe an efficient implementation of the
CSLM using graphical processing units from
Nvidia. By these means, we are able to train
an CSLM on more than 500 million words in
20 hours. This CSLM provides an improve-
ment of up to 1.8 BLEU points with respect to
the best back-off language model that we were
able to build.
1 Introduction
Language models are used to estimate the proba-
bility of a sequence of words. They are an impor-
tant module in many areas of natural language pro-
cessing, in particular large vocabulary speech recog-
nition (LVCSR) and statistical machine translation
(SMT). The goal of LVCSR is to convert a speech
signal x into a sequence of words w. This is usually
approached with the following fundamental equa-
tion:
w? = argmax
w
P (w|x)
= argmax
w
P (x|w)P (w) (1)
In SMT, we are faced with a sequence of words e
in the source language and we are looking for its
best translation f into the target language. Again,
we apply Bayes rule to introduce a language model:
f? = argmax
f
P (f |e)
= argmax
f
P (e|f)P (f) (2)
Although we use a language model to evaluate the
probability of the produced sequence of words, w
and f respectively, we argue that the task of the lan-
guage model is not exactly the same for both ap-
plications. In LVCSR, the LM must choose among
a large number of possible segmentations of the
phoneme sequence into words, given the pronuncia-
tion lexicon. It is also the only component that helps
to select among homonyms, i.e. words that are pro-
nounced in the same way, but that are written dif-
ferently and which have usually different meanings
(e.g. ate/eight or build/billed). In SMT, on the other
hand, the LM has the responsibility to chose the best
translation of a source word given the context. More
importantly, the LM is a key component which has
to sort out good and bad word reorderings. This
is known to be a very difficult issue when translat-
ing from or into languages like Chinese, Japanese or
German. In LVCSR, the word order is given by the
time-synchronous processing of the speech signal.
Finally, the LM helps to deal with gender, number,
11
etc accordance of morphologically rich languages,
when used in an LVCSR as well as an SMT system.
Overall, one can say that the semantic level seems
to be more important for language modeling in SMT
than LVCSR. In both applications, so called back-off
n-gram language models are the de facto standard
since several decades. They were first introduced
in the eighties, followed by intensive research on
smoothing methods. An extensive comparison can
be found in (Chen and Goodman, 1999). Modified-
Kneser Ney smoothing seems to be the best perform-
ing method and it is this approach that is almost ex-
clusively used today.
Some years ago, there was a clear tendency in
SMT to use huge LMs trained on hundreds on bil-
lions (1011) of words (Brants et al, 2007). The au-
thors report continuous improvement of the trans-
lation quality with increasing size of the LM train-
ing data, but these models require a large cluster to
train and to perform inference using distributed stor-
age. Therefore, several approaches were proposed
to limit the storage size of large LMs, for instance
(Federico and Cettolo, 2007; Talbot and Osborne,
2007; Heafield, 2011).
1.1 Continuous space language models
The main drawback of back-off n-gram language
models is the fact that the probabilities are estimated
in a discrete space. This prevents any kind of inter-
polation in order to estimate the LM probability of
an n-gram which was not observed in the training
data. In order to attack this problem, it was pro-
posed to project the words into a continuous space
and to perform the estimation task in this space. The
projection as well as the estimation can be jointly
performed by a multi-layer neural network (Bengio
and Ducharme, 2001; Bengio et al, 2003). The ba-
sic architecture of this approach is shown in figure 1.
A standard fully-connected multi-layer per-
ceptron is used. The inputs to the neural
network are the indices of the n?1 pre-
vious words in the vocabulary hj=wj?n+1,
. . . , wj?2, wj?1 and the outputs are the posterior
probabilities of all words of the vocabulary:
P (wj = i|hj) ?i ? [1, N ] (3)
where N is the size of the vocabulary. The input
uses the so-called 1-of-n coding, i.e., the ith word of
projection
layer hidden
layer
output
layerinput
projections
shared
continuous
representation: representation:
indices in wordlist
LM probabilitiesdiscrete
for all words
probability estimation
Neural Network
N
wj?1 P
H
N
P (wj=1|hj)
wj?n+1
wj?n+2
P (wj=i|hj)
P (wj=N|hj)
P dimensional vectors
ck
oiM
Vdj
p1 =
pN =
pi =
Figure 1: Architecture of the continuous space LM. hj
denotes the context wj?n+1, . . . , wj?1. P is the size of
one projection andH ,N is the size of the hidden and out-
put layer respectively. When short-lists are used the size
of the output layer is much smaller then the size of the
vocabulary.
the vocabulary is coded by setting the ith element of
the vector to 1 and all the other elements to 0. The
ith line of the N ? P dimensional projection matrix
corresponds to the continuous representation of the
ith word. Let us denote cl these projections, dj the
hidden layer activities, oi the outputs, pi their soft-
max normalization, and mjl, bj , vij and ki the hid-
den and output layer weights and the corresponding
biases. Using these notations, the neural network
performs the following operations:
dj = tanh
(
?
l
mjl cl + bj
)
(4)
oi =
?
j
vij dj + ki (5)
pi = e
oi /
N?
r=1
eor (6)
The value of the output neuron pi is used as the prob-
ability P (wj = i|hj). Training is performed with
the standard back-propagation algorithm minimiz-
ing the following error function:
E =
N?
i=1
ti log pi + ?
?
?
?
jl
m2jl +
?
ij
v2ij
?
? (7)
where ti denotes the desired output, i.e., the proba-
12
bility should be 1.0 for the next word in the training
sentence and 0.0 for all the other ones. The first part
of this equation is the cross-entropy between the out-
put and the target probability distributions, and the
second part is a regularization term that aims to pre-
vent the neural network from overfitting the training
data (weight decay). The parameter ? has to be de-
termined experimentally.
The CSLM has a much higher complexity than a
back-off LM, in particular because of the high di-
mension of the output layer. Therefore, it was pro-
posed to limit the size of the output layer to the most
frequent words, the other ones being predicted by
a standard back-off LM (Schwenk, 2004). All the
words are still considered at the input layer.
It is important to note that the CSLM is still an
n-gram approach, but the notion of backing-off to
shorter contexts does not exist any more. The model
can provide probability estimates for any possible
n-gram. It also has the advantage that the complex-
ity only slightly increases for longer context win-
dows, while it is generally considered to be unfea-
sible to train back-off LMs on billions of words for
orders larger than 5.
The CSLM was very successfully applied to large
vocabulary speech recognition. It is usually used to
rescore lattices and improvements of the word er-
ror rate of about one point were consistently ob-
served for many languages and domains, for in-
stance (Schwenk and Gauvain, 2002; Schwenk,
2007; Park et al, 2010; Liu et al, 2011; Lamel et
al., 2011). More recently, the CSLM was also suc-
cessfully applied to statistical machine translation
(Schwenk et al, 2006; Schwenk and Este`ve, 2008;
Schwenk, 2010; Le et al, 2010)
During the last years, several extensions were pro-
posed in the literature, for instance:
? Mikolov and his colleagues are working on
the use of recurrent neural networks instead of
multi-layer feed-forward architecture (Mikolov
et al, 2010; Mikolov et al, 2011).
? A simplified calculation of the short-list prob-
ability mass and the addition of an adaptation
layer (Park et al, 2010; Liu et al, 2011)
? the so-called SOUL architecture which allows
to cover all the words at the output layer instead
of using a short-list (Le et al, 2011a; Le et al,
2011b), based on work by (Morin and Bengio,
2005; Mnih and Hinton, 2008).
? alternative sampling in large corpora (Xu et al,
2011)
Despite significant and consistent gains in
LVCSR and SMT, CSLMs are not yet in widespread
use. Possible reasons for this could be the large com-
putational complexity which requires flexible and
carefully tuned software so that the models can be
build and used in an efficient manner.
In this paper we provide a detailed comparison of
the current most promising language modeling tech-
niques for SMT: huge back-off LMs that integrate
all available data, LMs trained on data selected with
respect to its relevance to the task by a recently pro-
posed method (Moore and Lewis, 2010), and a new
very efficient implementation of the CSLM which
integrates data selection.
2 Continuous space LM toolkit
Free software to train and use CSLM was proposed
in (Schwenk, 2010). The first version of this toolkit
provided no support for short lists or other means to
train CSLMs with large output vocabularies. There-
fore, it was not possible to use it for LVCSR and
large SMT tasks. We extended our tool with full
support for short lists during training and inference.
Short lists are implemented as proposed in (Park et
al., 2010), i.e. we add one extra output neuron for
all words that are not in the short list. This prob-
ability mass is learned by the neural network from
the training data. However, we do not use this prob-
ability mass to renormalize the output distribution,
we simply assume that it is sufficiently close to the
probability mass reserved by the back-off LM for
words that are not in the short list. In summary, dur-
ing inference words in the short-list are predicted by
the neural network and all the other ones by a stan-
dard back-off LM. No renormalization is performed.
We have performed some comparative experiments
with renormalization during inference and we could
not observe significant differences. The toolkit sup-
ports LMs in the SRILM format, an interface to the
popular KENLM is planed.
13
2.1 Parallel processing
The computational power of general purpose pro-
cessors like those build by Intel or AMD has con-
stantly increased during the last decades and opti-
mized libraries are available to take advantage of the
multi-core capabilities of modern CPUs. Our CSLM
toolkit fully supports parallel processing based on
Intel?s MKL library.1 Figure 2 shows the time used
to train a large neural network on 1M examples. We
trained a 7-gram CSLM with a projection layer of
size 320, two hidden layers of size 1024 and 512 re-
spectively, and an output layer of dimension 16384
(short-list). We compared two hardware architec-
tures:
? a top-end PC with one Intel Core i7 3930K pro-
cessor (3.2 GHz, 6 cores).
? a typical server with two Intel Xeon X5675 pro-
cessors (2? 3.06 GHz, 6 cores each).
We did not expect a linear increase of the speed
with the number of threads run in parallel, but nev-
ertheless, there is a clear benefit of using multiple
cores: processing is about 6 times faster when run-
ning on 12 cores instead of a single one. The Core i7
3930K processor is actually slightly faster than the
Xeon X5675, but we are limited to 6 cores since it
can not interact with a second processor.
2.2 Running on a GPU
In parallel to the development efforts for fast general
purpose CPUs, dedicated hardware has been devel-
oped in order to satisfy the computational needs of
realistic 3D graphics in high resolutions, so called
graphical processing units (GPU). Recently, it was
realized that this computational power can be in
fact used for scientific computing, e.g. in chem-
istry, molecular physics, earth quake simulations,
weather forecasts, etc. A key factor was the avail-
ability of libraries and toolkits to simplify the pro-
gramming of GPU cards, for instance the CUDA
toolkit of Nvidia.2 The machine learning commu-
nity has started to use GPU computing and several
toolkits are available to train generic networks. We
have also added support for Nvidia GPU cards to the
1http://software.intel.com/en-us/articles/intel-mkl
2http://developer.nvidia.com/cuda-downloads
 100
 200
 400
 800
 1600
 3200
 0  2  4  6  8  10  12
tim
e in
 se
c
number of CPU cores
Intel Xeon X5675Intel Core7 3690KNvidia Tesla M2090Nvidia GTX 580
Figure 2: Time to train on 1M examples on various hard-
ware architectures (the speed is shown in log scale).
CSLM toolkit. Timing experiments were performed
with two types of GPU cards:
? a Nvidia GTX 580 GPU card with 3 GB of
memory. It has 512 cores running at 1.54 GHz.
? a Nvidia Tesla M2090 card with 6 GB of mem-
ory. It has 512 cores running at 1.3 GHz.
As can be seen from figure 2, for these network
sizes the GTX 580 is about 3 times faster than two
Intel X5675 processors (12 cores). This speed-up
is smaller than the ones observed in other studies to
run machine learning tasks on a GPU, probably be-
cause of the large number of parameters which re-
quire many accesses to the GPU memory. For syn-
thetic benchmarks, all the code and data often fits
into the fast shared memory of the GPU card. We
are continuing our work to improve the speed of our
toolkit on GPU cards. The Tesla M2090 is a little bit
slower than the GTX 580 due to the lower core fre-
quency. However, it has a much better support for
double precision floating point calculations which
could be quite useful when training large neural net-
works.
3 Experimental Results
In this work, we present comparative results for var-
ious LMs when integrated into a large-scale SMT
system to translate from Arabic into English. We use
the popular Moses toolkit to build the SMT system
(Koehn et al, 2007). As in our previous works, the
CSLM is used to rescore 1000-best lists. The sen-
tence probability calculated by the CSLM is added
14
AFP APW NYT XIN LTW WPB CNA
old avrg recent old avrg recent old avrg recent old avrg recent all all all
Using all the data:
Words 151M 547M 371M 385M 547M 444M 786M 543M 364M 105M 147M 144M 313M 20M 39M
Perplex 167.7 141.0 138.6 192.7 170.3 163.4 234.1 203.5 197.1 162.9 126.4 121.8 170.3 269.3 266.5
After data selection:
Words
36M 77M 96M 62M 77M 89M 110M 54M 44M 23M 35M 38M 69M 6M 7M
23% 26% 26% 16% 14% 20% 14% 10% 12% 22% 24% 26% 22% 30% 18%
Perplex 160.9 135.0 131.6 185.3 153.2 151.1 201.2 173.6 169.5 159.6 123.4 117.7 153.1 263.9 253.2
Table 1: Perplexities on the development data (news wire genre) of the individual sub-corpora in the LDC Gigaword
corpus, before and after data selection by the method of (Moore and Lewis, 2010).
as 15th feature function and the coefficients of all
the feature functions are optimized by MERT. The
CSLM toolkit includes scripts to perform this task.
3.1 Baseline systems
The Arabic/English SMT system was trained on par-
allel and monolingual data similar to those avail-
able in the well known NIST OpenMT evaluations.
About 151M words of bitexts are available from
LDC out of which we selected 41M words to build
the translation model. The English side of all the
bitexts was used to train the target language model.
In addition, we used the LDC Gigaword corpus
version 5 (LDC2011T07). It contains about 4.9 bil-
lion words coming from various news sources (AFP
and XIN news agencies, New York Times, etc) for
the period 1994 until 2010. All corpus sizes are
given after tokenization.
For development and tuning, we used the
OpenMT 2009 data set which contains 1313 sen-
tences. The corresponding data from 2008 was used
as internal test set. We report separate results for the
news wire part (586 sentence, 24k words) and the
web part (727 sentences, 24k words) since we want
to compare the performance of the various LMs for
formal and more informal language. Four reference
translations were available. Case and punctuation
were preserved for scoring.
It is well known that it is better to build LMs on
the individual sources and to interpolate them, in-
stead of building one LM on all the concatenated
data. The interpolation coefficients are tuned by op-
timizing the perplexity on the development corpus
using an EM procedure. We split the huge Giga-
word corpora AFP, APW, NYT and XIN into three
parts according to the time period (old, average and
recent). This gives in total 15 sub-corpora. The sizes
and the perplexities are given in Table 1. The inter-
polated 4-gram LM of these 15 corpora has a per-
plexity of 87 on the news part.
If we add the English side of all the bitexts, the
perplexity can be lowered to 71.1. All the observed
n-grams were preserved, e.g. the cut-off for n-gram
counts was set to 1 for all orders. This gives us an
huge LM with 1.4 billion 4-grams, 548M 3-grams
and 83M bigrams which requires more 26 GBytes
to be stored on disk. This LM is loaded into mem-
ory by the Moses decoder. This takes more than 10
minutes and requires about 70 GB of memory.
Moses supports memory mapped LMs, like
IRSTLM or KENLM, but this was not explored in
this study. We call this LM ?big LM?. We believe
that it could be considered as a very strong base-
line for a back-off LM. We did not attempt to build
higher order back-off LM given the size require-
ments. For comparison, we also build a small LM
which was trained on the English part of the bitexts
and the recent XIN corpus only. It has a perplexity
of 78.9 and occupies 2 GB on disk (see table 2).
3.2 Data selection
We have reimplemented the method of Moore and
Lewis (2010) to select the most appropriate LM data
based on the difference between the sentence-wise
entropy of an in-domain and out-of domain LM.
In our experiments, we have observed exactly the
same behavior than reported by the authors: the per-
plexity decreases when less, but more appropriate
15
 170
 180
 190
 200
 210
 220
 230
 240
 0  10  20  30  40  50  60  70  80  90  100
Per
ple
xity
Percentage of corpus 
AFPAPWNYTXIN
Figure 3: Decrease in perplexity when selecting data with
the method proposed in (Moore and Lewis, 2010).
data is used, reaching a minimum using about 20%
of the data only. The improvement in the perplexity
can reach 20% relative. Figure3 shows the perplex-
ity for some corpora in function of the size of the
selected data. Detailed statistics for all corpora are
given in Table 1 for the news genre.
Unfortunately, these improvements in perplexity
almost vanish when we interpolate the individual
language models: the perplexity is 86.6 instead of
87.0 when all the data from the Gigaword corpus is
used. This LM achieves the same BLEU score on
the development data, and there is a small improve-
ment of 0.24 BLEU on the test set (Table 2). Never-
theless, the last LM has the advantage of being much
smaller: 7.2 instead of 25 GBytes. We have also per-
formed the data selection on the concatenated texts
of 4.9 billion words. In this case, we do observe an
decrease of the perplexity with respect to a model
trained on all the concatenated data, but overall, the
perplexity is higher than with an interpolated LM (as
expected).
Px BLEU
LM Dev Size Dev Test
Small 78.9 2.0 GB 56.89 49.66
Big 71.1 26 GB 58.66 50.75
Giga 87.0 25.0 GB 57.08 50.08
GigaSel 86.6 7.2 GB 57.03 50.32
Table 2: Comparison of several 4-gram back-off lan-
guage models. See text for explanation of the models.
3.3 Continuous space language models
The CSLM was trained on all the available
data using the resampling algorithm described in
(Schwenk, 2007). At each epoch we randomly re-
sampled about 15M examples. We build only one
CSLM resampling simultaneously in all the corpora.
The short list was set to 16k ? this covers about 92%
of the n-gram requests. Since it is very easy to use
large context windows with an CSLM, we trained
right away 7-grams. We provide a comparison of
different context lengths later in this section. The
networks were trained for 20 epochs. This can be
done in about 64 hours on a server with two Intel
X5675 processors and in 20 hours on a GPU card.
This CSLM achieves a perplexity of 62.9, to be
compared to 71.1 for the big back-off LM. This is a
relative improvement of more than 11%, but actually
we can do better. If we train the CSLM on the small
corpus only, i.e. the English side of the bitexts and
the recent part of the XIN corpus, we achieve a per-
plexity of 61.9 (see table 3). This clearly indicates
that it is better to focus the CSLM on relevant data.
Random resampling is a possibility to train a neu-
ral network on very large corpora, but it does not
guarantee that all the examples are used. Even if
we resampled different examples at each epoch, we
would process at most 300M different examples (20
epochs times 15M examples). There is no reason to
believe that we randomly select examples which are
appropriate to the task (note, however, that the re-
sampling coefficients are different for the individual
LM Corpus Sent. Perplex
size select. on Dev
Back-off 4-gram LM:
Small 196M no 78.9
Big 5057M no 71.1
CSLM 7-gram:
big 5057M no 62.9
Small 196M no 61.9
Small 92M yes 60.9
6x Giga 425M yes. 57.9
10x Giga 553M yes. 56.9
Table 3: Perplexity on the development data (news genre)
for back-off and continuous space language models.
16
Small LM Huge LM CSLM
Genre 4-gram back-off 7-gram
News 49.66 50.75 52.28
Web / 35.17 36.53
Table 4: BLEU scores on the test set for the translation
from Arabic into English for various language models.
corpora similar to the coefficients of an interpolated
back-off LM). Therefore, we propose to use the data
selection method of Moore and Lewis (2010) to con-
centrate the training of the CSLM on the most in-
formative examples. Instead of sampling randomly
n-grams in all the corpora, we do this in the selected
data by the method of (Moore and Lewis, 2010). By
these means, it is more likely that we train the CSLM
on relevant data. Note that this has no impact on the
training speed since the amount of resampled data is
not changed.
The results for this method are summarized in Ta-
ble 3. In a first experiment, we used the selected part
of the recent XIN corpus only. This reduces the per-
plexity to 60.9. In addition, if we use the six or ten
most important Gigaword corpora, we achieve a per-
plexity of 57.9 and 56.9 respectively. This is 10%
better than resampling blindly in all the data (62.9
? 56.9). Overall, the 7-gram CSLM improves the
perplexity by 20% relative with respect to the huge
4-gram back-off LM (71.1? 56.9).
Finally, we used our best CSLM to rescore the
n-best lists of the Arabic/English SMT system. The
baseline BLEU score on the test set, news genre, is
49.66 with the small LM. This increases to 50.75
with the big LM. It was actually necessary to open
the beam of the Moses decoder in order to observe
such an improvement. The large beam had no effect
when the small LM was used. This is a very strong
baseline to improve upon. Nevertheless, this result
is further improved by the CSLM to 52.28, i.e. a
significant gain of 1.8 BLEU. We observe similar
behavior for the WEB genre.
All our networks have two hidden layers since
we have observed that this slightly improves perfor-
mance with respect to the standard architecture with
only one hidden layer. This is a first step towards
so-called deep neural networks (Bengio, 2007), but
we have not yet explored this systematically.
Order: 4-gram 5-gram 6-gram 7-gram
Px Dev: 63.9 59.5 57.6 56.9
BLEU Dev: 59.76 60.11 60.29 60.26
BLEU Test: 51.91 51.85 52.23 52.28
Table 5: Perplexity on the development data (news genre)
and BLEU scores of the continuous space language mod-
els in function of the context size.
In an 1000-best list for 586 sentences, we have a
total of 14M requests for 7-grams out of which more
than 13.5M were processed by the CSLM, e.g. the
short list hit rate is almost 95%. This resulted in only
2670 forward passes through the network. At each
pass, we collected in average 5350 probabilities at
the output layer. The processing takes only a couple
of minutes on a server with two Xeon X5675 CPUs.
One can of course argue that it is not correct
to compare 4-gram and 7-gram language models.
However, building 5-gram or higher order back-off
LMs on 5 billion words is computationally very ex-
pensive, in particular with respect to memory usage.
For comparison, we also trained lower order CSLM
models. It can be clearly seen from Table 5 that the
CSLM can take advantage of longer contexts, but
it already achieves a significant improvement in the
BLEU score at the same LM order (BLEU on the
test data: 50.75? 51.91).
The CSLM is very space efficient: a saved net-
work occupies about 600M on disk in function of
the network architecture, in particular in function of
the size of the continuous projection. Loading takes
only a couple of seconds. During training, 1 GByte
of main memory is sufficient. The memory require-
ment during n-best rescoring essentially depends on
the back-off LM that is eventually charged to deal
with out-off short-list words. Figure 4 shows some
example translations.
4 Conclusion
This paper presented a comparison of several pop-
ular techniques to build language models for sta-
tistical machine translation systems: huge back-off
models trained on billions of words, data selection
of most relevant examples and a highly efficient im-
plementation of continuous space methods.
Huge LMs perform well, but their storage may
require important computational resources ? in our
17
 ?????  ????  ???  ??  ????  ???  ????  ????  ?????? ?????  ????  ?????  ????  ????  ??  ??
 ????  ?????  ????  ???  ?????  ?????  ?  ?????  ??????  ???????  ???  ???  ????  ??????
.?????  ??????????  ?????
Back-off LM:The minister inspected the sub-committee integrated combat marine 
pollution with oil, which includes the latest equipment lose face marine pollution and 
chemical plant in the port specializing in monitoring the quality of the crude oil supplier 
and with the most modern technological devices.
CSLM: The minister inspected the integrated sub-committee to combat marine pollution 
with oil, which includes the latest equipment deal with marine pollution and inspect the 
chemical plant in the port specializing in monitoring the quality of the crude oil supplier, 
with the most modern technological devices.
Google: The minister also inspected the sub-center for integrated control of marine 
pollution with oil, which includes the latest equipment on the face of marine pollution and 
chemical plant loses port specialist in quality control of crude oil and supplied
????  ???????  ????  ????????  ??????  ??  ????????
Back-off LM:Pyongyang is to respect its commitments to end nuclear program.
CSLM: Pyongyang promised to respect its commitments to end the nuclear program.
Google: Pyongyang is to respect its obligations to end nuclear program.
. ????  ????  ??  ????  ???  ???  ?  ??????  ??????  ????  ? ??  ???
Back-off LM: The Taliban militants in kidnappings in the country over the past two years.
CSLM: Taliban militants have carried out kidnappings in the country repeatedly during 
the past two years.
Google:The Taliban kidnappings in the country frequently over the past two years.
Figure 4: Example translations when using the huge back-off and the continuous space LM. For comparison we also
provide the output of Google Translate.
case, 26 GB on disk and 70 GB of main memory for
a model trained on 5 billions words. The data selec-
tion method proposed in (Moore and Lewis, 2010)
is very effective at the corpus level, but the observed
gains almost vanish after interpolation. However,
the storage requirement can be divided by four.
The main contributions of this paper are sev-
eral improvements of the continuous space language
model. We have shown that data selection is very
useful to improve the resampling of training data
in large corpora. Our best model achieves a per-
plexity reduction of 20% relative with respect to
the best back-off LM we were able to build. This
gives an improvement of up to 1.8 BLEU points in a
very competitive Arabic/English statistical machine
translation system.
We have also presented a very efficient imple-
mentation of the CSLM. The tool can take advan-
tage of modern multi-core or multi-processor com-
puters. We also support graphical extension cards
like the Nvidia 3D graphic cards. By these means,
we are able to train a CSLM on 500M words in
about 20 hours. This tool is freely available.3 By
these means we hope to make large-scale continu-
ous space language modeling available to a larger
community.
3http://www-lium.univ-lemans.fr/ ?cslm
18
Acknowledgments
This work has been partially funded by the French
Government under the project COSMAT (ANR-09-
CORD-004) and the European Commission under
the project FP7 EuromatrixPlus.
References
Yoshua Bengio and Rejean Ducharme. 2001. A neu-
ral probabilistic language model. In NIPS, volume 13,
pages 932?938.
Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3(2):1137?1155.
Yoshua Bengio. 2007. learning deep architectures for
AI. Technical report, University of Montre?al.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In EMNLP, pages 858?867.
Stanley F. Chen and Joshua T. Goodman. 1999. An
empirical study of smoothing techniques for language
modeling. Computer Speech & Language, 13(4):359?
394.
Marcello Federico and Maura Cettolo. 2007. Efficient
handling of n-gram language models for statistical ma-
chine translation. In Second Workshop on SMT, pages
88?95.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Sixth Workshop on SMT,
pages 187?197.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
L. Lamel, J.-L. Gauvain, V.-B. Le, I. Oparin, , and
S. Meng. 2011. Improved models for mandarin
speech-to-text transcription. In ICASSP, pages 4660?
4663.
H.S. Le, A. Allauzen, G. Wisniewski, and F. Yvon. 2010.
Training continuous space language models: Some
practical issues. In EMNLP, pages 778?788.
H.S. Le, I. Oparin, A. Allauzen, J-L. Gauvain, and
F. Yvon. 2011a. Structured output layer neural net-
work language model. In ICASSP, pages 5524?5527.
H.S. Le, I. Oparin, A. Messaoudi, A. Allauzen, J-L. Gau-
vain, and F. Yvon. 2011b. Large vocabulary SOUL
neural network language models. In Interspeech.
X. Liu, M. J. F. Gales, and P. C. Woodland. 2011. Im-
proving LVCSR system combination using neural net-
work language model cross adaptation. In Interspeech,
pages 2857?2860.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Interspeech,
pages 1045?1048.
T. Mikolov, S. Kombrink, L. Burget, J.H. Cernocky, and
S. Khudanpur. 2011. Extensions of recurrent neural
network language model. In ICASSP, pages 5528?
5531.
Andriy Mnih and Geoffrey Hinton. 2008. A scalable
hierarchical distributed language model. In NIPS.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL,
pages 220?224.
Frederic Morin and Yoshua Bengio. 2005. Hierarchi-
cal probabilistic neural network language model. In
Proceedings of the Tenth International Workshop on
Artificial Intelligence and Statistics.
Junho Park, Xunying Liu, Mark J. F. Gales, and Phil C.
Woodland. 2010. Improved neural network based lan-
guage modelling and adaptation. In Interspeech, pages
1041?1044.
Holger Schwenk and Yannick Este`ve. 2008. Data selec-
tion and smoothing in an open-source system for the
2008 NIST machine translation evaluation. In Inter-
speech, pages 2727?2730.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In ICASSP, pages I: 765?
768.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 723?730.
Holger Schwenk. 2004. Efficient training of large neu-
ral networks for language modeling. In IJCNN, pages
3059?3062.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2010. Continuous space language
models for statistical machine translation. The Prague
Bulletin of Mathematical Linguistics, (93):137?146.
David Talbot and Miles Osborne. 2007. Smoothed
bloom filter language models: Tera-scale lms on the
cheap. In EMNLP, pages 468?476.
Puyang Xu, Asela Gunawardana, and Sanjeev Khudan-
pur. 2011. Efficient subsampling for training complex
language models. In EMNLP, pages 1128?1136.
19
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 369?373,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIUM?s SMT Machine Translation Systems for WMT 2012
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk and Lo??c Barrault
LIUM, University of Le Mans
72085 Le Mans cedex 9, FRANCE
FirstName.LastName@lium.univ-lemans.fr
Abstract
This paper describes the development of
French?English and English?French statisti-
cal machine translation systems for the 2012
WMT shared task evaluation. We developed
phrase-based systems based on the Moses de-
coder, trained on the provided data only. Ad-
ditionally, new features this year included im-
proved language and translation model adap-
tation using the cross-entropy score for the
corpus selection.
1 Introduction
This paper describes the statistical machine trans-
lation systems developed by the Computer Science
laboratory at the University of Le Mans (LIUM) for
the 2012 WMT shared task evaluation. We only
considered the translation between French and En-
glish (in both directions). The main differences with
respect to previous year?s system (Schwenk et al,
2011) are as follows: (i) use of more training data as
provided by the organizers and (ii) better selection
of the monolingual and parallel data according to
the domain, using the cross-entropy difference with
respect to in-domain and out-of-domain language
models (Moore and Lewis, 2010). We kept some
previous features: the improvement of the transla-
tion model adaptation by unsupervised training, a
parallel corpus retrieved by Information Retrieval
(IR) techniques and finally, the rescoring with a con-
tinuous space target language model for the trans-
lation into French. These different points are de-
scribed in the rest of the paper, together with a sum-
mary of the experimental results showing the impact
of each component.
2 Resources Used
The following sections describe how the resources
provided or allowed in the shared task were used to
train the translation and language models of the sys-
tem.
2.1 Bilingual data
The latest version of the News-Commentary (NC)
corpus and of the Europarl (Eparl) corpus (version
7) were used. We also took as training data a subset
of the French?English Gigaword (109) corpus. This
year we changed the filters applied to select this sub-
set (see Sect. 2.4). We also included in the training
data the test sets from previous shared tasks, that we
called the ntsXX corpus and which was composed
of newstest2008, newstest2009, newssyscomb2009.
2.2 Development data
Development was initially done on newstest2010,
and newstest2011 was used as internal test set (Sec-
tion 3.1). The development and internal test sets
were then (Section 4) switched (tuning was done
on newstest2011 and internal evaluation on new-
stest2010). The default Moses tokenization was
used. However, we added abbreviations for the
French tokenizer. All our models are case sensitive
and include punctuation. The BLEU scores reported
in this paper were calculated with the mteval-v13
tool and are case insensitive.
2.3 Use of Automatic Translations
Available human translated bitexts such as the Eu-
roparl or 109 corpus seem to be out-of domain for
this task. We used two types of automatically ex-
tracted resources to adapt our system to the domain.
369
First, we generated automatic translations of the
provided monolingual News corpus in French and
English, for years 2009, 2010 and 2011, and selected
the sentences with a normalised translation cost (re-
turned by the decoder) inferior to a threshold. The
resulting bitexts contain no new translations, since
all words of the translation output come from the
translation model, but they contain new combina-
tions (phrases) of known words, and reinforce the
probability of some phrase pairs (Schwenk, 2008).
Like last year, we directly used the word-to-word
alignments produced by the decoder at the output
instead of GIZA?s alignments. This speeds-up the
procedure and yields the same results in our experi-
ments. A detailed comparison is given in (Lambert
et al, 2011).
Second, as in last year?s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. We used the
AFP (Agence France Presse) and APW (Associated
Press Worldstream Service) news texts since there
are available in the French and English LDC Giga-
word corpora. The general architecture of our par-
allel sentence extraction system is described in de-
tail by Abdul-Rauf and Schwenk (2009). We first
translated 91M words from French into English us-
ing our first stage SMT system. These English sen-
tences were then used to search for translations in
the English AFP and APW texts of the Gigaword
corpus using information retrieval techniques. The
Lemur toolkit (Ogilvie and Callan, 2001) was used
for this purpose. Search was limited to a window of
?5 days of the date of the French news text. The re-
trieved candidate sentences were then filtered using
the Translation Error Rate (TER) with respect to the
automatic translations. In this study, sentences with
a TER below 75% were kept. Sentences containing
a large fraction of numbers were discarded. By these
means, about 27M words of additional bitexts were
obtained.
2.4 Domain-based Data selection
Before training the target language models, a text se-
lection has been made using the cross-entropy differ-
ence method (Moore and Lewis, 2010). This tech-
nique works by computing the difference between
two cross-entropy values.
We first score an out-of-domain corpus against
a language model trained on a set of in-domain
data and compute the cross-entropy for each sen-
tence. Then, we score the same out-of-domain cor-
pus against a language model trained on a random
sample of itself, with a size roughly equal to the in-
domain corpus. From this point, the difference be-
tween in-domain cross-entropy and out-of-domain
cross-entropy is computed for each sentence, and
these sentences are sorted regarding this score.
By estimating and minimizing on a development
set the perplexity of several percentages of the sorted
out-of-domain corpus, we can then estimate the the-
oretical best point of data size for this specific cor-
pus. According the original paper and given our re-
sults, this leads to better selection than the simple
perplexity sorting (Gao et al, 2002). This way, we
can be assured to discard the vast majority of noise
in the corpora and to select data well-related to the
task.
In this task, the French and English target lan-
guage models were trained on data selected from all
provided monolingual corpora. In addition, LDC?s
Gigaword collection was used for both languages.
Data corresponding to the development and test pe-
riods were removed from the Gigaword collections.
We had time to apply the domain-based data selec-
tion only for French. Thus all data were used for
English.
We used this method to filter the French?English
109 parallel corpus as well, based on the differ-
ence between in-domain cross-entropy and out-of-
domain cross-entropy calculated for each sentence
of the English side of the corpus. We kept 49 mil-
lion words (in the English side) to train our models,
called 109f .
3 Architecture of the SMT system
The goal of statistical machine translation (SMT) is
to produce a target sentence e from a source sentence
f . We have build phrase-based systems (Koehn et
al., 2003; Och and Ney, 2003), using the standard
log linear framework in order to introduce several
models explaining the translation process:
e? = argmax p(e|f)
= argmax
e
{exp(
?
i
?ihi(e, f))} (1)
370
The feature functions hi are the system models
and the ?i weights are typically optimized to maxi-
mize a scoring function on a development set (Och,
2003). The phrase-based system uses fourteen fea-
tures functions, namely phrase and lexical transla-
tion probabilities in both directions, seven features
for the lexicalized distortion model, a word and a
phrase penalty and a target language model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al, 2007) and is constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++ that
can appear with rare words.
Phrases and lexical reorderings are extracted us-
ing the default settings of the Moses toolkit. The
parameters of Moses were tuned using the MERT
tool. We repeated the training process three times,
each with a different seed value for the optimisation
algorithm. In this way we have a rough idea of the
error introduced by the tuning process.
4-gram back-off LMs were used. The word list
contains all the words of the bitext used to train the
translation model and all words that appear at least
ten times in the monolingual corpora. Words of the
monolingual corpora containing special characters
or sequences of uppercase characters were not in-
cluded in the word list. Separate LMs were build
on each data source with the SRI LM toolkit (Stol-
cke, 2002) and then linearly interpolated, optimizing
the coefficients with an EM procedure. The perplex-
ities of these LMs on newstest2011 were 119.1 for
French and 174.8 for English. In addition, we build a
5-gram continuous space language model for French
(Schwenk, 2007). These models were trained on
all the available texts using a resampling technique.
The continuous space language model is interpo-
lated with the 4-gram back-off model and used to
rescore n-best lists. This reduces the perplexity by
about 13% relative.
3.1 Number translation
We have also performed some experiments with
number translation. English and French do not use
1The source is available at http://www.cs.cmu.edu/
?qing/
the same conventions for integer and decimal num-
bers. For example, the English decimal number 0.99
is translated in French by 0,99. In the same way,
the English integer 32,000 is translated in French by
32 000. It should be possible to perform these mod-
ifications by rules.
In this study, we first replaced the numbers by a
tag @@NUM for integer and @@DEC for decimal num-
bers. Integers in the range 1 to 31 were not replaced
since they appear in dates. Then, we created the tar-
get language model using the tagged corpora. Ta-
ble 1 shows results of experiments performed with
and without rule-based number translation.
Corpus NT BLEU TER
NC no 26.57 (0.07) 58.13 (0.06)
NC yes 26.84 (0.15) 57.71 (0.34)
Eparl+NC no 29.28 (0.11) 55.28 (0.13)
Eparl+NC yes 29.26 (0.10) 55.44 (0.29)
Table 1: Results of the study on number translation (NT)
from English to French
We did observe small gains in the translation
quality when only the news-commentary bitexts are
used, but there were no differences when more train-
ing data is available. Due to time constraints, this
procedure was not used in the submitted system.
4 Results and Discussion
The results of our SMT systems are summarized in
Table 2. The MT metric scores for the development
set are the average of three optimisations performed
with different seeds (see Section 3). For the test set,
they are the average of four values: the three val-
ues corresponding to these different optimisations,
plus a fourth value obtained by taking as weight for
each model, the average of the weights obtained in
the three optimisations (Cettolo et al, 2011). The
numbers in parentheses are the standard deviation of
these three or four values. The standard deviation
gives a lower bound of the significance of the differ-
ence between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is not
significant. The reverse is not true.
The results of Table 2 show that adding several
adapted corpora (the filtered 109 corpus, the syn-
371
Bitext #Source newstest2011 newstest2010
Words (M) BLEU TER BLEU TER
Translation : En?Fr
Eparl+NC 57 30.91 (0.05) 53.61 (0.12) 28.45 (0.08) 56.29 (0.20)
Eparl+NC+ntsXX 58 31.12 (0.08) 53.67 (0.08) 28.49 (0.04) 56.45 (0.12)
Eparl+NC+ntsXX+109f 107 31.67 (0.06) 53.29 (0.03) 29.38 (0.12) 55.45 (0.15)
Eparl+NC+ntsXX+109f+IR 133 32.41 (0.02) 52.20 (0.02) 29.48 (0.11) 55.33 (0.20)
Eparl+NC+ntsXX+109f+news+IR 162 32.26 (0.04) 52.24 (0.12) 29.79 (0.12) 55.04 (0.20)
Translation : Fr?En
Eparl+NC 64 29.59 (0.12) 51.86 (0.06) 28.12 (0.05) 53.19 (0.06)
Eparl+NC+ntsXX 64 29.59 (0.04) 51.89 (0.14) 28.32 (0.08) 53.22 (0.08)
Eparl+NC+ntsXX+109f 120 30.69 (0.06) 50.77 (0.04) 28.95 (0.14) 52.62 (0.14)
Eparl+NC+ntsXX+109f+IR 149 30.56 (0.02) 50.94 (0.15) 28.67 (0.11) 52.78 (0.06)
Eparl+NC+ntsXX+109f+news+IR 179 30.85 (0.07) 50.72 (0.03) 28.94 (0.05) 52.57 (0.02)
Table 2: English?French and French?English results: number of source words (in million) and scores on the develop-
ment (newstest2011) and internal test (newstest2010) sets for the different systems developed. The BLEU scores and
the number in parentheses are the average and standard deviation over 3 or 4 values when available (see Section 4.)
thetic corpus and the corpus retrieved via IR meth-
ods) to the Eparl+NC+ntsXX baseline, a gain of 1.1
BLEU points and 1.4 TER points was achieved for
the English?French system.
On the other hand, adding the bitexts extracted
from the comparable corpus (IR) does actually hurt
the performance of the French?English system: the
BLEU score decreases from 28.95 to 28.67 on our
internal test set. During the evaluation period, we
added all the corpora at once and we observed this
only in our analysis after the evaluation.
In both translation directions our
best system was the one trained on
Eparl+NC+ntsXX+109f+News+IR. Finally, we
applied a continuous space language model for the
system translating into French.
Acknowledgments
This work has been partially funded by the Euro-
pean Union under the EuroMatrixPlus project ICT-
2007.2.2-FP7-231720 and the French government
under the ANR project COSMAT ANR-09-CORD-
004.
References
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages 16?
23, Athens, Greece.
Mauro Cettolo, Nicola Bertoldi, and Marcello Federico.
2011. Methods for smoothing the optimizer instability
in SMT. In Proc. of Machine Translation Summit XIII,
Xiamen, China.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57, Columbus, Ohio,
June. Association for Computational Linguistics.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to statistical
language modeling for chinese. In ACM Transactions
on Asian Language Information Processing.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127?133.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL,
demonstration session.
Patrik Lambert, Holger Schwenk, Christophe Servan, and
Sadaf Abdul-Rauf. 2011. Investigations on translation
372
model adaptation using monolingual data. In Sixth
Workshop on SMT, pages 284?293.
Robert C. Moore and William Lewis. 2010. Intelligent
selection of language model training data. In Proceed-
ings of the ACL 2010 Conference Short Papers.
Franz Josef Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignement models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the Annual
Meeting of the Association for Computational Linguis-
tics, pages 160?167.
Paul Ogilvie and Jamie Callan. 2001. Experiments using
the Lemur toolkit. In In Proceedings of the Tenth Text
Retrieval Conference (TREC-10), pages 103?108.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. Lium?s smt machine trans-
lation systems for WMT 2011. In Proceedings of
the Sixth Workshop on Statistical Machine Translation,
pages 464?469, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492?
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical machine
translation. In IWSLT, pages 182?189.
A. Stolcke. 2002. SRILM: an extensible language mod-
eling toolkit. In Proc. of the Int. Conf. on Spoken Lan-
guage Processing, pages 901?904, Denver, CO.
373
