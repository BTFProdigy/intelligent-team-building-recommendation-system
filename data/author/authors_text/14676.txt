Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1776?1786,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Harvesting Parallel News Streams to Generate Paraphrases of Event
Relations
Congle Zhang, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{clzhang,weld}@cs.washington.edu
Abstract
The distributional hypothesis, which states
that words that occur in similar contexts tend
to have similar meanings, has inspired sev-
eral Web mining algorithms for paraphras-
ing semantically equivalent phrases. Unfortu-
nately, these methods have several drawbacks,
such as confusing synonyms with antonyms
and causes with effects. This paper intro-
duces three Temporal Correspondence Heuris-
tics, that characterize regularities in parallel
news streams, and shows how they may be
used to generate high precision paraphrases
for event relations. We encode the heuristics
in a probabilistic graphical model to create
the NEWSSPIKE algorithm for mining news
streams. We present experiments demon-
strating that NEWSSPIKE significantly outper-
forms several competitive baselines. In order
to spur further research, we provide a large
annotated corpus of timestamped news arti-
cles as well as the paraphrases produced by
NEWSSPIKE.
1 Introduction
Paraphrasing, the task of finding sets of semantically
equivalent surface forms, is crucial to many natu-
ral language processing applications, including re-
lation extraction (Bhagat and Ravichandran, 2008),
question answering (Fader et al, 2013), summa-
rization (Barzilay et al, 1999) and machine transla-
tion (Callison-Burch et al, 2006). While the benefits
of paraphrasing have been demonstrated, creating a
large-scale corpus of high precision paraphrases re-
mains a challenge ? especially for event relations.
Many researchers have considered generating
paraphrases by mining the Web guided by the dis-
tributional hypothesis, which states that words oc-
curring in similar contexts tend to have similar
meanings (Harris, 1954). For example, DIRT (Lin
and Pantel, 2001) and Resolver (Yates and Etzioni,
2009) identify synonymous relation phrases by the
distributions of their arguments. However, the dis-
tributional hypothesis has several drawbacks. First,
it can confuse antonyms with synonyms because
antonymous phrases appear in similar contexts as of-
ten as synonymous phrases. For the same reasons, it
also often confuses causes with effects. For exam-
ple, DIRT reports that the closest phrase to fall is
rise, and the closest phrase to shoot is kill.1 Sec-
ond, the distributional hypothesis relies on statis-
tics over large corpora to produce accurate similarity
statistics. It remains unclear how to accurately para-
phrase less frequent relations with the distributional
hypothesis.
Another common approach employs the use of
parallel corpora. News articles are an interesting
target, because there often exist articles from dif-
ferent sources describing the same daily events.
This peculiar property allows the use of the tem-
poral assumption, which assumes that phrases in
articles published at the same time tend to have
similar meanings. For example, the approaches by
Dolan et al (2004) and Barzilay et al (2003) iden-
tify pairs of sentential paraphrases in similar arti-
cles that have appeared in the same period of time.
While these approaches use temporal information
as a coarse filter in the data generation stage, they
still largely rely on text metrics in the prediction
stage. This not only reduces precision, but also lim-
its the discovery of paraphrases with dissimilar sur-
1http://demo.patrickpantel.com/demos/
lexsem/paraphrase.htm
1776
face strings.
The goal of our research is to develop a technique
to generate paraphrases for large numbers of event
relation with high precision, using only minimal hu-
man effort. The key to our approach is a joint cluster
model using the temporal attributes of news streams,
which allows us to identify semantic equivalence
of event relation phrases with greater precision. In
summary, this paper makes the following contribu-
tions:
? We formulate a set of three temporal corre-
spondence heuristics that characterize regulari-
ties over parallel news streams.
? We develop a novel program, NEWSSPIKE,
based on a probabilistic graphical model that
jointly encodes these heuristics. We present in-
ference and learning algorithms for our model.
? We present a series of detailed experiments
demonstrating that NEWSSPIKE outperforms
several competitive baselines, and show through
ablation tests how each of the temporal heuris-
tics affects performance.
? To spur further research on this topic, we pro-
vide both our generated paraphrase clusters and
a corpus of 0.5M time-stamped news articles2,
collected over a period of about 50 days from
hundreds of news sources.
2 System Overview
The main goal of this work is to generate high preci-
sion paraphrases for relation phrases. News streams
are a promising resource, since articles from dif-
ferent sources tend to use semantically equivalent
phrases to describe the same daily events. For ex-
ample, when a recent scandal hit, headlines read:
?Armstrong steps down from Livestrong?; ?Arm-
strong resigns from Livestrong? and ?Armstrong
cuts ties with Livestrong?. From these we can con-
clude that the following relation phrases are seman-
tically similar: {step down from, resign from, cut ties
with}.
To realize this intuition, our first challenge is
to represent an event. In practice, a question like
?What happened to Armstrong and Livestrong on
Oct 17?? could often lead to a unique answer. It im-
2https://www.cs.washington.edu/node/
9473/
Given news streams 
OpenIE 
Joint inference 
model 
(a1,r,a2,t) 
Temporal Heuristics 
Temporal features 
& constraints 
Extracted Event 
candidates (EEC) & 
relation phrases 
r1 r2 r3   
(a1,a2,t) 
r1 r2 r3   r4   r5 r1 r2 r3   
(a1,a2,t) 
r1 r  r    r4   r  
Shallow timestamped 
extractions 
Group 
Relation phrases 
Describing the EEC 
r1 r2 r3   
(a1,a2,t) 
r1 r2 r3   r4   r5 r1 r2 r3   
(a1,a2,t) 
{r1, r3, r4} 
Paraphrase 
clusters 
Create  
clusters 
r1 r3 r4   
Figure 1: NEWSSPIKE first applies open informa-
tion extraction to articles in the news streams, obtain-
ing shallow extractions with time-stamps. Next, an
extracted event candidate (EEC) is obtained after group-
ing daily extractions by argument pairs. Temporal fea-
tures and constraints are developed based on our tempo-
ral correspondence heuristics and encoded into a joint in-
ference model. The model finally creates the paraphrase
clusters by predicting the relation phrases that describe
the EEC.
plies that using an argument pair and a time-stamp
could be an effective way to identify an event (e.g.
(Armstrong, Livestrong, Oct 17) for the previous
question). Based on this observation, this paper in-
troduces a novel mechanism to paraphrase relations
as summarized in Figure 1.
NEWSSPIKE first applies the ReVerb open infor-
mation extraction (IE) system (Fader et al, 2011)
on the news streams to obtain a set of (a1, r, a2, t)
tuples, where the ai are the arguments, r is a re-
lation phrase, and t is the time-stamp of the cor-
responding news article. When (a1, a2, t) suggests
a real word event, the relation r of (a1, r, a2, t) is
likely to describe that event (e.g. (Armstrong, resign
from, Livestrong, Oct 17). We call every (a1, a2, t)
an extracted event candidate (EEC), and every rela-
tion describing the event an event-mention.
For each EEC (a1, a2, t), suppose there are m ex-
traction tuples (a1, r1, a2, t) . . . (a1, rm, a2, t) shar-
ing the values of a1, a2, and t. We refer to this
set of extraction tuples as the EEC-set, and denote
it (a1, a2, t, {r1 . . . rm}). All the event-mentions in
the EEC-set may be semantically equivalent and are
hence candidates for a good paraphrase cluster.
Thus, the paraphrasing problem becomes a pre-
diction problem: for each relation ri in the EEC-set,
does it or does it not describe the hypothesized
event? We solve this problem in two steps. The
1777
next section proposes a set of temporal correspon-
dence heuristics that partially characterize semanti-
cally equivalent EEC-sets. Then, in Section 4, we
present a joint inference model designed to use these
heuristics to solve the prediction problem and to
generate paraphrase clusters.
3 Temporal Correspondence Heuristics
In this section, we propose a set of temporal heuris-
tics that are useful to generate paraphrases at high
precision. Our heuristics start from the basic obser-
vation mentioned previously ? events can often be
uniquely determined by their arguments and time.
Additionally, we find that it is not just the publica-
tion time of the news story that matters, the verb
tenses of the sentences are also important. For ex-
ample, the two sentences ?Armstrong was the chair-
man of Livestrong? and ?Armstrong steps down
from Livestrong? have past and present tense re-
spectively, which suggests that the relation phrases
are less likely to describe the same event and are
thus not semantically equivalent. To capture these
intuitions, we propose the Temporal Functionality
Heuristic:
Temporal Functionality Heuristic. News articles
published at the same time that mention the same
entities and use the same tense tend to describe the
same events.
Unfortunately, we find that not all the event can-
didates, (a1, a2, t), are equally good for paraphras-
ing. For example, today?s news might include
both ?Barack Obama heads to the White House?
and ?Barack Obama greets reporters at the White
House?. Although the two sentences are highly
similar, sharing a1 = ?Barack Obama? and a2 =
?White House,? and were published at the same
time, they describe different events.
From a probabilistic point of view, we can treat
each sentence as being generated by a particular hid-
den event which involves several actors. Clearly,
some of these actors, like Obama, participate in
many more events than others, and in such cases
we observe sentences generated from a mixture of
events. Since two event mentions from such a mix-
ture are much less likely to denote the same event
or relation, we wish to distinguish them from the
better (semantically homogeneous) EECs like the
(Armstrong, Livestrong) example. The question be-
comes ?How one can distinguish good entity pairs
from bad??
Our method rests on the simple observation that
an entity which participates in many different events
on one day is likely to have participated in events
in recent days. Therefore we can judge whether an
entity pair is good for paraphrasing by looking at
the history of the frequencies that the entity pair is
mentioned in the news streams, which is the time
series of that entity pair. The time series of the entity
pair (Barack Obama, the White House) tends to be
high over time, while the time series of the entity
pair (Armstrong, Livestrong) is flat for a long time
and suddenly spikes upwards on a single day. This
observation leads to:
Temporal Burstiness Heuristic. If an entity or an
entity pair appears significantly more frequently in
one day?s news than in recent history, the corre-
sponding event candidates are likely to be good to
generate paraphrase.
The temporal burstiness heuristic implies that a
good EEC (a1, a2, t) tends to have a spike in the time
series of its entities ai, or argument pair (a1, a2), on
day t.
However, even if we have selected a good EEC
for paraphrasing, it is likely that it contains a few
relation phrases that are related to (but not synony-
mous with) the other relations included in the EEC.
For example, it?s likely that the news story report-
ing ?Armstrong steps down from Livestrong.? might
also mention ?Armstrong is the founder of Live-
strong.? and so both ?steps down from? and ?is the
founder of? relation phrases would be part of the
same EEC-set. Inspired by the idea of one sense per
discourse from (Gale et al, 1992), we propose:
One Event-Mention Per Discourse Heuristic. A
news article tends not to state the same fact more
than once.
The one event-mention per discourse heuristic is
proposed in order to gain precision at the expense
of recall ? the heuristic directs an algorithm to
choose, from a news story, the single ?best? relation
phrase connecting a pair of two entities. Of course,
this doesn?t answer the question of deciding which
phrase is ?best.? In Section 4.3, we describe how
to learn a probabilistic graphical model which does
exactly this.
1778
4 Exploiting the Temporal Heuristics
In this section we propose several models to capture
the temporal correspondence heuristics, and discuss
their pros and cons.
4.1 Baseline Model
An easy way to use an EEC-set is to simply predict
that all ri in the EEC-set are event-mentions, and
hence are semantically equivalent. That is, given
EEC-set (a1, a2, t, {r1 . . . rm}), the output cluster is
{r1 . . . rm}.
This baseline model captures the most of the tem-
poral functionality heuristic, except for the tense re-
quirement. Our empirical study shows that it per-
forms surprisingly well. This demonstrates that the
quality of our input for the learning model is good:
the EEC-sets are promising resources for paraphras-
ing.
Unfortunately, the baseline model cannot deal
with the other heuristics, a problem we will remedy
in the following sections.
4.2 Pairwise Model
The temporal functionality heuristic suggests we ex-
ploit the tenses of the relations in an EEC-set; while
the temporal burstiness heuristic suggests we ex-
ploit the time series of its arguments. A pairwise
model can be designed to capture them: we compare
pairs of relations in the EEC-set, and predict whether
each pair is synonymous or non-synonymous. Para-
phrase clusters are then generated according to some
heuristic rules (e.g. assuming transitivity among
synonyms). The tenses of the relations and time se-
ries of the arguments are encoded as features, which
we call tense features and spike features respec-
tively. An example tense feature is whether one re-
lation is past tense while the other relation is present
tense; an example spike feature is the covariance of
the time series.
The pairwise model can be considered similar to
paraphrasing techniques which examine two sen-
tences and determine whether they are semantically
equivalent (Dolan and Brockett, 2005; Socher et al,
2011). Unfortunately, these techniques often based
purely on text metrics and does not consider any
temporal attributes. In section 5, we evaluate the
effect of applying these techniques.
1 
? (Armstrong,Livestrong,Oct.17) 
0 
1 
0 
? be founder of 
? step down 
? give speech at 
0 
1 
? be chairman of 
Article2 Article1 
? resign from 
?joint 
?Z 
?2Y 
?1Y 
Figure 2: an example model for EEC (Armstrong, Live-
strong, Oct 17). Y and Z are binary random variables.
?Y , ?Z and ?joint are factors. be founder of and step
down come from article 1 while give speech at, be chair-
man of and resign from come from article 2.
4.3 Joint Cluster Model
The pairwise model has several drawbacks: 1) it
lacks the ability to handle constraints, such as the
mutual exclusion constraint implied by the one-
mention per discourse heuristic; 2) ad-hoc rules,
rather than formal optimizations, are required to
generate clusters containing more than two relations.
A common approach to overcome the drawbacks
of the pairwise model and to combine heuristics to-
gether is to introduce a joint cluster model, in which
heuristics are encoded as features and constraints.
Data, instead of ad-hoc rules, determines the rel-
evance of different insights, which can be learned
as parameters. The advantage of the joint model
is analogous to that of cluster-based approaches for
coreference resolution (CR). In particular, a joint
model can better capture constraints on multiple
variables and can yield higher quality results than
pairwise CR models (Rahman and Ng, 2009).
We propose an undirected graphical model,
NEWSSPIKE, which jointly clusters relations. Con-
straints are captured by factors connecting multiple
random variables. We introduce random variables,
the factors, the objective function, the inference al-
gorithm, and the learning algorithm in the following
sections. Figure 2 shows an example model for EEC
(Armstrong, Livestrong, Oct 17).
4.3.1 Random Variables
For the EEC-set (a1, a2, t, {r1, . . . rm}), we intro-
duce one event variable and m relation variables, all
boolean valued. The event variable Z(a1,a2,t) indi-
1779
cates whether (a1, a2, t) is a good event for para-
phrasing. It is designed in accordance with the
temporal burstiness heuristic: for the EEC (Barack
Obama, the White House, Oct 17), Z should be as-
signed the value 0.
The relation variable Y r indicates whether rela-
tion r describes the EEC (a1, a2, t) or not (i.e. r is an
event-mention or not). The set of all event-mentions
with Y r = 1 define a paraphrase cluster, contain-
ing relation phrases. For example, the assignments
Y step down = Y resign from = 1 produce a paraphrase
cluster {step down, resign from}.
4.3.2 Factors and the Joint Distribution
In this section, we introduce a conditional proba-
bility model defining a joint distribution over all of
the event and relation variables. The joint distribu-
tion is a function over factors. Our model contains
event factors, relation factors and joint factors.
The event factor ?Z is a log-linear function with
spike features, used to distinguish good events. A re-
lation factor ?Y is also a log-linear function. It can
be defined for individual relation variables (e.g. ?Y1
in Figure 2) with features such as whether a relation
phrase comes from a clausal complement3. A rela-
tion factor can also be defined for a pair of relation
variables (e.g. ?Y2 in Figure 2) with features captur-
ing the pairwise evidence for paraphrasing, such as
if two relation phrases have the same tense.
The joint factors ?joint are defined to apply con-
straints implied by the temporal heuristics. They
play two roles in our model: 1) to satisfy the tempo-
ral burstiness heuristic, when the value of the event
variable is false, the EEC is not appropriate for para-
phrasing, and so all relation variables should also be
false; and 2) to satisfy the one-mention per discourse
heuristic, at most one relation variable from a single
article could be true.
We define the joint distribution over these vari-
ables and factors as follows. Let Y = (Y r1 . . . Y rm)
be the vector of relation variables; let x be the fea-
tures. The joint distribution is:
3Relation phrases in clausal complement are less useful for
paraphrasing because they often do not describe a fact. For ex-
ample, in the sentence He heard Romney had won the election,
the extraction (Romney, had won, the election) is not a fact at
all.
p(Z = z,Y = y|x; ?)
def
=
1
Zx
?Z(z,x)
?
?
d
?joint(z,yd,x)
?
i,j
?Y (yi, yj ,x)
where yd indicates the subset of relation variables
from a particular article d, and the parameter vector
? is the weight vector of the features in ?Z and ?Y ,
which are log-linear functions; i.e.,
?Y (yi, yj ,x)
def
= exp
?
?
?
j
?j?j(yi, yj ,x)
?
?
where ?j is the jth feature function.
The joint factors ?joint are used to apply the tem-
poral burstiness heuristic and the one event-mention
per discourse heuristic. ?joint is zero when the EEC
is not good for paraphrasing, but some yr = 1; or
when there is more than one r in a single article such
that yr = 1. Formally, it is calculated as:
?joint(z,yd,x)
def
=
?
??
??
0 if z = 0 ? ?yr = 1
0 if
?
yr?yd
yr > 1
1 otherwise
4.3.3 Maximum a Posteriori Inference
The goal of inference is to find the predictions z,y
which yield the greatest probability, i.e.,
z?,y? = arg max
z,y
p(Z = z,Y = y|x; ?)
This can be viewed as a MAP inference problem.
In general, inference in a graphical model is chal-
lenging. Fortunately, the joint factors in our model
are linear, and the event and relation factors are log-
linear; we can cast MAP inference as an integer lin-
ear programming (ILP) problem, and then compute
an approximation in polynomial time by means of
linear programming using randomized rounding, as
proposed in (Yannakakis, 1992).
We build one ILP problem for every EEC. The
variables of the ILP are Z and Y, which only take
values of 0 or 1. The objective function is the sum
of logs of the event and relation factors ?Z and
?Y . The temporal burstiness heuristic of ?joint is
encoded as a linear inequality constraint z ? yi; the
one-mention per discourse heuristic of ?joint is en-
coded as the constraint
?
yi?yd
yi ? 1.
1780
4.3.4 Learning
Our training data consists a set of N = 500 la-
beled EEC-sets each in the form of {(Ri, R
gold
i ) |
N
i=1
}. Each R is the set of all relations in the EEC-set
while Rgold is a manually selected subset of R con-
taining relations describing the EEC. Rgold could be
empty if the EEC was deemed poor for paraphras-
ing. For our model, the gold assignment yrgold = 1
if r ? Rgold; the gold assignment zgold = 1 if Rgold
is not empty.
Given {(Ri, R
gold
i ) |
N
i=1}, learning over similar
models is commonly done via maximum likelihood
estimation as follows:
L(?) = log
?
i
p(Zi = z
gold
i ,Yi = y
gold
i | xi,?)
For features in relation factors, the partial deriva-
tive for the ith model is:
?j(y
gold
i ,xi)? Ep(zi,yi|,xi,?)?j(yi,xi)
where ?j(yi,xi) =
?
?j(X,Y,x), the sum of val-
ues for the jth feature in the ith model; and values
of X,Y come from the assignment yi. For features
in event factors, the partial derivative is derived sim-
ilarly as
?j(z
gold
i ,xi)? Ep(zi,yi|,xi,?)?j(zi,xi)
It is unclear how to efficiently compute the expec-
tations in the above formula, a brute force approach
requires enumerating all assignments of yi, which
is exponentially large with the number of relations.
Instead, we opt to use a more tractable perceptron
learning approach (Collins, 2002; Hoffmann et al,
2011). Instead of computing the expectations, we
simply compute ?j(z?i ,xi) and ?j(y
?
i ,xi), where
z?i ,y
?
i is the assignment with the highest probabil-
ity, generated by the MAP inference algorithm us-
ing the current weight vector. The weight updates
are the following:
?j(y
gold
i ,xi)? ?j(y
?
i ,xi) (1)
?j(z
gold
i ,xi)? ?j(z
?
i ,xi) (2)
The updates can be intuitively explained as penal-
ties on errors. In sum, our learning algorithm con-
sists of iterating the following two steps: (1) in-
fer the most probable assignment given the current
weights; (2) update the weights by comparing in-
ferred assignments and the truth assignment.
5 Empirical Study
We first introduce the experimental setup for our em-
pirical study, and then we attempt to answer two
questions in sections 5.2 and 5.3 respectively: First,
does the NEWSSPIKE algorithm effectively exploit
the proposed heuristics and outperform other ap-
proaches which also use news streams? Secondly,
do the proposed temporal heuristics paraphrase re-
lations with greater precision than the distributional
hypothesis?
5.1 Experimental Setup
Since we were unable to find any elaborate time-
stamped, parallel, news corpus, we collected data
using the following procedure:
? Collect RSS news seeds, which contain the title,
time-stamp, and abstract of the news items.
? Use these titles to query the Bing news search
engine API and collect additional time-stamped
news articles.
? Strip HTML tags from the news articles using
Boilerpipe (Kohlschu?tter et al, 2010); keep only
the title and first paragraph of each article.
? Extract shallow relation tuples using the OpenIE
system (Fader et al, 2011).
We performed these steps every day from Jan-
uary 1 to February 22, 2013. In total, we collected
546,713 news articles, for which 2.6 million extrac-
tions had 529 thousand unique relations.
We used several types of features for paraphras-
ing: 1) spike features obtained from time series; 2)
tense features, such as whether two relation phrases
are both in the present tense; 3) cause-effect fea-
tures, such as whether two relation phrases often ap-
pear successively in the news articles; 4) text fea-
tures, such as whether sentences are similar; 5) syn-
tactic features, such as whether a relation phrase
appears in a clausal complement; and 6) semantic
features, such as whether a relation phrase contains
negative words.
Text and semantic features are encoded using the
relation factors of section 4.3.2. For example, in Fig-
ure 2, the factor ?Y2 includes the textual similarity
between the sentences containing the phrases ?step
down? and ?be chairman of? respectively; it also
includes the feature that the tense of ?step down?
(present) is different from the tense of ?be chairman
1781
output
{go into, go to, speak, return,
head to}
gold {go into, go to, approach, head to}
golddiv {go ?, approach, head to}
P/R precision = 3/5 recall = 3/4
P/Rdiv precisiondiv = 2/4 recalldiv = 2/3
Figure 3: an example pair of the output cluster and the
gold cluster, and the corresponding precision recall num-
bers.
of? (past).
5.2 Comparison with Methods using Parallel
News Corpora
We evaluated NEWSSPIKE against other methods
that also use time-stamped news. These include the
models mentioned in section 3 and state-of-the-art
paraphrasing techniques.
Human annotators created gold paraphrase clus-
ters for 500 EEC-sets; note that some EEC-sets
yield no gold cluster, since at least two synonymous
phrases. Two annotators were shown a set of candi-
date relation phrases in context and asked to select a
subset of these that described a shared event (if one
existed). There was 98% phrase-level agreement.
Precision and recall were computed by comparing
an algorithm?s output clusters to the gold cluster of
each EEC. We consider paraphrases with minor lex-
ical diversity, e.g. (go to, go into), to be of lesser in-
terest. Since counting these trivial paraphrases tends
to exaggerate the performance of a system, we also
report precision and recall on diverse clusters i.e.,
those whose relation phrases all have different head
verbs. Figure 3 illustrates these metrics with an ex-
ample; note under our diverse metrics, all phrases
matching go * count as one when computing both
precision and recall. We conduct 5-fold cross val-
idation on our labeled dataset to get precision and
recall numbers when the system requires training.
We compare NEWSSPIKE with the models in Sec-
tion 4, and also with the state-of-the-art paraphrase
extraction method:
Baseline: the model discussed in Section 4.1.
This system does not need any training, and gener-
ates outputs with perfect recall.
Pairwise: the pairwise model discussed in Sec-
tion 4.2 and using the same set of features as used
System
P/R P/R diverse
prec rec prec rec
Baseline 0.67 1.00 0.53 1.00
Pairwise 0.90 0.60 0.81 0.37
Socher 0.81 0.35 0.68 0.29
NEWSSPIKE 0.92 0.55 0.87 0.31
Table 1: Comparison with methods using parallel news
corpora
by NEWSSPIKE. To generate output clusters, transi-
tivity is assumed inside the EEC-set. For example,
when the pairwise model predicts that (r1, r2) and
(r1, r3) are both paraphrases, the resulting cluster is
{r1, r2, r3}.
Socher: Socher et al (2011) achieved the best re-
sults on the Dolan et al (2004) dataset, and released
their code and models. We used their off-the-shelf
predictor to replace the classifier in our Pairwise
model. Given sentential paraphrases, aligning rela-
tion phrases is natural, because OpenIE has already
identified the relation phrases.
Table 1 shows precision and recall numbers. It
is interesting that the basic model already obtains
0.67 precision overall and 0.53 in the diverse con-
dition. This demonstrates that the EEC-sets gen-
erated from the news streams are a promising re-
source for paraphrasing. Socher?s method performs
better, but not as well as Pairwise or NEWSSPIKE,
especially in the diverse cases. This is probably
due to the fact that Socher?s method is based purely
on text metrics and does not consider any tempo-
ral attributes. Taking into account the features used
by NEWSSPIKE, Pairwise significantly improves the
precision, which demonstrates the power of our tem-
poral correspondence heuristics. Our joint cluster
model, NEWSSPIKE, which considers both temporal
features and constraints, gets the best performance
in both conditions.
We conducted ablation testing to evaluate how
spike features and tense features, which are par-
ticularly relevant to the temporal aspects of news
streams, can improve performance. Figure 4 com-
pares the precision/recall curves for three systems
in the diverse condition: (1) NEWSSPIKE; (2)
w/oSpike: turning off all spike features; and (3)
w/oTense: turning off all features about tense.
(4) w/oDiscourse: turning off one event-mention
per discourse heuristic. There are some dips in
1782
0.1 0.2 0.3 0.4
0.6
0.7
0.8
0.9
1.0
w/oSpike
w/oTense
NewsSpike
Recall
Precision
w/oDiscourse
Figure 4: Precision recall curves on hard, diverse cases
for NewsSpike, w/oSpike, w/oTense and w/oDiscourse.
the curves because they are drawn after sorting
the predictions by the value of the corresponding
ILP objective functions, which do not perfectly re-
flect prediction accuracy. However, it is clear that
NEWSSPIKE produces greater precision over all
ranges of recall.
5.3 Comparison with Methods using the
Distributional Hypothesis
We evaluated our model against methods based on
the distributional hypothesis. We ran NEWSSPIKE
over all EEC-sets except for the development set and
compared to the following systems:
Resolver: Resolver (Yates and Etzioni, 2009)
uses a set of extraction tuples in the form of
(a1, r, a2) as the input and creates a set of relation
clusters as the output paraphrases. Resolver also
produces argument clusters, but this paper only eval-
uates relation clustering. We evaluated Resolver?s
performance with an input of the 2.6 million extrac-
tions described in section 5.1, using Resolver?s de-
fault parameters.
ResolverNYT: Since Resolver is supposed to
perform better when given more accurate statis-
tics from a larger corpus, we tried giving it more
data. Specifically, we ran ReVerb on 1.8 million NY
Times articles published between 1987 and 2007 ob-
tain 60 million extractions (Sandhaus, 2008). We ran
Resolver on the union of this and our standard test
set, but report performance only on clusters whose
relations were seen in our news stream.
System
all diverse
prec #rels prec #rels
Resolver 0.78 129 0.65 57
ResolverNyt 0.64 1461 0.52 841
ResolverNytTop 0.83 207 0.72 79
Cosine 0.65 17 0.33 9
CosineNyt 0.56 73 0.46 59
NEWSSPIKE 0.93 24843 0.87 5574
Table 2: Comparison with methods using the distribu-
tional hypothesis
ResolverNytTop: Resolver is designed to
achieve good performance on its top results. We thus
ranked the ResolverNYT outputs by their scores and
report the precision of the top 100 clusters.
Cosine: Cosine similarity is a basic metric for
the distributional hypothesis. This system employs
the same setup as Resolver in order to generate
paraphrase clusters, except that Resolver?s similar-
ity metric is replaced with the cosine. Each relation
is represented by a vector of argument pairs. The
similarity threshold to merge two clusters was 0.5.
CosineNYT: As for ResolverNYT, we ran Cosi-
neNYT with an extra 60 million extractions and re-
ported the performance on relations seen in our news
stream.
We measured the precision of each system by
manually labeling all output if 100 or fewer clus-
ters were generated (e.g. ResolverNytTop), other-
wise 100 randomly chosen clusters were sampled.
Annotators first determined the meaning of every
output cluster and then created a gold cluster by
choosing the correct relations. The gold cluster
could be empty if the output cluster was nonsensi-
cal. Unlike many papers that simply report recall on
the most frequent relations, we evaluated the total
number of returned relations in the output clusters.
As in Section 5.2, we also report numbers for the
case of lexically diverse relation phrases.
As can be seen in Table 2, NEWSSPIKE outper-
formed methods based on the distributional hypoth-
esis. The performance of the Cosine and Cosi-
neNyt was very low, suggesting that simple simi-
larity metrics are insufficient for handling the para-
phrasing problem, even when large-scale input is in-
volved. Resolver and ResolverNyt employ an ad-
vanced similarity measurement and achieve better
results. However, it is surprising that Resolver re-
sults in a greater precision than ResolverNyt. It
1783
is possible that argument pairs from news streams
spanning 20 years sometimes provide incorrect ev-
idence for paraphrasing. For example, there were
extractions like (the Rangers, be third in, the NHL)
and (the Rangers, be fourth in, the NHL) from news
in 2007 and 2003 respectively. Using these phrases,
ResolverNyt produced the incorrect cluster {be third
in, be fourth in}. NEWSSPIKE achieves greater pre-
cision than even the best results from ResolverNyt-
Top, because NEWSSPIKE successfully captures the
temporal heuristics, and does not confuse synonyms
with antonyms, or causes with effects. NEWSSPIKE
also returned on order of magnitude more relations
than other methods.
5.4 Discussion
Unlike some domain-specific clustering methods,
we tested on all relation phrases extracted by Ope-
nIE on the collected news streams. There are no
restrictions on the types of relations. Output para-
phrases cover a broad range, including politics,
sports, entertainment, health, science, etc. There
are 10 thousand nonempty clusters over 17 thousand
distinct phrases with average size 2.4. Unlike meth-
ods based on distributional similarity, NewsSpike
correctly clusters infrequently appearing phrases.
Since we focus on high precision, it is not sur-
prising that most clusters are of size 2 and 3. These
high precision clusters can contribute a lot to gen-
erate larger paraphrase clusters. For example, one
can invent the technique to merge smaller clusters
together. The work presented here provides a foun-
dation for future work to more closely examine these
challenges.
While this paper gives promising results, there
are still behaviors found in news streams that prove
challenging. Many errors are due to the discourse
context: the two sentences are synonymous in the
given EEC-set, but the relation phrases are not
paraphrases in general. For example, consider the
following two sentences: ?DA14 narrowly misses
Earth? and ?DA14 flies so close to Earth?. Statis-
tics information from large corpus would be helpful
to handle such challenges. Note in this paper, in or-
der to fairly compare with the distributional hypoth-
esis, we purposely forced NEWSSPIKE not to rely
on any distributional similarity. But NEWSSPIKE?s
graphical model has the flexibility to incorporate any
similarity metrics as features. Such a hybrid model
has great potential to increase both precision and re-
call, which is one goal for future work.
6 Related Work
The vast majority of paraphrasing work falls into
two categories: approaches based on the distribu-
tional hypothesis or those exploiting on correspon-
dences between parallel corpora (Androutsopoulos
and Malakasiotis, 2010; Madnani and Dorr, 2010).
Using Distribution Similarity: Lin and Pan-
tel?s (2001) DIRT employ mutual information statis-
tics to compute the similarity between relations rep-
resented in dependency paths. Resolver (Yates and
Etzioni, 2009) introduces a new similarity metric
called the Extracted Shared Property (ESP) and uses
a probabilistic model to merge ESP with surface
string similarity.
Identifying the semantic equivalence of relation
phrases is also called relation discovery or unsu-
pervised semantic parsing. Often techniques don?t
compute the similarity explicitly but rely implic-
itly on the distributional hypothesis. Poon and
Domingos? (2009) USP clusters relations repre-
sented with fragments of dependency trees by re-
peatedly merging relations having similar context.
Yao et al (2011; 2012) introduces generative mod-
els for relation discovery using LDA-style algorithm
over a relation-feature matrix. Chen et al (2011) fo-
cuses on domain-dependent relation discovery, ex-
tending a generative model with meta-constraints
from lexical, syntactic and discourse regularities.
Our work solves a major problem with these ap-
proaches, avoiding errors such as confusing syn-
onyms with antonyms and causes with effects. Fur-
thermore, NEWSSPIKE doesn?t require massive sta-
tistical evidence as do most approaches based on the
distributional hypothesis.
Using Parallel Corpora: Comparable and par-
allel corpora, including news streams and multiple
translations of the same story, have been used to
generate paraphrases, both sentential (Barzilay and
Lee, 2003; Dolan et al, 2004; Shinyama and Sekine,
2003) and phrasal (Barzilay and McKeown, 2001;
Shen et al, 2006; Pang et al, 2003). Typical meth-
ods first gather relevant articles and then pair sen-
tences that are potential paraphrases. Given a train-
ing set of paraphrases, models are learned and ap-
plied to unlabeled pairs (Dolan and Brockett, 2005;
1784
Socher et al, 2011). Phrasal paraphrases are often
obtained by running an alignment algorithm over the
paraphrased sentence pairs.
While prior work uses the temporal aspects of
news streams as a coarse filter, it largely relies on
text metrics, such as context similarity and edit dis-
tance, to make predictions and alignments. These
metrics are usually insufficient to produce high pre-
cision results; moreover they tend to produce para-
phrases that are simple lexical variants (e.g. {go to,
go into}.). In contrast, NEWSSPIKE generates para-
phrase clusters with both high precision and high di-
versity.
Others: Textual entailment (Dagan et al, 2009),
which finds a phrase implying another phrase,
is closely related to the paraphrasing task. Be-
rant et al (2011) notes the flaws in distributional
similarity and proposes local entailment classi-
fiers, which are able to combine many features.
Lin et al (2012) also uses temporal information to
detect the semantics of entities. In a manner similar
to our approach, Recasens et al (2013) mines paral-
lel news stories to find opaque coreferent mentions.
7 Conclusion
Paraphrasing event relations is crucial to many natu-
ral language processing applications, including re-
lation extraction, question answering, summariza-
tion, and machine translation. Unfortunately, previ-
ous approaches based on distribution similarity and
parallel corpora, often produce low precision clus-
ters. This paper introduces three Temporal Corre-
spondence Heuristics that characterize semantically
equivalent phrases in news streams. We present a
novel algorithm, NEWSSPIKE, based on a proba-
bilistic graphical model encoding these heuristics,
which harvests high-quality paraphrases of event re-
lations.
Experiments show NEWSSPIKE?s improvement
relative to several other methods, especially at pro-
ducing lexically diverse clusters. Ablation tests
confirm that our temporal features are crucial to
NEWSSPIKE?s precision. In order to spur future
research, we are releasing an annotated corpus of
time-stamped news articles and our harvested rela-
tion clusters.
Acknowledgments
We thank Oren Etzioni, Anthony Fader, Raphael
Hoffmann, Ben Taskar, Luke Zettlemoyer, and the
anonymous reviewers for providing valuable ad-
vice. We also thank Shengliang Xu for annotat-
ing the datasets. We gratefully acknowledge the
support of the Defense Advanced Research Projects
Agency (DARPA) Machine Reading Program under
Air Force Research Laboratory (AFRL) prime con-
tract no. FA8750-09-C-0181, ONR grant N00014-
12-1-0211, a gift from Google, and the WRF / TJ
Cable Professorship. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the author(s) and do not neces-
sarily reflect the view of DARPA, AFRL, or the US
government.
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. In Journal of Artificial Intelligence Re-
search, pages 135?187.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In HLT-NAACL, pages 16?23.
Association for Computational Linguistics.
Regina Barzilay and Kathleen R McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In ACL,
pages 50?57. Association for Computational Linguis-
tics.
Regina Barzilay, Kathleen R McKeown, and Michael El-
hadad. 1999. Information fusion in the context of
multi-document summarization. In ACL, pages 550?
557. Association for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
ACL-HLT, pages 610?619. Association for Computa-
tional Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In ACL, volume 8, pages 674?682. Associa-
tion for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In NAACL, pages 17?24. Associa-
tion for Computational Linguistics.
Harr Chen, Edward Benson, Tahira Naseem, and Regina
Barzilay. 2011. In-domain relation discovery with
meta-constraints via posterior regularization. In ACL-
HLT, pages 530?540. Association for Computational
Linguistics.
1785
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In ACL, pages 1?8. Asso-
ciation for Computational Linguistics.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(04):i?xvii.
William B Dolan and Chris Brockett. 2005. Automat-
ically constructing a corpus of sentential paraphrases.
In Proceedings of IWP.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Computational Linguistics, page 350. Association for
Computational Linguistics.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP. Association for Computational
Linguistics, July 27-31.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-driven learning for open question
answering. In ACL. Association for Computational
Linguistics.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One sense per discourse. In Pro-
ceedings of the workshop on Speech and Natural Lan-
guage, pages 233?237. Association for Computational
Linguistics.
Zellig S Harris. 1954. Distributional structure. Word.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction of
overlapping relations. In ACL-HLT, pages 541?550.
Christian Kohlschu?tter, Peter Fankhauser, and Wolfgang
Nejdl. 2010. Boilerplate detection using shallow text
features. In WSDM, pages 441?450. ACM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question-answering. Natural Language
Engineering, 7(4):343?360.
Thomas Lin, Oren Etzioni, et al 2012. No noun phrase
left behind: detecting and typing unlinkable entities.
In EMNLP, pages 893?903. Association for Computa-
tional Linguistics.
Nitin Madnani and Bonnie J Dorr. 2010. Gener-
ating phrasal and sentential paraphrases: A survey
of data-driven methods. Computational Linguistics,
36(3):341?387.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
NAACL, pages 102?109. Association for Computa-
tional Linguistics.
Hoifung Poon and Pedro Domingos. 2009. Unsuper-
vised semantic parsing. In EMNLP, pages 1?10. As-
sociation for Computational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In EMNLP, pages 968?
977. Association for Computational Linguistics.
Marta Recasens, Matthew Can, and Dan Jurafsky. 2013.
Same referent, different words: Unsupervised min-
ing of opaque coreferent mentions. In Proceedings of
NAACL-HLT, pages 897?906.
Evan Sandhaus. 2008. The New York Times annotated
corpus. Linguistic Data Consortium.
Siwei Shen, Dragomir R Radev, Agam Patel, and Gu?nes?
Erkan. 2006. Adding syntax to dynamic program-
ming for aligning comparable texts for the generation
of paraphrases. In Proceedings of the COLING/ACL
on Main conference poster sessions, pages 747?754.
Association for Computational Linguistics.
Yusuke Shinyama and Satoshi Sekine. 2003. Para-
phrase acquisition for information extraction. In Pro-
ceedings of the second international workshop on
Paraphrasing-Volume 16, pages 65?71. Association
for Computational Linguistics.
Richard Socher, Eric H Huang, Jeffrey Pennington, An-
drew Y Ng, and Christopher D Manning. 2011. Dy-
namic pooling and unfolding recursive autoencoders
for paraphrase detection. NIPS, 24:801?809.
Mihalis Yannakakis. 1992. On the approximation of
maximum satisfiability. In Proceedings of the third an-
nual ACM-SIAM symposium on Discrete algorithms,
SODA ?92, pages 1?9.
Limin Yao, Aria Haghighi, Sebastian Riedel, and Andrew
McCallum. 2011. Structured relation discovery using
generative models. In EMNLP, pages 1456?1466. As-
sociation for Computational Linguistics.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2012. Unsupervised relation discovery with sense dis-
ambiguation. In ACL, pages 712?720. Association for
Computational Linguistics.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34(1):255.
1786
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 286?295,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning 5000 Relational Extractors
Raphael Hoffmann, Congle Zhang, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA-98195, USA
{raphaelh,clzhang,weld}@cs.washington.edu
Abstract
Many researchers are trying to use information
extraction (IE) to create large-scale knowl-
edge bases from natural language text on the
Web. However, the primary approach (su-
pervised learning of relation-specific extrac-
tors) requires manually-labeled training data
for each relation and doesn?t scale to the thou-
sands of relations encoded in Web text.
This paper presents LUCHS, a self-supervised,
relation-specific IE system which learns 5025
relations ? more than an order of magnitude
greater than any previous approach ? with an
average F1 score of 61%. Crucial to LUCHS?s
performance is an automated system for dy-
namic lexicon learning, which allows it to
learn accurately from heuristically-generated
training data, which is often noisy and sparse.
1 Introduction
Information extraction (IE), the process of gen-
erating relational data from natural-language text,
has gained popularity for its potential applications
in Web search, question answering and other tasks.
Two main approaches have been attempted:
? Supervised learning of relation-specific ex-
tractors (e.g., (Freitag, 1998)), and
? ?Open? IE ? self-supervised learning of
unlexicalized, relation-independent extractors
(e.g., Textrunner (Banko et al, 2007)).
Unfortunately, both methods have problems.
Supervised approaches require manually-labeled
training data for each relation and hence can?t
scale to handle the thousands of relations encoded
in Web text. Open extraction is more scalable,
but has lower precision and recall. Furthermore,
open extraction doesn?t canonicalize relations, so
any application using the output must deal with
homonymy and synonymy.
A third approach, sometimes refered to as weak
supervision, is to heuristically match values from
a database to text, thus generating a set of train-
ing data for self-supervised learning of relation-
specific extractors (Craven and Kumlien, 1999).
With the Kylin system (Wu and Weld, 2007) ap-
plied this idea to Wikipedia by matching values
of an article?s infobox1 attributes to corresponding
sentences in the article, and suggested that their
approach could extract thousands of relations (Wu
et al, 2008). Unfortunately, however, they never
tested the idea on more than a dozen relations. In-
deed, no one has demonstrated a practical way to
extract more than about one hundred relations.
We note that Wikipedia?s infobox ?ontology? is
a particularly interesting target for extraction. As a
by-product of thousands of contributors, it is broad
in coverage and growing quickly. Unfortunately,
the schemata are surprisingly noisy and most are
sparsely populated; challenging conditions for ex-
traction.
This paper presents LUCHS, an autonomous,
self-supervised system, which learns 5025 rela-
tional extractors ? an order of magnitude greater
than any previous effort. Like Kylin, LUCHS cre-
ates training data by matching Wikipedia attribute
values with corresponding sentences, but by itself,
this method was insufficient for accurate extrac-
tion of most relations. Thus, LUCHS introduces
a new technique, dynamic lexicon features, which
dramatically improves performance when learning
from sparse data and that way enables scalability.
1.1 Dynamic Lexicon Features
Figure 1 summarizes the architecture of LUCHS.
At the highest level, LUCHS?s offline training pro-
cess resembles that of Kylin. Wikipedia pages
1A sizable fraction of Wikipedia articles have associated
infoboxes ? relational summaries of the key aspects of the
subject of the article. For example, the infobox for Alan Tur-
ing?s Wikipedia page lists the values of 10 attributes, includ-
ing his birthdate, nationality and doctoral advisor.
286
Matcher Harvester
CRF 
Learner
Filtered Lists
WWW
Lexicon 
Learner
Classifier
Learner
Training Data
Extractor
Training Data
Lexicons
TuplesPages
Article
Classifier ExtractorExtractorClassified Pages
Extraction
Learning
Figure 1: Architecture of LUCHS. In order to
handle sparsity in its heuristically-generated train-
ing data, LUCHS generates custom lexicon features
when learning each relational extractor.
containing infoboxes are used to train a classi-
fier that can predict the appropriate schema for
pages missing infoboxes. Additionally, the val-
ues of infobox attributes are compared with article
sentences to heuristically generate training data.
LUCHS?s major innovation is a feature-generation
process, which starts by harvesting HTML lists
from a 5B document Web crawl, discarding 98%
to create a set of 49M semantically-relevant lists.
When learning an extractor for relation R, LUCHS
extracts seed phrases from R?s training data and
uses a semi-supervised learning algorithm to cre-
ate several relation-specific lexicons at different
points on a precision-recall spectrum. These lex-
icons form Boolean features which, along with
lexical and dependency parser-based features, are
used to produce a CRF extractor for each relation
? one which performs much better than lexicon-
free extraction on sparse training data.
At runtime, LUCHS feeds pages to the article
classfier, which predicts which infobox schema
is most appropriate for extraction. Then a small
set of relation-specific extractors are applied to
each sentence, outputting tuples. Our experiments
demonstrate a high F1 score, 61%, across the 5025
relational extractors learned.
1.2 Summary
This paper makes several contributions:
? We present LUCHS, a self-supervised IE sys-
tem capable of learning more than an order
of magnitude more relation-specific extractors
than previous systems.
? We describe the construction and use of dy-
namic lexicon features, a novel technique, that
enables hyper-lexicalized extractors which
cope effectively with sparse training data.
? We evaluate the overall end-to-end perfor-
mance of LUCHS, showing an F1 score of 61%
when extracting relations from randomly se-
lected Wikipedia pages.
? We present a comprehensive set of additional
experiments, evaluating LUCHS?s individual
components, measuring the effect of dynamic
lexicon features, testing sensitivity to varying
amounts of training data, and categorizing the
types of relations LUCHS can extract.
2 Heuristic Generation of Training Data
Wikipedia is an ideal starting point for our long-
term goal of creating a massive knowledge base of
extracted facts for two reasons. First, it is com-
prehensive, containing a diverse body of content
with significant depth. Perhaps more importantly,
Wikipedia?s structure facilitates self-supervised
extraction. Infoboxes are short, manually-created
tabular summaries of many articles? key facts ?
effectively defining a relational schema for that
class of entity. Since the same facts are often ex-
pressed in both article and ontology, matching val-
ues of the ontology to the article can deliver valu-
able, though noisy, training data.
For example, the Wikipedia article on ?Jerry Se-
infeld? contains the sentence ?Seinfeld was born
in Brooklyn, New York.? and the article?s infobox
contains the attribute ?birth place = Brooklyn?.
By matching the attribute?s value ?Brooklyn? to
the sentence, we can heuristically generate train-
ing data for a birth place extractor. This data is
noisy; some attributes will not find matches, while
others will find many co-incidental matches.
3 Learning Extractors
We first assume that each Wikipedia infobox at-
tribute corresponds to a unique relation (but see
Section 5.6) for which we would like to learn a
specific extractor. A major challenge with such
an approach is scalability. Running a relation-
specific extractor for each of Wikipedia?s 34,000
unique infobox attributes on each of Wikipedia?s
50 million sentences would require 1.7 trillion ex-
tractor executions.
We therefore choose a hierarchical approach
that combines both article classifiers and rela-
tion extractors. For each infobox schema, LUCHS
trains a classifier that predicts if an article is likely
to contain that schema. Only when an article
287
is likely to contain a schema, does LUCHS run
that schema?s relation extractors. To extract in-
fobox attributes from all of Wikipedia, LUCHS
now needs orders of magnitude fewer executions.
While this approach does not propagate infor-
mation from extractors back to article classifiers,
experiments confirm that our article classifiers
nonetheless deliver accurate results (Section 5.2),
reducing the potential benefit of joint inference. In
addition, our approach reduces the need for extrac-
tors to keep track of the larger context, thus sim-
plifying the extraction problem.
We briefly summarize article classification: We
use a linear, multi-class classifier with six kinds of
features: words in the article title, words in the
first sentence, words in the first sentence which
are direct objects to the verb ?to be?, article sec-
tion headers, Wikipedia categories, and their an-
cestor categories. We use the voted perceptron al-
gorithm (Freund and Schapire, 1999) for training.
More challenging are the attribute extractors,
which we wish to be simple, fast, and able to well
capture local dependencies. We use a linear-chain
conditional random field (CRF) ? an undirected
graphical model connecting a sequence of input
and output random variables, x = (x0, . . . , xT )
and y = (y0, . . . , yT ) (Lafferty et al, 2001). In-
put variables are assigned words w. The states
of output variables represent discrete labels l, e.g.
Argi-of-Relj and Other. In our case, variables
are connected in a chain, following the first-order
Markov assumption. We train to maximize condi-
tional likelihood of output variables given an input
probability distribution. The CRF models p(y|x)
are represented with a log-linear distribution
p(y|x) =
1
Z(x)
exp
T?
t=1
K?
k=1
?kfk(yt?1, yt, x, t)
where feature functions, f , encode sufficient
statistics of (x, y), T is the length of the sequence,
K is the number of feature functions, and ?k are
parameters representing feature weights, which
we learn during training. Z(x) is a partition func-
tion used to normalize the probabilities to 1. Fea-
ture functions allow complex, overlapping global
features with lookahead.
Common techniques for learning the weights ?k
include numeric optimization algorithms such as
stochastic gradient descent or L-BFGS. In our ex-
periments, we again use the simpler and more effi-
cient voted-perceptron algorithm (Collins, 2002).
The linear-chain layout enables efficient interence
using the dynamic programming-based Viterbi al-
gorithm (Lafferty et al, 2001).
We evaluate nine kinds of Boolean features:
Words For each input word w we introduce fea-
ture fww (yt?1, yt, x, t) := 1[xt=w].
State Transitions For each transition be-
tween output labels li, lj we add feature
f tranli,lj (yt?1, yt, x, t) := 1[yt?1=li?yt=lj ].
Word Contextualization For parameters p and
s we add features fprevw (yt?1, yt, x, t) :=
1[w?{xt?p,...,xt?1}] and f
sub
w (yt?1, yt, x, t) :=
1[w?{xt+1,...,xt+s}] which capture a window of
words appearing before and after each position t.
Capitalization We add feature
fcap(yt?1, yt, x, t) := 1[xtis capitalized].
Digits We add feature fdig(yt?1, yt, x, t) :=
1[xtis digits].
Dependencies We set fdep(yt?1, yt, x, t) to the
lemmatized sequence of words from xt to the root
of the dependency tree, computed using the Stan-
ford parser (Marneffe et al, 2006).
First Sentence We set f fs(yt?1, yt, x, t) :=
1[xtin first sentence of article].
Gaussians For numeric attributes, we fit a Gaus-
sian (?, ?) and add feature fgaui (yt?1, yt, x, t) :=
1[|xt??|<i?] for parameters i.
Lexicons For non-numeric attributes, and for a
lexicon l, i.e. a set of related words, we add fea-
ture f lexl (yt?1, yt, x, t) := 1[xt?l]. Lexicons are
explained in the following section.
4 Extraction with Lexicons
It is often possible to group words that are likely
to be assigned similar labels, even if many of these
words do not appear in our training set. The ob-
tained lexicons then provide an elegant way to im-
prove the generalization ability of an extractor, es-
pecially when only little training data is available.
However, there is a danger of overfitting, which
we discuss in Section 4.2.4.
The next section explains how we mine the Web
to obtain a large corpus of quality lists. Then Sec-
tion 4.2 presents our semi-supervised algorithm
for learning semantic lexicons from these lists.
288
4.1 Harvesting Lists from the Web
Domain-independence requires access to an ex-
tremely large number of lists, but our tight in-
tegration of lexicon acquisition and CRF learn-
ing requires that relevant lists be accessed instan-
taneously. Approaches using search engines or
wrappers at query time (Etzioni et al, 2004; Wang
and Cohen, 2008) are too slow; we must extract
and index lists prior to learning.
We begin with a 5 billion page Web crawl.
LUCHS can be combined with any list harvesting
technique, but we choose a simple approach, ex-
tracting lists defined by HTML <ul> or <ol>
tags. The set of lists obtained in this way is ex-
tremely noisy ? many lists comprise navigation
bars, tag sets, spam links, or a series of long text
paragraphs. This is consistent with the observation
that less than 2% of Web tables are relational (Ca-
farella et al, 2008).
We therefore apply a series of filtering steps.
We remove lists of only one or two items, lists
containing long phrases, and duplicate lists from
the same host. After filtering we obtain 49 million
lists, containing 56 million unique phrases.
4.2 Semi-Supervised Learning of Lexicons
While training a CRF extractor for a given rela-
tion, LUCHS uses its corpus of lists to automati-
cally generate a set of semantic lexicons ? spe-
cific to that relation. The technique proceeds in
three steps, which have been engineered to run ex-
tremely quickly:
1. Seed phrases are extracted from the labeled
training set.
2. A learning algorithm expands the seed
phrases into a set of lexicons.
3. The semantic lexicons are added as features
to the CRF learning algorithm.
4.2.1 Extracting Seed Phrases
For each training sentence LUCHS first identifies
subsequences of labeled words, and for each such
labeled subsequence, LUCHS creates one or more
seed phrases p. Typically, a set of seeds con-
sists precisely of the labeled subsequences. How-
ever, if the labeled subsequences are long and have
substructure, e.g., ?San Remo, Italy?, our system
splits at the separator token, and creates additional
seed sets from prefixes and postfixes.
4.2.2 From Seeds to Lexicons
To expand a set of seeds into a lexicon, LUCHS
must identify relevant lists in the corpus. Rele-
vancy can be computed by defining a similarity be-
tween lists using the vector-space model. Specifi-
cally, let L denote the corpus of lists, and P be the
set of unique phrases from L. Each list l0 ? L can
be represented as a vector of weighted phrases p ?
P appearing on the list, l0 = (l0p1 l
0
p2 . . . l
0
p|P|). Fol-
lowing the notion of inverse document frequency,
a phrase?s weight is inversely proportional to the
number of lists containing the phrase. Popular
phrases which appear on many lists thus receive
a small weight, whereas rare phrases are weighted
higher:
l0pi =
1
|{l ? L|p ? l}|
Unlike the vector space model for documents, we
ignore term frequency, since the vast majority of
lists in our corpus don?t contain duplicates. This
vector representation supports the simple cosine
definition of list similarity, which for lists l0, l1 ?
L is defined as
simcos :=
l0 ? l1
?l0??l1?
.
Intuitively, two lists are similar if they have many
overlapping phrases, the phrases are not too com-
mon, and the lists don?t contain many other
phrases. By representing the seed set as another
vector, we can find similar lists, hopefully contain-
ing related phrases. We then create a semantic lex-
icon by collecting phrases from a range of related
lists.
For example, one lexicon may be created as the
union of all phrases on lists that have non-zero
similarity to the seed list. Unfortunately, due to
the noisy nature of the Web lists such a lexicon
may be very large and may contain many irrele-
vant phrases. We expect that lists with higher sim-
ilarity are more likely to contain phrases which are
related to our seeds; hence, by varying the sim-
ilarity threshold one may produce lexicons rep-
resenting different compromises between lexicon
precision and recall. Not knowing which lexicon
will be most useful to the extractors, LUCHS gen-
erates several and lets the extractors learn appro-
priate weights.
However, since list similarities vary depending
on the seeds, fixed thresholds are not an option. If
#similarlists denotes the number of lists that have
non-zero similarity to the seed list and #lexicons
289
the total number of lexicons we want to generate,
LUCHS sets lexicon i ? {0, . . . ,#lexicons ? 1}
to be the union of prases on the
#similarlistsi/#lexicons
most similar lists.2
4.2.3 Efficiently Creating Lexicons
We create lexicons from lists that are similar to
our seed vector, so we only consider lists that have
at least one phrase in common. Importantly, our
index structures allow LUCHS to select the rele-
vant lists efficiently. For each seed, LUCHS re-
trieves the set of containing lists as a sorted se-
quence of list identifiers. These sequences are
then merged yielding a sequence of list identifiers
with associated seed-hit counts. Precomputed list
lengths and inverse document frequencies are also
retrieved from indices, allowing efficient compu-
tation of similarity. The worst case complexity is
O(log(S)SK) where S is the number of seeds and
K the maximum number of lists to consider per
seed.
4.2.4 Preventing Lexicon Overfitting
Finally, we integrate the acquired semantic lexi-
cons as features into the CRF. Although Section 3
discussed how to use lexicons as CRF features,
there are some subtleties. Recall that the lexi-
cons were created from seeds extracted from the
training set. If we now train the CRF on the same
examples that generated the lexicon features, then
the CRF will likely overfit, and weight the lexicon
features too highly!
Before training, we therefore split the training
set into k partitions. For each example in a par-
tition we assign features based on lexicons gener-
ated from only the k?1 remaining partitions. This
avoids overfitting and ensures that we will not per-
form much worse than without lexicon features.
When we apply the CRF to our test set, we use the
lexicons based on all k partitions. We refer to this
technique as cross-training.
5 Experiments
We start by evaluating end-to-end performance of
LUCHS when applied to Wikipedia text, then an-
alyze the characteristics of its components. Our
experiments use the 10/2008 English Wikipedia
dump.
2For practical reasons, we exclude the case i = #lexicons
in our experiments.
0.0 0.2 0.4 0.6 0.8 1.0
0.0
0.2
0.4
0.6
0.8
1.0
recall
precision
Figure 2: Precision / recall curve for end-to-end
system performance on 100 random articles.
5.1 Overall Extraction Performance
To evaluate the end-to-end performance of
LUCHS, we test the pipeline which first classifies
incoming pages, activating a small set of extrac-
tors on the text. To ensure adequate training and
test data, we limit ourselves to infobox classes
with at least ten instances; there exist 1,583 such
classes, together comprising 981,387 articles. We
only consider the first ten sentences for each ar-
ticle, and we only consider 5025 attributes.3 We
create a test set by sampling 100 articles ran-
domly; these articles are not used to train article
classifiers or extractors. Each test article is then
automatically classified, and a random attribute
of the predicted schema is selected for extraction.
Gold labels for the selected attribute and article are
created manually by a human judge and compared
to the token-level predictions from the extractors
which are trainined on the remaining articles with
heuristic matches.
Overall, LUCHS reaches a precision of .55 at a
recall of .68, giving an F1-score of .61 (Figure 2).
Analyzing the errors in more detail, we find that in
11 of 100 cases an article was incorrectly classi-
fied. We note that in at least two of these cases the
predicted class could also be considered correct.
For example, instead of Infobox Minor Planet the
extractor predicted Infobox Planet.
On five of the selected attributes the extrac-
tor failed because the attributes could be consid-
ered unlearnable: The flexibility of Wikipedia?s
infobox system allows contributors to introduce
attributes for formatting, for example defining el-
3Attributes were selected to have at least 10 heuristic
matches, to have 10% of values covered by matches, and 10%
of articles with attribute in infobox covered by matches.
290
ement order. In the future we wish to train LUCHS
to ignore this type of attribute.
We also compared the heuristic matches con-
tained in the selected 100 articles to the gold stan-
dard: The matches reach a precision of .90 at a
recall of .33, giving an F1-score of .48. So while
most heuristic matches hit mentions of attribute
values, many other mentions go unmatched. Man-
ual analysis shows that these values are often miss-
ing from an infobox, are formatted differently, or
are inconsistent to what is stated in the article.
So why did the low recall of the heuristic
matches not adversely affect recall of our extrac-
tors? For most articles, an attribute can be as-
signed a single unique value. When training an
attribute extractor, only articles that contained a
heuristic match for that attribute were considered,
thus avoiding many cases of unmatched mentions.
Subsequent experiments evaluate the perfor-
mance of LUCHS components in more detail.
5.2 Article Classification
The first step in LUCHS?s run-time pipeline is de-
termining which infobox schemata are most likely
to be found in a given article. To test this, we ran-
domly split our 981,387 articles into 4/5 for train-
ing and 1/5 for testing, and train a single multi-
class classifier. For this experiment, we use the
original infobox class of an article as its gold la-
bel. We compute the accuracy of the prediction at
.92. Since some classes can be considered inter-
changeable, this number represents a lower bound
on performance.
5.3 Factors Affecting Extraction Accuracy
We now evaluate attribute extraction assuming
perfect article classification. To keep training time
manageable, we sample 100 articles for training
and 100 articles for testing4 for each of 100 ran-
dom attributes. We again only consider the first
ten sentences of each article, and we only con-
sider articles that have heuristic matches with the
attribute. We measure F1-score at a token-level,
taking the heuristic matches as ground-truth.
We first test the performance of extractors
trained using our basic features (Section 3)5, not
including lexicons and Gaussians. We begin us-
ing word features and obtain a token-level F1-
score of .311 for text and .311 for numeric at-
tributes. Adding any of our additional features
4These numbers are smaller for attributes with less train-
ing data available, but the same split is maintained.
5For contextualization features we choose p, s = 5.
Features F1-Score
Text attributes
Baseline .491
Baseline + Lexicons w/o CT .367
Baseline + Lexicons .545
Numeric attributes
Baseline .586
Baseline + Gaussians w/o CT .623
Baseline + Gaussians .627
Table 1: Impact of Lexicon and Gaussian features.
Cross-Training (CT) is essential to improve per-
formance.
improves these scores, but the relative improve-
ments vary: For both text and numeric attributes,
contextualization and dependency features deliver
the largest improvement. We then iteratively add
the feature with largest improvement until no fur-
ther improvement is observed. We finally obtain
an F1-score of .491 for text and .586 for numeric
attributes. For text attributes the extractor uses
word, contextualization, first sentence, capitaliza-
tion, and digit features; for numeric attributes the
extractor uses word, contextualization, digit, first
sentence, and dependency features. We use these
extractors as a baseline to evaluate our lexicon and
Gaussian features.
Varying the size of the training sets affects re-
sults: Taking more articles raises the F1-score, but
taking more sentences per article reduces it. This
is because Wikipedia articles often summarize a
topic in the first few paragraphs and later discuss
related topics, necessitating reference resolution
which we plan to add in future work.
5.4 Lexicon and Gaussian Features
We next study how our distribution features6 im-
pact the quality of the baseline extractors (Table
1). Without cross-training we observe a reduction
in performance, due to overfitting. Cross-training
avoids this, and substantially improves results over
the baseline. While cross-training is particularly
critical for lexicon features, it is less needed for
Gaussians where only two parameters, mean and
deviation, are fitted to the training set.
The relative improvements depend on the num-
ber of available training examples (Table 2). Lex-
icon and Gaussian features especially benefit ex-
tractors for sparse attributes. Here we can also see
that the improvements are mainly due to increases
in recall.
6We set the number of lexicon and Gaussian features to 4.
291
# Train F1-B F1-LUCHS ?F1 ?Pr ?Re
Text attributes
10 .379 .439 +16% +10% +20%
25 .447 .504 +13% +7% +20%
100 .491 .545 +11% +5% +17%
Numeric attributes
10 .484 .531 +10% +4% +13%
25 .552 .596 +8% +4% +10%
100 .586 .627 +7% +5% +8%
Table 2: Lexicon and Gaussian features greatly ex-
pand F1 score (F1-LUCHS) over the baseline (F1-
B), in particular for attributes with few training ex-
amples. Gains are mainly due to increased recall.
5.5 Scaling to All of Wikipedia
Finally, we take our best extractors and run them
on all 5025 attributes, again assuming perfect ar-
ticle classification and using heuristic matches as
gold-standard. Figure 3 shows the distribution of
obtained F1 scores. 810 text attributes and 328 nu-
meric attributes reach a score of 0.80 or higher.
The performance depends on the number of
available training examples, and that number is
governed by a long-tailed distribution. For ex-
ample, 61% of the attributes in our set have 50
or fewer examples, 36% have 20 or fewer. Inter-
estingly, the number of training examples had a
smaller effect on performance than expected. Fig-
ure 4 shows the correlation between these vari-
ables. Lexicon and Gaussian features enables ac-
ceptable performance even for sparse attributes.
Averaging across all attributes we obtain F1
scores of 0.56 and 0.60 for textual and numeric
values respectively. We note that these scores
assume that all attributes are equally important,
weighting rare attributes just like common ones.
If we weight scores by the number of attribute in-
stances, we obtain F1 scores of 0.64 (textual) and
0.78 (numeric). In each case, precision is slightly
higher than recall.
5.6 Towards an Attribute Ontology
The true promise of relation-specific extractors
comes when an ontology ties the system together.
By learning a probabilistic model of selectional
preferences, one can use joint inference to improve
extraction accuracy. One can also answer scien-
tific questions, such as ?How many of the learned
Wikipedia attributes are distinct?? It is clear that
many duplicates exist due to collaborative sloppi-
ness, but semantic similarity is a matter of opinion
and an exact answer is impossible.
0% 20% 40% 60% 80% 100%
0.0
0.2
0.4
0.6
0.8
1.0
Text attr. (3962)
Numeric attr. (1063)
# Attributes
F1 Score
Figure 3: F1 scores among attributes, ranked by
score. 810 text attributes (20%) and 328 numeric
attributes (31%) had an F1-score of .80 or higher.
0 20 40 60 80 100
0.0
0.2
0.4
0.6
0.8
Text attr.
Numeric attr.
# Training Examples
Average F1 Sco
re
Figure 4: Average F1 score by number of training
examples. While more training data helps, even
sparse attributes reach acceptable performance.
Nevertheless, we clustered the textual attributes
in several ways. First, we cleaned the attribute
names heuristically and performed spell check.
The ?distance? between two attributes was calcu-
lated with a combination of edit distance and IR
metrics with Wordnet synonyms; then hierarchical
agglomerative clustering was performed. We man-
ually assigned names to the clusters and cleaned
them, splitting and joining as needed. The result is
too crude to be called an ontology, but we continue
its elaboration. There are a total of 3962 attributes
grouped in about 1282 clusters (not yet counting
attributes with numerical values); the largest clus-
ter, location, has 115 similar attributes. Figure 5
shows the confusion matrix between attributes in
the biggest clusters; the shade of the i, jth pixel
indicates the F1 score achieved by training on in-
stances of attribute i and testing on attribute j.
292
loca
tion
birt
hpla
cep
title cou
ntry
full
 nam
e
city nat
iona
lity
nati
ona
lity
birt
h na
me
date
 of 
birt
h
date
 of 
dea
th
date stat
es
Figure 5: Confusion matrix for extractor accuracy
training on one attribute then testing on another.
Note the extraction similarity between title and
full-name, as well as between dates of birth and
death. Space constraints allow us to show only
1000 of LUCHS?s 5025 extracted attributes, those
in the largest clusters.
6 Related Work
Large-scale extraction A popular approach to IE
is supervised learning of relation-specific extrac-
tors (Freitag, 1998). Open IE, self-supervised
learning of unlexicalized, relation-independent ex-
tractors (Banko et al, 2007), is a more scalable
approach, but suffers from lower precision and
recall, and doesn?t canonicalize the relations. A
third approach, weak supervision, performs self-
supervised learning of relation-specific extractors
from noisy training data, heuristically generated
by matching database values to text. (Craven and
Kumlien, 1999; Hirschman et al, 2002) apply this
technique to the biological domain, and (Mintz
et al, 2009) apply it to 102 relations from Free-
base. LUCHS differs from these approaches in that
its ?database? ? the set of infobox values ? itself
is noisy, contains many more relations, and has
few instances per relation. Whereas the existing
approaches focus on syntactic extraction patterns,
LUCHS focuses on lexical information enhanced
by dynamic lexicon learning.
Extraction from Wikipedia Wikipedia has
become an interesting target for extraction.
(Suchanek et al, 2008) build a knowledgebase
from Wikipedia?s semi-structured data. (Wang et
al., 2007) propose a semisupervised positive-only
learning technique. Although that extracts from
text, its reliance on hyperlinks and other semi-
structured data limits extraction. (Wu and Weld,
2007; Wu et al, 2008)?s systems generate train-
ing data similar to LUCHS, but were only on a few
infobox classes. In contrast, LUCHS shows that
the idea scales to more than 5000 relations, but
that additional techniques, such as dynamic lexi-
con learning, are necessary to deal with sparsity.
Extraction with lexicons While lexicons have
been commonly used for IE (Cohen and Sarawagi,
2004; Agichtein and Ganti, 2004; Bellare and Mc-
Callum, 2007), many approaches assume that lex-
icons are clean and are supplied by a user before
training. Other approaches (Talukdar et al, 2006;
Miller et al, 2004; Riloff, 1993) learn lexicons
automatically from distributional patterns in text.
(Wang et al, 2009) learns lexicons from Web lists
for query tagging. LUCHS differs from these ap-
proaches in that it is not limited to a small set of
well-defined relations. Rather than creating large
lexicons of common entities, LUCHS attempts to
efficiently instantiate a series of lexicons from a
small set of seeds to bias extractors of sparse at-
tributes. Crucual to LUCHS?s different setting is
also the need to avoid overfitting.
Set expansion A large amount of work has
looked at automatically generating sets of related
items. Starting with a set of seed terms, (Etzioni
et al, 2004) extract lists by learning wrappers for
Web pages containing those terms. (Wang and Co-
hen, 2007; Wang and Cohen, 2008) extend the
idea, computing term relatedness through a ran-
dom walk algorithm that takes into account seeds,
documents, wrappers and mentions. Other ap-
proaches include Bayesian methods (Ghahramani
and Heller, 2005) and graph label propagation al-
gorithms (Talukdar et al, 2008; Bengio et al,
2006). The goal of set expansion techniques is
to generate high precision sets of related items;
hence, these techniques are evaluated based on
lexicon precision and recall. For LUCHS, which is
evaluated based on the quality of an extractor us-
ing the lexicons, lexicon precision is not important
? as long as it does not confuse the extractor.
7 Future Work
We envision a Web-scale machine reading system
which simultaneously learns ontologies and ex-
tractors, and we believe that LUCHS?s approach
of leveraging noisy semi-structured information
(such as lists or formatting templates) is a key to-
wards this goal. For future work, we plan to en-
hance LUCHS in two major ways.
First, we note that a big weakness is that the
system currently only works for Wikipedia pages.
293
For example, LUCHS assumes that each page cor-
responds to exactly one schema and that the sub-
ject of relations on a page are the same. Also,
LUCHS makes predictions on a token basis, thus
sometimes failing to recognize larger segments.
To remove these limitations we plan to add a
deeper linguistic analysis, making better use of
parse and dependency information and including
coreference resolution. We also plan to employ
relation-independent Open extraction techniques,
e.g. as suggested in (Wu and Weld, 2008) (retrain-
ing).
Second, we note that LUCHS?s performance
may benefit substantially from an attribute ontol-
ogy. As we showed in Section 5.6, LUCHS?s cur-
rent extractors can also greatly facilitate learning
a full attribute ontology. We therefore plan to in-
terleave extractor learning and ontology inference,
hence jointly learning ontology and extractors.
8 Conclusion
Many researchers are trying to use IE to cre-
ate large-scale knowledge bases from natural lan-
guage text on the Web, but existing relation-
specific techniques do not scale to the thousands
of relations encoded in Web text ? while relation-
independent techniques suffer from lower preci-
sion and recall, and do not canonicalize the rela-
tions. This paper shows that ? with new techniques
? self-supervised learning of relation-specific ex-
tractors from Wikipedia infoboxes does scale.
In particular, we present LUCHS, a self-
supervised IE system capable of learning more
than an order of magnitude more relation-specific
extractors than previous systems. LUCHS uses
dynamic lexicon features that enable hyper-
lexicalized extractors which cope effectively with
sparse training data. We show an overall perfor-
mance of 61% F1 score, and present experiments
evaluating LUCHS?s individual components.
Datasets generated in this work are available to
the community7.
Acknowledgments
We thank Jesse Davis, Oren Etzioni, Andrey Kolobov,
Mausam, Fei Wu, and the anonymous reviewers for helpful
comments and suggestions.
This material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by the Air
Force Research Laboratory (AFRL) under prime contract no.
FA8750-09-C-0181. Any opinions, findings, and conclusion
or recommendations expressed in this material are those of
7http://www.cs.washington.edu/ai/iwp
the author(s) and do not necessarily reflect the view of the
Air Force Research Laboratory (AFRL).
References
Eugene Agichtein and Venkatesh Ganti. 2004. Mining refer-
ence tables for automatic text segmentation. In Proceed-
ings of the Tenth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining (KDD-2004),
pages 20?29.
So?ren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary G. Ives. 2007.
Dbpedia: A nucleus for a web of open data. In Proceed-
ings of the 6th International Semantic Web Conference
and 2nd Asian Semantic Web Conference (ISWC/ASWC-
2007), pages 722?735.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of the
20th International Joint Conference on Artificial Intelli-
gence (IJCAI-2007), pages 2670?2676.
Kedar Bellare and Andrew McCallum. 2007. Learning ex-
tractors from unlabeled text using relevant databases. In
Sixth International Workshop on Information Integration
on the Web.
Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux.
2006. Label propagation and quadratic criterion. In
Olivier Chapelle, Bernhard Scho?lkopf, and Alexander
Zien, editors, Semi-Supervised Learning, pages 193?216.
MIT Press.
Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang, Eu-
gene Wu, and Yang Zhang. 2008. Webtables: exploring
the power of tables on the web. Proceedings of the In-
ternational Conference on Very Large Databases (VLDB-
2008), 1(1):538?549.
Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr.,
and Tom M. Mitchell. 2009a. Coupling semi-supervised
learning of categories and relations. In NAACL HLT 2009
Workskop on Semi-supervised Learning for Natural Lan-
guage Processing.
Andrew Carlson, Scott Gaffney, and Flavian Vasile. 2009b.
Learning a named entity tagger from gazetteers with the
partial perceptron. In AAAI Spring Symposium on Learn-
ing by Reading and Learning to Read.
William W. Cohen and Sunita Sarawagi. 2004. Exploiting
dictionaries in named entity extraction: combining semi-
markov extraction processes and data integration methods.
In Proceedings of the Tenth ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining
(KDD-2004), pages 89?98.
Michael Collins. 2002. Discriminative training methods for
hidden markov models: Theory and experiments with per-
ceptron algorithms. In Proceedings of the 2002 Confer-
ence on Empirical Methods in Natural Language Process-
ing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing bi-
ological knowledge bases by extracting information from
text sources. In Proceedings of the Seventh International
Conference on Intelligent Systems for Molecular Biology
(ISMB-1999), pages 77?86.
294
Benjamin Van Durme and Marius Pasca. 2008. Finding cars,
goddesses and enzymes: Parametrizable acquisition of la-
beled instances for open-domain information extraction.
In Proceedings of the Twenty-Third AAAI Conference on
Artificial Intelligence (AAAI-2008), pages 1243?1248.
Oren Etzioni, Michael J. Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, Daniel S.
Weld, and Alexander Yates. 2004. Methods for domain-
independent information extraction from the web: An ex-
perimental comparison. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
2004), pages 391?398.
Dayne Freitag. 1998. Toward general-purpose learning for
information extraction. In Proceedings of the 17th inter-
national conference on Computational linguistics, pages
404?408. Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large margin
classification using the perceptron algorithm. Machine
Learning, 37(3):277?296.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Neural Information Processing Systems
(NIPS-2005).
Lynette Hirschman, Alexander A. Morgan, and Alexander S.
Yeh. 2002. Rutabaga by any other name: extracting
biological names. Journal of Biomedical Informatics,
35(4):247?259.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
Proceedings of the Eighteenth International Conference
on Machine Learning (ICML-2001), pages 282?289.
Marie-Catherine De Marneffe, Bill Maccartney, and Christo-
pher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the
fifth international conference on Language Resources and
Evaluation (LREC-2006).
Scott Miller, Jethran Guinness, and Alex Zamanian. 2004.
Name tagging with word clusters and discriminative train-
ing. In Proceedings of the Human Language Technology
Conference of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL-2004).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Jurafsky.
2009. Distant supervision for relation extraction without
labeled data. In The Annual Meeting of the Association
for Computational Linguistics (ACL-2009).
Marius Pasca. 2009. Outclassing wikipedia in open-domain
information extraction: Weakly-supervised acquisition of
attributes over conceptual hierarchies. In Proceedings
of the 12th Conference of the European Chapter of the
Association for Computational Linguistics (EACL-2009),
pages 639?647.
Ellen Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In Proceedings of the
11th National Conference on Artificial Intelligence (AAAI-
1993), pages 811?816.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2008. Yago: A large ontology from wikipedia and word-
net. Elsevier Journal of Web Semantics, 6(3):203?217.
Fabian M. Suchanek, Mauro Sozio, and Gerhard Weikum.
2009. Sofie: A self-organizing framework for informa-
tion extraction. In Proceedings of the 18th International
Conference on World Wide Web (WWW-2009).
Partha Pratim Talukdar, Thorsten Brants, Mark Liberman,
and Fernando Pereira. 2006. A context pattern induction
method for named entity extraction. In The Tenth Confer-
ence on Natural Language Learning (CoNLL-X-2006).
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of labeled
class instances using graph random walks. In EMNLP,
pages 582?590.
Richard C. Wang and William W. Cohen. 2007. Language-
independent set expansion of named entities using the
web. In Proceedings of the 7th IEEE International Con-
ference on Data Mining (ICDM-2007), pages 342?350.
Richard C. Wang and William W. Cohen. 2008. Iterative set
expansion of named entities using the web. In Proceed-
ings of the 8th IEEE International Conference on Data
Mining (ICDM-2008).
Gang Wang, Yong Yu, and Haiping Zhu. 2007. Pore:
Positive-only relation extraction from wikipedia text.
In Proceedings of the 6th International Semantic Web
Conference and 2nd Asian Semantic Web Conference
(ISWC/ASWC-2007), pages 580?594.
Ye-Yi Wang, Raphael Hoffmann, Xiao Li, and Alex Acero.
2009. Semi-supervised acquisition of semantic classes ?
from the web and for the web. In International Confer-
ence on Information and Knowledge Management (CIKM-
2009), pages 37?46.
Fei Wu and Daniel S. Weld. 2007. Autonomously seman-
tifying wikipedia. In Proceedings of the International
Conference on Information and Knowledge Management
(CIKM-2007), pages 41?50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635?644.
Fei Wu and Daniel S. Weld. 2010. Open information ex-
traction using wikipedia. In The Annual Meeting of the
Association for Computational Linguistics (ACL-2010).
Fei Wu, Raphael Hoffmann, and Daniel S. Weld. 2008. In-
formation extraction from wikipedia: moving down the
long tail. In Proceedings of the 14th ACM SIGKDD Inter-
national Conference on Knowledge Discovery and Data
Mining (KDD-2008), pages 731?739.
295
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 541?550,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Knowledge-Based Weak Supervision for Information Extraction
of Overlapping Relations
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. Weld
Computer Science & Engineering
University of Washington
Seattle, WA 98195, USA
{raphaelh,clzhang,xiaoling,lsz,weld}@cs.washington.edu
Abstract
Information extraction (IE) holds the promise
of generating a large-scale knowledge
base from the Web?s natural language text.
Knowledge-based weak supervision, using
structured data to heuristically label a training
corpus, works towards this goal by enabling
the automated learning of a potentially
unbounded number of relation extractors.
Recently, researchers have developed multi-
instance learning algorithms to combat the
noisy training data that can come from
heuristic labeling, but their models assume
relations are disjoint ? for example they
cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple).
This paper presents a novel approach for
multi-instance learning with overlapping re-
lations that combines a sentence-level extrac-
tion model with a simple, corpus-level compo-
nent for aggregating the individual facts. We
apply our model to learn extractors for NY
Times text using weak supervision from Free-
base. Experiments show that the approach
runs quickly and yields surprising gains in
accuracy, at both the aggregate and sentence
level.
1 Introduction
Information-extraction (IE), the process of generat-
ing relational data from natural-language text, con-
tinues to gain attention. Many researchers dream of
creating a large repository of high-quality extracted
tuples, arguing that such a knowledge base could
benefit many important tasks such as question an-
swering and summarization. Most approaches to IE
use supervised learning of relation-specific exam-
ples, which can achieve high precision and recall.
Unfortunately, however, fully supervised methods
are limited by the availability of training data and are
unlikely to scale to the thousands of relations found
on the Web.
A more promising approach, often called ?weak?
or ?distant? supervision, creates its own training
data by heuristically matching the contents of a
database to corresponding text (Craven and Kum-
lien, 1999). For example, suppose that r(e1, e2) =
Founded(Jobs,Apple) is a ground tuple in the
database and s =?Steve Jobs founded Apple, Inc.?
is a sentence containing synonyms for both e1 =
Jobs and e2 = Apple, then s may be a natural
language expression of the fact that r(e1, e2) holds
and could be a useful training example.
While weak supervision works well when the tex-
tual corpus is tightly aligned to the database con-
tents (e.g., matching Wikipedia infoboxes to as-
sociated articles (Hoffmann et al, 2010)), Riedel
et al (2010) observe that the heuristic leads to
noisy data and poor extraction performance when
the method is applied more broadly (e.g., matching
Freebase records to NY Times articles). To fix
this problem they cast weak supervision as a form of
multi-instance learning, assuming only that at least
one of the sentences containing e1 and e2 are ex-
pressing r(e1, e2), and their method yields a sub-
stantial improvement in extraction performance.
However, Riedel et al?s model (like that of
previous systems (Mintz et al, 2009)) assumes
that relations do not overlap ? there cannot
exist two facts r(e1, e2) and q(e1, e2) that are
both true for any pair of entities, e1 and e2.
Unfortunately, this assumption is often violated;
541
for example both Founded(Jobs, Apple) and
CEO-of(Jobs, Apple) are clearly true. In-
deed, 18.3% of the weak supervision facts in Free-
base that match sentences in the NY Times 2007 cor-
pus have overlapping relations.
This paper presents MULTIR, a novel model of
weak supervision that makes the following contri-
butions:
? MULTIR introduces a probabilistic, graphical
model of multi-instance learning which handles
overlapping relations.
? MULTIR also produces accurate sentence-level
predictions, decoding individual sentences as
well as making corpus-level extractions.
? MULTIR is computationally tractable. Inference
reduces to weighted set cover, for which it uses
a greedy approximation with worst case running
time O(|R| ? |S|) where R is the set of possi-
ble relations and S is largest set of sentences for
any entity pair. In practice, MULTIR runs very
quickly.
? We present experiments showing that MULTIR
outperforms a reimplementation of Riedel
et al (2010)?s approach on both aggregate (cor-
pus as a whole) and sentential extractions.
Additional experiments characterize aspects of
MULTIR?s performance.
2 Weak Supervision from a Database
Given a corpus of text, we seek to extract facts about
entities, such as the company Apple or the city
Boston. A ground fact (or relation instance), is
an expression r(e) where r is a relation name, for
example Founded or CEO-of, and e = e1, . . . , en
is a list of entities.
An entity mention is a contiguous sequence of tex-
tual tokens denoting an entity. In this paper we as-
sume that there is an oracle which can identify all
entity mentions in a corpus, but the oracle doesn?t
normalize or disambiguate these mentions. We use
ei ? E to denote both an entity and its name (i.e.,
the tokens in its mention).
A relation mention is a sequence of text (in-
cluding one or more entity mentions) which states
that some ground fact r(e) is true. For example,
?Steve Ballmer, CEO of Microsoft, spoke recently
at CES.? contains three entity mentions as well as a
relation mention for CEO-of(Steve Ballmer,
Microsoft). In this paper we restrict our atten-
tion to binary relations. Furthermore, we assume
that both entity mentions appear as noun phrases in
a single sentence.
The task of aggregate extraction takes two inputs,
?, a set of sentences comprising the corpus, and an
extraction model; as output it should produce a set
of ground facts, I , such that each fact r(e) ? I is
expressed somewhere in the corpus.
Sentential extraction takes the same input and
likewise produces I , but in addition it also produces
a function, ? : I ? P(?), which identifies, for
each r(e) ? I , the set of sentences in ? that contain
a mention describing r(e). In general, the corpus-
level extraction problem is easier, since it need only
make aggregate predictions, perhaps using corpus-
wide statistics. In contrast, sentence-level extrac-
tion must justify each extraction with every sentence
which expresses the fact.
The knowledge-based weakly supervised learning
problem takes as input (1) ?, a training corpus, (2)
E, a set of entities mentioned in that corpus, (3) R,
a set of relation names, and (4), ?, a set of ground
facts of relations in R. As output the learner pro-
duces an extraction model.
3 Modeling Overlapping Relations
We define an undirected graphical model that al-
lows joint reasoning about aggregate (corpus-level)
and sentence-level extraction decisions. Figure 1(a)
shows the model in plate form.
3.1 Random Variables
There exists a connected component for each pair of
entities e = (e1, e2) ? E ? E that models all of
the extraction decisions for this pair. There is one
Boolean output variable Y r for each relation name
r ? R, which represents whether the ground fact
r(e) is true. Including this set of binary random
variables enables our model to extract overlapping
relations.
Let S(e1,e2) ? ? be the set of sentences which
contain mentions of both of the entities. For each
sentence xi ? S(e1,e2) there exists a latent variable
Zi which ranges over the relation names r ? R and,
542
E ? E 
? 
R 
S 
?? 
(a)
Steve Jobs was founder  
of Apple. 
Steve Jobs, Steve Wozniak and 
Ronald Wayne founded Apple. 
Steve Jobs is CEO of  
Apple. 
founder ?? founder none 
0 
?????? 
1 0 0 
?? ?? 
... 
... 
... 
????????? ????????? ????????? 
(b)
Figure 1: (a) Network structure depicted as plate model and (b) an example network instantiation for the pair of entities
Steve Jobs, Apple.
importantly, also the distinct value none. Zi should
be assigned a value r ? R only when xi expresses
the ground fact r(e), thereby modeling sentence-
level extraction.
Figure 1(b) shows an example instantiation of the
model with four relation names and three sentences.
3.2 A Joint, Conditional Extraction Model
We use a conditional probability model that defines
a joint distribution over all of the extraction random
variables defined above. The model is undirected
and includes repeated factors for making sentence
level predictions as well as globals factors for ag-
gregating these choices.
For each entity pair e = (e1, e2), define x to
be a vector concatenating the individual sentences
xi ? S(e1,e2), Y to be vector of binary Y
r random
variables, one for each r ? R, and Z to be the vec-
tor of Zi variables, one for each sentence xi. Our
conditional extraction model is defined as follows:
p(Y = y,Z = z|x; ?)
def
=
1
Zx
?
r
?join(yr, z)
?
i
?extract(zi, xi)
where the parameter vector ? is used, below, to de-
fine the factor ?extract.
The factors ?join are deterministic OR operators
?join(yr, z)
def
=
{
1 if yr = true ? ?i : zi = r
0 otherwise
which are included to ensure that the ground fact
r(e) is predicted at the aggregate level for the as-
signment Y r = yr only if at least one of the sen-
tence level assignments Zi = zi signals a mention
of r(e).
The extraction factors ?extract are given by
?extract(zi, xi)
def
= exp
?
?
?
j
?j?j(zi, xi)
?
?
where the features ?j are sensitive to the relation
name assigned to extraction variable zi, if any, and
cues from the sentence xi. We will make use of the
Mintz et al (2009) sentence-level features in the ex-
peiments, as described in Section 7.
3.3 Discussion
This model was designed to provide a joint approach
where extraction decisions are almost entirely driven
by sentence-level reasoning. However, defining the
Y r random variables and tying them to the sentence-
level variables, Zi, provides a direct method for
modeling weak supervision. We can simply train the
model so that the Y variables match the facts in the
database, treating the Zi as hidden variables that can
take any value, as long as they produce the correct
aggregate predictions.
This approach is related to the multi-instance
learning approach of Riedel et al (2010), in that
both models include sentence-level and aggregate
random variables. However, their sentence level
variables are binary and they only have a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. Addition-
ally, their aggregate decisions make use of Mintz-
style aggregate features (Mintz et al, 2009), that col-
lect evidence from multiple sentences, while we use
543
Inputs:
(1) ?, a set of sentences,
(2)E, a set of entities mentioned in the sentences,
(3) R, a set of relation names, and
(4) ?, a database of atomic facts of the form
r(e1, e2) for r ? R and ei ? E.
Definitions:
We define the training set {(xi,yi)|i = 1 . . . n},
where i is an index corresponding to a particu-
lar entity pair (ej , ek) in ?, xi contains all of
the sentences in ? with mentions of this pair, and
yi = relVector(ej , ek).
Computation:
initialize parameter vector ?? 0
for t = 1...T do
for i = 1...n do
(y?, z?)? arg maxy,z p(y, z|xi; ?)
if y? 6= yi then
z? ? arg maxz p(z|xi,yi; ?)
?? ? + ?(xi, z?)? ?(xi, z?)
end if
end for
end for
Return ?
Figure 2: The MULTIR Learning Algorithm
only the deterministic OR nodes. Perhaps surpris-
ing, we are still able to improve performance at both
the sentential and aggregate extraction tasks.
4 Learning
We now present a multi-instance learning algo-
rithm for our weak-supervision model that treats the
sentence-level extraction random variables Zi as la-
tent, and uses facts from a database (e.g., Freebase)
as supervision for the aggregate-level variables Y r.
As input we have (1) ?, a set of sentences, (2)
E, a set of entities mentioned in the sentences, (3)
R, a set of relation names, and (4) ?, a database
of atomic facts of the form r(e1, e2) for r ? R and
ei ? E. Since we are using weak learning, the Y r
variables in Y are not directly observed, but can be
approximated from the database ?. We use a proce-
dure, relVector(e1, e2) to return a bit vector whose
jth bit is one if rj(e1, e2) ? ?. The vector does not
have a bit for the special none relation; if there is no
relation between the two entities, all bits are zero.
Finally, we can now define the training set to be
pairs {(xi,yi)|i = 1 . . . n}, where i is an index
corresponding to a particular entity pair (ej , ek), xi
contains all of the sentences with mentions of this
pair, and yi = relVector(ej , ek).
Given this form of supervision, we would like to
find the setting for ? with the highest likelihood:
O(?) =
?
i
p(yi|xi; ?) =
?
i
?
z
p(yi, z|xi; ?)
However, this objective would be difficult to op-
timize exactly, and algorithms for doing so would
be unlikely to scale to data sets of the size we con-
sider. Instead, we make two approximations, de-
scribed below, leading to a Perceptron-style addi-
tive (Collins, 2002) parameter update scheme which
has been modified to reason about hidden variables,
similar in style to the approaches of (Liang et al,
2006; Zettlemoyer and Collins, 2007), but adapted
for our specific model. This approximate algorithm
is computationally efficient and, as we will see,
works well in practice.
Our first modification is to do online learning
instead of optimizing the full objective. Define the
feature sums ?(x, z) =
?
j ?(xj , zj) which range
over the sentences, as indexed by j. Now, we can
define an update based on the gradient of the local
log likelihood for example i:
? logOi(?)
??j
= Ep(z|xi,yi;?)[?j(xi, z)]
?Ep(y,z|xi;?)[?j(xi, z)]
where the deterministic OR ?join factors ensure that
the first expectation assigns positive probability only
to assignments that produce the labeled facts yi but
that the second considers all valid sets of extractions.
Of course, these expectations themselves, espe-
cially the second one, would be difficult to com-
pute exactly. Our second modification is to do
a Viterbi approximation, by replacing the expecta-
tions with maximizations. Specifically, we compute
the most likely sentence extractions for the label
facts arg maxz p(z|xi,yi; ?) and the most likely ex-
traction for the input, without regard to the labels,
arg maxy,z p(y, z|xi; ?). We then compute the fea-
tures for these assignments and do a simple additive
update. The final algorithm is detailed in Figure 2.
544
5 Inference
To support learning, as described above, we need
to compute assignments arg maxz p(z|x,y; ?) and
arg maxy,z p(y, z|x; ?). In this section, we describe
algorithms for both cases that use the deterministic
OR nodes to simplify the required computations.
Predicting the most likely joint extraction
arg maxy,z p(y, z|x; ?) can be done efficiently
given the structure of our model. In particular, we
note that the factors ?join represent deterministic de-
pendencies between Z and Y, which when satisfied
do not affect the probability of the solution. It is thus
sufficient to independently compute an assignment
for each sentence-level extraction variable Zi, ignor-
ing the deterministic dependencies. The optimal set-
ting for the aggregate variables Y is then simply the
assignment that is consistent with these extractions.
The time complexity is O(|R| ? |S|).
Predicting sentence level extractions given weak
supervision facts, arg maxz p(z|x,y; ?), is more
challenging. We start by computing extraction
scores ?extract(xi, zi) for each possible extraction as-
signment Zi = zi at each sentence xi ? S, and
storing the values in a dynamic programming table.
Next, we must find the most likely assignment z that
respects our output variables y. It turns out that
this problem is a variant of the weighted, edge-cover
problem, for which there exist polynomial time op-
timal solutions.
Let G = (E ,V = VS ? Vy) be a complete
weighted bipartite graph with one node vSi ? V
S for
each sentence xi ? S and one node v
y
r ? Vy for each
relation r ? R where yr = 1. The edge weights are
given by c((vSi , v
y
r ))
def
= ?extract(xi, zi). Our goal is
to select a subset of the edges which maximizes the
sum of their weights, subject to each node vSi ? V
S
being incident to exactly one edge, and each node
vyr ? Vy being incident to at least one edge.
Exact Solution An exact solution can be obtained
by first computing the maximum weighted bipartite
matching, and adding edges to nodes which are not
incident to an edge. This can be computed in time
O(|V|(|E| + |V| log |V|)), which we can rewrite as
O((|R|+ |S|)(|R||S|+ (|R|+ |S|) log(|R|+ |S|))).
Approximate Solution An approximate solution
can be obtained by iterating over the nodes in Vy,
????????  ???????????  
?? ?? ?? 
????????????? ???????????????? 
????? 
Figure 3: Inference of arg maxz p(Z = z|x,y) requires
solving a weighted, edge-cover problem.
and each time adding the highest weight incident
edge whose addition doesn?t violate a constraint.
The running time is O(|R||S|). This greedy search
guarantees each fact is extracted at least once and
allows any additional extractions that increase the
overall probability of the assignment. Given the
computational advantage, we use it in all of the ex-
perimental evaluations.
6 Experimental Setup
We follow the approach of Riedel et al (2010) for
generating weak supervision data, computing fea-
tures, and evaluating aggregate extraction. We also
introduce new metrics for measuring sentential ex-
traction performance, both relation-independent and
relation-specific.
6.1 Data Generation
We used the same data sets as Riedel et al (2010)
for weak supervision. The data was first tagged with
the Stanford NER system (Finkel et al, 2005) and
then entity mentions were found by collecting each
continuous phrase where words were tagged iden-
tically (i.e., as a person, location, or organization).
Finally, these phrases were matched to the names of
Freebase entities.
Given the set of matches, define ? to be set of NY
Times sentences with two matched phrases, E to be
the set of Freebase entities which were mentioned in
one or more sentences, ? to be the set of Freebase
facts whose arguments, e1 and e2 were mentioned in
a sentence in ?, and R to be set of relations names
used in the facts of ?. These sets define the weak
supervision data.
6.2 Features and Initialization
We use the set of sentence-level features described
by Riedel et al (2010), which were originally de-
545
veloped by Mintz et al (2009). These include in-
dicators for various lexical, part of speech, named
entity, and dependency tree path properties of entity
mentions in specific sentences, as computed with the
Malt dependency parser (Nivre and Nilsson, 2004)
and OpenNLP POS tagger1. However, unlike the
previous work, we did not make use of any features
that explicitly aggregate these properties across mul-
tiple mention instances.
The MULTIR algorithm has a single parameter T ,
the number of training iterations, that must be spec-
ified manually. We used T = 50 iterations, which
performed best in development experiments.
6.3 Evaluation Metrics
Evaluation is challenging, since only a small per-
centage (approximately 3%) of sentences match
facts in Freebase, and the number of matches is
highly unbalanced across relations, as we will see
in more detail later. We use the following metrics.
Aggregate Extraction Let ?e be the set of ex-
tracted relations for any of the systems; we com-
pute aggregate precision and recall by comparing
?e with ?. This metric is easily computed but un-
derestimates extraction accuracy because Freebase
is incomplete and some true relations in ?e will be
marked wrong.
Sentential Extraction Let Se be the sentences
where some system extracted a relation and SF be
the sentences that match the arguments of a fact in
?. We manually compute sentential extraction ac-
curacy by sampling a set of 1000 sentences from
Se ? SF and manually labeling the correct extrac-
tion decision, either a relation r ? R or none. We
then report precision and recall for each system on
this set of sampled sentences. These results provide
a good approximation to the true precision but can
overestimate the actual recall, since we did not man-
ually check the much larger set of sentences where
no approach predicted extractions.
6.4 Precision / Recall Curves
To compute precision / recall curves for the tasks,
we ranked the MULTIR extractions as follows. For
sentence-level evaluations, we ordered according to
1http://opennlp.sourceforge.net/
Recall
Precision
0.00 0.05 0.10 0.15 0.20 0.25 0.30
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
Riedel et al, 2010
MULTIR
Figure 4: Aggregate extraction precision / recall curves
for Riedel et al (2010), a reimplementation of that ap-
proach (SOLOR), and our algorithm (MULTIR).
the extraction factor score ?extract(zi, xi). For aggre-
gate comparisons, we set the score for an extraction
Y r = true to be the max of the extraction factor
scores for the sentences where r was extracted.
7 Experiments
To evaluate our algorithm, we first compare it to an
existing approach for using multi-instance learning
with weak supervision (Riedel et al, 2010), using
the same data and features. We report both aggregate
extraction and sentential extraction results. We then
investigate relation-specific performance of our sys-
tem. Finally, we report running time comparisons.
7.1 Aggregate Extraction
Figure 4 shows approximate precision / recall curves
for three systems computed with aggregate metrics
(Section 6.3) that test how closely the extractions
match the facts in Freebase. The systems include the
original results reported by Riedel et al (2010) as
well as our new model (MULTIR). We also compare
with SOLOR, a reimplementation of their algorithm,
which we built in Factorie (McCallum et al, 2009),
and will use later to evaluate sentential extraction.
MULTIR achieves competitive or higher preci-
sion over all ranges of recall, with the exception
of the very low recall range of approximately 0-
1%. It also significantly extends the highest recall
achieved, from 20% to 25%, with little loss in preci-
sion. To investigate the low precision in the 0-1% re-
call range, we manually checked the ten highest con-
546
Recall
Precision
0.0 0.1 0.2 0.3 0.4 0.5 0.6
0.0
0.2
0.4
0.6
0.8
1.0
SOLOR
MULTIR
Figure 5: Sentential extraction precision / recall curves
for MULTIR and SOLOR.
fidence extractions produced by MULTIR that were
marked wrong. We found that all ten were true facts
that were simply missing from Freebase. A manual
evaluation, as we perform next for sentential extrac-
tion, would remove this dip.
7.2 Sentential Extraction
Although their model includes variables to model
sentential extraction, Riedel et al (2010) did not re-
port sentence level performance. To generate the
precision / recall curve we used the joint model as-
signment score for each of the sentences that con-
tributed to the aggregate extraction decision.
Figure 4 shows approximate precision / recall
curves for MULTIR and SOLOR computed against
manually generated sentence labels, as defined in
Section 6.3. MULTIR achieves significantly higher
recall with a consistently high level of precision. At
the highest recall point, MULTIR reaches 72.4% pre-
cision and 51.9% recall, for an F1 score of 60.5%.
7.3 Relation-Specific Performance
Since the data contains an unbalanced number of in-
stances of each relation, we also report precision and
recall for each of the ten most frequent relations. Let
SMr be the sentences where MULTIR extracted an
instance of relation r ? R, and let SFr be the sen-
tences that match the arguments of a fact about re-
lation r in ?. For each r, we sample 100 sentences
from both SMr and S
F
r and manually check accu-
racy. To estimate precision P?r we compute the ratio
of true relation mentions in SMr , and to estimate re-
call R?r we take the ratio of true relation mentions in
SFr which are returned by our system.
Table 1 presents this approximate precision and
recall for MULTIR on each of the relations, along
with statistics we computed to measure the qual-
ity of the weak supervision. Precision is high for
the majority of relations but recall is consistently
lower. We also see that the Freebase matches are
highly skewed in quantity and can be low quality for
some relations, with very few of them actually cor-
responding to true extractions. The approach gener-
ally performs best on the relations with a sufficiently
large number of true matches, in many cases even
achieving precision that outperforms the accuracy of
the heuristic matches, at reasonable recall levels.
7.4 Overlapping Relations
Table 1 also highlights some of the effects of learn-
ing with overlapping relations. For example, in the
data, almost all of the matches for the administra-
tive divisions relation overlap with the contains re-
lation, because they both model relationships for a
pair of locations. Since, in general, sentences are
much more likely to describe a contains relation, this
overlap leads to a situation were almost none of the
administrate division matches are true ones, and we
cannot accurately learn an extractor. However, we
can still learn to accurately extract the contains rela-
tion, despite the distracting matches. Similarly, the
place of birth and place of death relations tend to
overlap, since it is often the case that people are born
and die in the same city. In both cases, the precision
outperforms the labeling accuracy and the recall is
relatively high.
To measure the impact of modeling overlapping
relations, we also evaluated a simple, restricted
baseline. Instead of labeling each entity pair with
the set of all true Freebase facts, we created a dataset
where each true relation was used to create a dif-
ferent training example. Training MULTIR on this
data simulates effects of conflicting supervision that
can come from not modeling overlaps. On average
across relations, precision increases 12 points but re-
call drops 26 points, for an overall reduction in F1
score from 60.5% to 40.3%.
7.5 Running Time
One final advantage of our model is the mod-
est running time. Our implementation of the
547
Relation
Freebase Matches MULTIR
#sents % true P? R?
/business/person/company 302 89.0 100.0 25.8
/people/person/place lived 450 60.0 80.0 6.7
/location/location/contains 2793 51.0 100.0 56.0
/business/company/founders 95 48.4 71.4 10.9
/people/person/nationality 723 41.0 85.7 15.0
/location/neighborhood/neighborhood of 68 39.7 100.0 11.1
/people/person/children 30 80.0 100.0 8.3
/people/deceased person/place of death 68 22.1 100.0 20.0
/people/person/place of birth 162 12.0 100.0 33.0
/location/country/administrative divisions 424 0.2 N/A 0.0
Table 1: Estimated precision and recall by relation, as well as the number of matched sentences (#sents) and accuracy
(% true) of matches between sentences and facts in Freebase.
Riedel et al (2010) approach required approxi-
mately 6 hours to train on NY Times 05-06 and 4
hours to test on the NY Times 07, each without pre-
processing. Although they do sampling for infer-
ence, the global aggregation variables require rea-
soning about an exponentially large (in the number
of sentences) sample space.
In contrast, our approach required approximately
one minute to train and less than one second to test,
on the same data. This advantage comes from the
decomposition that is possible with the determinis-
tic OR aggregation variables. For test, we simply
consider each sentence in isolation and during train-
ing our approximation to the weighted assignment
problem is linear in the number of sentences.
7.6 Discussion
The sentential extraction results demonstrates the
advantages of learning a model that is primarily
driven by sentence-level features. Although previ-
ous approaches have used more sophisticated fea-
tures for aggregating the evidence from individual
sentences, we demonstrate that aggregating strong
sentence-level evidence with a simple deterministic
OR that models overlapping relations is more effec-
tive, and also enables training of a sentence extractor
that runs with no aggregate information.
While the Riedel et al approach does include a
model of which sentences express relations, it makes
significant use of aggregate features that are primar-
ily designed to do entity-level relation predictions
and has a less detailed model of extractions at the
individual sentence level. Perhaps surprisingly, our
model is able to do better at both the sentential and
aggregate levels.
8 Related Work
Supervised-learning approaches to IE were intro-
duced in (Soderland et al, 1995) and are too nu-
merous to summarize here. While they offer high
precision and recall, these methods are unlikely to
scale to the thousands of relations found in text on
the Web. Open IE systems, which perform self-
supervised learning of relation-independent extrac-
tors (e.g., Preemptive IE (Shinyama and Sekine,
2006), TEXTRUNNER (Banko et al, 2007; Banko
and Etzioni, 2008) and WOE (Wu and Weld, 2010))
can scale to millions of documents, but don?t output
canonicalized relations.
8.1 Weak Supervision
Weak supervision (also known as distant- or self su-
pervision) refers to a broad class of methods, but
we focus on the increasingly-popular idea of using
a store of structured data to heuristicaly label a tex-
tual corpus. Craven and Kumlien (1999) introduced
the idea by matching the Yeast Protein Database
(YPD) to the abstracts of papers in PubMed and
training a naive-Bayes extractor. Bellare and Mc-
Callum (2007) used a database of BibTex records
to train a CRF extractor on 12 bibliographic rela-
tions. The KYLIN system aplied weak supervision
to learn relations from Wikipedia, treating infoboxes
as the associated database (Wu and Weld, 2007);
Wu et al (2008) extended the system to use smooth-
ing over an automatically generated infobox taxon-
548
omy. Mintz et al (2009) used Freebase facts to train
100 relational extractors on Wikipedia. Hoffmann
et al (2010) describe a system similar to KYLIN,
but which dynamically generates lexicons in order
to handle sparse data, learning over 5000 Infobox
relations with an average F1 score of 61%. Yao
et al (2010) perform weak supervision, while using
selectional preference constraints to a jointly reason
about entity types.
The NELL system (Carlson et al, 2010) can also
be viewed as performing weak supervision. Its ini-
tial knowledge consists of a selectional preference
constraint and 20 ground fact seeds. NELL then
matches entity pairs from the seeds to a Web cor-
pus, but instead of learning a probabilistic model,
it bootstraps a set of extraction patterns using semi-
supervised methods for multitask learning.
8.2 Multi-Instance Learning
Multi-instance learning was introduced in order to
combat the problem of ambiguously-labeled train-
ing data when predicting the activity of differ-
ent drugs (Dietterich et al, 1997). Bunescu and
Mooney (2007) connect weak supervision with
multi-instance learning and extend their relational
extraction kernel to this context.
Riedel et al (2010), combine weak supervision
and multi-instance learning in a more sophisticated
manner, training a graphical model, which assumes
only that at least one of the matches between the
arguments of a Freebase fact and sentences in the
corpus is a true relational mention. Our model may
be seen as an extension of theirs, since both models
include sentence-level and aggregate random vari-
ables. However, Riedel et al have only a single ag-
gregate variable that takes values r ? R ? {none},
thereby ruling out overlapping relations. We have
discussed the comparison in more detail throughout
the paper, including in the model formulation sec-
tion and experiments.
9 Conclusion
We argue that weak supervision is promising method
for scaling information extraction to the level where
it can handle the myriad, different relations on the
Web. By using the contents of a database to heuris-
tically label a training corpus, we may be able to
automatically learn a nearly unbounded number of
relational extractors. Since the processs of match-
ing database tuples to sentences is inherently heuris-
tic, researchers have proposed multi-instance learn-
ing algorithms as a means for coping with the result-
ing noisy data. Unfortunately, previous approaches
assume that all relations are disjoint ? for exam-
ple they cannot extract the pair Founded(Jobs,
Apple) and CEO-of(Jobs, Apple), because
two relations are not allowed to have the same argu-
ments.
This paper presents a novel approach for multi-
instance learning with overlapping relations that
combines a sentence-level extraction model with a
simple, corpus-level component for aggregating the
individual facts. We apply our model to learn extrac-
tors for NY Times text using weak supervision from
Freebase. Experiments show improvements for both
sentential and aggregate (corpus level) extraction,
and demonstrate that the approach is computation-
ally efficient.
Our early progress suggests many interesting di-
rections. By joining two or more Freebase tables,
we can generate many more matches and learn more
relations. We also wish to refine our model in order
to improve precision. For example, we would like
to add type reasoning about entities and selectional
preference constraints for relations. Finally, we are
also interested in applying the overall learning ap-
proaches to other tasks that could be modeled with
weak supervision, such as coreference and named
entity classification.
The source code of our system, its out-
put, and all data annotations are available at
http://cs.uw.edu/homes/raphaelh/mr.
Acknowledgments
We thank Sebastian Riedel and Limin Yao for shar-
ing their data and providing valuable advice. This
material is based upon work supported by a WRF /
TJ Cable Professorship, a gift from Google and by
the Air Force Research Laboratory (AFRL) under
prime contract no. FA8750-09-C-0181. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the author(s)
and do not necessarily reflect the view of the Air
Force Research Laboratory (AFRL).
549
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of 46th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-08), pages
28?36.
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In Proceedings
of the 20th International Joint Conference on Artificial
Intelligence (IJCAI-07), pages 2670?2676.
Kedar Bellare and Andrew McCallum. 2007. Learn-
ing extractors from unlabeled text using relevant
databases. In Sixth International Workshop on Infor-
mation Integration on the Web.
Razvan Bunescu and Raymond Mooney. 2007. Learning
to extract relations from the web using minimal super-
vision. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
07).
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In Proceedings of the AAAI
Conference on Artificial Intelligence (AAAI-10).
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the 2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP-2002).
Mark Craven and Johan Kumlien. 1999. Constructing
biological knowledge bases by extracting information
from text sources. In Proceedings of the Seventh Inter-
national Conference on Intelligent Systems for Molec-
ular Biology, pages 77?86.
Thomas G. Dietterich, Richard H. Lathrop, and Toma?s
Lozano-Pe?rez. 1997. Solving the multiple instance
problem with axis-parallel rectangles. Artificial Intel-
ligence, 89:31?71, January.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics (ACL-
05), pages 363?370.
Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.
2010. Learning 5000 relational extractors. In Pro-
ceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL-10), pages
286?295.
Percy Liang, A. Bouchard-Co?te?, Dan Klein, and Ben
Taskar. 2006. An end-to-end discriminative approach
to machine translation. In International Conference on
Computational Linguistics and Association for Com-
putational Linguistics (COLING/ACL).
Andrew McCallum, Karl Schultz, and Sameer Singh.
2009. Factorie: Probabilistic programming via imper-
atively defined factor graphs. In Neural Information
Processing Systems Conference (NIPS).
Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-
sky. 2009. Distant supervision for relation extrac-
tion without labeled data. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics (ACL-2009), pages 1003?1011.
Joakim Nivre and Jens Nilsson. 2004. Memory-based
dependency parsing. In Proceedings of the Conference
on Natural Language Learning (CoNLL-04), pages
49?56.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions without
labeled text. In Proceedings of the Sixteenth Euro-
pean Conference on Machine Learning (ECML-2010),
pages 148?163.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the North American Chapter
of the Association for Computation Linguistics (HLT-
NAACL-06).
Stephen Soderland, David Fisher, Jonathan Aseltine, and
Wendy G. Lehnert. 1995. Crystal: Inducing a concep-
tual dictionary. In Proceedings of the Fourteenth In-
ternational Joint Conference on Artificial Intelligence
(IJCAI-1995), pages 1314?1321.
Fei Wu and Daniel S. Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the Inter-
national Conference on Information and Knowledge
Management (CIKM-2007), pages 41?50.
Fei Wu and Daniel S. Weld. 2008. Automatically refin-
ing the wikipedia infobox ontology. In Proceedings of
the 17th International Conference on World Wide Web
(WWW-2008), pages 635?644.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In The Annual Meeting of
the Association for Computational Linguistics (ACL-
2010), pages 118?127.
Limin Yao, Sebastian Riedel, and Andrew McCallum.
2010. Collective cross-document relation extraction
without labelled data. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-2010), pages 1013?1023.
Luke Zettlemoyer and Michael Collins. 2007. Online
learning of relaxed CCG grammars for parsing to log-
ical form. In Proceedings of the Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL-2007).
550
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Adaptive Parser-Centric Text Normalization
Congle Zhang?
Dept of Computer Science and Engineering
University of Washington, Seattle, WA 98195, USA
clzhang@cs.washington.edu
Tyler Baldwin Howard Ho Benny Kimelfeld Yunyao Li
IBM Research - Almaden
650 Harry Road, San Jose, CA 95120, USA
{tbaldwi,ctho,kimelfeld,yunyaoli}@us.ibm.com
Abstract
Text normalization is an important first
step towards enabling many Natural Lan-
guage Processing (NLP) tasks over infor-
mal text. While many of these tasks, such
as parsing, perform the best over fully
grammatically correct text, most existing
text normalization approaches narrowly
define the task in the word-to-word sense;
that is, the task is seen as that of mapping
all out-of-vocabulary non-standard words
to their in-vocabulary standard forms. In
this paper, we take a parser-centric view
of normalization that aims to convert raw
informal text into grammatically correct
text. To understand the real effect of nor-
malization on the parser, we tie normal-
ization performance directly to parser per-
formance. Additionally, we design a cus-
tomizable framework to address the often
overlooked concept of domain adaptabil-
ity, and illustrate that the system allows for
transfer to new domains with a minimal
amount of data and effort. Our experimen-
tal study over datasets from three domains
demonstrates that our approach outper-
forms not only the state-of-the-art word-
to-word normalization techniques, but also
manual word-to-word annotations.
1 Introduction
Text normalization is the task of transforming in-
formal writing into its standard form in the lan-
guage. It is an important processing step for a
wide range of Natural Language Processing (NLP)
tasks such as text-to-speech synthesis, speech
recognition, information extraction, parsing, and
machine translation (Sproat et al, 2001).
?This work was conducted at IBM.
The use of normalization in these applications
poses multiple challenges. First, as it is most often
conceptualized, normalization is seen as the task
of mapping all out-of-vocabulary non-standard
word tokens to their in-vocabulary standard forms.
However, the scope of the task can also be seen as
much wider, encompassing whatever actions are
required to convert the raw text into a fully gram-
matical sentence. This broader definition of the
normalization task may include modifying punc-
tuation and capitalization, and adding, removing,
or reordering words. Second, as with other NLP
techniques, normalization approaches are often fo-
cused on one primary domain of interest (e.g.,
Twitter data). Because the style of informal writ-
ing may be different in different data sources,
tailoring an approach towards a particular data
source can improve performance in the desired do-
main. However, this is often done at the cost of
adaptability.
This work introduces a customizable normal-
ization approach designed with domain transfer in
mind. In short, customization is done by provid-
ing the normalizer with replacement generators,
which we define in Section 3. We show that the
introduction of a small set of domain-specific gen-
erators and training data allows our model to out-
perform a set of competitive baselines, including
state-of-the-art word-to-word normalization. Ad-
ditionally, the flexibility of the model also allows it
to attempt to produce fully grammatical sentences,
something not typically handled by word-to-word
normalization approaches.
Another potential problem with state-of-the-art
normalization is the lack of appropriate evaluation
metrics. The normalization task is most frequently
motivated by pointing to the need for clean text
for downstream processing applications, such as
syntactic parsing. However, most studies of nor-
malization give little insight into whether and to
what degree the normalization process improves
1159
the performance of the downstream application.
For instance, it is unclear how performance mea-
sured by the typical normalization evaluation met-
rics of word error rate and BLEU score (Pap-
ineni et al, 2002) translates into performance on
a parsing task, where a well placed punctuation
mark may provide more substantial improvements
than changing a non-standard word form. To ad-
dress this problem, this work introduces an eval-
uation metric that ties normalization performance
directly to the performance of a downstream de-
pendency parser.
The rest of this paper is organized as follows.
In Section 2 we discuss previous approaches to
the normalization problem. Section 3 presents
our normalization framework, including the actual
normalization and learning procedures. Our in-
stantiation of this model is presented in Section 4.
In Section 5 we introduce the parser driven eval-
uation metric, and present experimental results of
our model with respect to several baselines in three
different domains. Finally, we discuss our exper-
imental study in Section 6 and conclude in Sec-
tion 7.
2 Related Work
Sproat et al (2001) took the first major look at
the normalization problem, citing the need for nor-
malized text for downstream applications. Unlike
later works that would primarily focus on specific
noisy data sets, their work is notable for attempt-
ing to develop normalization as a general process
that could be applied to different domains. The re-
cent rise of heavily informal writing styles such as
Twitter and SMS messages set off a new round of
interest in the normalization problem.
Research on SMS and Twitter normalization has
been roughly categorized as drawing inspiration
from three other areas of NLP (Kobus et al, 2008):
machine translation, spell checking, and automatic
speech recognition. The statistical machine trans-
lation (SMT) metaphor was the first proposed to
handle the text normalization problem (Aw et al,
2006). In this mindset, normalizing SMS can be
seen as a translation task from a source language
(informal) to a target language (formal), which can
be undertaken with typical noisy channel based
models. Work by Choudhury et al (2007) adopted
the spell checking metaphor, casting the problem
in terms of character-level, rather than word-level,
edits. They proposed an HMM based model that
takes into account both grapheme and phoneme
information. Kobus et al (2008) undertook a
hybrid approach that pulls inspiration from both
the machine translation and speech recognition
metaphors.
Many other approaches have been examined,
most of which are at least partially reliant on
the above three metaphors. Cook and Steven-
son (2009) perform an unsupervised method,
again based on the noisy channel model. Pen-
nell and Liu (2011) developed a CRF tagger for
deletion-based abbreviation on tweets. Xue et
al. (2011) incorporated orthographic, phonetic,
contextual, and acronym expansion factors to nor-
malize words in both Twitter and SMS. Liu et
al. (2011) modeled the generation process from
dictionary words to non-standard tokens under an
unsupervised sequence labeling framework. Han
and Baldwin (2011) use a classifier to detect ill-
formed words, and then generate correction can-
didates based on morphophonemic similarity. Re-
cent work has looked at the construction of nor-
malization dictionaries (Han et al, 2012) and on
improving coverage by integrating different hu-
man perspectives (Liu et al, 2012).
Although it is almost universally used as a mo-
tivating factor, most normalization work does not
directly focus on improving downstream appli-
cations. While a few notable exceptions high-
light the need for normalization as part of text-
to-speech systems (Beaufort et al, 2010; Pennell
and Liu, 2010), these works do not give any di-
rect insight into how much the normalization pro-
cess actually improves the performance of these
systems. To our knowledge, the work presented
here is the first to clearly link the output of a nor-
malization system to the output of the downstream
application. Similarly, our work is the first to pri-
oritize domain adaptation during the new wave of
text message normalization.
3 Model
In this section we introduce our normalization
framework, which draws inspiration from our pre-
vious work on spelling correction for search (Bao
et al, 2011).
3.1 Replacement Generators
Our input the original, unnormalized text, repre-
sented as a sequence x = x1, x2, . . . , xn of tokens
xi. In this section we will use the following se-
1160
quence as our running example:
x = Ay1 woudent2 of3 see4 ?em5
where space replaces comma for readability, and
each token is subscripted by its position. Given the
input x, we apply a series of replacement genera-
tors, where a replacement generator is a function
that takes x as input and produces a collection of
replacements. Here, a replacement is a statement
of the form ?replace tokens xi, . . . , xj?1 with s.?
More precisely, a replacement is a triple ?i, j, s?,
where 1 ? i ? j ? n + 1 and s is a sequence of
tokens. Note that in the case where i = j, the se-
quence s should be inserted right before xi; and in
the special case where s is empty, we simply delete
xi, . . . , xj?1. For instance, in our running exam-
ple the replacement ?2, 3,would not? replaces
x2 = woudent with would not; ?1, 2,Ay? re-
places x1 with itself (hence, does not change x);
?1, 2, ? (where  is the empty sequence) deletes
x1; ?6, 6,.? inserts a period at the end of the se-
quence.
The provided replacement generators can be ei-
ther generic (cross domain) or domain-specific, al-
lowing for domain customization. In Section 4,
we discuss the replacement generators used in our
empirical study.
3.2 Normalization Graph
Given the input x and the set of replacements pro-
duced by our generators, we associate a unique
Boolean variable Xr with each replacement r. As
expected, Xr being true means that the replace-
ment r takes place in producing the output se-
quence.
Next, we introduce dependencies among vari-
ables. We first discuss the syntactic consistency
of truth assignments. Let r1 = ?i1, j1, s1? and
r2 = ?i2, j2, s2? be two replacements. We say
that r1 and r2 are locally consistent if the inter-
vals [i1, j1) and [i2, j2) are disjoint. Moreover,
we do not allow two insertions to take place at
the same position; therefore, we exclude [i1, j1)
and [i2, j2) from the definition of local consistency
when i1 = j1 = i2 = j2. If r1 and r2 are locally
consistent and j1 = i2, then we say that r2 is a
consistent follower of r1.
A truth assignment ? to our variables Xr is
sound if every two replacements r and r? with
?(Xr) = ?(Xr?) = true are locally consis-
tent. We say that ? is complete if every token
of x is captured by at least one replacement r
with ?(Xr) = true. Finally, we say that ?
is legal if it is sound and complete. The out-
put (normalized sequence) defined by a legal as-
signment ? is, naturally, the concatenation (from
left to right) of the strings s in the replacements
r = ?i, j, s? with ?(Xr) = true. In Fig-
ure 1, for example, if the nodes with a grey
shade are the ones associated with true vari-
ables under ?, then the output defined by ? is
I would not have seen them.
Our variables carry two types of interdependen-
cies. The first is that of syntactic consistency: the
entire assignment is required to be legal. The sec-
ond captures correlation among replacements. For
instance, if we replace of with have in our run-
ning example, then the next see token is more
likely to be replaced with seen. In this work,
dependencies of the second type are restricted to
pairs of variables, where each pair corresponds to
a replacement and a consistent follower thereof.
The above dependencies can be modeled over a
standard undirected graph using Conditional Ran-
dom Fields (Lafferty et al, 2001). However, the
graph would be complex: in order to model lo-
cal consistency, there should be edges between ev-
ery two nodes that violate local consistency. Such
a model renders inference and learning infeasi-
ble. Therefore, we propose a clearer model by a
directed graph, as illustrated in Figure 1 (where
nodes are represented by replacements r instead
of the variables Xr, for readability). To incorpo-
rate correlation among replacements, we introduce
an edge from Xr to Xr? whenever r? is a consis-
tent follower of r. Moreover, we introduce two
dummy nodes, start and end, with an edge from
start to each variable that corresponds to a prefix
of the input sequence x, and an edge from each
variable that corresponds to a suffix of x to end.
The principal advantage of modeling the depen-
dencies in such a directed graph is that now, the le-
gal assignments are in one-to-one correspondence
with the paths from start to end; this is a straight-
forward observation that we do not prove here.
We appeal to the log-linear model formulation
to define the probability of an assignment. The
conditional probability of an assignment ?, given
an input sequence x and the weight vector ? =
??1, . . . , ?k? for our features, is defined as p(? |
1161
?1, 2,I?
end
?2, 4,would not have?
?1, 2,Ay?
?5, 6,them?
?4, 5,seen?
?2, 3,would?
?4, 6,see him?
?3, 4,of?
start
?6, 6, .?
Figure 1: Example of a normalization graph; the
nodes are replacements generated by the replace-
ment generators, and every path from start to end
implies a legal assignment
x,?) = 0 if ? is not legal, and otherwise,
p(? | x,?) = 1Z(x)
?
X?Y ??
exp(
?
j
?j?j(X,Y,x)) .
Here, Z(x) is the partition function, X ? Y ? ?
refers to an edge X ? Y with ?(X) = true and
?(Y ) = true, and ?1(X,Y,x), . . . , ?k(X,Y,x)
are real valued feature functions that are weighted
by ?1, . . . , ?k (the model?s parameters), respec-
tively.
3.3 Inference
When performing inference, we wish to select
the output sequence with the highest probability,
given the input sequence x and the weight vector
? (i.e., MAP inference). Specifically, we want an
assignment ?? = arg max? p(? | x,?).
While exact inference is computationally hard
on general graph models, in our model it boils
down to finding the longest path in a weighted
and acyclic directed graph. Indeed, our directed
graph (illustrated in Figure 1) is acyclic. We as-
sign the real value ?j ?j?j(X,Y,x) to the edge
X ? Y , as the weight. As stated in Section 3.2,
a legal assignment ? corresponds to a path from
start to end; moreover, the sum of the weights on
that path is equal to log p(? | x,?) + logZ(x).
In particular, a longer path corresponds to an as-
signment with greater probability. Therefore, we
can solve the MAP inference within our model by
finding the weighted longest path in the directed
acyclic graph. The algorithm in Figure 2 summa-
rizes the inference procedure to normalize the in-
put sequence x.
Input:
1. A sequence x to normalize;
2. A weight vector ? = ??1, . . . , ?k?.
Generate replacements: Apply all replace-
ment generators to get a set of replacements r,
each r is a triple ?i, j, s?.
Build a normalization graph:
1. For each replacement r, create a node Xr.
2. For each r? and r, create an edge Xr to
Xr? if r? is a consistent follower of r.
3. Create two dummy nodes start and end,
and create edges from start to all prefix
nodes and end to all suffix nodes.
4. For each edge X ? Y , compute the fea-
tures ?j(X,Y,x), and weight the edge by?
j ?j?j(X,Y,x).
MAP Inference: Find a weighted longest path
P from start to end, and return ??, where
??(Xr) = true iff Xr ? P .
Figure 2: Normalization algorithm
3.4 Learning
Our labeled data consists of pairs (xi,ygoldi ),
where xi is an input sequence (to normalize) and
ygoldi is a (manually) normalized sequence. We
obtain a truth assignment ?goldi from each ygoldi
by selecting an assignment ? that minimizes the
edit distance between ygoldi and the normalized
text implied by ?:
?goldi = arg min? DIST(y(?),y
gold
i ) (1)
Here, y(?) denotes the normalized text implied by
?, and DIST is a token-level edit distance. We
apply a simple dynamic-programming algorithm
to compute ?goldi . Finally, the items in our training
data are the pairs (xi, ?goldi ).
Learning over similar models is commonly
done via maximum likelihood estimation:
L(?) = log
?
i
p(?i = ?goldi | xi,?)
Taking the partial derivative gives the following:
?
i
(
?j(?goldi ,xi)? Ep(?i|xi,?)?j(?i,xi)
)
where ?j(?,x) = ?X?Y ?j(X,Y,x), that is,
the sum of values for the jth feature along the
1162
Input:
1. A set {(xi,ygoldi )}
n
i=1 of sequences andtheir gold normalization;
2. Number T of iterations.
Initialization: Initialize each ?j as zero, and
obtain each ?goldi according to (1).
Repeat T times:
1. Infer each ??i from xi using the current ?;
2. ?j ? ?j+?i(?j(?goldi ,xi)??j(??i ,xi))
for all j = 1, . . . , k.
Output: ? = ??1, . . . , ?k?
Figure 3: Learning algorithm
path defined by ?, andEp(?i|xi,?)?j(?i,xi) is the
expected value of that sum (over all legal assign-
ments ?i), assuming the current weight vector.
How to efficiently compute
Ep(?i|xi,?)?j(?i,xi) in our model is un-
clear; naively, it requires enumerating all legal
assignments. We instead opt to use a more
tractable perceptron-style algorithm (Collins,
2002). Instead of computing the expectation,
we simply compute ?j(??i ,xi), where ??i is the
assignment with the highest probability, generated
using the current weight vector. The result is then:
?
i
(
?j(?goldi ,xi)? ?j(??i ,xi)
)
Our learning applies the following two steps it-
eratively. (1) Generate the most probable sequence
within the current weights. (2) Update the weights
by comparing the path generated in the previous
step to the gold standard path. The algorithm in
Figure 3 summarizes the procedure.
4 Instantiation
In this section, we discuss our instantiation of the
model presented in the previous section. In partic-
ular, we describe our replacement generators and
features.
4.1 Replacement Generators
One advantage of our proposed model is that
the reliance on replacement generators allows for
strong flexibility. Each generator can be seen as a
black box, allowing replacements that are created
heuristically, statistically, or by external tools to be
incorporated within the same framework.
Generator From To
leave intact good good
edit distance bac back
lowercase NEED need
capitalize it It
Google spell disspaear disappear
contraction wouldn?t would not
slang language ima I am going to
insert punctuation  .
duplicated punctuation !? !
delete filler lmao 
Table 1: Example replacement generators
To build a set of generic replacement generators
suitable for normalizing a variety of data types, we
collected a set of about 400 Twitter posts as devel-
opment data. Using that data, a series of gener-
ators were created; a sample of them are shown
in Table 1. As shown in the table, these gener-
ators cover a variety of normalization behavior,
from changing non-standard word forms to insert-
ing and deleting tokens.
4.2 Features
Although the proposed framework supports real
valued features, all features in our system are bi-
nary. In total, we used 70 features. Our feature set
pulls information from several different sources:
N-gram: Our n-gram features indicate the fre-
quency of the phrases induced by an edge. These
features are turned into binary ones by bucketing
their log values. For example, on the edge from
?1, 2,I? to ?2, 3,would? such a feature will indi-
cate whether the frequency of I would is over
a threshold. We use the Corpus of Contemporary
English (Davies, 2008 ) to produce our n-gram in-
formation.
Part-of-speech: Part-of-speech information
can be used to produce features that encourage
certain behavior, such as avoiding the deletion of
noun phrases. We generate part-of-speech infor-
mation over the original raw text using a Twit-
ter part-of-speech tagger (Ritter et al, 2011). Of
course, the part-of-speech information obtained
this way is likely to be noisy, and we expect our
learning algorithm to take that into account.
Positional: Information from positions is used
primarily to handle capitalization and punctuation
insertion, for example, by incorporating features
for capitalized words after stop punctuation or the
insertion of stop punctuation at the end of the sen-
tence.
Lineage: Finally, we include binary features
1163
that indicate which generator spawned the replace-
ment.
5 Evaluation
In this section, we present an empirical study of
our framework. The study is done over datasets
from three different domains. The goal is to eval-
uate the framework in two aspects: (1) usefulness
for downstream applications (specifically depen-
dency parsing), and (2) domain adaptability.
5.1 Evaluation Metrics
A few different metrics have been used to evaluate
normalizer performance, including word error rate
and BLEU score. While each metric has its pros
and cons, they all rely on word-to-word matching
and treat each word equally. In this work, we aim
to evaluate the performance of a normalizer based
on how it affects the performance of downstream
applications. We find that the conventional metrics
are not directly applicable, for several reasons. To
begin with, the assumption that words have equal
weights is unlikely to hold. Additionally, these
metrics tend to ignore other important non-word
information such as punctuation or capitalization.
They also cannot take into account other aspects
that may have an impact on downstream perfor-
mance, such as the word reordering as seen in the
example in Figure 4. Therefore, we propose a new
evaluation metric that directly equates normaliza-
tion performance with the performance of a com-
mon downstream application?dependency pars-
ing.
To realize our desired metric, we apply the fol-
lowing procedure. First, we produce gold standard
normalized data by manually normalizing sen-
tences to their full grammatically correct form. In
addition to the word-to-word mapping performed
in typical normalization gold standard generation,
this annotation procedure includes all actions nec-
essary to make the sentence grammatical, such as
word reordering, modifying capitalization, and re-
moving emoticons. We then run an off-the-shelf
dependency parser on the gold standard normal-
ized data to produce our gold standard parses. Al-
though the parser could still produce mistakes on
the grammatical sentences, we feel that this pro-
vides a realistic benchmark for comparison, as it
represents an upper bound on the possible perfor-
mance of the parser, and avoids an expensive sec-
ond round of manual annotation.
Test Gold SVO
I kinda wanna get
ipad NEW
I kind of want to
get a new iPad.
verb(get) verb(want)verb(get)
precisionv = 11
recallv = 12
subj(get,I)
subj(get,wanna)
obj(get,NEW)
subj(want,I)
subj(get,I)
obj(get,iPad)
precisionso = 13
recallso = 13
Figure 4: The subjects, verbs, and objects identi-
fied on example test/gold text, and corresponding
metric scores
To compare the parses produced over automati-
cally normalized data to the gold standard, we look
at the subjects, verbs, and objects (SVO) identi-
fied in each parse. The metric shown in Equa-
tions (2) and (3) below is based on the identified
subjects and objects in those parses. Note that SO
denotes the set of identified subjects and objects
whereas SOgold denotes the set of subjects and
objects identified when parsing the gold-standard
normalization.
precisionso =
|SO ? SOgold|
|SO | (2)
recallso = |SO ? SO
gold|
|SOgold|
(3)
We similarly define precisionv and recallv, where
we compare the set V of identified verbs to V gold
of those found in the gold-standard normalization.
An example is shown in Figure 4.
5.2 Results
To establish the extensibility of our normaliza-
tion system, we present results in three different
domains: Twitter posts, Short Message Service
(SMS) messages, and call-center logs. For Twitter
and SMS messages, we used established datasets
to compare with previous work. As no estab-
lished call-center log dataset exists, we collected
our own. In each case, we ran the proposed system
with two different configurations: one using only
the generic replacement generators presented in
Section 4 (denoted as generic), and one that adds
additional domain-specific generators for the cor-
responding domain (denoted as domain-specific).
All runs use ten-fold cross validation for training
and evaluation. The Stanford parser1 (Marneffe
et al, 2006) was used to produce all dependency
1Version 2.0.4, http://nlp.stanford.edu/
software/lex-parser.shtml
1164
parses. We compare our system to the following
baseline solutions:
w/oN: No normalization is performed.
Google: Output of the Google spell checker.
w2wN: The output of the word-to-word normal-
ization of Han and Baldwin (2011). Not available
for call-center data.
Gw2wN: The manual gold standard word-to-
word normalizations of previous work (Choud-
hury et al, 2007; Han and Baldwin, 2011). Not
available for call-center data.
Our results use the metrics of Section 5.1.
5.2.1 Twitter
To evaluate the performance on Twitter data, we
use the dataset of randomly sampled tweets pro-
duced by (Han and Baldwin, 2011). Because the
gold standard used in this work only provided
word mappings for out-of-vocabulary words and
did not enforce grammaticality, we reannotated the
gold standard data2. Their original gold standard
annotations were kept as a baseline.
To produce Twitter-specific generators, we ex-
amined the Twitter development data collected for
generic generator production (Section 4). These
generators focused on the Twitter-specific notions
of hashtags (#), ats (@), and retweets (RT). For
each case, we implemented generators that al-
lowed for either the initial symbol or the entire to-
ken to be deleted (e.g., @Hertz to Hertz, @Hertz
to ).
The results are given in Table 2. As shown,
the domain-specific generators yielded perfor-
mance significantly above the generic ones and all
baselines. Even without domain-specific genera-
tors, our system outperformed the word-to-word
normalization approaches. Most notably, both
the generic and domain-specific systems outper-
formed the gold standard word-to-word normal-
izations. These results validate the hypothesis that
simple word-to-word normalization is insufficient
if the goal of normalization is to improve depen-
dency parsing; even if a system could produce
perfect word-to-word normalization, it would pro-
duce lower quality parses than those produced by
our approach.
2Our results and the reannotations of the Twitter and SMS
data are available at https://www.cs.washington.
edu/node/9091/
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 83.7 68.1 75.1 31.7 38.6 34.8
Google 88.9 78.8 83.5 36.1 46.3 40.6
w2wN 87.5 81.5 84.4 44.5 58.9 50.7
Gw2w 89.8 83.8 86.7 46.9 61.0 53.0
generic 91.7 88.9 90.3 53.6 70.2 60.8
domain specific 95.3 88.7 91.9 72.5 76.3 74.4
Table 2: Performance on Twitter dataset
5.2.2 SMS
To evaluate the performance on SMS data, we use
the Treasure My Text data collected by Choud-
hury et al (2007). As with the Twitter data, the
word-to-word normalizations were reannotated to
enforce grammaticality. As a replacement genera-
tor for SMS-specific substitutions, we used a map-
ping dictionary of SMS abbreviations.3 No further
SMS-specific development data was needed.
Table 3 gives the results on the SMS data. The
SMS dataset proved to be more difficult than the
Twitter dataset, with the overall performance of
every system being lower. While this drop of per-
formance may be a reflection of the difference in
data styles between SMS and Twitter, it is also
likely a product of the collection methodology.
The collection methodology of the Treasure My
Text dataset dictated that every message must have
at least one mistake, which may have resulted in a
dataset that was noisier than average.
Nonetheless, the trends on SMS data mirror
those on Twitter data, with the domain-specific
generators achieving the greatest overall perfor-
mance. However, while the generic setting still
manages to outperform most baselines, it did not
outperform the gold word-to-word normalization.
In fact, the gold word-to-word normalization was
much more competitive on this data, outperform-
ing even the domain-specific system on verbs
alone. This should not be seen as surprising, as
word-to-word normalization is most likely to be
beneficial for cases like this where the proportion
of non-standard tokens is high.
It should be noted that the SMS dataset as avail-
able has had all punctuation removed. While this
may be appropriate for word-to-word normaliza-
tion, this preprocessing may have an effect on the
parse of the sentence. As our system has the abil-
ity to add punctuation but our baseline systems do
not, this has the potential to artificially inflate our
results. To ensure a fair comparison, we manually
3http://www.netlingo.com/acronyms.php
1165
System Verb Subject-ObjectRec Pre F1 Rec Pre F1
w/oN 76.4 48.1 59.0 19.5 21.5 20.4
Google 85.1 61.6 71.5 22.4 26.2 24.1
w2wN 78.5 61.5 68.9 29.9 36.0 32.6
Gw2wN 87.6 76.6 81.8 38.0 50.6 43.4
generic 86.5 77.4 81.7 35.5 47.7 40.7
domain specific 88.1 75.0 81.0 41.0 49.5 44.8
Table 3: Performance on SMS dataset
System Verb Subject-ObjectPre Rec F1 Pre Rec F1
w/oN 98.5 97.1 97.8 69.2 66.1 67.6
Google 99.2 97.9 98.5 70.5 67.3 68.8
generic 98.9 97.4 98.1 71.3 67.9 69.6
domain specific 99.2 97.4 98.3 87.9 83.1 85.4
Table 4: Performance on call-center dataset
added punctuation to a randomly selected small
subset of the SMS data and reran each system.
This experiment suggested that, in contrast to the
hypothesis, adding punctuation actually improved
the results of the proposed system more substan-
tially than that of the baseline systems.
5.2.3 Call-Center
Although Twitter and SMS data are unmistakably
different, there are many similarities between the
two, such as the frequent use of shorthand word
forms that omit letters. The examination of call-
center logs allows us to examine the ability of our
system to perform normalization in more disparate
domains. Our call-center data consists of text-
based responses to questions about a user?s expe-
rience with a call-center (e.g., their overall satis-
faction with the service). We use call-center logs
from a major company, and collect about 150 re-
sponses for use in our evaluation. We collected
an additional small set of data to develop our call-
center-specific generators.
Results on the call-center dataset are in Table 4.
As shown, the raw call-center data was compar-
atively clean, resulting in higher baseline perfor-
mance than in other domains. Unlike on previ-
ous datasets, the use of generic mappings only
provided a small improvement over the baseline.
However, the use of domain-specific generators
once again led to significantly increased perfor-
mance on subjects and objects.
6 Discussion
The results presented in the previous section sug-
gest that domain transfer using the proposed nor-
malization framework is possible with only a
small amount of effort. The relatively modest
set of additional replacement generators included
in each data set alowed the domain-specific ap-
proaches to significantly outperform the generic
approach. In the call-center case, performance im-
provements could be seen by referencing a very
small amount of development data. In the SMS
case, the presence of a domain-specific dictionary
allowed for performance improvements without
the need for any development data at all. It is
likely, though not established, that employing fur-
ther development data would result in further per-
formance improvements. We leave further investi-
gation to future work.
The results in Section 5.2 establish a point that
has often been assumed but, to the best of our
knowledge, has never been explicitly shown: per-
forming normalization is indeed beneficial to de-
pendency parsing on informal text. The parse of
the normalized text was substantially better than
the parse of the original raw text in all domains,
with absolute performance increases ranging from
about 18-25% on subjects and objects. Further-
more, the results suggest that, as hypothesized,
preparing an informal text for a parsing task re-
quires more than simple word-to-word normaliza-
tion. The proposed approach significantly outper-
forms the state-of-the-art word-to-word normal-
ization approach. Perhaps most interestingly, the
proposed approach performs on par with, and in
several cases superior to, gold standard word-to-
word annotations. This result gives strong evi-
dence for the conclusion that parser-targeted nor-
malization requires a broader understanding of the
scope of the normalization task.
While the work presented here gives promis-
ing results, there are still many behaviors found
in informal text that prove challenging. One
such example is the word reordering seen in Fig-
ure 4. Although word reordering could be incor-
porated into the model as a combination of a dele-
tion and an insertion, the model as currently de-
vised cannot easily link these two replacements
to one another. Additionally, instances of re-
ordering proved hard to detect in practice. As
such, no reordering-based replacement generators
were implemented in the presented system. An-
other case that proved difficult was the insertion
of missing tokens. For instance, the informal
sentence ?Day 3 still don?t freaking
1166
feel good!:(? could be formally rendered
as ?It is day 3 and I still do not
feel good!?. Attempts to address missing to-
kens in the model resulted in frequent false pos-
itives. Similarly, punctuation insertion proved to
be challenging, often requiring a deep analysis
of the sentence. For example, contrast the sen-
tence ?I?m watching a movie I don?t
know its name.? which would benefit from
inserted punctuation, with ?I?m watching a
movie I don?t know.?, which would not.
We feel that the work presented here provides a
foundation for future work to more closely exam-
ine these challenges.
7 Conclusions
This work presents a framework for normalization
with an eye towards domain adaptation. The pro-
posed framework builds a statistical model over a
series of replacement generators. By doing so, it
allows a designer to quickly adapt a generic model
to a new domain with the inclusion of a small set of
domain-specific generators. Tests over three dif-
ferent domains suggest that, using this model, only
a small amount of domain-specific data is neces-
sary to tailor an approach towards a new domain.
Additionally, this work introduces a parser-
centric view of normalization, in which the per-
formance of the normalizer is directly tied to the
performance of a downstream dependency parser.
This evaluation metric allows for a deeper under-
standing of how certain normalization actions im-
pact the output of the parser. Using this met-
ric, this work established that, when dependency
parsing is the goal, typical word-to-word normal-
ization approaches are insufficient. By taking a
broader look at the normalization task, the ap-
proach presented here is able to outperform not
only state-of-the-art word-to-word normalization
approaches but also manual word-to-word annota-
tions.
Although the work presented here established
that more than word-to-word normalization was
necessary to produce parser-ready normalizations,
it remains unclear which specific normalization
tasks are most critical to parser performance. We
leave this interesting area of examination to future
work.
Acknowledgments
We thank the anonymous reviewers of ACL for
helpful comments and suggestions. We also thank
Ioana R. Stanoi for her comments on a prelim-
inary version of this work, Daniel S. Weld for
his support, and Alan Ritter, Monojit Choudhury,
Bo Han, and Fei Liu for sharing their tools and
data. The first author is partially supported by the
DARPA Machine Reading Program under AFRL
prime contract numbers FA8750-09-C-0181 and
FA8750-09-C-0179. Any opinions, findings, con-
clusions, or recommendations expressed in this
material are those of the authors and do not neces-
sarily reflect the views of DARPA, AFRL, or the
US government. This work is a part of IBM?s Sys-
temT project (Chiticariu et al, 2010).
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normal-
ization. In ACL, pages 33?40.
Zhuowei Bao, Benny Kimelfeld, and Yunyao Li. 2011.
A graph approach to spelling correction in domain-
centric search. In ACL, pages 905?914.
Richard Beaufort, Sophie Roekhaut, Louise-Ame?lie
Cougnon, and Ce?drick Fairon. 2010. A hybrid
rule/model-based finite-state framework for normal-
izing sms messages. In ACL, pages 770?779.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Sriram Raghavan, Frederick Reiss, and Shivaku-
mar Vaithyanathan. 2010. SystemT: An algebraic
approach to declarative information extraction. In
ACL, pages 128?137.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. IJDAR, 10(3-4):157?174.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization. In
CALC, pages 71?78.
Mark Davies. 2008-. The corpus of contempo-
rary american english: 450 million words, 1990-
present. Avialable online at: http://corpus.
byu.edu/coca/.
Bo Han and Timothy Baldwin. 2011. Lexical normali-
sation of short text messages: Makn sens a #twitter.
In ACL, pages 368?378.
1167
Bo Han, Paul Cook, and Timothy Baldwin. 2012. Au-
tomatically constructing a normalisation dictionary
for microblogs. In EMNLP-CoNLL, pages 421?432.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two
metaphors better than one? In COLING, pages 441?
448.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282?289.
Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.
2011. Insertion, deletion, or substitution? normal-
izing text messages without pre-categorization nor
supervision. In ACL, pages 71?76.
Fei Liu, Fuliang Weng, and Xiao Jiang. 2012. A
broad-coverage normalization system for social me-
dia language. In ACL, pages 1035?1044.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In ACL, pages 311?
318.
Deana Pennell and Yang Liu. 2010. Normalization of
text messages for text-to-speech. In ICASSP, pages
4842?4845.
Deana Pennell and Yang Liu. 2011. A character-level
machine translation approach for normalization of
SMS abbreviations. In IJCNLP, pages 974?982.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in Tweets: An ex-
perimental study. In EMNLP, pages 1524?1534.
Richard Sproat, Alan W. Black, Stanley F. Chen,
Shankar Kumar, Mari Ostendorf, and Christopher
Richards. 2001. Normalization of non-standard
words. Computer Speech & Language, 15(3):287?
333.
Zhenzhen Xue, Dawei Yin, and Brian D. Davison.
2011. Normalizing microtext. In Analyzing Micro-
text, volume WS-11-05 of AAAI Workshops.
1168
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 87?95,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Machine Reading at the University of Washington
Hoifung Poon, Janara Christensen, Pedro Domingos, Oren Etzioni, Raphael Hoffmann,
Chloe Kiddon, Thomas Lin, Xiao Ling, Mausam, Alan Ritter, Stefan Schoenmackers,
Stephen Soderland, Dan Weld, Fei Wu, Congle Zhang
Department of Computer Science & Engineering
University of Washington
Seattle, WA 98195
{hoifung,janara,pedrod,etzioni,raphaelh,chloe,tlin,xiaoling,mausam,
aritter,stef,soderland,weld,wufei,clzhang}@cs.washington.edu
Abstract
Machine reading is a long-standing goal of AI
and NLP. In recent years, tremendous progress
has been made in developing machine learning
approaches for many of its subtasks such as
parsing, information extraction, and question
answering. However, existing end-to-end so-
lutions typically require substantial amount of
human efforts (e.g., labeled data and/or man-
ual engineering), and are not well poised for
Web-scale knowledge acquisition. In this pa-
per, we propose a unifying approach for ma-
chine reading by bootstrapping from the easi-
est extractable knowledge and conquering the
long tail via a self-supervised learning pro-
cess. This self-supervision is powered by joint
inference based on Markov logic, and is made
scalable by leveraging hierarchical structures
and coarse-to-fine inference. Researchers at
the University of Washington have taken the
first steps in this direction. Our existing work
explores the wide spectrum of this vision and
shows its promise.
1 Introduction
Machine reading, or learning by reading, aims to
extract knowledge automatically from unstructured
text and apply the extracted knowledge to end tasks
such as decision making and question answering. It
has been a major goal of AI and NLP since their
early days. With the advent of the Web, the billions
of online text documents contain virtually unlimited
amount of knowledge to extract, further increasing
the importance and urgency of machine reading.
In the past, there has been a lot of progress in
automating many subtasks of machine reading by
machine learning approaches (e.g., components in
the traditional NLP pipeline such as POS tagging
and syntactic parsing). However, end-to-end solu-
tions are still rare, and existing systems typically re-
quire substantial amount of human effort in manual
engineering and/or labeling examples. As a result,
they often target restricted domains and only extract
limited types of knowledge (e.g., a pre-specified re-
lation). Moreover, many machine reading systems
train their knowledge extractors once and do not
leverage further learning opportunities such as ad-
ditional text and interaction with end users.
Ideally, a machine reading system should strive to
satisfy the following desiderata:
End-to-end: the system should input raw text, ex-
tract knowledge, and be able to answer ques-
tions and support other end tasks;
High quality: the system should extract knowledge
with high accuracy;
Large-scale: the system should acquire knowledge
at Web-scale and be open to arbitrary domains,
genres, and languages;
Maximally autonomous: the system should incur
minimal human effort;
Continuous learning from experience: the
system should constantly integrate new infor-
mation sources (e.g., new text documents) and
learn from user questions and feedback (e.g.,
via performing end tasks) to continuously
improve its performance.
These desiderata raise many intriguing and chal-
lenging research questions. Machine reading re-
search at the University of Washington has explored
87
a wide spectrum of solutions to these challenges and
has produced a large number of initial systems that
demonstrated promising performance. During this
expedition, an underlying unifying vision starts to
emerge. It becomes apparent that the key to solving
machine reading is to:
1. Conquer the long tail of textual knowledge via
a self-supervised learning process that lever-
ages data redundancy to bootstrap from the
head and propagates information down the long
tail by joint inference;
2. Scale this process to billions of Web documents
by identifying and leveraging ubiquitous struc-
tures that lead to sparsity.
In Section 2, we present this vision in detail, iden-
tify the major dimensions these initial systems have
explored, and propose a unifying approach that sat-
isfies all five desiderata. In Section 3, we reivew
machine reading research at the University of Wash-
ington and show how they form synergistic effort
towards solving the machine reading problem. We
conclude in Section 4.
2 A Unifying Approach for Machine
Reading
The core challenges to machine reading stem from
the massive scale of the Web and the long-tailed dis-
tribution of textual knowledge. The heterogeneous
Web contains texts that vary substantially in subject
matters (e.g., finance vs. biology) and writing styles
(e.g., blog posts vs. scientific papers). In addition,
natural languages are famous for their myraid vari-
ations in expressing the same meaning. A fact may
be stated in a straightforward way such as ?kale con-
tains calcium?. More often though, it may be stated
in a syntactically and/or lexically different way than
as phrased in an end task (e.g., ?calcium is found in
kale?). Finally, many facts are not even stated ex-
plicitly, and must be inferred from other facts (e.g.,
?kale prevents osteoporosis? may not be stated ex-
plicitly but can be inferred by combining facts such
as ?kale contains calcium? and ?calcium helps pre-
vent osteoporosis?). As a result, machine reading
must not rely on explicit supervision such as manual
rules and labeled examples, which will incur pro-
hibitive cost in the Web scale. Instead, it must be
able to learn from indirect supervision.





	






Figure 1: A unifying vision for machine reading: boot-
strap from the head regime of the power-law distribu-
tion of textual knowledge, and conquer the long tail in
a self-supervised learning process that raises certainty on
sparse extractions by propagating information via joint
inference from frequent extractions.
A key source of indirect supervision is meta
knowledge about the domains. For example, the
TextRunner system (Banko et al, 2007) hinges on
the observation that there exist general, relation-
independent patterns for information extraction. An-
other key source of indirect supervision is data re-
dundancy. While a rare extracted fact or inference
pattern may arise by chance of error, it is much less
likely so for the ones with many repetitions (Downey
et al, 2010). Such highly-redundant knowledge can
be extracted easily and with high confidence, and
can be leveraged for bootstrapping. For knowledge
that resides in the long tail, explicit forms of redun-
dancy (e.g., identical expressions) are rare, but this
can be circumvented by joint inference. For exam-
ple, expressions that are composed with or by sim-
ilar expressions probably have the same meaning;
the fact that kale prevents osteoporosis can be de-
rived by combining the facts that kale contains cal-
cium and that calcium helps prevent osteoporosis via
a transitivity-through inference pattern. In general,
joint inference can take various forms, ranging from
simple voting to shrinkage in a probabilistic ontol-
ogy to sophisticated probabilistic reasoning based
on a joint model. Simple ones tend to scale bet-
ter, but their capability in propagating information
is limited. More sophisticated methods can uncover
implicit redundancy and propagate much more in-
88
formation with higher quality, yet the challenge is
how to make them scale as well as simple ones.
To do machine reading, a self-supervised learning
process, informed by meta knowledege, stipulates
what form of joint inference to use and how. Effec-
tively, it increases certainty on sparse extractions by
propagating information from more frequent ones.
Figure 1 illustrates this unifying vision.
In the past, machine reading research at the Uni-
versity of Washington has explored a variety of so-
lutions that span the key dimensions of this uni-
fying vision: knowledge representation, bootstrap-
ping, self-supervised learning, large-scale joint in-
ference, ontology induction, continuous learning.
See Section 3 for more details. Based on this ex-
perience, one direction seems particularly promising
that we would propose here as our unifying approach
for end-to-end machine reading:
Markov logic is used as the unifying framework for
knowledge representation and joint inference;
Self-supervised learning is governed by a joint
probabilistic model that incorporates a small
amount of heuristic knowledge and large-scale
relational structures to maximize the amount
and quality of information to propagate;
Joint inference is made scalable to the Web by
coarse-to-fine inference.
Probabilistic ontologies are induced from text to
guarantee tractability in coarse-to-fine infer-
ence. This ontology induction and popula-
tion are incorporated into the joint probabilistic
model for self-supervision;
Continuous learning is accomplished by combin-
ing bootstrapping and crowdsourced content
creation to synergistically improve the reading
system from user interaction and feedback.
A distinctive feature of this approach is its empha-
sis on using sophisticated joint inference. Recently,
joint inference has received increasing interest in
AI, machine learning, and NLP, with Markov logic
(Domingos and Lowd, 2009) being one of the lead-
ing unifying frameworks. Past work has shown that
it can substantially improve predictive accuracy in
supervised learning (e.g., (Getoor and Taskar, 2007;
Bakir et al, 2007)). We propose to build on these ad-
vances, but apply joint inference beyond supervised
learning, with labeled examples supplanted by indi-
rect supervision.
Another distinctive feature is that we propose
to use coarse-to-fine inference (Felzenszwalb and
McAllester, 2007; Petrov, 2009) as a unifying
framework to scale inference to the Web. Es-
sentially, coarse-to-fine inference leverages the
sparsity imposed by hierarchical structures that
are ubiquitous in human knowledge (e.g., tax-
onomies/ontologies). At coarse levels (top levels in
a hierarchy), ambiguities are rare (there are few ob-
jects and relations), and inference can be conducted
very efficiently. The result is then used to prune un-
promising refinements at the next level. This process
continues down the hierarchy until decision can be
made. In this way, inference can potentially be sped
up exponentially, analogous to binary tree search.
Finally, we propose a novel form of continuous
learning by leveraging the interaction between the
system and end users to constantly improve the per-
formance. This is straightforward to do in our ap-
proach given the self-supervision process and the
availability of powerful joint inference. Essentially,
when the system output is applied to an end task
(e.g., answering questions), the feedback from user
is collected and incorporated back into the system
as a bootstrap source. The feedback can take the
form of explicit supervision (e.g., via community
content creation or active learning) or indirect sig-
nals (e.g., click data and query logs). In this way,
we can bootstrap an online community by an initial
machine reading system that provides imperfect but
valuable service in end tasks, and continuously im-
prove the quality of system output, which attracts
more users with higher degree of participation, thus
creating a positive feedback loop and raising the ma-
chine reading performance to a high level that is dif-
ficult to attain otherwise.
3 Summary of Progress to Date
The University of Washington has been one of the
leading places for machine reading research and has
produced many cutting-edge systems, e.g., WIEN
(first wrapper induction system for information ex-
traction), Mulder (first fully automatic Web-scale
question answering system), KnowItAll/TextRunner
(first systems to do open-domain information extrac-
89
tion from the Web corpus at large scale), Kylin (first
self-supervised system for Wikipedia-based infor-
mation extraction), UCR (first unsupervised corefer-
ence resolution system that rivals the performance of
supervised systems), Holmes (first Web-scale joint
inference system), USP (first unsupervised system
for semantic parsing).
Figure 2 shows the evolution of the major sys-
tems; dashed lines signify influence in key ideas
(e.g., Mulder inspires KnowItAll), and solid lines
signify dataflow (e.g., Holmes inputs TextRunner tu-
ples). These systems span a wide spectrum in scal-
ability (assessed by speed and quantity in extrac-
tion) and comprehension (assessed by unit yield of
knowledge at a fixed precision level). At one ex-
treme, the TextRunner system is highly scalable, ca-
pable of extracting billions of facts, but it focuses on
shallow extractions from simple sentences. At the
other extreme, the USP and LOFT systems achieve
much higher level of comprehension (e.g., in a task
of extracting knowledge from biomedical papers and
answering questions, USP obtains more than three
times as many correct answers as TextRunner, and
LOFT obtains more than six times as many correct
answers as TextRunner), but are much less scalable
than TextRunner.
In the remainder of the section, we review the
progress made to date and identify key directions for
future work.
3.1 Knowledge Representation and Joint
Inference
Knowledge representations used in these systems
vary widely in expressiveness, ranging from sim-
ple ones like relation triples (<subject, relation,
object>; e.g., in KnowItAll and TextRunner), to
clusters of relation triples or triple components (e.g.,
in SNE, RESOLVER), to arbitrary logical formulas
and their clusters (e.g., in USP, LOFT). Similarly,
a variety forms of joint inference have been used,
ranging from simple voting to heuristic rules to so-
phisticated probabilistic models. All these can be
compactly encoded in Markov logic (Domingos and
Lowd, 2009), which provides a unifying framework
for knowledge representation and joint inference.
Past work at Washington has shown that in su-
pervised learning, joint inference can substantially
improve predictive performance on tasks related to
machine reading (e.g., citation information extrac-
tion (Poon and Domingos, 2007), ontology induc-
tion (Wu and Weld, 2008), temporal information
extraction (Ling and Weld, 2010)). In addition, it
has demonstrated that sophisticated joint inference
can enable effective learning without any labeled
information (UCR, USP, LOFT), and that joint in-
ference can scale to millions of Web documents by
leveraging sparsity in naturally occurring relations
(Holmes, Sherlock), showing the promise of our uni-
fying approach.
Simpler representations limit the expressiveness
in representing knowledge and the degree of sophis-
tication in joint inference, but they currently scale
much better than more expressive ones. A key direc-
tion for future work is to evaluate this tradeoff more
thoroughly, e.g., for each class of end tasks, to what
degree do simple representations limit the effective-
ness in performing the end tasks? Can we automate
the choice of representations to strike the best trade-
off for a specific end task? Can we advance joint
inference algorithms to such a degree that sophisti-
cated inference scales as well as simple ones?
3.2 Bootstrapping
Past work at Washington has identified and lever-
aged a wide range of sources for bootstrapping. Ex-
amples include Wikipedia (Kylin, KOG, IIA, WOE,
WPE), Web lists (KnowItAll, WPE), Web tables
(WebTables), Hearst patterns (KnowItAll), heuristic
rules (TextRunner), semantic role labels (SRL-IE),
etc.
In general, potential bootstrap sources can be
broadly divided into domain knowledge (e.g., pat-
terns and rules) and crowdsourced contents (e.g., lin-
guistic resources, Wikipedia, Amazon Mechanical
Turk, the ESP game).
A key direction for future work is to combine
bootstrapping with crowdsourced content creation
for continuous learning. (Also see Subsection 3.6.)
3.3 Self-Supervised Learning
Although the ways past systems conduct self-
supervision vary widely in detail, they can be di-
vided into two broad categories. One uses heuristic
rules that exploit existing semi-structured resources
to generate noisy training examples for use by su-
pervised learning methods and with cotraining (e.g.,
90

	


		

	
 	