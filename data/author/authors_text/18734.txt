Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 165?168,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Learning of Acoustic Sub-word Units
Balakrishnan Varadarajan? and Sanjeev Khudanpur?
Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD 21218
{bvarada2,khudanpur}@jhu.edu
Emmanuel Dupoux
Laboratoire de Science Cognitive
et Psycholinguistique
75005, Paris, France
emmanuel.dupoux@gmail.com
Abstract
Accurate unsupervised learning of phonemes
of a language directly from speech is demon-
strated via an algorithm for joint unsupervised
learning of the topology and parameters of
a hidden Markov model (HMM); states and
short state-sequences through this HMM cor-
respond to the learnt sub-word units. The
algorithm, originally proposed for unsuper-
vised learning of allophonic variations within
a given phoneme set, has been adapted to
learn without any knowledge of the phonemes.
An evaluation methodology is also proposed,
whereby the state-sequence that aligns to
a test utterance is transduced in an auto-
matic manner to a phoneme-sequence and
compared to its manual transcription. Over
85% phoneme recognition accuracy is demon-
strated for speaker-dependent learning from
fluent, large-vocabulary speech.
1 Automatic Discovery of Phone(me)s
Statistical models learnt from data are extensively
used in modern automatic speech recognition (ASR)
systems. Transcribed speech is used to estimate con-
ditional models of the acoustics given a phoneme-
sequence. The phonemic pronunciation of words
and the phonemes of the language, however, are
derived almost entirely from linguistic knowledge.
In this paper, we investigate whether the phonemes
may be learnt automatically from the speech signal.
Automatic learning of phoneme-like units has sig-
nificant implications for theories of language ac-
quisition in babies, but our considerations here are
somewhat more technological. We are interested in
developing ASR systems for languages or dialects
? This work was partially supported by National Science
Foundation Grants No
?
IIS-0534359 and OISE-0530118.
for which such linguistic knowledge is scarce or
nonexistent, and in extending ASR techniques to
recognition of signals other than speech, such as ma-
nipulative gestures in endoscopic surgery. Hence an
algorithm for automatically learning an inventory of
intermediate symbolic units?intermediate relative
to the acoustic or kinematic signal on one end and
the word-sequence or surgical act on the other?is
very desirable.
Except for some early work on isolated word/digit
recognition (Paliwal and Kulkarni, 1987; Wilpon
et al, 1987, etc), not much attention has been
paid to automatic derivation of sub-word units from
speech, perhaps because pronunciation lexicons are
now available1 in languages of immediate interest.
What has been investigated is automatically learn-
ing allophonic variations of each phoneme due to
co-articulation or contextual effects (Takami and
Sagayama, 1992; Fukada et al, 1996); the phoneme
inventory is usually assumed to be known.
The general idea in allophone learning is to be-
gin with an inventory of only one allophone per
phoneme, and incrementally refine the inventory to
better fit the speech signal. Typically, each phoneme
is modeled by a separate HMM. In early stages of
refinement, when very few allophones are available,
it is hoped that ?similar? allophones of a phoneme
will be modeled by shared HMM states, and that
subsequent refinement will result in distinct states
for different allophones. The key therefore is to de-
vise a scheme for successive refinement of a model
shared by many allophones. In the HMM setting,
this amounts to simultaneously refining the topol-
ogy and the model parameters. A successive state
splitting (SSS) algorithm to achieve this was pro-
posed by Takami and Sagayama (1992), and en-
1See http://www.ldc.upenn.edu/Catalog/byType.jsp
165
hanced by Singer and Ostendorf (1996). Improve-
ments in phoneme recognition accuracy using these
derived allophonic models over phonemic models
were obtained.
In this paper, we investigate directly learning the
allophone inventory of a language from speech with-
out recourse to its phoneme set. We begin with a
one-state HMM for all speech sounds and modify
the SSS algorithm to successively learn the topol-
ogy and parameters of HMMs with even larger num-
bers of states. States sequences through this HMM
are expected to correspond to allophones. The most
likely state-sequence for a speech segment is inter-
preted as an ?allophonic labeling? of that speech by
the learnt model. Performance is measured by map-
ping the resultant state-sequence to phonemes.
One contribution of this paper is a significant im-
provement in the efficacy of the SSS algorithm as
described in Section 2. It is based on observing
that the improvement in the goodness of fit by up
to two consecutive splits of any of the current HMM
states can be evaluated concurrently and efficiently.
Choosing the best subset of splits from among these
is then cast as a constrained knapsack problem, to
which an efficient solution is devised. Another con-
tribution of this paper is a method to evaluate the
accuracy of the resulting ?allophonic labeling,? as
described in Section 3. It is demonstrated that if
a small amount of phonetically transcribed speech
is used to learn a Markov (bigram) model of state-
sequences that arise from each phone, an evalua-
tion tool results with which we may measure phone
recognition accuracy, even though the HMM labels
the speech signal not with phonemes but merely a
state-sequence. Section 4 presents experimental re-
sults, where the performance accuracies with differ-
ent learning setups are tabulated. We also see how as
little as 5 minutes of speech is adequate for learning
the acoustic units.
2 An Improved and Fast SSS Algorithm
The improvement of the SSS algorithm of Takami
and Sagayama (1992), renamed ML-SSS by Singer
and Ostendorf (1996), proceeds roughly as follows.
1. Model all the speech2 using a 1-state HMM
with a diagonal-covariance Gaussian. (N=1.)
2Note that the original application of SSS was for learning
Figure 1: Modified four-way split of a state s.
2. For each HMM state s, compute the gain in log-
likelihood (LL) of the speech by either a con-
textual or a temporal split of s into two states
s1 and s2. Among the N states, select and and
split the one that yields the most gain in LL.
3. If the gain is above a threshold, retain the split
and set N = N + 1; furthermore, if N is less
than desired, re-estimate all parameters of the
new HMM, and go to Step 2.
Note that the key computational steps are the for-
loop of Step 2 and the re-estimation of Step 3.
Modifications to the ML-SSS Algorithm: We
made the following modifications that are favorable
in terms of greater speed and larger search space,
thereby yielding a gain in likelihood that is poten-
tially greater than the original ML-SSS.
1. Model all the speech using a 1-state HMM with
a full-covariance Gaussian density. Set N = 1.
2. Simultaneously replace each state s of the
HMM with the 4-state topology shown in Fig-
ure 1, yielding a 4N -state HMM. If the state s
had parameters (?s,?s), then means of its 4-
state replacement are ?s1 = ?s? ? = ?s4 and
?s2 = ?s +? = ?s3 , with ? = ?
?v?, where ??
and v? are the principal eigenvalue and eigen-
vector of ?s and 0 <  1 is typically 0.2.
3. Re-estimate all parameters of this (overgrown)
HMM. Gather the Gaussian sufficient statistics
for each of the 4N states from the last pass
of re-estimation: the state occupancy pisi . The
sample mean ?si , and sample covariance ?si .
4. Each quartet of states (see Figure 1) that re-
sulted from the same original state s can be
the allophonic variations of a phoneme; hence the phrase ?all
the speech? meant all the speech corresponding separately to
each phoneme. Here it really means all the speech.
166
merged back in different ways to produce 3, 2
or 1 HMM states. There are 6 ways to end up
with 3 states, and 7 to end up with 2 states. Re-
tain for further consideration the 4 state split of
s, the best merge back to 3 states among the 6
ways, the best merge back to 2 states among the
7 ways, and the merge back to 1 state.
5. Reduce the number of states from 4N toN+?
by optimally3 merging back quartets that cause
the least loss in log-likelihood of the speech.
6. Set N = N + ?. If N is less than the desired
HMM size, retrain the HMM and go to Step 2.
Observe that the 4-state split of Figure 1 permits a
slight look-ahead in our scheme in the sense that the
goodness of a contextual or temporal split of two dif-
ferent states can be compared in the same iteration
with two consecutive splits of a single state. Also,
the split/merge statistics for a state are gathered in
our modified SSS assuming that the other states have
already been split, which facilitates consideration of
concurrent state splitting. If s1, . . . , sm are merged
into s?, the loss of log-likelihood in Step 4 is:
d
2
m?
i=1
pisi log |?s?| ?
d
2
m?
i=1
pisi log |?si | , (1)
where ?s? =
?m
i=1 pisi
(
?si + ?si?
?
si
)
?m
i=1 pisi
? ?s??
?
s?.
Finally, in selecting the best ? states to add to the
HMM, we consider many more ways of splitting the
N original states than SSS does. E.g. going up from
N = 6 toN+? = 9 HMM states could be achieved
by a 4-way split of a single state, a 3-way split of one
state and 2-way of another, or a 2-way split of three
distinct states; all of them are explored in the process
of merging from 4N = 24 down to 9 states. Yet, like
SSS, no original state s is permitted to merge with
another original state s?. This latter restriction leads
to an O(N5) algorithm for finding the best states to
merge down4. Details of the algorithm are ommited
for the sake of brevity.
In summary, our modified ML-SSS algorithm can
leap-frog by ? states at a time, e.g. ? = ?N , com-
pared to the standard algorithm, and it has the benefit
of some lookahead to avoid greediness.
3This entails solving a constrained knapsack problem.
4This is a restricted version of the 0-1 knapsack problem.
3 Evaluating the Goodness of the Labels
The HMM learnt in Section 2 is capable of assign-
ing state-labels to speech via the Viterbi algorithm.
Evaluating whether these labels are linguistically
meaningful requires interpreting the labels in terms
of phonemes. We do so as follows.
Some phonetically transcribed speech is labeled
with the learnt HMM, and the label sequences cor-
responding to each phone segment are extracted.
Since the HMM was learnt from unlabeled speech,
the labels and short label-sequences usually corre-
spond to allophones, not phonemes. Therefore, for
each triphone, i.e. each phone tagged with its left-
and right-phone context, a simple bigram model of
label sequences is estimated. An unweighted ?phone
loop? that accepts all phone sequences is created,
and composed with these bigram models to cre-
ate a label-to-phone transducer capable of mapping
HMM label sequences to phone sequences.
Finally, the test speech (not used for HMM learn-
ing, nor for estimating the bigram model) is treated
as having been ?generated? by a source-channel
model in which the label-to-phone transducer is the
source?generating an HMM state-sequence?and
the Gaussian densities of the learnt HMM states con-
stitute the channel?taking the HMM state-sequence
as the channel input and generating the observed
speech signal as the output. Standard Viterbi decod-
ing determines the most likely phone sequence for
the test speech, and phone accuracy is measured by
comparison with the manual phonetic transcription.
4 Experimental Results
4.1 Impact of the Modified State Splitting
The ML-SSS procedure estimates 2N different
N+1-state HMMs to grow from N to N+1 states.
Our procedure estimates one 4N state HMM to
grow to N+?, making it hugely faster for large N .
Table 1 compares the log-likelihood of the train-
ing speech for ML-SSS and our procedure. The re-
sults validate our modifications, demonstrating that
at least in the regimes feasible for ML-SSS, there is
no loss (in fact a tiny gain) in fitting the speech data,
and a big gain in computational effort5.
5ML-SSS with ?=1 was impractical beyond N=22.
167
# of states SSS (? = 1) ? = 3 ? = N
8 -7.14 -7.13 -7.13
10 -7.08 -7.06 -7.06
22 -6.78 -6.76 N/A
40 N/A -6.23 -6.20
Table 1: Aggressive state splitting does not cause any
degradation in log-likelihood relative to ML-SSS.
4.2 Unsupervised Learning of Sub-word Units
We used about 30 minutes of phonetically tran-
scribed Japanese speech from one speaker6 provided
by Maekawa (2003) for our unsupervised learning
experiments. The speech was segmented via silence
detection into 800 utterances, which were further
partitioned into a 24-minute training set (80%) and
6-minute test set (20%).
Our first experiment was to learn an HMM from
the training speech using our modified ML-SSS pro-
cedure; we tried N = 22, 70 and 376. For each N ,
we then labeled the training speech using the learnt
HMM, used the phonetic transcription of the train-
ing speech to estimate label-bigram models for each
triphone, and built the label-to-phone transducer as
described in Section 3. We also investigated (i) using
only 5 minutes of training speech to learn the HMM,
but still labeling and using all 24 minutes to build
the label-to-phone transducer, and (ii) setting aside
5 minutes of training speech to learn the transducer
and using the rest to learn the HMM. For each learnt
HMM+transducer pair, we phonetically labeled the
test speech.
The results in the first column of Table 2 suggest
that the sub-word units learnt by the HMM are in-
deed interpretable as phones. The second column
suggests that a small amount of speech (5 minutes)
may be adequate to learn these units consistently.
The third column indicates that learning how to map
the learnt (allophonic) units to phones requires rela-
tively more transcribed speech.
4.3 Inspecting the Learnt Sub-word Units
The most frequent 3-, 4- and 5-state sequences in the
automatically labeled speech consistently matched
particular phones in specific articulatory contexts, as
6We heeded advice from the literature indicating that au-
tomatic methods model gross channel- and speaker-differences
before capturing differences between speech sounds.
HMM 24 min 5 min 19 min
label-to-phone 24 min 24 min 5 min
27 states 71.4% 70.9% 60.2%
70 states 84.4% 84.7% 75.8%
376 states 87.2% 86.8% 76.6%
Table 2: Phone recognition accuracy for different HMM
sizes (N), and with different amounts of speech used to
learn the HMM labeler and the label-to-phone transducer.
shown below, i.e. the HMM learns allophones.
HMM labels L-contxt Phone R-contxt
11, 28, 32 vowel t [e|a|o]
15, 17, 2 [g|k] [u|o] [?]
3, 17, 2 [k|t|g|d] a [k|t|g|d]
31, 5, 13, 5 vowel [s|sj|sy] vowel
17, 2, 31, 11 [g|t|k|d] [a|o] [t|k]
3, 30, 22, 34 [?] a silence
6, 24, 8, 15, 22 [?] o silence
4, 3, 17, 2, 21 [k|t] a [k|t]
4, 17, 24, 2, 31 [s|sy|z] o [t|d]
[t|d] o [s|sy|z]
For instance, the label sequence 3, 17, 2, corre-
sponds to an ?a? surrounded by stop consonants
{t, d, k, g}; further restricting the sequence to
4, 3, 17, 2, 21, results in restricting the context to the
unvoiced stops {t, k}. That such clusters are learnt
without knowledge of phones is remarkable.
References
T. Fukada, M. Bacchiani, K. K. Paliwal, and Y. Sagisaka.
1996. Speech recognition based on acoustically de-
rived segment units. In ICSLP, pages 1077?1080.
K. Maekawa. 2003. Corpus of spontaneous japanese:
its design and evaluation. In ISCA/IEEE Workshop on
Spontaneous Speech Processing and Recognition.
K. K. Paliwal and A. M. Kulkarni. 1987. Segmenta-
tion and labeling using vector quantization and its ap-
plication in isolated word recognition. Journal of the
Acoustical Society of India, 15:102?110.
H. Singer and M. Ostendorf. 1996. Maximum likelihood
successive state splitting. In ICASSP, pages 601?604.
J. Takami and S. Sagayama. 1992. A successive state
splitting algorithm for efficient allophone modeling.
In ICASSP, pages 573?576.
J. G. Wilpon, B. H. Juang, and L. R. Rabiner. 1987. An
investigation on the use of acoustic sub-word units for
automatic speech recognition. In ICASSP, pages 821?
824.
168
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2326?2334, Dublin, Ireland, August 23-29 2014.
Unsupervised Word Segmentation in Context
Gabriel Synnaeve and Isabelle Dautriche
LSCP, DEC
ENS Ulm, Paris, France
gabriel.synnaeve@gmail.com
isabelle.dautriche@gmail.com
Benjamin B
?
orschinger
Institut f?ur Computerlinguistik
Universit?at Heidelberg, Heidelberg, Germany.
benjamin.boerschinger@gmail.com
Mark Johnson
Department of Computer Science
Macquarie University, Sydney, Australia
mark.johnson@mq.edu.au
Emmanuel Dupoux
LSCP, DEC
EHESS, Paris, France
emmanuel.dupoux@gmail.com
Abstract
This paper extends existing word segmentation models to take non-linguistic context into ac-
count. It improves the token F-score of a top performing segmentation models by 2.5% on a 27k
utterances dataset. We posit that word segmentation is easier in-context because the learner is
not trying to access irrelevant lexical items. We use topics from a Latent Dirichlet Allocation
model as a proxy for ?activities? contexts, to label the Providence corpus. We present Adaptor
Grammar models that use these context labels, and we study their performance with and without
context annotations at test time.
1 Introduction and Previous Works
Segmentation of the speech stream into lexical units plays a central role in early language acquisition.
Because words are generally not uttered in isolation, one of the first task for infants learning a language is
to extract the words that make up the utterances they hear. Experimental research has shown that infants
are able to segment fluent speech into word-like units within the first year of life (Jusczyk and Aslin,
1995). How does this ability emerge? There is evidence that infants use a broad array of linguistic cues
to perform word segmentation (e.g., phonotactics (Jusczyk et al., 1993a), prosodic information (Jusczyk
et al., 1993b), statistical regularities (Saffran et al., 1996)). Past experimental and modeling research on
speech segmentation has mainly focused on linguistic cues, treating them as independent from other non-
linguistic cues naturally occurring in the child learning environment. Yet, language appears in context
and is constrained by the events occurring in the daily life of the child. For example, during an eating
event one is most likely to speak about food, while during a zoo-visit event, people are more likely
to talk about the animals they see. Activity contexts may provide a natural structure to speech that
would be readily be accessible to children. A recent study using dense recordings of a single child?s
language development (Roy et al., 2006) showed that words appearing in specific activity contexts are
learned faster (Roy et al., 2012). Relatedly, Johnson et al. (2010) showed that Adaptor Grammars (AGs)
performed better on a segmentation task when the model has access to a hand-annotated set of objects
present in the environment, that it can use to learn simultaneously word-object associations (see also
(Frank et al., 2009)). This supports the view that integrating multiple sources of information, linguistic
and non-linguistic, can improve learning.
Following this idea, we posit that information from the broader context in which a word has been
uttered may simplify the learning problem faced by the child. In particular, our hypothesis postulates
that speech segmentation is easier when using vocabularies that are related to a specific activity (eating,
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
2326
Table 1: Most probable words in the 7 final topics
egg book ball truck name color block
apple shape cat car school bear battery
banana square hat fire time crayon minute
milk circle tree piece today hair phone
butter triangle fish train day head puzzle
?food ?shapes ?playing ?toys ?time ?drawing ?garbage?
playing...), or place (kitchen, bedroom...). To evaluate this hypothesis, we applied topic modeling (Blei
et al., 2003) to automatically derive activity contexts on a corpus of child directed speech, the Provi-
dence corpus (Demuth et al., 2006), and tested the influence of such topics on a word segmentation task
extending the AG models used in (B?orschinger et al., 2012). We found that a model augmented with
the assumption that words are dependent upon the topic of the discourse (as a proxy for activity context)
performs better than the same model without access to the discourse topic. This suggests that the broader
context in which sentences are uttered may help in the word segmentation process, and could presumably
be used at various stages of language development.
The paper is structured as follows. Section 2 presents a novel approach to augment a corpus with
contextual annotations derived from topic models. Section 3 quickly explains Adaptor Grammars, the
framework that we used to express all our models. Section 4 presents all the models that were used in
the results. Section 5 describes the Providence corpus and the experimental setup. Section 6 shows our
quantitative and qualitative results. Finally, we discuss the implications for models of language learning.
2 Topics as Proxies for Contexts
Roy et al. (2012) found high correlations between human-annotated activity contexts and topics from a
latent Dirichlet allocation model (LDA) (Blei et al., 2003), thus showing that using topics as proxies for
contexts is a sound approach. Topic modeling infers a topic distribution for each ?document? (a bag of
words) in the corpus. Since ?documents? were not annotated in our corpus, we developed the following
3-step approach to automatically segment it into documents.
Firstly, for all the children of the Providence corpus, we used recording sessions as hard document
boundaries. We considered as a ?possible document? every contiguous sequence of sentences separated
by at least 10 seconds of silence, according to the orthographic transcript. We also identified ?possible
documents? using cues such as ?bye/hi?, indicating a change of participants. This segmentation resulted
in an over-segmented corpus (compared to context switches), yielding a total of 16, 742 documents.
Secondly, we used the gensim software (
?
Reh?u?rek and Sojka, 2010) to train a topic model (LDA)
1
,
and get the topic distributions for each of these documents. We used the symmetric KL-divergence to
measure the distance between two topic distributions before and after a ?possible document? boundary.
If the distance was above a threshold, we considered this boundary as a document boundary. Otherwise
we merged both ?possible documents? through this silence. The threshold was set empirically to dis-
criminate between two topic distributions that correspond to different activity contexts. After this step,
we assume that each of the resulting 8, 634 documents maps to an activity context.
Thirdly, we applied LDA again on this new segmentation to get the topic distribution, hence the activity
context, of each document. The number of topics is qualitatively chosen to correspond to the number of
main activity contexts (eating / playing / drawing / etc.) that occur in the Providence dataset (we used 7
topics), the resulting most topic specific words are shown in Table 1. Finally, for each document, we got
a distribution on topics, and we annotated the document with the most probable topic. By doing that, we
throw away graded information about the distribution on topics for each document. We could make use
of the full distribution, but here we are only interested in the most probable topic as a proxy for activity
context. We do not posit that the infants learn the topic models on linguistic cues while bootstrapping
speech and segmentation, but rather that they get activity context from non-linguistic cues.
1
We did LDA only on nouns (as they contain most of the semantics), weighted by TF-IDF.
2327
3 Adaptor Grammars
Adaptor Grammars (Johnson et al., 2007) are an extension of probabilistic context-free gram-
mars (PCFGs) that learn probability of entire subtrees as well as probabilities of rules. A PCFG
(N,W,R, S, ?) consists of a start symbol S, N and W disjoints sets of nonterminals and terminal sym-
bols respectively. R is a set of rules producing elements of N or W . Finally, ? is a set of distributions
over the rules R
X
,?X ? N (R
X
are the rules that expand X). An AG (N,W,R, S, ?, A,C) extends
the above PCFG with a subset (A ? N ) of adapted nonterminals, each of them (X ? A) having an
associated adaptor (C
X
? C). An AG defines a distribution over trees G
X
, ?X ? N ?W . If X /? A,
then G
X
is defined exactly as for a PCFG:
G
X
=
?
X?Y
1
...Y
n
?R
X
?
X?Y
1
...Y
n
TD
X
(G
Y
1
. . . G
Y
n
)
With TD
X
(G
1
. . . G
n
) the distribution over trees with root node X and each subtree t
i
? G
i
i.i.d. If
X ? A, then there is an additional indirection (composition) with the distribution H
X
:
G
X
=
?
X?Y
1
...Y
n
?R
X
?
X?Y
1
...Y
n
TD
X
(H
Y
1
. . . H
Y
n
)
H
X
? C
X
(G
X
)
We used C
X
adaptors following the Pitman-Yor process (PYP) (Perman et al., 1992; Teh, 2006) with
parameters a and b. The PYP generates (Zipfian) type frequencies that are similar to those that occur
in natural language (Goldwater et al., 2011). Metaphorically, if there are n customers and m tables, the
n+ 1th customer is assigned to table z
n+1
according to (?
k
is the Kronecker delta function):
z
n+1
|z
1
. . . z
n
?
ma+ b
n+ b
?
m+1
+
m
?
k=1
n
k
? a
n+ b
?
k
For an AG, this means that adapted non-terminals (X ? A) either expand to a previously generated
subtree (T (X)
k
) with probability proportional to how often it was visited (n
k
), or to a new subtree
(T (X)
m+1
) generated through the PCFG with probability proportional to ma+ b.
4 Word segmentation models
4.1 Unigram model
This most basic model just generates words as sequences of phonemes. AsWord is underlined, it means
it is adapted, and thus we learn a ?word unit -like? vocabulary. Phon is a nonterminal that expands to
all the phonemes of the language under consideration.
Sentence?Word
+
Word? Phon
+
where :
Word
+
?
{
Words?Word
Words?Word Words
4.2 Collocations and Syllabification
The baseline that we are using is commonly called the ?colloc-syll? model (Johnson, 2008; B?orschinger
et al., 2012) and is reported at 78% token F-score on the standard Brent version of the Bernstein-Ratner
corpus corpus (Johnson, 2008). It posits that sentences are collocations of words, and words are com-
posed of syllables. (Goldwater et al., 2009) showed how an assumption of independence between words
(a unigram model) led to under-segmentation. So, above the Word level, we take the collocations (co-
occurring sequences) of words into account.
2328
Furthermore, there is evidence that 8-month-old infants track syllable frequencies (Saffran et al.,
1996), and the ?colloc-syll? model can take that into account. Word splits into general syllables and
initial- or final- specific syllables. Syllables consist of onsets or codas (producing consonants), and nu-
clei (vowels). Onsets, nuclei and codas are adapted, thus allowing this model to memorize sequences or
consonants or sequences of vowels, dependent on their position in the word. Consonants and vowels are
the pre-terminals, their derivation is specified in the grammar into phonemes of the language.
Sentence? Colloc
+
Colloc?Word
+
Word? StructSyll
For notations purposes, all this syllabification is appended after Word by Word ? StructSyll.
All details about the collocations and syllabification grammars can be found in (Johnson, 2008). Here
is an example of a (good) parse of ?yuwanttusiD6bUk? with this model, skipping the StructSyll
derivations:
Sentence
Colloc
Word
bUk
Word
D6
Colloc
Word
si
Colloc
Word
tu
Word
want
Word
yu
4.3 Including topics (contexts)
To allow for the model to make use of the topics (used as proxies for contexts), we modify the grammar
by prefixing utterances with topic number (similarly to (Johnson et al., 2010)), ?K ? #topics:
Sentence? tK Colloc
+
tK
Colloc
tK
?Word
+
tK
For each Word
tK
, we can derive it into a common adapted Word by Word
tK
?Word. Consider this
lower level adaptor (Word): it learns a shared vocabulary independently of the topic (all contexts that
will derive b U k will increment the Word(b U k) pseudo-count). This Word-hierarchical model is
called share vocab.
Alternatively, we can learn a separate vocabulary for each topic, by having directly: Word
tK
?
StructSyll (note that all words then share the same syllabic structure). Words are split across different
topics and need to be adapted for each topic in which they appear. This flat structure vocabulary model
is called split vocab.
4.4 Allowing for non context-specific words
Sentences are not composed only of context-specific words, thus we need a third type of extension that
allows for topic-independent and topic-specific words to mix. For this, we add topic-independent types
of Colloc and Word that can be used across all topics, but we force each sentence to have at least one
topical collocation:
Sentence ? tK (Colloc
+
|Colloc
+
tK
) Colloc
+
tK
(Colloc
+
|Colloc
+
tK
)
Colloc
tK
? Word
+
tK
Colloc ? Word
+
Word
tK
? StructSyll
Word ? StructSyll
2329
Parentheses denote that these terms are optionals, and ?|? denotes ?or?. Both Word
tK
and Word are
adapted, but this time on the same level of hierarchy. This model allows the use of both topic-specific and
common words in sentences, and it learns #topics + 1 vocabularies. We call this model with common.
An example of a correct parse with this model is given by:
Sentence
Colloc t3
Word t3
bUk
Word t3
D6
Word t3
si
Colloc
Word
tu
Word
want
Word
yu
t3
5 Experimental setup
The Providence corpus (Demuth et al., 2006) consists of audio and video, weekly or bi-weekly, record-
ings of 6 monolingual English-speaking children home interactions. Each recording is approximatively
1 hour long. This corpus spans approximatively from their first to third year. We used the whole corpus
to extract the topics to get more stable and general activity contexts. For all the following results, we
used only the Naima portion between 11 months and 24 months, consisting in 26,425 utterances (sen-
tences) and 135,389 tokens (words). The input consist in DARPABET-encoded sequences of phonemes
with about 4200 word-types in the Naima subset. We followed the same preparation procedure as in
(B?orschinger et al., 2012), where more details about the corpus can be found.
We used the last version of Mark Johnson?s Adaptor Grammars software
2
. All the additional code
(preparation, topics, grammars, learning) to reproduce these experiments and results is freely available
online
3
, along with the datasets annotations derived from topic modeling
4
. For the adaptors, we used a
Beta(1, 1) (uniform) prior on the PYP a parameter, and a sparse Gamma(100, 0.01) prior on the PYP
b parameter. We ran 500 iterations (finishing at ? 0.05% of log posterior variation between the lasts
iterations) with several runs for each subset of the Naima dataset.
6 Results
6.1 Unsupervised words segmentation
Table 2: Mean (token and boundary) F-scores (f), precisions (p), and recalls (r) for different models
depending on the size of dataset (age range).
months baseline share vocab split vocab with common
token f p r f p r f p r f p r
11-12 .80 .79 .81 .77 .76 .78 .77 .75 .78 .77 .75 .78
11-15 .81 .81 .82 .76 .78 .75 .81 .79 .82 .82 .81 .83
11-19 .82 .82 .83 .77 .78 .76 .81 .81 .82 .83 .82 .84
11-22 .81 .82 .81 .77 .79 .75 .82 .81 .83 .83 .82 .84
boundary f p r f p r f p r f p r
11-12 .90 .88 .91 .88 .87 .89 .87 .85 .90 .88 .85 .90
11-15 .91 .91 .92 .89 .91 .86 .91 .89 .92 .91 .90 .93
11-19 .92 .92 .93 .90 .92 .88 .92 .91 .93 .92 .91 .94
11-22 .92 .93 .91 .90 .93 .87 .92 .91 .93 .93 .91 .94
The key metric of interest is the token F-score (harmonic mean of precision and recall of words).
Table 2 gives all the scores for an increasingly large dataset (as in (B?orschinger et al., 2012)). Figure 1
shows the month-by-month evolution of the token F-score of the different models. We can see that
2
http://web.science.mq.edu.au/
?
mjohnson/
3
https://github.com/SnippyHolloW/contextual_word_segmentation
4
https://github.com/SnippyHolloW/contextual_word_segmentation/tree/master/
ProvidenceFinal/Final
2330
Figure 1: Token F-scores (and standard deviations) evolution with an increasingly bigger and richer
dataset (11 months to ?X-axis value? months), computed on 8 runs of 500 iterations per data point.
0.750
0.775
0.800
0.825
24232221201918171615141312 age in months
token f
?score modelbaselineshare vocabsplit vocabwith common
context-based models need more data to get good performances (several vocabularies to learn), but they
seem more resilient to over-segmentation.
Preliminary results confirm the trend of baseline scores getting slowly worse at 25 and 26 months
while with common and split vocab stabilize (not plotted here). We also tried models for which we
can have the ?common vocabulary? derived only at the level of the collocations (making topic-specific
collocations topic-pure as in split vocab for instance), or only at the level of the words (allowing for
topic-specific collocations deriving in only common words if needed). Both models are worse than split
vocab and with common.
Using a shared global vocabulary while being able to learn (through adaptation) different topic-specific
vocabularies does not seem to be a solution: share vocab performs worse than the baseline. Token recall
and boundary recall are worse off (see Table 2), suggesting that fewer words are correctly adapted.
Maybe that is because this is the only model with two levels of adapted word hierarchies (Word
tK
and
Word). Sharing a lower-level vocabulary (Word) still does not allow for context vocabularies (Word
tK
)
to mix, thus is simply harder to train. Having only one vocabulary per context (split vocab) is a slight
improvement over the baseline, even though it is not significant (95% confidence interval) before 22
months. Models allowing for both topic-specific vocabularies and a common vocabulary to be learned
are the best: with common is significantly (95% confidence interval) better than the baseline, starting
from 20 months (Figure 1). The improvement seems to be due to better token (and boundary) recall
(Table 2), suggesting that more words are learned. By looking at their lexicons at 24 months, topic-
dependent models have slightly larger lexicon recalls and worse lexicon precisions than the baseline.
This means that the additional true word-types that they learn are more frequently correctly used than the
false word-types (otherwise the token F-scores would be reversed, e.g. between split vocab and baseline).
2331
Figure 2: Mean token F-scores (and standard deviations) on 20% held-out test data for 6 different random
splits of Naima from 11 to 22 months, 500 iterations each. Grey for baseline on test, green and blue for
context-dependent models on test and no prefix conditions respectively.
Table 3: Most probable words (? P (word|topic = k)) in the 7 recovered topics at test time without topic
annotations (no prefix condition) for the with common model (we omitted phonemes clusters yielding
non-words).
bread elephant lego Michael skinny stick bubble
delicious owl doctor shorts massage remember pasta
avocado wearing brush towel ostrich track spirals
porridge turkey change shirts nurse forget squirrels
raisin haircut squeeze pirates hammer oink thumb
biscuit turtle music tangled ruby towed pentagon
food animals play clothes (messy) verbs ?shapes
6.2 Recovery of the topics on held-out data
To check whether these models generalize to unseen utterances, and possibly unseen vocabulary, we
looked at the scores of held-out data (80/20% train/test split of the Naima 11 to 22 months dataset).
Token F-scores for this test condition are shown in green and grey in Figure 2. This separates low-
frequency collocations to be used at test time and those seen at training time, both for context aware
models and the basic baseline model. The F-scores show the same pattern as in the previous experiment,
with context-aware models (with common and split vocab here) performing better than the baseline.
The topics are learned on the orthographic transcription of the whole Providence corpus (6 children),
while we test only on the Naima dataset. Still, to check that these results are not simply due to additional
information (leaked somehow in the form of the tK prefix), we produced another held-out condition,
without topic ( tK) prefixes. Models can use topic-specific vocabularies learned during training, but
they are given no context information at test time. Token F-scores for this no prefix condition are shown
in blue (and grey for the baseline) in Figure 2. The fact that no prefix performance is on par with the
test condition means that contextual cues are not only important at test time, but particularly so while
learning the vocabulary. In other words, the model acquires its vocabularies making use of the additional
context. In the test setting, it is evaluated on novel utterances for which additional context information
is available. In the no prefix condition it is evaluated on novel utterances for which no additional context
information is available. This means that topic-specific vocabulary learned during training is successfully
used in a consistent way at test time. To confirm this qualitatively, we looked at the most probable words
(after unsupervised segmentation from the phonemic input) in recovered topics at test time in the no
prefix condition. They are shown in Table 3, and they exhibit some of the topics that were found on the
orthographic transcript (as they are not limited to nouns, a topic for ?verbs? appears).
2332
7 Conclusion
We have shown that contextual information helps segmenting speech into word-like units. We used
topic modeling as a proxy for richer contextual annotations, as (Roy et al., 2012) have shown high cor-
relation between contexts and automatically derived topics. We modified existing Adaptor Grammar
segmentation models (Johnson, 2008; Johnson and Goldwater, 2009), to be able to learn topic-specific
vocabularies. We applied this approach to a large child directed speech corpus that was previously used
for segmentation (B?orschinger et al., 2012). Our model with the capacity to use both a topic-specific
vocabulary and a common vocabulary (with common) produces better segmentation scores, ending up
with at least 2.5% better absolute F-scores than its context-oblivious counterpart (baseline). More gen-
erally, both models that learn specialized vocabularies do not get worse F-scores with increasing data
(Figure 1). Particularly, they seem to fix a well-known problem of previous models like ?colloc-syll?
(our baseline), that ?overlearn? by over-segmenting frequent morphemes as single words (B?orschinger
et al., 2012). We have controlled for the additional information of giving the topic ( tK), and we have
found out that contextual information helps at training time.
It would be interesting to look into the link between semantics and syntax in recovered topics. Fur-
ther work should integrate syntax (e.g. function words), stress cues and prosody from the audio signal
(B?orschinger and Johnson, 2014), use even less supervision for contexts, and be applied to other lan-
guages. We believe that language acquisition is not a simple sequential process and that segmentation,
syntax, and word meaning bootstrap each others. This is only a first step towards integrating multiple
sources of information and different modalities at all steps of language acquisition.
Acknowledgments
This project is funded in part by the European Research Council (ERC-2011-AdG-295810 BOOT-
PHON), the Agence Nationale pour la Recherche (ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02
PSL*), the Fondation de France, the Ecole de Neurosciences de Paris, and the Region Ile de France (DIM
cerveau et pense).
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. J. Mach. Learn. Res.,
3:993?1022, March.
Benjamin B?orschinger and Mark Johnson. 2014. Exploring the role of stress in Bayesian word segmentation using
Adaptor Grammars. Transactions of the Association of Computational Linguistics, 2:93?104, February.
Benjamin B?orschinger, Katherine Demuth, and Mark Johnson. 2012. Studying the effect of input size for bayesian
word segmentation on the providence corpus. In COLING, pages 325?340.
Katherine Demuth, Jennifer Culbertson, and Jennifer Alter. 2006. Word-minimality, epenthesis and coda licensing
in the early acquisition of english. Language and Speech, 49(2):137?173.
Michael C Frank, Noah D Goodman, and Joshua B Tenenbaum. 2009. Using speakers? referential intentions to
model early cross-situational word learning. Psychological Science, 20(5):578?585.
Sharon Goldwater, Thomas L Griffiths, and Mark Johnson. 2009. A bayesian framework for word segmentation:
Exploring the effects of context. Cognition, 112(1):21?54.
Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2011. Producing power-law distributions and damping
word frequencies with two-stage language models. Journal of Machine Learning Research, 12(Jul):2335?2382.
Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric bayesian inference: experiments on
unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 317?325. Association for Computational Linguistics.
Mark Johnson, Thomas L Griffiths, and Sharon Goldwater. 2007. Adaptor grammars: A framework for specifying
compositional nonparametric bayesian models. Advances in neural information processing systems, 19:641.
2333
Mark Johnson, Katherine Demuth, Michael Frank, and Bevan Jones. 2010. Synergies in learning words and their
referents. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in
Neural Information Processing Systems 23, pages 1018?1026.
Mark Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic
structure. In ACL, pages 398?406.
Peter W. Jusczyk and Richard N. Aslin. 1995. Infants detection of the sound patterns of words in fluent speech.
Cognitive psychology, 29(1):123.
Peter W. Jusczyk, Anne Cutler, and Nancy J. Redanz. 1993a. Infants? preference for the predominant stress
patterns of english words. Child development, 64(3):675687.
Peter W. Jusczyk, Angela D. Friederici, Jeanine MI Wessels, Vigdis Y. Svenkerud, and Ann Marie Jusczyk.
1993b. Infants sensitivity to the sound patterns of native language words. Journal of Memory and Language,
32(3):402420.
Mihael Perman, Jim Pitman, and Marc Yor. 1992. Size-biased sampling of poisson point processes and excursions.
Probability Theory and Related Fields, 92(1):21?39.
Radim
?
Reh?u?rek and Petr Sojka. 2010. Software Framework for Topic Modelling with Large Corpora. In Proceed-
ings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45?50, Valletta, Malta, May.
ELRA. http://is.muni.cz/publication/884893/en.
Deb Roy, Rupal Patel, Philip DeCamp, Rony Kubat, Michael Fleischman, Brandon Roy, Nikolaos Mavridis, Ste-
fanie Tellex, Alexia Salata, Jethran Guinness, et al. 2006. The human speechome project. In Symbol Grounding
and Beyond, pages 192?196. Springer.
Brandon C Roy, Michael C Frank, and Deb Roy. 2012. Relating activity contexts to early word learning in dense
longitudinal data. In Proceedings of the 34th Annual Cognitive Science Conference.
Jenny R. Saffran, Richard N. Aslin, and Elissa L. Newport. 1996. Statistical learning by 8-month old infants.
Science, 274(5294):1926?1928.
Yee Whye Teh. 2006. A hierarchical Bayesian language model based on Pitman-Yor processes. In Proceedings
of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 985?992.
2334
Proceedings of the ACL Student Research Workshop, pages 165?171,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A corpus-based evaluation method for Distributional Semantic Models
Abdellah Fourtassi1,2 Emmanuel Dupoux2,3
abdellah.fourtassi@gmail.com emmanuel.dupoux@gmail.com
1Institut d?Etudes Cognitives, Ecole Normale Superieure, Paris
2Laboratoire de Sciences Cognitives et Psycholinguistique, CNRS, Paris
3Ecole des Hautes Etudes en Sciences Sociales, Paris
Abstract
Evaluation methods for Distributional Se-
mantic Models typically rely on behav-
iorally derived gold standards. These
methods are difficult to deploy in lan-
guages with scarce linguistic/behavioral
resources. We introduce a corpus-based
measure that evaluates the stability of the
lexical semantic similarity space using a
pseudo-synonym same-different detection
task and no external resources. We show
that it enables to predict two behavior-
based measures across a range of parame-
ters in a Latent Semantic Analysis model.
1 Introduction
Distributional Semantic Models (DSM) can be
traced back to the hypothesis proposed by Harris
(1954) whereby the meaning of a word can be in-
ferred from its context. Several implementations
of Harris?s hypothesis have been proposed in the
last two decades (see Turney and Pantel (2010) for
a review), but comparatively little has been done
to develop reliable evaluation tools for these im-
plementations. Models evaluation is however an
issue of crucial importance for practical applica-
tions, i.g., when trying to optimally set the model?s
parameters for a given task, and for theoretical rea-
sons, i.g., when using such models to approximate
semantic knowledge.
Some evaluation techniques involve assigning
probabilities to different models given the ob-
served corpus and applying maximum likelihood
estimation (Lewandowsky and Farrell, 2011).
However, computational complexity may prevent
the application of such techniques, besides these
probabilities may not be the best predictor for the
model performance on a specific task (Blei, 2012).
Other commonly used methods evaluate DSMs by
comparing their semantic representation to a be-
haviorally derived gold standard. Some standards
are derived from the TOEFL synonym test (Lan-
dauer and Dumais, 1997), or the Nelson word
associations norms (Nelson et al, 1998). Oth-
ers use results from semantic priming experiments
(Hutchison et al, 2008) or lexical substitutions er-
rors (Andrews et al, 2009). Baroni and Lenci
(2011) set up a more refined gold standard for En-
glish specifying different kinds of semantic rela-
tionship based on dictionary resources (like Word-
Net and ConceptNet).
These behavior-based evaluation methods are
all resource intensive, requiring either linguistic
expertise or human-generated data. Such meth-
ods might not always be available, especially in
languages with fewer resources than English. In
this situation, researchers usually select a small set
of high-frequency target words and examine their
nearest neighbors (the most similar to the target)
using their own intuition. This is used in partic-
ular to set the model parameters. However, this
rather informal method represents a ?cherry pick-
ing? risk (Kievit-Kylar and Jones, 2012), besides
it is only possible for languages that the researcher
speaks.
Here we introduce a method that aims at pro-
viding a rapid and quantitative evaluation for
DSMs using an internal gold standard and re-
quiring no external resources. It is based on a
simple same-different task which detects pseudo-
synonyms randomly introduced in the corpus. We
claim that this measure evaluates the intrinsic
ability of the model to capture lexical semantic
similarity. We validate it against two behavior-
based evaluations (Free association norms and the
TOEFL synonym test) on semantic representa-
tions extracted from a Wikipedia corpus using one
of the most commonly used distributional seman-
tic models : the Latent Semantic Analysis (LSA,
Landauer and Dumais (1997)).
In this model, we construct a word-document
matrix. Each word is represented by a row, and
165
each document is represented by a column. Each
matrix cell indicates the occurrence frequency of
a given word in a given context. Singular value
decomposition (a kind of matrix factorization) is
used to extract a reduced representation by trun-
cating the matrix to a certain size (which we call
the semantic dimension of the model). The cosine
of the angle between vectors of the resulting space
is used to measure the semantic similarity between
words. Two words end up with similar vectors if
they co-occur multiple times in similar contexts.
2 Experiment
We constructed three successively larger corpora
of 1, 2 and 4 million words by randomly select-
ing articles from the original ?Wikicorpus? made
freely available on the internet by Reese et al
(2010). Wikicorpus is itself based on articles from
the collaborative encyclopedia Wikipedia. We se-
lected the upper bound of 4 M words to be com-
parable with the typical corpus size used in theo-
retical studies on LSA (see for instance Landauer
and Dumais (1997) and Griffiths et al (2007)). For
each corpus, we kept only words that occurred at
least 10 times and we excluded a stop list of high
frequency words with no conceptual content such
as: the, of, to, and ... This left us with a vocab-
ulary of 8 643, 14 147 and 23 130 words respec-
tively. For the simulations, we used the free soft-
ware Gensim (R?ehu?r?ek and Sojka, 2010) that pro-
vides an online Python implementation of LSA.
We first reproduced the results of Griffiths et al
(2007), from which we derived the behavior-based
measure. Then, we computed our corpus-based
measure with the same models.
2.1 The behavior-based measure
Following Griffiths et al (2007), we used the
free association norms collected by Nelson et al
(1998) as a gold standard to study the psychologi-
cal relevance of the LSA semantic representation.
The norms were constructed by asking more than
6000 participants to produce the first word that
came to mind in response to a cue word. The
participants were presented with 5,019 stimulus
words and the responses (word associates) were
ordered by the frequency with which they were
named. The overlap between the words used in
the norms and the vocabulary of our smallest cor-
pus was 1093 words. We used only this restricted
overlap in our experiment.
In order to evaluate the performance of LSA
models in reproducing these human generated
data, we used the same measure as in Griffiths
et al (2007): the median rank of the first associates
of a word in the semantic space. This was done in
three steps : 1) for each word cue Wc, we sorted
the list of the remaining words Wi in the overlap
set, based on their LSA cosine similarity with that
cue: cos(LSA(Wc), LSA(Wi)), with highest co-
sine ranked first. 2) We found the ranks of the first
three associates for that cue in that list. 3) We ap-
plied 1) and 2) to all words in the overlap set and
we computed the median rank for each of the first
three associates.
Griffiths et al (2007) tested a set of seman-
tic dimensions going from 100 to 700. We ex-
tended the range of dimensions by testing the
following set : [2,5,10,20,30,40,50,100, 200,
300,400,500,600,700,800,1000]. We also manip-
ulated the number of successive sentences to be
taken as defining the context of a given word (doc-
ument size), which we varied from 1 to 100.
In Figure 1 we show the results for the 4 M size
corpus with 10 sentences long documents.
Figure 1 : The median rank of the three associates as a
function of the semantic dimensions (lower is better)
For the smaller corpora we found similar results
as we can see from Table 1 where the scores rep-
resent the median rank averaged over the set of
dimensions ranging from 10 to 1000. As found
in Griffiths et al (2007), the median rank measure
predicts the order of the first three associates in the
norms.
In the rest of the article, we will need to char-
acterize the semantic model by a single value. In-
stead of taking the median rank of only one of the
166
Size associate 1 associate 2 associate 3
1 M 78.21 152.18 169.07
2 M 57.38 114.57 131
4 M 54.57 96.5 121.57
Table 1 : The median rank of the first three associates for
different sizes
associates, we will consider a more reliable mea-
sure by averaging over the median ranks of the
three associates across the overlap set. We will
call this measure the Median Rank.
2.2 The Pseudo-synonym detection task
The measure we introduce in this part is based
on a Same-Different Task (SDT). It is described
schematically in Figure 2, and is computed as
follows: for each corpus, we generate a Pseudo-
Synonym-corpus (PS-corpus) where each word in
the overlap set is randomly replaced by one of two
lexical variants. For example, the word ?Art? is
replaced in the PS-corpus by ?Art1? or ?Art2?. In
the derived corpus, therefore, the overlap lexicon
is twice as big, because each word is duplicated
and each variant appears roughly with half of the
frequency of the original word.
The Same-Different Task is set up as follows: a
pair of words is selected at random in the derived
corpus, and the task is to decide whether they are
variants of one another or not, only based on their
cosine distances. Using standard signal detection
techniques, it is possible to use the distribution
of cosine distances across the entire list of word
pairs in the overlap set to compute a Receiver
Operating Characteristic Curve (Fawcett, 2006),
from which one derives the area under the curve.
We will call this measure : SDT-?. It can be
interpreted as the probability that given two pairs
of words, of which only one is a pseudo-synonym
pair, the pairs are correctly identified based on
cosine distance only. A value of 0.5 represents
pure chance and a value of 1 represents perfect
performance.
It is worth mentioning that the idea of gen-
erating pseudo-synonyms could be seen as the
opposite of the ?pseudo-word? task used in
evaluating word sense disambiguation models
(see for instance Gale et al (1992) and Dagan
et al (1997)). In this task, two different words
w1 and w2 are combined to form one ambiguous
pseudo-word W12 = {w1, w2} which replaces
both w1 and w2 in the test set.
We now have two measures evaluating the
quality of a given semantic representation: The
Median Rank (behavior-based) and the SDT-?
(corpus-based). Can we use the latter to predict
the former? To answer this question, we compared
the performance of both measures across differ-
ent semantic models, document lengths and cor-
pus sizes.
3 Results
In Figure 3 (left), we show the results of the
behavior-based Median Rank measure, obtained
from the three corpora across a number of seman-
tic dimensions. The best results are obtained with
a few hundred dimensions. It is important to high-
light the fact that small differences between high
dimensional models do not necessarily reflect a
difference in the quality of the semantic repre-
sentation. In this regard, Landauer and Dumais
(1997) argued that very small changes in com-
puted cosines can in some cases alter the LSA or-
dering of the words and hence affect the perfor-
mance score. Therefore only big differences in the
Median Ranks could be explained as a real dif-
ference in the overall quality of the models. The
global trend we obtained is consistent with the re-
sults in Griffiths et al (2007) and with the findings
in Landauer and Dumais (1997) where maximum
performance for a different task (TOEFL synonym
test) was obtained over a broad region around 300
dimensions.
Besides the effect of dimensionality, Figure 3 (left)
indicates that performance gets better as we in-
crease the corpus size.
In Figure 3 (right) we show the corresponding re-
sults for the corpus-based SDT-? measure. We can
see that SDT-? shows a parallel set of results and
correctly predicts both the effect of dimensionality
and the effect of corpus size. Indeed, the general
trend is quite similar to the one described with the
Median Rank in that the best performance is ob-
tained for a few hundred dimensions and the three
curves show a better score for large corpora.
Figure 4 shows the effect of document length on
the Median Rank and SDT-?. For both measures,
we computed these scores and averaged them over
the three corpora and the range of dimensions go-
ing from 100 to 1000. As we can see, SDT-? pre-
dicts the psychological optimal document length,
167
Figure 2 : Schematic description of the Same-Different Task used.
which is about 10 sentences per document. In the
corpus we used, this gives on average of about 170
words/document. This value confirms the intuition
of Landauer and Dumais (1997) who used a para-
graph of about 150 word/document in their model.
Finally, Figure 5 (left) summarizes the entire
set of results. It shows the overall correlation
between SDT-? and the Median Rank. One
point in the graph corresponds to a particular
choice of semantic dimension, document length
and corpus size. To measure the correlation, we
use the Maximal Information Coefficient (MIC)
recently introduced by Reshef et al (2011). This
measure captures a wide range of dependencies
between two variables both functional and not.
For functional and non-linear associations it gives
a score that roughly equals the coefficient of
determination (R2) of the data relative to the
regression function. For our data this correlation
measure yields a score of MIC = 0.677 with
(p < 10?6).
In order to see how the SDT-? measure would
correlate with another human-generated bench-
mark, we ran an additional experiment using the
TOEFL synonym test (Landauer and Dumais,
1997) as gold standard. It contains a list of
80 questions consisting of a probe word and
four answers (only one of which is defined as
the correct synonym). We tested the effect of
semantic dimensionality on a 6 M word sized
Wikipedia corpus where documents contained
respectively 2, 10 and 100 sentences for each
series of runs. We kept only the questions for
which the probes and the 4 answers all appeared
in the corpus vocabulary. This left us with a
set of 43 questions. We computed the response
of the model on a probe word by selecting the
answer word with which it had the smallest cosine
angle. The best performance (65.1% correct) was
obtained with 600 dimensions. This is similar
to the result reported in Landauer and Dumais
(1997) where the best performance obtained was
64.4% (compared to 64.5% produced by non-
native English speakers applying to US colleges).
The correlation with SDT-? is shown in Figure
5 (right). Here again, our corpus-based measure
predicts the general trend of the behavior-based
measure: higher values of SDT-? correspond
to higher percentage of correct answers. The
correlation yields a score of MIC = 0.675 with
(p < 10?6).
In both experiments, we used the overlap set of
the gold standard with the Wikicorpus to compute
the SDT-? measure. However, as the main idea
is to apply this evaluation method to corpora for
which there is no available human-generated gold
standards, we computed new correlations using a
SDT-? measure computed, this time, over a set
of randomly selected words. For this purpose we
used the 4M corpus with 10 sentences long docu-
ments and we varied the semantic dimensions. We
used the Median Rank computed with the Free as-
sociation norms as a behavior-based measure.
We tested both the effect of frequency and size:
we varied the set size from 100 to 1000 words
which we randomly selected from three frequency
ranges : higher than 400, between 40 and 400 and
between 40 and 1. We chose the limit of 400 so
that we can have at least 1000 words in the first
range. On the other hand, we did not consider
words which occur only once because the SDT-?
requires at least two instances of a word to gener-
ate a pseudo-synonym.
The correlation scores are shown in Table 2.
Based on the MIC correlation measure, mid-
168
Figure 3 : The Median rank (left) and SDT-? (right) as a function of a number of dimensions and corpus sizes. Document size
is 10 sentences.
Figure 4 : The Median rank (left) and SDT-? (right) as a function of document length (number of sentences). Both measures
are averaged over the three corpora and over the range of dimensions going from 100 to 1000.
Figure 5 : Overall correlation between Median Rank and SDT-? (left) and between Correct answers in TOEFL synonym test
and SDT-? (right) for all the runs. .
169
Freq. x 1 < x < 40 40 < x < 400 x > 400 All Overlap
Size 100 500 1000 100 500 1000 100 500 1000 ? 4 M 1093
MIC 0.311 0.219 0.549? 0.549? 0.717? 0.717? 0.311 0.205 0.419 0.549? 0.717?
* : p < 0.05
Table 2 : Correlation scores of the Median Rank with the SDT-? measure computed over randomly selected words from the
corpus, the whole lexicon and the overlap with the free association norms. We test the effect of frequency and set size.
frequency words yield better scores. The corre-
lations are as high as the one computed with the
overlap even with a half size set (500 words).
The overlap is itself mostly composed of mid-
frequency words, but we made sure that the ran-
dom test sets have no more than 10% of their
words in the overlap. Mid-frequency words are
known to be the best predictors of the conceptual
content of a corpus, very common and very rare
terms have a weaker discriminating or ?resolving?
power (Luhn, 1958).
4 Discussion
We found that SDT-? enables to predict the out-
come of behavior-based evaluation methods with
reasonable accuracy across a range of parameters
of a LSA model. It could therefore be used as a
proxy when human-generated data are not avail-
able. When faced with a new corpus and a task
involving similarity between words, one could im-
plement this rather straightforward method in or-
der, for instance, to set the semantic model param-
eters.
The method could also be used to compare the
performance of different distributional semantic
models, because it does not depend on a partic-
ular format for semantic representation. All that is
required is the existence of a semantic similarity
measure between pairs of words. However, fur-
ther work is needed to evaluate the robustness of
this measure in models other than LSA.
It is important to keep in mind that the correla-
tion of our measure with the behavior-based meth-
ods only indicates that SDT-? can be trusted, to
some extent, in evaluating these semantic tasks.
It does not necessarily validate its ability to as-
sess the entire semantic structure of a distribu-
tional model. Indeed, the behavior-based methods
are dependent on particular tasks (i.g., generating
associates, or responding to a multiple choice syn-
onym test) hence they represent only an indirect
evaluation of a model, viewed through these spe-
cific tasks.
It is worth mentioning that Baroni and Lenci
(2011) introduced a comprehensive technique that
tries to assess simultaneously a variety of seman-
tic relations like meronymy, hypernymy and coor-
dination. Our measure does not enable us to as-
sess these relations, but it could provide a valu-
able tool to explore other fine-grained features of
the semantic structure. Indeed, while we intro-
duced SDT-? as a global measure over a set of test
words, it can also be computed word by word. In-
deed, we can compute how well a given seman-
tic model can detect that ?Art1? and ?Art2? are
the same word, by comparing their semantic dis-
tance to that of random pairs of words. Such a
word-specific measure could assess the semantic
stability of different parts of the lexicon such as
concrete vs. abstract word categories, or the distri-
bution properties of different linguistic categories
(verb, adjectives, ..). Future work is needed to as-
sess the extent to which the SDT-? measure and
its word-level variant provide a general framework
for DSMs evaluation without external resources.
Finally, one concern that could be raised by our
method is the fact that splitting words may affect
the semantic structure of the model we want to as-
sess because it may alter the lexical distribution in
the corpus, resulting in unnaturally sparse statis-
tics. There is in fact evidence that corpus attributes
can have a big effect on the extracted model (Srid-
haran and Murphy, 2012; Lindsey et al, 2007).
However, as shown by the high correlation scores,
the introduced pseudo-synonyms do not seem to
have a dramatic effect on the model, at least as far
as the derived SDT-? measure and its predictive
power is concerned. Moreover, we showed that in
order to apply the method, we do not need to use
the whole lexicon, on the contrary, a small test set
of about 500 random mid-frequency words (which
represents less than 2.5 % of the total vocabulary)
was shown to lead to better results. However, even
if the results are not directly affected in our case,
future work needs to investigate the exact effect
word splitting may have on the semantic model.
170
References
Andrews, M., G. Vigliocco, and D. Vinson (2009).
Integrating experiential and distributional data
to learn semantic representations. Psychologi-
cal Review 116, 463?498.
Baroni, M. and A. Lenci (2011). How we
BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP 2011 Geometri-
cal Models for Natural Language Semantics
(GEMS 2011) Workshop, East Stroudsburg PA:
ACL, pp. 1?10.
Blei, D. (2012). Probabilistic topic models. Com-
munications of the ACM 55(4), 77?84.
Dagan, I., L. Lee, and F. Pereira (1997).
Similarity-based methods for word sense dis-
ambiguation. In Proceedings of the 35th
ACL/8th EACL, pp. 56?63.
Fawcett, T. (2006). An introduction to ROC anal-
ysis. Pattern Recognition Letters 27(8), 861?
874.
Gale, W., K. Church, and D. Yarowsky (1992).
Work on statistical methods for word sense dis-
ambiguation. Workings notes, AAAI Fall Sym-
posium Series, Probabilistic Approaches to Nat-
ural Language, 54?60.
Griffiths, T., M. Steyvers, and J. Tenenbaum
(2007). Topics in semantic representation. Psy-
chological Review 114, 114?244.
Harris, Z. (1954). Distributional structure.
Word 10(23), 146?162.
Hutchison, K., D. Balota, M. Cortese, and J. Wat-
son (2008). Predicting semantic priming at the
item level. Quarterly Journal of Experimental
Psychology 61(7), 1036?1066.
Kievit-Kylar, B. and M. N. Jones (2012). Visualiz-
ing multiple word similarity measures. Behav-
ior Research Methods 44(3), 656?674.
Landauer, T. and S. Dumais (1997). A solution
to plato?s problem: The latent semantic anal-
ysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological Re-
view 104(2), 211?240.
Lewandowsky, S. and S. Farrell (2011). Compu-
tational modeling in cognition : principles and
practice. Thousand Oaks, Calif. : Sage Publi-
cations.
Lindsey, R., V. Veksler, and A. G. andWayne Gray
(2007). Be wary of what your computer reads:
The effects of corpus selection on measuring
semantic relatedness. In Proceedings of the
Eighth International Conference on Cognitive
Modeling, pp. 279?284.
Luhn, H. P. (1958). The automatic creation of lit-
erature abstracts. IBM Journal of Research and
Development 2(2), 157?165.
Nelson, D., C. McEvoy, and T. Schreiber (1998).
The university of south florida word association,
rhyme, and word fragment norms.
Reese, S., G. Boleda, M. Cuadros, L. Padro, and
G. Rigau (2010). Wikicorpus: A word-sense
disambiguated multilingual wikipedia corpus.
In Proceedings of 7th Language Resources and
Evaluation Conference (LREC?10), La Valleta,
Malta.
R?ehu?r?ek, R. and P. Sojka (2010). Software frame-
work for topic modelling with large corpora. In
Proceedings of the LREC 2010 Workshop on
New Challenges for NLP Frameworks, Valletta,
Malta, pp. 45?50.
Reshef, D., Y. Reshef, H. Finucane, S. Gross-
man, G. McVean, P. Turnbaugh, E. Lander,
M. Mitzenmacher, and P. Sabeti (2011). De-
tecting novel associations in large datasets. Sci-
ence 334(6062), 1518?1524.
Sridharan, S. and B. Murphy (2012). Modeling
word meaning: distributional semantics and the
sorpus quality-quantity trade-off. In Proceed-
ings of the 3rd Workshop on Cognitive Aspects
of the Lexicon, COLING 2012, Mumbai, pp.
53?68.
Turney, P. D. and P. Pantel (2010). From frequency
to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research 37,
141?188.
171
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 282?292,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Modelling function words improves unsupervised word segmentation
Mark Johnson
1,2
, Anne Christophe
3,4
, Katherine Demuth
2,6
and Emmanuel Dupoux
3,5
1
Department of Computing, Macquarie University, Sydney, Australia
2
Santa Fe Institute, Santa Fe, New Mexico, USA
3
Ecole Normale Sup?erieure, Paris, France
4
Centre National de la Recherche Scientifique, Paris, France
5
Ecole des Hautes Etudes en Sciences Sociales, Paris, France
6
Department of Linguistics, Macquarie University, Sydney, Australia
Abstract
Inspired by experimental psychological
findings suggesting that function words
play a special role in word learning, we
make a simple modification to an Adaptor
Grammar based Bayesian word segmenta-
tion model to allow it to learn sequences
of monosyllabic ?function words? at the
beginnings and endings of collocations
of (possibly multi-syllabic) words. This
modification improves unsupervised word
segmentation on the standard Bernstein-
Ratner (1987) corpus of child-directed En-
glish by more than 4% token f-score com-
pared to a model identical except that it
does not special-case ?function words?,
setting a new state-of-the-art of 92.4% to-
ken f-score. Our function word model as-
sumes that function words appear at the
left periphery, and while this is true of
languages such as English, it is not true
universally. We show that a learner can
use Bayesian model selection to determine
the location of function words in their lan-
guage, even though the input to the model
only consists of unsegmented sequences of
phones. Thus our computational models
support the hypothesis that function words
play a special role in word learning.
1 Introduction
Over the past two decades psychologists have in-
vestigated the role that function words might play
in human language acquisition. Their experiments
suggest that function words play a special role in
the acquisition process: children learn function
words before they learn the vast bulk of the asso-
ciated content words, and they use function words
to help identify context words.
The goal of this paper is to determine whether
computational models of human language acqui-
sition can provide support for the hypothesis that
function words are treated specially in human
language acquisition. We do this by comparing
two computational models of word segmentation
which differ solely in the way that they model
function words. Following Elman et al (1996)
and Brent (1999) our word segmentation models
identify word boundaries from unsegmented se-
quences of phonemes corresponding to utterances,
effectively performing unsupervised learning of a
lexicon. For example, given input consisting of
unsegmented utterances such as the following:
j u w ? n t t u s i ? ? b ? k
a word segmentation model should segment this as
ju w?nt tu si ?? b?k, which is the IPA representation
of ?you want to see the book?.
We show that a model equipped with the abil-
ity to learn some rudimentary properties of the
target language?s function words is able to learn
the vocabulary of that language more accurately
than a model that is identical except that it is inca-
pable of learning these generalisations about func-
tion words. This suggests that there are acqui-
sition advantages to treating function words spe-
cially that human learners could take advantage of
(at least to the extent that they are learning similar
generalisations as our models), and thus supports
the hypothesis that function words are treated spe-
cially in human lexical acquisition. As a reviewer
points out, we present no evidence that children
use function words in the way that our model does,
and we want to emphasise we make no such claim.
While absolute accuracy is not directly relevant
to the main point of the paper, we note that the
models that learn generalisations about function
words perform unsupervised word segmentation
at 92.5% token f-score on the standard Bernstein-
Ratner (1987) corpus, which improves the previ-
ous state-of-the-art by more than 4%.
As a reviewer points out, the changes we make
to our models to incorporate function words can
be viewed as ?building in? substantive informa-
tion about possible human languages. The model
282
that achieves the best token f-score expects func-
tion words to appear at the left edge of phrases.
While this is true for languages such as English,
it is not true universally. By comparing the pos-
terior probability of two models ? one in which
function words appear at the left edges of phrases,
and another in which function words appear at the
right edges of phrases ? we show that a learner
could use Bayesian posterior probabilities to deter-
mine that function words appear at the left edges
of phrases in English, even though they are not
told the locations of word boundaries or which
words are function words.
This paper is structured as follows. Section 2
describes the specific word segmentation mod-
els studied in this paper, and the way we ex-
tended them to capture certain properties of func-
tion words. The word segmentation experiments
are presented in section 3, and section 4 discusses
how a learner could determine whether function
words occur on the left-periphery or the right-
periphery in the language they are learning. Sec-
tion 5 concludes and describes possible future
work. The rest of this introduction provides back-
ground on function words, the Adaptor Grammar
models we use to describe lexical acquisition and
the Bayesian inference procedures we use to infer
these models.
1.1 Psychological evidence for the role of
function words in word learning
Traditional descriptive linguistics distinguishes
function words, such as determiners and prepo-
sitions, from content words, such as nouns and
verbs, corresponding roughly to the distinction be-
tween functional categories and lexical categories
of modern generative linguistics (Fromkin, 2001).
Function words differ from content words in at
least the following ways:
1. there are usually far fewer function word
types than content word types in a language
2. function word types typically have much
higher token frequency than content word
types
3. function words are typically morphologically
and phonologically simple (e.g., they are typ-
ically monosyllabic)
4. function words typically appear in peripheral
positions of phrases (e.g., prepositions typi-
cally appear at the beginning of prepositional
phrases)
5. each function word class is associated with
specific content word classes (e.g., deter-
miners and prepositions are associated with
nouns, auxiliary verbs and complementisers
are associated with main verbs)
6. semantically, content words denote sets of
objects or events, while function words de-
note more complex relationships over the en-
tities denoted by content words
7. historically, the rate of innovation of function
words is much lower than the rate of innova-
tion of content words (i.e., function words are
typically ?closed class?, while content words
are ?open class?)
Properties 1?4 suggest that function words
might play a special role in language acquisition
because they are especially easy to identify, while
property 5 suggests that they might be useful for
identifying lexical categories. The models we
study here focus on properties 3 and 4, in that
they are capable of learning specific sequences of
monosyllabic words in peripheral (i.e., initial or
final) positions of phrase-like units.
A number of psychological experiments have
shown that infants are sensitive to the function
words of their language within their first year of
life (Shi et al, 2006; Hall?e et al, 2008; Shafer
et al, 1998), often before they have experienced
the ?word learning spurt?. Crucially for our pur-
pose, infants of this age were shown to exploit
frequent function words to segment neighboring
content words (Shi and Lepage, 2008; Hall?e et
al., 2008). In addition, 14 to 18-month-old
children were shown to exploit function words to
constrain lexical access to known words - for in-
stance, they expect a noun after a determiner (Cau-
vet et al, 2014; Kedar et al, 2006; Zangl and
Fernald, 2007). In addition, it is plausible that
function words play a crucial role in children?s
acquisition of more complex syntactic phenom-
ena (Christophe et al, 2008; Demuth and McCul-
lough, 2009), so it is interesting to investigate the
roles they might play in computational models of
language acquisition.
1.2 Adaptor grammars
Adaptor grammars are a framework for Bayesian
inference of a certain class of hierarchical non-
parametric models (Johnson et al, 2007b). They
define distributions over the trees specified by
a context-free grammar, but unlike probabilistic
context-free grammars, they ?learn? distributions
over the possible subtrees of a user-specified set of
?adapted? nonterminals. (Adaptor grammars are
non-parametric, i.e., not characterisable by a finite
283
set of parameters, if the set of possible subtrees
of the adapted nonterminals is infinite). Adaptor
grammars are useful when the goal is to learn a
potentially unbounded set of entities that need to
satisfy hierarchical constraints. As section 2 ex-
plains in more detail, word segmentation is such
a case: words are composed of syllables and be-
long to phrases or collocations, and modelling this
structure improves word segmentation accuracy.
Adaptor Grammars are formally defined in
Johnson et al (2007b), which should be consulted
for technical details. Adaptor Grammars (AGs)
are an extension of Probabilistic Context-Free
Grammars (PCFGs), which we describe first. A
Context-Free Grammar (CFG) G = (N,W,R, S)
consists of disjoint finite sets of nonterminal sym-
bols N and terminal symbols W , a finite set of
rules R of the form A?? where A ? N and
? ? (N ? W )
?
, and a start symbol S ? N . (We
assume there are no ??-rules? inR, i.e., we require
that |?| ? 1 for each A?? ? R).
A Probabilistic Context-Free Grammar (PCFG)
is a quintuple (N,W,R, S,?) where N , W , R
and S are the nonterminals, terminals, rules and
start symbol of a CFG respectively, and ? is a vec-
tor of non-negative reals indexed by R that sat-
isfy
?
??R
A
?
A??
= 1 for each A ? N , where
R
A
= {A?? : A?? ? R} is the set of rules
expanding A.
Informally, ?
A??
is the probability of a node
labelled A expanding to a sequence of nodes la-
belled ?, and the probability of a tree is the prod-
uct of the probabilities of the rules used to con-
struct each non-leaf node in it. More precisely, for
each X ? N ?W a PCFG associates distributions
G
X
over the set of trees T
X
generated by X as
follows:
If X ? W (i.e., if X is a terminal) then G
X
is the distribution that puts probability 1 on the
single-node tree labelled X .
If X ? N (i.e., if X is a nonterminal) then:
G
X
=
?
X?B
1
...B
n
?R
X
?
X?B
1
...B
n
TD
X
(G
B
1
, . . . , G
B
n
) (1)
where R
X
is the subset of rules in R expanding
nonterminal X ? N , and:
TD
X
(G
1
, . . . , G
n
)
(
.
X
t
1
t
n
. . .
)
=
n
?
i=1
G
i
(t
i
).
That is, TD
X
(G
1
, . . . , G
n
) is a distribution over
the set of trees T
X
generated by nonterminal X ,
where each subtree t
i
is generated independently
from G
i
. The PCFG generates the distribution G
S
over the set of trees T
S
generated by the start sym-
bol S; the distribution over the strings it generates
is obtained by marginalising over the trees.
In a Bayesian PCFG one puts Dirichlet priors
Dir(?) on the rule probability vector ?, such that
there is one Dirichlet parameter ?
A??
for each
rule A?? ? R. There are Markov Chain Monte
Carlo (MCMC) and Variational Bayes procedures
for estimating the posterior distribution over rule
probabilities ? and parse trees given data consist-
ing of terminal strings alone (Kurihara and Sato,
2006; Johnson et al, 2007a).
PCFGs can be viewed as recursive mixture
models over trees. While PCFGs are expres-
sive enough to describe a range of linguistically-
interesting phenomena, PCFGs are parametric
models, which limits their ability to describe phe-
nomena where the set of basic units, as well as
their properties, are the target of learning. Lexi-
cal acqusition is an example of a phenomenon that
is naturally viewed as non-parametric inference,
where the number of lexical entries (i.e., words)
as well as their properties must be learnt from the
data.
It turns out there is a straight-forward modifica-
tion to the PCFG distribution (1) that makes it suit-
ably non-parametric. As Johnson et al (2007b)
explain, by inserting a Dirichlet Process (DP)
or Pitman-Yor Process (PYP) into the generative
mechanism (1) the model ?concentrates? mass on
a subset of trees (Teh et al, 2006). Specifically,
an Adaptor Grammar identifies a subset A ? N
of adapted nonterminals. In an Adaptor Gram-
mar the unadapted nonterminals N \ A expand
via (1), just as in a PCFG, but the distributions of
the adapted nonterminals A are ?concentrated? by
passing them through a DP or PYP:
H
X
=
?
X?B
1
...B
n
?R
X
?
X?B
1
...B
n
TD
X
(G
B
1
, . . . , G
B
n
)
G
X
= PYP(H
X
, a
X
, b
X
)
Here a
X
and b
X
are parameters of the PYP asso-
ciated with the adapted nonterminal X . As Gold-
water et al (2011) explain, such Pitman-Yor Pro-
cesses naturally generate power-law distributed
data.
Informally, Adaptor Grammars can be viewed
as caching entire subtrees of the adapted nonter-
minals. Roughly speaking, the probability of gen-
erating a particular subtree of an adapted nonter-
minal is proportional to the number of times that
subtree has been generated before. This ?rich get
284
richer? behaviour causes the distribution of sub-
trees to follow a power-law (the power is speci-
fied by the a
X
parameter of the PYP). The PCFG
rules expanding an adapted nonterminal X de-
fine the ?base distribution? of the associated DP
or PYP, and the a
X
and b
X
parameters determine
how much mass is reserved for ?new? trees.
There are several different procedures for infer-
ring the parse trees and the rule probabilities given
a corpus of strings: Johnson et al (2007b) describe
aMCMC sampler and Cohen et al (2010) describe
a Variational Bayes procedure. We use the MCMC
procedure here since this has been successfully ap-
plied to word segmentation problems in previous
work (Johnson, 2008).
2 Word segmentation with Adaptor
Grammars
Perhaps the simplest word segmentation model is
the unigram model, where utterances are modeled
as sequences of words, and where each word is
a sequence of segments (Brent, 1999; Goldwater
et al, 2009). A unigram model can be expressed
as an Adaptor Grammar with one adapted non-
terminal Word (we indicate adapted nonterminals
by underlining them in grammars here; regular ex-
pressions are expanded into right-branching pro-
ductions).
Sentence?Word
+
(2)
Word?Phone
+
(3)
The first rule (2) says that a sentence consists of
one or more Words, while the second rule (3)
states that a Word consists of a sequence of one or
more Phones; we assume that there are rules ex-
panding Phone into all possible phones. Because
Word is an adapted nonterminal, the adaptor gram-
mar memoises Word subtrees, which corresponds
to learning the phone sequences for the words of
the language.
The more sophisticated Adaptor Grammars dis-
cussed below can be understood as specialis-
ing either the first or the second of the rules
in (2?3). The next two subsections review the
Adaptor Grammar word segmentation models pre-
sented in Johnson (2008) and Johnson and Gold-
water (2009): section 2.1 reviews how phonotac-
tic syllable-structure constraints can be expressed
with Adaptor Grammars, while section 2.2 re-
views how phrase-like units called ?collocations?
capture inter-word dependencies. Section 2.3
presents the major novel contribution of this paper
by explaining how we modify these adaptor gram-
mars to capture some of the special properties of
function words.
2.1 Syllable structure and phonotactics
The rule (3) models words as sequences of inde-
pendently generated phones: this is what Gold-
water et al (2009) called the ?monkey model? of
word generation (it instantiates the metaphor that
word types are generated by a monkey randomly
banging on the keys of a typewriter). However, the
words of a language are typically composed of one
or more syllables, and explicitly modelling the in-
ternal structure of words typically improves word
segmentation considerably.
Johnson (2008) suggested replacing (3) with the
following model of word structure:
Word? Syllable
1:4
(4)
Syllable?(Onset)Rhyme (5)
Onset?Consonant
+
(6)
Rhyme?Nucleus (Coda) (7)
Nucleus?Vowel
+
(8)
Coda?Consonant
+
(9)
Here and below superscripts indicate iteration
(e.g., a Word consists of 1 to 4 Syllables), while
an Onset consists of an unbounded number of
Consonants), while parentheses indicate option-
ality (e.g., a Rhyme consists of an obligatory
Nucleus followed by an optional Coda). We as-
sume that there are rules expanding Consonant
and Vowel to the set of all consonants and vow-
els respectively (this amounts to assuming that the
learner can distinguish consonants from vowels).
Because Onset, Nucleus and Coda are adapted,
this model learns the possible syllable onsets, nu-
cleii and coda of the language, even though neither
syllable structure nor word boundaries are explic-
itly indicated in the input to the model.
The model just described assumes that word-
internal syllables have the same structure as word-
peripheral syllables, but in languages such as
English word-peripheral onsets and codas can
be more complex than the corresponding word-
internal onsets and codas. For example, the
word ?string? begins with the onset cluster str,
which is relatively rare word-internally. Johnson
(2008) showed that word segmentation accuracy
improves if the model can learn different conso-
nant sequences for word-inital onsets and word-
final codas. It is easy to express this as an Adaptor
285
Grammar: (4) is replaced with (10?11) and (12?
17) are added to the grammar.
Word? SyllableIF (10)
Word? SyllableI Syllable
0:2
SyllableF (11)
SyllableIF?(OnsetI)RhymeF (12)
SyllableI?(OnsetI)Rhyme (13)
SyllableF?(Onset)RhymeF (14)
OnsetI?Consonant
+
(15)
RhymeF?Nucleus (CodaF) (16)
CodaF?Consonant
+
(17)
In this grammar the suffix ?I? indicates a word-
initial element, and ?F? indicates a word-final el-
ement. Note that the model simply has the abil-
ity to learn that different clusters can occur word-
peripherally and word-internally; it is not given
any information about the relative complexity of
these clusters.
2.2 Collocation models of inter-word
dependencies
Goldwater et al (2009) point out the detrimental
effect that inter-word dependencies can have on
word segmentation models that assume that the
words of an utterance are independently gener-
ated. Informally, a model that generates words in-
dependently is likely to incorrectly segment multi-
word expressions such as ?the doggie? as single
words because the model has no way to capture
word-to-word dependencies, e.g., that ?doggie? is
typically preceded by ?the?. Goldwater et alshow
that word segmentation accuracy improves when
the model is extended to capture bigram depen-
dencies.
Adaptor grammar models cannot express bi-
gram dependencies, but they can capture similiar
inter-word dependencies using phrase-like units
that Johnson (2008) calls collocations. John-
son and Goldwater (2009) showed that word seg-
mentation accuracy improves further if the model
learns a nested hierarchy of collocations. This can
be achieved by replacing (2) with (18?21).
Sentence?Colloc3
+
(18)
Colloc3?Colloc2
+
(19)
Colloc2?Colloc1
+
(20)
Colloc1?Word
+
(21)
Informally, Colloc1, Colloc2 and Colloc3 define a
nested hierarchy of phrase-like units. While not
designed to correspond to syntactic phrases, by ex-
amining the sample parses induced by the Adaptor
Grammar we noticed that the collocations often
correspond to noun phrases, prepositional phrases
or verb phrases. This motivates the extension to
the Adaptor Grammar discussed below.
2.3 Incorporating ?function words? into
collocation models
The starting point and baseline for our extension
is the adaptor grammar with syllable structure
phonotactic constraints and three levels of collo-
cational structure (5-21), as prior work has found
that this yields the highest word segmentation to-
ken f-score (Johnson and Goldwater, 2009).
Our extension assumes that the Colloc1 ?
Colloc3 constituents are in fact phrase-like, so we
extend the rules (19?21) to permit an optional se-
quence of monosyllabic words at the left edge
of each of these constituents. Our model thus
captures two of the properties of function words
discussed in section 1.1: they are monosyllabic
(and thus phonologically simple), and they appear
on the periphery of phrases. (We put ?function
words? in scare quotes below because our model
only approximately captures the linguistic proper-
ties of function words).
Specifically, we replace rules (19?21) with the
following sequence of rules:
Colloc3?(FuncWords3)Colloc2
+
(22)
Colloc2?(FuncWords2)Colloc1
+
(23)
Colloc1?(FuncWords1)Word
+
(24)
FuncWords3?FuncWord3
+
(25)
FuncWord3? SyllableIF (26)
FuncWords2?FuncWord2
+
(27)
FuncWord2? SyllableIF (28)
FuncWords1?FuncWord1
+
(29)
FuncWord1? SyllableIF (30)
This model memoises (i.e., learns) both the in-
dividual ?function words? and the sequences of
?function words? that modify the Colloc1 ?
Colloc3 constituents. Note also that ?function
words? expand directly to SyllableIF, which in
turn expands to a monosyllable with a word-initial
onset and word-final coda. This means that ?func-
tion words? are memoised independently of the
?content words? that Word expands to; i.e., the
model learns distinct ?function word? and ?con-
tent word? vocabularies. Figure 1 depicts a sample
parse generated by this grammar.
286
.Sentence
Colloc3
FuncWords3
FuncWord3
you
FuncWord3
want
FuncWord3
to
Colloc2
Colloc1
Word
see
Colloc1
FuncWords1
FuncWord1
the
Word
book
Figure 1: A sample parse generated by the ?func-
tion word? Adaptor Grammar with rules (10?18)
and (22?30). To simplify the parse we only show
the root node and the adapted nonterminals, and
replace word-internal structure by the word?s or-
thographic form.
This grammar builds in the fact that function
words appear on the left periphery of phrases. This
is true of languages such as English, but is not true
cross-linguistically. For comparison purposes we
also include results for a mirror-image model that
permits ?function words? on the right periphery,
a model which permits ?function words? on both
the left and right periphery (achieved by changing
rules 22?24), as well as a model that analyses all
words as monosyllabic.
Section 4 explains how a learner could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the posterior probability of the data
under our ?function word? Adaptor Grammar to
that obtained using a grammar which is identi-
cal except that rules (22?24) are replaced with the
mirror-image rules in which ?function words? are
attached to the right periphery.
3 Word segmentation results
This section presents results of running our Adap-
tor Grammar models on subsets of the Bernstein-
Ratner (1987) corpus of child-directed English.
We use the Adaptor Grammar software available
from http://web.science.mq.edu.au/?mjohnson/
with the same settings as described in Johnson
and Goldwater (2009), i.e., we perform Bayesian
inference with ?vague? priors for all hyperpa-
rameters (so there are no adjustable parameters
in our models), and perform 8 different MCMC
runs of each condition with table-label resampling
for 2,000 sweeps of the training data. At every
10th sweep of the last 1,000 sweeps we use the
model to segment the entire corpus (even if it
is only trained on a subset of it), so we collect
Model
Token
f-score
Boundary
precision
Boundary
recall
Baseline 0.872 0.918 0.956
+ left FWs 0.924 0.935 0.990
+ left + right FWs 0.912 0.957 0.953
Table 1: Mean token f-scores and boundary preci-
sion and recall results averaged over 8 trials, each
consisting of 8 MCMC runs of models trained
and tested on the full Bernstein-Ratner (1987) cor-
pus (the standard deviations of all values are less
than 0.006; Wilcox sign tests show the means of
all token f-scores differ p < 2e-4).
800 sample segmentations of each utterance.
The most frequent segmentation in these 800
sample segmentations is the one we score in the
evaluations below.
3.1 Word segmentation with ?function word?
models
Here we evaluate the word segmentations found
by the ?function word? Adaptor Grammar model
described in section 2.3 and compare it to the base-
line grammar with collocations and phonotactics
from Johnson and Goldwater (2009). Figure 2
presents the standard token and lexicon (i.e., type)
f-score evaluations for word segmentations pro-
posed by these models (Brent, 1999), and Table 1
summarises the token and lexicon f-scores for the
major models discussed in this paper. It is interest-
ing to note that adding ?function words? improves
token f-score by more than 4%, corresponding to
a 40% reduction in overall error rate.
When the training data is very small the Mono-
syllabic grammar produces the highest accuracy
results, presumably because a large proportion of
the words in child-directed speech are monosyl-
labic. However, at around 25 sentences the more
complex models that are capable of finding multi-
syllabic words start to become more accurate.
It?s interesting that after about 1,000 sentences
the model that allows ?function words? only on
the right periphery is considerably less accurate
than the baseline model. Presumably this is be-
cause it tends to misanalyse multi-syllabic words
on the right periphery as sequences of monosyl-
labic words.
The model that allows ?function words? only on
the left periphery is more accurate than the model
that allows them on both the left and right periph-
ery when the input data ranges from about 100 to
about 1,000 sentences, but when the training data
287
0.00
0.25
0.50
0.75
1.00
1 10 100 1000 10000NumberWofWtrainingWsentences
Tok
enWf
-sco
re
Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WFWs
0.00
0.25
0.50
0.75
1.00
1 10 100 1000 10000NumberWofWtrainingWsentences
Lex
icon
Wf-sc
ore
Model
Monosyllables
Baseline
+WleftWFWs
+WrightWFWs
+WleftW+WrightWFWs
Figure 2: Token and lexicon (i.e., type) f-score on the Bernstein-Ratner (1987) corpus as a function of
training data size for the baseline model, the model where ?function words? can appear on the left pe-
riphery, a model where ?function words? can appear on the right periphery, and a model where ?function
words? can appear on both the left and the right periphery. For comparison purposes we also include
results for a model that assumes that all words are monosyllabic.
is larger than about 1,000 sentences both models
are equally accurate.
3.2 Content and function words found by
?function word? model
As noted earlier, the ?function word? model gen-
erates function words via adapted nonterminals
other than the Word category. In order to bet-
ter understand just how the model works, we give
the 5 most frequent words in each word category
found during 8 MCMC runs of the left-peripheral
?function word? grammar above:
Word : book, doggy, house, want, I
FuncWord1 : a, the, your, little
1
, in
FuncWord2 : to, in, you, what, put
FuncWord3 : you, a, what, no, can
Interestingly, these categories seem fairly rea-
sonable. The Word category includes open-class
nouns and verbs, the FuncWord1 category in-
cludes noun modifiers such as determiners, while
the FuncWord2 and FuncWord3 categories in-
clude prepositions, pronouns and auxiliary verbs.
1
The phone ?l? is generated by both Consonant and
Vowel, so ?little? can be (incorrectly) analysed as one syl-
lable.
Thus, the present model, initially aimed at seg-
menting words from continuous speech, shows
three interesting characteristics that are also ex-
hibited by human infants: it distinguishes be-
tween function words and content words (Shi and
Werker, 2001), it allows learners to acquire at least
some of the function words of their language (e.g.
(Shi et al, 2006)); and furthermore, it may also al-
low them to start grouping together function words
according to their category (Cauvet et al, 2014;
Shi and Melanc?on, 2010).
4 Are ?function words? on the left or
right periphery?
We have shown that a model that expects function
words on the left periphery performs more accu-
rate word segmentation on English, where func-
tion words do indeed typically occur on the left
periphery, leaving open the question: how could
a learner determine whether function words gen-
erally appear on the left or the right periphery of
phrases in the language they are learning? This
question is important because knowing the side
where function words preferentially occur is re-
288
lated to the question of the direction of syntac-
tic headedness in the language, and an accurate
method for identifying the location of function
words might be useful for initialising a syntac-
tic learner. Experimental evidence suggests that
infants as young as 8 months of age already ex-
pect function words on the correct side for their
language ? left-periphery for Italian infants and
right-periphery for Japanese infants (Gervain et
al., 2008) ? so it is interesting to see whether
purely distributional learners such as the ones
studied here can identify the correct location of
function words in phrases.
We experimented with a variety of approaches
that use a single adaptor grammar inference pro-
cess, but none of these were successful. For ex-
ample, we hoped that given an Adaptor Gram-
mar that permits ?function words? on both the
left and right periphery, the inference procedure
would decide that the right-periphery rules simply
are not used in a language like English. Unfortu-
nately we did not find this in our experiments; the
right-periphery rules were used almost as often as
the left-periphery rules (recall that a large fraction
of the words in English child-directed speech are
monosyllabic).
In this section, we show that learners could use
Bayesian model selection to determine that func-
tion words appear on the left periphery in English
by comparing the marginal probability of the data
for the left-periphery and the right-periphery mod-
els.
Instead, we used Bayesian model selection
techniques to determine whether left-peripheral
or a right-peripheral model better fits the un-
segmented utterances that constitute the training
data.
2
While Bayesian model selection is in prin-
ciple straight-forward, it turns out to require the ra-
tio of two integrals (for the ?evidence? or marginal
likelihood) that are often intractable to compute.
Specifically, given a training corpusD of unseg-
mented sentences and model families G
1
and G
2
(here the ?function word? adaptor grammars with
left-peripheral and right-peripheral attachment re-
spectively), the Bayes factor K is the ratio of the
marginal likelihoods of the data:
K =
P(D | G
1
)
P(D | G
2
)
2
Note that neither the left-peripheral nor the right-
peripheral model is correct: even strongly left-headed lan-
guages like English typically contain a few right-headed con-
structions. For example, ?ago? is arguably the head of the
phrase ?ten years ago?.
0
2000
4000
6000
1 10 100 1000 10000Number of training sentences
log B
ayes
 fact
or
Figure 3: Bayes factor in favour of left-peripheral
?function word? attachment as a function of the
number of sentences in the training corpus, cal-
culated using the Harmonic Mean estimator (see
warning in text).
where the marginal likelihood or ?evidence? for a
model G is obtained by integrating over all of the
hidden or latent structure and parameters ?:
P(D | G) =
?
?
P(D,? | G) d? (31)
Here the variable ? ranges over the space ? of all
possible parses for the utterances inD and all pos-
sible configurations of the Pitman-Yor processes
and their parameters that constitute the ?state? of
the Adaptor Grammar G. While the probability of
any specific Adaptor Grammar configuration ? is
not too hard to calculate (the MCMC sampler for
Adaptor Grammars can print this after each sweep
through D), the integral in (31) is in general in-
tractable.
Textbooks such as Murphy (2012) describe a
number of methods for calculating P(D | G), but
most of them assume that the parameter space ?
is continuous and so cannot be directly applied
here. The Harmonic Mean estimator (32) for (31),
which we used here, is a popular estimator for
(31) because it only requires the ability to calcu-
late P(D,? | G) for samples from P(? | D,G):
P(D | G) ?
(
1
n
n
?
i=1
1
P(D,?
i
| G)
)
?1
where ?
i
, . . . ,?
n
are n samples from P(? |
289
D,G), which can be generated by the MCMC pro-
cedure.
Figure 3 depicts how the Bayes factor in favour
of left-peripheral attachment of ?function words?
varies as a function of the number of utter-
ances in the training data D (calculated from the
last 1000 sweeps of 8 MCMC runs of the cor-
responding adaptor grammars). As that figure
shows, once the training data contains more than
about 1,000 sentences the evidence for the left-
peripheral grammar becomes very strong. On the
full training data the estimated log Bayes factor is
over 6,000, which would constitute overwhelming
evidence in favour of left-peripheral attachment.
Unfortunately, as Murphy and others warn, the
Harmonic Mean estimator is extremely unstable
(Radford Neal calls it ?the worst MCMC method
ever? in his blog), so we think it is important to
confirm these results using a more stable estima-
tor. However, given the magnitude of the differ-
ences and the fact that the two models being com-
pared are of similar complexity, we believe that
these results suggest that Bayesian model selec-
tion can be used to determine properties of the lan-
guage being learned.
5 Conclusions and future work
This paper showed that the word segmentation
accuracy of a state-of-the-art Adaptor Grammar
model is significantly improved by extending it
so that it explicitly models some properties of
function words. We also showed how Bayesian
model selection can be used to identify that func-
tion words appear on the left periphery of phrases
in English, even though the input to the model only
consists of an unsegmented sequence of phones.
Of course this work only scratches the surface
in terms of investigating the role of function words
in language acquisition. It would clearly be very
interesting to examine the performance of these
models on other corpora of child-directed English,
as well as on corpora of child-directed speech in
other languages. Our evaluation focused on word-
segmentation, but we could also evaluate the ef-
fect that modelling ?function words? has on other
aspects of the model, such as its ability to learn
syllable structure.
The models of ?function words? we investi-
gated here only capture two of the 7 linguistic
properties of function words identified in section 1
(i.e., that function words tend to be monosyllabic,
and that they tend to appear phrase-peripherally),
so it would be interesting to develop and explore
models that capture other linguistic properties of
function words. For example, following the sug-
gestion by Hochmann et al (2010) that human
learners use frequency cues to identify function
words, it might be interesting to develop computa-
tional models that do the same thing. In an Adap-
tor Grammar the frequency distribution of func-
tion words might be modelled by specifying the
prior for the Pitman-Yor Process parameters asso-
ciated with the function words? adapted nontermi-
nals so that it prefers to generate a small number
of high-frequency items.
It should also be possible to develop models
which capture the fact that function words tend not
to be topic-specific. Johnson et al (2010) and
Johnson et al (2012) show how Adaptor Gram-
mars can model the association between words
and non-linguistic ?topics?; perhaps these models
could be extended to capture some of the semantic
properties of function words.
It would also be interesting to further explore
the extent to which Bayesian model selection is a
useful approach to linguistic ?parameter setting?.
In order to do this it is imperative to develop better
methods than the problematic ?Harmonic Mean?
estimator used here for calculating the evidence
(i.e., the marginal probability of the data) that can
handle the combination of discrete and continuous
hidden structure that occur in computational lin-
guistic models.
As well as substantially improving the accuracy
of unsupervised word segmentation, this work is
interesting because it suggests a connection be-
tween unsupervised word segmentation and the in-
duction of syntactic structure. It is reasonable to
expect that hierarchical non-parametric Bayesian
models such as Adaptor Grammars may be useful
tools for exploring such a connection.
Acknowledgments
This work was supported in part by the Aus-
tralian Research Council?s Discovery Projects
funding scheme (project numbers DP110102506
and DP110102593), the European Research Coun-
cil (ERC-2011-AdG-295810 BOOTPHON), the
Agence Nationale pour la Recherche (ANR-10-
LABX-0087 IEC, and ANR-10-IDEX-0001-02
PSL*), and the Mairie de Paris, Ecole des Hautes
Etudes en Sciences Sociales, the Ecole Normale
Sup?erieure, and the Fondation Pierre Gilles de
Gennes.
290
References
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck, ed-
itors, Children?s Language, volume 6, pages 159?
174. Erlbaum, Hillsdale, NJ.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
E. Cauvet, R. Limissuri, S. Millotte, K. Skoruppa,
D. Cabrol, and A. Christophe. 2014. Function
words constrain on-line recognition of verbs and
nouns in French 18-month-olds. Language Learn-
ing and Development, pages 1?18.
A. Christophe, S. Millotte, S. Bernal, and J. Lidz.
2008. Bootstrapping lexical and syntactic acquisi-
tion. Language and Speech, 51(1-2):61?75.
S. B. Cohen, D. M. Blei, and N. A. Smith. 2010. Vari-
ational inference for adaptor grammars. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 564?572,
Los Angeles, California, June. Association for Com-
putational Linguistics.
K. Demuth and E. McCullough. 2009. The prosodic
(re-)organization of childrens early English articles.
Journal of Child Language, 36(1):173?200.
J. Elman, E. Bates, M. H. Johnson, A. Karmiloff-
Smith, D. Parisi, and K. Plunkett. 1996. Rethink-
ing Innateness: A Connectionist Perspective on De-
velopment. MIT Press/Bradford Books, Cambridge,
MA.
V. Fromkin, editor. 2001. Linguistics: An Introduction
to Linguistic Theory. Blackwell, Oxford, UK.
J. Gervain, M. Nespor, R. Mazuka, R. Horie, and
J. Mehler. 2008. Bootstrapping word order in
prelexical infants: A japaneseitalian cross-linguistic
study. Cognitive Psychology, 57(1):56 ? 74.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2009.
A Bayesian framework for word segmentation: Ex-
ploring the effects of context. Cognition, 112(1):21?
54.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2011.
Producing power-law distributions and damping
word frequencies with two-stage language models.
Journal of Machine Learning Research, 12:2335?
2382.
P. A. Hall?e, C. Durand, and B. de Boysson-Bardies.
2008. Do 11-month-old French infants process ar-
ticles? Language and Speech, 51(1-2):23?44.
J.-R. Hochmann, A. D. Endress, and J. Mehler. 2010.
Word frequency as a cue for identifying function
words in infancy. Cognition, 115(3):444 ? 457.
M. Johnson and S. Goldwater. 2009. Improving non-
parameteric Bayesian inference: experiments on un-
supervised word segmentation with adaptor gram-
mars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 317?325, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
M. Johnson, T. Griffiths, and S. Goldwater. 2007a.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 139?
146, Rochester, New York. Association for Compu-
tational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007b.
Adaptor Grammars: A framework for specifying
compositional nonparametric Bayesian models. In
B. Sch?olkopf, J. Platt, and T. Hoffman, editors, Ad-
vances in Neural Information Processing Systems
19, pages 641?648. MIT Press, Cambridge, MA.
M. Johnson, K. Demuth, M. Frank, and B. Jones.
2010. Synergies in learning words and their refer-
ents. In J. Lafferty, C. K. I. Williams, J. Shawe-
Taylor, R. Zemel, and A. Culotta, editors, Advances
in Neural Information Processing Systems 23, pages
1018?1026.
M. Johnson, K. Demuth, and M. Frank. 2012. Exploit-
ing social information in grounded language learn-
ing via grammatical reduction. In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics, pages 883?891, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
M. Johnson. 2008. Using Adaptor Grammars to iden-
tify synergies in the unsupervised acquisition of lin-
guistic structure. In Proceedings of the 46th Annual
Meeting of the Association of Computational Lin-
guistics, pages 398?406, Columbus, Ohio. Associ-
ation for Computational Linguistics.
Y. Kedar, M. Casasola, and B. Lust. 2006. Getting
there faster: 18- and 24-month-old infants? use of
function words to determine reference. Child De-
velopment, 77(2):325?338.
K. Kurihara and T. Sato. 2006. Variational
Bayesian grammar induction for natural language.
In Y. Sakakibara, S. Kobayashi, K. Sato, T. Nishino,
and E. Tomita, editors, Grammatical Inference: Al-
gorithms and Applications, pages 84?96. Springer.
K. P. Murphy. 2012. Machine learning: a probabilistic
perspective. The MIT Press.
V. L. Shafer, D. W. Shucard, J. L. Shucard, and
L. Gerken. 1998. An electrophysiological study of
infants? sensitivity to the sound patterns of English
291
speech. Journal of Speech, Language and Hearing
Research, 41(4):874.
R. Shi and M. Lepage. 2008. The effect of functional
morphemes on word segmentation in preverbal in-
fants. Developmental Science, 11(3):407?413.
R. Shi and A. Melanc?on. 2010. Syntactic categoriza-
tion in French-learning infants. Infancy, 15(517?
533).
R. Shi and J. Werker. 2001. Six-months old infants?
preference for lexical words. Psychological Science,
12:71?76.
R. Shi, A. Cutler, J. Werker, and M. Cruickshank.
2006. Frequency and form as determinants of func-
tor sensitivity in English-acquiring infants. The
Journal of the Acoustical Society of America,
119(6):EL61?EL67.
Y. W. Teh, M. Jordan, M. Beal, and D. Blei. 2006. Hi-
erarchical Dirichlet processes. Journal of the Amer-
ican Statistical Association, 101:1566?1581.
R. Zangl and A. Fernald. 2007. Increasing flexibil-
ity in children?s online processing of grammatical
and nonce determiners in fluent speech. Language
Learning and Development, 3(3):199?231.
292
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 1?6,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Exploring the Relative Role of Bottom-up and Top-down Information in
Phoneme Learning
Abdellah Fourtassi
1
, Thomas Schatz
1,2
, Balakrishnan Varadarajan
3
, Emmanuel Dupoux
1
1
Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris, France
2
SIERRA Project-Team, INRIA/ENS/CNRS, Paris, France
3
Center for Language and Speech Processing, JHU, Baltimore, USA
{abdellah.fourtassi; emmanuel.dupoux; balaji.iitm1}@gmail
thomas.schatz@laposte.net
Abstract
We test both bottom-up and top-down ap-
proaches in learning the phonemic status
of the sounds of English and Japanese. We
used large corpora of spontaneous speech
to provide the learner with an input that
models both the linguistic properties and
statistical regularities of each language.
We found both approaches to help dis-
criminate between allophonic and phone-
mic contrasts with a high degree of accu-
racy, although top-down cues proved to be
effective only on an interesting subset of
the data.
1 Introduction
Developmental studies have shown that, during
their first year, infants tune in on the phonemic cat-
egories (consonants and vowels) of their language,
i.e., they lose the ability to distinguish some
within-category contrasts (Werker and Tees, 1984)
and enhance their ability to distinguish between-
category contrasts (Kuhl et al, 2006). Current
work in early language acquisition has proposed
two competing hypotheses that purport to account
for the acquisition of phonemes. The bottom-up
hypothesis holds that infants converge on the lin-
guistic units of their language through a similarity-
based distributional analysis of their input (Maye
et al, 2002; Vallabha et al, 2007). In contrast,
the top-down hypothesis emphasizes the role of
higher level linguistic structures in order to learn
the lower level units (Feldman et al, 2013; Mar-
tin et al, 2013). The aim of the present work is
to explore how much information can ideally be
derived from both hypotheses.
The paper is organized as follows. First we de-
scribe how we modeled phonetic variation from
audio recordings, second we introduce a bottom-
up cue based on acoustic similarity and top-
down cues based of the properties of the lexicon.
We test their performance in a task that consists
in discriminating within-category contrasts from
between-category contrasts. Finally we discuss
the role and scope of each cue for the acquisition
of phonemes.
2 Modeling phonetic variation
In this section, we describe how we modeled the
representation of speech sounds putatively pro-
cessed by infants, before they learn the relevant
phonemic categories of their language. Following
Peperkamp et al (2006), we make the assumption
that this input is quantized into context-dependent
phone-sized unit we call allophones. Consider the
example of the allophonic rule that applies to the
French /r/:
/r/?
{
[X] / before a voiceless obstruent
[K] elsewhere
Figure 1: Allophonic variation of French /r/
The phoneme /r/ surfaces as voiced ([K]) before
a voiced obstruent like in [kanaK Zon] (?canard
jaune?, yellow duck) and as voiceless ([X]) before
a voiceless obstruent as in [kanaX puXpK] (?ca-
nard pourpre?, purple duck). Assuming speech
sounds are coded as allophones, the challenge fac-
ing the learner is to distinguish the allophonic vari-
ation ([K], [X]) from the phonemic variation (re-
lated to a difference in the meaning) like the con-
trast ([K],[l]).
Previous work has generated allophonic varia-
tion using random contexts (Martin et al, 2013).
This procedure does not take into account the fact
that contexts belong to natural classes. In addition,
it does not enable to compute an acoustic distance.
Here, we generate linguistically and acoustically
controlled allophones using Hidden Markov Mod-
els (HMMs) trained on audio recordings.
1
2.1 Corpora
We use two speech corpora: the Buckeye Speech
corpus (Pitt et al, 2007), which consists of 40
hours of spontaneous conversations with 40 speak-
ers of American English, and the core of the Cor-
pus of Spontaneous Japanese (Maekawa et al,
2000) which also consists of about 40 hours of
recorded spontaneous conversations and public
speeches in different fields. Both corpora are time-
aligned with phonetic labels. Following Boruta
(2012), we relabeled the japanese corpus using 25
phonemes. For English, we used the phonemic
version which consists of 45 phonemes.
2.2 Input generation
2.2.1 HMM-based allophones
In order to generate linguistically and acoustically
plausible allophones, we apply a standard Hidden
Markov Model (HMM) phoneme recognizer with
a three-state per phone architecture to the signal,
as follows.
First, we convert the raw speech waveform of
the corpora into successive vectors of Mel Fre-
quency Cepstrum Coefficients (MFCC), computed
over 25 ms windows, using a period of 10 ms
(the windows overlap). We use 12 MFCC coeffi-
cients, plus the energy, plus the first and second or-
der derivatives, yielding 39 dimensions per frame.
Second, we start HMM training using one three-
state model per phoneme. Third, each phoneme
model is cloned into context-dependent triphone
models, for each context in which the phoneme
actually occurs (for example, the phoneme /A/ oc-
curs in the context [d?A?g] as in the word /dAg/
(?dog?). The triphone models are then retrained on
only the relevant subset of the data, corresponding
to the given triphone context. These detailed mod-
els are clustered back into inventories of various
sizes (from 2 to 20 times the size of the phone-
mic inventory) using a linguistic feature-based de-
cision tree, and the HMM states of linguistically
similar triphones are tied together so as to max-
imize the likelihood of the data. Finally, the tri-
phone models are trained again while the initial
gaussian emission models are replaced by mix-
ture of gaussians with a progressively increasing
number of components, until each HMM state is
modeled by a mixture of 17 diagonal-covariance
gaussians. The HMM were built using the HMM
Toolkit (HTK: Young et al, 2006).
2.2.2 Random allophones
As a control, we also reproduce the random al-
lophones of Martin et al (2013), in which allo-
phonic contexts are determined randomly: for a
given phoneme /p/, the set of all possible con-
texts is randomly partitioned into a fixed number
n of subsets. In the transcription, the phoneme /p/
is converted into one of its allophones (p
1
,p
2
,..,p
n
)
depending on the subset to which the current con-
text belongs.
3 Bottom-up and top-down hypotheses
3.1 Acoustic cue
The bottom-up cue is based on the hypothesis that
instances of the same phoneme are likely to be
acoustically more similar than instances of two
different phonemes (see Cristia and Seidl, in press)
for a similar proposition). In order to provide
a proxy for the perceptual distance between al-
lophones, we measure the information theoretic
distance between the acoustic HMMs of these al-
lophones. The 3-state HMMs of the two allo-
phones were aligned with Dynamic Time Warping
(DTW), using as a distance between pairs of emit-
ting states, a symmetrized version of the Kullback-
Leibler (KL) divergence measure (each state was
approximated by a single non-diagonal Gaussian):
A(x, y) =
?
(i,j)?DTW (x,y)
KL(N
x
i
||N
y
j
) +KL(N
y
j
||N
x
i
)
Where {(i, j) ? DTW (x, y)} is the set of in-
dex pairs over the HMM states that correspond to
the optimal DTW path in the comparison between
phone model x and y, and N
x
i
the full covariance
Gaussian distribution for state i of phone x. For
obvious reasons, the acoustic distance cue cannot
be computed for Random allophones.
3.2 Lexical cues
The top-down information we use in this study, is
based on the insight of Martin et al (2013). It rests
on the idea that true lexical minimal pairs are not
very frequent in human languages, as compared to
minimal pairs due to mere phonological processes.
In fact, the latter creates variants (alternants) of the
same lexical item since adjacent sounds condition
the realization of the first and final phoneme. For
example, as shown in figure 1, the phoneme /r/ sur-
faces as [X] or [K] depending on whether or not the
2
next sound is a voiceless obstruent. Therefore, the
lexical item /kanar/ surfaces as [kanaX] or [kanaK].
The lexical cue assumes that a pair of words dif-
fering in the first or last segment (like [kanaX] and
[kanaK]) is more likely to be the result of a phono-
logical process triggered by adjacent sounds, than
a true semantic minimal pair.
However, this strategy clearly gives rise to false
alarms in the (albeit relatively rare) case of true
minimal pairs like [kanaX] (?duck?) and [kanal]
(?canal?), where ([X], [l]) will be mistakenly la-
beled as allophonic.
In order to mitigate the problem of false alarms,
we also use Boruta (2011)?s continuous version,
where each pair of phones is characterized by the
number of lexical minimal pairs it forms.
B(x, y) = |(Ax,Ay) ? L
2
|+ |(xA, yA) ? L
2
|
where {Ax ? L} is the set of words in the lex-
icon L that end in the phone x, and {(Ax,Ay) ?
L
2
} is the set of phonological minimal pairs in
L? L that vary on the final segment.
In addition, we introduce another cue that could
be seen as a normalization of Boruta?s cue:
N (x, y) =
|(Ax,Ay)?L
2
|+|(xA,yA)?L
2
|
|{Ax?L}|+|{Ay?L}|+|{xA?L}|+|{yA?L}|
4 Experiment
4.1 Task
For each corpus we list all the possible pairs of
attested allophones. Some of these pairs are allo-
phones of the same phoneme (allophonic pair) and
others are allophones of different phonemes (non-
allophonic pairs). The task is a same-different
classification, whereby each of these pairs is given
a score from the cue that is being tested. A good
cue gives higher scores to allophonic pairs.
4.2 Evaluation
We use the same evaluation procedure as in Mar-
tin et al (2013). It is carried out by computing
the area under the curve of the Receiver Operat-
ing Characteristic (ROC). A value of 0.5 repre-
sents chance and a value of 1 represents perfect
performance.
In order to lessen the potential influence of the
structure of the corpus (mainly the order of the ut-
terances) on the results, we use a statistical resam-
pling scheme. The corpus is divided into small
blocks (of 20 utterances each). In each run, we
draw randomly with replacement from this set of
blocks a sample of the same size as the original
corpus. This sample is then used to retrain the
acoustic models and generate a phonetic inven-
tory that we use to re-transcribe the corpus and
re-compute the cues. We report scores averaged
over 5 such runs.
4.3 Results
Table 1 shows the classification scores for the lex-
ical cues when we vary the inventory size from
2 allophones per phoneme in average, to 20 al-
lophones per phoneme, using the Random allo-
phones. The top-down scores are very high, repli-
cating Martin et al?s results, and even improving
the performance using Boruta?s cue and our new
Normalized cue.
? English Japanese
Allo./phon. M B N M B N
2 0.784 0.935 0.951 0.580 0.989 1.00
5 0.845 0.974 0.982 0.653 0.978 0.991
10 0.886 0.974 0.981 0.733 0.944 0.971
20 0.918 0.961 0.966 0.785 0.869 0.886
Table 1 : Same-different scores for top-down cues on
Random allophones, as a function of the average number of
allophones per phoneme. M=Martin et al, B=Boruta, N=
Normalized
Table 2 shows the results for HMM-based allo-
phones. The acoustic score is very accurate for
both languages and is quite robust to variation.
Top-down cues, on the other hand, perform, sur-
prisingly, almost at chance level in distinguish-
ing between allophonic and non-allophonic pairs.
A similar discrepancy for the case of Japanese
was actually noted, but not explained, in Boruta
(2012).
? English Japanese
Allo./phon. A M B N A M B N
2 0.916 0.592 0.632 0.643 0.885 0.422 0.524 0.537
5 0.918 0.592 0.607 0.611 0.908 0.507 0.542 0.551
10 0.893 0.569 0.571 0.571 0.827 0.533 0.546 0.548
20 0.879 0.560 0.560 0.559 0.876 0.541 0.543 0.543
Table 2 : Same-different scores for bottom-up and top-down
cues on HMM-based allophones, as a function of the
average number of allophones per phoneme. A=Acoustic,
M=Martin et al, B=Boruta, N= Normalized
5 Analysis
5.1 Why does the performance drop for
realistic allophones?
When we list all possible pairs of allophones in
the inventory, some of them correspond to lexi-
3
cal alternants ([X], [K]) ? ([kanaX] and [kanaK]),
others to true minimal pairs ([K], [l]) ? ([kanaK]
and [kanal]), and yet others will simply not gen-
erate lexical variation at all, we will call those:
invisible pairs. For instance, in English, /h/ and
/N/ occur in different syllable positions and thus
cannot appear in any minimal pair. As defined
above, top-down cues are set to 0 in such pairs
(which means that they are systematically classi-
fied as non-allophonic). This is a correct decision
for /h/ vs. /N/, but not for invisible pairs that also
happen to be allophonic, resulting in false nega-
tives. In tables 3, we show that, indeed, invisible
pairs is a major issue, and could explain to a large
extent the pattern of results found above. In fact,
the proportion of visible allophonic pairs (?allo?
column) is way lower for HMM-based allophones.
This means that the majority of allophonic pairs in
the HMM case are invisible, and therefore, will be
mistakenly classified as non-allophonic.
? Random HMM
? English Japanese English Japanese
Allo./phon. allo ? allo allo ? allo allo ? allo allo ? allo
2 92.9 36.3 100 83.9 48.9 25.3 37.1 53.2
5 97.2 28.4 99.6 69.0 31.1 14.3 25.0 25.9
10 96.8 19.9 96.7 50.1 19.8 4.23 21.0 14.4
20 94.3 10.8 83.4 26.4 14.0 1.89 12.4 4.04
Table 3 : Proportion (in %) of allophonic pairs (allo), and
non-allophonic pairs (? allo) associated with at least one
lexical minimal pair, in Random and HMM allophones.
There are basically two reasons why an allo-
phonic pair would be invisible ( will not generate
lexical alternants). The first one is the absence of
evidence, e.g., if the edges of the word with the
underlying phoneme do not appear in enough con-
texts to generate the corresponding variants. This
happens when the corpus is so small that no word
ending with, say, /r/ appears in both voiced and
voiceless contexts. The second, is when the allo-
phones are triggered on maximally different con-
texts (on the right and the left) as illustrated below:
/p/?
{
[p
1
] / A B
[p
2
] / C D
When A doesn?t overlap with C and B does not
overlap with D, it becomes impossible for the pair
([p
1
], [p
2
]) to generate a lexical minimal pair. This
is simply because a pair of allophones needs to
share at least one context to be able to form vari-
ants of a word (the second or penultimate segment
of this word).
When asked to split the set of contexts in two
distinct categories that trigger [p
1
] and [p
2
] (i.e.,
A B and C D), the random procedure will of-
ten make A overlap with B and C overlap with D
because it is completely oblivious to any acous-
tic or linguistic similarity, thus making it always
possible for the pair of allophones to generate lex-
ical alternants. A more realistic categorization
(like the HMM-based one), will naturally tend to
minimize within-category distance, and maximize
between-category distance. Therefore, we will
have less overlap, making the chances of the pair
to generate a lexical pair smaller. The more al-
lophones we have, the bigger is the chance to end
up with non-overlapping categories (invisible allo-
phonic pairs), and the more mistakes will be made,
as shown in Table 3.
5.2 Restricting the role of top-down cues
The analysis above shows that top-down cues can-
not be used to classify all contrasts. The approxi-
mation that consists in considering all pairs that do
not generate lexical pairs as non-allophonic, does
not scale up to realistic input. A more intuitive,
but less ambitious, assumption is to restrict the
scope of top-down cues to contrasts that do gen-
erate lexical variation (lexical alternants or true
minimal pairs). Thus, they remain completely ag-
nostic to the status of invisible pairs. This restric-
tion makes sense since top-down information boils
down to knowing whether two word forms belong
to the same lexical category (reducing variation to
allophony), or to two different categories (varia-
tion is then considered non-allophonic). Phonetic
variation that does not cause lexical variation is, in
this particular sense, orthogonal to our knowledge
about the lexicon.
We test this hypothesis by applying the cues
only to the subset of pairs that are associated with
at least one lexical minimal pair. We vary the num-
ber of allophones per phoneme on the one hand
(Table 4) and the size of the input on the other
hand (Table 5). We refer to this subset by an aster-
isk (*), by which we also mark the cues that apply
to it. Notice that, in this new framing, the M cue is
completely uninformative since it assigns the same
value to all pairs.
As predicted, the cues perform very well on this
subset, especially the N cue. The combination of
top-down and bottom-up cues shows that the for-
mer is always useful, and that these two sources of
4
? English Japanese
? ? Individual cues Combination ? Individual cues Combination
Allo./phon. * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
2 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
4 14.3 0.918 0.964 0.858 0.951 0.975 0.991 30.88 0.908 0.917 0.850 0.936 0.934 0.976
10 4.24 0.893 0.937 0.813 0.939 0.960 0.968 16.06 0.827 0.839 0.899 0.957 0.904 0.936
20 1.67 0.879 0.907 0.802 0.907 0.942 0.940 5.02 0.876 0.856 0.882 0.959 0.913 0.950
Table 4 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of average
number of allophones per phonemes.
? English Japanese
? ? Individual cues Combination ? Individual cues Combination
Size (hours) * (%) A A* B* N* A*+B* A*+N* * (%) A A* B* N* A*+B* A*+N*
1 9.87 0.885 0.907 0.741 0.915 0.927 0.969 34.78 0.890 0.883 0.835 0.915 0.889 0.934
4 18.3 0.918 0.958 0.798 0.917 0.967 0.989 48.00 0.917 0.939 0.860 0.937 0.938 0.973
8 21.3 0.916 0.964 0.837 0.942 0.971 0.992 51.71 0.915 0.940 0.889 0.937 0.954 0.977
20 24.4 0.911 0.960 0.827 0.936 0.969 0.994 58.12 0.921 0.954 0.865 0.912 0.945 0.971
40 26.6 0.916 0.965 0.840 0.950 0.971 0.994 60.92 0.885 0.909 0.859 0.906 0.918 0.946
? 34.82 ? ? ? ? ? ? 72.16 ? ? ? ? ? ?
Table 5 : Same-different scores for different cues and their combinations with HMM-allophones, as a function of corpus size.
* (%) refers to the proportion of the subset of contrasts associated with at least one minimal pair. The cues applied to this
subset are marked with an asterisk (*)
information are not completely redundant. How-
ever, the scope of top-down cues (the proportion of
the subset * ) shrinks as we increase the number of
allophones. Table 5 shows that this problem can,
in principle, be mitigated by increasing the amount
of data available to the learner. As we were limited
to only 40 hours of speech, we generated an artifi-
cial corpus that uses the same lexicon but with all
possible word orders so as to maximize the num-
ber of contexts in which words appear. This artifi-
cial corpus increases the proportion of the subset,
but we are still not at 100 % coverage, which ac-
cording the analysis above, is due (at least in part)
to the irreducible set of non-overlapping pairs.
6 Conclusion
In this study we explored the role of both bottom-
up and top-down hypotheses in learning the
phonemic status of the sounds of two typologically
different languages. We introduced a bottom-up
cue based on acoustic similarity, and we used al-
ready existing top-down cues to which we pro-
vided a new extension. We tested these hypothe-
ses on English and Japanese, providing the learner
with an input that mirrors closely the linguistic
and acoustic properties of each language. We
showed, on the one hand, that the bottom-up cue is
a very reliable source of information, across differ-
ent levels of variation and even with small amount
of data. Top-down cues, on the other hand, were
found to be effective only on a subset of the data,
which corresponds to the interesting contrasts that
cause lexical variation. Their role becomes more
relevant as the learner gets more linguistic experi-
ence, and their combination with bottom-up cues
shows that they can provide non-redundant infor-
mation. Note, finally, that even if this work is
based on a more realistic input compared to previ-
ous studies, it still uses simplifying assumptions,
like ideal word segmentation, and no low-level
acoustic variability. Those assumptions are, how-
ever, useful in quantifying the information that can
ideally be extracted from the input, which is a nec-
essary preliminary step before modeling how this
input is used in a cognitively plausible way. Inter-
ested readers may refer to (Fourtassi and Dupoux,
2014; Fourtassi et al, 2014) for a more learning-
oriented approach, where some of the assumptions
made here about high level representations are re-
laxed.
Acknowledgments
This project is funded in part by the Euro-
pean Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la
Recherche (ANR-10-LABX-0087 IEC, ANR-10-
IDEX-0001-02 PSL*), the Fondation de France,
the Ecole de Neurosciences de Paris, and the
R?egion Ile de France (DIM cerveau et pens?ee). We
thank Luc Boruta, Sanjeev Khudanpur, Isabelle
Dautriche, Sharon Peperkamp and Benoit Crabb?e
for highly useful discussions and contributions.
5
References
Luc Boruta. 2011. Combining Indicators of Al-
lophony. In Proceedings ACL-SRW, pages 88?93.
Luc Boruta. 2012. Indicateurs d?allophonie et
de phon?emicit?e. Doctoral dissertation, Universit
?e
Paris-Diderot - Paris VII.
A. Cristia and A. Seidl. In press. The hyperarticula-
tion hypothesis of infant-directed speech. Journal
of Child Language.
Naomi H. Feldman, Thomas L. Griffiths, Sharon Gold-
water, and James L. Morgan. 2013. A role for the
developing lexicon in phonetic category acquisition.
Psychological Review, 120(4):751?778.
Abdellah Fourtassi and Emmanuel Dupoux. 2014. A
rudimentary lexicon and semantics help bootstrap
phoneme acquisition. In Proceedings of the 18th
Conference on Computational Natural Language
Learning (CoNLL).
Abdellah Fourtassi, Ewan Dunbar, and Emmanuel
Dupoux. 2014. Self-consistency as an inductive
bias in early language acquisition. In Proceedings
of the 36th Annual Meeting of the Cognitive Science
Society.
Patricia K. Kuhl, Erica Stevens, Akiko Hayashi,
Toshisada Deguchi, Shigeru Kiritani, and Paul Iver-
son. 2006. Infants show a facilitation effect for na-
tive language phonetic perception between 6 and 12
months. Developmental Science, 9(2):F13?F21.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In LREC, pages 947?952, Athens, Greece.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37(1):103?124.
J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sen-
sitivity to distributional information can affect pho-
netic discrimination. Cognition, 82:B101?B111.
Sharon Peperkamp, Rozenn Le Calvez, Jean-Pierre
Nadal, and Emmanuel Dupoux. 2006. The acqui-
sition of allophonic rules: Statistical learning with
linguistic constraints. Cognition, 101(3):B31?B41.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
G.K. Vallabha, J.L. McClelland, F. Pons, J.F. Werker,
and S. Amano. 2007. Unsupervised learning
of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sciences,
104(33):13273.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49 ? 63.
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
6
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 1?9,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Testing the Robustness of Online Word Segmentation:
Effects of Linguistic Diversity and Phonetic Variation
Luc Boruta1,2, Sharon Peperkamp2, Beno??t Crabbe?1, and Emmanuel Dupoux2
1 Univ. Paris Diderot, Sorbonne Paris Cite?, ALPAGE, UMR-I 001 INRIA, F-75205, Paris, France
2 LSCP?DEC, E?cole des Hautes E?tudes en Sciences Sociales, E?cole Normale Supe?rieure,
Centre National de la Recherche Scientifique, F-75005, Paris, France
luc.boruta@inria.fr, peperkamp@ens.fr, benoit.crabbe@inria.fr, emmanuel.dupoux@gmail.com
Abstract
Models of the acquisition of word segmen-
tation are typically evaluated using phonem-
ically transcribed corpora. Accordingly, they
implicitly assume that children know how to
undo phonetic variation when they learn to ex-
tract words from speech. Moreover, whereas
models of language acquisition should per-
form similarly across languages, evaluation
is often limited to English samples. Us-
ing child-directed corpora of English, French
and Japanese, we evaluate the performance
of state-of-the-art statistical models given in-
puts where phonetic variation has not been re-
duced. To do so, we measure segmentation
robustness across different levels of segmen-
tal variation, simulating systematic allophonic
variation or errors in phoneme recognition.
We show that these models do not resist an in-
crease in such variations and do not generalize
to typologically different languages. From the
perspective of early language acquisition, the
results strengthen the hypothesis according to
which phonological knowledge is acquired in
large part before the construction of a lexicon.
1 Introduction
Speech contains very few explicit boundaries be-
tween linguistic units: silent pauses often mark ut-
terance boundaries, but boundaries between smaller
units (e.g. words) are absent most of the time. Pro-
cedures by which infants could develop word seg-
mentation strategies have been discussed at length,
from both a psycholinguistic and a computational
point of view. Many models relying on statistical
information have been proposed, and some of them
exhibit satisfactory performance: MBDP-1 (Brent,
1999), NGS-u (Venkataraman, 2001) and DP (Gold-
water, Griffiths and Johnson, 2009) can be consid-
ered state-of-the-art. Though there is evidence that
prosodic, phonotactic and coarticulation cues may
count more than statistics (Johnson and Jusczyk,
2001), it is still a matter of interest to know how
much can be learned without linguistic cues. To use
Venkataraman?s words, we are interested in ?the per-
formance of bare-bones statistical models.?
The aforementioned computational simulations
have two major downsides. First, all models of
language acquisition should generalize to typolog-
ically different languages; however, the word seg-
mentation experiments mentioned above have never
been carried out on phonemically transcribed, child-
directed speech in languages other than English.
Second, these experiments use phonemically tran-
scribed corpora as the input and, as such, make the
implicit simplifying assumption that, when children
learn to segment speech into words, they have al-
ready learned phonological rules and know how to
reduce the inherent variability in speech to a finite
(and rather small) number of abstract categories: the
phonemes. Rytting, Brew and Fosler-Lussier (2010)
addressed this issue and replaced the usual phone-
mic input with probability vectors over a finite set
of symbols. Still, this set of symbols is limited to
the phonemic inventory of the language: the reduc-
tion of phonetic variation is taken for granted. In
other words, previous simulations evaluated the per-
formance of the models given idealized input but of-
fered no guarantee as to the performance of the mod-
1
els on realistic input.
We present a comparative survey that evaluates
the extent to which state-of-the-art statistical models
of word segmentation resist segmental variation. To
do so, we designed a parametric benchmark where
more and more variation was gradually introduced
into phonemic corpora of child-directed speech.
Phonetic variation was simulated applying context-
dependent allophonic rules to phonemic corpora.
Other corpora in which noise was created by ran-
dom phoneme substitutions were used as controls.
Furthermore, to draw language-independent conclu-
sions, we used corpora from three typologically dif-
ferent languages: English, French and Japanese.
2 Robustness benchmark
2.1 Word segmentation models
The segmentation task can be summarized as fol-
lows: given a corpus of utterances in which word
boundaries have been deleted, the model has to put
them back. Though we did not challenge the usual
idealization that children are able to segment speech
into discrete, phoneme-sized units, modeling lan-
guage acquisition imposes significant constraints on
the models (Brent, 1999; Gambell and Yang, 2004):
they must generalize to different (if not all) lan-
guages, start without any knowledge specific to a
particular language, learn in an unsupervised man-
ner and, most importantly, operate incrementally.
Online learning is a sound desideratum for any
model of language acquisition: indeed, human
language-processors do not wait, in Brent?s words,
?until the corpus of all utterances they will ever
hear becomes available?. Therefore, we favored an
?infant-plausible? setting and only considered on-
line word segmentation models, namely MBDP-1
(Brent, 1999) and NGS-u (Venkataraman, 2001).
Even if DP (Goldwater et al, 2009) was shown to
be more flexible than both MBDP-1 and NGS-u,
we did not include Goldwater et al?s batch model,
nor recent online variants by Pearl et al (in press),
in the benchmark. All aforementioned models rely
on word n-grams statistics and have similar perfor-
mance, but MBDP-1 and NGS-u are minimally suf-
ficient in providing an quantitative evaluation of how
cross-linguistic and/or segmental variation impact
the models? performance. We added two random
segmentation models as baselines. The four models
are described below.
2.1.1 MBDP-1
The first model is Heinz?s implementation of
Brent?s MBDP-1 (Brent, 1999; Heinz, 2006). The
general idea is that the best segmentation of an ut-
terance can be inferred from the best segmentation
of the whole corpus. However, explicitly search-
ing the space of all possible segmentations of the
corpus dramatically increases the model?s computa-
tional complexity. The implementation thus uses an
incremental approach: when the ith utterance is pro-
cessed, the model computes the best segmentation of
the corpus up to the ith utterance included, assuming
the segmentation of the first i?1 utterances is fixed.
2.1.2 NGS-u
This unigram model was described and imple-
mented by Venkataraman (2001). MBDP-1?s prob-
lems of complexity were circumvented using an in-
trinsically incremental n-gram approach. The strat-
egy is to find the most probable word sequence for
each utterance, according to information acquired
while processing previous utterances. In the end,
the segmentation of the entire corpus is the con-
catenation of each utterance?s best segmentation. It
is worth noting that NGS-u satisfies all three con-
straints proposed by Brent: strict incrementality,
non-supervision and universality.
2.1.3 Random
This dummy model rewrites its input, uniformly
choosing after each segment whether to insert a
word boundary or not. It defines a chance line at
and below which models can be considered ineffi-
cient. The only constraint is that no empty word is
allowed, hence no consecutive boundaries.
2.1.4 Random+
The second baseline is weakly supervised: though
each utterance is segmented at uniformly-chosen
random locations, the correct number of word
boundaries is given. This differs from Brent?s base-
line, which was given the correct number of bound-
aries to insert in the entire corpus. As before, con-
secutive boundaries are forbidden.
2
English French Japanese
Tokens Types Tokens Types Tokens Types
U 9,790 5,921 10,000 7,660 10,000 6,315
W 33,399 1,321 51,069 1,893 26,609 4,112
P 95,809 50 121,486 35 102,997 49
Table 1: Elementary corpus statistics, including number
of utterances (U), words (W) and phonemes (P).
2.2 Corpora
The three corpora we used were derived from tran-
scribed adult-child verbal interactions collected in
the CHILDES database (MacWhinney, 2000). For
each sample, elementary textual statistics are pre-
sented in Table 1. The English corpus contains
9790 utterances from the Bernstein?Ratner corpus
that were automatically transcribed and manually
corrected by Brent and Cartwright (1996). It has
been used in many word segmentation experiments
(Brent, 1999; Venkataraman, 2001; Batchelder,
2002; Fleck, 2008; Goldwater et al, 2009; among
others) and can be considered a de facto standard.
The French and the Japanese corpora were both
made by Le Calvez (2007), the former by automati-
cally transcribing the Champaud, Leveille? and Ron-
dal corpora, the latter by automatically transcribing
the Ishii and Noji corpora from ro?maji to phonemes.
To get samples comparable in size to the English
corpus, 10,000 utterances were selected at random
in each of Le Calvez?s corpora. All transcription
choices made by the authors in terms of phonemic
inventory and word segmentation were respected.1
2.3 Variation sources
The main effect of the transformations we applied
to the phonemic corpora was the increase in the av-
erage number of word forms per word. We refer to
this quantity, similar to a type-token ratio, as the cor-
pora?s lexical complexity. As allophonic variation
is context-dependent, the increase in lexical com-
plexity is, in this condition, limited by the phono-
tactic constraints of the language: the fewer con-
texts a phoneme appears in, the fewer contextual al-
lophones it can have. By contrast, the upper limit
is much higher in the control condition, as phoneme
1Some transcription choices made by Brent and Cartwright
are questionable (Blanchard and Heinz, 2008). Yet, we used the
canonical version of the corpus for the sake of comparability.
substitutions are context-free.
From a computational point of view, the applica-
tion of allophonic rules increases both the number of
symbols in the alphabet and, as a byproduct, the lex-
ical complexity. Obviously, when any kind of noise
or variation is added, there is less information in the
data to learn from. We can therefore presume that
the probability mass will be scattered, and that as a
consequence, statistical models relying on word n-
grams statistics will do worse than with phonemic
inputs. Yet, we are interested in quantifying how
such interference impacts the models? performance.
2.3.1 Allophonic variation
In this experiment, we were interested in the per-
formance of online segmentation models given rich
phonetic transcriptions, i.e. the input children pro-
cess before the acquisition of allophonic rules. Con-
sider the following rule that applies in French:
/r/ ?
{
[X] before a voiceless consonant
[K] otherwise
The application of this rule creates two contextual
variants for /kanar/ (canard, ?duck?): [kanaK Zon]
(canard jaune, ?yellow duck?) and [kanaX flotA?] (ca-
nard flottant, ?floating duck?). Before learning the
rule, children have to store both [kanaK] and [kanaX]
in their emerging lexicon as they are not yet able to
undo allophonic variation and construct a single lex-
ical entry: /kanar/.
Daland and Pierrehumbert (2010) compared the
performance of a phonotactic segmentation model
using canonical phonemic transcripts and transcripts
implementing conversational reduction processes.
They found that incorporating pronunciation vari-
ation has a mild negative impact on performance.
However, they used adult-directed speech. Even if,
as they argue, reduced adult-directed speech may
present a worst-case scenario for infants (compared
to hyperarticulated child-direct speech), it offers no
quantitative evaluation of the models? performance
using child-directed speech.
Because of the lack of phonetically transcribed
child-directed speech data, we emulated rich tran-
scriptions applying allophonic rules to the phonemic
corpora. To do so, we represented the internal struc-
ture of the phonemes in terms of articulatory fea-
tures and used the algorithm described by Boruta
3
(2011) to create artificial allophonic grammars of
different sizes containing assimilatory rules whose
application contexts span phonologically similar
contexts of the target phoneme. Compared to Da-
land and Pierrehumbert?s manual inspection of the
transcripts, this automatic approach gives us a finer
control on the degree of pronunciation variation.
The rules were then applied to our phonemic cor-
pora, thus systematizing coarticulation between ad-
jacent segments. We made two simplifying assump-
tions about the nature of the rules. First, all al-
lophonic rules we generated are of the type p ?
a / c where a phoneme p is realized as its allo-
phone a before context c. Thus, we did not model
rules with left-hand or bilateral contexts. Second,
we ensured that no two allophonic rules introduced
the same allophone (as in English flapping, where
both /t/ and /d/ have an allophone [R]), using parent
annotation: each phone is marked by the phoneme
it is derived from (e.g. [R]/t/ and [R]/d/). This was
done to avoid probability mass derived from differ-
ent phonemes merging onto common symbols.
The amount of variation in the corpora is de-
termined by the average number of allophones per
phoneme. We refer to this quantity as the corpora?s
allophonic complexity. Thus, at minimal allophonic
complexity, each phoneme has only one possible re-
alization (i.e. phonemic transcription), whereas at
maximal allophonic complexity, each phoneme has
as many realizations as attested contexts. For each
language, the range of attested lexical and allo-
phonic complexities obtained using Boruta?s (2011)
algorithm are reported in Figure 1.
2.3.2 Phoneme substitutions
Allophonic variation is not the only type of varia-
tion that may interfere with word segmentation. In-
deed, the aforementioned simulations assumed that
all phonemes are recognized with 100% accuracy,
but ?due to factors such as noise or speech rate?
human processors may mishear words. In this con-
trol condition, we examined the models? perfor-
mance on corpora in which some phonemes were
replaced by others. Thus, substitutions increase the
corpus? lexical complexity without increasing the
number of symbols: phoneme misrecognitions give
a straightforward baseline against which to compare
the models? performance when allophonic variation
5 10 15 20
1.0
1.5
2.0
2.5
3.0
3.5
4.0
l
l
l
l
l
l
l
l
l English
French
Japanese
Figure 1: Lexical complexity (the average number of
word forms per word) as a function of allophonic com-
plexity (the average number of allophones per phoneme).
has not been reduced. Such corpora can be consid-
ered the output of a hypothetical imperfect speech-
to-phoneme system or a winner-take-all scalar re-
duction of Rytting et al?s (2010) probability vectors.
We used a straightforward model of phoneme
misrecognition: substitutions are based neither on
a confusion matrix (Nakadai et al, 2007) nor on
phoneme similarity. Starting from the phonemic
corpus, we generated 10 additional corpora con-
trolling the proportion of misrecognized phonemes,
ranging from 0 (perfect recognition) to 1 (constant
error) in increments of 0.1. A noise intensity of n
means that each phoneme has probability n of being
rewritten by another phoneme. The random choice
of the substitution phoneme is weighted by the rela-
tive frequencies of the phonemes in the corpus. The
probability P (p ? x) that a phoneme x rewrites a
phoneme p is defined as
P (p? x) =
?
?
?
1? n if p = x
n
(
f(x) +
f(p)
|P| ? 1
)
otherwise
where n is the noise intensity, f(x) the relative fre-
quency of phoneme x in the corpus andP the phone-
mic inventory of the language.
4
2.4 Evaluation
We used Venkataraman?s (2001) implementation of
the now-standard evaluation protocol proposed by
Brent (1999) and then extended by Goldwater et al
(2009). Obviously, orthographic words are not the
optimal target for a model of language acquisition.
Yet, in line with previously reported experiments,
we used the orthographic segmentation as the stan-
dard of correct segmentation.
2.4.1 Scoring
For each model, we report (as percentages) the
following scores as functions of the lexical complex-
ity of the corpus:
? Ps, Rs, Fs: precision, recall and F -score on
word segmentation as defined by Brent;
? Pl, Rl, Fl: precision, recall and F -score on the
induced lexicon of word types: let L be the
standard lexicon and L? the one discovered by
the algorithm, we define Pl = |L ? L?|/|L?|,
Rl = |L?L?|/|L| and Fl = 2?Pl ?Rl/(Pl+Rl).
The difference between scoring the segmenta-
tion and the lexicon can be exemplified consider-
ing the utterance [@wUd?2kwUd?2kwUd] (a wood-
chuck would chuck wood). If it is segmented as
[@ wUd?2k wUd ?2k wUd], both the segmentation
and the induced lexicon are correct. By contrast, if
it is segmented as [@ wUd ?2k wUd?2k wUd], the
lexicon is still accurate while the word segmentation
is incorrect. A good segmentation inevitably yields a
good lexicon, but the reverse is not necessarily true.
2.4.2 k-shuffle cross-validation
As the segmental variation procedures and the
segmentation baselines are non-deterministic pro-
cesses, all scores were averaged over multiple simu-
lations. Moreover, as MBDP-1 and NGS-u operate
incrementally, their output is conditioned by the or-
der in which utterances are processed. To lessen the
influence of the utterance order, we shuffled the cor-
pora for each simulation. Testing all permutations of
the corpora for each combination of parameter val-
ues is computationally intractable. Thus, scores re-
ported below were averaged over three distinct sim-
ulations with shuffled corpora.
JP
FR
EN
a. Segmentation F?score
0 10 20 30 40 50 60 70 80 90
JP
FR
EN
b. Lexicon F?score
0 10 20 30 40 50 60 70 80 90
MBDP?1
NGS?u
Random+
Random
Figure 2: Cross-linguistic performance of MBDP-1 and
NGS-u on child-directed phonemic corpora in English
(EN), French (FR) and Japanese (JP).
3 Results and discussion
3.1 Cross-linguistic evaluation
Performance of the segmentation models2 on phone-
mic corpora is presented in Figure 1 in terms of Fs-
and Fl-score (upper and lower panel, respectively).
We were able to replicate previous results on En-
glish by Brent and Venkataraman almost exactly; the
small difference, less than one percent, was probably
caused by the use of different implementations.
From a cross-linguistic point of view, the main
observation is that these models do not seem
to generalize to typologically different languages.
Whereas MBDP-1 and NGS-u?s Fs value is 69%
for English, it is only 54% for French and 41% for
Japanese. Similar observations can be made for Fl.
Purely statistical strategies seem to be particularly
ineffective on our Japanese sample: inserting word
boundaries at random yields a better lexicon than us-
ing probabilistic models.
A crude way to determine whether a word seg-
mentation model tends to break words apart (over-
segmentation) or to cluster various words in a single
chunk (under-segmentation) is to compare the aver-
age word length (AWL) in its output to the AWL in
the standard segmentation. If the output?s AWL is
greater than the standard?s, then the output is under-
segmented, and vice versa. Even if NGS-u produces
2The full table of scores for each language, variation source,
and segmentation model was not included due to space limita-
tions. It is available upon request from the first author.
5
shorter words than MBDP-1, both models exhibit,
once again, similar within-language behaviors. En-
glish was slightly under-segmented by MBDP-1 and
over-segmented by NGS-u: ouputs? AWL are re-
spectively 3.1 and 2.7, while the standard is 2.9.
Our results are consistent with what Goldwater et al
(2009) observed for DP: error analysis shows that
both MBDP-1 and NGS-u also break off frequent
English morphological affixes, namely /IN/ (-ing)
and /s,z/ (-s). As for French, AWL values suggest
the corpus was under-segmented: 3.1 for MBDP-1?s
output and 2.9 for NGS-u?s, while the standard is
2.4. On the contrary, Japanese was heavily over-
segmented: many monophonemic words emerged
and, whereas the standard AWL is 3.9, the ouputs?
AWL is 2.7 for both models.
Over-segmentation may be correlated to the num-
ber of syllable types in the language: English
and French phonotactics allow consonantal clusters,
bringing the number of syllable types to a few thou-
sands. By contrast, Japanese has a much simpler
syllabic structure and less syllable types which, as
a consequence, are often repeated and may (incor-
rectly) be considered as words by statistical mod-
els. The fact that the models do worse for French
and Japanese is not especially surprising: both lan-
guages have many more affixal morphemes than En-
glish. Consider French, where the lexical autonomy
of clitics is questionable: whereas /s/ (s? or c?) or
/k/ (qu?) are highly frequent words in our ortho-
graphic standard, many errors are due to the aggluti-
nation of these clitics to the following word. These
are counted as segmentation errors, but should they?
Furthermore, none of the segmentation models
we benchmarked exhibit similar performance across
languages: invariably, they perform better on En-
glish. There may be a correlation between the per-
formance of segmentation models and the percent-
age of word hapaxes, i.e. words which occur only
once in the corpus: the English, French and Japanese
corpora contain 31.7%, 37.1% and 60.7% of word
hapaxes, respectively. The more words tend to occur
only once, the less MBDP-1, NGS-u and DP per-
form on segmentation. This is consistent with the
usual assumption that infants use familiar words to
find new ones. It may also be the case that these
models are not implicitly tuned to English, but that
the contribution of statistical cues to word segmen-
tation differs across languages. In French, for exam-
ple, stress invariably marks the end of a word (al-
though the end of a word is not necessarily marked
by stress). By contrast, there are languages like
English or Spanish where stress is less predictable:
children cannot rely solely on this cue to extract
words and may thus have to give more weight to
statistics.
3.2 Robustness to segmental variation
The performance of MBDP-1, NGS-u and the two
baselines on inputs altered by segmental variation
is presented in Figure 2.3 The first general observa-
tion is that, as predicted, MBDP-1 and NGS-u do not
seem to resist an increase in lexical complexity. In
the case of allophonic variation, their performance
is inversely related to the corpora?s allophonic com-
plexity. However, as suggested by the change in
the graphs? slope, performance for English seems
to stabilize at 2 word forms per word. Similar ob-
servations can be made for French and Japanese on
which the performance of the models is even worse:
Fl values are below chance at 1.7 and 3 variants per
word for Japanese and French, respectively; like-
wise, Fs is below chance at 1.5 for Japanese and
2.5 for French. Phoneme substitutions also impede
the performance of MBDP-1 and NGS-u: the more
phonemes are substituted, the more difficult it be-
comes for the algorithms to learn how to insert word
boundaries. Furthemore, Fl is below chance for
complexities greater than 4 for French, and approx-
imately 2.5 for Japanese. It is worth noting that, in
both conditions, the models exhibit similar within-
language performance as the complexity increases.
The potential lexicon that can be built by com-
bining segments into words may account for the
discrepancy between the two conditions, as it is in
fact the models? search space. In the control con-
dition, substituting phonemes does not increase its
size. However, the likelihood of a given phoneme in
a given word being replaced by the same substitu-
tion phoneme decreases as words get longer. Thus,
the proportion of hapax increases, making statisti-
cal segmentation harder to achieve. By contrast, the
3For the control condition, we did not graph scores for noise
intensities greater than 0.2: 80% accuracy is comparable to the
error rates of state-of-the-art systems in speaker-independent,
continuous speech recognition (Makhoul and Schwartz, 1995).
6
1 2 3 4 5 6 7 8
10
20
30
40
50
60
70
a. English: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l l
l l l
l
l
1 2 3 4 5 6 7 8
10
20
30
40
50
60
70
b. English: lexicon F?score
Le
xic
on
 F?
sco
re
l MBDP?1
NGS?u
Random+
Random
Allophony
Substitutions
l
l
l l l l l l
l
l
1 2 3 4 5 6
10
20
30
40
50
60
c. French: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l l
l
l l l
l
l
1 2 3 4 5 6
10
20
30
40
50
60
d. French: lexicon F?score
Le
xic
on
 F?
sco
re
l
l
l
l l l l l
l
l
l
1.0 1.5 2.0 2.5 3.0
10
20
30
40
50
e. Japanese: segmentation F?score
Seg
me
nta
tio
n F
?sc
ore
l
l
l
l
l
l
l
l
l
l
1.0 1.5 2.0 2.5 3.0
10
20
30
40
50
f. Japanese: lexicon F?score
Le
xic
on
 F?
sco
re
l
l l l
l
l
l
l
l
l
Figure 3: Fs-score (left column) and Fl-score (right column) as functions of the lexical complexity, i.e. the number of
word forms per word, in the English (top row), French (middle row) and Japanese (bottom row) corpora.
7
application of allophonic rules increases the number
of objects to build words with; as a consequence, the
size of the potential lexicon explodes.
As neither MBDP-1 nor NGS-u is designed to
handle noise, the results are unsurprising. Indeed,
any word form found by these models will be incor-
porated in the lexicon: if [l?NgwI?] and [l?NgwI?]
are both found in the corpus, these variants will be
included as is in the lexicon. There is no mechanism
for ?explaining away? data that appear to have been
generated by systematic variation or random noise.
It is an open issue for future research to create ro-
bust models of word segmentation that can handle
segmental variation.
4 Conclusions
We have shown, first, that online statistical mod-
els of word segmentation that rely on word n-gram
statistics do not generalize to typologically differ-
ent languages. As opposed to French and Japanese,
English seems to be easier to segment using only
statistical information. Such differences in perfor-
mance from one language to another emphasize the
relevance of cross-linguistic studies: any conclusion
drawn from the monolingual evaluation of a model
of language acquisition should be considered with
all proper reservations. Second, our results quan-
tify how imperfect, though realistic, inputs impact
MBDP-1?s and NGS-u?s performance. Indeed, both
models become less and less efficient in discover-
ing words in transcribed child-directed speech as
the number of variants per word increases: though
the performance drop we observed is not surpris-
ing, it is worth noting that both models are less ef-
ficient than random procedures at about twenty al-
lophones per phoneme. However, the number of
context-dependent allophones we introduced is far
less than what is used by state-of-the-art models of
speech recognition (Makhoul and Schwartz, 1995).
To our knowledge, there is no computational
model of word segmentation that both respects the
constraints imposed on a human learner and accom-
modates noise. This highlights the complexity of
early language acquisition: while no accurate lex-
icon can be learned without a good segmentation
strategy, state-of-the-art models fail to deliver good
segmentations in non-idealized settings. Our re-
sults also emphasize the importance of other cues
for word segmentation: statistical learning may be
helpful or necessary for word segmentation, but it is
unlikely that it is sufficient.
The mediocre performance of the models
strengthens the hypotheses that phonological
knowledge is acquired in large part before the
construction of a lexicon (Jusczyk, 1997), or that
allophonic rules and word segmentations could be
acquired jointly (so that neither is a prerequisite
for the other): children cannot extract words from
fluent speech without knowing how to undo at least
part of contextual variation. Thus, the knowledge
of allophonic rules seems to be a prerequisite for
accurate segmentation. Recent simulations of word
segmentation and lexical induction suggest that
using phonological knowledge (Venkataraman,
2001; Blanchard and Heinz, 2008), modeling
morphophonological structure (Johnson, 2008) or
preserving subsegmental variation (Rytting et al,
2010) invariably increases performance. Vice
versa, Martin et al (submitted) have shown that the
algorithm proposed by Peperkamp et al (2006) for
undoing allophonic variation crashes in the face of
realistic input (i.e. many allophones), and that it
can be saved if it has approximate knowledge of
word boundaries. Further research is needed, at
both an experimental and a computational level, to
explore the performance and suitability of an online
model that combines the acquisition of allophonic
variation with that of word segmentation.
References
E. Batchelder. 2002. Bootstrapping the lexicon: a com-
putational model of infant speech segmentation. Cog-
nition, 83:167?206.
D. Blanchard and J. Heinz. 2008. Improving word seg-
mentation by simultaneously learning phonotactics. In
Proceedings of the Conference on Natural Language
Learning, pages 65?72.
L. Boruta. 2011. A note on the generation of allophonic
rules. Technical Report 0401, INRIA.
M. R. Brent and T. A. Cartwright. 1996. Distributional
regularity and phonotactic constraints are useful for
segmentation. Cognition, 61:93?125.
M. R. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery. Ma-
chine Learning, 34(1?3):71?105.
8
R. Daland and J. B. Pierrehumbert. 2010. Learn-
ing diphone-based segmentation. Cognitive Science,
35(1):119?155.
M. Fleck. 2008. Lexicalized phonotactic word segmen-
tation. In Proceedings of ACL-2008, pages 130?138.
T. Gambell and C. Yang. 2004. Statistics learning and
universal grammar: Modeling word segmentation. In
Proceedings of the 20th International Conference on
Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2009. A
bayesian framework for word segmentation: exploring
the effects of context. Cognition, 112(1):21?54.
J. Heinz. 2006. MBDP-1, OCaml implementation. Re-
trieved from http://phonology.cogsci.udel.edu/?heinz/
on January 26, 2009.
E. K. Johnson and P. W. Jusczyk. 2001. Word segmenta-
tion by 8-month-olds: When speech cues count more
than statistics. Journal of Memory and Language,
44:548?567.
M. Johnson. 2008. Unsupervised word segmentation for
Sesotho using adaptor grammars. In Proceedings of
the 10th Meeting of ACL SIGMORPHON, pages 20?
27.
P. Jusczyk. 1997. The Discovery of Spoken Language.
MIT Press.
R. Le Calvez. 2007. Approche computationnelle de
l?acquisition pre?coce des phone`mes. Ph.D. thesis,
UPMC.
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Elbraum Associates.
J. Makhoul and R. Schwartz. 1995. State of the art in
continuous speech recognition. PNAS, 92:9956?9963.
A. Martin, S. Peperkamp, and E. Dupoux. Submitted.
Learning phonemes with a pseudo-lexicon.
K. Nakadai, R. Sumiya, M. Nakano, K. Ichige, Y. Hi-
rose, and H. Tsujino. 2007. The design of phoneme
grouping for coarse phoneme recognition. In IEA/AIE,
pages 905?914.
L. Pearl, Sh. Goldwater, and M. Steyvers. In press. On-
line learning mechanisms for bayesian models of word
segmentation. Research on Language and Computa-
tion.
S. Peperkamp, R. Le Calvez, J. P. Nadal, and E. Dupoux.
2006. The acquisition of allophonic rules: statisti-
cal learning with linguistic constraints. Cognition,
101(3):B31?B41.
C. A. Rytting, C. Brew, and E. Fosler-Lussier. 2010.
Segmenting words from natural speech: subsegmen-
tal variation in segmental cues. Journal of Child Lan-
guage, 37:513?543.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational Lin-
guistics, 27(3):351?372.
9
Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 1?10,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Whyisenglishsoeasytosegment?
Abdellah Fourtassi1, Benjamin Bo?rschinger2,3
Mark Johnson3 and Emmanuel Dupoux1
(1) Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris
(2) Department of Computing, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
{abdellah.fourtassi, emmanuel.dupoux}@gmail.com , {benjamin.borschinger, mark.johnson}@mq.edu.au
?
Abstract
Cross-linguistic studies on unsupervised
word segmentation have consistently
shown that English is easier to segment
than other languages. In this paper, we
propose an explanation of this finding
based on the notion of segmentation
ambiguity. We show that English has a
very low segmentation ambiguity com-
pared to Japanese and that this difference
correlates with the segmentation perfor-
mance in a unigram model. We suggest
that segmentation ambiguity is linked
to a trade-off between syllable structure
complexity and word length distribution.
1 Introduction
During the course of language acquisition, in-
fants must learn to segment words from continu-
ous speech. Experimental studies show that they
start doing so from around 7.5 months of age
(Jusczyk and Aslin, 1995). Further studies indi-
cate that infants are sensitive to a number of word
boundary cues, like prosody (Jusczyk et al, 1999;
Mattys et al, 1999), transition probabilities (Saf-
fran et al, 1996; Pelucchi et al, 2009), phonotac-
tics (Mattys et al, 2001), coarticulation (Johnson
and Jusczyk, 2001) and combine these cues with
different weights (Weiss et al, 2010).
Computational models of word segmentation
have played a major role in assessing the relevance
and reliability of different statistical cues present
in the speech input. Some of these models focus
mainly on boundary detection, and assess differ-
ent strategies to identify them (Christiansen et al,
1998; Xanthos, 2004; Swingley, 2005; Daland and
Pierrehumbert, 2011). Other models, sometimes
called lexicon-building algorithms, learn the lexi-
con and the segmentation at the same time and use
knowledge about the extracted lexicon to segment
novel utterances. State-of-the-art lexicon-building
segmentation algorithms are typically reported to
yield better performance than word boundary de-
tection algorithms (Brent, 1999; Venkataraman,
2001; Batchelder, 2002; Goldwater, 2007; John-
son, 2008b; Fleck, 2008; Blanchard et al, 2010).
As seen in Table 1, however, the performance
varies considerably across languages with English
winning by a high margin. This raises a general-
izability issue for NLP applications, but also for
the modeling of language acquisition since, obvi-
ously, it is not the case that in some languages,
infants fail to acquire an adult lexicon. Are these
performance differences only due to the fact that
the algorithms might be optimized for English? Or
do they also reflect some intrinsic linguistic differ-
ences between languages?
Lang. F-score Model Reference
English 0.89 AG Johnson (2009)
Chinese 0.77 AG Johnson (2010)
Spanish 0.58 DP Bigram Fleck (2008)
Arabic 0.56 WordEnds Fleck (2008)
Sesotho 0.55 AG Johnson (2008)
Japanese 0.55 BootLex Batchelder (2002)
French 0.54 NGS-u Boruta (2011)
Table 1: State-of-the-art unsupervised segmentation scores
for eight languages.
The aim of the present work is to understand
why English usually scores better than other lan-
guages, as far as unsupervised segmentation is
concerned. As a comparison point, we chose
Japanese because it is among the languages that
have given the poorest word segmentation scores.
In fact, Boruta et al (2011) found an F-score
around 0.41 using both Brent (1999)?s MBDP-1
and Venkataraman (2001)?s NGS-u models, and
Batchelder (2002) found an F-score that goes
from 0.40 to 0.55 depending on the corpus used.
Japanese also differs typologically from English
along several phonological dimensions such as
1
number of syllabic types, phonotactic constraints
and rhythmic structure. Although most lexicon-
building segmentation algorithms do not attempt
to model these dimensions, they still might be rel-
evant to speech segmentation and help explain the
performance difference.
The structure of the paper is as follows. First,
we present the class of lexical-building segmen-
tation algorithm that we use in this paper (Adap-
tor Grammar), and our English and Japanese cor-
pora. We then present data replicating the basic
finding that segmentation performance is better for
English than for Japanese. We then explore the hy-
pothesis that this finding is due to an intrinsic dif-
ference in segmentation ambiguity in the two lan-
guages, and suggest that the source of this differ-
ence rests in the structure of the phonological lexi-
con in the two languages. Finally, we use these in-
sights to try and reduce the gap between Japanese
and English segmentation through a modification
of the Unigram model where multiple linguistic
levels are learned jointly.
2 Computational Framework and
Corpora
2.1 Adaptor Grammar
In this study, we use the Adaptor Grammar frame-
work (Johnson et al, 2007) to test different mod-
els of word segmentation on English and Japanese
Corpora. This framework makes it possible to
express a class of hierarchical non-parametric
Bayesian models using an extension of probabilis-
tic context-free grammars called Adaptor Gram-
mar (AG). It allows one to easily define models
that incorporate different assumptions about lin-
guistic structure and is therefore a useful practical
tool for exploring different hypotheses about word
segmentation (Johnson, 2008b; Johnson, 2008a;
Johnson et al, 2010; Bo?rschinger et al, 2012).
For mathematical details and a description of
the inference procedure for AGs, we refer the
reader to Johnson et al (2007). Briefly, AG uses
the non-parametric Pitman-Yor-Process (Pitman
and Yor, 1997) which, as in Minimum Descrip-
tion lengths models, finds a compact representa-
tion of the input by re-using frequent structures
(here, words).
2.2 Corpora
In the present study, we used both Child Di-
rected Speech (CDS) and Adult Directed Speech
(ADS) corpora. English CDS was derived from
the Bernstein-Ratner corpus (Bernstein-Ratner,
1987), which consists in transcribed verbal inter-
action of parents with nine children between 1
and 2 years of age. We used the 9,790 utter-
ances that were phonemically transcribed by Brent
and Cartwright (1996). Japanese CDS consists in
the first 10, 000 utterances of the Hamasaki cor-
pus (Hamasaki, 2002). It provides a phonemic
transcript of spontaneous speech to a single child
collected from when the child was 2 up to when
it was 3.5 years old. Both CDS corpora are avail-
able from the CHILDES database (MacWhinney,
2000).
As for English ADS, we used the first 10,000
utterances of the Buckeye Speech Corpus (Pitt et
al., 2007) which consists in spontaneous conver-
sations with 40 speakers in American English. To
make it comparable to the other corpora in this
paper, we only used the idealized phonemic tran-
scription. Finally, for Japanese ADS, we used
the first 10,000 utterances of a phonemic tran-
scription of the Corpus of Spontaneous Japanese
(Maekawa et al, 2000). It consists of recorded
spontaneous conversations, or public speeches in
different fields ranging from engineering to hu-
manities. For each corpus, we present elementary
statistics in Table 2.
3 Unsupervised segmentation with the
Unigram Model
3.1 Setup
In this experiment we used the Adaptor Gram-
mar framework to implement a Unigram model of
word segmentation (Johnson et al, 2007). This
model has been shown to be equivalent to the orig-
inal MBDP-1 segmentation model (see Goldwater
(2007)). The model is defined as:
?
Utterance?Word+
Word? Phoneme+
?
In the AG framework, an underlined non-
terminal indicates that this non-terminal is
adapted, i.e. that the AG will cache (and learn
probabilities for) entire sub-trees rooted in this
non-terminal. Here, Word is the only unit that the
model effectively learns, and there are no depen-
dencies between the words to be learned. This
grammar states that an utterance must be analyzed
in terms of one or more Words, where a Word is a
2
Corpus Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
Tokens
? Utterances 9, 790 10, 000 10, 000 10, 000
? Words 33, 399 27, 362 57, 185 87, 156
? Phonemes 95, 809 108, 427 183, 196 289, 264
Types
? Words 1, 321 2, 389 3, 708 4, 206
? Phonemes 50 30 44 25
Average Lengths
? Words per utterance 3.41 2.74 5.72 8.72
? Phonemes per utterance 9.79 10.84 18.32 28.93
? Phonemes per word 2.87 3.96 3.20 3.32
Table 2 : Characteristics of phonemically transcribed corpora
sequence of Phonemes.
We ran the model twice on each corpus for
2,000 iterations with hyper-parameter sampling
and we collected samples throughout the process,
following the methodology of Johnson and Gold-
water (2009)1. For evaluation, we performed their
Minimum Bayes Risk decoding using the col-
lected samples to get a single score.
3.2 Evaluation
For the evaluation, we used the same measures as
Brent (1999), Venkataraman (2001) and Goldwa-
ter (2007), namely token Precision (P), Recall (R)
and F-score (F). Precision is defined as the num-
ber of correct word tokens found out of all tokens
posited. Recall is the number of correct word to-
kens found out of all tokens in the gold standard.
The F-score is defined as the harmonic mean of
Precision and Recall , F = 2?P?RP+R .
We will refer to these scores as the segmentation
scores. In addition, we define similar measures for
word boundaries and word types in the lexicon.
3.3 Results and discussion
The results are shown in Table 3. As expected,
the model yields substantially better scores in En-
glish than Japanese, for both CDS and ADS. In
addition, we found that in both languages, ADS
yields slightly worse results than CDS. This is to
be expected because ADS uses between 60% and
300% longer utterances than CDS, and as a result
presents the learner with a more difficult segmen-
tation problem. Moreover, ADS includes between
1We used incremental initialization
70% and 280% more word types than CDS, mak-
ing it a more difficult lexical learning problem.
Note, however, that despite these large differences
in corpus statistics, the difference in segmentation
performance between ADS and CDS are small
compared to the differences between Japanese and
English.
An error analysis on English data shows that
most errors come from the Unigrammodel mistak-
ing high frequency collocations for single words
(see also Goldwater (2007)). This leads to an
under-segmentation of chunks like ?a boy? or ?is
it? 2. Yet, the model also tends to break off fre-
quent morphological affixes, especially ?-ing? and
?-s? , leading to an over-segmentation of words
like ?talk ing? or ?black s?.
Similarly, Japanese data shows both over-
and under-segmentation errors. However, over-
segmentation is more severe than for English, as
it does not only affect affixes, but surfaces as
breaking apart multi-syllabic words. In addition,
Japanese segmentation faces another kind of er-
ror which acts across word boundaries. For exam-
ple, ?ni kashite? is segmented as ?nika shite? and
?nurete inakatta? as ?nure tei na katta?. This leads
to an output lexicon that, on the one hand, allows
for a more compact analysis of the corpus than
the true lexicon: the number of word types drops
from 2,389 to 1,463 in CDS and from 4,206 to
2,372 in ADS although the average token length ?
and consequently, overall number of tokens ? does
not change as dramatically, dropping from 3.96 to
2For ease of presentation, we use orthography to present
examples although all experiments are run on phonemic tran-
scripts.
3
? Child Directed Speech Adult Directed Speech
? English Japanese English Japanese
? F P R F P R F P R F P R
Segmentation 0.77 0.76 0.77 0.55 0.51 0.61 0.69 0.66 0.73 0.50 0.48 0.52
Boundaries 0.87 0.87 0.88 0.72 0.63 0.83 0.86 0.81 0.91 0.76 0.74 0.79
Lexicon 0.62 0.65 0.59 0.33 0.43 0.26 0.41 0.48 0.36 0.30 0.42 0.23
Table 3 : Word segmentation scores of the Unigram model
3.31 for CDS and from 3.32 to 3.12 in ADS. On
the other hand, however, most of the output lex-
icon items are not valid Japanese words and this
leads to the bad lexicon F-scores. This, in turn,
leads to the bad overall segmentation performance.
In brief, we have shown that, across two dif-
ferent corpora, English yields consistently better
segmentation results than Japanese for the Uni-
gram model. This confirms and extends the results
of Boruta et al (2011) and Batchelder (2002). It
strongly suggests that the difference is neither due
to a specific choice of model nor to particularities
of the corpora, but reflects a fundamental property
of these two languages.
In the following section, we introduce the no-
tion of segmentation ambiguity, it to English and
Japanese data, and show that it correlates with seg-
mentation performance.
4 Intrinsic Segmentation Ambiguity
Lexicon-based segmentation algorithms like
MBDP-1, NGS-u and the AG Unigram model
learn the lexicon and the segmentation at the
same time. This makes it difficult, in case of
poor performance, to see whether the problem
comes from the intrinsic segmentability of the
language or from the quality of the extracted
lexicon. Our claim is that Japanese is intrinsically
more difficult to segment than English, even when
a good lexicon is already assumed. We explore
this hypothesis by studying segmentation alone,
assuming a perfect (Gold) lexicon.
4.1 Segmentation ambiguity
Without any information, a string of N phonemes
could be segmented in 2N?1 ways. When a lexi-
con is provided, the set of possible segmentations
is reduced to a smaller number. To illustrate this,
suppose we have to segment the input utterance:
/ay s k r iy m/ 3, and that the lexicon contains the
following words : /ay/ (I), /s k r iy m/ (scream),
/ay s/ (ice), /k r iy m/ (cream). Only two segmen-
tations are possible : /ay skriym/ (I scream) and
/ays kriym/ (ice cream).
We are interested in the ambiguity generated by
the different possible parses that result from such a
supervised segmentation. In order to quantify this
idea in general, we define a Normalized Segmenta-
tion Entropy. To do this, we need to assign a prob-
ability to every possible segmentation. To this end,
we use a unigram model where the probability of a
lexical item is its normalized frequency in the cor-
pus and the probability of a parse is the product
of the probabilities of its terms. In order to obtain
a measure that does not depend on the utterance
length, we normalize by the number of possible
boundaries in the utterance. So for an utterance of
length N , the Normalized Segmentation Entropy
(NSE) is computed using Shannon formula (Shan-
non, 1948) as follows:
?
NSE = ?
?
i Pilog2(Pi)/(N ? 1)
?
where Pi is the probability of the parse i .
For CDS data we found Normalized Segmen-
tation Entropies of 0.0021 bits for English and
0.0156 bits for Japanese. In ADS data we
found similar results with 0.0032 bits for English
and 0.0275 bits for Japanese. This means that
Japanese needs between 7 and 8 times more bits
than English to encode segmentation information.
This is a very large difference, which is of the
same magnitude in CDS and ADS. These differ-
ences clearly show that intrinsically, Japanese is
more ambiguous than English with regards to seg-
mentation.
One can refine this analysis by distinguishing
two sources of ambiguity: ambiguity across word
boundaries, as in ?ice cream / [ay s] [k r iy m]?
3We use ARPABET notation to represent phonemic input.
4
Figure 1 : Correlation between Normalized Segmentation Entropy (in bits) and the segmentation F-score for CDS (left) and
ADS (Right)
vs ?I scream / [ay] [s k r iy m]?. And ambigu-
ity within the lexicon, that occurs when a lexical
item is composed of two or more sub-words (like
in ?Butterfly?).
Since we are mainly investigating lexicon-
building models, it is important to measure the am-
biguity within the lexicon itself, in the ideal case
where this lexicon is perfect. To this end, we com-
puted the average number of segmentations for a
lexicon item. For example, the word ?butterfly?
has two possible segmentations : the original word
?butterfly? and a segmentation comprising the two
sub-words : ?butter? and ?fly?. For English to-
kens, we found an average of 1.039 in CDS and
1.057 in ADS. For Japanese tokens, we found an
average of 1.811 in CDS and 1.978 in ADS. En-
glish?s averages are close to 1, indicating that it
doesn?t exhibit lexicon ambiguity. Japanese, how-
ever, has averages close to 2 which means that lex-
ical ambiguity is quite systematic in both CDS and
ADS.
4.2 Segmentation ambiguity and supervised
segmentation
The intrinsic ambiguity in Japanese only shows
that a given sentence has multiple possible seg-
mentations. What remains to be demonstrated is
that these multiple segmentations result in system-
atic segmentation errors. To do this we propose
a supervised segmentation algorithm that enumer-
ates all possible segmentations of an utterance
based on the gold lexicon, and selects the segmen-
tation with the highest probability. In CDS data,
this algorithm yields a segmentation F-score equal
to 0.99 for English and 0.95 for Japanese. In ADS
we find an F-score of 0.96 for English and 0.93 for
Japanese. These results show that lexical informa-
tion alone plus word frequency eliminates almost
all segmentation errors in English, especially for
CDS. As for Japanese, even if the scores remain
impressively high, the lexicon alone is not suffi-
cient to eliminate all the errors. In other words,
even with a gold lexicon, English remains easier
to segment than Japanese.
To quantify the link between segmentation en-
tropy and segmentation errors, we binned the sen-
tences of our corpus in 10 bins according to the
Normalized Segmentation Entropy, and correlate
this with the average segmentation F-score for
each bin. As shown Figure 1, we found significant
correlations: (R = ?0.86, p < 0.001) for CDS
and (R = ?0.93, p < 0.001) for ADS, showing
that segmentation ambiguity has a strong effect
even on supervised segmentation scores. The cor-
relation within language was also significant but
only in the Japanese data : R = ?0.70 for CDS
and R = ?0.62 for ADS.
?
Next, we explore one possible reason for this
structural difference between Japanese and En-
glish, especially at the level of the lexicon.
4.3 Syllable structure and lexical
composition of Japanese and English
One of the most salient differences between En-
glish and Japanese phonology concerns their syl-
lable structure. This is illustrated in Figure 2
(above), where we plotted the frequency of the dif-
ferent syllabic structures of monosyllabic tokens
in English and Japanese CDS. The statistics show
that English has a very rich syllabic composition
where a diversity of consonant clusters is allowed,
whereas Japanese syllable structure is quite simple
and mostly composed of the default CV type. This
difference is bound to have an effect on the struc-
ture of the lexicon. Indeed, Japanese has to use
5
Figure 2 : Trade-off between the complexity of syllable structure (above) and the word token length in terms of syllables
(below) for English and Japanese CDS.
multisyllabic words in order to achieve a large size
lexicon, whereas, in principle, English could use
mostly monosyllables. In Figure 2 (below) we dis-
play the distribution of word length as measured
in syllables in the two languages for the CDS cor-
pora. The English data is indeed mostly composed
of mono-syllabic words whereas the Japanese one
is made of words of more varied lengths. Overall,
we have documented a trade-off between the di-
versity of syllable structure on the one hand, and
the diversity of word lengths on the other (see Ta-
ble 4 for a summary of this tradeoff expressed in
terms of entropy).
? CDS ADS
? Eng. Jap. Eng. Jap.
Syllable types 2.40 1.38 2.58 1.03
Token lengths 0.62 2.04 0.99 1.69
Table 4 : Entropies of syllable types and token lengths in
terms of syllables (in bits)
We suggest that this trade-off is responsible for
the difference in the lexicon ambiguity across the
two languages. Specifically, the combination of
a small number of syllable types and, as a conse-
quence, the tendency for multi-syllabic word types
in Japanese makes it likely that a long word will
be composed of smaller ones. This cannot happen
very often in English, since most words are mono-
syllabic, and words smaller than a syllable are not
allowed.
5 Improving Japanese unsupervised
segmentation
We showed in the previous section that ambigu-
ity impacts segmentation even with a gold lexicon,
mainly because the lexicon itself could be ambigu-
ous. In an unsupervised segmentation setting, the
problem is worse because ambiguity within and
across word boundaries leads to a bad lexicon,
which in turn results in more segmentation errors.
In this section, we explore the possibility of miti-
gating some of these negative consequences.
In section 3, we saw that when the Unigram
model tries to learn Japanese words, it produces an
output lexicon composed of both over- and under-
segmented words in addition to words that re-
sult from a segmentation across word boundaries.
One way to address this is by learning multiple
kinds of units jointly, rather than just words; in-
deed, previous work has shown that richer mod-
els with multiple levels improve segmentation for
English (Johnson, 2008a; Johnson and Goldwater,
2009).
5.1 Two dependency levels
As a first step, we will allow the model to not
just learn words but to also memorize sequences of
words. Johnson (2008a) introduced these units as
?collocations? but we choose to use the more neu-
tral notion of level for reasons that become clear
shortly. Concretely, the grammar is:
6
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.81 0.77 0.86 0.42 0.33 0.55 0.70 0.63 0.78 0.42 0.35 0.50
? Boundaries 0.91 0.84 0.98 0.63 0.47 0.96 0.86 0.76 0.98 0.73 0.61 0.90
? Lexicon 0.64 0.79 0.54 0.18 0.55 0.10 0.36 0.56 0.26 0.15 0.68 0.08
Level 2
? Segmentation 0.33 0.45 0.26 0.59 0.65 0.53 0.50 0.60 0.43 0.45 0.54 0.38
? Boundaries 0.56 0.98 0.40 0.71 0.87 0.60 0.76 0.95 0.64 0.73 0.92 0.60
? Lexicon 0.36 0.25 0.59 0.47 0.44 0.49 0.46 0.38 0.56 0.43 0.37 0.50
Table 5 : Word segmentation scores of the two levels model
?
Utterance? level2+
level2? level1+
level1? Phoneme+
?
We run this model under the same conditions
as the Unigram model but evaluate two different
situations. The model has no inductive bias that
would force it to equate level1 with words, rather
than level2. Consequently, we evaluate the seg-
mentation that is the result of taking there to be a
boundary between every level1 constituent (Level
1 in Table 5) and between every level2 constituent
(Level 2 in Table 5 ). From these results , we see
that English data has better scores when the lower
level represents the Word unit and when the higher
level captures regularities above the word. How-
ever, Japanese data is best segmented when the
higher level is the Word unit and the lower level
captures sub-word regularities.
Level 1 generally tends to over-segment utter-
ances as can be seen by comparing the Boundary
Recall and Precision scores (Goldwater, 2007). In
fact when the Recall is much higher than the Pre-
cision, we can say that the model has a tendency
to over-segment. Conversely, we see that Level 2
tends to under-segment utterances as the Bound-
ary Precision is higher than the Recall.
Over-segmentation at Level 1 seems to benefit
English since it counteracts the tendency of the
Unigram model to cluster high frequency colloca-
tions. As far as segmentation is concerned, this
effect seems to outweigh the negative effect of
breaking words apart (especially in CDS), as En-
glish words are mostly monosyllabic.
For Japanese, under-segmentation at Level 2
seems to be slightly less harmful than over-
segmentation at Level 1, as it prevents, to some
extent, multi-syllabic words to be split. However,
the scores are not very different from the ones we
had with the Unigram model and slightly worse
for the ADS. What seems to be missing is an inter-
mediate level where over- and under-segmentation
would counteract one another.
5.2 Three dependency levels
We add a third dependency level to our model as
follows :
?
Utterance? level3+
level3? level2+
level2? level1+
level1? Phoneme+
?
As with the previous model, we test each of the
three levels as the word unit, the results are shown
in Table 6.
Except for English CDS, all the corpora
have their best scores with this intermediate
level. Level 1 tends to over-segment Japanese
utterances into syllables and English utterances
into morphemes. Level 3, however, tends to
highly under-segment both languages. English
CDS seems to be already under-segmented at
Level 2, very likely caused by the large number
of word collocations like ?is-it? and ?what-is?,
an observation also made by Bo?rschinger et al
(2012) using different English CDS corpora.
English ADS is quantitatively more sensitive to
over-segmentation than CDS mainly because it
has a richer morphological structure and relatively
longer words in terms of syllables (Table 4).
7
? CDS ADS
? English Japanese English Japanese
? F P R F P R F P R F P R
Level 1
? Segmentation 0.79 0.74 0.85 0.27 0.20 0.41 0.35 0.28 0.48 0.37 0.30 0.47
? Boundaries 0.89 0.81 0.99 0.56 0.39 0.99 0.68 0.52 0.99 0.70 0.57 0.93
? Lexicon 0.58 0.76 0.46 0.10 0.47 0.05 0.13 0.39 0.07 0.10 0.70 0.05
Level 2
? Segmentation 0.49 0.60 0.42 0.70 0.70 0.70 0.77 0.76 0.79 0.60 0.65 0.55
? Boundaries 0.71 0.97 0.56 0.81 0.82 0.81 0.90 0.88 0.92 0.81 0.90 0.74
? Lexicon 0.51 0.41 0.64 0.53 0.59 0.47 0.58 0.69 0.50 0.51 0.57 0.46
Level 3
? Segmentation 0.18 0.31 0.12 0.39 0.53 0.30 0.43 0.55 0.36 0.28 0.42 0.21
? Boundaries 0.26 0.99 0.15 0.46 0.93 0.31 0.71 0.98 0.55 0.59 0.96 0.43
? Lexicon 0.17 0.10 0.38 0.32 0.25 0.41 0.37 0.28 0.51 0.27 0.20 0.42
Table 6 : Word segmentation scores of the three levels model
6 Conclusion
In this paper we identified a property of lan-
guage, segmentation ambiguity, which we quan-
tified through Normalized Segmentation Entropy.
We showed that this quantity predicts performance
in a supervised segmentation task.
With this tool we found that English was in-
trinsically less ambiguous than Japanese, account-
ing for the systematic difference found in this pa-
per. More generally, we suspect that Segmentation
Ambiguity would, to some extent, explain much
of the difference observed across languages (Ta-
ble 1). Further work needs to be carried out to test
the robustness of this hypothesis on a larger scale.
We showed that allowing the system to learn
at multiple levels of structure generally improves
performance, and compensates partially for the
negative effect of segmentation ambiguity on un-
supervised segmentation (where a bad lexicon am-
plifies the effect of segmentation ambiguity). Yet,
we end up with a situation where the best level of
structure may not be the same across corpora or
languages, which raises the question as to how to
determine which level is the correct lexical level,
i.e., the level that can sustain successful grammat-
ical and semantic learning. Further research is
needed to answer this question.
Generally speaking, ambiguity is a challenge in
many speech and language processing tasks: for
example part-of-speech tagging and word sense
disambiguation tackle lexical ambiguity, proba-
bilistic parsing deals with syntactic ambiguity and
speech act interpretation deals with pragmatic am-
biguities. However, to our knowledge, ambiguity
has rarely been considered as a serious problem in
word segmentation tasks.
As we have shown, the lexicon-based approach
does not completely solve the segmentation am-
biguity problem since the lexicon itself could be
more or less ambiguous depending on the lan-
guage. Evidently, however, infants in all lan-
guages manage to overcome this ambiguity. It has
to be the case, therefore, that they solve this prob-
lem through the use of alternative strategies, for
instance by relying on sub-lexical cues (see Jarosz
and Johnson (2013)) or by incorporating semantic
or syntactic constraints (Johnson et al, 2010). It
remains a major challenge to integrate these strate-
gies within a common model that can learn with
comparable performance across typologically dis-
tinct languages.
Acknowledgements
The research leading to these results has received funding
from the European Research Council (FP/2007-2013) / ERC
Grant Agreement n. ERC-2011-AdG-295810 BOOTPHON,
from the Agence Nationale pour la Recherche (ANR-2010-
BLAN-1901-1 BOOTLANG, ANR-11-0001-02 PSL* and
ANR-10-LABX-0087) and the Fondation de France. This
research was also supported under the Australian Research
Council?s Discovery Projects funding scheme (project num-
bers DP110102506 and DP110102593).
8
References
Eleanor Olds Batchelder. 2002. Bootstrapping the lex-
icon: A computational model of infant speech seg-
mentation. Cognition, 83(2):167?206.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. In K. Nelson and A. van Kleeck,
editors, Children?s Language, volume 6. Erlbaum,
Hillsdale, NJ.
Daniel Blanchard, Jeffrey Heinz, and Roberta
Golinkoff. 2010. Modeling the contribution of
phonotactic cues to the problem of word segmenta-
tion. Journal of Child Language, 37(3):487?511.
Benjamin Bo?rschinger, Katherine Demuth, and Mark
Johnson. 2012. Studying the effect of input size
for Bayesian word segmentation on the Providence
corpus. In Proceedings of the 24th International
Conference on Computational Linguistics (Coling
2012), pages 325?340, Mumbai, India. Coling 2012
Organizing Committee.
Luc Boruta, Sharon Peperkamp, Beno??t Crabbe?, and
Emmanuel Dupoux. 2011. Testing the robustness
of online word segmentation: Effects of linguistic
diversity and phonetic variation. In Proceedings of
the 2nd Workshop on Cognitive Modeling and Com-
putational Linguistics, pages 1?9, Portland, Oregon,
USA, June. Association for Computational Linguis-
tics.
M. Brent and T. Cartwright. 1996. Distributional regu-
larity and phonotactic constraints are useful for seg-
mentation. Cognition, 61:93?125.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
Morten H Christiansen, Joseph Allen, and Mark S Sei-
denberg. 1998. Learning to segment speech using
multiple cues: A connectionist model. Language
and cognitive processes, 13(2-3):221?268.
Robert Daland and Janet B Pierrehumbert. 2011.
Learning diphone-based segmentation. Cognitive
Science, 35(1):119?155.
Margaret M. Fleck. 2008. Lexicalized phonotac-
tic word segmentation. In Proceedings of ACL-08:
HLT, pages 130?138, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Sharon Goldwater. 2007. Nonparametric Bayesian
Models of Lexical Acquisition. Ph.D. thesis, Brown
University.
Naomi Hamasaki. 2002. The timing shift of two-year-
olds responses to caretakers yes/no questions. In
Studies in language sciences (2)Papers from the 2nd
Annual Conference of the Japanese Society for Lan-
guage Sciences, pages 193?206.
Gaja Jarosz and J Alex Johnson. 2013. The richness
of distributional cues to word boundaries in speech
to young children. Language Learning and Devel-
opment, (ahead-of-print):1?36.
Mark Johnson and Katherine Demuth. 2010. Unsuper-
vised phonemic Chinese word segmentation using
Adaptor Grammars. In Proceedings of the 23rd In-
ternational Conference on Computational Linguis-
tics (Coling 2010), pages 528?536, Beijing, China,
August. Coling 2010 Organizing Committee.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317?325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Elizabeth K. Johnson and Peter W. Jusczyk. 2001.
Word segmentation by 8-month-olds: When speech
cues count more than statistics. Journal of Memory
and Language, 44:1?20.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139?146, Rochester, New York. Associ-
ation for Computational Linguistics.
Mark Johnson, Katherine Demuth, Michael Frank, and
Bevan Jones. 2010. Synergies in learning words
and their referents. In J. Lafferty, C. K. I. Williams,
J. Shawe-Taylor, R.S. Zemel, and A. Culotta, ed-
itors, Advances in Neural Information Processing
Systems 23, pages 1018?1026.
Mark Johnson. 2008a. Unsupervised word segmen-
tation for Sesotho using Adaptor Grammars. In
Proceedings of the Tenth Meeting of ACL Special
Interest Group on Computational Morphology and
Phonology, pages 20?27, Columbus, Ohio, June.
Association for Computational Linguistics.
Mark Johnson. 2008b. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398?406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Peter W Jusczyk and Richard N Aslin. 1995. Infants
detection of the sound patterns of words in fluent
speech. Cognitive psychology, 29(1):1?23.
Peter W. Jusczyk, E. A. Hohne, and A. Bauman.
1999. Infants? sensitivity to allophonic cues for
word segmentation. Perception and Psychophysics,
61:1465?1476.
9
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk. Transcription, format and
programs, volume 1. Lawrence Erlbaum.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In proc. LREC, volume 2, pages 947?952.
Sven L Mattys, Peter W Jusczyk, Paul A Luce, James L
Morgan, et al 1999. Phonotactic and prosodic ef-
fects on word segmentation in infants. Cognitive
psychology, 38(4):465?494.
Sven L Mattys, Peter W Jusczyk, et al 2001. Do
infants segment words or recurring contiguous pat-
terns? Journal of experimental psychology, human
perception and performance, 27(3):644?655.
Bruna Pelucchi, Jessica F Hay, and Jenny R Saffran.
2009. Learning in reverse: Eight-month-old infants
track backward transitional probabilities. Cognition,
113(2):244?247.
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855?900.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
J. Saffran, R. Aslin, and E. Newport. 1996. Sta-
tistical learning by 8-month-old infants. Science,
274:1926?1928.
Claude Shannon. 1948. A mathematical theory of
communication. Bell System Technical Journal,
27(3):379?423.
Daniel Swingley. 2005. Statistical clustering and the
contents of the infant vocabulary. Cognitive Psy-
chology, 50:86?132.
A. Venkataraman. 2001. A statistical model for word
discovery in transcribed speech. Computational
Linguistics, 27(3):351?372.
Daniel J Weiss, Chip Gerfen, and Aaron D Mitchel.
2010. Colliding cues in word segmentation: the
role of cue strength and general cognitive processes.
Language and Cognitive Processes, 25(3):402?422.
Aris Xanthos. 2004. Combining utterance-boundary
and predictability approaches to speech segmenta-
tion. In First Workshop on Psycho-computational
Models of Human Language Acquisition, page 93.
10
Proceedings of the Eighteenth Conference on Computational Language Learning, pages 191?200,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
A Rudimentary Lexicon and Semantics Help Bootstrap Phoneme
Acquisition
Abdellah Fourtassi ???????????? Emmanuel Dupoux
Laboratoire de Sciences Cognitives et Psycholinguistique, ENS/EHESS/CNRS, Paris
{abdellah.fourtassi, emmanuel.dupoux}@gmail.com
Abstract
Infants spontaneously discover the rele-
vant phonemes of their language without
any direct supervision. This acquisition
is puzzling because it seems to require
the availability of high levels of linguistic
structures (lexicon, semantics), that logi-
cally suppose the infants having a set of
phonemes already. We show how this cir-
cularity can be broken by testing, in real-
size language corpora, a scenario whereby
infants would learn approximate represen-
tations at all levels, and then refine them in
a mutually constraining way. We start with
corpora of spontaneous speech that have
been encoded in a varying number of de-
tailed context-dependent allophones. We
derive, in an unsupervised way, an approx-
imate lexicon and a rudimentary seman-
tic representation. Despite the fact that
all these representations are poor approxi-
mations of the ground truth, they help re-
organize the fine grained categories into
phoneme-like categories with a high de-
gree of accuracy.
One of the most fascinating facts about human
infants is the speed at which they acquire their
native language. During the first year alone, i.e.,
before they are able to speak, infants achieve im-
pressive landmarks regarding three key language
components. First, they tune in on the phone-
mic categories of their language (Werker and Tees,
1984). Second, they learn to segment the continu-
ous speech stream into discrete units (Jusczyk and
Aslin, 1995). Third, they start to recognize fre-
quent words (Ngon et al., 2013), as well as the
semantics of many of them (Bergelson and Swing-
ley, 2012).
Even though these landmarks have been doc-
umented in detail over the past 40 years of re-
search, little is still known about the mechanisms
that are operative in infant?s brain to achieve such
a result. Current work in early language acquisi-
tion has proposed two competing but incomplete
hypotheses that purports to account for this stun-
ning development path. The bottom-up hypothesis
holds that infants converge onto the linguistic units
of their language through a statistical analysis over
of their input. In contrast, the top-down hypothesis
emphasizes the role of higher levels of linguistic
structure in learning the lower level units.
1 A chicken-and-egg problem
1.1 Bottom-up is not enough
Several studies have documented the fact that in-
fants become attuned to the native sounds of their
language, starting at 6 months of age (see Ger-
vain & Mehler, 2010 for a review). Some re-
searchers have claimed that such an early attune-
ment is due to a statistical learning mechanism that
only takes into account the distributional prop-
erties of the sounds present in the native input
(Maye et al., 2002). Unsupervised clustering al-
gorithms running on simplified input have, indeed,
provided a proof of principle for bottom-up learn-
ing of phonemic categories from speech (see for
instance Vallabha et al., 2007).
It is clear, however, that distributional learning
cannot account for the entire developmental pat-
tern. In fact, phoneme tokens in real speech ex-
hibit high acoustic variability and result in phone-
mic categories with a high degree of overlap (Hil-
lenbrand et al., 1995). When purely bottom up
clustering algorithms are tested on realistic input,
they ended up in either a too large number of sub-
phonemic units (Varadarajan et al., 2008) or a too
small number of coarse grained categories (Feld-
man et al., 2013a).
191
1.2 The top-down hypothesis
Inspection of the developmental data shows that
infants do not wait to have completed the acqui-
sition of their native phonemes to start to learn
words. In fact, lexical and phonological acquisi-
tion largely overlap. Infant can recognize highly
frequent word forms like their own names, by as
early as 4 months of age (Mandel et al., 1995).
Vice versa, the refinement of phonemic categories
does not stop at 12 months. The sensitivity to pho-
netic contrasts has been reported to continue at 3
years of age (Nittrouer, 1996) and beyond (Hazan
and Barrett, 2000), on par with the development of
the lexicon.
Some researchers have therefore suggested that
there might be a learning synergy which allows in-
fants to base some of their acquisition not only on
bottom up information, but also on statistics over
lexical items or even on the basis of word mean-
ing (Feldman et al., 2013a; Feldman et al., 2013b;
Yeung and Werker, 2009)
These experiments and computational models,
however, have focused on simplified input or/and
used already segmented words. It remains to be
shown whether the said top-down strategies scale
up when real size corpora and more realistic repre-
sentations are used. There are indeed indications
that, in the absence of a proper phonological repre-
sentation, lexical learning becomes very difficult.
For example, word segmentation algorithms that
work on the basis of phoneme-like units tend to
degrade quickly if phonemes are replaced by con-
textual allophones (Boruta et al., 2011) or with the
output of phone recognizers (Jansen et al., 2013;
Ludusan et al., 2014).
In brief, we are facing a chicken-and-egg prob-
lem: lexical and semantic information could help
to learn the phonemes, but phonemes are needed
to acquire lexical information.
1.3 Breaking the circularity: An incremental
discovery procedure
Here, we explore the idea that instead of learning
adult-like hierarchically organized representations
in a sequential fashion (phonemes, words, seman-
tics), infants learn approximate, provisional lin-
guistic representations in parallel. These approxi-
mate representations are subsequently used to im-
prove each other.
More precisely, we make four assumptions.
First, we assume that infants start by paying atten-
tion to fine grained variation in the acoustic input,
thus constructing perceptual phonetic categories
that are not phonemes, but segments encoding fine
grained phonetic details (Werker and Curtin, 2005;
Pierrehumbert, 2003). Second, we assume that
these units enable infants to segment proto-words
from continuous speech and store them in this de-
tailed format. Importantly, this proto-lexicon will
not be adult-like: it will contain badly segmented
word forms, and store several alternant forms for
the same word. Ngon et al. (2013) have shown
that 11 month old infants recognize frequent sound
sequences that do not necessarily map to adult
words. Third, we assume that infants can use this
imperfect lexicon to acquire some semantic repre-
sentation. As shown in Shukla et al. (2011), in-
fants can simultaneously segment words and asso-
ciate them with a visual referent. Fourth, we as-
sume that as their exposure to language develops,
infants reorganize these initial categories along the
relevant dimensions of their native language based
on cues from all these representations.
The aim of this work is to provide a proof of
principle for this general scenario, using real size
corpora in two typologically different languages,
and state-of-the-art learning algorithms.
The paper is organized as follows. We begin
by describing how we generated the input and
how we modeled different levels of representation.
Then, we explain how information from the higher
levels (word forms and semantics) can be used to
refine the learning of the lower level (phonetic cat-
egories). Next, we present the results of our sim-
ulations and discuss the potential implications for
the language learning process.
2 Modeling the representations
Here, we describe how we model different levels
of representation (phonetic categories, lexicon and
semantics) starting from raw speech in English
and Japanese.
2.1 Corpus
We use two speech corpora: the Buckeye Speech
corpus (Pitt et al., 2007), which contains 40 hours
of spontaneous conversations in American En-
glish, and the 40 hours core of the Corpus of Spon-
taneous Japanese (Maekawa et al., 2000), which
contains spontaneous conversations and public
speeches in different fields, ranging from engi-
neering to humanities. Following Boruta (2012),
192
we use an inventory of 25 phonemes for transcrib-
ing Japanese, and for English, we use the set of 45
phonemes in the phonemic transcription of Pitt et
al. (2007).
2.2 Phonetic categories
Here, we describe how we model the percep-
tual phonetic categories infants learn in a first
step before converging on the functional cate-
gories (phonemes). We make the assumption that
these initial categories correspond to fine grained
allophones, i.e., different systematic realizations
of phonemes, depending on context. Allophonic
variation can range from categorical effects due to
phonological rules to gradient effects due to coar-
ticulation, i.e, the phenomenon whereby adjacent
sounds affect the physical realization of a given
phoneme. An example of a rather categorical allo-
phonic rule is given by /r/ devoicing in French:
/r/?
{
[X] / before a voiceless obstruent
[K] elsewhere
Figure 1: Allophonic variation of French /r/
The phoneme /r/ surfaces as voiced ([K]) be-
fore a voiced obstruent like in [kanaK Zon] (?ca-
nard jaune?, yellow duck) and as voiceless ([X])
before a voiceless obstruent as in [kanaX puXpK]
(?canard pourpre?, purple duck). The challenge
facing the leaner is, therefore, to distinguish pairs
of segments that are in an allophonic relationship
([K], [X]) from pairs that are two distinct phonemes
and can carry a meaning difference ([K],[l]).
Previous work has generated allophonic varia-
tion artificially (Martin et al., 2013). Here, we fol-
low Fourtassi et al. (2014b) in using a linguisti-
cally and statistically controlled method, starting
from audio recordings and using a standard Hid-
den Markov Models (HMM) phone recognizer to
generate them, as follows.
We convert the raw speech waveform into suc-
cessive 10ms frames containing a vector of Mel
Frequency Cepstrum Coefficients (MFCC). We
use 12 MFC coefficients (plus the energy) com-
puted over a 25ms window, to which we add the
first and second order derivatives, yielding 39 di-
mensions per frame.
The HMM training starts with one three-state
model per phoneme. Each state is modeled by
a mixture of 17 diagonal Gaussians. After train-
ing, each phoneme model is cloned into context-
dependent triphone models, for each context in
which the phoneme actually occurs (for example,
the phoneme /A/ occurs in the context [d?A?g] as
in the word /dAg/ (?dog?). The triphone models
cloned from the phonemes are then retrained, but,
this time, only on the relevant subset of the data,
corresponding to the given triphone context. Fi-
nally, these detailed models are clustered back into
inventories of various sizes (from 2 to 20 times
the size of the phonemic inventory) and retrained.
Clustering is done state by state using a phonetic
feature-based decision tree, and results in tying
together the HMM states of linguistically simi-
lar triphones so as to maximize the likelihood of
the data. The HMM were built using the HMM
Toolkit (HTK: Young et al., 2006).
2.3 The proto-lexicon
Finding word boundaries in the continuous se-
quence of phones is part of the problem infants
have to solve without direct supervision. We
model this segmentation using a state-of-the-art
unsupervised word segmentation model based on
the Adaptor Grammar framework (Johnson et al.,
2007). The input consists of a phonetic transcrip-
tion of the corpus, with boundaries between words
eliminated (we vary this transcription to corre-
spond to different inventories with different granu-
larity in the allophonic representation as explained
above). The model tries to reconstruct the bound-
aries based on a Pitman-Yor process (Pitman and
Yor, 1997), which uses a language-general sta-
tistical learning process to find a compact rep-
resentation of the input. The algorithm stores
high frequency chunks and re-uses them to parse
novel utterances. We use a grammar which learns
a hierarchy of three levels of chunking and use
the intermediate level to correspond to the lexi-
cal level. This grammar was shown by Fourtassi
et al. (2013) to avoid both over-segmentation and
under-segmentation.
2.4 The proto-semantics
It has been shown that infants can keep track of co-
occurrence statistics (see Lany and Saffran (2013)
for a review). This ability can be used to develop a
sense of semantic similarity as suggested by Har-
ris (1954). The intuition behind the distributional
hypothesis is that words that are similar in mean-
ing occur in similar contexts. In order to model
the acquisition of this semantic similarity from a
193
transcribed and segmented corpus, we use one of
the simplest and most commonly used distribu-
tional semantic models, Latent Semantic Analysis
(LSA: Landauer & Dumais, 1997). The LSA al-
gorithm takes as input a matrix consisting of rows
representing word types and columns represent-
ing contexts in which tokens of the word type oc-
cur. A context is defined as a fixed number of
utterances. Singular value decomposition (a kind
of matrix factorization) is used to extract a more
compact representation. The cosine of the angle
between vectors in the resulting space is used to
measure the semantic similarity between words.
Two words have a high semantic similarity if they
have similar distributions, i.e., if they co-occur in
most contexts. The model parameters, namely the
dimension of the semantic space and the number
of utterances to be taken as defining the context
of a given word form, are set in an unsupervised
way to optimize the latent structure of the seman-
tic model (Fourtassi and Dupoux, 2013). Thus, we
use 20 utterances as a semantic window and set the
semantic space to 100 dimensions.
3 Method
Here we explore whether the approximate high
level representations, built bottom-up and with-
out supervision, still contain useful information
one can use to refine the phonetic categories into
phoneme-like units. To this end, we extract po-
tential cues from the lexical and the semantic in-
formation, and test their performance in discrim-
inating allophonic contrasts from non-allophonic
(phonemic) contrasts.
3.1 Top down cues
3.1.1 Lexical cue
The top down information from the lexicon is
based on the insight of Martin et al. (2013). It rests
on the idea that true lexical minimal pairs are not
very frequent in human languages, as compared to
minimal pairs due to mere phonological processes
(figure 1). The latter creates alternants of the same
lexical item since adjacent sounds condition the
realization of the first and final phoneme. There-
fore, finding a minimal pair of words differing in
the first or last segment (as in [kanaX] and [kanaK])
is good evidence that these two phones ([K], [X])
are allophones of one another. Conversely, if a
pair of phones is not forming any minimal pair,
it is classified as non-allophonic (phonemic).
However, this binary strategy clearly gives rise
to false alarms in the (albeit relatively rare) case
of true minimal pairs like [kanaX] (?duck?) and
[kanal] (?canal?), where ([X], [l]) will be mis-
takenly labeled as allophonic. In order to miti-
gate the problem of false alarms, we use Boruta?s
continuous version (Boruta, 2011) and we define
the lexical cue of a pair of phones Lex(x, y) as
the number of lexical minimal pairs that vary on
the first segment (xA, yA) or the last segment
(Ax,Ay). The higher this number, the more the
pair of phones is likely to be considered as allo-
phonic.
The lexical cue is consistent with experimen-
tal findings. For example Feldman et al. (2013b)
showed that 8 month-old infants pay attention
to word level information, and demonstrated that
they do not discriminate between sound contrasts
that occur in minimal pairs (as suggested by our
cue), and, conversely, discriminate contrasts that
occur in non-minimal pairs.
3.1.2 Semantic cue
The semantic cue is based on the intuition that
true minimal pairs ([kanaX] and [kanal]) are asso-
ciated with different events, whereas alternants of
the same word ([kanaX] and [kanal]) are expected
to co-occur with similar events.
We operationalize the semantic cue associated
with a pair of phones Sem(x, y) as the average
semantic similarity between all the lexical mini-
mal pairs generated by this pair of phones. The
higher the average semantic similarity, the more
the learner is prone to classify them as allophonic.
We take as a measure of the semantic similar-
ity, the cosine of the angle between word vec-
tors of the pairs that vary on the final segment
cos(
?
Ax,Ay) or the first segment cos(
?
xA, yA).
This strategy is similar in principle to the phe-
nomenon of acquired distinctiveness, according
to which, pairing two target stimuli with distinct
events enhances their perceptual differentiation,
and acquired equivalence, whereby pairing two
target stimuli with the same event, impairs their
subsequent differentiation (Lawrence, 1949). In
the same vein, Yeung and Werker (2009) tested 9
month-olds english learning infants in a task that
consists in discriminating two non-native phonetic
categories. They found that infants succeeded only
when the categories co-occurred with two distinct
visual cues.
194
? Segmentation Lexicon
? English Japanese English Japanese
Allo./phon. F P R F P R F P R F P R
2 0.61 0.57 0.65 0.45 0.44 0.47 0.29 0.42 0.22 0.23 0.54 0.15
4 0.52 0.46 0.59 0.38 0.34 0.43 0.22 0.37 0.15 0.16 0.50 0.10
10 0.51 0.45 0.59 0.34 0.30 0.38 0.21 0.34 0.16 0.16 0.41 0.10
20 0.42 0.38 0.47 0.28 0.26 0.32 0.21 0.29 0.17 0.16 0.32 0.10
Table 1 : Scores of the segmentation and the resulting lexicon, as a function of the average number of
allophones per phoneme. P=Precison, R=Recall and F=F-score.
3.1.3 Combined cue
Finally, we consider the combination of both cues
in one single cue where the contextual information
(semantics) is used as a weighing scheme of the
lexical information, as follows:
Comb(x, y) =
?
(Ax,Ay)?L
2
cos(
?
Ax,Ay) +
?
(xA,yA)?L
2
cos(
?
xA, yA)
(1)
where {Ax ? L} is the set of words in the lex-
icon L that end in the phone x, and {(Ax,Ay) ?
L
2
} is the set of phonological minimal pairs in
L? L that vary on the final segment.
The lexical cue is incremented by one, for ev-
ery minimal pair. The combined cue is, instead,
incremented by one, times the cosine of the angle
between the word vectors of this pair. When the
words have similar distributions, the angle goes to
zero and the cosine goes to 1, and when the words
have orthogonal distributions, the angle goes to
90
?
and the cosine goes to 0.
The semantic information here would basically
enable us to avoid false alarms generated by poten-
tial true minimal pairs like the above-mentioned
example of ( [kanaX] and [kanal]). Such a pair will
probably score high as far as the lexical cue is con-
cerned, but it will score low on the semantic level.
Thus, by taking the combination, the model will
be less prone to mistakenly classify ([X], [l]) as al-
lophones.
3.2 Task
For each corpus we list all possible pairs of al-
lophones. Some of these pairs are allophones of
the same phoneme (allophonic pair) and others are
allophones of different phonemes (non-allophonic
pairs). The task is a same-different classification,
whereby each of these pairs is given a score from
the cue that is being tested. A good cue gives
higher scores to allophonic pairs.
Only pairs of phones that generate at least one
lexical minimal pair are considered. Phonetic vari-
ation that does not cause lexical variation is ?in-
visible? to top down strategies, and is, therefore,
more probably clustered through purely bottom up
strategies (Fourtassi et al., 2014b)
3.3 Evaluation
We use the same evaluation procedure as Martin et
al. (2013). This is carried out by computing the as-
sociated ROC curve (varying the z-score threshold
and computing the resulting proportions of misses
and false alarms). We then derive the Area Under
the Curve (AUC), which also corresponds to the
probability that given two pairs of phones, one al-
lophonic, one not, they are correctly classified on
the basis of the score. A value of 0.5 represents
chance and a value of 1 represents perfect perfor-
mance.
In order to lessen the potential influence of the
structure of the corpus (mainly the order of the ut-
terances) on the results, we use a statistical resam-
pling scheme. The corpus is divided into small
blocks of 20 utterances each (the semantic win-
dow). In each run, we draw randomly with re-
placement from this set of blocks a sample of
the same size as the original corpus. This sam-
ple is then used to retrain the acoustic models and
generate a phonetic inventory that we used to re-
transcribe the corpus and re-compute the cues. We
report scores averaged over 5 such runs.
4 Results and discussion
4.1 Segmentation
We first explore how phonetic variation influences
the quality of the segmentation and the resulting
lexicon. For the evaluation, we use the same mea-
sures as Brent (1999) and Goldwater et al. (2009),
namely Segmentation Precision (P), Recall (R)
and F-score (F). Segmentation precision is defined
195
as the number of correct word tokens found, out of
all tokens posited. Recall is the number of correct
word tokens found, out of all tokens in the ideal
segmentation. The F-score is defined as the har-
monic mean of Precision and Recall:
F =
2 ? P ?R
P + R
We define similar measures for word types (lex-
icon). Table 1 shows the scores as a function of
the number of allophones per phonemes. For both
corpora, the segmentation performance decreases
as we increase the number of allophones. As for
the lexicon, the recall scores show that only 15
to 22% of the ?words? found by the algorithm in
the English corpus are real words; in Japanese,
this number is even lower (between 10 and 15%).
This pattern can be attributed in part to the fact
that increasing the number of allophones increases
the number of word forms, which occur therefore
with less frequency, making the statistical learn-
ing harder. Table 2 shows the average number of
word forms per word as a function of the average
number of allophones per phoneme, in the case of
ideal segmentation.
Allo./Phon. W. forms/Word
? English Japanese
2 1.56 1.20
4 2.03 1.64
10 2.69 2.11
20 3.47 2.83
Table 2 : Average number of word-forms per
word as a function of the average number of
allophones per phoneme.
Another effect seen in Table 1 is the lower
overall performance of Japanese compared to En-
glish. This difference was shown by Fourtassi et
al. (2013) to be linked to the intrinsic segmenta-
tion ambiguity of Japanese, caused by the fact that
Japanese words contain more syllables compared
to English.
4.2 Allophonic vs phonemic status of sound
contrasts
Here we test the performance of the cues described
above, in discriminating between allophonic con-
trasts from phonemic ones. We vary the number
of allophones per phoneme, on the one hand (Fig-
ure 2a), and the amount of data available to the
learner, on the other hand, in the case of two allo-
phones per phonemes (Figure 2b). In both situa-
tions, we compare the case wherein the lexical and
semantic cues are computed on the output of the
unsupervised segmentation (right), to the control
case where these cues are computed on the ideally
segmented speech (left).
We see that the overall accuracy of the cues is
quite high, even in the case of bad word segmen-
tation and very small amount of data.
The lexical cue is robust to extreme variation
and to the scarcity of data. Indeed, it does not seem
to vary monotonically neither with the number of
allophones, nor with the size of the corpus. The as-
sociated f-score generally remains above the value
of 0.7 (chance level is 0.5). The semantics, on
the other hand, gets better as the variability de-
creases and as the amount of data increases. This
is a natural consequence of the fact that the se-
mantic structure is more accurate with more data
and with word forms consistent enough to sustain
a reasonable co-occurrence statistics.
The comparison with the ideal segmentation,
shows, interestingly, that the semantics is more ro-
bust to segmentation errors than the lexical cue. In
fact, while the lexical strategy performs, overall,
better than the semantics under the ideal segmen-
tation, the patterns reverses as we move to a a more
realistic (unsupervised) segmentation.
These results suggest that both lexical and se-
mantic strategies can be crucial to learning the
phonemic status of phonetic categories since they
provide non-redundant information. This finding
is summarized by the combined cue which resists
to both variation and segmentation errors, overall,
better than each of the cues taken alone.
From a developmental point of view, this shows
that infants can, in principle, benefit from higher
level linguistic structures to refine their phonetic
categories, even if these structures are rudimen-
tary. Previous studies about top down strategies
have mainly emphasized the role of word forms;
the results of this work show that the semantics
can be at least as useful. Note that the notion
of semantics used here is weaker than the clas-
sic notion of referential semantics as in a word-
concept matching. The latter might, indeed, not
be fully operative at the early stages of the child
development, since it requires some advanced con-
ceptual abilities (like forming symbolic represen-
tations and understanding a speaker?s referential
196
a)
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
2 5 10 20 2 5 10 20
Allophones/Phoneme
AU
C
English
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
2 5 10 20 2 5 10 20
Allophones/Phoneme
AU
C
Japanese
Cues
Lexical
Semantic
Combined
b)
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
1 2 4 8 20 40 1 2 4 8 20 40
Size (in hours)
AU
C
English
Ideal Unsupervised
0.5
0.6
0.7
0.8
0.9
1.0
1 2 4 8 20 40 1 2 4 8 20 40
Size (in hours)
AU
C
Japanese
Cues
Lexical
Semantic
Combined
Figure 2: Same-different scores (AUC) for different cues as a function of the average number of allo-
phones per phoneme (a), and as a function of the size of the corpus, in the case of two allophones per
phonemes (b). The scores are shown for both ideal and unsupervised word segmentation in English and
Japanese. The points show the mean scores over 5 runs. The lines are smoothed interpolations (local
regressions) through the means. The grey band shows a 95% confidence interval.
intentions) (Waxman and Gelman, 2009). What
we call the ?semantics? of a word in this study, is
the general context provided by the co-occurrence
with other words. Infants have been shown to have
a powerful mechanism for tracking co-occurrence
relationships both in the speech and the visual do-
main (Lany and Saffran, 2013) . Our experiments
demonstrate that a similar mechanism could be
enough to develop a sense of semantic similarity
that can successfully be used to refine phonetic
categories.
5 General discussion and future work
Phonemes are abstract categories that form the ba-
sis for words in the lexicon. There is a traditional
view that they should be defined by their ability to
contrast word meanings (Trubetzkoy, 1939). Their
full acquisition, therefore, requires lexical and se-
mantic top-down information. However, since the
quality of the semantic representations depends on
the quality of the phonemic representations that
are used to build the lexicon, we face a chicken-
and-egg problem. In this paper, we proposed a
way to break the circularity by building approxi-
mate representation at all the levels.
The infants? initial attunement to language-
specific categories was represented in a way that
mirrors the linguistic and statistical properties of
the speech closely. We showed that this de-
tailed (proto-phonemic) inventory enabled word
segmentation from continuous transcribed speech,
but, as expected, resulted in a low quality lexicon.
The poorly segmented corpus was then used to de-
rive a semantic similarity matrix between pairs of
words, based on their co-occurrence statistics. The
results showed that information from the derived
lexicon and semantics, albeit very rudimentary,
help discriminate between allophonic and phone-
mic contrasts, with a high degree of accuracy.
Thus, this works strongly support the claim that
the lexicon and semantics play a role in the re-
finement of the phonemic inventory (Feldman et
197
al., 2013a; Frank et al., 2014), and, interestingly,
that this role remains functional under more realis-
tic assumptions (unsupervised word segmentation,
and bottom-up inferred semantics). We also found
that lexical and semantic information were not re-
dundant and could be usefully combined, the for-
mer being more resistant to the scarcity of data
and variation, and the latter being more resistant
to segmentation errors.
That being said, this work relies on the assump-
tion that infants start with initial perceptual cate-
gories (allophones), but we did not show how such
categories could be constructed from raw speech.
More work is needed to explore the robustness of
the model when these units are learned in an unsu-
pervised fashion (Lee and Glass, 2012; Huijbregts
et al., 2011; Jansen and Church, 2011; Varadarajan
et al., 2008).
This work could be seen as a proof of princi-
ple for an iterative learning algorithm, whereby
phonemes emerge from the interaction of low level
perceptual categories, word forms, and the seman-
tics (see Werker and Curtin (2005) for a similar
theoretical proposition). The algorithm has yet to
be implemented, but it has to address at least two
major issues: First, the fact that some sound pairs
are not captured by top down cues because they
do not surface as minimal word forms. For in-
stance, in English, /h/ and /N/ occur in different
syllable positions and therefore, cannot appear in
any minimal pair. Second, even if we have enough
information about how phonetic categories are or-
ganized in the perceptual space, we still need to
know how many categories are relevant in a par-
ticular language (i.e., where to stop the categoriza-
tion process).
For the first problem, Fourtassi et al. (2014b)
showed that the gap could, in principle, be filled by
bottom-up information (like acoustic similarity).
As for the second problem, a possible direction
could be found in the notion of Self-Consistency.
In fact, (Fourtassi et al., 2014a) proposed that an
optimal level of clustering is also a level that glob-
ally optimizes the predictive power of the lexicon.
Too detailed allophones result in too many syn-
onyms. Too broad classes result in too many ho-
mophones. Somewhere in the middle, the optimal
number of phonemes optimizes how lexical items
predict each other. Future work will address these
issues in more detail in order to propose a com-
plete phoneme learning algorithm.
Acknowledgments
This work was supported in part by the Euro-
pean Research Council (ERC-2011-AdG-295810
BOOTPHON), the Agence Nationale pour la
Recherche (ANR-10-LABX-0087 IEC, ANR-10-
IDEX-0001-02 PSL*), the Fondation de France,
the Ecole de Neurosciences de Paris, and the
R?egion Ile de France (DIM cerveau et pens?ee).
References
Elika Bergelson and Daniel Swingley. 2012. At 6
to 9 months, human infants know the meanings of
many common nouns. Proceedings of the National
Academy of Sciences, 109(9).
Luc Boruta, Sharon Peperkamp, Beno??t Crabb?e, and
Emmanuel Dupoux. 2011. Testing the robustness
of online word segmentation: Effects of linguistic
diversity and phonetic variation. In Proceedings of
CMCL, pages 1?9. Association for Computational
Linguistics.
Luc Boruta. 2011. Combining Indicators of Al-
lophony. In Proceedings ACL-SRW, pages 88?93.
Luc Boruta. 2012. Indicateurs d?allophonie et
de phon?emicit?e. Doctoral dissertation, Universit
?e
Paris-Diderot - Paris VII.
M. Brent. 1999. An efficient, probabilistically sound
algorithm for segmentation and word discovery.
Machine Learning, 34:71?105.
N. Feldman, T. Griffiths, S. Goldwater, and J. Morgan.
2013a. A role for the developing lexicon in pho-
netic category acquisition. Psychological Review,
120(4):751?778.
N. Feldman, B. Myers, K. White, T. Griffiths, and
J. Morgan. 2013b. Word-level information influ-
ences phonetic learning in adults and infants. Cog-
nition, 127:427?438.
Abdellah Fourtassi and Emmanuel Dupoux. 2013. A
corpus-based evaluation method for distributional
semantic models. In 51st Annual Meeting of the
Association for Computational Linguistics Proceed-
ings of the Student Research Workshop, pages 165?
171, Sofia, Bulgaria. Association for Computational
Linguistics.
Abdellah Fourtassi, Benjamin B?orschinger, Mark
Johnson, and Emmanuel Dupoux. 2013. WhyisEn-
glishsoeasytosegment? In Proceedings of CMCL,
pages 1?10. Association for Computational Linguis-
tics.
Abdellah Fourtassi, Ewan Dunbar, and Emmanuel
Dupoux. 2014a. Self-consistency as an inductive
bias in early language acquisition. In Proceedings
of the 36th annual meeting of the Cognitive Science
Society.
198
Abdellah Fourtassi, Thomas Schatz, Balakrishnan
Varadarajan, and Emmanuel Dupoux. 2014b. Ex-
ploring the Relative Role of Bottom-up and Top-
down Information in Phoneme Learning. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics.
Stella Frank, Naomi Feldman, and Sharon Goldwater.
2014. Weak semantic context helps phonetic learn-
ing in a model of infant language acquisition. In
Proceedings of the 52nd Annual Meeting of the As-
sociation of Computational Linguistics.
Judit Gervain and Jacques Mehler. 2010. Speech per-
ception and language acquisition in the first year of
life. Annual Review of Psychology, 61:191?218.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21?54.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Valerie Hazan and Sarah Barrett. 2000. The develop-
ment of phonemic categorization in children aged 6
to12. Journal of Phonetics, 28:377?396.
James Hillenbrand, Laura A. Getty, Michael J. Clark,
and Kimberlee Wheeler. 1995. Acoustic charac-
teristics of american english vowels. Journal of the
Acoustical Society of America, 97:3099?3109.
M. Huijbregts, M. McLaren, and D. van Leeuwen.
2011. Unsupervised acoustic sub-word unit detec-
tion for query-by-example spoken term detection. In
Proceedings of ICASSP, pages 4436?4439.
A. Jansen and K. Church. 2011. Towards unsupervised
training of speaker independent acoustic models. In
Proceedings of INTERSPEECH, pages 1693?1696.
Aren Jansen, Emmanuel Dupoux, Sharon Goldwa-
ter, Mark Johnson, Sanjeev Khudanpur, Kenneth
Church, Naomi Feldman, Hynek Hermansky, Flo-
rian Metze, Richard Rose, Mike Seltzer, Pascal
Clark, Ian McGraw, Balakrishnan Varadarajan, Erin
Bennett, Benjamin Borschinger, Justin Chiu, Ewan
Dunbar, Abdallah Fourtassi, David Harwath, Chia
ying Lee, Keith Levin, Atta Norouzian, Vijay
Peddinti, Rachel Richardson, Thomas Schatz, and
Samuel Thomas. 2013. A summary of the 2012 jhu
clsp workshop on zero resource speech technologies
and models of early language acquisition. In Pro-
ceedings of ICASSP.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Sch?olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641?648. MIT Press, Cambridge,
MA.
Peter W Jusczyk and Richard N Aslin. 1995. Infants?
detection of the sound patterns of words in fluent
speech. Cognitive psychology, 29(1):1?23.
Thomas K Landauer and Susan T Dumais. 1997. A
solution to Plato?s problem: The Latent Semantic
Analysis theory of acquisition, induction and rep-
resentation of knowledge. Psychological Review,
104(2):211?240.
J. Lany and J. Saffran. 2013. Statistical learning mech-
anisms in infancy. In J. Rubenstein and P. Rakic, ed-
itors, Comprehensive Developmental Neuroscience:
Neural Circuit Development and Function in the
Brain, volume 3, pages 231?248. Elsevier, Amster-
dam.
D.H. Lawrence. 1949. Acquired distinctiveness of
cues: I. transfer between discriminations on the ba-
sis of familiarity with the stimulus. Journal of Ex-
perimental Psychology, 39(6):770?784.
C. Lee and J. Glass. 2012. A nonparametric bayesian
approach to acoustic model discovery. In Proceed-
ings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-
Volume 1, pages 40?49.
Bogdan Ludusan, Maarten Versteegh, Aren Jansen,
Guillaume Gravier, Xuan-Nga Cao, Mark Johnson,
and Emmanuel Dupoux. 2014. Bridging the gap be-
tween speech technology and natural language pro-
cessing: an evaluation toolbox for term discovery
systems. In Proceedings of LREC.
Kikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hi-
toshi Isahara. 2000. Spontaneous speech corpus of
japanese. In LREC, pages 947?952, Athens, Greece.
D.R. Mandel, P.W. Jusczyk, and D.B. Pisoni. 1995. In-
fants? recognition of the sound patterns of their own
names. Psychological Science, 6(5):314?317.
Andrew Martin, Sharon Peperkamp, and Emmanuel
Dupoux. 2013. Learning phonemes with a proto-
lexicon. Cognitive Science, 37(1):103?124.
J. Maye, J. F. Werker, and L. Gerken. 2002. Infant sen-
sitivity to distributional information can affect pho-
netic discrimination. Cognition, 82:B101?B111.
C. Ngon, A. Martin, E. Dupoux, D. Cabrol, M. Duthat,
and S. Peperkamp. 2013. (non)words, (non)words,
(non)words: evidence for a protolexicon during the
first year of life. Developmental Science, 16(1):24?
34.
S. Nittrouer. 1996. Discriminability and perceptual
weighting of some acoustic cues to speech percep-
tion by 3-year-olds. Journal of Speech and Hearing
Research, 39:278?297.
J. B. Pierrehumbert. 2003. Phonetic diversity, statis-
tical learning, and acquisition of phonology. Lan-
guage and Speech, 46(2-3):115?154.
199
J. Pitman and M. Yor. 1997. The two-parameter
Poisson-Dirichlet distribution derived from a stable
subordinator. Annals of Probability, 25:855?900.
M. A. Pitt, L. Dilley, K. Johnson, S. Kiesling, W. Ray-
mond, E. Hume, and Fosler-Lussier. 2007. Buckeye
corpus of conversational speech.
M Shukla, K White, and R Aslin. 2011. Prosody
guides the rapid mapping of auditory word forms
onto visual objects in 6-mo-old infants. Proceedings
of the National Academy of Sciences, 108(15):6038?
6043.
N. S. Trubetzkoy. 1939. Grundz?uge der Phonolo-
gie (Principles of phonology). Vandenhoeck &
Ruprecht, G?ottingen, Germany.
G. K. Vallabha, J. L. McClelland, F. Pons, J. F.
Werker, and S. Amano. 2007. Unsupervised learn-
ing of vowel categories from infant-directed speech.
Proceedings of the National Academy of Sciences,
104(33):13273.
Balakrishnan Varadarajan, Sanjeev Khudanpur, and
Emmanuel Dupoux. 2008. Unsupervised learning
of acoustic sub-word units. In Proceedings of ACL-
08: HLT, Short Papers, pages 165?168. Association
for Computational Linguistics.
Sandra R. Waxman and Susan A. Gelman. 2009. Early
word-learning entails reference, not merely associa-
tions. Trends in Cognitive Sciences, 13(6):258?263.
J. F. Werker and S. Curtin. 2005. PRIMIR: A develop-
mental framework of infant speech processing. Lan-
guage Learning and Development, 1(2):197?234.
Janet F. Werker and Richard C. Tees. 1984. Cross-
language speech perception: Evidence for percep-
tual reorganization during the first year of life. In-
fant Behavior and Development, 7(1):49 ? 63.
H Yeung and J Werker. 2009. Learning words? sounds
before learning how words sound: 9-month-olds use
distinct objects as cues to categorize speech infor-
mation. Cognition, 113:234?243.
Steve J. Young, D. Kershaw, J. Odell, D. Ollason,
V. Valtchev, and P. Woodland. 2006. The HTK Book
Version 3.4. Cambridge University Press.
200
