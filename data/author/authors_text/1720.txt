An Analysis of Clarification Dialogue for Question Answering
Marco De Boni

School of Computing
Leeds Metropolitan University
Leeds LS6 3QS, UK
Department of Computer Science
University of York
York Y010 5DD, UK
mdeboni@cs.york.ac.uk
Suresh Manandhar

 Department of Computer Science
University of York
York Y010 5DD, UK
suresh@cs.york.ac.uk



Abstract
We examine clarification dialogue, a
mechanism for refining user questions with
follow-up questions, in the context of open
domain Question Answering systems. We
develop an algorithm for clarification dialogue
recognition through the analysis of collected
data on clarification dialogues and examine
the importance of clarification dialogue
recognition for question answering. The
algorithm is evaluated and shown to
successfully recognize the occurrence of
clarification dialogue in the majority of cases
and to simplify the task of answer retrieval.
1 Clarification dialogues in Question
Answering
Question Answering Systems aim to determine an
answer to a question by searching for a response in a
collection of documents (see Voorhees 2002 for an
overview of current systems). In order to achieve this
(see for example Harabagiu et al 2002), systems narrow
down the search by using information retrieval
techniques to select a subset of documents, or
paragraphs within documents, containing keywords
from the question and a concept which corresponds to
the correct question type (e.g. a question starting with
the word ?Who?? would require an answer containing a
person). The exact answer sentence is then sought by
either attempting to unify the answer semantically with
the question, through some kind of logical
transformation (e.g. Moldovan and Rus 2001) or by
some form of pattern matching (e.g. Soubbotin 2002;
Harabagiu et al 1999).
Often, though, a single question is not enough to meet
user?s goals and an elaboration or clarification dialogue
is required, i.e. a dialogue with the user which would
enable the answering system to refine its understanding
of the questioner's needs (for reasons of space we shall
not investigate here the difference between elaboration
dialogues, clarification dialogues and coherent topical
subdialogues and we shall hence refer to this type of
dialogue simply as ?clarification dialogue?, noting that
this may not be entirely satisfactory from a theoretical
linguistic point of view). While a number of researchers
have looked at clarification dialogue from a theoretical
point of view (e.g. Ginzburg 1998; Ginzburg and Sag
2000; van Beek at al. 1993), or from the point of view
of task oriented dialogue within a narrow domain (e.g.
Ardissono and Sestero 1996), we are not aware of any
work on clarification dialogue for open domain question
answering systems such as the ones presented at the
TREC workshops, apart from the experiments carried
out for the (subsequently abandoned) ?context? task in
the TREC-10 QA workshop (Voorhees 2002; Harabagiu
et al 2002). Here we seek to partially address this
problem by looking at some particular aspect of
clarification dialogues in the context of open domain
question answering. In particular, we examine the
problem of recognizing that a clarification dialogue is
occurring, i.e. how to recognize that the current question
under consideration is part of a previous series (i.e.
clarifying previous questions) or the start of a new
series; we then show how the recognition that a
clarification dialogue is occurring can simplify the
problem of answer retrieval.
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 48-55
                                                         Proceedings of HLT-NAACL 2003
2 The TREC Context Experiments
The TREC-2001 QA track included a "context" task
which aimed at testing systems' ability to track context
through a series of questions (Voorhees 2002). In other
words, systems were required to respond correctly to a
kind of clarification dialogue in which a full
understanding of questions depended on an
understanding of previous questions. In order to test the
ability to answer such questions correctly, a total of 42
questions were prepared by NIST staff, divided into 10
series of related question sentences which therefore
constituted a type of clarification dialogue; the
sentences varied in length between 3 and 8 questions,
with an average of 4 questions per dialogue. These
clarification dialogues were however presented to the
question answering systems already classified and hence
systems did not need to recognize that clarification was
actually taking place.  Consequently systems that simply
looked for an answer in the subset of documents
retrieved for the first question in a series performed well
without any understanding of the fact that the questions
constituted a coherent series.
In a more realistic approach, systems would not be
informed in advance of the start and end of a series of
clarification questions and would not be able to use this
information to limit the subset of documents in which
an answer is to be sought.
3 Analysis of the TREC context questions
We manually analysed the TREC context question
collection in order to determine what features could be
used to determine the start and end of a question series,
with the following conclusions:
? Pronouns and possessive adjectives: questions such
as ?When was it born??, which followed ?What was
the first transgenic mammal??, were referring to
some previously mentioned object through a
pronoun (?it?). The use of personal pronouns (?he?,
?it?, ?) and possessive adjectives (?his?, ?her?,?)
which did not have any referent in the question
under consideration was therefore considered an
indication of a clarification question..
? Absence of verbs: questions such as ?On what body
of water?? clearly referred to some previous
question or answer.
? Repetition of proper nouns: the question series
starting with ?What type of vessel was the modern
Varyag?? had a follow-up question ?How long was
the Varyag??, where the repetition of the proper
noun indicates that the same subject matter is under
investigation.
? Importance of semantic relations: the first question
series started with the question ?Which museum in
Florence was damaged by a major bomb
explosion??; follow-up questions included ?How
many people were killed?? and ?How much
explosive was used??, where there is a clear
semantic relation between the ?explosion? of the
initial question and the ?killing? and ?explosive? of
the following questions. Questions belonging to a
series were ?about? the same subject, and this
aboutness could be seen in the use of semantically
related words.
4 Experiments in Clarification Dialogue
Recognition
It was therefore speculated that an algorithm which
made use of these features would successfully recognize
the occurrence of clarification dialogue. Given that the
only available data was the collection of ?context?
questions used in TREC-10, it was felt necessary to
collect further data in order to test our algorithm
rigorously. This was necessary both because of the
small number of questions in the TREC data and the
fact that there was no guarantee that an algorithm built
for this dataset would perform well on ?real? user
questions. A collection of 253 questions was therefore
put together by asking potential users to seek
information on a particular topic by asking a prototype
question answering system a series of questions, with
?cue? questions derived from the TREC question
collection given as starting points for the dialogues.
These questions made up 24 clarification dialogues,
varying in length from 3 questions to 23, with an
average length of 12 questions (the data is available
from the main author upon request).
The differences between the TREC ?context?
collection and the new collection are summarized in the
following table:

 Groups Qs Av. len Max Min
TREC 10 41 4 8 4
New 24 253 12 23 3

The questions were recorded and manually tagged to
recognize the occurrence of clarification dialogue.
The questions thus collected were then fed into a
system implementing the algorithm, with no indication
as to where a clarification dialogue occurred. The
system then attempted to recognize the occurrence of a
clarification dialogue. Finally the results given by the
system were compared to the manually recognized
clarification dialogue tags. In particular the algorithm
was evaluated for its capacity to:
? recognize a new series of questions (i.e. to tell that
the current question is not a clarification of any
previous question) (indicated by New in the results
table)
? recognize that the current question is clarifying a
previous question (indicated by Clarification in the
table)
5 Clarification Recognition Algorithm
Our approach to clarification dialogue recognition
looks at certain features of the question currently under
consideration (e.g. pronouns and proper nouns) and
compares the meaning of the current question with the
meanings of previous questions to determine whether
they are ?about? the same matter.
Given a question q0  and n  previously asked
questions q
-1..q-n  we have a function
Clarification_Question which is true if a question is
considered a clarification of a previously asked
question. In the light of empirical work such as
(Ginzburg 1998), which indicates that questioners do
not usually refer back to questions which are very
distant, we only considered the set of the previously
mentioned 10 questions.
A question is deemed to be a clarification of a
previous question if:
1. There are direct references to nouns mentioned in
the previous n  questions through  the use of
pronouns (he, she, it, ?) or possessive adjectives
(his, her, its?) which have no references in the
current question.
2. The question does not contain any verbs
3. There are explicit references to proper and common
nouns mentioned in the previous n  questions, i.e.
repetitions which refer to an identical object; or
there is a strong sentence similarity between the
current question and the previously asked
questions.
In other words:

Clarification_Question
   (qn,q-1..q-n)
is true if
1. q0  has pronoun and
possessive adjective
references to q
-1..q-n 
2. q0  does not contain any
verbs
3. q0 has repetition of
common or proper nouns
in q
-1..q-n or q0  has a
strong semantic
similarity to some q ?
q
-1..q-n 
6 Sentence Similarity Metric
A major part of our clarification dialogue recognition
algorithm is the sentence similarity metric which looks
at the similarity in meaning between the current
question and previous questions. WordNet (Miller 1999;
Fellbaum 1998), a lexical database which organizes
words into synsets, sets of synonymous words, and
specifies a number of relationships such as hypernym,
synonym, meronym which can exist between the synsets
in the lexicon, has been shown to be fruitful in the
calculation of semantic similarity. One approach has
been to determine similarity by calculating the length of
the path or relations connecting the words which
constitute sentences (see for example Green 1997 and
Hirst and St-Onge 1998); different approaches have
been proposed (for an evaluation see (Budanitsky and
Hirst 2001)), either using all WordNet relations
(Budanitsky and Hirst 2001) or only is-a relations
(Resnik 1995; Jiang and Conrath 1997; Mihalcea and
Moldvoan 1999). Miller (1999), Harabagiu et al (2002)
and De Boni and Manandhar (2002) found WordNet
glosses, considered as micro-contexts, to be useful in
determining conceptual similarity. (Lee et al 2002)
have applied conceptual similarity to the Question
Answering task, giving an answer A  a score dependent
on the number of matching terms in A  and the question.
Our sentence similarity measure followed on these
ideas, adding to the use of WordNet relations, part-of-
speech information, compound noun and word
frequency information.
In particular, sentence similarity was considered as a
function which took as arguments a sentence s1  and a
second sentence s2 and returned a value representing the
semantic relevance of s1  in respect of s2  in the context of
knowledge B, i.e.

semantic-relevance( s1, s2, B  ) = n ?
 


semantic-relevance(s1,s,B) < semantic-
relevance(s2,s, B) represents the fact that sentence s1  is
less relevant than s2  in respect to the sentence s and the
context B. In our experiments, B  was taken to be the set
of semantic relations given by WordNet. Clearly, the
use of a different knowledge base would give different
results, depending on its completeness and correctness.
In order to calculate the semantic similarity between
a sentence s1  and another sentence s2, s1  and s2  were
considered as sets P  and Q  of word stems. The
similarity between each word in the question and each
word in the answer was then calculated and the sum of
the closest matches gave the overall similarity. In other
words, given two sets Q  and P, where
Q={qw1,qw2,?,qwn} and P={pw1,pw2,?,pwm}, the
similarity between Q  and P  is given by
1<p<n Argmaxm similarity( qwp, pwm)

The function similarity( w1, w2) maps the stems of
the two words w1 and w2  to a similarity measure m
representing how semantically related the two words
are; similarity( wi, wj)< similarity( wi, wk) represents the
fact that the word wj is less semantically related than wk
in respect to the word wi. In particular similarity=0 if
two words are not at all semantically related and
similarity=1 if the words are the same.

similarity( w1, w2) = h  ?
 


where 0 ?  h  ?  1. In particular, similarity( w1, w2) = 0 if
w1?ST ? w2?ST, where ST is a set containing a number
of stop-words (e.g. ?the?, ?a?, ?to?) which are too
common to be able to be usefully employed to estimate
semantic similarity. In all other cases,  h  is calculated as
follows: the words w1 and w2  are compared using all the
available WordNet relationships (is-a, satellite, similar,
pertains, meronym, entails, etc.), with the additional
relationship, ?same-as?, which indicated that two words
were identical. Each relationship is given a weighting
indicating how related two words are, with a ?same as?
relationship indicating the closest relationship, followed
by synonym relationships, hypernym, hyponym, then
satellite, meronym, pertains, entails.
So, for example, given the question ?Who went to
the mountains yesterday?? and the second question ?Did
Fred walk to the big mountain and then to mount
Pleasant??, Q  would be the set {who, go, to, the,
mountain, yesterday} and P  would be the set {Did,
Fred, walk, to, the, big, mountain, and, then, to, mount,
Pleasant}.
In order to calculate similarity the algorithm would
consider each word in turn. ?Who? would be ignored as
it is a common word and hence part of the list of stop-
words. ?Go? would be related to ?walk? in a is-a
relationship and receive a score h1. ?To? and ?the?
would be found in the list of stop-words and ignored.
?Mountain? would be considered most similar to
?mountain? (same-as relationship) and receive a score
h2: ?mount? would be in a synonym relationship with
?mountain? and give a lower score, so it is ignored.
?Yesterday? would receive a score of 0 as there are no
semantically related words in Q. The similarity measure
of Q  in respect to P  would therefore be given by h1  + h2.
In order to improve performance of the similarity
measure, additional information was considered in
addition to simple word matching (see De Boni and
Manandhar 2003 for a complete discussion):
? Compound noun information.  The motivation
behind is similar to the reason for using chunking
information, i.e. the fact that the word ?United? in
?United States?  should not be considered similar to
?United? as in ?Manchester United?. As opposed to
when using chunking information, however, when
using noun compound information, the compound
is considered a single word, as opposed to a group
of words: chunking and compound noun
information may therefore be combined as in ?[the
[United States] official team]?.
? Proper noun information. The intuition behind this
is that titles (of books, films, etc.) should not be
confused with the ?normal? use of the same words:
?blue lagoon? as in the sentence ?the film Blue
Lagoon was rather strange? should not be
considered as similar to the same words in the
sentence ?they swan in the blue lagoon? as they are
to the sentence ?I enjoyed Blue Lagoon when I was
younger?.
? Word frequency information.  This is a step beyond
the use of stop-words, following the intuition that
the more a word is common the less it is useful in
determining similarity between sentence. So, given
the sentences ?metatheoretical reasoning is
common in philosophy? and ?metatheoretical
arguments are common in philosophy?, the word
?metatheoretical? should be considered more
important in determining relevance than the words
?common?, ?philosophy? and ?is? as it is much
more rare and therefore less probably found in
irrelevant sentences. Word frequency data was
taken from the Given that the questions examined
were generic queries which did not necessarily refer
to a specific set of documents, the word frequency
for individual words was taken to be the word
frequency given in the British National Corpus (see
BNCFreq 2003). The top 100 words, making up
43% of the English Language, were then used as
stop-words and were not used in calculating
semantic similarity.

7 Results
An implementation of the algorithm was evaluated
on the TREC context questions used to develop the
algorithm and then on the collection of 500 new
clarification dialogue questions. The results on the
TREC data, which was used to develop the algorithm,
were as follows (see below for discussion and an
explanation of each method):

TREC Meth.0 Meth.1 Meth.2 Meth.3aMeth.3b
New  90 90 90 60 80
Clarif. 47 53 59 78 72
Where ?New? indicates the ability to recognize
whether the current question is the first in a new series
of clarification questions and ?Clarif.? (for
?Clarification?) indicates the ability to recognize
whether the current question is a clarification question.
The results for the same experiments conducted on
the collected data were as follows:

Collected Meth.0 Meth.1 Meth.2 Meth.3a Meth.3b
New  100 100 100 67 83
Clarif. 64 62 66 91 89

Method 0. This method did not use any linguistic
information and simply took a question to be a
clarification question if it had any words in common
with the previous n questions, else took the question to
be the beginning of a new series. 64% of questions in
the new collection could be recognized with this simple
algorithm, which did not misclassify any "new"
questions.
Method 1. This method employed point 1 of the
algorithm described in section 5: 62% of questions in
the new collection could be recognized as clarification
questions simply by looking for "reference" keywords
such as he, she, this, so, etc. which clearly referred to
previous questions. Interestingly this did not misclassify
any "new" questions.
Method 2. This method employed points 1 and 2 of
the algorithm described in section 5: 5% of questions in
the new collection could be recognized simply by
looking for the absence of verbs, which, combined with
keyword lookup (Method 1), improved performance to
66%. Again this did not misclassify any "new"
questions.
Method 3a. This method employed the full
algorithm described in section 5 (point 3 is the
similarity measure algorithm described in section 6):
clarification recognition rose to 91% of the new
collection by looking at the similarity between nouns in
the current question and nouns in the previous
questions, in addition to reference words and the
absence of verbs. Misclassification was a serious
problem, however with correctly classified "new"
questions falling to 67%.
Method 3b. This was the same as method 3a, but
specified a similarity threshold when employing the
similarity measure described in section 6: this required
the nouns in the current question to be similar to nouns
in the previous question beyond a specified similarity
threshold. This brought clarification question
recognition down to 89% of the new collection, but
misclassification of "new" questions was reduced
significantly, with "new" questions being correctly
classified 83% of the time.
Problems noted were:
? False positives: questions following a similar but
unrelated question series. E.g. "Are they all Muslim
countries?" (talking about religion, but in the
context of a general conversation about Saudi
Arabia) followed by "What is the chief religion in
Peru?" (also about religion, but in a totally
unrelated context).
? Questions referring to answers, not previous
questions (e.g. clarifying the meaning of a word
contained in the answer, or building upon a concept
defined in the answer: e.g. "What did Antonio
Carlos Tobim play?" following "Which famous
musicians did he play with?" in the context of a
series of questions about Fank Sinatra: Antonio
Carlos Tobim was referred to in the answer to the
previous question, and nowhere else in the
exchange. These made up 3% of the missed
clarifications.
? Absence of relationships in WordNet, e.g. between
"NASDAQ" and "index" (as in share index).
Absence of verb-noun relationships in WordNet,
e.g. between to die and death, between "battle" and
"win" (i.e. after a battle one side generally wins and
another side loses), "airport" and "visit" (i.e. people
who are visiting another country use an airport to
get there)

As can be seen from the tables above, the same
experiments conducted on the TREC context questions
yielded worse results; it was difficult to say, however,
whether this was due to the small size of the TREC data
or the nature of the data itself, which perhaps did not
fully reflect ?real? dialogues.
 As regards the recognition of question in a series
(the recognition that a clarification I taking place), the
number of sentences recognized by keyword alone was
smaller in the TREC data (53% compared to 62%),
while the number of questions not containing verbs was
roughly similar (about 6%). The improvement given by
computing noun similarity between successive
questions gave worse results on the TREC data: using
method 3a resulted in an improvement to the overall
correctness of 19 percentage points, or a 32% increase
(compared to an improvement of 25 percentage points,
or a 38% increase on the collected data); using method
3b resulted in an improvement of 13 percentage points,
or a 22% increase (compared to an improvement of 23
percentage points or a 35% increase on the collected
data), perhaps indicating that in "real" conversation
speakers tend to use simpler semantic relationships than
what was observed in the TREC data.
8 Usefulness of Clarification Dialogue
Recognition
Recognizing that a clarification dialogue is occurring
only makes sense if this information can then be used to
improve answer retrieval performance.
We therefore hypothesized that noting that a
questioner is trying to clarify previously asked questions
is important in order to determine the context in which
an answer is to be sought: in other words, the answers to
certain questions are constrained by the context in
which they have been uttered. The question ?What does
attenuate mean??, for example, may require a generic
answer outlining all the possible meanings of
?attenuate? if asked in isolation, or a particular meaning
if asked after the word has been seen in an answer (i.e.
in a definite context which constrains its meaning).  In
other cases, questions do not make sense at all out of a
context. For example, no answer could be given to the
question ?where?? asked on its own, while following a
question such as ?Does Sean have a house anywhere
apart from Scotland?? it becomes an easily intelligible
query.
The usual way in which Question Answering
systems constrain possible answers is by restricting the
number of documents in which an answer is sought by
filtering the total number of available documents
through the use of an information retrieval engine. The
information retrieval engine selects a subset of the
available documents based on a number of keywords
derived from the question at hand. In the simplest case,
it is necessary to note that some words in the current
question refer to words in previous questions or answers
and hence use these other words when formulating the
IR query. For example, the question ?Is he married??
cannot be used as is  in order to select documents, as the
only word passed to the IR engine would be ?married?
(possibly the root version ?marry?) which would return
too many documents to be of any use. Noting that the
?he? refers to a previously mentioned person (e.g. ?Sean
Connery?) would enable the answerer to seek an answer
in a smaller number of documents. Moreover, given that
the current question is asked in the context of a previous
question, the documents retrieved for the previous
related question could provide a context in which to
initially seek an answer.
In order to verify the usefulness of constraining the
set of documents from in which to seek an answer, a
subset made of 15 clarification dialogues (about 100
questions) from the given question data was analyzed by
taking the initial question for a series, submitting it to
the Google Internet Search Engine and then manually
checking to see how many of the questions in the series
could be answered simply by using the first 20
documents retrieved for the first question in a series.
The results are summarized in the following diagram
(Fig. 1):

Fig. 1: Search technique used for Question
First Q in series
Words in Q
Coreference
Mini-clarification
Other

? 69% of clarification questions could be answered
by looking within the documents used for the
previous question in the series, thus indicating the
usefulness of noting the occurrence of clarification
dialogue.
? The remaining 31% could not be answered by
making reference to the previously retrieved
documents, and to find an answer a different
approach had to be taken. In particular:
? 6% could be answered after retrieving documents
simply by using the words in the question as search
terms (e.g. ?What caused the boxer uprising??);
? 14% required some form of coreference resolution
and could be answered only by combining the
words in the question with the words to which the
relative pronouns in the question referred (e.g.
?What film is he working on at the moment?, with
the reference to ?he? resolved, which gets passed to
the search engine as ?What film is Sean Connery
working on at the moment??);
? 7% required more than 20 documents to be
retrieved by the search engine or other, more
complex techniques. An example is a question such
as ?Where exactly?? which requires both an
understanding of the context in which the question
is asked (?Where?? makes no sense on its own) and
the previously given answer (which was probably a
place, but not restrictive enough for the questioner).
? 4% constituted mini-clarification dialogues within a
larger clarification dialogue (a slight deviation from
the main topic which was being investigated by the
questioner) and could be answered by looking at
the documents retrieved for the first question in the
mini-series.

Recognizing that a clarification dialogue is
occurring therefore can simplify the task of retrieving an
answer by specifying that an answer must be in the set
of documents used the previous questions. This is
consistent with the results found in the TREC context
task (Voorhees 2002), which indicated that systems
were capable of finding most answers to questions in a
context dialogue simply by looking at the documents
retrieved for the initial question in a series. As in the
case of clarification dialogue recognition, therefore,
simple techniques can resolve the majority of cases;
nevertheless, a full solution to the problem requires
more complex methods. The last case indicates that it is
not enough simply to look at the documents provided by
the first question in a series in order to seek an answer:
it is necessary to use the documents found for a
previously asked question which is related to the current
question (i.e. the questioner could "jump" between
topics). For example, given the following series of
questions starting with Q1:

Q1: When was the Hellenistic Age?
[?]
Q5: How did Alexander the great become ruler?
Q6: Did he conquer anywhere else?
Q7: What was the Greek religion in the Hellenistic Age?

where Q6  should be related to Q5  but Q7  should be
related to Q1, and not Q6. In this case, given that the
subject matter of Q1  is more immediately related to the
subject matter of Q7  than Q6  (although the subject
matter of Q6  is still broadly related, it is more of a
specialized subtopic), the documents retrieved for Q1
will probably be more relevant to Q7  than the
documents retrieved for Q6 (which would probably be
the same documents retrieved for Q5)

9 Conclusion
It has been shown that recognizing that a clarification
dialogue is occurring can simplify the task of retrieving
an answer by constraining the subset of documents in
which an answer is to be found. An algorithm was
presented to recognize the occurrence of clarification
dialogue and is shown to have a good performance. The
major limitation of our algorithm is the fact that it only
considers series of questions, not series of answers. As
noted above, it is often necessary to look at an answer to
a question to determine whether the current question is a
clarification question or not. Our sentence similarity
algorithm was limited by the number of semantic
relationships in WordNet: for example, a big
improvement would come from the use of noun-verb
relationships. Future work will be directed on extending
WordNet in this direction and in providing other useful
semantic relationships. Work also needs to be done on
using information given by answers, not just questions
in recognizing clarification dialogue and on coping with
the cases in which clarification dialogue recognition is
not enough to retrieve an answer and where other, more
complex, techniques need to be used. It would also be
beneficial to examine the use of a similarity function in
which similarity decayed in function of the distance in
time between the current question and the past
questions.
References
Ardissono, L. and Sestero, D. 1996. "Using dynamic
user models in the recognition of the plans of the
user". User Modeling and User-Adapted Interaction,
5(2):157-190.
BNCFreq. 2003. English Word Frequency List.
http://www.eecs.umich.edu/~qstout/586/bncfreq.html
(last accessed March 2003).
Budanitsky, A., and Hirst, G. 2001. ?Semantic distance
in WordNet: and experimental, application-oriented
evaluation of five measures?, in Proceedings of the
NAACL 2001 Workshop on WordNet and other
lexical resources, Pittsburgh.
De Boni, M. and Manandhar, S. 2003. ?The Use of
Sentence Similarity as a Semantic Relevance Metric
for Question Answering?. Proceedings of the AAAI
Symposium on New Directions in Question
Answering, Stanford.
De Boni, M. and Manandhar, S. 2002. ?Automated
Discovery of Telic Relations for WordNet?.
Proceedings of the First International WordNet
Conference, India.
Fellbaum, C. 1998. WordNet, An electronic Lexical
Database, MIT Press.
Ginzburg , J. 1998. "Clarifying Utterances" In: J.
Hulstijn and A. Nijholt (eds.) Proceedings of the 2nd
Workshop on the Formal Semantics and Pragmatics
of Dialogue, Twente.
Ginzburg and Sag, 2000. Interrogative Investigations,
CSLI.
Green, S. J. 1997. Automatically generating hypertext
by computing semantic similarity, Technical Report
n. 366, University of Toronto.
Harabagiu, S., Miller, A. G., Moldovan, D. 1999.
?WordNet2 - a morphologically and semantically
enhanced resource?, In Proceedings of SIGLEX-99,
University of Maryland.
Harabagiu, S., et al 2002. ?Answering Complex, List
and Context Questions with LCC?s Question-
Answering Server?, Proceedings of TREC-10, NIST.
Hirst, G., and St-Onge, D. 1998. ?Lexical chains as
representations of context for the detection and
correction of malapropisms?, in Fellbaum (ed.),
WordNet: and electronic lexical database, MIT
Press.
Jiang, J. J., and Conrath, D. W. 1997. ?Semantic
similarity based on corpus statistics and lexical
taxonomy?, in Proceedings of ICRCL, Taiwan.
Lee, G. G., et al 2002. ?SiteQ: Engineering High
Performance QA System Using Lexico-Semantic
Pattern Matching and Shallow NLP?, Proceedings of
TREC-10, NIST.
Lin, D. 1998. ?An information-theoretic definition of
similarity?, in Proceedings of the 15th International
Conference on Machine Learning, Madison.
Mihalcea, R. and Moldovan, D. 1999. ?A Method for
Word Sense Disambiguation of Unrestricted Text?,
in Proceedings of ACL ?99, Maryland, NY.
Miller, G. A. 1999. ?WordNet: A Lexical Database?,
Communications of the ACM, 38 (11).
Moldovan, D. and Rus, V. 2001. ?Logic Form
Transformation of WordNet and its Applicability to
Question Answering?, Proceedings of the 39th
conference of ACL, Toulouse.
Resnik, P. 1995. ?Using information content to evaluate
semantic similarity?, in Proceedings of the 14th
IJCAI, Montreal.
Soubbotin, M. M. 2002. :?Patterns of Potential Answer
Expressions as Clues to the Right Answers?,
Proceedings of TREC-10, NIST.
van Beek, P., Cohen, R. and Schmidt, K., 1993. ?From
plan critiquing to clarification dialogue for
cooperative response generation?, Computational
Intelligence  9:132-154.
Voorhees, E. 2002. ?Overview of the TREC 2001
Question Answering Track?, Proceedings of TREC-
10, NIST.
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 792?799,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Learning to Compose Effective Strategies from a Library of
Dialogue Components
Martijn Spitters? Marco De Boni? Jakub Zavrel? Remko Bonnema?
? Textkernel BV, Nieuwendammerkade 28/a17, 1022 AB Amsterdam, NL
{spitters,zavrel,bonnema}@textkernel.nl
? Unilever Corporate Research, Colworth House, Sharnbrook, Bedford, UK MK44 1LQ
marco.de-boni@unilever.com
Abstract
This paper describes a method for automat-
ically learning effective dialogue strategies,
generated from a library of dialogue content,
using reinforcement learning from user feed-
back. This library includes greetings, so-
cial dialogue, chit-chat, jokes and relation-
ship building, as well as the more usual clar-
ification and verification components of dia-
logue. We tested the method through a mo-
tivational dialogue system that encourages
take-up of exercise and show that it can be
used to construct good dialogue strategies
with little effort.
1 Introduction
Interactions between humans and machines have be-
come quite common in our daily life. Many ser-
vices that used to be performed by humans have
been automated by natural language dialogue sys-
tems, including information seeking functions, as
in timetable or banking applications, but also more
complex areas such as tutoring, health coaching and
sales where communication is much richer, embed-
ding the provision and gathering of information in
e.g. social dialogue. In the latter category of dia-
logue systems, a high level of naturalness of interac-
tion and the occurrence of longer periods of satisfac-
tory engagement with the system are a prerequisite
for task completion and user satisfaction.
Typically, such systems are based on a dialogue
strategy that is manually designed by an expert
based on knowledge of the system and the domain,
and on continuous experimentation with test users.
In this process, the expert has to make many de-
sign choices which influence task completion and
user satisfaction in a manner which is hard to assess,
because the effectiveness of a strategy depends on
many different factors, such as classification/ASR
performance, the dialogue domain and task, and,
perhaps most importantly, personality characteris-
tics and knowledge of the user.
We believe that the key to maximum dialogue ef-
fectiveness is to listen to the user. This paper de-
scribes the development of an adaptive dialogue sys-
tem that uses the feedback of users to automatically
improve its strategy. The system starts with a library
of generic and task-/domain-specific dialogue com-
ponents, including social dialogue, chit-chat, enter-
taining parts, profiling questions, and informative
and diagnostic parts. Given this variety of possi-
ble dialogue actions, the system can follow many
different strategies within the dialogue state space.
We conducted training sessions in which users inter-
acted with a version of the system which randomly
generates a possible dialogue strategy for each in-
teraction (restricted by global dialogue constraints).
After each interaction, the users were asked to re-
ward different aspects of the conversation. We ap-
plied reinforcement learning to use this feedback to
compute the optimal dialogue policy.
The following section provides a brief overview
of previous research related to this area and how our
work differs from these studies. We then proceed
with a concise description of the dialogue system
used for our experiments in section 3. Section 4
is about the training process and the reward model.
Section 5 goes into detail about dialogue policy op-
792
timization with reinforcement learning. In section 6
we discuss our experimental results.
2 Related Work
Previous work has examined learning of effective
dialogue strategies for information seeking spo-
ken dialogue systems, and in particular the use of
reinforcement learning methods to learn policies
for action selection in dialogue management (see
e.g. Levin et al, 2000; Walker, 2000; Scheffler and
Young, 2002; Peek and Chickering, 2005; Frampton
and Lemon, 2006), for selecting initiative and con-
firmation strategies (Singh et al, 2002); for detect-
ing speech recognition problem (Litman and Pan,
2002); changing the dialogue according to the ex-
pertise of the user (Maloor and Chai, 2000); adapt-
ing responses according to previous interactions
with the users (Rudary et al, 2004); optimizing
mixed initiative in collaborative dialogue (English
and Heeman, 2005), and optimizing confirmations
(Cuaya?huitl et al, 2006). Other researchers have
focussed their attention on the learning aspect of
the task, examining, for example hybrid reinforce-
ment/supervised learning (Henderson et al, 2005).
Previous work on learning dialogue management
strategies has however generally been limited to well
defined areas of the dialogue, in particular dealing
with speech recognition and clarification problems,
with small state spaces and a limited set of actions
to choose from (Henderson et al, 2005). In a num-
ber of contexts, however, dialogues need to have a
far greater degree of complexity not just in the num-
ber of states and possible actions but also in the va-
riety of dialogue acts: for example in motivational
dialogue systems where the task is not limited to
information gathering, slot-filling or querying of a
database, and where dialogues must contain more
social and relational elements to be successful (for
the usefulness of social dialogue see e.g. Bickmore,
2003; Liu and Picard, 2005). Only little effort has
been directed to the question what dialogue compo-
nents should make up the dialogue, involving deci-
sions like how much and what type of social interac-
tion should be used, different ways of forming a re-
lationship with the user such as using chit-chat (for
example asking about a user?s hobbies or asking for
the user?s name), using humour, as well as the more
conventional tasks of clarifying user input, estab-
lishing common ground and ensuring system replies
are appropriate. Our work has focused on these as-
pects of dialogue strategy construction, in order to
create good dialogue strategies incorporating appro-
priate levels of social interaction, humour, chit-chat,
as well as successful information gathering and pro-
vision.
3 A Motivational Dialogue System
The domain of our system is physical exercise. The
system is set up as an exercise advisor that asks
the user what is preventing him/her from exercis-
ing more. After the user has worded his/her exercise
?barrier?, the system will give motivational advice
for how to overcome this barrier. As an illustration,
Table 1 shows an example dialogue, generated by
our system. Our dialogue system is text-based, so
it does not involve speech recognition. This section
describes the system?s three main modules: the di-
alogue state/action manager, the utterance classifier
and the dialogue component library.
Speaker Utterance
system Good evening!
system What?s your name?
user my name is Brenda
system So Brenda, how are you today?
user fine thank you
system Tell me what is preventing you from exercising
more.
user I don?t have anyone to exercise with...
system So your barrier is Loneliness (e.g. You don?t
like exercising on your own ).
user sort of, yeah
system A solution for this barrier is: Try going on your
own and see whether it?s really that different to
going with a friend. Try talking to other people
who are exercising, you?re likely to find they
welcome a little conversation!
system Did you find this solution useful?
user I guess...
system Ok. Try to work on this to overcome your
barrier.
system Bye!
Table 1: Example dialogue.
3.1 Dialogue state/action management
The dialogue state is unique at every stage of
the conversation and is represented as a vector of
feature-values. We use only a limited set of fea-
tures because, as also noted in (Singh et al, 2002;
Levin et al, 2000), it is important to keep the state
space as small as possible (but with enough distinc-
793
tive power to support learning) so we can construct
a non-sparse Markov decision process (see section
5) based on our limited training dialogues. The state
features are listed in Table 2.
Feature Values Description
curnode c ? N the current dialogue node
actiontype utt, trans action type
trigger t ? T utterance classifier category
confidence 1, 0 category confidence
problem 1, 0 communication problem earlier
Table 2: Dialogue state features.
In each dialogue state, the dialogue manager will
look up the next action that should be taken. In our
system, an action is either a system utterance or a
transition in the dialogue structure. In the initial
system, the dialogue structure was manually con-
structed. In many states, the next action requires
a choice to be made. Dialogue states in which the
system can choose among several possible actions
are called choice-states. For example, in our sys-
tem, immediately after greeting the user, the dia-
logue structure allows for different directions: the
system can first ask some personal questions, or
it can immediately discuss the main topic without
any digressions. Utterance actions may also re-
quire a choice (e.g. directive versus open formula-
tion of a question). In training mode, the system will
make random choices in the choice-states. This ap-
proach will generate many different dialogue strate-
gies, i.e. paths through the dialogue structure.
User replies are sent to an utterance classifier. The
category assigned by this classifier is returned to
the dialogue manager and triggers a transition to the
next node in the dialogue structure. The system also
accommodates a simple rule-based extraction mod-
ule, which can be used to extract information from
user utterances (e.g. the user?s name, which is tem-
plated in subsequent system prompts in order to per-
sonalize the dialogue).
3.2 Utterance classification
The (memory-based) classifier uses a rich set of fea-
tures for accurate classification, including words,
phrases, regular expressions, domain-specific word-
relations (using a taxonomy-plugin) and syntacti-
cally motivated expressions. For utterance pars-
ing we used a memory-based shallow parser, called
MBSP (Daelemans et al, 1999). This parser pro-
vides part of speech labels, chunk brackets, subject-
verb-object relations, and has been enriched with de-
tection of negation scope and clause boundaries.
The feature-matching mechanism in our classifi-
cation system can match terms or phrases at speci-
fied positions in the token stream of the utterance,
also in combination with syntactic and semantic
class labels. This allows us to define features that are
particularly useful for resolving confusing linguis-
tic phenomena like ambiguity and negation. A base
feature set was generated automatically, but quite
a lot of features were manually tuned or added to
cope with certain common dialogue situations. The
overall classification accuracy, measured on the dia-
logues that were produced during the training phase,
is 93.6%. Average precision/recall is 98.6/97.3% for
the non-barrier categories (confirmation, negation,
unwillingness, etc.), and 99.1/83.4% for the barrier
categories (injury, lack of motivation, etc.).
3.3 Dialogue Component Library
The dialogue component library contains generic
as well as task-/domain-specific dialogue content,
combining different aspects of dialogue (task/topic
structure, communication goals, etc.). Table 3 lists
all components in the library that was used for train-
ing our dialogue system. A dialogue component is
basically a coherent set of dialogue node represen-
tations with a certain dialogue function. The library
is set up in a flexible, generic way: new components
can easily be plugged in to test their usefulness in
different dialogue contexts or for new domains.
4 Training the Dialogue System
4.1 Random strategy generation
In its training mode, the dialogue system uses ran-
dom exploration: it generates different dialogue
strategies by choosing randomly among the allowed
actions in the choice-states. Note that dialogue gen-
eration is constrained to contain certain fixed actions
that are essential for task completion (e.g. asking the
exercise barrier, giving a solution, closing the ses-
sion). This excludes a vast number of useless strate-
gies from exploration by the system. Still, given all
action choices and possible user reactions, the total
number of unique dialogues that can be generated by
794
Component Description pa pe
StartSession Dialogue openings, including various greetings ? ?
PersonalQuestionnaire Personal questions, e.g. name; age; hobbies; interests, how are you today? ?
ElizaChitChat Eliza-like chit-chat, e.g. please go on...
ExerciseChitChat Chit-chat about exercise, e.g. have you been doing any exercise this week? ?
Barrier Prompts concerning the barrier, e.g. ask the barrier; barrier verification; ask a rephrase ? ?
Solution Prompts concerning the solution, e.g. give the solution; verify usefulness ? ?
GiveBenefits Talk about the benefits of exercising
AskCommitment Ask user to commit his implementation of the given solution ?
Encourage Encourage the user to work on the given solution ? ?
GiveJoke The humor component: ask if the user wants to hear a joke; tell random jokes ? ?
VerifyCloseSession Verification for closing the session (are you sure you want to close this session?) ? ?
CloseSession Dialogue endings, including various farewells ? ?
Table 3: Components in the dialogue component library. The last two columns show which of the compo-
nents was used in the learned policy (pa) and the expert policy (pe), discussed in section 6. ? means the
component is always used, ? means it is sometimes used, depending on the dialogue state.
the system is approximately 345000 (many of which
are unlikely to ever occur). During training, the sys-
tem generated 490 different strategies. There are 71
choice-states that can actually occur in a dialogue.
In our training dialogues, the opening state was ob-
viously visited most frequently (572 times), almost
60% of all states was visited at least 50 times, and
only 16 states were visited less than 10 times.
4.2 The reward model
When the dialogue has reached its final state, a sur-
vey is presented to the user for dialogue evaluation.
The survey consists of five statements that can each
be rated on a five-point scale (indicating the user?s
level of agreement). The responses are mapped to
rewards of -2 to 2. The statements we used are partly
based on the user survey that was used in (Singh et
al., 2002). We considered these statements to reflect
the most important aspects of conversation that are
relevant for learning a good dialogue policy. The
five statements we used are listed below.
M1 Overall, this conversation went well
M2 The system understood what I said
M3 I knew what I could say at each point in the dialogue
M4 I found this conversation engaging
M5 The system provided useful advice
4.3 Training set-up
Eight subjects carried out a total of 572 conversa-
tions with the system. Because of the variety of pos-
sible exercise barriers known by the system (52 in
total) and the fact that some of these barriers are
more complex or harder to detect than others, the
system?s classification accuracy depends largely on
the user?s barrier. To prevent classification accuracy
distorting the user evaluations, we asked the subjects
to act as if they had one of five predefined exercise
barriers (e.g. Imagine that you don?t feel comfort-
able exercising in public. See what the advisor rec-
ommends for this barrier to your exercise).
5 Dialogue Policy Optimization with
Reinforcement Learning
Reinforcement learning refers to a class of machine
learning algorithms in which an agent explores an
environment and takes actions based on its current
state. In certain states, the environment provides
a reward. Reinforcement learning algorithms at-
tempt to find the optimal policy, i.e. the policy that
maximizes cumulative reward for the agent over the
course of the problem. In our case, a policy can be
seen as a mapping from the dialogue states to the
possible actions in those states. The environment is
typically formulated as a Markov decision process
(MDP).
The idea of using reinforcement learning to au-
tomate the design of strategies for dialogue systems
was first proposed by Levin et al (2000) and has
subsequently been applied in a.o. (Walker, 2000;
Singh et al, 2002; Frampton and Lemon, 2006;
Williams et al, 2005).
5.1 Markov decision processes
We follow past lines of research (such as Levin et
al., 2000; Singh et al, 2002) by representing a dia-
logue as a trajectory in the state space, determined
795
by the user responses and system actions: s1
a1,r1????
s2
a2,r2???? . . . sn
an,rn???? sn+1, in which si
ai,ri???? si+1
means that the system performed action ai in state
si, received1 reward ri and changed to state si+1.
In our system, a state is a dialogue context vector
of feature values. This feature vector contains the
available information about the dialogue so far that
is relevant for deciding what action to take next in
the current dialogue state. We want the system to
learn the optimal decisions, i.e. to choose the actions
that maximize the expected reward.
5.2 Q-value iteration
The field of reinforcement learning includes many
algorithms for finding the optimal policy in an MDP
(see Sutton and Barto, 1998). We applied the algo-
rithm of (Singh et al, 2002), as their experimental
set-up is similar to ours, constisting of: generation
of (limited) exploratory dialogue data, using a train-
ing system; creating an MDP from these data and
the rewards assigned by the training users; off-line
policy learning based on this MDP.
The Q-function for a certain action taken in a cer-
tain state describes the total reward expected be-
tween taking that action and the end of the dialogue.
For each state-action pair (s, a), we calculated this
expected cumulative reward Q(s, a) of taking action
a from state s, with the following equation (Sutton
and Barto, 1998; Singh et al, 2002):
Q(s, a) = R(s, a) + ?
?
s?
P (s?|s, a)max
a?
Q(s?, a?)
(1)
where: P (s?|s, a) is the probability of a transition
from state s to state s? by taking action a, and
R(s, a) is the expected reward obtained when tak-
ing action a in state s. ? is a weight (0 ? ? ? 1),
that discounts rewards obtained later in time when
it is set to a value < 1. In our system, ? was set
to 1. Equation 1 is recursive: the Q-value of a cer-
tain state is computed in terms of the Q-values of
its successor states. The Q-values can be estimated
to within a desired threshold using Q-value iteration
(Sutton and Barto, 1998). Once the value iteration
1In our experiments, we did not make use of immediate re-
warding (e.g. at every turn) during the conversation. Rewards
were given after the final state of the dialogue had been reached.
process is completed, by selecting the action with
the maximum Q-value (the maximum expected fu-
ture reward) at each choice-state, we can obtain the
optimal dialogue policy pi.
6 Results and Discussion
6.1 Reward analysis
Figure 1 shows a graph of the distribution of the five
different evaluation measures in the training data
(see section 4.2 for the statement wordings). M1
is probably the most important measure of success.
The distribution of this reward is quite symmetri-
cal, with a slightly higher peak in the positive area.
The distribution of M2 shows that M1 and M2 are
related. From the distribution of M4 we can con-
clude that the majority of dialogues during the train-
ing phase was not very engaging. Users obviously
had a good feeling about what they could say at each
point in the dialogue (M3), which implies good qual-
ity of the system prompts. The judgement about the
usefulness of the provided advice is pretty average,
tending a bit more to negative than to positive. We
do think that this measure might be distorted by the
fact that we asked the subjects to imagine that they
have the given exercise barriers. Furthermore, they
were sometimes confronted with advice that had al-
ready been presented to them in earlier conversa-
tions.
 0
 50
 100
 150
 200
 250
-2 -1  0  1  2
N
um
be
r o
f d
ia
lo
gu
es
Reward
Reward distributions
M1
M2
M3
M4
M5
Figure 1: Reward distributions in the training data.
In our analysis of the users? rewarding behavior,
we found several significant correlations. We found
that longer dialogues (> 3 user turns) are appreci-
ated more than short ones (< 4 user turns), which
seems rather logical, as dialogues in which the user
796
barely gets to say anything are neither natural nor
engaging.
We also looked at the relationship between user
input verification and the given rewards. Our intu-
ition is that the choice of barrier verification is one
of the most important choices the system can make
in the dialogue. We found that it is much better to
first verify the detected barrier than to immediately
give advice. The percentage of appropriate advice
provided in dialogues with barrier verification is sig-
nificantly higher than in dialogues without verifica-
tion.
In several states of the dialogue, we let the sys-
tem choose from different wordings of the system
prompt. One of these choices is whether to use an
open question to ask what the user?s barrier is (How
can I help you?), or a directive question (Tell me
what is preventing you from exercising more.). The
motivation behind the open question is that the user
gets the initiative and is basically free to talk about
anything he/she likes. Naturally, the advantage of
directive questions is that the chance of making clas-
sification errors is much lower than with open ques-
tions because the user will be better able to assess
what kind of answer the system expects. Dialogues
in which the key-question (asking the user?s barrier)
was directive, were rewarded more positively than
dialogues with the open question.
6.2 Learned dialogue policies
We learned a different policy for each evaluation
measure separately (by only using the rewards given
for that particular measure), and a policy based on
a combination (sum) of the rewards for all evalu-
ation measures. We found that the learned policy
based on the combination of all measures, and the
policy based on measure M1 alone (Overall, this
conversation went well) were nearly identical. Ta-
ble 4 compares the most important decisions of the
different policies. For convenience of comparison,
we only listed the main, structural choices. Table 3
shows which of the dialogue components in the li-
brary were used in the learned and the expert policy.
Note that, for the sake of clarity, the state descrip-
tions in Table 4 are basically summaries of a set of
more specific states since a state is a specific repre-
sentation of the dialogue context at a particular mo-
ment (composed of the values of the features listed
in Table 2). For instance, in the pa policy, the deci-
sion in the last row of the table (give a joke or not),
depends on whether or not there has been a classifi-
cation failure (i.e. a communication problem earlier
in the dialogue). If there has been a classification
failure, the policy prescribes the decision not to give
a joke, as it was not appreciated by the training users
in that context. Otherwise, if there were no commu-
nication problems during the conversation, the users
did appreciate a joke.
6.3 Evaluation
We compared the learned dialogue policy with a pol-
icy which was independently hand-designed by ex-
perts2 for this system. The decisions made in the
learned strategy were very similar to the ones made
by the experts, with only a few differences, indicat-
ing that the automated method would indeed per-
form as well as an expert. The main differences
were the inclusion of a personal questionnaire for re-
lation building at the beginning of the dialogue and
a commitment question at the end of the dialogue.
Another difference was the more restricted use of
the humour element, described in section 6.2 which
turns out to be intuitively better than the expert?s de-
cision to simply always include a joke. Of course,
we can only draw conclusions with regard to the ef-
fectiveness of these two policies if we empirically
compare them with real test users. Such evaluations
are planned as part of our future research.
As some additional evidence against the possibil-
ity that the learned policy was generated by chance,
we performed a simple experiment in which we took
several random samples of 300 training dialogues
from the complete training set. For each sample, we
learned the optimal policy. We mutually compared
these policies and found that they were very similar:
only in 15-20% of the states, the policies disagreed
on which action to take next. On closer inspection
we found that this disagreement mainly concerned
states that were poorly visited (1-10 times) in these
samples. These results suggest that the learned pol-
icy is unreliable at infrequently visited states. Note
however, that all main decisions listed in Table 4 are
2The experts were a team made up of psychologists with
experience in the psychology of health behaviour change and
a scientist with experience in the design of automated dialogue
systems.
797
State description Action choices p1 p2 p3 p4 p5 pa pe
After greeting the user - ask the exercise barrier ? ? ?
- ask personal information ? ? ? ?
- chit-chat about exercise
When asking the barrier - use a directive question ? ? ? ? ? ? ?
- use an open question
User gives exercise barrier - verify detected barrier ? ? ? ? ? ? ?
- give solution
User rephrased barrier - verify detected barrier ? ? ? ? ? ?
- give solution ?
Before presenting solution - ask if the user wants to see a solution for the barrier ?
- give a solution ? ? ? ? ? ?
After presenting solution - verify solution usefulness ? ? ? ? ? ?
- encourage the user to work on the given solution ?
- ask user to commit solution implementation
User found solution useful - encourage the user to work on the solution ? ? ? ?
- ask user to commit solution implementation ? ? ?
User found solution not useful - give another solution ? ? ? ? ? ? ?
- ask the user wants to propose his own solution
After giving second solution - verify solution usefulness ? ?
- encourage the user to work on the given solution ? ? ? ?
- ask user to commit solution implementation ?
End of dialogue - close the session ? ? ?
- ask if the user wants to hear a joke ? ? ? ?
Table 4: Comparison of the most important decisions made by the learned policies. pn is the policy based
on evaluation measure n; pa is the policy based on all measures; pe contains the decisions made by experts
in the manually designed policy.
made at frequently visited states. The only disagree-
ment in frequently visited states concerned system-
prompt choices. We might conclude that these par-
ticular (often very subtle) system-prompt choices
(e.g. careful versus direct formulation of the exercise
barrier) are harder to learn than the more noticable
dialogue structure-related choices.
7 Conclusions and Future Work
We have explored reinforcement learning for auto-
matic dialogue policy optimization in a question-
based motivational dialogue system. Our system can
automatically compose a dialogue strategy from a li-
brary of dialogue components, that is very similar
to a manually designed expert strategy, by learning
from user feedback.
Thus, in order to build a new dialogue system,
dialogue system engineers will have to set up a
rough dialogue template containing several ?multi-
ple choice?-action nodes. At these nodes, various
dialogue components or prompt wordings (e.g. en-
tertaining parts, clarification questions, social dia-
logue, personal questions) from an existing or self-
made library can be plugged in without knowing be-
forehand which of them would be most effective.
The automatically generated dialogue policy is
very similar (see Table 4) ?but arguably improved in
many details? to the hand-designed policy for this
system. Automatically learning dialogue policies
also allows us to test a number of interesting issues
in parallel, for example, we have learned that users
appreciated dialogues that were longer, starting with
some personal questions (e.g What is your name?,
What are your hobbies?). We think that altogether,
this relation building component gave the dialogue
a more natural and engaging character, although it
was left out in the expert strategy.
We think that the methodology described in this
paper may be able to yield more effective dialogue
policies than experts. Especially in complicated di-
alogue systems with large state spaces. In our sys-
tem, state representations are composed of multiple
context feature values (e.g. communication problem
earlier in the dialogue, the confidence of the utter-
ance classifier). Our experiments showed that some-
times different decisions were learned in dialogue
contexts where only one of these features was differ-
ent (for example use humour only if the system has
been successful in recognising a user?s exercise bar-
rier): all context features are implicitly used to learn
the optimal decisions and when hand-designing a di-
798
alogue policy, experts can impossibly take into ac-
count all possible different dialogue contexts.
With respect to future work, we plan to examine
the impact of different state representations. We did
not yet empirically compare the effects of each fea-
ture on policy learning or experiment with other fea-
tures than the ones listed in Table 2. As Tetreault and
Litman (2006) show, incorporating more or different
information into the state representation might how-
ever result in different policies.
Furthermore, we will evaluate the actual generic-
ity of our approach by applying it to different do-
mains. As part of that, we will look at automatically
mining libraries of dialogue components from ex-
isting dialogue transcript data (e.g. available scripts
or transcripts of films, tv series and interviews con-
taining real-life examples of different types of dia-
logue). These components can then be plugged into
our current adaptive system in order to discover what
works best in dialogue for new domains. We should
note here that extending the system?s dialogue com-
ponent library will automatically increase the state
space and thus policy generation and optimization
will become more difficult and require more train-
ing data. It will therefore be very important to care-
fully control the size of the state space and the global
structure of the dialogue.
Acknowledgements
The authors would like to thank Piroska Lendvai
Rudenko, Walter Daelemans, and Bob Hurling for
their contributions and helpful comments. We also
thank the anonymous reviewers for their useful com-
ments on the initial version of this paper.
References
Timothy W. Bickmore. 2003. Relational Agents: Ef-
fecting Change through Human-Computer Relationships.
Ph.D. Thesis, MIT, Cambridge, MA.
Heriberto Cuaya?huitl, Steve Renals, Oliver Lemon, and Hiroshi
Shimodaira. 2006. Learning multi-goal dialogue strate-
gies using reinforcement learning with reduced state-action
spaces. Proceedings of Interspeech-ICSLP.
Walter Daelemans, Sabine Buchholz, and Jorn Veenstra. 1999.
Memory-Based Shallow Parsing. Proceedings of CoNLL-
99, Bergen, Norway.
Michael S. English and Peter A. Heeman 2005. Learn-
ing Mixed Initiative Dialog Strategies By Using Reinforce-
ment Learning On Both Conversants. Proceedings of
HLT/NAACL.
Matthew Frampton and Oliver Lemon. 2006. Learning More
Effective Dialogue Strategies Using Limited Dialogue Move
Features. Proceedings of the Annual Meeting of the ACL.
James Henderson, Oliver Lemon, and Kallirroi Georgila. 2005.
Hybrid Reinforcement/Supervised Learning for Dialogue
Policies from COMMUNICATOR Data. IJCAI workshop on
Knowledge and Reasoning in Practical Dialogue Systems.
Esther Levin, Roberto Pieraccini, and Wieland Eckert. 2000. A
Stochastic Model of Human-Machine Interaction for Learn-
ing Dialog Strategies. IEEE Trans. on Speech and Audio
Processing, Vol. 8, No. 1, pp. 11-23.
Diane J. Litman and Shimei Pan. 2002. Designing and Eval-
uating an Adaptive Spoken Dialogue System. User Model-
ing and User-Adapted Interaction, Volume 12, Issue 2-3, pp.
111-137.
Karen K. Liu and Rosalind W. Picard. 2005. Embedded Em-
pathy in Continuous, Interactive Health Assessment. CHI
Workshop on HCI Challenges in Health Assessment, Port-
land, Oregon.
Preetam Maloor and Joyce Chai. 2000. Dynamic User Level
and Utility Measurement for Adaptive Dialog in a Help-
Desk System. Proceedings of the 1st Sigdial Workshop.
Tim Paek and David M. Chickering. 2005. The Markov As-
sumption in Spoken Dialogue Management. Proceedings of
SIGDIAL 2005.
Matthew Rudary, Satinder Singh, and Martha E. Pollack.
2004. Adaptive cognitive orthotics: Combining reinforce-
ment learning and constraint-based temporal reasoning. Pro-
ceedings of the 21st International Conference on Machine
Learning.
Konrad Scheffler and Steve Young. 2002. Automatic learning
of dialogue strategy using dialogue simulation and reinforce-
ment learning. Proceedings of HLT-2002.
Satinder Singh, Diane Litman, Michael Kearns, and Marilyn
Walker. 2002. Optimizing Dialogue Management with Re-
inforcement Learning: Experiments with the NJFun System.
Journal of Artificial Intelligence Research (JAIR), Volume
16, pages 105-133.
Richard S. Sutton and Andrew G. Barto. 1998. Reinforcement
Learning. MIT Press.
Joel R. Tetreault and Diane J. Litman 2006. Comparing
the Utility of State Features in Spoken Dialogue Using Re-
inforcement Learning. Proceedings of HLT/NAACL, New
York.
Marilyn A. Walker 2000. An Application of Reinforcement
Learning to Dialogue Strategy Selection in a Spoken Dia-
logue System for Email. Journal of Artificial Intelligence
Research, Vol 12., pp. 387-416.
Jason D. Williams, Pascal Poupart, and Steve Young. 2005.
Partially Observable Markov Decision Processes with Con-
tinuous Observations for Dialogue Management. Proceed-
ings of the 6th SigDial Workshop, September 2005, Lisbon.
799
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 138?147,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Argumentative Human Computer Dialogue for Automated Persuasion
Pierre Andrews* and Suresh Manandhar* and Marco De Boni**
* Department of Computer Science
University of York
York YO10 5DD
UK
{pandrews,suresh}@cs.york.ac.uk
** Unilever Corporate Research
Bedford MK44 1LQ
UK
Marco.De-Boni@unilever.com
Abstract
Argumentation is an emerging topic in the
field of human computer dialogue. In this
paper we describe a novel approach to dia-
logue management that has been developed to
achieve persuasion using a textual argumen-
tation dialogue system. The paper introduces
a layered management architecture that mixes
task-oriented dialogue techniques with chat-
bot techniques to achieve better persuasive-
ness in the dialogue.
1 Introduction
Human computer dialogue is a wide research area
in Artificial Intelligence. Computer dialogue is
now used at production stage for applications such
as tutorial dialogue ? that helps teaching students
(Freedman, 2000) ? task-oriented dialogue ? that
achieves a particular, limited task, such as book-
ing a trip (Allen et al, 2000) ? and chatbot dialogue
(Levy et al, 1997) ? that is used within entertain-
ment and help systems.
None of these approaches use persuasion as a
mechanism to achieve dialogue goals. However,
research towards the use of persuasion in Hu-
man Computer Interactions has spawned around the
field of natural argumentation (Norman and Reed,
2003). Similarly research on Embodied Con-
versational Agents (ECA) (Bickmore and Picard,
2005) is also attempting to improve the persuasive-
ness of agents with persuasion techniques; how-
ever, it concentrates on the visual representation
of the interlocutor rather than the dialogue man-
agement. Previous research on human computer
dialogue has rarely focused on persuasive tech-
niques (Guerini, Stock, and Zancanaro, 2004, initi-
ated some research in that field). Our dialogue man-
agement system applies a novel method, taking ad-
vantage of persuasive and argumentation techniques
to achieve persuasive dialogue.
According to the cognitive dissonance theory
(Festinger, 1957), people will try to minimise the
discrepancy between their behaviour and their be-
liefs by integrating new beliefs or distorting existing
ones. In this paper, we approach persuasion as a pro-
cess shaping user?s beliefs to eventually change their
behaviour.
The presented dialogue management system has
been developed to work on known limitations of cur-
rent dialogue systems:
The impression of lack of control is an issue when
the user is interacting with a purely task-oriented di-
alogue system (Farzanfar et al, 2005). The system
follows a plan to achieve the particular task, and the
user?s dialogue moves are dictated by the planner
and the plan operators.
The lack of empathy of computers is also a
problem in human-computer interaction for applica-
tions such as health-care, where persuasive dialogue
could be applied (Bickmore and Giorgino, 2004).
The system does not respond to the user?s personal
and emotional state, which sometimes lowers the
user?s implication in the dialogue. However, exist-
ing research (Klein, Moon, and Picard, 1999) shows
that a system that gives appropriate response to the
user?s emotion can lower frustration.
In human-human communication, these lim-
itations reduce the effectiveness of persuasion
138
(Stiff and Mongeau, 2002). Even if the response to-
wards the computer is not always identical to the
one to humans, it seems sensible to think that per-
suasive dialogue systems can be improved by apply-
ing known findings from human-human communi-
cation.
The dialogue management architecture described
in this paper (see Figure 1) addresses these dialogue
management issues by using a novel layered ap-
proach to dialogue management, allowing the mix-
ing of techniques from task-oriented dialogue man-
agement and chatbot techniques (see Section 4).
Figure 1: Layered Management Architecture
The use of a planner guarantees the consistency
of the dialogue and the achievement of persuasive
goals (see Section 4.2). Argumentative dialogue can
be seen as a form of task-oriented dialogue where
the system?s task is to persuade the user by present-
ing the arguments. Thus, the dialogue manager first
uses a task-oriented dialogue methodology to cre-
ate a dialogue plan that will determine the content
of the dialogue. The planning component?s role is
to guarantee the consistency of the dialogue and the
achievement of the persuasive goals.
In state-of-the-art task-oriented dialogue manage-
ment systems, the planner provides instructions for
a surface realizer (Green and Lehman, 2002), re-
sponsible of generating the utterance corresponding
to the plan step. Our approach is different to al-
low more reactivity to the user and give a feeling
of control over the dialogue. In this layered ap-
proach, the reactive component provides a direct re-
action to the user input, generating one or more ut-
terances for a given plan step, allowing for reactions
to user?s counter arguments as well as backchannel
and chitchat phases without cluttering the plan.
Experimental results show that this layered ap-
proach allows the user to feel more comfortable in
the dialogue while preserving the dialogue consis-
tency provided by the planner. Eventually, this trans-
lates into a more persuasive dialogue (see Section 6).
2 Related Work
Persuasion through dialogue is a novel
field of Human Computer Interaction.
Reiter, Robertson, and Osman (2003),Reed (1998)
and Carenini and Moore (2000) apply persuasive
communication principles to natural language
generation, but only focus on monologue.
The 3-tier planner for tutoring dialogue by
Zinn, Moore, and Core (2002) provides a di-
alogue management technique close to our
approach: a top-tier generates a dialogue plan,
the middle-tier generates refinements to the
plan and the bottom-tier generates utterances.
Mazzotta, de Rosis, and Carofiglio (2007) also
propose a planning framework for user-adapted
persuasion where the plan operators are mapped
to natural language (or ECA) generation. How-
ever, these planning approaches do not include a
mechanism to react to user?s counter arguments
that are difficult to plan beforehand. This paper
propose a novel approach that could improve
the user?s comfort in the dialogue as well as its
persuasiveness.
3 Case Study
Part of the problem in evaluating persuasive dia-
logue is using an effective evaluation framework.
Moon (1998) uses the Desert Survival Scenario to
evaluate the difference of persuasion and trust in
interaction between humans when face-to-face or
when mediated by a computer system (via an instant
messaging platform).
The Desert Survival Scenario
(Lafferty, Eady, and Elmers, 1974) is a negoti-
ation scenario used in team training. The team is
put in a scenario where they are stranded in the
desert after a plane crash. They have to negotiate a
ranking of the most eligible items (knife, compass,
map, etc.) that they should keep for their survival.
For the evaluation of the dialogue system, a simi-
lar scenario is presented to the participants. The user
has to choose an initial preferred ranking of items
139
and then engages in a discussion with the dialogue
system that tries to persuade the user to change the
ranking. At the end of the dialogue, the user has the
opportunity to either change or keep the ranking.
The architecture of the dialogue system is de-
scribed throughout this paper using examples from
the Desert Scenario. The full evaluation protocol is
described in Section 5 and 6.
4 Dialogue Management Architecture
The following sections provide a description of
the dialogue management architecture introduced in
Figure 1.
4.1 Argumentation Model
The Argumentation model represents the different
arguments (conclusions and premises) that can be
proposed by the user or by the system. Figure 2
gives a simplified example of the Desert Scenario
model.
Figure 2: Argumentation Model Sample
This model shows the different facts that are
known by the system and the relations be-
tween them. Arrows represent the support re-
lation between two facts. For example, res-
cue knows where you are is a support to the fact
goal(signal) (the user goal is to signal presence to
the rescue) as well as a support to goal(stay put) (the
user goal is to stay close to the wreckage). This
relational model is comparable to the argumenta-
tion framework proposed by Dung (1995), but stores
more information about each argument for reason-
ing within the planning and reactive component (see
Section 4.2).
Each fact in this model represents a belief to be
introduced to the user. For example, when the dia-
logue tries to achieve the goal reorder(flashlight >
air map): the system wants the user to believe that
the ?flashlight? item should be ranked higher than
the ?air map? item. The argumentation model de-
scribes the argumentation process that is required
to introduce this new belief: the system first has to
make sure the user believes in rate lower(air map)
and rate higher(flashlight).
Lower level facts (see Figure 2) are the goal facts
of the dialogue, the ones the system chooses as di-
alogue goals, according to known user beliefs and
the system?s goal beliefs (e.g. according to the rank-
ing the system is trying to defend). The facts in the
middle of the hierarchy are intermediate facts that
need to be asserted during the dialogue. The top-
level facts are world knowledge: facts that require
minimum defense and can be easily grounded in the
dialogue.
4.2 Planning Component
The planning component?s task is to find a plan us-
ing the argumentation model to introduce the re-
quired facts in the user?s belief to support the per-
suasive goals. The plan is describes a path in the ar-
gumentation model beliefs hierarchy that translates
to argumentation segments in the dialogue.
In our current evaluation method, the goal of the
dialogue is to change the user?s beliefs about the
items so that the user eventually changes the rank-
ing. At the beginning of the dialogue, the ranking of
the system is chosen and persuasive goals are com-
puted for the dialogue. These persuasive goals cor-
respond to the lower level facts in the argumentation
model ? like ?reorder(flashlight > air map)? in our
previous example. The available planning operators
are:
use world(fact) describes a step in the dialogue
that introduces a simple fact to the user.
ground(fact) describes a step in the dialogue that
grounds a fact in the user beliefs. Grounding a fact
is a different task from the use world operator as it
will need more support during the dialogue.
do support([fact0, fact1, . . . ], fact2) describes a
complex support operation. The system will initiate
a dialogue segment supporting fact2 with the facts
fact1 and fact0, etc. that have previously been intro-
duced in the user beliefs.
The planning component can also use two
non-argumentative operators, do greetings and
140
do farewells, that are placed respectively at the be-
ginning and the end of the dialogue plan to open and
close the session.
Here is an example plan using the two argu-
ments described in Figure 2 to support the goal re-
order(flashlight > air map):
Step 1 do greetings
Step 2 use world(goal(be found))
ground(rescue knows where you are)
ground(can(helpatnight,
item(flashlight)))
Step 3 do support([can(helpatnight,
item(flashlight))],
rate higher(item(flashlight)))
do support(
[rescue knows where you are,
goal(be found)],
goal(stay put))
Step 4 do support([goal(stay put)],
rate lower(item(air map)))
Step 5 do support(...,
reorder(item(flashlight),
item(air map)))
Step 6 do farewells
The plan is then interpreted by the reactive com-
ponent that is responsible for realizing each step in
a dialogue segment.
4.3 The Reactive Component
The reactive component?s first task is to realize the
operators chosen by the planning component into di-
alogue utterance(s). However, it should not be mis-
taken for a surface language realizer. The reactive
component?s task, when realizing the operator, is to
decide how to present the particular argumentation
operator and its parameters to the user according to
the dialogue context and the user?s reaction to the
argument. This reactive process is described in the
following sections.
4.3.1 Realization and Reaction Strategies
Each step of the plan describes the general topic
of a dialogue segment1. A dialogue segment is
a set of utterances from the system and from
1i.e. it is not directly interpreted as an instruction to generate
one unique utterance.
the user that are related to a particular argument.
For example, in the Desert Scenario, the operator
ground(can(helpatnight, item(flashlight))) may re-
sult in the following set of utterances:
S(ystem) I think the flashlight could
be useful as it could help us at
night,
U(ser) How is that? We are not going
to move during the night.
S well, if we want to collect water,
it will be best to do things at
night and not under the burning
sun.
U I see. It could be useful then.
In this example, the ground operator has been re-
alized by the reactive component in two different ut-
terances to react to the user?s interaction.
The goal of the reactive component is to make the
user feel that the system understands what has been
said. It is also important to avoid replanning as it
tries to defend the arguments chosen in the plan.
As described in Section 4.2, the planner relies on
the argumentation model to create a dialogue plan.
Encoding all possible defenses and reactions to the
user directly in this model will explode the search
space of the planner and require careful authoring
to avoid planning inconsistencies2 . In addition, pre-
dicting at the planning level what counter arguments
a user is likely to make requires a prior knowledge
of the user?s beliefs. At the beginning of a one-off
dialogue, it is not possible to make prior assump-
tions on the user?s beliefs; the system has a shal-
low knowledge of the user?s beliefs and will discover
them as the dialogue goes.
Hence, it is more natural to author a reactive di-
alogue that will respond to the user?s counter ar-
guments as they come and extends the user beliefs
model as it goes. In our architecture if the user is
disagreeing with an argument, the plan is not revised
directly; if possible, the reactive component selects
new, contextually appropriate, supporting facts for
the current plan operator. It can do this multiple
consecutive local repairs if the user needs more con-
vincing and the domain model provides enough de-
fenses. This allows for a simpler planning frame-
work.
2a new plan could go against the previously used arguments.
141
In addition, when available, and even if the user
agrees with the current argument, the reactive com-
ponent can also choose from a set of ?dialogue
smoothing? or backchannel utterances to make the
dialogue feel more natural. Here is an example from
the Desert Scenario:
S We don?t have much water, we need to
be rescued as soon as possible.
(from plan step: user world( goal(be found)))
U right
S I am glad we agree.(backchannel)
S There is a good chance that the
rescue team already knows our
whereabouts. We should be
optimistic and plan accordingly,
don?t you think?
(from plan step:
use world( rescue knows where you are))
4.3.2 Detecting user reactions
The reactive component needs to detect if the user
is agreeing to its current argument or resisting the
new fact that is presented. Because the dialogue
management system was developed from the per-
spective of a system that could be easily ported to
different domains, choice was made to use a domain
independent and robust agreement/disagreement de-
tection.
The agreement/disagreement detection is based
on an utterance classifier. The classifier is a cas-
cade of binary Support Vector Machines (SVM)
(Vapnik, 2000) trained on the ICSI Meeting cor-
pus (Janin et al, 2003). The corpus contains 8135
spurts3 annotated with agreement/disagreement in-
formation Hillard, Ostendorf, and Shriberg (2003).
A multi-class SVM classifier is trained on local
features of the spurts such as a) the length of the
spurt, b) the first word of the spurt, c) the bigrams of
the spurts, and d) part of speech tags. The classifica-
tion achieves an accuracy of 83.17% with an N-Fold
4 ways split cross validation. Additional results and
comparison with state-of-the-art are available in Ap-
pendix A.
During the dialogue, the classifier is applied on
each of the user?s utterances, trying to determine if
the user is agreeing or disagreeing with the system.
3speech utterances that have no pauses longer than .5 sec-
onds.
According to this labelling, the strategies described
in section 4.3.1 and 4.3.3 are applied.
4.3.3 Revising the plan
The reactive component will attempt local repairs
to the plan by defending the argumentation move
chosen by the planning component. However, there
are cases when the user will still not accept an ar-
gument. In these cases, imposing the belief to the
user is counter-productive and the current goal be-
lief should be dropped from the plan.
For each utterance chosen by the reactive com-
ponent, the belief model of the user is updated to
represent the system knowledge of the user?s be-
liefs. Every time the user agrees to an utterance
from the system, the belief model is extended with
a new belief; in the previous example, when the
user says ?I see, it could be useful then.?, the sys-
tem detects an agreement (see the Section 4.3.2)
and extends the user?s beliefs model with the be-
lief: can(helpatnight, item(flashlight)). The agree-
ment is then followed by a local repair, since the
user doesn?t disagree with the statement made, the
system also extends the belief model with beliefs rel-
evant to the content of the local repair, thus learning
more about the user?s belief model.
As a result of this process, when the system de-
cides to revise the plan, the planning component
does not start from the same beliefs state as previ-
ously. In effect, the system is able to learn user?s be-
liefs based on the agreement/disagreement with the
user, it can therefore make a more effective use of
the argumentation hierarchy to find a better plan to
achieve the persuasive goals.
Still, there are some cases when the planning
component will be unable to find a new plan from
the current belief state to the goal belief state ? this
can happen when the planner has exhausted all its
argumentative moves for a particular sub-goal. In
these cases, the system has to make concessions and
drop the persuasive goals that it cannot fulfil. By
dropping goals, the system will lower the final per-
suasiveness, but guarantees not coercing the user.
4.3.4 Generation
Utterance generation is made at the reactive com-
ponent level. In the current version of the dia-
logue management system, the utterance generation
142
is based on an extended version of Alicebot AIML
4
.
AIML is an XML language that provides a pat-
tern/template generation model mainly used for
chatbot systems. An AIML bot defines a set of
categories that associate a topic, the context of the
previous bot utterance (called that in the AIML ter-
minology), a matching pattern that will match the
last user utterance and a generation template. The
topic, matching and that field define matching pat-
terns that can contain * wildcards accepting any to-
ken(s) of the user utterance (e.g. HELLO * would
match any utterance starting by ?Hello?). They are
linked to a generation template that can reuse the to-
kens matched by the patterns wildcards to generate
an utterance tailored to the user input and the dia-
logue context.
For the purpose of layered dialogue management,
the AIML language has been extended to include
more features: 1) A new pattern slot has been in-
troduced to link a set of categories to a particular ar-
gumentation operator; 2) Utterances generations are
linked to the belief they are trying to introduce to
the user and if an agreement is detected, this belief
is added to the user belief model.
For example, a set of matching categories for the
Desert Scenario could be:
Plan operator: use world(goal(survive))
Category 1 :
Pattern *
Template Surviving is our
priority, do you want
to hear about my desert
survival insights?
Category 2 :
Pattern * insights
That * survival insights
Template I mean, I had a few
ideas ...common knowledge I
suppose.
Category 3 :
Pattern *
That * survival insights
Template Well, we are in this
together. Let me tell you
of what I think of desert
survival, ok?
4http://www.alicebot.org/
These three categories can be used to match
the user reaction during the dialogue seg-
ment corresponding to the plan operator:
use world(goal(survive)). Category 1 is used
as the initiative taking generation. It will be the
first one to be used when the system comes from
a previously finished step. Categories 2-3 are all
?defenses? that support Category 1. They will be
used to react to the user if no agreement is detected
from the last utterances. For example, if the user
says ?what kind of survival insights??? as a reply
to the generation from Category 1, a disagreement
is detected and the reactive component will have a
contextualised answer as given by category 2 whose
that pattern matches the last utterance from the
system, the pattern pattern matches the user
utterance.
The dialogue management system uses 187 cate-
gories tailored to the Desert Scenario as well as 3737
general categories coming from the Alice chatbot
and used to generate the dialogue smoothing utter-
ances. Developing domain specific reactions is a te-
dious and slow process that was iteratively achieved
with Wizard of OZ experiments with real users. In
these experiments, users were told they were going
to have a dialogue with another human in the Desert
Scenario context. The dialogue system manages the
whole dialogue, except for the generation phase that
is mediated by an expert that can either choose the
reaction of the system from an existing set of utter-
ances, or type a new one.
5 Persuasiveness Metric
Evaluating a behavior change would require a long-
term observation of the behavior that would be de-
pendent to external elements (Bickmore and Picard,
2005). To evaluate our system, an evaluation proto-
col measuring the change in the beliefs underlying
the behavior was chosen. As explained in Section 3,
the Desert Scenario is used as a base for the evalu-
ation. Each participant is told that he is stranded in
the desert. The user gives a preferred initial rank-
ing Ri of the items (knife, compass, map, etc.). The
user then engages in a dialogue with the system. The
system then attempts to change the user?s ranking to
a different ranking Rs through persuasive dialogue.
At the end of the dialogue, the user can change this
143
choice to arrive at a final ranking Rf .
The persuasiveness of the dialogue can be mea-
sured as the evolution of the distance between
the user ranking (Ri, Rf ) and the system ranking
(Rs). The Kendall ? distance (Kendall, 1938) is
used to compute the pairwise disagreement between
two rankings. The change of the Kendall ? dis-
tance during the dialogue gives an evaluation of the
persuasiveness of the dialogue: Persuasiveness =
K?(Ri, Rs) ? K?(Rf , Rs). In the current evalu-
ation protocol, the Rs is always the reverse of the
Ri, so K?(Ri, Rs) is always the maximum distance
possible: n?(n?1)2 where n is the number of items to
rank. The minimum Kendall tau distance is 0. If the
system was persuasive enough to make the user in-
vert the initial ranking, Persuasiveness of the system
is maximum and equal to: n?(n?1)2 . If the system
does not succeed in changing the user ranking, then
Persuasiveness is zero.
6 Evaluation Results and Discussion
16 participants have been recruited from a variety of
ages (from 20 to 59) and background. They were
all told to use a web application that describes the
Desert Scenario (see Section 3) and proposes to un-
dertake two instant messaging chats with two human
users5. However, both discussions are managed by
different versions of the dialogue system, following
a similar protocol:
? one version of the dialogue is managed by a
limited version of the dialogue system, with no
reactive component. This version is similar to
a purely task-oriented system, planning and re-
vising the plan directly on dialogue failures,
? the second version is the full dialogue system
as described in this paper.
Each participant went through one dialogue with
each system, in a random order. This comparison
shows that the dialogue flexibility provided by the
reactive component allows a more persuasive dia-
logue. In addition, when faced with the second dia-
logue, the participant has formed more beliefs about
the scenario and is more able to counter argue.
5The evaluation is available Online at
http://www.cs.york.ac.uk/aig/eden
Figure 3: Comparative Results. interpret, not coercive,
perceived persuasion are on a scale of [0 ? 4] (see Ap-
pendix B). Persuasiveness is on a scale of [?10, 10].
Figure 3 reports the independent Persuasiveness
metric results as well as interesting answers to a
questionnaire that the participants filled after each
dialogue (see the Appendix B for detailed results
and questionnaire).
Over all the dialogues, the full system is 18%
more persuasive than the limited system. This is
measured by the Persuasiveness metric introduced in
Section 5. With the full system, the participants did
an average of 1.33 swaps of items towards the sys-
tem?s ranking. With the limited system, the partic-
ipants did an average of 0.47 swaps of items away
from the system?s ranking. However, the answers
to the self evaluated perceived persuasion question
show that the participants did not see any significant
difference in the ability to persuade of the limited
and the full systems.
According to the question interpret, the partici-
pants found that the limited system understood bet-
ter what they said. This last result might be ex-
plained by the behavior of the systems: the limited
system drops an argument at every user disagree-
ment, making the user believe that the disagreement
was understood. The full system tries to defend the
argument; if possible with a contextually tailored
support, however, if this is not available, it may use a
generic support, making the user believe he was not
fully understood.
Our interpretation of the fact that the discrepancy
between user self evaluation of the interaction with
the system and the measured persuasion is that, even
if the full system is more argumentative, the user
144
didn?t feel coerced6. These results show that a more
persuasive dialogue can be achieved without deteri-
orating the user perception of the interaction.
7 Conclusion
Our dialogue management system introduces a
novel approach to dialogue management by using
a layered model mixing the advantages of state-of-
the-art dialogue management approaches. A plan-
ning component tailored to the task of argumenta-
tion and persuasion searches the ideal path in an ar-
gumentation model to persuade the user. To give a
reactive and natural feel to the dialogue, this task-
oriented layer is extended by a reactive component
inspired from the chatbot dialogue management ap-
proach. The Desert Scenario evaluation, providing
a simple and independent metric for the persuasive-
ness of the dialogue system provided a good proto-
col for the evaluation of the dialogue system. This
one showed to be 18% more persuasive than a purely
task-oriented system that was not able to react to the
user interaction as smoothly.
Our current research on the dialogue management
system consists in developing another evaluation do-
main where a more complex utterance generation
can be used. This will allow going further than the
simple template based system, offering more diverse
answers to the user and avoiding repetitions; it will
also allow us to experiment textual persuasion tai-
lored to other parameters of the user representation,
such as the user personality.
References
Allen, J. F., G. Ferguson, B. W. Miller, E. K. Ringger, and
T. Sikorski. 2000. Dialogue Systems: From Theory to
Practice in TRAINS-96, chapter 14.
Bickmore, T. and T. Giorgino. 2004. Some novel aspects
of health communication from a dialogue systems per-
spective. In AAAI Fall Symposium.
Bickmore, T. W. and R. W. Picard. 2005. Establishing and
maintaining long-term human-computer relationships.
ACM Trans. Comput.-Hum. Interact., 12(2):293?327.
Carenini, G. and J. Moore. 2000. A strategy for generat-
ing evaluative arguments. In International Conference
on Natural Language Generation.
Dung, P. M. 1995. On the acceptability of arguments
and its fundamental role in nonmonotonic reasoning,
6The answers to the not coercive question do not show any
significant difference in the perception of coercion of the two
system.
logic programming and n-person games. Artif. Intell.,
77(2):321?357.
Farzanfar, R., S. Frishkopf, J. Migneault, and R. Fried-
man. 2005. Telephone-linked care for physical ac-
tivity: a qualitative evaluation of the use patterns of
an information technology program for patients. J. of
Biomedical Informatics, 38(3):220?228.
Festinger, Leon. 1957. A Theory of Cognitive Disso-
nance. Stanford University Press.
Freedman, R. 2000. Plan-based dialogue management in
a physics tutor. In Proceedings of ANLP ?00.
Galley, M., K. Mckeown, J. Hirschberg, and E. Shriberg.
2004. Identifying agreement and disagreement in con-
versational speech: use of bayesian networks to model
pragmatic dependencies. In Proceedings of ACL?04.
Green, N. and J. F. Lehman. 2002. An integrated dis-
course recipe-based model for task-oriented dialogue.
Discourse Processes, 33(2):133?158.
Guerini, M., O. Stock, and M. Zancanaro. 2004. Per-
suasive strategies and rhetorical relation selection. In
Proceedings of ECAI-CMNA.
Hillard, D., M. Ostendorf, and E. Shriberg. 2003. Detec-
tion of agreement vs. disagreement in meetings: train-
ing with unlabeled data. In Proceedings of NAACL?03.
Janin, A., D. Baron, J. Edwards, D. Ellis, D. Gelbart,
N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-
cke, and C. Wooters. 2003. The ICSI meeting corpus.
In Proceedings of ICASSP?03.
Kendall, M. G. 1938. A new measure of rank correlation.
Biometrika, 30(1/2):81?93.
Klein, J., Y. Moon, and R. W. Picard. 1999. This com-
puter responds to user frustration. In CHI?99.
Lafferty, J. C., Eady, and J. Elmers. 1974. The desert
survival problem.
Levy, D., R. Catizone, B. Battacharia, A. Krotov, and
Y. Wilks. 1997. Converse:a conversational compan-
ion. In Proceedings of 1st International Workshop on
Human-Computer Conversation.
Mazzotta, I., F. de Rosis, and V. Carofiglio. 2007. Por-
tia: A user-adapted persuasion system in the healthy-
eating domain. Intelligent Systems, IEEE, 22(6).
Moon, Y. 1998. The effects of distance in local versus
remote human-computer interaction. In Proceedings
of SIGCHI?98.
Norman, Timothy J. and Chris Reed. 2003. Argumenta-
tion Machines : New Frontiers in Argument and Com-
putation (Argumentation Library). Springer.
Reed, C. 1998. Generating Arguments in Natural Lan-
guage. Ph.D. thesis, University College London.
Reiter, E., R. Robertson, and L. M. Osman. 2003.
Lessons from a failure: generating tailored smoking
cessation letters. Artif. Intell., 144(1-2):41?58.
Stiff, J. B. and P. A. Mongeau. 2002. Persuasive Commu-
nication, second edition.
Vapnik, V. N. 2000. The Nature of Statistical Learning
Theory.
Zinn, C., J. D. Moore, and M. G. Core. 2002. A 3-tier
planning architecture for managing tutorial dialogue.
In Proceedings of ITS ?02.
145
A Agreement/Disagreement Classification
Setup 1 Setup 2
Galley et al, global features 86.92% 84.07%
Galley et al, local features 85.62% 83.11%
Hillard et al 82% NA
SVM 86.47% 83.17%
Table 1: Accuracy of different agreement/disagreement
classification approaches.
The accuracy of state-of-the-art techniques
(Hillard, Ostendorf, and Shriberg (2003) and
Galley et al (2004)) are reported in Table 1 and
compared to our SVM classifier. Two experimental
setups were used:
Setup 1 reproduces Hillard, Ostendorf, and Shriberg
(2003) training/testing split between meetings;
Setup 2 reproduces the N-Fold, 4 ways split used by
Galley et al (2004).
The SVM results are arguably lower than Galley et al
system with labeled dependencies. However, this is be-
cause our system only relies on local features of each
utterance, while Galley et al (2004) use global features
(i.e. features describing relations between consecutive ut-
terances) suggest that adding global features would also
improve the SVM classifier.
B Evaluation Questionnaire
In the evaluation described in section 6, the participants
were asked to give their level of agreement with each
statement on the scale: Strongly disagree (0), Disagree
(1), Neither agree nor disagree (2), Agree (3), Strongly
Agree(4). Table 2 provides a list of questions with the
average agreement level and the result of a paired t-test
between the two system results.
146
label question full system limited system ttest
interpret ?In the conversation, the other user inter-
preted correctly what you said?
1.73 2.13 0.06
perceived persuasion ?In the conversation, the other user was
persuasive?
2.47 2.53 0.44
not coercive ?The other user was not forceful in
changing your opinion?
2.4 2.73 0.15
sluggish ?The other user was sluggish and slow to
reply to you in this conversation?
1.27 1.27 0.5
understand ?The other user was easy to understand
in the conversation?
3.2 3.13 0.4
pace ?The pace of interaction with the other
user was appropriate in this conversa-
tion?
2.73 3.07 0.1
friendliness ?The other user was friendly? 2.93 2.87 0.4
length length of the dialogue 12min 19s 08min 33s 0.07
persuasiveness Persuasiveness 1.33 -0.47 0.05
Table 2: Results from the evaluation questionnaire.
147
