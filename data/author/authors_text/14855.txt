Concrete Sentence Spaces for Compositional Distributional
Models of Meaning
Edward Grefenstette?, Mehrnoosh Sadrzadeh?, Stephen Clark?, Bob Coecke?, Stephen Pulman?
?Oxford University Computing Laboratory, ?University of Cambridge Computer Laboratory
firstname.lastname@comlab.ox.ac.uk, stephen.clark@cl.cam.ac.uk
Abstract
Coecke, Sadrzadeh, and Clark [3] developed a compositional model of meaning for distributional
semantics, in which each word in a sentence has a meaning vector and the distributional meaning of the
sentence is a function of the tensor products of the word vectors. Abstractly speaking, this function is the
morphism corresponding to the grammatical structure of the sentence in the category of finite dimensional
vector spaces. In this paper, we provide a concrete method for implementing this linear meaning map,
by constructing a corpus-based vector space for the type of sentence. Our construction method is based
on structured vector spaces whereby meaning vectors of all sentences, regardless of their grammatical
structure, live in the same vector space. Our proposed sentence space is the tensor product of two noun
spaces, in which the basis vectors are pairs of words each augmented with a grammatical role. This
enables us to compare meanings of sentences by simply taking the inner product of their vectors.
1 Background
Coecke, Sadrzadeh, and Clark [3] develop a mathematical framework for a compositional distributional
model of meaning, based on the intuition that syntactic analysis guides the semantic vector composition.
The setting consists of two parts: a formalism for a type-logical syntax and a formalism for vector space
semantics. Each word is assigned a grammatical type and a meaning vector in the space corresponding to
its type. The meaning of a sentence is obtained by applying the function corresponding to the grammatical
structure of the sentence to the tensor product of the meanings of the words in the sentence. Based on the
type-logic used, some words will have atomic types and some compound function types. The compound
types live in a tensor space where the vectors are weighted sums (i.e. superpositions) of the pairs of bases
from each space. Compound types are ?applied? to their arguments by taking inner products, in a similar
manner to how predicates are applied to their arguments in Montague semantics.
For the type-logic we use Lambek?s Pregroup grammars [7]. The use of pregoups is not essential, but
leads to a more elegant formalism, given its proximity to the categorical structure of vector spaces (see [3]).
A Pregroup is a partially ordered monoid where each element has a right and left cancelling element, referred
to as an adjoint. It can be seen as the algebraic counterpart of the cancellation calculus of Harris [6]. The
operational difference between a Pregroup and Lambek?s Syntactic Calculus is that, in the latter, the monoid
multiplication of the algebra (used to model juxtaposition of the types of the words) has a right and a left
adjoint, whereas in the pregroup it is the elements themselves which have adjoints. The adjoint types are
used to denote functions, e.g. that of a transitive verb with a subject and object as input and a sentence as
output. In the Pregroup setting, these function types are still denoted by adjoints, but this time the adjoints
of the elements themselves.
As an example, consider the sentence ?dogs chase cats?. We assign the type n (for noun phrase) to ?dog?
and ?cat?, and nrsnl to ?chase?, where nr and nl are the right and left adjoints of n and s is the type of a
125
(declarative) sentence. The type nrsnl expresses the fact that the verb is a predicate that takes two arguments
of type n as input, on its right and left, and outputs the type s of a sentence. The parsing of the sentence is
the following reduction:
n(nrsnl)n ? 1s1 = s
This parse is based on the cancellation of n and nr, and also nl and n; i.e. nnr ? 1 and nln ? 1 for 1
the unit of juxtaposition. The reduction expresses the fact that the juxtapositions of the types of the words
reduce to the type of a sentence.
On the semantic side, we assign the vector space N to the type n, and the tensor space N ?S?N to the
type nrsnl. Very briefly, and in order to introduce some notation, recall that the tensor space A?B has as a
basis the cartesian product of a basis of A with a basis of B. Recall also that any vector can be expressed as
a weighted sum of basis vectors; e.g. if (??v1 , . . . ,??vn) is a basis of A then any vector ??a ? A can be written as
??a =?i Ci??vi where each Ci ? R is a weighting factor. Now for (??v1 , . . . ,??vn) a basis of A and (
??
v?1 , . . . ,
??
v?n)
a basis of B, a vector ??c in the tensor space A ? B can be expressed as follows:
?
ij
Cij (??vi ?
??
v?j )
where the tensor of basis vectors ??vi ?
??
v?j stands for their pair (??vi ,
??
v?j ). In general ??c is not separable into
the tensor of two vectors, except for the case when ??c is not entangled. For non-entangled vectors we can
write ??c = ??a ???b for ??a =
?
i Ci
??vi and
??b =
?
j C ?j
??
v?j ; hence the weighting factor of ??c can be obtained
by simply multiplying the weights of its tensored counterparts, i.e. Cij = Ci ? C ?j . In the entangled case
these weights cannot be determined as such and range over all the possibilities. We take advantage of this
fact to encode meanings of verbs, and in general all words that have compound types and are interpreted as
predicates, relations, or functions. For a brief discussion see the last paragraph of this section. Finally, we
use the Dirac notation to denote the dot or inner product of two vectors ???a | ??b ? ? R defined by
?
i Ci?C ?i.
Returning to our example, for the meanings of nouns we have
???
dogs,??cats ? N , and for the meanings of
verbs we have
????
chase ? N ? S ? N , i.e. the following superposition:
?
ijk
Cijk (??ni ???sj ???nk)
Here ??ni and ??nk are basis vectors of N and ??sj is a basis vector of S. From the categorical translation method
presented in [3] and the grammatical reduction n(nrsnl)n ? s, we obtain the following linear map as the
categorical morphism corresponding to the reduction:
N ? 1s ? N : N ? (N ? S ? N)? N ? S
Using this map, the meaning of the sentence is computed as follows:
???????????
dogs chase cats = (N ? 1s ? N )
(???
dogs ?????chase ???cats
)
= (N ? 1s ? N )
?
?
???
dogs ?
?
?
?
ijk
Cijk(??ni ???sj ???nk)
?
????cats
?
?
=
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats?
The key features of this operation are, first, that the inner-products reduce dimensionality by ?consuming?
tensored vectors and by virtue of the following component function:
N : N ? N ? R :: ??a ?
??b 7? ???a | ??b ?
126
Thus the tensored word vectors
???
dogs ? ????chase ? ??cats are mapped into a sentence space S which is common
to all sentences regardless of their grammatical structure or complexity. Second, note that the tensor product???
dogs?????chase???cats does not need to be calculated, since all that is required for computation of the sentence
vector are the noun vectors and the Cijk weights for the verb. Note also that the inner product operations
are simply picking out basis vectors in the noun space, an operation that can be performed in constant
time. Hence this formalism avoids two problems faced by approaches in the vein of [9, 2], which use
the tensor product as a composition operation: first, that the sentence meaning space is high dimensional
and grammatically different sentences have representations with different dimensionalities, preventing them
from being compared directly using inner products; and second, that the space complexity of the tensored
representation grows exponentially with the length and grammatical complexity of the sentence. In constrast,
the model we propose does not require the tensored vectors being combined to be represented explicitly.
Note that we have taken the vector of the transitive verb, e.g.
????
chase, to be an entangled vector in the
tensor space N ? S ?N . But why can this not be a separable vector, in which case the meaning of the verb
would be as follows:
????
chase =
?
i
Ci??ni ?
?
j
C ?j??sj ?
?
k
C ??k??nk
The meaning of the sentence would then become ?1?2
?
j C ?j
??sj for ?1 =
?
i Ci?
???
dogs | ??ni? and ?2 =
?
k C ??k ?
??
cats | ??nk?. The problem is that this meaning only depends on the meaning of the verb and is
independent of the meanings of the subject and object, whereas the meaning from the entangled case,
i.e. ?1?2
?
ijk Cijk
??sj , depends on the meanings of subject and object as well as the verb.
2 From Truth-Theoretic to Corpus-based Meaning
The model presented above is compositional and distributional, but still abstract. To make it concrete, N and
S have to be constructed by providing a method for determining the Cijk weightings. Coecke, Sadrzadeh,
and Clark [3] show how a truth-theoretic meaning can be derived in the compositional framework. For
example, assume that N is spanned by all animals and S is the two-dimensional space spanned by ??true and???
false. We use the weighting factor to define a model-theoretic meaning for the verb as follows:
Cijk??sj =
{??
true chase(??ni ,??nk) = true ,???
false o.w.
The definition of our meaning map ensures that this value propagates to the meaning of the whole sentence.
So chase(???dogs,???cats) becomes true whenever ?dogs chase cats? is true and false otherwise. This is exactly
how meaning is computed in the model-theoretic view on semantics. One way to generalise this truth-
theoretic meaning is to assume that chase(??ni ,??nk) has degrees of truth, for instance by defining chase as a
combination of run and catch, such as:
chase = 23run+
1
3catch
Again, the meaning map ensures that these degrees propagate to the meaning of the whole sentence. For a
worked out example see [3]. But neither of these examples provide a distributional sentence meaning.
Here we take a first step towards a corpus-based distributional model, by attempting to recover a meaning
for a sentence based on the meanings of the words derived from a corpus. But crucially this meaning goes
beyond just composing the meanings of words using a vector operator, such as tensor product, summation
or multiplication [8]. Our computation of sentence meaning treats some vectors as functions and others as
127
function arguments, according to how the words in the sentence are typed, and uses the syntactic structure
as a guide to determine how the functions are applied to their arguments. The intuition behind this approach
is that syntactic analysis guides semantic vector composition.
The contribution of this paper is to introduce some concrete constructions for a compositional distri-
butional model of meaning. These constructions demonstrate how the mathematical model of [3] can be
implemented in a concrete setting which introduces a richer, not necessarily truth-theoretic, notion of natural
language semantics which is closer to the ideas underlying standard distributional models of word meaning.
We leave full evaluation to future work, in order to determine whether the following method in conjunction
with word vectors built from large corpora leads to improved results on language processing tasks, such as
computing sentence similarity and paraphrase evaluation.
Nouns and Transitive Verbs. We take N to be a structured vector space, as in [4, 5]. The bases of N are
annotated by ?properties? obtained by combining dependency relations with nouns, verbs and adjectives. For
example, basis vectors might be associated with properties such as ?arg-fluffy?, denoting the argument of
the adjective fluffy, ?subj-chase? denoting the subject of the verb chase, ?obj-buy? denoting the object of the
verb buy, and so on. We construct the vector for a noun by counting how many times in the corpus a word
has been the argument of ?fluffy?, the subject of ?chase?, the object of ?buy?, and so on.
The framework in [3] offers no guidance as to what the sentence space should consist of. Here we take
the sentence space S to be N ? N , so its bases are of the form ??sj = (??ni ,??nk). The intuition is that, for a
transitive verb, the meaning of a sentence is determined by the meaning of the verb together with its subject
and object.1 The verb vectors Cijk(??ni ,??nk) are built by counting how many times a word that is ni (e.g. has
the property of being fluffy) has been subject of the verb and a word that is nk (e.g. has the property that it?s
bought) has been its object, where the counts are moderated by the extent to which the subject and object
exemplify each property (e.g. how fluffy the subject is). To give a rough paraphrase of the intuition behind
this approach, the meaning of ?dog chases cat? is given by: the extent to which a dog is fluffy and a cat is
something that is bought (for the N ? N property pair ?arg-fluffy? and ?obj-buy?), and the extent to which
fluffy things chase things that are bought (accounting for the meaning of the verb for this particular property
pair); plus the extent to which a dog is something that runs and a cat is something that is cute (for the N ?N
pair ?subj-run? and ?arg-cute?), and the extent to which things that run chase things that are cute (accounting
for the meaning of the verb for this particular property pair); and so on for all noun property pairs.
Adjective Phrases. Adjectives are dealt with in a similar way. We give them the syntactic type nnl and
build their vectors in N ? N . The syntactic reduction nnln ? n associated with applying an adjective to a
noun gives us the map 1N ? N by which we semantically compose an adjective with a noun, as follows:
?????
red fox = (1N ? N )(
??
red ???fox) =
?
ij
Cij??ni???nj |
??
fox?
We can view the Cij counts as determining what sorts of properties the arguments of a particular adjective
typically have (e.g. arg-red, arg-colourful for the adjective ?red?).
Prepositional Phrases. We assign the type nrn to the whole prepositional phrase (when it modifies a noun),
for example to ?in the forest? in the sentence ?dogs chase cats in the forest?. The pregroup parsing is as
follows:
n(nrsnl)n(nrn) ? 1snl1n ? snln ? s1 = s
The vector space corresponding to the prepositional phrase will thus be the tensor space N ? N and the
categorification of the parse will be the composition of two morphisms: (1S?lN )?(rN?1S?1N?rN?1N ).
1Intransitive and ditransitive verbs are interpreted in an analagous fashion; see ?4.
128
The substitution specific to the prepositional phrase happens when computing the vector for ?cats in the
forest? as follows:
?????????????
cats in the forest = (rN ? 1N )
(??
cats ??????????in the forest
)
= (rN ? 1N )
(
??
cats ?
?
lw
Clw??nl ???nk
)
=
?
lw
Clw???cats | ??nl???nw
Here we set the weights Clw in a similar manner to the cases of adjective phrases and verbs with the counts
determining what sorts of properties the noun modified by the prepositional phrase has, e.g. the number of
times something that has attribute nl has been in the forest.
Adverbs. We assign the type srs to the adverb, for example to ?quickly? in the sentence ?Dogs chase cats
quickly?. The pregroup parsing is as follows:
n(nrsnl)n(srs) ? 1s1srs = ssrs ? 1s = s
Its categorification will be a composition of two morphisms (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S). The
substitution specific to the adverb happens after computing the meaning of the sentence without it, i.e. that
of ?Dogs chase cats?, and is as follows:
??????????????????
Dogs chase cats quickly = (rS ? 1S) ? (rN ? 1S ? lN ? 1S ? 1S)
(???
Dogs ?????chase ???cats ??????quickly
)
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?????
quickly
?
?
= (rS ? 1S)
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? ?
?
lw
Clw??sl ???sw
?
?
=
?
lw
Clw
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ??cats? | ??sl
?
??sk
The Clw weights are defined in a similar manner to the above cases, i.e. according to the properties the
adverb has, e.g. which verbs it has modified. Note that now the basis vectors ??sl and ??sw are themselves pairs
of basis vectors from the noun space, (??ni ,??nj). Hence, Clw(??ni ,??nj) can be set only for the case when l = i
and w = j; these counts determine what sorts of properties the verbs that happen quickly have (or more
specifically what properties the subjects and objects of such verbs have). By taking the whole sentence into
account in the interpretation of the adverb, we are in a better position to semantically distinguish between
the meaning of adverbs such as ?slowly? and ?quickly?, for instance in terms of the properties that the verb?s
subjects have. For example, it is possible that elephants are more likely to be the subject of a verb which is
happening slowly, e.g. run slowly, and cheetahs are more likely to be the subject of a verb which is happening
quickly.
3 Concrete Computations
In this section we first describe how to obtain the relevant counts from a parsed corpus, and then give some
similarity calculations for some example sentence pairs.
129
Let Cl be the set of grammatical relations (GRs) for sentence sl in the corpus. Define verbs(Cl) to be
the function which returns all instances of verbs in Cl, and subj (and similarly obj ) to be the function which
returns the subject of an instance Vinstance of a verb V , for a particular set of GRs for a sentence:
subj(Vinstance) =
{
noun if Vinstance is a verb with subject noun
?n o.w.
where ?n is the empty string. We express Cijk for a verb V as follows:
Cijk =
{
?
l
?
v?verbs(Cl) ?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? if ??sj = (??ni ,??nk)
0 o.w.
where ?(v, V ) = 1 if v = V and 0 otherwise. Thus we construct Cijk for verb V only for cases where
the subject property ni and the object property nk are paired in the basis ??sj . This is done by counting the
number of times the subject of V has property ni and the object of V has property nk, then multiplying them,
as prescribed by the inner products (which simply pick out the properties ni and nk from the noun vectors
for the subjects and objects).
The procedure for calculating the verb vectors, based on the formulation above, is as follows:
1. For each GR in a sentence, if the relation is subject and the head is a verb, then find the complementary
GR with object as a relation and the same head verb. If none, set the object to ?n.
2. Retrieve the noun vectors
?????subject,????object for the subject dependent and object dependent from previ-
ously constructed noun vectors.
3. For each (ni, nk) ? basis(N)? basis(N) compute the inner-product of ??ni with
?????subject and ??nk with????object (which involves simply picking out the relevant basis vectors from the noun vectors). Multiply
the inner-products and add this to Cijk for the verb, with j such that ??sj = (??ni ,??nk).
The procedure for other grammatical types is similar, based on the definitions of C weights for the semantics
of these types.
We now give a number of example calculations. We first manually define the distributions for nouns,
which in practice would be obtained from a corpus:
bankers cats dogs stock kittens
1. arg-fluffy 0 7 3 0 2
2. arg-ferocious 4 1 6 0 0
3. obj-buys 0 4 2 7 0
4. arg-shrewd 6 3 1 0 1
5. arg-valuable 0 1 2 8 0
We aim to make these counts match our intuitions, in that bankers are shrewd and a little ferocious but not
furry, cats are furry but not typically valuable, and so on.
We also define the distributions for the transitive verbs ?chase?, ?pursue? and ?sell?, again manually
specified according to our intuitions about how these verbs are used. Since in the formalism proposed above,
Cijk = 0 if ??sj 6= (??ni ,??nk), we can simplify the weight matrices for transitive verbs to two dimensional Cik
matrices as shown below, where Cik corresponds to the number of times the verb has a subject with attribute
ni and an object with attribute nk. For example, the matrix below encodes the fact that something ferocious
130
(i = 2) chases something fluffy (k = 1) seven times in the hypothetical corpus from which we might have
obtained these distributions.
Cchase =
?
?
?
?
?
?
1 0 0 0 0
7 1 2 3 1
0 0 0 0 0
2 0 1 0 1
1 0 0 0 0
?
?
?
?
?
?
Cpursue =
?
?
?
?
?
?
0 0 0 0 0
4 2 2 2 4
0 0 0 0 0
3 0 2 0 1
0 0 0 0 0
?
?
?
?
?
?
Csell =
?
?
?
?
?
?
0 0 0 0 0
0 0 3 0 4
0 0 0 0 0
0 0 5 0 8
0 0 1 0 1
?
?
?
?
?
?
These matrices can be used to perform sentence comparisons:
????????????dogs chase cats | ??????????????dogs pursue kittens? =
=
?
?
?
?
ijk
Cchaseijk ?
???
dogs | ??ni???sj ???nk | ??cats?
?
?
?
?
?
?
?
?
?
?
?
ijk
Cpursueijk ?
???
dogs | ??ni???sj ???nk |
?????
kittens?
?
?
?
=
?
ijk
Cchaseijk C
pursue
ijk ?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats????nk |
?????
kittens?
The raw number obtained from the above calculation is 14844. Normalising it by the product of the length
of both sentence vectors gives the cosine value of 0.979.
Consider now the sentence comparison ????????????dogs chase cats | ???????????cats chase dogs?. The sentences in this pair
contain the same words but the different word orders give the sentences very different meanings. The raw
number calculated from this inner product is 7341, and its normalised cosine measure is 0.656, which demon-
strates the sharp drop in similarity obtained from changing sentence structure. We expect some similarity
since there is some non-trivial overlap between the properties identifying cats and those identifying dogs
(namely those salient to the act of chasing).
Our final example for transitive sentences is ????????????dogs chase cats | ????????????bankers sell stock?, as two sentences that
diverge in meaning completely. The raw number for this inner product is 6024, and its cosine measure is
0.042, demonstrating the very low semantic similarity between these two sentences.
Next we consider some examples involving adjective-noun modification. The Cij counts for an adjective
A are obtained in a similar manner to transitive or intransitive verbs:
Cij =
{
?
l
?
a?adjs(Cl) ?(a,A)?
???????
arg-of(a) | ??ni? if ??ni = ??nj
0 o.w.
where adjs(Cl) returns all instances of adjectives in Cl; ?(a,A) = 1 if a = A and 0 otherwise; and
arg-of(a) = noun if a is an adjective with argument noun, and ?n otherwise.
As before, we stipulate the Cij matrices by hand (and we eliminate all cases where i 6= j since Cij = 0
by definition in such cases):
Cfluffy = [9 3 4 2 2] Cshrewd = [0 3 1 9 1] Cvaluable = [3 0 8 1 8]
We compute vectors for ?fluffy dog? and ?shrewd banker? as follows:
???????
fluffy dog = (3 ? 9)???????arg-fluffy+ (6 ? 3)?????????arg-ferocious+ (2 ? 4)??????obj-buys+ (5 ? 2)????????arg-shrewd+ (2 ? 2)?????????arg-valuable
???????????
shrewd banker = (0 ? 0)???????arg-fluffy+ (4 ? 3)?????????arg-ferocious+ (0 ? 0)??????obj-buys+ (6 ? 9)????????arg-shrewd+ (0 ? 1)?????????arg-valuable
Vectors for
???????
fluffy cat and
??????????
valuable stock are computed similarly. We obtain the following similarity mea-
sures:
cosine(???????fluffy dog,???????????shrewd banker) = 0.389 cosine(???????fluffy cat,??????????valuable stock) = 0.184
131
These calculations carry over to sentences which contain the adjective-noun pairings compositionally and
we obtain an even lower similarity measure between sentences:
cosine(????????????????????fluffy dogs chase fluffy cats,?????????????????????????shrewd bankers sell valuable stock) = 0.016
To summarise, our example vectors provide us with the following similarity measures:
Sentence 1 Sentence 2 Degree of similarity
dogs chase cats dogs pursue kittens 0.979
dogs chase cats cats chase dogs 0.656
dogs chase cats bankers sell stock 0.042
fluffy dogs chase fluffy cats shrewd bankers sell valuable stock 0.016
4 Different Grammatical Structures
So far we have only presented the treatment of sentences with transitive verbs. For sentences with intransitive
verbs, the sentence space suffices to be just N . To compare the meaning of a transitive sentence with an
intransitive one, we embed the meaning of the latter from N into the former N ? N , by taking ???n (the
?object? of an intransitive verb) to be
?
i
??ni , i.e. the superposition of all basis vectors of N .
Following the method for the transitive verb, we calculate Cijk for an instransitive verb V and basis pair??sj = (??ni ,??nk) as follows, where l ranges over the sentences in the corpus:
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni??
?????
obj(v) | ??nk? =
?
l
?
v?verbs(Cl)
?(v, V )?
??????
subj(v) | ??ni?????n | ??nk?
and ????n | ??ni? = 1 for any basis vector ni.
We can now compare the meanings of transitive and intransitive sentences by taking the inner product of
their meanings (despite the different arities of the verbs) and then normalising it by vector length to obtain
the cosine measure. For example:
????????????dogs chase cats | ????????dogs chase? =
?
?
?
?
ijk
Cijk?
???
dogs | ??ni???sj ???nk | ???cats ?
?
?
?
?
?
?
?
?
?
?
?
ijk
C ?ijk?
???
dogs | ??ni???sj
?
?
?
=
?
ijk
CijkC ?ijk?
???
dogs | ??ni??
???
dogs | ??ni????nk | ??cats?
The raw number for the inner product is 14092 and its normalised cosine measure is 0.961, indicating high
similarity (but some difference) between a sentence with a transitive verb and one where the subject remains
the same, but the verb is used intransitively.
Comparing sentences containing nouns modified by adjectives to sentences with unmodified nouns is straight-
forward:
?????????????????????fluffy dogs chase fluffy cats | ???????????dogs chase cats? =
?
ij
Cfluffyi C
fluffy
j Cchaseij Cchaseij ?
???dogs | ??ni?2???nj |
???cats?2 = 2437005
132
From the above we obtain the following similarity measure:
cosine(????????????????????fluffy dogs chase fluffy cats,???????????dogs chase cats) = 0.971
For sentences with ditransitive verbs, the sentence space changes to N ? N ? N , on the basis of the verb
needing two objects; hence its grammatical type changes to nrsnlnl. The transitive and intransitive verbs
are embedded in this larger space in a similar manner to that described above; hence comparison of their
meanings becomes possible.
5 Ambiguous Words
The two different meanings of a word can be distinguished by the different properties that they have. These
properties are reflected in the corpus, by the different contexts in which the words appear. Consider the
following example from [4]: the verb ?catch? has two different meanings, ?grab? and ?contract?. They are
reflected in the two sentences ?catch a ball? and ?catch a disease?. The compositional feature of our meaning
computation enables us to realise the different properties of the context words via the grammatical roles they
take in the corpus. For instance, the word ?ball? occurs as argument of ?round?, and so has a high weight
for the base ?arg-round?, whereas the word ?disease? has a high weight for the base ?arg-contagious? and as
?mod-of-heart?. We extend our example corpus from previously to reflect these differences as follows:
ball disease
1. arg-fluffy 1 0
2. arg-ferocious 0 0
3. obj-buys 5 0
4. arg-shrewd 0 0
5. arg-valuable 1 0
6. arg-round 8 0
7. arg-contagious 0 7
8. mod-of-heart 0 6
In a similar way, we build a matrix for the verb ?catch? as follows:
Ccatch =
?
?
?
?
?
?
?
?
?
?
?
?
3 2 3 3 3 8 6 2
3 2 3 0 1 4 7 4
2 4 7 1 1 6 2 2
3 1 2 0 0 3 6 2
1 1 1 0 0 2 0 1
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
?
?
?
?
?
?
?
?
?
?
?
?
The last three rows are zero because we have assumed that the words that can take these roles are mostly
objects and hence cannot catch anything. Given these values, we compute the similarity measure between
the two sentences ?dogs catch a ball? and ?dogs catch a disease? as follows:
?????????????dogs catch a ball | ??????????????dogs catch a disease? = 0
In an idealised case like this where there is very little (or no) overlap between the properties of the objects
associated with one sense of ?catch? (e.g. a disease), and those properties of the objects associated with an-
other sense (e.g. a ball), disambiguation is perfect in that there is no similarity between the resulting phrases.
133
In practice, in richer vector spaces, we would expect even diseases and balls to share some properties. How-
ever, as long as those shared properties are not those typically held by the object of catch, and as long as the
usages of catch play to distinctive properties of diseases and balls, disambiguation will occur by the same
mechanism as the idealised case above, and we can expect low similarity measures between such sentences.
6 Related Work
Mitchell and Lapata introduce and evaluate a multiplicative model for vector composition [8]. The particular
concrete construction of this paper differs from that of [8] in that our framework subsumes truth-theoretic
as well as corpus-based meaning, and our meaning construction relies on and is guided by the grammatical
structure of the sentence. The approach of [4] is more in the spirit of ours, in that extra information about
syntax is used to compose meaning. Similar to us, they use a structured vector space to integrate lexical
information with selectional preferences. Finally, Baroni and Zamparelli model adjective-noun combinations
by treating an adjective as a function from noun space to noun space, represented using a matrix, as we do
in this paper [1].
References
[1] M. Baroni and R. Zamparelli. Nouns are vectors, adjectives are matrices: Representing adjective-noun construc-
tions in semantic space. In Conference on Empirical Methods in Natural Language Processing (EMNLP-10),
Cambridge, MA, 2010.
[2] S. Clark and S. Pulman. Combining symbolic and distributional models of meaning. In Proceedings of AAAI
Spring Symposium on Quantum Interaction. AAAI Press, 2007.
[3] B. Coecke, M. Sadrzadeh, and S. Clark. Mathematical Foundations for a Compositional Dis-
tributional Model of Meaning, volume 36. Linguistic Analysis (Lambek Festschrift), 2010.
http://arxiv.org/abs/1003.4394.
[4] K. Erk and S. Pado?. A structured vector space model for word meaning in context. In Conference on Empirical
Methods in Natural Language Processing (EMNLP-08), pages 897?906, Honolulu, Hawaii, 2008.
[5] G. Grefenstette. Use of syntactic context to produce term association lists for text retrieval. In Nicholas J. Belkin,
Peter Ingwersen, and Annelise Mark Pejtersen, editors, SIGIR, pages 89?97. ACM, 1992.
[6] Z. Harris. Mathematical Structures of Language. Interscience Publishers John Wiley and Sons, 1968.
[7] J. Lambek. From Word to Sentence. Polimetrica, 2008.
[8] J. Mitchell and M. Lapata. Vector-based models of semantic composition. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguistics, pages 236?244, Columbus, OH, 2008.
[9] P. Smolensky and G. Legendre. The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar
Vol. I: Cognitive Architecture Vol. II: Linguistic and Philosophical Implications. MIT Press, 2005.
134
Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 41?51,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
The Frobenius Anatomy of Relative Pronouns
Stephen Clark
University of Cambridge
Computer Laboratory
sc609@cam.ac.uk
Bob Coecke
University of Oxford
Dept. of Computer Science
coecke@cs.ox.ac.uk
Mehrnoosh Sadrzadeh
Queen Mary, University of London
School of Electronic
Engineering and Computer Science
mehrnoosh.sadrzadeh@eecs.qmul.ac.uk
Abstract
This paper develops a compositional
vector-based semantics of relative pro-
nouns within a categorical framework.
Frobenius algebras are used to formalise
the operations required to model the se-
mantics of relative pronouns, including
passing information between the relative
clause and the modified noun phrase, as
well as copying, combining, and discard-
ing parts of the relative clause. We de-
velop two instantiations of the abstract se-
mantics, one based on a truth-theoretic ap-
proach and one based on corpus statistics.
1 Introduction
Ordered algebraic structures and sequent calculi
have been used extensively in Computer Science
and Mathematical Logic. They have also been
used to formalise and reason about natural lan-
guage. Lambek (1958) used the ordered alge-
bra of residuated monoids to model grammatical
types, their juxtapositions and reductions. Rela-
tional words such as verbs have implicative types
and are modelled using the residuals to the monoid
multiplication. Later, Lambek (1999) simplified
these algebras in favour of pregroups. Here, there
are no binary residual operations, but each element
of the algebra has a left and a right residual.
In terms of semantics, pregroups do not natu-
rally lend themselves to a model-theoretic treat-
ment (Montague, 1974). However, pregroups are
suited to a radically different treatment of seman-
tics, namely distributional semantics (Schu?tze,
1998). Distributional semantics uses vector spaces
based on contextual co-occurrences to model the
meanings of words. Coecke et al (2010) show
how a compositional semantics can be developed
within a vector-based framework, by exploiting
the fact that vector spaces with linear maps and
pregroups both have a compact closed categor-
ical structure (Kelly and Laplaza, 1980; Preller
and Lambek, 2007). Some initial attempts at im-
plementation include Grefenstette and Sadrzadeh
(2011a) and Grefenstette and Sadrzadeh (2011b).
One problem with the distributional approach is
that it is difficult to see how the meanings of some
words ? e.g. logical words such as and, or, and
relative pronouns such as who, which, that, whose
? can be modelled contextually. Our focus in this
paper is on relative pronouns in the distributional
compositional setting.
The difficulty with pronouns is that the contexts
in which they occur do not seem to provide a suit-
able representation of their meanings: pronouns
tend to occur with a great many nouns and verbs.
Hence, if one applies the contextual co-occurrence
methods of distributional semantics to them, the
result will be a set of dense vectors which do
not discriminate between different meanings. The
current state-of-the-art in compositional distribu-
tional semantics either adopts a simple method to
obtain a vector for a sequence of words, such as
adding or mutliplying the contextual vectors of
the words (Mitchell and Lapata, 2008), or, based
on the grammatical structure, builds linear maps
for some words and applies these to the vector
representations of the other words in the string
(Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011a). Neither of these approaches
produce vectors which provide a good representa-
tion for the meanings of relative clauses.
In the grammar-based approach, one has to as-
sign a linear map to the relative pronoun, for in-
stance a map f as follows:
??????????????
men who like Mary = f(???men,
???????
like Mary)
However, it is not clear what this map should be.
Ideally, we do not want it to depend on the fre-
quency of the co-occurrence of the relative pro-
noun with the relevant basis vectors. But both
41
of the above mentioned approaches rely heavily
on the information provided by a corpus to build
their linear maps. The work of Baroni and Zam-
parelli (2010) uses linear regression and approxi-
mates the context vectors of phrases in which the
target word has occurred, and the work of Grefen-
stette and Sadrzadeh (2011a) uses the sum of Kro-
necker products of the arguments of the target
word across the corpus.
The semantics we develop for relative pronouns
and clauses uses the general operations of a Frobe-
nius algebra over vector spaces (Coecke et al,
2008) and the structural categorical morphisms of
vector spaces. We do not rely on the co-occurrence
frequencies of the pronouns in a corpus and only
take into account the structural roles of the pro-
nouns in the meaning of the clauses. The computa-
tions of the algebra and vector spaces are depicted
using string diagrams (Joyal and Street, 1991),
which depict the interactions that occur among the
words of a sentence. In the particular case of rel-
ative clauses, they visualise the role of the rela-
tive pronoun in passing information between the
clause and the modified noun phrase, as well as
copying, combining, and even discarding parts of
the relative clause.
We develop two instantiations of the abstract se-
mantics, one based on a truth-theoretic approach,
and one based on corpus statistics, where for the
latter the categorical operations are instantiated as
matrix multiplication and vector component-wise
multiplication. As a result, we will obtain the fol-
lowing for the meaning of a subject relative clause:
??????????????
men who like Mary = ???men (love?
????
Mary)
The rest of the paper introduces the categorical
framework, including the formal definitions rel-
evant to the use of Frobenius algebras, and then
shows how these structures can be used to model
relative pronouns within the compositional vector-
based setting.
2 Compact Closed Categories and
Frobenius Algebras
This section briefly reviews compact closed cate-
gories and Frobenius algebras. For a formal pre-
sentation, see (Kelly and Laplaza, 1980; Kock,
2003; Baez and Dolan, 1995), and for an informal
introduction see Coecke and Paquette (2008).
A compact closed category has objects A,B;
morphisms f : A ? B; a monoidal tensor A? B
that has a unit I; and for each objectA two objects
Ar andAl together with the following morphisms:
A?Ar
rA?? I
?rA?? Ar ?A
Al ?A
lA?? I
?lA?? A?Al
These morphisms satisfy the following equalities,
sometimes referred to as the yanking equalities,
where 1A is the identity morphism on object A:
(1A ? 
l
A) ? (?
l
A ? 1A) = 1A
(rA ? 1A) ? (1A ? ?
r
A) = 1A
(lA ? 1A) ? (1Al ? ?
l
A) = 1Al
(1Ar ? 
r
A) ? (?
r
A ? 1Ar) = 1Ar
A pregroup is a partial order compact closed
category, which we refer to as Preg. This means
that the objects of Preg are elements of a par-
tially ordered monoid, and between any two ob-
jects p, q ? Preg there exists a morphism of type
p ? q iff p ? q. Compositions of morphisms
are obtained by transitivity and the identities by
reflexivity of the partial order. The tensor of the
category is the monoid multiplication, and the ep-
silon and eta maps are as follows:
rp = p ? p
r ? 1 ?rp = 1 ? p
r ? p
lp = p
l ? p ? 1 ?lp = 1 ? p ? p
l
Finite dimensional vector spaces and linear
maps also form a compact closed category, which
we refer to as FVect. Finite dimensional vector
spaces V,W are objects of this category; linear
maps f : V ? W are its morphisms with compo-
sition being the composition of linear maps. The
tensor product V ?W is the linear algebraic ten-
sor product, whose unit is the scalar field of vec-
tor spaces; in our case this is the field of reals R.
As opposed to the tensor product in Preg, the ten-
sor between vector spaces is symmetric; hence we
have a naturual isomorphism V ?W ?= W ? V .
As a result of the symmetry of the tensor, the two
adjoints reduce to one and we obtain the following
isomorphism:
V l ?= V r ?= V ?
where V ? is the dual of V . When the basis vectors
of the vector spaces are fixed, it is further the case
that the following isomorphism holds as well:
V ? ?= V
42
Elements of vector spaces, i.e. vectors, are rep-
resented by morphisms from the unit of tensor to
their corresponding vector space; that is??v ? V is
represented by the morphism R
??v
?? V ; by linear-
ity this morphism is uniquely defined when setting
1 7? ??v .
Given a basis {ri}i for a vector space V , the ep-
silon maps are given by the inner product extended
by linearity; i.e. we have:
l = r : V ? ? V ? R
given by:
?
ij
cij ?i ? ?j 7?
?
ij
cij??i | ?j?
Similarly, eta maps are defined as follows:
?l = ?r : R? V ? V ?
and are given by:
1 7?
?
i
ri ? ri
A Frobenius algebra in a monoidal category
(C,?, I) is a tuple (X,?, ?, ?, ?) where, for X
an object of C, the triple (X,?, ?) is an internal
comonoid; i.e. the following are coassociative and
counital morphisms of C:
?: X ? X ?X ? : X ? I
Moreover (X,?, ?) is an internal monoid; i.e. the
following are associative and unital morphisms:
? : X ?X ? X ? : I ? X
And finally the ? and ?morphisms satisfy the fol-
lowing Frobenius condition:
(?? 1X) ? (1X ??) = ? ? ? = (1X ? ?) ? (?? 1X)
Informally, the comultiplication ? decomposes
the information contained in one object into two
objects, and the multiplication ? combines the in-
formation of two objects into one.
Frobenius algebras were originally introduced
in the context of representation theorems for group
theory (Frobenius, 1903). Since then, they have
found applications in other fields of mathematics
and physics, e.g. in topological quantum field the-
ory (Kock, 2003). The above general categorical
definition is due to Carboni and Walters (1987). In
what follows, we use Frobenius algebras that char-
acterise vector space bases (Coecke et al, 2008).
In the category of finite dimensional vector
spaces and linear maps FVect, any vector space V
with a fixed basis {??vi}i has a Frobenius algebra
over it, explicitly given by:
? :: ??vi 7?
??vi ?
??vi ? ::
??vi 7? 1
? :: ??vi ?
??vj 7? ?ij
??vi ? :: 1 7?
?
i
??vi
where ?ij is the Kronecker delta.
Frobenius algebras over vector spaces with or-
thonormal bases are moreover isometric and com-
mutative. A commutative Frobenius Algebra satis-
fies the following two conditions for ? : X?Y ?
Y ?X , the symmetry morphism of (C,?, I):
? ?? = ? ? ? ? = ?
An isometric Frobenius Algebra is one that satis-
fies the following axiom:
? ?? = 1
The vector spaces of distributional models have
fixed orthonormal bases; hence they have isomet-
ric commutative Frobenius algebras over them.
The comultiplication ? of an isometric com-
mutative Frobenius Algebra over a vector space
encodes vectors of lower dimensions into vectors
of higher dimensional tensor spaces; this oper-
ation is referred to as copying. In linear alge-
braic terms, ?(??v ) ? V ? V is a diagonal matrix
whose diagonal elements are weights of ??v ? V .
The corresponding multiplication ? encodes vec-
tors of higher dimensional tensor spaces into lower
dimensional spaces; this operation is referred to
as combining. For ??w ? V ? V , we have that
?(??w ) ? V is a vector consisting only of the diag-
onal elements of ??w .
As a concrete example, take V to be a two di-
mensional space with basis {??v1 ,
??v2}; then the ba-
sis of V ?V is {??v1?
??v1 ,
??v1?
??v2 ,
??v2?
??v1 ,
??v2?
??v2}.
For a vector v = a??v1 + b
??n2 in V we have:
?(v) = ?
(
a
b
)
=
(
a 0
0 b
)
= a??v1?
??v1+b
??v2?
??v2
And for a matrix w = a??v1 ?
??v1 + b
??v1 ?
??v2 +
c??v2 ?
??v1 + d
??v2 ?
??v2 in V ? V , we have:
?(w) = ?
(
a b
c d
)
=
(
a
d
)
= a??v1 + d
??v2
43
3 String Diagrams
The framework of compact closed categories and
Frobenius algebras comes with a complete di-
agrammatic calculus that visualises derivations,
and which also simplifies the categorical and vec-
tor space computations. Morphisms are depicted
by boxes and objects by lines, representing their
identity morphisms. For instance a morphism
f : A ? B, and an object A with the identity ar-
row 1A : A? A, are depicted as follows:
f
A
B
A
The tensor products of the objects and mor-
phisms are depicted by juxtaposing their diagrams
side by side, whereas compositions of morphisms
are depicted by putting one on top of the other;
for instance the object A?B, and the morphisms
f ? g and f ? h, for f : A ? B, g : C ? D, and
h : B ? C, are depicted as follows:
f
A
B D
g
C f
A
B
h
C
A B
The  maps are depicted by cups, ? maps
by caps, and yanking by their composition and
straightening of the strings. For instance, the di-
agrams for l : Al ? A ? I , ? : I ? A ? Al and
(l ? 1A) ? (1A ? ?l) = 1A are as follows:
Al
A Al
A
Al A Al = A
The composition of the  and ? maps with other
morphisms is depicted as before, that is by juxta-
posing them one above the other. For instance the
diagrams for the compositions (1Bl ? f) ? 
l and
?l ? (1Al ? f) are as follows:
B
f
A
Bl
Al A
f
B
As for Frobenius algebras, the diagrams for the
monoid and comonoid morphisms are as follows:
(?, ?) (?, ?)
with the Frobenius condition being depicted as:
= =
The defining axioms guarantee that any picture de-
picting a Frobenius computation can be reduced to
a normal form that only depends on the number of
input and output strings of the nodes, independent
of the topology. These normal forms can be sim-
plified to so-called ?spiders?:
=
? ? ?
? ? ?
???
???
In the category FVect, apart from spaces V,W ,
which are objects of the category, we also have
vectors ??v ,??w . These are depicted by their repre-
senting morphisms and as triangles with a number
of strings emanating from them. The number of
strings of a triangle denote the tensor rank of the
vector; for instance, the diagrams for??v ? V,
??
v? ?
V ?W , and
??
v?? ? V ?W ? Z are as follows:
V W WV ZV
Application of a linear map to a vector is de-
picted using composition of their corresponding
morphisms. For instance, for f : V ? W and
??v ? V , the application f(??v ) is depicted by the
composition I
??v
?? V
f
??W .
V
f
W
44
Applications of the Frobenius maps to vectors
are depicted in a similar fashion; for instance
?(??v ? ??v ) is the composition I ? I
??v ???v
?? V ?
V
?
?? V and ?(??v ) is the composition I
??v
??
V
?
?? I , depicted as follows:
V V
V
V
4 Vector Space Interpretations
The grammatical structure of a language is en-
coded in the category Preg: objects are grammat-
ical types (assigned to words of the language) and
morphisms are grammatical reductions (encoding
the grammatical formation rules of the language).
For instance, the grammatical structure of the sen-
tence ?Men love Mary? is encoded in the assign-
ment of types n to the noun phrases ?men? and
?Mary? and nr ? s? nl to the verb ?love?, and in
the reduction map ln ? 1s ? 
r
n. The application
of this reduction map to the tensor product of the
word types in the sentence results in the type s:
(ln ? 1s ? 
r
n)(n? (n
r ? s? nl)? n)? s
To each reduction map corresponds a string dia-
gram that depicts the structure of reduction:
n nrsnl n
Men love Mary
In Coecke et al (2010) the pregroup types and
reductions are interpreted as vector spaces and lin-
ear maps, achieved via a homomorphic mapping
from Preg to FVect. Categorically speaking, this
map is a strongly monoidal functor:
F : Preg? FVect
It assigns vector spaces to the basic types as fol-
lows:
F (1) = I F (n) = N F (s) = S
and to the compound types by monoidality as fol-
lows; for x, y objects of Preg:
F (x? y) = F (x)? F (y)
Monoidal functors preserve the compact structure;
that is the following holds:
F (xl) = F (xr) = F (x)?
For instance, the interpretation of a transitive verb
is computed as follows:
F (nr ? s? nl) = F (nr)? F (s)? F (nl) =
F (n)? ? F (s)? F (n)? = N ? S ?N
This interpretation means that the meaning vector
of a transitive verb is a vector in N ? S ?N .
The pregroup reductions, i.e. the partial order
morphisms of Preg, are interpreted as linear maps:
whenever p ? q in Preg, we have a linear map
f? : F (p) ? F (q). The  and ? maps of Preg are
interpreted as the  and ? maps of FVect. For in-
stance, the pregroup reduction of a transitive verb
sentence is computed as follows:
F (rn ? 1s ? 
r
n) = F (
r
n)? F (1s)? F (
l
n) =
F (n)
? ? F (1s)? F (n)
? = N ? 1S ? N
The distributional meaning of a sentence is ob-
tained by applying the interpretation of the pre-
group reduction of the sentence to the tensor prod-
uct of the distributional meanings of the words
in the sentence. For instance, the distributional
meaning of ?Men love Mary? is as follows:
F (rn ? 1s ? 
l
n)(
???
Men?
???
love?
????
Mary)
This meaning is depictable via the following string
diagram:
N NSN N
Men love Mary
The next section applies these techniques to the
distributional interpretation of pronouns. The in-
terpretations are defined using:  maps, for appli-
cation of the semantics of one word to another; ?
maps, to pass information around by bridging in-
termediate words; and Frobenius operations, for
copying and combining the noun vectors and dis-
carding the sentence vectors.
5 Modelling Relative Pronouns
In this paper we focus on the subject and object
relative pronouns, who(m), which and that. Ex-
amples of noun phrases with subject relative pro-
nouns are ?men who love Mary?, ?dog which ate
cats?. Examples of noun phrases with object rela-
tive pronouns are ?men whom Mary loves?, ?book
45
that John read?. In the final example, ?book? is the
head noun, modified by the relative clause ?that
John read?. The intuition behind the use of Frobe-
nius algebras to model such cases is the following.
In ?book that John read?, the relative clause acts
on the noun (modifies it) via the relative pronoun,
which passes information from the clause to the
noun. The relative clause is then discarded, and
the modified noun is returned. Frobenius algebras
provide the machinery for all of these operations.
The pregroup types of the relative pronouns are
as follows:
nrnsln (subject)
nrnnllsl (object)
These types result in the following reductions:
nr s nl nn nr n sl n
Subject Rel-Pr Verb Object
nr s nlnn nr n nll sl
Object Rel-Pr Subject Verb
The meaning spaces of these pronouns are com-
puted using the mechanism described above:
F (nrnsln) = F (nr)? F (n)? F (sl)? F (n)
= N ?N ? S ?N
F (nrnnllsl) = F (nr)? F (n)? F (nll)? F (sl)
= N ?N ?N ? S
The semantic roles that these pronouns play are
reflected in their categorical vector space mean-
ings, depicted as follows:
Subj:
N N S N
Obj:
N N SN
with the following corresponding morphisms:
Subj: (1N ? ?N ? ?S ? 1N ) ? (?N ? ?N )
Obj: (1N ? ?N ? 1N ? ?S) ? (?N ? ?N )
The diagram of the meaning vector of the sub-
ject relative clause interacting with the head noun
is as follows:
N S N NN N NN S
Subject Rel-Pronoun Verb Object
The diagram for the object relative clause is:
N S NNN N NN S
Object Rel-Pronoun Subject Verb
These diagrams depict the flow of information in
a relative clause and the semantic role of its rel-
ative pronoun, which 1) passes information from
the clause to the head noun via the ? maps; 2) acts
on the noun via the ? map; 3) discards the clause
via the ? map; and 4) returns the modified noun
via 1N . The  maps pass the information of the
subject and object nouns to the verb and to the rel-
ative pronoun to be acted on. Note that there are
two different flows of information in these clauses:
the ones that come from the grammatical structure
and are depicted by maps (at the bottom of the di-
agrams), and the ones that come from the semantic
role of the pronoun and are depicted by ? maps (at
the top of the diagrams).
The normal forms of these diagrams are:
N S N NN
Subject Verb Object
N S N NN
Subject Verb Object
Symbolically, they correspond to the following
morphisms:
(?N ? ?S ? N )
(?????
Subject?
???
Verb?
????
Object
)
(N ? ?S ? ?N )
(?????
Subject?
???
Verb?
????
Object
)
The simplified normal forms will become useful in
practice when calculating vectors for such cases.
6 Vector Space Instantiations
In this section we demonstrate the effect of the
Frobenius operations using two example instan-
tiations. The first ? which is designed perhaps
46
as a theoretical example rather than a suggestion
for implementation ? is a truth-theoretic account,
similar to Coecke et al (2010) but also allow-
ing for degrees of truth. The second is based on
the concrete implementation of Grefenstette and
Sadrzadeh (2011a).
6.1 Degrees of Truth
Take N to be the vector space spanned by a set
of individuals {??n i}i that are mutually orthogo-
nal. For example, ??n 1 represents the individual
Mary, ??n 25 represents Roger the dog,
??n 10 rep-
resents John, and so on. A sum of basis vec-
tors in this space represents a common noun; e.g.
???man =
?
i
??n i, where i ranges over the basis vec-
tors denoting men. We take S to be the one dimen-
sional space spanned by the single vector
??
1 . The
unit vector spanning S represents truth value 1, the
zero vector represents truth value 0, and the inter-
mediate vectors represent degrees of truth.
A transitive verb w, which is a vector in the
space N ? S ?N , is represented as follows:
w :=
?
ij
??n i ? (?ij
??
1 )???n j
if ??n i w?s
??n j with degree ?ij , for all i, j.
Further, since S is one-dimensional with its
only basis vector being
??
1 , the transitive verb can
be represented by the following element ofN?N :
?
ij
?ij
??n i?
??n j if
??n i w?s
??n j with degree ?ij
Restricting to either ?ij = 1 or ?ij = 0 provides
a 0/1 meaning, i.e. either ??n i w?s
??n j or not.
Letting ?ij range over the interval [0, 1] enables
us to represent degrees as well as limiting cases
of truth and falsity. For example, the verb ?love?,
denoted by love, is represented by:
?
ij
?ij
??n i?
??n j if
??n i loves
??n jwith degree?ij
If we take ?ij to be 1 or 0, from the above we
obtain the following:
?
(i,j)?Rlove
??n i ?
??n j
where Rlove is the set of all pairs (i, j) such that
??n i loves
??n j .
Note that, with this definition, the sentence
space has already been discarded, and so for this
??????????????????
Subject who Verb Object :=
(?N ? N )
(?????
Subject?
???
Verb?
????
Object
)
=
(?N ? N )
?
?
?
k?K
??n k ?(
?
ij
?ij
??n i?
??n j)?
?
l?L
??n l
?
?
=
?
ij,k?K,l?L
?ij?N (
??n k ?
??n i)? N (
??n j ?
??n l)
=
?
ij,k?K,l?L
?ij?ki
??n i?jl
=
?
k?K,l?L
?kl
??n k
Figure 1: Meaning computation with a subject rel-
ative pronoun
instantiation the ? map, which is the part of the
relative pronoun interpretation designed to discard
the relative clause after it has acted on the head
noun, is not required.
For common nouns
?????
Subject =
?
k?K
??n k and
????
Object =
?
l?L
??n l, where k and l range over
the sets of basis vectors representing the respec-
tive common nouns, the truth-theoretic meaning of
a noun phrase modified by a subject relative clause
is computed as in Figure 1. The result is highly in-
tuitive, namely the sum of the subject individuals
weighted by the degree with which they have acted
on the object individuals via the verb. A similar
computation, with the difference that the ? and 
maps are swapped, provides the truth-theoretic se-
mantics of the object relative clause:
?
k?K,l?L
?kl
??n l
The calculation and final outcome is best under-
stood with an example.
Now only consider truth values 0 and 1. Con-
sider the noun phrase with object relative clause
?men whom Mary loves? and takeN to be the vec-
tor space spanned by the set of all people; then the
males form a subspace of this space, where the ba-
sis vectors of this subspace, i.e. men, are denoted
by ??ml, where l ranges over the set of men which
we denote byM . We set ?Mary? to be the individ-
ual
??
f 1, ?men? to be the common noun
?
l?M
??ml,
47
????????????????
men whom Mary loves :=
(N ? ?N )
?
?
??
f 1 ? (
?
(i,j)?Rlove
??
f i ?
??mj)?
?
l?M
??ml
?
?
=
?
l?M,(i,j)?Rlove
N (
??
f 1 ?
??
f i)? ?(
??mj ?
??ml)
=
?
l?M,(i,j)?Rlove
?1i?jl
??mj
=
?
(1,j)?Rlove|j?M
??mj
Figure 2: Meaning computation for example ob-
ject relative clause
and ?love? to be as follows:
?
(i,j)?Rlove
??
f i ?
??mj
The vector corresponding to the meaning of ?men
whom Mary loves? is computed as in Figure 2.
The result is the sum of the men basis vectors
which are also loved by Mary.
The second example involves degrees of truth.
Suppose we have two females Mary
??
f 1 and Jane??
f 2 and four men
??m1,
??m2,
??m3,
??m4. Mary loves
??m1 with degree 1/4 and
??m2 with degree 1/2; Jane
loves ??m3 with degree 1/5; and
??m4 is not loved. In
this situation, we have:
Rlove = {(1, 1), (1, 2), (2, 3)}
and the verb love is represented by:
1/4(
??
f 1?
??m1)+1/2(
??
f 1?
??m2)+1/5(
??
f 2?
??m3)
The meaning of ?men whom Mary loves? is com-
puted by substituting an ?1,j in the last line of Fig-
ure 2, resulting in the men whom Mary loves to-
gether with the degrees that she loves them:
?
(1,j)?Rlove|j?M
?1j
??mj = 1/4
??m1 + 1/2
??m2
?men whom women love? is computed as fol-
lows, where W is the set of women:
?
k?W,l?M,(i,j)?Rlove
?ijN (
??
f k ?
??
f i)? ?(
??mj ?
??ml)
=
?
k?W,l?M,(i,j)?Rlove
?ij?ki?jl
??mj
=
?
(i,j)?Rlove|i?W,j?M
?ij
??mj
= 1/4??m1 + 1/2
??m2 + 1/5
??m3
The result is the men loved by Mary or Jane to-
gether with the degrees to which they are loved.
6.2 A Concrete Instantiation
In the model of Grefenstette and Sadrzadeh
(2011a), the meaning of a verb is taken to be ?the
degree to which the verb relates properties of its
subjects to properties of its object?. Clark (2013)
provides some examples showing how this is an
intuitive defintion for a transitive verb in the cat-
egorical framework. This degree is computed by
forming the sum of the tensor products of the sub-
jects and objects of the verb across a corpus, where
w ranges over instances of the verb:
verb =
?
w
(
??
sbj?
??
obj)w
Denote the vector space of nouns by N ; the above
is a matrix in N ? N , depicted by a two-legged
triangle as follows:
N N
The verbs of this model do not have a sentence
dimension; hence no information needs to be dis-
carded when they are used in our setting, and so no
?map appears in the diagram of the relative clause.
Inserting the above diagram in the diagrams of the
normal forms results in the following for the sub-
ject relative clause (the object case is similar):
N N NN
Subject Verb Object
The abstract vectors corresponding to such dia-
grams are similar to the truth-theoretic case, with
the difference that the vectors are populated from
corpora and the scalar weights for noun vectors
48
are not necessarily 1 or 0. For subject and object
noun context vectors computed from a corpus as
follows:
?????
Subject =
?
k
Ck
??n k
????
Object =
?
l
Cl
??n l
and the verb a linear map:
Verb =
?
ij
Cij
??n i ?
??n j
computed as above, the concrete meaning of a
noun phrase modified by a subject relative clause
is as follows:
?
kijl
CkCijCl?N (
??n k ?
??n i)N (
??n j ?
??n l)
=
?
kijl
CkCijCl?ki
??n k?jl
=
?
kl
CkCklCl
??n k
Comparing this to the truth-theoretic case, we see
that the previous ?kl are now obtained from a cor-
pus and instantiated to CkCklCl. To see how the
above expression represents the meaning of the
noun phrase, decompose it into the following:
?
k
Ck
??n k 
?
kl
CklCl
??n l
Note that the second term of the above, which is
the application of the verb to the object, modifies
the subject via point-wise multiplication. A simi-
lar result arises for the object relative clause case.
As an example, suppose that N has two dimen-
sions with basis vectors ??n 1 and
??n 2, and consider
the noun phrase ?dog that bites men?. Define the
vectors of ?dog? and ?men? as follows:
??
dog = d1
??n 1+d2
??n 2
???men = m1
??n 1+m2
??n 2
and the matrix of ?bites? by:
b11??n 1???n 2+b12??n 1???n 2+b21??n 2???n 1+b22??n 2???n 2
Then the meaning of the noun phrase becomes:
?????????????
dog that bites men :=
d1b11m1??n 1 + d1b12m2??n 1 + d2b21m1??n 2
+ d2b22m2
??n 2 = (d1
??n 1 + d2
??n 2)
((b11m1 + b12m2)
??n 1 + (b21m1 + b22m2)
??n 2)
Using matrix notation, we can decompose the sec-
ond term further, from which the application of the
verb to the object becomes apparent:
(
b11 b12
b21 b22
)
?
(
m1
m2
)
Hence for the whole clause we obtain:
??
dog (bites????men)
Again this result is highly intuitive: assuming
that the basis vectors of the noun space represent
properties of nouns, the meaning of ?dog that bites
men? is a vector representing the properties of
dogs, which have been modified (via multiplica-
tion) by those properties of individuals which bite
men. Put another way, those properties of dogs
which overlap with properties of biting things get
accentuated.
7 Conclusion and Future Directions
In this paper, we have extended the compact cate-
gorical semantics of Coecke et al (2010) to anal-
yse meanings of relative clauses in English from
a vector space point of view. The resulting vec-
tor space semantics of the pronouns and clauses
is based on the Frobenius algebraic operations on
vector spaces: they reveal the internal structure, or
what we call anatomy, of the relative clauses.
The methodology pursued in this paper and the
Frobenius operations can be used to provide se-
mantics for other relative pronouns and also other
closed-class words such as prepositions and deter-
miners. In each case, the grammatical type of the
word and a detailed analysis of the role of these
words in the meaning of the phrases in which they
occur would be needed. In some cases, it may be
necessary to introduce a linear map to represent
the meaning of the word, for instance to distin-
guish the preposition on from in.
The contribution of this paper is best demon-
strated via the string diagrammatic representations
of the vector space meanings of these clauses. A
noun phrase modified by a subject relative clause,
which before this paper was depicted as follows:
N S N NN N NN S
Subject Rel-Pronoun Verb Object
will now include the internal anatomy of its rela-
tive pronoun:
49
N S N NN N NN S
Subject Rel-Pronoun Verb Object
This internal structure shows how the information
from the noun flows through the relative pronoun
to the rest of the clause and how it interacts with
the other words. We have instantiated this vector
space semantics using truth-theoretic and corpus-
based examples.
One aspect of our example spaces which means
that they work particularly well is that the sen-
tence dimension in the verb is already discarded,
which means that the ? maps are not required (as
discussed above). Another feature is that the sim-
ple nature of the models means that the ?map does
not lose any information, even though it takes the
diagonal of a matrix and hence in general throws
information away. The effect of the ? and ? maps
in more complex representations of the verb re-
mains to be studied in future work.
On the practical side, what we offer in this paper
is a method for building appropriate vector repre-
sentations for relative clauses. As a result, when
presented with a relative clause, we are able to
build a vector for it, only by relying on the vector
representations of the words in the clause and the
grammatical role of the relative pronoun. We do
not need to retrieve information from a corpus to
be able to build a vector or linear map for the rela-
tive pronoun, neither will we end up having to dis-
card the pronoun and ignore the role that it plays in
the meaning of the clause (which was perhaps the
best option available before this paper). However,
the Frobenius approach and our claim that the re-
sulting vectors are ?appropriate? requires an empir-
ical evaluation. Tasks such as the term definition
task from Kartsaklis et al (2013) (which also uses
Frobenius algebras but for a different purpose) are
an obvious place to start. More generally, the sub-
field of compositional distributional semantics is
a growing and active one (Mitchell and Lapata,
2008; Baroni and Zamparelli, 2010; Zanzotto et
al., 2010; Socher et al, 2011), for which we argue
that high-level mathematical investigations such
as this paper, and also Clarke (2008), can play a
crucial role.
Acknowledgements
We would like to thank Dimitri Kartsaklis and
Laura Rimell for helpful comments. Stephen
Clark was supported by ERC Starting Grant Dis-
CoTex (30692). Bob Coecke and Stephen Clark
are supported by EPSRC Grant EP/I037512/1.
Mehrnoosh Sadrzadeh is supported by an EPSRC
CAF EP/J002607/1.
References
J.C. Baez and J. Dolan. 1995. Higher-dimensional al-
gebra and topological quantum field theory. Journal
of Mathematical Physics, 36:6073?6105.
M. Baroni and R. Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-10), Cambridge, MA.
A. Carboni and R. F. C. Walters. 1987. Cartesian bicat-
egories. I. J. Pure and Appied Algebra, 49:11?32.
S. Clark. 2013. Type-driven syntax and seman-
tics for composing meaning vectors. In Chris He-
unen, Mehrnoosh Sadrzadeh, and Edward Grefen-
stette, editors, Quantum Physics and Linguistics:
A Compositional, Diagrammatic Discourse, pages
359?377. Oxford University Press.
D. Clarke. 2008. Context-theoretic Semantics for Nat-
ural Language: An Algebraic Framework. Ph.D.
thesis, University of Sussex.
B. Coecke and E. Paquette. 2008. Introducing cat-
egories to the practicing physicist. In B. Coecke,
editor, New Structures for Physics, volume 813 of
Lecture Notes in Physics, pages 167?271. Springer.
B. Coecke, D. Pavlovic, and J. Vicary. 2008. A
new description of orthogonal bases. Mathematical
Structures in Computer Science, 1:269?272.
B. Coecke, M. Sadrzadeh, and S. Clark. 2010.
Mathematical foundations for a compositional dis-
tributional model of meaning. Linguistic Analysis,
36:345?384.
F. G. Frobenius. 1903. Theorie der hyperkomplexen
Gro??en. Preussische Akademie der Wissenschaften
Berlin: Sitzungsberichte der Preu?ischen Akademie
der Wissenschaften zu Berlin. Reichsdr.
E. Grefenstette and M. Sadrzadeh. 2011a. Experimen-
tal support for a categorical compositional distribu-
tional model of meaning. In Proceedings of Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1394?1404.
E. Grefenstette and M. Sadrzadeh. 2011b. Experi-
menting with transitive verbs in a discocat. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics (GEMS).
50
A. Joyal and R. Street. 1991. The Geometry of Tensor
Calculus, I. Advances in Mathematics, 88:55?112.
D. Kartsaklis, M. Sadrzadeh, S. Pulman, and B. Co-
ecke. 2013. Reasoning about meaning in nat-
ural language with compact closed categories and
frobenius algebras. In J. Chubb, A. Eskandar-
ian, and V. Harizanov, editors, Logic and Algebraic
Structures in Quantum Computing and Information,
Association for Symbolic Logic Lecture Notes in
Logic. Cambridge University Press.
G. M. Kelly and M. L. Laplaza. 1980. Coherence for
compact closed categories. Journal of Pure and Ap-
plied Algebra, 19:193?213.
J. Kock. 2003. Frobenius algebras and 2D topological
quantum field theories, volume 59 of London Mathe-
matical Society student texts. Cambridge University
Press.
J. Lambek. 1958. The Mathematics of Sentence Struc-
ture. American Mathematics Monthly, 65:154?170.
J. Lambek. 1999. Type Grammar Revisited Logical
Aspects of Computational Linguistics. In Logical
Aspects of Computational Linguistics, volume 1582
of Lecture Notes in Computer Science, pages 1?27.
Springer Berlin / Heidelberg.
J. Mitchell and M. Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of ACL-
08, pages 236?244, Columbus, OH.
R. Montague. 1974. English as a formal language.
In R. H. Thomason, editor, Formal philosophy: Se-
lected Papers of Richard Montague, pages 189?223.
Yale University Press.
A. Preller and J. Lambek. 2007. Free compact 2-
categories. Mathematical Structures in Computer
Science, 17:309?340.
H. Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24:97?123.
R. Socher, J. Pennington, E. Huang, A. Y. Ng, and
C. D. Manning. 2011. Semi-supervised recur-
sive autoencoders for predicting sentiment distribu-
tions. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, Edin-
burgh, UK.
F. M. Zanzotto, I. Korkontzelos, F. Fallucchi, and
S. Manandhar. 2010. Estimating linear models for
compositional distributional semantics. In Proceed-
ings of COLING, Beijing, China.
51
