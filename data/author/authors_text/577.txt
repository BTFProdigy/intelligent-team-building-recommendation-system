The MATE Markup Framework 
Laila DYBKJ2,ER and Niels Ole BERNSEN 
Natural Interactive Systems Laboratory, University of Southern Denmark. 
Science Park 10, 5230 Odense M, Denmark 
laila@nis.sdu.dk, nob@nis.sdu.dk 
Abstract 
Since early 1998, the European Telematics 
project MATE has worked towards 
facilitating re-use of annotated spoken 
language data, addressing theoretical issues 
and implementing practical solutions which 
could serve as standards in the field. The 
resulting MATE Workbench for corpus 
annotation is now available as licensed open 
source software. 
This paper describes the MATE markup 
framework which bridges between the 
theoretical and the practical activities of 
MATE and is proposed as a standard for the 
definition and representation f markup for 
spoken dialogue corpora. We also present 
early experience from use of the framework. 
1. Introduction 
Spoken language engineering products 
proliferate in the market, commercial and 
research applications constantly increasing in 
variety and sophistication. These developments 
generate a growing need for tools and standards 
which can help improve the quality and 
efficiency of product development and 
evaluation. In the case of spoken language 
dialogue systems (SLDSs), for instance, the 
need is obvious for standards and standard-based 
tools for spoken dialogue corpus annotation and 
automatic infomaation extraction. Information 
extraction from annotated corpora is used in 
SLDSs engineering for many different purposes. 
For several years, annotated speech corpora have 
been used to train and test speech recognisers. 
More recently, corpus-based approaches are 
being applied regularly to other levels of 
processing, such as syntax and dialogue. For 
instance, annotated corpora can be used to 
construct lexicons and grammars or train a 
grammar to acquire preferences for frequently 
used rules. Similarly, programs for dialogue act 
recognition and prediction tend to be based on 
annotated corpus data. Evaluation of user- 
system interaction and dialogue success is also 
based on annotated corpus data. As SLDSs and 
other language products become more 
sophisticated, the demand will grow for corpora 
with multilevel and cross-level annotations, i.e. 
annotations which capture information in the 
raw data at several different conceptual levels or 
mark up phenomena which refer to more than 
one level. These developments will inevitably 
increase the demand for standard tools in 
support of the annotation process. 
The production (recording, transcription, 
annotation, evaluation) of corpus data fer spoken 
language applications continues to be time- 
consuming and costly. So is the construction of 
tools which facilitate annotation and information 
extraction. It is therefore desirable that already 
available annotated corpora and tools be used 
whenever possible. Re-use of annotated data and 
tools, however, confronts systems developers 
with numerous problems which basically derive 
from the lack of common standards. So far, 
language engineering projects usually have 
either developed the needed resources from 
scratch using homegrown formalisms and tools, 
or painstakingly adapted resources from 
previous projects to novel purposes. 
In recent years, several projects have addressed 
annotation formats and tools in support of 
annotation and information extraction (for an 
overview, see http://www.ldc.uperm.edu/- 
annotation/). Some projects have addressed the 
issue of markup standardisation from different 
perspectives. Examples are the Text Encoding 
Initiative (TEI) 0attp://www-tei.uic.edu/orgs/tei/ 
and http://etext.virginia.eduffEI.html), the 
Corpus Encoding Standard (CES) 
(http://www.cs.vassar.edu/CES/), and the 
European Advisory Group for Language 
Engineering Standards (EAGLES) (http://www.- 
19 
ilc.pi.cnr.it/EAGLES96/home.hurd). Whilst 
these initiatives have made good progress on 
written language and current coding practice, 
none of them have focused on the creation of 
standards and tools for cross-level spoken 
language corpus annotation. It is only recently 
that there has been a major effort in this domain. 
The project Multi-level ~anotafion Tools 
Engineering (MATE)  (http://mate.nis.sdu.dk) 
was launched in March 1998 in response to the 
need for standards and tools in support of 
creating, annotating, evaluating and exploiting 
spoken language resources. The central idea of 
MATE has been to work on both annotation 
theory and practice in order to connect the two 
through a flexible framework which can ensure a
common and user-friendly approach across 
annotation levels. On the tools side, this means 
that users are able to use level-independent tools 
and an interface representation which is 
independent of the internal coding file 
representation. 
This paper presents the MATE markup 
framework and its use in the MATE Workbench. 
In the following, Section 2 briefly reviews the 
MATE approach to annotation and tools 
standardisafion. Section 3 presents the MATE 
markup framework. Section 4 concludes the 
paper by reporting on early experiences with the 
practical use of the markup framework and 
discussing future work. " 
2 The MATE Approach 
This section first briefly describes the creation of 
the MATE markup framework and a set of 
example best practice coding schemes in 
accordance with the markup framework. Then it 
describes how a toolbox (the MATE 
Workbench) has been implemented to support 
the markup framework by enabling annotation 
on the basis of any coding scheme expressed 
according to the framework. 
2.1 Theory 
The theoretical objectives of MATE were to 
specify a standard markup framework and to 
identify or, when necessary, develop a series of 
best practice coding schemes for implementation 
in the MATE Workbench. To these ends, we 
began by collecting information on a large 
number of existing annotation schemes for the 
levels addressed in the project, i.e. prosody, 
(morpho-)syntax, co-reference, dialogue acts, 
communication problems, and cross-level issues. 
Cross-level issues are issues which relate to 
more than one annotation level. Thus, for 
instance, prosody may provide clues for a 
variety of phenomena in semantics and 
discourse. The resulting report (Klein et al, 
1998) describes more than 60 coding schemes, 
giving details per scheme on its coding book, the 
number of annotators who have worked with it, 
the number of annotated ialogues/segments/ 
utterances, evaluation results, the underlying 
task, a list of annotated phenomena, nd the 
markup language used. Annotation examples are 
provided as well. 
We found that the amount of pre-existing work 
varies enormously from level to level. There 
was, moreover, considerable variation in the 
quality of the descriptions of the individual 
coding schemes we analysed. Some did not 
include a coding book, others did not provide 
appropriate xamples, some had never been 
properly evaluated, etc. The differences in 
description made it extremely difficult to 
compare coding schemes even for the same 
annotation level, and constituted a rather 
confused and incomplete basis for the creation 
of standard re-usable tools within, as well as 
across, levels. 
The collected information formed the starting 
point for the devebpment of the MATE markup 
framework which is a proposal for a standard for 
the definition and representation f markup for 
spoken dialogue corpora (Dybkjmr et al, 1998). 
Analysis of the collected information on existing 
coding schemes as regards the information 
which came with the schemes as well as the 
information which was found missing, provided 
input to our proposal for a minimal set of 
information items which should be provided for 
a coding scheme to make it generally 
comprehensible and re-usable by others. For 
instance, a prescriptive coding procedure was 
included among the information items in the 
MATE markup framework despite the fact that 
most existing coding schemes did not come with 
this information. This list of information items 
which we call a coding module, is the core 
concept of the MATE markup framework and 
extends and formalises the concept of a coding 
scheme. The ten entries which constitute a 
coding module are shown in Figure 4. Roughly 
20 
speaking, a coding module includes or describes 
everything that is needed in order to perform a 
certain kind of markup of spoken language 
corpora. A coding module prescribes what 
constitutes a coding, including the representation 
of markup and the relations to other codings. 
Thus, the MATE coding module is a proposal 
for a standardised description of coding 
schemes. 
The above-mentioned five annotation levels 
and the issues to do with cross-level annotation 
were selected for consideration in MATE 
because they pose very different markup 
problems. If a common framework can be 
established and shown to work for those levels 
and across them, it would seem likely that the 
framework will work for other levels as well. 
For each annotation level, one or more existing 
coding schemes were selected to form the basis 
of the best practice coding modules 
implemented in the MATE Workbench (Mengel 
et al, 2000). Common to the selected coding 
schemes i that these are among the most widely 
used coding schemes for their respective l vels 
in current practice, each having been used by 
several annotators and for the annotation of 
many dialogues. Since all MATE best practice 
coding schemes are expressed in terms of coding 
modules, they should contain sufficient 
information for use by other annotators. Their 
uniform description in terms of coding modules 
makes it easy for the annotator to work on 
multiple coding schemes and/or levels, and to 
compare schemes ince these all contain the 
same categories of information. The use of 
coding modules also facilitates use of the same 
set of software tools and enables the same 
interface look-and-feel independently of level. 
2.2 Tooling 
The engineering objective of MATE has been to 
specify and implement a genetic annotation tool 
in support of the markup framework and the 
selected best practice coding schemes. Several 
existing annotation tools were reviewed early on 
to gather input for MATE workbench 
specification (Isard et al, 1998). Building on 
this specification, the MATE markup framework 
and the selected coding schemes, a java-based 
workbench as been implemented (Isard et al, 
2000) which includes the following major 
functionalities: 
The MATE best practice coding modules are 
included as working examples of the state of the 
art. Users can add new coding modules via the 
easy-to-use interface of the MATE coding 
module ditor. 
An audio tool enables listening to speech files 
and having sound files displayed as a waveform. 
For each coding module, a default stylesheet 
defines how output to the user is presented 
visually. Phenomena of interest in the corpus 
may be shown in, e.g., a certain colour or in 
boldface. Users can modify style sheets and 
define new ones. 
The workbench enables information extraction 
of any kind from annotated corpora. Query 
results are shown as sets of references to the 
queried corpora. Extraction of statistical 
information from corpora, such as the number of 
marked-up nouns, is also supported. 
Computation of important reliability measures, 
such as kappa values, is enabled. 
Import of files from XLabels and BAS Partitur 
to XML format is supported in order to 
demonstrate he usefulness of importing widely 
used annotation formats for further work in the 
Workbench. Similarly, a converter from 
Transcriber format (http://www.etca.fr/CTA/- 
gip/Projets/Transcriber/) to MATE format 
enables transcriptions made using Transcriber to 
be annotated using the MATE Workbench. 
Other converters can easily be added. Export to 
file formats other than XML can be achieved by 
using style sheets. For example, information 
extracted by the query tool may be exported to 
HTML to serve as input to a browser. 
On-line help is available at any time. 
The first release of the MATE Workbench 
appeared in November 1999 and was made 
available to the +80 members of the MATE 
Advisory Panel from across the world. Since 
then, several improved versions have appeared 
and in May 2000 access to executable versions 
of the Workbench was made public. The MATE 
Workbench is now publicly available both in an 
executable version and as open source software 
at http://mate.nis.sdu.dk. The Workbench is still 
being improved by the MATE consortium, so 
new versions will continue to appear. A 
discussion forum has recently been set up at the 
MATE web site where colleagues are invited to 
ask questions and provide information from their 
experience with the Workbench, including the 
21 
new tools they have added to the MATE 
Workbench to enhance its functionality. ? 
We have no exact figures on \]how many users 
are now using the workbench but we know that 
the MATE workbench is already being used by 
and is being considered for use in several 
European and national research projects. 
3 The MATE Markup Framework 
The MATE markup framework is a conceptual 
model which basically prescribes (i) how files 
are structured, for instance to enable multi-level 
annotation, (ii) how tag sets arc; represented in 
terms of elements and attributes, and (iii) how to 
provide essential information on markup, 
semantics, coding purpose tc. 
3.1 Files, elements and attributes 
When a coding module has been applied to a 
corpus, the result is a coding file. The coding file 
has a header which documents the coding 
context, such as who annotated the file, when, 
and the experience of the annotator, and a body 
which lists the coded elements. Figure 1 shows 
an example of how annotated communication 
problems are displayed to the user in the MATE 
Workbench. Figure 2 shows an excerpt of the 
internal representation f the file. 
~Clirl'rlilu i ru lon  licil:dlllrn ; :  oomph: l i t  i~rl~,~l~zloguo 
~.I -,idlor iz ~ ~ ponglll 
+r 
"r 
~s lhem are e llm~...r.,,~ at pt~o!21e ~ ~etll~i l l : t  I st ~oill,' lGG3t 
~l,ri It 
{: 
M11 ! 
O n I'~ W'~t~/~ili '~tlliltl ~ l'l~Sl~t'll IOhra~ 0 l IS II l l l l  t!l if'PIG 
I I  i l l l r~  il l l l lrf iinll 'l'l~ ~ l lm ~a l  i lo l  ~l l r l l i l  ol,t tllil ~ 
azlmd i : r in  Izt'l_ 5 l  ~ s ~ "ll'izl o I~ ti,1~"e: 2 lllrOl..l:ls or  ll~l~lile Crom 
I.~mdus I~mo ~mr,,t,d: 
!'.~.. O I l t t '~r ' l~ ",i~'11~ lidP ~<31~'r,l10ihrg~llz ~ ile trlie !~ l f?~ 
t~i I I I  It'll, (ll}lth\] ilfflll 1 t  $ I'4~15 fi'l hll$ lioilPl$~ie~: l~fl~l~l~ +.'lJ~ i 
IYCIrY~ E:~lmdUl h i  linllald ; l l t i i  ~ ll~lt~li ~ t tot ! l l l  I~,toiYI 
NIO 
Nl l  
commilr~,i,, expiml~t. 
Prc~nde Jrnrri~:llale " I I~  llai:ll, 
Donl ~.:~, qoo mti4.  
DOnll la, 
os mlr t~.  
in ~..Yfi rierfl inNrrnnton p~. ided to  ~ u~er on ~ 
? ~o ien{  gloup~s o\[ pl~opk~ l'~ 10?~1t~ d+l~in IrlOl C!e i~ '~.  Ojlly b 
l f l P~ i '+ l t l l~t l l+  ?I f l l t ( l  Oul!e~ICl lOf l~t ld l i+ l i l l  ~er  I~s~ 
Ilffll n'lIolcl'l ~lor oiql~li~cl II1 i~  pl'~%(llll~ I l i l~ranl~. 
: . . . .  
Figure 1. A screen shot from the MATE Workbench showing adialogue annotated with 
communication problems (top left-hand panel). Guidelines for cooperative dialogue behaviour are 
shown in the top fight-hand panel. Communication problems are categonsed astypes of violations of 
the coopemtivity guidelines. Violation types are shown in the bottom fight-hand panel. Notes may be 
added as part of the annotation. Notes are shown in the bottom left-hand panel. 
!?t, r r i~  b iti-'Ct?.l~ hqa-'!~JO.tl_m~llttr..~-7~. :- 
',c'?1t"I~ t J.,.'CT'1117 i"r~,,, ' , ,~l ~ 9,~;~ IK 14?. ~t 01 . '1~ 
!~Y.,OTllpt Oh- 
~?cm'ri0vo~ I it=" C~= 317" t-~+~,,ASt~l.,.+ +;iI,l,,'t4r;67)" ~ , 
,e4*'~?i~=%'17~ 0 ? l-vc .~,llbly p~l_11~ alil< O.O~l - ' i~  
-Rompi, oo* ? 
Figure 2. Excerpt of the internal XML 
representation f the annotated dialogue shown 
in Figure 1. The tags will be explained in 
Section 3.1.1 below. 
As shown in Figure 2, the annotated file 
representation is simply a list of references to 
the transcription file. The underlying file 
structure idea is depicted in Figure 3 which 
shows how coding files (bottom layer) refer to a 
transcription file and possibly to other coding 
files, cf. entry 5 in the coding module in Figure 
4. A transcription (which is also regarded as a 
coding file) refers to a resource file listing the 
mw data resources behind the corpus, such as 
sound files and log files. The resource file 
includes a description of the corpus: purpose of 
the dialogues, dialogue participants, 
experimenters, recording conditions, etc. A 
basic, sequential tirneline representation f the 
22 
spoken language data is defined. The firneline 
may be expressed as real time, e.g. in 
milliseconds, or as numbers indicating, e.g., the 
sequence of utterance starts. 
Raw data 
Resource file 
Transcriptions 
(coding files) 
Coding files 
Sound files I ~ video r pictures \[
Orthographic Phonetic 
\[ pCr?o~e~iCafi?n~----~ \[ Dialogue acts ~-----~\[ Prosody 
I 
Figure 3. The raw corpus data are listed in the resource file to which transcriptions refer. Coding files 
at levels other than transcription refer to a transcription and only indkectly to the raw data. Coding 
files may refer to each other. 
Given a coding purpose, such as to identify all 
communication problems in a particular corpus, 
and a coding module, the actual coding consists 
in using syntactic markup to encode the relevant 
phenomena found in the data. A coding is 
defined in terms of a tag set. The tag set is 
conceptually specified by, and presented to, the 
user in terms of elements and attributes, el. entry 
6 in the coding module in Figure 4. Importantly, 
workbench users can use this markup directly 
without having to know about complex formal 
standards, uch as SGML, XML or TEI. 
3.1.1 Elements 
The basic markup primitive is the dement (a 
term inherited from TEI and SGML) which 
represents a phenomenon such as a particular 
phoneme, word, utterance, dialogue act, or 
communication problem. Elements have 
attributes and relations to each other both within 
the cu~ent coding module and across coding 
modules. Considering a coding module M, the 
markup specification language is described as: 
* El ...E,: The non-empty list of tag elements. 
? For each element ~, the following properties 
may be defined: 
1. Ni: The name of El. 
Example: <u> 
2. Ei may contain a list of  elements ~ from 
M. 
Example: <u> may contain <t>: 
<u><t>Exarnple</t></u> 
3. Ei has ~ attributes Aij, where j = 1 .. n~. 
Example: <u> has attributes who and id, 
among others. 
4. Ei may refer to elements in coding module 
Mj, implying that M references Mj. 
Example: a dialogue act coding may refer 
to phonetic or syntactic ues. 
A concrete example is the coding module for 
communication problems which, i.a., has the 
element <eomprob>, el. the XML representation 
in Figure 2. <eomprob> has, i.a., the attributes id 
uref and vtype, uref is a reference to an utterance 
in the transcription coding, xtype is a reference 
to a type of violation of a guideline in the 
violation type coding. Due to the inflexibility of 
XML, this logical structure has to be represented 
slightly differently internally in the workbench. 
Thus, the urcf corresponds to the first href in 
23 
Figure 2 while vtype is wrapped up in a new 
element and corresponds to the second href. 
3.1.2 Attributes 
Attributes are assigned values during coding. 
For each attribute Aij the type of its values must 
be defined. There are standard attributes, user- 
defined attributes, and hidden attributes, as 
follows. 
Standard attributes are attributes prescribed by 
MATE. 
o id \[mandatory\]: ID. The element id is 
composed of the element name and a machine- 
generated number. 
Example: id=r~123 
Time start and end are optiorml. Elements must 
have time information, possibly indirectly by 
referencing other elements (in the same coding 
module or in other modules) which have time 
information. 
? TimeStart \[optional\]: TIME. Start of event. 
? YimeEnd \[optional\]: TIME. End of event. 
User-defined attributes are used to parametrise 
and extend the semantics of the elements they 
belong to. For instance, who is an attribute of 
element <u> designating by whom the utterance 
is spoken. There will be many user-defined 
attributes (and elements), el., e.g., the uref and 
vtype mentioned above. 
Hidden attributes are attributes which the user 
will neither define nor see but Which are used for 
internal representation purposes. An example is 
the following of coding elements which may 
refer to utterances in a transcription but which 
depend on the technical programming choice of 
the underlying, non-user related representation: 
ModuleRefs CDATA 'href:transcription#u' 
See Figure 2 for a concrete xample from the 
MATE Workbench. 
3.1.3 Attribute standard types 
The MATE markup framework proposes a set of 
predefined attribute value types (attributes are 
typed) which are supported by the workbench. 
By convention, types are written in capitals. The 
included standard types are: 
*TIME: in milliseconds, as a sequence of 
numbers, or as named points on the timeline. 
Values are numbers or identifiers, and the 
declaration of  the timeline states how to 
interpret them. 
Example: tirne=123200 dur=1280 (these are 
derived values, with time = TimeStart, and dur = 
TimeEnd- TirneStart). 
? HREF\[MODULE, ELEMENTLIST\]: Here 
MODULE is the name of another coding 
module, and ELEMENTLIST is a list of names 
of elements from MODULE. When applied as 
concrete attribute values, two parameters must 
be specified: 
The name of  the referenced coding file which 
is an application of the declared MODULE 
coding module. 
- The id of the element occurrence that is 
referred to. 
The values of  this attribute are of the form: 
...... CodeFileName"#'Elementld' . . . .  
Example: The declaration Occursln: 
href(lxanscription, u) allows an attribute used as, 
e.g., Occursln="base~_123", where base is a 
coding file using the transcription module and 
u_123 is the value of the id attribute of  a t~ 
element in that file. 
Example: For the declaration who: 
HREF\[transcription, participant\] an actual 
occurrence may look like who="#participant2 '' 
where the omitted coding file name by 
convention generically means the current 
coding file. 
The concept of  hyper-references together with 
parameters referencing coding modules (see 
point 5 in Figure 4) is what enables ccoding 
modules to handle cross-level markup. 
? ENUM: A finite closed set of  values. 
Values are of the form: "(" Identifier ("1" 
Identifier )* ")" 
Example: time (yearlmonthldaylhour) allows 
attributes such as time--day. 
The user may be anowed to extend the set, but 
never to change or delete values from the set. 
? TEXT: Any text not containing .... (which is 
used to delimit the attribute value). 
Example: The declaration dese TEXT allows 
uses such as: <event desc="Door is slammed">. 
? I13: Automatically generated i  for the element. 
Only available in the automatically added 
attribute id. 
24 
3.2 Coding modules 
In order for a coding module and the dialogues 
annotated using it to be usable and 
understandable y people other than its creator, 
some key information must be provided. The 
MATE coding module which is the central part 
of the markup framework, serves to capture this 
information. A coding module consists of the ten 
items hown in Figure 4. 
1. Name of the module. 
2. Coding purpose of the module. 
3. Coding level. 
4. The type of data source scoped by the 
module. 
5. References to other modules, if any. For 
transcriptions, the reference is to a resource. 
6. A declaration of the markup elements and 
their attributes. An element is a feature, or type 
of phenomenon, i  the corpus for which a tag is 
being defined. 
7. A supplementary informal description of 
the elements and their attributes, including: 
a. Purpose of the element, its attributes, 
and their values. 
b. Informal semantics describing how to 
interpret the element and attribute values. 
c. Example of each element and attribute. 
8. An example of the use of the elements and 
their attributes. 
9. A coding procedure. 
10. Creation otes. 
Figure 4. Main items of the MATE coding 
module. 
Some coding module items have a formal role, 
i.e. they can be interpreted and used by the 
MATE workbench. Thus, items (1) and (5) 
specify the coding module as a named 
parametrised module or class which builds on 
certain other predefined modules (no cycles 
allowed). Item (6) specifies the markup to be 
used in the coding. All elements, attribute 
names, and ids have a name space restricted to 
their module and its coding files, but are 
publicly referrable by prefixing the name of the 
coding module or coding file in which they 
occur. Other items provide directives and 
explanations to users. Thus, (2), (3) and (4) 
elaborate on the module itself, (7) and (8) 
elaborate on the markup, and (9) recommends 
coding procedure and quality measures. (10) 
provides information about the creation of the 
coding module, such as by whom and when. 
In the following, we show an abbreviated 
version of a coding module for communication 
problems to illustrate the 10 coding module 
entries. 
Name: Communication_problems. 
Coding purpose: Records the different ways in 
which generic and specific guidelines are 
violated in a given corpus. A communication 
problems coding file refers to a problem type 
coding file as well as to a transcription. 
Coding level: Communication problems. 
Data sources: Spoken human-machine dialogue 
corpora. 
Module references: Module Basic orthogra- 
phic transcription; Module Violation types. 
Markup declaration: 
ELEMENT eornprob 
ATTRIBUTES 
vtype: REFERENCE(Violation_types, vtype) 
wref: REFERENCE(Basic_orthographic 
transcription, (w,w)+) 
uref: REFERENCE(Basic_orthographic 
transcription, u+) 
caused by: REFERENCE(this, eomprob) 
temp: TEXT 
ELEMENT note 
ATTRIBUTES 
wref: REFERENCE(Basic_orthographic_ 
transcription, (w,w)+) 
uref: REFERENCE(Basic_orthographic_ 
transcription, u+) 
Description: In order to annotate 
communication problems produced by 
inadequate system utterance design we use the 
element eomprob. It refers to some kind of 
violation of one of the guidelines listed in Figure 
1, top fight-hand panel. The comprob element 
may be used to mark up any part of the dialogue 
which caused, or might cause, a communication 
problem. Thus, cornprob may be used to annotate 
one or more words, an entire utterance, or even 
several utterances inwhich an actual or potential 
communication problem was detected. The 
eomprob element has five attributes in addition to 
the automatically added id. 
25 
The attribute vtype is mandatory, vtype is a 
reference to a description of a guideline 
violation in a file which contains the different 
kinds of violations of the individual guidelines. 
Either wref or uref must be indicated. Both 
these attributes refer to an orthographic 
transcription, wref delimits the word(s) which 
caused or might cause a communication 
problem, and uref refers to one or more entire 
utterances which caused or might cause a 
problem. 
We stop the illustration here due to space 
limitations. The full description is available in 
(Mengel et al 2000). 
Example: 
In the following snippet of a transcription from 
the Sundial corpus: 
<u id="Sl:7-1-sun" who="S">flight information 
british airways good day can I help you</u> 
communication problems are marked up as 
follows: 
<comprob id="Y ' vtype="Sundial_problems#SG4-1" 
uref=" Sundial#S 1:7-1 -sun '7> 
We do not exemplify note here and do not 
show the violation type coding file due to space 
limitations. However, note that once a coding 
module is specified in the MATE workbench, 
the user does not have to bother about the 
markup shown in the example above. The user 
just selects the utterance to nark up and then 
clicks on the violation type palette, or, in case it 
is a new type, clicks on the violated 
cooperafivity guideline which means that a new 
violation type is added and text can be entered to 
describe it, el. Figure 1. 
Coding procedure: We recommend touse the 
same coding procedure for markup of 
cornrnunicafion problems as for violation types 
since the two actions are tightly connected. As a 
minimum, the following procedure should be 
followed: 
1. Encode by coders 1 and 2. 
2. Check and merge codings (performed by 
coders 1 and 2 until consensus). 
Creation notes: 
Authors: Hans Dybkj~er and Laila Dybkj~er. 
Version: 1 (25 November 1998), 2 (19 June 
1999). 
Comments: For guidance on how to identify 
communication problems and for a collection of 
examples the reader is invited to look at 
(Dybkj~er 1999). 
Literature: (Bernsen et al 1998). 
The MATE Workbench allows its users to 
specify a coding module via a coding module 
editor. A screen shot of the coding module ditor 
is shown in Figure 5. 
Figure 5. The MATE coding module ditor. 
26 
4 Early Experience and Future Work 
The MATE markup framework has been well 
received for its transparency and flexibility by 
the colleagues on the MATE Advisory Panel. 
The framework has been used to ensure a 
common description of coding modules at the 
MATE coding levels and has turned out to work 
well for all these levels. We therefore conclude 
that the framework is likely to work for other 
annotation levels not addressed by MATE. The 
use of a common representation a d a common 
information structure in all coding modules at 
the same level as well as across levels facilitates 
wilhin-level comparison, creation and use of 
new coding modules, and working at multiple 
levels. 
On the tools side, the markup framework has 
not been fully exploited as intended, i.e. as an 
intermediate layer between the user interface 
and the internal representation. This means that 
the user interface for adding new coding 
modules, in particular for the declaration of 
markup, and for defining new visualisations i  
still sub-optimal from a usability point of view. 
The coding module editor which is used for 
adding new coding modules, represents a major 
step forward compared to requiring users to 
write DTDs. The coding module editor 
automatically generates a DTD from the 
specified markup declaration. However, the 
XML format used for the underlying file 
representation has not been hidden completely 
from the editor's interface. Peculiarities and lack 
of flexibility in XML have been allowed to 
influence the way in which users must specify 
elements and attributes, making the process less 
logical and flexible than it could have been. It is 
high on our wish list to repair this shortcoming. 
As regards coding visualisation, XSLT-like 
style sheets are used to define haw codings are 
displayed to the user. Writing style sheets, 
however, is cumbersome and definitely not 
something users should be asked to do to define 
how codings based on a new coding module 
should be displayed. We either need a style sheet 
editor comparable tothe coding module ditor as 
regards ease of use, or, alternatively, a 
completely new iraerface concept should be 
implemented to replace the style sheets and 
enable users to easily define new visualisations. 
It is high on our wish-hst o better exploit he 
markup framework in the Workbench 
implementation in order to achieve a better user 
interface. 
Other frameworks have been proposed but to 
our knowledge the MATE markup framework is 
still the more comprehensive framework around. 
An example is the annotation framework 
recently proposed by Bird and Liberrnan (1999) 
which is based on annotation graphs. These are 
now being used in the ATLAS project (Bird et 
al., 2000) and in the Transcriber tool (Geoffrois 
et al, 2000). The annotation graphs erve as an 
intermediate r presentation layer in agreement 
with the argument above for having an 
intermediate layer of representation between the 
user interface and the intemal representation. 
Whilst Bird and Liberman do not consider 
coding modules or discuss the interface from a 
usability point of view, they present detailed 
considerations concerning time line 
representation a d time line reference. The two 
frameworks may, indeed, tuna out to 
complement each other nicely. 
Acknowledgements 
We gratefully acknowledge the support for the 
MATE project provided by the European 
Commission's Telematics/Language 
Engineering Programme. We would also like to 
thank all MATE partners. Without the very 
considerable joint efforts of the project 
consortium it would not have been possible to 
build the MATE Workbench. 
References 
Note on MATE deliverables: like the MATE 
Workbench, these are all obtainable from 
http://mate.nis.sdu.dk 
Annotation formats and tools: 
http://www.ldc.upenn.edu/annotation/ 
Bemsen, N. O., Dybkjmr, H. and Dybkj~er, L., 1998. 
Designing Interactive Speech Systems. From First 
Ideas to User Testing. Springer Verlag. 
Bird, S. and Liberrnan, M., 1999. A Formal 
Framework for Linguistic Annotation. Technical 
Report MS-CIS-99-01. Department of Computer 
and Information Science, University of 
Pennsylvania. 
Bird, S., Day, D., Garofolo, J., Henderson, J., Laprun, 
C. and Liberman, M., 2000. ATLAS: A Flexible 
and Extensible Architecture for Linguistic 
Annotation. Proceedings of the 2 ~d International 
27 
Conference on Language Resources and 
Evaluation (LREC 2000), Athens, 1699-1706. 
CES: http://www.cs.vassar.edu/CES/ 
Dybkkjj~er, L., 1999. CODIAL, a Tool in Support of 
Cooperative Dialogue Design. DISC Deliverable 
D2.8, April 1999. http://www.disc2.dk/tools/codial 
Dybkj2er, L., Bemsen, N. O., Dybk.ja~r, H., McKelvie, 
D. and Mengel, A., 1998. The MATE Markup 
Framework. MATE Deliverable D1.2. 
EAGLES: 
http://www.ilc.pi.cnr.it/EAGLE S/home.html 
Geoffrois, E., Barras, C., Bird, S. and Wu, Z., 2000. 
Transcribing with Annotation Graphs. Proceedings 
of the 2 ~d International Conference on Language 
Resources and Evaluation (LKEC 2000), Athens, 
1517-1521. 
Isard, A., McKelvie, D., Cappelli, B., Dybkj~er, L., 
Evert, S., Fitschen, A., Heid, U, Kipp, M., Klein, 
M., Mengel, A., Moiler, M. B. and Reithinger, N., 
1998. Specification of Workbench Architecture. 
MATE Deliverable D3.1. 
Isard, A., McKelvie, D., Mengel, A., Moiler, M. B., 
Grosse, M. and Olsen, M. V., 2000. Data 
Structures and APls for the MATE Workbench. 
MATE Deliverable D3.2. 
Klein, M., Bemsen, N. O., Davies, S., Dybkj~er, L., 
Garrido, J., Kasch, H., Mengel, A., Pirrelli, V., 
Poesio, M., QuaT~a, S. and Soda, S., 1998. 
Supported Coding Schemes. MATE Deliverable 
DI.1. 
MATE: http://mate.nis.sdu.dk 
Mengel, A., Dybkj~er, L., Garrido, Jr., Heid, U., Klein, 
M., Pirrelli, V., Poesio, M., Qoa~a, S., Schiffrin, 
A. and Soda, C., 2000. MATE Dialogue 
Annotation Guidelines. MATE Deliverable D2.1. 
TEI :  http://etext.virginia.edu/TEI.htrrd and 
http://www-tei.uic.edu/orgs/tei/ 
Transcriber: 
http://www.etea.fr/CTA/gip/Proj ets/Transeribe# 
28 
 Usability Evaluation in  
Spoken Language Dialogue Systems 
 
Laila Dybkj?r and Niels Ole Bernsen  
Natural Interactive Systems Laboratory, University of Southern Denmark 
Science Park 10, 5230 Odense M, Denmark 
laila@nis.sdu.dk, nob@nis.sdu.dk 
 
Abstract 
The paper first addresses a series of 
issues basic to evaluating the usability 
of spoken language dialogue systems, 
including types and purpose of 
evaluation, when to evaluate and 
which methods to use, user 
involvement, how to evaluate and 
what to evaluate. We then go on to 
present and discuss a comprehensive 
set of usability evaluation criteria for 
spoken language dialogue systems.  
1 Introduction 
Usability is becoming an increasingly important 
issue in the development and evaluation of 
spoken language dialogue systems (SLDSs). 
Many companies would pay large amounts to 
know exactly which features make SLDSs 
attractive to users and how to evaluate whether 
their system has these features. In spite of its 
key importance far less resources have been 
invested in the usability aspect of SLDSs over 
the years than in SLDS component technologies. 
The usability aspect has often been neglected in 
SLDS development and evaluation and there has 
been surprisingly little research in important 
user-related issues, such as user reactions to 
SLDSs in the field, users? linguistic behaviour, 
or the main factors which determine overall user 
satisfaction. However, there now seems to be 
growing recognition that usability is as 
important as, and partly independent of, the 
technical quality of any SLDS component and 
that quality usability constitutes an important 
competitive parameter. 
Most of today's SLDSs are walk-up-and-use 
systems for shared-goal tasks. Usability of walk-
up-and-use systems is of utmost importance, 
since users of such systems cannot be expected 
to undertake extensive training about the system 
or to read the user manual. Help must be 
available online and needed as infrequently as 
possible. 
There is at present no systematic 
understanding of which factors must be taken 
into account to optimise SLDS usability and 
thus also no consensus as to which usability 
evaluation criteria to use. Ideally, such an 
understanding should be comprehensive, i.e. 
include all major usability perspectives on 
SLDSs, and exhaustive, i.e. describe each 
perspective as it pertains to the detailed 
development and evaluation of any possible 
SLDS. This paper addresses the aspect of 
comprehensiveness by proposing a set of 
usability evaluation criteria. The criteria are 
derived from a set of usability issues that have 
resulted from a decomposition of the complex 
space of SLDS usability best practice.  
The present paper focuses on walk-up-and-
use SLDSs for shared-goal tasks and reviews 
substantial parts of the authors? work as 
presented in, e.g., (Dybkj?r and Bernsen 2000). 
For additional examples the reader is referred to 
this paper. Due to space limitations few 
examples are included in the present paper.  
In the following we first briefly address types 
and purpose of evaluation (Section 2), when to 
evaluate and which methods to use (Section 3), 
user involvement (Section 4), and how to 
evaluate (Section 5). Section 6 presents the 
proposed set of evaluation criteria and discusses 
the usability issues behind these. Section 7 
concludes the paper. 
2 Types and purpose of evaluation 
Evaluation can be quantitative or qualitative, 
subjective or objective. Quantitative evaluation 
 consists in quantifying some parameter through 
an independently meaningful number, 
percentage etc. which in principle allows 
comparison across systems. Qualitative 
evaluation consists in estimating or judging 
some parameter by reference to expert standards 
and rules. Subjective evaluation consists in 
judging some parameter by reference to users? 
opinions. Objective evaluation produces subject-
independent parameter assessment. Ideally, we 
would like to obtain quantitative and objective 
progress evaluation scores for usability which 
can be objectively compared to scores obtained 
from evaluation of other SLDSs. This is what 
has been attempted in the PARADISE 
framework based on the claim that task success 
and dialogue cost are potentially relevant 
contributors to user satisfaction (Walker, 
Litman, Kamm and Abella 1997). However, 
many important usability issues cannot be 
subjected to quantification and objective expert 
evaluation is sometimes highly uncertain or non-
existent.  
The purpose of evaluation may be to detect 
and analyse design and implementation errors 
(diagnostic evaluation), measure SLDS 
performance in terms of a set of quantitative 
and/or qualitative parameters (performance 
evaluation), or evaluate how well the system fits 
its purpose and meets actual user needs and 
expectations (adequacy evaluation), cf. 
(Hirschmann and Thompson 1996, Gibbon, 
Moore. and Winski 1997, Bernsen et al 1998). 
The latter purpose is the more important one 
from a usability point of view although the 
others are relevant as well. Which type of 
evaluation to use and for which purpose, 
depends on the evaluation criterion which is 
being applied (see below). Other general 
references to natural language systems 
evaluation are (EAGLES 1996, Gaizauskas 
1997, Sparck Jones and Galliers 1996). 
3 When to evaluate and methods to 
use  
Usability evaluation should start as early as 
possible and continue throughout development. 
In general, the earlier design errors are being 
identified, the easier and cheaper it is to correct 
them. Different methods of evaluation may have 
to be applied for evaluating a particular 
parameter depending on the phase in the 
lifecycle in which evaluation takes place. Early 
design evaluation can be based on mock-up 
experiments with users and on design walk-
throughs. Wizard of Oz simulations with 
representative task scenarios can provide 
valuable evaluation data. When the system has 
been implemented, controlled scenario-based 
tests with representative users and field tests can 
be used. Recorded dialogues with the 
(simulated) system should be carefully analysed 
for indications that the users have problems or 
expectations which exceed the capabilities of the 
system. Human-system interaction data should 
be complemented by interviews and 
questionnaires to enable assessment of user 
satisfaction. If users are interacting with the 
prototype on the basis of scenarios, there are at 
least two issues to be aware of. Firstly, scenarios 
should be designed to avoid priming the users 
on how to interact with the system. Secondly, 
sub-tasks covered by the scenarios will not 
necessarily be representative of the sub-tasks 
which real users (not using scenarios) would 
expect the system to cover.  
The final test of the system is often called the 
acceptance test. It involves real users and must 
satisfy the evaluation criteria defined as part of 
the requirements specification. 
4 User involvement 
In general, representative users from the target 
user group(s) should be involved in evaluation 
from early on. The developers themselves can 
certainly discover many of the usability 
problems with the early design and 
implementation, especially when supported by 
state-of-the-art usability standards, evaluation 
criteria and design support tools. The problem is 
that they know too well how to interact with the 
system in order to avoid creating interaction 
problems which the system cannot handle. For 
the time being, there is no alternative to 
involving the target users in all or most system 
evaluation phases and for most usability 
evaluation purposes. This is costly and complex 
to do. However, the data analysis which is 
crucial to benefiting from trials with the system, 
is as necessary after trials with developers as it 
is after trials with representative users. Even the 
early involvement of representative users is no 
guarantee that the system will ultimately 
produce sufficient user satisfaction. For one 
 thing, the data distribution they generate may 
not match the behaviour of the users of the 
system, once installed. For another, 
experimental user trials are different from real 
situations of use in which time, money and trust 
are really at stake. For these reasons, and 
particularly when introducing SLDSs which are 
innovative in some respect, it is necessary to 
prepare and budget for field trials with the 
implemented system as well as for the 
subsequent data analysis and fine-tuning of the 
system. Users who are ?only? involved in a test 
can be much more indifferent to, or more 
positive towards, a system with poor usability 
characteristics than real users who have 
something to loose if the system lets them down 
(Bernsen et al 1998). 
5 How to evaluate 
Evaluation, including usability evaluation, is 
non-trivial and cannot be explained simply by 
stating what to evaluate (cf. Section 6) and what 
the developers? options are. One of the most 
difficult questions in evaluation probably is how 
to do it properly. We have developed a template 
which supports consistent and detailed 
description of each evaluation criterion (Bernsen 
and Dybkj?r 2000). The template includes the 
following issues: what is being evaluated (e.g. 
feedback adequacy), the system part evaluated 
(e.g. the dialogue manager), type of evaluation 
(e.g. qualitative), method(s) of evaluation (e.g. 
controlled user experiments), symptoms to look 
for (e.g. user clarification questions), life cycle 
phase(s) (e.g. simulation), importance of 
evaluation (e.g. crucial), difficulty of evaluation 
(e.g. easy), cost of evaluation (e.g. expensive), 
and support tools (e.g. SMALTO), see 
(www.disc2.dk/tools). The idea is that the 
combined set of (i) design options for SLDS 
usability, (ii) usability evaluation criteria, and 
(iii) template-based characterisation of each 
criterion, will provide developers with sufficient 
information for proper evaluation of their 
SLDSs. 
6 What to evaluate 
In general terms, a usable SLDS must satisfy 
user needs which are similar to those which 
must be satisfied by other interactive systems. 
The SLDS must be easy to understand and 
interact with. Interaction should be smooth and 
the user should feel in control throughout the 
dialogue with the system. It is the task of the 
SLDS developer to meet those user needs 
considered as overall usability design goals. 
However, SLDSs are very different from more 
traditional interactive systems whose usability 
aspects have been investigated for decades, such 
as systems controlled through graphical user 
interfaces involving screen, keyboard and 
mouse. Perhaps the most important difference is 
that speech is perceptually transient rather than 
static. This means that the user must pick up the 
output information provided by the system the 
moment it is being provided or else miss it 
altogether. Moreover, the user has no way of 
inspecting the interface prior to interaction. If 
the interface is not self-evident all the way 
through the dialogue it must be learnt by trial-
and-error through repeated interaction, which is 
unsatisfactory for the casual walk-up-and-use 
user. Secondly, the processing (recognition, 
language understanding, dialogue management) 
of spoken input remains difficult to design and 
error-prone in execution, which is why SLDSs 
must be crafted with extreme care to ensure that 
users do not produce spoken input which the 
system is incapable of handling. 
In the following we present a set of usability 
evaluation criteria which are based on results 
achieved in the European DISC project 
(www.disc2.dk) on best practice in the 
development and evaluation of SLDSs 
(Failenschmid et al 1999). Our claim is that 
quality usability of SLDSs may be pursued by 
focusing on a comprehensive set of 15 usability 
issues which include all major usability 
perspectives on SLDSs. These usability issues - 
or a subset thereof, depending on how advanced 
and complex the system is to be ? should be 
represented in the system specification and 
should therefore also be reflected in a set of 
evaluation criteria for the system which would 
appear mandatory for evaluating the usability of 
the SLDS. More details on the usability issues 
can be found in (Dybkj?r and Bernsen 2000). 
6.1 Modality appropriateness 
The majority of task-oriented SLDSs have so far 
used speech as the only input/output modality. 
However, an increasing number of systems now 
combine spoken input/output with other 
modalities. 
 It is well-known that speech-only interaction 
is not appropriate for all tasks and applications, 
and the same is true for any particular modality 
combination which includes speech input and 
speech output. Few users would e.g. be happy to 
speak aloud their pin code to the bank teller 
machine in the street. The developers should 
attempt to make sure that spoken input and 
output, possibly combined with other 
input/output modalities, is an appropriate 
modality choice for the planned application. If 
the chosen modalities are inappropriate, chances 
are that the users either will not accept the 
application or will refrain from using some of 
the modalities it offers. Common sense, 
experimentation and/or the use of the tool 
SMALTO (www.disc2.dk/tools) may help the 
developers in making the right modality choice.  
6.2 Input recognition adequacy 
From the user?s point of view, good speech 
recognition means that the system rarely gets the 
user?s spoken input wrong or fails to recognise 
what the user just said. Recognition success, as 
perceived by the user, not only depends on 
recogniser quality but also on how other parts of 
the SLDS handle the user?s input. Good 
recogniser quality nevertheless remains the key 
factor in making users confident that the system 
will successfully get what they say. 
Walk-up-and-use systems may be used by 
many different users in highly different 
environments. The speech recogniser, therefore, 
and depending on more specific information on 
its intended users and environments of 
interaction, must be trained to recognise a 
variety of dialects and accents, speakers of 
different gender, age and voice quality, speaking 
with a low or a loud voice, in noisy or quiet 
environments, and with varying channel quality.  
Adequate information on users and 
environments is essential input to the selection 
and creation of training data. To assess the 
quality of the system?s recognition capabilities 
prior to running the full system, speech 
recognition accuracy may be tested on the 
recogniser with users from the target group(s). 
6.3 Naturalness of user speech  
Speaking to an SLDS should feel as easy and 
natural as possible. It does not help the user that 
the system?s speech recognition is perfect in 
principle if the input vocabulary and grammar 
expected from the user are not the ones which 
the user is likely to use and thus cannot be 
understood. Depending on, e.g., the task and 
users? experience, what is ?natural? input 
language may vary considerably.  
What is being experienced as natural input 
speech is also highly relative to the system?s 
output phrasing. For example, lengthy and/or 
overly polite system utterances are likely to 
invite similar linguistic user behaviour, thereby 
burdening input recognition and understanding 
unnecessarily. The system?s output language 
thus should be used to control users? input 
language so that the latter becomes manageable 
for the system whilst still feeling natural to the 
user. If the minimal constraints imposed by the 
task are satisfied and the system?s output 
language adequately controls the user?s input 
language, users may well feel that the dialogue 
is natural even if they are not inclined to engage 
in lengthy conversation.  
Analysis of data from system simulations, 
questionnaires and interviews is a useful tool for 
obtaining information on users? input language 
and on what they perceive as being natural input 
language. 
6.4 Output voice quality 
From the user?s point of view, good SLDS 
output voice quality means that the system?s 
speech is clear and intelligible, does not demand 
an extra listening effort, is not particularly noise 
sensitive or distorted by clicks and other 
extraneous sounds, has natural intonation and 
prosody, uses an appropriate speaking rate, and 
is pleasant to listen to (Karlsson 1999). Taken 
together, these requirements are difficult to meet 
today. 
There are three main types of output speech: 
recordings of entire system utterances, 
concatenation of recorded words and phrases, 
and text-to-speech (TTS). Concatenated speech 
is the most frequently used type of speech in 
today?s SLDSs. For walk-up-and-use systems in 
particular, TTS may simply be too difficult to 
understand for infrequent users while full 
recordings are much too inflexible. Moreover, 
too natural output speech, like full recordings, 
may suggest to users that the system is far more 
capable and human-like than it actually is, 
encouraging them to address the system in a 
 way which is more conversational and talkative 
than it can handle. 
The type of output voice chosen is likely to 
affect users? perception of the system as a 
whole. In particular, and together with the 
quality of the speech output, the voice type has a 
major influence on how pleasant users find the 
?system?s voice?. Voice type includes features 
such as male/female, deep/high voice, speaking 
rate, and emotions. 
In order to gather input on user preferences 
with respect to the system?s output voice, 
representative users of the system under 
development may be asked to listen to different 
?system voices? and provide feedback on which 
one they prefer and what they like and dislike 
about each of them. 
6.5 Output phrasing adequacy 
Regardless of the topic, the system should 
express itself co-operatively in order to 
maximise the likelihood that the task is achieved 
as smoothly and efficiently as possible . To 
facilitate successful interaction, the contents of 
the system?s output should be correct, relevant 
and sufficiently informative without being over-
informative. Users have good reason for 
dissatisfaction if the system provides false 
information, e.g., if the database is not being 
properly updated. Lack of relevance of system 
output caused by, e.g., misrecognition, will 
typically lead to meta-communication dialogue. 
System output should be sufficiently 
informative. Otherwise, misunderstandings may 
occur which are only detected much later during 
interaction, if at all, or which, at best, lead to 
immediate requests for clarification by the user. 
Conversely, the system should not provide too 
much or overly verbose information. Users may 
then e.g. become inattentive, try to take the 
dialogue initiative, or become confused and 
initiate clarification meta-communication.  
The form of system expressions should be 
clear and unambiguous, and language and, as far 
as possible, terminology should be consistent 
and familiar to the user (Bernsen et al 1998). 
Unclarity naturally leads to uncertainty and need 
for clarification. So does ambiguity if detected 
by the user. If undetected, as often happens, the 
effects of ambiguity can be severe. If the user 
unknowingly selects a non-intended meaning of 
a word or phrase uttered by the system, all sorts 
of things can go wrong. To help avoid ambiguity 
it is, moreover, advisable to use the same 
expressions for the same purposes throughout 
the dialogue. The system preferably should not 
use terms and expressions which are not familiar 
to most or all of its users. If the system must do 
that, unfamiliar terminology should be explained 
either proactively (before users ask) or through 
adequate measures for clarification meta-
communication. 
Developers may use CODIAL - a tool based 
on Cooperativity Theory ? as support for the 
design and evaluation of co-operative system 
dialogue (www.disc2.dk/tools).  
It is important to realise that the system?s 
output language tends to have a massive priming 
effect on the user?s language. It is, therefore, 
crucial that the words and grammar used in 
system output can be recognised and understood 
by the system itself. Similarly, the system 
should have a speaking style which induces 
users to provide input that is to the point and can 
be handled by the system.  
Interaction data analysis is needed to assess 
the efficiency of the input control strategies 
adopted. User contacts through interviews and 
questionnaires are good means for obtaining 
early input on how users experience the 
system?s output.  
6.6 Feedback adequacy 
Adequate feedback is essential for users to feel 
in control during interaction. The user must feel 
confident that the system has understood the 
information input in the way it was intended, 
and the user must be told which actions the 
system has taken and what the system is 
currently doing. A difficult thing is that telling 
the user is not always good enough ? the user 
must be told in such a way that the user notices 
what the system says. It may therefore be a good 
thing for SLDSs to provide several different 
kinds of feedback to their users. We distinguish 
between process feedback and information 
feedback.  
When the system processes information 
received from the user and hence may not be 
speaking for a while, process feedback ? which 
may be provided in many different ways - keeps 
the user informed on what is going on. A user 
who is uncertain about what the system is doing, 
if anything, is liable to produce unwanted input 
or to believe that the system has crashed and 
decide to hang up. Moreover, the uncertainty 
 itself is likely to affect negatively the user?s 
satisfaction with the system.  
Feedback on the system?s understanding of 
what the user just said and on the actions taken 
by the system helps ensure that, throughout the 
dialogue, the user is left in no doubt as to what 
the system has understood and is doing. 
Information feedback can be provided in 
different ways and more or less explicitly. The 
amount and nature of the information feedback 
depends e.g. on the cost and risk involved in the 
user-system transaction. A user who is uncertain 
as to what the system has understood, or done, is 
liable to produce unwanted input and to react 
negatively to the way the system works.  
6.7 Adequacy of dialogue initiative  
To support natural interaction, an SLDS needs a 
reasonable choice of dialogue initiative, an 
appropriate dialogue structure, sufficient task 
and domain coverage, and sufficient reasoning 
capabilities.  
Spoken human-human dialogue is 
prototypically mixed-initiative. However, many 
task-oriented dialogues tend to be directed 
primarily by one of the interlocutors. Users may 
even feel satisfied with less initiative when 
interacting with an SLDS than when talking to a 
person as long as the dialogue initiative distri-
bution fits the task(s) the system and the user 
must solve together, and provided that the rest of 
the best practice issues proposed in this paper 
are properly attended to. Thus, system directed 
dialogue can work well for tasks in which the 
system simply requires a series of specific 
pieces of information from the user, in particular 
if the user is new to the system. To satisfy 
experienced users, the system may have to be 
able to cope with the larger packages of input 
information which are natural to these users.  
In principle, a (mainly) user directed 
dialogue is as much of an aberration from mixed 
initiative dialogue as is the (mainly) system 
directed dialogue. Currently, user directed 
dialogue would seem to be appropriate primarily 
for applications designed for experienced users 
who know how to make themselves understood 
by the system. Unless supported by screen 
graphics or other additional modalities, 
inexperienced users are likely to address the 
system in ways it cannot cope with.  
Mixed initiative dialogue, i.e. a mixture of 
system and user initiative, is often both desirable 
and technically feasible. At some points in the 
dialogue it may be appropriate that the system 
takes the initiative to guide the user, obtain 
missing information, or handle an error. At other 
points, such as when the user needs information 
from the system, is already familiar with the 
system or wants to correct an error, it is 
appropriate for the user to take the initiative.  
6.8 Naturalness of the dialogue 
structure  
As long as we cannot build fully conversational 
systems, dialogue designers may have to impose 
some kind of structure onto the dialogue, 
determining which topics (or sub-tasks) could be 
addressed when. It is important that the structure 
imposed on the dialogue is natural to the user, 
reflecting the user?s intuitive expectations, 
especially in system directed dialogue in which 
the user is not supposed to interfere with the 
dialogue structure. Unnatural dialogue structure 
will often cause users to try to take the initiative 
in ways which the system cannot cope with. 
6.9 Sufficiency of task and domain 
coverage 
Sufficient task and domain coverage is also 
crucial to natural interaction. Even if unfamiliar 
with SLDSs, users normally have rather 
detailed expectations to the information or 
service which they should be able to obtain 
from the system. It is important that the system 
meet these expectations. If, for some reason, the 
system is not able to perform a certain sub-task 
which users would expect the system to handle, 
this has to be stated clearly. Even then, user 
satisfaction is likely to suffer.  
6.10 Sufficiency of the system?s 
reasoning capabilities 
Contextually adequate reasoning is a classical 
problem in the design of natural interaction. 
Even when users have been appropriately 
primed to expect a rather primitive interlocutor, 
they tend to assume that the system is able to 
perform the bits and pieces of reasoning which 
humans are able to do without thinking and 
which are inseparable parts of natural dialogue 
about the task. Typically, therefore, SLDSs must 
incorporate both facts and inferences about the 
task as well as general world knowledge in order 
 to act as adequate interlocutors. Defining which 
kinds of reasoning the system must be capable 
of is part and parcel of defining the system?s 
task and domain coverage and subject to 
similarly difficult decisions on task delimitation.  
It is possible to get rough ideas on initiative 
distribution, users? models of the task, and how 
to delimit the domain from studying recorded 
human-human dialogues on tasks similar to 
those which the system is intended to cover. 
However, the recordings should only be 
considered possible starting points. In particular, 
as task complexity grows, developers are likely 
to find themselves forced to adopt more 
restrictive task delimitations and impose a more 
rigid dialogue structure than those which they 
found in the human-human dialogues. Having 
done that, the resulting interaction model needs 
early testing and evaluation. In particular, if the 
developer is into relatively high task complexity 
compared to the state of the art, early testing is 
strongly recommended.  
6.11 Sufficiency of interaction guidance  
Sufficient interaction guidance is essential for 
users to feel in control during interaction. 
Interaction guidance can be particularly hard to 
get right in speech-only, walk-up-and-use 
SLDSs. Speech is inappropriate for providing 
lengthy and complex ?user manual? instructions 
up front for first-time users. Moreover, at any 
given time some users will already be familiar 
with the system whereas others will be novices. 
Issues to consider include cues for turn-taking 
vs. barge-in; help facilities; and highlighting of 
non-obvious system behaviour.  
Barge-in allows the user to speed up the 
interaction, e.g. by interrupting already familiar 
instruction prompts. If the system does not allow 
barge-in, it must provide clear cues for turn-
taking, making it completely clear to the user 
when to speak and when to refrain from 
speaking because the system does not listen. 
Cues can be explicit or implicit. If the user starts 
speaking while the system is still listening but 
processing the previous user input, the user?s 
new input may cause problems for the dialogue 
manager which has to generate an appropriate 
response to disjoint pieces of user input. And if 
the system is not listening any more, important 
input could be lost in cases when users do not 
merely repeat themselves.  
General and explicit instructions on what the 
system can and cannot do and how to interact 
with it may be provided in a spoken introduction 
which can be repeated on request or be skipped 
by experienced users. In fact, most speech-only 
SLDSs strictly need some up-front introduction 
to guide interaction. We already mentioned the 
when-(not)-to-speak issue above. Just as 
importantly, the system should be perfectly clear 
about the task(s) which the user can accomplish 
through interaction. The introduction should not 
be too long because then users cannot remember 
the instructions. Moreover, the instructions must 
be feasible for the user. If the instructions 
needed by the walk-up-and-use user are too 
many to be presented in the system?s 
introduction, some of them may be relocated for 
presentation at particular points during 
interaction and only when needed.  
Providing useful help mechanisms is a 
difficult interaction design task. Help may be an 
implicit part of the dialogue, be available on 
request by saying ?help?; or be automatically 
enabled if the user is having problems 
repeatedly, for instance in being recognised. In 
this case the system may, e.g., propose how to 
express input or inform the user on what can be 
said.  
Sufficiency of interaction guidance should be 
carefully evaluated by exposing the SLDS to 
interaction with representative users. 
6.12 Error handling adequacy 
Even if the best practice issues discussed so far 
have been taken into account carefully during 
specification, design and implementation, the 
SLDS and its users will still make errors during 
dialogue. In human-system interaction, error 
prevention is far preferable to error correction, 
and what those best practice issues do is to help 
prevent errors from occurring during interaction. 
Also as regards error handling current SLDSs 
are far inferior to their human interlocutors. This 
is why adequate error handling remains a 
difficult issue in SLDS development. Intuitively, 
this issue can be decomposed along two 
dimensions: (a) either the system initiates error-
handling meta-communication or the user 
initiates error-handling meta-communication. 
And (b) when error-handling meta-
communication is initiated, it is either because 
one party has failed to hear or understand the 
other or because what was heard or understood 
 is false, or it is because what was heard or 
understood is somehow in need of clarification. 
We distinguish, therefore, between system or 
user initiated repair meta-communication and 
system or user initiated clarification meta-
communication.  
System-initiated repair meta-communication 
is needed whenever the system either did not 
understand or was uncertain that it understood 
correctly what was said. In such cases, the 
system must ask for repetition, ask the user to 
speak louder or modify the way the input is 
being expressed in other specified ways, or tell 
the user what it did understand and ask for 
confirmation or correction. In case of a repeated 
misunderstanding the system may either choose 
to fall back on a human operator, close the 
dialogue, or, better, start graceful degradation, 
i.e. change the level of interaction into a simpler 
one. If users simply fail to respond, then the 
system should tell that it is expecting their input. 
Users may also be understood by the system to 
have said something which is false and hence 
needs to be corrected. User-initiated repair 
meta-communication can be designed in several 
different ways. Ideally, users should just initiate 
repair the same way they would have done in 
dialogue with a human, but since users may 
express their corrections in many different ways 
this is very difficult. Some systems require the 
user to use specifically designed keywords. The 
problem is that using keywords for correction is 
unnatural and hence difficult for the user to 
remember. A third approach is the ?eraser? 
principle where the user simply repeats his input 
until the system has received the message. 
Whilst this solution may work well for low-
complexity tasks, it may be difficult to keep 
track of in high-complexity tasks. And it will not 
work if the system cannot recognise input on 
any sub-task all the time.  
Very roughly speaking, clarification meta-
communication is more difficult to design for 
than repair meta-communication, and user-
initiated clarification meta-communication is 
more difficult to design for than system-initiated 
clarification meta-communication. System-
initiated clarification is needed when the user?s 
input is inconsistent, ambiguous or 
underspecified. In such cases, the system must 
ask for clarification, for instance by pointing out 
that an expression is inconsistent. User-initiated 
clarification is needed whenever the system 
produces inconsistent or ambiguous utterances, 
or uses terms with which the user is not familiar. 
Unfortunately, handling user clarification 
questions is difficult for SLDSs and the system 
developers might not have discovered all the 
potential problems in the first place. If they had, 
they could have tried to prevent all or most of 
the problems from occurring through adequate 
output phrasing or other means. Due to the 
nature of their domain, some tasks inherently 
require facilities for clarifying the terminology 
used because it may not be a practical option for 
e.g. a car sales system to explain all domain 
terms as it goes along.  
Most SLDSs need abilities for handling 
system- and user-initiated repair, and many 
SLDSs need system-initiated clarification 
abilities. There is no simple decision procedure 
for deciding which mechanisms to include in a 
particular SLDS. Sensible decisions very much 
depend on factors such as domain, task 
complexity, user population and peculiarities of 
user behaviour which can only be discovered 
through interaction data analysis. 
6.13 Sufficiency of adaptation to user 
differences 
It is useful to distinguish between four types of 
user: system expert/domain expert, system 
expert/domain novice, system novice/domain 
expert and system novice/domain novice. An 
SLDS needs not support all four groups, of 
course. If the target user group is domain and 
system experts only, then, obviously, the system 
is not a walk-up-and-use system and thus falls 
outside the group of SLDSs considered in this 
paper. If the primary target group is system 
novice users, on-line instructions and other help 
information is likely to be needed. This need 
tends to increase even further when the system 
novices are also domain novices who need 
explanation of domain technicalities.  
Given the relative simplicity of current 
SLDSs, walk-up-and-use users may quickly 
become (system) experts. This means that 
interaction should be supported and facilitated 
for both system novices and system experts. 
Special shortcuts for expert interaction can be a 
good solution. Such shortcuts include e.g. 
introductions which can be skipped easily 
through barge-in or explicit de-selection.  
 
 6.14 Number of interaction problems 
Lack of co-operativity in the system?s output 
may be diagnosed from the occurrence of 
communication problems in simulated or real 
user-system interaction. Data capture and 
analysis is costly, however, especially because 
large amounts of data may be needed for 
triggering most of the communication problems 
which the system is likely to cause. To reduce 
cost, and to help identify those kinds of lack of 
cooperativity which are less likely to cause 
communication problems, CODIAL may be 
used both for walk-throughs through the 
interaction design prior to data capture and for 
the actual data analysis.  
6.15 User satisfaction 
Objectively measured quality, technical and 
otherwise, does have an impact on user 
satisfaction, but this is far from being the whole 
story. User satisfaction is inherently subjective, 
building on personal preferences and contextual 
factors. Unfortunately, some of the most 
difficult usability issues exactly concern 
contextual adequacy, i.e. adequacy of the full set 
of contextual factors which contribute to making 
an SLDS acceptable to its users. These factors 
remain insufficiently explored both as regards 
which they are and as regards their individual 
contributions to user satisfaction. It is possible 
that contextual factors, such as service 
improvements or economical benefits, are 
among the most important factors influencing 
users? satisfaction with SLDSs.  
Much still remains to be discovered about 
how the behaviour of SLDSs affect the 
satisfaction of their users. Therefore, subjective 
evaluation remains a cornerstone in SLDS 
evaluation. User questionnaires and interviews 
remain core tools for gathering information on 
user satisfaction. 
7 Conclusion and future work 
We have presented a brief guide to practical 
evaluation of walk-up-and-use SLDSs for 
shared-goal tasks followed by a set of usability 
evaluation criteria. Within this framework, many 
issues remain unresolved or even unaddressed. 
Deployment usability issues are still poorly 
understood as are the usability issues arising 
from multimodal and natural interactive 
applications which integrate speech-only SLDSs 
into larger systems. Usability questionnaire 
design remains poorly understood. The same 
applies to cultural differences in the perception 
of SLDS usability. 
Much work remains to be done before we 
have a solid all-round understanding of usability 
evaluation of SLDSs. The authors work in two 
recently started projects on walk-up-and-use 
SLDSs, one commercial and one research 
system, in which the guidelines presented above 
will be tested and, very likely, extended.  
References 
Bernsen, N. O., Dybkj?r, H. and Dybkj?r, L. 
1998. Designing Interactive Speech Systems. 
From First Ideas to User Testing. Berlin, 
Springer. 
Bernsen, N. O. and Dybkj?r, L. 2000. A 
Methodology for Evaluating Spoken Language 
Dialogue Systems and Their Components. 
Proceedings of LREC 2000, Athens, May 2000, 
183-188. 
Cole, R. A., Mariani, J., Uszkoreit, H., 
Zaenen, A. and Zue, V. W. (Editorial Board), 
Varile, G. and Zampolli, A. (Managing Editors). 
1996. Survey of the State of the Art in Human 
Language Technology. Sponsors: National 
Science Foundation, Directorate XIII-E of the 
Commission of the European Communities, 
Center for Spoken Language Understanding, 
Oregon Graduate Institute. URL: 
http://www.cse.ogi.edu/CSLU/HLTsurvey/. 
Dybkj?r, L. and Bernsen, N.O. 2000. 
Usability Issues in Spoken Language Dialogue 
Systems. In Natural Language Engineering, 
Special Issue on Best Practice in Spoken 
Language Dialogue System Engineering, 
Volume 6 Parts 3 & 4 September 2000, 243-
272. 
EAGLES. 1996. Evaluation of Natural 
Language Processing Systems. Final Report, 
EAGLES Document EAG-EWG-PR2. 
Copenhagen, Center for Sprogteknologi, 
http://issco-
www.unige.ch/projects/eagles/ewg99/index.html 
Failenschmid, K., Williams, D., Dybkj?r, L. 
and Bernsen, N. O. 1999. Draft proposal on best 
practice methods and procedures in human 
factors. DISC Deliverable D3.6. 
http://www.disc2.dk. 
 Gaizauskas, R. (Ed.) 1997. Proceedings of 
the SALT Club Workshop on Evaluation in 
Speech and Language Technology, Sheffield. 
Gibbon, D., Moore, R. and Winski, R. (Eds.). 
1997. Handbook of standards and resources for 
spoken language systems. Mouton de Gruyter, 
Berlin, New York. 
Hirschmann, L. and Thompson, H. S. 1996. 
Overview of evaluation in speech and natural 
language processing. In Cole et al 1996, Section 
13.1. 
Karlsson, I. 1999. Draft proposal on best 
practice methods and procedures in speech 
generation. DISC Deliverable D3.3. 
http://www.disc2.dk 
Sparck Jones, K. and Galliers, J. 1996. 
Evaluating natural language processing 
systems. Lecture Notes in Artificial Intelligence 
1083. Berlin, Springer. 
Walker, M. A., Litman, D. J., Kamm, C. A. 
and Abella, A. 1997. Evaluating interactive 
dialogue systems: Extending component 
evaluation to integrated system evaluation. 
Proceedings of the ACL/EACL Workshop on 
Spoken Dialog Systems , Madrid, 1-8. 
 SPEECH-RELATED TECHNOLOGIES 
Where will the field go in 10 years? 
Niels Ole Bernsen, NISLab, Denmark (editor) 
Abstract 
This paper is a draft position paper for discussion at the ELSNET Brainstorming Workshop 
2000-2010 in Katwijk aan Zee, The Netherlands, on 23-24 November, 2000. The paper first 
describes some general emerging trends which are expected to deeply affect, or even 
transform, the field of speech technology research in the future, including trends towards 
advanced systems research, natural interactivity, multimodality, and medium-scale science. A 
timeline survey of future speech-related technologies is then presented followed by analysis 
of some of the implications of the proposed timelines. Timeline projections may turn out to 
have been false, of course, but even their turning out to be true is subject to future actions 
which are (not) taken to make them true. Accordingly, the final part of the paper discusses 
some actions which would seem desirable from the point of view of strengthening the 
position of European speech-related research. 
1. Introduction 
The term speech-related research has been chosen to designate the topic of the present paper 
for lack of ability to invent a more appropriate term, if there is one. At least, the term partly 
manages to convey the author?s expectation that the field of speech research will change 
rather dramatically in the coming ten years as speech technologies become merged with other 
technologies into a field which, so far, lacks a name. 
According to many observers, the coming decade will be the decade of speech technologies. 
Computer systems, whether stationary or mobile, wired or wireless, will increasingly offer 
users the opportunity to interact with information and people through speech. This has been 
made possible by the arrival of relatively robust, speaker-independent, spontaneous (or 
continuous) spoken dialogue systems in the late 1990s as well as through the constantly 
falling costs of computer speed, bandwidth, storage, and component miniaturisation. The 
presence of a speech recogniser in most appliances combined with distributed speech 
processing technologies will enable users to speak their native tongue when interacting with 
computer systems for a very large number of purposes. Although no doubt exaggerated as just 
presented, there probably is some truth to this vision of a breakthrough in the application of 
speech technologies in the coming years. If this is the case, it would seem worthwhile that we 
lift our sights and take a long-term view of the issues ahead. This may help setting a 
reasonable research agenda for the coming years of advanced speech systems research and 
development, one which does not succumb to the usual hype associated with fashionable 
technologies. Today, some believe that ?the speech problem? has been solved already. Some 
believe that speech, because of its naturalness, is the solution to every conceivable problem of 
user-system interaction. On the other hand, surprising as it may seem, some human factors 
and interactive systems experts believe that we have just arrived at the touch-tone telephony 
stage and share no notion of the actual state-of-the-art in the field with its practitioners. Since 
 all of those beliefs are far from the truth, it is important to provide a more balanced picture of 
the state-of-the-art in speech technologies in order to set the stage for solid progress. 
In what follows, Section 2 presents some trends in the speech-related research field. Section 3 
excels in guesswork by estimating the times of appearance of a range of novel speech-related 
technologies. Section 4 discusses implications of the timelines presented in Section 3. Section 
5 proposes a series of actions which would appear appropriate given the preceding discussion. 
2. Some Trends 
The speech field is making progress on a broad scale as demonstrated by the 900 or so papers 
and posters presented at the recent International Conference on Spoken Language Processing 
(ICSLP) in Beijing, October 2000. [To be illustrated by listing topics.] Three points may be 
made on the preceding list of current topics in speech research. Firstly, the wealth of topics 
that are being addressed in current fundamental and applied research obviously demonstrates 
that ?the speech problem? has not been solved but continues to pose a series of major research 
challenges. [Mention some of them.] Secondly, the breadth of the speech topics that are being 
addressed could be taken as evidence that the speech field is simply doing business as usual, 
albeit on a larger and more ambitious scale than ever before. Thirdly, however, it is clear from 
the topics list that the speech field is no longer separate from many other fields of research 
but is in a process of merging into something which might perhaps be called the general field 
of interactive technologies. This latter trend, it may be argued, is the single most important 
factor which will influence the speech field in the future and which already suggests that the 
field is in a state of profound transformation. 
Interactive technologies 
It is relatively straightforward to explain why the speech field is gradually merging into the 
general field of interactive technologies. Since speech now works for a broad range of 
application purposes, a rapidly growing fraction of the speech research community are 
becoming involved in advanced interactive systems research rather than continuing to work 
on improving the speech components which form part of those systems. In advanced 
interactive systems research, speech is increasingly being used not as a stand-alone interactive 
modality as in, e.g., spoken language dialogue systems over the telephone, speech dictation 
systems, or text-to-speech systems, but as a modality for exchanging information with 
computer systems in combination with other modalities of information representation and 
exchange. Moreover, speech is not just an interactive technology among many others. 
Spontaneous speech is an extremely powerful input/output modality for interacting with 
computer systems, a modality which, furthermore, is available and natural to the large 
majority of users without any need for training in using it for interactive purposes. 
The ongoing shift from speech components research to research on integrating speech in 
complex interactive systems has a number of important implications for the speech field. 
Speech researchers are becoming systems researchers and engineers. Far more than 
components research, systems research and engineering is exposed to the full complexity of 
today?s world of information and telecommunications technologies. Few, if any, groups can 
build full systems on their own from scratch. To stay competitive, they have to follow closely 
the global developments in relevant systems architectures, platforms, toolkits, available 
components of many different kinds, de facto standards, work in standards committees, 
market trends etc. They need larger and much more interdisciplinary teams in order to keep 
up with competitive developments. They need access to platforms and component 
technologies in order to avoid having to do everything by themselves. And they need 
expertise in software systems engineering best practice as specialised to the kind of systems 
 they are building, including expertise in systems and usability evaluation. As we shall see in 
Section 4, they need even more than this, such as hardware access or expertise, development 
resources, behavioural research in new domains, and skills in form and contents design.  
Compared to traditional research on improving a particular speech component technology, the 
world of advanced interactive systems research would appear to be orders of magnitude more 
complex. Moreover, that world is quite diffuse for the time being. It does not have a single 
associated research community, being inhabited instead by researchers from most traditional 
ITC (Information Technologies and Telecommunications) research communities. The world 
of advanced interactive systems research does not have any clear evolutionary direction, 
being characterised rather through ever-changing terms of fashion, such as ?ubiquitous 
computing?, ?things that think?, ?wearable computing?, ?the disappearing computer? or 
?ambient intelligence?. Significantly, all or most of those terms tend to refer to combined 
hardware and software systems rather than to components, and none of them refer to the 
traditional communities in the ITC field, such as speech processing, natural language (text) 
processing, machine vision, robotics, computer graphics, neural networks, machine learning, 
or telecommunication networks. Indeed, most of our current stock of inspired and visionary 
terms for describing the future of interactive technologies tends to be rather vague with regard 
to the technologies which they include or, if any, exclude. 
Rather than trying to clarify what might be meant by the terms of fashion mentioned above, it 
may be useful to look at two other developments in conceptualising the field of advanced 
interactive systems research of which speech research has begun to form a part. To be sure, 
the concepts to be discussed are expressed by fashion terms as well, but at least it would seem 
that those concepts are of a more systematic and theoretically stable nature at this point. 
Natural interactivity 
When being together, most humans interact through speech when they exchange information. 
The telephone allows them to use spoken interaction at a distance as well, and the function of 
the telephone will soon be shared, or even taken over, by computing systems. When humans 
interact through speech, it does not matter if they are just a twosome or if they are more than 
two together. Moreover, except when speaking over the telephone, speech is not their only 
modality for information exchange. Gesture, lip movements, facial expression, gaze, bodily 
posture, and object manipulation all contribute to adding information, however redundant, to 
the spoken message. Together with speech, those modalities constitute full natural human-
human communication. Moving beyond current technologies, we envision not just a single 
human speaking on the telephone or to a (desktop) computer in order to get a particular task 
done. Rather, the vision is one in which multiple humans speak together whether or not they 
are in the same physical location whilst using the system as an increasingly equal partner in 
communication. The system mediates their communication when needed, understands full 
natural communication, and produces full natural communication itself, increasingly acting as 
its human counterparts in communication. In order to take this vision into account, it would 
seem timely to abandon the traditional model of interaction which is called ?human-computer 
interaction?, and replace it with the more general model of natural human-human-system 
interaction (HHSI). Natural HHSI, it appears, it a necessary end-point of current research in 
speech technologies. Thus, natural interactivity may serve as an important, even if distant, 
guidepost for the role of speech research in the complex world of interactive systems 
research. 
The received picture of the role of theory in engineering goes something like this. It is hardly 
ever possible to deduce from theory a complete specification of the artefact that would 
constitute an optimal solution to some engineering problem. The reason is that the complexity 
 of the problem space involved always exceeds the power of theory. On the other hand, 
without theory (of physics, chemistry, computation etc.), it would not have been possible to 
build many of the artefacts we use in our daily lives. Thus, theory has a necessary supporting 
function in engineering. This is clear in the case of natural interactivity. To achieve the 
ultimate goal of natural HHSI, we need far better theory than is available at present: about 
how humans behave during natural interaction, about the behavioural phenomena which are 
relevant to the development of fully natural interactive systems, about how these phenomena 
are interrelated, about how they should be encoded etc. We also need a novel theory of 
natural communication which can replace speech acts theory and discourse theory by taking 
the notion of a complete communicative act as its basic notion. 
Multimodality 
The trend towards multimodal interactive systems reflects the trend towards blending of 
traditional research communities noted above as well as the increasing role of speech in future 
interactive systems. Multimodal systems are systems which offer the user combinations of 
input/output modalities for (or ways of) exchanging information with computer systems. 
Given the naturalness and expressive power of speech, speech input and speech output have 
the potential for becoming key modalities in future interactive systems. However, compared 
to natural interactivity, our current understanding of multimodality is much less capable of 
providing guideposts for future advanced interactive systems research in general and research 
on multimodal systems which include speech modalities in particular. Much too little is 
known about how to create good modality combinations which include speech for a variety of 
interactive purposes. This topic has become an active field of research, however (Bernsen 
1997a, Benoit et al 2000, Bernsen 2001). Further progress in this field is likely to 
complement research on natural interactivity in providing guideposts for speech-related 
research in the complex world of advanced interactive systems. In fact, these two research 
directions are intertwined in so far as it remains an open issue for which application purposes 
technologies, such as, e.g., animated speaking characters might provide useful solutions. 
Medium-scale science 
The final trend to be mentioned is the trend towards medium-scale science in advanced 
interactive systems research. Increasingly, it is becoming evident that the standard 3/4/5-team, 
low-budget, 3-year isolated advanced systems research project is often an inefficient means of 
achieving significant research progress. In many projects, the participants share discouraging 
experiences, such as the following: even if small, the project is only able to start almost one 
year after its conception because of the administrative processing needed to release the 
funding for the project; when the project begins, the participants discover that their objectives 
have already been achieved elsewhere; the participants spend the first half of the project 
trying to identify the best platform to work from only to discover that they cannot get access 
to it; the participants spend half of the project building and putting together a low-quality 
version of the contextual technologies they need before they can start addressing their core 
research objectives; at the start of the project, the participants realise that it will take too long 
to produce the data resources they need, such as tagged corpora, and decide instead to work 
with sub-optimal resources which they can get for free; etc. One way to avoid, or reduce the 
number of, such experiences is to launch larger-scale concerted research efforts which have a 
better chance of moving beyond the state of the art. World-wide, experiments are currently 
underway on how to carry out such medium-scale science. In the US DARPA Communicator 
project which addresses spoken language and multimodal dialogue systems, for instance, all 
participants start from shared core technologies without having to build these themselves 
(http://fofoca.mitre. org/). In the German SmartKom project which addresses multimodal 
 communication systems, the budget is large enough for the participants to build and integrate 
the technologies needed (http://smartkom.dfki.de/start.html). In the European Intelligent 
Information Interfaces (i3, http://www.i3net.org/) and CLASS (http://www.class-tech.org/) 
initiatives, whilst the traditional 3-year small-scale project topology has been preserved, 
major efforts are being made to promote cross-project collaboration, synergy, and critical 
mass.  
For reasons too obvious to mention, relatively small-scale research should continue to exist, 
of course. Still, the complexity of the world of advanced interactive systems research is not 
likely to go away. This raises the question of whether we need more medium-scale science 
and less small-scale science in order to make efficient use of the funds available for advanced 
interactive systems research. If this question is answered in the affirmative, the important 
issue becomes how best to do medium-scale science, i.e. which model(s) to adopt for the 
larger-scale research efforts to come. 
3. Estimated Technology Timelines 
This section attempts to estimate the time of first appearance of a broad selection of generic 
and/or landmark speech technologies including natural interactivity technologies and 
multimodal technologies involving speech. Some qualifications are necessary to the proper 
interpretation of the proposed predictions. Despite the numerous uncertainties involved in 
estimating technology progress, timelines, when properly estimated, qualified, and peer 
reviewed, do seem a useful means of conveying a field?s expectations to the outside world 
and serving as a basis for actions to be undertaken to support research in the field. 
Qualifications 
(a) As in all timeline forecasts, there is some uncertainty in the forecasts below with respect to 
whether the technology is deployable or will in fact have been deployed in products at the 
suggested time. The claim for the figures below rather tend towards the deployable 
interpretation which is the one closest to the point of view of research. The actual deployment 
of a deployable technology is subject to an additional number of factors some of which are 
unpredictable, such as company technology exploitation strategies, pricing strategies, and the 
market forecasts at deployability time. Thus, several years may pass before some of the 
technologies below go from deployability to actually being used in mass products. This 
implies that one cannot from the estimations below construct scenarios for the Information 
Society in which people in general will be using the described technologies at the times 
indicated. In other words, the years below refer to ?earliest opportunity? for actual 
deployment in what may be sometimes rather costly systems to be embraced by relatively few 
customers. Similarly, given the fact that there are thousands of languages in the world, it goes 
without saying that a technology has been established when it works in at least one of the top 
languages, a ?top language? being defined as a language used by developers in the more 
affluent parts of the world. 
(b) Another point related to (a) above is to do with underlying ?production platforms?. For 
many advanced, and still somewhat futuristic, speech and language -related systems, it is one 
thing to have produced a one-of-a-kind demonstrator system but quite another to have 
produced the system in a way which enables oneself or others to relatively quickly produce 
more-of-the-same systems in different application domains. An example is the so-called 
intelligent multimedia presentation systems which will be discussed in more detail in Section 
4. Several examples exist, such as the German WIP system and corresponding systems from 
the USA. However, as long as we haven?t solved the problem of how to produce this kind of 
system in a relatively quick and standardised way, intelligent multimedia presentation 
 systems are not going to be produced in numbers but will remain research landmarks. The 
timeline list below mostly avoids mentioning systems of this kind, assuming for the kinds of 
systems mentioned that the ?production platform? issue has been solved to some reasonable 
extent at the time indicated. 
(c) There is some, inevitable because of the brevity of the timeline entries, vagueness in what 
the described technologies can actually do.  
(d) It is assumed that, after a certain point in time which could be, say, 2006, the distinction 
between technology use for the web and technology use for other purposes will have 
vanished.  
(e) There is no assumption about who (which country, continent, etc.) will produce the 
described landmark results. However, given the virtually unlimited market opportunities for 
the technologies listed as a whole, it is expected that a consolidated technology timeline list 
will command keen interest among decision makers from industry and funding agencies.  
(f) There is nothing about (software) agent technologies below. It is simply assumed that what 
is currently called software agent technologies will be needed to achieve the results described 
and will be available as needed. 
(g) In principle, of course, any technology timeline list is subject to basic uncertainty due to 
the ?if anything is done about it? ?factor. If nothing will be done, nothing will happen, of 
course. However, most of the technologies listed below are being researched already and the 
rest will no doubt be investigated in due course. The uncertainty only attaches to who will get 
there first with respect to any given technology, who will produce the product winners, and 
how much effort will be invested in order to achieve those results before anybody else. 
Technology timelines 
Basic technologies 
Hypotheses lattices, island parsing, spotting in all shapes and sizes for spoken  
dialogue 2001 
Continuous speech recognisers in OSs for workstations in top languages 2002 
Continuous speech recognisers in mobile devices (10000 words vocabulary) in   
top languages 2003 
High quality competitive (with concatenated speech) formant speech synthesis   
in top languages 2003 
Task-oriented spoken dialogue interpretation by plausibility in context and situation 2003 
Generally usable cross-language text retrieval 2003 
Multilingual authoring in limited domains by constructing conceptual representations  2003 
Usable ontological lexicons for limited domains 2003 
Usable translation systems for written dialogues (multilingual chatting) 2003 
Useful speaker verification technology 2004 
Seamless integration of spoken human/machine and human/human communication 2004 
First on-line prosodic formant speech synthesis in top languages 2004 
Simple task-oriented animated character spoken dialogue for the web 2004 
Concept-to-speech synthesis 2004 
Stylistically correct presentation of database content 2004 
Superficial semantic processing based on ontological lexicons 2004 
 Max. 2000 words vocabulary task-oriented animated character dialogue for the web 2005 
Prosodic formant speech synthesis replaces concatenated speech in top languages 2005 
Full free linguistic generation (from concepts) 2005 
Robust, general meta-communication for spoken dialogue systems 2005 
Writer-independent handwriting recognition 2005 
Learning at the semantic and dialogue levels in spoken dialogue systems 2006 
Useful multiple-speaker meeting transcription systems 2006 
Task-oriented fully natural animated characters (speech, lips, facial expression,   
gesture) output (only) 2007 
Context sensitive summarization (responsive to user's specific needs) 2007 
Answering questions by making logical inferences from database content 2007 
Speech synthesis with several styles and emotions in top languages 2008 
Continuous speech understanding in workstations with standard dictionaries   
(50000 words) in top languages 2008 
Controlled languages with syntactic and semantic verification for specific domains 2008 
Large coverage grammars with automatic acquisition for syntactic and semantic   
processing for limited applications 2008 
Task-oriented fully natural speech, lips, facial expression, gesture  
input understanding and output generation 2010 
Systems 
First personalised spoken dialogue applications (book a personal service over the phone) 2002 
Useful speech recognition-based language tutor 2003 
Useful portable spoken sentence translation systems 2003 
Useful broadcast transcription systems for information extraction 2003 
First pro-active spoken dialogue with situation awareness 2003 
Current spoken dialogue systems technology for the web (office, home) 2004 
Satisfactory spoken car navigation systems 2004 
Current spoken dialogue systems technology for the web (in cars) 2005 
Useful special-purpose spoken sentence translation systems (portable, web etc.) 2005 
High quality translation systems for limited domains with automatic acquisition 2005 
Small-vocabulary (>1000 words) spoken conversational systems 2005 
Medium-complexity (wrt. semantic items and their allowed combinations) task-oriented  
spoken dialogue systems 2005 
Multiple-purpose personal assistants (spoken dialogue, animated characters) 2006 
Task-oriented spoken translation systems for the web 2006 
Useful speech summarisation systems in top languages 2006 
Useful meeting summarisation systems 2008 
Usable medium-vocabulary speech/text translation systems for all non-critical   
situations 2010 
Medium-size vocabulary conversational systems 2010 
 Tools, platforms, infrastructure 
Standard tool for cross-level, cross-modality coding of natural interactivity data 2002 
Infrastructure for rapid porting of spoken dialogue systems to new domains 2003 
Platform for generating intelligent multimedia presentation systems with spoken   
interaction 2005 
Science-based general portability of spoken dialogue systems across domains and tasks 2006 
 
Other problems which were strongly felt when producing the list above include: (i) the fact 
that there is plenty of continuity in technology development. ?Continuity? may not be the 
right term because what happens is that what is later perceived as a new technological step 
forward is constituted by a large number of smaller steps none of which could be mentioned 
in a coarse-grained timeline exercise such as the one above. General speaker identification, 
robust speech recognition in hard-to-model noise conditions, ?real? speaker-independent 
recognition (almost) no matter how badly people speak, or pronounce, some language, are all 
examples of minute-step progress. (ii) Another problem is to do with speech in fancy-termed 
circumstances, such as ?ambient intelligence? applications. It may be that there is a hard-core 
step of technological progress which is needed to achieve speech-related ambient intelligence 
but then again, may be there isn?t. Maybe this is all a matter of using the timelined speech 
technologies above for a wide range of systems and purposes. Similarly, it is tempting to ask, 
for instance: ?When will I have a speech-driven personal assistant??. But everything depends 
on what the personal assistant is supposed to be able to do. Some personal assistant 
technologies exist already. Thus, it does not seem possible to timeline the appearance of 
speech-driven personal assistants even if this might be attractive for the purpose of 
advertising the potential of speech technologies. 
How well is Europe doing? 
No attempt has been made, so far, to annotate the technology timelines with indications of 
how well, or how badly, European research is doing and hence how likely it is that a 
particular technology will be made deployable in Europe before anywhere else. In most of the 
timelined cases above, this would seem to depend primarily on the financial resources and 
research support mechanism which will be available to European research in the coming 
decade. In some cases, the US is presently ahead of Europe, such as with respect to 
continuous speech recognisers in workstations or broadcast transcription systems. In other 
cases, Europe has the lead, such as in building a standard tool for cross-level, cross-modality 
coding of natural interactivity data, continuous speech recognisers in mobile devices, 
advanced spoken dialogue systems, and spoken car navigation systems. 
Beyond 2010 
Beyond 2010 lie the dreams, such as unlimited-vocabulary spoken conversational systems, 
unlimited-vocabulary spoken translation systems, unlimited on-line generation of integrated 
natural speech, lips, facial expression and gesture communication, unlimited on-line 
understanding of natural speech, lips, facial expression and gesture communication by 
humans, summarisation-to-specification of any kind of communication, multimodal systems 
solutions on demand, and, of course, full natural interactive communication. 
4. Implications of the Timelines 
When analysing the implications of the timelines in Section 3, a number of uncertainties come 
up with respect to how the market for speech products will develop. At present, most speech 
 products are being marketed by some 5-10 major companies world-wide. These companies 
are growing fast as are hundreds of small start-up companies many of which use basic 
technologies from the larger technology providers. It may be assumed that this market 
structure will not continue in the future. Rather, speech recognition and synthesis 
technologies would seem likely to become cheap, or even free and open source, components 
which will come with all manner of software and hardware systems. The implication is that 
all ITC providers who want to, will provide value-added speech products and that the basic 
speech technologies will not be dominated by a small number of large suppliers. Some 
important share of the speech market, including de facto standards in various areas, will 
probably be picked up by large custom software and mobile phone technology suppliers, such 
as Microsoft and Nokia, but that is likely to happen in any realistic scenario for the coming 
decade. The conclusion is that, during the coming decade, speech will be everywhere, in all 
sorts of products made by all sorts of companies. But will speech be everywhere in bulk? This 
raises a second uncertainty. 
In one scenario, speech will be present in all or most ITC products by 2010, and speech will 
be popular and will be used as much as input keys, input buttons, and output graphics displays 
are being used today. In another scenario, however, speech uptake will be slow and arduous. 
Several reasons could be given for the latter scenario. Thus, (a) it may take quite some time 
before speech recognition is being perceived by users to be sufficiently robust to make users 
switch to speech where speech is better ideally. (b) It may take quite some time before the 
field and the market has sorted out when to use speech as a stand-alone modality and when to 
use speech in combination with other input/output modalities. If these two (a + b) take-up 
curves do not grow in any steep manner, speech may still be widespread by 2010, but speech 
will still not be as important an input/output modality as it is likely to become later on. For the 
time being, we would appear to have too little information to be able to decide between the 
two scenarios just discussed. There is simply not enough data available on user uptake of 
speech technologies to enable a rational decision to be made. 
Exploitation today 
Already today, there is a great exploitation potential for speech technologies because of the 
simple facts that (i) the technologies which already exist in a few top languages could be 
ported to hundreds of other languages, and (ii) the types of applications which already exist 
can be instantiated into numerous other applications of similar complexity. At this end of the 
speech technology spectrum, the emphasis is on flexible and versatile production platforms, 
quality products, and low-cost production rather than on research. This is particularly true of 
low-complexity over-the-phone spoken language dialogue information systems using 
continuous speech input. Users would seem to have adopted these systems to a reasonable 
extent already. The same degree of user acceptance does not appear to characterise the uptake 
of, e.g., spoken language dictation systems or simple spoken command systems for operating 
screen menus.  Even if purchased by widely different groups of users, the former would 
appear to be used primarily by professionals, such as lawyers and medical doctors, and the 
latter hardly seems to be used at all. Also, text-to-speech systems for the disabled and 
increasingly for all users, do appear to have a significant exploitation potential already. 
Key technologies: speech-only 
The timelines in Section 3 highlight a series of key speech-only technologies which are still at 
the research stage, including: 
? prosody in on-line speech synthesis; 
? multi-speaker broadcast and meeting transcription; 
 ? speech summarisation; 
? speech translation; and 
? conversational spoken dialogue. 
Prosody in on-line speech synthesis 
Prosody in on-line speech synthesis is probably important to the speed of take-up of speech 
technologies because users would appear likely to prefer prosodic speech output to non-
prosodic speech output. However, there do not seem to exist firm estimates as to how much 
prosody matters. Reasonably clear and intelligible non-prosodic text-to-speech already exists 
for some top languages and might turn out to be satisfactory for most applications in the 
short-to-medium term.  
Multi-speaker broadcast and meeting transcription 
Multi-speaker broadcast transcription forms the topic of massive US-initiated research at the 
moment and appears likely to start becoming widely used in practice relatively soon. Like 
meeting transcription technology, multi-speaker broadcast transcription technology has a 
large potential for practical application as well as for acting as a driving force in speech and 
natural language (text) processing research. Once multi-speaker broadcast speech audio and 
meeting speech audio can be useably transcribed so that first application paradigms for these 
technologies have been achieved, the transcriptions can be further processed by other 
technologies, such as speech summarisation and speech translation technologies. It would be 
very valuable for European speech research if Europe could launch a meeting transcription 
technology evaluation campaign before the US (evaluation campaigns will be discussed 
below). 
Speech summarisation 
Speech summarisation is being experimented with already, often by using text or transcribed 
speech instead of raw speech data. Speech and text summarisation technology including 
intelligent speech and text search would seem to hold enormous potential by enabling users to 
obtain at-a-glance information on the contents of large repositories of information. The same 
applies to related technologies, such as question-answer systems which enable the user to 
obtain answers to specific questions from large repositories of information. Progress in these 
fields is difficult because of the difficulty of the research which remains to be done. However, 
the difficulties ahead are counter-balanced by expectations that far-less-than-perfect solutions 
could help to establish first application paradigms which, in their turn, might help accelerate 
progress. 
Speech translation 
Despite the embattled 40-year history of language (text) translation systems, speech 
translation is now being researched across the world because of the realisation that far-less-
than-perfect paragraph-by-paragraph translation could yield useful applications in the shorter 
term. In their turn, those first application paradigms could serve as drivers of further progress. 
The German Verbmobil project (http://verbmobil.dfki.de/), for instance, demonstrated just 
how difficult human-human spoken dialogue translation is. Once application paradigms have 
been achieved, however, speech translation technology would appear set to gain an enormous 
market. Still, it may take quite some time before there is a massive growth in the market for 
speech translation products, due to the difficulty of the research which remains to be done.  
Conversational spoken dialogue 
For some time, the term ?conversational spoken dialogue? has been a catch-all for next-step 
spoken language dialogue systems, such as those explored in the DARPA Communicator 
 project. However, the DARPA Communicator agenda remains focused on task-oriented 
dialogue, such as flight ticket reservation. Even if conducted through mixed initiative spoken 
dialogue in which the human and the machine exchange dialogue initiative in the course of 
their dialogue about the task, task-oriented spoken dialogue might not qualify as 
conversational spoken dialogue. Conversational spoken dialogue is mixed-initiative, to be 
sure, but in conversational spoken dialogue there is no single task and no limited number of 
distinct tasks which have to be accomplished. Rather, spoken conversation systems may be 
characterised as topic-oriented. It is the breadth and complexity of the topic(s) on which the 
system is able to conduct conversation which determine its strength. Research on spoken 
conversation systems is still limited. Obviously, however, spoken conversation systems hold 
an enormous application potential because they represent the ultimate generalisation of the 
qualities which everybody seem to appreciate in task-oriented mixed initiative spoken 
language dialogue systems.  
Key technologies: multimodal systems 
In addition to speech-only technologies, the timelines in Section 3 highlight a series of 
multimodal speech systems technologies which are still at the research stage in most cases, 
including: 
? intelligent multimodal information presentation including speech; 
? natural interactivity; 
? immersive virtual reality and augmented reality. 
Intelligent multimodal information presentation including speech 
Intelligent multimodal information presentation including speech is a mixed bag of complex 
technologies which do not seem to have any clear research direction at the present time. The 
reason is that the term multimodality, as pointed out in Section 2 above, refers to a virtually 
unlimited space of combinations of (unimodal) modalities. Thus, Modality Theory (Bernsen 
1997b, 2001) has identified an exhaustive developers? toolbox of unimodal input/output 
modalities in the media of graphics (or vision), acoustics (or hearing), and haptics (or touch) 
consisting of more than a hundred unimodal modalities. The number of possible combinations 
of these unimodal input/output modalities is evidently staggering and, so far, at least, no way 
has been found to systematically generate a subset of good and useful modality combinations 
which could be recommended to system developers. The best current approach is to list 
modality combinations which have been found useful already in experimental or development 
practice. Obviously, given the limited exploration of the space of possible modality 
combinations which has taken place so far, those combinations constitute but a tiny fraction 
of the modality combinations which eventually will be used in HHSI. The same lack of 
systematicity applies to the subset of useful modality combinations which include speech 
output and/or speech input. Thus, for instance, it is known that speech and static graphics 
image output is a useful modality combination for some purposes and that the same holds for 
combined speech and pen input into various output domains as well as for speech and 
pointing gesture input into, e.g., a static graphics map output domain. The qualifying term 
intelligent is being used to distinguish intelligent multimodal information presentation 
systems from traditional multimedia presentations. In traditional multimedia presentations, 
the user uses keyboard and mouse (or similar devices) to navigate among a fixed set of output 
options all of which have been incorporated into the system at design-time. In intelligent 
multimodal information presentation systems, the system itself generates intelligent 
multimodal output at run-time. This may happen through run-time language and/or speech 
generation coordinated with run-time graphics image generation and in many other ways as 
 well. Some years ago, a reference model for intelligent multimodal information presentation 
systems was proposed by an international consortium of developers (Computer Standards and 
Interfaces 18, 6-7, 1997). Since then, little systematic development has happened, it appears, 
which is probably due to the fact that the field is as open-ended at it is. Still, it would appear 
that (i) the field of intelligent multimodal information presentation systems is an extremely 
promising approach to complex interactive information presentation, such as in interactive 
systems for instruction tasks for which several output modalities are needed, including 
speech. In order to advance research in this field, research is needed on Modality Theory in 
order to identify potentially useful modality combinations as well as on next-step 
architectures and platforms for intelligent multimodal information presentation. 
Natural interactivity 
As argued in Section 2, fully natural interactive systems represent a necessary vision for a 
large part of the field of interactive systems. Furthermore, spontaneous speech input/output is 
fundamental to natural interactive systems. Given this (latter) fact, it would seem that speech 
research is set to take the leading role in the development of increasingly natural interactive 
systems. Already today, this research and development process can be broken down into a 
comprehensive, semi-ordered agenda of research steps. The steps include, at least, (i) 
fundamental research on human communicative behaviour, including identification of the 
relevant phenomena which are being coordinated in human behaviour across abstraction 
levels and modalities, such as speech prosody and facial expression; validated coding 
schemes for these phenomena; and standard tools for coding the phenomena in order to create 
research and training resources in an efficient and re-usable fashion; (ii) speech and graphics 
integration in order to achieve full run-time coordination of spoken output with lip 
movement, facial expression, gaze, gesture and hand manipulation, and bodily posture; (iii) 
speech and machine vision integration in order to enable the system to carry out run-time 
understanding of spoken input in combination with lip movement, facial expression, gaze, 
gesture and hand manipulation, and bodily posture; and (iv) conversational spoken dialogue 
as discussed above. Other relevant technologies include, i.a., machine learning and 3D 
graphics modelling of human behaviour. Although research in underway on (i) through (iv), 
there is no doubt that the field might benefit strongly from a focused effort which could 
connect the disparate research communities involved and set a stepwise agenda for achieving 
rapid progress. The application prospects are virtually unlimited, as witnessed by the 
consensus in the field that increased natural interaction tends to generate increased trust in 
HHSI.  
Immersive virtual reality and augmented reality 
It is perhaps less clear what are the speech technology application prospects of immersive 
virtual reality. Today, immersive virtual reality requires that users are wired up with 3D 
goggles, force feedback data gloves, data suits, and/or wired surfaces and other wired 
equipment, such as flight cockpits or bicycles. At the present time, it seems uncertain to 
which extent and for which purposes immersive virtual reality technologies will be found 
useful in the future. The primary purposes for which these technologies are being used to day 
are advanced technology exhibition and demonstration, and the building of rather expensive 
simulation setups, such as flight simulators. Furthermore, it is far from clear which role(s) 
speech will come to play in immersive virtual environments. These remarks also apply to 
augmented reality technology. 
Other research and supporting measures needed 
In order to promote efficient research progress on advanced interactive systems which include 
speech as a modality, technology research is far from sufficient. As pointed out in Section 2, 
 present and future advanced systems research takes place in an extremely complex context in 
which leading research efforts must incorporate global state-of-the-art developments in many 
different fields. World-leading speech-related systems research should be accompanied by the 
following kinds of research, at least: 
? state-of-the-art generic platforms; 
? generic architectures; 
? hardware; 
? specialised best practice in development and evaluation; 
? standard re-usable resources; 
? behavioural research; 
? neural basis for human natural communicative behaviour; 
? design of form and contents; 
? porting technologies to languages, cultures and the web; 
? the disabled; 
? maintenance for uptake. 
State-of-the-art generic platforms 
In order to effectively aim at exploitable results from early on, speech-related systems 
research needs to build upon existing state-of-the-art generic platforms including APIs. If a 
state-of-the-art generic platform is not available to the researchers, either because it does not 
yet exist or because it is inaccessible for proprietary reasons, researchers have to build it 
themselves. This is not possible in small-scale research projects which have an additional 
research agenda which presupposes a working platform. The consequence is that the research 
project will either build upon some sub-optimal platform in order to complete the research 
agenda, or build a better platform but not complete the research agenda. Both consequences 
are unacceptable, of course, but the former may work temporarily if the research aims are 
very advanced ones. However, when the research aims have been achieved or, at least, 
somehow explored, there will typically be no practical way of continuing the research in order 
to produce a state-of-the-art generic platform which could bring the research results towards 
the market. Two implications seem to follow: (i) it would be highly desirable if companies 
could be encouraged to make their most advanced platforms accessible to researchers. (ii) If a 
state-of-the-art generic platform is missing altogether, it should either be produced in a 
separate project or projects should be made so large as to include platform development. Both 
implications would seem to require a transformation of existing European research funding 
mechanisms. 
Generic architectures 
It would seem likely that overall research speed and efficiency in Europe could be accelerated 
by research on generic architectures for future systems, such as conversational spoken 
dialogue systems, intelligent multimodal information presentation systems which include 
speech, or natural interactive systems. In the absence of research initiatives on generic 
architectures for future systems, research projects are likely to specify idiosyncratic 
architectures which may satisfy their present needs but which do not sufficiently take into 
account global developments nor prepare for the next steps in advanced systems development. 
For the time being, there does not appear to be any European speech-related initiative in this 
field apart from the CLASS project which was launched in the autumn of 2000 
(http://www.class-tech.org/). For efficiency, work on generic architectures should be done as 
 a collaborative effort between many small-scale research projects and industry as in CLASS, 
or between a medium-scale research project and industry.  
Hardware 
Increasingly, advanced systems demonstrators require hardware design and development. For 
many research laboratories, this is a new challenge which they are ill-prepared to meet. 
Moreover, there is no strong tradition for involving hardware producers in the field of speech 
technologies, primarily because the need for involving them is a rather recent one. Ways must 
be found to forge links with leading hardware producers in order to make emerging hardware 
available to researchers. This problem has much in common with the platform issue discussed 
above. 
Specialised best practice in development and evaluation 
Advanced speech systems research is conducted in a software engineering space bounded by, 
on the one hand, general software engineering best development and evaluation practice and, 
on the other, emerging ISO standards and de facto standards imposed by global industrial 
competition. Between these boundaries lies software engineering best practice in development 
and evaluation specialised for various speech-related systems and component technologies. 
This field remains ill-described in the literature. Apart from the DISC project on best practice 
in the development and evaluation of spoken language dialogue systems (www.disc2.dk), 
some work on evaluation in EAGLES Working Groups during the 1990s 
(http://www.ilc.pi.cnr.it/ EAGLES96/home.html), various national evaluation campaigns, and 
planned work in CLASS, little work has been done in Europe. By contrast, massive work has 
been done on component evaluation in the US over the last fifteen years. The result is that the 
speech-related technology field is replete with trial and error, repetitions of mistakes, and 
generally sub-state-of-the-art approaches. These negative effects are multiplied by the 
presence in the field of a large number of developers who are new to the field. 
Admittedly, the field of software engineering best practice in development and evaluation 
specialised for various speech systems and component technologies is difficult and costly to 
do something about under present conditions. Technology evaluation campaigns are costly to 
do and require serious logistics. Yet the US experience would seem to indicate that 
technology evaluation campaigns are worth the effort if carried out for key emerging 
technologies including some of the technologies described in this paper. When a technology 
has gone to the market, industry does not want to participate any more and rather wants, e.g., 
evaluation toolkits for internal use. For emerging technologies, however, technology 
evaluation campaigns are an efficient means of producing focused progress. In fact, all 
participants tend to become winners in the campaigns irrespective of their comparative 
scorings according to the metrics employed, because everybody involved learns how to 
improve, or when to discard, their technologies and approaches. For Europe, technology 
evaluation campaigns for key emerging technologies could be a means of creating lasting 
advances on its global competitors. In order to take care of the complex logistics needed for 
the campaigns, it is worth considering to establish a European agency similar to the US NIST 
(National Institute for Standards in Technology) whose comprehensive experience with 
technology evaluation campaigns makes it comparatively easy to plan and launch campaigns 
in novel emerging technologies. Alternatively, NIST might be asked to undertake to run 
technology development and evaluation campaigns in Europe, provided that this does not 
offend political and industrial sensibilities too much. 
Effective development best practice work specialised for speech technologies is difficult to do 
under the current European funding mechanisms. The reason is that development best practice 
work requires access to many different components, systems and approaches in order to 
 create an effective environment for the discussion and identification of best practice. This 
environment can only be established across many different small-scale projects or within 
medium-scale projects. CLASS is the first example of such an environment. 
Standard re-usable resources 
The term resources covers raw data resources, annotated data resources, annotation schemes 
for data annotation, and annotation tools for efficient automatic, semi-automatic or manual 
annotation of data. Resources are crucial for many different purposes, such as research into 
coding schemes or the training of components. Also, resources tend to be costly to produce. 
This means that, if the relevant resources are not available, research projects often take the 
easy way out which is to use less relevant but existing and accessible resources for their 
research. The results are sub-optimal research results and slowed-down progress. Common to 
resources of any kind is the need for standardisation. If some resource is not up to the 
required standards, its production is often a waste of effort because the created resource 
cannot be used for anything useful. In its strategy paper from 1991, ELSNET 
(http://www.elsnet.org/) proposed the establishment of a European resources agency. This 
recommendation was adopted through the creation of ELRA (European Language Resources 
Agency http://www.icp.inpg.fr/ELRA/ home.html) in 1995. ELRA is now a world-recognised 
counterpart to the US LDC (Linguistic Data Consortium, http://www.ldc.upenn.edu/). Still, 
ELRA is far from having the capacity to produce on its own all the resources and standards 
needed for efficient research progress. By contrast with technology evaluation campaigns, 
Europe has been active in the resources area during the 1990s. Today, there is a strong need to 
continue activities in producing publicly available resources and standards for advanced 
natural language processing, natural interactive systems development, evaluation campaigns 
as described above, etc. Recently, the ISLE (International Standards for Language 
Engineering) Working Group on Natural Interactivity and Multimodality 
(http://www.isle.nis.sdu.dk) has launched cross-Atlantic collaboration in the field of resources 
for natural interactivity and multimodality. 
Behavioural research 
Humans are still far superior to current systems in all aspects of natural interactive 
communication. Furthermore, far too little is known about the natural interactive behaviour 
which future systems need to be able to reproduce as output or understand as input. There is a 
strong need for basic research into human natural communicative behaviour in order to chart 
the phenomena which future systems need to reproduce or understand. This research will 
immediately feed into the production of natural interactive resources for future systems and 
components development, as described above. 
Neural basis for human natural communicative behaviour 
Related to, but distinct from, basic research into human natural communicative behaviour is 
basic research into the neural basis for human natural communicative behaviour. In the 
heydays of cognitive science in the 1980s, many researchers anticipated steady progress in the 
collaboration between research on speech and language processing, on the one hand, and 
research into the neural machinery which produces human speech and language on the other. 
However, massive difficulties of access to how human natural communicative behaviour is 
being produced by the brain turned out to prevent rapid progress in linking neuroscience with 
speech and language processing research. Today, however, due to the availability of 
technologies such as MR imaging and PET scanning, as well as the increasing sophistication 
of the research agenda for the speech technology field, the question arises if it might be timely 
to re-open the cognitive science agenda just described. Potential results include, among 
others, input to generic architecture development (cf. above), identification of biologically 
 motivated units of processing, such as speech and lip movement coordination, and 
identification of biologically motivated modalities for information representation and 
exchange. Relevant research is already going on in the field of neuroscience but, so far, few 
links have been established to the fields of speech technologies and natural interactive 
systems more generally. 
Design of form and contents 
Yet another consequence of the increasing emphasis on systems as opposed to system 
components is the growing importance of form and contents design. It is a well-established 
fact that design and development for the web requires skills in contents design and contents 
expression which are significantly different from those which have been developed through 
centuries for text on paper. In order to develop good demonstrator systems for the web or 
otherwise, there is a need for strongly upgraded skills in the design and expression of 
multimodal digital contents. For instance, it is far from sufficient to have somehow gleaned 
that speech might be an appropriate modality for some intelligent multimodal information 
presentation instruction system and to have available a state-of-the-art development platform 
for building the system. To actually develop the system, professional expertise in form and 
contents design is required. At the present time, few groups or projects in the speech field are 
adequately staffed to meet this challenge.  
Porting technologies to languages, cultures and the web 
Right now, the gap between the ?have? countries whose researchers have access to advanced 
speech and natural interactivity components and platforms, and the ?have-not? countries 
whose researchers cannot use those technologies for their own purposes because they speak 
different languages and behave differently in natural interactive communication, seems to be 
increasing. There is therefore a need to port advanced technologies to different languages and 
cultures both in Europe and across the world. The market will close the gap eventually in its 
own way, of course. However, in order to rally the full European research potential in the 
field in a timely fashion, it would appear necessary to actively stimulate the porting of 
technologies to new languages and cultures. From a research point of view, the best way to 
make this happen might be to include in medium-to-large-scale projects the best researchers 
from ?have-not? countries even if, by definition, those researchers have to spend significant 
time catching up on basic technologies and resources before being able to actively 
contributing to the research agenda.  
There is another sense of the ?porting technologies? -phrase in which Europe as a whole risks 
falling behind global developments. It is that of porting speech, multimodal and natural 
interactivity technologies to the web. The claim here is not that this is not happening already. 
The claim is that this cannot happen fast enough. In order to increase the speed of porting 
technology to the web, it would seem necessary to strongly promote advanced components 
and systems development for the web. It is far from sufficient to wait until some non-speech 
technology has been marketed for the web, such as electronic commerce applications, and 
then try to ?add speech? to the technology. A much more pro-active stance would appear 
advisable, including a strongly increased emphasis on form and contents design as argued 
above. 
The disabled 
Advanced technologies for the disabled have a tendency to lag behind technology 
development more generally for the simple reason that the potential markets for technologies 
for the disabled are less profitable. Correspondingly, advanced technologies development for 
the disabled tends to be supported by small separate funding programmes rather than being 
integrated into mainstream programme research. In many cases, however, it would appear that 
 systems and components technologies could be developed for any particular group of users 
before being transferred into applications for many other user groups. To the extent that this is 
the case, there may be less of a reason to confine the development of technologies for the 
disabled to any particular research sub-programme. 
Maintenance for uptake 
Finally, the small-scale science paradigm of small and isolated research projects does not at 
all cater for the fact that, in the complex world of advanced systems research, a wealth of 
prototype systems, proto-standard resources, web-based specialised best practice guides, etc., 
are being produced which have nowhere to go at the end of the projects in which they were 
developed. Their chances of industrial uptake, re-use by industry and research, impact on 
their intended users, etc., might become very substantially increased if it were possible to 
maintain them and make them publicly accessible for, say, two years after the end of projects. 
For this to happen, there is a need for (i) a stable web portal which can host the results, such 
as the present HLT (Human Language Technologies) portal under development 
(http://www.HLTCentral.org); (ii) open source clauses in research contracts for technologies 
which have nowhere to go at the end of a project; and (iii) financial support for maintenance. 
These requirement are likely to impose considerable strain of current European research 
support mechanisms. However, with some legal effort and a modest amount of financial 
support, the many research results produced in the speech-related field in Europe which are 
not being taken up immediately and which are not within the remit of ELRA, could gain 
much more impact than is presently the case. 
5. Proposed Actions 
Early preparations for the European Commission?s 6th Framework Programme (FP6) 
including IST (Information Society Technologies) research are now in progress. It is 
premature to make predictions with any degree of certainty as to how the IST part of FP6 will 
shape up. Current information suggests an increased emphasis on basic research compared to 
the present FP5. In addition, it is possible that FP6 will include opportunities for the medium-
scale research initiatives which were called for on several occasions above, i.e. large-scale 
?clusters? of projects all addressing the same research topic in a coordinated fashion. Finally, 
the current covering title for FP6 IST research is ?ambient intelligence? which is one of the 
terms of fashion quoted in the present paper. Given the timelines and their analysis above, it 
does not seem to matter much which covering term is being chosen for FP6. ?Ambient 
intelligence? is as apt as several others for FP6 and future advanced interactive systems 
research but, as argued in Section 3, it is far from clear if ambient intelligence requires us to 
focus on any particular segment of future speech-related technologies. However, the possible, 
increased emphasis on basic research as well as the possibility of carrying out medium-scale 
science in speech-related technologies are to be welcomed in the light of the argument above. 
5.1 Research priorities for speech-related technologies 2000-2010 
Taking into our stride the transformations of the field of speech-related research from speech-
only to interactive systems in general, and from components research to interactive systems 
research, the top priorities in speech-related technologies research are: 
? multi-speaker meeting transcription development and evaluation campaigns; 
? speech summarisation development and evaluation campaigns; 
? speech translation prototypes, generic platforms, and generic architectures. 
Development and evaluation campaigns are highly desirable; 
 ? conversational spoken dialogue prototypes, generic platforms, and generic 
architectures. Development and evaluation campaigns are highly desirable; 
? next-step prototypes, generic platforms, and generic architectures for intelligent 
multimodal information presentation; 
? next-step prototypes, generic platforms, and generic architectures for natural 
interactive systems. 
As soon as theoretically and practically feasible, all of the above advanced speech, 
multimodal and natural interactivity technologies should be developed for the web including 
hardware, form and contents design. The fact that some top research priorities have been 
mentioned above emphatically does not preclude the desirability of continuing ?business as 
usual? in the field of speech-related research, including continued research into all of the 
technologies which have been mentioned earlier in the present paper. On the contrary, 
business as usual is actually assumed by the above top priorities list which focuses on 
technologies over and above business as usual. This also applies to next-step research into 
already deployed speech-related technologies, such as mixed initiative, task-oriented spoken 
dialogue systems. 
For basic research leading to novel concepts, theories and formalisations, the top priorities 
are: 
? basic research into human natural communicative behaviour; 
? a novel theory of natural communication which can replace speech acts theory and 
discourse theory by taking the notion of a complete communicative act as its basic 
notion; 
? research on Modality Theory in order to identify potentially useful modality 
combinations; 
? establishment of collaborative links to research into the neural basis for human natural 
communicative behaviour. 
5.2 Research organisation needed 
Medium-scale science is needed for, at least, the coordinated development of natural 
interactive systems prototypes, generic platforms, generic architectures, best practice in 
development and evaluation, and standard resources. A large, medium-scale science project 
with these objectives should include the porting of technologies to new languages and 
cultures. 
It is quite possible that the medium-scale science model could be applied to research into 
other speech-related technologies, such as speech translation technologies, conversational 
spoken dialogue systems, or speech technologies for ambient intelligence. 
For researchers in small-scale speech-related projects, in particular, the creation of a generic 
platforms and hardware ?bourse? through contributions from European industry would be of 
great importance. 
Finally, we should stop having research programme ghettos for technologies for the disabled. 
5.3 Infrastructural actions needed 
In order to promote maximum uptake of the research results produced, it would be highly 
desirable to have funding for low-cost ways of maintaining research results for later uptake. 
Given the emphasis on technology development and evaluation campaigns above, Europe 
needs to establish an evaluation and standards agency. It is not evident to the present author 
 that current political and industrial sensibilities would allow the US NIST to undertake to run 
technology development and evaluation campaigns in Europe.  
This having been said, there is much to be said for increasing global collaboration on many 
aspects of speech-related research, such as creating a coordinated global infrastructure for 
resources distribution.  
References 
Benoit, C., Martin, J. C., Pelachaud, C., Schomaker, L., and Suhm, B.: Audio-Visual and 
Multimodal Speech-Based Systems. In D. Gibbon, I. Mertens and R. Moore (Eds.): 
Handbook of Multimodal and Spoken Dialogue Systems. Dordrecht: Kluwer Academic 
Publishers 2000, 102-203. 
Bernsen, N. O.(1997a): Towards a tool for predicting speech functionality. Speech 
Communication 23, 1997, 181-210. 
Bernsen, N. O. (1997b): Defining a Taxonomy of Output Modalities from an HCI 
Perspective. Computer Standards and Interfaces, Special Double Issue, 18, 6-7, 1997, 537-
553. 
Bernsen, N. O.: Multimodality in language and speech systems - from theory to design 
support tool. In Granstr?m, B. (Ed.): Multimodality in Language and Speech Systems. 
Dordrecht: Kluwer Academic Publishers 2001 (to appear). 
CLASS: http://www.class-tech.org/ 
Computer Standards and Interfaces, Special Double Issue, 18, 6-7, 1997. 
DARPA Communicator: http://fofoca.mitre.org/ 
DISC www.disc2.dk 
EAGLES: http://www.ilc.pi.cnr.it/EAGLES96/home.html 
ELRA: http://www.icp.inpg.fr/ELRA/home.html 
ELSNET http://www.elsnet.org/ 
i3: http://www.i3net.org/ 
ISLE: http://www.ilc.pi.cnr.it/EAGLES96/isle/ISLE_Home_Page.htm 
ISLE Working Group on Natural Interactivity and Multimodality: http://www.isle.nis.sdu.dk 
HLT portal: http://www.HLTCentral.org 
LDC http://www.ldc.upenn.edu/ 
SmartKom: http://smartkom.dfki.de/start.html 
Verbmobil: http://verbmobil.dfki.de/ 
 
CHALLENGES APPROACHES/SOLUTIONS
? ?
ONTOLOGY Challenges
Ontology Creation/Population 
Tasks <-> Tools - reusable across domains 
Understand a process model (and humans
role in this) 
Semantic Web 
User-centered process view 
Convert the (HCI) disbelievers and keep them
practicing 
"top" or core ontology (use this to bootstrap
new domains), Ontology integration 
Rapid customization (to specific domains) 
Use domain specific ontologies to organize
massive documents 
Find, learn, collaboration with domain
ontology creators 
Integration of shallow/deep methods
ONTOLOGY Problems
- Ontology quality
- Access to info, knowledge visualizations
- Understanding
- Ambiguity
ONTOLOGY Approaches
Relation of HLT to ontological tasks 
KR, linguisits, & ontologies to jointly
address 
Component based methods for 
Life cycle 
Re-use 
Decomposition 
Use HLT to support knowledge audits
> Identify IP -> innovation 
Context capture 
Controlled, language management
ONTOLOGY Solutions
Plug-in (for IE) 
Semantic Web 
Tools to leverage small ontologies ->
large ontologies
 
? ?
?
SUMMARIZATION Challenges: 
level/depth of analysis/representation ?E.g., Speech acts,
RST, semantic rels? 
Sumarization presentation/visualization 
Speech ?not good for long texts? 
Indicative vs. inforamtive, concepts vs. ideas 
Action-oriented summaries ?e.g., executive/management 
summaries?
?
SUMMARIZATION Solutions
- Analysis -> transformation -> presentation
25 Apr 2007  02:10Generated by HTML_ToPDF at rustyparts.com - 1 -
Notes
? ?
MULTILINGUAL Problems
Relational between cultures, languages, lexical resources,
ontologies 
Domain knowledge 
Fine-grained linguistic knowledge ?e.g., stylistic details? 
Size, complexity 200 languages -> 39k language pairs 
Language invisibility
large-scale, robust NLP 
Adaptation/integration of semantic resources 
Content-driven hypertextual authoring 
Cross-lingual news linking 
Advanced software technologies/platform 
Communication/transaction success
?
?
 
MULTILINGUAL Solutions
resources: wordnet, euronet, application
database, text resources 
Interlingua approach 
Statistical -> deeply annotated data + machine
learning 
Translation memories + ML 
Multimodal/multimedia sols 
Multiple ontologies tailored to users, tasks
? ?
MULTIMEDIA Challenges 
- Processing centralized/mobile
- Privacy, security, scaleability
Remembering + forgeting 
multilingual and multisource IE incremental
information building 
cross-document co-ref resolution
MULTIMEDIA Solutions
Location-based services 
"forgetting"
Input to a Technology Road Map:
Enabling Technologies/Infrastructure
Mobile communications 
Push service failures (e.g., pointcast) 
Satellite communication bankruptcy 
Fibre explosion
Services
25 Apr 2007  02:10Generated by HTML_ToPDF at rustyparts.com - 2 -
Notes
video on demand failure need for content based access
Resources
RDF, DAML, OIL? 
Ease of integration 
IE, NE
Fundamental/Hard Problems
Noisy Speech Recognition 
Non-literal language 
Semantic web (e.g., who is going to populate it)
Ontologies
Auto Web Taxonomy Generation 
High Quality MT
/\
||
- Tools for ontology generation, merging
Free CYC?
Summarization
"conceptual" or "content" level diff (email, documents, patents)
Query dependent, Multiple perspective Summarization (representation and output)
/\ /\ /\
|| || ||
entity discourse co-ref
Multilingual
interlingua 
deeply annotated data + ML 
user appropriate translations 
English Interlingua
Multimedia
25 Apr 2007  02:10Generated by HTML_ToPDF at rustyparts.com - 3 -
Notes
personalized content based news 
multimedia I/O (maps, gesture)
/\
||
multimedia data and annotation (images, maps, video, medical)
Standards
Process Reusable interchangeable modules (e.g., POS, NE) 
Data (XML, text encoding, W3C)
NLP
Robust, deep language processing (e.g. LFG parsing which is fast but inaccurate still)
KM/Information Integration
Integrated mining, query of mail, DB, process knowledge
CORE ENABLING RESOURCES
- (intelligent) text annotation (feeds all areas)
- large annotated corpora
 
25 Apr 2007  02:10Generated by HTML_ToPDF at rustyparts.com - 4 -
Notes
