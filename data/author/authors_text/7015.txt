Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 49?56,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Re-Ranking Algorithms for Name Tagging 
 
Heng Ji Cynthia Rudin Ralph Grishman 
Dept. of Computer Science Center for Neural Science and Courant 
Institute of Mathematical Sciences 
Dept. of Computer Science 
New York University 
New York, N.Y. 10003 
hengji@cs.nyu.edu rudin@nyu.edu grishman@cs.nyu.edu 
 
 
 
 
Abstract 
Integrating information from different 
stages of an NLP processing pipeline can 
yield significant error reduction. We dem-
onstrate how re-ranking can improve name 
tagging in a Chinese information extrac-
tion system by incorporating information 
from relation extraction, event extraction, 
and coreference. We evaluate three state-
of-the-art re-ranking algorithms (MaxEnt-
Rank, SVMRank, and p-Norm Push Rank-
ing), and show the benefit of multi-stage 
re-ranking for cross-sentence and cross-
document inference. 
1 Introduction 
In recent years, re-ranking techniques have been 
successfully applied to enhance the performance 
of NLP analysis components based on generative 
models. A baseline generative model produces N-
best candidates, which are then re-ranked using a 
rich set of local and global features in order to 
select the best analysis. Various supervised learn-
ing algorithms have been adapted to the task of re-
ranking for NLP systems, such as MaxEnt-Rank 
(Charniak and Johnson, 2005; Ji and Grishman, 
2005), SVMRank (Shen and Joshi, 2003), Voted 
Perceptron (Collins, 2002; Collins and Duffy, 
2002; Shen and Joshi, 2004), Kernel Based Meth-
ods (Henderson and Titov, 2005), and RankBoost 
(Collins, 2002; Collins and Koo, 2003; Kudo et al, 
2005). 
These algorithms have been used primarily 
within the context of a single NLP analysis com-
ponent, with the most intensive study devoted to 
improving parsing performance. The re-ranking 
models for parsing, for example, normally rely on 
structures generated within the baseline parser 
itself. Achieving really high performance for some 
analysis components, however, requires that we 
take a broader view, one that looks outside a sin-
gle component in order to bring to bear knowl-
edge from the entire NL analysis process.  In this 
paper we will demonstrate the potential of this 
approach in enhancing the performance of Chi-
nese name tagging within an information extrac-
tion application.  
Combining information from other stages in the 
analysis pipeline allows us to incorporate informa-
tion from a much wider context, spanning the en-
tire document and even going across documents.  
This will give rise to new design issues; we will 
examine and compare different re-ranking algo-
rithms when applied to this task.  
We shall first describe the general setting and 
the special characteristics of re-ranking for name 
tagging. Then we present and evaluate three re-
ranking algorithms ? MaxEnt-Rank, SVMRank 
and a new algorithm, p-Norm Push Ranking ? for 
this problem, and show how an approach based on 
multi-stage re-ranking can effectively handle fea-
tures across sentence and document boundaries. 
2 Prior Work 
2.1 Ranking 
We will describe the three state-of-the-art super-
vised ranking techniques considered in this work. 
Later we shall apply and evaluate these algorithms 
for re-ranking in the context of name tagging. 
Maximum Entropy modeling (MaxEnt) has 
been extremely successful for many NLP classifi-
49
cation tasks, so it is natural to apply it to re-
ranking problems. (Charniak and Johnson, 2005) 
applied MaxEnt to improve the performance of a 
state-of-art parser; also in (Ji and Grishman, 2005) 
we used it to improve a Chinese name tagger.  
Using SVMRank, (Shen and Joshi, 2003) 
achieved significant improvement on parse re-
ranking. They compared two different sample 
creation methods, and presented an efficient train-
ing method by separating the training samples into 
subsets.  
The last approach we consider is a boosting-
style approach. We implement a new algorithm 
called p-Norm Push Ranking (Rudin, 2006). This 
algorithm is a generalization of RankBoost 
(Freund et al 1998) which concentrates specifi-
cally on the top portion of a ranked list. The pa-
rameter ?p? determines how much the algorithm 
concentrates at the top.  
2.2 Enhancing Named Entity Taggers 
There have been a very large number of NE tagger 
implementations since this task was introduced at 
MUC-6 (Grishman and Sundheim, 1996).  Most 
implementations use local features and a unifying 
learning algorithm based on, e.g., an HMM, Max-
Ent, or SVM. Collins (2002) augmented a baseline 
NE tagger with a re-ranker that used only local, 
NE-oriented features.  Roth and Yih (2002) com-
bined NE and semantic relation tagging, but 
within a quite different framework (using a linear 
programming model for joint inference). 
3 A Framework for Name Re-Ranking 
3.1 The Information Extraction Pipeline 
The extraction task we are addressing is that of the 
Automatic Content Extraction (ACE)1 evaluations. 
The 2005 ACE evaluation had 7 types of entities, 
of which the most common were PER (persons), 
ORG (organizations), LOC (natural locations) and 
GPE (?geo-political entities? ? locations which are 
also political units, such as countries, counties, 
and cities).  There were 6 types of semantic rela-
tions, with 18 subtypes.  Examples of these rela-
tions are ?the CEO of Microsoft? (an 
organization-affiliation relation), ?Fred?s wife? (a 
                                                          
1 The ACE task description can be found at 
http://www.itl.nist.gov/iad/894.01/tests/ace/ 
personal-social relation), and ?a military base in 
Germany? (a located relation). And there were 8 
types of events, with 33 subtypes, such as ?Kurt 
Schork died in Sierra Leone yesterday? (a Die 
event), and ?Schweitzer founded a hospital in 
1913? (a Start-Org event). 
  To extract these elements we have developed a 
Chinese information extraction pipeline that con-
sists of the following stages: 
? Name tagging and name structure parsing 
(which identifies the internal structure of some 
names); 
? Coreference resolution, which links "men-
tions" (referring phrases of selected semantic 
types) into "entities": this stage is a combina-
tion of high-precision heuristic rules and 
maximum entropy models; 
? Relation tagging, using a K-nearest-neighbor 
algorithm to identify relation types and sub-
types; 
? Event patterns, semi-automatically extracted 
from ACE training corpora. 
3.2 Hypothesis Representation and Genera-
tion 
Again, the central idea is to apply the baseline 
name tagger to generate N-Best multiple hypothe-
ses for each sentence; the results from subsequent 
components are then exploited to re-rank these 
hypotheses and the new top hypothesis is output 
as the final result. 
In our name re-ranking model, each hypothesis 
is an NE tagging of the entire sentence. For ex-
ample, ?<PER>John</PER> was born in 
<GPE>New York</GPE>.? is one hypothesis 
for the sentence ?John was born in New York?. 
We apply a HMM tagger to identify four named 
entity types: Person, GPE, Organization and Loca-
tion. The HMM tagger generally follows the 
Nymble model (Bikel et al 1997), and uses best-
first search to generate N-Best hypotheses. It also 
computes the ?margin?, which is the difference 
between the log probabilities of the top two hy-
potheses.  This is used as a rough measure of con-
fidence in the top hypothesis. A large margin 
indicates greater confidence that the first hypothe-
sis is correct. The margin also determines the 
number of hypotheses (N) that we will store. Us-
ing cross-validation on the training data, we de-
termine the value of N required to include the best 
50
hypothesis, as a function of the margin.  We then 
divide the margin into ranges of values, and set a 
value of N for each range, with a maximum of 30. 
To obtain the training data for the re-ranking 
algorithm, we separate the name tagging training 
corpus into k folders, and train the HMM name 
tagger on k-1 folders. We then use the HMM to 
generate N-Best hypotheses H = {h1, h2,?,hN} for 
each sentence in the remaining folder.  Each hi in 
H is then paired with its NE F-measure, measured 
against the key in the annotated corpus. 
We define a ?crucial pair? as a pair of hypothe-
ses such that, according to F-Measure, the first 
hypothesis in the pair should be more highly 
ranked than the second. That is, if for a sentence, 
the F-Measure of hypothesis hi is larger than that 
of hj, then (hi, hj) is a crucial pair. 
3.3 Re-Ranking Functions 
We investigated the following three different for-
mulations of the re-ranking problem: 
? Direct Re-Ranking by Score 
For each hypothesis hi, we attempt to learn a scor-
ing function f : H ? R, such that f(hi) > f(hj) if the 
F-Measure of hi is higher than the F-measure of hj. 
? Direct Re-Ranking by Classification 
For each hypothesis hi, we attempt to learn f : H 
? {-1, 1}, such that f(hi) = 1 if hi has the top F-
Measure among H; otherwise f(hi) = -1. This can 
be considered a special case of re-ranking by 
score. 
? Indirect Re-Ranking Function 
For each ?crucial? pair of hypotheses (hi, hj), we 
learn f : H ? H ? {-1, 1}, such that f(hi, hj) = 1 if 
hi is better than hj; f (hi, hj) = -1 if hi is worse than 
hj. We call this ?indirect? ranking because we 
need to apply an additional decoding step to pick 
the best hypothesis from these pair-wise compari-
son results. 
4 Features for Re-Ranking 
4.1 Inferences From Subsequent Stages 
Information extraction is a potentially symbiotic 
pipeline with strong dependencies between stages 
(Roth and Yih, 2002&2004; Ji and Grishman, 
2005). Thus, we use features based on the output 
of four subsequent stages ? name structure parsing, 
relation extraction, event patterns, and coreference 
analysis ? to seek the best hypothesis.  
We included ten features based on name struc-
ture parsing to capture the local information 
missed by the baseline name tagger such as details 
of the structure of Chinese person names. 
The relation and event re-ranking features are 
based on matching patterns of words or constitu-
ents.  They serve to correct name boundary errors 
(because such errors would prevent some patterns 
from matching).  They also exert selectional pref-
erences on their arguments, and so serve to correct 
name type errors.  For each relation argument, we 
included a feature whose value is the likelihood 
that relation appears with an argument of that se-
mantic type (these probabilities are obtained from 
the training corpus and binned).  For each event 
pattern, a feature records whether the types of the 
arguments match those required by the pattern. 
Coreference can link multiple mentions of 
names provided they have the same spelling 
(though if a name has several parts, some may be 
dropped) and same semantic type. So if the 
boundary or type of one mention can be deter-
mined with some confidence, coreference can be 
used to disambiguate other mentions, by favoring 
hypotheses which support more coreference. To 
this end, we incorporate several features based on 
coreference, such as the number of mentions re-
ferring to a name candidate.  
Each of these features is defined for individual 
name candidates; the value of the feature for a 
hypothesis is the sum of its values over all names 
in the hypothesis. The complete set of detailed 
features is listed in (Ji and Grishman, 2006). 
4.2 Handling Cross-Sentence Features by 
Multi-Stage Re-Ranking 
Coreference is potentially a powerful contributor 
for enhancing NE recognition, because it provides 
information from other sentences and even docu-
ments, and it applies to all sentences that include 
names. For a name candidate, 62% of its corefer-
ence relations span sentence boundaries.  How-
ever, this breadth poses a problem because it 
means that the score of a hypothesis for a given 
51
sentence may depend on the tags assigned to the 
same names in other sentences.2 
Ideally, when we re-rank the hypotheses for one 
sentence S, the other sentences that include men-
tions of the same name should already have been 
re-ranked, but this is not possible because of the 
mutual dependence. Repeated re-ranking of a sen-
tence would be time-consuming, so we have 
adopted an alternative approach. Instead of incor-
porating coreference evidence with all other in-
formation in one re-ranker, we apply two re-
rankers in succession.  
   In the first re-ranking step, we generate new 
rankings for all sentences based on name structure, 
relation and event features, which are all sentence-
internal evidence.  Then in a second pass, we ap-
ply a re-ranker based on coreference between the 
names in each hypothesis of sentence S and the 
mentions in the top-ranking hypothesis (from the 
first re-ranker) of all other sentences.3  In this way, 
the coreference re-ranker can propagate globally 
(across sentences and documents) high-confidence 
decisions based on the other evidence. In our final 
MaxEnt Ranker we obtained a small additional 
gain by further splitting the first re-ranker into 
three separate steps: a name structure based re-
ranker, a relation based re-ranker and an event 
based re-ranker; these were incorporated in an 
incremental structure.   
4.3 Adding Cross-Document Information 
The idea in coreference is to link a name mention 
whose tag is locally ambiguous to another men-
tion that is unambiguously tagged based on local 
evidence.  The wider a net we can cast, the greater 
the chance of success.  To cast the widest net pos-
sible, we have used cross-document coreference 
for the test set. We cluster the documents using a 
cross-entropy metric and then treat the entire clus-
ter as a single document.      
We take all the name candidates in the top N 
hypotheses for each sentence in each cluster T to 
construct a ?query set? Q. The metric used for the 
clustering is the cross entropy H(T, d) between the 
distribution of the name candidates in T and 
                                                          
2 For in-document coreference, this problem could be avoided if the tagging of 
an entire document constituted a hypothesis, but that would be impractical ? a 
very large N would be required to capture sufficient alternative taggings in an 
N-best framework. 
3 This second pass is skipped for sentences for which the confidence in the top 
hypothesis produced by the first re-ranker is above a threshold. 
document d. If H(T, d) is smaller than a threshold 
then we add d to T. H(T, d) is defined by: 
?
?
??=
Qx
xdprobxTprobdTH ),(log),(),( . 
We built these clusters two ways: first, just 
clustering the test documents; second, by aug-
menting these clusters with related documents 
retrieved from a large unlabeled corpus (with 
document relevance measured using cross-
entropy). 
5 Re-Ranking Algorithms 
We have been focusing on selecting appropriate 
ranking algorithms to fit our application. We 
choose three state-of-the-art ranking algorithms 
that have good generalization ability. We now 
describe these algorithms. 
5.1 MaxEnt-Rank 
5.1.1  Sampling and Pruning 
 
Maximum Entropy models are useful for the task 
of ranking because they compute a reliable rank-
ing probability for each hypothesis.  We have tried 
two different sampling methods ? single sampling 
and pairwise sampling.  
The first approach is to use each single hy-
pothesis hi as a sample. Only the best hypothesis 
of each sentence is regarded as a positive sample; 
all the rest are regarded as negative samples. In 
general, absolute values of features are not good 
indicators of whether a hypothesis will be the best 
hypothesis for a sentence; for example, a co-
referring mention count of 7 may be excellent for 
one sentence and poor for another.  Consequently, 
in this single-hypothesis-sampling approach, we 
convert each feature to a Boolean value, which is 
true if the original feature takes on its maximum 
value (among all hypotheses) for this hypothesis.  
This does, however, lose some of the detail about 
the differences between hypotheses. 
In pairwise sampling we used each pair of hy-
potheses (hi, hj) as a sample. The value of a fea-
ture for a sample is the difference between its 
values for the two hypotheses.  However, consid-
ering all pairs causes the number of samples to 
grow quadratically (O(N2)) with the number of 
hypotheses, compared to the linear growth with 
best/non-best sampling. To make the training and 
52
test procedures more efficient, we prune the data 
in several ways.  
We perform pruning by beam setting, removing 
candidate hypotheses that possess very low prob-
abilities from the HMM, and during training we 
discard the hypotheses with very low F-measure 
scores. Additionally, we incorporate the pruning 
techniques used in (Chiang 2005), by which any 
hypothesis with a probability lower than?times 
the highest probability for one sentence is dis-
carded. We also discard the pairs very close in 
performance or probability. 
 
5.1.2 Decoding 
 
If f is the ranking function, the MaxEnt model 
produces a probability for each un-pruned ?cru-
cial? pair: prob(f(hi, hj) = 1), i.e., the probability 
that for the given sentence, hi is a better hypothe-
sis than hj. We need an additional decoding step to 
select the best hypothesis. Inspired by the caching 
idea and the multi-class solution proposed by 
(Platt et al 2000), we use a dynamic decoding 
algorithm with complexity O(n) as follows. 
We scale the probability values into three types: 
CompareResult (hi, hj) = ?better? if prob(f(hi, hj) = 
1) >?1, ?worse? if prob(f(hi, hj) = 1) <?2, and 
?unsure? otherwise, where ?1??2. 4  
 
Prune 
for i = 1 to n 
Num = 0; 
for j = 1 to n and j?i 
If CompareResult(hi, hj) = ?worse? 
Num++; 
    if Num>?then discard hi from H 
 
Select 
Initialize: i = 1, j = n 
while (i<j) 
if CompareResult(hi, hj) = ?better? 
discard hj from H; 
j--; 
else if CompareResult(hi, hj) = ?worse? 
discard hi from H; 
i++; 
else break; 
 
                                                          
4 In the final stage re-ranker we use?1=?2 so that we don?t generate the 
output of ?unsure?, and one hypothesis is finally selected. 
Output 
If the number of remaining hypotheses in H is 1, 
then output it as the best hypothesis; else propa-
gate all hypothesis pairs into the next re-ranker. 
5.2 SVMRank 
We implemented an SVM-based model, which 
can theoretically achieve very low generalization 
error. We use the SVMLight package (Joachims, 
1998), with the pairwise sampling scheme as for 
MaxEnt-Rank. In addition we made the following 
adaptations: we calibrated the SVM outputs, and 
separated the data into subsets. 
To speed up training, we divided our training 
samples into k subsets. Each subset contains N(N-
1)/k pairs of hypotheses of each sentence.  
In order to combine the results from these dif-
ferent SVMs, we must calibrate the function val-
ues; the output of an SVM yields a distance to the 
separating hyperplane, but not a probability. We 
have applied the method described in (Shen and 
Joshi, 2003), to map SVM?s results to probabili-
ties via a sigmoid. Thus from the kth SVM, we get 
the probability for each pair of hypotheses: 
)1),(( =jik hhfprob , 
namely the probability of hi being better than hj. 
Then combining all k SVMs? results we get: 
       ? ==
k
jikji hhfprobhhZ )1),((),( . 
So the hypothesis hi with maximal value is cho-
sen as the top hypothesis:  
?
j
ji
h
hhZ
i
)),((maxarg . 
5.3 P-Norm Push Ranking 
The third algorithm we have tried is a general 
boosting-style supervised ranking algorithm called 
p-Norm Push Ranking (Rudin, 2006). We de-
scribe this algorithm in more detail since it is quite 
new and we do not expect many readers to be fa-
miliar with it.  
The parameter ?p? determines how much em-
phasis (or ?push?) is placed closer to the top of the 
ranked list, where p?1. The p-Norm Push Ranking 
algorithm generalizes RankBoost (take p=1 for 
RankBoost). When p is set at a large value, the 
rankings at the top of the list are given higher pri-
ority (a large ?push?), at the expense of possibly 
making misranks towards the bottom of the list. 
53
Since for our application, we do not care about the 
rankings at the bottom of the list (i.e., we do not 
care about the exact rank ordering of the bad hy-
potheses), this algorithm is suitable for our prob-
lem. There is a tradeoff for the choice of p; larger 
p yields more accurate results at the very top of 
the list for the training data. If we want to consider 
more than simply the very top of the list, we may 
desire a smaller value of p. Note that larger values 
of p also require more training data in order to 
maintain generalization ability (as shown both by 
theoretical generalization bounds and experi-
ments). If we want large p, we must aim to choose 
the largest value of p that allows generalization, 
given our amount of training data. When we are 
working on the first stage of re-ranking, we con-
sider the whole top portion of the ranked list, be-
cause we use the rank in the list as a feature for 
the next stage. Thus, we have chosen the value 
p1=4 (a small ?push?) for the first re-ranker. For 
the second re-ranker we choose p2=16 (a large 
?push?). 
The objective of the p-Norm Push Ranking al-
gorithm is to create a scoring function f: H?R 
such that for each crucial pair (hi, hj), we shall 
have f(hi) > f(hj). The form of the scoring function 
is f(hi) = ??kgk(hi), where gk is called a weak 
ranker: gk : H ? [0,1]. The values of ?k are de-
termined by the p-Norm Push algorithm in an it-
erative way.  
The weak rankers gk are the features described 
in Section 4. Note that we sometimes allow the 
algorithm to use both gk and g?k(hi)=1-gk(hi) as 
weak rankers, namely when gk has low accuracy 
on the training set; this way the algorithm itself 
can decide which to use.  
As in the style of boosting algorithms, real-
valued weights are placed on each of the training 
crucial pairs, and these weights are successively 
updated by the algorithm. Higher weights are 
given to those crucial pairs that were misranked at 
the previous iteration, especially taking into ac-
count the pairs near the top of the list. At each 
iteration, one weak ranker gk is chosen by the al-
gorithm, based on the weights. The coefficient ?k 
is then updated accordingly.  
6 Experiment Results 
6.1 Data and Resources 
We use 100 texts from the ACE 04 training corpus 
for a blind test. The test set included 2813 names: 
1126 persons, 712 GPEs, 785 organizations and 
190 locations. The performance is measured via 
Precision (P), Recall (R) and F-Measure (F). 
The baseline name tagger is trained from 2978 
texts from the People?s Daily news in 1998 and 
also 1300 texts from ACE training data.  
The 1,071,285 training samples (pairs of hy-
potheses) for the re-rankers are obtained from the 
name tagger applied on the ACE training data, in 
the manner described in Section 3.2. 
We use OpenNLP5 for the MaxEnt-Rank ex-
periments. We use SVMlight (Joachims, 1998) for 
SVMRank, with a linear kernel and the soft mar-
gin parameter set to the default value. For the p-
Norm Push Ranking, we apply 33 weak rankers, 
i.e., features described in Section 4. The number 
of iterations was fixed at 110, this number was 
chosen by optimizing the performance on a devel-
opment set of 100 documents. 
6.2 Effect of Pairwise Sampling 
We have tried both single-hypothesis and pairwise 
sampling (described in section 5.1.1) in MaxEnt-
Rank and p-Norm Push Ranking. Table 1 shows 
that pairwise sampling helps both algorithms. 
MaxEnt-Rank benefited more from it, with preci-
sion and recall increased 2.2% and 0.4% respec-
tively. 
 
Model P R F 
Single Sampling 89.6 90.2 89.9MaxEnt-
Rank Pairwise Sampling 91.8 90.6 91.2
Single Sampling 91.4 89.6 90.5p-Norm 
Push Pairwise Sampling 91.2 90.8 91.0
 
Table 1. Effect of Pairwise Sampling 
6.3 Overall Performance 
In Table 2 we report the overall performance for 
these three algorithms. All of them achieved im-
provements on the baseline name tagger. MaxEnt 
yields the highest precision, while p-Norm Push 
Ranking with p2 = 16 yields the highest recall. 
A larger value of ?p? encourages the p-Norm 
Push Ranking algorithm to perform better near the 
top of the ranked list. As we discussed in section 
                                                          
5 http://maxent.sourceforge.net/index.html 
54
5.3, we use p1 = 4 (a small ?push?) for the first re-
ranker and p2 = 16 (a big ?push?) for the second 
re-ranker. From Table 2 we can see that p2 = 16 
obviously performed better than p2 = 1. In general, 
we have observed that for p2 ?16, larger p2 corre-
lates with better results. 
 
Model P R F 
Baseline  87.4 87.6 87.5
MaxEnt-Rank 91.8 90.6 91.2
SVMRank 89.5 90.1 89.8
p-Norm Push Ranking (p2 =16) 91.2 90.8 91.0
p-Norm Push Ranking  
(p2 =1, RankBoost) 
89.3 89.7 89.5
 
Table 2. Overall Performance  
The improved NE results brought better per-
formance for the subsequent stages of information 
extraction too. We use the NE outputs from Max-
Ent-Ranker as inputs for coreference resolver and 
relation tagger. The ACE value6 of entity detec-
tion (mention detection + coreference resolution) 
is increased from 73.2 to 76.5; the ACE value of 
relation detection is increased from 34.2 to 34.8. 
6.4 Effect of Cross-document Information 
As described in Section 4.3, our algorithm incor-
porates cross-document coreference information. 
The 100 texts in the test set were first clustered 
into 28 topics (clusters). We then apply cross-
document coreference on each cluster. Compared 
to single document coreference, cross-document 
coreference obtained 0.5% higher F-Measure, us-
ing MaxEnt-Ranker, improving performance for 
15 of these 28 clusters. 
These clusters were then extended by selecting 
84 additional related texts from a corpus of 15,000 
unlabeled Chinese news articles (using a cross-
entropy metric to select texts). 24 clusters gave 
further improvement, and an overall 0.2% further 
improvement on F-Measure was obtained.  
6.5 Efficiency 
Model Training Test 
MaxEnt-Rank 7 hours 55 minutes
SVMRank 48 hours 2 hours 
p-Norm Push Ranking 3.2 hours 10 minutes
 
Table 3. Efficiency Comparison 
                                                          
6 The ACE04 value scoring metric can be found at: 
http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-v7.pdf 
In Table 3 we summarize the running time of 
these three algorithms in our application. 
7 Discussion 
We have shown that the other components of an 
IE pipeline can provide information which can 
substantially improve the performance of an NE 
tagger, and that these improvements can be real-
ized through a variety of re-ranking algorithms.  
MaxEnt re-ranking using binary sampling and p-
Norm Push Ranking proved about equally effec-
tive.7  p-Norm Push Ranking was particularly ef-
ficient for decoding (about 10 documents / 
minute), although no great effort was invested in 
tuning these procedures for speed. 
We presented methods to handle cross-sentence 
inference using staged re-ranking and to incorpo-
rate additional evidence through document clus-
tering. 
An N-best / re-ranking strategy has proven ef-
fective for this task because with relatively small 
values of N we are already able to include highly-
rated hypotheses for most sentences.  Using the 
values of N we have used throughout (dependent 
on the margin of the baseline HMM, but never 
above 30), the upper bound of N-best performance 
(if we always picked the top-scoring hypothesis) 
is 97.4% recall, 96.2% precision, F=96.8%. 
Collins (2002) also applied re-ranking to im-
prove name tagging. Our work has addressed both 
name identification and classification, while his 
only evaluated name identification.  Our re-ranker 
used features from other pipeline stages, while his 
were limited to local features involving lexical 
information and 'word-shape' in a 5-token window.  
Since these feature sets are essentially disjoint, it 
is quite possible that a combination of the two 
could yield even further improvements. His boost-
ing algorithm is a modification of the method in 
(Freund et al, 1998), an adaptation of AdaBoost, 
whereas our p-Norm Push Ranking algorithm can 
emphasize the hypotheses near the top, matching 
our objective. 
Roth and Yih (2004) combined information 
from named entities and semantic relation tagging, 
adopting a similar overall goal but using a quite 
different approach based on linear programming.  
                                                          
7 The features were initially developed and tested using the MaxEnt re-ranker, 
so it is encouraging that they worked equally well with the p-Norm Push 
Ranker without further tuning. 
55
They limited themselves to name classification, 
assuming the identification given.  This may be a 
natural subtask for English, where capitalization is 
a strong indicator of a name, but is much less use-
ful for Chinese, where there is no capitalization or 
word segmentation, and boundary errors on name 
identification are frequent. Expanding their ap-
proach to cover identification would have greatly 
increased the number of hypotheses and made 
their approach slower.  In contrast, we adjust the 
number of hypotheses based on the margin in or-
der to maintain efficiency while minimizing the 
chance of losing a high-quality hypothesis. 
In addition we were able to capture selectional 
preferences (probabilities of semantic types as 
arguments of particular semantic relations as 
computed from the corpus), whereas Roth and Yih 
limited themselves to hard (boolean) type con-
straints. 
Acknowledgment 
This material is based upon work supported by the 
Defense Advanced Research Projects Agency un-
der Contract No. HR0011-06-C-0023, and the Na-
tional Science Foundation under Grant IIS-
00325657 and a postdoctoral research fellowship.  
Any opinions, findings and conclusions expressed 
in this material are those of the authors and do not 
necessarily reflect the views of the U. S. Govern-
ment. 
References  
Daniel M. Bikel, Scott Miller, Richard Schwartz, and 
Ralph Weischedel. 1997. Nymble: a high-
performance Learning Name-finder.  Proc. 
ANLP1997. pp. 194-201. Washington, D.C.  
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
Fine N-Best Parsing and MaxEnt Discriminative 
Reranking. Proc. ACL2005. pp. 173-180. Ann Arbor, 
USA 
David Chiang. 2005. A Hierarchical Phrase-Based 
Model for Statistical Machine Translation. Proc. 
ACL2005. pp. 263-270. Ann Arbor, USA 
Michael Collins. 2002. Ranking Algorithms for 
Named-Entity Extraction: Boosting and the Voted 
Perceptron. Proc. ACL 2002. pp. 489-496 
Michael Collins and Nigel Duffy. 2002. New Ranking 
Algorithms for Parsing and Tagging: Kernels over 
Discrete Structures, and the Voted Perceptron. Proc. 
ACL2002. pp. 263-270. Philadelphia, USA 
Michael Collins and Terry Koo. 2003. Discriminative 
Reranking for Natural Language Parsing. Journal of 
Association for Computational Linguistics. pp. 175-
182. 
Yoav Freund, Raj Iyer, Robert E. Schapire and Yoram 
Singer. 1998. An efficient boosting algorithm for 
combining  preferences. Machine Learning: Pro-
ceedings of the Fifteenth International Conference. 
pp. 170-178 
Ralph Grishman and Beth Sundheim.  1996. Message 
understanding conference - 6: A brief history. Proc. 
COLING1996,. pp. 466-471. Copenhagen. 
James Henderson and Ivan Titov. 2005. Data-Defined 
Kernels for Parse Reranking Derived from Probabil-
istic Models. Proc. ACL2005. pp. 181-188. Ann Ar-
bor, USA. 
Heng Ji and Ralph Grishman. 2005. Improving Name 
Tagging by Reference Resolution and Relation De-
tection. Proc. ACL2005. pp. 411-418. Ann Arbor, 
USA. 
Heng Ji and Ralph Grishman. 2006. Analysis and Re-
pair of Name Tagger Errors. Proc. ACL2006 
(POSTER). Sydney, Australia. 
Thorsten Joachims. 1998. Making large-scale support 
vector machine learning practical. Advances in Ker-
nel Methods: Support Vector Machine. MIT Press. 
Taku Kudo, Jun Suzuki and Hideki Isozaki. 2005. 
Boosting-based Parse Reranking Derived from 
Probabilistic Models. Proc. ACL2005. pp. 189-196. 
Ann Arbor, USA. 
John Platt, Nello Cristianini, and John Shawe-Taylor. 
2000.  Large margin dags for multiclass classifica-
tion. Advances in Neural Information Processing 
Systems 12. pp. 547-553 
Dan Roth and Wen-tau Yih. 2004. A Linear Program-
ming Formulation for Global Inference in Natural 
Language Tasks. Proc. CONLL2004. pp. 1-8 
Dan Roth and Wen-tau Yih. 2002. Probabilistic Rea-
soning for Entity & Relation Recognition. Proc. 
COLING2002. pp. 835-841 
Cynthia Rudin. 2006. Ranking with a p-Norm Push. 
Proc. Nineteenth Annual  Conference on Computa-
tional Learning Theory (CoLT 2006), Pittsburgh, 
Pennsylvania. 
Libin Shen and Aravind K. Joshi. 2003. An SVM 
Based Voting Algorithm with Application to Parse 
ReRanking. Proc. HLT-NAACL 2003 workshop on 
Analysis of Geographic References. pp. 9-16 
Libin Shen and Aravind K. Joshi. 2004. Flexible Mar-
gin Selection for Reranking with Full Pairwise Sam-
ples. Proc.IJCNLP2004. pp. 446-455. Hainan Island, 
China. 
56
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 117?120,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Arabic Morphological Tagging, Diacritization, and Lemmatization
Using Lexeme Models and Feature Ranking
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin
Center for Computational Learning Systems
Columbia University
New York, NY 10115 USA
{ryanr,rambow,habash,mdiab,rudin}@ccls.columbia.edu,
Abstract
We investigate the tasks of general morpho-
logical tagging, diacritization, and lemmatiza-
tion for Arabic. We show that for all tasks we
consider, both modeling the lexeme explicitly,
and retuning the weights of individual classi-
fiers for the specific task, improve the perfor-
mance.
1 Previous Work
Arabic is a morphologically rich language: in our
training corpus of about 288,000 words we find 3279
distinct morphological tags, with up to 100,000 pos-
sible tags.1 Because of the large number of tags, it
is clear that morphological tagging cannot be con-
strued as a simple classification task. Hajic? (2000)
is the first to use a dictionary as a source of possible
morphological analyses (and hence tags) for an in-
flected word form. He redefines the tagging task as
a choice among the tags proposed by the dictionary,
using a log-linear model trained on specific ambi-
guity classes for individual morphological features.
Hajic? et al (2005) implement the approach of Hajic?
(2000) for Arabic. In previous work, we follow the
same approach (Habash and Rambow, 2005), using
SVM-classifiers for individual morphological fea-
tures and a simple combining scheme for choosing
among competing analyses proposed by the dictio-
nary. Since the dictionary we use, BAMA (Buck-
walter, 2004), also includes diacritics (orthographic
1This work was funded under the DARPA GALE, program,
contract HR0011-06-C-0023. We thank several anonymous re-
viewers for helpful comments. A longer version of this paper is
available as a technical report.
marks not usually written), we extend this approach
to the diacritization task in (Habash and Rambow,
2007). The work presented in this paper differs from
this previous work in that (a) we introduce a new
task for Arabic, namely lemmatization; (b) we use
an explicit modeling of lexemes as a component in
all tasks discussed in this paper (morphological tag-
ging, diacritization, and lemmatization); and (c) we
tune the weights of the feature classifiers on a tuning
corpus (different tuning for different tasks).
2 Morphological Disambiguation Tasks for
Arabic
We define the task of morphological tagging
as choosing an inflectional morphological tag (in
this paper, the term ?morphological tagging? never
refers to derivational morphology). The morphol-
ogy of an Arabic word can be described by the 14
(nearly) orthogonal features shown in Figure 1. For
different tasks, different subsets may be useful: for
example, when translating into a language without
case, we may want to omit the case feature. For the
experiments we discuss in this paper, we investigate
three variants of the morphological tagging tasks:
MorphPOS (determining the feature POS, which is
the core part-of-speech ? verb, noun, adjective, etc.);
MorphPart (determining the set of the first ten basic
morphological features listed in Figure 1); andMor-
phAll (determining the full inflectional morpholog-
ical tag, i.e., all 14 features).
The task of diacritization involves adding diacrit-
ics (short vowels, gemination marker shadda, and
indefiniteness marker nunation) to the standard writ-
ten form. We have two variants of the diacritization
117
Feature name Explanation
POS Simple part-of-speech
CNJ Presence of a conjunction clitic
PRT Presence of a particle clitic
PRO Presence of a pronominal clitic
DET Presence of the definite deter-
miner
GEN Gender
NUM Number
PER Person
VOX Voice
ASP Aspect
MOD Mood
NUN Presence of nunation (indefinite-
ness marker)
CON Construct state (head of a geni-
tive construction)
CAS Case
Figure 1: List of (inflectional) morphological features
used in our system; the first ten are features which
(roughly) can be determined with higher accuracy since
they rely less on syntactic context and more on visible
inflectional morphology
tasks: DiacFull (predicting all diacritics of a given
word), which relates to lexeme choice and morphol-
ogy tagging, and DiacPart (predicting all diacritics
of a given word except those associated with the fi-
nal letter), which relates largely to lexeme choice.
Lemmatization (LexChoice) for Arabic has not
been discussed in the literature to our knowledge. A
lexeme is an abstraction over a set of inflected word
forms, and it is usually represented by its citation
form, also called lemma.
Finally, AllChoice is the combined task of choos-
ing all inflectional and lexemic aspects of a word in
context.
This gives us a total of seven tasks. AllChoice is
the hardest of our tasks, since it subsumes all other
tasks. MorphAll is the hardest of the three mor-
phological tagging tasks, subsuming MorphPart
and MorphPOS, and DiacFull is the hardest lexical
task, subsuming DiacPart, which in turn subsumes
LexChoice. However, MorphAll and DiacFull are
(in general) orthogonal, since MorphAll has no lex-
emic component, while DiacFull does.
3 Our System
Our system, MADA, makes use of 19 orthogonal
features to select, for each word, a proper anal-
ysis from a list of potential analyses provided by
the BAMA dictionary. The BAMA analysis which
matches the most of the predicted features wins; the
weighting of the features is one of the topics of this
paper. These 19 features consist of the 14 morpho-
logical features shown in Figure 1, which MADA
predicts using 14 distinct Support Vector Machines
trained on ATB3-Train (as defined by Zitouni et al
(2006)). In addition, MADA uses five additional
features. Spellmatch determines whether the dia-
critized form of the suggested analysis and the input
word match if both are stripped of all of their di-
acritics. This is useful because sometimes BAMA
suggests analyses which imply a different spelling
of the undiacritized word, but these analyses are of-
ten incorrect. Isdefault identifies those analyses that
are the default output of BAMA (typically, these
are guesses that the word in question is a proper
noun); these analyses are less likely to be correct
than others suggested by BAMA. MADA can de-
rive the values of Spellmatch and Isdefault by di-
rect examination of the analysis in question, and
no predictive model is needed. The fourteen mor-
phological features plus Spellmatch and Isdefault
form a feature collection that is entirely based on
morphological (rather than lexemic) features; we re-
fer to this collection as BASE-16. UnigramDiac
and UnigramLex are unigram models of the sur-
face diacritized form and the lexeme respectively,
and contain lexical information. We also build 4-
gram lexeme models using an open-vocabulary lan-
guage model with Kneser-Ney smoothing, by means
of the SRILM toolkit (Stolcke, 2002). The model is
trained on the same corpus used to train the other
classifiers, ATB3-Train. (We also tested other n-
grammodels, and found that a 4-gram lexeme model
outperforms the other orders with n ? 5, although
the improvement over the trigram and 5-gram mod-
els was less than 0.01%.) The 4-gram model, on
its own, correctly selects the lexeme of words in
ATB3-DevTest 94.1% of the time. The 4-gram lex-
eme model was incorporated into our system as a
full feature (NGRAM). We refer to the feature set
consisting of BASE-16 plus the two unigram mod-
118
els and NGRAM as FULL-19.
Optimizing the feature weights is a machine
learning task. To provide learning data for this task,
we take the ATB3-DevTest data set and divide it into
two sections; the first half (?26K words) is used
for tuning the weights and the second half (?25K
words) for testing. In a pre-processing step, each
analysis in appended with a set of labels which in-
dicate whether the analysis is correct according to
seven different evaluation metrics. These metrics
correspond in a one-to-one manner to the seven dif-
ferent disambiguation tasks discussed in Section 2,
and we use the task name for the evaluation la-
bel. Specifically, the MorphPOS label is positive
if the analysis has the same POS value as the cor-
rect analysis in the gold standard; the LexChoice
label provides the same information about the lex-
eme choice. The MorphPart label is positive if the
analysis agrees with the gold for each of the 10 ba-
sic features used by Habash and Rambow (2005).
A positive MorphAll label requires that the analy-
sis match the gold in all morphological features, i.e.,
in every feature except the lexeme choice and dia-
critics. The DiacFull label is only positive if the
surface diacritics of the analysis match the gold di-
acritics exactly; DiacPart is less strict in that the
trailing sequence diacritic markers in each surface
diacritic are stripped before the analysis and the gold
are compared. Finally, AllChoice is only positive if
the analysis was one chosen as correct in the gold;
this is the strictest form of evaluation, and there can
be only one positive AllChoice label per word.
In addition to labeling as described in the preced-
ing paragraph, we run MADA on the tuning and test
sets. This gives us a set of model predictions for ev-
ery feature of every word in the tuning and test sets.
We use an implementation of a Downhill Simplex
Method in many dimensions based on the method
developed by Nelder and Mead (1965) to tune the
weights applied to each feature. In a given itera-
tion, the Simplex algorithm proposes a set of feature
weights. These weights are given to a weight eval-
uation function; this function determines how effec-
tive a particular set of weights is at a given disam-
biguation task by calculating an overall score for
the weight set: the number of words in the tuning
set that were correctly disambiguated. In order to
compute this score, the weight evaluation function
examines each proposed analysis for each word in
the tuning set. If the analysis and the model predic-
tion for a feature of a given word agree, the analysis
score for that analysis is incremented by the weight
corresponding to that feature. The analysis with the
highest analysis score is selected as the proper anal-
ysis for that word. If the selected analysis has a pos-
itive task label (i.e., it is a good answer for the dis-
ambiguation task in question), the overall score for
the proposed weight set is incremented. The Sim-
plex algorithm seeks to maximize this overall score
(and thus choose the weight set that performs best
for a given task).
Once the Simplex algorithm has converged, the
optimal feature weights for a given task are known.
Our system makes use of these weights to select a
correct analysis in the test set. Each analysis of each
word is given a score that is the sum of optimal fea-
ture weights for features where the model predic-
tion and the analysis agree. The analysis with the
highest score is then chosen as the correct analysis
for that word. The system can be evaluated simply
by comparing the chosen analysis to the gold stan-
dard. Since the Simplex weight evaluation function
and the system use identical means of scoring anal-
yses, the Simplex algorithm has the potential to find
very optimized weights.
4 Experiments
We have three main research hypotheses: (1) Using
lexemic features helps in all tasks, but especially in
the diacritization and lexeme choice tasks. (2) Tun-
ing the weights helps over using identical weights.
(3) Tuning to the task that is evaluated improves over
tuning to other tasks. For each of the two feature
sets, BASE-16 and FULL-19, we tune the weights
using seven tuning metrics, producing seven sets of
weights. We then evaluate the seven automatically
weighted systems using seven evaluation metrics.
The tuning metrics are identical to the evaluation
metrics and they correspond to the seven tasks de-
scribed in Section 2. Instead of showing 98 results,
we show in Figure 2 four results for each of the
seven tasks: for both the BASE-16 and FULL-19
feature sets, we give the untuned performance, and
then the best-performing tuned performance. We in-
dicate which tuning metric provided the best tun-
119
BASE-16 (Morph Feats Only) FULL-19 (All Feats)
Task Baseline Not Tuned Tuned Tuning metric Not Tuned Tuned Tuning metric
MorphPOS 95.5 95.6 96.0 MorphAll 96.0 96.4 MorphPOS
MorphPart 93.8 94.1 94.8 AllChoice 94.7 95.1 DiacPart
MorphAll 83.8 84.0 84.8 AllChoice 82.2 85.1 MorphAll
LexChoice 85.5 86.6 87.5 MorphAll 95.4 96.3 LexChoice
DiacPart 85.1 86.4 87.3 AllChoice 94.8 95.4 DiacPart
DiacFull 76.0 77.1 78.2 MorphAll 82.6 86.1 MorphAll
AllChoice 73.3 74.5 75.6 AllChoice 80.3 83.8 MorphAll
Figure 2: Results for morphological tagging tasks (percent correct); the baseline uses only 14 morphological features
with identical weights; ?Tuning Metric? refers to the tuning metric that produced the best tuned results, as shown in
the ?Tuned? column
ing performance. The Baseline indicated in Fig-
ure 2 uses the 14 morphological features (listed in
Figure 1) only, with no tuning (i.e., all 14 features
have a weight of 1). The untuned results were deter-
mined by also setting almost all feature weights to 1;
the only exception is the Isdefault feature, which is
given a weight of -(8/14) when included in untuned
sets. Since this feature is meant to penalize analy-
ses, its value must be negative; we use this particu-
lar value so that our results can be readily compared
to previous work. All results are the best published
results to date on these test sets; for a deeper discus-
sion, see the longer version of this paper which is
available as a technical report.
We thus find our three hypotheses confirmed: (1)
Using lexemic features reduces error for the mor-
phological tagging tasks (measured on tuned data)
by 3% to 11%, but by 36% to 71% for the diacritic
and lexeme choice tasks. The highest error reduc-
tion is indeed for the lexical choice task. (2) Tuning
the weights helps over using identical weights. With
only morphological features, we obtain an error re-
duction of between 4% and 12%; with all features,
the error reduction from tuning ranges between 8%
and 20%. (3) As for the correlation between tuning
task and evaluation task, it turned out that when we
use only morphological features, two tuning tasks
worked best for all evaluation tasks, namely Mor-
phAll and AllChoice, thus not confirming our hy-
pothesis. We speculate that in the absence of the lex-
ical features, more features is better (these two tasks
are the two hardest tasks for morphological features
only). If we add the lexemic features, we do find
our hypothesis confirmed, with almost all evaluation
tasks performing best when the weights are tuned for
that task. In the case of the three exceptions, the dif-
ferences between the best performance and perfor-
mance when tuned to the same task are very slight
(< 0.06%).
References
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer version 2.0.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In ACL?05, Ann
Arbor, MI, USA.
Nizar Habash and Owen Rambow. 2007. Arabic di-
acritization through full morphological tagging. In
NAACL HLT 2007 Companion Volume, Short Papers,
Rochester, NY, USA.
Jan Hajic?, Otakar Smrz?, Tim Buckwalter, and Hubert
Jin. 2005. Feature-based tagger of approximations
of functional Arabic morphology. In Proceedings of
the Workshop on Treebanks and Linguistic Theories
(TLT), Barcelona, Spain.
Jan Hajic?. 2000. Morphological tagging: Data vs. dic-
tionaries. In 1st Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?00), Seattle, WA.
J.A Nelder and R Mead. 1965. A simplex method for
function minimization. In Computer Journal, pages
303?333.
Andreas Stolcke. 2002. Srilm - an extensible language
toolkit. In Proceedings of the International Confer-
ence on Spoken Language Processing (ICSLP).
Imed Zitouni, Jeffrey S. Sorensen, and Ruhi Sarikaya.
2006. Maximum entropy based restoration of arabic
diacritics. In Coling-ACL?06, pages 577?584, Sydney,
Australia.
120
