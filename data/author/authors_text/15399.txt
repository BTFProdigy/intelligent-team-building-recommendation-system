Spotting the ?Odd-one-out?: Data-Driven Error Detection and Correction
in Textual Databases
Caroline Sporleder, Marieke van Erp, Tijn Porcelijn and Antal van den Bosch
ILK / Language and Information Science
Tilburg University, P.O. Box 90153,
5000 LE Tilburg, The Netherlands
{C.Sporleder,M.G.J.vanErp,M.Porcelijn,Antal.vdnBosch}@uvt.nl
Abstract
We present two methods for semi-
automatic detection and correction of er-
rors in textual databases. The first method
(horizontal correction) aims at correct-
ing inconsistent values within a database
record, while the second (vertical correc-
tion) focuses on values which were en-
tered in the wrong column. Both methods
are data-driven and language-independent.
We utilise supervised machine learning,
but the training data is obtained automat-
ically from the database; no manual anno-
tation is required. Our experiments show
that a significant proportion of errors can
be detected by the two methods. Further-
more, both methods were found to lead to
a precision that is high enough to make
semi-automatic error correction feasible.
1 Introduction
Over the last decades, more and more information
has become available in digital form; a major part
of this information is textual. While some tex-
tual information is stored in raw or typeset form
(i.e., as more or less flat text), a lot is semi-
structured in databases. A popular example of
a textual database is Amazon?s book database,1
which contains fields for ?author?, ?title?, ?pub-
lisher?, ?summary? etc. Information about collec-
tions in the cultural heritage domain is also fre-
quently stored in (semi-)textual databases. Exam-
ples of publicly accessible databases of this type
are the University of St. Andrews?s photographic
1http://www.amazon.com
collection2 or the Nederlands Soortenregister.3
Such databases are an important resource for
researchers in the field, especially if the contents
can be systematically searched and queried. How-
ever, information retrieval from databases can be
adversely affected by errors and inconsistencies in
the data. For example, a zoologist interested in
finding out about the different biotopes (i.e., habi-
tats) in which a given species was found, might
query a zoological specimens database for the con-
tent of the BIOTOPE column for all specimens
of that species. Whenever information about the
biotope was entered in the wrong column, that par-
ticular record will not be retrieved by such a query.
Similarly, if an entry erroneously lists the wrong
species, it will also not be retrieved.
Usually it is impossible to avoid errors com-
pletely, even in well maintained databases. Errors
can arise for a variety of reasons, ranging from
technical limitations (e.g., copy-and-paste errors)
to different interpretations of what type of infor-
mation should be entered into different database
fields. The latter situation is especially preva-
lent if the database is maintained by several peo-
ple. Manual identification and correction of er-
rors is frequently infeasible due to the size of the
database. A more realistic approach would be to
use automatic means to identify potential errors;
these could then be flagged and presented to a hu-
man expert, and subsequently corrected manually
or semi-automatically. Error detection and correc-
tion can be performed as a pre-processing step for
information extraction from databases, or it can be
interleaved with it.
In this paper, we explore whether it is possi-
2http://special.st-andrews.ac.uk/
saspecial/
3http://www.nederlandsesoorten.nl
40
ble to detect and correct potential errors in tex-
tual databases by applying data-driven clean-up
methods which are able to work in the absence
of background knowledge (e.g., knowledge about
the domain or the structure of the database) and
instead rely on the data itself to discover inconsis-
tencies and errors. Ideally, error detection should
also be language independent, i.e., require no or
few language specific tools, such as part-of-speech
taggers or chunkers. Aiming for language in-
dependence is motivated by the observation that
many databases, especially in the cultural heritage
domain, are multi-lingual and contain strings of
text in various languages. If textual data-cleaning
methods are to be useful for such databases, they
should ideally be able to process all text strings,
not only those in the majority language.
While there has been a significant amount of
previous research on identifying and correcting er-
rors in data sets, most methods are not particularly
suitable for textual databases (see Section 2). We
present two methods which are. Both methods are
data-driven and knowledge-lean; errors are iden-
tified through comparisons with other database
fields. We utilise supervised machine learning,
but the training data is derived directly from the
database, i.e., no manual annotation of data is nec-
essary. In the first method, the database fields of
individual entries are compared, and improbable
combinations are flagged as potential errors. Be-
cause the focus is on individual entries, i.e., rows
in the database, we call this horizontal error cor-
rection. The second method aims at a different
type of error, namely values which were entered
in the wrong column of the database. Potential
errors of this type are determined by comparing
the content of a database cell to (the cells of) all
database columns and determining which column
it fits best. Because the focus is on columns, we
refer to this method as vertical error correction.
2 Related Work
There is a considerable body of previous work
on the generic issue of data cleaning. Much
of the research directed specifically at databases
focuses on identifying identical records when
two databases are merged (Herna?ndez and Stolfo,
1998; Galhardas et al, 1999). This is a non-trivial
problem as records of the same objects coming
from different sources typically differ in their pri-
mary keys. There may also be subtle differences
in other database fields. For example, names may
be entered in different formats (e.g., John Smith
vs. Smith, J.) or there may be typos which make it
difficult to match fields (e.g., John Smith vs. Jon
Smith).4
In a wider context, a lot of research has
been dedicated to the identification of outliers in
datasets. Various strategies have been proposed.
The earliest work uses probability distributions to
model the data; all instances which deviate too
much from the distributions are flagged as out-
liers (Hawkins, 1980). This approach is called
distribution-based. In clustering-based methods,
a clustering algorithm is applied to the data and
instances which cannot be grouped under any clus-
ter, or clusters which only contain very few in-
stances are assumed to be outliers (e.g., Jiang et
al. (2001)). Depth-based methods (e.g., Ruts and
Rousseeuw (1996)) use some definition of depth
to organise instances in layers in the data space;
outliers are assumed to occupy shallow layers.
Distance-based methods (Knorr and Ng, 1998)
utilise a k-nearest neighbour approach where out-
liers are defined, for example, as those instances
whose distance to their nearest neighbour exceeds
a certain threshold. Finally, Marcus and Maletic
(2000) propose a method which learns association
rules for the data; records that do not conform to
any rules are then assumed to be potential outliers.
In principle, techniques developed to detect out-
liers can be applied to databases as well, for in-
stance to identify cell values that are exceptional in
the context of other values in a given column, or to
identify database entries that seem unlikely com-
pared to other entries. However, most methods
are not particularly suited for textual databases.
Some approaches only work with numeric data
(e.g., distribution-based methods), others can deal
with categorical data (e.g., distance-based meth-
ods) but treat all database fields as atoms. For
databases with free text fields it can be fruitful to
look at individual tokens within a text string. For
instance, units of measurement (m, ft, etc.) may be
very common in one column (such as ALTITUDE)
but may indicate an error when they occur in an-
other column (such as COLLECTOR).
4The problem of whether two proper noun phrases re-
fer to the same entity has also received attention outside the
database community (Bagga, 1998).
41
3 Data
We tested our error correction methods on a
database containing information about animal
specimens collected by researchers at Naturalis,
the Dutch Natural History Museum.5 The
database contains 16,870 entries and 35 columns.
Each entry provides information about one or sev-
eral specimens, for example, who collected it,
where and when it was found, its position in the
zoological taxonomy, the publication which first
described and classified the specimen, and so on.
Some columns contain fairly free text (e.g., SPE-
CIAL REMARKS), others contain textual content6
of a specific type and in a relatively fixed format,
such as proper names (e.g., COLLECTOR or LO-
CATION), bibliographical information (PUBLICA-
TION), dates (e.g., COLLECTION DATE) or num-
bers (e.g., REGISTRATION NUMBER).
Some database cells are left unfilled; just un-
der 40% of all cells are filled (i.e., 229,430 cells).
There is a relatively large variance in the number
of different values in each column, ranging from
three for CLASS (i.e., Reptilia, Amphibia, and a
remark pointing to a taxonomic inconsistency in
the entry) to over 2,000 for SPECIAL REMARKS,
which is only filled for a minority of the entries.
On the other hand there is also some repetition
of cell contents, even for the free text columns,
which often contain formulaic expressions. For
example, the strings no further data available or
(found) dead on road occur repeatedly in the spe-
cial remarks field. A certain amount of repetition
is characteristic for many textual databases, and
we exploit this in our error correction methods.
While most of the entries are in Dutch or En-
glish, the database also contains text strings in sev-
eral other languages, such as Portuguese or French
(and Latin for the taxonomic names). In principle,
there is no limit to which languages can occur in
the database. For example, the PUBLICATION col-
umn often contains text strings (e.g., the title of
the publication) in languages other than Dutch or
English.
4 Horizontal Error Correction
The different fields in a database are often not
statistically independent; i.e., for a given entry,
5http://www.naturalis.nl
6We use the term textual content in the widest possible
sense, i.e., comprising all character strings, including dates
and numbers.
the likelihood of a particular value in one field
may be dependent on the values in (some of) the
other fields. In our database, for example, there
is an interdependency between the LOCATION and
the COUNTRY columns: the probability that the
COUNTRY column contains the value South Africa
increases if the LOCATION column contains the
string Tafel Mountain (and vice versa). Similar
interdependencies hold between other columns,
such as LOCATION and ALTITUDE, or COUNTRY
and BIOTOPE, or between the columns encoding
a specimen?s position in the zoological taxonomy
(e.g., SPECIES and FAMILY). Given enough data,
many of these interdependencies can be deter-
mined automatically and exploited to identify field
values that are likely to be erroneous.
This idea bears some similarity to the approach
by Marcus and Maletic (2000) who infer associ-
ation rules for a data set and then look for out-
liers relative to these rules. However, we do not
explicitly infer rules. Instead, we trained TiMBL
(Daelemans et al, 2004), a memory-based learner,
to predict the value of a field given the values of
other fields for the entry. If the predicted value
differs from the original value, it is signalled as a
potential error to a human annotator.
We applied the method to the taxonomic fields
(CLASS, ORDER, FAMILY, GENUS, SPECIES and
SUB-SPECIES), because it is possible, albeit some-
what time-consuming, for a non-expert to check
the values of these fields against a published zoo-
logical taxonomy. We split the data into 80% train-
ing set, 10% development set and 10% test set. As
not all taxonomic fields are filled for all entries,
the exact sizes for each data set differ, depending
on which field is to be predicted (see Table 1).
We used the development data to set TiMBL?s
parameters, such as the number of nearest neigh-
bours to be taken into account or the similarity
metric (van den Bosch, 2004). Ideally, one would
want to choose the setting which optimised the er-
ror detection accuracy. However, this would re-
quire manual annotation of the errors in the devel-
opment set. As this is fairly time consuming, we
abstained from it. Instead we chose the parameter
setting which maximised the value prediction ac-
curacy for each taxonomic field, i.e. the setting for
which the disagreement between the values pre-
dicted by TiMBL and the values in the database
was smallest. The motivation for this was that a
high prediction accuracy will minimise the num-
42
ber of potential errors that get flagged (i.e., dis-
agreements between TiMBL and the database) and
thus, hopefully, lead to a higher error detection
precision, i.e., less work for the human annotator
who has to check the potential errors.
training devel. test
CLASS 7,495 937 937
ORDER 7,493 937 937
FAMILY 7,425 928 928
GENUS 7,891 986 986
SPECIES 7,873 984 984
SUB-SPECIES 1,949 243 243
Table 1: Data set sizes for taxonomic fields
We also used the development data to perform
some feature selection. We compared (i) using
the values of all other fields (for a given entry) as
features and (ii) only using the other taxonomic
fields plus the author field, which encodes which
taxonomist first described the species to which a
given specimen belongs.7 The reduced feature set
was found to lead to better or equal performance
for all taxonomic fields and was thus used in the
experiments reported below.
For each taxonomic field, we then trained
TiMBL on the training set and applied it to the
test set, using the optimised parameter settings.
Table 2 shows the value prediction accuracies for
each taxonomic field and the accuracies achieved
by two baseline classifiers: (i) randomly select-
ing a value from the values found in the training
set (random) and (ii) always predicting the (train-
ing set) majority value (majority). The predic-
tion accuracies are relatively high, even for the
lowest fields in the taxonomy, SPECIES and SUB-
SPECIES, which should be the most difficult to pre-
dict. Hence it is in principle possible to predict the
value of a taxonomic field from the values of other
fields in the database. To determine whether the
taxonomic fields are exceptional in this respect,
we also tested how well non-taxonomic fields can
be predicted. We found that all fields can be pre-
dicted with a relatively high accuracy. The low-
est accuracy (63%) is obtained for the BIOTOPE
field. For most fields, accuracies of around 70%
7The author information provides useful cues for the pre-
diction of taxonomic fields because taxonomists often spe-
cialise on a particular zoological group. For example, a tax-
onomist who specialises on Ranidae (frogs) is unlikely to
have published a description of a species belonging to Ser-
pentes (snakes).
are achieved; this applies even to the ?free text?
fields like SPECIAL REMARKS.
TiMBL random majority
CLASS 99.87% 50.00% 54.98%
ORDER 98.29% 1.92% 18.59%
FAMILY 98.02% 0.35% 10.13%
GENUS 92.57% 10.00% 44.76%
SPECIES 89.93% 0.20% 7.67%
SUB-SPECIES 95.03% 0.98% 21.35%
Table 2: Test set prediction accuracies for taxo-
nomic field values (horizontal method)
To determine whether this method is suitable
for semi-automatic error correction, we looked at
the cases in which the value predicted by TiMBL
differed from the original value. There are three
potential reasons for such a disagreement: (i) the
value predicted by TiMBL is wrong, (ii) the value
predicted by TiMBL is correct and the original
value in the database is wrong, and (iii) both val-
ues are correct and the two terms are (zoological)
synonyms. For the fields CLASS, ORDER, FAM-
ILY and GENUS, we checked the values predicted
by TiMBL against two published zoological tax-
onomies8 and counted how many times the pre-
dicted value was the correct value. We did not
check the two lowest fields (SUB SPECIES and
SPECIES), as the correct values for these fields can
only be determined reliably by looking at the spec-
imens themselves, not by looking at the other tax-
onomic values for an entry. For the evaluation, we
focused on error correction rather than error detec-
tion, hence cases where both the value predicted
by TiMBL and the original value in the database
were wrong, were counted as TiMBL errors.
Table 3 shows the results (the absolute numbers
of database errors, synonyms and TiMBL errors
are shown in brackets). It can be seen that TiMBL
detects several errors in the database and predicts
the correct values for them. It also finds several
synonyms. For GENUS, however, the vast ma-
jority of disagreements between TiMBL and the
database is due to TiMBL errors. This can be ex-
plained by the fact that GENUS is relatively low
in the taxonomy (directly above SPECIES). As the
values of higher fields only provide limited cues
8We used the ITIS Catalogue of Life (http:
//www.species2000.org/2005/search.php)
and the EMBL Reptile Database (http://www.
embl-heidelberg.de/?uetz/LivingReptiles.
html).
43
disagreements database errors synonyms TiMBL errors
CLASS 2 50.00% (1) 0% (0) 50.00% (1)
ORDER 26 38.00% (10) 19.00% (5) 43.00% (11)
FAMILY 33 9.09% (3) 36.36% (12) 54.55% (18)
GENUS 135 5.93% (8) 4.44% (6) 89.63% (121)
Table 3: Error correction precision (horizontal method)
for the value of a lower field, the lower a field is in
the taxonomy the more difficult it is to predict its
value accurately.
So far we have only looked at the precision
of our error detection method (i.e., what propor-
tion of flagged errors are real errors). Error de-
tection recall (i.e., the proportion of real errors
that is flagged) is often difficult to determine pre-
cisely because this would involve manually check-
ing the dataset (or a significant subset) for errors,
which is typically quite time-consuming. How-
ever, if errors are identified and corrected semi-
automatically, recall is more important than pre-
cision; a low precision means more work for the
human expert who is checking the potential errors,
a low recall, however, means that many errors are
not detected at all, which may severely limit the
usefulness of the system.
To estimate the recall obtained by the horizontal
error detection method, we introduced errors arti-
ficially and determined what percentage of these
artificial errors was detected. For each taxonomic
field, we changed the value of 10% of the entries,
which were randomly selected. In these entries,
the original values were replaced by one of the
other attested values for this field. The new value
was selected randomly and with uniform probabil-
ity for all values. Of course, this method can only
provide an estimate of the true recall, as it is possi-
ble that real errors are distributed differently, e.g.,
some values may be more easily confused by hu-
mans than others. Table 4 shows the results. The
estimated recall is fairly high; in all cases above
90%. This suggests that a significant proportion
of the errors is detected by our method.
5 Vertical Error Correction
While the horizontal method described in the pre-
vious section aimed at correcting values which
are inconsistent with the remaining fields of a
database entry, vertical error correction is aimed
at a different type of error, namely, text strings
which were entered in the wrong column of the
recall
CLASS 95.56%
ORDER 96.82%
FAMILY 96.15%
GENUS 93.09%
SPECIES 96.75%
SUB SPECIES 95.38%
Table 4: Recall for artificially introduced errors
(horizontal method)
database. For example, in our database, informa-
tion about the biotope in which a specimen was
found may have been entered in the SPECIAL RE-
MARKS column rather than the BIOTOPE column.
Errors of this type are quite frequent. They can
be accidental, i.e., the person entering the infor-
mation inadvertently chose the wrong column, but
they can also be due to misinterpretation, e.g., the
person entering the information may believe that it
fits the SPECIAL REMARKS column better than the
BIOTOPE column or they may not know that there
is a BIOTOPE column. Some of these errors may
also stem from changes in the database structure
itself, e.g., maybe the BIOTOPE column was only
added after the data was entered.9
Identifying this type of error can be recast as a
text classification task: given the content of a cell,
i.e., a string of text, the aim is to determine which
column the string most likely belongs to. Text
strings which are classified as belonging to a dif-
ferent column than they are currently in, represent
a potential error. Recasting error detection as a
text classification problem allows the use of super-
vised machine learning methods, as training data
(i.e., text strings labelled with the column they be-
long to) can easily be obtained from the database.
We tokenised the text strings in all database
fields10 and labelled them with the column they
9Many databases, especially in the cultural heritage do-
main, are not designed and maintained by database experts.
Over time, such database are likely to evolve and change
structurally. In our specimens database, for example, several
columns were only added at later stages.
10We used a rule-based tokeniser for Dutch developed by
44
occur in. Each string was represented as a vec-
tor of 48 features, encoding the (i) string itself and
some of its typographical properties (13 features),
and (ii) its similarity with each of the 35 columns
(in terms of weighted token overlap) (35 features).
The typographical properties we encoded were:
the number of tokens in the string and whether it
contained an initial (i.e., an individual capitalised
letter), a number, a unit of measurement (e.g., km),
punctuation, an abbreviation, a word (as opposed
to only numbers, punctuation etc.), a capitalised
word, a non-capitalised word, a short word (< 4
characters), a long word, or a complex word (e.g.,
containing a hyphen).
The similarity between a string, consisting of a
set T of tokens t1 . . . tn, and a column colx was
defined as:
sim(T, colx) =
?n
i=1 ti ? tfidfti,colx
|T |
where tfidfticolx is the tfidf weight (term fre-
quency - inverse document frequency, cf. (Sparck-
Jones, 1972)) of token ti in column colx. This
weight encodes how representative a token is of
a column. The term frequency, tfti,colx , of a token
ti in column colx is the number of occurrences of
ti in colx divided by the number of occurrences
of all tokens in colx. The term frequency is 0 if
the token does not occur in the column. The in-
verse document frequency, idfti , of a token ti is
the number of all columns in the database divided
by the number of columns containing ti. Finally,
the tfidf weight for a term ti in column colx is de-
fined as:
tfidfti,colx = tfti,colx log idfti
A high tfidf weight for a given token in a given
column means that the token frequently occurs in
that column but rarely in other columns, thus the
token is a good indicator for that column. Typ-
ically tfidf weights are only calculated for con-
tent words, however we calculated them for all
tokens, partly because the use of stop word lists
to filter out function words would have jeopar-
dised the language independence of our method
and partly because function words and even punc-
tuation can be very useful for distinguishing dif-
ferent columns. For example, prepositions such as
under often indicate BIOTOPE, as in under a stone.
Sabine Buchholz. The inclusion of multi-lingual abbrevi-
ations in the rule set ensures that this tokeniser is robust
enough to also cope with text strings in English and other
Western European languages.
To assign a text string to one of the 35 database
columns, we trained TiMBL (Daelemans et al,
2004) on the feature vectors of all other database
cells labelled with the column they belong to.11
Cases where the predicted column differed from
the current column of the string were recorded as
potential errors.
We applied the classifier to all filled database
cells. For each of the strings identified as potential
errors, we checked manually (i) whether this was
a real error (i.e., error detection) and (ii) whether
the column predicted by the classifier was the cor-
rect one (i.e., error correction). While checking
for this type of error is much faster than checking
for errors in the taxonomic fields, it is sometimes
difficult to tell whether a flagged error is a real er-
ror. In some cases it is not obvious which col-
umn a string belongs to, for example because two
columns are very similar in content (such as LO-
CATION and FINDING PLACE), in other cases the
content of a database field contains several pieces
of information which would best be located in dif-
ferent columns. For instance, the string found with
broken neck near Karlobag arguably could be split
between the SPECIAL REMARKS and the LOCA-
TION columns. We were conservative in the first
case, i.e., we did not count an error as correctly
identified if the string could belong to the origi-
nal column, but we gave the algorithm credit for
flagging potential errors where part of the string
should be in a different column.
The results are shown in the second column (un-
filtered) in Table 5. The classifier found 836 poten-
tial errors, 148 of these were found to be real er-
rors. For 100 of the correctly identified errors the
predicted column was the correct column. Some
of the corrected errors can be found in Table 6.
Note that the system corrected errors in both En-
glish and Dutch text strings without requiring lan-
guage identification or any language-specific re-
sources (apart from tokenisation).
We also calculated the precision of error detec-
tion (i.e., the number of real errors divided by the
number of flagged errors) and the error correction
accuracy (i.e., the number of correctly corrected
errors divided by the number correctly identified
errors). The error detection precision is relatively
low (17.70%). In general a low precision means
relatively more work for the human expert check-
11We used the default settings (IB1, Weighted Overlap
Metric, Information Gain Ratio weighting) and k=3.
45
string original column corrected column
op boom ongeveer 2,5 m boven grond SPECIAL REMARKS BIOTOPE
(on a tree about 2.5 m above ground)
25 km N.N.W Antalya SPECIAL REMARKS LOCATION
1700 M BIOTOPE ALTITUDE
gestorven in gevangenschap 23 september 1994 LOCATION SPECIAL REMARKS
(died in captivity 23 September 1994)
roadside bordering secondary forest LOCATION BIOTOPE
Suriname Exp. 1970 COLLECTION NUMBER COLLECTOR
(Surinam Expedition 1970)
Table 6: Examples of automatically corrected errors (vertical method)
unfiltered filtered
flagged errors 836 262
real errors 148 67
correctly corrected 100 54
precision error detection 17.70 % 25.57%
accuracy error correction 67.57% 80.60%
Table 5: Results automatic error detection and cor-
rection for all database fields (vertical method)
ing the flagged errors. However, note that the sys-
tem considerably reduces the number of database
fields that have to be checked (i.e., 836 out of
229,430 filled fields). We also found that, for this
type of error, error checking can be done relatively
quickly even by a non-expert; checking the 836 er-
rors took less than 30 minutes. Furthermore, the
correction accuracy is fairly high (67.57%), i.e.,
for most of the correctly identified errors the cor-
rect column is suggested. This means that for most
errors the user can simply choose the column sug-
gested by the classifier.
In an attempt to increase the detection preci-
sion we applied two filters and only flagged errors
which passed these filters. First, we filtered out
potential errors if the original and the predicted
column were of a similar type (e.g., if both con-
tained person names or dates) as we noticed that
our method was very prone to misclassifications
in these cases.12 For example, if the name M.S.
Hoogmoed occurs several times in the COLLEC-
TOR column and a few times in the DONATOR col-
umn, the latter cases are flagged by the system as
potential errors. However, it is entirely normal for
a person to occur in both the COLLECTOR and the
DONATOR column. What is more, it is impossible
12Note, that this filter requires a (very limited) amount of
background knowledge, i.e. knowledge about which columns
are of a similar type.
to determine on the basis of the text string M.S.
Hoogmoed alone, whether the correct column for
this string in a given entry is DONATOR or COL-
LECTOR or both.13 Secondly, we only flagged er-
rors where the predicted column was empty for the
current database entry. If the predicted column is
already occupied, the string is unlikely to belong
to that column (unless the string in that column is
also an error). The third column in Table 5 (fil-
tered) shows the results. It can be seen that de-
tection precision increases to 25.57% and correc-
tion precision to 80.60%, however the system also
finds noticeably fewer errors (67 vs. 148).
Prec. Rec.
BIOTOPE 20.09% 94.00%
PUBLICATION 6.90% 100.00%
SPECIAL REMARKS 16.11% 24.00%
Table 7: Precision and Recall for three free text
columns (vertical method)
Estimating the error detection recall (i.e., the
number of identified errors divided by the over-
all number of errors in the database) would in-
volve manually identifying all the errors in the
database. This was not feasible for the database
as a whole. Instead we manually checked three
of the free text columns, namely, BIOTOPE, PUB-
LICATION and SPECIAL REMARKS, for errors and
calculated the recall and precision for these. Ta-
ble 7 shows the results. For BIOTOPE and PUB-
LICATION the recall is relatively high (94% and
100%, respectively), for SPECIAL REMARKS it is
much lower (24%). The low recall for SPECIAL
REMARKS is probably due to the fact that this col-
13Note, however, that the horizontal error detection method
proposed in the previous section might detect an erroneous
occurrence of this string (based on the values of other fields
in the entry).
46
umn is very heterogeneous, thus it is fairly difficult
to find the true errors in it. While the precision is
relatively low for all three columns, the number
of flagged errors (ranging from 58 for PUBLICA-
TION to 298 for SPECIAL REMARKS) is still small
enough for manual checking.
6 Conclusion
We have presented two methods for
(semi-)automatic error detection and correc-
tion in textual databases. The two methods are
aimed at different types of errors: horizontal
error correction attempts to identify and correct
inconsistent values within a database record;
vertical error correction is aimed at values which
were accidentally entered in the wrong column.
Both methods are data-driven and require little
or no background knowledge. The methods are
also language-independent and can be applied to
multi-lingual databases. While we utilise super-
vised machine learning, no manual annotation
of training data is required, as the training set is
obtained directly from the database.
We tested the two methods on an animal spec-
imens database and found that a significant pro-
portion of errors could be detected: up to 97% for
horizontal error detection and up to 100% for ver-
tical error detection. While the error detection pre-
cision was fairly low for both methods (up to 55%
for the horizontal method and up to 25.57% for the
vertical method), the number of potential errors
flagged was still sufficiently small to check manu-
ally. Furthermore, the automatically predicted cor-
rection for an error was often the right one. Hence,
it would be feasible to employ the two methods in
a semi-automatic error correction set-up where po-
tential errors together with a suggested correction
are flagged and presented to a user.
As the two error correction methods are to some
extent complementary, it would be worthwhile to
investigate whether they can be combined. Some
errors flagged by the horizontal method will not be
detected by the vertical method, for instance, val-
ues which are valid in a given column, but incon-
sistent with the values of other fields. On the other
hand, values which were entered in the wrong col-
umn should, in theory, also be detected by the hor-
izontal method. For example, if the correct FAM-
ILY for Rana aurora is Ranidae, it should make
no difference whether the (incorrect) value in the
FAMILY field is Bufonidae, which is a valid value
for FAMILY but the wrong family for Rana aurora,
or Amphibia, which is not a valid value for FAM-
ILY but the correct CLASS value for Rana aurora;
in both cases the error should be detected. Hence,
if both methods predict an error in a given field
this should increase the likelihood that there is in-
deed an error. This could be exploited to obtain a
higher precision. We plan to experiment with this
idea in future research.
Acknowledgments The research reported in
this paper was funded by NWO (Netherlands Or-
ganisation for Scientific Research) and carried out
at the Naturalis Research Labs in Leiden. We
would like to thank Pim Arntzen and Erik van
Nieukerken from Naturalis for guidance and help-
ful discussions. We are also grateful to two anony-
mous reviewers for useful comments.
References
A. Bagga. 1998. Coreference, Cross-Document Coref-
erence, and Information Extraction Methodologies.
Ph.D. thesis, Dept. of Computer Science, Duke Uni-
versity.
W. Daelemans, J. Zavrel, K. van der Sloot, A. van den
Bosch, 2004. TiMBL: Tilburg Memory Based
Learner, version 5.1, Reference Guide, 2004. ILK
Research Group Technical Report Series no. 04-02.
H. Galhardas, D. Florescu, D. Shasha, E. Simon. 1999.
An extensible framework for data cleaning. Tech-
nical Report RR-3742, INRIA Technical Report,
1999.
D. M. Hawkins. 1980. Identification of outliers. Chap-
man and Hall, London.
M. A. Herna?ndez, S. J. Stolfo. 1998. Real-world data
is dirty: Data cleansing and the merge/purge prob-
lem. Journal of Data Mining and Knowledge Dis-
covery, 2:1?31.
M.-F. Jiang, S.-S. Tseng, C.-M. Su. 2001. Two-phase
clustering process for outliers detection. Pattern
Recognition Letters, 22:691?700.
E. M. Knorr, R. T. Ng. 1998. Algorithms for min-
ing distance-based outliers in large datasets. In Pro-
ceedings of the 24th International Conference on
Very Large Data Bases (VLDB?98).
A. Marcus, J. I. Maletic. 2000. Utilizing association
rules for identification of possible errors in data sets.
Technical Report TR-CS-00-04, The University of
Memphis, Division of Computer Science, 2000.
I. Ruts, P. J. Rousseeuw. 1996. Computing depth
contours of bivariate point clouds. Computational
Statistics and Data Analysis, 23:153?168.
K. Sparck-Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28:11?21.
A. van den Bosch. 2004. Wrapped progressive sam-
pling search for optimizing learning algorithm pa-
rameters. In Proceedings of the 16th Belgian-Dutch
Conference on Artificial Intelligence, 219?226.
47
Proceedings of the Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2007), pages 17?24,
Prague, 28 June 2007. c?2007 Association for Computational Linguistics
Retrieving lost information from textual databases:
rediscovering expeditions from an animal specimen database
Marieke van Erp
Dept. of Language and Information Sciences
Tilburg University, P.O. Box 90153
NL-5000 LE Tilburg, The Netherlands
M.G.J.vanErp@uvt.nl
Abstract
Importing large amounts of data into
databases does not always go without the
loss of important information. In this work,
methods are presented that aim to rediscover
this information by inferring it from the in-
formation that is available in the database.
From and animal specimen database, the
information to which expedition an ani-
mal that was found belongs is rediscovered.
While the work is in an early stage, the ob-
tained results are promising, and prove that
it is possible to rediscover expedition infor-
mation from the database.
1 Introduction
Databases made up of textual material tend to con-
tain a wealth of information that remains unexplored
with simple keyword-based search. Maintainers of
the databases are often not aware of the possibilities
offered by text mining methods to discover hidden
information to enrich the basic data. In this work
several machine learning methods are explored to
investigate whether ?hidden information? can be ex-
tracted from an animal specimen database belonging
to the Dutch National Museum for Natural History,
Naturalis1. The database is a combination of infor-
mation about objects in the museum collection from
handwritten data sources in the museum, such as
journal-like entries that are kept by biologists while
collecting animal or plant specimens on expedition
1http://www.naturalis.nl
and tables that link the journal entries to the mu-
seum register. What is not preserved in the transition
from the written sources to the database is the name
of the expedition druing which an animal specimen
was found.
By expedition, the following event is implied: a
group of biologists went on expedition together in
a country during a certain time period. Entries in
the database that belong to this expedition can be
collected by one or a subset of the participating bi-
ologists. For researchers at the natural history mu-
seum it would be helpful to have access to expedi-
tion information in their database, as for biodiver-
sity research they sometimes need overviews of ex-
peditions. It may also help further enrichment of
the database and cleansing, because if the expedi-
tion information is available, missing information in
certain fields, such as the country where a specimen
was found, may be inferred from the information on
other specimens found during the same expedition.
Currently, if one wants to retrieve all objects from
the database that belong to an expedition, one would
have to create a database query that contains the ex-
act data boundaries of the expeditions and the names
of all collectors involved. Either one of these bits of
information is not enough, as the same group of bi-
ologists may have participated in an expedition more
than once, and the database may also contain expe-
ditions that overlap in time. In this paper a series
of experiments is described to find a way to infer
expedition information from the information avail-
able in the database. To this end, three approaches
are compared: supervised machine learning, unsu-
pervised machine learning, and rule-based methods.
17
The obtained results vary, but prove that it is pos-
sible to extract the expedition information from the
data at hand.
2 Related Work
The field of data mining, which is concerned with
the extraction of implicit, previously unknown and
potentially useful information from data (Frawley et
al., 1992), is a branch of research that has become
quite important recently as every day the world is
flooded with larger amounts of information that are
impossible to analyse manually. Data mining can,
for instance, help banks identify suspicious trans-
actions among the millions of transactions that are
executed daily (Fayyad and Uthurusamy, 1996), or
automatically classify protein sequences in genome
databases (Mewes et al, 1999), or aid a company
in creating better customer profiles to present cus-
tomers with personalised ads and notifications (Lin-
den et al, 2003). Knowledge discovery approaches
often rely on machine learning techniques as these
are particularly well suited to process large amounts
of data to find similarities or dissimilarities between
instances (Mitchell, 1997).
Traditionally, governments and companies have
been interested in gaining more insight into their
data by applying data mining techniques. Only re-
cently , digitisation of data in the cultural heritage
domain has taken off, which means that there has
not been much work done on knowledge discovery
in this domain. Databases in this domain are often
created and maintained manually and are thus of-
ten significantly smaller than automatically gener-
ated databases from, for example, customers? pur-
chase information in a large company.
This means it is not clear whether data mining
techniques, aimed at analysing enormous amounts
of data, will work for the data at hand. This is in-
vestigated here. Manual data typically also contains
more spelling variations/errors and other inconsis-
tencies than automatically generated databases, due
to different persons entering data into the database.
Therefore, before one can start the actual process of
knowledge discovery, it is very important to care-
fully select, clean and model the data one wants
to use in order to avoid using data that is too
sparse (Chapman, 2003). This applies in particular
to databases that contain large amounts of textual in-
formation, which are quite prevalent in the cultural
heritage domain. Examples of textual databases can
be found freely on the internet, such as the databases
of the Global Biodiversity Information Facility2, the
University of St. Andrews Photographic Collec-
tion3, and the Internet Movie Database4.
3 Data
The data that has been used in this experiment is an
animal specimen database from the Dutch National
Museum for Natural History. The database currently
contains 16,870 entries that each represent an object
stored in the museum?s reptiles and amphibians col-
lection. The entries provide a variety of information
about the objects in 37 columns, such as the scien-
tific name of the object, how the specimen is kept
(in alcohol, stuffed, pinned) and under which regis-
tration number, where it was found, by whom and
under which circumstances, the name of the person
who determined the species of the animal and the
name of the person who first described the species.
Most fields are rather compact; they only contain a
numeric value or a textual value consisting of one or
several words. The database also contains fields of
which the entries consist of longer stretches of text,
such as the ?special remarks? field, describing any-
thing about the object that did not fit in the other
database fields and ?biotope?, describing the biotic
and abiotic components of the habitat from which
the object was collected. Dutch is the most frequent
language in the database, followed by English. Also
some Portuguese and German entries occur. Taxo-
nomic values, i.e., the scientific names of the animal
specimens, are in a restricted type of Latin. A snip-
pet of the database can be found in Figure 1.
3.1 Data Construction
In order to be able the measure the performance of
the approaches used in the experiments, the database
was annotated manually with expedition informa-
tion. Adding this information was possible because
there was access to the original field books from
which the database is made up. Annotating 8166
2http://www.gbif.org/
3http://special.st-andrews.ac.uk/saspecial/
4http://www.imdb.com/
18
Collector Coll. Date Coll. # Class Genus Species Country Expedition
Buttikofer, J. 30-07-1881 424 Reptilia Lamprolepis lineatus 132 buttikoferliberia1881
Buttikofer, J. & Sala 09-10-1881 504 Amphibia Bufo regularis 132 buttikoferliberia1881
M. Dachsel 02-05-1971 971-MSH186 Reptilia Blanus mettetali 156 mshbrazil71
Hoogmoed, M.S. 04-05-1971 1971-MSH187 Reptilia Quendenfeldtia trachyblepharus 156 mshbrazil71
Hoogmoed, M.S. 09-05-1971 1971-MSH202 Reptilia Lacerta hispanica 156 mshbrazil71
C. Schuil 14-03-1972 1972-MSH35 Amphibia Ptychadaena sp. 92 mshghana72
P. Lavelle -03-1972 1972-MSH40 Reptilia Crotaphopeltis hotamboeia 92 mshghana72
Hoogmoed, M.S. 23-03-1972 1972-MSH55 Amphibia Phrynobatrachus plicatus 92 mshghana72
Figure 1: Snippet of the animal specimen database
entries with this information took one person about
2 days. There were 8704 entries to which no ex-
pedition is assigned, either because these specimens
were not collected during an expedition or because it
was not possible to determine the expedition. These
entries were excluded from the experiments . Ex-
peditions which contained 10 or fewer entries were
also excluded because these would make the data
set too sparse. A total of 7831 database entries
were used in this work, divided into 60 expeditions.
Although the ?smallest? expeditions were excluded
from the experiments, the sizes of the expeditions
still vary greatly: between 2170 and 11 items (? =
310.04). This is mainly due to the fact that new
items are still added to the database continuously, in
a rather random order, hence some expeditions are
more completely represented than others.
The database contains several fields that contain
information that will probably not be that useful for
this work. Information that was excluded was the
specimen?s sex, the number of specimens (in cases
where one database entry refers to several speci-
mens, for instance kept together in a jar), how the
animal is preserved, and fields that contain informa-
tion not on the specimen itself or how it was found
but on the database (e.g., when the database entry
was added and by whom). Values from the ?alti-
tude? and ?coordinates? fields were also not included
in the experiments as this is information is too of-
ten missing in the database to be of any use (altitude
information is missing in 85% of the entries and co-
ordinates in 96%).
Some information in the database is repetitive;
there is for instance a field called ?country? contain-
ing the name of the country in which a specimen was
found, but there is also a field called ?country-id? in
which the same information is encoded as a numer-
ical value. The latter is more often filled than the
?country? field, which also contains values in differ-
ent languages, and thus it makes more sense to only
include values from the ?country-id? field. A small
conversion is applied to rule out that an algorithm
will interpret the intervals between the different val-
ues as a measure of geographical proximity between
the values, as the country values are chosen alpha-
betically and do not encode geographical location.
In some cases it seemed useful to have an al-
gorithm employ interval relations between num-
bers. The fields ?registration number? and ?collec-
tion number? were used as such. These fields some-
times contain some alphabetical values: certain col-
lectors, for instance, included their initials in their
series of collection registration numbers. These
were converted to a numeric code to obtain com-
pletely numeric values with preservation of the col-
lector information. This also goes for the fields in
the database that contain information on dates, i.e.,
the ?date of determination?, the ?date the specimen
came into the museum? and the ?collection date?
fields. The collection date is the most important
date here as this directly links to an expedition. The
other dates might provide indirect information, for
instance if the collection date is missing (which is
the case in 14%). To aid clustering, the dates were
normalised to a number, possibly the algorithm ben-
efits from the fact that a small numerical interval
means that the dates are close together.
Person names from the ?author?, ?collector?, ?de-
terminer?, and ?donator? fields were normalised to
a ?first name - last name? format. From values
from the taxonomic fields (?class?, ?order?, ?fam-
ily?, ?genus?, ?species?, and ?sub species?), and
?town/village? and ?province/state? fields, as well as
from the person name fields, capitals, umlauts, ac-
cents and any other non-alphanumerical characters
were removed.
It proved that certain database fields were not suit-
able for inclusion in the experiments. This goes for
19
the free text fields ?biotope?, ?location? and ?special
remarks?. Treating these values as they are will re-
sult in data that is too sparse, as their values are ex-
tremely varied. Preliminary experiments to see if
it was possible to select only certain parts of these
fields did not yield any satisfying results and was
therefore abandoned.
This resulted in feature vectors containing 18 fea-
tures, plus the manually assigned expedition class.
4 Methodology
The majority of the experiments that were carried
out in an attempt to infer the expedition informa-
tion from the database involved machine learning.
Therefore in this section three algorithms for super-
vised learning are described, followed by a cluster-
ing algorithm for unsupervised learning. This sec-
tion is concluded with a description of the evaluation
metrics for clusters used by the different approaches.
Algorithms
The first algorithm that was used is the k-Nearest
Neighbour algorithm (k-NN) (Aha et al, 1991;
Cover and Hart, 1967; DeVijver and Kittler, 1982).
This algorithm is an example of a lazy learner: it
does not model the training data it is given, but sim-
ply stores each instance of the training data in mem-
ory. During classification it compares the item it
needs to classify to each item in its memory and
assigns the majority class of the closest k (in these
experiments k=1) instances to the new item. To
determine which instances are closest, a variety of
distance metrics can be applied. In this experi-
ment the standard settings in the TiMBL implemen-
tation (Daelemans et al, 2004), developed at the
ILK research group at Tilburg University, were used.
The standard distance metric in the TiMLB imple-
mentation of k-NN is the Overlap metric, given in
Equations1 and 2. ?(X,Y) is the distance between
instances X and Y, represented by n features, where
? is the distance between the features.
?(X,Y ) =
n?
i=1
?(xi, yi) (1)
where:
?(xi, yi) =
?
??
??
abs if numeric, else
0 ifxi = yi
1 ifxi 6= yi
(2)
The second algorithm that was used is the C4.5
decision tree algorithm (Quinlan, 1986). In the
learning phase, it creates a decision tree in a re-
cursive top-down process in which the database is
partitioned according to the feature that separates
the classes best; each node in the tree represents
one partition. Deeper nodes represent more class-
homogeneous partitions. During classification, C4.5
traverses the tree in a deterministic top-down pass
until it meets a class-homogeneous end node, or a
non-ending node when a feature-value test is not
represented in the tree.
Naive Bayes is the third algorithm that was used
in the experiments. It computes the probability
of a certain expedition, given the observed train-
ing data according to the formula given in Equa-
tion 3. In this formula ?NB is the target expedition
value, chosen from the maximally probably hypoth-
esis (
argmax
?jV P (?j), i.e., the expedition with the high-
est probability) given the product of the probabilities
of the features (
Q
i P (ai|?j)).
?NB =
argmax
?jV P (?j)
?
i
P (ai|?j) (3)
For both the C4.5 algorithm and Naive Bayes the
WEKA machine learning environment (Witten and
Frank, 2005), that was developed at the University
of Waikato, New Zealand, was used.
A quite different machine learning approach that
was applied to try to identify expeditions in the rep-
tiles and amphibians database is clustering. Clus-
tering methods are unsupervised, i.e., they do not
require annotated data, and in some cases not even
the number of expeditions that are in the data. Items
in the data set are grouped according to similarity.
A maximum dissimilarity between the group mem-
bers may be specified to steer the algorithm, but
other than that it runs on its own. For an exten-
sive overview of clustering methods see Jain et al,
(1999). For this work, the options in choosing an
implementation of a clustering algorithm were lim-
ited because many data mining tools are designed
20
only for numerical data, therefore the WEKA ma-
chine learning environment was also used for the
clustering experiments. As clustering is computa-
tionally expensive, it was only possible to run ex-
periments with WEKA?s implementation of the Ex-
pectation Maximisation (EM) algorithm (Dempster
et al, 1977). Preliminary experiments with other al-
gorithms indicated execution times in the order of
months. The EM algorithm iteratively tries to con-
verge to a maximum likelihood by first computing
an expectation of the likelihood of a certain cluster-
ing, then maximising this likelihood by computing
the maximum likelihood estimates of the features.
Termination of the algorithm occurs when the pre-
defined number of iterations has been carried out,
or when the overall likelihood (the measure of how
?good? a clustering is) does not increase significantly
with each iteration.
Cluster Evaluation
Since the data is annotated with expedition in-
formation it was possible to use external quality
measures (Steinbach et al, 2000). Three differ-
ent evaluation measures were used: accuracy, en-
tropy (Shannon, 1948), and the F-measure (van Ri-
jsbergen, 1979).
The evaluation of results for the supervised learn-
ing algorithms was calculated in a straightforward
way: because the classifier knows which expedi-
tions there are and which entries belong to which
expedition, it checks the expeditions it assigned to
the database entries to the manually assigned expe-
ditions and reports the overlap as accuracy.
It gets a little bit more complicated with entropy.
Entropy is a measure of informativity, i.e., the min-
imum number of bits of information needed to en-
code the classification of each instance. If the ex-
pedition clusters are uniform, i.e., all items in the
cluster are very similar, the entropy will be low. The
main problem with using entropy for evaluation of
clusters is that the best score (an entropy of 0) is
reached when every cluster contains exactly on in-
stance. Entropy is calculated as follows: first, the
main class distribution, i.e., per cluster the probabil-
ity that a member of that cluster belongs to a certain
cluster, is computed. Using that distribution the en-
tropy of each cluster is calculated via the formula in
Equation 4. For a set of clusters the total entropy
is then computed via the formula in Equation 5, in
which m is the total number of clusters, sy the size
of cluster y and n the total number of instances.
Ey = ?
?
x
Pxylog(Pxy) (4)
Etotal =
m?
y?1
sy ? Ey
n
(5)
The F-measure is the harmonic mean of precision
and recall, and is commonly used in information re-
trieval. In information retrieval recall is the propor-
tion of relevant documents retrieved out of the total
set of relevant documents. When applied to clus-
tering a ?relevant document? is an instance that is
assigned correctly to a certain expedition, the set of
all relevant documents is the set of all instances be-
loning to that expedition. Precision is the number of
relevant documents retrieved from the total number
of documents. So when applied to cluster evalua-
tion this means the number of instances of an expe-
dition that were retrieved from the total number of
instances (Larsen and Aone, 1999). This boils down
to Equations 6 and 7 in which x stands for expedi-
tion, y for cluster, nxy for the number of instances
belonging to expedition x that were assigned to clus-
ter y, and nx is the number of items in expedition x.
Recall(x, y) =
nxy
nx
(6)
Precision(x, y) =
nxy
ny
(7)
The F-measure for a cluster y with respect to ex-
pedition x is then computed via Equation 8. The
F-measure of the entire set of clusters is computed
through the function in Equation 9, which takes the
weighted average of the maximum F-measure per
expedition.
F (x, y) =
2 ? Recall(x, y) ? Precision(x, y)
Precision(x, y) +Recall(x, y)
(8)
F =
?
x
nx
n
max{F (x, y)} (9)
21
5 Experiments and Results
First, two baselines were set to illustrate the situation
if no machine learning or other techniques would be
applied to the database. if one were to randomly
assign one of the 60 expeditions t the entries this
would go well in 1.7% of the cases. If all entries
were labelled as belonging to the largest expedition
this would yield an accuracy of 28%. In all ma-
chine learning experiments 10-fold cross validation
was used for testing performance.
A series of supervised machine learning experi-
ments was carried out first to investigate whether it
is possible to extract the expeditions during which
the animal specimens were found at all. Three
learning algorithms were applied to the complete
data set, which yielded accuracies between 88%
and 98%. Feature selection experiments with the
C4.5 decision tree algorithm indicated that fea-
tures ?town/village?, ?collection number?, ?registra-
tion number?, ?collector? and ?collection date? were
considered most informative for this task, hence the
experiments were repeated with a data set contain-
ing only those features. The results of both series
of experiments are to be found in Table 1. For
the C4.5 and Naive Bayes experiments the accu-
racy deteriorates significantly when using only the
selected features (? = 0.05, computing using McNe-
mar?s test (McNemar, 1962)), but it stays stable for
the k-NN classifier. This indicates that not all data
is needed to infer the expeditions, but that it mat-
ters greatly which approach is taken. However, as
neither of the algorithm benefits from it, feature se-
lection was not further explored.
Algorithm All feat. Sel. feat.
k-NN 95.9% 95.9%
C.4.5 98.3% 94.4%
NaiveBayes 88.1% 73.5%
Table 1: Accuracy of supervised machine learning
experiments using all features and selected features
In these experiments all database entries were an-
notated with expedition information, which in a real
setting is of course not the case. Through running
a series of experiments with significantly smaller
amounts of training data it was found that by us-
ing only as little as 5% of the training data (amount-
ing to 392 instances) already an accuracy of 85% is
reached. Annotating this amount of data with expe-
dition information would take on person less than an
hour. By only using 45% of the training data an ac-
curacy of 97% is reached5. In Figure 2 the complete
learning curve of the k-NN classifier is shown.
Annotating this amount of data with expedition in-
formation would take one person less than an hour.
By only using 45% of the training data an accuracy
of 97% is reached5. In Figure 2 the complete learning
curve of the k -NN classifier is shown.
Fig. 2: Accuracy of k-NN per percentage of training
data
Ideally, one does not want to annotate data at all,
therefore the use of clustering algorithms was explored.
For this, the EM algorithm from the WEKA Machine
Learning environment was used. The results, as shown
in Table 2, are not quite satisfying, but still well above
the set baselines. As can be seen in Table 2, the clus-
tering algorithms do not come up with anywhere near
as many clusters as needed and unfortunately WEKA
does not present the user with many options to remedy
this. An intermediate experiment between completely
supervised and unsupervised was attempted, i.e., pre-
specifying a number of clusters for the algorithm to
define, but this was computationally too expensive to
carry out.
Algorithm # Clusters Accuracy
EM 7 46.0%
Table 2: Results of clustering experiments
Since clustering algorithms do not achieve an accu-
racy that is satisfying enough to use in a real setting
and supervised learning requires annotated data, also
a traditional, and quite different approach was tried,
namely finding expeditions via rules. Via a couple
of simple rules the dataset was split into possible ex-
peditions using only information on collection dates,
collector information and country information.
1. Sort dates in ascending order, start a new ex-
pedition when the distance between two dates is
greater than the average distance of the collection
dates
2. First sort collector information in ascending or-
der, then sort collection dates in ascending order,
start a new expedition when the distance between
two dates is greater than the average distance be-
tween dates or when a new collector is encoun-
tered
5 The slightly higher achieved accuracy in the learning curve
experiments is due to the fact the learning curve was not
computed via cross-validation
3. First sort by country information, then by collec-
tor, and finally by collection dates, start a new
expedition when country or collector change, or
when the distance between two dates is greater
than the average distance between dates
Surprisingly, only grouping collection dates already
yields an F-measure of .83, this includes 1299 en-
tries that contain no information on the collection
date, leaving this data out would increase precision
on the entries whose date is not missing to an F-
measure of .94. In Table 3 results of the rule-based
experiments are shown. It is expected that when the
database is further populated, the date-rule will work
less well as there will be expeditions that overlap, the
date+collector-rule should remedy this, although it
does not work very well yet as spelling variations in
the collector names are not taken into account.
Rules # clusters Fmeasure entropy
1 78 .83 .16
2 199 .75 .15
3 216 .73 .11
Table 3: Results of rule-based experiments
6 Conclusions and Future Work
In this work we have presented various approaches
to rediscover expedition information from an animal
specimen database. As expected, the supervised learn-
ing algorithms performed best, but the disadvantage in
using such an approach is the requirement to provide
annotated data; however, a series of experiments to
gain more insight into the quantities of data necessary
for a supervised approach to perform well indicate that
only a small set of annotated data is required to ob-
tain very reasonable accuracies. If no training data is
available, a rule-based approach is a more realistic al-
ternative. Although it must be kept in mind that rules
need to be created manually for every new data set.
For this data set relatively simple rules already proved
to be quite effective, but for other data sets deriving
rules can be much more complicated and thus more
expensive. This particular set of rules is also expected
to behave differently when the database is extended
with entries from overlapping expeditions.
For the experiments presented in this work, only
entries from the database of which the expedition
they belonged to was known were used, which con-
stitutes only half of the database entries. Researchers
at the natural history museum estimate that about
30% of the database entries do not belong to an ex-
pedition, while the other 20% not included here be-
long to unknown expeditions. The decision to exclude
the expedition-less entries was made as these entries
would imbalance the data and impair evaluation as it
would not be possible to check predictions against a
?real value?. If all database entries would belong to a
known expedition the performance of the approaches
described in this paper show that satisfactory results
should be achieved over the complete data set. To
prove this hypothesis one would need to test the ap-
proaches on other data sets which can be completely
5
Figure 2: Accuracy of k-NN per percentage of train-
ing data
Ideally, one does not want to annotate data at all,
therefore the use of a clustering algorithm was ex-
plored. For this, the EM algorithm from the WEKA
machine learning environment was used. The re-
sult, as shown in Table 2, is not quite satisfying, but
still well above the set baselines. As can be seen in
Table 2, th clustering algorithm does not com up
with anywhere near as many clusters as needed and
unfortunately WEKA does not present the user with
many options to remedy this. An intermediate ex-
periment between completely supervised and unsu-
pervised machine learning was attempted, i.e., pre-
specifying a number of clusters for the algorithm to
define, ut this was comp tatio ally too expensive
to carry out.
Algorithm # Clusters Accuracy
EM 7 46.0%
Table 2: Result of clustering experiment
Since the clus ering algorithm does not achieve an
accuracy that is satisfying enough to use in a real
setting and sup rvised lear ing requires annotated
data, also a traditional, and quite different approach
5The slightly higher achieved accuracy in the learning curve
experiments is due to the fact that the learning curve was not
computed via cross-validation
22
was tried: namely finding expeditions via rules. Via
a couple of simple rules the data set was split into
possible expeditions using only information on col-
lection dates, collector information and country in-
formation.
1. Sort dates in ascending order, start a new expe-
dition when the distance between two sequen-
tial dates is greater than the average distance of
the collection dates
2. First, sort collector information in ascending
order, then sort collection dates in ascending
order, start a new expeditions when the distance
between two dates is greater than the average
distance between dates or when a new collector
is encountered
3. First, sort by country information, then by col-
lector, and finally by collection date, start a new
expedition when country or collectors change,
or when the distance between two dates is
greater than the average distance between dates
Surprisingly, only grouping collection dates al-
ready yields an F-measure of .83. This includes
1299 entries that contain no information on the col-
lection date, leaving those out would increase preci-
sion on the entries whose collection date is not miss-
ing to an F-measure of .94. In Table 3 results of
the rule-based experiments are shown. It is expected
that when the database is further populated the date-
rule will work less well as there will be more expe-
ditions that overlap. The date+collector-rule should
remedy this, although it does not work very well yet
as spelling variations in the collector names are not
taken into account at the moment.
Rules # Clusters F-measure Entropy
1 78 .83 .16
2 199 .75 .15
3 216 .73 .11
Table 3: Results of the rule-based experiments
6 Conclusions and Future Work
In this work various approaches were presented to
rediscover expedition information from an animal
specimen database. As expected, the supervised
learning algorithms performed best, but the disad-
vantage in using such an approach is the require-
ment to provide annotated data. However, a series
of experiments to gain more insight into the quanti-
ties of data necessary for a supervised approach to
perform well, indicate that only a small set of an-
notated data is required in this case to obtain very
reasonable results. If no training data is available,
a rule-based approach is a realistic alternative. Al-
though it must be kept in mind that rules need to be
created manually for every new data set. For this
data set relatively simple rules already proved to be
quite effective, but for other data sets deriving rules
can be much more complicated and thus more ex-
pensive. This particular set of rules is also expected
to behave differently when the database is extended
with more entries from overlapping expeditions.
For the experiments presented in this work, only
entries from the database of which the expedition
they belonged to was known were used, which
constitutes only half of the database entries. Re-
searchers at Naturalis estimate that about 30% of
the database entries do not belong to an expedi-
tion, while the other 20% not included here belong
to unknown expeditions. The decision to exclude
the expedition-less entries was made as these en-
tries would imbalance the data and impair evalua-
tion as it would not be possible to check predictions
against a ?real value?. If all database entries would
belong to a known expedition the performance of
the approaches described in this paper that satisfac-
tory results could be achieved over the complete data
set. To prove this hypothesis one would need to
test the approaches on other data sets which can be
completely annotated. Performing such tests might
provide more insight into how well the approaches
would deal with a data set where all entries have
an associated expedition. The natural history mu-
seum has several other similar (but smaller) data
sets, which might be suitable for this task, and which
will be tested as part of future work for evaluating
the approaches described here. It may also be inter-
esting to investigate what can be inferred from the
other fields defined in other data sets.
A less satisfying aspect of the research described
in this paper is that many of the intended experi-
ments with unsupervised machine learning were too
23
computationally expensive to be executed. Potential
workarounds to the limitation of certain implemen-
tations of clustering algorithms, in that they only
work on numeric data, are sought in converting the
textual data to numeric values and in the investi-
gations into implementations of algorithms that can
deal with textual data.
A particular peculiarity of textual data, from
which the rule-based approach suffers, is the fact
that the same name or meaning can be conveyed in
several ways. Spelling variations and errors were
for instance not normalised. Hence the approaches
treated ?Hoogmoed? and ?M S Hoogmoed? as two
different values whereas they may very well refer to
the same entity.
From this work it can be concluded that the ex-
pedition information can definitely be reconstructed
from the animal specimen database that was used
here, but for it to be used in a real world applica-
tion it still needs to be tested and fine-tuned on other
data sets and extended to be able to deal with entries
that are not associated with any expedition.
Acknowledgments
The research reported in this paper was funded by
NWO, the Netherlands Organisation of Scientific
Research as part of the CATCH programme. The
author would like to thank the anonymous reviewers,
and Antal van den Bosch and Caroline Sporleder for
their helpful suggestions and comments.
References
David W. Aha, Dennis Kibler, and Mark K. Albert. 1991.
Instance-based learning algorithms. Machine Learn-
ing, 6:37?66.
Arthur D. Chapman. 2003. Notes on Environmen-
tal Data Quality-b. Data Cleaning Tools. Internal re-
port, Centro de Refere?ncia em Informac?a?o Ambiental
(CRIA).
T. M. Cover and P. E. Hart. 1967. Nearest neighbor
pattern classification. Institute of Electrical and Elec-
tronics Engineers Transactions on Information The-
ory, 13:21?27.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and
Antal Van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. Technical
Report 04-02, ILK/Tilburg University.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data using the
em algorithm. Journal of the Royal Statistical Society.
Series B (Methodology), 39(1):1?38.
P. A. DeVijver and J. Kittler. 1982. Pattern recognition.
A statistical approach. Prentice-Hall, London.
U. Fayyad and R. Uthurusamy. 1996. Data mining and
knowledge discovery in databases. Communications
of the ACM, 39(11):24?26.
William J. Frawley, Gregory Piatetsky-Shapiro, and
Christopher J. Matheus. 1992. Knowledge discovery
in databases: An overview. AI Magazine, 13:57?70.
A. K. Jain, M. N. Murty, and P. J. Flynn. 1999.
Data clustering: A review. ACM Computing Surveys,
31(3):264?323, September.
Bjorner Larsen and Chinatsu Aone. 1999. Fast and effec-
tive text mining using linear-time document clustering.
In Proceedings of KDD-99, San Diego, CA.
G. Linden, B. Smith, and J. York. 2003. Amazon.com
recommendations: item-to-item collaborative filtering.
IEEE Internet Computing, 7(1):76?80, Jan/Feb.
Q. McNemar. 1962. Psychological Statistics. Wiley,
New York.
H. W. Mewes, K. Heumann, A. Kaps, K. Mayer, F. Pfeif-
fer, S. Stocker, and D. Frishman. 1999. Mips: a
database for genomes and protein sequences. Nucleic
Acids Research, 27(1):44?48.
Tom M. Mitchell. 1997. Machine Learning. McGraw-
Hill.
J. R. Quinlan. 1986. Induction of decision trees. Ma-
chine Learning, 1:81?106.
Claude E. Shannon. 1948. A mathematical theory
of communication. Bell System Technical Journal,
27:379?423, July.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. Technical report, Department of Computer
Science, University of Minnesota.
Cornelis Joost van Rijsbergen. 1979. Information Re-
trieval. Buttersworth.
Ian H.Witten and Eibe Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
24
Proceedings of the EACL 2009 Workshop on Language Technology and Resources for Cultural Heritage,
Social Sciences, Humanities, and Education ?LaTeCH ? SHELT&R 2009, pages 60?68,
Athens, Greece, 30 March 2009. c?2009 Association for Computational Linguistics
Instance-driven Discovery of Ontological Relation Labels
Marieke van Erp, Antal van den Bosch, Sander Wubben, Steve Hunt
ILK Research Group
Tilburg centre for Creative Computing
Tilburg University
The Netherlands
{M.G.J.vanErp,Antal.vdnBosch,S.Wubben,S.J.Hunt}@uvt.nl
Abstract
An approach is presented to the auto-
matic discovery of labels of relations be-
tween pairs of ontological classes. Using
a hyperlinked encyclopaedic resource, we
gather evidence for likely predicative la-
bels by searching for sentences that de-
scribe relations between terms. The terms
are instances of the pair of ontological
classes under consideration, drawn from
a populated knowledge base. Verbs or
verb phrases are automatically extracted,
yielding a ranked list of candidate rela-
tions. Human judges rate the extracted
relations. The extracted relations provide
a basis for automatic ontology discovery
from a non-relational database. The ap-
proach is demonstrated on a database from
the natural history domain.
1 Introduction
The rapid growth in the digitisation of data has
caused many curators, researchers, and data man-
agers of cultural heritage institutions (libraries,
archives, museums) to turn to knowledge man-
agement systems. Using these systems typically
causes them to think about the ontological struc-
ture of their domain, involving the identification
of key classes in object data and metadata fea-
tures, and importantly, their relations. The start-
ing point of this process is often a more classi-
cal ?flat? database matrix model of size n ? m,
where n is the number of collection items, and m
is a fixed number of database columns, typically
denoting object metadata features, as cultural her-
itage institutions are generally well accustomed to
using databases of that type. An ontology can be
bootstrapped from such a database by first assum-
ing that the database columns can be mapped onto
the domain?s ontological classes. The next step
is then to determine which classes are related to
each other, and by which relation. In this paper
we present a method that partially automates this
process.
To gather evidence for a relation to exist be-
tween two ontological classes, it is not possible to
simply look up the classes in text. Rather, classes
are realised typically as a multitude of terms or
phrases. For example, the natural history class
?species? is realised as many different instances of
species names in text. The automatic discovery of
relations between ontological classes thus requires
at least a two-step approach: first, the identifica-
tion of instances of ontological classes in text and
their particular relations, and second, the aggrega-
tion of these analyses in order to find evidence for
a most likely relation.
It is common in ontology construction to use
predicative labels for relations. Although no regu-
lations for label names exist, often a verb or verb
phrase head is taken, optionally combined with a
prepositional head of the subsequent verb-attached
phrase (e. g., ?occurs in?, or ?donated by?). In
this study, we make the assumption that good can-
didate labels are frequent verbs or verb phrases
found between instances from a particular pair of
classes, and that this may sometimes involve a
verb-attached prepositional phrase containing one
of the two terms. In this paper we explore this
route, and present a case study on the discovery
of predicative labels on relations in an ontology
for animal specimen collections. The first step,
identifying instances of ontological classes, is per-
formed by selecting pairs of instances from a flat
n ?m specimen database, in which the instances
60
are organised by the database columns, and there
is a one-to-one relationship between the database
columns and the classes in our ontology.
Any approach that bases itself on text to dis-
cover relations, is dependent on the quality of
that text. In this study we opt for Wikipedia
as a resource from which to extract relations be-
tween terms. Although the status of Wikipedia
as a dependable resource is debated, in part be-
cause of its dynamic nature, there is some evi-
dence that Wikipedia can be as reliable a source
as one that is maintained solely by experts (Giles,
2005). Wikipedia is also an attractive resource due
to its size (currently nearly 12 million articles in
over 250 languages). Additionally, Wikipedia?s
strongly hyperlinked structure closely resembles a
semantic net, with its untyped (but directed) re-
lations between the concepts represented by the
article topics. Since the hyperlinks in Wikipedia
indicate a relations between two encyclopaedia ar-
ticles, we aim at discovering the type of relation
such a link denotes through the use of syntactic
parsing of the text in which the link occurs.
The idea of using Wikipedia for relation ex-
traction is not new (Auer and Lehmann, 2007;
Nakayama et al, 2008; Nguyen et al, 2007;
Suchanek et al, 2006; Syed et al, 2008). How-
ever, most studies so far focus on the structured
information already explicit in Wikipedia, such as
its infoboxes and categories. The main contribu-
tions of our work are that we focus on the in-
formation need emerging from a specific domain,
and that we test a method of pre-selection of sen-
tences to extract relations from. The selection is
based on the assumption that the strongest and
most reliable lexical relations are those expressed
by hyperlinks in Wikipedia pages that relate an ar-
ticle topic to another page (Kamps and Koolen,
2008). The selection procedure retains only sen-
tences in which the topic of the article, identified
by matching words in the article title, links to an-
other Wikipedia article. The benefit of the pre-
selection of sentences is that it reduces the work-
load for the syntactic parser.
Since the system is intentionally kept
lightweight, the extraction of relations from
Wikipedia is sufficiently fast, and we observe that
the results are sufficient to build a basic ontology
from the data. This paper is organised as follows.
In Section 2 we review related work. In Section 3
the data used in this work is described, followed
by the system in Section 4 and an explanation
of how we evaluated the possible relations our
system discovered is presented in Section 5. We
report on the results of our study in Section 6, and
formulate our conclusions and points for further
research in Section 7.
2 Related Work
A key property of Wikipedia is that it is for the
greater part unstructured. On the one hand, ed-
itors are encouraged to supply their articles with
categories. These categories can be subsumed by
broader categories, thus creating a taxonomy-like
structure. On the other hand, editors can link to
any other page in Wikipedia, no matter if it is part
of the same category, or any category at all. An
article can be assigned multiple categories, but the
number of hyperlinks provided in an average arti-
cle typically exceeds the number of categories as-
signed to it.
The free associative hyperlink structure of
Wikipedia is intrinsically different from the hier-
archical top down architecture as seen in Word-
Net, as a hyperlink has a direction, but not a
type. A Wikipedia article can contain any num-
ber of links, pointing to any other Wikipedia arti-
cle. Wikipedia guidelines state however that wik-
ilinks (hyperlinks referring to another Wikipedia
page) should only be added when relevant to the
topic of the article. Due to the fact that most users
tend to adhere to guidelines for editing Wikipedia
pages and the fact that articles are under constant
scrutiny of their viewers, most links in Wikipedia
are indeed relevant (Blohm and Cimiano, 2007;
Kamps and Koolen, 2008).
The structure and breadth of Wikipedia is a po-
tentially powerful resource for information extrac-
tion which has not gone unnoticed in the natu-
ral language processing (NLP) community. Pre-
processing of Wikipedia content in order to ex-
tract non-trivial relations has been addressed in a
number of studies. (Syed et al, 2008) for instance
utilise the category structure in Wikipedia as an
upper ontology to predict concepts common to a
set of documents. In (Suchanek et al, 2006) an
ontology is constructed by combining entities and
relations between these extracted from Wikipedia
through Wikipedia?s category structure and Word-
Net. This results in a large ?is-a? hierarchy, draw-
ing on the basis of WordNet, while further rela-
tion enrichments come from Wikipedia?s category
61
structure. (Chernov et al, 2006) also exploit the
Wikipedia category structure to which concepts in
the articles are linked to extract relations.
(Auer and Lehmann, 2007) take a different ap-
proach in that they focus on utilising the structure
present in infoboxes. Infoboxes are consistently
formatted tables in articles that provide summary
information, such as information about area, pop-
ulation and language for countries, and birth dates
and places for persons. Although infoboxes pro-
vide rich structured information, their templates
are not yet standardised, and their use has not per-
meated throughout the whole of Wikipedia.
Although the category and infobox structures in
Wikipedia already provide a larger coverage at the
concept or term level than for instance WordNet,
they do not express all possibly relevant seman-
tic relations. Especially in specific domains, re-
lations occur that would make the Wikipedia data
structure unnecessarily dense if added, thus an ap-
proach that exploits more of the linguistic content
of Wikipedia is desirable.
Such approaches can be found in (Nakayama et
al., 2008) and (Nguyen et al, 2007). In both works
full sections of Wikipedia articles are parsed, en-
tities are identified, and the verb between the enti-
ties is taken as the relation. They also extract re-
lations that are not backed by a link in Wikipedia,
resulting in common-sense factoids such as ?Bres-
cia is a city?. For a domain specific application
this approach lacks precision. In our approach, we
care more for high precision in finding relations
than for recall; hence, we carefully pre-select on-
tological classes among which relations need to be
found, and use these as filters on our search.
The usefulness of the link structure in
Wikipedia has been remarked upon by (Vo?lkel et
al., 2006). They acknowledge that the link struc-
ture in Wikipedia denotes a potentially meaning-
ful relation between two articles, though the re-
lation type is unknown. They propose an exten-
sion to the editing software of Wikipedia to enable
users to define the type of relation when they add
a link in Wikipedia. Potentially this can enrich
Wikipedia tremendously, but the work involved
would be tremendous as well. We believe some of
the type information is already available through
the linguistic content of Wikipedia.
3 Data Preparation
3.1 Data
The data used in this work comes from a manu-
ally created, non-relational research database of a
collection of reptiles and amphibians at a natural
history museum. The information contained in the
cells describes when a specimen entered the col-
lection, under what circumstances it was collected,
its current location, registration number, etc. We
argue that the act of retrieving information from
this flat database could be enhanced by providing
a meta-structure that describes relations between
the different database columns. If for instance a
relation of the type ?is part of? can be defined be-
tween the database columns province and country,
then queries for specimens found at a particular
location can be expanded accordingly.
Even though the main language of the database
is Dutch, we still chose to use the English
Wikipedia as the resource for retrieval of rela-
tion label candidates. Explicitly choosing the En-
glish Wikipedia has as a consequence that the
relation labels we are bound to discover will
be English phrases. Furthermore, articles in
the English Wikipedia on animal taxonomy have
a broader coverage and are far more elaborate
than those contained in the Dutch Wikipedia.
Since these database values use a Latin-based
nomenclature, using the wider-coverage English
Wikipedia yields a much higher recall than the
Dutch Wikipedia. The values of the other columns
mainly contain proper names, such as person
names and geographic locations and dates, which
are often the same; moreover, English and Dutch
are closely related languages. Different names ex-
ist for different countries in each language, but
here the inconsistency of the database aids us, as
it in fact contains many database entries partially
or fully in English, as well as some in German and
Portuguese.
The database contains 16,870 records in 39
columns. In this work we focus on 20 columns;
the rest are discarded as they are either extrinsic
features not directly pertaining to the object they
describe, e.g., a unique database key, or elaborate
textual information that would require a separate
processing approach. The columns we focus on
describe the position of the specimen in the zo-
ological taxonomy (6 columns), the geographical
location in which it was found (4 columns), some
of its physical properties (3 columns), its collector
62
Column Name Value
Taxonomic Class Reptilia
Taxonomic Order Crocodylia
Amphisbaenia
Taxonomic Genus Acanthophis
Xenobatrachus
Country Indonesia
Suriname
Location city walls
near Lake Mahalona
Collection Date 01.02.1888
02.01.1995
Type holotype
paralectotype
Determinator A. Dubois
M. S. Hoogmoed
Species defined by (Linnaeus, 1758)
(LeSueur, 1827)
Table 1: Example classes from test data
and/or determiner, donator and associated date (4
columns), and other information (3 columns). The
values in most columns are short, often consist-
ing of a single word. Table 1 lists some example
database values.
3.2 Preprocessing
As the database was created manually, it was nec-
essary to normalise spelling errors, as well as
variations on diacritics, names and date formats.
The database values were also stripped of all non-
alphanumeric characters.
In order to find meaningful relations between
two database columns, query pairs are generated
by combining two values occurring together in a
record. This approach already limits the number
of queries applied to Wikipedia, as no relations are
attempted to be found between values that would
not normally occur together. This approach yields
a query pair such as Reptilia Crocodylia from the
taxonomic class and order columns, but not Am-
phibia Crocodylia. Because not every database
field is filled, and some combinations occur more
often, this procedure results in 186,141 query
pairs.
For this study we use a database snapshot of the
English Wikipedia of July 27, 2008. This dump
contains about 2.5 million articles, including a vast
amount of domain-specific articles that one would
typically not find in general encyclopaedias. An
index was built of a subset of the link structure
present in Wikipedia. The subset of links included
in the index is constrained to those links occur-
ring in sentences from each article in which the
main topic of the Wikipedia article (as taken from
the title name) occurs. For example, from the
Wikipedia article on Anura the following sentence
would be included in the experiments1:
The frog is an [[amphibian]] in the order Anura
(meaning ?tail-less?, from Greek an-, without +
oura, tail), formerly referred to as Salientia (Latin
saltare, to jump)
whereas we would exclude the sentence:
An exception is the [[fire-bellied toad]] (Bombina
bombina): while its skin is slightly warty, it
prefers a watery habitat.
This approach limits the link paths to only
those between pages that are probably semanti-
cally strongly connected to each other. In the
following section the computation of the link
paths indicating semantic relatedness between two
Wikipedia pages is explained.
3.3 Computing Semantic Relatedness
Relation discovery between terms (instantiations
of different ontological classes) that have a page
in Wikipedia is best performed after establishing
if a sufficiently strong relation between the two
terms under consideration actually exists. To do
this, the semantic relatedness of those two terms
or concepts needs to be computed first. Seman-
tic relatedness can denote every possible relation
between two concepts, unlike semantic similarity,
which typically denotes only certain hierarchical
relations (like hypernymy and synonymy) and is
often computed using hierarchical networks like
WordNet (Budanitsky and Hirst, 2006).
A simple and effective way of computing se-
mantic relatedness between two concepts c1 and c2
is measuring their distance in a semantic network.
This results in a semantic distance metric, which
can be inversed to yield a semantic relatedness
metric. Computing the path-length between terms
c1 and c2 can be done using Formula 1 where P is
the set of paths connecting c1 to c2 and Np is the
number of nodes in path p.
1The double brackets indicate Wikilinks
63
relpath(c1, c2) = argmaxp?P
1
Np
(1)
We search for shortest paths in a semantic net-
work that is constructed by mapping the concepts
in Wikipedia to nodes, and the links between the
concepts to edges. This generates a very large
network (millions of nodes and tens of millions
of edges), but due to the fact that Wikipedia is
scale-free (Barabasi and Albert, 1999) (its con-
nectedness degree distribution follows a power-
law), paths stay relatively short. By indexing
both incoming and outgoing links, a bidirectional
breadth-first search can be used to find shortest
paths between concepts. This means that the
search is divided in two chains: a forward chain
from c1 and a backward chain to c2. As soon as the
two chains are connected, a shortest path is found.
4 Extracting Relations from Wikipedia
Each query pair containing two values from two
database columns are sent to the system. The sys-
tem processes each term pair in four steps. A
schematic overview of the system is given in Fig-
ure 1.
Indexed 
Wikipedia 
corpus
Term 1 Term 2
Art. 
1
Art. 
2
find path length
...<term 1>...<term 2>...
<term 1><relation><term 2>
extract
if path length == 1
if path 
length == 2
find intermediate 
value in database
Term 1
Term 2
Interm. 
Term 
Interm. 
Term 
if found
Step 4
Step 1
Step 3
Step 2
Figure 1: Schematic overview of the system
Step 1 We look for the most relevant Wikipedia
page for each term, by looking up the term in ti-
tles of Wikipedia articles. As Wikipedia format-
ting requires the article title to be an informative
and concise description of the article?s main topic,
we assume that querying only for article titles will
yield reliable results.
Step 2 The system finds the shortest link path be-
tween the two selected Wikipedia articles. If the
path distance is 1, this means that the two con-
cepts are linked directly to each other via their
Wikipedia articles. This is for instance the case
for Megophrys from the genus column, and Anura
from the order column. In the Wikipedia article on
Megophrys, a link is found to the Wikipedia arti-
cle on Anura. There is no reverse link from Anura
to Megophrys; hierarchical relationships in the zo-
ological taxonomy such as this one are often uni-
directional in Wikipedia as to not overcrowd the
parent article with links to its children.
Step 3 The sentence containing both target con-
cepts as links is selected from the articles.
From the Megophrys article this is for instance
?Megophrys is a genus of frogs, order [[Anura]],
in the [[Megophryidae]] family.?
Step 4 If the shortest path length between two
Wikipedia articles is 2, the two concepts are linked
via one intermediate article. In that case the sys-
tem checks whether the title of the intermediate ar-
ticle occurs as a value in a database column other
than the two database columns in focus for the
query. If this is indeed the case, the two addi-
tional relations between the first term and the in-
termediate article are also investigated, as well as
the second term and that of the intermediate ar-
ticle. Such a bridging relation pair is found for
instance for the query pair Hylidae from the tax-
onomic order column, and Brazil from the coun-
try column. Here, the initial path we find is Hyl-
idae? Sphaenorhynchys? Brazil. We find that
the article-in-the-middle value (Sphaenorhynchys)
indeed occurs in our database, in the taxonomic
genus column. We assume this link is evi-
dence for co-occurrence. Thus, the relevant sen-
tences from the Wikipedia articles on Hylidae
and Sphaenorhynchys, and between articles on
Sphaenorhynchys and Brazil are added to the pos-
sible relations between ?order? ? ?genus? and
?genus? ? ?country?.
Subsequently, the selected sentences are POS-
tagged and parsed using the Memory Based Shal-
low Parser (Daelemans et al, 1999). This parser
provides tokenisation, POS-tagging, chunking,
and grammatical relations such as subject and di-
rect object between verbs and phrases, and is
based on memory-based classification as imple-
mented in TiMBL (Daelemans et al, 2004). The
five most frequently recurring phrases that occur
64
between the column pairs, where the subject of the
sentence is a value from one of the two columns,
are presented to the human annotators. The cut-off
of five was chosen to prevent the annotators from
having to evaluate too many relations and to only
present those that occur more often, and are hence
less likely to be misses. Misses can for instance
be induced by ambiguous person names that also
accidentally match location names (e.g., Dakota).
In Section 7 we discuss methods to remedy this in
future work.
5 Evaluating Relations from Wikipedia
Four human judges evaluated the relations be-
tween the ontological class pairs that were ex-
tracted from Wikipedia. Evaluating semantic rela-
tions automatically is hard, if not impossible, since
the same relation can be expressed in many ways,
and would require a gold standard of some sort,
which for this domain (as well as for many cul-
tural heritage domains) is not available.
The judges were presented with the five highest-
ranked candidate labels per column pair, as well a
longer snippet of text containing the candidate la-
bel, to resolve possible ambiguity. The items in
each list were scored according to the total recip-
rocal rank (TRR) (Radev et al, 2002). For every
correct answer 1/n points are given, where n de-
notes the position of the answer in the ranked list.
If there is more than 1 correct answer the points
will be added up. For example, if in a list of five,
two correct answers occur on positions 2 and 4, the
TRR would be calculated as (1/2 + 1/4) = .75.
The TRR scores were normalised for the number
of relation candidates that were retrieved, as for
some column pairs less than five relation candi-
dates were retrieved.
As an example, for the column pair ?Province?
and ?Genus?, the judges were presented with the
relations shown in Table 2. The direction arrow
in the first column denotes that the ?Genus? value
occurred before the ?Province? value.
The human judges were sufficiently familiar
with the domain to evaluate the relations, and had
the possibility to gain extra knowledge about the
class pairs through access to the full Wikipedia
articles from which the relations were extracted.
Inter-annotator agreement was measured using
Fleiss?s Kappa coefficient (Fleiss, 1971).
6 Results and Evaluation
As expected, between certain columns there are
more relations than between others. In total 140
relation candidates were retrieved directly, and
303 relation label candidates were retrieved via an
intermediate Wikipedia article. We work with the
assumption that these columns have a stronger on-
tological relation than others. For some database
columns we could not retrieve any relations, such
as the ?collection date? field. This is not sur-
prising, as even though Wikipedia contains pages
about dates (?what happened on this day?), it is
unlikely that it would link to such a domain spe-
cific event such as an animal specimen collec-
tion. Relations between instances denoting per-
sons and other concepts in our domain are also not
discovered through this approach. This is due to
the fact that many of the biologists named in the
database do not have a Wikipedia page dedicated
to them, indicating the boundaries of Wikipedia?s
domain specific content. Although not ideal, a
named-entity recognition filter could be applied to
the database after which person names can be re-
trieved from other resources.
Occasionally we retrieve a Wikipedia article for
a value from a person name column, but in most
cases this mistakenly matches with a Wikipedia
article on a location, as last names in Dutch are
often derived from place names. Another problem
induced by incorrect data is the incorrect match
of Wikipedia pages on certain values from the
?Town? and ?Province? columns. Incorrect rela-
tion candidates are retrieved because for instance
the value ?China? occurs in both the ?Town? and
the ?Province? columns. A data cleaning step
would solve these two problems.
From each column pair the highest rated rela-
tion was selected with which we constructed the
ontology displayed in Figure 2. As the figure
shows, the relations that are discovered are not
only ?is a?-relations one would find in strictly hier-
archical resources such as a zoological taxonomy
or geographical resource.
The numbers in the relation labels in Figure 2
denote the average TRR scores given by the four
judges on all relation label candidates that the
judges were presented with for that column pair.
The scores for the relations between the taxo-
nomic classes in our domain were particularly
high, meaning that in many cases all relation can-
didates presented to the judges were assessed as
65
Direction Label Snippet
? is found in is a genus of venomous pitvipers found in Asia from Pakistan, through India,
? is endemic to Cross Frogs) is a genus of microhylid frogs endemic to Southern Philippine,
? are native to are native to only two countries: the United States and
? is known as is a genus of pond turtles also known as Cooter Turtles, especially in the state of
Table 2: Relation candidates for Province and Genus column pair
correct. The inter-annotator agreement was ? =
0.63, which is not perfect, but reasonable. Most
disagreement is due to vague relation labels such
as ?may refer to? as found between ?Province? and
?Country?. If a relation that occurred fewer than 5
times was judged incorrect by the majority of the
judges the relation was not included in Figure 2.
Manual fine-tuning and post-processing of the
results could filter out synonyms such as those
found for relations between ?Town? and other
classes in the domain. This would for instance de-
fine one particular relation label for the relations
?is a town in? and ?is a municipality in? that the sys-
tem discovered between ?Town? and ?Province?
and ?Town? and ?Country?, respectively.
7 Conclusion and Future Work
In this work we have shown that it is possible
to extract ontological relation labels for domain-
specific data from Wikipedia. The main contri-
bution that makes our work different from other
work on relation extraction from Wikipedia is that
the link structure is used as a strong indication of
the presence of a meaningful relation. The pres-
ence of a link is incorporated in our system by only
using sentences from Wikipedia articles that con-
tain links to other Wikipedia articles. Only those
sentences are parsed that contain the two terms we
aim to find a relation between, after which the verb
phrase and possibly the article or preposition fol-
lowing it are selected for evaluation by four human
judges.
The advantage of the pre-selection of content
that may contain a meaningful relation makes our
approach fast, as it is not necessary to parse the
whole corpus. By adding the constraint that at
least one of the query terms should be the sub-
ject of a sentence, and by ranking results by fre-
quency, our system succeeds in extracting correct
and informative relations labels. However, there is
clearly some room for improvement, for instance
in the coverage of more general types of infor-
mation such as dates and person names. For this
we intend to incorporate more domain specific re-
sources, such as research papers from the domain
that may mention persons from our database. We
are also looking into sending queries to the web,
whilst keeping the constraint of hyperlink pres-
ence.
Another factor that may help back up the rela-
tions already discovered is more evidence for ev-
ery relation. Currently we only include sentences
in our Wikipedia corpus that contain the literal
words from the title of the article, to ensure we
have content that is actually about the article and
not a related topic. This causes many sentences
in which the topic is referred to via anaphoric ex-
pressions to be missed. (Nguyen et al, 2007) take
the most frequently used pronoun in the article as
referring to the topic. This still leaves the prob-
lem of cases in which a person is first mentioned
by his/her full name and subsequently only by last
name. Coreference resolution may help to solve
this, although accuracies of current systems for en-
cyclopaedic text are often not much higher than
baselines such as those adopted by (Nguyen et al,
2007).
Errors in the database lead to some noise in
the selection of the correct Wikipedia article. The
queries we used are mostly single-word and two-
word terms, which makes disambiguation hard.
Fortunately, we have access to the class label (i.e.,
the database column name) which may be added
to the query to prevent retrieval of an article about
a country when a value from a person name col-
umn is queried. We would also like to inves-
tigate whether querying terms from a particular
database column to Wikipedia can identify incon-
sistencies in the database and hence perform a
database cleanup. Potentially, extraction of re-
lation labels from Wikipedia articles can also be
used to assign types to links in Wikipedia.
Acknowledgments
The authors would like to thank the anonymous
reviewers for their comments. This research
66
Type
Location
on the island of 
(0.500)
Genus
Order
is a 
(1.000)
Family
Class
is a 
(0.750)
Country
is in 
(0.500)
Species
is a 
(1.000)
is a 
(0.833)
Type Name
Province
occur in 
(0.333)
occur in 
(0.750)
may refer to 
(0.560)
is a 
(0.854)
is found in 
(0.635)
Town
is found in 
(0.566)
is a town in 
(0.794)
may refer to 
(0.482)
is found in 
(0.573)
is a municipality in 
(0.891)
is a town in 
(0.759)
is a 
(1.000)
Figure 2: Graph of relations between columns, with TRR scores in parentheses
was funded as part of the Continuous Access to
Cultural Heritage (CATCH) programme of the
Netherlands Organisation for Scientific Research
(NWO).
References
So?ren Auer and Jens Lehmann. 2007. What have inns-
bruck and leipzig in common? extracting seman-
tics from wiki content. In Franconi et al, editor,
Proceedings of European Semantic Web Conference
(ESWC?07), volume 4519 of Lecture Notes in Com-
puter Science, pages 503?517, Innsbruck, Austria,
June 3 - 7. Springer.
A. L. Barabasi and R. Albert. 1999. Emer-
gence of scaling in random networks. Science,
286(5439):509?512, October.
Sebastian Blohm and Philipp Cimiano. 2007. Using
the web to reduce data sparseness in pattern-based
information extraction. In Proceedings of the 11th
European Conference on Principles and Practice of
Knowledge Discovery in Databases (PKDD), War-
saw, Poland, September. Springer.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
Sergey Chernov, Tereza Iofciu, Wolfgang Nejdl, and
Xuan Zhou. 2006. Extracting semantic relation-
ships between wikipedia categories. In Proceedings
of the First Workshop on Semantic Wikis - From Wiki
to Semantics [SemWiki2006] - at ESWC 2006, pages
153 ? 163, Karlsruhe, Germany, May 15.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL?99, pages 53?60, Bergen, Nor-
way, June 12.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,
and Antal Van den Bosch. 2004. Timbl: Tilburg
memory based learner, version 5.1, reference guide.
Technical Report 04-02, ILK/Tilburg University.
J. L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
Jim Giles. 2005. Internet encyclopaedias go head to
head. Nature, 438:900?901.
Jaap Kamps and Marijn Koolen. 2008. The impor-
tance of link evidence in wikipedia. In Craig Mac-
donald, Iadh Ounis, Vassilis Plachouras, Ian Rutven,
and Ryen W. White, editors, Advances in Infor-
mation Retrieval: 30th European Conference on
IR Research (ECIR 2008), volume 4956 of Lecture
Notes in Computer Science, pages 270?282, Glas-
gow, Scotland, March 30 - April 3. Springer Verlag.
Kotaro Nakayama, Takahiro Hara, and Shojiro Nishio.
2008. Wikipedia link structure and text mining for
semantic relation extraction towards a huge scale
global web ontology. In Proceedings of Sem-
Search 2008 CEUR Workshop, pages 59?73, Tener-
ife, Spain, June 2.
Dat P. T. Nguyen, Yutaka Matsuo, and Mitsuru
Ishizuka. 2007. Exploiting syntactic and semantic
information for relation extraction from wikipedia.
In Proceedings of Workshop on Text-Mining & Link-
Analysis (TextLink 2007) at IJCAI 2007, pages
1414?1420, Hyderabad, India, January 7.
67
Dragomir R. Radev, Hong Q, Harris Wu, and Weiguo
Fan. 2002. Evaluating web-based question answer-
ing systems. In Demo section, LREC 2002, Las Pal-
mas, Spain, June.
F. M. Suchanek, G. Ifrim, and G. Wiekum. 2006.
Leila: Learning to extract information by linguistic
analysis. In Proceedings of the ACL-06 Workshop
on Ontology Learning and Population, pages 18?25,
Sydney, Australia, July.
Zareen Saba Syed, Tim Finin, and Anupam Joshi.
2008. Wikitology: Using wikipedia as an ontology.
Technical report, University of Maryland, Baltimore
County.
Max Vo?lkel, Markus Kro?tzsch, Denny Vrandecic,
Heiko Haller, and Rudi Studer. 2006. Semantic
wikipedia. In WWW 2006, pages 585?594, Edin-
burgh, Scotland.
68
Proceedings of the 8th International Conference on Computational Semantics, pages 282?285,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Comparing Alternative Data-Driven Ontological Vistas of
Natural History
Marieke van Erp, Piroska Lendvai, and Antal van den Bosch
Tilburg centre for Creative Computing, Tilburg University, The Netherlands
{M.G.J.vanErp, P.Lendvai, Antal.vdnBosch}@uvt.nl
Abstract
Traditionally, domain ontologies are created manually, based on human experts?
views on the classes and relations of the domain at hand. We present ongoing
work on two approaches to the automatic construction of ontologies from a flat
database of records, and compare them to a manually constructed ontology. The
latter CIDOC-CRM ontology focusses on the organisation of classes and relations.
In contrast, the first automatic method, based on machine learning, focuses on the
mutual predictiveness between classes, while the second automatic method, created
with the aid of Wikipedia, stresses meaningful relations between classes. The three
ontologies show little overlap; their differences illustrate that a different focus during
ontology construction can lead to radically different ontologies. We discuss the
implications of these differences, and argue that the two alternative ontologies may
be useful in higher-level information systems such as search engines.
1 Introduction
Ontologies are explicit conceptualisations of domains. A vast amount of work on on-
tologies in the knowledge representation field has focussed on their use in facilitating
knowledge sharing between knowledge-based systems, and in the interaction between
such systems and human users [3]. Ontologies can for instance offer help in visualis-
ing the domain for users, and hence improve their understanding of the information, or
they can be employed to enhance searching in domain data through for instance query
expansion or faceted navigation.
It is conceivable to have different ontologies for a single domain. Although developers
of traditional ontologies tend to stress that ?true? ontologies are function-independent,
in a more practical sense it is possible to design ontologies with a particular functionality
in mind, such as an embedding in an information system. This may influence design
choices. For certain tasks, a more formal and elaborate ontology is required, whereas
for other tasks a simpler conceptualisation of the domain that only contains the most
important classes and relations may suffice. Such considerations may influence choices
when designing an ontology, as ontology construction is an expensive task, traditionally
requiring knowledge from and formalisation by or with domain experts.
282
In the past decade, an increasing amount of work is invested in the development of
support systems for automatic or semi-automatic ontology construction, with workshops
devoted to this topic at several AI conferences such as ECAI and IJCAI [1]. In this study,
three ontologies for a single domain are presented. The scenario is that at the outset we
have a database of records, each describing an instance of an object ? in our case study,
a zoological specimen in a natural history collection, described by several textual fields.
The database column labels can serve as starting points for naming the class nodes in
our ontology. The task then is to find out how these classes are related to each other; we
let two data-driven methods induce these relations. As a gold standard for comparing
our two automatic ontology construction methods, we also have a manually designed
ontology for the domain.
2 Three Ontologies
The database used as a starting point in this paper describes key characteristics of
reptile and amphibian (R&A) specimens present in the collection of the Dutch National
Museum for Natural History
1
, using mostly textual database fields. It is constructed
manually, and contains 16,870 records in 39 columns. Most values are limited to one
or a few words, some fields contain longer stretches of text, for instance describing the
climatological circumstances or the location at which a specimen was found.
2.1 A Hierarchical Ontology
As a baseline, an ontology was manually constructed following the CIDOC-CRM concep-
tual model standards [2] (henceforth: CIDOC). It is relatively straightforward to match
each column (representing a salient domain concept) and its relevant relations from the
R&A database to a class in CIDOC. The prime goal of the CIDOC reference ontology
is to offer a framework to model the organisation of processes and entities within a
cultural heritage collection. This goal leads to a richness in hierarchical relations, ex-
pressing mainly hyponymy and meronymy relations. In Figure 2.3, these relations are
indicated by the uninterrupted lines.
2.2 A Mutual Predictiveness Ontology
The second ontology is based on the application of machine learning methods to the R&A
database. It aims to reflect the predictability of values in one database column on the
basis of values in other columns. In ontological terms: knowing the values of certain fields
in one instance, the values of certain other fields may be predicted accurately. Indeed, we
show by performing machine learning experiments that in our database certain columns
are conditionally dependent on each other. For instance, if the ?Province? field of a
record has the value West Java, most machine learning methods can be trained to induce
that the value in the ?Country? field must be Indonesia given enough training samples
1
http://www.naturalis.nl
283
of database records. Such conditional dependencies can be directly used for our current
goal: establishing relations between classes. When a machine learning algorithm (such as
a machine learning algorithm adopting an explicit feature selection preprocessing step)
actively selects a database column to predict the values of another column, we assume
that in the ontology the class nodes belonging to the two database fields are connected
by a directed ?predictiveness? relation. In Figure 2.3, the dotted lines represent the
relations between a class and its single-most predictive class.
2.3 A Hybrid Ontology
The second data-driven ontology proposed here again utilises the R&A database, as
well as a human-made semantic network resource, in order to look for possible relations
between the classes. The database is a handy starting point, as each record is already
a structured piece of information carrying instances of paired values. These pairs are
subsequently looked up in the external semantic network resource, to verify whether this
resource knows the particular relation between this pair ? which may in turn be a good
candidate label for the relation between the pairs? classes.
To this purpose we chose to use the online encyclopaedia Wikipedia
2
. Wikipedia?s
link structure can be considered a semantic network, as the guidelines for contributors
state that links from the page of one concept to another should only be added when they
are meaningfully related [4]. We find candidate relation labels between database columns
by discovering relations, i.e. linguistic predicates between values from these columns co-
occurring within the limits of a sentence, given that their Wikipedia pages are linked.
The obtained verb phrases are ranked by frequency and annotated by human evaluators.
In Figure 2.3 the relations in this hybrid ontology are indicated by the dashed lines. For
the sake of clarity not all concepts within the domain are shown in the graph and relation
labels are also excluded.
COLLECTOR
PROVINCE
GENUS
CITY
FAMILY
LOCATION
COUNTRY
PRESERVATION METHOD
DETERMINATOR
CLASS
SPECIES
LABEL INFO
COLLECTION NUMBER
RECORDER DATE
COLLECTION DATE OLD FORMAT
NUMBER
COUNTRY CODE
COLLECTION DATE EXPEDITION
RECORDER
ORDE
ALTITUDE 
Figure 1: Relations from hierarchical, data-driven and hybrid ontologies
2
http://www.wikipedia.org/
284
3 Discussion
The three ontologies presented in the previous section are remarkably different from each
other; their overlap is minimal. This can only be attributed to the different building
principles of the three ontologies.
In the machine-learning-based ontology, a relation signifies a conditional dependency
relation between two classes. Interestingly, this method uncovers relations between
classes of radically different entity types (such as between collectors and locations) that
are yet meaningful in the domain. Conditional dependence can also be a guideline for
data storage, as it indicates which information is redundant, and can thus be compressed
or stored optimally.
The hybrid ontology offers a middle ground between the machine learning and
CIDOC ontologies. It is created via analysing human-generated content in an exter-
nal semantic resource, namely Wikipedia. The obtained relations originate from a pool
of possible rankings by human judges, therefore we argue that this ontology represents
relations in the natural history domain that are fairly accepted. Compared to the hy-
brid ontology, the CIDOC ontology is rather sparse; for instance, between the biological
taxon concepts it strictly encodes the hypernym relations between parent and child
nodes, whereas the hybrid ontology tends to link everything that is somehow related
according to encyclopaedic evidence.
To conclude, we believe the hybrid approach is still crude, but it does produce possible
links between domain concepts attested in an external encyclopeadic resource, while
requiring little effort in development. We also believe that conditional dependence, as
detectable through machine learning, should be considered as a ground for establishing
relations between concepts. While the final decision should be left to human experts,
both methods may serve as useful ontology expansion proposal methods.
References
[1] Paul Buitelaar, Philipp Cimiano, and Bernardo Magnini, editors. Ontology Learning
from Text: Methods, Evaluation and Applications. IOS Press, 2005.
[2] Nick Crofts, Martin Doerr, Tony Gill, Stephen Stead, and Matthew Stiff. Definition
of the cidoc conceptual reference model. Technical report, ICOM/CIDOC CRM
Special Interest Group, 2008.
[3] Thomas R. Gruber. Formal Ontology in Conceptual Analysis and Knowledge Repre-
sentation, chapter Toward Principles for the Design of Ontologies used for knowledge
sharing, pages 907?928. Kluwer Academic Publishers, 1995.
[4] Jaap Kamps and Marijn Koolen. The importance of link evidence in Wikipedia.
In Craig Macdonald et al editor, Advances in Information Retrieval: 30th European
Conference on IR Research (ECIR 2008), volume 4956 of Lecture Notes in Computer
Science, pages 270?282, Heidelberg, 2008. Springer Verlag.
285
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1691?1701,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Offspring from Reproduction Problems:
What Replication Failure Teaches Us
Antske Fokkens and Marieke van Erp
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
{a.s.fokkens,m.g.j.van.erp}@vu.nl
Marten Postma
Utrecht University
Utrecht, The Netherlands
martenp@gmail.com
Ted Pedersen
Dept. of Computer Science
University of Minnesota
Duluth, MN 55812 USA
tpederse@d.umn.edu
Piek Vossen
The Network Institute
VU University Amsterdam
Amsterdam, The Netherlands
piek.vossen@vu.nl
Nuno Freire
The European Library
The Hague, The Netherlands
nfreire@gmail.com
Abstract
Repeating experiments is an important in-
strument in the scientific toolbox to vali-
date previous work and build upon exist-
ing work. We present two concrete use
cases involving key techniques in the NLP
domain for which we show that reproduc-
ing results is still difficult. We show that
the deviation that can be found in repro-
duction efforts leads to questions about
how our results should be interpreted.
Moreover, investigating these deviations
provides new insights and a deeper under-
standing of the examined techniques. We
identify five aspects that can influence the
outcomes of experiments that are typically
not addressed in research papers. Our use
cases show that these aspects may change
the answer to research questions leading
us to conclude that more care should be
taken in interpreting our results and more
research involving systematic testing of
methods is required in our field.
1 Introduction
Research is a collaborative effort to increase
knowledge. While it includes validating previous
approaches, our experience is that most research
output in our field focuses on presenting new ap-
proaches, and to a somewhat lesser extent building
upon existing work.
In this paper, we argue that the value of research
that attempts to replicate previous approaches goes
beyond simply validating what is already known.
It is also an essential aspect for building upon
existing approaches. Especially when validation
fails or variations in results are found, systematic
testing helps to obtain a clearer picture of both the
approach itself and of the meaning of state-of-the-
art results leading to a better insight into the qual-
ity of new approaches in relation to previous work.
We support our claims by presenting two use
cases that aim to reproduce results of previous
work in two key NLP technologies: measuring
WordNet similarity and Named Entity Recogni-
tion (NER). Besides highlighting the difficulty of
repeating other researchers? work, new insights
about the approaches emerged that were not pre-
sented in the original papers. This last point shows
that reproducing results is not merely part of good
practice in science, but also an essential part in
gaining a better understanding of the methods we
use. Likewise, the problems we face in reproduc-
ing previous results are not merely frustrating in-
conveniences, but also pointers to research ques-
tions that deserve deeper investigation.
We investigated five aspects that cause exper-
imental variation that are not typically described
in publications: preprocessing (e.g. tokenisa-
tion), experimental setup (e.g. splitting data for
cross-validation), versioning (e.g. which version
of WordNet), system output (e.g. the exact fea-
tures used for individual tokens in NER), and sys-
tem variation (e.g. treatment of ties).
As such, reproduction provides a platform for
systematically testing individual aspects of an ap-
proach that contribute to a given result. What is
the influence of the size of the dataset, for exam-
ple? How does using a different dataset affect the
results? What is a reasonable divergence between
different runs of the same experiment? Finding
answers to these questions enables us to better in-
terpret our state-of-the-art results.
1691
Moreover, the experiments in this paper show
that even while strictly trying to replicate a pre-
vious experiment, results may vary up to a point
where they lead to different answers to the main
question addressed by the experiment. The Word-
Net similarity experiment use case compares the
performance of different similarity measures. We
will show that the answer as to which measure
works best changes depending on factors such as
the gold standard used, the strategy towards part-
of-speech or the ranking coefficient, all aspects
that are typically not addressed in the literature.
The main contributions of this paper are the
following:
1) An in-depth analysis of two reproduction use
cases in NLP
2) New insights into the state-of-the-art results
for WordNet similarities and NER, found because
of problems in reproducing prior research
3) A categorisation of aspects influencing
reproduction of experiments and suggestions on
testing their influence systematically
The code, data and experimental setup
for the WordNet experiments are avail-
able at http://github.com/antske/
WordNetSimilarity, and for the NER exper-
iments at http://github.com/Mvanerp/
NER. The experiments presented in this paper
have been repeated by colleagues not involved in
the development of the software using the code
included in these repositories. The remainder of
this paper is structured as follows. In Section 2,
previous work is discussed. Sections 3 and 4
describe our real-world use cases. In Section 5,
we present our observations, followed by a more
general discussion in Section 6. In Section 7, we
present our conclusions.
2 Background
This section provides a brief overview of recent
work addressing reproduction and benchmark re-
sults in computer science related studies and dis-
cusses how our research fits in the overall picture.
Most researchers agree that validating results
entails that a method should lead to the same over-
all conclusions rather than producing the exact
same numbers (Drummond, 2009; Dalle, 2012;
Buchert and Nussbaum, 2012, etc.). In other
words, we should strive to reproduce the same an-
swer to a research question by different means,
perhaps by re-implementing an algorithm or eval-
uating it on a new (in domain) data set. Replica-
tion has a somewhat more limited aim, and simply
involves running the exact same system under the
same conditions in order to get the exact same re-
sults as output.
According to Drummond (2009) replication is
not interesting, since it does not lead to new in-
sights. On this point we disagree with Drum-
mond (2009) as replication allows us to: 1) vali-
date prior research, 2) improve on prior research
without having to rebuild software from scratch,
and 3) compare results of reimplementations and
obtain the necessary insights to perform reproduc-
tion experiments. The outcome of our use cases
confirms the statement that deeper insights into an
approach can be obtained when all resources are
available, an observation also made by Ince et al
(2012).
Even if exact replication is not a goal many
strive for, Ince et al (2012) argue that insightful
reproduction can be an (almost) impossible un-
dertaking without the source code being available.
Moreover, it is not always clear where replication
stops and reproduction begins. Dalle (2012) dis-
tinguishes levels of reproducing results related to
how close they are to the original work and how
each contributes to research. In general, an in-
creasing awareness of the importance of reproduc-
tion research and open code and data can be ob-
served based on publications in high-profile jour-
nals (e.g. Nature (Ince et al, 2012)) and initiatives
such as myExperiment.1
Howison and Herbsleb (2013) point out that,
even though this is important, often not enough
(academic) credit is gained from making resources
available. What is worse, the same holds for re-
search that investigates existing methods rather
than introducing new ones, as illustrated by the
question that is found on many review forms ?how
novel is the presented approach??. On the other
hand, initiatives for journals addressing exactly
this issue (Neylon et al, 2012) and tracks focus-
ing on results verification at conferences such as
VLDB2 show that this opinion is not universal.
A handful of use cases on reproducing or repli-
cating results have been published. Louridas and
Gousios (2012) present a use case revealing that
source code alone is not enough for reproducing
1http://www.myexperiment.org
2http://www.vldb.org/2013/
1692
results, a point that is also made by Mende (2010)
who provides an overview of all information re-
quired to replicate results.
The experiments in this paper provide use cases
that confirm the points brought out in the litera-
ture mentioned above. This includes both obser-
vations that a detailed level of information is re-
quired for truly insightful reproduction research as
well as the claim that such research leads to better
understanding of our techniques. Furthermore, the
work in this paper relates to Bikel (2004)?s work.
He provides all information needed in addition to
Collins (1999) to replicate Collins? benchmark re-
sults. Our work is similar in that we also aim to fill
in the blanks needed to replicate results. It must
be noted, however, that the use cases in this paper
have a significantly smaller scale than Bikel?s.
Our research distinguishes itself from previous
work, because it links the challenges of reproduc-
tion to what they mean for reported results be-
yond validation. Ruml (2010) mentions variations
in outcome as a reason not to emphasise compar-
isons to benchmarks. Vanschoren et al (2012)
propose to use experimental databases to system-
atically test variations for machine learning, but
neither links the two issues together. Raeder et al
(2010) come closest to our work in a critical study
on the evaluation of machine learning. They show
that choices in the methodology, such as data sets,
evaluation metrics and type of cross-validation can
influence the conclusions of an experiment, as we
also find in our second use case. However, they
focus on the problem of evaluation and recom-
mendations on how to achieve consistent repro-
ducible results. Our contribution is to investigate
how much results vary. We cannot control how
fellow researchers carry out their evaluation, but
if we have an idea of the variations that typically
occur within a system, we can better compare ap-
proaches for which not all details are known.
3 WordNet Similarity Measures
Patwardhan and Pedersen (2006) and Pedersen
(2010) present studies where the output of a va-
riety of WordNet similarity and relatedness mea-
sures are compared. They rank Miller and Charles
(1991)?s set (henceforth ?mc-set?) of 30 word
pairs according to their semantic relatedness with
several WordNet similarity measures.
Each measure ranks the mc-set of word pairs
and these outputs are compared to Miller and
Charles (1991)?s gold standard based on human
rankings using the Spearman?s Correlation Coeffi-
cient (Spearman, 1904, ?). Pedersen (2010) also
ranks the original set of 65 word pairs ranked
by humans in an experiment by Rubenstein and
Goodenough (1965) (rg-set) which is a superset of
Miller and Charles?s set.
3.1 Replication Attempts
This research emerged from a project run-
ning a similar experiment for Dutch on Cor-
netto (Vossen et al, 2013). First, an attempt
was made to reproduce the results reported in
Patwardhan and Pedersen (2006) and Peder-
sen (2010) on the English WordNet using their
WordNet::Similarity web-interface.3 Results dif-
fered from those reported in the aforementioned
works, even when using the same versions as
the original, WordNet::Similarity-1.02 and Word-
Net 2.1 (Patwardhan and Pedersen, 2006) and
WordNet::Similarity-2.05 and WordNet 3.0 (Ped-
ersen, 2010), respectively.4
The fact that results of similarity measures on
WordNet can differ even while the same software
and same versions are used indicates that proper-
ties which are not addressed in the literature may
influence the output of similarity measures. We
therefore conducted a range of experiments that,
in addition to searching for the right settings to
replicate results of previous research, address the
following questions:
1) Which properties have an impact on the per-
formance of WordNet similarity measures?
2) How much does the performance of individ-
ual measures vary?
3) How do commonly used measures compare
when the variation of their performance are taken
into account?
3.2 Methodology and first observations
The questions above were addressed in two stages.
In the first stage, Fokkens, who was not involved
in the first replication attempt implemented a
script to calculate similarity measures using Word-
Net::Similarity. This included similarity mea-
sures introduced by Wu and Palmer (1994) (wup),
3Obtained from http://talisker.d.umn.edu/
cgi-bin/similarity/similarity.cgi, Word-
Net::Similarity version 2.05. This web interface has now
moved to http://maraca.d.umn.edu
4WordNet::Similarity were obtained http://
search.cpan.org/dist/WordNet-Similarity/.
1693
Leacock and Chodorow (1998) (lch), Resnik
(1995) (res), Jiang and Conrath (1997) (jcn),
Lin (1998) (lin), Banerjee and Pedersen (2003)
(lesk), Hirst and St-Onge (1998) (hso) and
Patwardhan and Pedersen (2006) (vector and
vpairs) respectively.
Consequently, settings and properties were
changed systematically and shared with Pedersen
who attempted to produce the new results with his
own implementations. First, we made sure that
the script implemented by Fokkens could produce
the same WordNet similarity scores for each in-
dividual word pair as those used to calculate the
ranking on the mc-set by Pedersen (2010). Finally,
the gold standard and exact implementation of the
Spearman ranking coefficient were compared.
Differences in results turned out to be related
to variations in the experimental setup. First,
we made different assumptions on the restriction
of part-of-speech tags (henceforth ?PoS-tag?) con-
sidered in the comparison. Miller and Charles
(1991) do not discuss how they deal with words
with more than one PoS-tag in their study. Ped-
ersen therefore included all senses with any PoS-
tag in his study. The first replication attempt had
restricted PoS-tags to nouns based on the idea
that most items are nouns and subjects would be
primed to primarily think of the noun senses. Both
assumptions are reasonable. Pos-tags were not re-
stricted in the second replication attempt, but be-
cause of a bug in the code only the first identified
PoS-tag (?noun? in all cases) was considered. We
therefore mistakenly assumed that PoS-tag restric-
tions did not matter until we compared individual
scores between Pedersen and the replication at-
tempts.
Second, there are two gold standards for the
Miller and Charles (1991) set: one has the scores
assigned during the original experiment run by
Rubenstein and Goodenough (1965), the other
has the scores assigned during Miller and Charles
(1991)?s own experiment. The ranking correlation
between the two sets is high, but they are not iden-
tical. Again, there is no reason why one gold stan-
dard would be a better choice than the other, but in
order to replicate results, it must be known which
of the two was used. Third, results changed be-
cause of differences in the treatment of ties while
calculating Spearman ?. The influence of the ex-
act gold standard and calculation of Spearman ?
could only be found because Pedersen could pro-
measure Spearman ? Kendall ? ranking
min max min max variation
path based similarity
path 0.70 0.78 0.55 0.62 1-8
wup 0.70 0.79 0.53 0.61 1-6
lch 0.70 0.78 0.55 0.62 1-7
path based information content
res 0.65 0.75 0.26 0.57 4-11
lin 0.49 0.73 0.36 0.53 6-10
jcn 0.46 0.73 0.32 0.55 5, 7-11
path based relatedness
hso 0.73 0.80 0.36 0.41 1-3,5-10
dictionary and corpus based relatedness
vpairs 0.40 0.70 0.26 0.50 7-11
vector 0.48 0.92 0.33 0.76 1,2,4,6-11
lesk 0.66 0.83 -0.02 0.61 1-8,11,12
Table 1: Variation WordNet measures? results
vide the output of the similarity measures he used
to calculate the coefficient. It is unlikely we would
have been able to replicate his results at all with-
out the output of this intermediate step. Finally,
results for lch, lesk and wup changed accord-
ing to measure specific configuration settings such
as including a PoS-tag specific root node or turn-
ing on normalisation.
In the second stage of this research, we ran ex-
periments that systematically manipulate the influ-
ential factors described above. In this experiment,
we included both the mc-set and the complete rg-
set. The implementation of Spearman ? used in
Pedersen (2010) assigned the lowest number in
ranking to ties rather than the mean, resulting in
an unjustified drop in results for scores that lead
to many ties. We therefore experimented with a
different correlation measure, Kendall tau coeffi-
cient (Kendall, 1938, ? ) rather than two versions
of Spearman ?.
3.3 Variation per measure
All measures varied in their performance.
The complete outcome of our experiments
(both the similarity measures assigned to
each pair as well as the output of the ranking
coefficients) are included in the data set pro-
vided at http://github.com/antske/
WordNetSimilarity. Table 1 presents an
overview of the main point we wish to make
through this experiment: the minimal and maxi-
mal results according to both ranking coefficients.
Results for similarity measures varied from 0.06-
0.42 points for Spearman ? and from 0.05-0.60
points for Kendall ? . The last column indi-
cates the variation of performance of a measure
1694
compared to the other measures, where 1 is the
best performing measure and 12 is the worst.5
For instance, path has been best performing
measure, second best, eighth best and all positions
in between, vector has ranked first, second and
fourth, but also occupied all positions from six to
eleven.
In principle, it is to be expected that num-
bers are not exactly the same while evaluating
against a different data set (the mc-set versus the
rg-set), taking a different set of synsets to evalu-
ate on (changing PoS-tag restrictions) or changing
configuration settings that influence the similarity
score. However, a variation of up to 0.44 points
in Spearman ? and 0.60 in Kendall ? 6 leads to
the question of how indicative these results really
are. A more serious problem is the fact that the
comparative performance of individual measure
changes. Which measure performs best depends
on the evaluation set, ranking coefficient, PoS-tag
restrictions and configuration settings. This means
that the answer to the question of which similarity
measure is best to mimic human similarity scores
depends on aspects that are often not even men-
tioned, let alne systematically compared.
3.4 Variation per category
For each influential category of experimental vari-
ation, we compared the variation in Spearman ?
and Kendall ? , while similarity measure and other
influential categories were kept stable. The cat-
egories we varied include WordNet and Word-
Net::Similarity version, the gold standard used to
evaluate, restrictions on PoS-tags, and measure
specific configurations. Table 2 presents the maxi-
mum variation found across measures for each cat-
egory. The last column indicates how often the
ranking of a specific measure changed as the cat-
egory changed, e.g. did the measure ranking third
using specific configurations, PoS-tag restrictions
and a specific gold standard using WordNet 2.1
still rank third when WordNet 3.0 was used in-
stead? The number in parentheses next to the ?dif-
ferent ranks? in the table presents the total num-
ber of scores investigated. Note that this num-
ber changes for each category, because we com-
5Some measures ranked differently as their individual
configuration settings changed. In these cases, the measure
was included in the overall ranking multiple times, which is
why there are more ranking positions than measures.
6Section 3.4 explains why the variation in Kendall is this
extreme and ? is more appropriate for this task.
Variation Maximum difference Different
Spearman ? Kendall ? rank (tot)
WN version 0.44 0.42 223 (252)
gold standard 0.24 0.21 359 (504)
PoS-tag 0.09 0.08 208 (504)
configuration 0.08 0.60 37 (90)
Table 2: Variations per category
pared two WordNet versions (WN version), three
gold standard and PoS-tag restriction variations
and configuration only for the subset of scores
where configuration matters.
There are no definite statements to make as to
which version (Patwardhan and Pedersen (2006)
vs Pedersen (2010)), PoS-tag restriction or con-
figuration gives the best results. Likewise, while
most measures do better on the smaller data set,
some achieve their highest results on the full set.
This is partially due to the fact that ranking coef-
ficients are sensitive to outliers. In several cases
where PoS-tag restrictions led to different results,
only one pair received a different score. For in-
stance, path assigns a relatively high score to
the pair chord-smile when verbs are included, be-
cause the hierarchy of verbs in WordNet is rela-
tively flat. This effect is not observed in wup and
lch which correct for the depth of the hierarchy.
On the other hand, res, lin and jcn score bet-
ter on the same set when verbs are considered, be-
cause they cannot detect any relatedness for the
pair crane-implement when restricted to nouns.
On top of the variations presented above, we no-
tice a discrepancy between the two coefficients.
Kendall ? generally leads to lower coefficiency
scores than Spearman ?. Moreover, they each
give different relative indications: where lesk
achieves its highest Spearman ?, it has an ex-
tremely low Kendall ? of 0.01. Spearman ? uses
the difference in rank as its basis to calculate a cor-
relation, where Kendall ? uses the number of items
with the correct rank. The low Kendall ? for lesk
is the result of three pairs receiving a score that is
too high. Other pairs that get a relatively accurate
score are pushed one place down in rank. Because
only items that receive the exact same rank help to
increase ? , such a shift can result in a drastic drop
in the coefficient. In our opinion, Spearman ? is
therefore preferable over Kendall ? . We included
? , because many authors do not mention the rank-
ing coefficient they use (cf. Budanitsky and Hirst
(2006), Resnik (1995)) and both ? and ? are com-
1695
monly used coefficients.
Except for WordNet, which Budanitsky and
Hirst (2006) hold accountable for minor variations
in a footnote, the influential categories we investi-
gated in this paper, to our knowledge, have not yet
been addressed in the literature. Cramer (2008)
points out that results from WordNet-Human sim-
ilarity correlations lead to scattered results report-
ing variations similar to ours, but she compares
studies using different measures, data and exper-
imental setup. This study shows that even if
the main properties are kept stable, results vary
enough to change the identity of the measure that
yields the best performance. Table 1 reveals a
wide variation in ranking relative to alternative ap-
proaches. Results in Table 2 show that it is com-
mon for the ranking of a score to change due to
variations that are not at the core of the method.
This study shows that it is far from clear how
different WordNet similarity measures relate to
each other. In fact, we do not know how we can
obtain the best results. This is particularly chal-
lenging, because the ?best results? may depend on
the intended use of the similarity scores (Meng
et al, 2013). This is also the reason why we
presented the maximum variation observed, rather
than the average or typical variation (mostly be-
low 0.10 points). The experiments presented in
this paper resulted in a vast amount of data. An
elaborate analysis of this data is needed to get a
better understanding of how measures work and
why results vary to such an extent. We leave this
investigation to future work. If there is one take-
home message from this experiment, it is that one
should experiment with parameters such as restric-
tions on PoS-tags or configurations and determine
which score to use depending on what it is used
for, rather than picking something that did best in
a study using different data for a different task and
may have used a different version of WordNet.
4 Reproducing a NER method
Freire et al (2012) describe an approach to clas-
sifying named entities in the cultural heritage do-
main. The approach is based on the assumption
that domain knowledge, encoded in complex fea-
tures, can aid a machine learning algorithm in
NER tasks when only little training data is avail-
able. These features include information about
person and organisation names, locations, as well
as PoS-tags. Additionally, some general features
are used such as a window of three preceding and
two following tokens, token length and capitalisa-
tion information. Experiments are run in a 10-fold
cross-validation setup using an open source ma-
chine learning toolkit (McCallum, 2002).
4.1 Reproducing NER Experiments
This experiment can be seen as a real-world case
of the sad tale of the Zigglebottom tagger (Peder-
sen, 2008). The (fictional) Zigglebottom tagger is
a tagger with spectacular results that looks like it
will solve some major problems in your system.
However, the code is not available and a new im-
plementation does not yield the same results. The
original authors cannot provide the necessary de-
tails to reproduce their results, because most of the
work has been done by a PhD student who has fin-
ished and moved on to something else. In the end,
the newly implemented Zigglebottom tagger is not
used, because it does not lead to the promised bet-
ter results and all effort went to waste.
Van Erp was interested in the NER approach
presented in Freire et al (2012). Unfortunately,
the code could not be made available, so she de-
cided to reimplement the approach. Despite feed-
back from Freire about particular details of the
system, results remained 20 points below those
reported in Freire et al (2012) in overall F-score
(Van Erp and Van der Meij, 2013).
The reimplementation process involved choices
about seemingly small details such as rounding
to how many decimals, how to tokenise or how
much data cleanup to perform (normalisation of
non-alphanumeric characters for example). Try-
ing different parameter combinations for feature
generation and the algorithm never yielded the ex-
act same results as Freire et al (2012). The results
of the best run in our first reproduction attempt,
together with the original results from Freire et al
(2012) are presented in Table 3. Van Erp and Van
der Meij (2013) provide an overview of the imple-
mentation efforts.
4.2 Following up from reproduction
Since the experiments in Van Erp and Van der Meij
(2013) introduce several new research questions
regarding the influence of data cleaning and the
limitations of the dataset, we performed some ad-
ditional experiments.
First, we varied the tokenisation, removing non-
alphanumeric characters from the data set. This
yielded a significantly smaller data set (10,442
1696
(Freire et al, 2012) results Van Erp and Van der Meij?s replication results
Precision Recall F?=1 Precision Recall F?=1
LOC (388) 92% 55% 69 77.80% 39.18% 52.05
ORG (157) 90% 57% 70 65.75% 30.57% 41.74
PER (614) 91% 56% 69 73.33% 37.62% 49.73
Overall (1,159) 91% 55% 69 73.33% 37.19% 49.45
Table 3: Precision, recall and F?=1 scores for the original experiments from Freire et al 2012 and our
replication of their approach as presented in Van Erp and Van der Meij (2013)
tokens vs 12,510), and a 15 point drop in over-
all F-score. Then, we investigated whether vari-
ation in the cross-validation splits made any dif-
ference as we noticed that some NEs were only
present in particular fields in the data, which can
have a significant impact on a small dataset. We
inspected the difference between different cross-
validation folds by computing the standard devi-
ations of the scores and found deviations of up
to 25 points in F-score between the 10 splits. In
the general setup, database records were randomly
distributed over the folds and cut off to balance the
fold sizes. In a different approach to dividing the
data by distributing individual sentences from the
records over the folds, performance increases by
8.57 points in overall F-score to 58.02. This is not
what was done in the original Freire et al (2012)
paper, but shows that the results obtained with this
dataset are quite fragile.
As we worried about the complexity of the fea-
ture set relative to the size of the data set, we de-
viated somewhat from Freire et al (2012)?s exper-
iments in that we switched some features on and
off. Removal of complex features pertaining to the
window around the focus token improved our re-
sults by 3.84 points in overall F-score to 53.39.
The complex features based on VIAF,7 GeoN-
ames8 and WordNet do contribute to the classifica-
tion in the Mallet setup as removing them and only
using the focus token, window and generic fea-
tures causes a slight drop in overall F-score from
49.45 to 47.25.
When training the Stanford NER system (Finkel
et al, 2005) on just the tokens from the
Freire data set and the parameters from en-
glish.all.3class.distsim.prop (included in the Stan-
ford NER release, see also Van Erp and Van der
Meij (2013)), our F-scores come very close to
those reported by Freire et al (2012), but mostly
with a higher recall and lower precision. It is puz-
zling that the Stanford system obtains such high
7http://www.viaf.org
8http://www.geonames.org
results with only very simple features, whereas
for Mallet the complex features show improve-
ment over simpler features. This leads to ques-
tions about the differences between the CRF im-
plementations and the influence of their parame-
ters, which we hope to investigate in future work.
4.3 Reproduction difficulties explained
Several reasons may be the cause of why we fail to
reproduce results. As mentioned, not all resources
and data were available for this experiment, thus
causing us to navigate in the dark as we could not
reverse-engineer intermediate steps, but only com-
pare to the final precision, recall and F-scores.
The experiments follow a general machine
learning setup consisting roughly of four steps:
preprocess data, generate features, train model and
test model. The novelty and replication problems
lie in the first three steps. How the data was pre-
processed is a major factor here. The data set con-
sisted of XML files marked up with inline named
entity tags. In order to generate machine learn-
ing features, this data has to be tokenised, possi-
bly cleaned up and the named entity markup had
to be converted to a token-based scheme. Each of
these steps can be carried out in several ways, and
choices made here can have great influence on the
rest of the pipeline.
Similar choices have to be made for prepro-
cessing external resources. From the descriptions
in the original paper, it is unclear how records
in VIAF and GeoNames were preprocessed, or
even which versions of these resources were used.
Preprocessing and calculating occurrence statis-
tics over VIAF takes 30 hours for each run. It
is thus not feasible to identify the main potential
variations without the original data to verify this
prepatory step.
Numbers had to be rounded when generating
the features, leading to the question of how many
decimals are required to be discriminative with-
out creating an overly sparse dataset. Freire recalls
that encoding features as multi-value discrete fea-
1697
tures versus several boolean features can have sig-
nificant impact. These settings are not mentioned
in the paper, making reproduction very difficult.
As the project in which the original research
was performed has ended, and there is no cen-
tral repository where such information can be re-
trieved, we are left to wonder how to reuse this
approach in order to further domain-specific NER.
5 Observations
In this section, we generalise the observations
from our use cases to the main categories that can
influence reproduction.
Despite our efforts to describe our systems as
clearly as possible, details that can make a tremen-
dous difference are often omitted in papers. It will
be no surprise to researchers in the field that pre-
processing of data can make or break an experi-
ment.
The choice of which steps we perform, and how
each of these steps is carried out exactly are part
of our experimental setup. A major difference in
the results for the NER experiments was caused by
variations in the way in which we split the data for
cross-validation.
As we fine-tune our techniques, software gets
updated, data sets are extended or annotation bugs
are fixed. In the WordNet experiment, we found
that there were two different gold standard data
sets. There are also different versions of Word-
Net, and the WordNet::Similarity packages. Sim-
ilarly for the NER experiment, GeoNames, VIAF
and Mallet are updated regularly. It is therefore
critical to pay attention to versioning.
Our experiments often consist of several differ-
ent steps whose outputs may be difficult to retrace.
In order to check the output of a reproduction ex-
periment at every step of the way, system out-
put of experiments, including intermediate steps,
is vital. The WordNet replication was only pos-
sible, because Pedersen could provide the similar-
ity scores of each word pair. This enabled us to
compare the intermediate output and identify the
source of differences in output.
Lastly, there may be inherent system variations
in the techniques used. Machine learning algo-
rithms may for instance use coin flips in case of
a tie. This was not observed in our experiments,
but such variations may be determined by running
an experiment several times and taking the average
over the different runs (cf. Raeder et al (2010)).
All together, these observations show that shar-
ing data and software play a key role in gaining in-
sight into how our methods work. Vanschoren et
al. (2012) propose a setup that allows researchers
to provide their full experimental setup, which
should include exact steps followed in preprocess-
ing the data, documentation of the experimen-
tal setup, exact versions of the software and re-
sources used and experimental output. Having
access to such a setup allows other researchers
to validate research, but also tweak the approach
to investigate system variation, systematically test
the approach in order to learn its limitations and
strengths and ultimately improve on it.
6 Discussion
Many of the aspects addressed in the previous sec-
tion such as preprocessing are typically only men-
tioned in passing, or not at all. There is often not
enough space to capture all details, and they are
generally not the core of the research described.
Still, our use cases have shown that they can have a
tremendous impact on reproduction, and can even
lead to different conclusions. This leads to serious
questions on how we can interpret our results and
how we can compare the performance of different
methods. Is an improvement of a few per cent re-
ally due to the novelty of the approach if larger
variations are found when the data is split differ-
ently? Is a method that does not quite achieve the
highest reported state-of-the-art result truly less
good? What does a state-of-the-art result mean if
it is only tested on one data set?
If one really wants to know whether a result
is better or worse than the state-of-the-art, the
range of variation within the state-of-the-art must
be known. Systematic experiments such as the
ones we carried out for WordNet similarity and
NER, can help determine this range. For results
that fall within the range, it holds that they can
only be judged by evaluations going beyond com-
paring performance numbers, i.e. an evaluation of
how the approach achieves a given result and how
that relates to alternative approaches.
Naturally, our use cases do not represent the en-
tire gamut of research methodologies and prob-
lems in the NLP community. However, they do
represent two core technologies and our observa-
tions align with previous literature on replication
and reproduction.
Despite the systematic variation we employed
1698
in our experiments, they do not answer all ques-
tions that the problems in reproduction evoked.
For the WordNet experiments, deeper analysis is
required to gain full understanding of how indi-
vidual influential aspects interact with each mea-
surement. For the NER experiments, we are yet to
identify the cause of our failure to reproduce.
The considerable time investment required for
such experiments forms a challenge. Due to pres-
sure to publish or other time limitations, they can-
not be carried out for each evaluation. There-
fore, it is important to share our experiments, so
that other researchers (or students) can take this
up. This could be stimulated by instituting repro-
duction tracks in conferences, thus rewarding sys-
tematic investigation of research approaches. It
can also be aided by adopting initiatives that en-
able authors to easily include data, code and/or
workflows with their publications such as the
PLOS/figshare collaboration.9 We already do a
similar thing for our research problems by organ-
ising challenges or shared tasks, why not extend
this to systematic testing of our approaches?
7 Conclusion
We have presented two reproduction use cases for
the NLP domain. We show that repeating other
researchers? experiments can lead to new research
questions and provide new insights into and better
understanding of the investigated techniques.
Our WordNet experiments show that the perfor-
mance of similarity measures can be influenced by
the PoS-tags considered, measure specific varia-
tions, the rank coefficient and the gold standard
used for comparison. We not only find that such
variations lead to different numbers, but also dif-
ferent rankings of the individual measures, i.e.
these aspects lead to a different answer to the
question as to which measure performs best. We
did not succeed in reproducing the NER results
of Freire et al (2012), showing the complexity
of what seems a straightforward reproduction case
based on a system description and training data
only. Our analyses show that it is still an open
question whether additional complex features im-
prove domain specific NER and that this may par-
tially depend on the CRF implementation.
Some observations go beyond our use cases. In
particular, the fact that results vary significantly
9http://blogs.plos.org/plos/2013/01/
easier-access-to-plos-data/
because of details that are not made explicit in
our publications. Systematic testing can provide
an indication of this variation. We have classi-
fied relevant aspects in five categories occurring
across subdisciplines of NLP: preprocessing, ex-
perimental setup, versioning, system output,
and system variation.
We believe that knowing the influence of differ-
ent aspects in our experimental workflow can help
increase our understanding of the robustness of
the approach at hand and will help understand the
meaning of the state-of-the-art better. Some tech-
niques are reused so often (the papers introducing
WordNet similarity measures have around 1,000-
2,000 citations each as of February 2013, for ex-
ample) that knowing their strengths and weak-
nesses is essential for optimising their use.
As mentioned many times before, sharing is key
to facilitating reuse, even if the code is imper-
fect and contains hacks and possibly bugs. In the
end, the same holds for software as for documen-
tation: it is like sex: if it is good, it is very good
and if it is bad, it is better than nothing!10 But
most of all: when reproduction fails, regardless of
whether original code or a reimplementation was
used, valuable insights can emerge from investi-
gating the cause of this failure. So don?t let your
failing reimplementations of the Zigglebottom tag-
ger collect dusk on a shelf while others reimple-
ment their own failing Zigglebottoms. As a com-
munity, we need to know where our approaches
fail, as much ?if not more? as where they succeed.
Acknowledgments
We would like to thank the anonymous review-
ers for their eye to detail and useful comments
to make this a better paper. We furthermore
thank Ruben Izquierdo, Lourens van der Meij,
Christoph Zwirello, Rebecca Dridan and the Se-
mantic Web Group at VU University for their
help and useful feedback. The research leading to
this paper was supported by the European Union?s
7th Framework Programme via the NewsReader
Project (ICT-316404), the Agora project, by NWO
CATCH programme, grant 640.004.801, and the
BiographyNed project, a joint project with Huy-
gens/ING Institute of the Dutch Academy of Sci-
ences funded by the Netherlands eScience Center
(http://esciencecenter.nl/).
10The documentation variant of this quote is attributed to
Dick Brandon.
1699
References
Stanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the Eighteenth International Joint
Conference on Artificial Intelligence, pages 805?
810, Acapulco, August.
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30(4):479?511.
Tomasz Buchert and Lucas Nussbaum. 2012. Lever-
aging business workflows in distributed systems re-
search for the orchestration of reproducible and scal-
able experiments. In Anne Etien, editor, 9e`me
e?dition de la confe?rence MAnifestation des JE-
unes Chercheurs en Sciences et Technologies de
l?Information et de la Communication - MajecSTIC
2012 (2012).
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating WordNet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Phd dissertation,
University of Pennsylvania.
Irene Cramer. 2008. How well do semantic related-
ness measures perform? a meta-study. In Semantics
in Text Processing. STEP 2008 Conference Proceed-
ings, volume 1, pages 59?70.
Olivier Dalle. 2012. On reproducibility and trace-
ability of simulations. In WSC-Winter Simulation
Conference-2012.
Chris Drummond. 2009. Replicability is not repro-
ducibility: nor is it good science. In Proceedings of
the Twenty-Sixth International Conference on Ma-
chine Learning: Workshop on Evaluation Methods
for Machine Learning IV.
Jenny Finkel, Trond Grenager, and Christopher D.
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 363?370, Ann Arbor, USA.
Nuno Freire, Jose? Borbinha, and Pa?vel Calado. 2012.
An approach for named entity recognition in poorly
structured data. In Proceedings of ESWC 2012.
Graeme Hirst and David St-Onge. 1998. Lexical
chains as representations of context for the detection
and correction of malapropisms. In C. Fellbaum, ed-
itor, WordNet: An electronic lexical database, pages
305?332. MIT Press.
James Howison and James D. Herbsleb. 2013. Shar-
ing the spoils: incentives and collaboration in sci-
entific software development. In Proceedings of the
2013 conference on Computer Supported Coopera-
tive Work, pages 459?470.
Darrel C. Ince, Leslie Hatton, and John Graham-
Cumming. 2012. The case for open computer pro-
grams. Nature, 482(7386):485?488.
Jay J. Jiang and David W. Conrath. 1997. Semantic
similarity based on corpus statistics and lexical tax-
onomy. In Proceedings of the International Confer-
ence on Research in Computational Linguistics (RO-
CLING X), pages 19?33, Taiwan.
Maurice Kendall. 1938. A new measure of rank corre-
lation. Biometrika, 30(1-2):81?93.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and WordNet similarity for
word sense identification. In C. Fellbaum, edi-
tor, WordNet: An electronic lexical database, pages
265?283. MIT Press.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th In-
ternational Conference on Machine Learning, pages
296?304, Madison, USA.
Panos Louridas and Georgios Gousios. 2012. A note
on rigour and replicability. SIGSOFT Softw. Eng.
Notes, 37(5):1?4.
Andrew K. McCallum. 2002. MALLET: A machine
learning for language toolkit. http://mallet.
cs.umass.edu.
Thilo Mende. 2010. Replication of defect prediction
studies: problems, pitfalls and recommendations. In
Proceedings of the 6th International Conference on
Predictive Models in Software Engineering. ACM.
Lingling Meng, Runqing Huang, and Junzhong Gu.
2013. A review of semantic similarity measures in
wordnet. International Journal of Hybrid Informa-
tion Technology, 6(1):1?12.
George A. Miller and Walter G. Charles. 1991. Con-
textual correlates of semantic similarity. Language
and Cognitive Processes, 6(1):1?28.
Cameron Neylon, Jan Aerts, C Titus Brown, Si-
mon J Coles, Les Hatton, Daniel Lemire, K Jar-
rod Millman, Peter Murray-Rust, Fernando Perez,
Neil Saunders, Nigam Shah, Arfon Smith, Gae?l
Varoquaux, and Egon Willighagen. 2012. Chang-
ing computational research. the challenges ahead.
Source Code for Biology and Medicine, 7(2).
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing wordnet based context vectors to estimate the
semantic relatedness of concepts. In Proceedings of
the EACL 2006 Workshop Making Sense of Sense -
Bringing Computational Linguistics and Psycholin-
guistics Together, pages 1?8, Trento, Italy.
Ted Pedersen. 2008. Empiricism is not a matter of
faith. Computational Linguistics, 34(3):465?470.
1700
Ted Pedersen. 2010. Information content measures
of semantic similarity perform better without sense-
tagged text. In Proceedings of the 11th Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT
2010), pages 329?332, Los Angeles, USA.
Troy Raeder, T. Ryan Hoens, and Nitesh V. Chawla.
2010. Consequences of variability in classifier per-
formance estimates. In Proceedings of ICDM?2010.
Philip Resnik. 1995. Using information content to
evaluate semantic similarity in a taxonomy. In Pro-
ceedings of the 14th International Joint Conference
on Artificial Intelligence (IJCAI), pages 448?453,
Montreal, Canada.
Herbert Rubenstein and John B. Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Wheeler Ruml. 2010. The logic of benchmarking: A
case against state-of-the-art performance. In Pro-
ceedings of the Third Annual Symposium on Combi-
natorial Search (SOCS-10).
Charles Spearman. 1904. Proof and measurement of
association between two things. American Journal
of Psychology, 15:72?101.
Marieke Van Erp and Lourens Van der Meij. 2013.
Reusable research? a case study in named entity
recognition. CLTL 2013-01, Computational Lexi-
cology & Terminology Lab, VU University Amster-
dam.
Joaquin Vanschoren, Hendrik Blockeel, Bernhard
Pfahringer, and Geoffrey Holmes. 2012. Experi-
ment databases. Machine Learning, 87(2):127?158.
Piek Vossen, Isa Maks, Roxane Segers, Hennie van der
Vliet, Marie-Francine Moens, Katja Hofmann, Erik
Tjong Kim Sang, and Maarten de Rijke. 2013. Cor-
netto: a Combinatorial Lexical Semantic Database
for Dutch. In Peter Spyns and Jan Odijk, editors, Es-
sential Speech and Language Technology for Dutch
Results by the STEVIN-programme, number XVII in
Theory and Applications of Natural Language Pro-
cessing, chapter 10. Springer.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the
32nd Annual Meeting of the Association for Com-
putational Linguistics, pages 133?138, Las Cruces,
USA.
1701
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 11?20,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
GAF: A Grounded Annotation Framework for Events
Antske Fokkens, Marieke van Erp, Piek Vossen
The Network Institute
VU University Amsterdam
antske.fokkens@vu.nl
marieke.van.erp@vu.nl
piek.vossen@vu.nl
Sara Tonelli
FBK
Trento, Italy
satonelli@fbk.eu
Willem Robert van Hage
SynerScope B.V.
Eindhoven, The Netherlands
willem.van.hage
@synerscope.com
Luciano Serafini, Rachele Sprugnoli
FBK
Trento, Italy
serafini@fbk.eu
sprugnoli@fbk.eu
Jesper Hoeksema
The Network Institute
VU University Amsterdam
j.e.hoeksema@vu.nl
Abstract
This paper introduces GAF, a grounded an-
notation framework to represent events in a
formal context that can represent information
from both textual and extra-textual sources.
GAF makes a clear distinction between men-
tions of events in text and their formal rep-
resentation as instances in a semantic layer.
Instances are represented by RDF compliant
URIs that are shared across different research
disciplines. This allows us to complete textual
information with external sources and facili-
tates reasoning. The semantic layer can inte-
grate any linguistic information and is com-
patible with previous event representations in
NLP. Through a use case on earthquakes in
Southeast Asia, we demonstrate GAF flexibil-
ity and ability to reason over events with the
aid of extra-linguistic resources.
1 Introduction
Events are not only described in textual documents,
they are also represented in many other non-textual
sources. These sources include videos, pictures,
sensors or evidence from data registration such as
mobile phone data, financial transactions and hos-
pital registrations. Nevertheless, many approaches
to textual event annotation consider events as text-
internal-affairs, possibly across multiple documents
but seldom across different modalities. It follows
from the above that event representation is not ex-
clusively a concern for the NLP community. It also
plays a major role in several other branches of in-
formation science such as knowledge representation
and the Semantic Web, which have created their own
models for representing events.
We propose a grounded annotation framework
(GAF) that allows us to interconnect different ways
of describing and registering events, including non-
linguistic sources. GAF representations can be used
to reason over the cumulated and linked sources of
knowledge and information to interpret the often in-
complete and fragmented information that is pro-
vided by each source. We make a clear distinction
between mentions of events in text or any other form
of registration and their formal representation as in-
stances in a semantic layer.
Mentions in text are annotated using the Terence
Annotation Format (Moens et al, 2011, TAF) on top
of which the semantic layer is realized using Seman-
tic Web technologies and standards. In this semantic
layer, instances are denoted with Uniform Resource
Identifiers (URIs). Attributes and relations are ex-
pressed according to the Simple Event Model (Van
Hage et al, 2011, SEM) and other established on-
tologies. Statements are grouped in named graphs
based on provenance and (temporal) validity, en-
abling the representation of conflicting information.
External knowledge can be related to instances from
a wide variety of sources such as those found in the
Linked Open Data Cloud (Bizer et al, 2009a).
Instances in the semantic layer can optionally be
linked to one or more mentions in text or to other
sources. Because linking instances is optional, our
11
representation offers a straightforward way to in-
clude information that can be inferred from text,
such as implied participants or whether an event is
part of a series that is not explicitly mentioned. Due
to the fact that each URI is unique, it is clear that
mentions connected to the same URI have a coref-
erential relation. Other relations between instances
(participants, subevents, temporal relations, etc.) are
represented explicitly in the semantic layer.
The remainder of this paper is structured as fol-
lows. In Section 2, we present related work and ex-
plain the motivation behind our approach. Section 3
describes the in-text annotation approach. Our se-
mantic annotation layer is presented in Section 4.
Sections 5-7 present GAF through a use case on
earthquakes in Indonesia. This is followed by our
conclusions and future work in section 8.
2 Motivation and Background
Annotation of events and of relations between them
has a long tradition in NLP. The MUC confer-
ences (Grishman and Sundheim, 1996) in the 90s
did not explicitly annotate events and coreference
relations, but the templates used for evaluating the
information extraction tasks indirectly can be seen
as annotation of events represented in newswires.
Such events are not ordered in time or further related
to each other. In response, Setzer and Gaizauskas
(2000) describe an annotation framework to create
coherent temporal orderings of events represented
in documents using closure rules. They suggest that
reasoning with text independent models, such as a
calendar, helps annotating textual representations.
More recently, generic corpora, such as Prop-
bank (Palmer et al, 2005) and the Framenet cor-
pus (Baker et al, 2003) have been built according to
linguistic principles. The annotations aim at prop-
erly representing verb structures within a sentence
context, focusing on verb arguments, semantic roles
and other elements. In ACE 2004 (Linguistic Data
Consortium, 2004b), event detection and linking is
included as a pilot task for the first time, inspired by
annotation schemes developed for named entities.
They distinguish between event mentions and the
trigger event, which is the mention that most clearly
expresses its occurrence (Linguistic Data Consor-
tium, 2004a). Typically, agreement on the trigger
event is low across annotators (around 55% (Moens
et al, 2011)). Timebank (Pustejovsky et al, 2006b)
is a more recent corpus for representing events and
time-expressions that includes temporal relations in
addition to plain coreference relations.
All these approaches have in common that they
consider the textual representation as a closed world
within which events need to be represented. This
means that mentions are linked to a trigger event
or to each other but not to an independent semantic
representation. More recently, researchers started to
annotate events across multiple documents, such as
the EventCorefBank (Bejan and Harabagiu, 2010).
Cross-document coreference is more challenging for
establishing the trigger event, but it is in essence not
different from annotating textual event coreference
within a single document. Descriptions of events
across documents may complement each other pro-
viding a more complete picture, but still textual de-
scriptions tend to be incomplete and sparse with re-
spect to time, place and participants. At the same
time, the comparison of events becomes more com-
plex. We thus expect even lower agreement in as-
signing trigger events across documents. Nothman
et al (2012) define the trigger as the first new ar-
ticle that mentions an event, which is easier than
to find the clearest description and still report inter-
annotator agreement of .48 and .73, respectively.
Recent approaches to automatically resolve event
coreference (cf. Chambers and Jurafsky (2011a),
Bejan and Harabagiu (2010)) use some background
data to establish coreference and other relations be-
tween events in text. Background information, in-
cluding resources, and models learned from textual
data do not represent mentions of events directly but
are useful to fill gaps of knowledge in the textual
descriptions. They do not alter the model for anno-
tation as such.
We aim to take these recent efforts one step fur-
ther and propose a grounded annotation framework
(GAF). Our main goal is to integrate information
from text analysis in a formal context shared with
researchers across domains. Furthermore, GAF is
flexible enough to contain contradictory informa-
tion. This is both important to represent sources
that (partially) contradict each other and to com-
bine alternative annotations or output of different
NLP tools. Because conflicting information may be
12
present, provenance of information is provided in
our framework, so that we may decide which source
to trust more or use it as a feature to decide which in-
terpretation to follow. Different models of event rep-
resentation exist that can contribute valuable infor-
mation. Therefore our model is compliant with prior
approaches regardless of whether they are manual or
automatic. Finally, GAF makes a clear distinction
between instances and instance mentions avoiding
the problem of determining a trigger event. Addi-
tionally, it facilitates the integration of information
from extra-textual sources and information that can
be inferred from texts, but is not explicitly men-
tioned. Sections 5 to 7 will explain how we can
achieve this with GAF.
3 The TERENCE annotation format
The TERENCE Annotation Format (TAF) is de-
fined within the TERENCE Project1 with the goal
to include event mentions, temporal expressions and
participant mentions in a single annotation proto-
col (Moens et al, 2011). TAF is based on ISO-
TimeML (Pustejovsky et al, 2010), but introduces
several adaptations in order to fit the domain of chil-
dren?s stories for which it was originally developed.
The format has been used to annotate around 30 chil-
dren stories in Italian and 10 in English.
We selected TAF as the basis for our in-text anno-
tation for three reasons. First, it incorporates the (in
our opinion crucial) distinction between instances
and instance mentions. Second, it adapts some con-
solidated paradigms for linguistic annotation such as
TimeML for events and temporal expressions and
ACE for participants and participant mentions (Lin-
guistic Data Consortium, 2005). It is thus compat-
ible with other annotation schemes. Third, it inte-
grates the annotation of event mentions, participants
and temporal expressions into a unified framework.
We will elaborate briefly on these properties below.
As mentioned, TAF makes a clear distinction be-
tween instances and instance mentions. Originally,
this distinction only applied to nominal and named
entities, similar to ACE (Linguistic Data Consor-
tium, 2005), because children?s stories can gener-
ally be treated as a closed world, usually present-
1ICT FP7 Programme, ICT-2010-25410, http://www.
terenceproject.eu/
ing a simple sequence of events that do not corefer.
Event coreference and linking to other sources was
thus not relevant for this domain. In GAF, we ex-
tend the distinction between instances and instance
mentions to events to model event coreference, link
them to other sources and create a consistent model
for all instances.
Children?s stories usually include a small set of
characters, event sequences (mostly in chronologi-
cal order), and a few generic temporal expressions.
In the TERENCE project, modeling characters in
the stories is necessary. This requires an extension
of TimeML to deal with event participants. Puste-
jovsky et al (2006a) address the need to include ar-
guments in TimeML annotations, but that proposal
did not include specific examples and details on how
to perform annotation (e.g., on the participants? at-
tributes). Such guidelines were created for TAF.
The TAF annotation of event mentions largely
follows TimeML in annotating tense, aspect, class,
mood, modality and polarity and temporal expres-
sions. However, there are several differences be-
tween TAF and TimeML. First, temporal expres-
sions are not normalized into the ISO-8601 form,
because most children?s stories are not fixed to a spe-
cific date. In GAF, the normalization of expressions
takes place in the semantic layer as these go beyond
the scope of the text. As a result, temporal vague-
ness of linguistic expressions in text do not need to
be normalized in the textual representation to actual
time points and remain underspecified.2
In TAF, events and participant mentions are linked
through a has participant relation, which is defined
as a directional, one-to-one relation from the event
to the participant mentions. Only mentions corre-
sponding to mandatory arguments of the events in
the story are annotated. Annotators look up each
verb in a reference dictionary providing information
on the predicate-argument structure of each verb.
This makes annotation easier and generally not con-
troversial. However, this kind of information can be
provided only by annotators having a good knowl-
edge of linguistics.
All annotations are performed with the Celct An-
2Note that we can still use existing tools for normalization
at the linguistic level: early normalizations can be integrated
in the semantic layer alongside normalizations carried out at a
later point.
13
sem:sub
EventOf
sem:Event sem:Actor sem:Place sem:Time
sem:hasTime
sem:hasActor
sem:hasPlace
sem:PlaceType
sem:placeType
sem:EventType
sem:eventType
sem:ActorType
sem:actorType
sem:TimeType
sem:Type
sem:timeType
sem:Core
sem:subTypeOf
C
o
r
e
 
C
l
a
s
s
e
s
(
F
o
r
e
i
g
n
)
T
y
p
e
 
S
y
s
t
e
m
Literal sem:hasTimeStamp
Literal sem:hasTimeStamp
Figure 1: The SEM ontology
notation Tool (Bartalesi Lenzi et al, 2012), an online
tool supporting TimeML that can easily be extended
to include participant information. The annotated
file can be exported to various XML formats and im-
ported into the semantic layer. The next section de-
scribes SEM, the event model used in our semantic
layer, and how it complements the TAF annotations.
4 The Simple Event Model
The Simple Event Model (SEM) is an RDF
schema (Carroll and Klyne, 2004; Guha and Brick-
ley, 2004) to express who did what, where, and
when. There are many RDF schemas and OWL on-
tologies (Motik et al, 2009) that describe events,
e.g., Shaw et al (2009), Crofts et al (2008) and
Scherp et al (2009). SEM is among the most
flexible and easiest to adapt to different domains.
SEM describes events and related instances such as
the place, time and participants (called Actors in
SEM) by representing the interactions between the
instances with RDF triples. SEM models are se-
mantic networks that include events, places, times,
participants and all related concepts, such as their
types.
An overview of all the classes in the SEM ontol-
ogy and the relations connecting them is shown in
Figure 1. Nodes can be identified by URIs, which
universally identify them across all RDF models. If
for example one uses the URI used by DBpedia3
(Bizer et al, 2009b) for the 2004 catastrophe in In-
3http://dbpedia.org
donesia, then one really means the same event as ev-
erybody else who uses that URI. SEM does not put
any constraints on the RDF vocabulary, so vocabu-
laries can easily be reused. Places and place types
can for example be imported from GeoNames4 and
event types from the RDF version of WordNet.
SEM supports two types of abstraction: gener-
alization with hierarchical relations from other on-
tologies, such as the subclass relation from RDFS,
and aggregation of events into superevents with the
sem:subEventOf relation, as exemplified in Fig-
ure 2. Other types of abstractions can be represented
using additional schemas or ontologies in combina-
tion with SEM. For instance, temporal aggregation
can be done with constructs from the OWL Time
ontology (Hobbs and Pan, 2004).
Relations between events and other instances,
which could be other events, places, actors, times,
or external concepts, can be modeled using the
sem:eventProperty relation. This relation can
be refined to represent specific relations, such as
specific participation, causality or simultaneity rela-
tions. The provenance of information in the SEM
graph is captured through assigning contexts to
statements using the PROV Data Model (Moreau et
al., 2012). In this manner, all statements derived
from a specific newspaper article are stored in a
named graph that represents that origin. Conflicting
statements can be stored in different named graphs,
and can thus coexist. This gives us the possibility
4http://www.geonames.org/ontology/
14
sem:Event
sem:Place
sem:EventType
sem:Time
dbpedia:2004_Indian_Ocean_
earthquake_and_ tsunami
rdf:type
"December 2004 
Earthquake and 
Tsunami"@en
rdfs:label
rdf:type
rdf:type
"3.316"^^xsd:decimal
"2004-12-26"^^xsd:date
"95.854"^^xsd:decimal
wgs84:long
wgs84:lat
owltime:inXSD
DateTime
sem:hasPlace sem:hasTime
naacl:INSTANCE_186
rdf:type
sem:subEventOf
wn30:synset-
earthquake-noun-1
sem:eventType
rdf:type
naacl:INSTANCE_188
rdf:type
sem:subEventOf
naacl:INSTANCE_198
sem:hasTime
naacl:TIMEX3_81 "2004"str:anchorOfnwr:denotedBy
naacl:INSTANCE_MENTION_118
nwr:denotedBy "temblor"@en
str:anchorOf
nwr:denotedBy
"tsunami"@en
naacl:INSTANCE_MENTION_120
str:anchorOf
naacl:INSTANCE_189
sem:subEventOf
naacl:INSTANCE_MENTION_121
nwr:denotedBy
"swept"@en
str:anchorOf
sem:hasPlace
naacl:INSTANCE_67
naacl:INSTANCE_MENTION_19nwr:denotedBy
"Indian Ocean"@en
str:anchorOf
taf:LOCATION
taf:NSUBJ
geonames:1545739
skos:exactMatch
gaf:G1
gaf:G2
gaf:G3
gaf:G4
gaf:G5
dbpedia:Bloomberg
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
gaf:annotation_
2013_04_29
sem:accordingTo
taf:annotation_
2013_03_24
sem:accordingTo
sem:
derived
From
gaf:causes
Figure 2: Partial SEM representation of December 26th 2004 Earthquake
of delaying or ignoring the resolution of the conflict,
which enables use cases that require the analysis of
the conflict itself.
5 The GAF Annotation Framework
This section explains the basic idea behind GAF by
using texts on earthquakes in Indonesia. GAF pro-
vides a general model for event representation (in-
cluding textual and extra-textual mentions) as well
as exact representation of linguistic annotation or
output of NLP tools. Simply put, GAF is the combi-
nation of textual analyses and formal semantic rep-
resentations in RDF.
5.1 A SEM for earthquakes
We selected newspaper texts on the January 2009
West Papua earthquakes from Bejan and Harabagiu
(2010) to illustrate GAF. This choice was made be-
cause the topic ?earthquake? illustrates the advan-
tage of sharing URIs across domains. Gao and
Hunter (2011) propose a Linked Data model to cap-
ture major geological events such as earthquakes,
volcano activity and tsunamis. They combine infor-
mation from different seismological databases with
the intention to provide more complete information
to experts which may help to predict the occurrence
of such events. The information can also be used
in text interpretation. We can verify whether in-
terpretations by NLP tools correspond to the data
and relations defined by geologists or, through gen-
eralization, which interpretation is the most sensi-
ble given what we know about the events. General
information on events obtained from automatic text
processing, such as event templates (Chambers and
Jurafsky, 2011b) or typical event durations (Gusev
et al, 2010) can be integrated in SEM in a similar
manner. Provenance indications can be used to in-
dicate whether information is based on a model cre-
ated by an expert or an automatically derived model
obtained by a particular approach.
Figure 2 provides a fragment of a SEM represen-
tation for the earthquake and tsunami of December
26 2004.5 The model is partially inspired by Gao
and Hunter (2011)?s proposal. It combines infor-
mation extracted from texts with information from
DBpedia. The linking between the two can be es-
tablished either manually or automatically through
5The annotation and a larger representation including the
sentence it represents can be found on the GAF website http:
//wordpress.let.vu.nl/gaf.
15
an entity linking system.6 The combined event of
the earthquake and tsunami is represented by a DB-
pedia URI. The node labeled naacl:INSTANCE 186
represents the earthquake itself. The unambiguous
representation of the 2004 earthquake leads us to ad-
ditional information about it, for instance that the
earthquake is an event (sem:Event) and that the
sem:EventType is an earthquake, in this case
represented by a synset from WordNet, but also the
exact date it occurred and the exact location (cf
sem:hasTime, sem:hasPlace).
5.2 Integrating TAF representations into SEM
TAF annotations are converted to SEM relations.
For example, the TAF as participant relations
are translated to sem:hasActor relations, and
temporal relations are translated to sem:hasTime.
We use the relation nwr:denotedBy to link in-
stances to their mentions in the text which are repre-
sented by their unique identifiers in Figure 2.
Named graphs are used to model the source of
information as discussed in Section 4. The re-
lation sem:accordingTo indicates provenance
of information in the graph.7 For instance, the
mentions from the text in named graph gaf:G1
come from the source dbpedia:Bloomberg.
Relations between instances (e.g. between IN-
STANCE 189 and INSTANCE 188) are derived
from a specific grammatical relation in the text
(here, that tsunami is subject of swept) indicated
by the nwr:derivedFrom relation from gaf:G5
to gaf:G4. The grammatical relations included
in graph gaf:G5 come from a TAF annotation
(tag:annotation 2013 03 24).
6 GAF Earthquake Examples
This section takes a closer look at a few selected sen-
tences from the text that illustrate different aspects
of GAF. Figure 2 showed how a URI can provide a
formal context including important background in-
6Entity linking is the task of associating a mention to an
instance in a knowledge base. Several approaches and tools for
entity linking w.r.t. DBpedia and other data sets in the Linked
Open Data cloud are available and achieve good performances,
such as DBpedia Spotlight (Mendes et al, 2011); see (Rizzo
and Troncy, 2011) for a comparison of tools.
7The use of named graphs in this way to denote context is
compatible with the method used by Bozzato et al (2012).
formation on the event. Several texts in the corpus
refer to the tsunami of December 26, 2004, a 9.1
temblor in 2004 caused a tsunami and The catastro-
phe four years ago, among others. Compared to time
expressions such as 2004 and four years ago, time
indications extracted from external sources like DB-
pedia are not only more precise, but also permit us to
correctly establish the fact that these expressions re-
fer to the same event and thus indicate the same time.
The articles were published in January 2009: a direct
normalization of time indications would have placed
the catastrophe in 2005. The flexibility to combine
these seemingly conflicting time indications and de-
lay normalization can be used to correctly interpret
that four years ago early January 2009 refers to an
event taking place at the end of December 2004.
A fragment relating to one of the earthquakes of
January 2009: The quake struck off the coast [...] 75
kilometers (50 miles) west of [....] Manokwari pro-
vides a similar example. The expressions 75 kilo-
meters and 50 miles are clearly meant to express
the same distance, but not identical. The location
is most likely neither exactly 75 km nor 50 miles.
SEM can represent an underspecified location that
is included in the correct region. The exact location
of the earthquake can be found in external resources.
We can include both distances as expressions of the
location and decide whether they denote the general
location or include the normalized locations as alter-
natives to those from external resources.
Different sources may report different details.
Details may only be known later, or sources may
report from a different perspective. As provenance
information can be incorporated into the semantic
layer, we can represent different perspectives, and
choose which one to use when reasoning over the
information. For example, the following phrases
indicate the magnitude of the earthquakes that
struck Manokwari on January 4, 2009:
the 7.7 magnitude quake (source: Xinhuanet)
two quakes, measuring 7.6 and 7.4 (source: Bloomberg)
One 7.3-magnitude tremor (source: Jakartapost)
The first two magnitude indicators (7.7, 7.6)
are likely to pertain to the same earthquake, just as
the second two (7.4, 7.3) are. Trust indicators can
be found through the provenance trace of each men-
16
tion. Trust indicators can include the date on which
it was published, properties of the creation process,
the author, or publisher (Ceolin et al, 2010).
Furthermore, because the URIs are shared across
domains, we can link the information from the text
to information from seismological databases, which
may contain the exact measurement for the quake.
Similarly, external information obtained through
shared links can help us establish coreference. Con-
sider the sentences in Figure 3. There are several
ways to establish that the same event is meant in all
three sentences by using shared URIs and reasoning.
All sentences give us approximate time indications,
location of the affected area and casualties. Rea-
soning over these sentences combined with external
knowledge allows us to infer facts such as that un-
dersea [...] off [...] Aceh will be in the Indian Ocean,
or that the affected countries listed in the first sen-
tence are countries around the Indian Ocean, which
constitutes the Indian Ocean Community. The num-
ber of casualties in combination of the approximate
time indication or approximate location suffices to
identify the earthquake and tsunami in Indonesia on
December 26, 2004. The DBpedia representation
contains additional information such as the magni-
tude, exact location of the quake and a list of affected
countries, which can be used for additional verifica-
tion. This example illustrates how a formal context
using URIs that are shared across disciplines of in-
formation science can help to determine exact refer-
ents from limited or imprecise information.
7 Creating GAF
GAF entails integrating linguistic information
(e.g. TAF annotations) into RDF models (e.g. SEM).
The information in the model includes provenance
that points back to specific annotations. There are
two approaches to annotate text according to GAF.
The first approach is bottom-up. Mentions are
marked in the text as well as relations between them
(participants, time, causal relations, basically any-
thing except coreference). Consequently, these an-
notations are converted to SEM representations as
explained above. Coreference is established by link-
ing mentions to the same instance in SEM. The sec-
ond approach is top-down. Here, annotators mark
relations between instances (events, their partici-
pants, time relations, etc.) directly into SEM and
then link these to mentions in the text.
As mention in Section 2, inter-annotator agree-
ment on event annotation is generally low showing
that it is challenging. The task is somewhat simpli-
fied in GAF, since it removes the problem of identi-
fying an event trigger in the text. The GAF equiva-
lent of the event trigger in other linguistic annotation
approaches is an instance in SEM. However, other
challenges such as which mentions to select are in
principle not addressed by GAF, though differences
in inter-annotator agreement may be found depend-
ing on whether the bottom-up approach or the top-
down approach is selected. The formal context of
SEM may help frame annotations, especially for do-
mains such as earthquakes, where expert knowledge
was used to create basic event models. This may
help annotators while defining the correct relations
between events. On the other hand, the top-down
approach may lead to additional challenges, because
annotators are forced to link events to unambiguous
instances leading to hesitations as to when new in-
stances should be introduced.
Currently, we only use the bottom-up approach.
The main reason is the lack of an appropriate anno-
tation tool to directly annotate information in SEM.
We plan to perform comparative studies between the
two annotation approaches in future work.
8 Conclusion and Future Work
We presented GAF, an event annotation framework
in which textual mentions of events are grounded in
a semantic model that facilitates linking these events
to mentions in external (possibly non-textual) re-
sources and thereby reasoning. We illustrated how
GAF combines TAF and SEM through a use case
on earthquakes. We explained that we aim for a
representation that can combine textual and extra-
linguistic information, provides a clear distinction
between instances and instance mentions, is flexi-
ble enough to include conflicting information and
clearly marks the provenance of information.
GAF ticks all these boxes. All instances are rep-
resented by URIs in a semantic layer following stan-
dard RDF representations that are shared across re-
search disciplines. They are thus represented com-
pletely independent of the source and clearly distin-
17
There have been hundreds of earthquakes in Indonesia since a 9.1 temblor in 2004 caused a
tsunami that swept across the Indian Ocean, devastating coastal communities and leaving more
than 220,000 people dead in Indonesia, Sri Lanka, India, Thailand and other countries.
(Bloomberg, 2009-01-07 01:55 EST)
The catastrophe four years ago devastated Indian Ocean community and killed more than 230,000
people, over 170,000 of them in Aceh at northern tip of Sumatra Island of Indonesia.
(Xinhuanet, 2009-01-05 13:25:46 GMT)
In December 2004, a massive undersea quake off the western Indonesian province of Aceh
triggered a giant tsunami that left at least 230,000 people dead and missing in a dozen
countries facing the Indian Ocean. (Aljazeera, 2009-01-05 08:49 GMT)
Figure 3: Sample sentences mentioning the December 2004 Indonesian earthquake from sample texts
guished from mentions in text or mentions in other
sources. The Terence Annotation Format (TAF) pro-
vides a unified framework to annotate events, par-
ticipants and temporal expressions (and the corre-
sponding relations) by leaning on past, consolidated
annotation experiences such TimeML and ACE. We
will harmonize TAF, the Kyoto Annotation Format
(Bosma et al, 2009, KAF) and the NLP Interchange
Format (Hellmann et al, 2012, NIF) with respect
to the textual representation in the near future. The
NAF format includes the lessons learned from these
predecessors: layered standoff representations using
URI as identifiers and where possible standardized
data categories. The formal semantic model (SEM)
provides the flexibility to include conflicting infor-
mation as well as indications of the provenance of
this information. This allows us to use inferencing
and reasoning over the cumulated and aggregated
information, possibly exploiting the provenance of
the type of information source. This flexibility also
makes our representation compatible with all ap-
proaches dealing with event representation and de-
tections mentioned in Section 2. It can include au-
tomatically learned templates as well as specific re-
lations between events and time expressed in text.
Moreover, it may simultaneously contain output of
different NLP tools.
The proposed semantic layer may be simple, its
flexibility in importing external knowledge may in-
crease complexity in usage as it can model events in
every thinkable domain. To resolve this issue, it is
important to scope the domain by importing the ap-
propriate vocabularies, but no more. When keeping
this in mind, reasoning with SEM is shown to be rich
but still versatile (Van Hage et al, 2012).
While GAF provides us with the desired granu-
larity and flexibility for the event annotation tasks
we envision, a thorough evaluation still needs to be
carried out. This includes an evaluation of the anno-
tations created with GAF compared to other anno-
tation formats, as well as testing it within a greater
application. A comparative study of top-down and
bottom-up annotation will also be carried out. As al-
ready mentioned in Section 7, there is no appropriate
modeling tool for SEM yet. We are currently using
the CAT tool to create TAF annotations and convert
those to SEM, but will develop a tool to annotate the
semantic layer directly for this comparative study.
The most interesting effect of the GAF annota-
tions is that it provides us with relatively simple ac-
cess to a vast wealth of extra-linguistic information,
which we can utilize in a variety of NLP tasks; some
of the reasoning options that are made available by
the pairing up with Semantic Web technology may
for example aid us in identifying coreference rela-
tions between events. Investigating the implications
of this combination of NLP and Semantic Web tech-
nologies lies at the heart of our future work.
Acknowledgements
We thank Francesco Corcoglioniti for his helpful
comments and suggestions. The research lead-
ing to this paper was supported by the European
Union?s 7th Framework Programme via the News-
Reader Project (ICT-316404) and by the Biogra-
phyNed project, funded by the Netherlands eScience
Center (http://esciencecenter.nl/). Partners in Biog-
raphyNed are Huygens/ING Institute of the Dutch
Academy of Sciences and VU University Amster-
dam.
18
References
Collin F. Baker, Charles J. Fillmore, and Beau Cronin.
2003. The structure of the FrameNet database. Inter-
national Journal of Lexicography, 16(3):281?296.
Valentina Bartalesi Lenzi, Giovanni Moretti, and Rachele
Sprugnoli. 2012. CAT: the CELCT Annotation Tool.
In Proceedings of LREC 2012.
Cosmin Bejan and Sandra Harabagiu. 2010. Unsuper-
vised event coreference resolution with rich linguistic
features. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1412?1422.
Christian Bizer, Tom Heath, and Tim Berners-Lee.
2009a. Linked data - the story so far. International
Journal on Semantic Web and Information Systems,
5(3):1?22.
Christian Bizer, Jens Lehmann, Georgi Kobilarov, So?ren
Auer, Christian Becker, Richard Cyganiak, and Sebas-
tian Hellmann. 2009b. DBpedia - A crystallization
point for the Web of Data. Web Semantics: Science,
Services and Agents on the World Wide Web, 7(3):154
? 165.
Wauter Bosma, Piek Vossen, Aitor Soroa, German Rigau,
Maurizio Tesconi, Andrea Marchetti, Monica Mona-
chini, and Carlo Aliprandi. 2009. KAF: a generic se-
mantic annotation format. In Proceedings of the 5th
International Conference on Generative Approaches
to the Lexicon GL 2009, Pisa, Italy.
Loris Bozzato, Francesco Corcoglioniti, Martin Homola,
Mathew Joseph, and Luciano Serafini. 2012. Manag-
ing contextualized knowledge with the ckr (poster). In
Proceedings of the 9th Extended Semantic Web Con-
ference (ESWC 2012), May 27-31.
Jeremy J. Carroll and Graham Klyne. 2004. Re-
source description framework (RDF): Concepts and
abstract syntax. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-concepts-
20040210/.
Davide Ceolin, Paul Groth, and Willem Robert Van Hage.
2010. Calculating the trust of event descriptions using
provenance. Proceedings Of The SWPM.
Nathanael Chambers and Dan Jurafsky. 2011a.
Template-based information extraction without the
templates. In Proceedings of ACL-2011.
Nathanael Chambers and Dan Jurafsky. 2011b.
Template-based information extraction without the
templates. In Proceedings of ACL-2011, Portland, OR.
Nick Crofts, Martin Doerr, Tony Gill, Stephen Stead,
and Matthew Stiff. 2008. Definition of the CIDOC
Conceptual Reference Model. Technical report,
ICOM/CIDOC CRM Special Interest Group. version
4.2.5.
Lianli Gao and Jane Hunter. 2011. Publishing, link-
ing and annotating events via interactive timelines: an
earth sciences case study. In DeRiVE 2011 (Detec-
tion, Representation, and Exploitation of Events in the
Semantic Web) Workshop in conjunction with ISWC
2011, Bonn, Germany.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics (COLING?96), pages 466?471.
Ramanathan V. Guha and Dan Brickley. 2004.
RDF vocabulary description language 1.0: RDF
schema. W3C recommendation, W3C, Febru-
ary. http://www.w3.org/TR/2004/REC-rdf-schema-
20040210/.
Andrey Gusev, Nathanael Chambers, Pranav Khaitan,
Divye Khilnani, Steven Bethard, and Dan Jurafsky.
2010. Using query patterns to learn the duration of
events. In Proceedings of ISWC 2010.
Sebastian Hellmann, Jens Lehmann, and So?ren Auer.
2012. NIF: An ontology-based and linked-data-aware
NLP Interchange Format. Working Draft.
Jerry R Hobbs and Feng Pan. 2004. An ontology of time
for the semantic web. ACM Transactions on Asian
Language Information Processing (TALIP), 3(1):66?
85.
Linguistic Data Consortium. 2004a. Annotation
Guidelines for Event Detection and Characterization
(EDC). http://projects.ldc.upenn.edu/
ace/docs/EnglishEDCV2.0.pdf.
Linguistic Data Consortium. 2004b. The ACE 2004
Evaluation Plan. Technical report, LDC.
Linguistic Data Consortium. 2005. ACE (Automatic
Content Extraction) English annotation guidelines for
entities. Version 6.6, July.
Pablo N. Mendes, Max Jakob, Andre?s Garc??a-Silva, and
Christian Bizer. 2011. Dbpedia spotlight: shedding
light on the web of documents. In Proceedings of the
7th International Conference on Semantic Systems, I-
Semantics ?11, pages 1?8.
Marie-Francine Moens, Oleksandr Kolomiyets,
Emanuele Pianta, Sara Tonelli, and Steven Bethard.
2011. D3.1: State-of-the-art and design of novel
annotation languages and technologies: Updated
version. Technical report, TERENCE project ? ICT
FP7 Programme ? ICT-2010-25410.
Luc Moreau, Paolo Missier, Khalid Belhajjame, Reza
B?Far, James Cheney, Sam Coppens, Stephen Cress-
well, Yolanda Gil, Paul Groth, Graham Klyne, Timo-
thy Lebo, Jim McCusker, Simon Miles, James Myers,
Satya Sahoo, and Curt Tilmes. 2012. PROV-DM: The
PROV Data Model. Technical report.
Boris Motik, Bijan Parsia, and Peter F. Patel-
Schneider. 2009. OWL 2 Web Ontology
19
Language structural specification and functional-
style syntax. W3C recommendation, W3C,
October. http://www.w3.org/TR/2009/
REC-owl2-syntax-20091027/.
Joel Nothman, Matthew Honnibal, Ben Hachey, and
James R. Curran. 2012. Event linking: Ground-
ing event reference in a news archive. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers),
pages 228?232, Jeju Island, Korea, July. Association
for Computational Linguistics.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106, 2013/03/12.
James Pustejovsky, Jessica Littman, and Roser Saur?`.
2006a. Argument Structure in TimeML. In Dagstuhl
Seminar Proceedings. Internationales Begegnungs-
und Forschungszentrum.
James Pustejovsky, Jessica Littman, Roser Saur??, and
Marc Verhagen. 2006b. Timebank 1.2 documentation.
Technical report, Brandeis University, April.
James Pustejovsky, Kiyong Lee, Harry Bunt, and Lau-
rent Romary. 2010. ISO-TimeML: An international
standard for semantic annotation. In Proceedings o
the Fifth International Workshop on Interoperable Se-
mantic Annotation.
Giuseppe Rizzo and Raphae?l Troncy. 2011. NERD:
A framework for evaluating named entity recognition
tools in the Web of data. In Workshop on Web Scale
Knowledge Extraction, colocated with ISWC 2011.
Ansgar Scherp, Thomas Franz, Carsten Saathoff, and
Steffen Staab. 2009. F?a model of events based on
the foundational ontology dolce+ dns ultralight. In
Proceedings of the fifth international conference on
Knowledge capture, pages 137?144. ACM.
Andrea Setzer and Robert J. Gaizauskas. 2000. Annotat-
ing events and temporal information in newswire texts.
In LREC. European Language Resources Association.
Ryan Shaw, Raphae?l Troncy, and Lynda Hardman. 2009.
LODE: Linking Open Descriptions of Events. In 4th
Annual Asian Semantic Web Conference (ASWC?09),
Shanghai, China.
Willem Robert Van Hage, Ve?ronique Malaise?, Roxane
Segers, Laura Hollink, and Guus Schreiber. 2011. De-
sign and use of the simple event model (SEM). Jour-
nal of Web Semantics.
Willem Robert Van Hage, Marieke Van Erp, and
Ve?ronique Malaise?. 2012. Linked open piracy: A
story about e-science, linked data, and statistics. Jour-
nal on Data Semantics, 1(3):187?201.
20
