Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 238?247, Prague, June 2007. c?2007 Association for Computational Linguistics
A Sequence Alignment Model Based on the Averaged Perceptron
Dayne Freitag
Fair Isaac Corporation
3661 Valley Centre Drive
San Diego, CA 92130, USA
DayneFreitag@fairisaac.com
Shahram Khadivi
Lehrstuhl fu?r Informatik 6
Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
khadivi@cs.rwth-aachen.de
Abstract
We describe a discriminatively trained se-
quence alignment model based on the av-
eraged perceptron. In common with other
approaches to sequence modeling using per-
ceptrons, and in contrast with comparable
generative models, this model permits and
transparently exploits arbitrary features of
input strings. The simplicity of perceptron
training lends more versatility than compa-
rable approaches, allowing the model to be
applied to a variety of problem types for
which a learned edit model might be useful.
We enumerate some of these problem types,
describe a training procedure for each, and
evaluate the model?s performance on sev-
eral problems. We show that the proposed
model performs at least as well as an ap-
proach based on statistical machine transla-
tion on two problems of name translitera-
tion, and provide evidence that the combina-
tion of the two approaches promises further
improvement.
1 Introduction
Sequence alignment is a problem that crops up in
many forms, both in computational linguistics (CL)
and in other endeavors. The ability to find an op-
timal alignment between two sequences has found
application in a number of areas of CL, includ-
ing phonetic modeling (Ristad and Yianilos, 1998),
name transcription (Huang et al, 2004), and dupli-
cate detection or information integration (Bilenko
and Mooney, 2003; McCallum et al, 2005). Se-
quence alignment is a member of a broader class of
problems which we might call sequence transduc-
tion, to which one of the core CL challenges, ma-
chine translation, belongs.
Under the assumption that one string (the target)
is produced through a series of local edits to another
string (the source), and given an edit cost matrix,
the optimal sequence of edits can be efficiently com-
puted through dynamic programming (Needleman
and Wunsch, 1970). While the cost matrix tradition-
ally has been set by hand, several recent papers have
proposed determining edit costs empirically. These
proposals arise from a variety of learning paradigms,
including generative models (Ristad and Yianilos,
1998; Bilenko and Mooney, 2003), conditional ran-
dom fields (McCallum et al, 2005), maximum-
margin methods (Joachims, 2003), and gradient
boosting (Parker et al, 2006). While approaches
based on generative models support only limited fea-
ture engineering, discriminative approaches share
the advantage of allowing arbitrary features of the
input sequences.
We describe a new sequence alignment model
based on the averaged perceptron (Collins, 2002),
which shares with the above approaches the ability
to exploit arbitrary features of the input sequences,
but is distinguished from them by its relative sim-
plicity and the incremental character of its training
procedure. The fact that it is an online algorithm
makes it straightforward to adapt to a range of prob-
lems. To show this, we evaluate the approach on
several different tasks, some of them merely illus-
trative, but some with clear practical significance,
238
particularly the problem of named entity transcrip-
tion.
2 The Algorithm
2.1 The Formalism
Suppose we are given two sequences, sm1 ? ??s
and tn1 ? ??t . We desire a real-valued function
A(s, t) which assigns high scores to pairs s, t with
high affinity, where affinity is an application-specific
notion (e.g., t is a likely phoneme sequence repre-
sented by the letter sequence s). If we stipulate that
this score is the sum of the individual scores of a
series of edits, we can find the highest-scoring such
series through a generalization of the standard edit
distance:
A(si1, tj1) =
max
?
?
?
?
?
a?,tj (s, i, t, j) + A(si1, t
j?1
1 )
asi,?(s, i, t, j) + A(si?11 , t
j
1)
asi,tj (s, i, t, j) + A(si?11 , t
j?1
1 )
(1)
with A(?, ?) = 0. The function asi,tj (s, i, t, j) rep-
resents the score of substituting tj for si; a?,tj and
asi,? represent insertion and deletion, respectively. If
we assume constant-time computation of primitive
edit costs, this recursive definition of A allows us to
find the highest scoring series of edits for a given
sequence pair in time proportional to the product of
their lengths. Note that a is indexed by the charac-
ters involved in an edit (i.e., inserting ?e? generally
has a different cost than inserting ?s?). Note further
that the score associated with a particular operation
may depend on any features computable from the
respective positions in the two sequences.
In the experiments reported in this paper, we as-
sume that each local function a is defined in terms
of p + q features, {f1, ? ? ? , fp, fp+1, ? ? ? , fp+q},
and that these features have the functional form
?? ?N 7? R. In other words, each feature takes
a sequence and an index and returns a real value.
The first p features are defined over sequences from
the source alphabet, while the remaining q are de-
fined over the target alhabet.1 In this paper we use
character n-gram indicator features.
1Of course, features that depend jointly on both sequences
may also be of interest.
1: Given a set S of source sequences
2: V ? [], an empty list
3: ?? 0, a weight vector
4: for some number of iterations do
5: for s in S do
6: Pick t, t?, t having higher affinity with s
7: ?e, v? ? A?(s, t)
8: ?e?, v?? ? A?(s, t?)
9: if v? ? v then
10: ?? ? + ?(s, t, e)? ?(s, t?, e?)
11: end if
12: Append ? to V
13: end for
14: end for
15: Return the mean ? from V
Table 1: The training algorithm. A? is the affinity
function under model parameters ?, returning edit
sequence e and score v.
The score of a particular edit is a linear combina-
tion of the corresponding feature values:
a(s, i, t, j) =
p
?
k=1
?k ? fk(s, i) +
p+q
?
k=p+1
?k ? fk(t, j)
(2)
The weights ?k are what we seek to optimize in or-
der to tune the model for our particular application.
2.2 A Perceptron-Based Edit Model
In this section we present a general-purpose exten-
sion of perceptron training for sequence labeling,
due to Collins (2002), to the problem of sequence
alignment. Take ? to be a model parameterization,
and let A?(s, t) return an optimal edit sequence e,
with its score v, given input sequences s and t un-
der ?. Elements of sequence e are character pairs
?cs, ct?, with cs ? ?s ? {?} and ct ? ?t ? {?},
where ? represents the empty string. Let ?(s, t, e)
be a feature vector, having the same dimensional-
ity as ?, for a source, target, and corresponding edit
sequence. This feature vector is the sum of feature
vectors at each point in e as it is played out along
input sequences s and t.
Table 1 shows the basic algorithm. Starting with
a zero parameter vector, we iterate through the col-
lection of source sequences. For each sequence, we
pick two target sequences having unequal affinity
239
with the source sequence (Line 6). If the scores re-
turned by our current model (Lines 7 and 8) agree
with our ordering, we do nothing. Otherwise, we
update the model using the perceptron training rule
(Line 10). Ultimately, we return ? averaged over all
datapoint presentations.
2.3 Training Modes
The algorithm presented in Table 1 does not specify
how the two target sequences t and t? are to be cho-
sen in Line 6. The answer to this question depends
on the application. There are fundamentally two set-
tings, depending on whether or not target strings are
drawn from the same set as source strings; we will
call the setting in which source and target strings in-
habit the same set the affinity setting, and refer to the
the case where they form different sets as the trans-
duction setting. Here, we sketch four problem sce-
narios, two from each setting, and specify a target
selection procedure appropriate for each.
Affinity, ranking. The task poses a latent affinity
between strings, but we can measure it only indi-
rectly. In particular, we can order some of the target
sequences according to their affinity with a source
sequence s. In this case, we train as follows: Order
a sample of the target sequences according to this
partial order. Let t and t? be two sequences from
this order, such that t is ordered higher than t?.
Affinity, classification. The sequences in ?? can
be grouped into classes, and we wish the model to
assign high affinity to co-members of a class and
low affinity to members of different classes. Train
as follows: For each s, sample t from among its co-
members and t? from among the members of other
classes.
Transduction, ranking. The data is presented as
source-target pairs, where each t is a transduction
of the corresponding s. We wish to learn a model
which, given a novel s, will enable us to rank can-
didate transductions. Train as follows: Given s, let
t be the target sequence provided to us. Sample t?
from among the other target sequences.
Transduction, generation. We are again given
source-target pairs. We wish to learn to generate a
probable target string, given a novel source string.
Train as follows: Generate a t? that is approximately
optimal according to the current model. Note that
since edit decisions are based in part on (arbitrary)
edit
?
ing ? STR
?
INGS
fs,it ft,TR
fs,t ft,R
fs, in f?
fs, i
Table 2: Features with non-zero value for an exam-
ple string pair and a model of order 2.
features of the target sequence, and since generation
involves construction of the target sequence, it is not
uncommon for a greedy generator to make edit de-
cisions which are locally optimal, but which result
several edits later in a partially constructed sequence
in which no good edits are available. Thus, the prob-
lem of generation does not correspond to a simple
recurrence relation like Equation 1. Consequently,
we experimented with several heuristic approaches
to generation and found that a beam search works
well.
3 Evaluation
To establish the effectiveness of the model, we
trained it on a range of problems, including instances
of each of the four settings enumerated above. Prob-
lems ranged from the merely illustrative to a non-
trivial application of computational linguistics.
3.1 Feature Construction
Without exception, the features we provide to the
algorithm are the same in all experiments. Given
a user-specified order k, we define a Boolean fea-
ture for every distinct character gram observed in the
data of length k or smaller. Recall that there are two
disjoint sets of features, those defined over strings
drawn from the source and target alhabets, respec-
tively. Given a source string and index, those fea-
tures have value 1 whose corresponding grams (of
size k or smaller) are observed preceding or follow-
ing the index (preceding features are distinct from
following ones); given a target string and index, we
observe only preceding grams. Although it is pos-
sible to observe following grams in the target string
in some settings, it is not possible in general (i.e.,
not when generating strings). We therefore adhere
to this restriction for convenience and uniformity.
An example will make this clear. In Table 2 we
240
are midway through the conversion of the source
string ?editing? into the target string ?STRINGS?.
Below the two strings are those gram features which
have non-zero value at the indicated cursors. The
underbar character encodes on which side of the cur-
sor a gram is observed. Note that an empty-gram
feature, which always tests true, is also included, al-
lowing us to experiment with 0-order models.
3.2 Illustrative Problems
To test the ability of the model to recover known
edit affinities, we experimented with a simple artifi-
cial problem. Using a large list of English words,
we define an edit affinity that is sensitive only to
consonants. Specifically, the affinity between two
words is the maximum number of consonant self-
substitutions, with any substitutions involving the
first five consonants counting for five normal substi-
tutions. Thus, substituting ?b? for ?b? contributes 5 to
the score, substituting ?z? for ?z? contributes 1, while
operations other than self-subsitutions, and any op-
erations involving vowels, contribute 0.
One epoch of training is conducted as follows.
For each word s in the training set, we choose 10
other words from the set at random and sort these
words according to both the true and estimated affin-
ity. Let t be the string with highest true affinity; let
t? (the decoy) be the string with highest estimated
affinity. We performed 3-fold cross-validation on a
collection of 33,432 words, in each fold training the
model for 5 epochs.2
Our performance metric is ranking accuracy, the
fraction of target string pairs to which the estimated
ranking assigns the same order as the true one. Dur-
ing testing, for each source string, we sample at ran-
dom 1000 other strings from the hold-out data, and
count the fraction of all pairs ordered correctly ac-
cording to this criterion.
A 0-order model successfully learns to rank
strings according to this affinity with 99.3% ac-
curacy, while ranking according to the unmodified
Levenshtein distance yields 76.4%. Table 3 shows
the 6 averaged weights with the highest magnitude
2Here and in other experiments involving the edit model, the
number of epochs was set arbitrarily, and not based on perfor-
mance on a development set. Beyond the number of epochs re-
quired for convergence, we have not observed much sensitivity
in test accuracy to the number of epochs.
?d, d?: f? 61.1
?c, c?: f? 60.6
?g, g?: f? 60.3
?b, b?: f? 59.1
?f, f?: f? 57.0
?t, t?: f? 18.6
Table 3: Largest weights in a consonant-preserving
edit affinity in which the first five consonants are
given 5 times as much weight as others.
from a model trained on one of the folds. In pre-
senting weights, we follow the formatting conven-
tion edit:feature. Since the model in question is of
order 0, all features in Table 3 are the ?empty fea-
ture.? Note how idempotent substitutions involving
the 5 highly weighted consonants are weighted sig-
nificantly higher than the remaining operations.
3.3 Rhyming
While the above problem illustrates the ability of
the proposed algorithm to learn latent alignment
affinities, it is expressible as a order-0 model. A
somewhat more interesting problem is that of mod-
eling groups of rhyming words. This problem is
an instance of what we called the ?classification?
scenario in Section 2.3. Because English letters
have long since lost their direct correspondence to
phonemes, the problem of distinguishing rhyming
English words is difficult for a knowledge-lean edit
model. What?s more, the importance of a letter is
dependent on context; letters near the end of a word
are more likely to be significant.
We derived groups of rhyming words from the
CMU pronouncing dictionary (CMU, 1995), dis-
carding any singleton groups. This yielded 21,396
words partitioned into 3,799 groups, ranging in size
from 464 words (nation, location, etc.) down to 2.
We then divided the words in this data set at random
into three groups for cross-validation.
Training was conducted as follows. For each
word in the training set, we selected at random up to
5 rhyming words and 5 non-rhyming words. These
words were ranked according to affinity with the
source word under the current model. Let t be
the lowest scoring rhyming word, and let t? be the
highest-scoring non-rhyming word.
241
Model Precision
Levenshtein 0.126
Longest common suffix 0.130
PTEM, Order 0 0.505
PTEM, Order 3 0.790
Table 4: Micro-averaged break-even precision on
the task of grouping rhyming English words.
For each word in the hold-out set, we scored and
ranked all rhyming words in the same set, as well
as enough non-rhyming words to total 1000. We
then recorded the precision at the point in this rank-
ing where recall and precision are most nearly equal.
Our summary statistic is the micro-averaged break-
even precision.
Table 4 presents the performance of the proposed
model and compares it with two simple baselines.
Not surprisingly, performance increases with in-
creasing order. The simple heuristic approaches fare
quite poorly by comparison, reflecting the subtlety
of the problem.
3.4 Transcription
Our work was motivated by the problem of named
entity transcription. Out-of-vocabulary (OOV)
terms are a persistent problem in statistical machine
translation. Often, such terms are the names of en-
tities, which typically have low corpus frequencies.
In translation, the appropriate handling of names is
often to transcribe them, to render them idiomati-
cally in the target language in a way that preserves,
as much as possible, their phonetic structure. Even
when an OOV term is not a name, transcribing it
preserves information that would otherwise be dis-
carded, leaving open the possibility that downstream
applications will be able to make use of it.
The state of the art in name transcription involves
some form of generative model, sometimes in com-
bination with additional heuristics. The generative
component may involve explicitly modeling pho-
netics. For example, Knight and Graehl (1998)
employ cascaded probabilistic finite-state transduc-
ers, one of the stages modeling the orthographic-
to-phonetic mapping. Subsequently, Al-Onaizan
and Knight (2002) find they can boost perfor-
mance by combining a phonetically-informed model
Task Train Dev Eval ELen FLen
A-E 8084 1000 1000 6.5 4.9
M-E 2000 430 1557 16.3 23.0
Table 5: Characteristics of the two transcription data
sets, Arabic-English (A-E) and Mandarin-English
(M-E), including number of training, development,
and evaluation pairs (Train, Dev, and Eval), and
mean length in characters of English and foreign
strings (ELen and FLen).
with one trained only on orthographic correspon-
dences. Huang et al (2004), construct a probabilis-
tic Chinese-English edit model as part of a larger
alignment solution, setting edit weights in a heuris-
tic bootstrapped procedure.
In rendering unfamiliar written Arabic words or
phrases in English, it is generally impossible to
achieve perfect performance, because many sounds,
such as short vowels, diphthong markers, and dou-
bled consonants, are conventionally not written in
Arabic. We calculate from our experimental datasets
that approximately 25% of the characters in the En-
glish output must be inferred. Thus, a character error
rate of 25% can be achieved through simple translit-
eration.
3.4.1 Transcribing names
We experimented with a list of 10,084 personal
names distributed by the Linguistic Data Consor-
tium (LDC). Each entry in the database includes
an arabic name in transliterated ASCII (SATTS
method) and its English rendering. The Arabic
names appear as they would in conventional writ-
ten Arabic, i.e., lacking short vowels and other di-
acritics. We randomly segregated 1000 entries for
evaluation and used the rest for training. The A-E
row in Table 5 summarizes some of this data set?s
characteristics.
We trained the edit model as follows. For each
training pair the indicated English rendering con-
stitutes our true target (t), and we use the current
model to generate an alternate string (t?), updating
the model in the event t? yields a higher score than t.
This was repeated for 10 epochs. We experimented
with a model of order 3.
Under this methodology, we observed a 1-best ac-
242
?p, h?: ft,a 38.5
?p, t?: ft,a 30.8
?p, h?: ft,ya 11.8
?p, t?: fs, p<e> -8.6
?p, h?: ft,rya -12.1
?p, t?: ft,uba -14.4
Table 6: Some of the weights governing the han-
dling of the tah marbouta (   ) in an order-3 Arabic-
English location name transcription model. Buck-
walter encoding of Arabic characters is used here for
purposes of display. The symbol ?<e>? represents
end of string.
curacy of 0.552. It is difficult to characterize the
strength of this result relative to those reported in the
literature. Al-Onaizan and Knight (2002) report a 1-
best accuracy of 0.199 on a corpus of Arabic person
names (but an accuracy of 0.634 on English names),
using a ?spelling-based? model, i.e., a model which
has no access to phonetic information. However,
the details of their experiment and model differ from
ours in a number of respects.
It is interesting to see how a learned edit model
handles ambiguous letters. Table 6 shows the
weights of some of the features governing the han-
dling of the character
 
 (tah marbouta) from exper-
iments with Arabic place names. This character,
which represents the ?t? sound, typically appears at
the end of words. It is generally silent, but is spoken
in certain grammatical constructions. In its silent
form, it is typically transcribed ?ah? (or ?a?); in its
spoken form, it is transcribed ?at?. The weights in
the table reflect this ambiguity and illustrate some
of the criteria by which the model chooses the ap-
propriate transcription. For example, the negative
weight on the feature fs, p<e> inhibits the produc-
tion of ?t? at the end of a phrase, where ?h? is almost
always more appropriate. Similarly, ?h? is more
common following ?ya? in the target string (often as
part of the larger suffix ?iyah?). However, the pre-
ceding context ?rya? is usually observed in the word
?qaryat?, meaning ?village? as in ?the village of ...?
In this grammatical usage, the tah marbouta is spo-
ken and therefore rendered with a ?t?. Consequently,
the corresponding weight in the ?h? interpretation is
inhibitory.
The Al-Onaizan and Knight spelling model can
be regarded as a statistical machine translation
(SMT) system which translates source language
characters to target language characters in the ab-
sence of phonetic information. For comparison
with state of the art, we used the RWTH phrase-
based SMT system (Zens et al, 2005) to build an
Arabic-to-English transliteration system. This sys-
tem frames the transcription problem as follows. We
are given a sequence of source language charac-
ters sm1 representing a name, which is to be trans-
lated into a sequence of target language characters
tn1 . Among all possible target language character se-
quences, we will choose the character sequence with
the highest probability:
t?n?1 = argmax
n,tn1
{Pr(tn1 |sm1 )} (3)
The posterior probability Pr(tn1 |sm1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002), including a character-
based phrase translation model, a character-based
lexicon model, a 4-gram character sequence model,
a character penalty and a phrase penalty. The first
two models are used for both directions: Arabic
to English and English to Arabic. We do not use
any reordering model because the target character
sequence is always monotone with respect to the
source character sequence. More details about the
baseline system can be found in (Zens et al, 2005).
We remark in passing that while the perceptron-
based edit model is a general algorithm for learn-
ing sequence alignments using simple features, the
above SMT approach combines several models,
some of which have been the subject of research in
the fields of speech recognition and machine trans-
lation for several years. Furthermore, we made an
effort to optimize the performance of the SMT ap-
proach on the tasks presented here.
Table 7 compares this system with the edit model.
The difference between the 1-best accuracies of the
two systems is significant at the 95% level, using
the bootstrap for testing. However, we can improve
on both systems by combining them. We segregated
1000 training documents to form a development set,
and used it to learn linear combination coefficients
over our two systems, resulting in a combined sys-
tem that scored 0.588 on the evaluation set?a sta-
243
Model 1best 5best
SMT 0.528 0.824
PTEM, Order 3 0.552 0.803
Linear combination 0.588 0.850
Table 7: 1-best and 5-best transcription accuracies.
The successive improvements in 1-best accuracy are
significant at the 95% confidence level.
tistically significant improvement over both systems
at the 95% confidence level.
3.4.2 Ranking transcriptions
In some applications, instead of transcribing a
name in one language into another, it is enough just
to rank candidate transcriptions. For example, we
may be in possession of comparable corpora in two
languages and the means to identify named entities
in each. If we can rank the likely transcriptions of
a name, we may be able to align a large portion of
the transliterated named entities, potentially extend-
ing the coverage of our machine translation system,
which will typically have been developed using a
smaller parallel corpus. This idea is at the heart of
several recent attempts to improve the handling of
named entities in machine translation (Huang et al,
2004; Lee and Chang, 2003). A core component
of all such approaches is a generative model simi-
lar in structure to the ?spelling? model proposed by
Al-Onaizan and Knight.
When ranking is the objective, we can adopt a
training procedure that is much less expensive than
the one used for generation. Let t be the correct tran-
scription for a source string (s). Sample some num-
ber of strings at random (200 in the following exper-
iments) from among the transcriptions in the training
set of strings other than s. Let t? be the string having
highest affinity with s, updating the model, as usual,
if t? scores higher than t.
In addition to the Arabic-English corpus, we also
experiment with a corpus distributed by the LDC
of full English names paired with their Mandarin
spelling. The M-E row of Table 5 summarizes char-
acteristics of this data set. Because we are inter-
ested in an approximate comparison with similar ex-
periments in the literature, we selected at random
2430 for training and 1557 for evaluation, which
are the data sizes used by Lee and Chang (2003)
for their experiments. In these experiments, the
Chinese names are represented as space-separated
pinyin without tonal markers.
Note that this problem is probably harder than the
Arabic one, for several reasons. For one thing, the
letters in a Mandarin transcription of a foreign name
represent syllables, leading to a somewhat lossier
rendering of foreign names in Mandarin than in Ara-
bic. On a more practical level, this data set is noisier,
occasionally containing character sequences in one
string for which corresponding characters are lack-
ing from its paired string. On the other hand, the
Mandarin problem contains full names, rather than
name components, which provides more context for
ranking.
We trained the edit model on both data sets us-
ing both the sampling procedure outlined above and
the self-generation training regime, in each case for
20 epochs, producing models of orders from 1 to 3.
However, we found that the efficiency of the phrase-
based SMT system described in the previous section
would be limited for this task, mainly due to two
reasons: the character-based phrase models due to
possible unseen phrases in an evaluation corpus, and
the character sequence model as all candidate tran-
scriptions confidently belong to the target language.
Therefore, to make the phrase-based SMT system
robust against data sparseness for the ranking task,
we also make use of the IBM Model 4 (Brown et
al., 1993) in both directions. The experiments show
that IBM Model 4 is a reliable model for the ranking
task. For each evaluation pair, we then ranked all
available evaluation transcriptions, recording where
in this list the true transcription fell.
Table 8 compares the various models, showing
the fraction of cases for which the true transcription
was ranked highest, and its mean reciprocal rank
(MRR). Both the phrase-based SMT model and the
edit model perform well on this task. While the best
configuration of PTEM out-performs the best SMT
model, the differences are not significant at the 95%
confidence level. However, compare these perfor-
mance scores to those returned by the system of Lee
and Chang (2003), who reported a peak MRR of
0.82 in similar experiments involving data different
from ours.
The PTEM rows in the table are separated into
244
Model C-E Task A-E Task
ACC MRR ACC MRR
SMT 0.795 0.797 0.982 0.985
SMT w/o LM 0.797 0.798 0.983 0.985
IBM 4 0.961 0.971 0.978 0.987
SMT + IBM 4 0.971 0.977 0.991 0.994
PTEMG, Ord. 1 0.843 0.877 0.959 0.975
PTEMG, Ord. 2 0.970 0.978 0.968 0.980
PTEMG, Ord. 3 0.975 0.982 0.971 0.983
PTEMR, Ord. 1 0.961 0.973 0.992 0.995
PTEMR, Ord. 2 0.960 0.972 0.989 0.993
PTEMR, Ord. 3 0.960 0.972 0.989 0.994
Table 8: Performance on two transcription ranking
tasks, showing fraction of cases in which the correct
transcription was ranked highest, accuracy (ACC)
and mean reciprocal rank of the correct transcription
(MRR).
those in which the model was trained using the
same procedure as for generation (PTEMG), and
those in which the quicker ranking-specific train-
ing regime was used (PTEMR). The comparison is
interesting, inasmuch it does not support the con-
clusion that one regime is uniformly superior to the
other. While generation regime yields the best per-
formance on Arabic (using a high-order model), the
ranking regime scores best on Mandarin (with a low-
order model). When training a model to generate, it
seems clear that more context in the form of larger
n-grams is beneficial. This is particularly true for
Mandarin, where an order-1 model probably does
not have the capacity to generate plausible decoys.
4 Discussion
This paper is not the first to show that perceptron
training can be used in the solution of problems
involving transduction. Both Liang, et al(2006),
and Tillmann and Zhang (2006) report on effective
machine translation (MT) models involving large
numbers of features with discriminatively trained
weights. The training of these models is an in-
stance of the ?Generation? scenario outlined in Sec-
tion 2.3. However, because machine translation is
a more challenging problem than name transcrip-
tion (larger vocabularies, higher levels of ambigu-
ity, non-monotonic transduction, etc.), our general-
purpose approach to generation training may be in-
tractable for MT. Instead, much of the focus of these
papers are the heuristics that are required in order to
train such a model in this fashion, including feature
selection using external resources (phrase tables),
staged training, and generating to BLEU-maximal
sequences, rather than the reference target.
Klementiev and Roth (2006) explore the use of a
perceptron-based ranking model for the purpose of
finding name transliterations across comparable cor-
pora. They do not calculate an explicit alignment be-
tween strings. Instead, they decompose a string pair
into a collection of features derived from charac-
ter n-grams heuristically paired based on their loca-
tions in the respective strings. Thus, Klementiev and
Roth, in common with the two MT approaches de-
scribed above, carefully control the features used by
the perceptron. In contrast to these approaches, our
algorithm discovers latent alignments, essentially
selecting those features necessary for good perfor-
mance on the task at hand.
As noted in the introduction, several previous pa-
pers have proposed general, discriminatively trained
sequence alignment models, as alternatives to the
generative model proposed by Ristad and Yianilos.
McCallum, et al (2005), propose a conditional ran-
dom field for sequence alignment, designed for the
important problem of duplicate detection and infor-
mation integration. Comprising two sub-models,
one for matching strings and one for non-matching,
the model is trained on sequence pairs explicitly
labeled ?match? or ?non-match,? and some care
is apparently needed in selecting appropriate non-
matching strings. It is therefore unclear how this
model would be extended to problems involving
ranking or generation.
Joachims (2003) proposes SVM-align, a sequence
alignment model similar in structure to that de-
scribed here, but which sets weights through di-
rect numerical optimization. Training involves ex-
posing the model to sequence pairs, along with the
correct alignment and some number of ?decoy? se-
quences. The reliance on an explicit alignment and
hand-chosen decoys yields a somewhat less flexi-
ble solution than that presented here. It is not clear
whether these features of the training regime are in-
dispensable, or whether they might be generalized to
245
increase the approach?s scope. Note that where di-
rectly maximizing the margin is feasible, it has been
shown empirically to be superior to perceptron train-
ing (Altun et al, 2003).
Parker et al (2006), propose to align sequences by
gradient tree boosting. This approach has the attrac-
tive characteristic that it supports a factored repre-
sentation of edits (a characteristic it shares with Mc-
Callum et al). Although this paper does not evaluate
the method on any problems from computational lin-
guistics (the central problem is musical information
retrieval), gradient tree boosting has been shown to
be an effective technique for other sorts of sequence
modeling drawn from computational linguistics (Di-
etterich et al, 2004).
5 Conclusion
Motivated by the problem of Arabic-English tran-
scription of names, we adapted recent work in per-
ceptron learning for sequence labeling to the prob-
lem of sequence alignment. The resulting algorithm
shows clear promise not only for transcription, but
also for ranking of transcriptions and structural clas-
sification. We believe this versatility will lead to
other successful applications of the idea, both within
computational linguistics and in other fields involv-
ing sequential learning.
Acknowledgment of support
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA/IPTO) under Contract HR0011-06-C-
0023. Any opinions, findings and conclusions
or recommendations expressed in this material are
those of the authors and do not necessarily reflect the
views of the Defense Advanced Research Projects
Agency (DARPA).
References
Y. Al-Onaizan and K. Knight. 2002. Machine translit-
eration of names in Arabic text. In Proceedings of
the ACL-02 workshop on computational approaches to
semitic languages.
Y. Altun, I. Tsochantaridis, and T. Hofmann. 2003. Hid-
den Markov support vector machines. In Proceedings
of ICML-2003.
M. Bilenko and R. Mooney. 2003. Adaptive duplicate
detection using learnable string similarity measures.
In Proceedings of KDD-2003.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2), June.
CMU. 1995. The CMU pronouncing dictionary.
http://www.speech.cs.cmu.edu/cgi-bin/cmudict. Ver-
sion 0.6.
M. Collins. 2002. Discriminative training methods for
hidden Markov models: theory and experiments with
perceptron algorithms. In Proceedings of EMNLP-
2002.
T. Dietterich, A. Ashenfelter, and Y. Bulatov. 2004.
Training conditional random fields via gradient tree
boosting. In Proceedings of ICML-2004.
F. Huang, S. Vogel, and A. Waibel. 2004. Improving
named entity translation combining phonetic and se-
mantic similarities. In Proceedings of HLT-NAACL
2004.
T. Joachims. 2003. Learning to align sequences: a
maximum-margin approach. Technical report, Cornell
University.
A. Klementiev and D. Roth. 2006. Weakly supervised
named entity transliteration and discovery from mul-
tilingual comparable corpora. In Proceedings of Col-
ing/ACL 2006.
K. Knight and J. Graehl. 1998. Machine transliteration.
Computational Linguistics, 24(4).
C.-J. Lee and J.S. Chang. 2003. Acquisition of English-
Chinese transliterated word pairs from parallel-aligned
texts using a statistical machine transliteration model.
In Proceedings of the HLT-NAACL 2003 Workshop on
Building and Using Parallel Texts.
P. Liang, A. Bouchard-Co?te?, D. Klein, and B. Taskar.
2006. An end-to-end discriminative approach to
machine translation. In Proceedings of COLING
2006/ACL 2006.
A. McCallum, K. Bellare, and F. Pereira. 2005. A condi-
tional random field for discriminatively-trained finite-
state string edit distance. In Proceedings of UAI-2005.
S.B. Needleman and C.D. Wunsch. 1970. A general
method applicable to the search for similarities in the
amino acid sequence of two proteins. Journal of
Molecular Biology, 48.
F.J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In ACL02, pages 295?302, Philadelphia,
PA, July.
246
C. Parker, A. Fern, and P Tadepalli. 2006. Gradient
boosting for sequence alignment. In Proceedings of
AAAI-2006.
E.S. Ristad and P.N. Yianilos. 1998. Learning string-edit
distance. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 20.
C. Tillmann and T. Zhang. 2006. A discriminative global
training algorithm for statistical MT. In Proceedings
of Coling/ACL 2006.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
247
Statistical Approaches to
Computer-Assisted Translation
Sergio Barrachina?
Universitat Jaume I
Oliver Bender??
RWTH Aachen
Francisco Casacuberta?
Universitat Polite`cnica de Vale`ncia
Jorge Civera?
Universitat Polite`cnica de Vale`ncia
Elsa Cubel?
Universitat Polite`cnica de Vale`ncia
Shahram Khadivi??
RWTH Aachen
Antonio Lagarda?
Universitat Polite`cnica de Vale`ncia
Hermann Ney??
RWTH Aachen
Jesu?s Toma?s?
Universitat Polite`cnica de Vale`ncia
Enrique Vidal?
Universitat Polite`cnica de Vale`ncia
Juan-Miguel Vilar?
Universitat Jaume I
Current machine translation (MT) systems are still not perfect. In practice, the output
from these systems needs to be edited to correct errors. A way of increasing the productivity of
the whole translation process (MT plus human work) is to incorporate the human correction
activities within the translation process itself, thereby shifting the MT paradigm to that of
computer-assisted translation. This model entails an iterative process in which the human
translator activity is included in the loop: In each iteration, a prefix of the translation is validated
(accepted or amended) by the human and the system computes its best (or n-best) translation
suffix hypothesis to complete this prefix. A successful framework for MT is the so-called statis-
tical (or pattern recognition) framework. Interestingly, within this framework, the adaptation
of MT systems to the interactive scenario affects mainly the search process, allowing a great
reuse of successful techniques and models. In this article, alignment templates, phrase-based
models, and stochastic finite-state transducers are used to develop computer-assisted translation
systems. These systems were assessed in a European project (TransType2) in two real tasks: The
translation of printer manuals; manuals and the translation of the Bulletin of the European
Union. In each task, the following three pairs of languages were involved (in both translation
directions): English?Spanish, English?German, and English?French.
? Departament d?Enginyeria i Cie`ncies dels Computadors, Universitat Jaume I, 12071 Castello? de la Plana,
Spain.
?? Lehrstuhl fu?r Informatik VI, RWTH Aachen University of Technology, D-52056 Aachen, Germany.
? Institut Tecnolo`gic d?Informa`tica, Departament de Sistemes Informa`tics i Computacio?, Universitat
Polite`cnica de Vale`ncia, 46071 Vale`ncia, Spain.
? Institut Tecnolo`gic d?Informa`tica, Departament de Comunicacions, Universitat Polite`cnica de Vale`ncia,
46071 Vale`ncia, Spain.
? Departament de Llenguatges i Sistemes Informa`tics, Universitat Jaume I, 12071 Castello? de la Plana,
Spain.
Submission received: 1 June 2006; revised submission received: 20 September 2007; accepted for publication:
19 December 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
1. Introduction to Computer-Assisted Translation
Research in the field of machine translation (MT) aims to develop computer systems
which are able to translate text or speech without human intervention. However,
present translation technology has not been able to deliver fully automated high-quality
translations. Typical solutions to improving the quality of the translations supplied by
an MT system require manual post-editing. This serial process prevents the MT system
from taking advantage of the knowledge of the human translator, and the human
translator cannot take advantage of the adaptive ability of the MT system.
An alternative way to take advantage of the existing MT technologies is to use
them in collaboration with human translators within a computer-assisted translation
(CAT) or interactive framework (Isabelle and Church 1997). Historically, CAT and MT
have been considered different but close technologies (Kay 1997) and more so for one
of the most popular CAT technologies, namely, translation memories (Bowker 2002;
Somers 2003). Interactivity in CAT has been explored for a long time. Systems have
been designed to interact with human translators in order to solve different types
of (lexical, syntactic, or semantic) ambiguities (Slocum 1985; Whitelock et al 1986).
Other interaction strategies have been considered for updating user dictionaries or for
searching through dictionaries (Slocum 1985; Whitelock et al 1986). Specific proposals
can be found in Tomita (1985), Zajac (1988), Yamron et al (1993), and Sen, Zhaoxiong,
and Heyan (1997), among others.
An important contribution to CAT technology, carried out within the TransType
project, is worth mentioning (Foster, Isabelle, and Plamondon 1997; Langlais, Foster,
and Lapalme 2000; Foster 2002; Langlais, Lapalme, and Loranger 2002). It entailed an
interesting focus shift in which interaction is directly aimed at the production of the
target text, rather than at the disambiguation of the source text, as in earlier interactive
systems. The idea proposed in that work was to embed data-driven MT techniques
within the interactive translation environment. The hope was to combine the best of
both paradigms: CAT, in which the human translator ensures high-quality output, and
MT, in which the machine ensures a significant gain in productivity.
Following these TransType ideas, the innovative embedding proposed here con-
sists in using a complete MT system to produce full target sentence hypotheses, or
portions thereof, which can be accepted or amended by a human translator. Each cor-
rect text segment is then used by the MT system as additional information to achieve
further, hopefully improved, suggestions. More specifically, in each iteration, a prefix
of the target sentence is somehow fixed by the human translator and, in the next itera-
tion, the system predicts a best (or n-best) translation suffix(es)1 to complete this prefix.
We will refer to this process as interactive-predictive machine translation (IPMT).
This approach introduces two important requirements: First, the models have to
provide adequate completions and, second, this has to happen efficiently. Taking these
requirements into account, stochastic finite-state transducers (SFSTs), alignment tem-
plates (ATs), and phrase-based models (PBMs) are compared in this work. In previous
works these models have proven adequate for conventional MT (Vidal 1997; Amengual
et al 2000; Ney et al 2000; Toma?s and Casacuberta 2001; Och and Ney 2003; Casacuberta
and Vidal 2004; Och and Ney 2004; Vidal and Casacuberta 2004). This article shows that
1 The terms prefix and suffix are used here to denote any substring at the beginning and end (respectively)
of a string of characters (including spaces and punctuation), with no implication of morphological
significance as is usually implied by these terms in linguistics.
4
Barrachina et al Statistical Computer-Assisted Translation
existing efficient searching algorithms can be adapted in order to provide completions
(rather than full translations) also in a very efficient way.
The work presented here has been carried out in the TransType2 (TT2) project
(SchlumbergerSema S.A. et al 2001), which is considered as a follow-up to the inter-
active MT concepts introduced in the precursory TransType project cited previously.
We should emphasize the novel contributions of the present work with respect
to TransType. First, we show how fully fledged statistical MT (SMT) systems can be
extended to handle IPMT. In particular, the TT2 systems always produce complete
sentence hypotheses on which the human translator can work. This is an important
difference to previous work, in which the use of basic MT techniques only allowed the
prediction of single tokens (c.f., Section 2.2). Second, using fully fledged SMT systems,
we have performed systematic offline experiments to simulate the specific conditions of
interactive translation and we report and study the results of these experiments. Thirdly,
the IPMT systems presented in this article were successfully used in several field trials
with professional translators (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).
We should finally mention that the work developed in TT2 has gone beyond con-
ventional keyboard-and-mouse interaction, leading to the development of advanced
multi-modal interfaces. Speech is the most natural form of human communication and
its use as feedback in the IPMT framework has been explored by Vidal et al (2006).
On the other hand, human translators can be faster dictating the translation text rather
than typing it, thus it has also been investigated how to improve system performance
and usability when the user dictates the translation first and then edits the recognized
text (Khadivi, Zolnay, and Ney 2005; Khadivi, Zens, and Ney 2006).
The rest of the article is structured as follows. The next section introduces the
general setting for SMT and IPMT. In Section 3, AT, PBM, and SFST are briefly surveyed
along with the corresponding learning procedures. In Section 4, general search proce-
dures for the previous models are outlined and a detailed description of the extension
of these procedures to IPMT scenarios is presented. Section 5 is devoted to introducing
the tasks used for the assessment of the proposal presented in the previous sections:
the pairs of languages, corpora, and assessment procedures. The results are reported in
Section 6. A discussion of these results and the conclusions which can be drawn from
this work are presented in the final section.
2. Statistical Framework
The statistical or pattern recognition framework constitutes a very successful frame-
work for MT. As we will see here, this framework also proves adequate for IPMT.
2.1 Statistical Machine Translation
Assuming that we are given a sentence s in a source language, the text-to-text translation
problem can be stated as finding its translation t in a target language. Using statistical
decision theory, the best translation is given by the equation2
t? = argmax
t
Pr(t|s) (1)
2 We follow the common notation of Pr(x) for Pr(X = x) and Pr(x|y) for Pr(X = x|Y = y), for any random
variables X and Y. Similarly, Pr() will be used to denote ?true? probability functions, and p() or q() will
denote model approximations.
5
Computational Linguistics Volume 35, Number 1
Using Bayes?s Theorem, we arrive at
t? = argmax
t
Pr(t) ? Pr(s|t) (2)
This equation is generally interpreted as follows. The best translation must be a correct
sentence in the target language that conveys the meaning of the source sentence. The
probability Pr(t) represents the well-formedness of t and it is generally called the
language model probability (n-gram models are usually adopted [Jelinek 1998]). On
the other hand, Pr(s|t) represents the relationship between the two sentences (the source
and its translation). It should be of a high value if the source is a good translation of
the target and of a low value otherwise. Note that the translation direction is inverted
from what would be normally expected; correspondingly the models built around this
equation are often called inverted translation models (Brown et al 1990, 1993). As we
will see in Section 3, these models are based on the notion of alignment. It is interesting to
note that if we had perfect models, the use of Equation (1) would suffice. Given that we
have only approximations, the use of Equation (2) allows the language model to correct
deficiencies in the translation model.
In practice all of these models (and possibly others) are often combined into a log-
linear model for Pr(t | s) (Och and Ney 2004):
t? = argmax
t
{
N
?
i=1
?i ? log fi(t, s)
}
(3)
where fi(t, s) can be a model for Pr(s|t), a model for Pr(t|s), a target language model
for Pr(t), or any model that represents an important feature for the translation. N is the
number of models (or features) and ?i are the weights of the log-linear combination.
When using SFSTs, a different transformation can be used. These transducers
have an implicit target language model (which can be obtained from the finite-state
transducer by dropping the source symbols of each transition (Vidal et al 2005)). There-
fore, this separation is no longer needed. SFSTs model joint probability distributions;
therefore, Equation (1) has to be rewritten as
t? = argmax
t
Pr(s, t) (4)
This is the approach followed in GIATI (Casacuberta et al 2004a; Casacuberta and Vidal
2004), but other models for the joint probability can be adopted.
If the input is a spoken sentence, instead of a written one, the problem becomes
more complex; we will not deal with this here. The interested reader may consult
Amengual et al (2000), Ney et al (2000), or Casacuberta et al (2004a, 2004b), for
instance.
2.2 Statistical Interactive-Predictive Machine Translation
Unfortunately, current models and therefore the systems which can be built from them
are still far from perfect. This implies that, in order to achieve good, or even acceptable,
translations, manual post-editing is needed. An alternative to this serial approach (first
MT, then manual correction) is given by the IPMT paradigm. Under this paradigm,
translation is considered as an iterative process where human and computer activity
6
Barrachina et al Statistical Computer-Assisted Translation
Figure 1
Typical example of IPMT with keyboard interaction. The aim is to translate the English sentence
Click OK to close the print dialog into Spanish. Each step starts with a previously fixed target
language prefix tp, from which the system suggests a suffix t?s. Then the user accepts a part of this
suffix (a) and types some keystrokes (k), possibly in order to amend the remaining part of ts.
This produces a new prefix, composed by the prefix from the previous iteration and the accepted
and typed text, (a) (k), to be used as tp in the next step. The process ends when the user enters
the special keystroke ?#?. System suggestions are printed in italics and user input in boldface
typewriter font. In the final translation t, text that has been typed by the user is underlined.
are interwoven. This way, the models take into account both the input sentence and the
corrections of the user.
As previously mentioned, this idea was originally proposed in the TransType
project (Foster, Isabelle, and Plamondon 1997; Langlais, Foster, and Lapalme 2000;
Langlais, Lapalme, and Loranger 2002). In that project, the parts proposed by the sys-
tems were produced using a linear combination of a target language model (trigrams)
and a lexicon model (so-called IBM-1 or -2) (Langlais, Lapalme, and Loranger 2002). As
a result, TransType allowed only single-token completions, where a token could be either
a word or a short sequence of words from a predefined set of sequences. This proposal
was extended to complete full target sentences in the TT2 project, as discussed hereafter.
The approach taken in TT2 is exemplified in Figure 1. Initially, the system provides
a possible translation. From this translation, the user marks a prefix as correct and
provides, as a hint, the beginning of the rest of the translation. Depending on the system
or the user preferences, the hint can be the next word or some letters from it (in the
figure, hints are assumed to be words and are referred to as k). Let us use tp for the prefix
validated by the user together with the hint. The system now has to produce (predict)
a suffix ts to complete the translation. The cycle continues with a new validation and
hint from the user until the translation is completed. This justifies our choice of the term
?interactive-predictive machine translation? for this approach.
The crucial step of the process is the production of the suffix. Again, decision theory
tells us to maximize the probability of the suffix given the available information. That
is, the best suffix will be
t?s = argmax
ts
Pr(ts|s, tp) (5)
which can be straightforwardly rewritten as
t?s = argmax
ts
Pr(tp, ts|s) (6)
7
Computational Linguistics Volume 35, Number 1
Note that, because tpts = t, this equation is very similar to Equation (1). The main
difference is that the argmax search now is performed over the set of suffixes ts that
complete tp instead of complete sentences (t in Equation (1)). This implies that we can
use the same models if the search procedures are adequately modified (Och, Zens, and
Ney 2003).
The situation with respect to finite-state models is similar. Now, Equation (5) is
rewritten as
t?s = argmax
ts
Pr(tp, ts, s) (7)
which allows the use of the same models as in Equation (4) as long as the search
procedure is changed appropriately (Cubel et al 2003, 2004; Civera et al 2004a,
2004b).
3. Statistical and Finite-State Models
The models used are presented in the following subsections: Section 3.1 for the condi-
tional distribution Pr(s|t) in Equation (2) and Section 3.2 for the joint distribution Pr(s, t)
in Equation (4).
3.1 Statistical Alignment Models
The translation models which Brown et al (1993) introduced to deal with Pr(s|t) in
Equation (2) are based on the concept of alignment between the components of a pair
(s, t) (thus they are called statistical alignment models). Formally, if the number of
the source words in s is J and the number of target words in t is I, an alignment is a
function a : {1, ..., J} ? {0, ..., I}. The image of j by a will be denoted as aj, in which the
particular case aj = 0 means that the position j in s is not aligned with any position of t.
By introducing the alignment as a hidden variable in Pr(s|t),
Pr(s|t) =
?
a
Pr(s, a|t) (8)
The alignment that maximizes Pr(s, a|t) is shown to be very useful in practice for
training and for searching.
Different approaches have been proposed for modeling Pr(s, a|t) in Equation (8):
Zero-order models such as model 1, model 2, and model 3 (Brown et al 1993) and the first-
order models such as model 4, model 5 (Brown et al 1993), hidden Markov model (Ney
et al 2000), and model 6 (Och and Ney 2003).
In all these models, single words are taken into account. Moreover, in practice the
summation operator is replaced with the maximization operator, which in turn reduces
the contribution of each individual source word in generating a target word. On the
other hand, modeling word sequences rather than single words in both the alignment
and lexicon models cause significant improvement in translation quality (Och and Ney
8
Barrachina et al Statistical Computer-Assisted Translation
2004). In this work, we use two closely related models: ATs (Och and Ney 2004) and
PBMs (Toma?s and Casacuberta 2001; Koehn, Och, and Marcu 2003; Zens and Ney 2004).
Both models are based on bilingual phrases3 (pairs of segments or word sequences)
in which all words within the source-language phrase are aligned only to words of
the target-language phrase and vice versa. Note that at least one word in the source-
language phrase must be aligned to one word of the target-language phrase, that is,
there are no empty phrases similar to the empty word of the word-based models. In
addition, no gaps and no overlaps between phrases are allowed.
We introduce some notation to deal with phrases. As before, s denotes a source-
language sentence; ?s denotes a generic phrase in s, and ?sk the kth phrase in s. sj denotes
the jth source word in s; s
j?
j denotes the contiguous sequence of words in s beginning
at position j and ending at position j? (inclusive); obviously, if s has J words, s
J
1 denotes
the whole sentence s. An analogous notation is used for target words, phrases, and
sequences in target sentence t.
3.1.1 Alignment Templates. The ATs are based on the bilingual phrases but they are
generalized by replacing words with word classes and by storing the alignment in-
formation for each phrase pair. Formally, an AT Z is a triple (S,T,?a), where S and
T are a source class sequence and a target class sequence, respectively, and ?a is an
alignment from the set of positions in S to the set of positions in T.4 Mapping of source
and target words to bilingual word classes is automatically trained using the method
described by Och (1999). The method is actually an unsupervised clustering method
which partitions the source and target vocabularies, so that assigning words to classes
is a deterministic operation. It is also possible to employ parts-of-speech or semantic
categories instead of the unsupervised clustering method used here. More details can
be found in Och (1999) and Och and Ney (2004). However, it should be mentioned
that the whole AT approach (and similar PBM approaches as they are now called) is
independent of the word clustering concept. In particular, for large training corpora,
omitting the word clustering in the AT system does not much affect the translation
accuracy.
To arrive at our translation model, we first perform a segmentation of the source
and target sentences into K ?blocks? dk ? (ik; bk, jk) (ik ? {1, . . . , I} and jk, bk ? {1, . . . , J}
for 1 ? k ? K). For a given sentence pair (sJ1, t
I
1), the kth bilingual segment (?sk,
?tk)
is (s
jk
bk?1+1
, t
ik
ik?1+1
) (Och and Ney 2003). The AT Zk = (Sk,Tk,?ak) associated with the kth
bilingual segment is: Sk the sequence of word classes in ?sk; Tk the sequence of word
classes in ?tk, and ?ak the alignment between positions in a source class sequence S and
positions in a target class sequence T.
For translating a given source sentence s we use the following decision rule as an
approximation to Equation (1):
(I?, t?I?1) = argmax
I,tI1
{
max
K,dK1 ,?a
K
1
log PAT(s
J
1, t
I
1; d
K
1 ,?a
K
1 )
}
(9)
3 Although the term ?phrase? has a more restricted meaning, in this article it refers to a word sequence.
4 Note that the phrases in an AT are sequences of word classes rather than words, which motivates the use
of a different notation.
9
Computational Linguistics Volume 35, Number 1
We use a log-linear model combination:
log PAT(s
J
1, t
I
1; d
K
1 ,?a
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[ ?4 + ?5 ? log q(bk|jk?1)+ ?6 ? log p(Tk,?ak|Sk)+
ik
?
i=ik?1+1
?7 ? log p(ti|?sk,?ak) ] (10)
with weights ?i, i = 1, ? ? ? , 7. The weights ?1 and ?4 play a special role and are used
to control the number I of words and number K of segments for the target sentence
to be generated, respectively. The log-linear combination uses the following set of
models:
 p(ti|t
i?1
i?2): Word-based trigram language model
 p(Ti|T
i?1
i?4 ): Class-based five-gram language model
 p(Tk,?ak|Sk): AT at class level, model parameters are estimated directly
from frequency counts in a training corpus
 p(ti|?sk,?ak): Single word model based on a statistical dictionary and ?ak. As
in the preceding model, the model parameters are estimated by using
frequency counts
 q(bk|jk?1) = e|bk?jk?1+1|: Re-ordering model using absolute j distance of
the phrases.
As can be observed, all models are implemented as feature functions which depend on
the source and the target language sentences, as well as on the two hidden variables
(?aK1 , b
K
1 ). Other feature functions can be added to this sort of model as needed. For a
more detailed description the reader is referred to Och and Ney (2004).
Learning alignment templates. To learn the probability of applying an AT, p(Z =
(S,T,?a)|?s ), all bilingual phrases that are consistent with the segmentation are extracted
from the training corpus together with the alignment within these phrases. Thus, we
obtain a count N(Z) of how often an AT occurred in the aligned training corpus. Using
the relative frequency
p(Z) = (S,T,?a)|?s) =
N(Z) ? ?(S,C(?s))
N(C(?s))
(11)
we estimate the probability of applying an AT Z to translate the source language phrase
?s, in which ? is Kronecker?s delta function. The class function C maps words onto their
10
Barrachina et al Statistical Computer-Assisted Translation
classes. To reduce the memory requirements, only probabilities for phrases up to a
maximal length are estimated, and phrases with a probability estimate below a certain
threshold are discarded.
The weights ?i in Equation (10) are usually estimated using held-out data with
respect to the automatic evaluation metric employed using the downhill simplex al-
gorithm from Press et al (2002).
3.1.2 Phrase-Based Models. A simple alternative to AT has been introduced in recent
works: The PBM approach (Toma?s and Casacuberta 2001; Marcu and Wong 2002; Zens,
Och, and Ney 2002; Toma?s and Casacuberta 2003; Zens and Ney 2004). These methods
learn the probability that a sequence of contiguous words?the source phrase?(as a
whole unit) in a source sentence is a translation of another sequence of contiguous
words?the target phrase?(as a whole unit) in the target sentence. In this case, the
statistical dictionaries of single word pairs are substituted by statistical dictionaries of
bilingual phrases or bilingual segments. These models are simpler than ATs, because no
alignments are assumed between word positions inside a bilingual segment and word
classes are not used in the definition of a bilingual phrase.
The simplest formulation is for monotone PBMs (Toma?s and Casacuberta 2007),
assuming a uniform distribution of the possible segmentations of the source and of the
target sentences. In this case, the approximation to Equation (1) is:
(I?, t?I?1) = argmax
I,tI1
{
max
K,dK1
log PPBM(s
J
1, t
I
1; d
K
1 )
}
(12)
In our implementation of this approach, we have also adopted a log-linear model
log PPBM(s
J
1, t
I
1; d
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[
?4 + ?5 ? log p(?tk|?sk)
]
(13)
with weights ?i, i = 1, ? ? ? , 5. The weights ?1 and ?4 play a special role and are used
to control the number I of words and number K of segments for the target sentence
to be generated, respectively. The log-linear combination uses the following set of
models:
 p(ti|t
i?1
i?2): Word-based trigram language model
 p(Ti|T
i?1
i?4 ): Class-based five-gram language model
 p(?tk|?sk): Statistical dictionary of bilingual phrases.
11
Computational Linguistics Volume 35, Number 1
If segment re-ordering is desired (non-monotone models), the probability of phrase-
alignment q can be introduced (a first-order distortion model is assumed):
log PPBM(s
J
1, t
I
1; d
K
1 ) =
I
?
i=1
[
?1 + ?2 ? log p(ti|t
i?1
i?2)+ ?3 ? log p(Ti|T
i?1
i?4 )
]
+
K
?
k=1
[
?4 + ?5 ? log p(?tk|?sk)+ ?6 ? log q(bk|jk?1)
]
(14)
with the additional model q, similar to the one used for AT.
Learning phrase-based alignment models. The parameters of each model and the weights
?i in Equations (13) and (14) have to be estimated. There are different approaches to
estimating the parameters of each model (Toma?s and Casacuberta 2007). Some of these
techniques correspond to a direct learning of the parameters from a sentence-aligned
corpus using a maximum likelihood approach (Toma?s and Casacuberta 2001; Marcu
and Wong 2002). Other techniques are heuristics based on the previous computation
of word alignments in the training corpus (Zens, Och, and Ney 2002; Koehn, Och, and
Marcu 2003). On the other hand, as for AT, the weights ?i in Equation (13) are usually
optimized using held-out data.
3.2 Stochastic Finite-State Transducers
SFSTs constitute an important framework in syntactic pattern recognition and nat-
ural language processing. The simplicity of finite-state models has given rise to some
concerns about their applicability to real tasks. Specifically in the field of language
translation, it is often argued that natural languages are so complex that these simple
models are never able to cope with the required source-target mappings. However, one
should take into account that the complexity of the mapping between the source and
target domains of a transducer is not always directly related to the complexity of the
domains themselves. Instead, a key factor is the degree of monotonicity or sequentiality
between source and target subsequences of these domains (Casacuberta, Vidal, and
Pico? 2005). Finite-state transducers have been shown to be adequate to handle complex
mappings efficiently (Berstel 1979) and SFSTs are closely related to monotone PBMs.
In Equation (4), Pr(s, t) can be modeled by an SFST T, which is defined as a tuple
??,?,Q, q0, p, f ?, where ? is a finite set of source symbols,? is a finite set of target symbols
(? ?? = ?), Q is a finite set of states, q0 is the initial state, p and f are two functions
p : Q ? ??? ? Q ? [0, 1] (for the probabilities of transitions) and f : Q ? [0, 1] (for the
probabilities of final states) that satisfy ?q ? Q:
f (q) +
?
(s,?t,q? )?????Q
p(q, s,?t, q?) = 1 (15)
Given T, a path with J transitions associated with the translation pair (s, t) ?
?? ??? is a sequence of transitions ? = (q0, s1 , t?1, q1) (q1, s2 , t?2, q2) (q2, s3 , t?3, q3) . . .
(qJ?1, sJ , t?J, qJ ), such that s1 s2 . . . sJ = s and t?1 t?2 . . . t?J = t. The probability of a path is
12
Barrachina et al Statistical Computer-Assisted Translation
the product of its transition probabilities, times the final-state probability of the last
state in the path:
PT(?) =
J
?
j=1
p(qj?1, sj , t?j, qj) ? f (qJ ) (16)
The probability of a translation pair (s, t) according to T is then defined as the sum of
the probabilities of all the paths associated with (s, t):
PT(s, t) =
?
?
PT(?) (17)
Learning finite-state transducers. There are different families of techniques to train an
SFST from a parallel corpus of source?target sentences (Casacuberta and Vidal 2007).
One of the techniques that has been adopted in this work is the grammatical inference
and alignments for transducer inference (GIATI) technique. This technique is in the
category of hybrid methods which use statistical techniques to guide the SFST structure
learning and simultaneously train the associated probabilities.
Given a finite sample of string pairs, the inference of SFSTs using the GIATI tech-
nique is performed as follows (Casacuberta and Vidal 2004; Casacuberta, Vidal, and
Pico? 2005): i) Building training strings: Each training pair is transformed into a single
string from an extended alphabet to obtain a new sample of strings. ii) Inferring a
(stochastic) regular grammar. Typically, a smoothed n-gram is inferred from the sample
of strings obtained in the previous step. iii) Transforming the inferred regular grammar
into a transducer: The symbols associated with the grammar rules are converted back
into input/output symbols, thereby transforming the grammar inferred in the previous
step into a transducer. The transformation of a parallel corpus into a string corpus
is performed using statistical alignments. These alignments are obtained using the
GIZA++ software (Och and Ney 2003).
4. Searching
Searching is an important computational problem in SMT. Algorithmic solutions de-
veloped for SMT can be adapted to the IPMT framework. The main general search
procedures for each model in Section 3 are presented in the following subsections,
each followed by a detailed description of the necessary adaptations to the interactive
framework.
4.1 Searching with Alignment Templates
In offline MT, the generation of the best translation for a given source sentence s is
carried out by producing the target sentence in left-to-right order using the model of
Equation (10). At each step of the generation algorithm we maintain a set of active
hypotheses and choose one of them for extension. A word of the target language is
then added to the chosen hypothesis and its costs get updated. This kind of generation
fits nicely into a dynamic programming (DP) framework, as hypotheses which are
indistinguishable by both language and translation models (and that have covered
the same source positions) can be recombined. Because the DP search space grows
13
Computational Linguistics Volume 35, Number 1
Figure 2
Example of a word graph for the source German sentence was hast du gesagt? (English reference
translation: ?what did you say??).
exponentially with the size of the input, standard DP search is prohibitive, and we resort
to a beam-search heuristic.
4.1.1 Adaptation to the Interactive-Predictive Scenario. The most important modification
is to rely on a word graph that represents possible translations of the given source
sentence. This word graph is generated once for each source sentence. During the
process of human?machine interaction the system makes use of this word graph in
order to complete the prefixes accepted by the human translator. In other words, after
the human translator has accepted a prefix string, the system finds the best path in the
word graph associated with this prefix string so that it is able to complete the target
sentence. Using the word graph in such a way, the system is able to interact with the
human translator in a time efficient way. In Och, Zens, and Ney (2003), an efficient
algorithm for interactive generation using word graphs was presented. A word graph
is a weighted directed acyclic graph, in which each node represents a partial translation
hypothesis and each edge is labeled with a word of the target sentence and is weighted
according to the language and translation model scores. In Ueffing, Och, and Ney (2002),
the authors give a more detailed description of word graphs and show how they can be
easily produced as a by-product of the search process. An example of a word graph is
shown in Figure 2.
The computational cost of this approach is much lower, as the whole search for the
translation must be carried out only once, and the generated word graph can be reused
for further completion requests.
For a fixed source sentence, if no pruning is applied in the production of the word
graph, it represents all possible sequences of target words for which the posterior
probability is greater than zero, according to the models used. However, because of
the pruning generally needed to render the problem computationally feasible, the
resulting word graph only represents a subset of the possible translations. Therefore,
it may happen that the user sets prefixes which cannot be found in the word graph. To
circumvent this problem some heuristics need to be implemented.
First, we look for the node with minimum edit distance to the prefix except for
its last (partial) word.5 Then we select the completion path which starts with the last
5 The edit distance concept for finding the prefix string in a word graph could be refined by casting the edit
distance operations into a suitable probabilistic model.
14
Barrachina et al Statistical Computer-Assisted Translation
(partial) word of the prefix and has the best backward score?this is the score associated
with a path going from the node to the final node. Now, because the original word graph
may not be compatible with the new information provided by the prefix, it might be
impossible to find a completion in this word graph due to incompatibility with the
last (partial) word in the prefix. This problem can be solved to a certain degree by
searching for a completion of the last word with the highest probability using only the
language model. This supplementary heuristic to the usual search increases the perfor-
mance of the system, because some of the rejected words in the pruning process can
be recovered.
A desirable feature of an IPMT system is the possibility of producing a list of
alternative target suffixes, instead of only one. This feature can be easily added by
computing the n-best hypotheses. Of course, these n-best hypotheses do not refer to
the whole target sentence, but only to the suffixes. However, the problem is that in
many cases the sentence hypotheses in the n-best list differ in only one or two words.
Therefore, we introduce the additional requirement that the first four words of the n-
best hypotheses must be different.
4.2 Searching with Phrase-Based Models
The generation of the best translation with PBMs is similar to the one described in the
previous section. Each hypothesis is composed of a prefix of the target sentence, a subset
of source positions that are aligned with the positions of the prefix of the target sentence,
and a score. In this case, we adopted an extension of the best-first strategy where the
hypotheses are stored in several sorted lists, depending on which words in the source
sentence have been translated. This strategy is related to the well-known multi-stack-
decoding algorithm (Berger et al 1996; Toma?s and Casacuberta 2004). In each iteration,
the algorithm extends the best hypothesis from each available list.
While target words are always generated from left to right, there are two alter-
natives in the source word extraction: Monotone search, which takes the source words
from left to right, and non-monotone search, which can take source words in any
order.
4.2.1 Adaptation to the Interactive-Predictive Scenario. Only a simple modification of this
search algorithm is necessary: If the new extended hypothesis is not compatible with
the fixed target prefix, tp, then this hypothesis is not considered. This compatibility is
verified at the character level; therefore the user does not need to type the whole target
word at the end of the target prefix.
In the interactive scenario, speed is a critical aspect. In the PBM approach, monotone
search is much faster than non-monotone search in the tasks which are considered in this
work (Toma?s and Casacuberta 2006). However, monotone search presents a problem for
interactive operation: If a user introduces a prefix that cannot be obtained in a monotone
way from the source, the search algorithm is not able to complete this prefix. In order
to solve this problem without losing computational efficiency, we use the following ap-
proach: Non-monotone search is used for target prefixes, whereas completions (suffixes)
are generated using monotone search.
As for AT models, a list of target suffixes can also be produced. This list can be
obtained easily by keeping the n-best hypotheses in each sorted list. To avoid generating
very similar hypotheses in the n-best list, we apply the following procedure: Starting
from the n-best list resulting from the normal search, we first add hypotheses obtained
15
Computational Linguistics Volume 35, Number 1
by translating a single untranslated word from the source, along with hypotheses
consisting of a single high-probability word according to the target language model; we
then re-order the hypotheses, maximizing the diversity at the beginning of the suffixes,
and keep only the n first hypotheses in the re-ordered list.
4.3 Searching with Stochastic Finite-State Transducers
As discussed by Pico? and Casacuberta (2001), the computation of Equation (4) for SFSTs
under a maximum approximation (i.e., using maximization in Equation (17) instead
of the sum) amounts to a conventional Viterbi search. The algorithm finds the most
probable path among those paths in the SFST which are compatible with the source
sentence s. The corresponding translation, t?, is simply obtained by concatenating the
target strings of the edges of this path.
4.3.1 Adaptation to the Interactive-Predictive Scenario. Here, Equation (7) is used wherein
the optimization is performed over the set of target suffixes (completions) rather than
the set of complete target sentences. To solve this maximization problem, an approach
similar to that proposed for AT in Section 4.1 has been adopted.
First, given the source sentence, a word graph is extracted from the SFST. In this
case, the word graph is just (a pruned version of) the Viterbi search trellis obtained when
translating the whole source sentence. The main difference between the word graphs
generated with ATs and SFSTs is how the nodes and edges are defined in each case. On
the one hand, the nodes are defined as partial hypotheses of the search procedure in
the AT approach, whereas the nodes in the case of SFSTs can be directly mapped into
states in the SFST representing a joint (source word/target string) language model. On
the other hand, the scores associated with the edges in the AT approach are computed
from a combination of the language and translation models, whereas in the case of
SFSTs these scores simply come from the joint language model estimated by the GIATI
technique.
Once the word graph has been generated, the search for the most probable com-
pletion as stated in Equation (6) is carried out in two steps, in a similar way to that
explained for the AT approach. In this case, the computation entailed by both the edit-
distance (prefix error-correcting) and the remaining search is significantly accelerated
by visiting the nodes in topological order and by the incorporation of the beam-search
technique (Amengual and Vidal 1998). Moreover, the error-correcting algorithm takes
advantage of the incremental way in which the user prefix is generated, parsing only
the new suffix appended by the user in the last interaction.
It may be the case that a user prefix ends in an incomplete word during the inter-
active translation process. Therefore, it is necessary to start the translation completion
with a word whose prefix matches this unfinished word. The proposed algorithm thus
searches for such a word. First, it considers the target words of the edges leaving
the nodes returned by the error-correcting algorithm. If this initial search fails, then
a matching word is looked up in the word-graph vocabulary. Finally, as a last resort,
the whole transducer vocabulary is taken into consideration to find a matching word;
otherwise this incomplete word is treated as an entire word.
This error-correcting algorithm returns a set of nodes from which the best comple-
tion would be selected according to the best backward score. Moreover, n-best com-
pletions can also be produced. Among many weighted-graph n-best path algorithms
which are available, the recursive enumeration algorithm presented in Jime?nez and
16
Barrachina et al Statistical Computer-Assisted Translation
Marzal (1999) was adopted for its simplicity in calculating best paths on demand and its
smooth integration with the error-correcting algorithm.
5. Experimental Framework
The models and search procedures introduced in the previous sections were assessed
through a series of IPMT experiments with different corpora. These corpora, along with
the corresponding pre- and post-processing and assessment procedures, are presented
in this section.
5.1 Pre- and Post-Processing
Usually, MT models are trained on a pre-processed version of an original corpus. Pre-
processing provides a simpler representation of the training corpus which makes token
or word forms more homogeneous. In this way automatic training of the MT models is
boosted, and the amount of computation decreases.
The pre-processing steps are: tokenization, removing unnecessary case information,
and tagging some special tokens like numerical sequences, e-mail addresses, and URLs
(?categorization?). In translation from a source language to a target language, there are
some words which are translated identically (because they have the same spelling in
both languages). Therefore, we identify them in the corpus and replace them with some
generic tags to help the translation system.
Post-processing takes place after the translation in order to hide the internal repre-
sentation of the text from the user. Thus, the user will only work with an output which
is very similar to human-generated texts. In detail, the post-processing steps are: de-
tokenization, true-casing, and replacing the tags with their corresponding words.
In an IPMT scenario, the pre-/post-processing must run in real-time and should be
reversible as much as possible. In each human?machine interaction, the current prefix
has to be pre-processed for the interactive-predictive engine and then the generated
completion has to be post-processed for the user. It is crucial that the pre-processing of
prefixes is fully compatible with the training corpus.
5.2 Xerox and EU Corpora
Six bilingual corpora were used for two different tasks and three different language
pairs in the framework of the TT2 project (SchlumbergerSema S.A. et al 2001).
The language pairs involved were English?Spanish, English?French, and English?
German (Khadivi and Goutte 2003), and the tasks were Xerox (Xerox printer manuals)
and EU (Bulletin of the European Union).
The three Xerox corpora were obtained from different user manuals for Xerox print-
ers (SchlumbergerSema S.A. et al 2001). The main features of these corpora are shown
in Table 1. Dividing the corpora into training and test sets was performed by randomly
selecting (without replacement) a specified amount of test sentences and leaving the
remaining ones for training. It is worth noting that the manuals were not the same in
each pair of languages. Even though all training and test sets have similar size, this
probably explains why the perplexity varies considerably over the different language
pairs. The vocabulary size was computed using the tokenized and true-case corpus.
The three bilingual EU corpora were extracted from the Bulletin of the European
Union, which exists in all official languages of the European Union (Khadivi and Goutte
17
Computational Linguistics Volume 35, Number 1
Table 1
The Xerox corpora. For all the languages, the training/test full-sentence overlap and the rate of
out-of-vocabulary test-set words were less than 10% and 1%, respectively. Trigram models were
used to compute the test word perplexity. (K and M denote thousands and millions,
respectively.)
English/Spanish English/German English/French
T
ra
in Sent. pairs (K) 56 49 53
Running words (M) 0.7/0.7 0.6/0.5 0.6/0.7
Vocabulary (K) 15/17 14/25 14/16
T
e
st
Sentences (K) 1.1 1.0 1.0
Running words (K) 8/10 12/12 11/12
Running chars. (K) 46/59 63/73 56/65
Perplexity 99/58 57/93 109/70
2003) and is publicly available on the Internet. The corpora used in the experiments
which are described subsequently were again acquired and processed in the framework
of the TT2 project. The main features of these corpora are shown in Table 2. The
vocabulary size and the training and test set partitions were obtained in a similar way
as with the Xerox corpora.
5.3 Assessment
In all the experiments reported in this article, system performance is assessed by
comparing test sentence translations produced by the translation systems with the
corresponding target language references of the test set. Some of the computed assess-
ment figures measure the quality of the translation engines without any system?user
interactivity:
 Word error rate (WER): The minimum number of substitution, insertion,
and deletion operations needed to convert the word strings produced by
the translation system into the corresponding single-reference word
strings. WER is normalized by the overall number of words in the
reference sentences (Och and Ney 2003).
Table 2
The EU corpora. For all the languages, the training/test full-sentence overlap and the rate of
out-of-vocabulary test-set words were less than 3% and 0.2%, respectively. Trigram models were
used to compute the test word perplexity. (K and M denote thousands and millions,
respectively.)
English/Spanish English/German English/French
T
ra
in Sent. pairs (K) 214 223 215
Running words (M) 5.2/5.9 5.7/5.4 5.3/6.0
Vocabulary (K) 84/97 86/153 84/91
T
e
st
Sentences (K) 0.8 0.8 0.8
Running words (K) 20/23 20/19 20/23
Running chars. (K) 119/135 120/134 119/134
Perplexity 58/46 57/87 58/45
18
Barrachina et al Statistical Computer-Assisted Translation
 Bilingual evaluation understudy (BLEU): This is based on the coverage of
n-grams of the hypothesized translation which occur in the reference
translations (Papineni et al 2001).
Other assessment figures are aimed at estimating the effort needed by a human
translator to produce correct translations using the interactive system. To this end, the
target translations which a real user would have in mind are simulated by the given
references. The first translation hypothesis for each given source sentence is compared
with a single reference translation and the longest common character prefix (LCP) is
obtained. The first non-matching character is replaced by the corresponding reference
character and then a new system hypothesis is produced. This process is iterated until
a full match with the reference is obtained.
Each computation of the LCP would correspond to the user looking for the next
error and moving the pointer to the corresponding position of the translation hypothesis.
Each character replacement, on the other hand, would correspond to a keystroke of
the user. If the first non-matching character is the first character of the new system
hypothesis in a given iteration, no LCP computation is needed; that is, no pointer
movement would be made by the user. Bearing this in mind, we define the following
interactive-predictive performance measures:
 Keystroke ratio (KSR): Number of keystrokes divided by the total number
of reference characters.
 Mouse-action ratio (MAR): Number of pointer movements plus one more
count per sentence (aimed at simulating the user action
needed to accept the final translation), divided by the total number of
reference characters.
 Keystroke and mouse-action ratio (KSMR): KSR plus MAR.
Note that KSR estimates only the user?s actions on the keyboard whereas MAR
estimates actions for which the user would typically use the mouse. From a user
point of view the two types of actions are different and require different types of
effort (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006). In any case, as an
approximation, KSMR accounts for both KSR and MAR, assuming that both actions
require a similar effort.
In the case of SMT systems, it is well known that an automatically computed
quality measure like BLEU correlates quite well with human judgment (Callison-Burch,
Osborne, and Koehn 2006). In the case of IPMT, we should keep in mind that the
main goal of (automatic) assessment is to estimate the effort of the human translator.
Moreover, translation quality is not an issue here, because the (simulated) human
intervention ensures ?perfect? translation results. The important question is whether
the (estimated) productivity of the human translator can really be increased or not by
the IPMT approach. In order to answer this question, the KSR and KSMR measures will
be used in the IPMT experiments to be reported in the next section.
In order to show the statistical significance of the results, all the assessment figures
reported in the next section are accompanied by the corresponding 95% confidence
intervals. These intervals have been computed using bootstrap sampling techniques, as
proposed by Bisani and Ney (2004), Koehn (2004), and Zhang and Vogel (2004).
19
Computational Linguistics Volume 35, Number 1
6. Results
Two types of results are reported for each corpus and for each translation approach.
The first are conventional MT results, obtained as a reference to give an idea of the
?classical? MT difficulty of the selected tasks. The second aim is to assess the interactive
MT (IPMT) approach proposed in this article.
The results are presented in different subsections. The first two subsections present
the MT and IPMT results for the 1-best translation obtained by the different techniques
in the Xerox and EU tasks, respectively. The third subsection presents further IPMT
results for the 5-best translations on a single pair of languages.
Some of these results may differ from results presented in previous works (Cubel
et al 2003; Och, Zens, and Ney 2003; Civera et al 2004a; Cubel et al 2004; Bender
et al 2005). The differences are due to variations in the pre-/post-processing procedures
and/or recent improvements of the search techniques used by the different systems.
6.1 Experiments with the Xerox Corpora
In this section, the translation results obtained using ATs, PBMs, and SFSTs for all six
language pairs of the Xerox corpus are reported. Word-based trigram and class-based
five-gram target-language models were used for the AT models (the parameters of the
log-linear model are tuned so as to minimize WER on a development corpus); word-
based trigram target-language models were used for PBMs and trigrams were used to
infer GIATI SFSTs.
Off-line MT Results. MT results with ATs, PBMs, and SFSTs are presented in Figure 3.
Results obtained using the PBMs are slightly but consistently better that those achieved
using the other models. In general, the different techniques perform similarly for the
various translation directions. However, the English?Spanish language pair is the one
for which the best translations can be produced.
IPMT Results. Performance has been measured in terms of KSRs and MARs (KSR and
MAR are represented as the lower and upper portions of each bar, respectively, and
KSMR is the whole bar length). The results are shown in Figure 4.
Figure 3
Off-line MT results (BLEU and WER) for the Xerox corpus. Segments above the bars show the
95% confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
20
Barrachina et al Statistical Computer-Assisted Translation
Figure 4
IPMT results for the Xerox corpus. In each bar, KSR is represented by the lower portion, MAR by
the upper portion, and KSMR is the whole bar. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
According to these results, a human translator assisted by an AT-based or a SFST-
based interactive system would only need an effort equivalent to typing about 20% of
the characters in order to produce the correct translations for the Spanish to English
task; or even less than 20% if a PBM-based system is used.
For the Xerox task, off-line MT performance and IPMT results show similar tenden-
cies. The PBMs show better performance for both the off-line MT and for the IPMT
assessment figures. The AT and SFST models perform more or less equivalently. In
both scenarios, the best results were achieved for the Spanish?English language pair
followed by French?English and German?English.
The computing times needed by all the systems involved in these experiments were
well within the range of the on-line operational requirements. The average initial time
for each source test sentence was very low (less than 50 msec) for PBMs and SFSTs
and adequate for ATs (772 msec). In the case of ATs and SFSTs, this included the time
required for the generation of the initial word-graph of each sentence. Moreover, the
most critical times incurred in the successive IPMT iterations were very low in all
the cases: 18 msec for ATs, 99 msec for PBMs, and 9 msec for SFSTs. Note, however,
that these average times are not exactly comparable because of the differences in the
computer hardware used by each system (2 Ghz AMD, 1.5 Ghz Pentium, and 2.4 Ghz
Pentium for ATs, PBMs, and SFSTs, respectively).
6.2 Experiments with the EU Corpora
The translation results using the AT, PBM, and SFST approaches for all six language
pairs of the EU corpus are reported in this section. As for the Xerox corpora, in the AT
experiments, word-based trigram and class-based five-gram target-language models
were used; in the PBM experiments, word-based trigram and class-based five-gram
target-language models were also used and five-grams were used to infer GIATI SFSTs.
Off-line MT Results. Figure 5 presents the results obtained using ATs, PBMs, and SFSTs.
Generally speaking, the results are comparable to those obtained on the Xerox corpus
with the exception of the English?Spanish language pair, which were better. With these
corpora, the best results were obtained with the ATs and PBMs for all the pairs and the
best translation direction was French-to-English with all the models used.
21
Computational Linguistics Volume 35, Number 1
Figure 5
Off-line MT results (BLEU and WER) for the EU corpus. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
IPMT Results. Figure 6 shows the performance of the AT, PBM, and SFST systems in
terms of KSRs and MARs in a similar way as for the Xerox corpora.
As in the MT experiments, the results are comparable to those obtained on the Xerox
corpus, with the exception of the English?Spanish pair. Similarly, as in MT, the best
results were obtained for the French-to-English translation direction.
Although EU is a more open-domain task, the results demonstrate again the poten-
tial benefit of computer-assisted translation systems. Using PBMs, a human translator
would only need an effort equivalent to typing about 20% of the characters in order
to produce the correct translations for French-to-English translation direction, whereas
for ATs and SFSTs the effort would be about 30%. For the other language pairs, the
efforts would be about 20?30% and 35% of the characters for PBMs and ATs/SFSTs,
respectively.
The systemwise correlation between MT and IPMT results on this corpus is not
as clear as in the Xerox case. One possible cause is the much larger size of the EU
corpus compared to the Xerox corpus. In order to run the EU experiments within rea-
sonable time limits, all the systems have required the use of beam search and/or other
Figure 6
IPMT results for the EU corpus. In each bar, KSR is represented by the lower portion, MAR by
the upper portion and KSMR is the whole bar. Segments above the bars show the 95%
confidence intervals. En = English; Sp = Spanish; Fr = French; Ge = German.
22
Barrachina et al Statistical Computer-Assisted Translation
Table 3
IPMT results (%) for the Xerox corpus (English?Spanish) using ATs, PBMs, and SFSTs for the
1-best hypothesis and 5-best hypotheses. 95% confidence intervals are shown.
1-best 5-best
Technique KSR KSMR KSR KSMR
AT 12.9?0.9 23.2?1.3 11.1?0.8 20.3?1.2
PBM 8.9?0.8 16.7?1.2 7.3?0.6 15.4?1.1
SFST 13.0?1.0 21.8?1.4 11.2?1.0 19.2?1.3
suboptimal pruning techniques, although this was largely unnecessary for the Xerox
corpus. Clearly, the pruning effects are different in the off-line (MT) and the on-line
(IPMT) search processes and the differences may lead to wide performance variations
for the AT, PBM, and SFST approaches.
Nevertheless, as can be seen in Bender et al (2005), the degradation in system
performance due to pruning is generally not too substantial and sufficiently accurate
real-time interactive operation could also be achieved in the EU task with the three
systems tested.
6.3 Results with n-Best Hypotheses
Further experiments were carried out to study the usefulness of n-best hypotheses in
the interactive framework. In this scenario, the user can choose one out of n proposed
translation suffixes and then proceed as in the usual IPMT paradigm. As with the
previous experiments, the automated evaluation is based on a selected target sentence
that best matches a prefix of the reference translation in each IPMT iteration (therefore
KSR is minimized).
Here, only IPMT results for the English-to-Spanish translation direction are re-
ported for both Xerox and EU tasks, using a list of the five best translations. These results
are shown in Tables 3 and 4.
In all the cases there is a clear and significant accuracy improvement when moving
from single-best to 5-best translations. This gain in translation quality diminishes in a
log-wise fashion as we increase the number of best translations. From a practical point
of view, the improvements provided by using n-best completions would come at the
cost of the user having to ponder which of these completions is more suitable. In a
real operational environment, this additional user effort may or may not outweigh the
Table 4
IPMT results (%) for the EU corpus (English?Spanish) using ATs, PBMs, and SFSTs for the 1-best
hypothesis and 5-best hypotheses. 95% confidence intervals are shown.
1-best 5-best
Technique KSR KSMR KSR KSMR
AT 20.2?0.9 32.6?1.3 18.5?0.8 29.9?1.2
PBM 16.3?0.7 27.8?1.1 13.2?0.6 25.0?1.1
SFST 21.3?0.9 33.0?1.3 19.3?0.9 29.9?1.3
23
Computational Linguistics Volume 35, Number 1
benefits of the n-best increased accuracy. Consequently, this feature should be offered to
the users as an option.
7. Practical Issues
IPMT results reported in the previous section provide reasonable estimations of potential
savings of human translator effort, assuming that the goal is to obtain high quality
translations. In real work, however, several practical issues not discussed in this article
may significantly affect the actual system usability and overall user productivity.
One of the most obvious issues is that a carefully designed graphical user interface
(GUI) is needed to let the users actually be in command of the translation process, so
that they really feel the system is assisting them rather than the other way around. In
addition, an adequate GUI has to provide adequate means for the users to easily and
intuitively change at will IPMT engine parameters that may have an impact on their
way of working with the system. To name just a few: The maximum length of system
hypotheses, the value of n for n-best suggestions, or the ?interaction step granularity?;
that is, whether the system should react at each user keystroke, or at the end of each
complete typed word, or after a sufficiently long typing pause, and so on.
Clearly, all these important issues are beyond the scope of the present article. But
we can comment that, in the TT2 project, complete prototypes of some of the systems
presented in this article, including the necessary GUI, were actually implemented and
thoroughly evaluated by professional human translators in their working environ-
ment (Macklovitch, Nguyen, and Silva 2005; Macklovitch 2006).
The results of these field tests showed that the actual productivity depended not
only on the individual translators, but also on the given test texts. In cases where these
texts were quite unrelated to the training data, the system did not significantly help
the human translators to increase their productivity. However, when the test texts were
reasonably well related to the training data, high productivity gains were registered?
close to what could be expected according to the KSR/MAR empirical results.
8. Concluding Remarks
The IPMT paradigm proposed in this article allows for a close collaboration between a
human translator and a machine translation system. This paradigm entails an iterative
process where, in each iteration, a data-driven machine translation engine suggests a
completion for the current prefix of a target sentence which a human translator can
accept, modify, or ignore.
This idea was originally proposed in the TransType project (Langlais, Foster, and
Lapalme 2000), where a simple engine was used which only supported single-token
suggestions. Furthering these ideas, in the TransType2 project (SchlumbergerSema S.A.
et al 2001), state-of-the-art statistical machine translation systems have been developed
and integrated in the IPMT framework.
In a laboratory environment, results on two different tasks suggest that the pro-
posed techniques can reduce the typing effort needed to produce a high-quality transla-
tion of a given source text by as much as 80% with respect to the effort needed to simply
type the whole translation. In real conditions, a high productivity gain was achieved in
many cases.
We have studied here IPMT from the point of view of a standalone CAT tool.
Nevertheless, IPMT can of course be easily and conveniently combined with other
popular translator workbench tools. More specifically, IPMT lends itself particularly
24
Barrachina et al Statistical Computer-Assisted Translation
well to addressing the typical lack of generalization capabilities of translation memories.
When used as a CAT tool, translation memories allow the human translator to keep
producing increasingly long segments of correct target text. Clearly, these segments can
be used by an IPMT engine to suggest to the translator possible translations for source
text segments that are not found in the translation memories as exact matches.
Acknowledgments
This work has been partially supported by
the ST Programme of European Union under
grant IST-2001-32091, by the Spanish project
TIC?2003-08681-C02-02, and the Spanish
research programme Consolider
Ingenio-2010 CSD2007-00018. The authors
wish to thank the anonymous reviewers for
their criticisms and suggestions.
References
Amengual, J. C., J. M. Bened??, A. Castan?o,
A. Castellanos, V. M. Jime?nez, D. Llorens,
A. Marzal, M. Pastor, F. Prat, E. Vidal, and
J. M. Vilar. 2000. The EuTrans-I speech
translation system. Machine Translation,
15:75?103.
Amengual, J. C. and E. Vidal. 1998. Efficient
error-correcting Viterbi parsing. IEEE
Transactions on Pattern Analysis and Machine
Intelligence, 20(10):1109?1116.
Bender, O., S. Hasan, D. Vilar, R. Zens, and
H. Ney. 2005. Comparison of generation
strategies for interactive machine
translation. In Proceedings of the 10th
Annual Conference of the European
Association for Machine Translation (EAMT
05), pages 33?40, Budapest.
Berger, A. L., P. F. Brown, S. A. Della Pietra,
V. J. Della Pietra, J. R. Gillett, A. S. Kehler,
and R. L. Mercer. 1996. Language
translation apparatus and method of using
context-based translation models. United
States Patent No. 5510981, April.
Berstel, J. 1979. Transductions and Context-Free
Languages. B. G. Teubner, Stuttgart.
Bisani, M. and H. Ney. 2004. Bootstrap
estimates for confidence intervals in ASR
performance evaluation. In Proceedings of
the International Conference on Acoustic,
Speech and Signal Processing (ICASSP 04),
volume 1, pages 409?412, Montreal.
Bowker, L. 2002. Computer-Aided Translation
Technology: A Practical Introduction,
chapter 5: Translation-memory systems.
Didactics of Translation. University of
Ottawa Press, pages 92?127.
Brown, P. F., J. Cocke, S. A. Della Pietra,
V. J. Della Pietra, F. Jelinek, J. D. Lafferty,
R. L. Mercer, and P. S. Roosin. 1990.
A statistical approach to machine
translation. Computational Linguistics,
16(2):79?85.
Brown, P. F., S. A. Della Pietra, V. J.
Della Pietra, and R. L. Mercer. 1993. The
mathematics of statistical machine
translation: Parameter estimation.
Computational Linguistics, 19(2):263?310.
Callison-Burch, C., M. Osborne, and
P. Koehn. 2006. Re-evaluating the role of
BLEU in machine translation research. In
Proceedings of the 10th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL 06),
pages 249?256, Trento.
Casacuberta, F., H. Ney, F. J. Och, E. Vidal,
J. M. Vilar, S. Barrachina, I. Garc??a-Varea,
D. Llorens, C. Mart??nez, S. Molau,
F. Nevado, M. Pastor, D. Pico?, A. Sanchis,
and C. Tillmann. 2004a. Some approaches
to statistical and finite-state
speech-to-speech translation. Computer
Speech and Language, 18:25?47.
Casacuberta, F. and E. Vidal. 2004. Machine
translation with inferred stochastic
finite-state transducers. Computational
Linguistics, 30(2):205?225.
Casacuberta, F. and E. Vidal. 2007. Learning
finite-state models for machine translation.
Machine Learning, 66(1):69?91.
Casacuberta, F., E. Vidal, and D. Pico?. 2005.
Inference of finite-state transducers from
regular languages. Pattern Recognition,
38:1431?1443.
Casacuberta, F., E. Vidal, A. Sanchis, and
J. M. Vilar. 2004b. Pattern recognition
approaches for speech-to-speech
translation. Cybernetic and Systems: an
International Journal, 35(1):3?17.
Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,
S. Barrachina, E. Vidal, F. Casacuberta,
D. Pico?, and J. Gonza?lez. 2004a. From
machine translation to computer assisted
translation using finite-state models. In
Proceedings of the Conference on Empirical
Methods for Natural Language Processing
(EMNLP 04), pages 349?356, Barcelona.
Civera, J., J. M. Vilar, E. Cubel, A. L. Lagarda,
S. Barrachina, F. Casacuberta, E. Vidal,
D. Pico?, and J. Gonza?lez. 2004b. A syntactic
pattern recognition approach to computer
assisted translation. In Advances in
25
Computational Linguistics Volume 35, Number 1
Statistical, Structural and Syntactical Pattern
Recognition, Proceedings of the Joint IAPR
International Workshops on Syntactical and
Structural Pattern Recognition (SSPR 04)
and Statistical Pattern Recognition
(SPR 04)), Lisbon, Portugal, August 18?20,
volume 3138 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 207?215.
Cubel, E., J. Civera, J. M. Vilar, A. L. Lagarda,
S. Barrachina, E. Vidal, F. Casacuberta,
D. Pico?, J. Gonza?lez, and L. Rodr??guez.
2004. Finite-state models for computer
assisted translation. In Proceedings of the
16th European Conference on Artificial
Intelligence (ECAI 04), pages 586?590,
Valencia.
Cubel, E., J. Gonza?lez, A. Lagarda,
F. Casacuberta, A. Juan, and E. Vidal. 2003.
Adapting finite-state translation to the
TransType2 project. In Proceedings of the
Joint Conference Combining the 8th
International Workshop of the European
Association for Machine Translation and the
4th Controlled Language Applications Workshop
(EAMT-CLAW 03), pages 54?60, Dublin.
Foster, G. 2002. Text Prediction for Translators.
Ph.D. thesis, Universite? de Montre?al,
Canada.
Foster, G., P. Isabelle, and P. Plamondon.
1997. Target-text mediated interactive
machine translation. Machine Translation,
12(1?2):175?194.
Isabelle, P. and K. Church. 1997. Special issue
on new tools for human translators.
Machine Translation, 12(1?2).
Jelinek, F. 1998. Statistical Methods for Speech
Recognition. The MIT Press, Cambridge,
MA.
Jime?nez, V. M. and A. Marzal. 1999.
Computing the k shortest paths: a new
algorithm and an experimental
comparison. In Algorithm Engineering:
Proceedings of the 3rd International
Workshop (WAE 99), London, UK, July 19?21,
volume 1668 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 15?29.
Kay, M. 1997. The proper place of men and
machines in language translation. Machine
Translation, 12:3?23. [This article first
appeared as a Xerox PARC Working Paper
in 1980].
Khadivi, S. and C. Goutte. 2003. Tools for
corpus alignment and evaluation of the
alignments (deliverable d4.9). Technical
report, TransType2 (IST-2001-32091).
Khadivi, S., R. Zens, and H. Ney. 2006.
Integration of speech to computer-assisted
translation using finite-state automata.
In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics
and 21th International Conference on
Computational Linguistics (COLING/ACL
06), pages 467?474, Sydney.
Khadivi, S., A. Zolnay, and H. Ney. 2005.
Automatic text dictation in
computer-assisted translation. In
Proceedings of the European Conference on
Speech Communication and Technology,
(INTERSPEECH 05-EUROSPEECH),
pages 2265?2268, Lisbon.
Koehn, P. 2004. Statistical significance
tests for machine translation evaluation.
In Proceedings of the Conference on
Empirical Methods for Natural Language
Processing (EMNLP 04), pages 388?395,
Barcelona.
Koehn, P., F. J. Och, and D. Marcu. 2003.
Statistical phrase-based translation. In
Proceedings of the 2003 Meeting of the North
American Chapter of the Association for
Computational Linguistics (NAACL 03),
pages 127?133, Edmonton.
Langlais, P., G. Foster, and G. Lapalme. 2000.
TransType: a computer-aided translation
typing system. In Proceedings of the
NAACL/ANLP Workshop on Embedded
Machine Translation Systems, pages 46?52,
Seattle, WA.
Langlais, P., G. Lapalme, and M. Loranger.
2002. Transtype: Development-evaluation
cycles to boost translator?s productivity.
Machine Translation, 15(4):77?98.
Macklovitch, E. 2006. TransType2: The last
word. In Proceedings of the 5th International
Conference on Languages Resources and
Evaluation (LREC 06), pages 167?172,
Genoa.
Macklovitch, E., N. T. Nguyen, and R. Silva.
2005. User evaluation report. Technical
report, TransType2 (IST-2001-32091).
Marcu, D. and W. Wong. 2002. A
phrase-based, joint probability model
for statistical machine translation.
In Proceedings of the Conference on
Empirical Methods for Natural Language
Processing (EMNLP 02), pages 133?139,
Philadelphia, PA.
Ney, H., S. Nie?en, F. Och, H. Sawaf,
C. Tillmann, and S. Vogel. 2000.
Algorithms for statistical translation of
spoken language. IEEE Transactions on
Speech and Audio Processing, 8(1):24?36.
Och, F. J. 1999. An efficient method for
determining bilingual word classes. In
Proceedings of the 9th Conference of the
European Chapter of the Association for
26
Barrachina et al Statistical Computer-Assisted Translation
Computational Linguistics (EACL 99),
pages 71?76, Bergen.
Och, F. J. and H. Ney. 2003. A systematic
comparison of various statistical
alignment models. Computational
Linguistics, 29(1):19?51.
Och, F. J. and H. Ney. 2004. The alignment
template approach to statistical machine
translation. Computational Linguistics,
30(4):417?450.
Och, F. J., R. Zens, and H. Ney. 2003.
Efficient search for interactive statistical
machine translation. In Proceedings of
the 10th Conference of the European Chapter
of the Association for Computational
Linguistics (EACL 03), pages 387?393,
Budapest.
Papineni, K., S. Roukos, T. Ward, and
W. Zhu. 2001. BLEU: a method for
automatic evaluation of machine
translation. Technical Report RC22176,
Thomas J. Watson Research Center.
Pico?, D. and F. Casacuberta. 2001. Some
statistical-estimation methods for
stochastic finite-state transducers. Machine
Learning, 44:121?142.
Press, W. H., S. A. Teukolsky, W. T.
Vetterling, and B. P. Flannery. 2002.
Numerical Recipes in C++: The Art of
Scientific Computing. Cambridge University
Press, Cambridge, UK.
SchlumbergerSema S.A., Intituto Tecnolo?gico
de Informa?tica, Rheinisch Westfa?lische
Technische Hochschule Aachen Lehrstul
fu?r Informatik VI, Recherche Applique?e
en Linguistique Informatique Laboratory
University of Montreal, Celer Soluciones,
Socie?te? Gamma, and Xerox
Research Centre Europe. 2001. TT2.
TransType2?computer-assisted
translation. Project technical annex.
Information Society Technologies (IST)
Programme, IST-2001-32091.
Sen, Z., Ch. Zhaoxiong, and H. Heyan. 1997.
Interactive approach in machine translation
systems. In Proceedings of IEEE International
Conference on Intelligent Processing Systems
(ICIPS 97), pages 1814?1819, Beijing.
Slocum, J. 1985. A survey of machine
translation: Its history, current status and
future prospects. Computational Linguistics,
11(1):1?17.
Somers, H., 2003. Computers and Translation: a
Translator?s Guide, chapter 3: Translation
memory systems. John Benjamins,
Amsterdam, pages 31?48.
Toma?s, J. and F. Casacuberta. 2001.
Monotone statistical translation using
word groups. In Proceedings of the Machine
Translation Summit VIII (MT SUMMIT
VIII), pages 357?361, Santiago de
Compostela.
Toma?s, J. and F. Casacuberta. 2003.
Combining phrase-based and
template-based alignment models in
statistical translation. In Pattern Recognition
and Image Analysis, Proceedings of the First
Iberian Conference (IbPRIA 03), Puerto
de Andratx, Mallorca, Spain, June 4-6,
volume 2652 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 1020?1031.
Toma?s, J. and F. Casacuberta. 2004. Statistical
machine translation decoding using
target word reordering. In Advances in
Statistical, Structural and Syntactical Pattern
Recognition, Proceedings of the Joint IAPR
International Workshops on Syntactical
and Structural Pattern Recognition
(SSPR 04) and Statistical Pattern Recognition
(SPR 04), Lisbon, Portugal, August 18?20,
volume 3138 of Lecture Notes in Computer
Science. Springer-Verlag, Heidelberg,
pages 734?743.
Toma?s, J. and F. Casacuberta. 2006. Statistical
phrase-based models for interactive
computer-assisted translation. In
Proceedings of the 44th Annual Meeting
of the Association for Computational
Linguistics and 21th International
Conference on Computational Linguistics
(COLING/ACL 06), pages 835?841, Sydney.
Toma?s, J. and F. Casacuberta. 2007. A
pattern recognition approach to
machine translation: Monotone and
non-monotone phrase-based statistical
models. Technical Report DSIC-II/18/07,
Departamento de Sistemas Informa?ticos y
Computacio?n, Universidad Polite?cnica
de Valencia.
Tomita, M. 1985. Feasibility study of
personal/interactive machine translation
systems. In Proceedings of the First
International Conference on Theoretical
and Methodological Issues in Machine
Translation (TMI 85), pages 289?297,
New York, NY.
Ueffing, N., F. J. Och, and H. Ney. 2002.
Generation of word graphs in statistical
machine translation. In Proceedings of
the Conference on Empirical Methods for
Natural Language Processing (EMNLP 02),
pages 156?163, Philadelphia, PA.
Vidal, E. 1997. Finite-state speech-to-speech
translation. In Proceedings of the
International Conference on Acoustic,
Speech and Signal Processing (ICASSP 97),
volume 1, pages 111?114, Munich.
27
Computational Linguistics Volume 35, Number 1
Vidal, E. and F. Casacuberta. 2004. Learning
finite-state models for machine translation.
In Grammatical Inference: Algorithms and
Applications, Proceedings of the 7th
International Coloquium on Grammatical
Inference (ICGI 04), Athens, Greece,
October 11?13, volume 3264 of Lecture
Notes in Artificial Intelligence. Springer,
Heidelberg, pages 16?27.
Vidal, E., F. Casacuberta, L. Rodr??guez,
J. Civera, and C. Mart??nez. 2006.
Computer-assisted translation using
speech recognition. IEEE Transactions
on Speech and Audio Processing,
14(3):941?951.
Vidal, E., F. Thollard, F. Casacuberta
C. de la Higuera, and R. Carrasco. 2005.
Probabilistic finite-state machines?
part II. IEEE Transactions on Pattern
Analysis and Machine Intelligence,
27(7):1025?1039.
Whitelock, P. J., M. McGee Wood, B. J.
Chandler, N. Holden, and H. J. Horsfall.
1986. Strategies for interactive machine
translation: The experience and
implications of the UMIST Japanese
project. In Proceedings of the 11th
International Conference on Computational
Linguistics (COLING 86), pages 329?334,
Bonn.
Yamron, J., J. Baker, P. Bamberg,
H. Chevalier, T. Dietzel, J. Elder,
F. Kampmann, M. Mandel, L. Manganaro,
T. Margolis, and E. Steele. 1993.
LINGSTAT: an interactive, machine-aided
translation system. In Proceedings of the
Workshop on Human Language Technology,
pages 191?195, Princeton, NJ.
Zajac, R. 1988. Interactive translation: A new
approach. In Proceedings of the 12th
International Conference on Computational
Linguistics (COLING 88), pages 785?790,
Budapest.
Zens, R. and H. Ney. 2004. Improvements
in phrase-based statistical machine
translation. In Proceedings of the Human
Language Technology Conference / North
American Chapter of the Association for
Computational Linguistics Annual Meeting
(HLT-NAACL 04), pages 257?264,
Boston, MA.
Zens, R., F. J. Och, and H. Ney. 2002.
Phrase-based statistical machine
translation. In Advances in Artificial
Intelligence. 25th Annual German Conference
on Artificial Intelligence (KI 02), Aachen,
Germany, September 16?22, Proceedings,
volume 2479 of Lecture Notes on Artificial
Intelligence. Springer Verlag, Heidelberg,
pages 18?32.
Zhang, Y. and S. Vogel. 2004. Measuring
confidence intervals for the machine
translation evaluation metrics. In
Proceedings of the Tenth International
Conference on Theoretical and
Methodological Issues in Machine
Translation (TMI 04), pages 294?301,
Baltimore, MD.
28
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 467?474,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Integration of Speech to Computer-Assisted Translation Using
Finite-State Automata
Shahram Khadivi Richard Zens
Lehrstuhl fu?r Informatik 6 ? Computer Science Department
RWTH Aachen University, D-52056 Aachen, Germany
{khadivi,zens,ney}@cs.rwth-aachen.de
Hermann Ney
Abstract
State-of-the-art computer-assisted transla-
tion engines are based on a statistical pre-
diction engine, which interactively pro-
vides completions to what a human trans-
lator types. The integration of human
speech into a computer-assisted system is
also a challenging area and is the aim of
this paper. So far, only a few methods
for integrating statistical machine transla-
tion (MT) models with automatic speech
recognition (ASR) models have been stud-
ied. They were mainly based on N -
best rescoring approach. N -best rescor-
ing is not an appropriate search method
for building a real-time prediction engine.
In this paper, we study the incorporation
of MT models and ASR models using
finite-state automata. We also propose
some transducers based on MT models for
rescoring the ASR word graphs.
1 Introduction
A desired feature of computer-assisted transla-
tion (CAT) systems is the integration of the hu-
man speech into the system, as skilled human
translators are faster at dictating than typing the
translations (Brown et al, 1994). Additionally,
incorporation of a statistical prediction engine, i.e.
a statistical interactive machine translation system,
to the CAT system is another useful feature. A sta-
tistical prediction engine provides the completions
to what a human translator types (Foster et al,
1997; Och et al, 2003). Then, one possible proce-
dure for skilled human translators is to provide the
oral translation of a given source text and then to
post-edit the recognized text. In the post-editing
step, a prediction engine helps to decrease the
amount of human interaction (Och et al, 2003).
In a CAT system with integrated speech, two
sources of information are available to recognize
the speech input: the target language speech
and the given source language text. The target
language speech is a human-produced translation
of the source language text. Statistical machine
translation (MT) models are employed to take into
account the source text for increasing the accuracy
of automatic speech recognition (ASR) models.
Related Work
The idea of incorporating ASR and MT models
was independently initiated by two groups:
researchers at IBM (Brown et al, 1994),
and researchers involved in the TransTalk
project (Dymetman et al, 1994; Brousseau
et al, 1995). In (Brown et al, 1994), the
authors proposed a method to integrate the IBM
translation model 2 (Brown et al, 1993) with
an ASR system. The main idea was to design
a language model (LM) to combine the trigram
language model probability with the translation
probability for each target word. They reported a
perplexity reduction, but no recognition results.
In the TransTalk project, the authors improved
the ASR performance by rescoring the ASR
N -best lists with a translation model. They also
introduced the idea of a dynamic vocabulary for
a speech recognition system where translation
models were generated for each source language
sentence. The better performing of the two is the
N -best rescoring.
Recently, (Khadivi et al, 2005) and (Paulik et
al., 2005a; Paulik et al, 2005b) have studied the
integration of ASR and MT models. The first
work showed a detailed analysis of the effect of
different MT models on rescoring the ASR N -best
lists. The other two works considered two parallel
N -best lists, generated by MT and ASR systems,
467
respectively. They showed improvement in the
ASR N -best rescoring when some proposed fea-
tures are extracted from the MT N -best list. The
main concept among all features was to generate
different kinds of language models from the MT
N -best list.
All of the above methods are based on an N -
best rescoring approach. In this paper, we study
different methods for integrating MT models to
ASR word graphs instead of N -best list. We
consider ASR word graphs as finite-state automata
(FSA), then the integration of MT models to ASR
word graphs can benefit from FSA algorithms.
The ASR word graphs are a compact representa-
tion of possible recognition hypotheses. Thus, the
integration of MT models to ASR word graphs can
be considered as an N -best rescoring but with very
large value for N . Another advantage of working
with ASR word graphs is the capability to pass
on the word graphs for further processing. For
instance, the resulting word graph can be used in
the prediction engine of a CAT system (Och et al,
2003).
The remaining part is structured as follows: in
Section 2, a general model for an automatic text
dictation system in the computer-assisted transla-
tion framework will be described. In Section 3,
the details of the machine translation system and
the speech recognition system along with the lan-
guage model will be explained. In Section 4,
different methods for integrating MT models into
ASR models will be described, and also the exper-
imental results will be shown in the same section.
2 Speech-Enabled CAT Models
In a speech-enabled computer-assisted translation
system, we are given a source language sentence
fJ1 = f1 . . . fj . . . fJ , which is to be translated into
a target language sentence eI1 = e1 . . . ei . . . eI ,
and an acoustic signal xT1 = x1 . . . xt . . . xT ,
which is the spoken target language sentence.
Among all possible target language sentences, we
will choose the sentence with the highest probabil-
ity:
e?I?1= argmax
I,eI1
{Pr(eI1|fJ1 , xT1 )} (1)
?= argmax
I,eI1
{Pr(eI1)Pr(fJ1 |eI1)Pr(xT1 |eI1)}(2)
Eq. 1 is decomposed into Eq. 2 by assuming
conditional independency between xT1 and fJ1 .
The decomposition into three knowledge sources
allows for an independent modeling of the target
language model Pr(eI1), the translation model
Pr(fJ1 |eI1) and the acoustic model Pr(xT1 |eI1).
Another approach for modeling the posterior
probability Pr(eI1|fJ1 , xT1 ) is direct modeling us-
ing a log-linear model. The decision rule is given
by:
e?I?1 = argmax
I,eI1
{ M?
m=1
?mhm(eI1, fJ1 , xT1 )
}
(3)
Each of the terms hm(eI1, fJ1 , xT1 ) denotes one
of the various models which are involved in the
recognition procedure. Each individual model is
weighted by its scaling factor ?m. As there is
no direct dependence between fJ1 and xT1 , the
hm(eI1, fJ1 , xT1 ) is in one of these two forms:
hm(eI1, xT1 ) and hm(eI1, fJ1 ). Due to the argmax
operator which denotes the search, no renormal-
ization is considered in Eq. 3. This approach has
been suggested by (Papineni et al, 1997; Papineni
et al, 1998) for a natural language understanding
task, by (Beyerlein, 1998) for an ASR task, and
by (Och and Ney, 2002) for an MT task. This
approach is a generalization of Eq. 2. The di-
rect modeling has the advantage that additional
models can be easily integrated into the overall
system. The model scaling factors ?M1 are trained
on a development corpus according to the final
recognition quality measured by the word error
rate (WER)(Och, 2003).
Search
The search in the MT and the ASR systems is
already very complex, therefore a fully integrated
search to combine ASR and MT models will
considerably increase the complexity. To reduce
the complexity of the search, we perform two
independent searches with the MT and the ASR
systems, the search result of each system will be
represented as a large word graph. We consider
MT and ASR word graphs as FSA. Then, we are
able to use FSA algorithms to integrate MT and
ASR word graphs. The FSA implementation of
the search allows us to use standard optimized
algorithms, e.g. available from an open source
toolkit (Kanthak and Ney, 2004).
The recognition process is performed in two
steps. First, the baseline ASR system generates a
word graph in the FSA format for a given utterance
xT1 . Second, the translation models rescore each
word graph based on the corresponding source
language sentence. For each utterance, the deci-
sion about the best sentence is made according to
the recognition and the translation models.
468
3 Baseline Components
In this section, we briefly describe the basic sys-
tem components, namely the MT and the ASR
systems.
3.1 Machine Translation System
We make use of the RWTH phrase-based statis-
tical machine translation system for the English
to German automatic translation. The system in-
cludes the following models: an n-gram language
model, a phrase translation model and a word-
based lexicon model. The latter two models are
used for both directions: German to English and
English to German. Additionally, a word penalty
and a phrase penalty are included. The reordering
model of the baseline system is distance-based, i.e.
it assigns costs based on the distance from the end
position of a phrase to the start position of the next
phrase. More details about the baseline system
can be found in (Zens and Ney, 2004; Zens et al,
2005).
3.2 Automatic Speech Recognition System
The acoustic model of the ASR system is trained
on the VerbMobil II corpus (Sixtus et al, 2000).
The corpus consists of German large-vocabulary
conversational speech: 36k training sentences
(61.5h) from 857 speakers. The test corpus is
created from the German part of the bilingual
English-German XEROX corpus (Khadivi et al,
2005): 1562 sentences including 18k running
words (2.6h) from 10 speakers. The test cor-
pus contains 114 out-of-vocabulary (OOV) words.
The remaining part of the XEROX corpus is used
to train a back off trigram language model us-
ing the SRI language modeling toolkit (Stolcke,
2002). The LM perplexity of the speech recogni-
tion test corpus is about 83. The acoustic model of
the ASR system can be characterized as follows:
? recognition vocabulary of 16716 words;
? 3-state-HMM topology with skip;
? 2500 decision tree based generalized within-
word triphone states including noise plus one
state for silence;
? 237k gender independent Gaussian densities
with global pooled diagonal covariance;
? 16 MFCC features;
? 33 acoustic features after applying LDA;
? LDA is fed with 11 subsequent MFCC vec-
tors;
? maximum likelihood training using Viterbi
approximation.
Table 1: Statistics of the machine translation cor-
pus.
English German
Train: Sentences 47 619
Running Words 528 779 467 633
Vocabulary 9 816 16 716
Singletons 2 302 6 064
Dev: Sentences 700
Running Words 8 823 8 050
Unknown words 56 108
Eval: Sentences 862
Running Words 11 019 10 094
Unknown words 58 100
The test corpus recognition word error rate is
20.4%. Compared to the previous system (Khadivi
et al, 2005), which has a WER of 21.2%, we
obtain a 3.8% relative improvement in WER. This
improvement is due to a better and complete opti-
mization of the overall ASR system.
4 Integration Approaches
In this section, we will introduce several ap-
proaches to integrate the MT models with the ASR
models. To present the content of this section in a
more reader-friendly way, we will first explain the
task and corpus statistics, then we will present the
results of N -best rescoring. Afterwards, we will
describe the new methods for integrating the MT
models with the ASR models. In each sub-section,
we will also present the recognition results.
4.1 Task
The translation models are trained on the part of
the English-German XEROX corpus which was
not used in the speech recognition test corpus. We
divide the speech recognition test corpus into two
parts, the first 700 utterances as the development
corpus and the rest as the evaluation corpus. The
development corpus is used to optimize the scal-
ing factors of different models (explained in Sec-
tion 2). The statistics of the corpus are depicted in
Table 1. The German part of the training corpus is
also used to train the language model.
4.2 N -best Rescoring
To rescore the N -best lists, we use the method
of (Khadivi et al, 2005). But the results shown
here are different from that work due to a better
optimization of the overall ASR system, using a
469
Table 2: Recognition WER [%] using N -best
rescoring method.
Models Dev Eval
MT 47.1 50.5
ASR 19.3 21.3
ASR+MT IBM-1 17.8 19.0
HMM 18.2 19.2
IBM-3 17.1 18.4
IBM-4 17.1 18.3
IBM-5 16.6 18.2
Phrase
-based 18.8 20.3
better MT system, and generating a larger N -best
list from the ASR word graphs. We rescore the
ASR N -best lists with the standard HMM (Vogel
et al, 1996) and IBM (Brown et al, 1993) MT
models. The development and evaluation sets N -
best lists sizes are sufficiently large to achieve
almost the best possible results, on average 1738
hypotheses per each source sentence are extracted
from the ASR word graphs.
The recognition results are summarized in Ta-
ble 2. In this table, the translation results of the
MT system are shown first, which are obtained
using the phrase-based approach. Then the recog-
nition results of the ASR system are shown. After-
wards, the results of combined speech recognition
and translation models are presented.
For each translation model, the N -best lists
are rescored based on the translation probability
p(eI1|fJ1 ) of that model and the probabilities of
speech recognition and language models. In the
last row of Table 2, the N -best lists are rescored
based on the full machine translation system ex-
plained in Section 3.1.
The best possible hypothesis achievable from
the N -best list has the WER (oracle WER) of
11.2% and 12.4% for development and test sets,
respectively.
4.3 Direct Integration
At the first glance, an obvious method to combine
the ASR and MT systems is the integration at the
level of word graphs. This means the ASR system
generates a large word graph for the input target
language speech, and the MT system also gener-
ates a large word graph for the source language
text. Both MT and ASR word graphs are in the
target language. These two word graphs can be
considered as two FSA, then using FSA theory,
we can integrate two word graphs by applying the
composition algorithm.
We conducted a set of experiments to integrate
the ASR and MT systems using this method. We
obtain a WER of 19.0% and 20.9% for devel-
opment and evaluation sets, respectively. The
results are comparable to N -best rescoring results
for the phrase-based model which is presented in
Table 2. The achieved improvements over the
ASR baseline are statistically significant at the
99% level (Bisani and Ney, 2004). However, the
results are not promising compared to the results
of the rescoring method presented in Table 2 for
HMM and IBM translation models. A detailed
analysis revealed that only 31.8% and 26.7% of
sentences in the development and evaluation sets
have identical paths in both FSA, respectively. In
other words, the search algorithm was not able to
find any identical paths in two given FSA for the
remaining sentences. Thus, the two FSA are very
different from each other. One explanation for
the failure of this method is the large difference
between the WERs of two systems, as shown in
Table 2 the WER for the MT system is more than
twice as high as for the ASR system.
4.4 Integrated Search
In Section 4.3, two separate word graphs are
generated using the MT and the ASR systems.
Another explanation for the failure of the direct
integration method is the independent search to
generate the word graphs. The search in the MT
and the ASR systems is already very complex,
therefore a full integrated search to combine ASR
and MT models will considerably increase the
complexity.
However, it is possible to reduce this problem
by integrating the ASR word graphs into the gen-
eration process of the MT word graphs. This
means, the ASR word graph is used in addition to
the usual language model. This kind of integration
forces the MT system to generate identical paths to
those in the ASR word graph. Using this approach,
the number of identical paths in MT and ASR
word graphs are increased to 39.7% and 34.4%
of the sentences in development and evaluation
sets, respectively. The WER of the integrated
system are 19.0% and 20.7% for development and
evaluation sets.
4.5 Lexicon-Based Transducer
The idea of a dynamic vocabulary, restricting and
weighting the word lexicon of the ASR was first
470
introduced in (Brousseau et al, 1995). The idea
was also seen later in (Paulik et al, 2005b), they
extract the words of the MT N -best list to restrict
the vocabulary of the ASR system. But they both
reported a negative effect from this method on
the recognition accuracy. Here, we extend the
dynamic vocabulary idea by weighting the ASR
vocabulary based on the source language text and
the translation models. We use the lexicon model
of the HMM and the IBM MT models. Based on
these lexicon models, we assign to each possible
target word e the probability Pr(e|fJ1 ). One way
to compute this probability is inspired by IBM
Model 1:
Pr(e|fJ1 ) =
1
J + 1
J?
j=0
p(e|fj)
We can design a simple transducer (or more pre-
cisely an acceptor) using probability in Eq. 4 to
efficiently rescore all paths (hypotheses) in the
word graph with IBM Model 1:
PIBM-1(eI1|fJ1 ) =
1
(J + 1)I
I?
i=1
J?
j=0
p(ei|fj)
=
I?
i=1
1
(J + 1) ? p(ei|f
J
1 )
The transducer is formed by one node and a num-
ber of self loops for each target language word. In
each arc of this transducer, the input label is target
word e and the weight is ? log 1J+1 ? p(e|fJ1 ).
We conducted experiments using the proposed
transducer. We built different transducers with the
lexicons of HMM and IBM translation models. In
Table 3, the recognition results of the rescored
word graphs are shown. The results are very
promising compared to the N -best list rescoring,
especially as the designed transducer is very sim-
ple. Similar to the results for the N -best rescoring
approach, these experiments also show the benefit
of using HMM and IBM Models to rescore the
ASR word graphs.
Due to its simplicity, this model can be easily
integrated into the ASR search. It is a sentence
specific unigram LM.
4.6 Phrase-Based Transducer
The phrase-based translation model is the main
component of our translation system. The pairs
of source and corresponding target phrases are
extracted from the word-aligned bilingual training
Table 3: Recognition WER [%] using lexicon-
based transducer to rescore ASR word graphs.
Models Dev Eval
ASR 19.3 21.3
ASR+MT IBM-1 17.5 19.0
HMM 17.8 19.2
IBM-3 17.7 18.8
IBM-4 17.8 18.8
IBM-5 17.6 18.9
corpus (Zens and Ney, 2004). In this section, we
design a transducer to rescore the ASR word graph
using the phrase-based model of the MT system.
For each source language sentence, we extract all
possible phrases from the word-aligned training
corpus. Using the target part of these phrases
we build a transducer similar to the lexicon-based
transducer. But instead of a target word on each
arc, we have the target part of a phrase. The weight
of each arc is the negative logarithm of the phrase
translation probability.
This transducer is a good approximation of non-
monotone phrase-based-lexicon score. Using the
designed transducer it is possible that some parts
of the source texts are not covered or covered more
than once. Then, this model can be compared
to the IBM-3 and IBM-4 models, as they also
have the same characteristic in covering the source
words. The above assumption is not critical for
rescoring the ASR word graphs, as we are con-
fident that the word order is correct in the ASR
output. In addition, we assume low probability for
the existence of phrase pairs that have the same
target phrase but different source phrases within a
particular source language sentence.
Using the phrase-based transducer to rescore
the ASR word graph results in WER of 18.8%
and 20.2% for development and evaluation sets,
respectively. The improvements are statistically
significant at the 99% level compared to the ASR
system. The results are very similar to the results
obtained using N -best rescoring method. But
the transducer implementation is much simpler
because it does not consider the word-based lex-
icon, the word penalty, the phrase penalty, and
the reordering models, it just makes use of phrase
translation model. The designed transducer is
much faster in rescoring the word graph than the
MT system in rescoring the N -best list. The av-
erage speed to rescore the ASR word graphs with
this transducer is 49.4 words/sec (source language
471
text words), while the average speed to translate
the source language text using the MT system is
8.3 words/sec. The average speed for rescoring
the N -best list is even slower and it depends on
the size of N -best list.
A surprising result of the experiments as has
also been observed in (Khadivi et al, 2005), is that
the phrase-based model, which performs the best
in MT, has the least contribution in improving the
recognition results. The phrase-based model uses
more context in the source language to generate
better translations by means of better word selec-
tion and better word order. In a CAT system, the
ASR system has much better recognition quality
than MT system, and the word order of the ASR
output is correct. On the other hand, the ASR
recognition errors are usually single word errors
and they are independent from the context. There-
fore, the task of the MT models in a CAT system is
to enhance the confidence of the recognized words
based on the source language text, and it seems
that the single word based MT models are more
suitable than phrase-based model in this task.
4.7 Fertility-Based Transducer
In (Brown et al, 1993), three alignment models
are described that include fertility models, these
are IBM Models 3, 4, and 5. The fertility-based
alignment models have a more complicated struc-
ture than the simple IBM Model 1. The fertility
model estimates the probability distribution for
aligning multiple source words to a single target
word. The fertility model provides the probabili-
ties p(?|e) for aligning a target word e to ? source
words. In this section, we propose a method for
rescoring ASR word graphs based on the lexicon
and fertility models.
In (Knight and Al-Onaizan, 1998), some trans-
ducers are described to build a finite-state based
translation system. We use the same transduc-
ers for rescoring ASR word graphs. Here, we
have three transducers: lexicon, null-emitter, and
fertility. The lexicon transducer is formed by
one node and a number of self loops for each
target language word, similar to IBM Model 1
transducer in Section 4.5. On each arc of the
lexicon transducer, there is a lexicon entry: the
input label is a target word e, the output label is
a source word f , and the weight is ? log p(f |e).
The null-emitter transducer, as its name states,
emits the null word with a pre-defined probability
after each input word. The fertility transducer is
also a simple transducer to map zero or several
instances of a source word to one instance of the
source word.
The ASR word graphs are composed succes-
sively with the lexicon, null-emitter, fertility trans-
ducers and finally with the source language sen-
tence. In the resulting transducer, the input labels
of the best path represent the best hypothesis.
The mathematical description of the proposed
method is as follows. We can decompose Eq. 1
using Bayes? decision rule:
e?I?1= argmax
I,eI1
{Pr(eI1|fJ1 , xT1 )} (4)
?= argmax
I,eI1
{Pr(fJ1 )Pr(eI1|fJ1 )Pr(xT1 |eI1)}(5)
In Eq. 5, the term Pr(xT1 |eI1) is the acoustic model
and can be represented with the ASR word graph1,
the term Pr(eI1|fJ1 ) is the translation model of
the target language text to the source language
text. The translation model can be represented
by lexicon, fertility, and null-emitter transducers.
Finally, the term Pr(fJ1 ) is a very simple language
model, it is the source language sentence.
The source language model in Eq. 5 can be
formed into the acceptor form in two different
ways:
1. a linear acceptor, i.e. a sequence of nodes
with one incoming arc and one outgoing arc,
the words of source language text are placed
consecutively in the arcs of the acceptor,
2. an acceptor containing possible permuta-
tions. To limit the permutations, we used an
approach as in (Kanthak et al, 2005).
Each of these two acceptors results in different
constraints for the generation of the hypotheses.
The first acceptor restricts the system to generate
exactly the same source language sentence, while
the second acceptor forces the system to generate
the hypotheses that are a reordered variant of
the source language sentence. The experiments
conducted do not show any significant difference
in the recognition results among the two source
language acceptors, except that the second accep-
tor is much slower than the first acceptor. There-
fore, we use the first model in our experiments.
Table 4 shows the results of rescoring the ASR
word graphs using the fertility-based transducers.
1Actually, the ASR word graph is obtained by using
Pr(xT1 |eI1) and Pr(eI1) models. However, It does not cause
any problem in the modeling, especially when we make use
of the direct modeling, Eq. 3
472
Table 4: Recognition WER [%] using fertility-
based transducer to rescore ASR word graphs.
Models Dev Eval
ASR 19.3 21.3
ASR+MT IBM-3 17.4 18.6
IBM-4 17.4 18.5
IBM-5 17.6 18.7
As Table 4 shows, we get alost the same
or slightly better results when compared to the
lexicon-based transducers.
Another interesting point about Eq. 5 is its simi-
larity to speech translation (translation from target
spoken language to source language text). Then,
we can describe a speech-enabled CAT system
as similar to a speech translation system, except
that we aim to get the best ASR output (the best
path in the ASR word graph) rather than the best
translation. This is because the best translation,
which is the source language sentence, is already
given.
5 Conclusion
We have studied different approaches to integrate
MT with ASR models, mainly using finite-state
automata. We have proposed three types of trans-
ducers to rescore the ASR word graphs: lexicon-
based, phrase-based and fertility-based transduc-
ers. All improvements of the combined models
are statistically significant at the 99% level with
respect to the baseline system, i.e. ASR only.
In general, N -best rescoring is a simplification
of word graph rescoring. As the size of N -best
list is increased, the results obtained by N -best
list rescoring approach the results of the word
graph rescoring. But we should consider that the
statement is correct when we use exactly the same
model and the same implementation to rescore the
N -best list and word graph. Figure 1 shows the
effect of the N -best list size on the recognition
WER of the evaluation set. As we expected, the
recognition results of N -best rescoring improve
as N becomes larger, until the point that the
recognition result converges to its optimum value.
As shown in Figure 1, we should not expect that
word graph rescoring methods outperform the N -
best rescoring method, when the size of N -best
lists are large enough. In Table 2, the recognition
results are calculated using a large enough size for
N -best lists, a maximum of 5,000 per sentence,
which results in the average of 1738 hypotheses
 18
 18.5
 19
 19.5
 20
 20.5
 21
 21.5
 1  10  100  1000  10000
W
E
R
 
[
%
]
Size of N-best list (N), in log scale
IBM-1HMMIBM-3IBM-4IBM-5
Figure 1: The N -best rescoring results for differ-
ent N -best sizes on the evaluation set.
per sentence. An advantage of the word graph
rescoring is the confidence of achieving the best
possible results based on a given rescoring model.
The word graph rescoring methods presented in
this paper improve the baseline ASR system with
statistical significance. The results are competitive
with the best results of N -best rescoring. For the
simple models like IBM-1, the transducer-based
integration generates similar or better results than
N -best rescoring approach. For the more com-
plex translation models, IBM-3 to IBM-5, the
N -best rescoring produces better results than the
transducer-based approach, especially for IBM-
5. The main reason is due to exact estimation
of IBM-5 model scores on the N -best list, while
the transducer-based implementation of IBM-3 to
IBM-5 is not exact and simplified. However, we
observe that the fertility-based transducer which
can be considered as a simplified version of IBM-
3 to IBM-5 models can still obtain good results,
especially if we compare the results on the evalu-
ation set.
Acknowledgement
This work has been funded by the European
Union under the RTD project TransType2 (IST
2001 32091) and the integrated project TC-
STAR - Technology and Corpora for Speech
to Speech Translation -(IST-2002-FP6-506738,
http://www.tc-star.org).
References
P. Beyerlein. 1998. Discriminative model combina-
tion. In Proc. IEEE Int. Conf. on Acoustics, Speech,
and Signal Processing (ICASSP), volume 1, pages
481 ? 484, Seattle, WA, May.
473
M. Bisani and H. Ney. 2004. Bootstrap estimates
for confidence intervals in ASR performance evalu-
ationx. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 409?412,
Montreal, Canada, May.
J. Brousseau, C. Drouin, G. Foster, P. Isabelle,
R. Kuhn, Y. Normandin, and P. Plamondon. 1995.
French speech recognition in an automatic dictation
system for translators: the transtalk project. In Pro-
ceedings of Eurospeech, pages 193?196, Madrid,
Spain.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
P. F. Brown, S. F. Chen, S. A. Della Pietra, V. J. Della
Pietra, A. S. Kehler, and R. L. Mercer. 1994. Au-
tomatic speech recognition in machine-aided trans-
lation. Computer Speech and Language, 8(3):177?
187, July.
M. Dymetman, J. Brousseau, G. Foster, P. Isabelle,
Y. Normandin, and P. Plamondon. 1994. Towards
an automatic dictation system for translators: the
TransTalk project. In Proceedings of ICSLP-94,
pages 193?196, Yokohama, Japan.
G. Foster, P. Isabelle, and P. Plamondon. 1997. Target-
text mediated interactive machine translation. Ma-
chine Translation, 12(1):175?194.
S. Kanthak and H. Ney. 2004. FSA: An efficient
and flexible C++ toolkit for finite state automata
using on-demand computation. In Proc. of the 42nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 510?517, Barcelona,
Spain, July.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and
H. Ney. 2005. Novel reordering approaches in
phrase-based statistical machine translation. In 43rd
Annual Meeting of the Assoc. for Computational
Linguistics: Proc. Workshop on Building and Using
Parallel Texts: Data-Driven Machine Translation
and Beyond, pages 167?174, Ann Arbor, Michigan,
June.
S. Khadivi, A. Zolnay, and H. Ney. 2005. Automatic
text dictation in computer-assisted translation. In
Interspeech?2005 - Eurospeech, 9th European Con-
ference on Speech Communication and Technology,
pages 2265?2268, Portugal, Lisbon.
K. Knight and Y. Al-Onaizan. 1998. Translation
with finite-state devices. In D. Farwell, L. Gerber,
and E. H. Hovy, editors, AMTA, volume 1529 of
Lecture Notes in Computer Science, pages 421?437.
Springer Verlag.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical ma-
chine translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 295?302, Philadelphia, PA, July.
F. J. Och, R. Zens, and H. Ney. 2003. Efficient search
for interactive statistical machine translation. In
EACL03: 10th Conf. of the Europ. Chapter of the
Association for Computational Linguistics, pages
387?393, Budapest, Hungary, April.
F. J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of the 41th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 160?167, Sapporo,
Japan, July.
K. A. Papineni, S. Roukos, and R. T. Ward. 1997.
Feature-based language understanding. In EU-
ROSPEECH, pages 1435?1438, Rhodes, Greece,
September.
K. A. Papineni, S. Roukos, and R. T. Ward. 1998.
Maximum likelihood and discriminative training
of direct translation models. In Proc. IEEE Int.
Conf. on Acoustics, Speech, and Signal Processing
(ICASSP), volume 1, pages 189?192, Seattle, WA,
May.
M. Paulik, S. Stu?ker, C. Fu?gen, , T. Schultz, T. Schaaf,
and A. Waibel. 2005a. Speech translation enhanced
automatic speech recognition. In Automatic Speech
Recognition and Understanding Workshop (ASRU),
pages 121?126, Puerto Rico, San Juan.
M. Paulik, C. Fu?gen, S. Stu?ker, T. Schultz, T. Schaaf,
and A. Waibel. 2005b. Document driven machine
translation enhanced ASR. In Interspeech?2005 -
Eurospeech, 9th European Conference on Speech
Communication and Technology, pages 2261?2264,
Portugal, Lisbon.
A. Sixtus, S. Molau, S.Kanthak, R. Schlu?ter, and
H. Ney. 2000. Recent improvements of the
RWTH large vocabulary speech recognition system
on spontaneous speech. In Proc. IEEE Int. Conf. on
Acoustics, Speech, and Signal Processing (ICASSP),
pages 1671 ? 1674, Istanbul, Turkey, June.
A. Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proc. of the Int. Conf.
on Speech and Language Processing (ICSLP), vol-
ume 2, pages 901?904, Denver, CO, September.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL),
pages 257?264, Boston, MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system.
In Proceedings of the International Workshop on
Spoken Language Translation (IWSLT), pages 155?
162, Pittsburgh, PA, October.
474
Proceedings of the Workshop on Statistical Machine Translation, pages 15?22,
New York City, June 2006. c?2006 Association for Computational Linguistics
Morpho-syntactic Arabic Preprocessing for Arabic-to-English Statistical
Machine Translation
Anas El Isbihani Shahram Khadivi Oliver Bender
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{isbihani,khadivi,bender,ney}@informatik.rwth-aachen.de
Hermann Ney
Abstract
The Arabic language has far richer sys-
tems of inflection and derivation than En-
glish which has very little morphology.
This morphology difference causes a large
gap between the vocabulary sizes in any
given parallel training corpus. Segmen-
tation of inflected Arabic words is a way
to smooth its highly morphological na-
ture. In this paper, we describe some
statistically and linguistically motivated
methods for Arabic word segmentation.
Then, we show the efficiency of proposed
methods on the Arabic-English BTEC and
NIST tasks.
1 Introduction
Arabic is a highly inflected language compared to
English which has very little morphology. This mor-
phological richness makes statistical machine trans-
lation from Arabic to English a challenging task. A
usual phenomenon in Arabic is the attachment of a
group of words which are semantically dependent on
each other. For instance, prepositions like ?and? and
?then? are usually attached to the next word. This
applies also to the definite article ?the?. In addi-
tion, personal pronouns are attached to the end of
verbs, whereas possessive pronouns are attached to
the end of the previous word, which constitutes the
possessed object. Hence, an Arabic word can be de-
composed into ?prefixes, stem and suffixes?. We re-
strict the set of prefixes and suffixes to those showed
in Table 1 and 2, where each of the prefixes and suf-
fixes has at least one meaning which can be repre-
sented by a single word in the target language. Some
prefixes can be combined. For example the word
wbAlqlm (????AK. ? which means ?and with the pen?)
has a prefix which is a combination of three pre-
fixes, namely w, b and Al. The suffixes we handle
in this paper can not be combined with each other.
Thus, the compound word pattern handled here is
?prefixes-stem-suffix?.
All possible prefix combinations that do not con-
tain Al allow the stem to have a suffix. Note that
there are other suffixes that are not handled here,
such as At ( H@), An ( 	?@) and wn ( 	??) which make
the plural form of a word. The reason why we omit
them is that they do not have their own meaning. The
impact of Arabic morphology is that the vocabulary
size and the number of singletons can be dramati-
cally high, i.e. the Arabic words are not seen often
enough to be learned by statistical machine transla-
tion models. This can lead to an inefficient align-
ment.
In order to deal with this problem and to improve
the performance of statistical machine translation,
each word must be decomposed into its parts. In
(Larkey et al, 2002) it was already shown that word
segmentation for Arabic improves information re-
trieval. In (Lee et al, 2003) a statistical approach
for Arabic word segmentation was presented. It de-
composes each word into a sequence of morphemes
(prefixes-stem-suffixes), where all possible prefixes
and suffixes (not only those we described in Table 1
and 2) are split from the original word. A compa-
rable work was done by (Diab et al, 2004), where
a POS tagging method for Arabic is also discussed.
As we have access to this tool, we test its impact
on the performance of our translation system. In
15
Table 1: Prefixes handled in this work and their meanings.
Prefix ? 	? ? ? H. ?@
Transliteration w f k l b Al
Meaning and and then as, like in order to with, in the
(Habash and Rambow, 2005) a morphology analyzer
was used for the segementation and POS tagging. In
contrast to the methods mentioned above, our seg-
mentation method is unsupervised and rule based.
In this paper we first explain our statistical ma-
chine translation (SMT) system used for testing the
impact of the different segmentation methods, then
we introduce some preprocessing and normalization
tools for Arabic and explain the linguistic motiva-
tion beyond them. Afterwards, we present three
word segmentation methods, a supervised learning
approach, a finite state automaton-based segmenta-
tion, and a frequency-based method. In Section 5,
the experimental results are presented. Finally, the
paper is summarized in Section 6 .
2 Baseline SMT System
In statistical machine translation, we are given a
source language sentence fJ1 = f1 . . . fj . . . fJ ,
which is to be translated into a target language sen-
tence eI1 = e1 . . . ei . . . eI . Among all possible tar-
get language sentences, we will choose the sentence
with the highest probability:
e?I?1 = argmax
I,eI1
{
Pr(eI1|f
J
1 )
} (1)
The posterior probability Pr(eI1|fJ1 ) is modeled di-
rectly using a log-linear combination of several
models (Och and Ney, 2002):
Pr(eI1|f
J
1 ) =
exp
(?M
m=1 ?mhm(e
I
1, f
J
1 )
)
?
e?I
?
1
exp
(?M
m=1 ?mhm(e
?I?
1 , f
J
1 )
)
(2)
The denominator represents a normalization factor
that depends only on the source sentence fJ1 . There-
fore, we can omit it during the search process. As a
decision rule, we obtain:
e?I?1 = argmax
I,eI1
{
M?
m=1
?mhm(e
I
1, f
J
1 )
}
(3)
This approach is a generalization of the source-
channel approach (Brown et al, 1990). It has the
advantage that additional models h(?) can be eas-
ily integrated into the overall system. The model
scaling factors ?M1 are trained with respect to the fi-
nal translation quality measured by an error criterion
(Och, 2003).
We use a state-of-the-art phrase-based translation
system including the following models: an n-gram
language model, a phrase translation model and a
word-based lexicon model. The latter two mod-
els are used for both directions: p(f |e) and p(e|f).
Additionally, we use a word penalty and a phrase
penalty. More details about the baseline system can
be found in (Zens and Ney, 2004; Zens et al, 2005).
3 Preprocessing and Normalization Tools
3.1 Tokenizer
As for other languages, the corpora must be first to-
kenized. Here words and punctuations (except ab-
breviation) must be separated. Another criterion is
that Arabic has some characters that appear only at
the end of a word. We use this criterion to separate
words that are wrongly attached to each other.
3.2 Normalization and Simplification
The Arabic written language does not contain vow-
els, instead diacritics are used to define the pronun-
ciation of a word, where a diacritic is written under
or above each character in the word. Usually these
diacritics are omitted, which increases the ambigu-
ity of a word. In this case, resolving the ambiguity
of a word is only dependent on the context. Some-
times, the authors write a diacritic on a word to help
the reader and give him a hint which word is really
meant. As a result, a single word with the same
meaning can be written in different ways. For exam-
ple $Eb (I. ? ?) can be read1 as sha?ab (Eng. nation)
or sho?ab (Eng. options). If the author wants to give
the reader a hint that the second word is meant, he
1There are other possible pronunciations for the word $Eb
than the two mentioned.
16
Table 2: Suffixes handled in this work and their meanings.
Suffix ?


?


	
G ? 	?? , ?? , A??
Transliteration y ny k kmA, km, kn
Meaning my me you, your (sing.) you, your (pl.)
Suffix A 	K ? A? 	?? , ?? , A??
Transliteration nA h hA hmA, hm, hn
Meaning us, our his, him her them, their
can write $uEb (I. ?

?) or $uEab (I. ?

?). To avoid
this problem we normalize the text by removing all
diacritics.
After segmenting the text, the size of the sen-
tences increases rapidly, where the number of the
stripped article Al is very high. Not every article in
an Arabic sentence matches to an article in the target
language. One of the reasons is that the adjective in
Arabic gets an article if the word it describes is def-
inite. So, if a word has the prefix Al, then its adjec-
tive will also have Al as a prefix. In order to reduce
the sentence size we decide to remove all these arti-
cles that are supposed to be attached to an adjective.
Another way for determiner deletion is described in
(Lee, 2004).
4 Word Segmentation
One way to simplify inflected Arabic text for a SMT
system is to split the words in prefixes, stem and
suffixes. In (Lee et al, 2003), (Diab et al, 2004)
and (Habash and Rambow, 2005) three supervised
segmentation methods are introduced. However, in
these works the impact of the segmentation on the
translation quality is not studied. In the next subsec-
tions we will shortly describe the method of (Diab et
al., 2004). Then we present our unsupervised meth-
ods.
4.1 Supervised Learning Approach (SL)
(Diab et al, 2004) propose solutions to word seg-
mentation and POS Tagging of Arabic text. For the
purpose of training the Arabic TreeBank is used,
which is an Arabic corpus containing news articles
of the newswire agency AFP. In the first step the text
must be transliterated to the Buckwalter translitera-
tion, which is a one-to-one mapping to ASCII char-
acters. In the second step it will be segmented and
tokenized. In the third step a partial lemmatization is
done. Finally a POS tagging is performed. We will
test the impact of the step 3 (segmentation + lemma-
tization) on the translation quality using our phrase
based system described in Section 2.
4.2 Frequency-Based Approach (FB)
We provide a set of all prefixes and suffixes and
their possible combinations. Based on this set, we
may have different splitting points for a given com-
pound word. We decide whether and where to split
the composite word based on the frequency of dif-
ferent resulting stems and on the frequency of the
compound word, e.g. if the compound word has a
higher frequency than all possible stems, it will not
be split. This simple heuristic harmonizes the cor-
pus by reducing the size of vocabulary, singletons
and also unseen words from the test corpus. This
method is very similar to the method used for split-
ting German compound words (Koehn and Knight,
2003).
4.3 Finite State Automaton-Based Approach
(FSA)
To segment Arabic words into prefixes, stem and one
suffix, we implemented two finite state automata.
One for stripping the prefixes and the other for the
suffixes. Then, we append the suffix automaton to
the other one for stripping prefixes. Figure 1 shows
the finite state automaton for stripping all possible
prefix combinations. We add the prefix s (?), which
changes the verb tense to the future, to the set of
prefixes which must be stripped (see table 1). This
prefix can only be combined with w and f. Our mo-
tivation is that the future tense in English is built by
adding the separate word ?will?.
The automaton showed in Figure 1 consists of the
following states:
? S: the starting point of the automaton.
? E: tne end state, which can only be achieved if
17
S K
AL
B
L
WF
C
E
Figure 1: Finite state automaton for stripping pre-
fixes off Arabic words.
the resulting stem exists already in the text.
? WF: is achieved if the word begins with w or f.
? And the states , K, L, B and AL are achieved if
the word begins with s, k, l, b and Al, respec-
tively.
To minimize the number of wrong segmentations,
we restricted the transition from one state to the
other to the condition that the produced stem occurs
at least one time in the corpus. To ensure that most
compound words are recognized and segmented, we
run the segmenter itteratively, where after each it-
eration the newly generated words are added to the
vocabulary. This will enable recognizing new com-
pound words in the next iteration. Experiments
showed that running the segmenter twice is suffi-
cient and in higher iterations most of the added seg-
mentations are wrong.
4.4 Improved Finite State Automaton-Based
Approach (IFSA)
Although we restricted the finite state segmenter in
such a way that words will be segmented only if the
yielded stem already exists in the corpus, we still get
some wrongly segmented words. Thus, some new
stems, which do not make sense in Arabic, occur
in the segmented text. Another problem is that the
finite state segmenter does not care about ambigui-
ties and splits everything it recognizes. For example
let us examine the word frd (XQ 	?). In one case, the
character f is an original one and therefore can not
be segmented. In this case the word means ?per-
son?. In the other case, the word can be segmented
to ?f rd? (which means ?and then he answers? or
?and then an answer?). If the words Alfrd, frd and
rd(XQ 	? , XQ 	?? @ and XP) occur in the corpus, then the fi-
nite state segmenter will transform the Alfrd (which
means ?the person?) to Al f rd (which can be trans-
lated to ?the and then he answers?). Thus the mean-
ing of the original word is distorted. To solve all
these problems, we improved the last approach in a
way that prefixes and suffixes are recognized simul-
taneously. The segmentation of the ambiguous word
will be avoided. In doing that, we intend to postpone
resolving such ambiguities to our SMT system.
The question now is how can we avoid the seg-
mentation of ambiguous words. To do this, it is suf-
ficient to find a word that contains the prefix as an
original character. In the last example the word Al-
frd contains the prefix f as an original character and
therefore only Al can be stripped off the word. The
next question we can ask is, how can we decide if a
character belongs to the word or is a prefix. We can
extract this information using the invalid prefix com-
binations. For example Al is always the last prefix
that can occur. Therefore all characters that occur in
a word after Al are original characters. This method
can be applied for all invalid combinations to extract
new rules to decide whether a character in a word is
an original one or not.
On the other side, all suffixes we handle in this
work are pronouns. Therefore it is not possible to
combine them as a suffix. We use this fact to make
a decision whether the end characters in a word are
original or can be stripped. For example the word
trkhm (???QK) means ?he lets them?. If we suppose
that hm is a suffix and therefore must be stripped,
then we can conclude that k is an original character
and not a suffix. In this way we are able to extract
from the corpus itself decisions whether and how a
word can be segmented.
In order to implement these changes the original
automaton was modified. Instead of splitting a word
we mark it with some properties which corespond
to the states traversed untill the end state. On the
18
other side, we use the technique described above to
generate negative properties which avoid the corre-
sponding kind of splitting. If a property and its nega-
tion belong to the same word then the property is re-
moved and only the negation is considered. At the
end each word is split corresponding to the proper-
ties it is marked with.
5 Experimental Results
5.1 Corpus Statistics
The experiments were carried out on two tasks: the
corpora of the Arabic-English NIST task, which
contain news articles and UN reports, and the
Arabic-English corpus of the Basic Travel Expres-
sion Corpus (BTEC) task, which consists of typi-
cal travel domain phrases (Takezawa et al, 2002).
The corpus statistics of the NIST and BTEC corpora
are shown in Table 3 and 5. The statistics of the
news part of NIST corpus, consisting of the Ummah,
ATB, ANEWS1 and eTIRR corpora, is shown in Ta-
ble 4. In the NIST task, we make use of the NIST
2002 evaluation set as a development set and NIST
2004 evaluation set as a test set. Because the test
set contains four references for each senence we de-
cided to use only the first four references of the de-
velopment set for the optimization and evaluation.
In the BTEC task, C-Star?03 and IWSLT?04 copora
are considered as development and test sets, respec-
tively.
5.2 Evaluation Metrics
The commonly used criteria to evaluate the trans-
lation results in the machine translation commu-
nity are: WER (word error rate), PER (position-
independent word error rate), BLEU (Papineni et
al., 2002), and NIST (Doddington, 2002). The four
criteria are computed with respect to multiple ref-
erences. The number of reference translations per
source sentence varies from 4 to 16 references. The
evaluation is case-insensitive for BTEC and case-
sensitive for NIST task. As the BLEU and NIST
scores measure accuracy, higher scores are better.
5.3 Translation Results
To study the impact of different segmentation meth-
ods on the translation quality, we apply different
word segmentation methods to the Arabic part of the
BTEC and NIST corpora. Then, we make use of the
phrase-based machine translation system to translate
the development and test sets for each task.
First, we discuss the experimental results on the
BTEC task. In Table 6, the translation results on the
BTEC corpus are shown. The first row of the table is
the baseline system where none of the segmentation
methods is used. All segmentation methods improve
the baseline system, except the SL segmentation
method on the development corpus. The best per-
forming segmentation method is IFSA which gener-
ates the best translation results based on all evalua-
tion criteria, and it is consistent over both develop-
ment and evaluation sets. As we see, the segmen-
tation of Arabic words has a noticeable impact in
improving the translation quality on a small corpus.
To study the impact of word segmentation meth-
ods on a large task, we conduct two sets of experi-
ments on the NIST task using two different amounts
of the training corpus: only news corpora, and full
corpus. In Table 7, the translation results on the
NIST task are shown when just the news corpora
were used to train the machine translation models.
As the results show, except for the FB method, all
segmentation methods improve the baseline system.
For the NIST task, the SL method outperforms the
other segmentation methods, while it did not achieve
good results when comparing to the other methods
in the BTEC task.
We see that the SL, FSA and IFSA segmentation
methods consistently improve the translation results
in the BTEC and NIST tasks, but the FB method
failed on the NIST task, which has a larger training
corpus . The next step is to study the impact of the
segmentation methods on a very large task, the NIST
full corpus. Unfortunately, the SL method failed on
segmenting the large UN corpus, due to the large
processing time that it needs. Due to the negative
results of the FB method on the NIST news corpora,
and very similar results for FSA and IFSA, we were
interested to test the impact of IFSA on the NIST
full corpus. In Table 8, the translation results of the
baseline system and IFSA segmentation method for
the NIST full corpus are depicted. As it is shown in
table, the IFSA method slightly improves the trans-
lation results in the development and test sets.
The IFSA segmentation method generates the
best results among our proposed methods. It
acheives consistent improvements in all three tasks
over the baseline system. It also outperforms the SL
19
Table 3: BTEC corpus statistics, where the Arabic part is tokenized and segmented with the SL, FB, FSA
and the IFSA methods.
ARABIC ENGLISH
TOKENIZED SL FB FSA IFSA
Train: Sentences 20K
Running Words 159K 176.2K 185.5K 190.3K 189.1K 189K
Vocabulary 18,149 14,321 11,235 11,736 12,874 7,162
Dev: Sentences 506
Running Words 3,161 3,421 3,549 3,759 3,715 5,005
OOVs (Running Words) 163 129 149 98 118 NA
Test: Sentences 500
Running Words 3,240 3,578 3,675 3,813 3,778 4,986
OOVs (Running Words) 186 120 156 92 115 NA
Table 4: Corpus statistics for the news part of the NIST task, where the Arabic part is tokenized and seg-
mented with SL, FB, FSA and IFSA methods.
ARABIC ENGLISH
TOKENIZED SL FB FSA IFSA
Train: Sentences 284.9K
Running Words 8.9M 9.7M 12.2M 10.9M 10.9M 10.2M
Vocabulary 118.7K 90.5K 43.1K 68.4K 62.2K 56.1K
Dev: Sentences 1,043
Running Words 27.7K 29.1K 37.3K 34.4K 33.5K 33K
OOVs (Running Words) 714 558 396 515 486 NA
Test: Sentences 1,353
Running Words 37.9K 41.7K 52.6K 48.6K 48.3K 48.3K
OOVs (Running Words) 1,298 1,027 612 806 660 NA
segmentation on the BTEC task.
Although the SL method outperforms the IFSA
method on the NIST tasks, the IFSA segmentation
method has a few notable advantages over the SL
system. First, it is consistent in improving the base-
line system over the three tasks. But, the SL method
failed in improving the BTEC development corpus.
Second, it is fast and robust, and capable of being
applied to the large corpora. Finally, it employs an
unsupervised learning method, therefore can easily
cope with a new task or corpus.
We observe that the relative improvement over
the baseline system is decreased by increasing the
size of the training corpus. This is a natural effect
of increasing the size of the training corpus. As
the larger corpus provides higher probability to have
more samples per word, this means higher chance
to learn the translation of a word in different con-
texts. Therefore, larger training corpus makes a bet-
ter translation system, i.e. a better baseline, then it
would be harder to outperform this better system.
Using the same reasoning, we can realize why the
FB method achieves good results on the BTEC task,
but not on the NIST task. By increasing the size
of the training corpus, the FB method tends to seg-
ment words more than the IFSA method. This over-
segmentation can be compensated by using longer
phrases during the translation, in order to consider
the same context compared to the non-segmented
corpus. Then, it would be harder for a phrase-based
machine translation system to learn the translation
of a word (stem) in different contexts.
6 Conclusion
We presented three methods to segment Arabic
words: a supervised learning approach, a frequency-
20
Table 5: NIST task corpus statistics, where the Arabic part is tokenized and segmented with the IFSA
method.
ARABIC ENGLISH
TOKENIZED IFSA
Train: Sentences 8.5M
Running Words 260.5M 316.8M 279.2M
Vocabulary 510.3K 411.2K 301.2K
Dev: Sentences 1043
Running Words 30.2K 33.3K 33K
OOVs (Running Words) 809 399 NA
Test: Sentences 1353
Running Words 40K 47.9K 48.3K
OOVs (Running Words) 871 505 NA
Table 6: Case insensitive evaluation results for translating the development and test data of BTEC task after
performing divers preprocessing.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 21.4 24.6 63.9 10.0 23.5 27.2 58.1 9.6
SL Segmenter 21.2 24.4 62.5 9.7 23.4 27.4 59.2 9.7
FB Segmenter 20.9 24.4 65.3 10.1 22.1 25.8 59.8 9.7
FSA Segmenter 20.1 23.4 64.8 10.2 21.1 25.2 61.3 10.2
IFSA Segmenter 20.0 23.3 65.0 10.4 21.2 25.3 61.3 10.2
based approach and a finite state automaton-based
approach. We explained that the best of our pro-
posed methods, the improved finite state automaton,
has three advantages over the state-of-the-art Arabic
word segmentation method (Diab, 2000), supervised
learning. They are: consistency in improving the
baselines system over different tasks, its capability
to be efficiently applied on the large corpora, and its
ability to cope with different tasks.
7 Acknowledgment
This material is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
Any opinions, findings and conclusions or recom-
mendations expressed in this material are those of
the author(s) and do not necessarily reflect the views
of the Defense Advanced Research Projects Agency
(DARPA).
References
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and
P. S. Roossin. 1990. A statistical approach to machine
translation. Computational Linguistics, 16(2):79?85,
June.
M. Diab, K. Hacioglu, and D. Jurafsky. 2004. Automatic
tagging of arabic text: From raw text to base phrase
chunks. In D. M. Susan Dumais and S. Roukos, edi-
tors, HLT-NAACL 2004: Short Papers, Boston, Mas-
sachusetts, USA, May 2 - May 7. Association for
Computational Linguistics.
M. Diab. 2000. An unsupervised method for multi-
lingual word sense tagging using parallel corpora: A
preliminary investigation. In ACL-2000 Workshop on
Word Senses and Multilinguality, pages 1?9, Hong
Kong, October.
G. Doddington. 2002. Automatic evaluation of machine
translation quality using n-gram co-occurrence statis-
tics. In Proc. ARPA Workshop on Human Language
Technology.
21
Table 7: Case sensitive evaluation results for translating the development and test data of the news part of
the NIST task after performing divers preprocessing.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 43.7 56.4 43.6 9.9 46.1 58.0 37.4 9.1
SL Segmenter 42.0 54.7 45.1 10.2 44.3 56.3 39.9 9.6
FB Segmenter 43.4 56.1 43.2 9.8 45.6 57.8 37.2 9.2
FSA Segmenter 42.9 55.7 43.7 9.9 44.8 56.9 38.7 9.4
IFSA Segmenter 42.6 55.0 44.6 9.9 44.5 56.6 38.8 9.4
Table 8: Case-sensitive evaluation results for translating development and test data of NIST task.
Dev Test
mPER mWER BLEU NIST mPER mWER BLEU NIST
[%] [%] [%] [%] [%] [%]
Non-Segmented Data 41.5 53.5 46.4 10.3 42.5 53.9 42.6 10.0
IFSA Segmenter 41.1 53.2 46.7 10.2 42.1 53.6 43.4 10.1
N. Habash and O. Rambow. 2005. Arabic tokeniza-
tion, part-of-speech tagging and morphological dis-
ambiguation in one fell swoop. In Proc. of the 43rd
Annual Meeting of the Association for Computational
Linguistics (ACL?05), pages 573?580, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
P. Koehn and K. Knight. 2003. Empirical methods for
compound splitting. In Proc. 10th Conf. of the Europ.
Chapter of the Assoc. for Computational Linguistics
(EACL), pages 347?354, Budapest, Hungary, April.
L. S. Larkey, L. Ballesteros, and M. E. Connell. 2002.
Improving stemming for arabic information retrieval:
light stemming and co-occurrence analysis. In Proc.
of the 25th annual of the international Association
for Computing Machinery Special Interest Group on
Information Retrieval (ACM SIGIR), pages 275?282,
New York, NY, USA. ACM Press.
Y. S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word seg-
mentation. In E. Hinrichs and D. Roth, editors, Proc.
of the 41st Annual Meeting of the Association for Com-
putational Linguistics.
Y. S. Lee. 2004. Morphological analysis for statisti-
cal machine translation. In D. M. Susan Dumais and
S. Roukos, editors, HLT-NAACL 2004: Short Papers,
pages 57?60, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
F. J. Och and H. Ney. 2002. Discriminative training
and maximum entropy models for statistical machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 295?302, Philadelphia, PA, July.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. of the 41th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 160?167, Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 311?318, Philadelphia, PA, July.
T. Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, and
S. Yamamoto. 2002. Toward a broad-coverage bilin-
gual corpus for speech translation of travel conver-
sations in the real world. In Proc. of the Third Int.
Conf. on Language Resources and Evaluation (LREC),
pages 147?152, Las Palmas, Spain, May.
R. Zens and H. Ney. 2004. Improvements in phrase-
based statistical machine translation. In Proc. of the
Human Language Technology Conf. (HLT-NAACL),
pages 257?264, Boston, MA, May.
R. Zens, O. Bender, S. Hasan, S. Khadivi, E. Matusov,
J. Xu, Y. Zhang, and H. Ney. 2005. The RWTH
phrase-based statistical machine translation system. In
Proceedings of the International Workshop on Spoken
Language Translation (IWSLT), pages 155?162, Pitts-
burgh, PA, October.
22
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 318?322,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Using Context Vectors in Improving a Machine Translation System 
with Bridge Language 
Samira Tofighi Zahabi       Somayeh Bakhshaei       Shahram Khadivi 
Human Language Technology Lab 
Amirkabir University of Technology 
Tehran, Iran 
{Samiratofighi,bakhshaei,khadivi}@aut.ac.ir 
 
 
 
Abstract 
Mapping phrases between languages as 
translation of each other by using an intermediate 
language (pivot language) may generate 
translation pairs that are wrong. Since a word or a 
phrase has different meanings in different 
contexts, we should map source and target 
phrases in an intelligent way. We propose a 
pruning method based on the context vectors to 
remove those phrase pairs that connect to each 
other by a polysemous pivot phrase or by weak 
translations. We use context vectors to implicitly 
disambiguate the phrase senses and to recognize 
irrelevant phrase translation pairs. 
Using the proposed method a relative 
improvement of 2.8 percent in terms of BLEU 
score is achieved. 
 
1 Introduction 
Parallel corpora as an important component of a 
statistical machine translation system are 
unfortunately unavailable for all pairs of 
languages, particularly in low resource languages 
and also producing it consumes time and cost. 
So, new ideas have been developed about how to 
make a MT system which has lower dependency 
on parallel data like using comparable corpora 
for improving performance of a MT system with 
small parallel corpora or making a MT system 
without parallel corpora. Comparable corpora 
have segments with the same translations. These 
segments might be in the form of words, phrases 
or sentences. So, this extracted information can 
be added to the parallel corpus or might be used 
for adaption of the language model or translation 
model. 
Comparable corpora are easily available 
resources. All texts that are about the same topic 
can be considered as comparable corpora. 
Another idea for solving the scarce resource 
problem is to use a high resource language as a 
pivot to bridge between source and target 
languages. In this paper we use the bridge 
technique to make a source-target system and we 
will prune the phrase table of this system. In 
Section 2, the related works of the bridge 
approach are considered, in Section 3 the 
proposed approach will be explained and it will 
be shown how to prune the phrase table using 
context vectors, and experiments on German-
English-Farsi systems will be presented in 
Section 4. 
2 Related Works 
There are different strategies of bridge 
techniques to make a MT system. The simplest 
way is to build two MT systems in two sides: one 
system is source-pivot and the other is pivot-
target system, then in the translation stage the 
output of the first system is given to the second 
system as an input and the output of the second 
system is the final result. The disadvantage of 
this method is its time consuming translation 
process, since until the first system?s output is 
not ready; the second system cannot start the 
translation process. This method is called 
cascading of two translation systems. 
In the other approach the target side of the 
training corpus of the source-pivot system is 
given to the pivot-target system as its input. The 
output of the pivot-target system is parallel with 
the source side of the training corpus of the 
source-pivot system. A source-to-target system 
can be built by using this noisy parallel corpus 
which in it each source sentence is directly 
translated to a target sentence. This method is 
called the pseudo corpus approach. 
  Another way is combining the phrase tables of 
the source-pivot and pivot-target systems to 
directly make a source-target phrase table. This 
combination is done if the pivot phrase is 
318
identical in both phrase tables. Since one phrase 
has many translations in the other language, a 
large phrase table will be produced. This method 
is called combination of phrase tables approach. 
Since in the bridge language approach two 
translation systems are used to make a final 
translation system, the errors of these two 
translation systems will affect the final output. 
Therefore in order to decrease the propagation of 
these errors, a language should be chosen as 
pivot which its structure is similar to the source 
and target languages. But even by choosing a 
good language as pivot there are some other 
errors that should be handled or decreased such 
as the errors of ploysemous words and etc. 
For making a MT system using pivot language 
several ideas have been proposed. Wu and Wang 
(2009) suggested a cascading method which is 
explained in Section 1. 
Bertoldi (2008) proposed his method in 
bridging at translation time and bridging at 
training time by using the cascading method and 
the combination of phrase tables. 
Bakhshaei (2010) used the combination of 
phrase tables of source-pivot and pivot-target 
systems and produced a phrase table for the 
source-target system. 
Paul (2009) did several experiments to show 
the effect of pivot language in the final 
translation system. He showed that in some cases 
if training data is small the pivot should be more 
similar to the source language, and if training 
data is large the pivot should be more similar to 
the target language. In Addition, it is more 
suitable to use a pivot language that its structure 
is similar to both of source and target languages. 
Saralegi (2011) showed that there is not 
transitive property between three languages. So 
many of the translations produced in the final 
phrase table might be wrong. Therefore for 
pruning wrong and weak phrases in the phrase 
table two methods have been used. One method 
is based on the structure of source dictionaries 
and the other is based on distributional similarity.  
Rapp (1995) suggested his idea about the 
usage of context vectors in order to find the 
words that are the translation of each other in 
comparable corpora. 
In this paper the combination of phrase tables 
approach is used to make a source-target system. 
We have created a base source-target system just 
similar to previous works. But the contribution of 
our work compared to other works is that here 
we decrease the size of the produced phrase table 
and improve the performance of the system. Our 
pruning method is different from the method that 
Saralegi (2011) has used. He has pruned the 
phrase table by computing distributional 
similarity from comparable corpora or by the 
structure of source dictionaries. Here we use 
context vectors to determine the concept of 
phrases and we use the pivot language to 
compare source and target vectors. 
3 Approach 
For the purpose of showing how to create a 
pruned phrase table, in Section 3.1 we will 
explain how to create a simple source-to-target 
system. In Section 3.2 we will explain how to 
remove wrong and weak translations in the 
pruning step. Figure 1 shows the pseudo code of 
the proposed algorithm. 
In the following we have used these 
abbreviations: f, e stands for source and target 
phrases. pl, src-pl, pl-trg, src-trg respectively 
stand for pivot phrase, source-pivot phrase table, 
pivot-target phrase table and source-target 
phrase table.  
3.1 Creating source-to-target system 
At first, we assume that there is transitive 
property between three languages in order to 
make a base system, and then we will show in 
different ways that there is not transitive property 
between three languages. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. Pseudo code for proposed method 
 
for each source phrase f 
     pls = {translations of f in src-pl } 
     for each pl in pls 
        Es ={ translations of pl in pl-trg } 
        for each e in Es 
            p(e|f) =p(pl|f)*p(e|pl) and add (e,f) to src-
trg 
create source-to-destination system with src-trg 
create context vector V for each source phrase f 
using source corpora 
create context vector V? for each target phrase e 
using target corpora 
convert Vs to pivot language vectors using src-pl 
system 
convert V? s to pivot language vectors using pl-trg 
system 
for each f in src-trg 
Es = {translations of f in src-trg} 
For each e in Es calculate similarity of its context 
vector with f context vector 
     Select k top similar as translations of f 
      delete other translations of f in src-trg 
319
For each phrase f in src-pl phrase table, all the 
phrases pl which are translations of f, are 
considered. Then for each of these pls every 
phrase e from the pl-trg phrase table that are 
translations of pl, are found. Finally f is mapped 
to all of these es in the new src-trg phrase table. 
The probability of these new phrases is 
calculated using equation (1) through the 
algorithm that is shown in figure 1. 
( | ) ( | ) ( | )p e f p pl f p e pl= ?            (1)  
A simple src-trg phrase table is created by 
this approach. Pl phrases might be ploysemous 
and produce target phrases that have different 
meaning in comparison to each other. The 
concept of some of these target phrases are 
similar to the corresponding source phrase and 
the concept of others are irrelevant to the source 
phrase.  
The language model can ignore some of these 
wrong translations. But it cannot ignore these 
translations if they have high probability. 
Since the probability of translations is 
calculated using equation (1), therefore wrong 
translations have high probability in three cases: 
first when p(pl|f) is high, second when p(e|pl) is 
high and third when p(pl|f) and p(e|pl) are high. 
In the first case pl might be a good translation 
for f and refers to concept c, but pl and e refer to 
concept ?? so mapping f to e as a translation of 
each other is wrong. The second case is similar 
to the first case but e might be a good translation 
for pl. The third case is also similar to the first 
case, but pl is a good translation for both f and e. 
The pruning method that is explained in 
Section 3.2, tries to find these translations and 
delete them from the src-trg phrase table. 
3.2 Pruning method 
To determine the concept of each phrase (p) in 
language L at first a vector (V) with length N is 
created. Each element of V is set to zero and N is 
the number of unique phrases in language L. 
In the next step all sentences of the corpus in 
language L are analyzed. For each phrase p if p 
occurs with ?? in the same sentence the element 
of context vector ? that corresponds to ?? is 
pulsed by 1. This way of calculating context 
vectors is similar to Rapp (1999), but here the 
window length of phrase co-occurrence is 
considered a sentence. Two phrases are 
considered as co-occurrence if they occur in the 
same sentence. The distance between them does 
not matter. In other words phrase ? might be at 
the beginning of the sentence while ?? being at 
the end of the sentence, but they are considered 
as co-occurrence phrases. 
    For each source (target) phrase its context 
vector should be calculated within the source 
(target) corpus as shown in figure 1. 
The number of unique phrases in the source 
(target) language is equal to the number of 
unique source (target) phrases in the src-trg 
phrase table that are created in the last Section.  
So, the length of source context vectors is m 
and the length of target context vectors is n. 
These variables (m and n) might not be equal. In 
addition to this, source vectors and target vectors 
are in two different languages, so they are not 
comparable. 
One method to translate source context vectors 
to target context vectors is using an additional 
source-target dictionary. But instead here, source 
and target context vectors are translated to pivot 
context vectors. In other words if source context 
vectors have length m and target context vectors 
have length n, they are converted to pivot 
context vectors with length z. The variable z is 
the number of unique pivot phrases in src-pl or 
pl-trg phrase tables.  
To map the source context vector 
?(?1, ?2, ? , ??) to the pivot context vector, we 
use a fixed size vector ?1
?. Elements of vector 
?1
? = (?1,?2, ? , ??) are the unique phrases 
extracted from src-pl or pl-trg phrase tables. 
?1
? = (?1,?2, ? , ??) = (0, 0, ? , 0) 
 In the first step ??s are set to 0. For each 
element, ??, of vector S if ?? > 0 it will be 
translated to k pivot phrases. These phrases are 
the output of k-best translations of ?? by using 
the src-pl phrase table.  
{ }
 
1 1 2( , ,..., )
phrase ta
k
e
i
b
k
l
s V v v v
?
? ? ? ?=
src pl
?
 
For each element ??of ?1
?? its corresponding 
element ? of ?1
? which are equal, will be found, 
then the amount of ? will be increased by ??. 
? ??  ?  ?1
?? ???? (? ?  ?1
?) ? ? = ?? 
???(?) ? ???(?) + ?? 
Using K-best translations as middle phrases is 
for reducing the effect of translation errors that 
cause wrong concepts. This work is done for 
each target context vector. Source and target 
context vectors will be mapped to identical 
length vectors and are also in the same language 
(pivot language).Now source and target context 
vectors are comparable, so with a simple 
similarity metric their similarity can be 
calculated. 
Here we use cosine similarity. The similarity 
between each source context vector and each 
320
target context vector that are translations of the 
source phrase in src-trg, are calculated. For 
each source phrase, the N-most similar target 
phrases are kept as translations of the source 
phrase. These translations are also similar in 
context. Therefore this pruning method deletes 
irrelevant translations form the src-trg phrase 
table. The size of the phrase table is decreased 
very much and the system performance is 
increased. Reduction of the phrase table size is 
considerable while its performance is increased. 
4 Experiments 
In this work, we try to make a German-Farsi 
system without using parallel corpora. We use 
English language as a bridge between German 
and Farsi languages because English language is 
a high resource language and parallel corpora of 
German-English and English-Farsi are available. 
We use Moses0F1 (Koehn et al, 2007) as the MT 
decoder and IRSTLM1F2 tools for making the 
language model. Table 1 shows the statistics of 
the corpora that we have used in our 
experiments. The German-English corpus is from 
Verbmobil project (Ney et al, 2000). We 
manually translate 22K English sentences to 
Farsi to build a small Farsi-English-German 
corpus. Therefore, we have a small English-
German corpus as well. 
With the German-English parallel corpus and 
an additional German-English dictionary with 
118480 entries we have made a German-English 
(De-En) system and with English-Farsi parallel 
corpus we have made a German-Farsi (En-Fa) 
system. The BLEU score of these systems are 
shown in Table 1. 
Now, we create a translation system by 
combining phrase tables of De-En and En-Fa 
systems. Details of creating the source-target 
system are explained in Section 3.1. The size of 
this phrase table is very large because of 
ploysemous and some weak translations.  
 
 Sentences BLEU 
German-English 58,073 40.1 
English-Farsi 22,000  31.6 
Table 1. Information of two parallel systems that 
are used in our experiments. 
 
The size of the phrase table is about 55.7 MB. 
Then, we apply the pruning method that we 
                                                          
1Available under the LGPL from 
http://sourceforge.net/projects/mosesdecoder/ 
2Available under the LGPL from 
http://hlt.fbk.eu/en/irstlm 
explained in Section 3.2. With this method only 
the phrases are kept that their context vectors are 
similar to each other. For each source phrase the 
35-most similar target translations are kept. The 
number of phrases in the phrase table is 
decreased dramatically while the performance of 
the system is increased by 2.8 percent BLEU. 
The results of these experiments are shown in 
Table 2. The last row in this table is the result of 
using small parallel corpus to build German-
Farsi system. We observe that the pruning 
method has gain better results compared to the 
system trained on the parallel corpus. This is 
maybe because of some translations that are 
made in the parallel system and do not have 
enough training data and their probabilities are 
not precise. But when we use context vectors to 
measure the contextual similarity of phrases and 
their translations, the impact of these training 
samples are decreased. In Table 3, two wrong 
phrase pairs that pruning method has removed 
them are shown. 
 
 BLEU # of phrases 
Base bridge system 25.1   500,534 
Pruned system 27.9   26,911 
Parallel system 27.6   348,662 
Table 2. The MT results of the base system, the 
pruned system and the parallel system. 
 
German phrase Wrong 
translation 
Correct 
translation 
vorschlagen , wir ??????? ?????? ?? ???? 
um neun Uhr 
morgens 
???? ?? ??? ?? 
Table 3. Sample wrong translations that the 
prunning method removed them. 
 
In Table 4, we extend the experiments with 
two other methods to build German- 
Farsi system using English as bridging language. 
We see that the proposed method obtains 
competitive result with the pseudo parallel 
method. 
 
System BLEU size (MB) 
Phrase tables combination 25.1 55.7  
Cascade method 25.2 NA 
Pseudo parallel corpus  28.2 73.2  
Phrase tables comb.+prune 27.9 3.0  
Table 4. Performance results of different ways of 
bridging 
 
321
Now, we run a series of significance tests to 
measure the superiority of each method. In the 
first significance test, we set the pruned system 
as our base system and we compare the result of 
the pseudo parallel corpus system with it, the 
significance level is 72%. For another 
significance test we set the combined phrase 
table system without pruning as our base system 
and we compare the result of the pruned system 
with it, the significance level is 100%. In the last 
significance test we set the combined phrase 
table system without pruning as our base system 
and we compare the result of the pseudo system 
with it, the significance level is 99%. Therefore, 
we can conclude the proposed method obtains 
the best results and its difference with pseudo 
parallel corpus method is not significant. 
5 Conclusion and future work 
With increasing the size of the phrase table, the 
MT system performance will not necessarily 
increase. Maybe there are wrong translations 
with high probability which the language model 
cannot remove them from the best translations. 
By removing these translation pairs, the 
produced phrase table will be more consistent, 
and irrelevant words or phrases are much less. In 
addition, the performance of the system will be 
increased by about 2.8% BLEU. 
In the future work, we investigate how to use the 
word alignments of the source-to-pivot and 
pivot-to-target systems to better recognize good 
translation pairs. 
References  
Somayeh Bakhshaei, Shahram Khadivi, and Noushin 
Riahi. 2010. Farsi-German statistical machine 
translation through bridge language. 
Telecommunications (IST) 5th International 
Symposium on, pages 557-561. 
Nicola Bertoldi, Madalina Barbaiani, Marcello 
Federico, and Roldano Cattoni. 2008. Phrase-
Based Statistical Machine Translation with Pivot 
Language. In Proc. Of IWSLT, pages 143-149, 
Hawaii, USA. 
Philipp Koehn. 2004. Statistical Significance Tests for 
Machine Translation Evaluation. In proc. of 
EMNLP, pages 388-395, Barcelona, Spain. 
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris 
C. Burch, Marcello Federico, Nicola Bertoldi, 
Brooke Cowan, WadeShen, Christine Moran, 
RichardZens, Chris Dyer, Ondrei Bojar, Alexandra 
Constantin, and Evan Herbst. 2007. Moses: Open 
source toolkit for statistical machine translation. In 
Proc. of ACL Demonstration Session, pages 177?
180, Prague. 
Hermann Ney, Franz. J. Och, Stephan Vogel. 2000. 
Statistical Translation of Spoken Dialogues in the 
Verbmobil System. In Workshop on Multi Lingual 
Speech Communication, pages 69-74. 
Michael Paul, Hirofumi Yamamoto, Eiichiro Sumita, 
and Satoshi  Nakamura. 2009. On the Importance 
of Pivot Language Selection for Statistical 
Machine Translation. In proc. Of NAACL HLT, 
pages 221-224, Boulder, Colorado. 
Reinhard Rapp. 1995. Identifying Word Translations 
in Non-Parallel Texts. In proc. Of ACL, pages 320-
322, Stroudsburg, PA, USA. 
Reinhard Rapp. 1999. Automatic Identification of 
Word Translations from Unrelated English and 
German Corpora. In Proc. Of ACL, pages 519-525, 
Stroudsburg, PA, USA. 
Xabeir Saralegi, Iker Manterola, and Inaki S. Vicente, 
2011.Analyzing Methods for Improving Precision 
of Pivot Based Bilingual Dictionaries. In proc. of 
the EMNLP, pages 846-856, Edinburgh, Scotland. 
Masao Utiyama and Hitoshi Isahara. 2007. A 
comparison of pivot methods for phrase-based 
SMT. InProc. of HLT, pages 484-491, New York, 
US. 
Hua Wu and Haifeng Wang. 2007. Pivot Language 
Approach for Phrase-Based SMT. In Proc. of ACL, 
pages 856-863, Prague, Czech Republic. 
 
322
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 18?26,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
WordNet Based Features for Predicting Brain 
Activity associated with meanings of nouns
Ahmad Babaeian Jelodar, Mehrdad Alizadeh, and Shahram Khadivi
Computer Engineering Department, Amirkabir University of Technology
424 Hafez Avenue, Tehran, Iran
{ahmadb_jelodar, mehr.alizadeh, khadivi}@aut.ac.ir
Abstract
Different studies have been conducted for 
predicting human brain activity associated 
with the semantics of nouns. Corpus based 
approaches have been used for deriving fea-
ture vectors of concrete nouns, to model the 
brain activity associated with that noun. In 
this paper a computational model is proposed 
in which, the feature vectors for each concrete 
noun is computed by the WordNet similarity 
of that noun with the 25 sensory-motor verbs 
suggested by psychologists. The feature vec-
tors are used for training a linear model to 
predict functional MRI images of the brain as-
sociated with nouns. The WordNet extracted 
features are also combined with corpus based 
semantic features of the nouns. The combined 
features give better results in predicting hu-
man brain activity related to concrete nouns.
1 Introduction
The study of human brain function has received 
great attention in recent years from the advent of 
functional Magnetic Resonance Imaging (fMRI). 
fMRI is a 3D imaging method, that gives the abili-
ty to perceive brain activity in human subjects. A 
three dimensional fMRI image contains approx-
imately 15000 voxels (3D pixels). Since its advent, 
fMRI has been used to conduct hundreds of studies 
that identify specific regions of the brain that are 
activated on average when a human performs a 
particular cognitive function (e.g., reading, mental 
imagery). A great body of these publications show 
that averaging together fMRI data collected over 
multiple time intervals, while the subject responds 
to some kind of repeated stimuli (reading words), 
can present descriptive statistics of brain activity 
(Mitchell et al, 2004).
Conceptual meanings of different words and pic-
tures trigger different brain activity. The represen-
tation of conceptual knowledge in the human brain 
has been studied by different science communities 
such as psychologists, neuroscientists, linguists, 
and computational linguists. Some of these ap-
proaches focus on visual features of picture stimuli 
to analyze fMRI activation associated with viewing 
the picture (O?Toole et al 2005) (Hardoon et al, 
2007). Recent work (Kay et al, 2008) has shown 
that it is possible to predict aspects of fMRI activa-
tion based on visual features of arbitrary scenes 
and to use this predicted activation to identify 
which of a set of candidate scenes an individual is 
viewing. Studies of neural representations in the 
brain have mostly focused on just cataloging the 
patterns of fMRI activity associated with specific 
categories of words. Mitchell et alpresent a ma-
chine learning approach that is able to predict the 
fMRI activity for arbitrary words (Mitchell et al, 
2008). 
In this paper a computational model similar to 
the computational model in (Mitchell et al, 2008) 
is proposed for predicting the neural activation of a 
given stimulus word. Mitchell et alperforms pre-
diction of the neural fMRI activation based on a 
feature vector for each noun. The feature vector is 
extracted by the co-occurrences of each individual 
concrete noun with each of the 25 sensory-motor 
verbs, gathered from a huge google corpus (Brants, 
2006). The feature vector of each noun is used to
18
Figure 1 - Structure of the model for predicting fMRI activation for arbitrary stimuli word w
predict the  activity  of  each voxel  in  the brain,  
by assuming a weighted linear model (Figure 1). 
The activity of a voxel is defined as a conti-
nuous value that is assigned to it in the functional 
imaging1 procedure. Mitchell et alapplied a linear 
model based on its high consistency with the wide-
spread use of linear models in fMRI analysis. In 
this paper focus is on using WordNet based fea-
tures (in comparison to co-occurrence based fea-
tures), therefore the linear model proposed and 
justified by Mitchell et alis used and other models 
like SVM are not even considered. Mitchell et al 
suggests that the trained model is able to predict 
brain activity  even for unseen concepts and there-
fore notes that a great step forward in modeling 
brain activity is taken in comparison to the pre-
vious cataloging approaches for brain activity. This 
model does not work well in case of ambiguity in 
meaning, for example a word like saw has two 
meanings, as a noun and as a verb, making it diffi-
cult to construct the suitable feature vector for this 
word. We try to alleviate this problem in this paper 
and achieve better models by combining different 
models in case of ambiguity.  
In our work, we use the sensory-motor verbs 
which are suggested by psychologists and are also 
used by (Mitchell et al, 2008), to extract the fea-
                                                          
1 Functional images were acquired on a Siemens (Erlangen, 
Germany) Allegra 3.0T scanner at the Brain Imaging Re-
search Center of Carnegie Mellon University and the Univer-
sity of Pittsburgh (supporting online material of Mitchell et al 
2008).
ture vectors. But, instead of using a corpus to ex-
tract the co-occurrences of concrete nouns with 
these verbs we use WordNet to find the similarities 
of each noun with the 25 sensory-motor verbs. We 
also combine the WordNet extracted model with 
the corpus based model, and achieve better results 
in matching predicted fMRI images (from the 
model) to their own observed images.
This paper is organized as follows: in section 2 a 
brief introduction to WordNet measures is de-
scribed. In section 3, the WordNet approaches ap-
plied in the experiments and the Mitchell et al
linear model are explained. The results of the expe-
riments are discussed in section 4 and finally in 
section 5 the results and experiments are con-
cluded. 
2 WordNet-based Similarity
2.1 WordNet
WorNet is a semantic lexicon database for English 
language and is one of the most important and 
widely used lexical resources for natural language 
processing tasks (Fellbaum, 1998), such as word 
sense disambiguation, information retrieval, auto-
matic text classification, and automatic text sum-
marization. 
WordNet is a network of concepts in the form of 
word nodes organized by semantic relations be-
tween words according to meaning. Semantic rela-
tion is a relation between concepts, and each node 
consists of a set of words (synsets) representing the 
19
real world concept associated with that node. Se-
mantic relations are like pointers between synsets. 
The synsets in WordNet are divided into four dis-
tinct categories, each corresponding to four of the 
parts of speech ? nouns, verbs, adjectives and ad-
verbs (Pathwarden, 2003).
WordNet is a lexical inheritance system. The re-
lation between two nodes show the level of gene-
rality in an is?a hierarchy of concepts. For 
example the relation between horse and mammal
shows the inheritance of horse is-a mammal.
2.2 Similarity
Many attempts have investigated to approximate 
human judgment of similarity between objects. 
Measures of similarity use information found in is?
a hierarchy of concepts (or synsets), and quantify 
how much concept A is like concept B (Pedersen, 
2004). Such a measure might show that a horse is 
more like a cat than it is like a window, due to the 
fact that horse and cat share mammal as an ances-
tor in the WordNet noun hierarchy.
Similarity is a fundamental and widely used 
concept and refers to relatedness between two con-
cepts in WordNet. Many similarity measures have 
been proposed for WordNet?based measures of 
semantic similarity, such as information content 
(Resnik, 1995), JCN (Jiang and Conrath, 1997), 
LCH (Leacock and Chodorow, 1998), and Lin 
(Lin, 1998).
These measures have limited the part of speech 
(POS) of words, for example it is not defined to 
measure the similarity between verb see and noun 
eye. There is another set of similarity measures 
which work beyond this boundary of POS limita-
tion. These measures are called semantic related-
ness measures; such as Lesk (Banerjee and 
Pedersen, 2003), and Vector (Patwardhan, 2003). 
The simple idea behind the LCH method is to 
compute the shortest path of two concepts in a 
WordNet unified hierarchy tree. The LCH measure
is defined as follows (Leacock and Chodorow, 
1998):  
  
(1)
Similarity is measured between concepts c1 and 
c2, and D is the maximum depth of taxonomy; 
therefore the longest path is at most 2D. 
Statistical information from large corpora is 
used to estimate the information content of con-
cepts. Information content of a concept measures 
the specificity or the generality of that concept.
IC(c)= - log ( freq(c)freq(root) )                                   (2)
freq(c) is defined as the sum of frequencies of all 
concepts in subtree of concept c. The frequency of 
each concept is counted in a large corpus. There-
fore freq(root) includes frequency count of all con-
cepts. 
The LCS (Longest Common Subsummer) of 
concepts A and B is the most specific concept that 
is an ancestor of both A and B. Resnik defined the 
similarity of two concepts as follows (Resnik, 
1995):
relatednessres(c1,c2)=IC(lcs(c1,c2))                    (3)
IC(lcs(c1,c2)) is the information content of Longest 
Common Subsummer of concepts c1 and c2. 
The Lin measure, augment the information con-
tent of the LCS with the sum of the information 
content of concepts c1 and c2. The Lin measure 
scales the information content of the LCS by this 
sum. The similarity measure proposed by Lin, is 
defined as follows (Lin, 1998):
relatednesslin(c1,c2)= 2.IC(lcs(c1,c2))IC(c1)+IC(c2)                      (4)
IC(c1) and IC(c2) are information content of con-
cepts c1 and c2, respectively. 
Jiang and Conrath proposed another formula 
named JCN as a similarity measure which is shown 
below (Jiang and Conrath, 1997):
relatednessjcn(c1,c2)= 1IC(c1)+IC(c2)-2.IC(lcs(c1,c2))   (5)
The Lesk is a measure of semantic relatedness 
between concepts that is based on the number of
shared words (overlaps) in their definitions 
(glosses). This measure extends the glosses of the 
concepts under consideration to include the
glosses of other concepts to which they are re-
lated according to a given concept hierarchy (Ba-
nerjee and Pedersen, 2003). This method makes it 
possible to measure similarity between nouns and 
verbs. 
The Vector measure creates a co?occurrence 
matrix for each word used  in theWordNet glosses 
from a given corpus, and then represents each 
gloss/concept with a vector that is the average of 
these co?occurrence vectors (Patwardhan, 2003).
20
3 Approaches
As mentioned in the previous section, different 
WordNet measures can be used to compute the 
similarities between two concepts. The WordNet 
similarity measures are used to compute the verb-
concept similarities. The feature matrix comprises 
of the similarities between 25 verbs (features) and 
60 concrete nouns (instances). In this section the 
computational model proposed by (Mitchell et al, 
2008), WordNet-based models, and combinatory 
models are briefly described.
3.1 Mitchell et alBaseline Model
In our paper we used the Mitchell et alregression 
model for predicting human brain actively as our 
baseline. In all of the experiments in this paper, we 
use the fMRI data gathered by 
Mitchell et al The fMRI data were collected from 
nine healthy, college age participants who viewed 
60 different word-picture pairs presented six times 
each (Mitchell et al 2008). In Mitchell et al for 
each concept, a feature vector containing norma-
lized co-occurrences with 25 sensory-motor verbs, 
gathered from a huge google corpus (Brants, 
2006), is constructed. The computational model 
was evaluated using the collected fMRI data ga-
thered by Mitchell et al Mean fMRI images were 
constructed from the primary fMRI images, before 
training. A linear regression model was trained, 
using 58 (from 60) average brain images for each 
subject that maps these features to the correspond-
ing brain image. For testing the model, the two left 
out brain images were compared with their corres-
ponding predicted images, obtained from the 
trained model. The Pearson correlation (Equation 
6) was used for comparing whether each predicted 
image has more similarity with its own observed 
image (match1) or the other left out observed im-
age (match2). 
match1(p1=i1 & p2=i2) = 
pearsonCorrelation(p1,i1)+       
                  pearsonCorrelation(p2,i2)              (6)
p1 ,p2 are predicted images, and i1, i2 are corres-
ponding observed images. 
For calculating the accuracy we check whether 
the classification is done correctly or not. By se-
lecting two arbitrary concepts (of sixty concepts) 
as test, there would be 1770 different classifica-
tions. The overall percentage of correct classifica-
tion represents the accuracy of the model. 
We tried to use the same implementations in 
(Mitchell et al, 2008) as our baseline. We imple-
mented the training and test models as described in 
the supporting online material of Mitchell et als 
paper, but due to some probable unseen differences 
for example in the voxel selection, the classifica-
tion accuracies achieved by our replicated baseline 
of Mitchell et als is in average less than the accu-
racies attained by (Mitchell et al, 2008). In the test 
phase we used 500 selected voxels for comparison. 
The training is done for all 9 participants. 
This procedure is used in all the other approach-
es mentioned in this section. We have contacted 
the authors of the paper and we are trying to re-
solve the problem of our baseline.
3.2 WordNet based Models
As mentioned in section 2, several WordNet?based 
similarity measures have been proposed (Pedersen, 
2004). We apply some of the known measures to 
construct the feature matrix, and use them to train 
the models of 9 participants. 
WordNet::Similarity is a utility program availa-
ble on web2 to compute information content val-
ues. WordNet::Similarity implements measures of 
similarity and relatedness that are all in some way 
based on the structure and content of WordNet 
(Resnik, 2004). 
As mentioned in section 2, every concept in 
WordNet consists of a set of words (synsets). The 
similarity between two concepts is defined as a 
series of similarities between synsets of the first 
concept and synsets of the second concept. In this 
paper the maximum similarity between synsets of 
two concepts is considered as the candidate simi-
larity between two concepts.
In contrary to relatedness measures, similarity 
measures have the limitation of POS of words. In 
our case the verb-noun pair similarity is not de-
fined when using similarity measures. To solve this 
problem the sense (POS) of verb features are as-
sumed to be free (verb, noun, adjective and ad-
verb). For most cases the meaning of a verb sense 
of a word is close to the non-verb senses of that 
                                                          
2 http://wn-similarity.sourceforge.net/
21
word. For example the verb clean can be seen as a 
noun, adjective, and adverb which all have close 
meanings. Some problems arise by this assump-
tion. For example the verb Watch has a far mean-
ing of the noun Watch or some verbs like eat do 
not have a non-verb sense. To handle these issues 
the combination of the relatedness measures and 
similarity measures is used. This approach is dis-
cussed in section 3.3 to make a more suitable fea-
ture matrix. 
The two leave out cross-validation accuracies 
of regression models trained by feature matrices 
(computed from WordNet similarities) are depicted 
in Figure 2. The results helped us to select two 
measures for a final feature construction. The re-
sults are discussed and analyzed in the next sec-
tion. 
3.3 Lin/Lesk and JCN/Lesk based features
The experiments show that, JCN similarity meas-
ure gives the best results on extracting the feature 
vectors for predicting brain activity. Unfortunately, 
some similarity measures like JCN and Lin feature 
matrices are to some extent sparse. In some cases, 
the feature (sensory-motor verb) or even a concept 
is represented by a null vector. The null input data 
do not affect the linear regression training, but lead 
to less data for training the model. This anomaly is 
originated from the fact that some verbs do not 
have related non-verb senses (POS). 
On the other hand, relatedness measures (like 
Lesk) do not limit the POS of words. In conse-
quence, we have non-zero values for every element 
of the feature matrix. This motivates us to combine 
Lesk similarity measure with Lin to alleviate the 
defect mentioned above. 
Combination is based on finding a better feature 
matrix from the two Lin (JCN) and Lesk feature 
matrices. For this, a linear averaging is considered 
between Lin (JCN) and Lesk feature matrices.
3.4 Combinatory Schemes
In this paper, a new approach for extracting the 
feature matrix using WordNet is presented and dif-
ferent similarity measures for representing this fea-
ture matrix are investigated. 
In this section, we propose new combinatory 
approaches for combining Mitchell et als corpus 
based approach with our WordNet based approach. 
We assume that we have two feature matrices, one 
based on the corpus-based (baseline) method and 
the other based on a WordNet-based (Lin/Lesk 
similarity measure) method. 
3.4.1 Linear combination
The first approach for combining WordNet and 
baseline models, is based on assigning weights 
(?,1- ?) to the models, for calculation of match1
and match2.  match1 of baseline model is assigned 
weight ?, and match1 of WordNet model is as-
signed weight (1- ?), for calculating the final 
match1 of the system (Equation 7). 
match1=      
?.(match1Baseline)+(1-?).(match1WordNet) (7)
match2 is calculated in the same way. Classifica-
tion is assumed to be correct when match1 gets a 
greater value than match2. The parameter ? needs 
to be tuned. Different values of ? were tested and 
their output accuracies are depicted in Figure 2.
Figure 2 ? accuracies of different ? values
3.4.2 Concept based combination
The performance of computational models can be 
analyzed from a different view. We are looking for 
a combination mechanism based on model accura-
cies for classifying a concept pair. This combina-
tion mechanism estimates weights for WordNet 
and baseline models for testing a left out pair. To 
have a system with the ability to work properly on 
unseen nouns, we leave out all the concept pairs 
that have concepts c1 or c2 (117 pairs). This guar-
antees that the trained model is blind to concepts c1
and c2. The remaining concept pairs are used for 
22
Table 1- Voting mechanism
training (1653 pairs).
The accuracies of WordNet and baseline models 
for the training set are derived and weight of base-
line model is calculated as follows: 
? = Accuracy(Base)Accuracy(Base) + Accuracy(WordNet)                    (8) 
weight of WordNet model is calculated in a similar 
way. Relation 7 is used for calculating match1 and 
match2. For calculating the accuracy we check 
whether the classification is done correctly or not. 
This procedure is repeated for each arbitrary pair 
(1770 iterations) to calculate the overall accuracy 
of the combinatory system.
3.4.3 Voting based combination schemes
In many intelligent combinatory systems, the ma-
jority voting scheme is an approach for determin-
ing the final output. Mitchell et alcollected data 
for 9 participants. In this approach a voting is per-
formed on the models of 8 participants (participant 
j=1:9, j?i) for each concept pair (the two left out 
concepts), to select the better model amongst 
WordNet and baseline models. The better model is 
the model that leads to higher accuracy in classify-
ing the left out concepts of 8 participants (partici-
pant j=1:9, j?i). The selected model is used to test 
the model for pi (participant i).
Votes for selecting the better model for each 
participant is calculated as shown in Table 1. 
match1Base and match1WordNet represent match1
for baseline and WordNet models. 
match1= voteBase8 (match1Base) +
voteWordNet
8 (match1WordNet)        (9)
Another approach is linear voting combination. 
This approach is based on calculating match1 and 
match2 for a model, based on a weighted linear 
combination (relation 9). The weights for a combi-
natory model are calculated by a voting mechan-
ism (Table 1).
4 Results and Discussion
As mentioned in section 2, it is possible to con-
struct the feature matrix based on WordNet simi-
larity measures. Seven different measures were 
tested and models for 9 participants were trained 
using a 2-leave out cross validation. Four similarity 
measures (Lin, JCN, LCH, and Resnik), two simi-
larity relatedness measures (Lesk and Vector), a 
combination of Lin/ Lesk and a combination of
JCN/ Lesk are compared to the baseline. The re-
sults based on accuracies of these tests are shown 
in Table 3. The accuracies are calculated from 
counts of match scores. The match score between 
the two predicted and the two observed fMRI im-
ages was determined by which match (match1 or 
match2) had a higher Pearson correlation, eva-
luated over 500 voxels with the most stable res-
ponses across training presentations. 
The results of WordNet-based models are shown 
in Table 3. As described in section 2 the similarity 
measures have limitation of POS. The JCN meas-
ure has the best accuracy among all single similari-
ty measures. The JCN measure has a better average 
accuracy (0.65) in comparison to the Lin measure 
(0.63). The relatedness similarity does not have the 
limitation of POS. In spite of this advantage the 
Lesk and Vector measures do not provide a better 
accuracy than the JCN similarity measure. The 
Vector average accuracy (0.529) is worse than 
Lesk (0.622) and therefore just Lesk is considered 
as a candidate of combination with other similarity 
measures like JCN and Lin. In section 3 the idea of 
combining Lin (JCN) and Lesk measures was men-
tioned. These combinatory schemes led to better 
23
accuracies among all single measures (Table 3). 
Despite the lower average accuracy of the Lin me-
thod, the combination of Lin/Lesk achieved a bet-
ter average accuracy in comparison to JCN/Lesk 
combination. This is probably because of the lower 
correlation between Lin/Lesk feature vectors in 
comparison to JCN/Lesk feature vectors. The cor-
relation between different pairs of feature matrices 
extracted by WordNet-based similarity measures 
are shown in Table 2. The result shows that Lesk 
feature matrix has minimum correlations with all 
other WordNet-based feature matrices. This is a 
good motivation to have the Lesk measure as a 
candidate to mix with other measures to extract a 
more informative feature matrix. The Lesk feature 
matrix has the least correlation with Lin feature 
matrix among all WordNet-based feature matrices. 
Therefore as noted before, results of Table 3 show 
better accuracy for Lin/Lesk in comparison to 
JCN/Lesk. But these accuracies are less than the 
accuracies attained by the base method proposed 
by Mitchell et al 
Measure 1 Measure 2 Correlation
Lesk Lin 0.3929
Lesk Resnik 0.4528
Lesk JCN 0.5129
Lesk LCH 0.5556
Lin LCH 0.6182
JCN Res 0.6357
JCN Lin 0.7175
JCN LCH 0.7234
Lin Res 0.7400
LCH Res 0.7946
Table 2? correlation between different pairs of Word-
Net-based similarity (relatedness) measures
One important reason of this shortage can be the 
difference in sense (POS) between concepts (with 
noun POS) and features (with verb POS). This 
leads to limitation of WorldNet-based measures for 
constructing better feature matrices. Investigating 
new features of the same sense of POS between 
concepts and features (associated with sensory-
motor verbs) might lead to even better results. 
The Base and WordNet use ultimately different 
approaches to compute the similarity of each pair 
of concepts. Several experiments like the union of 
features and the combination of system outputs 
was designed. The union of the two feature matric-
es (baseline feature matrix and Lin/Lesk feature 
matrix) does not lead to a better result (0.646). In 
contrary to the united features the combination of 
these systems gives a better performance. Three 
different schemes of combinatory systems are pro-
posed in section 4. The first scheme (linear combi-
nation) uses a fixed ratio (?) for combining the 
output match of the two systems. As depicted in 
Figure 2 the ? value is tuned and an optimum value 
of ?=0.64 achieved an average accuracy of 0.775 
(Table 4). 
Figure 3- Improvement of linear combinatory scheme
The accuracies of participants P1 and P5 for our 
implemented baseline are almost the same as the 
accuracies of P1 and P5 in Mitchell et al A com-
parison of the accuracies for P1 and P5 attained by 
the baseline model and the linear combination 
scheme is illustrated in Figure 3. The results show 
considerable improvement in accuracies when the 
combinatory model is used. 
Figure 4- Comparison of linear Combinatory scheme 
with Baseline and WordNet
24
Measure/ Partic-
ipant P1 P2 P3 P4 P5 P6 P7 P8 P9 Average
Baseline 0.828 0.845 0.752 0.798 0.776 0.658 0.705 0.615 0.680 0.740
Lin 0.73 0.624 0.739 0.727 0.591 0.507 0.64 0.501 0.632 0.632
Lesk 0.725 0.629 0.668 0.688 0.601 0.519 0.604 0.584 0.580 0.622
Vector 0.603 0.599 0.551 0.553 0.567 0.451 0.509 0.446 0.476 0.529
LCH 0.685 0.613 0.671 0.617 0.574 0.468 0.577 0.506 0.587 0.589
RES 0.610 0.558 0.594 0.622 0.505 0.555 0.603 0.449 0.490 0.554
JCN 0.797 0.638 0.765 0.713 0.671 0.525 0.504 0.568 0.642 0.647
Lin/Lesk 0.807 0.677 0.767 0.812 0.672 0.645 0.690 0.502 0.697 0.697
JCN/Lesk 0.790 0.604 0.718 0.789 0.641 0.593 0.593 0.514 0.667 0.656
Table 3- Results of Different similarity measures compared to baseline
Approach/ Par-
ticipant P1 P2 P3 P4 P5 P6 P7 P8 P9 Average
Linear 0.877 0.847 0.827 0.862 0.798 0.696 0.734 0.605 0.728 0.775
Concept-based 0.887 0.832 0.836 0.87 0.793 0.687 0.736 0.588 0.734 0.774
Binary voting 0.894 0.837 0.829 0.858 0.796 0.684 0.758 0.612 0.736 0.778
Weighted voting 0.905 0.840 0.861 0.882 0.808 0.710 0.761 0.614 0.755 0.793
Table 4- Accuracies of different combinatory approaches
The improvement of this combinatory scheme 
can be viewed from another aspect. Concept accu-
racy, defined as classification   accuracy   of the
concept paired with each of the other 59 concepts, 
shows the performance of the system for each con-
cept (Figure 4). The concept accuracies of the li-
near combinatory scheme are compared with the 
Baseline   and  WordNet   systems  and  results   
are illustrated in Figure 4. The accuracy of some 
ambiguous concrete nouns like ?saw? are improved 
in WordNet-based model and this improvement is 
maintained by linear combinatory model. Im-
provements have been seen in combinatory model. 
The second scheme uses a cross validation of 
the remaining 58 concepts to train the system, for 
deciding on each pair of concepts. After training, 
each system (WordNet and Base) is assigned a 
weight according to its accuracy. Decision on the 
test pair is based on a weighted combination of the 
systems. The results of this scheme are shown in 
Table 4. It has an improvement of 3.4% in compar-
ison to the baseline model.
The third scheme chooses another combinatory 
strategy to decide on each test pair of concepts for 
participant Pi. This scheme gathers votes from the 
other 8 participants as described in section 3. The 
results are shown in Table 4. Improvement of bi-
nary voting scheme to baseline is almost as much 
as the Improvement of linear and concept-based 
schemes to baseline. The weighted voting used a 
more flexible combination scheme, and led to an 
improvement of about 5.3% in comparison to base-
line. 
A result is called statistically significant if it is 
improbable to have occurred by   chance. T-test 
Participant H-value P-value
P1 1 7.73e-12
P2 0 0.6610
P3 1 5.55e-17
P4 1 2.61e-12
P5 1 0.0051
P6 1 0.0004
P7 1 8.28e-05
P8 0 0.5275
P9 1 3.95e-07
Table 5- t-test of baseline and weighted voting output 
values for 9 participants
25
was used to show whether the improvement 
achieved in this paper is statistically significant or 
not. The t-test was tested on output accuracies of 
baseline (with average accuracy 0.74) and
weighted voting combinatory scheme (with aver-
age accuracy 0.793) for 9 participants. The results 
are shown in Table 5. The weighted voting scheme 
does not have improvement on P2  and P8  and  
results  are  almost similar to baseline, therefore 
the null hypothesis of equal mean is not rejected 
(H-value=0) at 0.05 confidence level. For all par-
ticipants with improvement on results, null hypo-
thesis of equal mean is rejected (H-value=1) at 
0.05 confidence level. This rejection shows that the 
improvements are approved to be statistically sig-
nificant for all participants with improvement. The 
t-test on overall 9 participants rejected null hypo-
thesis with a P-value of almost zero. This experi-
ment shows the improvement achieved in this 
paper is statistical significant.
5 Conclusion
In this work, a new WordNet-based similarity ap-
proach for deriving the sensory-motor feature vec-
tors associated with the concrete nouns was 
introduced. A correlation based combination of 
WordNet measures is used to attain more informa-
tive feature vectors.  The computational model 
trained by these feature vectors are combined with 
the computational model trained with feature vec-
tors extracted by a corpus based method. 
The combinatory scheme achieves a better aver-
age accuracy in predicting the brain activity asso-
ciated with the meaning of concrete nouns. 
Investigating new features of the same sense (POS) 
between concepts and non-verb features (asso-
ciated with sensory-motor verbs) might lead to 
even better results for WordNet-based Models.
Acknowledgements
The authors would like to thank the anonymous 
reviewers for their thorough review and their con-
structive comments.
References
Banerjee, S., and Pedersen, T. 2003. Extended gloss 
overlaps as a measure of semantic relatedness. In Pro-
ceedings of the Eighteenth International Joint Confe-
rence on Artificial Intelligence, 805?810. 
Brants T., and Franz A., 2006, 
www.ldc.upenn.edu/Catalog/CatalogEntr
y.jsp?catalogId=LDC2006T13. Linguistic 
Data Consortium, Philadelphia.
Fellbaum C., 1998. WordNet: An Electronic Lexical 
Database. The MIT Press, Cambridge, MA.
Hardoon D. R., Mourao-Miranda J., M. Brammer, 
Shawe-Taylor J. 2007. unsupervised analysis of fMRI 
data using kernel canonical correlation. Neuroimage, 
pp. 1250-1259.
Kay K. N., Naselaris T., Prenger R. J., Gallant J. L. 
2008. Identifying Natural Images from Human Brain 
Activity, Nature, pp. 352-355.
Leacock C. and Chodorow M. 1998. Combining lo-
cal context andWordNet similarity for word sense iden-
tification. In C. Fellbaum, editor, WordNet: An 
electronic lexical database, pages 265?283. MIT Press.
Lin D. 1998. An information-theoretic definition of 
similarity. In Proceedings of the International Confe-
rence on Machine Learning, Madison.
Mitchell T. M., et al 2008. Predicting Human Brain 
Activity Associated with the Meanings of Nouns, Ameri-
can Association for the Advancement of Science. 
Mitchell T. M., Hutchinson R. A., Niculescu R. S., 
Pereira F., and Wang X..  2004. Learning to Decode 
Cognitive States from Brain Images, Machine Learning, 
pp. 145-175. 
O?Toole A. J., Jiang F., Abdi H., and Haxby J. V.. 
2005. Partially distributed representations of objects 
and faces in ventral temporal cortex. Journal of Cogni-
tive Neuroscience, pp. 580-590.
Patwardhan S. 2003. Incorporating dictionary and 
corpus information into a context vector measure of 
semantic relatedness. Master?s thesis, University of 
Minnesota, Duluth.
Pedersen T., Patwardhan S., and Michelizzi J. 2004. 
WordNet::Similarity - Measuring the relatedness of 
Concepts. Proceedings of Fifth Annual Meeting of the 
North American Chapter of the Association for Compu-
tational Linguistics (NAACL-04), pp. 38-41. 
Resnik. P. 1995. Using information content to eva-
luate semantic similarity in taxonomy. In Proceedings of 
the 14th International Joint Conference on Artificial 
Intelligence, pages 448?453.
26
