Modular MT with a learned bilingual dictionary: rapid deployment
of a new language pair
Jessie Pinkham Martine Smets
Microsoft Research
One Microsoft Way
Redmond, WA 98052
{jessiep martines}@microsoft.com
Abstract
The MT system described in this paper
combines hand-built analysis and generation
components with automatically learned
example-based transfer patterns. Up to now,
the transfer component used a traditional
bilingual dictionary to seed the transfer
pattern learning process and to provide
fallback translations at runtime. This paper
describes an improvement to the system by
which the bilingual dictionary used for these
purposes is instead learned automatically
from aligned bilingual corpora, making the
system?s transfer knowledge entirely
derivable from corpora. We show that this
system with a fully automated transfer
process performs better than the system
with a hand-crafted bilingual dictionary.
More importantly, this has enabled us to
create in less than one day a new language
pair, French-Spanish, which, for a technical
domain, surpasses the quality bar of the
commercial system chosen for comparison.
1 Introduction
The phrase ?MT in a day? is strongly associated
with research in statistical MT. In this paper we
demonstrate that ?MT in a day? is possible with
a non-statistical MT system provided that the
transfer component is learned from aligned
bilingual corpora (bi-texts), and does not rely on
any large hand-crafted bilingual resource. We
propose instead to use a bilingual dictionary
learned only from the same bi-texts. Section 4.2
describes the creation of the new language pair,
French-Spanish, and gives evaluation results.
Section 4.1 examines the impact of the learned
dictionary on our existing French-English
system.
2 Previous work
Commercial systems and other large-scale
systems have traditionally relied heavily on the
knowledge encoded in their bilingual
dictionaries. Gerber & Yang (1997) clearly
state that Systran?s translation capabilities are
dependent on ?large, carefully encoded, high-
quality dictionaries?. With the advent of bi-
texts, efforts to derive bilingual lexicons have
led to substantial research (Melamed 1996,
Moore 2001 for discussion), including resources
for semi-automatic creation of bilingual lexica
such as SABLE (Melamed 1997), used for
instance in Palmer et al (1998). Statistical MT
systems have relied on bi-texts to automatically
create word-alignments; in many statistical MT
systems however, the authors state that use of a
conventional bilingual dictionary enhances the
performance of the system (Al-Onaizan et al
1999, Koehn & Knight 2001). We find then,
that in spite of the movement to create bilingual
dictionaries automatically, there is still a heavy
reliance on hand-crafted and hand-edited
resources. We found no full-scale MT system
that relied only on learned bilingual dictionaries
and certainly none that was found better in
performance for doing so.
Rapid deployment of a new language pair
has been one of the strong features of statistical
MT systems. For example, ?MT in a day? was a
stated goal of the workshop on statistical MT
(Al-Onaizan et al 1999). The system deployed
was of low quality, in part because of the small
size of the corpus used, and the difficulty of the
language pair chosen (Chinese to English). We
have chosen French-Spanish, because we are
constrained by the availability of well-
developed analysis and generation components
in our experiment. Those, needless to say, were
not created in one day, nor were the large size
monolingual dictionaries that they rely on. But
given the assumption that these modules are
available and of good quality, we demonstrate
that training the transfer dictionary1 and
example base on bi-texts is sufficient to create a
new language pair which is of comparable
quality to others based on the same source
language. This, to our knowledge, has not been
done before in the context of a large hybrid MT
system
3 System overview
The MT system discussed here uses a source
language broad coverage analyzer, a large multi-
purpose source language dictionary, an
application-independent natural language
generation component which can access a full
monolingual dictionary for the target language,
and a transfer component. The transfer
component, described in detail in Menezes
(2001), consists of high-quality transfer patterns
automatically acquired from sentence-aligned
bilingual corpora.
The innovation of this work is the use of an
unedited, automatically created dictionary which
contains translation pairs and parts of speech,
without any use of a broad domain, general
purpose hand-crafted dictionary resource. The
architecture of the MT system as described
elsewhere (Richardson et al 2001) used both a
traditional bilingual dictionary and an
automatically derived word-association file at
training time, but it used only the traditional
bilingual dictionary at runtime. We refer to this
below as the HanC system, because it uses a
Hand-crafted Dictionary2. We changed this so
that a learned dictionary consisting of word-
associations (Moore 2001) with parts of speech
and a function word only bilingual dictionary
(prepositions, conjunctions and pronouns)
replaces the previous combination both at
training and at runtime3. We refer to this as the
1 In both French-English and French-Spanish, we use
a hand-crafted bilingual function word dictionary of
about 500 entries. It includes conjunctions,
prepositions and pronouns; see section 4.1.4.
2 The dictionaries are automatically converted from
electronic dictionaries acquired from publishers, and
are updated by hand over time.
3 The same statistical techniques identify certain
multi-word terms for parsing and transfer. This
LeaD system (Learned Dictionary). We
demonstrate that this change improves sentences
that differ between both systems, and show that
we can now adapt quickly to new language pairs
with excellent results.
Analysis of the consequences of removing
the standard hand-crafted bilingual dictionary
from the system (and having no dictionary as a
fallback at all) are provided in Pinkham &
Smets (2002). It proved important to have a
dictionary containing parts of speech to use as a
fallback, motivating the work described here.
4 Experiments
We conducted two experiments. In the first one,
we compared the performance of the HanC
(Hand-Crafted dictionary) MT system to the
performance of our LeaD (Learned Dictionary)
system. The French-English system is trained
on 200,000 sentences in the computer domain,
and tested on unseen sentences from the same
domain.
In the second experiment, we created a new
language pair, French-Spanish, in less than 8
hours. The French-Spanish system was trained
on 220,000 sentences from the same computer
domain, and also tested on unseen computer
domain data.
4.1 French-English translation
with a learned bilingual
dictionary
4.1.1 Comparing HanC to LeaD
In this first experiment, we compare the
performance of the HanC system and the LeaD
system for French-English versus the same
competitor.
Translations produced by the two versions of
our system differ in 30% of the cases. Out of
the 2000 sentences in our test set, only 595 were
translated differently. In about half of these
cases, there was an overt difference in the word
chosen as a fallback translation at runtime. In
the other half, the translation example-base
patterns were different.
learned dictionary stays constant during the French-
English experiments.
We evaluated 400 of the 595 ?diff? sentences
mentioned. A complete description of the
evaluation method is given in Richardson
(2001), and repeated in Appendix A. Evaluation
for each version of the system was conducted
against the competitor system, which we use as
a benchmark of quality. Our current benchmark
for French-English is Systran4, which uses
relevant dictionaries available but has not been
otherwise customized to the domain in any way.
Scores Signif. Size
HanC system
(diffs only)
-.1777 +/-.087 > .999 400
LeaD system
(diffs only)
-.0735 +/-.182 .97 400
French-English
HanC system
+.2626 +/- .103 > .999 400
French-English
LeaD system
+.2804 +/-.115 > .999 400
Table 1: LeaD vs. HanC for FE
We also evaluated a set of 400 sentences
taken randomly from the 2000 test sentence set.
They were translated with both the HanC system
and the LeaD system, and evaluated against the
same competitor, Systran.
4.1.2 Results
The random test has a score representative of
the quality of the system (December 2001
system), and is significantly better than the
competitor given the score of +0.2804 (0 means
the systems are the same, -1 the competitor is
better, 1 the competitor is worse). See Table 1.
Sentences whose translations differ between
the HanC and LeaD versions of our system are
less well translated overall. Through
examination of the data, we have found that
reliance on the fallback translation at runtime
tends to indicate a failure to learn or apply
transfer patterns from the example-base, both of
which are often due to faulty analysis of the
source sentence. There are also cases where
4 Systran was chosen on the basis of its ranking as
the best FE system in the IDC report (Flanagan &
McClure, 2000)
translations are not learned because of sparse
data, but these tend to be rare in our technical
corpus.
More importantly, we see that the LeaD
version of the system has a significantly higher
score than the HanC version (p=0.002 in a one-
tailed t-test). Replacing the conventional
bilingual dictionary with the learned bilingual
dictionary combined with the small function
word dictionary has led to significant
improvement in quality when measured on
?diff? sentences, i.e. cases where all the
sentences are different. However, when we take
400 random sentences, the difference between
the two versions only affects 30% of the
sentences (133 or thereabouts) and therefore
does not result in a significant difference
(p=0.13 in a one tailed t-test).
4.1.3 Translation examples
In this section, we give examples of translation
with both versions of our system, and compared
to Systran. The LeaD version of our system
uses the correct translation of ?casiers?, in this
specific context, while both our HanC version of
the system and Systran use terms inappropriate
for this domain. By using a learned dictionary,
the LeaD system is better suited to the domain.
Source Le finisseur est trait? comme trois
casiers individuels,
Reference The Finisher is addressed as three
individual bins
LeaD The finisher is processed like three
individual bins.
HanC The finisher is processed like three
individual pigeonholes.
Systran The finisher is treated like three
individual racks,
4.1.4 Creation of the learned bilingual
dictionary
The learned dictionary with parts of speech was
created by the same method (Moore, 2001) as
the previously used word-association file, with
the exception that parts of speech were
appended to lemmas in the first step of the
process. We are easily able to modify the input
this way, because we use the output of the
analysis of the training data to create the file that
is the input to the word alignment process.
Appending the part of speech disambiguates
homographs such as ?use?, causing them to be
treated as separate entities in the word-
association process:
use^^Verb
use^^Noun
The word-association process assigns scores
to each pair of words. We have established a
threshold below which the pairs are discarded.
Here are the top word pairs in the learned
dictionary for this domain:
utiliser^^Verb use^^Verb
fichier^^Noun file^^Noun
serveur^^Noun server^^Noun
Because the input to the learning process is
derived from Logical Forms (the output of our
analysis systems), and because this format no
longer includes lemmas for function words,
there are no function words in the learned
dictionaries. This is the primary reason why we
complemented the learned dictionary with a
function word dictionary. See the future work
section for ideas on learning the function words
as well.
Both the French-English and the French-
Spanish were arbitrarily cut off at the same
threshold, and were not edited in any way,
resulting in a file with 24,000 translation pairs
for French-English and 28,000 translation pairs
for French-Spanish. The dictionary for function
words contains about 500 word pairs. The
traditional French-English dictionary had
approximately 40,000 entries.
4.2 French-Spanish
4.2.1 Creating French-Spanish
Our group currently has both a French-English
system and an English-Spanish system. In
choosing the new language pair to develop, we
were constrained by the availability of good
quality analysis and generation systems. This is
a limiting factor, but will become less so once
we have more generation modules available for
use5, as we currently have seven fully developed
analysis modules. We were fortunate to have
220,000 aligned sentences for French-Spanish
from the technical domain (manuals, help files),
5 Members of our group (Corston-Oliver et al) are
developing an automatic generation component.
This could speed up the development of generation
modules, giving us a potential of 42 different
language-pairs trainable on bi-texts.
which enabled the construction of the learned
bilingual dictionaries, and the automatic
creation of the transfer pattern example base.
For reasons explained above, our first
learned dictionary made no attempt to learn
function word translations. We needed,
therefore, to complement the learned French-
Spanish dictionary with a French-Spanish
function word bilingual dictionary, which was
bootstrapped from our French-English and
English-Spanish bilingual dictionaries. All the
translations for prepositions, conjunctions and
pronouns were created using both of these, and
hand-edited by a lexicographer bilingual in
French and Spanish.
The creation process, including the hand-
editing work, took less than 8 hours.
4.2.2 Results
The test was conducted on 250 test sentences
from the same technical domain as the training
corpus, using the methodology described in
Appendix A. All test data is distinct from
training data and unseen by developers. The
Sail Labs French-Spanish system is the
benchmark used as comparison. The technical
domain dictionary on the website was applied to
the Sail Labs translation, but it was not
otherwise customized to the domain.
The Sail Labs translation included brackets
around unfound words, which were thought to
interfere with the raters? ability to compare the
sentences; the brackets were removed for the
evaluation.
Condition Scores Signif Size
FS LeaD +.2278
+/- .117
> .999 250
French-English +.2804
+/- .114
> .999 400
Table 2: French Spanish results
As seen in Table 2, where the French-
Spanish system is ranked at +0.228, it is
significantly better than the Sail Labs French-
Spanish system in this technical domain. The
score is very similar to the French-English score
as measured against Systran (+.2804). Since
these are being compared against different
competitors, we also wanted to measure their
absolute quality. On a scale of 1 to 4, where 4 is
the best, we found that both Systran and Sail
Labs were comparable in quality, and that our
system scored slightly higher in both cases, but
not significantly so, if one considers the
confidence measures (Table 3). The details of
the scoring for absolute evaluations are given in
Appendix B. As a brief illustration, the LeaD
French-English translation in 4.1.3 has a score
of 3, while the LeaD French-Spanish translation
in 4.2.3 received a score of 2.5.
Absolute score
FS LeaD 2.676 +/- .329 250
FS Sail Labs 2.444 +/- .339 250
French-English 2.321 +/- .21 400
FE Systran 2.259 +/- .291 250
Table 3: Absolute scores FS and FE
4.2.3 Translation Example for French-
Spanish
This section gives examples of translation from
French into Spanish. The LeaD translation has
the correct translation for domain specific terms
such as ?hardware? and ?casilla de
verificaci?n?, while Sails Labs translation does
not in spite of the use of a domain bilingual
dictionary.
Source Si la case ? cocher Supprimer de ce
profil mat?riel est activ?e, le
p?riph?rique est supprim? du profil
mat?riel.
Reference Si la casilla de verificaci?n Quitar este
perfil de hardware est? activada, se ha
quitado el dispositivo del perfil de
hardware.
LeaD Si se activa la casilla de verificaci?n
Eliminar de este perfil de hardware, el
dispositivo se quita del perfil de
hardware.
Sails Labs Si la coloca a marcar Suprimir de este
perfil material es activada, el perif?rico
se suprime del perfil material.
5 Future Work
We are planning to experiment with lowering
the threshold for the cutoff of information in the
learned bilingual dictionary, in an attempt to
include more word pairs (some words remain
untranslated).
To further validate the Learned Dictionary
approach, we are experimenting with other
domains. One might assume, for instance, that
as the domain becomes broader, learned
dictionaries would be less effective due to
sparse data. We have preliminary experiments
on Hansard French-English data which indicate
that this is not the case.
6 Conclusion
We have demonstrated that we can replace the
traditional bilingual dictionary with a
combination of a small bilingual function word
dictionary and a bilingual dictionary learned
from bi-texts. This removes the reliance on
acquired or hand-built bilingual dictionaries,
which can be expensive and time-consuming to
create. One can estimate that for any new
domain application, this could save as much as
1-2 person years of customization. This also
removes a major obstacle to quick deployment
of a new language pair.
We believe that high-quality linguistic
analysis is a necessary ingredient for successful
MT. In our system, it has enabled automation of
the transfer component, both in the learning of
the bilingual dictionary and in the creation of
example-based patterns.
Appendix A: Relative Evaluation Method
For each version of the system to be tested,
seven evaluators were asked to evaluate the
same set of blind test sentences. For each
sentence, raters were presented with a reference
sentence, the original English sentence from
which the human French translation was
derived. In order to maintain consistency among
raters who may have different levels of fluency
in the source language, raters were not shown
the original French sentence. Raters were also
shown two machine translations, one from the
system with the component being tested, and
one from the comparison system (Systran for
French-English, Sails Lab for French-Spanish).
Because the order of the two machine
translation sentences was randomized on each
sentence, evaluators could not determine which
sentence was from which system. The order of
presentation of sentences was also randomized
for each rater in order to eliminate any ordering
effect.
The raters were asked to make a three-way
choice. For each sentence, the raters were to
determine which of the two automatically
translated sentences was the better translation of
the (unseen) source sentence, assuming that the
reference sentence was a perfect translation,
with the option of choosing ?neither? if the
differences were negligible. Raters were
instructed to use their best judgment about the
relative importance of fluency/style and
accuracy/content preservation. We chose to use
this simple three-way scale in order to avoid
making any a priori judgments about the relative
judgments of quality. The three-way scale also
allowed sentences to be rated on the same scale,
regardless of whether the differences between
output from system 1 and system 2 were
substantial or relatively small; and regardless of
whether either version of the system produced
an adequate translation.
The scoring system was similarly simple;
each judgment by a rater was represented as 1
(sentence from our system judged better), 0
(neither sentence judged better), or -1 (sentence
from Systran or Sails Labs judged better). The
score for each version of the system was the
mean of the scores of all sentences for all raters.
The significance of the scores was calculated in
two ways. First, we determined the range around
the mean which we could report with 95%
confidence (i.e. a confidence interval at .95),
taking into account both variations in the
sentences and variations across the raters'
judgments. In order to determine the effects of
each stage of development on the overall quality
of the system, we calculated the significance of
the difference in the scores across the different
versions of the system to determine whether the
difference between them was statistically
meaningful. We used a one-tailed t-test, since
our a priori hypothesis was that the system with
more development would show improvement
(that is, a statistically meaningful change in
quality with respect to the competitor).
Appendix B: Absolute Evaluation
Method
At the same time as the relative evaluations are
made, all the raters enter scores from 1 to 4
reflecting the absolute quality of the translation,
as compared to the reference translation given.
The grading is done according to these
guidelines:
1 unacceptable:
Absolutely not comprehensible and/or little
or no information transferred accurately
2 possibly acceptable:
Possibly comprehensible (given enough context
and/or time to work it out); some information
transferred accurately
3 acceptable:
Not perfect (stylistically or grammatically odd),
but definitely comprehensible, AND with
accurate transfer of all important information
4 ideal:
Not necessarily a perfect translation, but
grammatically correct, and with all information
accurately transferred
References
Al-Onaizan, Y & Curin, J. & Jahr, M. & Knight
K. & Lafferty, J. & Melamed, D. & Och, F-J,
& Purdy, D. & Smith, N. A. & Yarowsky, D.
(1999). Statistical Machine Translation: Final
Report, Johns Hopkins University 1999
Summer Workshop on Language
Engineering, Center for Speech and Language
Processing, Baltimore, MD.
Corston-Oliver, S., M. Gamon, E. Ringger, R.
Moore. 2002. An overview of Amalgam: A
machine-learned generation module. To
appear in Proceedings of the International
Natural Language Generation Conference.
New York, USA
Flanagan, M and McClure, S. (2000) Machine
Translation Engines: An Evaluation of Output
Quality, IDC publication 22722.
Gerber, L. & Yang,J. (1997) Systran MT
Dictionary Development in the Proceedings
of the MT Summit V, San Diego.
Koehn, P. & Knight, K. (2001) Knowledge
Sources for Word-Level Translation Models,
Proceedings of the conference on Empirical
Methods in Natural Language Processing
(EMNLP)
Melamed, D. (1998). Empirical Methods for MT
Lexicon Construction, in L. Gerber and D.
Farwell, Eds., Machine Translation and the
Information Soup, Springer-Verlag.
Melamed, D. (1997). A Scalable Architecture
for Bilingual Lexicography, Dept. of
Computer and Information Science Technical
Report #MS-CIS-91-01.
Melamed, D. (1996). Automatic Construction of
Clean Broad-Coverage Translation Lexicons,
Proceeding of the 2nd Conference of the
Association for Machine Translation in the
Americas (AMTA'96), Montreal, Canada.
Menezes, A. & Richardson, S. (2001). A Best-
First Alignment Algorithm for Automatic
Extraction of Transfer Mappings from
Bilingual Corpora. In Proceedings of the
Workshop on Data-Driven Machine
Translation, ACL Conference, June 2001.
Moore, R.C. (2001). Towards a Simple and
Accurate Statistical Approach to Learning
Translation Relationships Between Words. In
Proceedings of the Workshop on Data-Driven
Machine Translation, ACL Conference, June
2001.
Pinkham, J & Smets, M (2002) Machine
Translation without a bilingual dictionary
Proceedings of the TMI conference, Kyoto,
Japan.
Palmer, M. & Rambow, O. & Nasr, A. (1998).
Rapid Prototyping of Domain-Specific
Machine Translation Systems, in Proceedings
of the AMTA ?98.
Richardson, S. & Dolan, W. & Menezes, A. &
Corston-Oliver, M. (2001). Overcoming the
Customisation Bottleneck Using Example-
Based MT. In Proceedings of the Workshop
on Data-Driven Machine Translation, ACL
Conference, June 2001.
Linguistically Informed Statistical Models of Constituent Structure for 
Ordering in Sentence Realization 
Eric RINGGER1, Michael GAMON1, Robert C. MOORE1, 
David ROJAS2, Martine SMETS1, Simon CORSTON-OLIVER1 
 
1Microsoft Research 
One Microsoft Way 
Redmond, Washington 98052, USA 
{ringger, mgamon, bobmoore, msmets, 
simonco}@microsoft.com 
2Butler Hill Group, LLC 
& Indiana University Linguistics Dept. 
1021 East 3rd Street, MM 322 
Bloomington, Indiana 47405, USA 
drojas@indiana.edu 
 
 
Abstract 
We present several statistical models of syntactic 
constituent order for sentence realization. We 
compare several models, including simple joint 
models inspired by existing statistical parsing 
models, and several novel conditional models. The 
conditional models leverage a large set of linguistic 
features without manual feature selection. We apply 
and evaluate the models in sentence realization for 
French and German and find that a particular 
conditional model outperforms all others. We 
employ a version of that model in an evaluation on 
unordered trees from the Penn TreeBank. We offer 
this result on standard data as a reference-point for 
evaluations of ordering in sentence realization. 
1 Introduction 
Word and constituent order play a crucial role in 
establishing the fluency and intelligibility of a 
sentence. In some systems, establishing order 
during the sentence realization stage of natural 
language generation has been accomplished by 
hand-crafted generation grammars in the past (see 
for example, Aikawa et al (2001) and Reiter and 
Dale (2000)). In contrast, the Nitrogen (Langkilde 
and Knight, 1998a, 1998b) system employs a word 
n-gram language model to choose among a large 
set of word sequence candidates which vary in 
constituent order, word order, lexical choice, and 
morphological inflection. Nitrogen?s model does 
not take into consideration any non-surface 
linguistic features available during realization.  
The Fergus system (Bangalore and Rambow, 
2000) employs a statistical tree model to select 
probable trees and a word n-gram model to rank 
the string candidates generated from the best trees. 
Like Nitrogen, the HALogen system (Langkilde, 
2000; Langkilde-Geary, 2002a, 2002b) uses word 
n-grams, but it extracts the best-scoring surface 
realizations efficiently from a packed forest by 
constraining the search first within the scope of 
each constituent.  
Our research is carried out within the Amalgam 
broad coverage sentence realization system. 
Amalgam generates sentence strings from abstract 
predicate-argument structures (Figure 1), using a 
pipeline of stages, many of which employ 
machine-learned models to predict where to 
perform specific linguistic operations based on the 
linguistic context (Corston-Oliver et al, 2002; 
Gamon et al, 2002a, 2002b; Smets et al, 2003). 
Amalgam has an explicit ordering stage that 
determines the order of constituents and their 
daughters. The input for this stage is an unordered 
tree of constituents; the output is an ordered tree of 
constituents or a ranked list of such trees. For 
ordering, Amalgam leverages tree constituent 
structure and, importantly, features of those 
constituents and the surrounding context. By 
separately establishing order within constituents, 
Amalgam heavily constrains the possible 
alternatives in later stages of the realization 
process.  The design allows for interaction between 
ordering choices and other realization decisions, 
such as lexical choice (not considered in the 
present work), through score combinations from 
distinct Amalgam pipeline stages. 
Most previous research into the problem of 
establishing order for sentence realization has 
focused on English, a language with fairly strict 
word and constituent order. In the experiments 
described here we first focus on German and 
French which present novel challenges.1 We also 
describe an English experiment involving data 
from the Penn Treebank. Our ultimate goal is to 
develop a model that handles all ordering 
phenomena in a unified and elegant way across 
typologically diverse languages. In the present 
paper, we explore the space of possible models and 
examine some of these closely. 
                                                          
1
 For an overview of some of the issues in 
determining word and constituent order in German and 
French, see (Ringger et al, 2003).  
 Figure 1: Abstract predicate-argument structure (NLPWin logical form) for the German sentence: 
In der folgenden Tabelle werden die Optionen sowie deren Funktionen aufgelistet. 
(The options and their functions are listed in the following table.) 
2 Models of Constituent Order 
In order to develop a model of constituent 
structure that captures important order phenomena, 
we will consider the space of possible joint and 
conditional models in increasing complexity. For 
each of the models, we will survey the 
independence assumptions and the feature set used 
in the models. 
Our models differ from the previous statistical 
approaches in the range of input features. Like the 
knowledge-engineered approaches, the models 
presented here incorporate lexical features, parts-
of-speech, constituent-types, constituent 
boundaries, long-distance dependencies, and 
semantic relations between heads and their 
modifiers. 
Our experiments do not cover the entire space of 
possible models, but we have chosen significant 
points in the space for evaluation and comparison. 
2.1 Joint Models 
We begin by considering joint models of 
constituent structure of the form ( ),P ? ?  over 
ordered syntax trees ?  and unordered syntax trees 
? . An ordered tree ?  contains non-terminal 
constituents C, each of which is the parent of an 
ordered sequence of daughters ( 1,..., nD D ), one of 
which is the head constituent H.2 Given an ordered 
tree ? , the value of the function 
_ ( )unordered tree ?  is an unordered tree ?  
corresponding to ?  that contains a constituent B 
for each C in ? , such that 
( ) ( )
1
_ ( )
{ ,..., }n
unordered set daughters Cdaughters B
D D
=
=
 
again with iH D=  for some i in ( )1..n . The 
hierarchical structure of ?  is identical to that of 
? . 
We employ joint models for scoring alternative 
ordered trees as follows: given an unordered 
syntax tree ? , we want the ordered syntax tree ??  
that maximizes the joint probability: 
                                                          
2 All capital Latin letters denote constituents, and 
corresponding lower-case Latin letters denote their 
labels (syntactic categories). 
( ) ( )
: _ ( )
? arg max , arg max
unordered tree
P P
? ? ? ?
? ? ? ?
=
= =    (1) 
As equation (1) indicates, we can limit our search 
to those trees ?  which are alternative orderings of 
the given tree ? . 
Inter-dependencies among ordering decisions 
within different constituents (e.g., for achieving 
parallelism) make the global sentence ordering 
problem challenging and are certainly worth 
investigating in future work.  For the present, we 
constrain the possible model types considered here 
by assuming that the ordering of any constituent is 
independent of the ordering within other 
constituents in the tree, including its daughters; 
consequently, 
( ) ( )
( )C constits
P P C
?
?
?
= ?  
Given this independence assumption, the only 
possible ordered trees are trees built with non-
terminal constituents computed as follows: for 
each ( )B constits ?? , 
( )
: _ ( )
* arg max
C B unordered set C
C P C
=
=  
In fact, we can further constrain our search for the 
best ordering of each unordered constituent B, 
since C?s head must match B?s head: 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C
=
=
=  
Thus, we have reduced the problem to finding the 
best ordering of each constituent of the unordered 
tree. 
Now if we wish to condition on some feature ( )x f ?= , then we must first predict it as follows: 
( ) ( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P x P C x
=
=
=  
If x is truly a feature of ?  and does not depend on 
any particular ordering of any constituent in ? , 
then ( )P x  is constant, and we do not need to 
compute it in practice. In other words, 
( )
: _ ( )
& ( ) ( )
* arg max
C B unordered set C
head B head C
C P C x
=
=
=       (2) 
Hence, even for a joint model ( )P C , we can 
condition on features that are fixed in the given 
unordered tree ?  without first predicting them. 
The joint models described here are of this form. 
For this reason, when we describe a distribution ( )P C x , unless we explicitly state otherwise, we 
are actually describing the part of the joint model 
that is of interest. As justified above, we do not 
need to compute ( )P x  and will simply present 
alternative forms of ( )P C x . 
We can factor a distribution ( )P C x  in many 
different ways using the chain rule. As our starting 
point we adopt the class of models called Markov 
grammars.3 We first consider a left-to-right 
Markov grammar of order j that expands C by 
predicting its daughters 1,..., nD D  from left-to-
right, one at a time, as shown in Figure 2: in the 
figure. iD  depends only on ( i jD ? , ?, 1iD ? ), and 
the parent category C ., according to the 
distribution in equation (3). 
 
i?
Figure 2: Left-to-right Markov grammar. 
( ) ( )1
1
,..., , ,
n
i i i j
i
P C h P d d d c h
? ?
=
= ?  (3) 
In order to condition on another feature of each 
ordered daughter iD , such as its semantic relation 
i? to the head constituent H, we also first predict 
it, according to the chain rule. The result is the 
semantic Markov grammar in equation (4):  
( ) ( )( )
1 1
1 1 1
, ,..., , , ,
, , ,..., , , ,
n i i i i j i j
i i i i i i j i j
P d d c h
P C h
P d d d c h
? ? ?
? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (4) 
Thus, the model predicts semantic relation i? and 
then the label id  in the context of that semantic 
relation. We will refer to this model as Type 1 
(T1). 
As an extension to model Type 1, we include 
features computed by the following functions on 
the set i?  of daughters of C already ordered (see 
Figure 2): 
? Number of daughters already ordered (size 
of i? ) 
? Number of daughters in i?  having a 
particular label for each of the possible 
constituent labels {NP, AUXP, VP, etc.} 
(24 for German, 23 for French) 
We denote that set of features in shorthand as ( )if ? . With this extension, a model of Markov 
                                                          
3
 A ?Markov grammar? is a model of constituent 
structure that starts at the root of the tree and assigns 
probability to the expansion of a non-terminal one 
daughter at a time, rather than as entire productions 
(Charniak, 1997 & 2000). 
order j can potentially have an actual Markov order 
greater than j. Equation (5) is the extended model, 
which we will refer to as Type 2 (T2): 
( ) ( )( )( )( )
1 1
1 1 1
, ,..., , , , ,
, , ,..., , , , ,
n i i i i j i j i
i i i i i i j i j i
P d d c h f
P C h
P d d d c h f
? ? ? ?
? ? ? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
=
? ??
? ?
? ?
?  (5) 
As an alternative to a left-to-right expansion, we 
can also expand a constituent in a head-driven 
fashion. We refer the reader to (Ringger et al, 
2003) for details and evaluations of several head-
driven models (the missing ?T3?, ?T4?, and ?T6? 
in this discussion). 
2.2 Conditional Models 
We now consider more complex models that use 
additional features. We define a function ( )g X on 
constituents, where the value of ( )g X represents a 
set of many lexical, syntactic, and semantic 
features of X (see section 5.2 for more details). No 
discourse features are included for the present 
work. We condition on 
? ( )g B , where B is the unordered constituent 
being ordered 
? ( )g H , where H is the head of B 
? ( )Bg P , where BP  is the parent of B, and 
? ( )Bg G , where BG  is the grandparent of B. 
These features are fixed in the given unordered tree 
? , as in the discussion of equation (2), hence the 
resulting complex model is still a joint model.   
Up until this point, we have been describing joint 
generative models that describe how to generate an 
ordered tree from an unordered tree. These models 
require extra effort and capacity to accurately 
model the inter-relations among all features. Now 
we move on to truly conditional models by 
including features that are functions on the set i?  
of daughters of C yet to be ordered. In the 
conditional models we do not need to model the 
interdependencies among all features. We include 
the following: 
? Number of daughters remaining to be 
ordered (size of i? ) 
? Number of daughters in i?  having a 
particular label 
As before, we denote these feature sets in 
shorthand as ( )if ? . The resulting distribution is 
represented in equation (6), which we will refer to 
as Type 5 (T5): 
( )
( )
( )
1 1
1 1 1
( ), ( ), ( ), ( )
, ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
, , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
B B
i i i j i j
i
n B B i i
i i i i i j i j
i
B B i i
P C g H g B g P g G
d d c h
P
g H g B g P g G f f
d d c h
P d
g H g B g P g G f f
? ?
?
? ?
? ? ?
? ?
? ? ? ?
=
? ? ? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
=
? ?
? ?
? ?
?
? ?
? ?
? ?
? ?
? ?
? ?
?
   (6) 
All models in this paper are nominally Markov 
order 2, although those models incorporating the 
additional feature functions ( )if ?  and ( )if ?  
defined in Section 2.2 can be said to have higher 
order. 
2.3 Binary conditional model 
We introduce one more model type called the 
binary conditional model. It estimates a much 
simpler distribution over the binary variable ?  
called ?sort-next? with values in {yes, no} 
representing the event that an as-yet unordered 
member D of i?  (the set of as-yet unordered 
daughters of parent C, as defined above) should be 
?sorted? next, as illustrated in Figure 3. 
i?i?
?
 
Figure 3: Binary conditional model. 
The conditioning features are almost identical to 
those used in the left-to-right conditional models 
represented in equation (6) above, except that id  
and i?  (the semantic relation of D with head H) 
appear in the conditional context and need not first 
be predicted. In its simple form, the model 
estimates the following distribution: 
( )
1 1, , , ,..., , , , ,
( ), ( ), ( ), ( ), , ( )
i i i i i j i j
i
B B i i
d d d c h
P
g H g B g P g G f f
? ? ?
?
? ?
? ? ? ?
? ?
? ?
? ?
? ?
   (7) 
In our shorthand, we will call this Type 7 (T7). We 
describe how to apply this model directly in a left-
to-right ?sorting? search later in the section on 
search. 
3 Estimation 
We estimate a model?s distributions with 
probabilistic decision trees (DTs).4 We build 
decision trees using the WinMine toolkit 
(Chickering, 2002). WinMine-learned decision 
trees are not just classifiers; each leaf is a 
conditional probability distribution over the target 
random variable, given all features available in 
training; hence the tree as a whole is an estimate of 
the conditional distribution of interest. The primary 
advantage of using decision trees, is the automatic 
feature selection and induction from a large pool of 
features. 
We train four models for German and French 
each. One model is joint (T1); one is joint with 
additional features on the set of daughters already 
ordered (T2); one is conditional (T5). In addition, 
we employ one binary conditional DT model (T7), 
both with and without normalization (see equation 
(8)). 
                                                          
4
 Other approaches to feature selection, feature 
induction, and distribution estimation are certainly 
possible, but they are beyond the scope of this paper. 
One experiment using interpolated language modeling 
techniques is described in (Ringger et al, 2003) 
4 Search 
4.1 Exhaustive search 
Given an unordered tree ?  and a model of 
constituent structure O of any of the types already 
presented, we search for the best ordered tree ?  
that maximizes ( )OP ?  or ( )OP ? ? , as 
appropriate, with the context varying according to 
the complexity of the model. Each of our models 
(except the binary conditional model) estimates the 
probability of an ordering of any given constituent 
C in ? , independently of the ordering inside other 
constituents in ? . The complete search is a 
dynamic programming (DP) algorithm, either left-
to-right in the daughters of C (or head-driven, 
depending on the model type). The search can 
optionally maintain one non-statistical constraint 
we call Input-Output Coordination Consistency 
(IOCC), so that the order of coordinated 
constituents is preserved as they were specified in 
the given unordered tree. For these experiments, 
we employ the constraint. 
4.2 Greedy search for binary conditional 
model 
The binary conditional model can be applied in a 
left-to-right ?sorting? mode (Figure 3). At stage i, 
for each unordered daughter jD , in i? , the model 
is consulted for the probability of j yes? = , 
namely the probability that jD  should be placed to 
the right of the already ordered sister constituents 
i? . The daughter in i?  with the highest 
probability is removed from i?  to produce 1i? +  
and added to the right of i? to produce 1i? + . The 
search proceeds through the remaining unordered 
constituents until all constituents have been 
ordered in this greedy fashion. 
4.3 Exhaustive search for binary conditional 
model 
In order to apply the binary conditional model in 
the exhaustive DP search, we normalize the model 
at every stage of the search and thereby coerce it 
into a probability distribution over the remaining 
daughters in i? . We represent the distribution in 
?equation? (7) in short-hand as ( ), , iP d? ? ? , 
with i?  representing the contextual features for the 
given search hypothesis at search stage i. Thus, our 
normalized distribution for stage i is given by 
equation (8). Free variable j represents an index on 
unordered daughters in i? , as does k. 
( ) ( )
( )
1
, ,
, ,
, ,
i
j j j i
j j j i
k k k i
k
P yes d
P D d
P yes d
?
? ?
?
? ?
=
= ?
? =
= ?
?
 (8) 
This turns out to be the decision tree analogue of a 
Maximum Entropy Markov Model (MEMM) 
(McCallum et al, 2000), which we can refer to as a 
DTMM. 
5 Experiments 
5.1 Training 
We use a training set of 20,000 sentences, both 
for French and German. The data come from 
technical manuals in the computer domain. For a 
given sentence in our training set, we begin by 
analyzing the sentence as a surface syntax tree and 
an abstract predicate argument structure using the 
NLPWin system (Heidorn, 2000). By consulting 
these two linked structures, we produce a tree with 
all of the characteristics of trees seen by the 
Amalgam ordering stage at generation run-time 
with one exception: these training trees are 
properly ordered. The training trees include all 
features of interest, including the semantic 
relations among a syntactic head and its modifiers. 
We train our order models from the constituents of 
these trees. NLPWin parser output naturally 
contains errors; hence, the Amalgam training data 
is imperfect. 
5.2 Selected Features 
A wide range of linguistic features is extracted 
for the different decision tree models. The number 
of selected features for German reaches 280 (out of 
651 possible features) in the binary conditional 
model T7. For the French binary conditional 
model, the number of selected features is 218 (out 
of 550). The binary conditional models draw from 
the full set of available features, including: 
? lexical sub-categorization features such as 
transitivity and compatibility with clausal 
complements 
? lemmas (word-stems) 
? semantic features such as the semantic 
relation and the presence of 
quantificational operators 
? length of constituent in words 
? syntactic information such as the label and 
the presence of syntactic modifiers 
5.3 Evaluation 
To evaluate the constituent order models in 
isolation, we designed our experiments to be 
independent of the rest of the Amalgam sentence 
realization process. We use test sets of 1,000 
sentences, also from technical manuals, for each 
language. To isolate ordering, for a given test 
sentence, we process the sentence as in training to 
produce an ordered tree ?  (the reference for 
evaluation) and from it an unordered tree ? . 
Given ? , we then search for the best ordered tree 
hypothesis ??  using the model in question. 
We then compare ?  and ?? . Because we are 
only ordering constituents, we can compare ? and 
??  by comparing their respective constituents. For 
each C in ? , we measure the per-constituent edit 
distance D, between C and its counterpart C? in ??  
as  follows: 
1. Let d be the edit distance between the 
ordered set of daughters in each, with the 
only possible edit operators being insert and 
delete 
2. Let the number of moves / 2m d= , since 
insertions and deletions can be paired 
uniquely 
3. Divide by the total number of 
daughters: ( )/D m daughters C=  
This metric is like the ?Generation Tree Accuracy? 
metric of Bangalore & Rambow (2000), except 
that there is no need to consider cross-constituent 
moves. The total score for the hypothesis tree ??  is 
the mean of the per-constituent edit distances. 
For each of the models under consideration and 
each language, we report in Table 1 the average 
score across the test set for the given language. The 
first row is a baseline computed from randomly 
scrambling constituents (mean over four 
iterations). 
Model German French 
Baseline (random) 35.14 % 34.36 % 
T1: DT joint 5.948% 3.828% 
T2: DT joint 
with ( )if ?   5.852% 4.008% 
T5: DT conditional 6.053% 4.271% 
T7: DT binary cond., 
greedy search 3.516% 1.986% 
T7: DT normalized 
binary conditional, 
exhaustive search 
3.400% 1.810% 
Table 1: Mean per-constituent edit distances for 
German & French. 
5.4 Discussion 
For both German and French, the binary 
conditional DT model outperforms all other 
models. Normalizing the binary conditional model 
and applying it in an exhaustive search performs 
better than a greedy search. All score differences 
are statistically significant; moreover, manual 
inspection of the differences for the various models 
also substantiates the better quality of those models 
with lower scores. 
With regard to the question of conditional versus 
joint models, the joint models (T1, T2) outperform 
their conditional counterparts (T5). This may be 
due to a lack of sufficient training data for the 
conditional models. At this time, the training time 
of the conditional models is the limiting factor. 
There is a clear disparity between the 
performance of the German models and the 
performance of the French models. The best 
German model is twice as bad as the best French 
model.  (For a discussion of the impact of 
modeling German verb position, please consult 
(Ringger et al, 2003).) 
 Baseline 
(random) 
Greedy, 
IOCC Greedy 
DP,  
IOCC DP 
Total Sentences 2416 2416 2416 2416 2416 
Mean Tokens/Sentence 23.59 23.59 23.59 23.59 23.59 
Time/Input (sec.) n/a 0.01 0.01 0.39 0.43 
Exact Match 0.424% 33.14% 27.53% 33.53% 35.72% 
Coverage 100% 100% 100% 100% 100% 
Mean Per-Const. Edit Dist. 38.3% 6.02% 6.84% 5.25% 4.98% 
Mean NIST SSA -16.75 74.98 67.19 74.65 73.24 
Mean IBM Bleu Score 0.136 0.828 0.785 0.817 0.836 
Table 2: DSIF-Amalgam ordering performance on WSJ section 23. 
6 Evaluation on the Penn TreeBank 
Our goal in evaluating on Penn Tree Bank (PTB) 
data is two-fold: (1) to enable a comparison of 
Amalgam?s performance with other systems 
operating on similar input, and (2) to measure 
Amalgam?s capabilities on less domain-specific 
data than technical software manuals. We derive 
from the bracketed tree structures in the PTB using 
a deterministic procedure an abstract 
representation we refer to as a Dependency 
Structure Input Format (DSIF), which is only 
loosely related to NLPWin?s abstract predicate-
argument structures. 
The PTB to DSIF transformation pipeline 
includes the following stages, inspired by 
Langkilde-Geary?s (2002b) description: 
A. Deserialize the tree 
B. Label heads, according to Charniak?s head 
labeling rules (Charniak, 2000) 
C. Remove empty nodes and flatten any 
remaining empty non-terminals 
D. Relabel heads to conform more closely to the 
head conventions of NLPWin 
E. Label with logical roles, inferred from PTB 
functional roles 
F. Flatten to maximal projections of heads 
(MPH), except in the case of conjunctions 
G. Flatten non-branching non-terminals 
H. Perform dictionary look-up and 
morphological analysis 
I. Introduce structure for material between 
paired delimiters and for any coordination 
not already represented in the PTB 
J. Remove punctuation 
K. Remove function words 
L. Map the head of each maximal projection to 
a dependency node, and map the head?s 
modifiers to the first node?s dependents, 
thereby forming a complete dependency tree. 
To evaluate ordering performance alone, our data 
are obtained by performing all of the steps above 
except for (J) and (K). We employ only a binary 
conditional ordering model, found in the previous 
section to be the best of the models considered. To 
train the order models, we use a set of 10,000 
sentences drawn from the standard PTB training 
set, namely sections 02?21 from the Wall Street 
Journal portion of the PTB (the full set contains 
approx. 40,000 sentences). For development and 
parameter tuning we used a separate set of 2000 
sentences drawn from sections 02?21. 
Decision trees are trained for each of five 
constituent types characterized by their head 
labels: adjectival, nominal, verbal, conjunctions 
(coordinated material), and other constituents not 
already covered. The split DTs can be thought of 
as a single DT with a five-way split at the top 
node. 
Our DSIF test set consists of the blind test set 
(section 23) of the WSJ portion of the PTB. At 
run-time, for each converted tree in the test set, all 
daughters of a given constituent are first permuted 
randomly with one another (scrambled), with the 
option for coordinated constituents to remain 
unscrambled, according to the Input-Output 
Coordination Consistency (IOCC) option. For a 
given unordered (scrambled) constituent, the 
appropriate order model (noun-head, verb-head, 
etc.) is used in the ordering search to order the 
daughters. Note that for the greedy search, the 
input order can influence the final result; therefore, 
we repeat this process for multiple random 
scramblings and average the results. 
We use the evaluation metrics employed in 
published evaluations of HALogen, FUF/SURGE, 
and FERGUS (e.g., Calloway, 2003), although our 
results are for ordering only. Coverage, or the 
percentage of inputs for which a system can 
produce a corresponding output, is uninformative 
for the Amalgam system, since in all cases, it can 
generate an output for any given DSIF. In addition 
to processing time per input, we apply four other 
metrics: exact match, NIST simple string accuracy 
(the complement of the familiar word error rate), 
the IBM Bleu score (Papineni et al, 2001), and the 
intra-constituent edit distance metric introduced 
earlier. 
We evaluate against ideal trees, directly 
computed from PTB bracketed tree structures. The 
results in Table 2 show the effects of varying the 
IOCC parameter. For both trials involving a greedy 
search, the results were averaged across 25 
iterations. As should be expected, turning on the 
input-output faithfulness option (IOCC) improves 
the performance of the greedy search. Keeping 
coordinated material in the same relative order 
would only be called for in applications that plan 
discourse structure before or during generation. 
7 Conclusions and Future Work 
The experiments presented here provide 
conclusive reasons to favor the binary conditional 
model as a model of constituent order. The 
inclusion of linguistic features is of great value to 
the modeling of order, specifically in verbal 
constituents for both French and German. 
Unfortunately space did not permit a thorough 
discussion of the linguistic features used. Judging 
from the high number of features that were 
selected during training for participation in the 
conditional and binary conditional models, the 
availability of automatic feature selection is 
critical. 
Our conditional and binary conditional models 
are currently lexicalized only for function words; 
the joint models not at all. Experiments by Daum? 
et al(2002) and the parsing work of Charniak 
(2000) and others indicate that further 
lexicalization may yield some additional 
improvements for ordering. However, the parsing 
results of Klein & Manning (2003) involving 
unlexicalized grammars suggest that gains may be 
limited. 
For comparison, we encourage implementers of 
other sentence realization systems to conduct 
order-only evaluations using PTB data. 
Acknowledgements 
We wish to thank Irene Langkilde-Geary and 
members of the MSR NLP group for helpful 
discussions.  Thanks also go to the anonymous 
reviewers for helpful feedback. 
References  
Aikawa T., Melero M., Schwartz L. Wu A. 2001. 
Multilingual sentence generation. In Proc. of 8th 
European Workshop on NLG. pp. 57-63. 
Bangalore S. Rambow O. 2000. Exploiting a 
probabilistic hierarchical model for generation. 
In Proc. of COLING. pp. 42-48. 
Calloway, C. 2003. Evaluating Coverage for Large 
Symbolic NLG Grammars.  In Proc. of IJCAI 
2003. pp 811-817. 
Charniak E. 1997. Statistical Techniques for 
Natural Language Parsing, In AI Magazine. 
Charniak E. 2000. A Maximum-Entropy-Inspired 
Parser. In Proc. of ACL. pp.132-139. 
Chickering D. M. 2002. The WinMine Toolkit. 
Microsoft Technical Report 2002-103. 
Corston-Oliver S., Gamon M., Ringger E., Moore 
R. 2002. An overview of Amalgam: a machine-
learned generation module. In Proc. of INLG. 
pp.33-40. 
Daum? III H., Knight K., Langkilde-Geary I., 
Marcu D., Yamada K. 2002. The Importance of 
Lexicalized Syntax Models for Natural 
Language Generation Tasks. In Proc. of INLG. 
pp. 9-16. 
Gamon M., Ringger E., Corston-Oliver S. 2002a. 
Amalgam: A machine-learned generation 
module. Microsoft Technical Report 2002-57. 
Gamon M., Ringger E., Corston-Oliver S., Moore 
R. 2002b. Machine-learned contexts for 
linguistic operations in German sentence 
realization. In Proc. of ACL. pp. 25-32. 
Heidorn G. 2000. Intelligent Writing Assistance. In 
A Handbook of Natural Language Processing,, 
R. Dale, H. Moisl, H. Somers (eds.). Marcel 
Dekker, NY. 
Klein D., Manning C. 2003. "Accurate 
Unlexicalized Parsing." In Proceedings of ACL-
03. 
Langkilde I. 2000. Forest-Based Statistical 
Sentence generation. In Proc. of NAACL. pp. 
170-177. 
Langkilde-Geary I. 2002a. An Empirical 
Verification of Coverage and Correctness for a 
General-Purpose Sentence Generator. In Proc. of 
INLG. pp.17-24. 
Langkilde-Geary, I. 2002b. A Foundation for 
General-purpose Natural Language Generation: 
Sentence Realization Using Probabilistic Models 
of Language. PhD Thesis, University of 
Southern California. 
Langkilde I., Knight K. 1998a. The practical value 
of n-grams in generation. In Proc. of 9th 
International Workshop on NLG. pp. 248-255. 
Langkilde I., Knight K. 1998b. Generation that 
exploits corpus-based statistical knowledge. In 
Proc. of ACL and COLING. pp. 704-710. 
McCallum A., Freitag D., & Pereira F. 2000. 
?Maximum Entropy Markov Models for 
Information Extraction and Segmentation.? In 
Proc. Of ICML-2000. 
Papineni, K.A., Roukos, S., Ward, T., and Zhu, 
W.J. 2001. Bleu: a method for automatic 
evaluation of machine translation. IBM 
Technical Report RC22176 (W0109-022). 
Reiter E. and Dale R. 2000. Building natural 
language generation systems. Cambridge 
University Press, Cambridge. 
Ringger E., Gamon M., Smets M., Corston-Oliver 
S. and Moore R. 2003 Linguistically informed 
models of constituent structure for ordering in 
sentence realization. Microsoft Research 
technical report MSR-TR-2003-54. 
Smets M., Gamon M., Corston-Oliver S. and 
Ringger E. (2003) The adaptation of a machine-
learned sentence realization system to French. 
In Proceedings of EACL. 
323
324
325
326
327
328
329
330
Machine Translation as a testbed for multilingual analysis 
Richard Campbell, Carmen Lozano, Jessie Pinkham and Martine Smets* 
Microsoft Research 
One Microsoft Way 
Redmond, WA 98052  USA 
{richcamp, clozano, jessiep, martines}@microsoft.com 
*to whom all correspondence should be addressed 
 
 
Abstract 
We propose that machine translation (MT) is a 
useful application for evaluating and deriving 
the development of NL components, 
especially in a wide-coverage analysis system. 
Given the architecture of our MT system, 
which is a transfer system based on linguistic 
modules, correct analysis is expected to be a 
prerequisite for correct translation, suggesting 
a correlation between the two, given relatively 
mature transfer and generation components.  
We show through error analysis that there is 
indeed a strong correlation between the quality 
of the translated output and the subjectively 
determined goodness of the analysis.  We use 
this correlation as a guide for development of 
a coordinated parallel analysis effort in 7 
languages. 
1  Introduction 
The question of how to test natural language 
analysis systems has been central to all natural 
language work in the past two decades.  It is a 
difficult question, for which researchers have 
found only partial answers.  The most common 
answer is component testing, where the component 
is compared against a standard of goodness, 
usually the Penn Treebank for English (Marcus et 
al., 1993),  allowing a numerical score of precision 
and recall (e.g. Collins, 1997). 
 Such methods have limitations, however, and 
need to be supplemented by additional methods.  
One limitation is the availability of annotated 
corpora, which do not exist for all languages.  
Secondly, comparison to an annotated corpus can 
only measure how well a system produces the kind 
of analysis for which the corpus is annotated, e.g. 
labeled bracketing of surface syntax.  Evaluation 
of analysis of deeper, more semantically 
descriptive, levels requires additional annotated 
corpora, which may not exist.  A more 
fundamental limitation of such methods is that 
they measure the goodness of a grammar without 
taking into account what the grammar is good for.  
This limitation is overcome, we claim, only by 
measuring the goodness of a grammar by its 
success in real-world applications. 
 We propose that machine translation (MT) is a 
good application to evaluate and drive the 
development of analysis components when the 
transfer component is based on linguistic modules.  
Multi-lingual applications such as MT allow 
evaluation of system components that overcomes 
the limitations mentioned above, and therefore 
serves as a useful complement to other evaluation 
techniques.  Another significant advantage to 
using MT as a testbed for the analysis system is 
that it prioritizes analysis problems, highlighting 
those problems that have the greatest negative 
effect on translation output. 
 In this paper, we give an overview of 
NLPWin, a multi-application natural language 
analysis and generation system under development 
at Microsoft Research (Jensen et al, 1993; Gamon 
et al, 1997; Heidorn 2000), incorporating analysis 
systems for 7 languages (Chinese, English, French, 
German, Japanese, Korean and Spanish). Our 
discussion focuses on a description of the three 
components of the analysis system (called sketch, 
portrait and logical form) with a particular 
emphasis on the logical form derived as the end-
product, which serves as the medium for transfer 
in our MT system.  
 We also give an overview of the architecture 
of the MSR-MT system, and of the evaluation we 
use to measure correctness of the translations. We 
demonstrate the correlation between the scores 
assigned to translation outputs and the correctness 
of the analysis, using as illustration two language-
pairs at different stages of development:  Spanish-
English (SE) translation, as a testbed for the 
Spanish analysis system, and French-English (FE) 
translation, as a testbed for the French analysis 
system.   
2  Overview of the analysis component of 
NLPWin 
Analysis produces three representations for the 
input sentence: sketch, portrait and logical form1.  
Sketch is the initial tree representation for the 
sentence, along with its associated attribute-value 
structure. An example of sketch is given in Figure 
1, which shows the sketch tree for sentence (1). 
 
(1) 
Ce  format est pris   en charge par Windows 2000 
this format is   taken in charge by  Windows 2000 
?This format is supported by Windows 2000? 
 
 
Figure 1 :  Sketch analysis of (1) 
 
Attachment sites for post-modifiers are not 
determined in sketch.  In most cases, the 
information available as the syntactic tree is built 
is not sufficient to determine where e.g. 
prepositional phrases or relative clauses should be 
attached. Post-modifiers are thus systematically 
attached to the closest possible attachment site, 
and reattached, if necessary, by the reattachment 
module, a set of heuristic rules. 
 Reattachment rules apply to the sketch to 
produce the portrait; the portrait analysis of (1) is 
given in Figure 2, where the PP expressing the 
agent of the passive construction, originally 
attached to PP1 in sketch (see Figure 1) has been 
reattached at the sentence level. 
 
                                                     
1 The presentation of the analysis module is very 
simplified, but sufficient for our current discussion. 
More details can be found in the references.  
 
Figure 2: Portrait analysis of (1) 
 
 The portrait is the input to the computation of 
the logical form (LF), a labeled directed unordered 
graph representing the deep syntactic relations 
among the content words of the sentence (i.e., 
basic predicate-argument structure), along with 
some semantic information, such as functional 
relations expressed by certain prepositions.2 At 
this level, the difference between active and 
passive constructions is normalized; control 
relations and long-distance dependencies, such as 
subjects of infinitives, arguments associated with 
gaps, etc., are resolved.  The LF of (1) is shown in 
Figure 3.  Note that the surface subject of the 
passive is rendered as the Dobj (deep object) in 
LF, and the par-phrase as the Dsub (deep subject). 
 
 
Figure 3 :  LF analysis of (1) 
 
 Modifications to any of the analysis 
components are tested using monolingual 
regression files containing thousands of analyzed 
sentences; differences caused by the modification 
are examined manually by the linguist responsible 
for the change (Suzuki, 2002).  This process serves 
as an initial screening to ensure that modifications 
to the analysis have the desired effect. 
3 MSR-MT 
In this section we review the basics of the MSR-
MT translation system and its evaluation.  The 
reader is referred to Pinkham et al (2001) and 
Richardson et al (2001) for further details on the 
French and Spanish versions of the system. The 
overall architecture and basic component structure 
                                                     
2   LF as described here corresponds to the PAS 
representation of Campbell and Suzuki (2002). 
are the same for both the FE and SE versions of 
the system. 
3.1 Overview 
MSR-MT uses the broad coverage analysis system 
described in Section 2, a large multi-purpose 
source-language dictionary, a learned bilingual 
dictionary, an application independent target-
language generation component and a transfer 
component. 
 The transfer component consists of transfer 
patterns automatically acquired from sentence-
aligned bilingual corpora (described below) using 
an alignment algorithm described in detail in 
Menezes and Richardson (2001). Training takes 
place on aligned sentences which have been 
analyzed by the source- and target-language 
analysis systems to yield logical forms. The 
logical form structures, when aligned, allow the 
extraction of lexical and structural translation 
correspondences which are stored for use at 
runtime in the transfer database. See Figure 4 for 
an overview of the training process. 
 The transfer database is trained on 350,000 
pairs of aligned sentences from computer manuals 
for SE, and 500,000 pairs of aligned Canadian 
parliamentary data (the Hansard corpus) for FE.  
 
 
Figure 4:  MSR-MT training phase 
3.2   Evaluation of MSR-MT 
Seven evaluators are asked to evaluate the same 
set of sentences. For each sentence, raters are 
presented with a reference sentence, the original 
English sentence from which the human French 
and Spanish translations were derived, and MSR-
MT?s machine translation.3 In order to maintain 
                                                     
3 Microsoft manuals are written in English and 
translated by hand into other languages. We use these 
translations as input to our system, and translate them 
back into English. 
consistency among raters who may have different 
levels of fluency in the source language, raters are 
not shown the original French or Spanish sentence 
(for similar methodologies, see Ringger et al, 
2001; White et al, 1993).  
 All the raters enter scores reflecting the 
absolute quality of the translation as compared to 
the reference translation given. The overall score 
of a sentence is the average of the scores given by 
the seven raters. Scores range from 1 to 4, with 1 
meaning unacceptable (not comprehensible), 2 
meaning possibly acceptable (some information is 
transferred accurately), 3 meaning acceptable (not 
perfect, but accurate transfer of all important 
information, and 4 meaning ideal (grammatically 
correct and all the important information is 
transferred).  
4 Examples from FE and SE 
In this section we discuss specific examples to 
illustrate how results from MT evaluation help us 
to test and develop the analysis system. 
4.1  FE translation: the Hansard corpus 
The evaluation we are discussing in this section 
was performed in January 2002, at the beginning 
of our effort on the Hansard corpus. The 
evaluation was performed on a corpus of 250 
sentences, of which 55.6% (139 sentences) were 
assigned a score of 2 or lower, 30.4% (76 
sentences) were assigned a score greater than 2 but 
not greater than 3, and 14% (35 sentences) were 
assigned a score greater than 3. 
 Examination of French sentences receiving 
low-score translations led to the identification of 
some classes of analysis problems, such as the 
following: 
- mis-identification of vocatives 
- clefts not represented correctly 
- mis-analysis of ce qui / ce que free relatives 
- bad representation of complex inversion 
(pronoun-doubling of inverted subject) 
- no treatment of reflexives 
- fitted parses (i.e., not spanning the sentence) 
Most of the problematic structures are 
characteristic of spoken language as opposed to 
more formal, written styles (vocatives, clefts, 
direct questions), and had not been encountered in 
our previous work, which had involved mostly 
translation of technical manuals. Other problems 
(free relatives, reflexives) are analysis issues that 
we had not yet addressed. Fitted parses are parses 
that do not span the whole sentence, but are pieced 
together by the parser from partial parses; fitted 
parses usually result in poor translations. 
 Examples of translations together with their 
score are given in Table I. The source sentences 
are the French sentences, the reference sentence is 
the human translation to which the translation is 
compared by the evaluators, and the translation is 
the output of MSR-MT. Each of the three 
categories considered above is illustrated by an 
example. 
 Sentence (2) (with a score of 1.5) is a direct 
question with complex inversion and the doubled 
subject typical of that construction. In the LF for 
(2), les ministres des finances is analyzed as a 
modifier, because the verb r?unir already has a 
subject, the pronoun ils ?they?.  There are a couple 
of additional problems with this sentence: si is 
analyzed as the adverb meaning ?so? instead of as 
the conjunction meaning ?if?, and a direct question 
is analyzed as a complement clause; the sketch and 
LF analyses of this sentence are given in the 
Appendix..  The MSR-MT translation of this 
sentence has a very low score, reflecting the 
severity of the analysis problems. 
 The two other sentences, on the other hand, do 
not have analysis problems: the poor translation of 
(3) (score 2.16) is caused by bad alignment (droit 
translates as right instead of law), and the 
translation of (4) (score 3) is not completely fluent, 
but this is due to an English generation problem, 
rather than to a French analysis problem. This last 
sentence is the most correct with appropriate 
lexical items and has the highest score of the three. 
 Of the 139 sentences with score 2 or lower, 
73% were due to analysis problems, and 24% to 
alignment problems. Most of the rest had bugs 
related to the learned dictionary.  There were a few 
cases of very free translations, where the reference 
translation was very far from the French sentence, 
and our translation, based on the source sentence, 
was therefore penalized.  
 These figures show that, at this stage of 
development of our system, most of the problems 
in translation come from analysis. Translation can 
be improved by tackling analysis problems 
exhibited by the lowest scoring sentences, and, 
conversely, analysis issues can be discovered by 
looking at the sentences with the lowest translation 
score.  
 The next section gives examples of issues with 
the SE system, which is more mature than the FE 
system. 
 
4.2  SE translation: Technical manuals 
An evaluation of the Spanish-English MT system 
was also performed in January 2002, after work on 
the MT system had been progressing for 
approximately a year and a half.  The SE system 
was developed and tested using a corpus of 
sentences from Microsoft technical manuals.  A 
set of 600 unseen sentences was used for the 
evaluation.  
 Out of a total of 600 sentences, the number of 
sentences with a score from 3 to 4 was 251 (42%), 
the number of sentences with a score greater than 
2 but less than 3 was 186 (31%), and the 
remaining 163 sentences, (27%) had a score of 2 
or lower. Of these 163 sentences with the lowest 
scores, 50% (82 sentences) had analysis problems, 
and 17% of them (29 sentences) had fitted parses.  
A few of the fitted parses, 7 sentences out of 29, 
had faulty input, e.g. input that contained unusual 
characters or punctuation, typos, or sentence 
fragments.   
 Typical analysis problems that led to poor 
translation in the SE system include the following: 
- incorrect analysis of arguments in relative 
clauses,  especially those with a single 
argument (and a possible non-overt subject) 
- failure to identify the referent of clitic le (i.e. 
usted ?you?) in imperative sentences in LF 
- mis-analysis of Spanish reflexive or 
se constructions in LF 
- incorrect syntactic analysis of homographs 
- incorrect analysis of coordination  
- mis-identification of non-overt or controlled 
subjects  
- fitted parses  
 Table II contains sample sentences from the 
SE evaluation.  For each row, the second column 
displays the Spanish source sentence with the 
reference sentence in the next column, the 
translation produced by the MT system is in the 
fourth column, and the score for the translation 
assigned by the human evaluators in the last 
column.    
# Source  Reference Translation Score
(2) Si tel n'?tait pas le cas, pourquoi les 
ministres des Finances des provinces se 
seraient-ils r?unis hier pour essayer de 
s'entendre sur un programme commun ? 
soumettre au ministre des Finances? 
If that were not the case, 
why were the finance 
ministers of the provinces 
coalescing yesterday to try 
and come up with a joint 
program to bring to the 
finance minister?. 
Not was the case that they have 
the ministers met why 
yesterday Finances of the 
provinces trying to agree on a 
common program to bring 
Finances for the minister this so 
like? 
1.5 
(3) Nous ne pouvons pas appuyer cette 
motion apr?s que le Bloc qu?b?cois ait 
refus? de reconna?tre la primaut? du droit 
et de la d?mocratie pour  tous. 
 
We cannot support this 
motion after seeing the 
Bloc Quebecois refuse to 
recognize the rule of law 
and the principle of 
democracy for all. 
We cannot support this motion 
after the Bloc Quebecois has 
refused to recognize the rule of 
the right and democracy for all. 
2.16 
(4) En tant que membre de l'opposition 
officielle, je continuerai d'exercer des 
pressions sur le gouvernement pour qu'il 
tienne ses promesses ? cet ?gard. 
As a member of the official 
opposition I will continue 
to pressure the government 
to fulfil its promises in this 
regard. 
As member of the official 
opposition, I will continue to 
exercise pressures on the 
government for it to keep its 
promises in this regard. 
3 
Table I:  Examples of FE translation 
# Source Reference Translation Score
(5) Este procedimiento s?lo es aplicable si 
est? ejecutando una versi?n de idioma de 
Windows 2000 que no coincida con el 
idioma en el que desee escribir. 
This procedure applies only 
if you are running a 
language version of 
Windows 2000 that doesn't 
match the language you 
want to type 
This procedure only applies if 
you are running a Windows 
2000 language version that does 
not match the language that you 
want to type. 
3.8 
(6) Repita este proceso hasta que haya 
eliminado todos los componentes de red 
desde las propiedades de Red, haga clic 
en Aceptar y, a continuaci?n, haga clic 
en S? cuando se le pregunte si desea 
reiniciar el equipo. 
Repeat this process until 
you have deleted all of the 
network components from 
Network properties, click 
OK, and then click Yes 
when you are prompted to 
restart your computer. 
Repeat this process until you 
have deleted all of the network 
components from the Network 
properties, you click OK, and 
you click Yes then when asking 
that to restart the computer is 
wanted for him. 
2.0 
(7) En el siguiente ejemplo se muestra el 
nombre de la presentaci?n que se est? 
ejecutando en la ventana de presentaci?n 
con diapositivas uno. 
The following example 
displays the name of the 
presentation that's currently 
running in slide show 
window one. 
In the following example, the 
display name that is being run 
in the slide show window is 
displayed I join. 
1.4 
Table II:  Examples of SE translation 
 
 In the evaluation process, human evaluators 
compared the MT translation to the reference 
sentence, in the manner described in Section 4.1.   
 Example (5), with a score of 3.8, illustrates the 
fact that human evaluators considered the 
translation ?a Windows 2000 language version? to 
be a slightly worse translation than ?a language 
version of Windows 2000? for una version de 
idioma de Windows 2000; however the difference 
is so slight as to not be considered an analysis 
problem. 
 Example (6) illustrates the failure to identify 
usted ?you? (understood as the subject of the 
imperative) as the referent of the pronominal clitic 
le; as mentioned above, this is a common source of 
bad SE translations.  The last example (7) is a 
sentence with a fitted parse due to misanalysis of a 
word as its homograph :  uno is analyzed as the 
first person singular present form of the verb unir 
?join? instead of as the noun uno ?one?; the LF of 
this sentence is given in the Appendix. 
4.3 Discussion 
The examples discussed in this section are typical:  
The sentences for which MSR-MT produces better 
translations tend to be the ones with fewer analysis 
errors, while those which are misanalyzed tend to 
be mistranslated. 
 In this way, evaluation of MT output serves as 
one way to prioritize analysis problems; that is, to 
decide which among the many different analysis 
problems lead to the most serious problems.  For 
example, the poor quality of the translation of (2) 
highlights the need for an improved analysis of 
complex inversion in the French grammar, which 
will need to be incorporated into the sketch and/or 
LF components.  Similarly, the poor translation of 
(7) indicates the need to deal better with 
homographs in the Spanish morphological or 
sketch   component. 
 More generally, the analysis of FE and SE 
translation problems has led to the lists of analysis 
problems given in Sections 4.1 and 4.2, 
respectively.  Analysis problems identified in this 
way then become priorities for grammar/LF 
development. 
5 Conclusion 
We have outlined how the output of MT can be 
used as testbed for linguistic analysis in the source 
language, supplementing other methods.  The 
main advantage of this approach, in our view, is 
that it helps to prioritize analysis problems, 
highlighting those which have the most direct 
bearing on the application(s), the correct 
functioning of which is the main goal of the 
system. 
Acknowledgements 
This paper represents the work of many people in 
the NLP group at MSR; we acknowledge their 
contributions. 
References  
Campbell, R. and H. Suzuki.  2002.  Language-neutral 
representation of syntactic structure.  In R. Malaka, 
R. Porzel and M. Stube, eds., Proceedings of the First 
International Workshop on Scalable Natural 
Language Understanding. 
Collins, M. 1997. Three generative, lexicalised models 
for statistical parsing. Proceedings of the 35th Annual 
Meeting of the ACL, Madrid.  
Gamon, M., C. Lozano, J. Pinkham and T. Reutter. 
1997. Practical experience with grammar sharing in 
multilingual NLP. In Burstein J., Leacock C., eds, 
Proceedings of the Workshop on Making NLP Work, 
ACL Conference, Madrid. 
Heidorn, G.  2000.  Intelligent writing assistance.  In R. 
Dale, H. Moisl and H. Somers, eds., Handbook of 
Natural Language Processing. 
Jensen, K., G. Heidorn and S. Richardson, eds. 1993. 
Natural Language Processing: The PLNLP Approach, 
Boston, Kluwer. 
Marcus, M., B. Santorini, and M. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the 
Penn Treebank. In Proceedings of the 31st Annual 
Meeting of the ACL. 
Menezes, A. and S. Richardson. 2001. A Best-First 
Alignment Algorithm for Automatic Extraction of 
Transfer Mappings from Bilingual Corpora. In 
Proceedings of the Data-Driven MT workshop, ACL 
2001. 
Pinkham, J., M. Corston-Oliver, M. Smets and M. 
Pettenaro, 2001.  Rapid assembly of a large-scale 
French-English MT system. In Proceedings of the 
2001 MT Summit. 
Richardson, S., W.B. Dolan, A. Menezes and J. 
Pinkham. 2001. Achieving commercial-quality 
translation with example-based methods.  In 
Proceedings of the 2001 MT Summit. 
Ringger, E.K., M. Corston-Oliver, and R.C. Moore. 
2001. Using Word-Perplexity for Automatic 
Evaluation of Machine Translation. Unpublished ms. 
Suzuki, H.  2002.  A development environment for 
large-scale multi-lingual parsing systems.  Workshop 
on Grammar Engineering and Evaluation, COLING 
2002. 
White, J.S., T.A. O'Connell, and L.M. Carlson. 1993. 
Evaluation of machine translation. In Human 
Language Technology: Proceedings of a Workshop 
(ARPA). 206-210. 
Appendix 
 
Figure 5 :  Sketch analysis of (2) 
 
 
Figure 6 :  LF analysis of (2) 
 
 
Figure 7 :  LF analysis of (7)
 
