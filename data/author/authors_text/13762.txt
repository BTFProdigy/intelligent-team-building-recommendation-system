Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 698?706,
Beijing, August 2010
Semantic Role Labeling for News Tweets 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 3Zhongyang Xiong and 2Changning Huang 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
zyxiong@cqu.edu.cn 
v-cnh@microsoft.com 
 
Abstract 
News tweets that report what is happen-
ing have become an important real-time 
information source. We raise the prob-
lem of Semantic Role Labeling (SRL) 
for news tweets, which is meaningful for 
fine grained information extraction and 
retrieval. We present a self-supervised 
learning approach to train a domain spe-
cific SRL system to resolve the problem. 
A large volume of training data is auto-
matically labeled, by leveraging the ex-
isting SRL system on news domain and 
content similarity between news and 
news tweets. On a human annotated test 
set, our system achieves  state-of-the-art 
performance, outperforming the SRL 
system trained on news. 
1 Introduction 
Tweets are text messages up to 140 characters. 
Every day, more than 50 million tweets are gen-
erated by millions of Twitter users. According to 
the investigation by Pear Analytics (2009), about 
4% tweets are related to news1. 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
1 http://blog.twitter.com/2010/02/measuring-tweets.html 
We divide news related tweets into two cate-
gories: those excerpted from news articles and 
those not. The former kind of tweets, hereafter 
called news excerpt, is formally written while 
the latter, hereafter called news tweet, varies in 
style and often is not grammatically correct. To 
understand the proportion of news tweets, we 
randomly selected 1000 tweets related to news, 
and got 865 news tweets. Following is an exam-
ple of anews tweet, containing oh, yea, which 
usually appear in spoken language, and :-(, an 
emoticon. 
oh yea and Chile earthquake the earth off it's 
axis according to NASA and shorten the day 
by a wee second :-(                                     (S1) 
News tweets arean important information 
source because they keep reporting what is hap-
pening in real time. For example, the earthquake 
near Los Angeles that happened on Tuesday, 
July 29, 2008 was first reported through news 
tweets only seconds later than the outbreak of 
the quake. Official news did not emerge about 
this event until four minutes later. By then, 
"Earthquake" was trending on Twitter Search 
with thousands of updates2. 
However, it is a daunting task for people to 
find out information they are interested in from 
such a huge number of news tweets, thus moti-
vating us to conduct some kind of information 
                                                 
2 http://blog.twitter.com/2008/07/twitter-as-news-wire.html 
698
extraction such as event mining, where SRL 
plays a crucial  role (Surdeanu et al, 2003). 
Considering Sentence 1, suppose the agent 
earthquake and the patient day for the predicate 
shorten are identified. Then it is straightforward 
to output the event Chile earthquake shorten the 
day, which captures the essential information 
encoded in this tweet. 
Following M?rquez (2009), we define SRL 
for news tweets as the task of identifying the 
arguments of a given verb as predicate in a news 
tweet and assigning them semantic labels de-
scribing the roles they play for the predicate. To 
make our method applicable to general infor-
mation extraction tasks,  rather than only to 
some special scenarios such as arresting event 
extraction, we adopt general semantic roles, i.e., 
Agent(A0), Patient(A1), Location(AM-LOC), 
Temporal(AM-TMP),etc., instead of situation-
specific roles (Fillmore et al, 2004) such as 
Suspect, Authorities, and Offense in an arrest 
frame.  
Our first attempt is to directly apply the state-
of-art SRL system (Meza-Ruiz and Riedel, 2009) 
that trained on the CoNLL 08 shared task da-
taset(Surdeanu et al, 2008), hereafter called 
SRL-BS, to news tweets. Not surprisingly, we 
observe its F1 score drops sharply from 75.5% 
on news corpus to 43.3% on our human annotat-
ed news tweets, owing much to the informal 
written style of news tweets. 
Therefore, we have to build a domain specific 
SRL system for news tweets. Given the diversi-
fied styles of news tweets, building such a sys-
tem requires a larger number of annotated news 
tweets, which are not available, and are not af-
fordable for human labeling. We propose a novel 
method to automatically annotate news tweets, 
which leverages the existing resources of SRL 
for news domain, and content similarity between 
news and news tweets. We argue that the same 
event is likely to be reported by both news and 
news tweets, which results in  content similarity 
between the news and news tweet. Further, we 
argue that the news and news tweets reporting 
the same event tend to have similar predicate-
argument structures. We tested our assumptions 
on the event Chile earthquake that happened on 
Match 2nd, 2010. We got 261 news and 722 news 
tweets published on the same day that described 
this event.  Sentence 2 and 3 are two examples 
of the news excerpts and Sentence 1 is one ex-
ample of news tweets for this event.   
Chile Earthquake Shortened Earth Day    (S2) 
Chile Earthquake Shortened Day              (S3) 
Obviously Sentence 1, 2 and 3 all have predi-
FDWH ?shortened? with the same A0 and A1 ar-
guments. Our manually checking showed that in 
average each news tweet in those 993 samples 
had 2.4 news excerpts that had the same predi-
cate-argument structures.  
Our news tweet annotation approach consists 
of four steps. First, we submit hot queries to 
Twitter and for each query we obtain a list of 
tweets. Second, for each list of tweets, we single 
out news excerpts using heuristic rules and re-
move them from the list, conduct SRL on news 
excerpts using SRL-BS, and cluster them in 
terms of the similarity in content and predicate-
argument structures. Third, for each list of 
tweets, we try to merge every remaining tweet 
into one news excerpt cluster according to its 
content similarity to the cluster. Those that can 
be put into one news group are regarded as news 
tweet. Finally, semantic structures of news ex-
cerpts are passed to the news tweet in the same 
group through word alignment. 
Our domain specific SRL system is then 
trained on automatically constructed training 
data using the Conditional Random Field (CRF: 
Lafferty et al, 2001) learning framework. Our 
system is evaluated on a human labeled dataset, 
and achieves state-of-the-art performance, out-
performing the baseline SRL-BS.  
Our contributions can be summarized as fol-
lows: 
1) We propose to conduct SRL for news 
tweets for fine grained information ex-
traction and retrieval;  
2) We present a semi-supervised learning 
approach to train a domain specific SRL 
system for news tweets, which outper-
forms SRL-BS and achieves the state-of-
the-art performance on a human labeled 
dataset. 
The rest of this paper is organized as follows: 
In the next section, we review related work.  In 
Section 3 we detail key components of our ap-
proach. In Section 4, we setup experiments and 
evaluate the effectiveness of our method.  Final-
699
ly, Section 5 concludes and presents the future 
work. 
2 Related Work 
Our related work falls into two categories: SRL 
on news and domain adaption. 
As for SRL on news, most researchers used 
the pipelined approach, i.e., dividing the task 
into several phases such as argument identifica-
tion, argument classification, global inference, 
etc.,  and conquering them individually (Xue and 
Palmer, 2004; Koomen et al, 2005; Cohn and 
Blunsom, 2005; Punyakanok et al, 2008; 
Toutanova et al, 2005; Toutanova et al, 2008). 
Exceptions to the pipelined approach exist.  
M?rquez et al (2005) sequentially labeled the 
words according to their positions relative to an 
argument (i.e., inside, outside or at the beginning 
of it). Carreras et al (2004) and Surdeanu et al 
(2007) jointly labeled all the predicates. Vickrey 
and Koller(2008) simplified the input sentence 
by hand-written and machine learnt rules before 
conducting SRL. Some other approaches simul-
taneously resolved all the sub-tasks by integrat-
ing syntactic parsing and SRL into a single mod-
el (Musillo and Merlo, 2006; Merlo and Musillo, 
2008), or by using Markov Logic Networks 
(MLN, Richardson and Domingos, 2005) as the 
learning framework (Riedel and Meza-Ruiz, 
2008; Meza-Ruiz and Riedel, 2009). 
All the above approaches focus on sentences 
from news articles or other formal documents, 
and depend on human annotated corpus for 
training. To our knowledge, little study has been 
carried out on SRL for news tweets.  
As for domain adaption, some researchers re-
garded the out-of-GRPDLQ GDWD DV ?SULRU
NQRZOHGJH?DQGestimated the model parameters 
by maximizing the posterior under this prior dis-
tribution, and successfully applied their ap-
proach to language modeling (Bacchiani and 
Roark, 2003) and parsing (Roark and Bacchiani, 
2003). Daum? III and Marcu (2006) presented a 
QRYHO IUDPHZRUN E\ GHILQLQJ D ?JHQHUDO Go-
PDLQ?EHWZHHQWKH?WUXO\LQ-GRPDLQ?DQG?WUXO\
out-of-GRPDLQ?   
Unlike existing domain adaption approaches, 
our method is about adapting SRL system on 
news domain to the news tweets domain, two 
domains that differ in writing style but are linked 
through content similarity. 
3 Our Method 
Our method of SRL for news tweets is to train a 
domain specific SRL on automatically annotated 
training data as briefed in Section 1.  
In this section we present details of the five 
crucial components of our method, i.e., news 
excerpt identification, news excerpt clustering, 
news tweets identification, semantic structure 
mapping, and the domain specific SRL system 
constructing. 
3.1 News Excerpt Identification 
We use one heuristic rule to decide whether or 
not a tweet is news excerpt:  if a tweet has a link 
to a news article and its text content is included 
by the news article, it is news excerpt, otherwise 
not. 
Given a tweet, to apply this rule, we first ex-
tract the content link and expand it, if any, into 
the full link with the unshorten service3. This 
step is necessary because content link in tweet is 
usually shortened to reduce the total amount of 
characters. Next, we check if the full link points 
to any of the pre-defined news sites, which, in 
our experiments, are 57 English news websites. 
If yes, we download the web page and check if it 
exactly contains the text content of the input 
tweet. Figure 1 illustrates this process.  
Figure 1. An illustration of news excerpt identi-
fication. 
To test the precision of this approach, while 
preparing for the training data for the experi-
ments, we checked 100 tweets that were identi-
fied as news excerpt by this rule to find out they 
all are excerpted from news. 
                                                 
3 http://unshort.me 
700
3.2 News Excerpt Clustering 
Given as input a list of news excerpts concerning 
the same query and published in the same time 
scope, this component uses the hierarchical ag-
glomerative clustering algorithm (Manning et 
al., 2008) to divide news excerpts into groups in 
terms of the similarity in content and predicate-
argument structures.  
Before clustering, for every news excerpt, we 
remove the content link and other metadata such 
as author, retweet marks (starting with RT @), 
reply marks (starting with @ immediately after 
the author), hash tags (starting with #), etc., and 
keep only the text content; then it is further 
parsed into tokens, POS tags, chunks and syntac-
tic tree using the OpenNLP toolkit4.  After that,  
SRL is conducted with SRL-BS to get predicate-
argument structures. Finally, every news excerpt 
is represented as frequency a vector of terms, 
including tokens, POS tagger, chunks, predicate-
argument structures, etc. A news cluster is re-
garded as a ?macro? news excerpt and is also 
represented as a term frequency vector, i.e., the 
sum of all the term vectors in the cluster.  Noisy 
terms, such as numbers and predefined stop 
words are excluded from the frequency vector. 
To reduce data sparseness, words are stemmed 
by Porter stemmer (Martin F. Porter, 1980). 
The cosine similarity is used to measure the 
relevance between two clusters, as defined in 
Formula 1.  
   ,
'
'
'
CVCV
CVCV
CCCS
u
?               (1) 
Where C, &? denote two clusters, CV, CV? de-
note  the term frequency vectors of C and  &? 
respectively, and CS(C, &?) stands for the  co-
sine similarity between C and  &?. 
Initially, one news excerpt forms one cluster.  
Then the clustering process repeats merging the 
two most similar clusters into one till the simi-
larity between any pair of clusters is below a 
threshold, which is experimentally set to 0.7 in 
our experiments. 
During the training data preparation process, 
we randomly selected 100 clusters, each with 3.2 
pieces of news in average. For every pair of 
news excerpts in the same cluster, we checked if 
                                                 
4 http://opennlp.sourceforge.net/ 
they shared similar contents and semantic struc-
tures, and found out that 91.1% were the cases. 
3.3 News Tweets Identification 
After news excerpts are identified and removed 
from the list, every remaining tweet is checked if 
it is a news tweet. Here we group news excerpts 
and news tweets together in two steps because 1) 
news excerpts count for only a small proportion 
of all the tweets in the list, making our two-step 
clustering algorithm more efficient; and 2) one-
step clustering tends to output meaningless clus-
ters that include no news tweets. 
Intuitively, news tweet, more often than not, 
have news counterparts that report similar con-
tents. Thus we use the following rule to identify 
news tweets: if the content similarity between 
the tweet and any news excerpt cluster is greater 
than a threshold, which is experimentally set to 
0.7 in our experiments, the tweet is a news tweet, 
otherwise it is not. Furthermore, each news 
tweet is merged into the cluster with most simi-
lar content. Finally, we re-label any news tweet 
as news excerpt, which is then process by SRL-
BS, if its content similarity to the cluster exceeds 
a threshold, which is experimentally set to 0.9 in 
our experiments. 
Again, the cosine similarity is used to meas-
ure the content similarity between tweet and 
news excerpt cluster. Each tweet is repressed as 
a term frequency vector. Before extracting terms 
from tweet, tweet metadata is removed and a 
rule-based normalization process is conducted to 
restore abnormal strLQJVVD\?	DSRV?LQWRWKHLU
KXPDQ IULHQGO\ IRUP VD\ ?? ? 1H[W VWHPPLQJ
tools and OpenNLP are applied to get lemmas, 
POS tags, chunks, etc., and noisy terms are fil-
tered.  
We evaluated the performance of this ap-
proach when preparing for the training data. We 
randomly sampled 500 tweets that were identi-
fied as news tweets, to find that 93.8% were true 
news tweets. 
3.4 Semantic Structure Mapping 
Semantic structure mapping is formed as the 
task of word alignment from news excerpt to 
news tweet. A HMM alignment model is trained 
with GIZA++ (Franz and Hermann, 2000) on all 
(news excerpt, news tweet) pairs in the same 
cluster. After word alignment is done, semantic 
701
information attached to a word in a news excerpt 
is passed to the corresponding word in the news 
tweet as illustrated in Figure 2. 
 
Chile Earthquake Shortened Earth Day
A0 predicate A1
NASA and shorten the day by a wee second :-(
oh yea and Chile earthquake the earth off it's axis according to
 
Figure 2. An example of mapping semantic 
structures from news excerpts to news tweets. 
In Figure 2, shorten, earthquake and day in 
two sentences are aligned, respectively; and two 
predicate-argument structures in the first sen-
tence, i.e., (shortened, earthquake, A0), (short-
ened, day, A1), are passed to the second. 
News tweets may receive no semantic infor-
mation from related news excerpts after mapping, 
because of word alignment errors or no news 
excerpt in the cluster with similar semantic 
structures.  Such tweets are dropped. 
Mapping may also introduce cases that violate 
the following two structural constraints in SRL 
(Meza-Ruiz and Riedel, 2009): 1) one (predi-
cate, argument) pair has only one role label in 
one sentence; and 2) for each predicate, each of 
the proper arguments (A0~A5) can occur at most 
once. Those conflicts are largely owing to the 
noisy outputs of SRL trained on news and to the 
alignment errors. While preparing for the train-
ing data for our experiments, we found 38.9% of 
news tweets had such conflicts.  
A majority voting schema and the structural 
constrains are used to resolve the conflicts as 
described below.   
1) Step 1, for every cluster, each (predicate, 
argument, role) is weighted according to 
its frequency in the cluster; 
2) Step 2, for every cluster, detect conflicts 
using the structural constrains; if no con-
flicts exist, stop; otherwise go to Step 3;   
3) Step 3, for every cluster, keep the one 
with higher weight in each conflicting 
(predicate, argument, role) pair; if the 
weights are equal,  drop both; 
Here is an example to show the conflicting 
resolution process.  Consider the cluster includ-
ing Sentence 1, 2 and 3, where (shorten, earth-
quake, A0), (shorten, earthquake, A1), (shorten, 
axis, A0), and (shorten, day, A1) occur 6, 4, 1 
and 3 times, respectively.  This cluster includes 
three conflicting pairs:   
1) (shorten, earthquake, A0) vs. (shorten, 
earthquake, A1); 
2) (shorten, earthquake, A1) vs. (shorten, 
day, A1); 
3) (shorten, earthquake, A0) vs. (shorten, ax-
is, A0); 
The first pair is first resolved, causing (short-
en, earthquake, A0) to be kept and (shorten, 
earthquake, A1) removed, which leads to the 
second pair being resolved as well; then we pro-
cess the third pair resulting in (shorten, earth-
quake, A0) being kept and (shorten, axis, A0) 
dropped; finally (shorten, earthquake, A0) and 
(shorten, day, A1) stay in the cluster. 
The conflicting resolution algorithm is sensi-
tive to the order of conflict resolution in Step 3. 
Still consider the three conflicting pairs listed 
above. If the second pair is first processed, only 
(shorten, earthquake, A0) will be left. Our strat-
egy is to first handle the conflict resolving which 
leads to most conflicts resolved. 
We tested the performance of this semantic 
structure mapping strategy while preparing for 
the training data. We randomly selected 56 news 
tweets with conflicts and manually annotated 
them with SRL. After the conflict resolution 
method was done, we observed that 38 news 
tweets were resolved correctly, 9 resolved but 
incorrectly, and 9 remain unresolved, suggesting 
the high precision of this method, which fits our 
task.  We leave it to our future work to study 
more advanced approach for semantic structure 
mapping. 
3.5 SRL System for News Tweets 
Following M?rquez et al (2005), we regard SRL 
for tweets as a sequential labeling task, because 
of its joint inference ability and its openness to 
support other languages. 
We adopt conventional features for each token 
defined in M?rquez et al(2005),  such as the 
lemma/POS tag of the current/previous/next to-
ken, the lemma of predicate and its combination 
with the lemma/POS tag of the current token, the 
voice of the predicate (active/passive), the dis-
tance between the current token and the predi-
cate, the relative position of the current token to 
702
the predicate, and so on. We do not use features 
related to syntactic parsing trees, to allow our 
system not to rely on any syntactic parser, whose 
performance depends on style and language of 
text, which limits the generality of our system. 
Before extracting features, we perform a pre-
processing step to remove tweet metadata and 
normalize tweet text content, as described in 
Section 3.3. The OpenNLP toolkit is used for 
feature extraction, and the CRF++ toolkit 5  is 
used to train the model. 
4 Experiments 
In this section, we evaluate our SRL system on a 
gold-standard dataset consisting of 1,110 human 
annotated news tweets and show that our system 
achieves the state-of-the-art performance com-
pared with SRL-BS that is trained on news. Fur-
thermore, we study the contribution of automati-
cally generated training data. 
4.1 Evaluation Metric 
We adopt the widely used precision (Pre.), recall 
(Rec.) and F-score (F., the harmonic mean of 
precision and recall) as evaluation metrics.  
4.2 Baseline System 
We use SRL-BS as our baseline because of its 
state-of-art performance on news domain, and its 
readiness to use as well. 
4.3 Data Preparation 
We restrict to English news tweets to test our 
method. Our method can label news tweets of 
other languages, given that the related tools such 
as the SRL system on news domain, the word 
alignment tool, OpenNLP, etc., can support oth-
er languages.  
We build two corpora for our experiments: 
one is the training dataset of 10,000 news tweets 
with semantic roles automatically labeled; the 
other is the gold-standard dataset of 1,110 news 
tweets with semantic roles manually labeled. 
Training Dataset 
We randomly sample 80 queries from 300 
English queries extracted from the top stories of 
Bing news, Google news and Twitter trending 
topics from March 1, 2010 to March 4, 2010.  
                                                 
5 http://crfpp.sourceforge.net/ 
Submitting the 80 queries to Twitter search, 
we retrieve and download 512,000 tweets, from 
which we got 4,785 news excerpts and 11,427 
news tweets, which were automatically annotat-
ed using the method described in Section 3.   
Furthermore, 10,000 tweets are randomly se-
lected from the automatically annotated news 
tweets, forming the training dataset, while the 
other 1,427 news tweets are used to construct the 
gold-standard dataset. 
Gold-standard Dataset 
We ask two people to annotate the 1,427 news 
tweets, following the Annotation guidelines for 
PropBank6 with one exception: for phrasal ar-
guments, only the head word is labeled as the 
argument, because our system and SRL-BS con-
duct word level SRL. 
317 news tweets are dropped because of in-
consistent annotation, and the remaining 1,110 
news tweets form the gold-standard dataset.  
Quality of Training dataset 
Since the news tweets in the gold-standard da-
taset are randomly sampled from the automati-
cally labeled corpus and are labeled by both hu-
man and machine, we use them to estimate the 
quality of training data, i.e., to which degree the 
automatically generated results are similar to 
humans?.   
We find that our method achieves 75.6% F1 
score, much higher than the baseline, suggesting 
the relatively high quality of the training data. 
4.4 Result and Analysis 
Table 1 reports the experimental results of our 
system (SRL-TS) and the baseline on the gold-
standard dataset. 
 
 Precision Recall F-Score 
SRL-BS 36.0 % 54.5% 43.3% 
SRL-TS 78.0% 57.1% 66.0% 
Table 1. Performances of our system and the 
baseline on the gold-standard dataset. 
As shown in Table 1, our system performs 
much better than the baseline on the gold-
standard dataset in terms of all metrics. We ob-
serve two types of errors that are often made by 
                                                 
6 http://verbs.colorado.edu/~mpalmer/projects/ace/PB 
guidelines.pdf 
703
SRL-BS but not so often by our system, which 
largely explains the difference in performance.  
The first type of errors, which accounts for 
25.3% of the total errors made by SRL-BS, is 
caused by the informal written style, such as el-
lipsis, of news tweets. For instance, for the ex-
ample Sentence 1 listed in Section 1, the SRL-
BS incorrectly identify earth as the A0 argument 
of the predicate shorten. The other type of errors, 
which accounts for 10.2% of the total errors 
made by SRL-BS, is related to the discretionary 
combination of news snippets. For example, 
consider the following news tweet: 
The Chile earthquake shifted the earth's axis, 
"shortened the length of an Earth day by 1.26 
miliseconds".                                              (S4) 
We analyze the errors made by our system 
and find that 12.5% errors are attributed to the 
complex syntactic structures, suggesting that 
combining our system with systems on news 
domain is a promising direction. For example, 
our system cannot identify the A0 argument of 
the predicate shortened, because of its blindness 
of attributive clause; in contrast, SRL-BS works 
on this case.  
wow..the earthquake that caused the 2004 In-
dian Ocean tsunami shortened the day by al-
most 3 microseconds..what does that even 
mean?! HOW?                                           (S5) 
We also find that 32.3% of the errors made by 
our system are more or less related to the train-
ing data, which has noise and cannot fully repre-
sent the knowledge of SRL on news tweets. For 
instance, our system fails to label the following 
sentence, partially because the predicate strike 
does not occur in the training set. 
8.8-Magnitude-Earthquake-Strikes-Chile (S6) 
We further study how the size of automatical-
O\ODEHOHGWUDLQLQJGDWDDIIHFWVRXUV\VWHP?VSHr-
formance, as illustrated in Figure 3. We conduct 
two sets of experiments: in the first set, the train-
ing data is automatically labeled and the testing 
data is the gold-standard dataset; in the second 
set, half of the news tweets from the gold-
standard dataset are added to the training data, 
the remaining half forms the testing dataset. 
Curve 1 and 2 represent the experimental results 
of set 1 and 2, respectively. 
From Curve 1, we see that RXUV\VWHP?VSHr-
formance increases sharply when the training 
data size varies from 5,000 to 6,000; then in-
creases relatively slowly with more training data; 
and finally reaches the highest when all training 
data is used.  Curve 2 reveals a similar trend. 
 
 
Figure 3. Performance on training data of vary-
ing size. 
This phenomenon is largely due to the com-
peting between two forces: the noise in the train-
ing data, and the knowledge of SRL encoded in 
the training data.  
Interestingly, from Figure 3, we observe that 
the contribution of human labeled data is no 
longer significant after 6,000 automatically la-
beled training data is used, reaffirming the effec-
tiveness of the training data. 
5 Conclusions and Future Work 
We propose to conduct SRL on news tweets for 
fine grained information extraction and retrieval. 
We present a self-supervised learning approach 
to train a domain specific SRL system for news 
tweets. Leveraging the SRL system on news 
domain and content similarity between news and 
news tweets, our approach automatically labels a 
large volume of training data by mapping SRL-
BS generated results of news excerpts to news 
tweets. Experimental results show that our sys-
tem outperforms the baseline and achieves the 
state-of-the-art performance.  
In the future, we plan to enlarge training data 
size and test our system on a larger dataset; we 
also plan to further boost the performance of our 
system by incorporating tweets specific features 
such as hash tags, reply/re-tweet marks into our 
704
CRF model, and by combining our system with 
SRL systems trained on news.  
 
References 
Bacchiani, Michiel and Brian Roark. 2003. Unsuper-
vised language model adaptation. Proceedings of 
the 2003 International Conference on Acoustics, 
Speech and Signal Processing, volume 1, pages: 
224-227 
Carreras, Xavier, Llu?s M?rquez, and Grzegorz 
&KUXSD?D+LHUDUFKLFDOUHFRJQLWLRQRISURSo-
sitional arguments with Perceptrons. Proceedings 
of the Eighth Conference on Computational Natu-
ral Language Learning, pages: 106-109. 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labeling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Daum?, Hal III and Daniel Marcu. 2006. Domain 
adaptation for statistical classifiers. Journal of Ar-
tificial Intelligence Research, 26(1), 101-126. 
Fillmore, Charles J., Josef Ruppenhofer, Collin F. 
Baker. 2004. FrameNet and Representing the Link 
between Semantic and Syntactic Relations. Com-
putational Linguistics and Beyond, Institute of 
Linguistics, Academia Sinica. 
Kelly, Ryan, ed. 2009. Twitter Study Reveals Inter-
esting Results About Usage. San Antonio, Texas: 
Pear Analytics. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
Lafferty, John D., Andrew McCallum, Fernando C. 
N. Pereira. 2001. Conditional Random Fields: 
Probabilistic Models for Segmenting and Labeling 
Sequence Data. Proceedings of the Eighteenth In-
ternational Conference on Machine Learning, 
pages: 282-289. 
Manning, Christopher D., Prabhakar Raghavan and 
Hinrich Schtze. 2008. Introduction to Information 
Retrieval. Cambridge University Press, Cam-
bridge, UK. 
M?rquez, Llu?s, Jesus Gim?nez Pere Comas and 
Neus Catal?. 2005. Semantic Role Labeling as Se-
quential Tagging. Proceedings of the Ninth Con-
ference on Computational Natural Language 
Learning, pages: 193-196. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses us-
ing Markov Logic. Human Language Technolo-
gies: The 2009 Annual Conference of the North 
American Chapter of the ACL, pages: 155-163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Och, Franz Josef, Hermann Ney. Improved Statistical 
Alignment Models. Proceedings of the 38th Annu-
al Meeting of the Association for Computational 
Linguistics, pages: 440-447. 
Porter, Martin F. 1980. An algorithm for suffix strip-
ping. Program, 14(3), 130-137. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and in-
ference in semantic role labeling. Journal of Com-
putational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. Collec-
tive semantic role labelling with Markov Logic. 
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, pages: 193-
197. 
Roark, Brian and Michiel Bacchiani. 2003. Super-
vised and unsupervised PCFG adaptation to novel 
domains. Proceedings of the 2003 Conference of 
the North American Chapter of the Association for 
Computational Linguistics on Human Language 
Technology, volume 1, pages: 126-133. 
Surdeanu, Mihai, Sanda Harabagiu, JohnWilliams 
and Paul Aarseth. 2003. Using predicate-argument 
structures for information extraction. Proceedings 
of the 41st Annual Meeting on Association for 
Computational Linguistics, volume 1, pages: 8-15. 
Surdeanu, Mihai, Llu?s M?rquez, Xavier Carreras and 
Pere R. Comas. 2007. Combination strategies for 
semantic role labeling. Journal of Artificial Intelli-
gence Research, 29(1), 105-151. 
705
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Vickrey, David and Daphne Koller. 2008. Applying 
sentence simplification to the conll-2008 shared 
task. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, pag-
es: 268-272  
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
706
Coling 2010: Poster Volume, pages 725?729,
Beijing, August 2010
Collective Semantic Role Labeling on Open News Corpus  
by Leveraging Redundancy 
 
 
1,2Xiaohua Liu, 3Kuan Li*, 4Bo Han*, 2Ming Zhou,  
2Long Jiang, 5Daniel Tse* and 3Zhongyang Xiong 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3College of Computer Science 
Chongqing University 
4School of Software 
Dalian University of Technology 
5School of Information Technologies 
The University of Sydney 
{xiaoliu, v-kuli, v-bohan, mingzhou, longj} 
@microsoft.com 
dtse6695@it.usyd.edu.au 
zyxiong@cqu.edu.cn 
 
  
Abstract 
We propose a novel MLN-based method 
that collectively conducts SRL on 
groups of news sentences. Our method is 
built upon a baseline SRL, which uses 
no parsers and leverages redundancy. 
We evaluate our method on a manually 
labeled news corpus and demonstrate 
that news redundancy significantly im-
proves the performance of the baseline, 
e.g., it improves the F-score from 
64.13% to 67.66%.  * 
1 Introduction 
Semantic Role Labeling (SRL, M?rquez, 2009) 
is generally understood as the task of identifying 
the arguments of a given predicate and assigning 
them semantic labels describing the roles they 
play. For example, given a sentence The luxury 
auto maker sold 1,214 cars., the goal is to iden-
tify the arguments of sold and produce the fol-
lowing output: [A0 The luxury auto maker] [V 
sold] [A1 1,214 cars]. Here A0 represents the 
seller, and A1 represents the things sold (CoNLL 
2008 shared task, Surdeanu et al, 2008). 
                                                 
* This work has been done while the author was visiting 
Microsoft Research Asia. 
Gildea and Jurafsky (2002) first tackled SRL 
as an independent task, which is divided into 
several sub-tasks such as argument identifica-
tion, argument classification, global inference, 
etc. Some researchers (Xue and Palmer, 2004; 
Koomen et al, 2005; Cohn and Blunsom, 2005; 
Punyakanok et al, 2008; Toutanova et al, 2005; 
Toutanova et al, 2008) used a pipelined ap-
proach to attack the task. Some others resolved 
the sub-tasks simultaneously. For example, some 
work (Musillo and Merlo, 2006; Merlo and Mu-
sillo, 2008) integrated syntactic parsing and SRL 
into a single model, and another (Riedel and 
Meza-Ruiz, 2008; Meza-Ruiz and Riedel, 2009) 
jointly handled all sub-tasks using Markov Log-
ic Networks (MLN, Richardson and Domingos, 
2005). 
All the above methods conduct sentence level 
SRL, and rely on parsers. Parsers have showed 
great effects on SRL performance. For example, 
Xue and Palmer (2004) reported that SRL per-
formance dropped more than 10% when they 
used syntactic features from an automatic parser 
instead of the gold standard parsing trees. Even 
worse, parsers are not robust and cannot always 
analyze any input, due to the fact that some in-
puts are not in the language described by the 
parser?s formal grammar, or adequately repre-
sented within the parser?s training data. 
725
We propose a novel MLN-based method that 
collectively conducts SRL on groups of news 
sentences to leverage the content redundancy in 
news. To isolate the negative effect of noise 
from parsers and thus focus on the study of the 
contribution of redundancy to SRL, we use no 
parsers in our approach. We built a baseline SRL, 
which depends on no parsers, and use the MLN 
framework to exploit  redundancy. Our intuition 
is that SRL on one sentence can help that on 
other differently phrased sentences with similar 
meaning. For example, consider the following 
sentence from a news article: 
A suicide bomber blew himself up Sunday in 
market in Pakistan's northwest crowded with 
shoppers ahead of a Muslim holiday, killing 
12 people, including a mayor who once sup-
ported but had turned against the Taliban, of-
ficials said. 
The state-of-art MLN-based system (Meza-Ruiz 
and Riedel, 2009), hereafter referred to as 
MLNBS for brevity, incorrectly labels northwest 
instead of bomber as A0 of killing. Now consider 
another sentence from another news article: 
Police in northwestern Pakistan say that a su-
icide bomber has killed at least 13 people and 
wounded dozens of others. 
Here MLNBS correctly identify bomber as A0 
of killing. When more sentences are observed 
where bomber as A0 of killing is correctly identi-
fied, we will be more confident that bomber 
should be labeled as A0 of killing, and that 
northwest should not be the A0 of killing accord-
ing to the constraint that one predicate has at 
most one A0. 
We manually construct a news corpus to 
evaluate our method. In the corpus, semantic 
role information is annotated and sentences with 
similar meanings are grouped together. Experi-
mental results show that news redundancy can 
significantly improve the performance of the 
baseline system. 
Our contributions can be summarized as fol-
lows: 
1. We present a novel method that conducts 
SRL on a set of sentences collectively, in-
stead of on a single sentence, by extend-
ing MLNBS to leverage redundancy. 
2. We show redundancy can significantly 
improve the performance of the baseline 
system, indicating a promising research 
direction towards open SRL. 
In the next section, we introduce news sen-
tence extraction and clustering. In Section 3, we 
describe our collective inference method. In Sec-
tion 4, we show our experimental results. Finally, 
in Section 5 we conclude our paper with a dis-
cussion of future work. 
2 Extraction and Clustering of News 
Sentences 
To construct a corpus to evaluate our method, 
we extract sentences from clustered news arti-
cles returned by news search engines such as 
Bing and Google, and divide them into groups 
so that sentences in a group have similar mean-
ing. 
News articles in the same cluster are supposed 
to report the same event. Thus we first group 
sentences according to the news cluster they 
come from. Then we split sentences in the same 
cluster into several groups according to the simi-
larity of meaning. We assume that two sentences 
are more similar in meaning if they share more 
synonymous proper nouns and verbs. The syno-
nyms of verbs, like plod and trudge, are mainly 
extracted from the Microsoft Encarta Diction-
ary1, and the proper nouns thesaurus, containing 
synonyms such as U.S. and the United States, is 
manually compiled. 
As examples, below are two sentence groups 
which are extracted from a news cluster describ-
ing Hurricane Ida. 
Group 1: 
? Hurricane Ida, the first Atlantic hurri-
cane to target the U.S. this year, plod-
ded yesterday toward the Gulf Coast? 
? Hurricane Ida trudged toward the Gulf 
Coast? 
? ? 
Group 2: 
? It could make landfall as early as Tues-
day morning, although it was forecast to 
weaken further. 
                                                 
1
http://uk.encarta.msn.com/encnet/features/dictionary/dictio
naryhome.aspx 
726
? Authorities said Ida could make landfall 
as early as Tuesday morning, although 
it was forecast to weaken by then. 
? ? 
3 Collective Inference Based on MLN 
Our method includes two core components: a 
baseline system that conducts SRL on every sen-
tence; and a collective inference system that ac-
cepts as input a group of sentences with prelimi-
nary SRL information provided by the baseline. 
We build the baseline by removing formulas 
involving syntactic parsing information from 
MLNBS (while keeping other rules) and retrain-
ing the system using the tool and scripts provid-
ed by Riedel and Meza-Ruiz (2008) on the man-
ually annotated news corpus described in Sec-
tion 4. 
A collective inference system is constructed 
to leverage redundancy in the SRL information 
from the baseline.  
We first redefine the predicate role and treat it 
as observed: 
predicate role: Int x Int x Int x Role; 
role has four parameters: the first one stands for 
the number of sentence in the input, which is 
necessary to distinguish the sentences in a group; 
the other three are taken from the arguments of 
the role predicate defined by Riedel and Meza-
Ruiz (2008), which denote the positions of the 
predicate and the argument in the sentence and 
the role of the argument, respectively. If the 
predication holds, it returns 1, otherwise 0.  
A hidden predicate final_role is defined to 
present the final output, which has the same pa-
rameters as the predicate role: 
predicate final_role: Int x Int x Int x Role; 
We introduce the following formula, which 
directly passes the semantic role from the base-
line to the final output: 
role(s, p, a, +r)=> final_role (s, p, a, +r)    (1) 
Here s is the sentence number in a group; p and 
a denote the positions of the predicate and ar-
gument in s, respectively; r stands for the role of 
the argument; the ?+? before the variable r indi-
cates that different r has different weight. 
Then we define another formula for collective 
inference: 
s1?s2^lemma(s1,p1,p_lemma)^lemma(s2,p2, 
p_lemma)^lemma(s1,a1,a_lemma)^lemma(s2,
a2,a_lemma)^role(s2,p2,a2,+r)=>final_role 
(s1,p1,a1,+r)                                                 (2) 
Here p_lemma(a_lemma) stands for the lemma 
of the predicate(argument), which is obtained 
from the lemma dictionary. This dictionary is 
extracted from the dataset of CoNLL 2008 
shared task and is normalized using synonym 
dictionary described in Section 2; lemma is an 
observed predicate that states whether or not the 
word has the lemma. 
Formula 2 encodes our basic ideas about col-
lective SRL: given several sentences expressing 
similar meaning, if one sentence has a predicate 
p with an argument a of role r, the other sen-
tences would be likely to have a predicate p? 
with an argument a? of role r, where p? and a? 
are the same or synonymous with p and a, re-
spectively, as illustrated by the example in Sec-
tion 1. 
Besides, we also apply structural constraints 
(Riedel and Meza-Ruiz, 2008) to final_role. 
To learn parameters of the collective infer-
ence system, we use  thebeast (Riedel and Meza-
Ruiz, 2008),  which is an open Markov Logic 
Engine, and train it on manually annotated news 
corpus described in Section 4. 
4 Experiments 
To train and test the collective inference system, 
we extract 1000 sentences from news clusters, 
and group them into 200 clusters using the 
method described in Section 2. For every sen-
tence, POS tagging is conducted with the 
OpenNLP toolkit (Jason Baldridge et al, 2009), 
lemma of each word is obtained through the 
normalized lemma dictionary described in Sec-
tion 3, and SRL is manually labeled. To reduce 
human labeling efforts, we retrain our baseline 
on the WSJ corpus of CoNLL 2008 shared task 
and run it on our news corpus, and then edit the 
SRL outputs by hand. 
We implement the collective inference system 
with the thebeast toolkit. Precision, recall, and 
F-score are used as evaluation metrics.  In both 
training and evaluation, we follow the CoNLL 
2008 shared task and regard only heads of 
phrases as arguments. 
727
Table 1 shows the averaged 10-fold cross val-
idation results of our systems and the baseline, 
where the third and second line report the results 
of using and not using Formula 1 in our collec-
tive inference system, respectively. 
 
Systems Pre. (%) Rec. (%) F-score (%) 
Baseline 69.87 59.26 64.13 
CI-1 62.99 72.96 67.61 
CI 67.01 68.33 67.66 
Table 1. Averaged 10-fold cross validation re-
sults (Pre.: precision; Rec.: recall). 
Experimental results show that the two collec-
tive inference engines (CI-1 and CI) perform 
significantly better than the baseline in terms of 
the recall and F-score, though a little worse in 
the precision. We observe that predicate-
argument relationships in sentences with com-
plex syntax are usually not recognized by the 
baseline, but some of them are correctly identi-
fied by the collective inference systems. This, 
we guess, explains in large part the difference in 
performance. For instance, consider the follow-
ing sentences in a group, where order and tell 
are synonyms: 
? Colombia said on Sunday it will appeal 
to the U.N. Security Council and the 
OAS after Hugo Chavez, the fiery leftist 
president of neighboring Venezuela, or-
dered his army to prepare for war in or-
der to assure peace. 
? President Hugo Chavez ordered Vene-
zuela's military to prepare for a possible 
armed conflict with Colombia, saying 
yesterday that his country's soldiers 
should be ready if the U.S. tries to pro-
voke a war between the South American 
neighbors. 
? Venezuelan President Hugo Chavez told 
his military and civil militias to prepare 
for a possible war with Colombia as ten-
sions mount over an agreement giving 
U.S. troops access to Colombian mili-
tary bases. 
The baseline cannot label (ordered, Chavez, A0) 
for the first sentence, partially owing to the syn-
tactic complexity of the sentence, but can identi-
fy the relationship for the second and third sen-
tence. In contrast, the collective inference sys-
tems can identify Chavez in the first sentence as 
A0 of order because of its occurrence in the oth-
er sentences of the same group. 
As Table 1 shows, the CI system achieves the 
highest F-score (67.66%), and a higher precision 
than the CI-1 system, indicating the effective-
ness of Formula 1. Consider the above three sen-
tences. CI-1 mislabels (ordered, Venezuela, A1) 
for the first sentence because the baseline labels 
it for the second sentence. In contrast, CI does 
not label it for the first sentence because the 
baseline does not and (ordered, Venezuela, A1) 
rarely occurs in the outputs of the baseline for 
this sentence group. 
We also find cases where the collective infer-
ence systems do not but should help. For exam-
ple, consider the following group of sentences: 
? A Brazilian university expelled a woman 
who was heckled by hundreds of fellow 
students when she wore a short, pink 
dress to class, taking out newspaper ads 
Sunday to publicly accuse her of immo-
rality.  
? The university also published newspaper 
ads accusing the student, Geisy Arruda, 
of immorality. 
The baseline has identified (published, univer-
sity, A0) for the second sentence. But neither 
the baseline nor our method labels (taking, uni-
versity, A0) for the first one.  This happens be-
cause publish is not considered as a synonym 
of take, and thus (published, university, A0) in 
the second provides no evidence for (taking, 
university, A0) in the first. We plan to develop 
a context based synonym detection component 
to address this issue in the future. 
5 Conclusions and Future Work 
We present a novel MLN-based method that col-
lectively conducts SRL on groups of sentences. 
To help build training and test corpora, we de-
sign a method to collect news sentences and to 
divide them into groups so that sentences of sim-
ilar meaning fall into the same cluster. Experi-
mental results on a manually labeled news cor-
pus show that collective inference, which lever-
ages redundancy, can effectively improve the 
performance of the baseline. 
728
In the future, we plan to evaluate our method 
on larger news corpora, and to extend our meth-
od to other genres of corpora, such as tweets. 
 
References  
Baldridge, Jason, Tom Morton, and Gann. 2009. 
OpenNLP, http://opennlp.sourceforge.net/ 
Cohn, Trevor and Philip Blunsom. 2005. Semantic 
role labelling with tree conditional random fields. 
Proceedings of the Ninth Conference on Computa-
tional Natural Language Learning, pages: 169-
172. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Journal of Computa-
tional Linguistics, 28(3):245?288. 
Koomen, Peter, Vasin Punyakanok, Dan Roth, and 
Wen-tau Yih. 2005. Generalized inference with 
multiple semantic role labeling systems. Proceed-
ings of the Ninth Conference on Computational 
Natural Language Learning, pages: 181-184. 
M?rquez, Llu?s. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 
2009.   
Merlo, Paola and Gabriele Musillo. 2008. Semantic 
parsing for high-precision semantic role labelling. 
Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 1-8. 
Meza-Ruiz, Ivan and Sebastian Riedel. 2009. Jointly 
Identifying Predicates, Arguments and Senses 
using Markov Logic. Human Language 
Technologies: The 2009 Annual Conference of the 
North American Chapter of the ACL, pages: 155-
163.  
Musillo, Gabriele and Paola Merlo. 2006. Accurate 
Parsing of the proposition bank. Proceedings of 
the Human Language Technology Conference of 
the NAACL, pages: 101-104. 
Punyakanok, Vasin, Dan Roth and Wen-tau Yih. 
2008. The importance of syntactic parsing and 
inference in semantic role labeling. Journal of 
Computational Linguistics, 34(2), 257-287. 
Richardson, Matthew and Pedro Domingos. 2005. 
Markov logic networks. Technical Report, Univer-
sity of Washington, 2005. 
Riedel, Sebastian and Ivan Meza-Ruiz. 2008. 
Collective semantic role labelling with Markov 
Logic. Proceedings of the Twelfth Conference on 
Computational Natural Language Learning, 
pages: 193-197. 
Surdeanu, Mihai, Richard Johansson, Adam Meyers, 
Llu?s M?rquez, and Joakim Nivre. 2008. The conll 
2008 shared task on joint parsing of syntactic and 
semantic dependencies. Proceedings of the Twelfth 
Conference on Computational Natural Language 
Learning, pages: 159-177. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2005. Joint learning improves seman-
tic role labeling. Proceedings of the 43rd Annual 
Meeting of the Association for Computational Lin-
guistics, pages: 589-596. 
Toutanova, Kristina, Aria Haghighi and Christopher 
D. Manning. 2008. A global joint model for se-
mantic role labeling. Journal of Computational 
Linguistics, 34(2), 161-191. 
Xue, Nianwen and Martha Palmer. 2004. Calibrating 
features for semantic role labeling. Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing, pages: 88-94. 
 
729
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068?1076,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
 
 
Abstract 
In this paper we develop an approach to tackle 
the problem of verb selection for learners of 
English as a second language (ESL) by using 
features from the output of Semantic Role La-
beling (SRL). Unlike existing approaches to 
verb selection that use local features such as 
n-grams, our approach exploits semantic fea-
tures which explicitly model the usage context 
of the verb. The verb choice highly depends 
on its usage context which is not consistently 
captured by local features. We then combine 
these semantic features with other local fea-
tures under the generalized perceptron learn-
ing framework. Experiments on both in-
domain and out-of-domain corpora show that 
our approach outperforms the baseline and 
achieves state-of-the-art performance.1 
1 Introduction 
Verbs in English convey actions or states of being. 
In addition, they also communicate sentiments and 
imply circumstances, e.g., in ?He got [gained] the 
scholarship after three interviews.?, the verb 
?gained? may indicate that the ?scholarship? was 
competitive and required the agent?s efforts; in 
contrast, ?got? sounds neutral and less descriptive. 
                                                          
* This work has been done while the author was visiting Mi-
crosoft Research Asia. 
Since verbs carry multiple important functions, 
misusing them can be misleading, e.g., the native 
speaker could be confused when reading ?I like 
looking [reading] books?. Unfortunately, accord-
ing to (Gui and Yang, 2002; Yi et al, 2008), more 
than 30% of the errors in the Chinese Learner Eng-
lish Corpus (CLEC) are verb choice errors. Hence, 
it is useful to develop an approach to automatically 
detect and correct verb selection errors made by 
ESL learners. 
However, verb selection is a challenging task 
because verbs often exhibit a variety of usages and 
each usage depends on a particular context, which 
can hardly be adequately described by convention-
al n-gram features. For instance, both ?made? and 
?received? can complete ?I have __ a telephone 
call.?, where the usage context can be represented 
as ?made/received a telephone call?; however, in 
?I have __ a telephone call from my boss?, the 
prepositional phrase ?from my boss? becomes a 
critical part of the context, which now cannot be 
described by n-gram features, resulting in only 
?received? being suitable. 
Some researchers (Tetreault and Chodorow, 
2008) exploited syntactic information and n-gram 
features to represent verb usage context. Yi et al 
(2008) introduced an unsupervised web-based 
proofing method for correcting verb-noun colloca-
tion errors. Brockett et al (2006) employed phrasal 
Statistical Machine Translation (SMT) techniques 
to correct countability errors. None of their meth-
ods incorporated semantic information. 
SRL-based Verb Selection for ESL 
1,2Xiaohua Liu, 3Bo Han*, 4Kuan Li*, 5Stephan Hyeonjun Stiller and 2Ming Zhou 
1School of Computer Science and Technology 
Harbin Institute of Technology 
2Microsoft Research Asia 
3Department of Computer Science and Software Engineering 
The University of Melbourne 
4College of Computer Science 
Chongqing University 
5Computer Science Department 
Stanford University 
{xiaoliu,  mingzhou, v-kuli}@microsoft.com 
 b.han@pgrad.unimelb.edu.au 
sstiller@stanford.edu 
1068
Unlike the other papers, we derive features from 
the output of an SRL (M?rquez, 2009) system to 
explicitly model verb usage context. SRL is gener-
ally understood as the task of identifying the argu-
ments of a given verb and assigning them semantic 
labels describing the roles they play. For example, 
given a sentence ?I want to watch TV tonight? and 
the target predicate ?watch?, the output of SRL 
will be something like ?I [A0] want to watch [tar-
get predicate] TV [A1] tonight [AM-TMP].?, 
meaning that the action ?watch? is conducted by 
the agent ?I?, on the patient ?TV?, and the action 
happens ?tonight?. 
We believe that SRL results are excellent fea-
tures for characterizing verb usage context for 
three reasons: (i) Intuitively, the predicate-
argument structures generated by SRL systems 
capture major relationships between a verb and its 
contextual participants and consequently largely 
determine whether or not the verb usage is proper. 
For example, in ?I want to watch a match tonight.?, 
?match? is the patient of ?watch?, and ?watch ? 
match? forms a collocation, suggesting ?watch? is 
appropriately used. (ii) Predicate-argument struc-
tures abstract away syntactic differences in sen-
tences with similar meanings, and therefore can 
potentially filter out lots of noise from the usage 
context. For example, consider ?I want to watch a 
football match on TV tonight?: if ?match? is suc-
cessfully identified as the agent of ?watch?, 
?watch ? football?, which is unrelated to the us-
age of ?watch? in this case, can be easily excluded 
from the usage context. (iii) Research on SRL has 
made great achievements, including human-
annotated training corpora and state-of-the-art sys-
tems, which can be directly leveraged. 
Taking an English sentence as input, our method 
first generates correction candidates by replacing 
each verb with verbs in its pre-defined confusion 
set; then for every candidate, it extracts SRL-
derived features; finally our method scores every 
candidate using a linear function trained by the 
generalized perceptron learning algorithm (Collins, 
2002) and selects the best candidate as output. 
Experimental results show that SRL-derived fea-
tures are effective in verb selection, but we also 
observe that noise in SRL output adversely in-
creases feature space dimensions and the number 
of false suggestions. To alleviate this issue, we use 
local features, e.g., n-gram-related features, and 
achieve state-of-the-art performance when all fea-
tures are integrated. 
Our contributions can be summarized as follows: 
1. We propose to exploit SRL-derived fea-
tures to explicitly model verb usage con-
text. 
2. We propose to use the generalized percep-
tron framework to integrate SRL-derived 
(and other) features  and achieve state-of-
the-art performance on both in-domain and 
out-of-domain test sets. 
Our paper is organized as follows: In the next 
section, we introduce related work. In Section 3, 
we describe our method. Experimental results and 
analysis on both in-domain and out-of-domain cor-
pora are presented in Section 4. Finally, we con-
clude our paper with a discussion of future work in 
Section 5. 
2 Related Work 
SRL results are used in various tasks. Moldovan et 
al. (2004) classify the semantic relations of noun 
phrases based on SRL. Ye and Baldwin (2006) 
apply semantic role?related information to verb 
sense disambiguation. Narayanan and Harabagiu 
(2004) use semantic role structures for question 
answering. Surdeanu et al (2003) employ predi-
cate-argument structures for information extrac-
tion. 
However, in the context of ESL error detection 
and correction, little study has been carried out on 
clearly exploiting semantic information. Brockett 
et al (2006) propose the use of the phrasal statisti-
cal machine translation (SMT) technique to identi-
fy and correct ESL errors. They devise several 
heuristic rules to generate synthetic data from a 
high-quality newswire corpus and then use the syn-
thetic data together with their original counterparts 
for SMT training. The SMT approach on the artifi-
cial data set achieves encouraging results for cor-
recting countability errors. Yi et al (2008) use web 
frequency counts to identify and correct determiner 
and verb-noun collocation errors. Compared with 
these methods, our approach explicitly models 
verb usage context by leveraging the SRL output. 
The SRL-based semantic features are integrated, 
along with the local features, into the generalized 
perceptron model. 
 
1069
3 Our Approach 
Our method can be regarded as a pipeline consist-
ing of three steps. Given as input an English sen-
tence written by ESL learners, the system first 
checks every verb and generates correction candi-
dates by replacing each verb with its confusion set. 
Then a feature vector that represents verb usage 
context is derived from the outputs of an SRL sys-
tem and then multiplied with the feature weight 
vector trained by the generalized perceptron. Final-
ly, the candidate with the highest score is selected 
as the output. 
3.1 Formulation 
We formulate the task as a process of generating 
and then selecting correction candidates: 
           
? ? ? ?sScores sGENs 'maxarg* ??
                     (1) 
Here 's  denotes the input sentence for proofing, 
? ?'sGEN  is the set of correction candidates, and 
? ?sScore  is the linear model trained by the percep-
tron learning algorithm, which will be discussed in 
section 3.4. 
We call every target verb in 's  a checkpoint. 
For example, ?sees? is a checkpoint in ?Jane sees 
TV every day.?. Correction candidates are generat-
ed by replacing each checkpoint with its confu-
sions. Table 1 shows a sentence with one 
checkpoint and the corresponding correction can-
didates. 
 
Input Jane sees TV every day. 
Candidates Jane watches TV every day. 
Jane looks TV every day. 
? 
Table 1. Correction candidate list. 
One state-of-the-art SRL system (Riedel and 
Meza-Ruiz, 2008) is then utilized to extract predi-
cate-argument structures for each verb in the input, 
as illustrated in Table 2. 
Semantic features are generated by combining 
the predicate with each of its arguments; e.g., 
?watches_A0_Jane?, ?sees_A0_Jane?, ?watch-
es_A1_TV? and ?sees_A1_TV? are semantic fea-
tures derived from the semantic roles listed in Ta-
ble 2. 
 
Sentence Semantic roles 
Jane sees TV every day Predicate: sees; 
A0: Jane; 
A1: TV; 
Jane watches TV every 
day 
Predicate: watches; 
A0: Jane; 
A1: TV; 
Table 2. Examples of SRL outputs. 
At the training stage, each sentence is labeled by 
the SRL system. Each correction candidate s  is 
represented as a feature vector dRs ?? )( , where 
d  is the total number of features. The feature 
weight vector is denoted as dRw?? , and ? ?sScore  
is computed as follows: 
             ? ? wssScore ???? )(                        (2) 
Finally, ? ?sScore  is applied to each candidate, 
and *s , the one with the highest score, is selected 
as the output, as shown in Table 3. 
 
 Correction candidate Score 
*s  Jane watches TV every day. 10.8 
 Jane looks TV every day. 0.8 
 Jane reads TV every day. 0.2 
 ? ? 
Table 3.  Correction candidate scoring. 
In the above framework, the basic idea is to 
generate correction candidates with the help of pre-
defined confusion sets and apply the global linear 
model to each candidate to compute the degree of 
its fitness to the usage context that is represented 
as features derived from SRL results. 
To make our idea practical, we need to solve the 
following three subtasks: (i) generating the confu-
sion set that includes possible replacements for a 
given verb; (ii) representing the context with se-
mantic features and other complementary features; 
and (iii) training the feature weight. We will de-
scribe our solutions to those subtasks in the rest of 
this section. 
1070
3.2 Generation of Verb Confusion Sets 
Verb confusion sets are used to generate correction 
candidates. Due to the great number of verbs and 
their diversified usages, manually collecting all 
verb confusions in all scenarios is prohibitively 
time-consuming. To focus on the study of the ef-
fectiveness of semantic role features, we restrict 
our research scope to correcting verb selection er-
rors made by Chinese ESL learners and select fifty 
representative verbs which are among the most 
frequent ones and account for more than 50% of 
ESL verb errors in the CLEC data set. For every 
selected verb we manually compile a confusion set 
using the following data sources: 
1. Encarta treasures. We extract all the syno-
nyms of verbs from the Microsoft Encarta Diction-
ary, and this forms the major source for our 
confusion sets. 
2. English-Chinese Dictionaries. ESL learners 
may get interference from their mother tongue (Liu 
et al, 2000). For example, some Chinese people 
mistakenly say ?see newspaper?, partially because 
the translation of ?see? co-occurs with ?newspa-
per? in Chinese. Therefore English verbs in the 
dictionary sharing more than two Chinese mean-
ings are collected. For example, ?see? and ?read? 
are in a confusion set because they share the mean-
ings of both ??? (?to see?, ?to read?) and ???? 
(?to grasp?) in Chinese. 
3. An SMT translation table. We extract para-
phrasing verb expressions from a phrasal SMT 
translation table learnt from parallel corpora (Och 
and Ney, 2004). This may help us use the implicit 
semantics of verbs that SMT can capture but a dic-
tionary cannot, such as the fact that the verb  
Note that verbs in any confusion set that we are 
not interested in are dropped, and that the verb it-
self is included in its own confusion set. We leave 
it to our future work to automatically construct 
verb confusions. 
3.3 Verb Usage Context Features 
The verb usage context1 refers to its surrounding 
text, which influences the way one understands the 
expression. Intuitively, verb usage context can take 
the form of a collocation, e.g., ?watch ? TV? in ?I 
saw [watched] TV yesterday.? ; it can also simply 
be idioms, e.g., we say ?kick one?s habit? instead 
of ?remove one?s habit?.  
We use features derived from the SRL output to 
represent verb usage context. The SRL system ac-
cepts a sentence as input and outputs all arguments 
and the semantic roles they play for every verb in 
the sentence. For instance, given the sentence ?I 
have opened an American bank account in Bos-
ton.? and the predicate ?opened?, the output of 
SRL is listed in Table 4, where A0 and A1 are two 
core roles, representing the agent and patient of an 
action, respectively, and other roles starting with 
?AM-?are adjunct roles, e.g., AM-LOC indicates 
the location of an action. Predicate-argument struc-
tures keep the key participants of a given verb 
while dropping other unrelated words from its us-
age context. For instance, in ?My teacher said Chi-
nese is not easy to learn.?, the SRL system 
recognizes that ?Chinese? is not the A1-argument 
of ?said?. So ?say _ Chinese?, which is irrelevant 
to the usage of said, is not extracted as a feature. 
The SRL system, however, may output 
erroneous predicate-argument structures, which 
negatively affect the performance of verb 
selection.  For instance,  for the sentence ?He 
hasn?t done anything but take [make] a lot of 
money?, ?lot? is incorrectly identified as the patient 
of ?take?, making it hard to select ?make? as the 
proper verb even though ?make money? forms a 
sound collocation. To tackle this issue, we use 
local textual features, namely features related to n-
gram, chunk and chunk headword, as shown in 
Table 5.  Back-off features are generated by 
replacing the word with its POS tag to alleviate 
data sparseness. 
 
                                                          
1 http://en.wikipedia.org/wiki/Context_(language_use) 
I have made[opened] an American bank account in Boston . 
[A0] 
 
[Predicate] 
 
 
 
[A1] [AM-LOC] 
 
 
Table 4. An example of SRL output. 
1071
Table 5. An example of feature set. 
3.4 Perceptron Learning 
We choose the generalized perceptron algorithm as 
our training method because of its easy implemen-
tation and its capability of incorporating various 
features. However, there are still two concerns 
about this perceptron learning approach: its inef-
fectiveness in dealing with inseparable samples 
and its ignorance of weight normalization that po-
tentially limits its ability to generalize. In section 
4.4 we show that the training error rate drops sig-
nificantly to a very low level after several rounds 
of training, suggesting that the correct candidates 
can almost be separated from others. We also ob-
serve that our method performs well on an out-of-
domain test corpus, indicating the good generaliza-
tion ability of this method. We leave it to our fu-
ture work to replace perceptron learning with other 
models like Support Vector Machines (Vapnik, 
1995). 
In Figure 1, 
is  is the ith correct sentence within 
the training data. T and N represent the number of 
training iterations and training examples, respec-
tively. )( isGEN  is the function that outputs all the 
possible corrections for the input sentence is  with 
each checkpoint substituted by one of its confu-
sions, as described in Section 3.1. We observe that 
the generated candidates sometimes contain rea-
sonable outputs for the verb selection task, which 
should be removed. For instance, in ?? reporters 
could not take [make] notes or tape the conversa-
tion?, both ?take? and ?make? are suitable verbs in 
this context. To fix this issue, we trained a trigram 
language model using SRILM (Stolcke, 2002) on 
LDC data21, and calculated the logarithms of the 
language model score for the original sentence and 
its artificial manipulations. We only kept manipu-
lations with a language model score that is t lower 
than that of the original sentence. We experimen-
tally set t = 5. 
 
Inputs: training examples is , i=1?N 
Initialization: 0?w?  
Algorithm: 
   For r= 1.. T, i= 1..N    
   Calculate 
wso isGens ???? ? )(maxarg )(
 
   If os i ?  
         )()( osww o ????? ??  
Outputs: w?  
Figure 1. The perceptron algorithm, adapted from Co-
lins (2002). 
?  in Figure 1 is the feature extraction function. 
)(o? and )( is? are vectors extracted from the out-
put and oracle, respectively. A vector field is filled 
with 1 if the corresponding feature exists, or 0 oth-
erwise; w?  is the feature weight vector, where posi-
tive elements suggest that the corresponding 
features support the hypothesis that the candidate 
is correct. 
The training process is to update w? , when the 
output differs from the oracle. For example, when 
o is ?I want to look TV? and is  is ?I want to watch 
TV?, w?  will be updated. 
We use the averaged Perceptron algorithm (Col-
lins, 2002) to alleviate overfitting on the training 
data. The averaged perceptron weight vector is 
defined as 
                 
?
??
?
TrN
riwTN ..1,..1i
,1 ???
                    (3) 
where riw ,? is the weight vector immediately af-
ter the ith sentence in the  rth iteration. 
 
                                                          
2 http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp ?cata-
logId=LDC2005T12 
Local: trigrams 
   have_opened 
   have_opened_a 
   opened_an_American 
   PRP_VBP_opened 
   VBP_opened_DT 
   opened_DT_JJ 
Local: chunk 
   have_opened 
   opened_an_American_investment_bank 
_account 
   PRP_opened 
   opened_NN 
Semantic: SRL derived features 
   A0_I_opened 
   opened_A1_account 
   opened_AM-LOC_in 
   ... 
1072
4 Experiments 
In this section, we compare our approach with the 
SMT-based approach. Furthermore, we study the 
contribution of predicate-argument-related 
features, and the performances on verbs with 
varying distance to their arguments. 
4.1 Experiment Preparation 
The training corpus for perceptron learning was 
taken from LDC2005T12. We randomly selected 
newswires containing target verbs from the New 
York Times as the training data. We then used the 
OpenNLP package31to extract sentences from the 
newswire text and to parse them into the corre-
sponding tokens, POS tags, and chunks. The SRL 
system is built according to Riedel and Meza-Ruiz 
(2008), using the CoNLL-2008 shared task data for 
training. We assume that the newswire data is of 
high quality and free of linguistic errors, and final-
ly we gathered 20000 sentences that contain any of 
the target verbs we were focusing on.  We experi-
mentally set the number of training rounds to T = 
50. 
We constructed two sets of testing data for in-
domain and out-of-domain test purposes, respec-
tively. To construct the in-domain test data, we 
first collected all the sentences that contain any of 
the verbs we were interested in from the previous 
unused LDC dataset; then we replaced any target 
verb in our list with a verb in its confusion set; 
next, we used the language-model-based pruning 
strategy described in 3.4 to drop possibly correct 
manipulations from the test data; and finally we 
randomly sampled 5000 sentences for testing. 
To build the out-of-domain test dataset, we 
gathered 186 samples that contained errors related 
to the verbs we were interested in from English 
blogs written by Chinese and from the CLEC cor-
pus, which were then corrected by an English na-
tive speaker. Furthermore, for every error 
involving the verbs in our target list, both the verb 
and the word that determines the error are marked 
by the English native speaker. 
4.2 Baseline 
We built up a phrasal SMT system with the word 
re-ordering feature disabled, since our task only 
concerns the substitution of the target verb. To 
                                                          
3 http://opennlp.sourceforge.net/ 
construct the training corpus, we followed the idea 
in Brockett et al (2006), and applied a similar 
strategy described in section 3.4 to the SRL sys-
tem?s training data to generate aligned pairs. 
4.3 Evaluation Metric 
We employed the following metrics adapted from 
(Yi et al, 2008): revised precision (RP), recall of 
the correction (RC) and false alarm (FA). 
         
 sCheckpoint All of #
Proofings Correct of #RP ?
                      (4)      
RP reflects how many outputs are correct usag-
es. The output is regarded as a correct suggestion if 
and only if it is exactly the same as the answer. 
Paraphrasing scenarios, for example, the case that 
the output is ?take notes? and the answer is ?make 
notes?, are counted as errors. 
Errors Total of# 
Proofings Modified Correct of# RC ?
                (5) 
RC indicates how many erroneous sentences are 
corrected among all the errors. It measures the sys-
tem?s coverage of verb selection errors. 
     
sCheckpoint All of# 
sCheckpoint Modified Incorrect of# FA ?
        (6) 
FA is related to the cases where a correct verb is 
mistakenly replaced by an inappropriate one. The-
se false suggestions are likely to disturb or even 
annoy users, and thus should be avoided as much 
as possible. 
4.4 Results and Analysis 
The training error curves of perceptron learning 
with different feature sets are shown in Figure 2. 
They drop to a low error rate and then stabilize 
after a few number of training rounds, indicating 
that most of the cases are linearly separable and 
that perceptron learning is applicable to the verb 
selection task. 
We conducted feature selection by dropping fea-
tures that occur less than N times. Here N was ex-
perimentally set to 5. We observe that, after feature 
selection, some useful features such as 
?watch_A1_TV? and ?see_A1_TV? were kept, but 
some noisy features like ?Jane_A0_sees? and 
?Jane_A0_watches? were removed, suggesting the 
effectiveness of this feature selection approach. 
 
1073
 Figure 2. Training error curves of the perceptron. 
We tested the baseline and our approach on the 
in-domain and out-of-domain corpora. The results 
are shown in Table 7 and 8, respectively. 
In the in-domain test, the SMT-based approach 
has the highest false alarm rate, though its output 
with word insertions or deletions is not considered 
wrong if the substituted verb is correct. Our ap-
proach, regardless of what feature sets are used, 
outperforms the SMT-based approach in terms of 
all metrics, showing the effectiveness of percep-
tron learning for the verb selection task. Under the 
perceptron learning framework, we can see that the 
system using only SRL-related features has higher 
revised precision and recall of correction, but also 
a slightly higher false alarm rate than the system 
based on only local features. When local features 
and SRL-derived features are integrated together, 
the state-of-the-art performance is achieved with a 
5% increase in recall, and minor changes in preci-
sion and false alarm. 
In the out-of-domain test, the SMT-based ap-
proach performs much better than in the in-domain 
test, especially in terms of false alarm rate, indicat-
ing the SMT-based approach may favor short sen-
tences. However, its recall drops greatly. We ob-
serve similar performance differences between the 
systems with different feature sets under the same 
perceptron learning framework, reaffirming the 
usefulness of the SRL-based features for verb se-
lection. 
We also conducted significance test. The results 
confirm that the improvements (SRL+Local vs. 
SMT-based) are statistically significant (p-value < 
0.001) for both the open-domain and the in-domain 
experiments. 
Furthermore, we studied the performance of our 
system on verbs with varying distance to their ar-
guments on the out-of-domain test corpus. 
 
Local d<=2 2<d<=4 d>4 
RP 64.3% 60.3% 59.4% 
RC 34.6% 33.1% 28.9% 
FA 3.0% 6.3% 5.0% 
SRL d<=2 2<d<=4 d>4 
RP 65.1% 60.1% 62.1% 
RC 40.3% 34.0% 36.9% 
FA 5.0% 6.7% 6.3% 
Table 9. Performance on verbs with different distance to 
their arguments on out-of-domain test data. 
Table 9 shows that the system with only SRL-
derived features performs significantly better than 
the system with only local features on the verb 
whose usage depends on a distant argument, i.e., 
one where the number of words between the predi-
cate and the argument is larger than 4. To under-
stand the reason, consider the following sentence: 
?It's raining outside. Please wear[take] the 
black raincoat with you.? 
 SMT-based Our method 
SRL Local SRL + Local 
RP 48.4% 64.5% 62.2% 66.4% 
RC 23.5% 40.2% 32.9% 46.4% 
FA 13.3% 5.6% 4.2% 6.8% 
Table 7. In-domain test results. 
 SMT-based Our method 
SRL Local SRL + Local 
RP 50.7% 64.0% 62.6% 65.5% 
RC 13.5% 39.0% 33.3% 44.0% 
FA 6.1% 5.5% 4.0% 6.5% 
Table 8. Out-of-domain test results. 
 
1074
Intuitively, ?wear? and ?take? seem to fill the 
blank well, since they both form a collocation with 
?raincoat?; however, when ?with [AM-MNR] you? 
is considered as part of the context, ?wear? no 
longer fits it and ?take? wins. In this case, the long-
distance feature devised from AM-MNR helps se-
lect the suitable verb, while the trigram features 
cannot because they cannot represent the long dis-
tance verb usage context. 
We also find some typical cases that are beyond 
the reach of the SRL-derived features. For instance, 
consider ?Everyone doubts [suspects] that Tom is 
a spy.?. Both of the verbs can be followed by a 
clause. However, the SRL system regards ?is?, the 
predicate of the clause, as the patient, resulting in 
features like ?doubt_A1_is? and ?suspect_A1_is?, 
which capture nothing about verb usage context. 
However, if we consider the whole clause ?sus-
pect_Tom is a spy? as the patient, this could result 
in a very sparse feature that would be filtered. In 
the future, we will combine word-level and phrase-
level SRL systems to address this problem. 
Besides its incapability of handling verb selec-
tion errors involving clauses, the SRL-derived fea-
tures fail to work when verb selection depends on 
deep meanings that cannot be captured by current 
shallow predicate-argument structures. For exam-
ple, in ?He was wandering in the park, spending 
[killing] his time watching the children playing.?, 
though ?spending? and ?killing? fit the syntactic 
structure and collocation agreement, and express 
the meaning ?to allocate some time doing some-
thing?, the word ?wandering? suggests that ?kill-
ing? may be more appropriate. Current SRL 
systems cannot represent the semantic connection 
between two predicates and thus are helpless for 
this case. We argue that the performance of our 
system can be improved along with the progress of 
SRL. 
5 Conclusions and Future Work 
Verb selection is challenging because verb usage 
highly depends on the usage context, which is hard 
to capture and represent. In this paper, we propose 
to utilize the output of an SRL system to explicitly 
model verb usage context. We also propose to use 
the generalized perceptron learning framework to 
integrate SRL-derived features with other features. 
Experimental results show that our method outper-
forms the SMT-based system and achieves state-
of-the-art performance when SRL-related features 
and other local features are integrated. We also 
show that, for cases where the particular verb us-
age mainly depends on its distant arguments, a sys-
tem with only SRL-derived features performs 
much better than the system with only local fea-
tures. 
In the future, we plan to automatically construct 
confusion sets, expand our approach to more verbs 
and test our approach on a larger size of real data. 
We will try to combine the outputs of several SRL 
systems to make our system more robust. We also 
plan to further validate the effectiveness of the 
SRL-derived features under other learning methods 
like SVMs. 
Acknowledgment 
We thank the anonymous reviewers for their valu-
able comments. We also thank Changning Huang, 
Yunbo Cao, Dongdong Zhang, Henry Li and Mu 
Li for helpful discussions. 
References  
Francis Bond, Kentaro Ogura, and Satoru Ikehara. 1994. 
Countability and number in Japanese to English ma-
chine translation. Proc. of the 15th conference on 
Computational Linguistics, pages 32-38. 
Chris Brockett, William B. Dolan, and Michael Gamon. 
2006. Correcting ESL errors using phrasal SMT 
techniques. Proc. of the 21st International Confer-
ence on Computational Linguistics and the 44th An-
nual Meeting on Association for Computational 
Linguistics, pages 249-256. 
Michael Collins. 2002. Discriminative training methods 
for hidden Markov models: theory and experiments 
with perceptron algorithms. Proc. of the ACL-02 
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1-8. 
Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic 
Grammar Checking for Second Language Learners ? 
the Use of Prepositions. Proc. of NoDaliDa. 
Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-
dre Klementiev, William B. Dolan, Dmitrtiy Belen-
ko, and Lucy Vanderwende. 2008. Using Contextual 
Speller Techniques and Language Modeling for ESL 
Error Correction. Proc. of the International Joint 
Conference on Natural Language Processing. 
Shichun Gui and Huizhong Yang. 2002. Chinese Learn-
er English Corpus. Shanghai Foreign Languages Ed-
ucation Press, Shanghai, China. 
1075
Julia E. Heine. 1998. Definiteness predictions for Japa-
nese noun phrases. Proc. of the 36th Annual Meeting 
of the Association for Computational Linguistics and 
17th International Conference on Computational 
Linguistics, pages 519-525. 
John Lee and Stephanie Seneff. 2008. Correcting mis-
use of verb forms. Proc. of the 46th Annual Meeting 
on Association for Computational Linguistics, pages 
174-182. 
Ting Liu, Ming Zhou, Jianfeng Gao, Endong Xun and 
Changning Huang. 2000. PENS: A Machine-aided 
English Writing System for Chinese Users. Proc. of 
the 38th Annual Meeting on Association for Compu-
tational Linguistics, pages 529-536. 
Llu?s M?rquez. 2009. Semantic Role Labeling Past, 
Present and Future, Tutorial of ACL-IJCNLP 2009.   
Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel 
Antohe and Roxana Girju. 2004. Models for the se-
mantic classification of noun phrases. Proc. of the 
HLT-NAACL Workshop on Computational Lexical 
Semantics, pages 60-67. 
Srini Narayanan and Sanda Harabagiu. 2004. Question 
answering based on semantic structures. Proc. of the 
20th International Conference on Computational 
Linguistics, pages 693-701. 
Franz J. Och and Hermann Ney. 2004. The Alignment 
Template Approach to Statistical Machine Transla-
tion. Journal of Computational Linguistics, 30(4), 
pages 417-449. 
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective 
semantic role labelling with Markov Logic. Proc. of 
the Twelfth Conference on Computational Natural 
Language Learning, pages 193-197. 
Andreas Stolcke. 2002. SRILM -- An Extensible Lan-
guage Modeling Toolkit. Proc. of International Con-
ference on Spoken Language Processing, pages: 901-
904. 
Mihai Surdeanu, Lluis M?rquez, Xavier Carreras, and 
Pere R. Comas. 2007. Combination strategies for se-
mantic role labeling. Journal of Artificial Intelligence 
Research, page 105-151. 
Mihai Surdeanu, Sanda Harabagiu, John Williams, and 
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. Proc. of the 41st 
Annual Meeting on Association for Computational 
Linguistics, pages 8-15. 
Joel R. Tetreault and Martin Chodorow. 2008. The ups 
and downs of preposition error detection in ESL writ-
ing. Proc. of the 22nd international Conference on 
Computational Linguistics, pages 865-872. 
Vladimir N. Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer-Verlag, New York. 
Patrick Ye and Timothy Baldwin. 2006. Verb Sense 
Disambiguation Using Selectional Preferences 
Extracted with a State-of-the-art Semantic Role 
Labeler. Proc. of the Australasian Language 
Technology Workshop, pages 141-148. 
Xing Yi, Jianfeng Gao, and William B. Dolan. 2008. A 
Web-based English Proofing System for English as a 
Second Language Users. Proc. of International Joint 
Conference on Natural Language Processing, pages 
619-624. 
1076
