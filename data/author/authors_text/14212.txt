Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 31?34,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Just Title It! (by an Online Application)
Ce?dric Lopez, Violaine Prince, and Mathieu Roche
LIRMM, CNRS, University of Montpellier 2
161, rue Ada
Montpellier, France
{lopez,prince,mroche}@lirmm.fr
Abstract
This paper deals with an application of au-
tomatic titling. The aim of such application
is to attribute a title for a given text. So,
our application relies on three very differ-
ent automatic titling methods. The first one
extracts relevant noun phrases for their use
as a heading, the second one automatically
constructs headings by selecting words ap-
pearing in the text, and, finally, the third
one uses nominalization in order to propose
informative and catchy titles. Experiments
based on 1048 titles have shown that our
methods provide relevant titles.
1 Introduction
The important amount of textual documents is
in perpetual growth and requires robust applica-
tions. Automatic titling is an essential task for
several applications: Automatic titling of e-mails
without subjects, text generation, summarization,
and so forth. Furthermore, a system able to ti-
tle HTML documents and so, to respect one of
the W3C standards about Web site accessibility,
is quite useful. The titling process goal is to pro-
vide a relevant representation of a document con-
tent. It might use metaphors, humor, or emphasis,
thus separating a titling task from a summariza-
tion process, proving the importance of the rhetor-
ical status in both tasks.
This paper presents an original application con-
sisting in titling all kinds of texts. For that pur-
pose, our application offers three main meth-
ods. The first one (called POSTIT) extracts noun
phrases to be used as headings, the second one
(called CATIT) automatically builds titles by se-
lecting words appearing in the text, and, finally,
the third one (called NOMIT) uses nominalization
in order to propose relevant titles. Morphologic
and semantic treatments are applied to obtain ti-
tles close to real titles. In particular, titles have to
respect two characteristics: Relevance and catch-
iness.
2 Text Titling Application
The application presented in this paper was de-
veloped with PHP, and it is available on the
Web1. It is based on several methods using Nat-
ural Language Processing (NLP) and Information
Retrieval (IR) techniques. So, the input is a text
and the output is a set of titles based on different
kinds of strategies.
A single automatic titling method is not suffi-
cient to title texts. Actually, it cannot respect di-
versity, noticed in real titles, which vary accord-
ing to the writer?s personal interests or/and his/her
writing style. With the aim of getting closer to this
variety, the user can choose the more relevant title
according to his personal criteria among a list of
titles automatically proposed by our system.
A few other applications have focused on ti-
tling: One of the most typical, (Banko, 2000),
consists in generating coherent summaries that
are shorter than a single sentence. These sum-
maries are called ?headlines?. The main diffi-
culty is to adjust the threshold (i.e, the headline
length), in order to obtain syntactically correct
titles. Whereas our methods create titles which
are intrinsically correct, both syntactically and se-
mantically.
In this section, we present the POSTIT, CATIT,
and NOMIT methods. These three methods run
1https://www2.lirmm.fr/?lopez/Titrage_
general/
31
in parallel, without interaction with each other.
Three very different titles are thus determined for
every text. For each of them, an example of the
produced title is given on the following sample
text: ?In her speech, Mrs Merkel has promised
concrete steps towards a fiscal union - in effect
close integration of the tax-and-spend polices of
individual eurozone countries, with Brussels im-
posing penalties on members that break the rules.
[...]?. Even if examples are in English, the ap-
plication is actually in French (but easily repro-
ducible in English). The POS tagging was per-
formed by Sygfran (Chauche?, 1984).
2.1 POSTIT
(Jin, 2001) implemented a set of title generation
methods and evaluated them: The statistical ap-
proach based on the TF-IDF obtains the best re-
sults. In the same way, the POSTIT (Titling using
Position and Statistical Information) method uses
statistical information. Related works have shown
that verbs are not as widely spread as nouns,
named entities, and adjectives (Lopez, 2011a).
Moreover, it was noticed that elements appearing
in the title are often present in the body of the text
(Zajic et al 2002). (Zhou and Hovy, 2003) sup-
ports this idea and shows that the covering rate of
those words present in titles, is very high in the
first sentences of a text. So, the main idea is to
extract noun phrases from the text and to select
the more relevant for its use as title. The POSTIT
approach is composed of the following steps:
1. Candidate Sentence Determination. We as-
sume that any text contains only a few rel-
evant sentences for titling. The goal of this
step consists in recognizing them. Statistical
analysis shows that, very often, terms useful
for titling are located in the first sentences of
the text.
2. Extracting Candidate Noun Phrases for Ti-
tling. This step uses syntactical filters re-
lying on the statistical studies previously
led. For that purpose, texts are tagged with
Sygfran. Our syntactical patterns allowing
noun phrase extraction are also inspired from
(Daille, 1996).
3. Selecting a Title. Last, candidate noun
phrases (t) are ranked according to a score
based on the use of TF-IDF and information
about noun phrase position (NPPOS) (see
Lopez, 2011a). With ? = 0.5, this method
obtains good results (see Formula 1).
NPscore(t) = ??NPPOS(t)
+ (1? ?)?NPTF?IDF (t) (1)
Example of title with POSTIT: Concrete steps
towards a fiscal union.
On one hand, this method proposes titles which
are syntactically correct. But on the other hand,
provided titles can not be considered as original.
Next method, called CATIT, enables to generate
more ?original? titles.
2.2 CATIT
CATIT (Automatic Construction of Titles) is an
automatic process that constructs short titles. Ti-
tles have to show coherence with both the text and
the Web, as well as with their dynamic context
(Lopez, 2011b). This process is based on a global
approach consisting in three main stages:
1. Generation of Candidates Titles. The pur-
pose is to extract relevant nouns (using TF-
IDF criterion) and adjectives (using TF cri-
terion) from the text. Potential relevant cou-
ples (candidate titles) are built respecting the
?Noun Adjective? and/or ?Adjective Noun?
syntactical patterns.
2. Coherence of Candidate Titles. Among the
list of candidate titles, which ones are gram-
matically and semantically consistent ? The
produced titles are supposed to be consis-
tent with the text through the use of TF-
IDF. To reinforce coherence, we set up a
distance coefficient between a noun and an
adjective which constitutes a new coherence
criterion in candidate titles. Besides, the fre-
quency of appearance of candidate titles on
the Web (with Dice measure) is used in order
to measure the dependence between the noun
and the adjective composing a candidate ti-
tle. This method thus automatically favors
well-formed candidates.
3. Dynamic Contextualization of Candidate Ti-
tles. To determine the most relevant candi-
date title, the text context is compared with
the context in which these candidates are met
32
on the Web. They are both modeled as vec-
tors, according to Salton?s vector model.
Example of title with CATIT: Fiscal penalties.
The automatic generation of titles is a complex
task because titles have to be coherent, grammat-
ically correct, informative, and catchy. These cri-
teria are a brake in the generation of longer ti-
tles (being studied). That is why we suggest a
new approach consisting in reformulating rele-
vant phrases in order to determine informative and
catchy ?long? titles.
2.3 NOMIT
Based on statistical analysis, NOMIT (Nominal-
ization for Titling) provides original titles relying
on several rules to transform a verbal phrase in a
noun phrase.
1. Extracting Candidates. First step consists in
extracting segments of phrases which con-
tain a past participle (in French). For exam-
ple: In her speech, Mrs Merkel has promised
?concrete steps towards a fiscal union? -
in effect close integration of the tax-and-
spend polices of individual eurozone coun-
tries, with Brussels imposing penalties on
members that break the rules.
2. Linguistic Treatment. The linguistic treat-
ment of the segments retained in the previous
step consists of two steps aiming at nominal-
izing the ?auxiliary + past participle? form
(very frequent in French). First step consists
in associating a noun for each past participle.
Second step uses transforming rules in order
to obtain nominalized segments. For exam-
ple: has promised? promise.
3. Selecting a Title. Selection of the most rel-
evant title relies on a Web validation. The
interest of this validation is double. On one
hand, the objective is to validate the connec-
tion between the nominalized past partici-
ple and the complement. On the other hand,
the interest is to eliminate incorrect semantic
constituents or not popular ones (e.g., ?an-
nunciation of the winners ?), to prefer those
which are more popular on Web (e.g. , ?an-
nouncement of the winners?).
Figure 1: Screenshot of Automatic Titling Evaluation
Example of title with NOMIT: Mrs Merkel:
Promise of a concrete step towards a fiscal union.
This method enables to obtain even more orig-
inal titles than the previous one (i.e. CATIT).
A positive aspect is that new transforming rules
can be easily added in order to respect morpho-
syntactical patterns of real titles.
3 Evaluations
3.1 Protocol Description
An online evaluation has been set up, accessi-
ble to all people (cf. Figure 1)2. The benefit of
such evaluation is to compare different automatic
methods according to several judgements. So, for
each text proposed to the human user, several ti-
tles are presented, each one resulting from one of
the automatic titling methods presented in this pa-
per (POSTIT, CATIT, and NOMIT). Furthermore,
random titles stemming from CATIT and POSTIT
methods are evaluated (CATIT-R, and POSTIT-
R), i.e., candidate titles built by our methods but
not selected because of their bad score. The idea
is to measure the efficiency of our ranking func-
tions.
This evaluation is run on French articles stem-
ming from the daily newspaper ?Le Monde?. We
retained the first article published every day for
the year 1994, up to a total of 200 journalistic ar-
ticles. 190 people have participated to the online
experiment, evaluating a total of 1048 titles. On
average, every person has evaluated 41 titles. Ev-
ery title has been evaluated by several people (be-
tween 2 and 10). The total number of obtained
evaluations is 7764.
2URL: http://www2.lirmm.fr/?lopez/
Titrage_general/evaluation_web2/
33
3.2 Results
Results of this evaluation indicate that the most
adapted titling method for articles is NOMIT. This
one enables to title 82.7% of texts in a relevant
way (cf. Table 1). However, NOMIT does not de-
termine titles for all the texts (in this evaluation,
NOMIT determined a title for 58 texts). Indeed,
if no past participle is present in the text, there is
no title returned with this method. It is thus essen-
tial to consider the other methods which assure a
title for every text. POSTIT enables to title 70%
of texts in a relevant way. It is interesting to note
that both gathered methods POSTIT and NOMIT
provide at least one relevant title for 74 % of texts
(cf. Table 2). Finally, even if CATIT obtains a
weak score, this method provides a relevant title
where POSTIT and NOMIT are silent. So, these
three gathered methods propose at least one rele-
vant title for 81% of journalistic articles.
Concerning catchiness, the three methods seem
equivalent, proposing catchy titles for approxi-
mately 50% of texts. The three gathered methods
propose at least one catchy title for 78% of texts.
Real titles (RT) obtain close score (80.5%).
% POSTIT POSTIT-R CATIT CATIT-R NOMIT RT
Very relevant (VR) 39.1 16.4 15.7 10.3 60.3 71.4
Relevant (R) 30.9 22.3 21.3 14.5 22.4 16.4
(VR) and (R) 70.0 38.7 37.0 24.8 82.7 87.8
Not relevant 30.0 61.4 63.0 75.2 17.2 12.3
Catchy 49.1 30.9 47.2 32.2 53.4 80.5
Not catchy 50.9 69.1 52.8 67.8 46.6 19.5
Table 1: Average scores of our application.
% POSTIT & NOMIT POSTIT & CATIT NOMIT & CATIT POSTIT, CATIT, & NOMIT
(VR) 47 46 28 54
(R) or (VR) 74 78 49 81
Catchy 57 73 55 78
Table 2: Results of gathered methods.
Also, let us note that our ranking functions
are relevant since CATIT-R and POSTIT-R obtain
weak results compared with CATIT and POSTIT.
4 Conclusions
In this paper, we have compared the efficiency of
three methods using various techniques. POSTIT
uses noun phrases extracted from the text, CATIT
consists in constructing short titles, and NOMIT
uses nominalization. We proposed three different
methods to approach the real context. Two per-
sons can propose different titles for the same text,
depending on personal criteria and on its own in-
terests. That is why automatic titling is a complex
task as much as evaluation of catchiness which
remains subjective. Evaluation shows that our ap-
plication provides relevant titles for 81% of texts
and catchy titles for 78 % of texts. These re-
sults are very encouraging because real titles ob-
tain close results.
A future work will consist in taking into ac-
count a context defined by the user. For exam-
ple, the generated titles could depend on a polit-
ical context if the user chooses to select a given
thread. Furthermore, an ?extended? context, au-
tomatically determined from the user?s choice,
could enhance or refine user?s desiderata.
A next work will consist in adapting this appli-
cation for English.
References
Michele Banko, Vibhu O. Mittal, and Michael J Wit-
brock. 1996. Headline generation based on statisti-
cal translation. COLING?96. p. 318?325.
Jacques Chauche?. 1984. Un outil multidimensionnel
de l?analyse du discours. COLING?84. p. 11-15.
Be?atrice Daille. 1996. Study and implementation
of combined techniques for automatic extraction of
terminology. The Balancing Act: Combining Sym-
bolic and Statistical Approaches to language. p. 29-
36.
Rong Jin, and Alexander G. Hauptmann. 1996. Au-
tomatic title generation for spoken broadcast news.
Proceedings of the first international conference on
Human language technology research. p. 1?3.
Ce?dric Lopez, Violaine Prince, and Mathieu Roche.
2011. Automatic titling of Articles Using Position
and Statistical Information. RANLP?11. p. 727-
732.
Ce?dric Lopez, Violaine Prince, and Mathieu Roche.
2011. Automatic Generation of Short Titles.
LTC?11. p. 461-465.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval.
Information Processing and Management 24. p.
513-523.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. International Confer-
ence on New Methods in Language Processing. p.
44-49.
Franck Smadja, Kathleen R. McKeown, and Vasileios
Hatzivassiloglou. 1996. Translating collocations
for bilingual lexicons: A statistical approach. Com-
putational linguistics, 22(1). p. 1-38.
David Zajic, Bonnie Door, and Rich Schwarz. 2002.
Automatic headline generation for newspaper sto-
ries. ACL 2002. Philadelphia.
Liang Zhou and Eduard Hovy. 2002. Headline sum-
marization at ISI. DUC 2003. Edmonton, Alberta,
Canada.
34
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 274?283,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
NOMIT: Automatic Titling by Nominalizing
C?dric Lopez, Violaine Prince, and Mathieu Roche
LIRMM, CNRS, Univ. Montpellier 2
161, rue Ada
Montpellier, France
{lopez,prince,mroche}@lirmm.fr
Abstract
The important mass of textual documents is
in perpetual growth and requires strong ap-
plications to automatically process informa-
tion. Automatic titling is an essential task for
several applications: ?No Subject? e-mails ti-
tling, text generation, summarization, and so
forth. This study presents an original ap-
proach consisting in titling journalistic articles
by nominalizing. In particular, morphological
and semantic processing are employed to ob-
tain a nominalized form which has to respect
titles characteristics (in particular, relevance
and catchiness). The evaluation of the ap-
proach, described in the paper, indicates that
titles stemming from this method are informa-
tive and/or catchy.
1 Introduction
A title establishes a link between a reader and a
text. It has two main functions. First of all, a ti-
tle can be informative (it conveys relevant informa-
tion about the text content and aim), and second, it
can be catchy or incentive (Herrero Cecilia, 2007).
A heading is said to be catchy when it succeeds in
capturing the reader?s attention on an aspect of the
announced event, in a ingenious, metaphoric, enig-
matic, or shocking way. From a syntactic point of
view, a title can be a word, a phrase, an expression,
a sentence, that designates a paper or one of its parts,
by giving its subject.
Titles are used within applications such as auto-
matic generation of contents, or summarization. So,
it is interesting to automate the process that produces
relevant titles by extracting them from texts, and
supplying other applications with such data, while
avoiding any human intervention: Direct applica-
tions (as automatic titling of "no object" e-mails) are
thus possible.
The point is that several titles can be relevant for a
same text: This constitutes the main difficulty of au-
tomatic titling. Some writers prefer informative ti-
tles, whereas others prefer catchy ones. Others jug-
gle with both criteria according to the context and
the type of the publication. So, evaluation of au-
tomatic titling is a complex step requiring a human
intervention. Indeed, how can titles relevance be es-
timated ? How an automatic title can be compared
to a human-written ("real") title, knowing that both
can have a very different morphosyntactic structure?
Automatic titling is a full process, possessing its
own functions. It has to be sharply differentiated
from summarization and indexation tasks. Its pur-
pose is to propose title(s) that have to be short, infor-
mative and/or catchy, and keep a coherent syntactic
structure. NLP1 methods will be exploited in order
to abide by language morphosyntactic and semantic
constraints in titling.
In this paper, we describe an approach of auto-
matic titling relying on nominalization, i.e. rules
transforming a verb phrase into a noun phrase (e.g.
"the president left" is nominalized into " President?s
Departure"). This study raises two crucial questions:
(1) Determining sentences and phrases containing
relevant information (2) Nominalizing a chosen item
and using it as a title. Example: From the fol-
lowing pair of sentences "The disappointing perfor-
1Natural Language Processing
274
mance, on Sunday October 9th, of S?gol?ne Royal,
amazed the French citizens. For months, they de-
fended their candidate on the Web.", containing the
relevant information about an article in the French
press in 2007, the idea is to built the following title:
"S?gol?ne Royal: Surprise of the French citizens".
In fact, other titles could apply such as "S?gol?ne
Royal?s Disappointing Performance" or "Surprising
the French Citizens", but notice that both are less in-
formative, since they drop a part of the information.
This article is organized as such: The follow-
ing section briefly positions automatic titling in its
research environment and describes previous work
(section 2). The next one describes NOMIT, our ap-
proach of automatic titling by nominalization, which
consists in three successive steps: Extracting candi-
date headings from the document (section 3.1), pro-
cessing them linguistically (section 3.2), and last,
selecting one among the produced headings, which
will play the role of the system heading suggestion
(section 3.3). Finally, the results of NOMIT evalua-
tion are presented and discussed (section 4).
2 Previous Work
Automatic titling of textual documents is a subject
often confused with summarization and indexation
tasks. While a summary has to give an outline of the
text contents, the title has to indicate the subject of
the text without revealing all the contents. The pro-
cess of summarization can use titles, as in (Blais et
al., 2007) and (Amini et al, 2005), thus demonstrat-
ing their importance. Automatic summarization pro-
vides a set of relevant sentences extracted from the
text: The total number of sentences is diminished,
but sentences are not shortened by themselves. Ul-
timately reducing the number to one does not pro-
vide a title, since the latter is very rarely a sentence,
but needs to be grammatically consistent. It is also
necessary to differentiate automatic titling from text
compression: Text compression might shorten sen-
tences but keep the original number of sentences
(Yousfi-Monod and Prince, 2008). Mixing both ap-
proaches appears as a very costly process to under-
take, more adapted to a summarization task, when
titling might be obtained by less expansive tech-
niques.
Titling must also be differentiated from indexa-
tion because titles do not always contain the text
key-words: Headings can present a partial or total
reformulation of the text, not relevant for an index,
which role is to facilitate the user?s search and re-
trieval. Once again, the construction of an index can
use titles appearing in the document. So, if deter-
mining relevant titles is a successful task, the quality
of indexation will largely be improved.
An automatic titling approach, named POSTIT,
extracts relevant noun phrases to be used as titles
(Lopez et al, 2011b). One of its benefits is that long
titles, syntactically correct, can be proposed. The
main inconvenience is that it cannot provide orig-
inal titles, using a funny form for example, unless
this one already appears in the text (which can be
rather scarce, even in newspapers articles). In the
same environment, a variant of this approach, called
CATIT, constructing short titles, has been developed
by the same authors (Lopez et al, 2011a). It tries to
built titles which are relevant to the texts. It evalu-
ates their quality by browsing the Web (popular and
recognized expressions), as well as including those
titles dynamic context. Applied to a corpus of jour-
nalistic articles, CATIT was able to provide head-
ings both informative and catchy. However, syntac-
tical patterns used for titles building were short (two
terms) and experience showed that longer titles were
often preferred.
Another approach, presented by (Banko et al,
2000), consists in generating coherent summaries
that are shorter than a single sentence. These sum-
maries are called "headlines". The main difficulty is
to adjust the threshold (i.e., the length of the head-
line), in order to obtain syntactically correct titles.
This is the main difference with our method NOMIT,
which ensures that its produced titles are always syn-
tactically correct.
If a system were to produce informative, catchy,
and variable-sized (in number of words) titles, the
nominalization of constituents seems to be an inter-
esting approach. Nominalization is a process trans-
forming an adjective or a verb into a noun or noun
phrase. In a nominalized constituent, the time of the
event is not in touch with the time of the speech of
the event (for example, "President?s departure" does
not infer that the president already left, contrary to
"The president left"). In some languages such as
German and French, nominalization answers an ac-
275
tivity of conceptualization and conciseness. In a ti-
tle, it allows to focus, according to the context of
the author, on the dimension of the event consid-
ered the most relevant. (Moirand, 1975) already no-
ticed that in French journalistic articles, numerous
titles appear with a nominalized form. This obser-
vation was recently confirmed by (Herrero Cecilia,
2007). It is thus interesting to study automatic ti-
tling by nominalization of constituents when dealing
with languages where it is often used. In English, the
method stays the same, but the pattern changes: En-
glish headings patterns incline towards progressive
present (e.g. "Tempest looming"), an infinitive form
with a past participle (e.g. "Conference to be held"),
and always with a deletion of articles. This paper fo-
cuses mostly on French because of its available data,
but a shift in languages and patterns is contemplated
in a further step.
3 NOMIT: Titling by Nominalizing
Since nominalization converts a sentence into a noun
or a noun phrase, it can always be described by a
transformation. Some transformations are easy-to-
do, in particular, transforming verb participles into
names or adjectives (such as defined by (Dubois and
Dubois-Charlier, 1970)). For example, "arriv?(e)"
(arrived is a French verbal participle which is equal
to its nominalized shape "arriv?e" (arrival). Others
are more complex, for example the past participle
"parti" (gone) which nominalized form is "d?part"
(departure). For these last ones, the use of a lexicon
is necessary.
The nominalization process embedded in NOMIT
develops three successive stages. The first one con-
cerns the extraction of candidates according to a
classical process in NLP: Data preparation, mor-
phosyntactic labeling, selection of the data to be
studied. The second phase consists in performing
a linguistic process, including morphosyntactic and
semantic aspects. Finally, the third phase focuses on
selecting a relevant title. Figure 1 presents the global
process, detailed in the following sub-sections.
We chose to focus our study on journalistic ar-
ticles stemming from Le Monde (year 1994), a fa-
mous French daily paper, since their electronic form
is available for scientific investigation. Note that the
method presented in this paper is applicable to all
Figure 1: Global process of NOMIT
types of texts (articles, news, blogs, and so forth).
3.1 Extracting Candidates
This first phase consists in extracting the candidates
(cf. section 3.2), which will be considered as poten-
tial titles after a linguistic treatment. It consists, in
turn, of four steps. The first step determines the ar-
ticle relevant data (i.e. fragments or reformulations
representing at best the main information emanating
from the text).
The described approach relies on the assumption
that good candidate phrases can be found in the first
two sentences of the article. Actually the best cov-
ering rate of the words of real titles is obtained with
these first sentences (see (Baxendale, 1958), (Vinet,
1993), (Jacques and Rebeyrolle, 2004), and (Lopez
et al, 2011b) regarding the POSTIT approach), jus-
tifying this choice. So, here, the selection of relevant
sentences (cf. Fig. 1, step 1.a) is limited to extract-
ing the first two sentences of the text.
Step 1.b (cf. Fig. 1) consists in labeling these
two sentences via SYGFRAN (Chauch? and Prince,
276
2007), a morphosyntactic parser that tags words.
Thus, the presence of a "auxiliary + past partici-
ple" form syntactic pattern is tested2 (for example,
"a augment?" meaning has increased). If such a pat-
tern is recognized in the sentence, then it is retained
and goes into the following stages. Otherwise, the
sentence is ignored. Then, sentences are pruned ac-
cording to two heuristics.
(Knight and Marcu, 2002) have studied sentence
compression by using a noisy-channel model which
consists in making the following hypothesis: The
sentence to be compressed was formerly short and
the author has extended it with additional informa-
tion (noise). Sentence compression, could, at a first
glance, appear as a possible clue, however, our ap-
proach does not aim at reducing at most the treated
sentence. Indeed, elements which can be pruned to
obtain a good summary do not always need to be
pruned to obtain a good title. So, the NOMIT sen-
tence pruning step (cf. Fig. 1, step 1.c) does not only
preserve the governors3. Here, the text is pruned
according to three heuristics, inspired from (Yousfi-
Monod and Prince, 2008), focusing on the function
and position of constituents in the syntactic tree:
1. Elimination of dates (for example "The disap-
pointing performance, on Sunday, October 9th,
of S?gol?ne Royal" becomes "The disappoint-
ing performance of S?gol?ne Royal "),
2. Elimination of phrases directly juxtaposed to a
past participle (for example "He chose, while
he was still hesitating, to help him" becomes
"He chose to help him"),
3. Elimination of the relative pronoun and the
proposition introduced by it ("Its presence,
which was not moreover wished, was noticed"
becomes "Its presence was noticed ").
These three heuristics are crucial to obtain a co-
herent title. In this step, grammaticality4 and conci-
sion5 must be respected.
2the pattern features are tuned to French, but the same struc-
ture globally applies to English too.
3governors of constituents considered as indispensable to
the grammatical and semantic coherence of the sentence
4The sentence must be well formed and must obey the lan-
guage grammar.
5a pruned sentence has to contain the relevant information
of the original sentence.
Finally, both sentences are segmented accord-
ing to punctuation (points, commas, colons, brack-
ets, interrogation marks, exclamation marks, and so
forth6) and only segments containing a "auxiliary +
past participle" pattern are preserved (cf. Fig. 1,
step 1.d). Also, segments containing pronouns are
not retained in the following steps to avoid problems
related to referents 7.
In the following example, each step is indicated
by a reference sending back to the global process
presented in Figure 1:
Original text:
? Yet they truly believed in it. The disappointing
performance, on Sunday, October 9th, of S?-
gol?ne Royal, amazed the French citizens. For
months, they defended their candidate on the
Web.
Treatments:
? (1.a) Yet they truly believed in it. The disap-
pointing performance, on Sunday, October 9th,
of S?gol?ne Royal, amazed the French citizens.
? (1.b) The disappointing performance, on Sun-
day, October 9th, of S?gol?ne Royal, amazed
the French citizens.
? (1.c) The disappointing performance of S?-
gol?ne Royal, amazed the French citizens.
? (1.d) amazed the French citizens8.
The following step enables to determine a relevant
title from the result obtained at step 1.d.
3.2 Linguistic Treatment
The linguistic treatment of segments, present in
those sentences retained in the previous section, is
constituted by two stages aiming at nominalizing the
6Points marking an abbreviation are not obviously taken into
account in this step.
7For example, the title "Disappointment of her partisans"
would not be very informative because of the presence of "her"
(unknown referent).
8We shall see in the section 3.2.2 how, in some cases, it is
possible to take into account the subject, i.e. S?gol?ne Royal in
this example.
277
"auxiliary + past participle" pattern. Here, the verbal
basis is transformed into an action noun.
The first step consists in obtaining the infinitive
of the verb to be nominalized from the past partici-
ple. Then, from the infinitive, possible nominalized
forms are returned. Even if several linguistic stud-
ies propose classifications by families of suffixes, it
is complex to process them automatically. The use
of a lexicon is a good solution allowing to ensure a
correct nominalized form.
3.2.1 Semantic Treatment
From past participle towards infinitive verb.
In step 1.b, segments of sentences containing the
"auxiliary + past participle" syntactic pattern were
extracted. For every past participle extracted, the
endings of conjugation are eliminated, and only
radicals are preserved (for example, "mang?es"
(eaten) becomes "mang" (eat) (cf. Fig. 1, step
2.a). Afterwards, every radical is associated with its
infinitive verb using a lexicon9 built for that purpose
from the data established by the parser SYGFRAN
(cf. Fig. 1, step 2.b).
From infinitive verb towards the verb action.
JeuxDeMots10 is a French serious game enabling
the construction of a lexical network via a recre-
ational activity proposed on the Web. The prototype
was created in 2008 (Lafourcade and Zampa, 2007).
Today, more than 238,000 terms and more than
1,200,000 relations constitute the network. This
popular, evolutionary, and good quality network,
possesses a satisfactory knowledge coverage. All in
all, more than 40 types of relations were recorded
in the network. One of them interests us more par-
ticularly: The relation called "verb action". This
"action" is very interesting for obtaining a nominal-
ized form, in particular for verbs having their struc-
ture modified during their nominalization (addition
of suffix or prefix in particular). For example, we
obtain "d?part" (departure) from the infinitive "par-
tir" (to leave)(cf. Fig. 1, step 2.c).
Let us note that several action names can exist for
the same verb. For example, "annonce" (announce-
ment) and "annonciation" (annunciation) are two ac-
tions of the verb "annoncer" (to announce). At this
9this lexicon contains 5,897 entries.
10http://www.jeuxdemots.org
stage, all action names are preserved and will be
considered in the next phase, consisting in nominal-
izing the candidates determined in the step before.
3.2.2 Morphosyntactic Treatment
The morphosyntactic processing aims at estab-
lishing rules that automatically transform a con-
stituent into its nominalized form. The purpose is
not to establish an exhaustive list of transformation
rules but to assure a correct transformation.
To transpose the agents of a verb into a nominal-
ized constituent, the French language makes a pro-
ficient use of prepositions. So when nominalizing
"auxiliary + past participle" in order to connect it
with its complement, the preposition "de" ("of") is
mandatory11. In English, although "X of Y" is an
accepted pattern, the genitive form "Y(?s) X" would
be preferred. If the complement does not exist, the
subject takes its place.
? Rule 1: Subject + Aux + PP + Complement =>
Verb action + (de) + Complement
? Original sentence: Il a annonc? les gag-
nants (He announced the winners)
? Radicalisation (2.a): Annonc
? Infinitive (2.b): Annoncer
? Actions associated to the infinitive (2.c):
Annonce ; annonciation
? Nominalization (2.d): Annonce des gag-
nants (Announcement of the winners or
Winners? announcement ) ; annonciation
des gagnants (Annunciation of the winners
or Winners? annunciation)
? Rule 2: Subject + Aux + PP => Action of the
verb + (de) + Subject
? Original sentence: Le pr?sident a d?mis-
sionn? (The president resigned)
? Radicalisation (2.a): D?mission
? Infinitive (2.b): D?missionner
? Actions associated to the infinitive (2.c):
D?mission (Resignation)
? Nominalization (2.d): D?mission du
pr?sident (Resignation of the president or
President?s resignation)
11The preposition can be contracted if needed ("de le" = "du",
"de les" = "des", and so forth.)
278
In section 3.1, relative subordinate pronoun and
subordinate clauses are eliminated because the in-
formation they convey is too secondary to be empha-
sized in a title. For example, "My cousin, who lives
in Paris, moved" becomes "My cousin moved". So,
according to the second rule, the nominalized form
will be "Moving of my cousin" and not "Moving of
my cousin who lives in Paris".
The third rule leads to titles with a very popular
form in French newspapers. It is about contextual-
izing the information via the use of a proper noun.
So, if in the treated constituent a single proper noun
appears (easily locatable by the presence of a capital
letter), the common noun can be put in connection
with the nominalized past participle (without con-
cluding that this common noun is an agent of the
nominalized verb). This new rule produces titles
with the following form: "Proper noun: verb action
+ Prep + Complement". For example, "S?gol?ne re-
turned to Strasburg" becomes "S?gol?ne: Strasburg
comeback".
? Rule 3: Subject + Aux + PP => Proper Noun:
Verb action + (de) + Complement (if it exists
only one proper noun in the subject)
? Original sentence: Bon nombre de par-
ticuliers se sont pr?cipit?s (rushed)aux
guichets des banques pour souscrire ? des
PEL (Several individuals rushed to bank
counters and subscribed to home-buying
savings plans)
? Radicalisation (2.a): Pr?cipit
? Infinitive (2.b): Pr?cipiter
? Action associated to the infinitive (2.c):
Pr?cipitation
? Nominalization (2.d): PEL : pr?cipitation
aux guichets des banques (Home Buying
Saving plans: Rush at Banks Counters)
Section 3.2.1, pointed that several nominalized
forms were possible for the same verb. So, the phase
of linguistic treatment enables to determine a list of
possible noun forms for every constituent. For ex-
ample, if in step 1 we had "The restaurant Gazza,
situated in a business area, announced a new price",
rule 1 would transform this sentence into two can-
didates: "Gazza: New price announcement" and
"Gazza: New price annunciation" (queer indeed!).
The following phase consists in selecting the most
relevant candidate.
3.3 Selecting a Title
The selection of the most relevant title relies on a
Web validation (cf. Fig. 1, stage 3). A segment that
frequently appears on the Web tends to be seen as:
(1) popular, (2) structurally sound. Thus, the fre-
quency of appearance of n-grams on the Web (via
the Google search engine) appears as a good indica-
tor of the n-gram popularity/soundness (Keller and
Lapata, 2003) . In our case, a n-gram is a segment of
the nominalized constituent, constituted by the nom-
inalized past participle (NPP) and by the preposition
followed by the short complement (i.e. reduced to
the common noun).
The benefit of this validation is double. On one
hand, it backs up the connection between the NPP
and the complement (or subject according to the rule
of used transformation). On the other hand, it helps
eliminating semantically incorrect or unpopular con-
stituents (for example, "Winners? annunciation") to
prefer those which are more popular on the Web (for
example, "Winners? announcement") 12.
3.4 Discussion
Our automatic titling approach (NOMIT) proposes
titles for journalistic articles containing a "auxiliary
+ past participle" form in at least one of its first two
sentences. The rationale for such a method is not
only conciseness, but also presentation: How to gen-
erate a heading inciting the reader to go further on.
Of course, transformation rules such as those pre-
sented here, can be numerous and various, and de-
pend on language, genre, and purpose. The basic
purpose of this work is to provide a sort of a "proof
of concept", in which relevant titles might be auto-
matically shaped.
12We do not here claim to select the most coherent con-
stituents regarding the text. Since the main hypothesis underly-
ing this study is that the first two sentences of the article contain
the necessary and sufficient information to determine a relevant
title, we consider implicitly obtaining nominalized constituents,
that are relevant to the text
279
4 Evaluation
Evaluation of titles is a difficult and boring task.
That is why we set up an online evaluation to share
the amount of work. A call for participation was
submitted in the French community of researchers
(informatics, linguistics). Even if we do not know
the information relative to every annotator (national-
ity, age, etc.), we think that a great majority of these
annotators have a rather good level in French, to
judge titles (this is confirmed by the well-writing of
the collected definitions for "relevance" and "catch-
iness").
NOMIT has been evaluated according to two pro-
tocols. The first one consisted in a quantitative
evaluation, stemming from an on-line user evalua-
tion13. 103 people have participated to this evalua-
tion. The second was an evaluation performed by 3
judges. This last one enables to compute the agree-
ment inter-judges on the various criteria of the eval-
uation process. In both cases, the French daily paper
Le Monde (1994) is used, thus avoiding any con-
nection to the subjectivity of recent news personal
analysis.
4.1 Quantitative Evaluation
4.1.1 Protocol Description
As previously seen, titles proposed by automatic
methods cannot be automatically evaluated. So, an
on-line evaluation was set up, opened to every per-
son. The interest of such an evaluation is to compare
the various methods of automatic titling (cf. section
2) according to several judgments. So, for every text
proposed to the human judges, four titles were pre-
sented, each resulting from different methods of ti-
tling:
? NOMIT: Automatic Titling by Nominalizing.
? POSTIT: Based on the extraction of noun
phrases to propose them as titles.
? CATIT: Based on the construction of short ti-
tles.
? Real Title (RT).
13http://www.lirmm.fr/~lopez/Titrage_
general/evaluation_web2/
For every title, the user had to attribute one of the
following labels: "relevant", "rather relevant", "irrel-
evant", "neutral". Also, the user had to estimate the
catchiness, by choosing one of the following labels:
"catchy", "not catchy", "neutral". Before beginning
the evaluation, the user is asked about his/her own
definition of a relevant title and of a catchy title
(all in all, 314 definitions were collected). Globally,
there is a popular consensus saying that a title is rel-
evant if it is syntactically correct while reflecting the
essential idea conveyed in the document. However,
definitions of catchiness were less consensual. Here
are some collected definitions:
1. A title is catchy if the words association is syn-
tactically correct but semantically "surprising".
However, a catchy title has to be close to the
contents of the text.
2. A catchy title is a title which tempts the reader
into going through the article.
3. A title which holds attention, a title which we
remember, a funny title for example.
4. A title which is going to catch my attention be-
cause it corresponds to my expectations or my
centers of personal interests.
5. A catchy title is a short and precise title.
The titled texts were distributed to the judges in a
random way. Every title was estimated by a number
of persons between 2 and 10. All in all, 103 persons
participated in the evaluation of NOMIT.
Let p1 be the number of titles considered relevant,
p2 the number of titles considered rather relevant,
and let p3 be the number of titles considered irrel-
evant. Within the framework of this evaluation, it
is considered that a title is relevant if p1 ? p3, and
rather relevant if p2 ? p3.
A title is considered "catchy" if at least two judges
considered it catchy.
4.1.2 Results
In spite of the weak number of titles estimated in
this first evaluation, the significant number of judges
helped obtaining representative results. In our ex-
periments, 53 titles generated by the NOMIT ap-
proach were evaluated representing a total of 360
280
evaluations. These results were compared with the
200 titles generated with POSTIT, 200 with CATIT,
and 200 RT (653 titles and 8354 evaluations). Re-
sults (cf. Table 1) show that 83% of the titles pro-
posed by NOMIT were seen as relevant or rather
relevant, against 70% for the titles stemming from
the POSTIT approach, and 37% for the titles stem-
ming from CATIT. Besides, NOMIT determines ti-
tles appreciably more catchy than both POSTIT
and CATIT. Concerning the real titles (RT), 87.8%
were judged relevant and 80.5% were catchy, mean-
ing that humans still perform better than automated
techniques, but only slightly for the relevance crite-
rion, and anyway, are not judged as perfect (refer-
ence is far from absolute!).
en % Relevant Weak relevant Irrelevant Catchy Not catchy
POSTIT 39.1 30.9 30 49.1 50.9
CATIT 15.7 21.3 63 47.2 52.8
NOMIT 60.3 22.4 17.2 53.4 46.6
RT 71.4 16.4 12.3 80.5 19.5
Table 1: Evaluation Results for POSTIT, CATIT,
NOMIT, and RT (Real Titles).
4.2 Agreement Inter-judges
4.2.1 Protocol Description
This evaluation is similar to the previous one
(same Web interface). The main difference is that
we retained the first 100 articles appeared in Le
Monde 1994 which enables our approach to return
a title. Three judges estimated the real title as well
as the NOMIT title for each of the texts, that is, a
total of 600 evaluations.
4.2.2 Results
Kappa coefficient (noted K) is a measure defined
by (Cohen, 1960) calculating the agreement between
several annotators. It is based on the rate of ob-
served concordances (Po) and on the rate of ran-
dom concordances (Pe). Here the Kappa coeffi-
cient estimates the agreement inter-judges about the
relevance and of catchiness of NOMIT titles (cf. Ta-
bles 2 - 4). Considering the results and according to
(Landis and Koch, 1977), judges seem to obtain an
average concordance for the relevance of NOMIT ti-
tles. This can be justified by the fact that there is a
consensus between the three judges about the defini-
tion of what is a relevant title (cf. Table 3). Approxi-
mately 71% of the titles were considered relevant by
three judges (cf. Table 2).
On the other hand, the three judges obtain a bad
concordance regarding catchiness; a catchy title for
the one, could not be catchy for the other one. This
is perfectly coherent with the definitions given by
the three judges:
1. A title is catchy if the association of the words
is syntactically correct but semantically "sur-
prising".
2. A catchy title is a title which drives you to read
the article.
3. A catchy title is a title which holds attention of
the reader and tempts him/her to read the con-
cerned text .
So, people have judged catchiness according to
syntax, the relation between semantics of the title
and semantic of the text, or have evaluated catchi-
ness according to personal interests. The notion of
catchiness is based on these three criteria. So, we
could not expect a strong agreement between the as-
sessors concerning the catchy character of a title (cf.
Table 3).
in % Relevant Irrelevant Neutral Total
Relevant 70.7 10.3 0.7 81.7
Irrelevant 6.0 10.3 0.7 17.0
Neutral 1.0 0.3 0.0 1.3
Total 77.7 21.0 0.7 100.0
Table 2: Contingency Matrix for NOMIT (relevance).
in % Catchy Not Catchy Neutral Total
Catchy 13.3 7.7 0.0 21.0
Not catchy 34.7 41.0 1.3 77.0
Neutral 0.7 1.3 0.0 2.0
Total 48.7 50.0 1.3 100.0
Table 3: Contingency Matrix for NOMIT (catchiness).
As a rough guide, short journalistic articles14 ob-
tain better results than long articles (93% are rele-
vant in that case and 69% are catchy). It thus seems
14We consider that an article is short when its number of
words is less than 100.
281
K avg. Po avg. Pe avg.
Relevance 0.42 0.81 0.67
Catchiness 0.10 0.54 0.49
Average 0.28 0.68 0.58
Table 4: Kappa average for relevance and catchiness of
titles obtained with NOMIT.
that our approach of automatic titling by nominaliza-
tion is more adapted to short texts. We are extremely
prudent concerning this interpretation because it is
based on only 29 articles.
5 Conclusion
Automatic titling is a complex task because titles
must be at once informative, catchy, and syntacti-
cally correct. Based on linguistic and semantic treat-
ments, our approach determines titles among which
approximately 80% were evaluated as relevant and
more than 60% were qualified as catchy. Experiment
and results discussion have pointed at the following
liability: The value of Kappa, the inter-judges agree-
ment coefficient, is very difficult to evaluate, mostly
when catchiness is at stake. The main cause is that it
depends on personal interests. It is thus necessary to
ask the following question: Do we have to consider
that a title is definitely catchy when at least one per-
son judges it so? Otherwise, how many people at
least? This is still an open question and needs to be
further investigated.
Also, some interesting extensions could be en-
visaged: The approach presented in this paper uses
three rules of transformation based on the presence
of an auxiliary followed by a past participle. The ad-
dition of new rules would enable a syntactic enrich-
ment of the titles. So, it might be profitable to set up
rules taking into account the presence of syntactical
patterns (others than "auxiliary + past participle") to
allow more texts to be titled by NOMIT.
Taking the punctuation of the end of sentences
into account might also be a promising track. For
example, "did it use an electric detonator?" would
become "Use of an electric detonator?". It is an in-
teresting point because the presence of a punctuation
at the end of a title (in particular the exclamation or
the interrogation) constitutes a catchy criterion.
Last, NOMIT is a method (easily reproducible in
other languages, English in particular) that stepped
out of preceding attempts in automatic headings
generation (POSTIT, CATIT). Exploring syntac-
tic patterns, as it does, means that increasing the
amount of linguistic information in the process
might lead to a reliable heading method. One of
the perspectives can be to track the optimum point
between the richness of involved information and
processes, and the cost of the method. The in-
cremental methodology followed from POSTIT to
NOMIT tends to enhance the belief that parameters
(i.e. length, shape, relevance, etc...) for an auto-
matic heading procedure have to be studied and well
defined, thus leading to a customized titling process.
References
Massih R. Amini, Nicolas Usunier, and Patrick Gallinari.
2005. Automatic text summarization based on word-
clusters and ranking algorithms. Advances in Informa-
tion Retrieval, pages 142?156.
Michele Banko, Vibhu O. Mittal, and Michael J. Wit-
brock. 2000. Headline generation based on statis-
tical translation. In Proceedings of the 38th Annual
Meeting on Association for Computational Linguis-
tics, pages 318?325. Association for Computational
Linguistics.
Phyllis B. Baxendale. 1958. Man-made index for tech-
nical literature - an experiment. IBM Journal of Re-
search and Development., pages 354?361.
Antoine Blais, Iana Atanassova, Jean-Pierre Descl?s,
Mimi Zhang, and Leila Zighem. 2007. Discourse au-
tomatic annotation of texts: an application to summa-
rization. In Proceedings of the Twentieth International
Florida Artificial Intelligence Research Society Con-
ference, May, pages 7?9.
Jacques Chauch? and Violaine Prince, Vp. 2007. Clas-
sifying texts through natural language parsing and
semantic filtering. In 3rd International Language
and Technology Conference, pages 012?020, Poznan,
Pologne, October.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and psychological measure-
ment, 20(1):37?46.
Jean Dubois and Fran?oise Dubois-Charlier. 1970. El?-
ments de linguistique fran?aise: syntaxe. Larousse.
Juan Herrero Cecilia. 2007. Syntaxe, s?mantique
et pragmatique des titres des nouvelles de la presse
fran?aise construits en forme de phrase nominale ou
averbale: aspects cognitifs et communicatifs. In Lit-
t?rature, langages et arts: rencontres et cr?ation,
page 97. Servicio de Publicaciones.
282
Marie-Paule Jacques and Josette Rebeyrolle. 2004.
Titres et structuration des documents. In Actes In-
ternational Symposium: Discourse and Document.,
pages 125?152.
Franck Keller and Mirella Lapata. 2003. Using the web
to obtain frequencies for unseen bigrams. Computa-
tional linguistics, 29(3):459?484.
Kevin Knight and Daniel Marcu. 2002. Summariza-
tion beyond sentence extraction: A probabilistic ap-
proach to sentence compression. Artificial Intelli-
gence, 139(1):91?107.
Mathieu Lafourcade and Virginie Zampa. 2007. Making
people play for lexical acquisition. In SNLP 2007, 7th
Symposium on Natural Language Processing. Pattaya.
J. Richard Landis and Garry G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159.
C?dric Lopez, Violaine Prince, and Mathieu Roche.
2011a. Automatic generation of short titles. In 5th
Language and Technology Conference, LTC?11, pages
461?465.
C?dric Lopez, Violaine Prince, and Mathieu Roche.
2011b. Automatic titling of articles using position
and statistical information. In RANLP?11: Recent Ad-
vances in Natural Language Processing, pages 727?
732, Hissar, Bulgarie, September.
Sophie Moirand. 1975. Le r?le anaphorique de la nom-
inalisation dans la presse ?crite. Langue fran?aise,
28(1):60?78.
Marie-Th?r?se Vinet. 1993. L?aspect et la copule vide
dans la grammaire des titres. Persee, 100:83?101.
Mehdi Yousfi-Monod and Violaine Prince. 2008. Sen-
tence compression as a step in summarization or an
alternative path in text shortening. In Coling?08: In-
ternational Conference on Computational Linguistics,
Manchester, UK., pages 139?142.
283
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 33?37,
Beijing, August 2010
How to Expand Dictionaries with Web-Mining Techniques 
Nicolas B?chet LIRMM, UMR 5506, CNRS, Univ. Montpellier 2 
bechet@lirmm.fr 
Mathieu Roche LIRMM, UMR 5506, CNRS, Univ. Montpellier 2 
mroche@lirmm.fr 
 
Abstract 
This paper presents an approach to en-rich conceptual classes based on the Web. To test our approach, we first build conceptual classes using syntactic and semantic information provided by a cor-pus. The concepts can be the input of a dictionary. Our web-mining approach deals with a cognitive process which simulates human reasoning based on the enumeration principle. The experiments reveal the interest of our approach by adding new relevant terms to existing conceptual classes. 
1 Introduction 
Concepts have several definitions; one of the most general describes a concept ?as the mind?s representation of a thing or an item? (Desrosiers-Sabbath, 1984). In a domain such as ours, i.e. ontology building, semantic webs, and computa-tional linguistics, it seems appropriate to stick to the Aristotelian approach to a concept, and con-sider it as a set of knowledge (gathered informa-tion) on common semantic features. The choice of the features and how the knowledge is gath-ered depend on criteria we will explain below. In this paper, we deal with the building of conceptual classes, which can be defined as gathering semantically close terms. First, we suggest building specific conceptual classes by focusing on knowledge extracted from corpora. Conceptual classes are shaped by the study of syntactic dependencies between corpus terms (as described in section 2). Dependencies tackle re-lations such as Verb/Subject, Noun/Noun Phrase Complements, Verb/Object, Verb/Complements, 
and sometimes Sentence Head/Complements. In this paper, we focus on the Verb/Object depend-ency because it is representative of a field. For instance, in computer science, the verb ?to load? takes as objects, nouns of the conceptual class software (L?Homme, 1998). This feature also extends to ?download? or ?upload?, which have the same verbal root. Corpora are rich sources of terminological in-formation that can be mined. A terminology ex-traction of this kind is similar to a Harris-like distributional analysis (Harris, 1968) and many works in the literature have been the subject of distributional analysis to acquire terminological or ontological knowledge from textual data (e.g (Bourigault and Lame, 2002) for law, (Naza-renko et al, 2001; Weeds et al, 2005) for medi-cine). After building conceptual classes (section 2), we describe an approach to expand concepts by using a Web search engine to discover new terms (section 3). In section 4, experiments con-ducted on real data enable us to validate our ap-proach.  
2 Building Conceptual Classes  
2.1 Principle 
In our approach, a class can be defined as a gathering of terms with a common field. In this paper, we focus on objects of verbs judged to be semantically close by using a measure. These objects are thus considered as instances of con-ceptual classes. The first step in building concep-tual classes consists in extracting Verb/Object syntactic relations as explained in the following section. 
33
2.2 Mining for Verb/Object relations 
Our corpora are in French since our team is mostly devoted to French-based NLP applica-tions. However, the following method can be used for any other language, provided a reliable dependency parser is available. In our case, we use the SYGFRAN parser developed by (Chauch?, 1984). As an example, in the French sentence ?Thierry Dusautoir brandissant le dra-peau tricolore sur la pelouse de Cardiff apr?s la victoire.? (translation: ?Thierry Dusautoir bran-dishing the three colored flag on Cardiff lawn after the victory?), there is a verb-object syntac-tic relation: ?verb: brandir (to brandish), object: drapeau (flag)?, which is a good candidate for retrieval. The second step of the building process corresponds to the gathering of common objects related to semantically close verbs.  
  Figure 1: Common and complementary objects of the verbs ?to consume? and ?to eat?  Assumption of Semantic Closeness. The un-derlying linguistic hypothesis is the following: Verbs with a significant number of common ob-jects are semantically close. To measure closeness, the ASIUM score (Faure and Nedellec, 1999; Faure, 2000) is used (see figure 1). This type of work is similar to distri-butional analysis approaches such as that of (Bourigault and Lame, 2002). As explained in the introduction, the measure considers two verbs to be close if they have a significant number of common features (ob-jects). Let p and q be verbs with their respective p1,...,pn and q1,...,qm objects. NbOCp(qi) is the number of occurrences of qi objects from q that are also objects of p (common objects). NbO(qi) is the number of occurrences of qi objects of q verb. The Asium measure is then:  
  Where logAsium(x) is equal to:  
? for x = 0, logAsium(x) = 0  
? else logAsium(x) = log(x) + 1  Therefore, conceptual classes instances are the common objects of close verbs, according to the ASIUM proximity measure. The following section describes the acquisi-tion of new terms starting with a list of terms/concepts obtained with the global process summarized in this section and detailed in (B?-chet et al, 2008). 
3 Expanding conceptual classes  
3.1 Acquisition of candidate terms 
The aim of this approach is to provide new can-didates for a given concept. It is based on enu-meration on the Web of terms that are semanti-cally close. For instance, with a query (string) ?bicycle, car, and?, we can find other vehicles. We propose to use the Web to acquire new can-didates. This kind of method uses information regarding the ?popularity? of the web and is in-dependent of a particular corpus. Our method of acquisition is quite similar to that of (Nakov and Hearst, 2008). These authors propose to query the Web using the Google search engine to characterize the semantic rela-tion between a pair of nouns. The Google star operator among others, is used to that end. (Na-kov and Hearst, 2008) refer to the study of (Lin and Pantel, 2001) who used a Web mining ap-proach to discover inference rules missed by humans.  To apply our method, we first consider the common objects of semantically close verbs, which are instances of reference concepts (e.g. vehicle). Let N concepts Ci?{1, N} and their respec-tive instances Ij(Ci). For each concept Ci, we submit to a search engine the following queries: ?IjA(Ci), IjB(Ci), and? and ?IjA(Ci), IjB(Ci), or? with jA and jB ? {1, ..., NbInstanceCi} and jA ? jB.  
34
The search engine returns a set of results from which we extract new candidate instances of a concept. For example, if we consider the query: ?bicycle, car, and?, one page returned by a search engine gives the following text:  Listen here for the Great Commuter Race (17/11/05) between bicycle, car and bus, as part of...  Having identified the relevant features in the result returned (in bold in our example), we add the term ?bus? to the initial concept ?vehicle?. In this way, we obtain new candidates for our con-cepts. The process can be repeated. In order to automatically determine which candidates are relevant, the candidates are filtered as shown in the following section. 
3.2 Filtering of candidates 
The quality of the extracted terms can be vali-dated by an expert, or automatically by using the Web to check if the extracted candidates (see section 3.1) are relevant. The principle is to con-sider a relevant term if it is often present with the terms of the original conceptual class (kernel of words). Thus, our aim is to validate a term ?in the context?. From that point of view, our method is close to that of (Turney, 2001), which queries the Web via the AltaVista search engine to determine appropriate synonyms for a given term. Like (Turney, 2001), we consider that in-formation concerning the number of pages re-turned by the queries can give an indication of the relevance of a term. Thus, we submit to a search engine different strings (using citation marks). A query consists of the new candidate and both terms of the con-cept. Formally, our approach can be defined as follows. Let N concepts Ci ? {1, N}, their respec-tive instances Ij(Ci) and the new candidates for a concept Ci, Nik ?  {1, NbNI(Ci)}. For each Ci, each new candidate Nik is sent as a query to a Web search engine. In practice the three terms are separated either by a comma or the word ?or? or  ?and?1. For each query, the search engine returns a num-ber of results (i.e. number of web pages). Then, the sum of these results is calculated using all possible combinations of ?or?, ?and?, or of the three words (words of the kernel plus candidate 
                                                           1 Note that the commas are automatically removed by the search engines. 
word to enrich it). Below is an example with the kernel words ?car?, ?bicycle? and the candidate ?bus? to test (using Yahoo):  
? ?car, bicycle, and bus?: 71 pages re-turned 
?  ?car, bicycle, or bus?: 268 pages re-turned 
? ?bicycle, bus, and car?: 208 pages re-turned 
? and so forth  Global result: 71 + 268 + 208...  The filtering of candidates consists in select-ing the k first candidates by class (i.e. with the highest sum), they are added as new instances of the initial concept. We can reiterate the acquisi-tion approach by including these new terms. The acquisition/filtering process can be repeated sev-eral times. In the next section, we present experiments conducted to evaluate the quality of our ap-proach. 
4 Experiments 
4.1 Evaluation protocol 
We used a French corpus from the Yahoo site (http://fr.news.yahoo.com/) composed of 8,948 news items (16.5 MB) from newspapers. Ex-periments were performed on 60,000 syntactic relations (B?chet et al, 2008; B?chet et al, 2009) to build original conceptual classes. We manually selected five concepts (see Figure 2). Instances of these concepts are the common ob-jects of verbs defining the concept (see section 2.2).  
  Figure 2: The five selected concepts and their instances. 
35
For our experiments, we use an API of the search engine Yahoo! to obtain new terms. We apply the following post-treatments for each new candidate term. They are initially lemmatized. Therefore, we only keep the nouns, after apply-ing a PoS (Part of Speech) tagger, the TreeTag-ger (Schmid, 1995). After these post-treatments, we manually validate the new terms using three experts. We compute the precision of our approach to each expert. The average is calculated to define the quality of the terms. Precision is defined as fol-lows.  Precision = Number of relevant terms given by our system Number of terms given by our system  In the next section, we present the evaluation of our method. 
4.2 Experimental results 
Table 1 gives the results of the term acquisition method (i.e. for each acquisition step, we apply our approach to filter candidate terms). For each step, the table lists the degree of precision ob-tained after expertise:  
? All candidates. We calculate the preci-sion before the filtering step.  
? Filtered candidates. After applying the automatic filtering by selecting k terms per class, we calculate the precision ob-tained. Note that the automatic filtering (see section 3.2) reduces the number of terms proposed, and thus reduces the re-call2.   
 Table 1: Results obtained with k=4 (i.e. auto-matic selection of the k first ranked terms by the filtering approach). 
                                                           2   The recall is not calculated because in an unsuper-vised context it is difficult to estimate. 
Finally Table 1 shows the number of terms generated by the acquisition system. These results show that a significant number of terms can be generated (i.e. 103 words). For example, for the concept ?feeling?, using the ini-tial terms given in figure 1, we obtained the fol-lowing eight French terms (in two steps): ?hor-reur (horror), satisfaction (satisfaction), d?prime (depression), faiblesse (weakness), tristesse (sadness), d?senchantement (disenchantment), folie (madness), fatalisme (fatalism)?. This approach is appropriate to produce new relevant terms to enrich conceptual classes, in particular when we select the first terms (k = 4) returned by the filtering system. In a future work, we plan to test other values of the auto-matic filtering. The precision obtained in the first two steps was high (i.e. 0.69 to 0.83). The third step returned lower scores; noise was introduced because we were too ?far? from the initial kernel words. 
5 Conclusion and Future Work 
This paper describes an approach for conceptual enrichment classes based on the Web. We apply the ?enumeration? principle to find new terms using Web search engines. This approach has the advantage of being less dependent on the corpus. Note that as the use of the Web requires valida-tion of candidates, we propose an automatic fil-tering method to select relevant terms to add to the concept. In a future work, we plan to use other statistical web measures (e.g. Mutual In-formation, Dice measure, and so forth) to auto-matically validate terms. 
References 
B?chet, N., M. Roche, and J. Chauch?e. 2008. How the ExpLSA approach impacts the document clas-sification tasks. In Proceedings of the Interna-tional Conference on Digital Information Man-agement, ICDIM?08, pages 241?246, University of East London, London, United Kingdom. 
B?chet, N., M. Roche, and J. Chauch?. 2009. To-wards the selection of induced syntactic relations. In European Conference on Information Retrieval (ECIR), Poster, pages 786?790. 
Bourigault, D. and G. Lame. 2002. Analyse distribu-tionnelle et structuration de terminologie. Applica-tion ? la construction d?une ontologie documen-taire du droit. In TAL, pages 43?51. 
1 0.69 0.83 292 0.69 0.77 473 0.56 0.65 103
Precision Terms numberSteps # All terms Filtered terms (without filter)
36
Chauch?, J. 1984. Un outil multidimensionnel de l?analyse du discours. In Proceedings of COLING, Standford University, California, pages 11?15. 
Desrosiers-Sabbath, R. 1984. Comment enseigner les concepts. Presses de l?Universit? du Qu?bec. 
Faure, D. and C. Nedellec. 1999. Knowledge acquisi-tion of predicate argument structures from techni-cal texts using machine learning: The system ASIUM. In Proceedings of the 11th European Workshop, Knowledge Acquisition, Modelling and Management, number 1937 in LNAI, pages 329?334. 
Faure, D. 2000. Conception de m?thode d?appren-tissage symbolique et automatique pour l?acquisi-tion de cadres de sous-cat?gorisation de verbes et de connaissances s?mantiques ? partir de textes : le syst?me ASIUM. Ph.D. thesis, Universit? Paris-Sud, 20 D?cembre. 
Harris, Z. 1968. Mathematical Structures of Lan-guage. John Wiley & Sons, New-York. 
L?Homme, M. C. 1998. Le statut du verbe en langue de sp?cialit? et sa description lexicographique. In Cahiers de Lexicologie 73, pages 61?84. 
Lin, Dekang and Patrick Pantel. 2001. Discovery of inference rules for question answering. Natural Language Engineering, 7:343?360. 
Nakov, Preslav and Marti A. Hearst. 2008. Solving relational similarity problems using the web as a corpus. In ACL, pages 452?460. 
Nazarenko, A., P. Zweigenbaum, B. Habert, and J. Bouaud. 2001. Corpus-based extension of a termi-nological semantic lexicon. In Recent Advances in Computational Terminology, pages 327?351. 
Schmid, H. 1995. Improvements in part-of-speech tagging with an application to german. In Proceed ngs of the ACL SIGDAT-Workshop, Dublin. 
Turney, P.D. 2001. Mining the Web for synonyms: PMI?IR versus LSA on TOEFL. In Proceedings of ECML?01, Lecture Notes in Computer Science, pages 491?502. 
Weeds, J., J. Dowdall, G. Schneider, B. Keller, and D. Weir. 2005. Weir using distributional similarity to organise biomedical terminology. In Proceed-ings of Terminology, volume 11, pages 107?141. 
37
