Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 523?532,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
A Japanese Predicate Argument Structure Analysis using Decision Lists
Hirotoshi Taira, Sanae Fujita, Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho,
Keihanna Science City,
Kyoto 619-0237, Japan
{{taira,sanae}@cslab.kecl, nagata.masaaki@lab}.ntt.co.jp
Abstract
This paper describes a new automatic method
for Japanese predicate argument structure
analysis. The method learns relevant features
to assign case roles to the argument of the tar-
get predicate using the features of the words
located closest to the target predicate under
various constraints such as dependency types,
words, semantic categories, parts of speech,
functional words and predicate voices. We
constructed decision lists in which these fea-
tures were sorted by their learned weights. Us-
ing our method, we integrated the tasks of se-
mantic role labeling and zero-pronoun iden-
tification, and achieved a 17% improvement
compared with a baseline method in a sen-
tence level performance analysis.
1 Introduction
Recently, predicate argument structure analysis has
attracted the attention of researchers because this
information can increase the precision of text pro-
cessing tasks, such as machine translation, informa-
tion extraction (Hirschman et al, 1999), question
answering (Narayanan and Harabagiu, 2004) (Shen
and Lapata, 2007), and summarization (Melli et
al., 2005). In English predicate argument structure
analysis, large corpora such as FrameNet (Fillmore
et al, 2001), PropBank (Palmer et al, 2005) and
NomBank (Meyers et al, 2004) have been created
and utilized. Recently, the GDA Corpus (Hashida,
2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al,
2002) and NAIST Text Corpus (Iida et al, 2007)
were constructed in Japanese, and these corpora
have become the target of an automatic Japanese
predicate argument structure analysis system. We
conducted Japanese predicate argument structure
(PAS) analysis for the NAIST Text Corpus, which
is the largest of these three corpora, and, as far as
we know, this is the first time PAS analysis has been
conducted for whole articles of the corpus.
The NAIST Text Corpus has the following char-
acteristics, i) semantic roles for both predicates and
event nouns are annotated in the corpus, ii) three ma-
jor case roles,1 namely the ga, wo and ni-cases in
Japanese are annotated for the base form of pred-
icates and event nouns, iii) both the case roles in
sentences containing the target predicates and those
outside the sentences (zero-pronouns) are annotated,
and iv) coreference relations are also annotated.
As regards i), recently there has been an increase
in the number of papers dealing with nominalized
predicates (Pradhan et al, 2004) (Jiang and Ng,
2006) (Xue, 2006) (Liu and Ng, 2007). For exam-
ple, ?trip? in the sentence ?During my trip to Italy, I
met him.? refers not only to the event ?I met him?
but also to the event ?I traveled to Italy.? As in this
example, nouns sometimes have argument structures
referring to an event. Such nouns are called event
nouns (Komachi et al, 2007) in the NAIST Text
Corpus. At the same time, the problems related to
compound nouns are also important. In Japanese, a
compound noun sometimes simultaneously contains
both an event noun and its arguments. For example,
the compound noun, ????? (corporate buyout)?
contains an event noun ??? (buyout)? and its ac-
cusative, ??? (corporate).? However, compound
1Kyoto Text Corpus has about 15 case roles.
523
nouns provide no information about syntactic de-
pendency or about case markers, so it is difficult to
specify the predicate-argument structure. Komachi
et al investigated the argument structure of event
nouns using the co-occurrence of target nouns and
their case roles in the same sentence (Komachi et
al., 2007). In these approaches, predicates and event
nouns are dealt with separately. Here, we try to
unify these different argument structures using de-
cision lists.
As regards ii), for example, in the causative sen-
tence, ???????????????? (Mary
makes Tom fix dinner),? the basic form of the
causative verb, ????? (make fix)? is ??? (fix),?
and its nominative is ??? (Tom)? and the ac-
cusative case role (wo-case) is ??? (dinner),? al-
though the surface case particle is ni (dative). We
must deal with syntactic transformations in passive,
causative, and benefactive constructions when ana-
lyzing the corpus.
As regards iii) and iv), in Japanese, zero pronouns
often occur, especially when the argument has al-
ready been mentioned in previous sentences. There
have been many studies of zero-pronoun identifica-
tion (Walker et al, 1994) (Nakaiwa, 1997) (Iida et
al., 2006).
In this paper, we present a general procedure for
handling both the case role assignment of predicates
and event nouns, and zero-pronoun identification.
We use the decision list learning of rules to find the
closest words with various constraints, because with
decision lists the readability of learned lists is high
and the learning is fast.
The rest of this paper is organized as follows. We
describe the NAIST Text Corpus, which is our tar-
get corpus in Section 2. We describe our proposed
method in Section 3. The result of experiments us-
ing the NAIST Text Corpus and our method are re-
ported in Section 4 and our conclusions are provided
in Section 5.
2 NAIST Text Corpus
In the NAIST Text Corpus, three major obligatory
Japanese case roles are annotated, namely the ga-
case (nominative or subjective case), the wo-case
(accusative or direct object) and the ni-case (da-
tive or in-direct object). The NAIST Text Corpus
is based on the Kyoto Text Corpus Ver. 3.0, which
contains 38,384 sentences in 2,929 texts taken from
news articles and editorials in a Japanese newspaper,
the ?Mainichi Shinbun?.
We divided these case roles into four types by lo-
cation in the article as in (Iida et al, 2006), i) the
case role depends on the predicate or the predicate
depends on the case role in the intra-sentence (?de-
pendency relations?), ii) the case role does not de-
pend on the predicate and the predicate does not de-
pend on the case role in the intra-sentence (?zero-
anaphoric (intra-sentential)?), iii) the case role is
not in the sentence containing the predicate (?zero-
anaphoric (inter-sentential)?), and iv) the case role
and the predicate are in the same phrase (?in same
phrase?). Here, we do not deal with exophora.
We show the distribution of the above four types
in test samples in our split of the NAIST Text
Corpus in Tables 1 and 2. In predicates, the
?dependency relations? type in the wo-case and
the ni-case occur frequently. In event nouns,
the ?zero-anaphoric (intra-sentential)? and ?zero-
anaphoric (inter-sentential)? types in the ga-case oc-
cur frequently. With respect to the ?in same phrase?
type, the wo-case occurs frequently.
3 Predicate Argument Structure Analysis
using Features of Closest Words
In this section, we describe our algorithm. In the
algorithm, we used various constraints when search-
ing for the words located closest to the target predi-
cate. We described these constraints as features with
the direct products of dependency types (ic, oc, ga c,
wo c, ni c, sc, nc, fw and bw), generalization levels
(words, semantic categories, parts of speech), func-
tional words and voices.
3.1 Dependency Types
In Japanese, the functional words in a phrase (Bun-
setsu in Japanese) and the interdependency of bun-
setsu phrases are important for determining the
predicate argument structure. In accordance with
the character of the dependency between the case
roles and the predicates or event nouns, we divided
Japanese word dependency into the following seven
types that cover all dependency types in Japanese.
Additionally, we use two optional dependency types.
524
Table 1: Distribution of case roles for predicates (Test Data)
predicate
ga (Nominative) wo (Accusative) ni (Dative)
all 15,996 (100.00%) 8,348 (100.00%) 4,871 (100.00%)
dependency relations 9,591 ( 59.96%) 7,184 ( 86.06%) 4,276 ( 87.78%)
zero-anaphoric (intra-sentential) 3,856 ( 24.11%) 870 ( 10.42%) 360 ( 7.39%)
zero-anaphoric (inter-sentential) 2,496 ( 15.60%) 225 ( 2.70%) 132 ( 2.71%)
in same phrase 53 ( 0.33%) 69 ( 0.83%) 103 ( 2.11%)
Table 2: Distribution of case roles for event nouns (Test Data)
event noun
ga (Nominative) wo (Accusative) ni (Dative)
all 4,099 (100.00%) 2,314 (100.00%) 423 (100.00%)
dependency relations 977 (23.84%) 648 (28.00%) 105 (24.82%)
zero-anaphoric (intra-sentential) 1,672 (40.79%) 348 (15.04%) 135 (31.91%)
zero-anaphoric (inter-sentential) 1,040 (25.37%) 165 (7.13%) 44 (10.40%)
in same phrase 410 (10.00%) 1,153 (49.83%) 139 (32.86%)
Figure 1: Type ic
3.1.1 Incoming Connection Type (ic)
With this type, the target case role is the head-
word of a bunsetsu phrase and the case role phrase
depends on the target predicate phrase (Figure 1).
3.1.2 Outgoing Connection Type (oc)
With this type, the target case role is the headword
of a phrase and a phrase containing a target predicate
or event noun depends on the case role phrase (Fig-
ure 2).
Figure 2: Type oc
525
Figure 3: Type sc
Figure 4: Type ga c, wo c, ni c
3.1.3 ?Within the Same Phrase? Type (sc)
With this type, the target case role and the target
predicate or event noun are in the same phrase (Fig-
ure 3).
3.1.4 ?Connection into Other Case role Types
(ga c, wo c, ni c)
With these types, a phrase containing the target
case role depends on a phrase containing another
predetermined case role (Figure 4). We use the terms
?ga c?, ?wo c? and ?ni c? when the predetermined
case roles are the ga-case, wo-case and ni-case, re-
spectively.
Figure 5: Type nc
3.1.5 Non-connection Type (nc)
With this type, a phrase containing the target case
role and a phrase containing the target predicate or
event noun are in the same article, but these phrases
do not depend on each other (Figure 5).
3.1.6 Optional Type (fw and bw)
Type fw and bw stand for ?forward? and ?back-
ward? types, respectively. Type fw means the word
located closest to the target predicate or event noun
without considering functional words or voices.
With fw, the word is located between the top of the
article containing the target predicate and the target
predicate or event noun. Similarly, type bw means
the word located closest to the target predicate or
noun, which is located between the targeted predi-
cate or event noun, and the tail of the article con-
taining the predicate.
3.2 Generalization Levels
We used three levels of generalization for every case
role candidate, that is, word, semantic category, and
part of speech. Every word is annotated with a part
of speech in the Kyoto Text Corpus, and we used
these annotations. With regard to semantic cate-
gories, we annotated every word with a semantic
category based on a Japanese thesaurus, Nihongo
Goi Taikei. The thesaurus consists of a hierarchy
of 2,710 semantic classes, defined for over 264,312
nouns, with a maximum depth of twelve (Ikehara et
al., 1997). We mainly used the semantic classes of
526
Figure 6: Top 3 levels of the Japanese thesaurus, ?Ni-
hongo Goi Taikei?
the third level, and partly the fourth level, which are
similar to semantic roles. We show the top three lev-
els of the Nihongo Goi Taikei common noun the-
saurus in Figure 6. We annotated the words with
their semantic category by hand.
3.3 Functional Word and Voice
We used a functional word in the phrase containing
the target case role and active and passive voices for
the predicate as base features.
3.4 Training Algorithm
The training algorithm used for our method is shown
in Figure 7. First, the algorithm constructs features
that search for the words located closest to the tar-
get predicate under various constraints. Next, the
algorithm learns by using linear Support Vector Ma-
chines (SVMs) (Vapnik, 1995). SVMs learn effec-
tive features by the one vs. rest method for every
case role. We used TinySVM 2 as an SVM imple-
mentation. Moreover, we construct decision lists
sorted by weight from linear SVMs. Finally, the al-
gorithm calculates the existing probabilities of case
roles for every predicate or event noun. This step
2http://chasen.org/t?aku/software/TinySVM/
produces the criterion that decides whether or not
we will determine the case roles when there is no in-
terdependency between the case role candidate and
the predicate.
Our split of the NAIST Text Corpus has only
62,264 training samples for 2,874 predicates, and we
predict that there will be a shortage of training sam-
ples when adopting traditional learning algorithms,
such as learning algorithms using entropy. So, we
used SVMs with a high generalization capability to
learn the decision lists.
3.5 Test Algorithm
The test algorithm of our method is shown in Fig-
ure 8. In the test phase, we analyzed test samples
using decision lists and the existing probabilities of
case roles learned in the training phase. In step 1, we
determined case roles using a decision list consisting
of features exhibiting case role and predicate inter-
dependency, that is, ic, oc, ga c, wo c, and ni c. This
is because there are many cases in Japanese where
the syntactic constraint is stronger than the seman-
tic constraint when we determine the case roles. In
step 2, we determined case roles using a decision list
of sc (?in same phrase?) for the case roles that were
not determined in step 1. This step was mainly for
event nouns. Japanese event nouns frequently form
compound nouns that contain case roles. In step 3,
we decided whether or not to proceed to the next
step by using the existing probabilities of case roles.
If the probability was less than a certain threshold
(50%), then the algorithm stopped. In step 4, we de-
termined case roles using a decision list of the fea-
tures that have no interdependency, that is, nc, fw
and bw. This step will be executed when the target
case role is syntactically necessary and determined
by the co-occurrence of the case roles and predicate
or event noun without syntactic clues, such as de-
pendency, functional words and voices.
4 Experimental Results
4.1 Experimental Setting
We performed our experiments using the NAIST
Text Corpus 1.4? (Iida et al, 2007). We used
49,527 predicates and 12,737 event nouns from arti-
cles published from January 1st to January 11th and
the editorials from January to August as training ex-
527
for each predicate pi in all predicates appeared in the training corpus do
feature list(pi) = {} ; n ? 0
clear (x, y)
for each instance pij of pi, in the training corpus do
Clear order() for all features
aij ? the article including pij
Wij ? the number of words in aij
pred index ? the word index of pij in aij
for (m = pred index? 1; m ? 1; m??) do
n + +
dep type = get dependency type(wm, pij)
if dep type == ?ic?, ?nc?, ?ga c?, ?wo c? or ?ni c? then inc order(n, dep type, wm, pij)
else if dep type == ?sc? then inc order(n, dep type, ??, ??)
endif
inc order(n, ?fw?, ??, ??)
if wm is the ga-case role then yn,ga ? 1 else yn,ga ? 0
if wm is the wo-case role then yn,wo ? 1 else yn,wo ? 0
if wm is the ni-case role then yn,ni ? 1 else yn,ni ? 0
end for
for (m = pred index + 1; m ? Wij ; m + +) do
n + +
dep type = get dependency type(wm, pij)
if dep type == ?oc?, ?nc?, ?ga c?, ?wo c? or ?ni c? then inc order(n, dep type, wm, pij)
else if dep type == ?sc? then inc order(n, dep type, ??, ??)
endif
inc order(n, ?bw?, ??, ??)
if wm is the ga-case role then yn,ga ? 1 else yn,ga ? 0
if wm is the wo-case role then yn,wo ? 1 else yn,wo ? 0
if wm is the ni-case role then yn,ni ? 1 else yn,ni ? 0
end for
end for
Learn linear SVMs using (x1, y1,ga), ..., (xn, yn,ga)
Learn linear SVMs using (x1, y1,wo), ..., (xn, yn,wo)
Learn linear SVMs using (x1, y1,ni), ..., (xn, yn,ni)
Make the decision list for pi, sorting features by weight.
Calculate the existing probabilities of case roles for pi.
end for
procedure get dependency type(wm, pij)
if phrase(wm) depends on phrase(pij) then return ?ic?
else if phrase(pij) depends on phrase(wm) then return ?oc?
else if phrase(wm) depends on phrase(pga) then return ?ga c?
else if phrase(wm) depends on phrase(pwo) then return ?wo c?
else if phrase(wm) depends on phrase(pni) then return ?ni c?
else if phrase(wm) equals phrase(pij) then return ?sc?
else return ?nc?
end procedure
procedure inc order(n, dep type, func, voice)
Set a feature fw = (wm, dep type, func, voice) ; order(fw)++ ; if order(fw) == 1 then xn,fw ? 1
Set a feature fs = (sem(wm), dep type, func, voice) ; order(fs)++ ; if order(fs) == 1 then xn,fs ? 1
Set a feature fp = (pos(wm), dep type, func, voice) ; order(fp)++ ; if order(fp) == 1 then xn,fp ? 1
feature list(pi) ? feature list(pi)
?
{fw, fs, fp}
end procedure
Figure 7: Training algorithm
528
Step 1. Determine case roles using a decision list concerning ic, oc, ga c, wo c and ni c.
Step 2. Determine case roles using a decision list concerning sc for undetermined case roles in
Step.1.
Step 3. If the existing probability of case roles < 50 % then the program ends.
Step 4. Determine case roles using a decision list concerning nc, fw and bw types.
Figure 8: Test algorithm
amples. We used 11,023 predicates and 3,161 event
nouns from articles published on January 12th and
13th and the September editorials as development
examples. And we used 19,501 predicate and 5,276
event nouns from articles dated January 14th to 17th
and editorials dated October to December as test ex-
amples. This is a typical way to split the data.
We used the annotations in the Kyoto Text Corpus
as the interdependency of bunsetsu phrases. We used
both individual and multiple words as case roles. We
used the phrase boundaries annotated in the NAIST
Text Corpus in the training phase, and used those
annotated automatically by our system using POSs
and simple rules in the test phase. The accuracy of
the automatic annotation is about 90%.
4.2 Baseline Method
To evaluate our algorithm, we conducted experi-
ments using a baseline method. With the method,
we used only nouns that depended on predicates or
event nouns as case role candidates. If the functional
word (post-positional case) in the phrase is ?ga?,?wo?
and ?ni?, we determined the ga-case, wo-case, or ni-
case for the candidates. Next, as regards event nouns
in compound nouns, if there was another word in a
compound noun containing an event noun and it co-
occurred with the event noun as a case role with a
higher probability in the training samples, then the
word was selected for the case role.
4.3 Entropy Method
The conventional approach for making decision lists
utilizes the entropy of samples selected by the
rules (Yarowsky, 1994) (Goodman, 2002). We per-
formed comparative experiments using Yarowsky?s
entropy algorithm (Yarowsky, 1994).
Table 3: Existing probabilities of case roles for predicates
and event nouns
Predicate Existing Probability
or Event Noun ga (NOM) wo (ACC) ni (DAT)
?? (use) 44.72% 82.92% 5.33%
?? (negotiation) 77.41% 30.70% 0.00%
?? (participation) 87.09% 0.00% 72.46%
??? (based on) 81.89% 0.00% 100.00%
4.4 Overall Results
The overall results are shown in Table 7. Here, ?en-
tropy? indicates Yarowsky?s algorithm, which uses
entropy (Yarowsky, 1994). Throughout the test data,
the F-measure (%) of our method exceeded that of
the baseline system and the ?entropy? system. With
the ga-case (nominative) in particular, the F-measure
increased 9 points.
Table 3 shows some examples of the existing
probabilities of case roles for predicates or event
nouns. When the probabilities are extreme values
such as the ni-case (dative) of?? (negotiation), the
wo-case (accusative) of?? (participation), and the
wo-case and ni-base of ??? (based on), we can
decide to fill the targeted case role or not with high
precision. However, it is difficult to decide to fill
the targeted case role or not when the probability is
close to 50 percent as in the ga-case of?? (use).
We show the learned decision list of the ic type
(the case role depends on the predicate or event
noun), sc type (in the same phrase) and the other
types for event noun?? (negotiation) in Tables 4, 5
and 6, respectively. Here, ?word? in the ?level?
column means ?base form of predicate? and ?sem?
means ?semantic category of predicate.? In the ic
and sc type decision lists, features with semantic
categories, such as ?REGION?, ?LOCATION? and
?EVENT?, occupy a higher order. In contrast, in
the list of the other types, the features that occupy
the higher order are the features of the word base
529
Table 4: Decision list for ic type of event noun?? (negotiation)
order case dep type level head word functional voice weight
word
1 ga ic word ???????? (North Korea) ? (of) active 0.9820
2 ga ic sem ?? (REGION) ? (of) active 0.6381
3 ga ic word ???? (both Japan and U.S.) ? (of) active 0.5502
4 wo ic word ?????? (establishment of joint ventures) ? (of) active 0.5288
5 wo ic word ?????? (telecommunications) ? (of) active 0.4142
6 wo ic word ???????? (North Korea) ?? (for) active 0.3168
7 wo ic word ?? (ACTION) ? (of) active 0.3083
8 ga ic sem ???? (OOV NOUN) ? (of) active 0.2939
9 wo ic word ????????? (car and auto parts sector) ? (of) active 0.2775
10 wo ic sem ? (LOCATION) ? (of) active 0.2471
Table 5: Decision list for sc type of event noun?? (negotiation)
order case dep type level head word weight
1 wo sc sem ?? (EVENT) 1.1738
2 wo sc word ?? (arrangement) 1.0000
3 ga sc word ???? (airline of Japan and China) 0.9392
4 wo sc sem ?? (MENTAL STATE) 0.8958
5 ga sc word ?????????? (financial services of Japan and U.S.) 0.8371
6 wo sc word ???? (contract extension) 0.7870
7 wo sc word ?? (joint venture) 0.7865
8 wo sc word ????? (intellectual property rights) 0.7224
9 wo sc word ??????? (car and auto parts) 0.7196
10 ga sc word ?? (Japan and North Korea) 0.6771
Table 6: Decision list for other types of event noun?? (negotiation)
order case dep type level head word functional word voice weight
1 ga fw word ?? (Japan and U.S.) 1.9954
2 ga fw word ?? (Taiwan) 1.9952
3 ga fw word ?? (U.S. and North Korea) 1.4979
4 ga fw word ?? (U.K. and China) 1.1773
5 ga nc word ?? (both nations) ? (TOP) active 1.1379
6 wo fw word ????? (diplomatic normalization) 1.0000
7 ga bw word ?? (U.S. and North Korea) 1.0000
8 ga fw word ?? (capital and labor) 1.0000
9 wo fw word ????? (automotive area) 1.0000
10 ga nc word ?? (both sides) ? (TOP) active 1.0000
Table 7: Overall results for NAIST Text Corpus (F-measure(%))
training data test data
sentence ga (NOM) wo (ACC) ni (DAT) sentence ga (NOM) wo (ACC) ni (DAT)
baseline 25.32 32.58 74.51 82.70 21.34 30.08 69.48 76.62
entropy 73.46 89.53 92.72 91.09 33.10 45.67 73.28 77.77
our method 64.81 86.76 92.52 92.20 38.06 55.07 75.82 80.45
530
Table 8: Results for predicates in test sets (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 34.44 / 57.40 77.00 / 79.50 79.83 / 83.15
dependency relations 51.96 / 75.53 85.42 / 88.20 81.83 / 89.51
zero-anaphoric (intra-sentential) 0.00 / 30.15 0.00 / 11.41 0.00 / 3.66
zero-anaphoric (inter-sentential) 1.85 / 23.45 3.00 / 9.32 0.00 / 11.76
in same phrase 0.00 / 75.00 0.00 / 51.78 0.00 / 84.65
Table 9: Results for event nouns (F-measure(%))
baseline / our method
ga (Nominative) wo (Accusative) ni (Dative)
all 11.05 / 45.64 32.30 / 61.80 20.85 / 38.88
dependency relations 12.98 / 68.01 25.00 / 62.46 40.00 / 56.05
zero-anaphoric (intra-sentential) 0.00 / 36.19 0.00 / 20.46 0.00 / 6.62
zero-anaphoric (inter-sentential) 1.40 / 23.25 1.06 / 10.37 0.00 / 3.51
in same phrase 58.76 / 78.93 47.44 / 77.96 28.91 / 58.13
form. This means local knowledge of relations be-
tween case roles and predicates or event nouns in
the word level is more important than semantic level
knowledge.
4.5 Results for Predicates in Test Sets
We show the results we obtained for predicates in
Table 8. The results reveal that our method is supe-
rior to the baseline system. Our algorithm is partic-
ularly effective in the ga-case.
4.6 Results for Event Nouns in Test Sets
We show the results we obtained for event nouns in
Table 9. This also shows that our method is superior
to the baseline system. The precision with sc type
is high and our method is effective as regards event
nouns.
5 Conclusion
We presented a new method for Japanese automatic
predicate argument structure analysis using deci-
sion lists based on the features of the words located
closest to the target predicate under various con-
straints. The method learns the relative weights of
these different features for case roles and ranks them
using decision lists. Using our method, we inte-
grated the knowledge of case role determination and
zero-pronoun identification, and generally achieved
a high precision in Japanese PAS analysis. In par-
ticular, we can extract knowledge at various levels
from the corpus for event nouns. In future, we will
use richer constraints and research better ways of
distinguishing whether or not cases are obligatory.
Acknowledgments
We thank Ryu Iida and Yuji Matsumoto of NAIST
for the definitions of the case roles in the NAIST
Text Corpus and functional words, and Franklin
Chang for valuable comments.
References
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proc. of the Pacific Asian
Conference on Language, Information and Computa-
tion (PACLING).
Joshua Goodman. 2002. An incremental decision
list learner. In Proc. of the ACL-02 Conference
on Empirical Methods in Natural Language Process-
ing(EMNLP02), pages 17?24.
Kouichi Hashida. 2005. Global document annotation
(GDA) manual. http://i-content.org/GDA/.
Lynette Hirschman, Patricia Robinson, Lisa Ferro, Nancy
Chinchor, Erica Brown, Ralph Grishman, and Beth
Sundheim. 1999. Hub-4 Event?99 general guidelines.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proc. of the 21st International Confer-
531
ence on Computational Linguistics and 44th Annual
Meeting of the ACL, pages 625?632.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Annotating a Japanese text corpus
with predicate-argument and coreference relations. In
Proc. of ACL 2007 Workshop on Linguistic Annota-
tion, pages 132?139.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Nihongo Goi
Taikei, A Japanese Lexicon. Iwanami Shoten, Tokyo.
Zheng Ping Jiang and Hwee Tou Ng. 2006. Semantic
role labeling of NomBank: A maximum entropy ap-
proach. In Proc. of the Conference on Empirical Meth-
ods in Natural Language Processing.
Daisuke Kawahara, Sadao Kurohashi, and Koichi
Hashida. 2002. Construction of a Japanese relevance-
tagged corpus (in Japanese). Proc. of the 8th Annual
Meeting of the Association for Natural Language Pro-
cessing, pages 495?498.
Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Mat-
sumoto. 2007. Learning-based argument structure
analysis of event-nouns in Japanese. In Proc. of the
Conference of the Pacific Association for Computa-
tional Linguistics (PACLING), pages 120?128.
Chang Liu and Hwee Tou Ng. 2007. Learning predictive
structures for semantic role labeling of NomBank. In
Proc. of the 45th Annual Meeting of the Association
for Computational Linguistics (ACL), pages 208?215.
Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of SQUASH,
the SFU question answering summary handler for the
DUC-2005 summarization task. In Proc. of DUC
2005.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In Proc. of HLT-NAACL 2004 Workshop on
Frontiers in Corpus Annotation.
Hiromi Nakaiwa. 1997. Automatic identification of zero
pronouns and their antecedents within aligned sen-
tence pairs. In Proc. of the 3rd Annual Meeting of
the Association for Natural Language Processing (in
Japanese).
Srini Narayanan and Sanda Harabagiu. 2004. Question
answering based on semantic structures. In Proc. of
the 20th International Conference on Computational
Linguistics (COLING).
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Sameer Pradhan, Waybe Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow seman-
tic parsing using support vector machines. In Proc.
of the Human Language Technology Conference/North
American Chapter of the Association of Computa-
tional Linguistics HLT/NAACL 2004.
Dan Shen and Mirella Lapata. 2007. Using semantic
roles to improve question answering. In Proc. of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP/CoNLL), pages 12?21.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer-Verlag, New York.
M. Walker, M. Iida, and S. Cote. 1994. Japanese dis-
course and the process of centering. Computational
Linguistics, 20(2):193?233.
Nianwen Xue. 2006. Semantic role labeling of nomi-
nalized predicates in Chinese. In Proc. of the HLT-
NAACL, pages 431?438.
David Yarowsky. 1994. Decision lists for lexical am-
biguity resolution: Application to accent restoration
in Spanish and French. In Proc. of the 32nd Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 88?95.
532
Question Classification using HDAG Kernel
Jun Suzuki, Hirotoshi Taira, Yutaka Sasaki, and Eisaku Maeda
NTT Communication Science Laboratories, NTT Corp.
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
  jun, taira, sasaki, maeda  @cslab.kecl.ntt.co.jp
Abstract
This paper proposes a machine learning
based question classification method us-
ing a kernel function, Hierarchical Di-
rected Acyclic Graph (HDAG) Kernel.
The HDAG Kernel directly accepts struc-
tured natural language data, such as sev-
eral levels of chunks and their relations,
and computes the value of the kernel func-
tion at a practical cost and time while re-
flecting all of these structures. We ex-
amine the proposed method in a ques-
tion classification experiment using 5011
Japanese questions that are labeled by
150 question types. The results demon-
strate that our proposed method improves
the performance of question classification
over that by conventional methods such as
bag-of-words and their combinations.
1 Introduction
Open-domain Question Answering (ODQA) in-
volves the extraction of correct answer(s) to a given
free-form factual question from a large collection
of texts. ODQA has been actively studied all over
the world since the start of the Question Answering
Track at TREC-8 in 1999.
The definition of ODQA tasks at the TREC QA-
Track has been revised and extended year after
year. At first, ODQA followed the Passage Retrieval
method as used at TREC-8. That is, the ODQA task
was to answer a question in the form of strings of
50 bytes or 250 bytes excerpted from a large set of
news wires. Recently, however, the ODQA task is
considered to be a task of extracting exact answers
to a question. For instance, if a QA system is given
the question ?When was Queen Victoria born??, it
should answer ?1832?.
Typically, QA systems have the following compo-
nents for achieving ODQA:
Question analysis analyzes a given question and
determines the question type and keywords.
Text retrieval finds the top  paragraphs or docu-
ments that match the result of the question anal-
ysis component.
Answer candidate extraction extracts answer can-
didates of the given question from the docu-
ments retrieved by the text retrieval component,
based on the results of the question types.
Answer selection selects the most plausible an-
swer(s) to the given question from among the
answer candidates extracted by the answer can-
didate extraction component.
One of the most important processes of those
listed above is identifying the target of intention in a
given question to determine the type of sought-after
answer. This process of determining the question
type for a given question is usually called question
classification. Without a question type, that is, the
result of question classification, it would be much
more difficult or even nearly infeasible to select cor-
rect answers from among the possible answer can-
didates, which would necessarily be all of the noun
phrases or named entities in the texts. Question clas-
sification provides the benefit of a powerful restric-
tion that reduces to a practical number of the answer
candidates that should be evaluated in the answer se-
lection process.
This work develops a machine learning approach
to question classification (Harabagiu et al, 2000;
Hermjakob, 2001; Li and Roth, 2002). We use the
Hierarchical Directed Acyclic Graph (HDAG) Ker-
nel (Suzuki et al, 2003), which is suited to handle
structured natural language data. It can handle struc-
tures within texts as the features of texts without
converting the structures to the explicit representa-
tion of numerical feature vectors. This framework is
useful for question classification because the works
of (Li and Roth, 2002; Suzuki et al, 2002a) showed
that richer information, such as structural and se-
mantical information inside a given question, im-
proves the question classification performance over
using the information of just simple key terms.
In Section 2, we present the question classifica-
tion problem. In Section 3, we explain our proposed
method for question classification. Finally, in Sec-
tion 4, we describe our experiment and results.
2 Question Classification
Question classification is defined as a task that maps
a given question to more than one of  question
types (classes).
In the general concept of QA systems, the result
of question classification is used in a downstream
process, answer selection, to select a correct answer
from among the large number of answer candidates
that are extracted from the source documents. The
result of the question classification, that is, the la-
bels of the question types, can reduce the number
of answer candidates. Therefore, we no longer have
to evaluate every noun phrase in the source docu-
ments to see whether it provides a correct answer to
a given question. Evaluating only answer candidates
that match the results of question classification is an
efficient method of obtaining correct answers. Thus,
question classification is an important process of a
QA system. Better performance in question classi-
fication will lead to better total performance of the
QA system.
2.1 Question Types: Classes of Questions
Numerous question taxonomies have been defined,
but unfortunately, no standard exists.
In the case of the TREC QA-Track, most systems
have their own question taxonomy, and these are re-
constructed year by year. For example, (Ittycheriah
et al, 2001) defined 31 original question types in
two levels of hierarchical structure. (Harabagiu et
al., 2000) also defined a large hierarchical question
taxonomy, and (Hovy et al, 2001) defined 141 ques-
tion types of a hierarchical question taxonomy.
Within all of these taxonomies, question types are
defined from the viewpoint of the target intention of
the given questions, and they have hierarchical struc-
tures, even though these question taxonomies are de-
fined by different researchers. This because the pur-
pose of question classification is to reduce the large
number of answer candidates by restricting the tar-
get intention via question types. Moreover, it is very
useful to handle question taxonomy constructed in a
hierarchical structure in the downstream processes.
Thus, question types should be the target intention
and constructed in a hierarchical structure.
2.2 Properties
Question classification is quite similar to Text Cate-
gorization, which is one of the major tasks in Nat-
ural Language Processing (NLP). These tasks re-
quire classification of the given text to certain de-
fined classes. In general, in the case of text catego-
rization, the given text is one document, such as a
newspaper article, and the classes are the topics of
the articles. In the case of question classification,
a given text is one short question sentence, and the
classes are the target answers corresponding to the
intention of the given question.
However, question classification requires much
more complicated features than text categorization,
as shown by (Li and Roth, 2002). They proved that
question classification needs richer information than
simple key terms (bag-of-words), which usually give
us high performance in text classification. More-
over, the previous work of (Suzuki et al, 2002a)
showed that the sequential patterns constructed by
different levels of attributes, such as words, part-of-
speech (POS) and semantical information, improve
the performance of question classification. The ex-
periments in these previous works indicated that
the structural and semantical features inside ques-
tions have the potential to improve the performance
of question classification. In other words, high-
performance question classification requires us to
extract the structural and semantical features from
the given question.
2.3 Learning and Classification Task
This paper focuses on the machine learning ap-
proach to question classification. The machine
learning approach has several advantages over man-
ual methods.
First, the construction of a manual classifier for
questions is a tedious task that requires the analy-
sis of a large number of questions. Moreover, map-
ping questions into question types requires the use
of lexical items and, therefore, an explicit represen-
tation of the mapping may be very large. On the
other hand, machine learning approaches only need
to define features. Finally, the classifier can be more
flexibly reconstructed than a manual one because it
can be trained on a new taxonomy in a very short
time.
As the machine learning algorithm, we chose the
Support Vector Machines (SVMs) (Cortes and Vap-
nik, 1995) because the work of (Joachims, 1998;
Taira and Haruno, 1999) reported state-of-the-art
performance in text categorization as long as ques-
tion classification is a similar process to text catego-
rization.
3 HDAG Kernel
Recently, the design of kernel functions has become
a hot topic in the research field of machine learning.
A specific kernel can drastically increase the perfor-
mance of specific tasks. Moreover, a specific kernel
can handle new feature spaces that are difficult to
manage directly with conventional methods.
The HDAG Kernel is a new kernel function that
is designed to easily handle structured natural lan-
guage data. According to the discussion in the pre-
vious section, richer information such as structural
and semantical information is required for high-
performance question classification.
We think that the HDAG Kernel is suitable for
improving the performance of question classifica-
tion: The HDAG Kernel can handle various linguis-
tic structures within texts, such as chunks and their
relations, as the features of the text without convert-
ing such structures to numerical feature vectors ex-
plicitly.
3.1 Feature Space
Figure 1 shows examples of the structures within
questions that are handled by the HDAG kernel.
As shown in Figure 1, the HDAG kernel accepts
several levels of chunks and their relations inside the
text. The nodes represent several levels of chunks in-
cluding words, and directed links represent their re-
lations. Suppose 
	  and 	  rep-
resent each node. Some nodes have a graph inside
themselves, which are called ?non-terminal nodes?.
Each node can have more than one attribute, such
as words, part-of-speech tags, semantic information
like WordNet (Fellbaum, 1998), and class names of
the named entity. Moreover, nodes are allowed to
not have any attribute, in other words, we do not
have to assign attributes to all nodes.
The ?attribute sequence? is a sequence of at-
tributes extracted from the node in sub-paths of
HDAGs. One type of attribute sequence becomes
one element in the feature vector. The framework of
the HDAG Kernel allows node skips during the ex-
traction of attribute sequences, and its cost is based
the decay factor ffProceedings of the ACL 2010 Conference Short Papers, pages 162?167,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Predicate Argument Structure Analysis using Transformation-based
Learning
Hirotoshi Taira Sanae Fujita Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Souraku-gun, Kyoto 619-0237, Japan
{taira,sanae}@cslab.kecl.ntt.co.jp nagata.masaaki@lab.ntt.co.jp
Abstract
Maintaining high annotation consistency
in large corpora is crucial for statistical
learning; however, such work is hard,
especially for tasks containing semantic
elements. This paper describes predi-
cate argument structure analysis using?
transformation-based learning. An advan-
tage of transformation-based learning is
the readability of learned rules. A dis-
advantage is that the rule extraction pro-
cedure is time-consuming. We present
incremental-based, transformation-based
learning for semantic processing tasks. As
an example, we deal with Japanese pred-
icate argument analysis and show some
tendencies of annotators for constructing
a corpus with our method.
1 Introduction
Automatic predicate argument structure analysis
(PAS) provides information of ?who did what
to whom? and is an important base tool for
such various text processing tasks as machine
translation information extraction (Hirschman et
al., 1999), question answering (Narayanan and
Harabagiu, 2004; Shen and Lapata, 2007), and
summarization (Melli et al, 2005). Most re-
cent approaches to predicate argument structure
analysis are statistical machine learning methods
such as support vector machines (SVMs)(Pradhan
et al, 2004). For predicate argument struc-
ture analysis, we have the following represen-
tative large corpora: FrameNet (Fillmore et al,
2001), PropBank (Palmer et al, 2005), and Nom-
Bank (Meyers et al, 2004) in English, the Chi-
nese PropBank (Xue, 2008) in Chinese, the
GDA Corpus (Hashida, 2005), Kyoto Text Corpus
Ver.4.0 (Kawahara et al, 2002), and the NAIST
Text Corpus (Iida et al, 2007) in Japanese.
The construction of such large corpora is strenu-
ous and time-consuming. Additionally, maintain-
ing high annotation consistency in such corpora
is crucial for statistical learning; however, such
work is hard, especially for tasks containing se-
mantic elements. For example, in Japanese cor-
pora, distinguishing true dative (or indirect object)
arguments from time-type argument is difficult be-
cause the arguments of both types are often ac-
companied with the ?ni? case marker.
A problem with such statistical learners as SVM
is the lack of interpretability; if accuracy is low, we
cannot identify the problems in the annotations.
We are focusing on transformation-based learn-
ing (TBL). An advantage for such learning meth-
ods is that we can easily interpret the learned
model. The tasks in most previous research are
such simple tagging tasks as part-of-speech tag-
ging, insertion and deletion of parentheses in syn-
tactic parsing, and chunking (Brill, 1995; Brill,
1993; Ramshaw and Marcus, 1995). Here we ex-
periment with a complex task: Japanese PASs.
TBL can be slow, so we proposed an incremen-
tal training method to speed up the training. We
experimented with a Japanese PAS corpus with a
graph-based TBL. From the experiments, we in-
terrelated the annotation tendency on the dataset.
The rest of this paper is organized as follows.
Section 2 describes Japanese predicate structure,
our graph expression of it, and our improved
method. The results of experiments using the
NAIST Text Corpus, which is our target corpus,
are reported in Section 3, and our conclusion is
provided in Section 4.
2 Predicate argument structure and
graph transformation learning
First, we illustrate the structure of a Japanese sen-
tence in Fig. 1. In Japanese, we can divide a sen-
tence into bunsetsu phrases (BP). A BP usually
consists of one or more content words and zero,
162
BP
CW FW
Kare no tabe ta okashi
He ?s
CW FW
eat PAST snack
wa kinou
TOP buy PAST
kat
CW FW CW FW
The snack he ate is one I bought at the store yesterday.
Kareno tabeta okashiwa kinou misede katta.
Sentence
Syntactic dependency between bunsetsus
PRED: Predicate
BPBPBPBP
yesterday
mise de ta
shop at
BP
CWCW FW
BP: Bunsetsu phrase
PRED
ARGARGARGARG
PRED
Nom. Acc.
Time Acc.
Loc.
CW: Content Word
FW: Functional Word
ARG: Argument
Nom: Nominative
Acc: Accusative
Time: Time
Loc: Location
Argument Types
Dat: Dative
Figure 1: Graph expression for PAS
one, or more than one functional words. Syn-
tactic dependency between bunsetsu phrases can
be defined. Japanese dependency parsers such as
Cabocha (Kudo and Matsumoto, 2002) can extract
BPs and their dependencies with about 90% accu-
racy.
Since predicates and arguments in Japanese are
mainly annotated on the head content word in
each BP, we can deal with BPs as candidates of
predicates or arguments. In our experiments, we
mapped each BP to an argument candidate node
of graphs. We also mapped each predicate to a
predicate node. Each predicate-argument relation
is identified by an edge between a predicate and an
argument, and the argument type is mapped to the
edge label. In our experiments below, we defined
five argument types: nominative (subjective), ac-
cusative (direct objective), dative (indirect objec-
tive), time, and location. We use five transforma-
tion types: a) add or b) delete a predicate node, c)
add or d) delete an edge between an predicate and
an argument node, e) change a label (= an argu-
ment type) to another label (Fig. 2). We explain
the existence of an edge between a predicate and
an argument labeled t candidate node as that the
predicate and the argument have a t type relation-
ship.
Transformation-based learning was proposed
by (Brill, 1995). Below we explain our learn-
ing strategy when we directly adapt the learning
method to our graph expression of PASs. First, un-
structured texts from the training data are inputted.
After pre-processing, each text is mapped to an
initial graph. In our experiments, the initial graph
has argument candidate nodes with corresponding
BPs and no predicate nodes or edges. Next, com-
a) `Add Pred Node?
PRED
BP BP BP
PRED
BP BP BP
b) `Delete Pred Node?
ARG
PRED
Nom.
ARG
PRED
c) `Add Edge?
d) `Delete Edge?
Nom.
ARG
PRED
Acc.
ARG
PRED
e) `Change Edge Label?
Figure 2: Transform types
paring the current graphs with the gold standard
graph structure in the training data, we find the dif-
ferent statuses of the nodes and edges among the
graphs. We extract such transformation rule candi-
dates as ?add node? and ?change edge label? with
constraints, including ?the corresponding BP in-
cludes a verb? and ?the argument candidate and the
predicate node have a syntactic dependency.? The
extractions are executed based on the rule tem-
plates given in advance. Each extracted rule is
evaluated for the current graphs, and error reduc-
tion is calculated. The best rule for the reduction
is selected as a new rule and inserted at the bottom
of the current rule list. The new rule is applied to
the current graphs, which are transferred to other
graph structures. This procedure is iterated until
the total errors for the gold standard graphs be-
come zero. When the process is completed, the
rule list is the final model. In the test phase, we it-
eratively transform nodes and edges in the graphs
mapped from the test data, based on rules in the
model like decision lists. The last graph after all
rule adaptations is the system output of the PAS.
In this procedure, the calculation of error reduc-
tion is very time-consuming, because we have to
check many constraints from the candidate rules
for all training samples. The calculation order is
O(MN), where M is the number of articles and
N is the number of candidate rules. Additionally,
an edge rule usually has three types of constraints:
?pred node constraint,? ?argument candidate node
constraint,? and ?relation constraint.? The num-
ber of combinations and extracted rules are much
larger than one of the rules for the node rules.
Ramshaw et al proposed an index-based efficient
reduction method for the calculation of error re-
duction (Ramshaw and Marcus, 1994). However,
in PAS tasks, we need to check the exclusiveness
of the argument types (for example, a predicate ar-
gument structure does not have two nominative ar-
163
guments), and we cannot directly use the method.
Jijkoun et al only used candidate rules that hap-
pen in the current and gold standard graphs and
used SVM learning for constraint checks (Jijkoun
and de Rijke, 2007). This method is effective
for achieving high accuracy; however, it loses the
readability of the rules. This is contrary to our aim
to extract readable rules.
To reduce the calculations while maintaining
readability, we propose an incremental method
and describe its procedure below. In this proce-
dure, we first have PAS graphs for only one arti-
cle. After the total errors among the current and
gold standard graphs become zero in the article,
we proceed to the next article. For the next article,
we first adapt the rules learned from the previous
article. After that, we extract new rules from the
two articles until the total errors for the articles be-
come zero. We continue these processes until the
last article. Additionally, we count the number of
rule occurrences and only use the rule candidates
that happen more than once, because most such
rules harm the accuracy. We save and use these
rules again if the occurrence increases.
3 Experiments
3.1 Experimental Settings
We used the articles in the NAIST Text Cor-
pus version 1.4? (Iida et al, 2007) based on the
Mainichi Shinbun Corpus (Mainichi, 1995), which
were taken from news articles published in the
Japanese Mainichi Shinbun newspaper. We used
articles published on January 1st for training ex-
amples and on January 3rd for test examples.
Three original argument types are defined in the
NAIST Text Corpus: nominative (or subjective),
accusative (or direct object), and dative (or indi-
rect object). For evaluation of the difficult anno-
tation cases, we also added annotations for ?time?
and ?location? types by ourselves. We show the
dataset distribution in Table 1. We extracted the
BP units and dependencies among these BPs from
the dataset using Cabocha, a Japanese dependency
parser, as pre-processing. After that, we adapted
our incremental learning to the training data. We
used two constraint templates in Tables 2 and 3
for predicate nodes and edges when extracting the
rule candidates.
Table 1: Data distribution
Training Test
# of Articles 95 74
# of Sentences 1,129 687
# of Predicates 3,261 2,038
# of Arguments 3,877 2,468
Nom. 1,717 971
Acc. 1,012 701
Dat. 632 376
Time 371 295
Loc. 145 125
Table 4: Total performances (F1-measure (%))
Type System P R F1
Pred. Baseline 89.4 85.1 87.2
Our system 91.8 85.3 88.4
Arg. Baseline 79.3 59.5 68.0
Our system 81.9 62.4 70.8
3.2 Results
Our incremental method takes an hour. In com-
parison, the original TBL cannot even extract one
rule in a day. The results of predicate and argu-
ment type predictions are shown in Table 4. Here,
?Baseline? is the baseline system that predicts the
BSs that contain verbs, adjectives, and da form
nouns (?to be? in English) as predicates and pre-
dicts argument types for BSs having syntactical
dependency with a predicted predicate BS, based
on the following rules: 1) BSs containing nomina-
tive (ga) / accusative (wo) / dative (ni) case mark-
ers are predicted to be nominative, accusative, and
dative, respectively. 2) BSs containing a topic case
marker (wa) are predicted to be nominative. 3)
When a word sense category from a Japanese on-
tology of the head word in BS belongs to a ?time?
or ?location? category, the BS is predicted to be a
?time? and ?location? type argument. In all preci-
sion, recall, and F1-measure, our system outper-
formed the baseline system.
Next, we show our system?s learning curve in
Fig. 3. The number of final rules was 68. This
indicates that the first twenty rules are mainly ef-
fective rules for the performance. The curve also
shows that no overfitting happened. Next, we
show the performance for every argument type in
Table 5. ?TBL,? which stands for ?transformation-
based learning,? is our system. In this table,
the performance of the dative and time types im-
proved, even though they are difficult to distin-
guish. On the other hand, the performance of the
location type argument in our system is very low.
Our method learns rules as decreasing errors of
164
Table 2: Predicate node constraint templates
Pred. Node Constraint Template Rule Example
Constraint Description Pred. Node Constraint Operation
pos1 noun, verb, adjective, etc. pos1=?ADJECTIVE? add pred node
pos2 independent, attached word, etc. pos2=?DEPENDENT WORD? del pred node
pos1 & pos2 above two features combination pos1=?VERB? & pos2=?ANCILLARY WORD? add pred node
?da? da form (copula) ?da form? add pred node
lemma word base form lemma=?%? add pred node
Table 3: Edge constraint templates
Edge Constraint Template Rule Example
Arg. Cand. Pred. Node Relation Edge Constraint OperationConst. Const. Const.
FW (=func.
word)
? dep(arg? pred) FW of Arg. =?wa(TOP)? & dep(arg? pred) add NOM edge
? FW dep(arg? pred) FW of Pred. =?na(ADNOMINAL)? & dep(arg
? pred)
add NOM edge
SemCat
(=semantic
category)
? dep(arg? pred) SemCat of Arg. = ?TIME? & dep(arg? pred) add TIME edge
FW passive form dep(arg? pred) FW of Arg. =?ga(NOM) & Pred.: passive form chg edge label
NOM? ACC
? kform (= type
of inflected
form)
? kform of Pred. = continuative ?ta? form add NOM edge
SemCat Pred. SemCat ? SemCat of Arg. = ?HUMAN? & Pred. SemCat
= ?PHYSICAL MOVE?
add NOM edge
0
10
20
30
40
50
60
70
80
F1-measure (%)
10 20 30 40 50 60
0
70
rules
Figure 3: Learning curves: x-axis = number of
rules; y-axis: F1-measure (%)
all arguments, and the performance of the location
type argument is probably sacrificed for total error
reduction because the number of location type ar-
guments is much smaller than the number of other
argument types (Table 1), and the improvement of
the performance-based learning for location type
arguments is relatively low. To confirm this, we
performed an experiment in which we gave the
rules of the baseline system to our system as initial
rules and subsequently performed our incremen-
tal learning. ?Base + TBL? shows the experiment.
The performance for the location type argument
improved drastically. However, the total perfor-
mance of the arguments was below the original
TBL. Moreover, the ?Base + TBL? performance
surpassed the baseline system. This indicates that
our system learned a reasonable model.
Finally, we show some interesting extracted
rules in Fig. 4. The first rule stands for an ex-
pression where the sentence ends with the per-
formance of something, which is often seen in
Japanese newspaper articles. The second and third
rules represent that annotators of this dataset tend
to annotate time types for which the semantic cate-
gory of the argument is time, even if the argument
looks like the dat. type, and annotators tend to an-
notate dat. type for arguments that have an dat.
165
if BP contains the word `%? ,
Add Pred. Node
PRED
Dat. / Time
ARG
PRED
if func. wd. is `DAT? case,
Rule No.20 CW
`%?
BP
Rule No.15
Time / Dat.
ARG
PRED
Rule No.16
Change Edge Label
Change Edge Label
Dat. ?Time
SemCat is `Time?
Example
Example
?? ?
BP
CW
BP
CW
BP
CW
kotae-ta hito-wa
87%-de
answer-ed people-TOP
87%-be
`People who answered are 87%?
PRED
7? ?
BP
CW
BP
CW
7ka-ni
staato-suru
7th DAT start will
`will start on the 7th
?
ARG
PRED
FW
FW FW FW
??? ?? ?
???? ??
Time
ARG
PRED
Dat.
Rule No.16 is applied
Figure 4: Examples of extracted rules
Table 5: Results for every arg. type (F-measure
(%))
System Args. Nom. Acc. Dat. Time Loc.
Base 68.0 65.8 79.6 70.5 51.5 38.0
TBL 70.8 64.9 86.4 74.8 59.6 1.7
Base + TBL 69.5 63.9 85.8 67.8 55.8 37.4
type case marker.
4 Conclusion
We performed experiments for Japanese predicate
argument structure analysis using transformation-
based learning and extracted rules that indicate the
tendencies annotators have. We presented an in-
cremental procedure to speed up rule extraction.
The performance of PAS analysis improved, espe-
cially, the dative and time types, which are difficult
to distinguish. Moreover, when time expressions
are attached to the ?ni? case, the learned model
showed a tendency to annotate them as dative ar-
guments in the used corpus. Our method has po-
tential for dative predictions and interpreting the
tendencies of annotator inconsistencies.
Acknowledgments
We thank Kevin Duh for his valuable comments.
References
Eric Brill. 1993. Transformation-based error-driven
parsing. In Proc. of the Third International Work-
shop on Parsing Technologies.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543?565.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank
which provides deep semantics. In Proc. of the Pa-
cific Asian Conference on Language, Information
and Computation (PACLING).
Kouichi Hashida. 2005. Global document annotation
(GDA) manual. http://i-content.org/GDA/.
Lynette Hirschman, Patricia Robinson, Lisa
Ferro, Nancy Chinchor, Erica Brown,
Ralph Grishman, and Beth Sundheim.
1999. Hub-4 Event?99 general guidelines.
http://www.itl.nist.gov/iaui/894.02/related projects/muc/.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proc. of ACL 2007 Workshop on Linguistic
Annotation, pages 132?139.
Valentin Jijkoun and Maarten de Rijke. 2007. Learn-
ing to transform linguistic graphs. In Proc. of
the Second Workshop on TextGraphs: Graph-
Based Algorithms for Natural Language Processing
(TextGraphs-2), pages 53?60. Association for Com-
putational Linguistics.
166
Daisuke Kawahara, Sadao Kurohashi, and Koichi
Hashida. 2002. Construction of a Japanese
relevance-tagged corpus (in Japanese). Proc. of the
8th Annual Meeting of the Association for Natural
Language Processing, pages 495?498.
Taku Kudo and Yuji Matsumoto. 2002. Japanese
dependency analysis using cascaded chunking. In
Proc. of the 6th Conference on Natural Language
Learning 2002 (CoNLL 2002).
Mainichi. 1995. CD Mainichi Shinbun 94. Nichigai
Associates Co.
Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.
Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,
and Fred Popowich. 2005. Description of
SQUASH, the SFU question answering summary
handler for the DUC-2005 summarization task. In
Proc. of DUC 2005.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank project:
An interim report. In Proc. of HLT-NAACL 2004
Workshop on Frontiers in Corpus Annotation.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion answering based on semantic structures. In
Proc. of the 20th International Conference on Com-
putational Linguistics (COLING).
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Sameer Pradhan, Waybe Ward, Kadri Hacioglu, James
Martin, and Dan Jurafsky. 2004. Shallow semantic
parsing using support vector machines. In Proc. of
the Human Language Technology Conference/North
American Chapter of the Association of Computa-
tional Linguistics HLT/NAACL 2004.
Lance Ramshaw and Mitchell Marcus. 1994. Explor-
ing the statistical derivation of transformational rule
sequences for part-of-speech tagging. In The Bal-
ancing Act: Proc. of the ACL Workshop on Com-
bining Symbolic and Statistical Approaches to Lan-
guage.
Lance Ramshaw and Mitchell Marcus. 1995. Text
chunking using transformation-based learning. In
Proc. of the third workshop on very large corpora,
pages 82?94.
Dan Shen and Mirella Lapata. 2007. Using se-
mantic roles to improve question answering. In
Proc. of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing
and Computational Natural Language Learning
(EMNLP/CoNLL), pages 12?21.
Nianwen Xue. 2008. Labeling Chinese predicates
with semantic roles. Computational Linguistics,
34(2):224?255.
167
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 383?386,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
MSS: Investigating the Effectiveness of Domain Combinations and
Topic Features for Word Sense Disambiguation
Sanae Fujita Kevin Duh Akinori Fujino Hirotoshi Taira Hiroyuki Shindo
NTT Communication Science Laboratories
{sanae, kevinduh, taira, a.fujino, shindo}@cslab.kecl.ntt.co.jp
Abstract
We participated in the SemEval-2010
Japanese Word Sense Disambiguation
(WSD) task (Task 16) and focused on
the following: (1) investigating domain
differences, (2) incorporating topic fea-
tures, and (3) predicting new unknown
senses. We experimented with Support
Vector Machines (SVM) and Maximum
Entropy (MEM) classifiers. We achieved
80.1% accuracy in our experiments.
1 Introduction
We participated in the SemEval-2010 Japanese
Word Sense Disambiguation (WSD) task (Task 16
(Okumura et al, 2010)), which has two new char-
acteristics: (1) Both training and test data across
3 or 4 domains. The training data include books
or magazines (called PB), newspaper articles (PN),
and white papers (OW). The test data also include
documents from a Q&A site on the WWW (OC);
(2) Test data include new senses (called X) that are
not defined in dictionary.
There is much previous research on WSD. In
the case of Japanese, unsupervised approaches
such as extended Lesk have performed well (Bald-
win et al, 2010), although they are outperformed
by supervised approaches (Tanaka et al, 2007;
Murata et al, 2003). Therefore, we selected a su-
pervised approach and constructed Support Vector
Machines (SVM) and Maximum Entropy (MEM)
classifiers using common features and topic fea-
tures. We performed extensive experiments to in-
vestigate the best combinations of domains for
training.
We describe the data in Section 2, and our sys-
tem in Section 3. Then in Section 4, we show the
results and provide some discussion.
2 Data Description
2.1 Given Data
We show an example of Iwanami Kokugo Jiten
(Nishio et al, 1994), which is a dictionary used as
a sense inventory. As shown in Figure 1, each en-
try has POS information and definition sentences
including example sentences.
We show an example of the given training data
in (1). The given data are morphologically ana-
lyzed and partly tagged with Iwanami?s sense IDs,
such as '37713-0-0-1-1( in (1).
(1) <mor pos='??-?}( rd='??( bfm='?
?( sense= '37713-0-0-1-1( >1<</mor>
This task includes 50 target words that were
split into 219 senses in Iwanami; among them, 143
senses including two Xs that were not defined in
Iwanami, appear in the training data. In the test
data, 150 senses including eight Xs appear. The
training and test data share 135 senses including
two Xs; that is, 15 senses including six Xs in the
test data are unseen in the training data.
2.2 Data Pre-processing
We performed two preliminary pre-processing
steps. First, we restored the base forms because
the given training and test data have no informa-
tion about the base forms. (1) shows an example
of the original morphological data, and then we
added the base form (lemma), as shown in (2).
(2) <mor pos=' ? ?-? } ( rd=' ? ? (
bfm=' ? ? ( sense='37713-0-0-1-1(
lemma='1d(>1<</mor>
Secondly, we extracted example sentences from
Iwanami, which is used as a sense inventory. To
compensate for the lack of training data, we an-
alyzed examples with a morphological analyzer,
Mecab1 UniDic version, because the training and
test data were tagged with POS based on UniDic.
1http://mecab.sourceforge.net/
383
??
?
?
?
HEADWORD Ad91d[ddNd:take (?; Transitive Verb)
37713-0-0-1-0
[
<1> ??<8[GCBk3D?= to get something left into one?s hand
]
37713-0-0-1-1
[
<y> 3??=53k-<??(6
take and hold by hand. 'to lead someone by the hand(
]
?
?
?
?
?
Figure 1: Simplified Entry for Iwanami Kokugo Jiten: Ad take
For example, from the entry for Ad take, as
shown in Figure 1, we extracted an example sen-
tence and morphologically analyzed it, as shown
in (3)2, for the second sense, 37713-0-0-1-1. In
(3), the underlined part is the headword and is
tagged with 37713-0-0-1-1.
(3) 3
hand
k
ACC
1<
take
?
and
?(
lead
?(I) take someone?s hand and lead him/her?
3 System Description
3.1 Features
In this section, we describe the features we gener-
ated.
3.1.1 Baseline Features
For each target word w, we used the surface form,
the base form, the POS tag, and the top POS cat-
egories, such as nouns, verbs, and adjectives of
w. Here the target is the ith word, so we also
used the same information of i? 2, i? 1, i+ 1, and
i+2th words. We used bigrams, trigrams, and skip-
bigrams back and forth within three words. We re-
fer to the model that uses these baseline features
as bl.
3.1.2 Bag-of-Words
For each target word w, we got all base forms of
the content words within the same document or
within the same article for newspapers (PN). We
refer to the model that uses these baseline features
as bow.
3.1.3 Topic Features
In the SemEval-2007 English WSD tasks, a sys-
tem incorporating topic features achieved the
highest accuracy (Cai et al, 2007). Inspired by
(Cai et al, 2007), we also used topic features.
Their approach uses Bayesian topic models (La-
tent Dirichlet Allocation: LDA) to infer topics in
an unsupervised fashion. Then the inferred topics
2We use ACC as an abbreviation of accusative
postposition.
are added as features to reduce the sparsity prob-
lem with word-only features.
In our proposed approach, we use the inferred
topics to find 'related?( words and directly add
these word counts to the bag-of-words representa-
tion.
We applied gibbslda++3 to the training and test
data to obtain multiple topic classification per doc-
ument or article for newspapers (PN). We used the
document or article topics for newspapers (PN) in-
cluding the target word. We refer to the model
that uses these topic features as tpX, where X is
the number of topics and tpdistX with the topics
weighted by distributions. In particular, the topic
distribution of each document/article is inferred by
the LDA topic model using standard Gibbs sam-
pling.
We also add the most typical words in the topic
as a bag-of-words. For example, one topic might
include ? city, ?? Tokyo, ? train line, ? ward
and so on. A second topic might include ?? dis-
section, ? after, ?? medicine, U grave and so
on. If a document is inferred to contain the first
topic, then the words (? city, ?? Tokyo, ? train
line, ...) are added to the bag-of-words feature. We
refer to these features as twdY, including the most
typical Y words as bag-of-words.
3.2 Investigation between Domains
In preliminary experiments, we used both SVM4
and MEM (Nigam et al, 1999), with optimization
method L-BFGS (Liu and Nocedal, 1989) to train
the WSD model.
First, we investigated the effect between do-
mains (PN, PB, and OW). For training data, we se-
lected words that occur in more than 50 sentences,
separated the training data by domain, and tested
different domain combinations.
Table 1 shows the SVM results of the domain
combinations. For Table 1, we did a 5-fold cross
validation for the self domain and for comparison
3http://gibbslda.sourceforge.net/
4http://www.csie.ntu.edu.tw/?cjlin/
libsvm/
384
Table 1: Investigation of Domain Combinations
on Training data (features: bl + bow, SVM)
Target Words 77, No. of Instances > 50
Domain Acc.(%) Diff. Comment
PN 78.7 - 63 words,
PN +OW 79.25 0.55 1094 instances
PN +PB 79.43 0.73
PN +ALL 79.34 0.64
PB 79.29 - 75 words,
PB +PN 78.85 -0.45 2463 instances
PB +OW 78.56 -0.73
PB +ALL 78.4 -0.89
OW 87.91 - 42 words,
OW +PN 89.05 1.14 703 instances
OW +PB 88.34 0.43
OW +ALL 89.05 1.14
with the results after adding the other domain data.
In Table 1, Diff. shows the differences to the self
domain.
As shown in Table 1, for PN and OW, using other
domains improved the results, but for PB, other do-
mains degraded the results. So we decided to se-
lect the domains for each target word.
In the formal run, for each pair of domain and
target words, we selected the combination of do-
main and dictionary examples that got the best
cross-validation result in the training data. Note
that in the case of no training data for the test data
domain, for example, since no OCs have training
data, we used all training data and dictionary ex-
amples.
We show the number of selected domain combi-
nations for each target domain in Table 2. Because
the distribution of target words is very unbalanced
in domains, not all types of target words appear in
every domain, as shown in Table 2.
3.3 Method for Predicting New Senses
We also tried to predict new senses (X) that didn?t
appear in the training data by calculating the en-
tropy for each target given in the MEM. We as-
sumed that high entropy (when the probabilities
of classes are uniformly dispersed) was indicative
of X; i.e., if [entropy > threshold] => predict X;
else => predict with MEM?s output sense tag.
Note that we used the words that were tagged
with Xs in the training data, except for the target
words. We compared the entropies of X and not
X of the words and heuristically tuned the thresh-
old based on the differences among entropies. Our
three official submissions correspond to different
thresholds.
Table 2: Used Domain Combinations
Used MEM SVM
Domain No. (%) No. (%)
Target: PB (48 types of target words)
ALL +EX 26 54.2 23 47.9
ALL 4 8.3 6 12.5
PB 11 22.9 8 16.7
PB +EX 1 2.1 1 2.1
PB +OW 1 2.1 3 6.3
PB +PN 5 10.4 7 14.6
Target: PN (46 types of target words)
ALL +EX 30 65.2 30 65.2
ALL 4 8.7 4 8.7
PN 4 8.7 1 2.2
PN +EX 0 0 1 2.2
PN +OW 2 4.3 2 4.3
PN +PB 6 13 8 17.4
Target: OW (16 types of target words)
ALL +EX 5 31.3 5 31.3
ALL 2 12.5 1 6.3
OW 6 37.5 3 18.8
OW +PB 3 18.8 3 18.8
OW +PN 0 0 4 25.0
Target: OC (46 types of target words)
ALL +EX 46 100 46 100
4 Results and Discussions
Our cross-validation experiments on the training
set showed that selecting data by domain combi-
nations works well, but unfortunately this failed
to achieve optimal results on the formal run. In
this section, we show the results using all of the
training data with no domain selections (also after
fixing some bugs).
Table 3 shows the results for the combination
of features on the test data. MEM greatly outper-
formed SVM. Its effective features are also quite
different. In the case of MEM, baseline features
(bl) almost gave the best result, and the topic fea-
tures improved the accuracy, especially when di-
vided into 200 topics. But for SVM, the topic
features are not so effective, and the bag-of-words
features improved accuracy.
For MEM with bl +tp200, which produced the
best result, the following are the best words: ?
outside (accuracy is 100%), C^ economy (98%),
?!d think (98%), d& big (98%), and %Z
culture (98%). On the other hand, the following
are the worst words: 1d take (36%), ? good
(48%), ?+d raise (48%), w2 put out (50%),
and ?= stand up (54%).
In Table 4, we show the results for each POS (bl
+tp200, MEM). The results for the verbs are com-
parably lower than the others. In future work, we
will consider adding syntactic features that may
improve the results.
385
Table 3: Comparisons among Features and Test data
TYPE Precision (%)
MEM SVM Explain
Base Line 68.96 68.96 Most Frequent Sense
bl 79.3 69.6 Base Line Features
bl +bow 77.0 70.8 + Bag-of-Words (BOW)
bl +bow +tp100 76.4 70.7 +BOW + Topics (100)
bl +bow +tp200 77.0 70.7 +BOW + Topics (200)
bl +bow +tp300 77.4 70.7 +BOW + Topics (300)
bl +bow +tp400 76.8 70.7 +BOW + Topics (400)
bl +bow +tpdist300 77.0 70.8 +BOW + Topics (300)*distribution
bl +bow +tp300 +twd100 76.2 70.8 + Topics (300) with 100 topic words
bl +bow +tp300 +twd200 76.0 70.8 + Topics (300) with 200 topic words
bl +bow +tp300 +twd300 75.9 70.8 + Topics (300) with 300 topic words
without bow
bl +tp100 79.3 69.6 + Topics (100)
bl +tp200 80.1 69.6 + Topics (200)
bl +tp300 79.6 69.6 + Topics (300)
bl +tp400 79.6 69.6 + Topics (400)
bl +tpdist100 79.3 69.6 + Topics (100)*distribution
bl +tpdist200 79.3 69.6 + Topics (200)*distribution
bl +tpdist300 79.3 69.6 + Topics (300)*distribution
bl +tp200 +twd100 74.6 69.6 + Topics (200) with 100 topic words
bl +tp300 +twd10 74.4 69.4 + Topics (300) with 10 topic words
bl +tp300 +twd20 75.2 69.3 + Topics (300) with 20 topic words
bl +tp300 +twd50 74.8 69.2 + Topics (300) with 50 topic words
bl +tp300 +twd200 74.6 69.6 + Topics (300) with 200 topic words
bl +tp300 +twd300 75.0 69.6 + Topics (300) with 300 topic words
bl +tp400 +twd100 74.1 69.6 + Topics (400) with 100 topic words
bl+tpdist100 +twd20 79.3 69.6 + Topics (100)*distribution with 20 topic words
bl+tpdist200 +twd20 79.3 69.6 + Topics (200)*distribution with 20 topic words
bl+tpdist400 +twd20 79.3 69.6 + Topics (400)*distribution with 20 topic words
Table 4: Results for each POS (bl +tp200, MEM)
POS No. of Types Acc. (%)
Nouns 22 85.5
Adjectives 5 79.2
Transitive Verbs 15 76.9
Intransitive Verbs 8 71.8
Total 50 80.1
In the formal run, we selected training data
for each pair of domain and target words and
used entropy to predict new unknown senses. Al-
though these two methods worked well in our
cross-validation experiments, they did not perform
well for the test data, probably due to domain mis-
match.
Finally, we also experimented with SVM and
MEM, and MEM gave better results.
References
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2010. A Re-
examination of MRD-based Word Sense Disambiguation.
Transactions on Asian Language Information Process, As-
sociation for Computing Machinery (ACM), 9(4):1?21.
Jun Fu Cai, Wee Sun Lee, and YW Teh. 2007. Improv-
ing Word Sense Disambiguation using Topic Features. In
Proceedings of EMNLP-CoNLL-2007, pp. 1015?1023.
Dong C. Liu and Jorge Nocedal. 1989. On the Limited Mem-
ory BFGS Method for Large Scale Optimization. Math.
Programming, 45(3, (Ser. B)):503?528.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and Hitoshi Isahara. 2003. CRL at Japanese
dictionary-based task of SENSEVAL-2. Journal of Nat-
ural Language Processing, 10(3):115?143. (in Japanese).
Kamal Nigam, John Lafferty, and Andrew McCallum. 1999.
Using Maximum Entropy for Text Classification. In
IJCAI-99 Workshop on Machine Learning for Information
Filtering, pp. 61?67.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and
Hikaru Yokono. 2010. SemEval-2010 Task: Japanese
WSD. In SemEval-2: Evaluation Exercises on Semantic
Evaluation.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae Fu-
jita, and Chikara Hashimoto. 2007. Word Sense Disam-
biguation Incorporating Lexical and Structural Semantic
Information. In Proceedings of EMNLP-CoNLL-2007, pp.
477?485.
386
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 111?118,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Zero Pronoun Resolution can Improve the Quality of J-E Translation
Hirotoshi Taira, Katsuhito Sudoh, Masaaki Nagata
NTT Communication Science Laboratories
2-4, Hikaridai, Seika-cho, Keihanna Science City
Kyoto 619-0237, Japan
{taira.hirotoshi,sudoh.katsuhito,nagata.masaaki}@lab.ntt.co.jp
Abstract
In Japanese, particularly, spoken Japanese,
subjective, objective and possessive cases are
very often omitted. Such Japanese sentences
are often translated by Japanese-English sta-
tistical machine translation to the English sen-
tence whose subjective, objective and posses-
sive cases are omitted, and it causes to de-
crease the quality of translation. We per-
formed experiments of J-E phrase based trans-
lation using Japanese sentence, whose omitted
pronouns are complemented by human. We
introduced ?antecedent F-measure? as a score
for measuring quality of the translated En-
glish. As a result, we found that it improves
the scores of antecedent F-measure while the
BLEU scores were almost unchanged. Every
effectiveness of the zero pronoun resolution
differs depending on the type and case of each
zero pronoun.
1 Introduction
Today, statistical translation systems have been able
to translate between languages at high accuracy us-
ing a lot of corpora . However, the quality of trans-
lation of Japanese to English is not high compar-
ing with the other language pairs that have the sim-
ilar syntactic structure such as the French-English
pair. Particularly, the quality of translation from
spoken Japanese to English is in low. There are
many reasons for the low quality. One is the dif-
ferent syntactic structures, that is, Japanese sentence
structure is SOV while English one is SVO. This
problem has been partly solved by head finalization
techniques (Isozaki et al, 2010). Another big prob-
lem is that subject, object and possessive cases are
often eliminated in Japanese, particularly, spoken
Japanese (Nariyama, 2003). In the case of Japanese
to English translation, the source language has lesser
information in surface than the target language, and
the quality of the translation tends to be low. We
show the example of the omissions in Fig 1. In this
example, the Japanese subject watashi wa (?I?) and
the object anata ni (?to you?) are eliminated in the
sentence. These omissions are not problems for hu-
man speakers and hearers because people easily rec-
ognize who is the questioner or responder (that is,
?I? and ?you?) from the context. However, gener-
ally speaking, the recognition is difficult for statisti-
cal translation systems.
Some European languages allow the elimination
of subject. We show an example in Spanish in Fig 2.
In this case, the subject is eliminated, and it leaves
traces including the case and the sex, on the related
verb. The Spanish word, tengo is the first person
singular form of the verb, tener (it means ?have?).
So it is easier to resolve elimination comparing with
Japanese one for SMT.
Otherwise, Japanese verbs usually have no inflec-
tional form depending on the case and sex. So,
we need take another way for elimination resolu-
tion. For example, if the eliminated Japanese sub-
ject is always ?I? when the sentence is declara-
tive, and the subject is always ?you? when the sen-
tence is a question sentence, phrase based transla-
tion systems are probably able to translate subject-
eliminated Japanese sentences to correct English
sentences. However, the hypothesis is not always
111
Jpn: (watashi wa)  (anata ni) shoushou ukagai tai  koto ga ari masu .
Eng:   I have     some  questions    to        ask     to   you    .
Omission of
subject
Omission of
object
Figure 1: Example of Japanese Ellipsis (Zero Pronoun)
Spa: (yo) Tengo   algunas preguntas  para  hacerle a  usted  .
Eng:   I have     some  questions    to        ask     to   you    .
Omission of subject
Figure 2: Spanish Ellipsis
true.
In this paper, we show that the quality of spoken
Japanese to English translation can improve using
a phrase-based translation system if we can use an
ideal elimination resolution system. However, we
also show that a simple elimination resolution sys-
tem is not effective to the improvement and it is nec-
essary to recognize correctly the modality of the sen-
tence.
2 Previous Work
There are a few researches for adaptation of ellip-
sis resolution to statistical translation systems while
there are a lot of researches for one to rule-based
translation systems in Japanese (Yoshimoto, 1988;
Dohsaka, 1990; Nakaiwa and Yamada, 1997; Ya-
mamoto et al, 1997).
As a research of SMT using elimination resolu-
tion, we have (Furuichi et al, 2011). However, the
target of the research is illustrative sentences in En-
glish to Japanese dictionary. Our research aims spo-
ken language translation and it is different from the
paper.
3 Setup of the Data of Subjects and
Objects Ellipsis in Spoken Japanese
3.1 Ellipsis Resolved Data by Human
In this section, we describe the data used in our ex-
periments. We used BTEC (Basic Travel Expres-
sion Corpus) corpus (Kikui et al, 2003) distributed
in IWSLT07 (Fordyce, 2007). The corpus consists
of tourism-related sentences similar to those that
are usually found in phrasebooks for tourists going
abroad. The characteristics of the dataset are shown
in Table 1. We used ?train? for training, ?devset1-
3? for tuning, and ?test? for evaluation. We did not
use the ?devset4? and ?devset5? sets because of the
different number of English references.
We annotated zero pronouns and the antecedents
to the sentences by hand. Here, zero pronoun is de-
fined as an obligatory case noun phrase that is not
expressed in the utterance but can be understood
through other utterances in the discourse, context, or
out-of-context knowledge (Yoshimoto, 1988). We
annotated the zero pronouns based on pronouns in
the translated English sentences. The BTEC corpus
has multi-references in English. We first chose the
most syntactically and lexically similar translation
in the references and annotated zero pronouns in it.
Our target pronouns are I, my, me, mine, myself, we,
our, us, ours, ourselves, you, your, yourself, your-
selves, he, his, him, himself, she, her, herself, it, its,
itself, they, their, them, theirs and themselves in En-
glish. We show the distribution of the annotation
types in the test set in Table 2.
3.2 Baseline System
We also examined a simple baseline zero pronoun
resolution system for the same data. We defined
112
Table 1: Data distribution
train devset1-3 devset4 devset5 test
# of References 1 16 7 7 16
# of Source Segments 39,953 1,512 489 500 489
Japanese predicate as verb, adjective, and copula (da
form) in the experiments. If the inputted Japanese
sentence contains predicates and it does not contain
?wa? (a binding particle and a topic marker), ?mo? (a
binding particle, which means ?also? and can often
replace ?wa? and ?ga?), and ?ga? (a case particle and
subjective marker), the system regards the sentence
as a candidate sentence to solve the zero pronouns.
Then, if the candidate sentence is declarative, the
system inserts ?watashi wa (I)? when the predicate
is a verb, and ?sore wa (it)? when the predicate is a
adjective or a copula. In the same way, if the candi-
date sentence is a question, the system inserts ?anata
wa (you)? when the predicate is a verb, and ?sore wa
(it)? when the predicate is a adjective or a copula.
These inserted position is the beginning of the sen-
tence. In the case that the sentence is imperative, the
system does not solve the zero pronouns (Fig. 3).
4 Experiments
4.1 Experimental Setting
Fig. 4 shows the outline of the procedure of our ex-
periment. We used Moses (Koehn et al, 2007) for
the training of the translation and language models,
tuning with MERT (Och, 2003) and the decoding.
First, we prepared the data for learning which con-
sists of parallel English and Japanese sentences. We
used MeCab 1 as Japanese tokenizer and the tok-
enizer in Moses Tool kit as English tokenizer. We
used default settings for the parameters of Moses.
Next, Moses learns language model and translation
model from the Japanese and English sentence pairs.
Then, the learned model was tuned by completed
sentences with MERT. and Moses decoded the com-
pleted Japanese sentences to English sentences.
4.2 Evaluation Method
We used BLEU (Papineni et al, 2002) and an-
tecedent Precision, Recall and F-measure for the
1http://mecab.sourceforge.net/
evaluation of the performances, comparing the sys-
tem outputs with the English references of test data.
Using only BLEU score is not adequate for evalua-
tion of pronoun translation (Hardmeier et al, 2010).
We were inspired empty node recovery evaluation
by (Johnson, 2002) and defined antecedent Preci-
sion (P), Recall (R) and F-measure (F) as follows,
P =
|G ? S|
|S|
R =
|G ? S|
|G|
F =
2PR
P +R
Here, S is the set of each pronoun in English
translated by decoder, G is the set of the gold stan-
dard zero pronoun.
We evaluated the effect of performance of every
case among completed sentences by human, ones by
the baseline system, and the original sentences.
4.3 Experimental Result
We show the BLEU scores in Table 3. and the an-
tecedent precision, recall and F-measure in Table 4.
The BLEU scores for experiments using our base-
line system and human annotation, are slightly bet-
ter than for one without ellipsis resolution, 45.4%
and 45.6%, respectively. However, the scores of an-
tecedent F-measure have major difference between
?original? and ?human?. Particularly, the recall is im-
proved. Each 1st, 2nd and 3rd person score is better
than original one.
5 Discussion and Conclusion
We performed experiments of J-E phrase based
translation using Japanese sentences, whose omit-
ted pronouns are complemented by human and a
baseline system. Using ?antecedent F-measure? as a
score for measuring the quality of the translated En-
glish, it improves the score of antecedent F-measure.
Every effectiveness of the zero pronoun resolution
113
ano eiga-wo mimashita.
the movie-OBJ     watched
Declarative sentence
Watashi-wa ano eiga-wo mimashita.
I-TOP the     movie-OBJ     watched
(=  ?I watched the movie.? )   
Question sentence
ano eiga-wo mimashita ka ?
the    movie-OBJ     watched       QUES  ?
Anata-wa ano eiga-wo mimashita ka ?
You-TOP the     movie-OBJ     watched      QUES  ?
(=  ?Did you watch the movie?? )   
Imperative sentence
ano eiga-wo minasai.
the    movie-OBJ     watch-IMP
ano eiga-wo minasai.
the    movie-OBJ     watch-IMP
(=  ?Watch the movie.? )   
Figure 3: Our baseline system of zero pronoun resolution
differed, depending on the type and case of each zero
pronoun. The F-measures for the first person pro-
noun were smaller than expected ones, Rather, the
scores for and possessive pronouns second person
were greater (Table. 3).
We show a better, a worse, and an unchanged
cases of translation using the baseline system of
the elimination resolution in Fig. 5. The left-hand
is the result of the alignment between the origi-
nal Japanese sentence and the decoded English sen-
tence. The right-hand is the result of one using
the Japanese the baseline system solved zero pro-
nouns. In the ?better? case, the alignment of todoke-
te (send) is better than one of the original sen-
tence, and ?Can you? is compensated by the solved
zero pronoun anata-wa (you-TOP). Otherwise, in
the ?worse? case, our baseline system could not rec-
ognize that the sentence is imperative, and inserted
watashi-wa (I-TOP) incorrectly into the sentence. It
indicates that we need a highly accurate recogni-
tion of the modalities of sentences for more correct
completion of the antecedent of zero pronouns. In
the ?unchanged? case, the translation results are the
same. However, the alignment of the right-hand is
more correct than one of the left-hand.
References
Kohji Dohsaka. 1990. Identifying the referents of zero-
pronouns in japanese based on pragmatic constraint in-
terpretation. In Proceedings of ECAI, pages 240?245.
C.S. Fordyce. 2007. Overview of the iwslt 2007 eval-
uation campaign. In Proceedings of the International
Workshop on Spoken Language Translation, pages 1?
12.
M. Furuichi, J. Murakami, M. Tokuhisa, and M. Murata.
2011. The effect of complement subject in japanese
to english statistical machine translation (in Japanese).
In Proceedings of the 17th Annual Meeting of The
114
English
Parallel Corpus for Training
Japanese
Shoushou ukagai tai koto ga ari masu ga.?
I have some questions to ask .
Decoder ?Moses?
Parallel Corpus for Test
Completed Sentences
honkon  ryokou ni tsuite 
siri  tain  desu  ga.
exo1 wa  honkon  ryokou ni tsuite 
siri  tain  desu  ga.
System Output
Training
Translation Model
Language Model
Decoding
I?d like to know about
the Hong Kong trip.
English
I would like to know about
the Hong Kong trip.
Evaluation
Japanese
- - - -
- - - -
- - - -
- - - -
Zero pronoun annotation by hand
or baseline system 
Tuning
Japanese English
Parallel Corpus for Tuning
- - - - - - - -
Zero pronoun annotation 
by hand or baseline system 
Completed Sentences
- - - - - - - -
Figure 4: Outline of the experiment
Association for Natural Language Processing (NLP-
2012).
C. Hardmeier, M. Federico, and F.B. Kessler. 2010.
Modelling pronominal anaphora in statistical machine
translation. In Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 283?289.
H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh. 2010.
Head finalization: A simple reordering rule for sov
languages. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 244?251. Association for Computational
Linguistics.
Mark Johnson. 2002. A simple pattern-matching al-
gorithm for recovering empty nodes and their an-
tecedents. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
136?143, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
G. Kikui, E. Sumita, T. Takezawa, and S. Yamamoto.
2003. Creating corpora for speech-to-speech transla-
tion. In Proceedings of EUROSPEECH, pages 381?
384.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: open source
toolkit for statistical machine translation. In Proc.
of the 45th Annual Conference of the Association for
115
Better
worse
Unchanged 
map-BY   point_out would       QUES
chizu-de   sashi-te morae-masu ka.
Would you point them out on this map ?
You-TOP map-BY   point_out would     QUES
anata-wa chizu-de   sashi-te morae-masu ka.
Would you point them out on this map ?
Hurry up 
Isoi-de  .
Hurry up .
(Ref)  Hurry up.
I-TOP    hurry up 
watashi-wa Isoi-de  .
I  ?m  in a hurry .
(Ref)  Would you point one out on this map?
Today?s    evening       by      send          would           QUES 
Kyou-no  yuugata made-ni todoke-te morae-masu ka .
It   by  this  evening  ?
(Ref) Can you deliver them by this evening?
you-TOP Today?s    evening       by      send          would           QUES 
anata-wa kyou-no  yuugata made-ni todoke-te morae-masu ka .
Can you   send it   by   this evening  ?
Figure 5: Effectiveness of zero pronoun resolution for decoding
Computational Linguistics (ACL-07), Demonstration
Session, pages 177?180.
H. Nakaiwa and S. Yamada. 1997. Automatic identifi-
cation of zero pronouns and their antecedents within
aligned sentence pairs. In Proc. of the 3rd Annual
Meeting of the Association for Natural Language Pro-
cessing.
S. Nariyama. 2003. Ellipsis and reference tracking
in Japanese, volume 66. John Benjamins Publishing
Company.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In Proc. of the ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proc. of the 40th An-
nual Conference of the Association for Computational
Linguistics (ACL-02).
K. Yamamoto, E. Sumita, O. Furuse, and H. Iida. 1997.
Ellipsis resolution in dialogues via decision-tree learn-
ing. In Proc. of NLPRS, volume 97. Citeseer.
K. Yoshimoto. 1988. Identifying zero pronouns in
japanese dialogue. In Proceedings of the 12th con-
ference on Computational linguistics-Volume 2, pages
779?784. Association for Computational Linguistics.
116
Table 2: The Type Distributions of Zero Pronouns in Test Set
Type Pronoun #
First personal pronoun i 121
my 39
me 32
mine 1
myself 0
we 7
our 2
us 2
ours 0
ourselves 0
total 204
Second personal pronoun you 95
your 23
yours 0
yourself 0
yourselves 0
total 118
Third personal pronoun he 1
his 0
him 0
himself 0
she 0
her 2
hers 0
herself 0
it 51
its 0
itself 0
they 2
their 0
them 5
theirs 0
themselves 0
total 61
all total 383
Table 3: BLEU score
BLEU F(Avg.) P R F (1st person) F (2nd person) F (3rd person)
original 45.1 59.7 63.8 56.1 61.6 59.9 52.3
baseline 45.4 58.5 64.1 53.7 61.2 59.2 47.7
human 45.6 71.8 67.5 76.7 70.6 77.6 63.7
117
Table 4: Antecedent precision, recall and F-measure for every pronoun
i (ref:121) my (ref:39) me (ref:32)
BLEU P R F P R F P R F
original 45.1 56.8 51.2 53.9 55.5 51.2 53.3 58.0 56.2 57.1
baseline 45.4 51.8 46.2 48.9 67.8 48.7 56.7 66.6 50.0 57.1
human 45.6 50.9 68.6 58.4 65.2 76.9 70.5 61.2 59.3 60.3
we (ref:7) our (ref:2) us (ref:2)
P R F P R F P R F
original 20.0 14.2 16.6 100.0 50.0 66.6 0.00 0.00 0.00
baseline 25.0 14.2 18.1 100.0 50.0 66.6 0.00 0.00 0.00
human 40.0 28.5 33.3 100.0 50.0 66.6 0.00 0.00 0.00
you (ref:95) your (ref:23)
P R F P R F
original 55.3 54.7 55.0 80.0 52.1 63.1
baseline 57.1 54.7 55.9 58.8 43.4 50.0
human 68.4 80.0 73.7 73.0 82.6 77.5
it (ref:51) its (ref:0)
P R F P R F
original 56.1 45.1 50.0 0.00 0.00 0.00
baseline 51.2 41.1 45.6 0.00 0.00 0.00
human 58.3 54.9 56.5 0.00 0.00 0.00
they (ref:2) their (ref:0) them (ref:5)
P R F P R F P R F
original 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00
baseline 100.0 50.0 66.6 0.00 0.00 0.00 0.00 0.00 0.00
human 58.3 54.9 56.5 0.00 0.00 0.00 0.00 0.00 0.00
118
