Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1438?1442,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
The VerbCorner Project: Toward an Empirically-Based Semantic
Decomposition of Verbs
Joshua K. Hartshorne
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, MA 02139, USA
jkhartshorne@gmail.com
Claire Bonial, Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Hellems 290, 295 UCB
Boulder, CO 80309, USA
{CBonial, MPalmer}@colorado.edu
Abstract
This research describes efforts to use crowd-
sourcing to improve the validity of the seman-
tic predicates in VerbNet, a lexicon of about
6300 English verbs. The current semantic
predicates can be thought of semantic prim-
itives, into which the concepts denoted by a
verb can be decomposed. For example, the
verb spray (of the Spray class), involves the
predicates MOTION, NOT, and LOCATION,
where the event can be decomposed into an
AGENT causing a THEME that was originally
not in a particular location to now be in that
location. Although VerbNet?s predicates are
theoretically well-motivated, systematic em-
pirical data is scarce. This paper describes a
recently-launched attempt to address this issue
with a series of human judgment tasks, posed
to subjects in the form of games.
1 Introduction
One key application of Natural Language Processing
(NLP) is meaning extraction. Of particular impor-
tance is propositional meaning: To understand ?Jes-
sica sprayed paint on the wall,? it is not enough to
know who Jessica is, what paint is, and where the
wall is, but that, by the end of the event, some quan-
tity of paint that was not previously on the wall now
is. One must extract not only meanings for individ-
ual words but also the relations between them.
One option is to learn these relations in a largely
bottom-up, data-driven fashion (Chklovski and Pan-
tel, 2004; Poon and Domingos, 2009). For instance,
Poon and Domingos (2009) first extracts depen-
dency trees, converts those into quasi-logical form,
recursively induces lambda expressions from them,
and uses clustering to derive progressively abstract
knowledge.
An alternative is to take a human-inspired ap-
proach, mapping the linguistic input onto the kinds
of representations that linguistic and psychologi-
cal research suggests are the representations em-
ployed by humans. While the exact characteriza-
tion of meaning (and by extension, thought) remains
an area of active research in the cognitive sciences
(Margolis and Laurence, 1999), decades of research
in linguistics and psychology suggests that much of
the meaning of a sentence ? as well as its syntactic
structure ? can be accounted for by invoking a small
number of highly abstract semantic features (usu-
ally represented as predicates), such as causation,
agency, basic topological relations, and directed mo-
tion (Ambridge et al, 2013; Croft, 2012; Jackend-
off, 1990; Levin and Rappaport Hovav, 2005; Peset-
sky, 1995; Pinker, 1989). For instance, a given verb
can appear in some syntactic frames (Sally broke the
vase. Sally broke the vase with the hammer. The vase
broke.) and not others (*Sally broke the vase to the
floor. *Sally broke John the vase.). When verbs are
classified according to the syntactic frames they can
appear in, most if not all the verbs in a class involve
the same set of abstract semantic features.1
Interestingly, roughly these same features (causa-
tion, etc.) have been singled out by developmental
psychologists as part of ?core knowledge? ? a set of
early-learned or perhaps innate concepts upon which
1Whether all verbs in a class share the same abstract pred-
icates or merely most is an area of active research (Levin and
Rappaport Hovav, 2005).
1438
the rest of cognition is built (Spelke and Kinzler,
2007). Thus these semantic features/predicates may
be not only crucial to describing linguistic mean-
ing but may be central organizing principles for a
human?s (reasonably successful) thinking about and
conceptualization of the world. As such, they pro-
vide a potentially rewarding target for NLP.
2 VerbNet
2.1 Overview and Structure
Perhaps the most comprehensive implementation
of this approach appears in VerbNet (Kipper et al,
2008; based on Levin, 1993). VerbNet classifies
verbs based on the syntactic frames they can appear
in, providing a semantic description of each frame
for each class. An example entry is shown below:
Syntactic Frame NP V NP PP.DESTINATION
Example Jessica sprayed the wall.
Syntax AGENT V THEME {+LOC|+DEST CONF}
DESTINATION
Semantics MOTION(DURING(E), THEME)
NOT(PREP(START(E), THEME, DESTINATION))
PREP(END(E), THEME, DESTINATION)
CAUSE(AGENT, E)
The ?Syntactic Frame? provides a flat syntactic
parse. ?Syntax? provides semantic role labels for
each of the NPs and PPs, which are invoked in ?Se-
mantics?. VerbNet decomposes the semantics of
this sentence into four separate predicates: 1) the
THEME (the paint) moves doing the event E; 2) at
the start of the event E, the THEME (the paint) is
not at the DESTINATION (on the wall), whereas 3)
at the end of the event E, the THEME (the paint) is
at the DESTINATION (on the wall), and; 4) the event
is caused by the AGENT (Sally). Note that this cap-
tures only the core aspects of semantics shared by all
verbs in the class; differences between verbs in the
same class (e.g., spray vs. splash) are omitted.
Importantly, the semantics of the sentence is de-
pendent on both the matrix verb (paint) and the syn-
tactic frame. Famously, when inserted in the slightly
different frame NP V NP.DESTINATION PP.THEME
? ?Sally sprayed the wall with paint? ? ?spray? en-
tails that destination (the wall) is now fully painted,
an entailment that does not follow in the example
above (Pinker, 1989).
2.2 Uses and Limitations
VerbNet has been used in a variety of NLP appli-
cations, such as semantic role labeling (Swier and
Stevenson, 2004), inferencing (Zaenen et al, 2008),
verb classification (Joanis et al, 2008), and informa-
tion extraction (Maynard, Funk, and Peters, 2009).
While such applications have been successful thus
far, an important constraint on how well VerbNet-
based NLP applications can be expected to perform
is the accuracy of the semantics encoded in Verb-
Net. Here, several issues arise. Leaving aside mis-
categorized verbs and other inaccuracies, as noted
above VerbNet assumes that all verbs in the same
class share the same core predicates, which may or
may not be empirically justified. Given the number
of semantic predicates (146),2 verb entries (6580),
and unique verb lemmas (6284) it is not feasible for
a single research team to check, particularly since af-
ter a certain number of verbs, intuitions become less
clear. In any case, it may not be ideal to rely solely
on the intuitions of invested researchers, whose in-
tuitions about subtle judgments may be clouded by
theoretical commitments (Gibson and Federenko,
2013); the only way to ensure this is not the case
is through independent validation. Unfortunately, of
the 280 verb classes in VerbNet, this has been done
for only a few (cf Ambridge et al, 2013).
3 VerbCorner
The VerbCorner project was designed to address
these issues by crowd-sourcing the semantic judg-
ments online (gameswithwords.org/VerbCorner/).
Several previous projects have successfully crowd-
sourced linguistic annotations, such as Phrase De-
tectives, where volunteers have contributed 2.5 mil-
lion judgments on anaphoric relations (Poesio et al,
2012). Below, we outline the VerbCorner project
and describe one specific annotation task in detail.
3.1 Developing Semantic Annotation Tasks
Collecting accurate judgments on subtle questions
from naive participants with limited metalinguistic
2Note that these vary in applicability from those specific to
a small number of verbs (CHARACTERIZE, CONSPIRE) to those
frequently invoked (BEGIN, EXIST).
1439
skills is difficult. Rare is the non-linguist who can
immediately answer the question, ?Does the verb
?throw,? when used transitively, entail a change of
location on the part of its THEME?? Thus, we began
by developing tasks that isolate semantic features in
a way accessible to untrained annotators.
We converted the metalinguistic judgments
(?Does this verb entail this abstract predicate??) into
real-world problems, which previous research sug-
gests should be easier (Cosmides and Tooby, 1992).
Each judgment tasks involved a fanciful backstory.
For instance, in ?Simon Says Freeze?, a task de-
signed to elicit judgments about movement, the
Galactic Overlord (Simon) decrees ?Galactic Stay
Where You Are Day,? during which nobody is al-
lowed to move from their current location. Partici-
pants read descriptions of events and decide whether
anyone violated the rule. In ?Explode on Contact?,
designed to elicit judgments about physical contact,
objects and people explode when they touch one an-
other. The participant reads descriptions of events
and decides whether anything has exploded.3
Each task was piloted until inter-coder reliability
was acceptably high and the modal response nearly
always corresponded with researcher intuitions. As
such, these tasks cannot be used to establish whether
researcher intuitions for the pilot stimuli are correct
(this would be circular); however, there is no guar-
antee that agreement with the researcher will gener-
alize to new items (the pilot stimuli cover a trivial
proportion of all verbs in VerbNet).
3.2 Crowd-sourcing Semantic Judgments
The pilot experiments showed that it is possible to
elicit reliable semantic judgments corresponding to
VerbNet predicates from naive participants (see sec-
tion 3.3). At the project website, volunteers choose
one of the tasks from a list and begin tagging sen-
tences. The sentences are sampled smartly, avoid-
ing sentences already tagged by that volunteer and
biased in favor of of the sentences with the fewest
3Note that each task is designed to elicit judgments about
entailments ? things that must be true rather than are merely
likely to be true. If John greeted Bill, they might have come
into contact (e.g., by shaking hands), but perhaps they did not.
Previous work suggests that it is entailments that matter, partic-
ularly for explaining the syntactic behavior of verbs (Levin and
Rappaport Hovav, 2005)
judgments so far. Rather than assessing annotator
quality through gold standard trials with known an-
swers (which wastes data ? the answers to these tri-
als are known), approximately 150 sentences were
chosen to be ?over-sampled.? As the volunteer tags
sentences, approximately one out of every five are
from this over-sampled set until that volunteer has
tagged all of them. This guarantees that any given
volunteer will have tried some sentences targeted
by many other volunteers, allowing inter-annotator
agreement to be used to assess annotator quality.
Following the example of Zooniverse (zooni-
verse.org), a popular ?Citizen Science? platform,
volunteers are encouraged but required to register
(requiring registration prior to seeing the tasks was
found to be a significant barrier to entry). Regis-
tration allows collecting linguistic and educational
background from the volunteer, and also makes it
possible to track the same volunteer across sessions.
Multiple gamification elements were incorporated
into VerbCorner in order to recruit and motivate vol-
unteers. Each task has a leaderboard, where the
volunteer can see his/her rank out of all volunteers
in terms of number of contributions made. In ad-
dition, there is a general leaderboard, which sums
across tasks. Volunteers can earn badges, displayed
on their homepage, for answering certain numbers
of questions in each task. Finally, at random inter-
vals bonus points are awarded, with the explanation
for the bonus points tailored to the task?s backstory.
VerbCorner was launched on May 21, 2013. After
six weeks, 555 volunteers had provided at least one
annotation, for a total of 39,274 annotations, demon-
strating the feasibility of collecting large numbers of
annotations through this method.
3.3 Case Study: Equilibrium
?Equilibrium? was designed to elicit judgments
about application of force, frequently argued to be
a core semantic feature in the sense discussed above
(Pinker, 1989). The backstory involves the ?Zen Di-
mension,? in which nobody is allowed to exert force
on anything else. The participant reads descriptions
of events (Sally sprayed paint onto the wall) and de-
cides whether they would be allowable in the Zen
Dimension ? and, in particular, which participants
in the event are illegally applying force.
In order to minimize unwanted effects of world
1440
knowledge, the verb?s arguments are replaced with
nonsense words or randomly chosen proper names
(Sally sprayed the dax onto the blicket). In the
context of the story, this is explained as necessary
anonymization: You are a government official de-
termining whether certain activities are allowable,
and ensuring anonymity is an important safeguard
against favoritism and corruption. An alternative
wouod be to use multiple different content words,
randomly chosen for each annotator. However, this
greatly increases the number of annotators needed
and quickly becomes infeasible.
3.3.1 Pilot Results
The task was piloted on 138 sentences, which com-
prised all possible syntactic frames for three verbs
from each of five verb classes in VerbNet. After
two rounds of piloting (between the first and second,
wording in the backstory was adjusted for clarity
based on pilot subject feedback and results), Kripp?s
alpha reached .76 for 8 annotators, which represents
a reasonably high level of inter-annotator agreement.
Importantly, the modal response matched the intu-
itions of the researchers in 137 of 138 cases.4
3.3.2 Preliminary VerbCorner Results
?Equillibrium? was one of the first tasks posted on
VerbCorner, with data currently being collected on
12 of the 280 VerbNet classes, for a total of 5,171
sentences. As of writing, 414 users have submitted
14,294 judgments. Individual annotators annotated
anywhere from 1 to 195 sentences (mean=8, me-
dian=4). While most sentences have relatively few
judgments, each of the 194 over-sampled sentences
has between 15 and 20 judgments.5
Comparing the modal response with the re-
searchers? intuitions resulted in a match for 184 of
194 sentences. In general, where the modal response
4The remaining case was ?The crose smashed sondily.? for
which four pilot subjects thought involved the crose applying
force ? matching researcher intuition ? and four thought did
not involve any application of force, perhaps interpreting the
sentence was a passive.
5These are the same 15 verbs used in the piloting. The num-
ber of sentences is larger in order to test a wider range of pos-
sible arguments. In particular, wherever appropriate, separate
sentences were constructed using animate and inanimate argu-
ments. Compare Sally sprayed the dax onto Mary and Sally
sprayed the dax onto the blicket.
did not match researcher intuitions, the modal re-
sponse was itself not popular, comprising an aver-
age of 53% of responses, compared with an aver-
age of 77% where the modal response matched re-
searcher intuitions. Thus, these appear to be cases of
disagreement, either because the correct intuition re-
quires more work to obtain or because of differences
across idiolects (at the moment, there is no obvious
pattern as to which sentences caused difficulty, but
the sample size is small). Thus, follow-up investi-
gation of sentences with little inter-coder agreement
may be warranted.
4 Conclusion and Future Work
Data-collection is ongoing. VerbNet identifies ap-
proximately 150 different semantic predicates. An-
notating every verb in each of its syntactic frames for
each semantic predicate would take many millions
of judgments. However, most of the semantic predi-
cates employed in VerbNet are very narrow in scope
and only apply to a few classes. Thus, we have be-
gun with broad predicates that are thought to apply
to many verbs and are adding progressively narrower
predicates as work progresses. At the current rate,
we should complete annotation for the half-dozen
most frequent semantic predicates in the space of a
year.
Future work will explore using an individual
annotator?s history across trials to weight that
user?s contributions, something that VerbCorner was
specifically designed to allow (see above). How to
assess annotator quality without gold standard data
is an active area of research (Passonneau and Car-
penter, 2013; Rzhetsky, Shatkay and Wilbur, 2009;
Whitehill et al, 2009). For instance, Whitehill and
colleagues (2009) provide an algorithm for jointly
estimating both annotator quality and annotation
difficulty (including the latter is important because
some annotators will have low agreement with oth-
ers due to their poor luck in being assigned difficult-
to-annotate sentences). This algorithm is shown to
outperform using the modal response.
Note that this necessarily biases against annota-
tors with few responses. In our case study above, ex-
cluding annotators who contributed small numbers
of annotations led to progressively worse match to
researcher intuition, suggesting that the loss in data
1441
caused by excluding these annotations may not be
worth the increased confidence in annotation quality.
Future research will be needed to assess this trade-
off.
The above work shows the feasibility of crowd-
sourcing VerbNet semantic entailments, as has been
shown for a handful of other linguistic judgments
(Artignan, Hascoet and Lafourcade, 2009; Poesio et
al., 2012; Venhuizen et al, 2013). There are many
domains in which gold standard human judgments
are scarce; crowd-sourcing has considerable poten-
tial at addressing this need.
References
B. Ambridge, J. M. Pine, C. F. Rowland, F. Chang, and
A. Bidgood. 2013. The retreat from overgeneral-
ization in child language acquisition: Word learning,
morphology, and verb argument structure. Wiley In-
terdisciplinary Reviews: Cognitive Science. 4:47-62.
G. Artignan, M. Hascoet, and M. Lafourcade. 2009.
Mutliscale visual analysis of lexical networks. Pro-
ceedings of the 13th International Conference on In-
formation Visualisation. Barcelona, Spain.
T. Chklovski and P. Pantel. 2004. VerbOcean: Mining
the Web for fine-grained semantic relations. Proceed-
ings of Empirical Methods in Natural Language Pro-
cessing (EMNLP). Barcelona, Spain.
L. Cosmides and J. Tooby. 1992. Cognitive adaptations
for social exchange. in The Adapted Mind. (J. Barkow,
L. Cosmides, and J. Tooby, Eds.) Oxford University
Press, Oxford, UK.
W. Croft. 2012. Verbs: Aspect and Argument Structure.
Oxford University Press, Oxford, UK.
D. R. Dowty. 1991. Thematic proto-roles and argument
selection. Language. 67:547-619.
E. Gibson and E. Fedorenko. 2013. The need for quanti-
tative methods in syntax and semantics research. Lan-
guage and Cognitive Processes. 28(1-2):88?124.
R. Jackendoff. 1990. Semantic Structures. The MIT
Press, Cambridge, MA.
E. Joanis, S. Stevenson, and D. James. 2008. A general
feature space for automatic verb classification. Natu-
ral Language Engineering. 14(3):337-367.
K. Kipper, A. Korhonen, N. Ryant and M. Palmer. 2008.
A large-scale classification of English verbs. Lan-
guage Resources and Evaluation Journal, 42:21?40
E. Margolis and S. Laurence 1999. Concepts: Core
Readings. The MIT Press, Cambridge, MA.
B. Levin. 1993. English Verb Classes and Alternations:
A Preliminary Investigation. University of Chicago
Press, Chicago.
B. Levin and M. Rappaport Hovav. 2005. Argument
Realization. Cambridge University Press, Cambridge,
UK.
D. Maynard, A. Funk, and W. Peters. 2009. Using
lexico-syntactic ontology design patterns for ontology
creation and population. Proceedings of Workshop on
Ontology Patterns (WOP 2009). Washington, DC
R. J. Passonneau and B. Carpenter 2013. The benefits
of a model of annotation. 7th Linguistic Annotation
Workshop and Interoperability with Discourse. Sofia,
Bulgaria.
D. Pesetsky. 1995. Zero Syntax: Experiencers and Cas-
cades. The MIT Press, Cambridge, MA.
S. Pinker. 1989. Learnability and Cognition. The MIT
Press, Cambridge, MA.
M. Poesio, J. Camberlain, U. Kruschwitz, L. Robaldo,
and L. Ducceschi. 2012. The Phrase Detective Multi-
lingual Corpus, Release 0.1. Proceedings of the Col-
laborative Resource Development and Delivery Work-
shop. Istanbul, Turkey
H. Poon and P. Domingos. 2009. Unsupervised seman-
tic parsing. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
Singapore.
A. Rzhetsky, H. Shatkay, and W. J. Wilbur. 2009. How
to get the most out of your curation effort. PLoS Com-
putational Biology, 5(5):1?13.
E. S. Spelke and K. D. Kinzler. 2007. Core knowledge.
Developmental Science, 10(1):89?96.
R. Swier and S. Stevenson. 2004. Unsupervised seman-
tic role labeling. Proceedings of the Generative Lexi-
con Conference, GenLex-09. Pisa, Italy.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. Proceedings of
the 10th International Conference on Computational
Semantics (IWCS 2013). Potsdam, Germany
J. Whitehill, P. Ruvolo, T. F. Wu, J. Bergsma. and J.
Movellan. 2009. Whose vote should count more: Op-
timal integration of labels from labelers of unknown
expertise. Advances in Neural Information Processing
Systems, 22. Vancouver, Canada
A. Zaenen, C. Condoravdi, and D G. Bobrow. 2008. The
encoding of lexical implications in VN. Proceedings
of LREC 2008. Morocco
1442
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 13?16,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Multilingual Propbank Annotation Tools:
Cornerstone and Jubilee
Jinho D. Choi, Claire Bonial, Martha Palmer
University of Colorado at Boulder, USA
{choijd;bonial;mpalmer}@colorado.edu
Abstract
This paper demonstrates two annotation
tools related to Propbank: Cornerstone
and Jubilee. Propbank is a corpus in
which the arguments of each verb pred-
icate are annotated with their semantic
roles. Propbank annotation also requires
the choice of a sense id for each predicate,
defined in the corresponding frameset file.
Jubilee expedites the annotation process
by displaying several resources of syntactic
and semantic information simultaneously;
easy access to each of these resources al-
lows the annotator to quickly absorb and
apply the necessary syntactic and semantic
information pertinent to each predicate for
consistent and efficient annotation. Cor-
nerstone is a user-friendly xml editor, cus-
tomized to allow frame authors to create
and edit frameset files. Both tools have
been successfully adapted to many Prop-
bank projects; they run platform indepen-
dently, are light enough to run as X11 ap-
plications and support multiple languages
such as Arabic, Chinese, English, Hindi
and Korean.
1 Introduction
Propbank is a corpus in which the arguments of
each verb predicate are annotated with their se-
mantic roles (Palmer et al, 2005). Propbank an-
notation also requires the choice of a sense id for
each predicate. Thus, for each predicate in the
Propbank, there exists a corresponding frame-
set file encompassing one or more senses of the
predicate. All frameset files are written in xml,
which is somewhat difficult to read and edit. Al-
though there already exist many xml editors,
most of them require some degree of knowledge
of xml, and none of them are specifically cus-
tomized for frameset files. This motivated the
development of our own frameset editor, Cor-
nerstone.
Jubilee is a Propbank instance editor. For
each verb predicate, we create a Propbank in-
stance that consists of the predicate?s sense id
and its arguments labeled with semantic roles.
Previously the allocation of tasks, the annota-
tion of argument labels and the frameset tagging
were all done as separate tasks. With Jubilee,
the entire annotation procedure can be done us-
ing one tool that simultaneously provides rich
syntactic information as well as comprehensive
semantic information.
Both Cornerstone and Jubilee are developed
in Java (Jdk 6.0), so they run on any plat-
form where the Java virtual machine is installed.
They are light enough to run as X11 applica-
tions. This aspect is important because Prop-
bank data are usually stored in a server, so
annotators need to update them remotely (via
ssh). One of the biggest advantages of using
these tools is that they accommodate several
languages; in fact, the tools have been used
for Propbank projects in Arabic (M.Diab et al,
2008), Chinese (Xue and Palmer, 2009), En-
glish (Palmer et al, 2005) and Hindi, and have
been tested in Korean (Han et al, 2002).
This demo paper details how to create Prop-
bank framesets in Cornerstone, and how to an-
notate Propbank instances using Jubilee. There
are two modes in which to run Cornerstone:
multi-lemma and uni-lemma mode. In multi-
lemma mode, a predicate can have multiple lem-
13
mas, whereas a predicate can have only one
lemma in uni-lemma mode. Jubilee also has
two modes: normal and gold mode. In normal
mode, annotators are allowed to view and edit
only tasks that have been claimed by themselves
or by one other annotator. In gold mode, adju-
dicators are allowed to view and edit all tasks
that have undergone at least single-annotation.
2 How to obtain the tools
Cornerstone and Jubilee are available as an open
source project on Google code.1 The webpage
gives detailed instructions of how to download,
install and launch the tools (Choi et al, 2009a;
Choi et al, 2009b).
3 Description of Cornerstone
3.1 Multi-lemma mode
Languages such as English and Hindi are ex-
pected to run in multi-lemma mode, due to the
nature of their verb predicates. In multi-lemma
mode, a predicate can have multiple lemmas
(e.g., ?run?, ?run out?, ?run up?). The xml struc-
ture of the frameset files for such langauges is
defined in a dtd file, frameset.dtd.
Figure 1 shows what appears when you open
a frameset file, run.xml, in multi-lemma mode.
The window consists of four panes: the frame-
set pane, predicate pane, roleset pane and roles
pane. The frameset pane contains a frameset
note reserved for information that pertains to all
predicate lemmas and rolesets within the frame-
set file. The predicate pane contains one or more
tabs titled by predicate lemmas that may in-
clude verb particle constructions. The roleset
pane contains tabs titled by roleset ids (e.g.,
run.01, run.02, corresponding to different senses
of the predicate) for the currently selected predi-
cate lemma (e.g., ?run?). The roles pane includes
one or more roles, which represent arguments
that the predicate requires or commonly takes
in usage.
3.2 Uni-lemma mode
Languages such as Arabic and Chinese are ex-
pected to run in uni-lemma mode. Unlike multi-
1http://code.google.com/p/propbank/
Figure 1: Open run.xml in multi-lemma mode
lemma mode, which allows a predicate to have
multiple lemmas, uni-lemma mode allows only
one lemma for a predicate. The xml structure
of the frameset files for such langauges is defined
in a dtd file, verb.dtd.
Figure 2: Open HAfaZ.xml in uni-lemma mode
Figure 2 shows what appears when you open a
frameset file, HAfaZ.xml, in uni-lemma mode.
The window consists of four panes: the verb
pane, frameset pane, frame pane and roles pane.
The verb pane contains a verb comment field
for information helpful to annotators about the
verb, as well as the attribute field, ID, which in-
dicates the predicate lemma of the verb, repre-
sented either in the Roman alphabet or charac-
ters in other languages. The frameset pane con-
tains several tabs titled by frameset ids (corre-
sponding to verb senses) for the predicate. The
frame pane contains a frame comment for op-
14
tional information about the frame and the map-
ping pane, which includes mappings between
syntactic constituents and semantic arguments.
The roles pane consists of a set of arguments
that the predicate requires or commonly takes.
4 Description of Jubilee
4.1 Normal mode
Annotators are expected to run Jubilee in
normal mode. In normal mode, annotators
are allowed to view and edit only tasks claimed
by themselves or one other annotator when
the max-number of annotators allowed is two.
Jubilee gives the option of assigning a different
max-number of annotators as well.
When you run Jubilee in normal mode, you
will see an open-dialog (Figure 3). There are
three components in the open-dialog. The
combo-box at the top shows a list of all Prop-
bank projects. Once you select a project (e.g.,
english.sample), both [New Tasks] and [My
Tasks] will be updated. [New Task] shows a
list of tasks that have either not been claimed,
or claimed by only one other annotator. [My
Tasks] shows a list of tasks that have been
claimed by the current annotator.
Figure 3: Open-dialog
Once you choose a task and click the [Enter]
button, Jubilee?s main window will be prompted
(Figure 4). There are three views available in
the main window: the treebank view, frame-
set view and argument view. By default, the
treebank view shows the first tree (in the Penn
Treebank format (Marcus et al, 1993)) in the
selected task. The frameset view displays role-
sets and allows the annotator to choose the sense
of the predicate with respect to the current tree.
The argument view contains buttons represent-
ing each of the Propbank argument labels.
Figure 4: Jubilee?s main window
4.2 Gold mode
Adjudicators are expected to run Jubilee in gold
mode. In gold mode, adjudicators are allowed to
view and edit all tasks that have undergone at
least single-annotation. When you run Jubilee
in gold mode, you will see the same open-dialog
as you saw in Figure. 3. The [New Tasks] shows
a list of tasks that have not been adjudicated,
and the [My Tasks] shows a list of tasks that
have been adjudicated. Gold mode does not al-
low adjudicators to open tasks that have not
been at least single-annotated.
5 Demonstrations
5.1 Cornerstone
We will begin by demonstrating how to view
frameset files in both multi-lemma and uni-
lemma mode. In each mode, we will open an
existing frameset file, compare its interface with
the actual xml file, and show how intuitive it is
to interact with the tool. Next, we will demon-
strate how to create and edit a new frameset file
either from scratch or using an existing frameset
file. This demonstration will reflect several ad-
vantages of using the tool. First, the xml struc-
ture is completely transparent to the frame au-
thors, so that no knowledge of xml is required to
manage the frameset files. Second, the tool au-
tomates some of the routine work for the frame
authors (e.g., assigning a new roleset/frameset
id) and gives lists of options to be chosen (e.g.,
15
a list of function tags) so that frameset creation,
and the entire annotation procedure in turn, be-
come much faster. Third, the tool checks for the
completion of required fields and formatting er-
rors so that frame authors do not have to check
them manually. Finally, the tool automatically
saves the changes so the work is never lost.
5.2 Jubilee
For the treebank view, we will compare Jubilee?s
graphical representation of the trees with the
parenthetical representation of former tools: the
clear visual representation of the phrase struc-
ture helps the annotator to better understand
the syntax of the instance and to annotate the
appropriate node within the correct span. For
the frameset view, we will detail what kind of
semantic information it provides as you choose
different rolesets. This will highlight how Ju-
bilee?s support of roleset id annotation not only
speeds up the annotation process, but also en-
sures consistent annotation because the roleset
information provides a guideline for the correct
annotation of a particular verb sense. For the
argument view, we will illustrate how to anno-
tate Propbank arguments and use the opera-
tors for concatenations and links; thereby also
demonstrating that having each of these labels
clearly visible helps the annotator to remember
and evaluate the appropriateness of each possi-
ble argument label. Finally, we will show how
intuitive it is to adjudicate the annotations in
gold mode.
6 Future work
Both Cornerstone and Jubilee have been suc-
cessfully adapted to Propbank projects in sev-
eral universities such as Brandeis University, the
University of Colorado at Boulder, and the Uni-
versity of Illinois at Urbana-Champaign. We
will continuously develop the tools by improv-
ing their functionalities through user-testing and
feedback, and also by applying them to more
languages.
Acknowledgments
Special thanks are due to Prof. Nianwen Xue of
Brandeis University for his very helpful insights
as well as Scott Cotton, the developer of RATS
and Tom Morton, the developer of WordFreak,
both previously used for PropBank annotation.
We gratefully acknowledge the support of the
National Science Foundation Grants CISE-CRI-
0551615, Towards a Comprehensive Linguistic
Annotation and CISE-CRI 0709167, Collabo-
rative: A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-
C-0022, subcontract from BBN, Inc. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the
authors and do not necessarily reflect the views
of the National Science Foundation.
References
Jinho D. Choi, Claire Bonial, and Martha Palmer.
2009a. Cornerstone: Propbank frameset editor
guideline (version 1.3). Technical report, Institute
of Cognitive Science, the University of Colorado at
Boulder.
Jinho D. Choi, Claire Bonial, and Martha Palmer.
2009b. Jubilee: Propbank instance editor guide-
line (version 2.1). Technical report, Institute of
Cognitive Science, the University of Colorado at
Boulder.
C. Han, N. Han, E. Ko, and M. Palmer. 2002. Ko-
rean treebank: Development and evaluation. In
Proceedings of the 3rd International Conference on
Language Resources and Evaluation.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Com-
putational Linguistics, 19(2):313?330.
M.Diab, A.Mansouri, M.Palmer, O.Babko-Malaya,
W Zaghouani, A.Bies, and M.Maamouri. 2008.
A pilot arabic propbank. In Proceedings of the 7th
International Conference on Language Resources
and Evaluation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71?106.
Nianwen Xue and Martha Palmer. 2009. Adding
semantic roles to the chinese treebank. Natural
Language Engineering, 15(1):143?172.
16
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 397?402,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
The VerbCorner Project: Findings from Phase 1 of Crowd-Sourcing a
Semantic Decomposition of Verbs
Joshua K. Hartshorne
Department of Brain and Cognitive Sciences
Massachusetts Institute of Technology
77 Massachusetts Avenue
Cambridge, MA 02139, USA
jkhartshorne@gmail.com
Claire Bonial, Martha Palmer
Department of Linguistics
University of Colorado at Boulder
Hellems 290, 295 UCB
Boulder, CO 80309, USA
{CBonial, MPalmer}@colorado.edu
Abstract
Any given verb can appear in some syntac-
tic frames (Sally broke the vase, The vase
broke) but not others (*Sally broke at the
vase, *Sally broke the vase to John). There
is now considerable evidence that the syn-
tactic behaviors of some verbs can be pre-
dicted by their meanings, and many cur-
rent theories posit that this is true for most
if not all verbs. If true, this fact would
have striking implications for theories and
models of language acquisition, as well as
numerous applications in natural language
processing. However, empirical investiga-
tions to date have focused on a small num-
ber of verbs. We report on early results
from VerbCorner, a crowd-sourced project
extending this work to a large, representa-
tive sample of English verbs.
1 Introduction
Verbs vary in terms of which syntactic frames they
can appear in (Table 1). In principle, this could be
an unpredictable fact about the verb that must be
acquired, much like the phonological form of the
verb.
However, most theorists posit that there is a sys-
tematic relationship between the semantics of a
verb and the syntactic frames in which it can ap-
pear (Levin and Hovav, 2005). For instance, it
is argued that verbs like break, which describe a
Frame hit like break
NP V NP x x x
NP V - - x
NP that S - x -
NP V at NP x - -
Table 1: Some of the syntactic frames available for
hit, like, and break.
caused change of state, can appear in both the NP
V NP form (Sally broke the vase) and the NP
V form (The vase broke). Verbs such as hit and
like do not describe a change of state and so can-
not appear in both forms.
1
Similarly, only verbs
that describe propositional attitudes, such as like,
can take a that complement (John liked that Sally
broke the vase).
1.1 The Semantic Consistency Hypothesis
This account has a natural consequence, which we
dub the Semantic Consistency Hypothesis: There
is some set of semantic features such that verbs
that share the same syntactic behavior are identi-
cal along those semantic features.
2
Note that on
certain accounts, this is a strong tendency rather
than a strict necessity (e.g., Goldberg, 1995).
It is widely recognized that a principled re-
lationship between syntax and semantics would
have broad implications. It is frequently invoked
in theories of language acquisition. For instance,
Pinker (1984, 1989) has described how this cor-
respondence could solve long-standing puzzles
about how children learn syntax in the first place.
Conversely, Gleitman (1990) has shown such a
syntax-semantics relationship could solve signif-
icant problems in vocabulary acquisition. In fact,
both researchers argue that a principled relation-
ship between syntax and semantics is necessary
for language to be learnable at all.
In computational linguistics and natural lan-
guage processing, some form of the Semantic
Consistency Hypothesis is often included in lin-
guistic resources and utilized in applications. We
1
Note that this is a simplification in that there are non-
causal verbs that appear in both the NP V NP frame and the
NP V frame. For details, see (Levin, 1993).
2
There is a long tradition of partitioning semantics into
those aspects of meaning which are ?grammatically relevant?
and those which are not. We refer the interested reader to
Pinker (1989), Jackendoff (1990), and Levin & Rappaport
Hovav (2005).
397
describe in detail one such resource, VerbNet,
which is highly relevant to our investigation.
1.2 VerbNet
VerbNet (Kipper et al, 2008; based on Levin,
1993) lists over 6,000 verbs, categorized into 280
classes according to the syntactic frames they can
appear in. That is, all verbs in the same class ap-
pear in the same set of syntactic frames. Impor-
tantly, in addition to characterizing the syntactic
frames associated with each class, VerbNet alo
characterizes the semantics of each class.
For instance, class 9.7, which comprises a
couple dozen verbs, allows 7 different syntactic
frames. The entry for one frame is shown below:
Syntactic Frame NP V NP PP.DESTINATION
Example Jessica sprayed the wall.
Syntax AGENT V THEME {+LOC|+DEST CONF}
DESTINATION
Semantics MOTION(DURING(E), THEME)
NOT(PREP(START(E), THEME, DESTINATION))
PREP(END(E), THEME, DESTINATION)
CAUSE(AGENT, E)
Importantly, the semantics listed here is not just
for the verb spray but applies to all verbs from the
Spray Class whenever they appear in that syntac-
tic frame ? that is, VerbNet assumes the Semantic
Consistency Hypothesis.
VerbNet and its semantic features have been
used in a variety of NLP applications, such as se-
mantic role labeling (Swier and Stevenson, 2004),
inferencing (Zaenen et al, 2008), verb classifica-
tion (Joanis et al, 2008), and information extrac-
tion (Maynard et al, 2009). It has also been em-
ployed in models of language acquisition (Parisien
and Stevenson, 2011; Barak et al, 2012). In gen-
eral, there has been interest in the NLP literature
in using these syntactially-relevant semantic fea-
tures for shallow semantic parsing (e.g., Giuglea
and Moschitti, 2006).
2 Empirical Status of the Semantic
Consistency Hypothesis
Given the prominence of the Semantic Consis-
tency Hypothesis in both theory and practice, one
might expect that it was on firm empirical foot-
ing. That is, ideally there would be some database
of semantic judgments for a comprehensive set
of verbs from each syntactic class. In princi-
ple, these judgments would come from naive an-
notators, since researchers? intuitions about sub-
tle judgments may be unconsciously clouded by
theoretical commitments (Gibson and Fedorenko,
2013). The Semantic Consistency Hypothesis
would be supported if, within that database, predi-
cates with the same syntactic properties were sys-
tematically related semantically.
No such database exists, whether consisting of
the judgments of linguists or naive annotators.
Most theoretical studies report researcher judg-
ments for only a handful of examples; how many
additional examples were considered by the re-
searcher goes unreported. In any case, to our
knowledge, of the 280 syntactic verb classes listed
by VerbNet, only a handful have been studied in
any detail.
The strongest evidence comes from experimen-
tal work on several so-called alternations (the pas-
sive, causative, locative, and dative alternations).
Here, there does appear to be a systematic seman-
tic distinction between the two syntactic frames in
each alternation, at least most of the time. This
has been tested with a reasonable sample of the
relevant verbs and also in both children and adults
(Ambridge et al, 2013; Pinker, 1989). However,
the relevant verbs make up a tiny fraction of all
English verbs, and even for these verbs, the syn-
tactic frames in question represent only a fraction
of the syntactic frames available to those verbs.
This is not an accidental oversight. The limit-
ing factor is scale: with many thousands of verbs
and over a hundred commonly-discussed seman-
tic features and syntactic frames, it is not feasi-
ble for a single researcher, or even team of re-
searchers, to check which verbs appear in which
syntactic frames and carry which semantic en-
tailments. Collecting data from naive subjects is
even more laborious, particularly since the aver-
age Man on the Street is not necessarily equipped
with metalinguistic concepts like caused change of
state and propositional attitude. The VerbCorner
Project is aimed at filling that empirical gap.
3 VerbCorner
The VerbCorner Project
3
is devoted to collecting
semantic judgments for a comprehensive set of
verbs along a comprehensive set of theoretically-
relevant semantic dimension. These data can be
used to test the Semantic Consistency Hypothesis.
3
http://gameswithwords.org/VerbCorner/
398
Independent of the validity of that hypothesis, the
semantic judgments themselves should prove use-
ful for any study of linguistic meaning or related
application.
We address the issue of scale through crowd-
sourcing: Recruiting large numbers of volunteers,
each of whom may provide only a few annota-
tions. Several previous projects have success-
fully crowd-sourced linguistic annotations, such
as Phrase Detectives, where volunteers have con-
tributed 2.5 million judgments on anaphoric rela-
tions (Poesio et al, 2012).
3.1 Integration with VerbNet
One significant challenge for any such project is
first classifying verbs according to the syntactic
frames they can appear in. Thus, at least initially,
we are focusing on the 6,000+ verbs already cata-
loged in VerbNet. As such, the VerbCorner Project
is also verifying and validating the semantics cur-
rently encoded in VerbNet. VerbNet will be edited
as necessary based on the empirical results.
Integration with VerbNet has additional bene-
fits, since VerbNet itself is integrated with a vari-
ety of linguistic resources, such as PropBank and
Penn TreeBank. This amplifies the impact of any
VerbCorner-inspired changes to VerbNet.
3.2 The Tasks
We selected semantic features of interest based on
those most commonly cited in the linguistics lit-
erature, with a particular focus on those that ? ac-
cording to VerbNet ? apply to many predicates.
Previous research has shown that humans find
it easier to reason about real-world scenarios than
make abstract judgments (Cosmides and Tooby,
1992). Thus, for each feature (e.g., MOVEMENT),
we converted the metalinguistic judgment (?Does
this verb entail movement on the part of some en-
tity??) into a real-world problem.
For example, in ?Simon Says Freeze,? a task
designed to elicit judgments about movement, the
Galactic Overlord (Simon) decrees ?Galactic Stay
Where You Are Day,? during which nobody is al-
lowed to move from their current location. Par-
ticipants read descriptions of events and decide
whether anyone violated the rule.
In ?Explode on Contact,? designed to elicit
judgments about physical contact, objects and
people explode when they touch one another. The
participant reads descriptions of events and de-
cides whether anything has exploded.
Note that each task is designed to elicit judg-
ments about entailments ? things that must be true
rather than are merely likely to be true. If John
greeted Bill, they might have come into contact
(e.g., by shaking hands), but perhaps they did not.
Previous work suggests that it is the semantic en-
tailments that matter, particularly for explaining
the syntactic behavior of verbs (Levin, 1993).
3.3 The Items
The exact semantics associated with a verb may
depend on its syntactic frame. Thus Sally rolled
the ball entails that somebody applied force to the
ball (namely: Sally), whereas The ball rolled does
not. Thus, we investigate the semantics of each
verb in each syntactic frame available to it (as de-
scribed by VerbNet). Below, the term item is the
unit of annotation: a verb in a frame.
In order to minimize unwanted effects of world
knowledge, the verb?s arguments are replaced with
nonsense words or randomly chosen proper names
(Sally sprayed the dax onto the blicket). The use
of novel words is explained by the story for each
task.
3.4 The Phases
Given the sheer scale of the project, data-
collection is expected to take several years at least.
Thus, data-collection has been broken up into a se-
ries of phases. Each phase focuses on a small num-
ber of classes and/or semantic entailments. This
ensures that there are meaningful intermediate re-
sults that can be disseminated prior to the comple-
tion of the entire project. This manuscript reports
the results of Phase 1.
4 Results
The full data and annotations will be released in
the near future and may be available now by re-
quest. Below, we summarize the main findings
thus far.
4.1 Description of Phase 1
In Phase 1 of the project, we focused on 11 verb
classes (Table 3) comprising 641 verbs and seven
different semantic entailments (Table 2). While
six of these entailments were chosen from among
those features widely believed to be relevant for
syntax, one was not: A Good World, which inves-
tigated evaluation (Is the event described by the
verb positive or negative?). Although evaluation
399
Task Semantic Feature Anns. Anns./Item Mode Consistency
Entropy PHYSICAL CHANGE 23,875 7 86% 95%
Equilibrium APPLICATION OF FORCE 27,128 8 79% 95%
Explode on Contact PHYSICAL CONTACT 23,590 7 93% 95%
Fickle Folk CHANGE OF MENTAL STATE 16,466 5 81% 96%
Philosophical Zombie Hunter MENTAL STATE 24,592 7 80% 89%
Simon Says Freeze LOCATION CHANGE 24,245 7 83% 88%
A Good World EVALUATION 22,668 7 72% 74%
Table 2: Respectively: Task, semantic feature tested, number of annotations, mean number of annotations
per item, mean percentage of participants choosing the modal response, consistency within class.
of events is an important component of human
psychology, to our knowledge no researcher has
suggested that it is relevant for syntax. As such,
this task provides a lower bound for how much se-
mantic consistency one might expect within a syn-
tactic verb class.
In all, we collected 162,564 judgments from
1,983 volunteers (Table 2).
4.2 Inter-annotator Agreement
Each task had been iteratively piloted and re-
designed until inter-annotator reliability was ac-
ceptable, as described in a previous publication.
However, these pilot studies involved a small num-
ber of items which were coded by all annota-
tors. How good was the reliability in the crowd-
sourcing context?
Because we recruited large numbers of an-
notators, most of whom annotated only a few
items, typical measures of inter-annotator agree-
ment such as Cohen?s kappa are not easily calcu-
lated. Instead, for each item, we calculated the
most common (modal) response. We then con-
sidered what proportion of all annotations were
accounted for by the modal response: a mean of
100% would indicate that there was no disagree-
ment among annotators for any item.
As can be seen in Table 2, for every task, the
modal response covered the bulk responses, rang-
ing from a low of 72% for EVALUATION to a high
of 93% for PHYSICAL CONTACT. Since there
were typically 4 or more possible answers per
item, inter-annotator agreement was well above
chance. This represents good performance given
that the annotators were entirely untrained.
In many cases, annotator disagreement seems
to be driven by syntactic constructions that are
only marginally grammatical. For instance, inter-
annotator agreement was typically low for class
63. VerbNet suggests two syntactic frames for
class 63, one of which (NP V THAT S) appears to
be marginal (?I control that Mary eats). In fact,
annotators frequently flagged these items as un-
grammatical, which is a valuable result in itself for
improving VerbNet.
Class Examples PChange Force Contact MChange Mental LChange
12 yank, press - x d - - d
18.1 hit, squash d x d - - d
29.5 believe, conjecture - - - - d -
31.1 amuse, frighten - - - x d -
31.2 like, fear - - - - x -
45.1 break, crack x d d - - d
51.3.1 bounce, roll - d d - - d
51.3.2 run, slink - d - - - d
51.6 chase, follow - - - - - d
61 attempt, try - - - - - -
63 control, enforce - - - - - -
Table 3: VerbNet classes investigated in Phase 1, with presence of semantic entailments as indicated by
data. x = feature present; - = feature absent; d = depends on syntactic frame.
400
4.3 Testing the Semantic Consistency
Hypothesis
4.3.1 Calculating consistency
We next investigated whether our results support
the Semantic Consistency Hypothesis. As noted
above, the question is not whether all verbs in the
same syntactic class share the same semantic en-
tailments. Even a single verb may have different
semantic entailments when placed in different syn-
tactic frames. Thus, calculating consistency of a
class must take differing frames into account.
There are many sophisticated rubrics for calcu-
lating consistency. However, for expository pur-
poses here, we use one that is intuitive and easy
to interpret. First, we determined the annotation
for each item (i.e., each verb/frame combination)
by majority vote. We then considered how many
verbs in each class had the same annotation in any
given syntactic frame.
For example, suppose a class had 10 verbs and
2 frames. In the first frame, 8 verbs received the
same annotation and 2 received others. The con-
sistency for this class/frame combination is 80%.
In the second frame, 6 verbs received the same
annotation and 4 verbs received others. The con-
sistency for this class/frame combination is 60%.
The consistency for the class as a whole is the av-
erage across frames: 70%.
4.3.2 Results
Mean consistency averaged across classes is
shown for each task in Table 2. As expected,
consistency was lowest for EVALUATION, which
is not expected to necessarily correlate with syn-
tax. Interestingly, consistency for EVALUATION
was nonetheless well above floor. This is per-
haps not surprising: two sentences that have the
same values for PHYSICAL CHANGE, APPLICA-
TION OF FORCE, PHYSICAL CONTACT, CHANGE
OF MENTAL STATE, MENTAL STATE, and LO-
CATION CHANGE are, on average, also likely to
be both good or both bad.
Consistency was much higher for the other
tasks, and in fact was close to ceiling for most of
them. It remains to be seen whether the items that
deviate from the mode represent true differences in
semantics or reflect merely noise. One way of ad-
dressing this question is to collect additional anno-
tations for those items that deviate from the mode.
4.4 Verb semantics
For each syntactic frame in each class, we deter-
mined the most common annotation. This is sum-
marized in Table 3. The semantic annotation de-
pended on syntactic frame nearly 1/4 of the time.
4
These frequently matched VerbNet?s seman-
tics, though not always. For instance, annota-
tors judged that class 18.1 verbs in the NP V NP
PP.INSTRUMENT entailed movement on the part
of the instrument (Sally hit the ball with the stick)
? something not reflected in VerbNet.
5 Conclusion and Future Work
Results of Phase 1 provide support for the Seman-
tic Consistency Hypothesis, at least as a strong
bias. More work will be needed to determine the
strength of that bias. The findings are largely con-
sistent with VerbNet?s semantics, but changes are
indicated in some cases.
We find that inter-annotator agreement is suf-
ficiently high that annotation can be done effec-
tively using the modal response with an average
of 6-7 responses per item. We are currently in-
vestigating whether we can achieve better reliabil-
ity with fewer responses per item by taking into
account an individual annotator?s history across
items, as recent work suggests is possible (Passon-
neau and Carpenter, 2013; Rzhetsky et al, 2009;
Whitehill et al, 2009).
Thus, crowd-sourcing VerbNet semantic entail-
ments appears to be both feasible and productive.
Data-collection continues. Phase 2, which added
over 10 new verb classes, is complete. Phase 3,
which includes both new classes and new entail-
ments, has been launched.
Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation Grant NSF-IIS-
1116782, DARPA Machine Reading FA8750-09-
C-0179, and funding from the Ruth L. Kirschstein
National Research Service Award. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
4
Note that this table was calculated based on whether the
semantic feature was present or not. In many cases, the data
was significantly richer. For instance, for APPLICATION OF
FORCE, annotators determined which participant in the event
was applying the force.
401
References
Ben Ambridge, Julian Pine, Caroline Rowland,
Franklin Chang, and Amy Bidgood. 2013. The re-
treat from overgeneralization in child language ac-
quisition: word learning, morphology and verb ar-
gument structure. Wiley Interdisciplinary Reviews:
Cognitive Science, 4(1):47?62.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2012. Modeling the acquisition of mental
state verbs. In Proceedings of the 3rd Workshop on
Cognitive Modeling and Computational Linguistics,
pages 1?10. Association for Computational Linguis-
tics.
Leda Cosmides and John Tooby. 1992. Cognitive
adaptations for social exchange. The Adapted Mind,
pages 163?228.
Edward Gibson and Evelina Fedorenko. 2013. The
need for quantitative methods in syntax and seman-
tics research. Language and Cognitive Processes,
28(1-2):88?124.
Ana-Maria Giuglea and Alessandro Moschitti. 2006.
Shallow semantic parsing based on framenet, verb-
net and propbank. In Proceedings of the 217th
European Conference on Artificial Intelligence,
pages 563?567, Amsterdam, The Netherlands, The
Netherlands. IOS Press.
Lila Gleitman. 1990. The structural sources of verb
meanings. Language Acquisition, 1(1):3?55.
Adele E. Goldberg. 1995. Constructions: A Construc-
tion Grammar approach to argument structure. Uni-
versity of Chicago Press.
Eric Joanis, Suzanne Stevenson, and David James.
2008. A general feature space for automatic
verb classification. Natural Language Engineering,
14(3):337?367.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Cambridge University Press.
Beth Levin. 1993. English Verb Classes and Alter-
nations: A preliminary Investigation. University of
Chicago press.
Diana Maynard, Adam Funk, and Wim Peters. 2009.
Using lexico-syntactic ontology design patterns for
ontology creation and population. In Proc. of the
Workshop on Ontology Patterns.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
learned verb classes. In Proceedings of the 33rd An-
nual Meeting of the Cognitive Science Society. Cite-
seer.
Rebecca J Passonneau and Bob Carpenter. 2013. The
benefits of a model of annotation. In Proceedings of
the 7th Linguistic Annotation Workshop and Inter-
operability with Discourse, pages 187?195.
Steven Pinker. 1984. Language Learnability and Lan-
guage Development. Harvard University Press.
Steven Pinker. 1989. Learnability and Cognition: The
Acquisition of Argument Structure. MIT Press.
Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,
Livio Robaldo, and Luca Ducceschi. 2012. The
phrase detective multilingual corpus, release 0.1. In
Collaborative Resource Development and Delivery
Workshop Programme, page 34.
Andrey Rzhetsky, Hagit Shatkay, and W John Wilbur.
2009. How to get the most out of your curation ef-
fort. PLoS Computational Biology, 5(5):1?13.
Robert S Swier and Suzanne Stevenson. 2004. Un-
supervised semantic role labeling. In Proceedings
of the Generative Lexicon Conference, volume 95,
page 102.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier R Movellan. 2009. Whose vote
should count more: Optimal integration of labels
from labelers of unknown expertise. In Advances in
Neural Information Processing Systems, volume 22,
pages 2035?2043.
Annie Zaenen, Daniel G Bobrow, and Cleo Condo-
ravdi. 2008. The encoding of lexical implications in
verbnet: Predicates of change of locations. In Lan-
guage Resources Evaluation Conference.
402
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 82?90,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
PropBank Annotation of Multilingual Light Verb Constructions 
 
 
Jena D. Hwang1, Archna Bhatia3, Clare Bonial1, Aous Mansouri1,  
Ashwini Vaidya1, Nianwen Xue2, and Martha Palmer1 
1Department of Linguistics, University of Colorado at Boulder, Boulder CO 80309 
2Department of Computer Science, Brandeis University, Waltham MA 02453 
3Department of Linguistics, University of Illinois at Urbana-Champaign, Urbana IL 61801 
{hwangd,claire.bonial,aous.mansouri,ashwini.vaidya,martha.palmer} 
@colorado.edu, bhatia@illinois.edu, xuen@brandeis.edu 
 
  
 
Abstract 
In this paper, we have addressed the task 
of PropBank annotation of light verb 
constructions, which like multi-word 
expressions pose special problems. To 
arrive at a solution, we have evaluated 3 
different possible methods of annotation. 
The final method involves three passes: 
(1) manual identification of a light verb 
construction, (2) annotation based on the 
light verb construction?s Frame File, and 
(3) a deterministic merging of the first 
two passes. We also discuss how in 
various languages the light verb 
constructions are identified and can be 
distinguished from the non-light verb 
word groupings.  
1 Introduction  
One of the aims in natural language processing, 
specifically the task of semantic role labeling 
(SRL), is to correctly identify and extract the 
different semantic relationships between words 
in a given text. In such tasks, verbs are 
considered important, as they are responsible for 
assigning and controlling the semantic roles of 
the arguments and adjuncts around it. Thus, the 
goal of the SRL task is to identify the arguments 
of the predicate and label them according to their 
semantic relationship to the predicate (Gildea 
and Jurafsky, 2002; Pradhan et al, 2003).  
To this end, PropBank (Palmer et. al., 2005) 
has developed semantic role labels and labeled 
large corpora for training and testing of 
supervised systems. PropBank identifies and 
labels the semantic arguments of the verb on a 
verb-by-verb basis, creating a separate Frame 
File that includes verb specific semantic roles to 
account for each subcategorization frame of the 
verb. It has been shown that training supervised 
systems with PropBank?s semantic roles for 
shallow semantic analysis yield good results (see 
CoNLL 2005 and 2008).  
However, semantic role labeling tasks are 
often complicated by multiword expressions 
(MWEs) such as idiomatic expressions (e.g., 
?Stop pulling my leg!?), verb particle 
constructions (e.g., ?You must get over your 
shyness.?), light verb constructions (e.g., ?take a 
walk?, ?give a lecture?), and other complex 
predicates (e.g., V+V predicates such as Hindi?s 
???? ??? nikal gayaa, lit. ?exit went?, means 
?left? or ?departed?). MWEs that involve verbs 
are especially challenging because the 
subcategorization frame of the predicate is no 
longer solely dependent on the verb alone. 
Rather, in many of these cases the argument 
structure is assigned by the union of two 
predicating elements. Thus, it is important that 
the manual annotation of semantic roles, which 
will be used by automatic SRL systems, define 
and label these MWEs in a consistent and 
effective manner. 
In this paper we focus on the PropBank 
annotation of light verb constructions (LVCs). 
We have developed a multilingual schema for 
annotating LVCs that takes into consideration the 
similarities and differences shared by the 
construction as it appears in English, Arabic, 
Chinese, and Hindi. We also discuss in some 
detail the practical challenges involved in the 
crosslinguistic analysis of LVCs, which we hope 
will bring us a step closer to a unified 
crosslinguistic analysis.    
Since NomBank, as a companion to 
PropBank, provides corresponding semantic role 
82
labels for noun predicates (Meyers et al, 2004), 
we would like to take advantage of NomBank?s 
existing nominalization Frame Files and 
annotations as much as possible.  A question that 
we must therefore address is, ?Are 
nominalization argument structures exactly the 
same whether or not they occur within an LVC?? 
as will be discussed in section 6.1. 
2 Identifying Light Verb Constructions 
Linguistically LVCs are considered a type of a 
complex predicate. Many studies from differing 
angles and frameworks have characterized 
complex predicates as a fusion of two or more 
predicative elements. For example, Rosen (1997) 
treats complex structures as complementation 
structures, where the argument structure of 
elements in a complex predicate are fused 
together.  Goldberg (1993) takes a constructional 
approach to complex predicates and arrives at an 
analysis that is comparable to viewing complex 
predicates as a single lexical item. Similarly, 
Mohanan (1997) assumes different levels of 
linguistic representation for complex predicates 
in which the elements, such as the noun and the 
light verb, functionally combine to give a single 
clausal nucleus. Alsina (1997) and Butt (1997) 
suggest that complex predicates may be formed 
by syntactically independent elements whose 
argument structures are brought together by a 
predicate composition mechanism.  
While there is no clear-cut definition of LVCs, 
let alne the whole range of complex predicates, 
for the purposes of this study, we have adapted 
our approach largely from Butt?s (2004) criteria 
for defining LVCs. LVCs are characterized by a 
light verb and a predicating complement 
(henceforth, true predicate) that ?combine to 
predicate as a single element.? (Ibid.) In LVC, 
the verb is considered semantically bleached in 
such a way that the verb does not hold its full 
predicating power. Thus, the light verb plus its 
true predicate can often be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression. For example, 
the light verb ?gave? and the predicate ?lecture? 
in ?gave a lecture?, together form a single 
predicating unit such that it can be paraphrased 
by ?lectured?. 
True predicates in LVCs can be a noun (the 
object of the verb or the object of the preposition 
in a prepositional phrase), an adjective, or a verb. 
One light verb plus true predicate combination 
found commonly across all our PropBank 
languages (i.e., English, Arabic, Chinese, and 
Hindi) is the noun as the object of the verb as in 
?Sara took [a stroll] along the beach?. In Hindi, 
true predicates can be adjectives or verbs, in 
addition to the nouns. 
??? ? ??? [?????]  ???         (Adjective) 
to-me  you [nice]  seem 
lit. ?You seem nice to me? 
'You (are) liked to me (=I like you).' 
?????  ?? ???  [??] ????   (Verb) 
I-ERG everything  [do] took 
lit. ?I took do everything? 
'I have done everything.' 
As for Arabic, the LVCs come in verb+noun 
pairings. However, they surface in two syntactic 
forms. It can either be the object of the verb just 
like in English: 
 
 ???? ????]?????? ] ????? ??  
gave.he Georges [lecture] PREP Lebanon 
lit.'Georges gave a lecture about Lebanon' 
?Georges lectured about Lebanon? 
or the complement can be the object of a 
preposition: 
 
 ??????]????? ]????? ????? 
conduct.I [PREP-visit] our.saint Ilias 
lit. ?I will conduct with visit Saint Ilias?s? 
?I will visit Saint Ilias?s? 
3 Standard PropBank  
Annotation Procedure 
The PropBank annotation process can be broken 
down into two major steps: creation of the Frame 
Files for verbs occurring in the data and 
annotation of the data using the Frame Files. 
During the creation of the Frame Files, the 
usages of the verbs in the data are examined by 
linguists (henceforth, ?framers?). Based on these 
observations, the framers create a Frame File for 
each verb containing one or more framesets, 
which correspond to coarse-grained senses of the 
predicate lemma. Each frameset specifies the 
PropBank labels (i.e., ARG0, ARG1,?ARG5) 
corresponding to the argument structure of the 
verb. Additionally, illustrative examples are 
included for each frameset, which will later be 
referenced by the annotators. These examples 
also include the use of the ARGM labels. 
Thus, the framesets are based on the 
examination of the data, the framers? linguistic 
knowledge and native-speaker intuition. At 
83
times, we also make use of the syntactic and 
semantic behavior of the verb as described by 
certain lexical resources. These resources include 
VerbNet (Kipper et. al., 2006) and FrameNet 
(Baker et. al., 1998) for English, a number of 
monolingual and bilingual dictionaries for 
Arabic, and Hindi WordNet and DS Parses 
(Palmer et. al., 2009) for Hindi. Additionally, if 
available, we consult existing framesets of words 
with similar meanings across different languages. 
The data awaiting annotation are passed onto 
the annotators for a double-blind annotation 
process using the previously created framesets. 
The double annotated data is then adjudicated by 
a third annotator, during which time the 
differences of the two annotations are resolved to 
produce the Gold Standard. 
Two major guiding considerations during the 
framing and annotating process are data 
consistency and annotator productivity. During 
the frameset creation process, verbs that share 
similar semantic and syntactic characteristics are 
framed similarly. During the annotation process, 
the data is organized by verbs so that each verb is 
tackled all at once. In doing so, we firstly ensure 
that the framesets of similar verbs, and in turn, 
the annotation of the verbs, will both be 
consistent across the data. Secondly, by tackling 
annotation on verb-by-verb basis, the annotators 
are able to concentrate on a single verb at a time, 
making the process easier and faster for the 
annotators. 
4 Annotating LVC 
A similar process must be followed when 
annotating light verb constructions The first step 
is to create consistent Frame Files for light verbs. 
Then in order to make the annotation process 
produce consistent data at a reasonable speed, we 
have decided to carry out the light verb 
annotation in three passes (Table 1):  (1) annotate 
the light verb, (2) annotate the true predicate, and 
(3) merge the two annotations into one. 
The first pass involves the identification of the 
light verb. The most important parts of this step 
are to identify a verb as having bleached 
meaning, thereafter assign a generic light verb 
frameset and identify the true predicating 
expression of the sentence, which would be 
marked with ARG-PRX (i.e., ARGument-
PRedicating eXpression). For English, for 
example, annotators were instructed to use Butt?s 
(2004) criteria as described in Section 2. These 
criteria required that annotators be able to 
recognize whether or not the complement of a 
potential light verb was itself a predicating 
element. To make this occasionally difficult 
judgment, annotators used a simple heuristic test 
of whether or not the complement was headed by 
an element that has a verbal counterpart.  If so, 
the light verb frameset was selected. 
The second pass involves the annotation of the 
sentence with the true predicate as the relation. 
During this pass, the true predicate is annotated 
with an appropriate frameset. In the third pass, 
the arguments and the modifiers of the two 
previous passes are reconciled and merged into a 
single annotation. In order to reduce the number 
of hand annotation, it is preferable for this last 
pass, the Pass 3, to be done automatically. 
Since the nature of the light verb is different 
from that of other verbs as described in Section 
2, the advantage of doing the annotation of the 
light verb and the true predicate on separate 
passes is that in the light verb pass the annotators 
will be able to quickly dispose of the verb as a 
light verb and in the second pass, they will be 
allowed to solely focus on the annotation of the 
light verb?s true predicate. 
The descriptions of how the arguments and 
modifiers of the light verbs and their true 
predicates are annotated are mentioned in Table 
1, but notably, none of the examples in it 
currently include the annotation of arguments 
 Pass 1: Pass 2: Pass 3: 
 Light Verb Annotation True Predicate Annotation Merge of Pass1&2 Annotation 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
and 
Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the light verb are annotated 
- Arguments and modifiers of 
the true predicate are annotated 
- Arguments and modifiers 
found in the two passes are 
merged, preferably 
automatically. 
Frameset Light verb frameset True predicate?s frameset LVC?s frameset 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG-MNR: brisk  
REL: walk 
REL: took walk 
ARG-MNR: brisk 
Table 1. Preliminary Annotation Scheme 
84
and modifiers.  This is intentional, as coming to 
an agreement concerning the details of what 
exactly each of the three passes looks like while 
meeting the needs of the four PropBank 
languages is quite challenging. Thus, for the rest 
of the paper we will discuss the strengths and 
weaknesses of the two trial methods of 
annotation we have considered and discarded in 
Section 5, as well as the final annotation scheme 
we chose in Section 6. 
5 Trials 
5.1 Method 1 
As our first attempt, the annotation of argument 
and adjuncts was articulated in the following 
manner (Table 2). 
Pass 1: Pass 2: 
Light verb True predicate 
- Predicating expression 
is labeled ARG-PRX 
- Annotate the Subject 
argument of the light 
verb as the Arg0. 
- Annotate the rest of the 
arguments and modifiers 
of the light verb with 
ARGM labels. 
- Annotate arguments 
and modifiers of the 
true predicate within 
its domain of locality. 
Generic light verb Frame 
File 
True predicate?s 
Frame File 
?-RKQ WRRN D EULVN ZDON WKURXJK WKH SDUN? 
ARG0: John 
REL: took 
ARG-PRX: a brisk walk 
ARG-DIR: through the park 
ARG-MNR: brisk  
REL: walk 
Table 2. Method 1 for annotation for Passes 1 and 2. 
Revised information is in italics. 
In Pass 1, in addition to annotating the 
predicating expression of the light verb with 
ARG-PRX, the subject argument was marked 
with an ARG0. The choice of ARG0, which 
corresponds to a proto-typical agent, was guided 
by the observation that English LVCs tend to 
lend a component of agentivity to the subject 
even in cases where the true predicate would not 
necessarily assign an agent as its subject. The 
rest of the arguments and modifiers were labeled 
with corresponding ARGM (i.e., modifier) 
labels. The assumption here is that the arguments 
of the light verb will also be the arguments of the 
true predicate.   
In Pass 2, then, the annotation of the 
arguments of the true predicate was restricted to 
its domain of locality (i.e., the span of the ARG-
PRX as marked in Pass1). That is, in the example 
?John took a brisk walk through the park?, the 
labeled spans for the true predicate would be 
limited to the NP ?a brisk walk? and neither 
?John? nor through the park? would be annotated 
as the arguments of the true predicate ?walk?. 
Frame Files: This method would require three 
Frame Files: a generic light verb Frame File, a 
true predicate Frame File, and an LVC Frame 
File. The Frame File for the light verb would not 
be specific to the form of the light verb (e.g., 
same frame for take and make). Rather, it would 
indicate a skeletal argument structure in order to 
reduce the amount of Frame Files made, 
including only Arg0 as its argument1.  
5.2 Weakness of Method 1 
This method has one glaring problem: the 
assumption that the semantic roles of the 
arguments as assigned by the light verb 
uniformly coincide with those assigned by the 
true predicate does not always hold. Consider the 
following English sentence2. 
whether Wu Shu-Chen would make another 
[appearance] in court was subject to observation 
In this example, ?Wu Shu-Chen? is the agent 
argument (Arg0) of the light verb ?make? and is 
the theme or patient argument (Arg1) of a typical  
?appearance? event. Also consider the following 
example from Hindi.  
It is possible that in a light verb construction, 
the light verb actually modifies the standard 
underlying semantics of a nominalization like 
appearance.  In any event, we cannot assume that 
the expected argument labels for the light verb 
and for the standard interpretation of the 
nominalization will always coincide. Thus, we 
could say that Pass 2?s true predicate annotation 
is only partial and is not representative of the 
complete argument structure. In particular, we 
are left with a very difficult merging problem, 
because the argument labels of the two separate 
passes conflict as seen in the above examples. 
5.3 Method 2 
In order to remedy the problem of conflicting 
argument labels, we revised Method 1?s Pass 2 
annotation scheme. This is shown in Table 3. 
Pass 1 remains unchanged from Method 1. 
In this method, both the light verb and the true 
predicate of the sentence receive complete sets of 
                                                          
1 This is why the rest of the argument/modifiers would be 
annotated using ARGM modifier labels. 
2  The light verb is in boldface, the true predicate is in bold 
and square brackets, and the argument/adjunct under 
consideration is underlined. 
85
argument and modifier labels. In Pass 2, the 
limitation of annotating within the domain of 
locality is removed. That is, the arguments and 
modifiers inside and outside the true predicate?s 
domain of control are annotated with respect to 
their semantic relationship to the true predicate 
(e.g., in the English example of Section 5.2, ?Wu 
Shu-Chen? would be considered ARG1 of 
?appearance?).  
Frame Files: This method would also require 
three Frame Files. The major difference is that 
with this method the Frame File for the true 
predicate includes arguments that are sisters to 
the light verb.  
5.4 Weaknesses of Method 2 
If in Method 1 we have committed the error of 
semantic unfaithfulness due to omission, in 
Method 2 we are faced with the problem of 
including too much. In the following sentence, 
consider the role of the underlined adjunct: 
A New York audience ? gave it a big round 
of applause when the music started to play. 
By the annotation in Method 2, the underlined 
temporal adjunct ?when the music started to 
play? is labeled as both the argument of ?give? 
and of ?applause?. The question here is does the 
argument apply to both the giving and the 
applauding event? In other words, does the 
adjunct play an equal role in both passes?  
 Since it could be easily said that the temporal 
phrase applies to both the applauding and the 
giving of the applause events, this example may 
not be particularly compelling. However, what if 
a syntactic complement of the light verb is a 
semantic argument of the true predicate and the 
true predicate only? This is seen more frequently 
in the cases where the light verb is less bleached 
than in the case of ?give? above. Consider the 
following Arabic example. 
 
 ????? ??]???????? ] ????? ????????? ??????? ??????  
took.we PREP DEF-consideration PREP 
prepertations.our possibility sustain.their losses 
?We took into [consideration] during our prepa-
rations the possibility of them sustaining losses? 
 
Here, even though the constituent ?of them 
sustaining losses? is the syntactic complement of 
the verb ?to take;? semantically, it modifies only 
the nominal object of the PP ?consideration.?  
There are similar phenomena in Chinese light 
verb constructions. Syntactic modifiers of the 
light verb are semantic arguments of the true 
predicate, which is usually a nominalization that 
serves as its complement.  
 
?? ?  ?    ? ? ??    [??]    ?? ? 
we now regarding this CL issue [conduct] discussion. 
lit.?We are conducting a discussion on this issue.? 
 ?We are discussing this issue.? 
 
The prepositional phrase ????? ?regarding 
this issue? is a sister to the light verb but 
semantically it is an argument of the nominalized 
predicate ?? ?discussion?. 
The logical next question would be: does the 
annotation of the arguments, adjuncts and 
modifiers have to be all or nothing? It could 
conceivably be possible to assign a selected set 
of arguments at the light verb or true predicate 
level. For example, in the Chinese sentence, the 
modifier ?regarding this CL issue?, though a 
syntactic adjunct to the light verb, could be left 
out from the semantic annotation in Pass 1 and 
included only in the Pass 2. 
However, the objection to this treatment 
comes from a more practical need. As mentioned 
above, in order to keep the manual annotation to 
a minimum, it would be necessary to keep Pass 3 
completely deterministic. As is, with the 
unmodified Method 2, there would be the need to 
choose between Pass 1 or Pass 2 annotation to 
when doing the automatic Pass 3. If we modify 
Method 2 by annotating only a selected set of 
syntactic arguments for the light verb or the true 
predicate, then this issue is exacerbated. In such 
a case there we would have to develop with strict 
rules for which arguments of which pass should 
be included in Pass 3. Pass 3 would no longer be 
automatic, and should be done manually.  
Pass 2: 
True predicate 
- Annotate the Subject argument of the light verb 
with the appropriate role of the true predicate 
- Annotate arguments and modifiers of the true 
predicate without limitation as to the domain of 
locality. 
True predicate?s Frame File 
?+H PDGH DQRWKHU DSSHDUDQFH DW WKH SDUW\? 
ARG1: He 
ARG-ADV: another 
REL: appearance 
ARG-DIR: at court 
Table 3. Method 2 for annotation for Pass 2. Pass 
1 as presented in Table 2 remains unchanged. 
Revised information for Pass 2 is in italics 
 
86
6 Final Annotation Scheme 
6.1 Semantic Fidelity 
Many of the objections so far to Methods 1 and 2 
have centered on the issue of semantic fidelity 
during the annotation of each of the two passes. 
The debate of whether both passes should be 
annotated and to what extent has practical 
implications for the third Pass, as described 
above. However, more importantly it comes 
down to whether or not the semantics of the final 
light verb plus true predicate combination is 
indeed distinct from the semantics of its parts 
(i.e. light verb and true predicate, separately). 
This may be a fascinating linguistic question, but 
it is not something our annotators can be 
debating for each and every instance.   
Instead, we argue that the semantic argument 
structure of the light verb plus true predicate 
combination can in practice be different from 
that of the expressions taken independently as 
has been proposed by various studies (Butt, 
2004; Rosen, 1997; Grimshaw & Mester, 1988). 
Thus, we resolve the cases in which the 
differences in argument roles as assigned by the 
light verb and the nominalization (Section 5.2) 
by handling the argument structure of the 
standard nominalization separately from that of 
the nominalization participating in the LVC. In 
the example ?Chen made another appearance in 
court?, we annotate ?Chen? as the Agent (ARG0) 
of the full predicate ?[make] [appearance]?, 
which is different from the argument structure of 
the standard nominalization which would label 
?Chen? to be the Patient argument (ARG1). 
6.2 Method 3: Final Method 
Our final method of light verb annotation reflects 
the notion that the noun, verb, or adjective as a 
true predicate within an LVC can have a 
different argument structure from that of the 
word alone. Table 4 shows the final annotation 
scheme for light verb construction.  
During Pass 1, the LVCs and their predicating 
expressions are identified in the data. Instances 
identified as LVCs in Pass 1 are then manually 
annotated during Pass 2, annotating the 
arguments and adjuncts of the light verb and the 
true predicate with roles that reflect their 
semantic relationships to the light verb plus true 
predicate. In practice, Pass 1 becomes a way of 
simply manually identifying the light verb 
usages. It is in Pass 2 that we make the final 
choice of argument labels for all of the 
arguments. Thus in Pass 3, the light verb and the 
true predicate lemmas from Pass 1 and 2 are 
joined into a single unit (e.g., in the example 
found in Table 4, the light verb ?took? would be 
joined with the true predicate ?walk? into 
?took+walk?) 3. In this final method, Pass 3 can 
be achieved completely deterministically. 
The major difference in this annotation 
scheme from that of Methods 1 and 2 is that 
instead of annotating in terms of the semantics of 
the bare noun, adjective or verb, the argument 
structure is determined for the entire predicate or 
the full event: semantics of the light verb plus the 
true predicate. This means that for the sentences 
where the argument roles of the verb and the 
nominalization disagree like ?Chen? in ?Chen 
                                                          
3 The order of Pass 2 and Pass 3 as presented in Table 4 is 
arguably a product of how the annotation tools for 
PropBank are set up for Arabic, Chinese, and English. That 
is, the order of the Pass 2 and Pass 3 could potentially be 
flipped provided that the tools and procedures of annotation 
support it, as is the case for Hindi PropBank. After the LVC 
and ARG-PRX are identified in Pass 1, the light verb and 
the true predicate can be deterministically joined into a 
single relation in Pass 2, leaving the manual annotation of 
LVC for Pass 3.  The advantage of this alternative ordering 
is that because the annotation of LVC is done around light 
verb plus the true predicate as a single relation, rather than 
the true predicate alone as in Table 4, the argument 
annotation may in actuality be more intuitive for annotators 
even with less training. 
 Pass 1: Pass 2:  Pass 3: 
 Light Verb Identification LVC Annotation Deterministic relation merge 
Relation Light verb True predicate Light verb + true predicate 
Arguments 
& Modifiers 
- Predicating expression is 
annotated with ARG-PRX 
- Arguments and modifiers of 
the LVCs are annotated 
- Arguments and modifiers 
are taken from Pass 2 
Frame File <no Frame File needed> LVC?s Frame File LVC?s Frame File 
 
Example 
?John took a brisk walk through the park.? 
REL: took 
ARG-PRX: a brisk walk 
ARG0: John 
ARG-MNR: brisk  
REL: walk 
ARGM-DIR: through the park 
ARG0: John 
ARG-MNR: brisk  
REL: [took][walk] 
ARGM-DIR: through the park 
Table 4. Final Annotation Scheme 
87
made another4 appearance in court?, we label the 
argument with the role that is consistent with the 
entire predicate (i.e. Agent, ARG0).  
Frame Files: The final advantage to this 
method is that only one Frame File is needed. 
Since Pass 1 is an identification round, no Frame 
File is required. A single Frame File for LVC 
that includes the argument structure with respect 
to the light verb plus true predicate combination 
will suffice for Pass 2 and Pass 3. 
7 Distinguishing LVCs from MWEs 
As we have discussed in Section 2, we adapted 
our approach from Butt?s (2004) definition of 
LVCs. That is, an LVC is characterized by a 
semantically bleached light verb and a true 
predicate. These elements combine as a single 
predicating unit, in such a way that the light verb 
plus its true predicate can be paraphrased by a 
verbal form of the true predicate without loss of 
the core meaning of the expression (e.g. 
?lectured? for ?gave a lecture?). Also, as 
discussed in Section 6.1, our approach advocates 
the notion that the semantic argument structure 
of the light verb plus true predicate is different 
from that of the expressions taken independently 
(as also proposed by Butt, 2004; Rosen, 1997; 
Grimshaw & Mester, 1988 among others). 
While these definitions are appropriate for the 
PropBank annotation task as we have presented 
it, there are still cases that merit closer attention. 
Even English with a rather limited set of verbs 
that are commonly cited as LVCs, includes a 
problematic mixture of what could arguably be 
termed either LVCs or idiomatic expressions: 
?make exception?, ?take charge?. This difficulty 
in part is the effect of frequency and 
entrenchment of particular constructions.  The 
light verbs themselves do not diminish in form 
over time in a manner similar to auxiliaries (Butt, 
2004), although the complements of common 
LVCs can change over time such that it is no 
longer clear that the complement is a predicating 
element.   
In the case of English, the expressions ?take 
charge? may be more commonly found today as a 
LVC than independently in its verbal form.  As 
we discovered with our annotators, native 
English speakers are uncomfortable using the 
verb ?charge? (i.e. to burden with a 
                                                          
4 The adjective ?another? is annotated as the modifier of the 
full predicate ?[make][appearance]? as it can be interpreted 
to mean that the make appearance event happened a 
previous appearance has been made. 
responsibility) as an independent matrix verb. A 
similar phenomenon can be seen in Arabic, 
where the predicate ??? ???? lit. ?release name? 
exemplifies a prototypical LVC that means ?to 
name?. However, in our data we see cases in 
which the complement is missing, while the 
semantics of the LVC remains intact: 
 ???? ???? ?? ??????? ??????  
CONJ REL be released.he PREP-him/it  
DEF-sector DEF-public 
lit ?Or what is released to it ?the public sector?? 
?Or what is called/named ?the public sector.?? 
This raises the question of: when does a 
construction that may have once been an LVC 
become more properly defined as an idiomatic 
expression due to such entrenchment?  Idiomatic 
expressions can potentially be distinguished from 
LVCs through judgments of how fixed or 
syntactically variable a construction is, and on 
the basis of how semantically transparent or 
decomposable the construction is (Nunberg et. 
al., 1994). However, sometimes the dividing line 
is hard to draw.  
A similar problem arises in determining 
whether a construction is a case of an LVC or 
simply a usage with a distinct sense of the verb. 
Take, for example, the following Arabic 
sentence. 
 ?????? ????? 
   take.he DEF-food 
lit. ?(he) took food? 
?he ate? 
Here, the Arabic word ???? ?food? is the noun 
derivation of the root shared by the verb ???? ?to 
eat?, in such a way that the sentence could be 
rephrased as ???? ?(he) ate?. This example falls 
neatly into the LVC category. However, further 
examples suggest that the example is a case of a 
distinct sense of ?to take orally? where the 
restrictions on the object are that the theme must 
be something that can be taken by mouth: 
?????? ????? 
take.he DEF-medicine 
?he took medicine? 
?????? ????? 
take.he DEF-soup 
?he took soup? 
Finally, determining the appropriate criteria to 
distinguish between a truly semantically 
bleached verb and verbs that seem to be 
participating in complex predication but 
contribute more to the semantics of the 
construction is a challenge for all languages. For 
example, in English data, there are potential 
LVCs with verbs that are not often thought of as 
light verbs, such as ?produce an alteration? and 
88
?issue a complaint?.  Although most English 
speakers would agree that the verbs in these 
constructions do not contribute to the semantics 
of the construction (e.g. ?issue a complaint? can 
be paraphrased to ?to complain?), there are 
similar constructions such as ?register a 
complaint,? wherein the verb cannot be 
considered light. For the purposes of annotation, 
where it is necessary for annotators to understand 
clear criteria for distinguishing light verbs, such 
cases are highly problematic because there is no 
deterministic way to measure the extent to which 
the verbal element contributes to the semantics 
of the construction.  In turn, there is not a good 
way to distinguish some of these borderline 
verbs from their normal, heavy usages.  
Such problems can be resolved by establishing 
language-specific semantic or syntactic tests that 
can be used for taking care of the borderline 
cases of LVCs. However, there is one other 
plausible manner we have identified that could 
help in detecting such atypical LVCs. This can 
be done by focusing on the argument structures 
of predicating complements rather than focusing 
on the verbs themselves.  Grimshaw & Mester 
(1988) suggest that the formation of LVCs 
involves argument transfer from the predicating 
complement to the verb, which is semantically 
bleached and thematically incomplete and 
assigns no thematic roles itself.  Similarly, 
Stevenson et al (2004) suggest that the 
acceptability of a potential LVC depends on the 
semantic properties of the complement.  Thus, 
atypical LVCs, such as the English construction 
?issue a complaint,? can potentially be detected 
during the annotation of eventive nouns, planned 
for all PropBank languages.  
This process will make our treatment of LVCs 
more comprehensive. Used with our language-
specific semantic and syntactic criteria relating to 
both the verb and the predicating complement, it 
will help us to more effectively capture as many 
types of LVCs as possible, including those of the 
V+ADJ and V+V varieties. 
8 Usefulness of our Approach 
Two basic approaches have previously been 
taken to handle all types of MWEs, including 
LVCs in natural language processing 
applications. The first is to treat MWEs quite 
simply as fixed expressions or long strings of 
words with spaces in between; the second is to 
treat MWEs as purely compositional (Sag et al, 
2002). The words-with-spaces approach is 
adequate for handling fixed idiomatic 
expressions, but issues of lexical proliferation 
and flexibility quickly arise when this approach 
is applied to light verbs, which are syntactically 
flexible and can number in the tens of thousands 
for a given language (Stevenson et al, 2004; Sag 
et al, 2002).  Nonetheless, large-scale lexical 
resources such as FrameNet (Baker et al, 1998) 
and WordNet (Fellbaum, 1999) continue to 
expand with entries that are MWEs.   
The purely compositional approach is also 
problematic for light verbs because it is 
notoriously difficult to predict which light verbs 
can grammatically combine with other 
predicating elements; thus, this approach leads to 
problems of overgeneration (Sag et al, 2002).  In 
order to overcome this problem, Stevenson et al 
(2004) attempted to determine which 
nominalizations could form a valid complement 
to the English light verbs take, give and make, 
using Levin?s (1993) verb classes to group 
similar nominalizations.  This approach was 
rather successful for take and give, but 
inconclusive for the verb make.  
Our approach can help to develop a resource 
that is useful whether one takes a words-with-
spaces approach or a compositional approach. 
Specifically, for those implementing a words-
with-spaces approach, the resulting PropBank 
annotation can serve as a lexical resource listing 
for LVCs. For those interested in implementing a 
compositional approach the PropBank annotation 
can serve to assist in predicting likely 
combinations. Moreover, information in the 
PropBank Frame Files can be used to generalize 
across classes of nouns that can occur with a 
given light verb with the help of lexical resources 
such as WordNet (Fellbaum, 1998), FrameNet 
(Baker et. al., 1998), and VerbNet (Kipper-
Schuler, 2005) (in a manner similar to the 
approach of Stevenson et al (2004)). 
Acknowledgements 
We also gratefully acknowledge the support of the 
National Science Foundation Grant CISE-CRI 
0709167, Collaborative: A Multi-Representational 
and Multi-Layered Treebank for Hindi/Urdu, and a 
grant from the Defense Advanced Research Projects 
Agency (DARPA/IPTO) under the GALE program, 
DARPA/CMO Contract No HR0011-06-C-0022, 
subcontract from BBN, Inc.  
Any opinions, findings, and conclusions or 
recommendations expressed in this material are those 
of the authors and do not necessarily reflect the views 
of the National Science Foundation. 
89
Reference 
Alsina, A. 1997. Causatives in Bantu and Romance. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 203-246. 
Baker, Collin F., Charles J. Fillmore, and John B. 
Lowe. 1998. The Berkeley FrameNet Project. In 
Proceedings of the 17th International Conference 
on Computational Linguistics (COLING/ACL-98), 
pages 86?90, Montreal. ACL. 
Butt, M. 2004.  The Light Verb Jungle. In G. Aygen, 
C. Bowern & C. Quinn eds.  Papers from the 
GSAS/Dudley House Workshop on Light Verbs. 
Cambridge, Harvard Working Papers in 
Linguistics, p. 1-50.   
Butt, M. 1997. Complex Predicates in Urdu. In A. 
Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 107-149. 
Fellbaum, Christine, ed.: 1998, WordNet: An 
Electronic Lexical Database, Cambridge, MA: 
MIT Press.  
Grimshaw, J., and A. Mester. 1988. Light verbs and 
?-marking. Linguistic Inquiry 19(2):205?232. 
Gildea, Daniel and Daniel Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics 28:3, 245-288. 
Goldberg, Adele E. 2003.  ?Words by Default: 
Inheritance and the Persian Complex Predicate 
Construction.? In E. Francis and L. Michaelis 
(eds). Mismatch: Form-Function Incongruity and 
the Architecture of Grammar. CSLI Publications.  
84-112. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad 
coverage, comprehensive verb lexicon. Ph.D. 
thesis, University of Pennsylvania. 
Levin, B. 1993. English Verb Classes and 
Alternations: A Preliminary Investigation. 
Chicago: Chicago Univ. Press.  
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, and R. Grishman. 2004. The 
NomBank Project: An interim report. In 
Proceedings of the HLT-NAACL 2004 Workshop: 
Frontiers in Corpus Annotation, pages 24- 31, 
Boston, MA. pages 430?437, Barcelona, Spain. 
Mohanan, T. 1997. Multidimensionality of 
Representation: NV Complex Predicates in Hindi. 
In A. Alsina, J. Bresnan, and P. Sells eds. Complex 
Predicates. Stanford, California: CSLI 
Publications, p. 431-471. 
Martha Palmer, Rajesh Bhatt, Bhuvana Narasimhan, 
Owen Rambow, Dipti Misra Sharma, Fei Xia, 
Hindi Syntax: Annotating Dependency, Lexical 
Predicate-Argument Structure, and Phrase 
Structure, In the Proceedings of the 7th 
International Conference on Natural Language 
Processing, ICON-2009, Hyderabad, India, Dec 
14-17, 2009 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury. 
2005. The Proposition Bank: An Annotated Corpus 
of Semantic Roles. Computational Linguistics, 
31(1):71?106. 
Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler, 
Wayne Ward, James H. Martin, Daniel Jurafsky. 
2004. Shallow Semantic Parsing using Support 
Vector Machines. University of Colorado 
Technical Report: TR-CSLR 2003-03. 
Rosen, C. 1997. Auxiliation and Serialization: On 
Discerning the Difference. In A. Alsina, J. 
Bresnan, and P. Sells eds. Complex Predicates. 
Stanford, California: CSLI Publications, p. 175-
202. 
Sag, I., Baldwin, T. Bond, F., Copestake, A., 
Flickinger, D. 2002.  Multiword expressions: A 
pain in the neck for NLP.  In Proceedings of teh 
Third International Conference on Intelligent Text 
processing and Computatinal Linguistics 
(CICLING 2002), p. 1-15, Mexico City, Mexico. 
ACL. 
Stevenson, S., Fazly, A., and North, R. (2004). 
Statistical measures of the semi-productivity of 
light verb constructions. In Proceedings of the 
ACL-04 Workshop on Multiword Expressions: 
Integrating Processing, p. 1?8. 
 
90
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 72?80,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Incorporating Coercive Constructions into a Verb Lexicon 
Claire Bonial*, Susan Windisch Brown*, Jena D. Hwang*, Christopher Parisien**, 
Martha Palmer* and Suzanne Stevenson** 
*Department of Linguistics, University of Colorado at Boulder 
**Department of Computer Science, University of Toronto 
{Claire.Bonial, Susan.Brown, hwangd, Martha.Palmer}@colorado.edu 
{chris, suzanne}@cs.toronto.edu 
 
 
Abstract 
We take the first steps towards augmenting a lexical 
resource, VerbNet, with probabilistic information 
about coercive constructions. We focus on CAUSED-
MOTION as an example construction occurring with 
verbs for which it is a typical usage or for which it 
must be interpreted as extending the event semantics 
through coercion, which occurs productively and adds 
substantially to the relational semantics of a verb. 
However, through annotation we find that VerbNet 
fails to accurately capture all usages of the 
construction. We use unsupervised methods to 
estimate  probabilistic measures from corpus data for 
predicting usage of the construction across verb 
classes in the lexicon and evaluate against VerbNet. 
We discuss how these methods will form the basis for 
enhancements for VerbNet supporting more accurate 
analysis of the relational semantics of a verb across 
productive usages. 
1 Introduction  
Automatic semantic analysis has been very successful 
when taking a supervised learning approach on data 
labeled with sense tags and semantic roles (e.g., see 
M?rquez et al, 2008). Underlying these recent successes 
are lexical resources, such as PropBank (Palmer et al, 
2005), VerbNet (Kipper et al, 2008), and FrameNet 
(Baker et al, 1998; Fillmore et al, 2002), which encode 
the relational semantics of numerous lexical items, 
especially verbs. However, because authors and speakers 
use verbs productively in previously unseen ways, 
semantic analysis systems must not be limited to direct 
extrapolation from previously seen usages licensed by 
static lexical resources (cf. Pustejovsky & Jezek, 2008). 
To achieve more accurate semantic analyses, we must 
augment such resources with knowledge of the 
extensibility of verbs. 
Central to verb extensibility is the process of semantic 
and syntactic coercion. Coercion allows a verb to be used 
in ?atypical? contexts that extend its relational semantics, 
thereby enabling expression of a novel concept, or simply 
more fluid expression of a complex concept. For 
example, consider a strictly intransitive action verb such 
as blink. This verb may instead be used in a construction 
with an object, as in She blinked the snow off her lashes, 
leading to an interpretation of the verb in which the object 
is causally affected and changes location (the CAUSED-
MOTION construction; Goldberg, 1995). This type of 
constructional coercion is common in language and 
underlies much extensibility of verb usages. 
Understanding such coercive processes thus has 
significant impact on how we should represent 
knowledge about verbs in a lexical resource. 
Importantly, constructional coercion is not an all-or-
nothing process ? a word must be semantically and 
syntactically compatible in some respects with a context 
in order for its use to be extended to that context, but the 
restrictions on compatibility are not hard-and-fast rules 
(Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 
2006; Goldberg, to appear). Gradience of compatibility 
plays an important role in coercion, suggesting that a 
probabilistic approach may be necessary for encoding 
knowledge of constructional coercion in a verb lexicon 
(cf. Lapata & Lascarides, 2003). 
Our hypothesis here is that, due to this gradient process 
of productivity, existing verb lexicons do not adequately 
capture the actual patterns of use of extensible 
constructions. In this paper, we focus on the CAUSED-
MOTION (CM) construction as an initial test case. We first 
annotate the classes of an extensive verb lexicon, 
VerbNet, as to whether the CM construction is allowed 
for all, some, or none of the verbs in the class, noting 
additionally whether it is a typical or coerced usage. We 
find that many of the classes that allow the construction 
for at least some verbs do not include the CM frame in 
their definition, indicating a significant shortcoming in the 
relational knowledge encoded in the lexicon. Next, we 
72
develop probabilistic measures for determining to what 
degree a class is likely to admit the CM construction. We 
then test our measures over corpus data, manually 
annotated for use of the CM construction. Finally, we 
present preliminary work on automatic techniques for 
calculating the proposed measures in an unsupervised 
way, to avoid the need for expensive manual annotation. 
This work forms the preliminary steps toward empirically 
augmenting VerbNet?s predictive capabilities concerning 
the event semantics of verbs in coercible constructions. 
2 Extensible Constructions and VerbNet 
Construction grammar has much insight to offer on the 
topic of productivity and on the resulting statistical 
patterns and gradience of usages (e.g., Langacker, 1987; 
Kay & Fillmore, 1999; Goldberg, 2006). A construction 
is formally defined to be any pairing of linguistic form 
(e.g., a syntactic frame) and meaning. Words can be used 
in constructions to the extent that their lexical semantics is 
compatible with ? or can be coerced to be compatible 
with ? the semantic constraints on the construction. 
It is this notion of constructional coercion, and degree 
of coercibility, that accounts for the richness of usages 
that go beyond those thought of as typical or definitional 
for a verb: by coercing a verb not normally associated 
with a particular frame to occur in it, the meaning of the 
event can take on additional properties not considered a 
core part of the verb?s semantics. For example, in the case 
of the sentence discussed above, She blinked the snow off 
her lashes, it is not the verb but rather the CM 
construction itself that licenses the direct object and adds 
the notion of ?motion causally affecting the object? to the 
event semantics. Amongst other examples of well-known 
constructional coercions are: (1) The CAUSE-RECEIVE 
construction has the syntactic form of NP-V-NP-NP. For 
example, in Bob painted Sally a picture, the simple 
transitive verb paint gains the CAUSE TO RECEIVE sense, 
in which Sally is the recipient and the picture is the 
transferred item. (2) The WAY construction has the form 
of NP-V-[POSS way]-PP. For example, in Frank found 
his way to New York, the construction allows the verb 
find to gain a motion reading (i.e., ?Frank traveled to New 
York?) that would not otherwise be allowed (e.g., *Frank 
found to New York).  
Recognizing such extensions to the relational 
semantics of verbs is very important for accurate 
semantic interpretation in NLP. However, precise 
specifications for capturing the notion of coercible 
constructions, such as are needed for a computational 
resource, have heretofore been lacking. 
2.1 VerbNet & Knowledge of Constructions 
Computational verb lexicons are key to supporting NLP 
systems aimed at semantic interpretation. Verbs express 
the semantics of an event being described as well as the 
relational information among participants in that event, 
and project the syntactic structures that encode that 
information. Verbs are also highly variable, displaying a 
rich range of semantic and syntactic behavior. 
Verb classifications help NLP systems to deal with 
this complexity by organizing verbs into groups that 
share core semantic and syntactic properties. For 
example, VerbNet (derived from Levin?s [1993] work, 
Kipper et al, 2008) is widely used for a number of 
semantic processing tasks, including semantic role 
labeling (Swier and Stevenson, 2004), the creation of 
semantic parse trees (Shi and Mihalcea, 2005), and 
implicit argument resolution (Gerber and Chai, 2010). 
The detailed semantic predicates listed with each 
VerbNet class also have the potential to contribute to text-
specific semantic representations and, thereby, to tasks 
requiring inferencing (Zaenen et al, 2008; Palmer et al, 
2009). 
VerbNet identifies semantic roles and syntactic 
patterns characteristic of the verbs in each class makes 
explicit the connections between the syntactic patterns 
and the underlying semantic relations that can be inferred 
for all members of the class. Each syntactic frame in a 
class has a corresponding semantic representation that 
details the semantic relations between event participants 
across the course of the event. For example, one of the 
characteristic patterns listed for the Pour class is a 
CAUSED-MOTION pattern, which accounts for sentences 
like She poured water from the pitcher into the bowl. This 
is represented in VerbNet as follows: 
Syntactic representation: 
NP V NP PP PP 
Agent V Theme Source Location 
Semantic representation: 
MOTION (DURING(E), THEME)  
NOT (PREP (START(E), THEME, LOCATION)) 
PREP (START(E), THEME, SOURCE) 
PREP (END(E), THEME, LOCATION) 
CAUSE (AGENT, E) 
This representation details connections between the 
syntax and semantics using the semantic roles as links, 
indicating that the Agent is the Subject NP and has 
CAUSED the Event, and that the Theme is the Object NP 
and has a new LOCATION at the end of the event. These 
types of inferences provide the foundation for deep 
semantic analysis of text.  
73
However, the specifications in VerbNet (as in other 
predicate lexicons, such as FrameNet, Baker et al, 1998; 
Fillmore et al, 2002) are seen as definitional ? they are 
restricted to the core usages of the verbs that are valid for 
all verbs in the class. However, as noted above, people 
often use verbs productively, in ways that go beyond the 
boundaries of the verb class structure. It is important to 
correctly identify these productive usages when they 
occur, since they may be explicitly adding crucial 
inferences. If a construction is not recognized in the form 
of a syntactic frame in VerbNet, such inferences are not 
possible, greatly reducing VerbNet?s utility and coverage. 
For example, creative uses of a verb, such as She blinked 
the snow off her lashes, would have no corresponding 
frame in blink?s class, the Hiccup class.  It contains one 
intransitive frame: 
 NP V 
Agent V 
  
 
BODY_PROCESS (E, AGENT) 
INVOLUNTARY (E, AGENT) 
 
Sentences that coerce the meaning of blink to fit with a 
CM event would currently be misanalysed. One option 
might be to augment the Hiccup class with the CM frame 
from the Pour class, which would ensure that such 
sentences would be analyzed more accurately. However, 
given the productive nature of constructional coercion 
and its widespread applicability, the approach of adding 
any possible pattern to each class is not appropriate: this 
would undermine the definitional distinctions between 
classes and greatly lessen their usefulness.  
Complicating the issue is the phenomenon of regular 
sense extensions (Dang et al, 1998), where what once 
may have been coercion has become entrenched and is 
now seen as a different sense of the verb. For example, 
the verbs in the Push class express the general meaning of 
exerting force on an object, such as She pushed on the 
wall. Often, the exertion of force moves the object, which 
can be expressed in a CM construction such as She 
pushed the box across the room. VerbNet accounts for 
this regular sense extension by including most of the Push 
verbs in the Carry class as well, which has the CM 
construction as one of its frames. Deciding when to 
include a verb in another class based on regular sense 
extensions, when to add a frame for a construction to a 
class, or when to reject the frame as a defining part of a 
class, is made difficult by the graded nature of matches 
between verbs and a construction. Our goal is to maintain 
the advantages of the class structure of VerbNet while 
enhancing it with a graded view of the applicability of a 
construction for each class. Noting the applicability of a 
construction will enable the inclusion of its appropriate 
semantic predicates, and the inferencing over them, 
which are currently not supported. 
3 Our Proposal: Constructional Profiles 
We aim to augment VerbNet with knowledge of 
constructions that are likely to be used extensibly with a 
range of verbs. Such extensible constructions will be core 
usages for some classes (such as the CM for the Pour 
class, as noted above) but will be less characteristic of the 
fundamental semantics of other verb classes (such as CM 
for the Hiccup class). We propose to identify such a 
construction and its varying roles in the different classes 
by using relevant statistics over usages of verbs in a 
corpus ? what we call a constructional profile. 
A constructional profile is a probabilistic assessment 
of the usage of a particular construction by the verbs in a 
class. We developed the following three measures to 
capture the relevant behavior, with the goal of providing 
both type- and token-based views of the behavior of a 
verb class with respect to a target construction: 
P1 Ptype(X|C): probability that a verb type in class C is 
attested in construction X 
P1 gives a type-based assessment, indicating how 
widespread the use of the construction is across the 
verb types in the class. For example, if 8 out of 10 
members of a class appear with the construction, we 
might estimate P1 as 0.8. 
P2 Ptoken(X|C): probability that the instances of a typical 
verb in class C occur in construction X 
P2 gives a token-based assessment, indicating, for a 
typical verb in the class, the relative amount of usage of 
the construction among all usages of the verb. For 
example, to estimate this, we might average across all 
verbs in the class, the percentage of tokens in this 
construction. 
P3: Ptoken(X|X-verbs-in-C): same as P2 but considering 
only verbs that have been attested in construction X 
P3 is the same as P2, but looking only at those verbs in 
the class that have an attested usage of the construction, 
removing verbs without attested usages. 
We hypothesize that these measures will have high 
values for those classes for which the construction should 
be definitional; very low values for those classes that are 
not compatible with the construction; and varying values 
for those classes that allow coerced usages to a greater or 
lesser extent. 
Although these probabilities are intuitively very 
simple, estimating them from corpus data poses a 
significant challenge. Since a construction is a pairing of 
form with meaning, recognizing the use of a particular 
74
construction is not simply a matter of determining the 
syntactic pattern of the usage; rather, certain semantic 
properties and relations must co-occur with the syntactic 
pattern. Earlier work has shown that a supervised learning 
method was able to discriminate potential usages of the 
CM construction given training sentences manually 
labeled as either CM or not (Hwang et al, 2010). Here, 
we aim instead to identify usages of the CM construction, 
but without requiring an expensive manual annotation 
effort. That is, we seek an unsupervised method for 
estimating the probabilities in P1?P3 above. 
We approach this goal in steps as follows. First, we 
examine all the classes in VerbNet to see which allow the 
CM construction (Section 4). This anno-tation reveals 
shortcomings in VerbNet?s representa-tion (classes that 
allow the CM construction but do not list it) and also 
provides a gold standard with which to evaluate our 
method of identifying an exten-sible construction using 
our constructional profiles. Second, we use the manually 
annotated CM construction data from Hwang et al 
(2010) to estimate probabilities P1?P3 using maximum 
likelihood formulations (Section 5). An analysis of the 
predictive power of these constructional profile measures 
shows a good match with the distinctions made in the 
human annotation of the classes. Thus, our annotation 
based constructional profile measures show promise for 
identifying relevant behaviors of the construction across 
the classes. Third, we explore automatic methods for 
estimating the constructional profile measures without the 
need for manual annotations (Section 6). We use a 
hierarchical Bayesian model that learns verb classes from 
corpus data to provide unsupervised estimates of the 
constructional profiles, which also exhibit the relevant 
distinctions across the classes. 
4 Annotating the VerbNet Resource  
We begin with a manual examination of the resource and 
a thorough annotation of the status of each class with 
respect to the CM construction. This effort reveals a 
number of shortcomings in VerbNet, and the need for 
developing methods that can support the extension of 
VerbNet to better reflect the coercive uses of 
constructions across the classes. The annotation described 
here also forms the basis for the evaluation in the 
following sections of our new probabilistic measures, by 
motivating hypotheses about the expected patterns of use 
of the CM construction across the classes. 
4.1 Annotation Guidelines and Results 
The first goal of our manual annotation of VerbNet 
classes was to determine which classes currently 
represent CM in one of their frames. To this end, we 
identified which classes contain the following frame:  
NP [Agent/Cause]-V-NP [Patient/Theme]- 
PP [Source/Destination/Recipient/Location]  
These frames correspond to classes such as Slide, with its 
frame NP-V-NP-PP.Destination: Carla slid the books to 
the floor. We also examined classes with the patterns NP-
V-NP-PP.Oblique, NP-V-NP-PP. Theme2, and NP-V-
NP-PP.Patient2. In these classes, annotators had to judge 
whether the final PP was compatible with CM. For 
example, the Breathe class contains the frame NP-V-
NP.Theme-PP.Oblique, The dragon breathed fire on 
Mary, which is compatible with CM; whereas the same 
basic frame in the Other_cos class is not: NP V NP 
PP.Oblique, The summer sun tanned her skin to a golden 
bronze. 
In addition, we annotated which classes were 
potentially compatible with CM for either all verbs in the 
class or only some verbs. The "some" classification has 
the drawback that it may be applied to classes with very 
different proportions of compatible verbs; while suitable 
for our exploratory work here, we plan to make finer 
distinctions in the future. A secondary determination was 
whether or not the class was compatible with CM as part 
of its core semantics, or if it was compatible with CM 
because it was coercible into the construction. A verb was 
considered ?compatible with CM? and ?not coerced? if 
the verb could be used in the CM construction and its 
semantics, as reflected in VerbNet?s semantic predicates, 
involved a CAUSE predicate in combination with another 
predicate such as CONTACT, TRANSFER, (EN)FORCE, 
EMIT, TAKE_IN (predicates potentially involving 
movement along some path). For example, although CM 
is not already included as a frame for the Bend class 
containing the verb fold, the semantics of this class 
include CAUSE and CONTACT, and the verb can be used 
in a CM construction: She folded the note into her 
journal. Therefore, this class would have been considered 
?compatible with CM? but ?not coerced?. Conversely, a 
verb was considered ?compatible with CM? and 
?coerced? if the verb could be used in the CM 
construction, yet its semantics, again as reflected in 
VerbNet, did not involve CAUSE and MOVEMENT 
ALONG A PATH (e.g., the verb wiggle of the 
Body_internal_motion class: She wiggled her foot out of 
the boot). 
In summary, as presented in the table below, we 
annotated each class according to whether (1) the CM 
construction was already represented in VerbNet for this 
class, (2) the construction was possible for all, some, or 
75
none of the verbs in that class, and (3) the verbs of any 
class compatible with CM were coerced into the 
construction or not. The classification for (3) was made 
regardless of whether ?all? verbs or only ?some? were 
compatible with CM. This determination was made 
uniformly for a class: there were no classes in which only 
certain CM-compatible verbs were considered ?coerced?.  
VN class example  
[# of classes like this] 
CM in 
VN 
CM is 
possible 
CM is 
coerced 
Banish [50] Yes All No 
Nonverbal_Expression [2] Yes All Yes 
Cheat [6] Yes Some No 
Exhale [18] No All No 
Hiccup [30] No All Yes 
Fill [46] No Some No 
Wish [54] No Some Yes 
Matter [64] No None N/A 
Notably, we identified 206 classes where at least some of 
the verbs in that class are compatible with the CM 
construction; however, VerbNet currently only 
recognizes the CM construction in 58 classes. There were 
several classes of interest: First, although it may seem 
unusual that CM is represented in 6 classes where we 
found that only ?some? verbs were compatible with CM 
(e.g., Cheat class), these were cases where only more 
restricted subclasses are compatible with CM, and this 
syntactic frame is listed for that subclass. This suggests 
subclasses may provide a more precise characterization 
of which verbs are compatible with a construction.  
Secondly, we identified 18 classes in which all verbs 
were compatible with CM without coercion; thus, these 
classes could likely be improved by the addition of the 
CM syntactic frame. Additionally, we found 30 classes in 
which all verbs are coercible into the CM construction; 
however, the actual likelihood of a verb in those classes 
occurring in a CM construction remains to be 
investigated in the following sections. Like those classes 
where it was determined that only ?some? verbs are 
compatible with CM, usefully incorporating the CM 
construction into classes that require coercion relies on 
accurately determining the probability that verbs in those 
classes will actually appear in the CM construction.  
For those classes in which ?all? verbs are compatible 
with CM, our intuition was that some aspect of the verb?s 
semantics either inherently includes or allows the verb to 
be coerced into the CM construction. Conversely, for 
those classes in which no verbs are compatible with CM, 
presumably some aspect of the verb?s semantics is 
logically incompatible with CM. Although pinpointing 
precisely what aspect of a verb?s semantics makes it 
compatible with CM may not be possible, we can 
investigate whether or not our intuitions are supported by 
examining the actual frequencies of CM constructions for 
given verbs or a given class.  
4.2 Hypotheses  
Using these annotations, we were able to develop two 
simple hypotheses. 
Hypothesis 1: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for those classes in which all verbs were found to 
be compatible with CM; lower for classes in which only 
some verbs were found to be compatible; and lowest for 
classes in which no verbs were found to be compatible. 
Hypothesis 2: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for verbs that fall into classes where CM is not 
considered coerced (for either some or all of the verbs in 
the class); lower for verbs that fall into classes in which 
the CM construction only works through coercion (for 
either some or all of the verbs in the class); and lowest for 
verbs that fall into classes in which no verbs are 
compatible with CM.  
To investigate Hypothesis 1, we grouped the annotated 
classes according to whether all, some, or no verbs in the 
class are compatible with CM: 
 Class example # of classes 
Allowed by All Bring, Carry 106 
Allowed by Some Appoint, Lodge 100 
Allowed by None Try, Own 64 
To investigate Hypothesis 2, we did a second grouping 
of the classes according to whether CM is not coerced, 
CM is coerced, or CM is simply not compatible with the 
class. This second grouping did not distinguish whether 
CM was compatible with ?all? or ?some? of the verbs in 
a given class. 
 Class example # of classes 
Not Coerced Put, Throw 120 
Coerced Floss, Wink 86 
Not Compatible Differ 64 
5 Evaluation using Constructional Profiles 
5.1 Annotated data description 
Our research uses the data annotated for Hwang et al 
(2010), in which 1800 instances in the form NP-V-NP-
PP were identified in the Wall Street Journal portion of 
the Penn Treebank II (Marcus et al, 1994). Each instance 
76
of the data was single annotated with one of the two 
labels: CM or non-CM. The annotation guidelines were 
based on the CM analysis of Goldberg (1995). 
Our analysis began with the same data but adopted a 
slightly narrower definition of CM. We diverged from 
the Hwang et al (2010) study in the following two ways: 
(1) sentences where the object NP is an item that is 
created by the event denoted by the verb were not 
considered CM (e.g., Mr. Pilson scribbled a frighteningly 
large figure on a slip of paper, where the figure is created 
through the scribbling event); and (2) sentences in which 
movement is prevented were not considered CM (e.g., 
He kept her at arm?s length). In agreement with Hwang 
et al, our annotation included both metaphorical senses 
(e.g., [It] cast a shadow over world oil markets) and 
literal senses (e.g., The company moved the employees to 
New York) of CM. Our annotation using the narrower 
guidelines resulted in 85.8% agreement with the original 
annotation.1  The distribution of labels in our data is 
21.8% for CM and 78.2% for NON-CM. 
5.2 Annotated data description 
Using statistics over the manually annotated data, we 
calculate maximum likelihood estimates of the three 
constructional profile measures introduced in Section 3, 
as follows. First, let the probability that a verb v is used in 
the CM construction be estimated as: 
P(CM|v,C) = 
#(CM usages of     ) 
#(CM+non-CM usages of    ) 
That is, P(CM|v,C) is estimated as the relative frequency 
of the CM construction for v out of all annotated usages 
of v that are labeled as class C. Now let CCM be all verbs v 
in C with at least one usage annotated as CM; i.e.: 
    *      |  (  |   )    + 
Then we calculate estimates of P1?P3 as: 
P1: Ptype(CM|C) = |CCM |/|C| 
This measure indicates how widespread the use of CM is 
across the verb types in the class. 
P2: Ptoken(CM|C) =,?  (  |   )   - | |?  
The average over all verbs v in C of P(CM|v,C) 
This indicates the relative amount of usage of CM among 
all usages of the verbs in the class.  
P3: Ptoken(CM|v,C) = [?  (  |   ))- |   |       
The average over all verbs v in CCM of P(CM|v,C) 
P3 narrows the P2 measure to only those verbs in the 
                                                          
1We found that 34.0% of the disagreements were directly due to 
the changes in annotation resulting from our two new criteria. 
class for which there is an attested usage of CM. 
5.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
measures P1-P3 for the groups of VerbNet classes as 
defined in section 4.2. For each group listed, we report 
the averages of P1-P3 over all classes in the group where 
at least one verb in the class occurred in the data 
manually annotated for CM usage. 
 P1 P2 P3 
CM Allowed by All 0.413 0.323 0.437 
CM Allowed by Some 0.087 0.078 0.224 
CM Not Allowed 0.055 0.055 0.083 
As seen here, the constructional profile measures over 
CM in the data corroborate our Hypothesis 1 (Section 
4.2). All three measures on average are highest for the 
classes that fall into the ?all allowed? group, next highest 
for those in the ?some allowed? group, and lowest for the 
?not allowed? classes.  
 P1 P2 P3 
CM Non-Coerced 0.354 0.274 0.418 
CM Coerced 0.091 0.091 0.185 
CM Not Allowed2 0.056 0.056 0.083 
Furthermore, the second table here confirms our 
expectations for Hypothesis 2 (Section 4.2). Again, all 
three measures on average are highest for classes that fall 
into the ?non-coerced? group, next highest for classes in 
the ?coerced? group (in which the construction is 
achievable only through coercion), and lowest for the 
?not allowed? group.  
Thus, our two hypotheses are borne out, showing that 
our constructional profile measures, when estimated over 
manually annotated data, can be useful in capturing 
important distinctions among classes of verbs with regard 
to their usage in an extensible construction such as CM. 
6 Automatic Creation of Constructional 
Profiles Using a Bayesian Model  
Manually annotating a corpus for usages of a con-
struction can be prohibitively expensive, so we also 
investigate the use of automatic methods to estimate 
constructional profile measures. By using a hierarchi-cal 
Bayesian model (HBM) that acquires latent prob-abilistic 
verb classes from corpus data, we provide unsupervised 
                                                          
2 Note the non-zero values result from actual CM verb usages in 
the data belonging to classes believed to be not compatible with 
CM by VerbNet expert annotators. 
77
estimates of the constructional profiles. 
6.1 Overview of Model and Data 
We use the HBM of Parisien & Stevenson (2011), a 
model that automatically acquires probabilistic 
knowledge about verb argument structure and verb 
classes from large-scale corpora. The model is based on a 
large body of research in nonparametric Bayesian topic 
modeling (e.g., Teh et al, 2004), a robust method of 
discovering syntactic and semantic structure in very large 
datasets. For each verb encountered in a corpus, the 
model provides an estimate of the verb?s expected overall 
pattern of usage. By using latent probabilistic verb classes 
to influence these expected usage patterns, the model can, 
for example, estimate the probability that a verb like blink 
might occur in a CM construction, even if no such 
attested usages appear in the corpus. 
In this preliminary study, we use the corpus data from 
Parisien & Stevenson (2011), since the model has been 
trained and evaluated on this data. As that study was 
aimed at modeling facts of child language acquisition, it 
uses child-directed speech from the Thomas corpus 
(Lieven et al, 2009), part of the CHILDES database 
(MacWhinney, 2000). In this preliminary study, we use 
their development dataset containing approx. 170,000 
verb usages, covering approx. 1,400 verb types. (We 
reserve the test set for future experiments.) For each verb 
usage in the input, a number of features are automatically 
extracted that indicate the number and type of syntactic 
arguments occurring with the verb and general semantic 
properties of the verb. The semantic features are drawn 
from the set of VerbNet semantic predicates, such as 
CAUSE, MOTION, and CONTACT. These are automatically 
extracted from all classes compatible with the verb (with 
no sense disambiguation). 
6.2 Measures for Constructional Profiles 
Using the argument structure constructions, verb usage 
patterns and classes learned by the model, we estimate 
the three constructional profile measures in Section 3, as 
follows. First, we note that since the constructions 
acquired by the model are probabilistic in nature, a 
particular CM instance may be a partial match to more 
than one of the model?s constructions.  
For each verb in the input, we consider the likelihood 
of use of the CM construction to be the likelihood of a 
contrived frame intended to capture the important 
properties of a CM usage. FCM is a usage taking a direct 
object and a prepositional phrase, and including the 
semantic features CAUSE and MOTION, with all other 
semantic features left unspecified. For a given verb v, we 
estimate the likelihood of this CM usage, over all 
constructions in the model, as follows: 
 (   | )  ? (   | ) (
 
 | ) 
Here, P(FCM |k) is the likelihood of the CM usage FCM 
being an instance of the probabilistic construction k, and 
P(k|v) is the likelihood that verb v occurs with 
construction k. These component probabilities are 
estimated using the probability distributions acquired by 
the model and averaged over 100 samples from the 
Markov Chain Monte Carlo simulation, as described in 
Parisien & Stevenson (2011). 
Now, we let CCM be the set of verbs in VerbNet class 
C where the expected likelihood of a CM usage is non-
negligible (akin to the set of verbs with attested usage in 
Section 5.2): 
CCM = {v C | P(FCM|v)>? } 
where ? is a small threshold, here 0.0001. Note that since 
v is not disambiguated for class in our data, all usages of v 
contribute to this estimate. 
The estimates of P1-P3 are comparable to those in 
Section 5.2. The difference is that since we are un-able to 
disambiguate individual usages of the verbs, each usage 
of v is considered to belong to all possible classes C of 
which v is a member. P1 is estimated as before; P2 and 
P3 are averages of P(FCM|v). 
6.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
estimates P1-P3 for the groups of VerbNet classes as 
given in Section 4.2. For each group listed, we report the 
averages of P1-P3 over all classes in the group where at 
least one of the verbs in the class occurred in the training 
input to the model. 
 P1 P2 P3 
All allowed 0.569 0.0180 0.0250 
Some allowed 0.449 0.0106 0.0192 
Not allowed 0.363 0.0044 0.0079 
These profile measures align with the hypotheses in 
Section 4.2 and with the measures based on manually 
annotated data in Section 5.2. The estimates are high-est 
for classes where all verbs permit the CM con-struction, 
second highest for classes where only some permit it, and 
lowest for classes that do not permit it. 
 P1 P2 P3 
CM non-coerced 0.546 0.0178 0.0260 
CM coerced 0.458 0.0095 0.0167 
CM not allowed 0.363 0.0044 0.0079 
78
Again, the overall patterns of the profile measures align 
with Sections 4.2 and 5.2. The profile estimates are 
highest for classes annotated to be non-coerced usages of 
CM, second highest for coerced classes, and lowest for 
?not allowed?.  
The measures show the overall differences among 
classes in the different groups (for both groupings) ? i.e., 
the average behavior among classes in the different 
groups varies as we predicted.  This indicates that the 
measures are tapping into aspects of construction usage 
that are relevant to making the desired distinctions in 
VerbNet, and validates the use of automatic 
techniques.  However, there is a substantial amount of 
variability in these measures across the classes, so we also 
consider how well the estimates can predict the 
appropriate group for individual classes. That is, can we 
automatically predict whether the CM construction can 
be used by all, some, or none of the verbs in a given verb 
class, and can we predict whether such usages are 
coerced? 
We consider the P3 measure as it provides the best 
separation among the class groupings. The tables below 
report precision (P), recall (R) and F-measures (F) for 
each group, where ?all? and ?some? have been collapsed. 
For exploratory purposes, we pick P3 = 0.006 as the 
value that optimizes F-measures of this classification. 
Future work will explore more principled means for 
setting these thresholds. 
 P R F 
CM allowed 0.880 0.742 0.806 
CM not allowed 0.407 0.636 0.497 
Only a 2-way distinction can be made reliably for the 
allowed grouping. The F-score of over 80% for the 
?allowed? label is very promising. The low precision for 
the ?not allowed? case suggests that the model can?t 
generalize sufficiently due to sparse data. 
 P R F 
CM non-coerced 0.691 0.491 0.574 
CM coerced 0.461 0.417 0.438 
CM not allowed 0.406 0.709 0.517 
We use thresholds of P3 = 0.021 to separate non-coerced 
from coerced classes, and P3 = 0.007 to separate coerced 
from not allowed classes. The model estimates show 
moderate success in distinguishing classes with coerced 
vs. non-coerced usage of the CM construction. However, 
our measures simply cannot distinguish non-occurrence 
due to semantic incompatibility from non-occurrence due 
to chance, given the expected low frequency of a novel 
coerced use of a construction.  To separate the allowed 
cases into whether they are coerced or not requires a 
more detailed assessment of the semantic compatibility of 
the class, which means looking at finer-grained features 
of verb usages that are indicative of the semantic 
predicates compatible with the particular construction.  
Moreover, this kind of assessment likely needs to be 
applied on a verb-specific (and not just class-specific) 
level, in order to identify those verbs out of a potentially 
coercible class that are indeed coercible (i.e., identifying 
the coercible verbs in a class labeled as "some allowed"). 
7 Conclusion 
Our investigation demonstrates that VerbNet does not 
currently represent the CM construction for all verbs or 
verb classes that are compatible with this construction, 
and the existing static representation of verbs is 
inadequate for analyzing extensions of verb meaning 
brought about by coercion. The utility of VerbNet would 
be greatly enhanced by an improved representation of 
constructions: specifically, the incorporation of 
probabilities that verbs in a given (sub)class would occur 
in a particular construction, and whether this constitutes a 
regular sense extension. This addition to VerbNet would 
increase the resource?s coverage of syntactic frames that 
are compatible with a given verb, and therefore enable 
appropriate inferences when coercion occurs. We have 
made preliminary steps towards developing this 
probabilistic distribution over both verb instances and 
classes, based on a large corpus. Unsupervised methods 
for estimating the probabilities achieve an F-score of over 
80% in distinguishing the classes that allow the target 
construction. However, making distinctions among 
coerced and non-coerced cases will require us to go 
beyond these class-based probabilities to finer-grained, 
corpus-based assessments of a verb?s semantic 
compatibility with a coercible construction.  
To move beyond these preliminary findings, we must 
therefore shift our focus to the behavior of individual 
verbs. Additionally, to reduce the impact of errors 
resulting from low-frequency verbs and classes, we plan 
to expand our research to more data, specifically the 
OntoNotes TreeBank data (Weischedel et al, 2011). 
Finally, to achieve our ultimate goal of creating a lexicon 
that can flexibly account for a variety of constructions, we 
will examine other constructions as well. While 
determining the set of coercible constructions in a 
language is itself a topic of current research, we propose 
initially to include the widely recognized CAUSE-
RECEIVE and WAY constructions in addition to CM. 
79
References  
Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. 
The Berkeley FrameNet Project. Proceedings of the 17th 
International Conference on Computational Linguistics 
(COLING/ACL-98), pp. 86?90, Montreal. 
Dang, HoaTrang, Karin Kipper, Martha Palmer, and Joseph 
Rosenzweig. 1998. Investigating regular sense extensions 
based on intersective Levin classes. Proceedings of 
COLING-ACL98, pp. 293?299. 
Fillmore, Charles J., Christopher R. Johnson, and Miriam R.L. 
Petruck. 2002. Background to FrameNet. International 
Journal of Lexicography, 16(3):235-250.  
Gerber, Matthew, and Joyce Y. Chai. 2010. Beyond 
NomBank: A study of implicit arguments for nominal 
predicates. Proceedings of the 48th Annual Meeting of the 
Association of Computational Linguistics, pp. 1583?1592, 
Uppsala, Sweden, July. 
Goldberg, A. E. 1995. Constructions: A construction 
grammar approach to argument structure. Chicago: 
University of Chicago Press. 
Goldberg, A. E. 2006. Constructions at work: The nature of 
generalization in language. Oxford: Oxford University 
Press. 
Goldberg, A. E. To appear. Corpus evidence of the viability of 
statistical preemption. Cognitive Linguistics. 
Hwang Jena D., Rodney D. Nielsen and Martha Palmer. 2010. 
Towards a domain-independent semantics: Enhancing 
semantic representation with construction grammar. 
Proceedings of Extracting and Using Constructions in 
Computational Linguistic Workshop, held with NAACL 
HLT 2010, Los Angeles, June. 
Kay, P., and C. J. Fillmore. 1999. Grammatical constructions 
and linguistic generalizations: The What's X Doing Y? 
construction. Language, 75:1?33. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha 
Palmer. 2008. A large-scale classification of English verbs. 
Language Resources and Evaluation Journal, 42:21?40. 
Langacker, R. W. 1987. Foundations of cognitive grammar: 
Theoretical prerequisites. Stanford, CA: Stanford 
University Press. 
Lapata, M., and A. Lascarides. 2003. Detecting novel 
compounds: The role of distributional evidence. 
Proceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics(EACL03), pp.235?242. Budapest, Hungary. 
Levin, B. 1993.English Verb Classes and Alternations: A 
Preliminary Investigation. Chicago: Chicago University 
Press.  
 
 
Lieven, E., D. Salomo, and M. Tomasello. 2009. Two-year-
old children?s production of multiword utterances: A 
usage-based analysis. Cognitive Linguistics 20(3):481?507. 
MacWhinney, B. 2000.The CHILDES Project: Tools for 
analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum. 
M?rquez, L., X. Carreras, K. Litkowski, and S. Stevenson. 
2008. Semantic role labeling: An introduction to the special 
issue. Computational Linguistics, 34(2): 145?159. 
Martha Palmer, Jena D. Hwang, Susan Windisch Brown, 
Karin Kipper Schuler and Arrick Lanfranchi. 2009. 
Leveraging lexical resources for the detection of event 
relations. Proceedings of the AAAI 2009 Spring 
Symposium on Learning by Reading, Stanford, CA, March. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005. 
The Proposition Bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1):71?106. 
Parisien, Christopher, and Suzanne Stevenson. 2011. To 
appear in Proceedings of the 33rd Annual Meeting of the 
Cognitive Science Society, Boston, MA, July. 
Pustejovsky, J., and E. Jezek. 2008. Semantic coercion in 
language: Beyond distributional analysis. Italian Journal of 
Linguistics/RivistaItaliana di Linguistica 20(1): 181?214. 
Shi, Lei, and Rada Mihalcea. 2005. Putting pieces together: 
Combining FrameNet, VerbNet and WordNet for robust 
semantic parsing. Proceedings of the 6th International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico. 
Swier, R., and S. Stevenson. 2004. Unsupervised semantic 
role labeling. Proceedings of the 2004 Conf. on Empirical 
Methods in Natural Language Processing, pp. 95?102, 
Barcelona, Spain. 
Teh, Y. W., M. I. Jordan, M. J.Beal, and D. M.Blei.2006. 
Hierarchical Dirichlet processes. Jrnl of the American 
Statistical Asscn, 101(476): 1566?1581. 
Weischedel, R., E. Hovy, M. Marcus, M. Palmer, .R. Belvin, 
S. Pradan, L. Ramshaw and N. Xue. 2011.OntoNotes: A 
Large Training Corpus for Enhanced Processing. In Part 1: 
Data Acquisition and Linguistic Resources of The 
Handbook of Natural Language Processing and Machine 
Translation: Global Automatic Language Exploitation, 
Eds.: Joseph Olive, Caitlin Christianson, John McCary. 
Springer Verlag, pp. 54-63. 
Zaenen, A., C. Condoravdi, and D. G. Bobrow. 2008. The 
encoding of lexical implications in VerbNet. Proceedings 
of LREC 2008, Morocco, May. 
80
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 178?186,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Abstract Meaning Representation for Sembanking
Laura Banarescu
SDL
lbanarescu@sdl.com
Claire Bonial
Linguistics Dept.
Univ. Colorado
claire.bonial@colorado.edu
Shu Cai
ISI
USC
shucai@isi.edu
Madalina Georgescu
SDL
mgeorgescu@sdl.com
Kira Griffitt
LDC
kiragrif@ldc.upenn.edu
Ulf Hermjakob
ISI
USC
ulf@isi.edu
Kevin Knight
ISI
USC
knight@isi.edu
Philipp Koehn
School of Informatics
Univ. Edinburgh
pkoehn@inf.ed.ac.uk
Martha Palmer
Linguistics Dept.
Univ. Colorado
martha.palmer@colorado.edu
Nathan Schneider
LTI
CMU
nschneid@cs.cmu.edu
Abstract
We describe Abstract Meaning Represen-
tation (AMR), a semantic representation
language in which we are writing down
the meanings of thousands of English sen-
tences. We hope that a sembank of simple,
whole-sentence semantic structures will
spur new work in statistical natural lan-
guage understanding and generation, like
the Penn Treebank encouraged work on
statistical parsing. This paper gives an
overview of AMR and tools associated
with it.
1 Introduction
Syntactic treebanks have had tremendous impact
on natural language processing. The Penn Tree-
bank is a classic example?a simple, readable file
of natural-language sentences paired with rooted,
labeled syntactic trees. Researchers have ex-
ploited manually-built treebanks to build statisti-
cal parsers that improve in accuracy every year.
This success is due in part to the fact that we have
a single, whole-sentence parsing task, rather than
separate tasks and evaluations for base noun iden-
tification, prepositional phrase attachment, trace
recovery, verb-argument dependencies, etc. Those
smaller tasks are naturally solved as a by-product
of whole-sentence parsing, and in fact, solved bet-
ter than when approached in isolation.
By contrast, semantic annotation today is balka-
nized. We have separate annotations for named en-
tities, co-reference, semantic relations, discourse
connectives, temporal entities, etc. Each annota-
tion has its own associated evaluation, and training
data is split across many resources. We lack a sim-
ple readable sembank of English sentences paired
with their whole-sentence, logical meanings. We
believe a sizable sembank will lead to new work in
statistical natural language understanding (NLU),
resulting in semantic parsers that are as ubiquitous
as syntactic ones, and support natural language
generation (NLG) by providing a logical seman-
tic input.
Of course, when it comes to whole-sentence se-
mantic representations, linguistic and philosophi-
cal work is extensive. We draw on this work to de-
sign an Abstract Meaning Representation (AMR)
appropriate for sembanking. Our basic principles
are:
? AMRs are rooted, labeled graphs that are
easy for people to read, and easy for pro-
grams to traverse.
? AMR aims to abstract away from syntac-
tic idiosyncrasies. We attempt to assign the
same AMR to sentences that have the same
basic meaning. For example, the sentences
?he described her as a genius?, ?his descrip-
tion of her: genius?, and ?she was a ge-
nius, according to his description? are all as-
signed the same AMR.
? AMR makes extensive use of PropBank
framesets (Kingsbury and Palmer, 2002;
Palmer et al, 2005). For example, we rep-
resent a phrase like ?bond investor? using
the frame ?invest-01?, even though no verbs
appear in the phrase.
? AMR is agnostic about how we might want
to derive meanings from strings, or vice-
versa. In translating sentences to AMR, we
do not dictate a particular sequence of rule
applications or provide alignments that re-
flect such rule sequences. This makes sem-
banking very fast, and it allows researchers
to explore their own ideas about how strings
178
are related to meanings.
? AMR is heavily biased towards English. It
is not an Interlingua.
AMR is described in a 50-page annotation guide-
line.1 In this paper, we give a high-level descrip-
tion of AMR, with examples, and we also provide
pointers to software tools for evaluation and sem-
banking.
2 AMR Format
We write down AMRs as rooted, directed, edge-
labeled, leaf-labeled graphs. This is a com-
pletely traditional format, equivalent to the sim-
plest forms of feature structures (Shieber et al,
1986), conjunctions of logical triples, directed
graphs, and PENMAN inputs (Matthiessen and
Bateman, 1991). Figure 1 shows some of these
views for the sentence ?The boy wants to go?. We
use the graph notation for computer processing,
and we adapt the PENMAN notation for human
reading and writing.
3 AMR Content
In neo-Davidsonian fashion (Davidson, 1969), we
introduce variables (or graph nodes) for entities,
events, properties, and states. Leaves are labeled
with concepts, so that ?(b / boy)? refers to an in-
stance (called b) of the concept boy. Relations link
entities, so that ?(d / die-01 :location (p / park))?
means there was a death (d) in the park (p). When
an entity plays multiple roles in a sentence, we
employ re-entrancy in graph notation (nodes with
multiple parents) or variable re-use in PENMAN
notation.
AMR concepts are either English words
(?boy?), PropBank framesets (?want-01?), or spe-
cial keywords. Keywords include special entity
types (?date-entity?, ?world-region?, etc.), quan-
tities (?monetary-quantity?, ?distance-quantity?,
etc.), and logical conjunctions (?and?, etc).
AMR uses approximately 100 relations:
? Frame arguments, following PropBank
conventions. :arg0, :arg1, :arg2, :arg3, :arg4,
:arg5.
? General semantic relations. :accompa-
nier, :age, :beneficiary, :cause, :compared-to,
:concession, :condition, :consist-of, :degree,
:destination, :direction, :domain, :duration,
1AMR guideline: amr.isi.edu/language.html
LOGIC format:
? w, b, g:
instance(w, want-01) ? instance(g, go-01) ?
instance(b, boy) ? arg0(w, b) ?
arg1(w, g) ? arg0(g, b)
AMR format (based on PENMAN):
(w / want-01
:arg0 (b / boy)
:arg1 (g / go-01
:arg0 b))
GRAPH format:
Figure 1: Equivalent formats for representating
the meaning of ?The boy wants to go?.
:employed-by, :example, :extent, :frequency,
:instrument, :li, :location, :manner, :medium,
:mod, :mode, :name, :part, :path, :polarity,
:poss, :purpose, :source, :subevent, :subset,
:time, :topic, :value.
? Relations for quantities. :quant, :unit,
:scale.
? Relations for date-entities. :day, :month,
:year, :weekday, :time, :timezone, :quarter,
:dayperiod, :season, :year2, :decade, :cen-
tury, :calendar, :era.
? Relations for lists. :op1, :op2, :op3, :op4,
:op5, :op6, :op7, :op8, :op9, :op10.
AMR also includes the inverses of all these rela-
tions, e.g., :arg0-of, :location-of, and :quant-of. In
addition, every relation has an associated reifica-
tion, which is what we use when we want to mod-
ify the relation itself. For example, the reification
of :location is the concept ?be-located-at-91?.
Our set of concepts and relations is designed to
allow us represent all sentences, taking all words
into account, in a reasonably consistent manner. In
the rest of this section, we give examples of how
AMR represents various kinds of words, phrases,
and sentences. For full documentation, the reader
is referred to the AMR guidelines.
179
Frame arguments. We make heavy use of
PropBank framesets to abstract away from English
syntax. For example, the frameset ?describe-01?
has three pre-defined slots (:arg0 is the describer,
:arg1 is the thing described, and :arg2 is what it is
being described as).
(d / describe-01
:arg0 (m / man)
:arg1 (m2 / mission)
:arg2 (d / disaster))
The man described the mission as a disaster.
The man?s description of the mission:
disaster.
As the man described it, the mission was a
disaster.
Here, we do not annotate words like ?as? or ?it?,
considering them to be syntactic sugar.
General semantic relations. AMR also in-
cludes many non-core relations, such as :benefi-
ciary, :time, and :destination.
(s / hum-02
:arg0 (s2 / soldier)
:beneficiary (g / girl)
:time (w / walk-01
:arg0 g
:destination (t / town)))
The soldier hummed to the girl as she
walked to town.
Co-reference. AMR abstracts away from co-
reference gadgets like pronouns, zero-pronouns,
reflexives, control structures, etc. Instead we re-
use AMR variables, as with ?g? above. AMR
annotates sentences independent of context, so if
a pronoun has no antecedent in the sentence, its
nominative form is used, e.g., ?(h / he)?.
Inverse relations. We obtain rooted structures
by using inverse relations like :arg0-of and :quant-
of.
(s / sing-01
:arg0 (b / boy
:source (c / college)))
The boy from the college sang.
(b / boy
:arg0-of (s / sing-01)
:source (c / college))
the college boy who sang ...
(i / increase-01
:arg1 (n / number
:quant-of (p / panda)))
The number of pandas increased.
The top-level root of an AMR represents the fo-
cus of the sentence or phrase. Once we have se-
lected the root concept for an entire AMR, there
are no more focus considerations?everything else
is driven strictly by semantic relations.
Modals and negation. AMR represents nega-
tion logically with :polarity, and it expresses
modals with concepts.
(g / go-01
:arg0 (b / boy)
:polarity -)
The boy did not go.
(p / possible
:domain (g / go-01
:arg0 (b / boy))
:polarity -))
The boy cannot go.
It?s not possible for the boy to go.
(p / possible
:domain (g / go-01
:arg0 (b / boy)
:polarity -))
It?s possible for the boy not to go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy))
:polarity -)
The boy doesn?t have to go.
The boy isn?t obligated to go.
The boy need not go.
(p / obligate-01
:arg2 (g / go-01
:arg0 (b / boy)
:polarity -))
The boy must not go.
It?s obligatory that the boy not go.
(t / think-01
:arg0 (b / boy)
:arg1 (w / win-01
:arg0 (t / team)
:polarity -))
The boy doesn?t think the team will win.
The boy thinks the team won?t win.
Questions. AMR uses the concept ?amr-
unknown?, in place, to indicate wh-questions.
(f / find-01
:arg0 (g / girl)
:arg1 (a / amr-unknown))
What did the girl find?
(f / find-01
:arg0 (g / girl)
:arg1 (b / boy)
:location (a / amr-unknown))
Where did the girl find the boy?
180
(f / find-01
:arg0 (g / girl)
:arg1 (t / toy
:poss (a / amr-unknown)))
Whose toy did the girl find?
Yes-no questions, imperatives, and embedded wh-
clauses are treated separately with the AMR rela-
tion :mode.
Verbs. Nearly every English verb and verb-
particle construction we have encountered has a
corresponding PropBank frameset.
(l / look-05
:arg0 (b / boy)
:arg1 (a / answer))
The boy looked up the answer.
The boy looked the answer up.
AMR abstracts away from light-verb construc-
tions.
(a / adjust-01
:arg0 (g / girl)
:arg1 (m / machine))
The girl adjusted the machine.
The girl made adjustments to the machine.
Nouns.We use PropBank verb framesets to rep-
resent many nouns as well.
(d / destroy-01
:arg0 (b / boy)
:arg1 (r / room))
the destruction of the room by the boy ...
the boy?s destruction of the room ...
The boy destroyed the room.
We never say ?destruction-01? in AMR. Some
nominalizations refer to a whole event, while oth-
ers refer to a role player in an event.
(s / see-01
:arg0 (j / judge)
:arg1 (e / explode-01))
The judge saw the explosion.
(r / read-01
:arg0 (j / judge)
:arg1 (t / thing
:arg1-of (p / propose-01))
The judge read the proposal.
(t / thing
:arg1-of (o / opine-01
:arg0 (g / girl)))
the girl?s opinion
the opinion of the girl
what the girl opined
Many ?-er? nouns invoke PropBank framesets.
This enables us to make use of slots defined for
those framesets.
(p / person
:arg0-of (i / invest-01))
investor
(p / person
:arg0-of (i / invest-01
:arg1 (b / bond)))
bond investor
(p / person
:arg0-of (i / invest-01
:manner (s / small)))
small investor
(w / work-01
:arg0 (b / boy)
:manner (h / hard))
the boy is a hard worker
the boy works hard
However, a treasurer is not someone who trea-
sures, and a president is not (just) someone who
presides.
Adjectives. Various adjectives invoke Prop-
Bank framesets.
(s / spy
:arg0-of (a / attract-01))
the attractive spy
(s / spy
:arg0-of (a / attract-01
:arg1 (w / woman)))
the spy who is attractive to women
?-ed? adjectives frequently invoke verb framesets.
For example, ?acquainted with magic? maps to
?acquaint-01?. However, we are not restricted to
framesets that can be reached through morpholog-
ical simplification.
(f / fear-01
:arg0 (s / soldier)
:arg1 (b / battle-01))
The soldier was afraid of battle.
The soldier feared battle.
The soldier had a fear of battle.
For other adjectives, we have defined new frame-
sets.
(r / responsible-41
:arg1 (b / boy)
:arg2 (w / work))
The boy is responsible for the work.
The boy has responsibility for the work.
While ?the boy responsibles the work? is not good
English, it is perfectly good Chinese. Similarly,
we handle tough-constructions logically.
181
(t / tough
:domain (p / please-01
:arg1 (g / girl)))
Girls are tough to please.
It is tough to please girls.
Pleasing girls is tough.
?please-01? and ?girl? are adjacent in the AMR,
even if they are not adjacent in English. ?-able?
adjectives often invoke the AMR concept ?possi-
ble?, but not always (e.g., a ?taxable fund? is actu-
ally a ?taxed fund?).
(s / sandwich
:arg1-of (e / eat-01
:domain-of (p / possible)))
an edible sandwich
(f / fund
:arg1-of (t / tax-01))
a taxable fund
Pertainym adjectives are normalized to root form.
(b / bomb
:mod (a / atom))
atom bomb
atomic bomb
Prepositions. Most prepositions simply sig-
nal semantic frame elements, and are themselves
dropped from AMR.
(d / default-01
:arg1 (n / nation)
:time (d2 / date-entity
:month 6))
The nation defaulted in June.
Time and location prepositions are kept if they
carry additional information.
(d / default-01
:arg1 (n / nation)
:time (a / after
:op1 (w / war-01))
The nation defaulted after the war.
Occasionally, neither PropBank nor AMR has an
appropriate relation, in which case we hold our
nose and use a :prep-X relation.
(s / sue-01
:arg1 (m / man)
:prep-in (c / case))
The man was sued in the case.
Named entities. Any concept in AMR can be
modified with a :name relation. However, AMR
includes standardized forms for approximately 80
named-entity types, including person, country,
sports-facility, etc.
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown"))
Mollie Brown
(p / person
:name (n / name
:op1 "Mollie"
:op2 "Brown")
:arg0-of (s / slay-01
:arg1 (o / orc)))
the orc-slaying Mollie Brown
Mollie Brown, who slew orcs
AMR does not normalize multiple ways of re-
ferring to the same concept (e.g., ?US? versus
?United States?). It also avoids analyzing seman-
tic relations inside a named entity?e.g., an orga-
nization named ?Stop Malaria Now? does not in-
voke the ?stop-01? frameset. AMR gives a clean,
uniform treatment to titles, appositives, and other
constructions.
(c / city
:name (n / name
:op1 "Zintan"))
Zintan
the city of Zintan
(p / president
:name (n / name
:op1 "Obama"))
President Obama
Obama, the president ...
(g / group
:name (n / name
:op1 "Elsevier"
:op2 "N.V.")
:mod (c / country
:name (n2 / name
:op1 "Netherlands"))
:arg0-of (p / publish-01))
Elsevier N.V., the Dutch publishing group...
Dutch publishing group Elsevier N.V. ...
Copula. Copulas use the :domain relation.
(w / white
:domain (m / marble))
The marble is white.
(l / lawyer
:domain (w / woman))
The woman is a lawyer.
(a / appropriate
:domain (c / comment)
:polarity -))
The comment is not appropriate.
182
The comment is inappropriate.
Reification. Sometimes we want to use an
AMR relation as a first-class concept?to be able
to modify it, for example. Every AMR relation has
a corresponding reification for this purpose.
(m / marble
:location (j / jar))
the marble in the jar ...
(b / be-located-at-91
:arg1 (m / marble)
:arg2 (j / jar)
:polarity -)
:time (y / yesterday))
The marble was not in the jar yesterday.
If we do not use the reification, we run into trou-
ble.
(m / marble
:location (j / jar
:polarity -)
:time (y / yesterday))
yesterday?s marble in the non-jar ...
Some reifications are standard PropBank frame-
sets (e.g., ?cause-01? for :cause, or ?age-01? for
:age).
This ends the summary of AMR content. For
lack of space, we omit descriptions of compara-
tives, superlatives, conjunction, possession, deter-
miners, date entities, numbers, approximate num-
bers, discourse connectives, and other phenomena
covered in the full AMR guidelines.
4 Limitations of AMR
AMR does not represent inflectional morphology
for tense and number, and it omits articles. This
speeds up the annotation process, and we do not
have a nice semantic target representation for these
phenomena. A lightweight syntactic-style repre-
sentation could be layered in, via an automatic
post-process.
AMR has no universal quantifier. Words like
?all? modify their head concepts. AMR does not
distinguish between real events and hypothetical,
future, or imagined ones. For example, in ?the boy
wants to go?, the instances of ?want-01? and ?go-
01? have the same status, even though the ?go-01?
may or may not happen.
We represent ?history teacher? nicely as ?(p /
person :arg0-of (t / teach-01 :arg1 (h / history)))?.
However, ?history professor? becomes ?(p / pro-
fessor :mod (h / history))?, because ?profess-01?
is not an appropriate frame. It would be reason-
able in such cases to use a NomBank (Meyers et
al., 2004) noun frame with appropriate slots.
5 Creating AMRs
We have developed a power editor for AMR, ac-
cessible by web interface.2 The AMR Editor al-
lows rapid, incremental AMR construction via text
commands and graphical buttons. It includes on-
line documentation of relations, quantities, reifi-
cations, etc., with full examples. Users log in,
and the editor records AMR activity. The ed-
itor also provides significant guidance aimed at
increasing annotator consistency. For example,
users are warned about incorrect relations, discon-
nected AMRs, words that have PropBank frames,
etc. Users can also search existing sembanks for
phrases to see how they were handled in the past.
The editor also allows side-by-side comparison of
AMRs from different users, for training purposes.
In order to assess inter-annotator agreement
(IAA), as well as automatic AMR parsing accu-
racy, we developed the smatch metric (Cai and
Knight, 2013) and associated script.3 Smatch re-
ports the semantic overlap between two AMRs by
viewing each AMR as a conjunction of logical
triples (see Figure 1). Smatch computes precision,
recall, and F-score of one AMR?s triples against
the other?s. To match up variables from two in-
put AMRs, smatch needs to execute a brief search,
looking for the variable mapping that yields the
highest F-score.
Smatch makes no reference to English strings
or word indices, as we do not enforce any par-
ticular string-to-meaning derivation. Instead, we
compare semantic representations directly, in the
same way that the MT metric Bleu (Papineni et
al., 2002) compares target strings without making
reference to the source.
For an initial IAA study, and prior to adjust-
ing the AMR Editor to encourage consistency, 4
expert AMR annotators annotated 100 newswire
sentences and 80 web text sentences. They then
created consensus AMRs through discussion. The
average annotator vs. consensus IAA (smatch) was
0.83 for newswire and 0.79 for web text. When
newly trained annotators doubly annotated 382
web text sentences, their annotator vs. annotator
IAA was 0.71.
2AMR Editor: amr.isi.edu/editor.html
3Smatch: amr.isi.edu/evaluation.html
183
6 Current AMR Bank
We currently have a manually-constructed AMR
bank of several thousand sentences, a subset of
which can be freely downloaded,4 the rest being
distributed via the LDC catalog.
In initially developing AMR, the authors built
consensus AMRs for:
? 225 short sentences for tutorial purposes
? 142 sentences of newswire (*)
? 100 sentences of web data (*)
Trained annotators at LDC then produced AMRs
for:
? 1546 sentences from the novel ?The Little
Prince?
? 1328 sentences of web data
? 1110 sentences of web data (*)
? 926 sentences from Xinhua news (*)
? 214 sentences from CCTV broadcast con-
versation (*)
Collections marked with a star (*) are also in
the OntoNotes corpus (Pradhan et al, 2007;
Weischedel et al, 2011).
Using the AMR Editor, annotators are able to
translate a full sentence into AMR in 7-10 minutes
and postedit an AMR in 1-3 minutes.
7 Related Work
Researchers working on whole-sentence semantic
parsing today typically use small, domain-specific
sembanks like GeoQuery (Wong and Mooney,
2006). The need for larger, broad-coverage sem-
banks has sparked several projects, including the
Groningen Meaning Bank (GMB) (Basile et al,
2012a), UCCA (Abend and Rappoport, 2013),
the Semantic Treebank (ST) (Butler and Yoshi-
moto, 2012), the Prague Dependency Treebank
(Bo?hmova? et al, 2003), and UNL (Uchida et al,
1999; Uchida et al, 1996; Martins, 2012).
Concepts. Most systems use English words
as concepts. AMR uses PropBank frames (e.g.,
?describe-01?), and UNL uses English WordNet
synsets (e.g., ?200752493?).
Relations. GMB uses VerbNet roles (Schuler,
2005), and AMR uses frame-specific PropBank
relations. UNL has a dedicated set of over 30 fre-
quently used relations.
Formalism. GMB meanings are written in
DRT (Kamp et al, 2011), exploiting full first-
4amr.isi.edu/download.html
order logic. GMB and ST both include universal
quantification.
Granularity. GMB and UCCA annotate short
texts, so that the same entity can participate in
events described in different sentences; other sys-
tems annotate individual sentences.
Entities. AMR uses 80 entity types, while
GMB uses 7.
Manual versus automatic. AMR, UNL, and
UCCA annotation is fully manual. GMB and ST
produce meaning representations automatically,
and these can be corrected by experts or crowds
(Venhuizen et al, 2013).
Derivations. AMR and UNL remain agnostic
about the relation between strings and their mean-
ings, considering this a topic of open research.
ST and GMB annotate words and phrases directly,
recording derivations as (for example) Montague-
style compositional semantic rules operating on
CCG parses.
Top-down verus bottom-up. AMR annota-
tors find it fast to construct meanings from the
top down, starting with the main idea of the sen-
tence (though the AMR Editor allows bottom-up
construction). GMB and UCCA annotators work
bottom-up.
Editors, guidelines, genres. These projects
have graphical sembanking tools (e.g., Basile et al
(2012b)), annotation guidelines,5 and sembanks
that cover a wide range of genres, from news to
fiction. UNL and AMR have both annotated many
of the same sentences, providing the potential for
direct comparison.
8 Future Work
Sembanking. Our main goal is to continue
sembanking. We would like to employ a large
sembank to create shared tasks for natural lan-
guage understanding and generation. These
tasks may additionally drive interest in theoreti-
cal frameworks for probabilistically mapping be-
tween graphs and strings (Quernheim and Knight,
2012b; Quernheim and Knight, 2012a; Chiang et
al., 2013).
Applications. Just as syntactic parsing has
found many unanticipated applications, we expect
sembanks and statistical semantic processors to be
used for many purposes. To get started, we are
exploring the use of statistical NLU and NLG in
5UNL guidelines: www.undl.org/unlsys/unl/unl2005
184
a semantics-based machine translation (MT) sys-
tem. In this system, we annotate bilingual Chi-
nese/English data with AMR, then train compo-
nents to map Chinese to AMR, and AMR to En-
glish. A prototype is described by Jones et al
(2012).
Disjunctive AMR. AMR aims to canonicalize
multiple ways of saying the same thing. We plan
to test how well we are doing by building AMRs
on top of large, manually-constructed paraphrase
networks from the HyTER project (Dreyer and
Marcu, 2012). Rather than build individual AMRs
for different paths through a network, we will con-
struct highly-packed disjunctive AMRs. With this
application in mind, we have developed a guide-
line6 for disjunctive AMR. Here is an example:
(o / *OR*
:op1 (t / talk-01)
:op2 (m / meet-03)
:OR (o2 / *OR*
:mod (o3 / official)
:arg1-of (s / sanction-01
:arg0 (s2 / state))))
official talks
state-sanctioned talks
meetings sanctioned by the state
AMR extensions. Finally, we would like
to deepen the AMR language to include more
relations (to replace :mod and :prep-X, for
example), entity normalization (perhaps wik-
ification), quantification, and temporal rela-
tions. Ultimately, we would like to also in-
clude a comprehensive set of more abstract
frames like ?Earthquake-01? (:magnitude, :epi-
center, :casualties), ?CriminalLawsuit-01? (:de-
fendant, :crime, :jurisdiction), and ?Pregnancy-
01? (:father, :mother, :due-date). Projects like
FrameNet (Baker et al, 1998) and CYC (Lenat,
1995) have long pursued such a set.
References
O. Abend and A. Rappoport. 2013. UCCA: A
semantics-based grammatical annotation scheme. In
Proc. IWCS.
C. Baker, C. Fillmore, and J. Lowe. 1998. The Berke-
ley FrameNet project. In Proc. COLING.
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012a.
Developing a large semantically annotated corpus.
In Proc. LREC.
6Disjunctive AMR guideline: amr.isi.edu/damr.1.0.pdf
V. Basile, J. Bos, K. Evang, and N. Venhuizen. 2012b.
A platform for collaborative semantic annotation. In
Proc. EACL demonstrations.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?.
2003. The Prague dependency treebank. In Tree-
banks. Springer.
A. Butler and K. Yoshimoto. 2012. Banking meaning
representations from treebanks. Linguistic Issues in
Language Technology, 7.
S. Cai and K. Knight. 2013. Smatch: An accu-
racy metric for abstract meaning representations. In
Proc. ACL.
D. Chiang, J. Andreas, D. Bauer, K. M. Hermann,
B. Jones, and K. Knight. 2013. Parsing graphs with
hyperedge replacement grammars. In Proc. ACL.
D. Davidson. 1969. The individuation of events.
In N. Rescher, editor, Essays in Honor of Carl G.
Hempel. D. Reidel, Dordrecht.
M. Dreyer and D. Marcu. 2012. Hyter: Meaning-
equivalent semantics for translation evaluation. In
Proc. NAACL.
B. Jones, J. Andreas, D. Bauer, K. M. Hermann, and
K. Knight. 2012. Semantics-based machine trans-
lation with hyperedge replacement grammars. In
Proc. COLING.
H. Kamp, J. Van Genabith, and U. Reyle. 2011. Dis-
course representation theory. In Handbook of philo-
sophical logic, pages 125?394. Springer.
P. Kingsbury and M. Palmer. 2002. From TreeBank to
PropBank. In Proc. LREC.
D. B. Lenat. 1995. Cyc: A large-scale investment in
knowledge infrastructure. Communications of the
ACM, 38(11).
R. Martins. 2012. Le Petit Prince in UNL. In Proc.
LREC.
C. M. I. M. Matthiessen and J. A. Bateman. 1991.
Text Generation and Systemic-Functional Linguis-
tics. Pinter, London.
A. Meyers, R. Reeves, C. Macleod, R. Szekely,
V. Zielinska, B. Young, and R. Grishman. 2004.
The NomBank project: An interim report. In HLT-
NAACL 2004 workshop: Frontiers in corpus anno-
tation.
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The
Proposition Bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1).
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In ACL, Philadelphia, PA.
185
S. Pradhan, E. Hovy, M. Marcus, M. Palmer,
L. Ramshaw, and R. Weischedel. 2007. Ontonotes:
A unified relational semantic representation. In-
ternational Journal of Semantic Computing (IJSC),
1(4).
D. Quernheim and K. Knight. 2012a. DAGGER: A
toolkit for automata on directed acyclic graphs. In
Proc. FSMNLP.
D. Quernheim and K. Knight. 2012b. Towards prob-
abilistic acceptors and transducers for feature struc-
tures. In Proc. SSST Workshop.
K. Schuler. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, University of
Pennsylvania.
S. Shieber, F. C. N. Pereira, L. Karttunen, and M. Kay.
1986. Compilation of papers on unification-based
grammar formalisms. Technical Report CSLI-86-
48, Center for the Study of Language and Informa-
tion, Stanford, California.
H. Uchida, M. Zhu, and T. Della Senta. 1996. UNL:
Universal Networking Language?an electronic lan-
guage for communication, understanding and col-
laboration. Technical report, IAS/UNU Tokyo.
H. Uchida, M. Zhu, and T. Della Senta. 1999. A
gift for a millennium. Technical report, IAS/UNU
Tokyo.
N. Venhuizen, V. Basile, K. Evang, and J. Bos. 2013.
Gamification for word sense labeling. In Proc.
IWCS.
R. Weischedel, E. Hovy, M. Marcus, M. Palmer,
R. Belvin, S. Pradhan, L. Ramshaw, and N. Xue.
2011. OntoNotes: A large training corpus for en-
hanced processing. In J. Olive, C. Christianson, and
J. McCary, editors, Handbook of Natural Language
Processing and Machine Translation. Springer.
Y. W. Wong and R. J. Mooney. 2006. Learning for se-
mantic parsing with statistical machine translation.
In Proc. HLT-NAACL.
186
Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 94?98,
Gothenburg, Sweden, 26-27 April 2014.
c?2014 Association for Computational Linguistics
An Approach to Take Multi-Word Expressions
Claire Bonial
?
Meredith Green
??
Jenette Preciado
??
Martha Palmer
?
?
Department of Linguistics, University of Colorado at Boulder
??
Institute of Cognitive Science, University of Colorado at Boulder
{Claire.Bonial,Laura.Green,Jenette.Preciado,Martha.Palmer}@colorado.edu
Abstract
This research discusses preliminary efforts to
expand the coverage of the PropBank lexicon
to multi-word and idiomatic expressions, such
as take one for the team. Given overwhelming
numbers of such expressions, an efficient way
for increasing coverage is needed. This re-
search discusses an approach to adding multi-
word expressions to the PropBank lexicon in
an effective yet semantically rich fashion. The
pilot discussed here uses double annotation
of take multi-word expressions, where anno-
tations provide information on the best strat-
egy for adding the multi-word expression to
the lexicon. This work represents an impor-
tant step for enriching the semantic informa-
tion included in the PropBank corpus, which
is a valuable and comprehensive resource for
the field of Natural Language Processing.
1 Introduction
The PropBank (PB) corpus provides informa-
tion associating semantic roles with certain syn-
tactic structures, thereby contributing valuable
training data for Natural Language Process-
ing (NLP) applications (Palmer et al., 2005).
For example, recent research shows that us-
ing semantic role information in machine trans-
lation systems improves performance (Lo, Be-
loucif & Wu, 2013). Despite these successes,
PB could be improved with greater coverage
of multi-word expressions (MWEs). The PB
lexicon (http://verbs.colorado.edu/PB/framesets-
english) is comprised of senses of verb, noun and
adjective relations, with a listing of their seman-
tic roles (thus a sense is referred to as a ?roleset?).
Although the lexicon encompasses nearly 12,000
rolesets, relatively few of these apply to instances
of MWEs. PB has previously treated language
as if it were purely compositional, and has there-
fore lumped the majority of MWEs in with lexi-
cal verb usages. For example, annotations of the
single PB sense of take meaning acquire, come to
have, choose, bring with you from somewhere in-
clude MWEs such as take measures, take comfort
and take advantage, and likely others. Although
PB senses typically, and this sense especially, are
quite coarse-grained, valuable semantic informa-
tion is lost when these distinct MWEs are lumped
together with other lexical senses.
The importance of coverage for MWEs is
underscored by their prevalence. Jackendoff
(1997:156) estimates that the number of MWEs
in a speaker?s lexicon is approximately equal to
the number of single words, and in WordNet
1.7 (Fellbaum, 1998), 41% of the entries were
MWEs (cited in Sag et al., 2002). Furthermore,
Sag (2002) estimates the vocabularies of special-
ized domains will continue to contribute more
MWEs than simplex words. For systems like PB
to continue to provide adequate training data for
NLP systems, coverage must extend to MWEs.
The lack of coverage in this area has already
become problematic for the recently developed
Abstract Meaning Representation (AMR) project
(Banarescu et al., 2013), which relies upon the PB
lexicon, or ?frame files? as the groundwork for its
annotations. As AMR and PB have extended into
more informal domains, such as online discussion
forums and SMS texts, the gaps in coverage of
MWEs have become more and more problematic.
To address this issue, this research discusses a pi-
lot approach to increasing the coverage of the PB
lexicon to a variety of MWEs involving the verb
take, demonstrating a methodology for efficiently
augmenting the lexicon with MWEs.
2 PB Background
PB annotation was developed to provide training
data for supervised machine learning classifiers.
It provides semantic information, including the
94
basic ?who is doing what to whom,? in the form of
predicate-by-predicate semantic role assignments.
The annotation firstly consists of the selection of a
roleset, or a coarse-grained sense of the predicate,
which includes a listing of the roles, expressed
as generic argument numbers, associated with
that sense. Here, for example, is the roleset for
Take.01, mentioned previously:
Take.01: acquire, come to have, choose, bring
Arg0: Taker
Arg1: Thing taken
Arg2: Taken-from, source of thing taken
Arg3: Destination
These argument numbers, along with a variety
of modifier tags, such as temporal and locative,
are assigned to natural language sentences drawn
from a variety of corpora. The roleset and example
sentences serve as a guide to annotators on how to
assign argument numbers to annotation instances.
The goal is to assign these simple, general-purpose
labels consistently across the many possible syn-
tactic realizations of the same event participant or
semantic role.
PB has recently undertaken efforts to expand
the types of predicates that are annotated. Pre-
viously, annotation efforts focused on verbs, but
events generally, and even the same event, can of-
ten be expressed with a variety of different parts of
speech, or with MWEs. For example,
1. He fears bears.
2. His fear of bears...
3. He is afraid of bears.
4. He has a fear of bears.
Thus, it has been necessary to expand PB annota-
tions to provide coverage for noun, adjective and
complex predicates. While this greatly enriches
the semantics that PB is able to capture, it has also
forced the creation of an overwhelming number of
new rolesets, as generally each new predicate type
receives its own set of rolesets. To alleviate this,
PB has opted to begin unifying frame files through
a process of ?aliasing?(Bonial et al., 2014). In
this process, etymologically related concepts are
aliased to each other, and aliased rolesets are uni-
fied, so that there is a single roleset representing,
for example the concept of ?fear,? and this roleset
is used for all syntactic instantiations of that con-
cept.
This methodology is suited to complex pred-
icates, such as light verb constructions (LVCs),
wherein the eventive noun, carrying the bulk of the
event semantics, may have an etymologically re-
lated verb that is identical in its participants or se-
mantic roles (for a description of LVC annotation,
see (Hwang et al., 2010). Thus, have a fear above
is aliased to fear, as take a bath would be aliased
to bathe. In this research, the possibility of ex-
tending aliasing to a variety of MWEs is explored,
such that take it easy, as in ?I?m just going to take
it easy on Saturday,? would be aliased to the exist-
ing lexical verb roleset for relax. In many cases,
the semantics of MWEs are quite complex, adding
shades of meaning that no lexical verb quite cap-
tures. Thus, additional strategies beyond aliasing
are developed; each strategy is discussed in the
following sections.
3 Take Pilot
For the purposes of this pilot, the take MWEs
were gathered from WordNet?s MWE and phrasal
verb entries (Fellbaum, 1998), the Prague Czech-
English Dependency Treebank (Haji?c-2012), and
Afsaneh Fazly?s dissertation work (Fazly, 2007).
Graduate student annotators were trained to use
WordNet, Sketch Engine (Kilgarriff et al., 2004)
and PB to complete double-blind annotation of
these MWEs as a candidate for one of the three fol-
lowing strategies for increasing roleset coverage:
1) Aliasing the MWE to a lexically-similar verb
or noun roleset from PB, 2) proposing the creation
of groups of expressions for which one or several
rolesets will be created, or 3) simply designating
the MWE as an idiomatic expression. First, anno-
tators were to try to choose a verb or noun roleset
from PB that most closely resembled the syntax
and semantics of the MWE. Annotators also made
comments as necessary for difficult cases. The
annotators were considered to have agreed if the
proposed lexical verb or noun alias was the same.
Strategies (2) and (3) were pursued during adjudi-
cation if the annotators were unable to agree upon
an appropriate alias. Each of the possible strate-
gies for increasing coverage is discussed in turn in
the following sections.
3.1 Aliasing
Aliasing involves proposing an existing roleset
from PB as a suitable roleset for future MWE an-
notation. LVCs were the simplest of these to alias
95
since the eventive or stative noun predicate (e.g.:
take a look) may already have an existing role-
set, or there is likely an existing, etymologically
related verb roleset (e.g. verb roleset Look.01).
Some other MWEs were not so straightforward.
For instance, take time off does not include an et-
ymologically related predicate that would easily
encompass the semantics of the MWE, so the an-
notators proposed a roleset that is not as intuitive,
but captures the semantics nonetheless: the role-
set for the noun vacation. This frame allows for
an Agent to take time off, and importantly, what
time is taken off from: take time off from work,
school etc. Selecting an appropriate alias is the
ideal strategy for increasing coverage, because it
does not require the time and effort of manually
creating a new roleset or rolesets.
Both of the instances discussed above are rather
simple cases, where their coverage can be ad-
dressed efficiently through aliasing. However,
many MWE instances were considerably more
difficult to assign to an equivalent roleset. One
such example includes take shape, for which the
annotators decided that shape was an appropriate
roleset. Yet, shape does not quite cover the unique
semantics of take shape, which lacks the possibil-
ity of an Agent. In these cases, the MWEs may
still be aliased, but they should also include an
semantic constraint to convey the semantic differ-
ence, such as ?-Agent? Thus, in some cases, these
types of semantic constraints were used for aliases
that were almost adequate, but lacked some shade
of meaning conveyed by the MWE. In other cases,
the semantic difference between an MWE and ex-
isting lexical verb or noun roleset was too great
to be captured by the addition of such constraints,
thus a new roleset or group of rolesets was created
to address coverage of such MWEs, as described
in the next section.
3.2 Groups of Syntactically/Lexically Similar
Rolesets
In cases in which it was not possible to find a
single adequate alias for an MWE, a group of
rolesets representing different senses of the same
MWE was created. For example, take down can
mean to write something down, to defeat some-
thing, or to deconstruct something. Thus, a group
of take down rolesets were added, with each role-
set reflecting one of these senses.
Similarly, some of the proposed rolesets for
take MWEs were easily subsumed under a more
coarse-grained, new frame in PB. For instance,
take one?s lumps and take it on the chin both
more or less mean to endure or atone for, so com-
bining these in a coarser-grained MWE frame is
both efficient and allows for valuable distinctions
in terms of semantic role labeling. Namely, the
Agent choosing to atone for something, and what
the entity is atoning for. However, such situations
in which it?s possible to create new coarse-grained
MWE rolesets seem to be rare. Some MWEs ini-
tially seem similar enough to combine into a sin-
gle roleset, but further exploration of usages shows
that they are semantically different. Take comfort
and take heart in both involve improving mood,
but take heart in might be more closely-related to
hope in meaning, while take comfort in might sim-
ply mean to cheer up.
3.3 Idiomatic Expression Designation
In cases in which PB annotation would be very dif-
ficult for annotators, due to polysemy or semantics
that cannot be conveyed by aliasing to an exist-
ing roleset, MWEs will be listed for future annota-
tion as Idiomatic Expressions (IE), which get spe-
cial treatment. This designation indicates that the
MWE is so unique that it would require its own
new roleset(s) in PB, and even with these role-
sets, annotators may still have difficulty determin-
ing the appropriate roleset choice or sense of the
MWE. As mentioned previously, creating multi-
ple rolesets for each expression is inefficient, es-
pecially so if the rolesets manually created will be
difficult to distinguish; thus, currently such cases
are simply marked with the generic IE roleset.
The MWE take the count is an illustrative exam-
ple of this type of case. Undergraduate and grad-
uate annotators trained in linguistics tend to have
difficulty with detailed sports references in anno-
tation instances, regardless of how much context
is provided. This MWE applies to several sports
scenarios: one can take the count in boxing or
take the (full) count in baseball, and some usages
were even found for football, where many speak-
ers would use run down the clock. Annotators
unfamiliar with the somewhat esoteric meanings
of these phrases would undoubtedly have trouble
distinguishing the rolesets and arguments of the
rolesets, thus take the count in sports contexts (as
opposed to the LVC take the count, meaning to
count) will simply be designated IE.
96
Currently, IE instances are simply set aside
from the rest of the PB corpus, so as to avoid these
instances adding noise to the data. In the future,
these IE expressions will need to be treated indi-
vidually to determine the best way to capture their
unique semantics.
4 Results & Conclusions
One way of analyzing the validity of this method-
ology is to examine the Inter-Annotator Agree-
ment (IAA) on the proposed alias. After the
training period (in which about 60 MWEs were
investigated as a group), annotators worked on
double-blind annotation of 100 additional MWEs.
Of these, 17 were found to be repeats of earlier
MWEs. Of the remaining 83, annotators agreed
on the exact alias in 32 cases, giving a rather poor,
simple IAA of about 39%. However, the stan-
dards used to calculate IAA were rigid, as only
instances in which the annotators aliased the mul-
tiword expressions to exactly the same lexical verb
or noun roleset were counted as an agreement.
Annotators often disagreed on lexical verbs, but
still chose verbs that were extraordinarily similar.
Take, for example, the MWE take back. One an-
notator chose to alias this MWE to retract while
the other annotator chose reclaim. It is safe to say
that both of these lexical verbs are equally logical
choices for take back and have similar semantic
and syntactic qualities. In other cases, annotators
had discovered different senses in their research
of usages, and therefore the aliases reflect differ-
ent senses of the MWE. Instances like these were
marked as disagreements, resulting in a mislead-
ingly low IAA. After discussion of disagreements,
IAA for these 83 MWEs rose to 78%, leaving 18
MWEs for which the annotators were unable to
agree on a strategy. Annotation proceeded with an
additional 76 MWEs, and for this set annotators
disagreed on only 6 MWEs. This process demon-
strates that although annotators may not agree on
the first alias that comes to mind, they tend to
agree on similar verbs that can capture the seman-
tics of an MWE appropriately. In a final adjudica-
tion pass, adjudicators discussed the cases of dis-
agreement with the annotators and made a final de-
cision on the strategy to be pursued.
In all, 159 unique MWEs were examined in
double-blind annotation. Of these, 21 were dis-
carded either because annotators felt they were
not truly MWEs, and could be treated composi-
tionally, or because they were very slight variants
of other MWEs. The following table shows how
many of the remaining 138 MWEs were agreed
upon for aliasing (and how many of these were
thought to be LVCs), how many cases led to the
addition of new rolesets, how many will be la-
beled IE in future annotation, and how many will
remain classed with the existing Take senses (note
that 4 MWEs were classed as having both a poten-
tial alias for LVC usages, and requiring rolesets
or another strategy for other usages; for example,
take the count discussed above). Overall, this pilot
MWE Example Strategy Count
take tumble Alias-LVC 45
take it easy Alias-nonLVC 55
take down Roleset(s) Created 20
take count IE 4
take home Take.XX 18
Table 1: MWE cases addressed by each strategy.
demonstrated that the approach is promising, con-
sidering that it requires only about 20 new rolesets
to be created, as opposed to over 138 (given that
some MWEs have multiple senses, requiring mul-
tiple rolesets). As annotations move on to addi-
tional MWEs involving other verbs, a similar re-
duction in the roleset workload will be invaluable
to expanding PB.
5 Future Work
The next step in this research is to complete the
roleset unification, which allows the aliasing to
take effect. This process is currently underway.
Once this is complete, an investigation of take
annotations using the unified rolesets will be un-
dertaken, with special focus on whether IAA for
take instances is improved, and whether perfor-
mance of automatic Semantic Role Labeling and
Word Sense Disambiguation applications trained
on this data is improved. If results in these areas
are promising, this research will shift to analyzing
make, get, and have MWEs with this methodology.
Acknowledgments
We gratefully acknowledge the support of the
National Science Foundation Grant NSF-IIS-
1116782, A Bayesian Approach to Dynamic Lex-
ical Resources for Flexible Language Process-
ing, and funding under the BOLT and Machine
97
Reading programs, HR0011-11-C-0145 (BOLT)
FA8750-09-C-0179 (M.R.). Any opinions, find-
ings, and conclusions or recommendations ex-
pressed in this material are those of the authors
and do not necessarily reflect the views of the Na-
tional Science Foundation.
References
L. Banarescu, C. Bonial, S. Cai, M. Georgescu, K.
Griffitt, U. Hermjakob, K. Knight, P. Koehn, M.
Palmer, and N. Schneider 2013. Abstract Mean-
ing Representation for Sembanking. Proceedings of
the Linguistic Annotation Workshop.
Claire Bonial, Julia Bonn, Kathryn Conger, Jena D.
Hwang and Martha Palmer. In preparation. Prop-
Bank: Semantics of New Predicate Types. Pro-
ceedings of the Language Resources and Evaluation
Conference - LREC-2014. Reykjavik, Iceland.
Jan Haji?c, Eva Haji?cov, Jarmila Panevov, Petr Sgall,
Silvie Cinkov, Eva Fu?ckov, Marie Mikulov, Petr
Pajas, Jan Popelka, Ji?r Semeck?y, Jana
?
Sindlerov,
Jan
?
St?epnek, Josef Toman, Zde?nka Ure?sov, Zden?ek
?
Zabokrtsk?y. 2012. Prague Czech-English Depen-
dency Treebank 2.0. Linguistic Data Consortium,
Philadelphia.
Afsaneh Fazly. 2007. Automatic Acquisition of Lexical
Knowledge about Multiword Predicates. PhD The-
sis, Department of Computer Science, University of
Toronto.
Christiane Fellbaum (Ed.) 1998. Wordnet: An Elec-
tronic Lexical Database. MIT press, Cambridge.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Nianwen Xue and
Martha Palmer. 2010. PropBank Annotation of
Multilingual Light Verb Constructions Proceedings
of the Linguistic Annotation Workshop held in con-
junction with ACL-2010. Uppsala, Sweden.
Adam Kilgarriff, Pavel Rychly, Pavel Smrz, and David
Tugwell. 2004. The Sketch Engine. Proceedings of
EURALEX. Lorient, France.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. 2013.
Improving machine translation into Chinese by tun-
ing against Chinese MEANT. Proceedings of 10th
International Workshop on Spoken Language Trans-
lation (IWSLT 2013). Heidelberg, Germany.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated cor-
pus of semantic roles. Computational Linguistics
31(1):71?106.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take and Dan Flickinger. 2002. Multiword Expres-
sions: A Pain in the Neck for NLP. In Proceedings
of the Third International Conference on Intelligent
Text processing and Computational Linguistics (CI-
CLING 2002) 1?15. Mexico City, Mexico
98
Proceedings of Frame Semantics in NLP: A Workshop in Honor of Chuck Fillmore (1929?2014), pages 13?17,
Baltimore, Maryland USA, June 27, 2014. c?2014 Association for Computational Linguistics
SemLink+: FrameNet, VerbNet and Event Ontologies 
 Martha Palmer, Claire Bonial Department of Linguistics University of Colorado  Boulder, CO  mpalmer/Claire.Bonial@colorado.edu 
 Diana McCarthy Department of Theoretical and Applied Linguistics (DTAL)  University of Cambridge  diana@dianamccarthy.co.uk     Abstract This paper reviews the significant contributions FrameNet has made to our understanding of lexical resources, semantic roles and event relations. 1 Introduction  One of the great challenges of Natural Language Processing (NLP) is the multitude of choices that language gives us for expressing the same thing in different ways. This is obviously true when taking other languages into consideration - the same thought can be expressed in English, French, Chinese or Russian, with widely varying results. However, it is also true when considering a single language such as English. Light verb constructions, nominalizations, idioms, slang, paraphrases, and synonyms all give us myriads of alternatives for ?coining a phrase.?  This causes immense difficulty for NLP systems.  No one has made greater contributions to advancing the state of the art of lexical semantics, and its applications to NLP, than Chuck Fillmore.  In this paper we focus on the central role that FrameNet has played in our development of SemLink+ and in our current explorations into event ontologies that can play a practical role in accurate automatic event extraction. 2 Detecting events An elusive goal of current NLP systems is the accurate detection of events ? recognizing the meaningful relations among the topics, people, 
places   and  events   buried  within text. These relations can be very complex, and are not always explicit, requiring subtle semantic interpretation of the data.  For instance, NLP systems must be able to automatically recognize that Stock prices sank and The stock market is falling can be describing the same event. Such an interpretation relies upon a  recognition of the similarity between sinking and falling, as well as noting the connection between stock prices and the stock market, and, finally, acknowledgment that they are playing the same role. A key element in event extraction is the identification of the participants of an event, such as the initiator of an action and any parties affected by it.  Basically who did what to whom, when, where, why and how? Many systems today rely on semantic role labeling to help identify participants, and lexical resources that provide an inventory of possible predicate argument structures for individual lexical items are crucial to the success of semantic role labeling (Palmer,et al., 2010).  3 SemLink+ and  Semantic Roles SemLink (Palmer, 2009) is an ongoing effort to map complementary lexical resources: PropBank (Palmer et al., 2005), VerbNet (Kipper et al., 2008), FrameNet (Fillmore et al., 2004), and the recently added OntoNotes (ON) sense groupings (Weischedel, et al., 2011). They all associate semantic information with the propositions in a sentence.  Each was created independently with somewhat differing goals, and they vary in the level and nature of semantic detail represented. FrameNet is the 
13
most fine-grained with the richest semantics, VerbNet     focuses    on     syntactically-based generalizations that carry semantic implications, and the relatively coarse-grained PropBank has been shown to provide the most effective training data for supervised Machine Learning techniques.  Nonetheless, they can be seen as complementary rather than conflicting, and together comprise a whole that is greater than the sum of its parts. SemLink serves as a platform to unify these resources.  The recent addition of ON sense groupings, which can be thought of as a more coarse-grained view of WordNet (Fellbaum, 1998), provides even broader coverage for verbs, and a level of representation that is appropriate for linking between VerbNet class members and FrameNet lexical units, as described below.    SemLink unifies these lexical resources at several different levels.  First by providing type-to-type mappings between the lexical units for each framework.  For PropBank these are the very coarse-grained rolesets, for VerbNet  they are verbs that are members of VerbNet classes, and for FrameNet they are the lexical units associated with each Frame.  The same lemma can have multiple PropBank rolesets and can be in several VerbNet classes and FrameNet frames, but always with different meanings. In general, the mappings from PropBank to VerbNet or FrameNet tend to be 1-many, while the mappings between VerbNet and FrameNet are more likely to be 1-1.  For example, the verb hear has just one coarse-grained sense in PropBank, with the following roleset:  Arg0: hearer Arg1: utterance, sound Arg2: speaker, source of sound  This roleset maps to both the Discover and See classes of VerbNet, and the Hear and Perception_experience frames of FrameNet.      Then, for each lexical unit, SemLink also supplies a mapping between the semantic roles of PropBank and VerbNet, as well as the roles of  VerbNet and FrameNet. PropBank uses very generic labels such as Arg0 and Arg1, which correspond to Dowty?s Prototypical Agent and Patient, respectively (Dowty, 1991).  PropBank has up to six numbered arguments 
for core verb specific roles and for adjuncts it has several generally applicable ArgModifiers that have function tag labels such as: MaNneR, TeMPoral, LOCation, DIRection, GOaL, etc. VerbNet uses more traditional linguistic thematic role labels, with about 30 in total, and assumes adjuncts (ArgM?s) will be supplied by PropBank based semantic role labelers.  FrameNet is even more fine-grained and has frame-specific core and peripheral roles called Frame Elements for each frame, amounting to over 2000 individual Frame Element types.  For example, He talked about politics would receive the following semantic role labels from each framework.1   PropBank (talk.01) HeArg0 talkedRELATION about politicsArg1    VerbNet (Talk-37.5):  HeAGENT talkedRELATION about politicsTOPIC  FrameNet (Statement frame):  HeSPEAKER talkedRELATION about politicsTOPIC      Thanks to Chuck Fillmore?s careful guidance, the rich, meticulously crafted Frames in FrameNet, with their detailed descriptions of all possible arguments and their relations to each other, offer the potential of providing a foundation for inferencing about events and their consequences.  In addition FrameNet has from the beginning been inclusive in its addition of nominal and adjectival forms to the Frames, which greatly increases our coverage of all predicating elements (Bonial, et al., 2014).  There is also a comprehensive FrameNet Constructicon that painstakingly lists many phrasal constructions, such as ?the Xer, the Yer? that cannot be found anywhere else (Fillmore, et al., 2012). Many of these frames, including the constructions, apply equally well to other languages,  as evidenced by the various efforts to develop FrameNets in other languages2 promising a likely benefit to multilingual information 
                                                            1 Arg0 maps to Agent maps to Speaker.  Arg1 maps to Topic maps to Topic. 2 See FrameNet projects in other languages listed at https://framenet.icsi.berkeley.edu/fndrupal/framenets_in_other_languages 
14
processing as well.  Given the close theoretical ties between PropBank, VerbNet and FrameNet, it should be possible to bootstrap from the successful PropBank-based automatic semantic role labelers to equally accurate FrameNet and VerbNet annotators, and to improve overall semantic role labeling performance (Bauer & Rambow, 2011; Dipanjan, et al., 2010; Giuglea & Moschitti, 2006; Merlo & der Plas, 2009; Yi, et al., 2007).  That is one of the primary goals of SemLink.     The first release of SemLink (1.1) contained mappings between these three lexical resources as well as a set of PropBank instances from the Wall Street Journal data with mappings to VerbNet classes and thematic roles (Palmer, 2009).  Our most recent release, SemLink 1.2,3 now includes mappings to FrameNet frames and Frame Elements wherever they are available (FN version 1.5), as well as ON sense groupings (Bonial, et al., 2013). The mapping files between PropBank and VerbNet (version 3.2), and FrameNet have also been checked for consistency and updated to more accurately reflect the current relations between these resources.    This annotated corpus can now be used to train and evaluate VerbNet Class and FrameNet Frame classifiers, to explore clusters of Frame Elements that map to the same VerbNet and PropBank semantic roles, and to evaluate approaches to semantic role labeling that use the type-to-type mappings to bootstrap VerbNet and FrameNet role labels from automatic PropBank semantic role labels. 4 Events, Event Types and Subevents Accurate and informative semantic role labels are an essential component of event extraction, but, although necessary, they are not sufficient. Automatic event detection also requires the ability to distinguish between events which are truly separate, such as Yesterday, John was throwing a ball to Mary and Bill was flying a kite, as opposed to related events such as John was washing the dishes and Mary was drying them.  The second pair could be seen as temporally related subevents of an overall doing the dishes or cleaning up                                                             3 available for download here: http://verbs.colorado.edu/semlink/ 
the kitchen event. It can sometimes be quite challenging to determine the relationship between two events. For instance, earthquakes are quite often associated with the collapse of buildings, as in the following example, The quake destroyed parts of Sausalito.  All tall buildings were demolished.  Many readers might agree that the earthquakes CAUSED the demolishment of the buildings. However, are the building collapses also SUBEVENTs of the earthquakes?  Sometimes they happen a few days later, or immediately, simultaneously with the earthquake. Are they both subevents? In general, for accurate event detection, it would be very useful to know which events must precede, must follow, or cannot be simultaneous with, which other events.   As discussed in the 2013 NAACL Events workshop and this year?s ACL Events workshop, clear, consistent annotation of events and their coreference and causal and temporal relations is a much desired but very challenging goal (Ikuta & Palmer, 2014).  Any assistance that can be provided by lexical resources is welcome. Another very important contribution that FrameNet has made is in the realm of defining these kinds of relations, and others, between frames.  Parent-Child Frame to Frame relations can include Inheritance, Subframe, Perspective On, Using, Causative Of, Inchoative of, and there is also a Precedes temporal ordering relation.   The DEFT working group in Richer Event Descriptions has recently been exploring expanding the ACE and ERE event types, and how they can be mapped onto a broader ontological context.  Exploring the FrameNet relations that the relevant lexical items participate in has been most informative. We first examined the simple LDC ERE classification of Conflict events, which has demonstrations and attacks as siblings (ERE guidelines). We find FrameNet?s classification of attacks as Hostile-Encounters quite useful, and have no argument with it having an Inheritance relation with Intentionally_act, and a Using relation with Taking_sides. Demonstrations, on the other hand, come under the Protest Frame, which has a Using relation with Taking_sides. The FrameNet 
15
organization of demonstrations and attacks, although perfectly justifiable, doesn?t map neatly onto the LDC organization since, although they are close, they are not siblings.  However, by also considering SUMO (Niles & Pease, 2001), the Predicate Matrix (de Lacalle , et al., 2014), WordNet and VerbNet, we were able to develop the upper level partial Event Ontology given in Figure 1, which comfortably incorporates the ERE and FrameNet relations within a broader framework, preserving the key aspects of each.   We are now discussing the ERE Life events, birth, death, injury, marriage, divorce, etc., and FrameNet is again proving to be inspirational.  SemLink+ will encompass our growing Event Ontology, as well as the mappings between the resources and the multiple layers of annotation on the same data.  
 Figure 1 ? SemLink+ Event Ontology, partial  5 Conclusion Since computers do not interact with and experience the world the same way humans do, how could they ever interpret language describing the world the same way humans do?  That NLP has made as much progress as it has is truly phenomenal, and there is much more still that can be done.  Rich, detailed, lexical resources like FrameNet are major stepping stones that will enable continued improvements in the automatic representation of sentences in context. FrameNet, and WordNet, PropBank, VerbNet and SemLink+, provide priceless, invaluable information about myriads of different types of events and the creative ways in which they can be expressed, 
as well as rich details about all of their possible participants.  If we can harness the power of distributional semantics to help us dynamically extend and enrich what has already been manually created, we may find our computers to be much smarter than we ever imagined them to be. Acknowledgments This work has benefited immensely from comments and suggestions during the discussions of the RED working group on Event Ontologies, especially from Teruko Mitamura, Annie Zaenen, Ann Bies, and German Rigau.  We also gratefully acknowledge the support of the National Science Foundation Grant NSF-IIS-1116782, A Bayesian Approach to Dynamic Lexical Resources for Flexible Language Processing, DARPA FA-8750-13-2-0045, subaward 560215 (via LDC) DEFT: Deep Exploration and Filtering of Text, DARPA Machine Reading (via BBN), and NIH: 1 R01 LM010090-01A1, THYME, (via Harvard).  The content is solely the responsibility of the authors and does not necessarily represent the official views of DARPA, NSF or NIH. References Daniel Bauer & Owen Rambow, 2011, Increasing Coverage of Syntactic Subcategorization Patterns in FrameNet Using VerbNet, In the Proceedings of the IEEE Fifth International Conference on Semantic Computing. Claire Bonial, Julia Bonn, Kathryn Conger, Jena Hwang and Martha Palmer, 2014.  PropBank: Semantics of New Predicate Types. The 9th edition of the Language Resources and Evaluation Conference. Reykjavik, Iceland.  Claire Bonial,  Kevin, Stowe, and Martha Palmer, 2013. Renewing and Revising SemLink. The GenLex Workshop on Linked Data in Linguistics, held with GenLex-13.  Dipanjan Das, Nathan Schneider, Desai Chen, and Noah A. Smith, 2010. Probabilistic Frame-Semantic Parsing. In Proceedings of the NAACL 2010.  David Dowty, 1991. Thematic Proto-Roles and Argument Selection. Language, 67:547-619 Christiane Fellbaum, 1998. WordNet: An Electronic Lexical Data-base. Language, Speech and Communications. MIT Press 
16
Charles. J. Fillmore; Collin F. Baker, and H. Sato, 2004. FrameNet as a ``Net".  In Proceedings of LREC 2004, 4, pages 1091-1094 Charles J. Fillmore, Russell R. Lee-Goldman, and Russell Rhodes. 2012. ?The FrameNet Constructicon? Boas, H.C. and Sag, I.A. (Eds.) Sign-based Construction Grammar, CSLI Publications. Ana-Maria Guiglea and Alessandro Moschitti. 2006. Semantic role labeling via FrameNet, VerbNet and PropBank. In Proceedings of Coling-ACL 2006, pages 929?936. Rei Ikuta and Martha Palmer (2014) Challenges of Adding Causation to Richer Event Descriptions, In the Proceedings of 2nd Events Workshop, held with ACL 2014, Baltimore, MD. Karin Kipper, Anna Korhonen, Neville Ryant and Martha Palmer. 2008. A Large-Scale Classification of English Verbs. Language Resources and Evaluation Journal, 42(1):21?40 Maddalen Lopez de Lacalle, Egoitz Laparra, German Rigau, 2014, Predicate Matrix: extending SemLink throughWordNet mappings, The 9th edition of the Language Resources and Evaluation Conference. Reykjavik, Iceland.  Ian Niles and Adam Pease, 2001. Towards a Standard Upper Ontology. In Proceedings of the 2nd International Conference on Formal 
Ontology in Information Systems (FOIS-2001), Chris Welty and Barry Smith, eds, Ogunquit, Maine, October 17-19, 2001. Paola Merlo and Lonneke van der Plas. 2009. Abstraction and generalization in semantic role labels: PropBank, VerbNet or both?, In the Proceedings of  ACL 2009. Martha Palmer, 2009. SemLink: Linking PropBank, VerbNet and FrameNet, In the Proceedings of the Generative Lexicon Conference, GenLex-09. Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71?106 Martha Palmer, Daniel Gildea and Nianwen Xue. Semantic Role Labeling. 2010. Synthesis Lectures on Human Language Technology Series, ed. Graeme Hirst, Morgan and Claypoole. Ralph Weischedel, Eduard Hovy, Mitchell Marcus, Martha Palmer, Robert Belvin, Sameer Pradan, Lance Ramshaw and Nianwen Xue. OntoNotes: A Large Training Corpus for Enhanced Processing, included in Part 1 : Data Acquisition and Linguistic Resources of the Handbook of Natural Language Processing and Machine Translation: Global Automatic Language Exploitation Editors: Joseph Olive, Caitlin Christianson, John McCary, Springer Verglag, pp 54-63, 201 
 
17
