Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 71?79,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
A generalized method for iterative error mining in parsing results
Danie?l de Kok
University of Groningen
d.j.a.de.kok@rug.nl
Jianqiang Ma
University of Groningen
j.ma@student.rug.nl
Gertjan van Noord
University of Groningen
g.j.m.van.noord@rug.nl
Abstract
Error mining is a useful technique for
identifying forms that cause incomplete
parses of sentences. We extend the iter-
ative method of Sagot and de la Clerg-
erie (2006) to treat n-grams of an arbi-
trary length. An inherent problem of in-
corporating longer n-grams is data sparse-
ness. Our new method takes sparseness
into account, producing n-grams that are
as long as necessary to identify problem-
atic forms, but not longer.
Not every cause for parsing errors can be
captured effectively by looking at word
n-grams. We report on an algorithm for
building more general patterns for min-
ing, consisting of words and part of speech
tags.
It is not easy to evaluate the various er-
ror mining techniques. We propose a new
evaluation metric which will enable us to
compare different error miners.
1 Introduction
In the past decade wide-coverage grammars and
parsers have been developed for various lan-
guages, such as the Alpino parser and grammar
(Bouma et al, 2001) for Dutch and the English
Resource Grammar (Copestake and Flickinger,
2000). Such grammars account for a large num-
ber of grammatical and lexical phenomena, and
achieve high accuracies. Still, they are usually
tailored to general domain texts and fail to reach
the same accuracy for domain-specific texts, due
to missing lexicon entries, fixed expressions, and
grammatical constructs. When parsing new texts
there are usually two types of parsing errors:
? The parser returns an incorrect parse. While
the parser may have constructed the correct
parse, the disambiguation model chose an in-
correct parse.
? The parser can not find an analysis that spans
the full sentence. If that sentence is allowed
in the language, the grammar or lexicon is in-
complete.
While the first type of errors can be alleviated
by improving the disambiguation model, the sec-
ond type of problems requires extension of the
grammar or lexicon. Finding incomplete descrip-
tions by hand can become a tedious task once a
grammar has wide coverage. Error mining tech-
niques aim to find problematic words or n-grams
automatically, allowing the grammar developer to
focus on frequent and highly suspicious forms
first.
2 Previous work
In the past, two major error mining techniques
have been developed by Van Noord (2004) and
Sagot and de la Clergerie (2006). In this paper we
propose a generalized error miner that combines
the strengths of these methods. Both methods fol-
low the same basic principle: first, a large (unan-
notated) corpus is parsed. After parsing, the sen-
tences can be split up in a list of parsable and a list
of unparsable sentences. Words or n-grams that
occur in the list of unparsable sentences, but that
do not occur in the list of parsable sentences have
a high suspicion of being the cause of the parsing
error.
2.1 Suspicion as a ratio
Van Noord (2004) defines the suspicion of a word
as a ratio:
S(w) =
C(w|error)
C(w)
(1)
where C(w) is the number of occurrences of
word w in all sentences, and C(w|error) is the
71
number of occurrences of w in unparsable sen-
tences. Of course, it is often useful to look at n-
grams as well. For instance, Van Noord (2004)
gives an example where the word via had a low
suspicion after parsing a corpus with the Dutch
Alpino parser, while the Dutch expression via via
(via a complex route) was unparsable.
To account for such phenomena, the notion of
suspicion is extended to n-grams:
S(wi..wj) =
C(wi..wj |error)
C(wi..wj)
(2)
Where a longer sequence wh...wi...wj ...wk is
only considered if its suspicion is higher than each
of its substrings:
S(wh...wi...wj ...wk) > S(wi...wj) (3)
While this method works well for forms that are
unambiguously suspicious, it also gives forms that
just happened to occur often in unparsable sen-
tences by ?bad luck? a high suspicion. If the occur-
rences in unparsable sentences were accompanied
by unambiguously suspicious forms, there is even
more reason to believe that the form is not prob-
lematic. However, in such cases this error mining
method will still assign a high suspicion to such
forms.
2.2 Iterative error mining
The error mining method described by Sagot and
de la Clergerie (2006) alleviates the problem of
?accidentally suspicious? forms. It does so by
taking the following characteristics of suspicious
forms into account:
? If a form occurs within parsable sentences, it
becomes less likely that the form is the cause
of a parsing error.
? The suspicion of a form should depend on the
suspicions of other forms in the unparsable
sentences in which it occurs.
? A form observed in a shorter sentence is ini-
tially more suspicious than a form observed
in a longer sentence.
To be able to handle the suspicion of a form
within its context, this method introduces the no-
tion of observation suspicion, which is the suspi-
cion of a form within a given sentence. The suspi-
cion of a form, outside the context of a sentence,
is then defined to be the average of all observation
suspicions:
Sf =
1
|Of |
?
oi,j?Of
Si,j (4)
HereOf is the set of all observations of the form
f , oi,j is the jth form of the ith sentence, and Si,j
is the observation suspicion of oi,j . The observa-
tion suspicions themselves are dependent on the
form suspicions, making the method an iterative
process. The suspicion of an observation is the
suspicion of its form, normalized by suspicions of
other forms occurring within the same sentence:
S(n+1)i,j = error(si)
S(n+1)F (oi,j)
?
1?j?|Si| S
(n+1)
F (oi,j)
(5)
Here error(si) is the sentence error rate, which
is normally set to 0 for parsable sentences and 1
for unparsable sentences. SF (oi,j) is the suspicion
of the form of observation oi,j .
To accommodate the iterative process, we will
have to redefine the form suspicion to be depen-
dent on the observation suspicions of the previous
cycle:
S(n+1)f =
1
|Of |
?
oi,j?Of
S(n)i,j (6)
Since there is a recursive dependence between
the suspicions and the observation suspicions,
starting and stopping conditions need to be defined
for this cyclic process. The observation suspicions
are initialized by uniformly distributing suspicion
over observed forms within a sentence:
S(0)i,j =
error(si)
|Si|
(7)
The mining is stopped when the process reaches
a fixed point where suspicions have stabilized.
This method solves the ?suspicion by accident?
problem of ratio-based error mining. However, the
authors of the paper have only used this method to
mine on unigrams and bigrams. They note that
they have tried mining with longer n-grams, but
encountered data sparseness problems. Their pa-
per does not describe criteria to determine when to
use unigrams and when to use bigrams to represent
forms within a sentence.
3 N-gram expansion
3.1 Inclusion of n-grams
While the iterative miner described by Sagot and
de la Clergerie (2006) only mines on unigrams and
72
bigrams, our prior experience with the miner de-
scribed by Van Noord (2004) has shown that in-
cluding longer n-grams in the mining process can
capture many additional phenomena. To give one
example: the words de (the), eerste (first), and
beste (best) had very low suspicions during er-
ror mining, while the trigram eerste de beste had
a very high suspicion. This trigram occurred in
the expression de eerste de beste (the first you can
find). While the individual words within this ex-
pression were described by the lexicon, this multi-
word expression was not.
3.2 Suspicion sharing
It may seem to be attractive to include all n-grams
within a sentence in the mining process. However,
this is problematic due to suspicion sharing. For
instance, consider the trigram w1, w2, w3 in which
w2 is the cause of a parsing error. In this case,
the bigrams w1, w2 and w2, w3 will become sus-
picious, as well as the trigram w1, w2, w3. Since
there will be multiple very suspicious forms within
the same sentence the unigramw2 will have no op-
portunity to manifest itself.
A more practical consideration is that the num-
ber of forms within a sentence grows at such a rate
(n + (n ? 1)... + 1) that error mining becomes
unfeasible for large corpora, both in time and in
space.
3.3 Expansion method
To avoid suspicion sharing we have devised a
method for adding and expanding n-grams when
it is deemed useful. This method iterates through
a sentence of unigrams, and expands unigrams to
longer n-grams when there is evidence that it is
useful. This expansion step is a preprocessor to
the iterative miner, that uses the same iterative al-
gorithm as described by Sagot and De la Clergerie.
Within this preprocessor, suspicion is defined in
the same manner as in Van Noord (2004), as a ra-
tio of occurrences in unparsable sentences and the
total number of occurrences.
The motivation behind this method is that there
can be two expansion scenarios. When we have
the bigram w1, w2, either one of the unigrams can
be problematic or the bigram w1, w2. In the for-
mer case, the bigram w1, w2 will also inherit the
high suspicion of the problematic unigram. In the
latter case, the bigram will have a higher suspicion
than both of its unigrams. Consequently, we want
to expand the unigram w1 to the bigram w1, w2 if
the bigram is more suspicious than both of its un-
igrams. If w1, w2 is equally suspicious as one of
its unigrams, it is not useful to expand to a bigram
since we want to isolate the cause of the parsing
error as much as possible.
The same methodology is followed when we
expand to longer n-grams. Expansion of w1, w2
to the trigram w1, w2, w3 will only be permitted
if w1, w2, w3 is more suspicious than its bigrams.
Since the suspicion of w3 aggregates to w2, w3,
we account for both w3 and w2, w3 in this com-
parison.
The general algorithm is that the expansion to
an n-gram i..j is allowed when S(i..j) > S(i..j?
1) and S(i..j) > S(i + 1..j). This gives us a sen-
tence that is represented by the n-grams n0..nx,
n1..ny, ... n|si|?1..n|si|?1.
3.4 Data sparseness
While initial experiments with the expansion al-
gorithm provided promising results, the expansion
algorithm was too eager. This eagerness is caused
by data sparseness. Since longer n-grams occur
less frequently, the suspicion of an n-gram oc-
curring in unparsable sentences goes up with the
length of the n-gram until it reaches its maximum
value. The expansion conditions do not take this
effect into account.
To counter this problem, we have introduced an
expansion factor. This factor depends on the fre-
quency of an n-gram within unparsable sentences
and asymptotically approaches one for higher fre-
quencies. As a result more burden of proof
is inflicted upon the expansion: the longer n-
gram either needs to be relatively frequent, or it
needs to be much more suspicious than its (n-1)-
grams. The expansion conditions are changed to
S(i..j) > S(i..j ? 1) ? extFactor and S(i..j) >
S(i + 1..j) ? extFactor, where
extFactor = 1 + e??|Of,unparsable| (8)
In our experiments ? = 1.0 proved to be a good
setting.
3.5 Pattern expansion
Previous work on error mining was primarily fo-
cused on the extraction of interesting word n-
grams. However, it could also prove useful to al-
low for patterns consisting of other information
than words, such as part of speech tags or lemmas.
We have done preliminary work on the integra-
tion of part of speech tags during the n-gram ex-
73
pansion. We use the same methodology as word-
based n-gram expansion, however we also con-
sider expansion with a part of speech tag.
Since we are interested in building patterns that
are as general as possible, we expand the pat-
tern with a part of speech tag if that creates a
more suspicious pattern. Expansion with a word
is attempted if expansion with a part of speech
tag is unsuccessful. E.g., if we attempt to ex-
pand the word bigram w1w2, we first try the tag
expansion w1w2t3. This expansion is allowed
when S(w1, w2, t3) > S(w1, w2) ? extFactor
and S(w1, w2, t3) > S(w2, t3) ? extFactor. If
the expansion is not allowed, then expansion to
S(w1, w2, w3) is attempted. As a result, mixed
patterns emerge that are as general as possible.
4 Implementation
4.1 Compact representation of data
To be able to mine large corpora some precau-
tions need to be made. During the n-gram expan-
sion stage, we need quick access to the frequen-
cies of arbitrary length n-grams. Additionally, all
unparsable sentences have to be kept in memory,
since we have to traverse them for n-gram expan-
sion. Ordinary methods for storing n-gram fre-
quencies (such as hash tables) and data will not
suffice for large corpora.
As Van Noord (2004) we used perfect hashing
to restrict memory use, since hash codes are gen-
erally shorter than the average token length. Addi-
tionally, comparisons of numbers are much faster
than comparisons of strings, which speeds up the
n-gram expansion step considerably.
During the n-gram expansion step the miner
calculates ratio-based suspicions of n-grams us-
ing frequencies of an n-gram in parsable and un-
parsable sentences. The n-gram can potentially
have the length of a whole sentence, so it is not
practical to store n-gram ratios in a hash table.
Instead, we compute a suffix array (Manber and
Myers, 1990) for the parsable and unparsable sen-
tences1. A suffix array is an array that contains in-
dices pointing to sequences in the data array, that
are ordered by suffix.
We use suffix arrays differently than Van No-
ord (2004), because our expansion algorithm re-
quires the parsable and unparsable frequencies of
the (n-1)-grams, and the second (n-1)-gram is not
1We use the suffix sorting algorithm by Peter M. McIlroy
and M. Douglas McIlroy.
(necessarily) adjacent to the n-gram in the suffix
array. As such, we require random access to fre-
quencies of n-grams occurring in the corpus. We
can compute the frequency of any n-gram by look-
ing up its upper and lower bounds in the suffix ar-
ray2, where the difference is the frequency.
4.2 Determining ratios for pattern expansion
While suffix arrays provide a compact and rela-
tively fast data structure for looking up n-gram fre-
quencies, they are not usable for pattern expansion
(see section 3.5). Since we need to look up fre-
quencies of every possible combination of repre-
sentations that are used, we would have to create
dl suffix arrays to be (theoretically) able to look
up pattern frequencies with the same time com-
plexity, where d is the number of dimensions and
l is the corpus length.
For this reason, we use a different method for
calculating pattern frequencies. First, we build a
hash table for each type of information that can
be used in patterns. A hash table contains an in-
stance of such information as a key (e.g. a specific
word or part of speech tag) and a set of corpus in-
dices where the instance occurred in the corpus as
the value associated with that key. Now we can
look up the frequency of a sequence i..j by calcu-
lating the set intersection of the indices of j and
the indices found for the sequence i..j ? 1, after
incrementing the indices of i..j ? 1 by one.
The complexity of calculating frequencies fol-
lowing this method is linear, since the set of in-
dices for a given instance can be retrieved with
a O(1) time complexity, while both increment-
ing the set indices and set intersection can be per-
formed in O(n) time. However, n can be very
large: for instance, the start of sentence marker
forms a substantial part of the corpus and is looked
up once for every sentence. In our implementation
we limit the time spent on such patterns by caching
very frequent bigrams in a hash table.
4.3 Removing low-suspicion forms
Since normally only one form within a sentence
will be responsible for a parsing error, many forms
will have almost no suspicion at all. However, dur-
ing the mining process, their suspicions will be
recalculated during every cycle. Mining can be
sped up considerably by removing forms that have
a negligible suspicion.
2Since the suffix array is sorted, finding the upper and
lower bounds is a binary search in O(log n) time.
74
If we do not drop forms, mining of the Dutch
Wikipedia corpus described in section 5.3, with
n-gram expansion and the extension factor en-
abled, resulted in 4.8 million forms with 13.4 mil-
lion form observations in unparsable sentences. If
we mine the same material and drop forms with
a suspicion below 0.001 there were 3.5 million
forms and 4.0 million form observations within
unparsable sentences left at the end of the iterative
mining process.
5 Evaluation
5.1 Methodology
In previous articles, error mining methods have
primarily been evaluated manually. Both Van No-
ord (2004) and Sagot and de la Clergerie (2006)
make a qualitative analysis of highly suspicious
forms. But once one starts experimenting with var-
ious extensions, such as n-gram expansion and ex-
pansion factor functions, it is difficult to qualify
changes through small-scale qualitative analysis.
To be able to evaluate changes to the error
miner, we have supplemented qualitative analysis
with a automatic quantitative evaluation method.
Since error miners are used by grammar engineers
to correct a grammar or lexicon by hand, the eval-
uation metric should model this use case:
? We are interested in seeing problematic forms
that account for errors in a large number of
unparsable sentences first.
? We are only interested in forms that actually
caused the parsing errors. Analysis of forms
that do not, or do not accurately pinpoint ori-
gin of the parsing errors costs a lot of time.
These requirements map respectively to the re-
call and precision metrics from information re-
trieval:
P =
|{Sunparsable} ? {Sretrieved}|
|{Sretrieved}|
(9)
R =
|{Sunparsable} ? {Sretrieved}|
|{Sunparsable}|
(10)
Consequently, we can also calculate the f-score
(van Rijsbergen, 1979):
F ? score =
(1 + ?2) ? (P ? R)
(?2 ? P + R)
(11)
The f-score is often used with ? = 1.0 to give
as much weight to precision as recall. In evalu-
ating error mining, this can permit cheating. For
instance, consider an error mining that recalls the
start of sentence marker as the first problematic
form. Such a strategy would instantly give a re-
call of 1.0, and if the coverage of a parser for a
corpus is relatively low, a relatively good initial f-
score will be obtained. Since error mining is often
used in situations where coverage is still low, we
give more bias to precision by using ? = 0.5.
We hope to provide more evidence in the future
that this evaluation method indeed correlates with
human evaluation. But in our experience it has the
required characteristics for the evaluation of error
mining. For instance, it is resistant to recalling
of different or overlapping n-grams from the same
sentences, or recalling n-grams that occur often in
both parsable and unparsable sentences.
5.2 Scoring methods
After error mining, we can extract a list of forms
and suspicions, and order the forms by their sus-
picion. But normally we are not only interested in
forms that are the most suspicious, but forms that
are suspicious and frequent. Sagot and de la Clerg-
erie (2006) have proposed three scoring methods
that can be used to rank forms:
? Concentrating on suspicions: Mf = Sf
? Concentrating on most frequent potential er-
rors: Mf = Sf |Of |
? Balancing between these possibilities: Mf =
Sf ? ln|Of |
For our experiments, we have replaced the ob-
servation frequencies of the form (|Of |) by the
frequency of observations within unparsable sen-
tences (|{Of,unparsable}|). This avoids assigning a
high score to very frequent unsuspicious forms.
5.3 Material
In our experiments we have used two corpora that
were parsed with the wide-coverage Alpino parser
and grammar for Dutch:
? Quantitative evaluation was performed on the
Dutch Wikipedia of August 20083. This cor-
pus consists of 7 million sentences (109 mil-
lion words). For 8.4% of the sentences no full
analysis could be found.
3http://ilps.science.uva.nl/WikiXML/
75
? A qualitative evaluation of the extensions was
performed on the Flemish Mediargus news-
paper corpus (up to May 31, 2007)4. This
corpus consists of 67 million sentences (1.1
billion words). For 9.2% of the sentences no
full analysis could be found.
Flemish is a variation of Dutch written and spo-
ken in Belgium, with a grammar and lexicon that
deviates slightly from standard Dutch. Previously,
the Alpino grammar and lexicon was never specif-
ically modified for parsing Flemish.
6 Results
6.1 Iterative error mining
We have evaluated the different mining methods
with the three scoring functions discussed in sec-
tion 5.2. In the results presented in this section we
only list the results with the scoring function that
performed best for a given error mining method
(section 6.3 provides an overview of the best scor-
ing functions for different mining methods).
Our first interest was if, and how much itera-
tive error mining outperforms error mining with
suspicion as a ratio. To test this, we compared
the method described by Van Noord (2004) and
the iterative error miner of Sagot and de la Clerg-
erie (2006). For the iterative error miner we eval-
uated both on unigrams, and on unigrams and bi-
grams where all unigrams and bigrams are used
(without further selection). Figure 6.1 shows the
f-scores for these miners after N retrieved forms.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  2000  4000  6000  8000  10000
F 0.5-
Scor
e
N
ratioiter.unigramsiter.uni.bigrams
Figure 1: F-scores after retrieving N forms for
ratio-based mining, iterative mining on unigrams
and iterative mining on uni- and bigrams.
4http://www.mediargus.be/
The unigram iterative miner outperforms the
ratio-based miner during the retrieval of the first
8000 forms. The f-score graph of the iterative
miner on unigrams flattens after retrieving about
4000 forms. At that point unigrams are not spe-
cific enough anymore to pinpoint more sophisti-
cated problems. The iterative miner on uni- and bi-
grams performs better than the ratio-based miner,
even beyond 8000 forms. More importantly, the
curves of the iterative miners are steeper. This is
relevant if we consider that a grammar engineer
will only look at a few thousands of forms. For
instance, the ratio-based miner achieves an f-score
of 0.4 after retrieving 8448 forms, while the iter-
ative miner on uni- and bigrams attains the same
f-score after retrieving 5134 forms.
6.2 N-gram expansion
In our second experiment we have compared the
performance of iterative mining on uni- and bi-
grams with an iterative miner using the n-gram
expansion algorithm described in section 3. Fig-
ure 6.2 shows the result of n-gram expansion com-
pared to mining just uni- and bigrams. Both the
results for expansion with and without use of the
expansion factor are shown.
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
 0  2000  4000  6000  8000  10000
F 0.5-
Scor
e
N
iter.uni.bigramiter.expansioniter.expansion.ef
Figure 2: F-scores after retrieving N forms for it-
erative mining on uni- and bigrams, and iterative
mining using n-gram expansion with and without
using an expansion factor.
We can see that the expansion to longer n-grams
gives worse results than mining on uni- and bi-
grams when data sparseness is not accounted for.
The expansion stage will select forms that may be
accurate, but that are more specific than needed.
As such, the recall per retrieved form is lower on
76
average, as can be seen in figure 6.2. But if sparse-
ness is taken into account through the use of the
expansion factor, we achieve higher f-scores than
mining on uni- and bigrams up to the retrieval of
circa five thousand forms. Since a user of an error
mining tool will probably only look at the first few
thousands of forms, this is a welcome improve-
ment.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0  2000  4000  6000  8000  10000
Reca
ll
N
iter.uni.bigramiter.expansioniter.expansion.ef
Figure 3: Recall after retrieving N forms for it-
erative mining on uni- and bigrams, and iterative
mining using n-gram expansion with and without
using an expansion factor.
Among the longer n-grams in the mining results
for the Mediargus corpus, we found many Flemish
idiomatic expressions that were not described in
the Alpino lexicon. For example:
? had er (AMOUNT) voor veil [had
(AMOUNT) for sale]
? (om de muren) van op te lopen [to get terribly
annoyed by]
? Ik durf zeggen dat [I dare to say that]
? op punt stellen [to fix/correct something]
? de daver op het lijf [shocked]
? (op) de tippen (van zijn tenen) [being very
careful]
? ben fier dat [am proud of]
? Nog voor halfweg [still before halfway]
? (om duimen en vingers) van af te likken [de-
licious]
Since these expressions are longer than bi-
grams, they cannot be captured properly without
using n-gram expansion. We also found longer
n-grams describing valid Dutch phrases that were
not described by the grammar or lexicon.
? Het stond in de sterren geschreven dat [It was
written in the stars that]
? zowat de helft van de [about half of the]
? er zo goed als zeker van dat [almost sure of]
? laat ons hopen dat het/dit lukt [let us hope that
it/this works]
6.3 Scoring methods
The miners that use n-gram expansion perform
best with the Mf = Sf |Of | function, while the
other miners perform best with the Mf = Sf ?
ln|Of | function. This is not surprising ? the it-
erative miners that do not use n-gram expansion
can not make very specific forms and give rela-
tively high scores to forms that happen to occur in
unparsable sentences (since some forms in a sen-
tence will have to take blame, if no specific sus-
picious form is found). If such forms also hap-
pen to be frequent, they may be ranked higher
than some more suspicious infrequent forms. In
the case of the ratio-based miner, there are many
forms that are ?suspicious by accident? which may
become highly ranked when they are more fre-
quent than very suspicious, but infrequent forms.
Since the miners with n-gram expansion can find
specific suspicious forms and shift blame to them,
there is less chance of accidentally ranking a form
to highly by directly including the frequency of
observations of that form within unparsable sen-
tences in the scoring function.
6.4 Pattern expansion
We have done some preliminary experiments with
pattern expansion, allowing for patterns consisting
of words and part of speech tags. For this exper-
iment we trained a Hidden Markov Model part of
speech tagger on 90% of the Dutch Eindhoven cor-
pus using a small tag set. We then extracted 50000
unparsable and about 495000 parsable sentences
from the Flemish Mediargus corpus. The pattern
expansion preprocessor was then used to find in-
teresting patterns.
We give two patterns that were extracted to give
an impression how patterns can be useful. A fre-
quent pattern was doorheen N (through followed
77
by a (proper) noun). In Flemish a sentence such
as We reden met de auto doorheen Frankrijk (lit-
eral: We drove with the car through France) is al-
lowed, while in standard Dutch the particle heen
is separated from the preposition door. Conse-
quently, the same sentence in standard Dutch is We
reden met de auto door Frankrijk heen. Mining
on word n-grams provided hints for this difference
in Flemish through forms such as doorheen Krot-
tegem, doorheen Engeland, doorheen Hawai, and
doorheen Middelkerke, but the pattern provides a
more general description with a higher frequency.
Another pattern that was found is wegens Prep
Adj (because of followed by a preposition and
an adjective). This pattern captures prepositional
modifiers where wegens is the head, and the fol-
lowing words within the constituent form an ar-
gument, such as in the sentence Dat idee werd
snel opgeborgen wegens te duur (literal: That idea
became soon archived because of too expensive).
This pattern provided a more general description
of forms such as wegens te breed (because it is
too wide), wegens te deprimerend (because it is
too depressing), wegens niet rendabel (because it
is not profitable), and wegens te ondraaglijk (be-
cause it is too unbearable).
While instances of both patterns were found us-
ing the word n-gram based miner, patterns consol-
idate different instances. For example, there were
120 forms with a high suspicion containing the
word wegens. If such a form is corrected, the other
examples may still need to be checked to see if a
solution to the parsing problem is comprehensive.
The pattern gives a more general description of the
problem, and as such, most of these 120 forms can
be represented by the pattern wegens Prep Adj.
Since we are still optimizing the pattern ex-
pander to scale to large corpora, we have not per-
formed an automatic evaluation using the Dutch
Wikipedia yet.
7 Conclusions
We combined iterative error mining with expan-
sion of forms to n-grams of an arbitrary length,
that are long enough to capture interesting phe-
nomena, but not longer. We dealt with the prob-
lem of data sparseness by introducing an expan-
sion factor that softens when the expanded form is
very frequent.
In addition to the generalization of iterative er-
ror mining, we introduced a method for automatic
evaluation. This allows us to test modifications to
the error miner without going through the tedious
task of ranking and judging the results manually.
Using this automatic evaluation method, we
have shown that iterative error mining improves
upon ratio-based error mining. As expected,
adding bigrams improves performance. Allowing
expansion beyond bigrams can lead to data sparse-
ness problems, but if we correct for data sparse-
ness the performance of the miner improves over
mining on just unigrams and bigrams.
We have also described preliminary work on
a preprocessor that allows for more general pat-
terns that incorporate additional information, such
as part of speech tags and lemmas. We hope to
optimize and improve pattern-based mining in the
future and evaluate it automatically on larger cor-
pora.
The error mining methods described in this pa-
per are generic, and can be used for any grammar
or parser, as long as the sentences within the cor-
pus can be divided in a list of parsable and un-
parsable sentences. The error miner is freely avail-
able5, and is optimized to work on large corpora.
The source distribution includes a graphical user
interface for browsing mining results, showing the
associated sentences, and removing forms when
they have been corrected in the grammar or lex-
icon.
References
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide-coverage Computational Anal-
ysis of Dutch. In Computational Linguistics in The
Netherlands 2000.
Ann Copestake and Dan Flickinger. 2000. An
open source grammar development environment and
broad-coverage English grammar using HPSG. In
Proceedings of LREC 2000, pages 591?600.
Udi Manber and Gene Myers. 1990. Suffix arrays: a
new method for on-line string searches. In SODA
?90: Proceedings of the first annual ACM-SIAM
symposium on Discrete algorithms, pages 319?327.
Society for Industrial and Applied Mathematics.
Beno??t Sagot and E?ric de la Clergerie. 2006. Error
mining in parsing results. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 329?336, Morristown, NJ, USA. Association
for Computational Linguistics.
5http://www.let.rug.nl/?dekok/errormining/
78
Gertjan Van Noord. 2004. Error mining for wide-
coverage grammar engineering. In ACL ?04: Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, page 446, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
C. J. van Rijsbergen. 1979. Information retrieval. But-
terworths, London, 2 edition.
79
Proceedings of the Twelfth Meeting of the Special Interest Group on Computational Morphology and Phonology (SIGMORPHON2012), pages 17?25,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Phrase-Based Approach for Adaptive Tokenization   Jianqiang Ma Dale Gerdemann Department of Linguistics                University of T?bingen  Department of Linguistics                  University of T?bingen Wilhelmstr. 19, T?bingen, 72074, Germany Wilhelmstr. 19, T?bingen, 72074, Germany jma@sfs.uni-tuebingen.de dg@sfs.uni-tuebingen.de     Abstract 
Fast re-training of word segmentation models is required for adapting to new resources or domains in NLP of many Asian languages without word delimiters. The traditional tokenization model is efficient but inaccurate. This paper proposes a phrase-based model that factors sentence tokenization into phrase tokenizations, the dependencies of which are also taken into account. The model has a good OOV recognition ability, which improves the overall performance significantly. The training is a linear time phrase extraction and MLE procedure, while the decoding is via dynamic programming based algorithms. 
1 Introduction In many Asian languages, including Chinese, a sentence is written as a character sequence without word delimiters, thus word segmentation remains a key research topic in language processing for these languages. Although many reports from evaluation tasks present quite positive results, a fundamental problem for real word applications is that most systems heavily depend on the data they were trained on. In order to utilize increasingly available language resources such as user contributed annotations and web lexicon and/or to dynamically construct models for new domains, we have to either frequently re-build models or rely on techniques such as incremental learning and transfer learning, which are unsolved problems themselves.      In the case of frequent model re-building, the most efficient approach is the tokenization model  
 (using the terminology in Huang et al, 2007), in which the re-training is just the update of the dictionary and the segmentation is a greedy string matching procedure using the dictionary and some disambiguation heuristics, e.g. Liang (1986) and Wang et al (1991). An extension of this approach is the dynamic programming search of the most probable word combination on the word lattice, such as Ma (1996) and Sproat et al (1996), which utilize information such as word frequency statistics in a corpus to build the model and are less efficient but more accurate.     However, all the methods mentioned above are mostly based on the knowledge of in-vocabulary words and usually suffer from poor performance, as the out-of-vocabulary words (OOV) rather than segmentation ambiguities turn out to the dominant error source for word segmentation on real corpora (Huang and Zhao, 2007). This fact has led to a shift of the research focus to modeling the roles of individual characters in the word formation process to tackle the OOV problem. Xue (2003) proposes a character classification model, which classifies characters according to their positions in a word using the maximum entropy classifier (Berger et al, 1996). Peng et al (2004) has further extended this model to its sequential form, i.e. sequence labeling, by adopting linear-chain conditional random fields (CRFs, Lafferty et al, 2001). As it is capable of capturing the morphological behaviors of characters, the character classification model has significantly better performance in OOV recognition and overall segmentation accuracy, and has been the state-of-art since its introduction, suggested by the leading performances of systems based on it in recent international Chinese word 
17
segmentation bakeoffs (Emerson, 2005; Levow, 2006; Zhao and Liu, 2010).      The tokenization model has advantages in simplicity and efficiency, as the basic operation in segmentation is string matching with linear time complexity to the sentence length and it only needs a dictionary thus requires no training as in the character classification model, which can easily have millions of features and require hundreds of iterations in the training phase. On the other hand, it has inferior performance, caused by its poor OOV induction ability.      This work proposes a framework called phrase-based tokenization as a generalization of the tokenization model to cope with its deficiencies in OOV recognition, while preserving its advantages of simplicity and efficiency, which are important for adaptive word segmentation. The segmentation hypothesis unit is extended from a word to a phrase, which is a character string of arbitrary length, i.e. combinations of partial and/or complete words. And the statistics of different tokenizations of the same phrase are collected and used for parameters estimation, which leads to a linear time model construction procedure. This extension makes hypothesis units capable of capturing richer context and describing morphological behavior of characters, which improves OOV recognition. Moreover, overlapping hypothesis units can be combined once certain consistency conditions are satisfied, which avoids the unrealistic assumption of independence among the tokenizations of neighboring phrases.      Phrase-based tokenization decomposes the sentence tokenization into phrase tokenizations. We use a graph called phrase tokenization lattice to represent all the hypotheses of phrase tokenization in a given sentence. Under such a formulation, tokenizing a sentence is transformed to the shortest path search problem on the graph, which can be efficiently solved by dynamic programming techniques similar to the Viterbi (1967) algorithm.   2 Phrase-Based Model The hypothesis unit of the tokenization model is the word, i.e. it selects the best word sequence from all the words that can be matched by substrings of the sentence (usually in a greedy manner). Once a word is chosen, the corresponding 
boundaries are determined. This implies that as the characters in a word are always considered as a whole, the morphological behavior of an individual character, e.g. the distribution of its positions in words, is ignored thus makes it impossible to model the word formation process and recognize OOV.       Critical tokenization (Guo, 1997) suggests a method of discovering all and only unambiguous token boundaries (critical points) and generating longest substrings with all inner positions ambiguous (critical fragments) under the assumption of complete dictionary. Then an example-based method using the context can be adopted to disambiguate the tokenization of critical fragments (Hu et al 2004). However, the complete dictionary assumption is not realistic in practice, as the word formation is so dynamic and productive that there is no dictionary that is even close to the complete lexicon. Given the presence of OOV, a word, including a monosyllabic word, in the original dictionary may be a substring, i.e. a partial word, of an OOV. In this case, the critical points found by the dictionary are not guaranteed to be unambiguous.      As the complete dictionary does not exist as a static object, a possible solution is to make a dynamic dictionary, which induces words on the fly. But this will not be discussed in this paper. Instead, we attempt to generalize the tokenization model to work without the complete dictionary. Different from making distinctions of critical fragments and ?non-critical? fragments in critical tokenization, we suggest using phrases to represent potentially ambiguous fragments of sentences in a unified way. We define a phrase as a substring of a sentence, the boundaries of which, depending on the tokenization, may or may not necessarily match word boundaries. The fact that partial words, including single characters, may appear on both ends of a phrase makes it possible to describe ?morphemes in the context? for OOV induction. A consequence of introducing phrase in tokenization is that a manually segmented corpus is needed in order to collect phrases. 2.1   Tokenization Tokenization is the process of separating words or word-like units from sentences or character strings. We can consider sentence tokenization as a mapping from each position in the sentence to a 
18
binary value, which indicates the presence (denoted as #) or the absence of word boundary (denoted as $) at that position. A specific tokenization realization of a sentence can be represented by a list of binary values, which can be generated by the concatenations of its sub-lists. In other words, a tokenization of a given sentence can be represented as the concatenation of the tokenizations of its component phrases.      If we assume that the tokenization of a phrase is independent of other phrases in the same sentence, the sentence tokenization problem is decomposed to smaller phrase tokenization problems, which are unrelated to each other. The independency assumption is not necessarily true but in general is a good approximation. We take this assumption by default, unless there exists evidence that suggests otherwise. In that case, we introduce a method called backward dependency match to fix the problem, which will be discussed in Section 3.3.  2.2   Phrase Tokenization Lattice Informally a phrase tokenization lattice, or lattice in short, is a set of hypothesized tokenization of phrases in the given sentence, which is a compact representation of all the possible tokenization for that sentence. Using the notations in Mohri (2002), we formally define a lattice as a weighted directed graph < V , E >  with a mapping 
 
W : E ! A
, where V  is the set of nodes, E  is the set of edges, and the mapping W  assigns each edge a weight w  from the semiring < A ,!,", 0,1> 1.      For a given sentence S [0... m ] , each node v ! V , denotes a sentence position (the position between a pair of adjacent characters in a untokenized sentence). Each edge e ! E  from node va to node v b , denotes a tokenization of the phrase between the positions defined by va  and v b . And for each edge e , a weight w  is determined by the mapping W , denotes the phrase tokenization probability, the probability of the phrase defined by the two nodes of the edge being tokenized as the tokenization defined by that edge. A path !  in the lattice is a sequence of consecutive edges, i.e. ! = e1e2...ek , where ei  and                                                             1 A semiring defines an algebra system with certain rules to compute path probabilities and the max probabilities from a node to another. See Mohri (2002) for details.   
e
i+1
are connected with a node. The weight for the path !  can be defined as: w ( ! ) = !i=1k w ( ei )                   (1) which is the product of the weights of its component edges. A path from the source node to the sink node, represents a tokenization of the sentence being factored as the concatenation of tokenizations of phrases represented by those edges of on that path.      For example, with some edges being pruned, the lattice for the sentence ????? ?Someone questions him? is shown in Figure 1.  
 Figure 1. A pruned phrase tokenization lattice. Edges are tokenizations of phrases, e.g. e 5  represents tokenizing ?? ?question? into a word and e 7  represents tokenizing?? ?doubt him? into a partial word ?  ?doubt? followed by a word ? ?him?.  
2.3   Tokenization as the Best Path Search After the introduction of the lattice, we formally describe the tokenization (disambiguation) problem as the best path searching on the lattice:  
 
T
!
= argmax
T ! D
w T
( )
               (2) where D  is the set of all paths from the source node to the sink node, and 
 
T
! is the path with the highest weight, which represents the best tokenization of the sentence. Intuitively, we consider the product of phrase tokenization probabilities as the probability of the sentence tokenization that is generated from the concatenation of these phrase tokenizations.      Note that every edge in the lattice is from a node represents an earlier sentence position to a node that represents a later one. In other words, the lattice is acyclic and has a clear topological order. 
19
In this case, the best path can be found using the Viterbi (1967) algorithm efficiently2. 3 Training and Inference Algorithms 
3.1   Model Training In order to use the lattice to tokenize unseen sentences, we first have to build a model that can generate the edges and their associated weight, i.e. the tokenization of all the possible phrases and their corresponding phrase tokenization probability. We do it by collecting all the phrases that have occurred in a training corpus and use maximum likelihood estimation (MLE) to estimate the phrase tokenization probabilities.  The estimation of the probability that a particular phrase A = a
1
a
2
... a
n
 being tokenized as the tokenization T = t
1
t
2
... t
m
 is given in equation (3), where C (? )  represents the empirical count, and the set of all T ' stands for all possible tokenizations of A . To avoid extreme cases in which there is no path at all, techniques such as smoothing can be applied. 
P ( T | A ) =
C ( T , A )
C ( T ' , A )
T '
!
=
C ( T , A )
C ( A )
               (3) 
    The result of the MLE estimation is stored in a data structure called phase tokenization table, from which one can retrieval all the possible tokenizations with their corresponding probabilities for the every phrase that has occurred in the training corpus. With this model, we can construct the lattice, i.e. determine the set of edges E and the mapping function W (defining nodes is trivial) for a given sentence in a simple string matching and table retrieval manner: when a substring of sentence is matched to a stored phrase, an edge is built from the its starting and ending node to represent a tokenization of that phrase, with the weight of the edge equals to the MLE estimation of the stored phrase-tokenization pair.  3.2   Simple Dynamic Programming Once the model is built, we can tokenize a given sentence by the inference on the lattice which represents that sentence. The proposed simple dynamic programming algorithm (Algorithm 1, as                                                             2 More rigid mathematical descriptions of this family of problems and generic algorithms based on semirings are discussed in Mohri (2002) and Huang (2008). 
shown in Figure 2) can be considered as the phrase tokenization lattice version of the evalUtterance algorithm in Venkataraman (2001). The best tokenization of the partial sentence up to a certain position is yielded by the best combination of one previous best tokenization and one of the phrase tokenizations under consideration at the current step.      The upper bound of the time complexity of Algorithm 1 is O ( k n 2 ) , where n  is the sentence length and k is the maximum number of the possible tokenization for a phrase. But in practice, it is neither necessary nor possible (due to data sparseness) to consider phrases of arbitrary length, so we set a constraint of maximum phrase length of about 10, which makes the time complexity de-facto linear.   
!"#$%&'()*+,**-&)."/*0123)&4*5%$#%3))&2#
!"#"#$%&'&(#)**!+",'#*-./#0&1,(&.0*-,23#*4!-5*62.7')***6#0(#07#**689:::;<62&'&3"&83'&$2,
=#'(67."#>*!?@&A#0'&.0*1#".*B#7(."
=#'(-./#0&1,(&.0>*!?@&A#0'&.0*0%33?'("&0C*B#7(."
!"#$%&'()9,
:$%*">D***(.***;***;$*,*************EE*")*7%""#0(*F.'&(&.0*&0*(+#*'#0(#07#
********:$%***#>"?D***(.***9****;$*,****EE*#)*'(,"(&0C*F.'&(&.0*.G*(+#*3,'(*F+",'#
****************$%&'()>68#)"<*************
****************&:***$%&'()***&0***!-*,**
************************+,-)."/'+",.>H#(-.F-./#0&1,(&.04!-I*$%&'()5**
************************+,-)."/'+",.0$&,1>H#(!".2,2&3&(J4!-I*+,-)."/'+",.5
************************(2,&)>=#'(67."#8#<*K*+,-)."/'+",.0$&,1*
************************&:***(2,&)L=#'(67."#8"<*,
********************************=#'(-./#0&1,(&.08"<>+,-)."/'+",.********************************=#'(67."#8"<3(2,&)********************************1'2-0$,".+)&4"53#***************
****************/"9/)
************************<%/3=****EE*&G*(+#*F+",'#*0.(*&0*!-I*#M&'(*(+#*&00#"*3..F
*
=#'(!,(+*N**!,(+*(",7#@*2,7/*G".A*2,7/OF.&0(#"8;<*
6#0(#07#-./#0&1,(&.0N*P.07,(#0,(&.0*.G*(+#*=#'(-./#0&1,&(.0*
#0("&#'*.G*(+#*#@C#'*.0*(+#*=#'(!,(+*4Q+&7+*,"#*F+",'#*(./#0&1,(&.0'5*
>7'.7')****6#0(#07#-./#0&1,(&.0!!  Figure 2. The pseudo code of Algorithm 1.        The key difference to a standard word-lattice based dynamic programming lies in the phrase lattice representation that the algorithm runs on. Instead of representing a word candidate as in Venkataraman (2001), each edge now represents a 
20
tokenization of a phrase defined by two nodes of the edge, which can include full and partial words. The combination of phrase tokenizations may yield new words that are not in the dictionary, i.e. our method can recognize OOVs.     Let us consider a slightly modified version of the lattice in Figure 1. Suppose edge e
5
 =#?$?# does not exist , i.e. the word ?? ?question? is not in the dictionary, and there is new edge e 5!= #?$ that links node 2 and node 3 and represents a partial word. Two of possible tokenizations of the sentence are path p
1
= e
1
e
4
e
6
e
8
and path p 2 = e2 e5!e7 . Note that p 2 recognizes the word??  ?question? by combining two partial words, even though the word itself has not seen before. Of course, this OOV is finally recognized only if a path that can yield it is the best path found by the decoding algorithm.     Once the best path is found, the procedure of mapping it back to segmented words is as follows. The phrase tokenizations represented by the edges of the best path are concatenated, before substituting meta symbols # and $ into white space and empty string, respectively. For example, if  p 2 = e2 e5!e7  is the best path, the concatenation of the phrase tokenizations of the three edges on the path will be #?#?##?$$?#?#, and removal of $ and substitution of # into the white space will further transform it into ?   ?   ??   ? ?Somebody questions him?, which is the final result of the algorithm.  3.3   Compatibility and Backward Dependency Match As mentioned in Section 2, the independency assumption of phrase tokenization is not always true. Considering the example in Figure 1, e
4
and and e
7
are not really compatible, as e
4
 represents a word while e
7
 represents a partial word that expects the suffix of its preceding phrase to form a word with its prefix. To solve this problem, we require that the last (meta) symbol of the preceding tokenization must equal to the first (meta) symbol of the following tokenization in order to concatenate the two. This, however, has the consequence that there may be no valid 
tokenization at all for some positions. As a result, we have to maintain the top k hypotheses and use the k-best path search algorithms instead of 1-best (Mohri, 2002). We adopt the na?ve k-best path search, but it is possible to use more advanced techniques (Huang and Chiang, 2005).      The compatibility problem is just the most salient example of the general problem of variable independency assumptions, which is the "unigram model" of phrase tokenization. A natural extension is a higher order Markov model. But that is inflexible, as it assumes a fixed variable dependency structure (the current variable is always dependent on previous n variables). So we propose a method called backward dependency match, in which we start from the independency assumption, then try to explore the longest sequence of adjacent dependencies that we can reach via string match for a given phrase and its precedent.     To simplify the discussion, we use sequence labeling, or conditional probability notation of the tokenization. A tokenization of the given character sequence (sentence) is represented as a specific label sequence of same length. The label can be those in the standard 4-tag set of word segmentation (Huang and Zhao, 2007) or the #/$ labels indicating the presence or absence of a word boundary after a specific character.      The possible tokenizations of character sequence 
a
1
a
2
a
3
are represented as the probability distribution P ( t
1
t
2
t
3
| a
1
a
2
a
3
) , where t
1
t
2
t
3
 are labels of a
1
a
2
a
3
. If a tokenization hypothesis of 
a
1
a
2
a
3
decomposes its tokenization into the concatenation of the tokenization of a
1
a
2
 and the tokenization of a
3
, this factorization can be expressed as 
P (t
1
t
2
| a
1
a
2
) ! P (t
3
| a
3
)
, as shown in Figure 3a. For a specific assignment 
< a
1
a
2
a
3
; t
1
t
2
t
3
>
, if we find that 
< a
2
a
3
>
 can be tokenized as 
< t
2
t
3
>
, it suggests that 
t
3
 may be dependent on 
a
2
and t
2
 as well, so we update the second part of the factorization (at least for this assignment) to: 
P(t
3
| a
3
; a
2
t
2
)
, which can be estimated as:  
P ( t
3
| a
3
; a
2
t
2
) =
C ( a
2
a
3
t
2
t
3
)
C ( a
2
a
3
t
2
t
3
)
t
3
!
                     (4)  
21
In this case, the factorization of the tokenization 
P ( t
1
t
2
t
3
| a
1
a
2
a
3
)  is 
P (t
1
t
2
| a
1
a
2
)! P (t
3
| a
3
; a
2
t
2
)
, as shown in Figure 3b.    
  Figure 3a. The factorization of P ( t
1
t
2
t
3
| a
1
a
2
a
3
)  into
P (t
1
t
2
| a
1
a
2
)! P (t
3
| a
3
)
.   
  Figure 3b. The factorization of P ( t
1
t
2
t
3
| a
1
a
2
a
3
)  into
P (t
1
t
2
| a
1
a
2
)! P ( t
3
| a
3
; a
2
t
2
)
. Note that in the 2nd factor, in addition to a3, a2 and t2 are also observed variables and all of them are treated as a unit (shown by the L-shape). The shadowed parts (a2 and t2)  represent the matched items.      Algorithm 2 is based on the k-best search algorithm, which calls the backward dependency match after a successful compatibility check, and match as far as possible to get the largest probability of each tokenization hypothesis. In 
extreme cases, where no tokenization hypothesis survives the compatibility check, the algorithm backs off to Algorithm 1. 4 Experiments We use the training and testing sets from the second international Chinese word segmentation bakeoff (Emerson, 2005), which are freely available and most widely used in evaluations. There are two corpora in simplified Chinese provided by Peking University (PKU) and Microsoft Research (MSR) and two corpora in traditional Chinese provided by Academic Sinica (AS) and the City University of Hong Kong (CityU). The experiments are conducted in a closed-test manner, in which no extra recourse other than the training corpora is used. We use the same criteria and the official script for evaluation from the bakeoff, which measure the overall segmentation performance in terms of F-scores, and the OOV recognition capacity in terms of Roov.     Precision is defined as the number of correctly segmented words divided by the total number of words in the segmentation result, where the correctness of the segmented words is determined by matching the segmentation with the gold standard test set. Recall is defined as the number of correctly segmented words divided by the total number of words in the gold standard test set. The evenly-weighted F-score is calculated by:  F = 2 ! p ! r / ( p + r )               (5) Roov is the recall of all the OOV words. And Riv is the recall of words that have occurred in the training corpus. The evaluation in this experiment is done automatically using the script provided with the second bakeoffs data.     We have implemented both Algorithm 1 and Algorithm 2 in Python with some simplifications, e.g. only processing phrase up to the length of 10 characters, ignoring several important details such as pruning. The performances are compared with the baseline algorithm maximum matching (MM), described in Wang et al (1991), and the best bakeoff results. The F-score, Roov and Riv are summarized in Table 1, Table 2, and Table 3, respectively.     All the algorithms have quite similar recall for the in-vocabulary words (Riv), but their Roov vary 
22
greatly, which leads to the differences in F-score. In general both Algorithm 1 and Algorithm 2 improves OOV Recall significantly, compared with the baseline algorithm, maximum matching, which has barely any OOV recognition capacity. This confirms the effectiveness of the proposed phrase-based model in modeling morphological behaviors of characters. Moreover, Algorithm 2 works consistently better than Algorithm 1, which suggests the usefulness of its strategy of dealing with dependencies among phrase tokenizations.     Besides, the proposed method has the linear training and testing (when setting a maximum phrase length) time complexity, while the training complexity of CRF is the proportional to the feature numbers, which are often over millions. Even with current prototype, our method takes only minutes to build the model, in contrast with several hours that CRF segmenter needs to train the model for the same corpus on the same machine.       Admittedly, our model still underperforms the best systems in the bakeoff. This may be resulted from that 1) our system is still a prototype that ignores many minor issues and lack optimization and 2) as a generative model, our model may suffer more from the data sparseness problem, compared with discriminative models, such as CRF.     As mentioned earlier, the OOV recognition is the dominant factor that influences the overall accuracy. Different from the mechanism of tokenization combination in our approach, state-of-art systems such as those based on MaxEnt or CRF, achieve OOV recognition basically in the same way as in-dictionary word recognition. The segmentation is modeled as assigning labels to characters. And the probability of the label assignment for a character token is mostly determined by its features, which are usually local contexts in the form of character co-occurrences.       There are many other OOV recognition methods proposed in literature before the rise of machine learning in the field. For example, the Sproat et al (1996) system can successfully recognize OOVs of strong patterns, such as Chinese personal names, transliterations, using finite-state techniques.  Another typical example is Ma and Chen (2003), which proposed context free grammar like rules together with a recursive bottom-up merge algorithm that merges possible morphemes after an initial segmentation using maximum matching. It 
would be fairer to compare the OOV recognition performance of our approach with these methods, rather than maximum matching. But most earlier works are not evaluated on standard bake-off corpora and the implementations are not openly available, so it is difficult to make direct comparisons.  F-score As CityU MSR PKU 
Best Bakeoff 0.952 0.943 0.964 0.950 
Algorithm 2 0.919 0.911 0.946 0.912 
Algorithm 1 0.897 0.888 0.922 0.890 
MM 0.882 0.833 0.933 0.869 
Table 1. The F-score over the bakeoff-2 data.  Roov AS CityU MSR PKU 
Best Bakeoff 0.696 0.698 0.717 0.636 
Algorithm 2 0.440 0.489 0.429 0.434 
Algorithm 1 0.329 0.367 0.411 0.416 
MM 0.004 0.000 0.000 0.059 Table 2. The Roov over the bakeoff-2 data.  
Riv AS CityU MSR PKU 
Best Bakeoff 0.963 0.961 0.968 0.972 
Algorithm 2 0.961 0.961 0.970 0.951 
Algorithm 1 0.955 0.940 0.950 0.940 
MM 0.950 0.952 0.981 0.956 Table 3. The Riv over the bakeoff-2 data. 
5 Conclusion In this paper, we have presented the phrase-based tokenization for adaptive word segmentation. The proposed model is efficient in both training and decoding, which is desirable for fast model re-construction. It generalizes the traditional 
23
tokenization model by considering the phrase instead of the word as the segmentation hypothesis unit, which is capable of describing ?morphemes in the context? and improves the OOV recognition performance significantly. Our approach decomposes sentence tokenization into phrase tokenizations. The final tokenization of the sentence is determined by finding the best combination of the tokenizations of phrases that cover the whole sentence. The tokenization hypotheses of a sentence are represented by a weighed directed acyclic graph called phrase tokenization lattice. Using this formalism, the sentence tokenization problem becomes a shortest path search problem on the graph.     In our model, one only needs to estimate the phrase tokenization probabilities in order to segment new sentences. The training is thus a linear time phrase extraction and maximum likelihood estimation procedure. We adopted a Viterbi-style dynamic programming algorithm to segment unseen sentences using the lattice. We also proposed a method called backward dependency match to model the dependencies of adjacent phrases to overcome the limitations of the assumption that tokenizations of neighboring phrases is independent. The experiment showed the effectiveness of the proposed phrase-based model in recognizing out-of-vocabulary words and its superior overall performance compared with the traditional tokenization model. It has both the efficiency of the tokenization model and the high performance of the character classification model.        One possible extension of the proposed model is to apply re-ranking techniques (Collins and Koo, 2005) to the k-best list generated by Algorithm 2. A second improvement would be to combine our model with other models in a log linear way as in Jiang et al (2008). Since phrase-based tokenization is a model that can be accompanied by different training algorithms, it is also interesting to see whether discriminative training can lead to better performance. Acknowledgments The research leading to these results has received funding from the European Commission?s 7th Framework Program under grant agreement n? 238405 (CLARA). 
References  Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1992. A Maximum Entropy Approach to Natural Language Processing. 1996. Computational Linguistics, 22(1): 39-71 Michael Collins and Terry Koo. 2005. Discriminative Reranking for Natural Language Parsing. Computational Linguistics, 31(1):25-69. Thomas Emerson. 2005. The second international Chi- nese word segmentation bakeoff. In Proceedings of Forth SIGHAN Workshop on Chinese Language Processing. Jeju Island, Korea. Jin Guo. 1997. Critical tokenization and its properties. Computational Linguistics, 23(4): 569-596 Qinan Hu, Haihua Pan, and Chunyu Kit. 2004. An example-based study on Chinese word segmentation using critical fragments. In Proceedings of IJCNLP-2004. Hainan Island, China Changning Huang and Hai Zhao. 2007. Chinese Word Segmentation: a Decade Review. Journal of Chinese Information Processing, 21(3): 8-20 Chu-Ren Huang, Petr Simon, Shu-Kai Hsieh, and Laurent Pr?vot. Rethinking Chinese word segmentation: tokenization, character classification, or wordbreak identification. In Proceedings of ACL-2007. Prague, Czech Liang Huang. 2008. Advanced dynamic programming in semiring and hypergraph frameworks. In Proceedings of COLING 2008. Manchester, UK. Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of the Ninth International Workshop on Parsing Technology. Vancouver, Canada Wenbin Jiang, Liang Huang, Qun Liu, Yajuan Lu. 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings of ACL 2008: HLT. Columbus, USA John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML 2001. Williamstown, MA, USA Gina-Anne Levow. 2006. The third international Chinese language processing bakeoff: Word segmentation and named entity recognition. In Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing. Sydney, Australia 
24
Nanyuan Liang. 1986. On computer automatic word segmentation of written Chinese. Journal of Chinese Information Processing, 1(1). Wei-Yun Ma and Keh-Jiann Chen. 2003. A bottom-up merging algorithm for Chinese unknown word extraction. In Proceedings of the second SIGHAN workshop on Chinese language processing. Sapporo, Japan Yan Ma. 1996. The study and realization of an evaluation-based automatic segmentation system. In Changning Huang and Ying Xia, editors, Essays in Language Information Processing. Tsinghua University Press, Beijing, China. Mehryar Mohri. 2002. Semiring frameworks and algorithms for shortest-distance problems. Journal of Automata, Languages and Combinatorics, 7(3):321?350. Fuchun Peng, Fangfang Feng, and Andrew McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. In Proceedings of COLING. Stroudsburg, PA, USA. Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-state word-segmentation algorithm for Chinese. Computational Linguistics, 22(3):377-404. Anand Venkataraman. 2001. A Statistical Model for Word Discovery in Transcribed Speech. Computational Linguistics, 27(3): 351-372 Andrew Viterbi (1967). Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. IEEE Transactions on Information Theory, 13 (2): 260?269. Xiaolong Wang, Kaizhu Wang, and Xiaohua Bai. 1991. Separating syllables and characters into words in natural language understanding. Journal of Chinese Information Processing, 5(3):48-58. Nianwen Xue. 2003. Chinese Word Segmentation as Characater Tagging. Computational Linguistics and Chinese Language Processing, 8(1): 29-48 Hongmei Zhao and Qun Liu. 2010. The CIPS-SIGHAN CLP 2010 Chinese Word Segmentation Bakeoff. In Proceedings of the First CPS-SIGHAN Joint Conference on Chinese Language Processing. Beijing, China.  
25
