Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 517?523,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Automatic prediction of aspectual class of verbs in context
Annemarie Friedrich and Alexis Palmer
Department of Computational Linguistics
Saarland University, Saarbr?ucken, Germany
{afried,apalmer}@coli.uni-saarland.de
Abstract
This paper describes a new approach to
predicting the aspectual class of verbs in
context, i.e., whether a verb is used in a
stative or dynamic sense. We identify two
challenging cases of this problem: when
the verb is unseen in training data, and
when the verb is ambiguous for aspec-
tual class. A semi-supervised approach us-
ing linguistically-motivated features and a
novel set of distributional features based
on representative verb types allows us to
predict classes accurately, even for unseen
verbs. Many frequent verbs can be either
stative or dynamic in different contexts,
which has not been modeled by previous
work; we use contextual features to re-
solve this ambiguity. In addition, we intro-
duce two new datasets of clauses marked
for aspectual class.
1 Introduction
In this work, we focus on the automatic prediction
of whether a verb in context is used in a stative or
in a dynamic sense, the most fundamental distinc-
tion in all taxonomies of aspectual class. The as-
pectual class of a discourse?s finite verbs is an im-
portant factor in conveying and interpreting tem-
poral structure (Moens and Steedman, 1988; Dorr,
1992; Klavans and Chodorow, 1992); others are
tense, grammatical aspect, mood and whether the
utterance represents an event as completed. More
accurate temporal information processing is ex-
pected to be beneficial for a variety of natural lan-
guage processing tasks (Costa and Branco, 2012;
UzZaman et al, 2013).
While most verbs have one predominant inter-
pretation, others are more flexible for aspectual
class and can occur as either stative (1) or dynamic
(2) depending on the context. There are also cases
that allow for both readings, such as (3).
(1) The liquid fills the container. (stative)
(2) The pool slowly filled with water. (dynamic)
(3) Your soul was made to be filled with God
Himself. (both) (Brown corpus, religion)
Cases like (3) do not imply that there is a third
class, but rather that two interpretations are avail-
able for the sentence, of which usually one will be
chosen by a reader.
Following Siegel and McKeown (2000), we aim
to automatically classify clauses for fundamental
aspectual class, a function of the main verb and
a select group of complements, which may dif-
fer per verb (Siegel and McKeown, 2000; Siegel,
1998b). This corresponds to the aspectual class
of the clause?s main verb when ignoring any as-
pectual markers or transformations. For exam-
ple, English sentences with perfect tense are usu-
ally considered to introduce states to the discourse
(Smith, 1991; Katz, 2003), but we are interested in
the aspectual class before this transformation takes
place. The clause John has kissed Mary introduces
a state, but the fundamental aspectual class of the
?tenseless? clause John kiss Mary is dynamic.
In contrast to Siegel and McKeown (2000), we
do not conduct the task of predicting aspectual
class solely at the type level, as such an approach
ignores the minority class of ambiguous verbs. In-
stead we predict the aspectual class of verbs in
the context of their arguments and modifiers. We
show that this method works better than using only
type-based features, especially for verbs with am-
biguous aspectual class. In addition, we show
that type-based features, including novel distribu-
tional features based on representative verbs, accu-
rately predict predominant aspectual class for un-
seen verb types. Our work differs from prior work
in that we treat the problem as a three-way clas-
sification task, predicting DYNAMIC, STATIVE or
BOTH as the aspectual class of a verb in context.
517
2 Related work
Aspectual class is well treated in the linguistic lit-
erature (Vendler, 1957; Dowty, 1979; Smith, 1991,
for example). Our notion of the stative/dynamic
distinction corresponds to Bach?s (1986) distinc-
tion between states and non-states; to states ver-
sus occurrences (events and processes) according
to Mourelatos (1978); and to Vendler?s (1957) dis-
tinction between states and the other three classes
(activities, achievements, accomplishments).
Early studies on the computational modeling
of aspectual class (Nakhimovsky, 1988; Passon-
neau, 1988; Brent, 1991; Klavans and Chodorow,
1992) laid foundations for a cluster of papers pub-
lished over a decade ago (Siegel and McKeown,
2000; Siegel, 1998b; Siegel, 1998a). Since then,
it has mostly been treated as a subtask within
temporal reasoning, such as in efforts related to
TimeBank (Pustejovsky et al, 2003) and the Tem-
pEval challenges (Verhagen et al, 2007; Verha-
gen et al, 2010; UzZaman et al, 2013), where
top-performing systems (Jung and Stent, 2013;
Bethard, 2013; Chambers, 2013) use corpus-based
features, WordNet synsets, parse paths and fea-
tures from typed dependencies to classify events
as a joint task with determining the event?s span.
Costa and Branco (2012) explore the usefulness of
a wider range of explicitly aspectual features for
temporal relation classification.
Siegel and McKeown (2000) present the most
extensive study of predicting aspectual class,
which is the main inspiration for this work. While
all of their linguistically motivated features (see
section 4.1) are type-based, they train on and eval-
uate over labeled verbs in context. Their data
set taken from medical discharge summaries com-
prises 1500 clauses containing main verbs other
than be and have which are marked for aspectual
class. Their model fails to outperform a baseline
of memorizing the most frequent class of a verb
type, and they present an experiment testing on un-
seen verb types only for the related task of classi-
fying completedness of events. We replicate their
method using publicly available software, create
a similar but larger corpus,
1
and show that it is
indeed possible to predict the aspectual class of
unseen verbs. Siegel (1998a) investigates a classi-
fication method for the verb have in context; in-
1
Direct comparison on their data is not possible; feature
values for the verbs studied are available, but full texts and
the English Slot Grammar parser (McCord, 1990) are not.
COMPLETE W/O have/be/none
genre clauses ? clauses ?
jokes 3462 0.85 2660 0.77
letters 1848 0.71 1444 0.62
news 2565 0.79 2075 0.69
all 7875 0.80 6161 0.70
Table 1: Asp-MASC: Cohen?s observed un-
weighted ?.
DYNAMIC STATIVE BOTH
DYNAMIC 4464 164 9
STATIVE 434 1056 29
BOTH 5 0 0
Table 2: Asp-MASC: confusion matrix for two
annotators, without have/be/none clauses, ? is 0.7.
spired by this work, our present work goes one
step further and uses a larger set of instance-based
contextual features to perform experiments on a
set of 20 verbs. To the best of our knowledge, there
is no previous work comprehensively addressing
aspectual classification of verbs in context.
3 Data
Verb type seed sets Using the LCS Database
(Dorr, 2001), we identify sets of verb types whose
senses are only stative (188 verbs, e.g. belong,
cost, possess), only dynamic (3760 verbs, e.g. al-
ter, knock, resign), or mixed (215 verbs, e.g. fill,
stand, take), following a procedure described by
Dorr and Olsen (1997).
Asp-MASC The Asp-MASC corpus consists of
7875 clauses from the letters, news and jokes sec-
tions of MASC (Ide et al, 2010), each labeled
by two annotators for the aspectual class of the
main verb.
2
Texts were segmented into clauses us-
ing SPADE (Soricut and Marcu, 2003) with some
heuristic post-processing. We parse the corpus us-
ing the Stanford dependency parser (De Marneffe
et al, 2006) and extract the main verb of each seg-
ment. We use 6161 clauses for the classification
task, omitting clauses with have or be as the main
verb and those where no main verb could be iden-
tified due to parsing errors (none). Table 1 shows
inter-annotator agreement; Table 2 shows the con-
fusion matrix for the two annotators. Our two an-
notators exhibit different preferences on the 598
cases where they disagree between DYNAMIC and
STATIVE. Such differences in annotation prefer-
2
Corpus freely available from
www.coli.uni-saarland.de/
?
afried.
518
DYNAMIC STATIVE BOTH
DYNAMIC 1444 201 54
STATIVE 168 697 20
BOTH 44 31 8
Table 3: Asp-Ambig: confusion matrix for two
annotators. Cohen?s ? is 0.6.
ences are not uncommon (Beigman Klebanov et
al., 2008). We observe higher agreement in the
jokes and news subcorpora than for letters; texts
in the letters subcorpora are largely argumentative
and thus have a different rhetorical style than the
more straightforward narratives and reports found
in jokes. Overall, we find substantial agreement.
The data for our experiments uses the label DY-
NAMIC or STATIVE whenever annotators agree,
and BOTH whenever they disagree or when at least
one annotator marked the clause as BOTH, assum-
ing that both readings are possible in such cases.
Because we don?t want to model the authors? per-
sonal view of the theory, we refrain from applying
an adjudication step and model the data as is.
Asp-Ambig: (Brown) In order to facilitate a
first study on ambiguous verbs, we select 20 fre-
quent verbs from the list of ?mixed? verbs (see
section 3) and for each mark 138 sentences. Sen-
tences are extracted randomly from the Brown cor-
pus, such that the distribution of stative/dynamic
usages is expected to be natural. We present
entire sentences to the annotators who mark the
aspectual class of the verb in question as high-
lighted in the sentence. The data is processed in
the same way as Asp-MASC, discarding instances
with parsing problems. This results in 2667 in-
stances. ? is 0.6, the confusion matrix is shown in
Table 3. Details are listed in Table 10.
4 Model and Features
For predicting the aspectual class of verbs in con-
text (STATIVE, DYNAMIC, BOTH), we assume a
supervised learning setting and explore features
mined from a large background corpus, distribu-
tional features, and instance-based features. If not
indicated otherwise, experiments use a Random
Forest classifier (Breiman, 2001) trained with the
implementation and standard parameter settings
from Weka (Hall et al, 2009).
4.1 Linguistic indicator features (LingInd)
This set of corpus-based features is a reimple-
mentation of the linguistic indicators of Siegel
FEATURE EXAMPLE FEATURE EXAMPLE
frequency - continuous continually
present says adverb endlessly
past said evaluation better
future will say adverb horribly
perfect had won manner furiously
progressive is winning adverb patiently
negated not/never temporal again
particle up/in/... adverb finally
no subject - in-PP in an hour
for-PP for an hour
Table 4: LingInd feature set and examples for lex-
ical items associated with each indicator.
FEATURE VALUES
part-of-speech tag of the verb VB, VBG, VBN, ...
tense present, past, future
progressive true/false
perfect true/false
voice active/passive
grammatical dependents WordNet lexname/POS
Table 5: Instance-based (Inst) features
and McKeown (2000), who show that (some of)
these features correlate with either stative or dy-
namic verb types. We parse the AFE and XIE sec-
tions of Gigaword (Graff and Cieri, 2003) with
the Stanford dependency parser. For each verb
type, we obtain a normalized count showing how
often it occurs with each of the indicators in Ta-
ble 4, resulting in one value per feature per verb.
For example, for the verb fill, the value of the
feature temporal-adverb is 0.0085, meaning
that 0.85% of the occurrences of fill in the corpus
are modified by one of the temporal adverbs on the
list compiled by Siegel (1998b). Tense, progres-
sive, perfect and voice are extracted using a set of
rules following Loaiciga et al (2014).
3
4.2 Distributional Features (Dist)
We aim to leverage existing, possibly noisy sets
of representative stative, dynamic or mixed verb
types extracted from LCS (see section 3), mak-
ing up for unseen verbs and noise by averaging
over distributional similarities. Using an exist-
ing large distributional model (Thater et al, 2011)
estimated over the set of Gigaword documents
marked as stories, for each verb type, we build
a syntactically informed vector representing the
contexts in which the verb occurs. We compute
three numeric feature values per verb type, which
correspond to the average cosine similarities with
the verb types in each of the three seed sets.
3
We thank the authors for providing us their code.
519
FEATURES ACCURACY (%)
Baseline (Lemma) 83.6
LingInd 83.8
Inst 70.8
Inst+Lemma 83.7
Dist 83.4
LingInd+Inst+Dist+Lemma 84.1
Table 6: Experiment 1: SEEN verbs, using Asp-
MASC. Baseline memorizes most frequent class
per verb type in training folds.
4.3 Instance-based features (Inst)
Table 5 shows our set of instance-based syntac-
tic and semantic features. In contrast to the above
described type-based features, these features do
not rely on a background corpus, but are ex-
tracted from the clause being classified. Tense,
progressive, perfect and voice are extracted from
dependency parses as described above. For fea-
tures encoding grammatical dependents, we focus
on a subset of grammatical relations. The fea-
ture value is either the WordNet lexical filename
(e.g. noun.person) of the given relation?s argu-
ment or its POS tag, if the former is not avail-
able. We simply use the most frequent sense for
the dependent?s lemma. We also include features
that indicate, if there are any, the particle of the
verb and its prepositional dependents. For the
sentence A little girl had just finished her first
week of school, the instance-based feature values
would include tense:past, subj:noun.person,
dobj:noun.time or particle:none.
5 Experiments
The experiments presented in this section aim to
evaluate the effectiveness of the feature sets de-
scribed in the previous section, focusing on the
challenging cases of verb types unseen in the train-
ing data and highly ambiguous verbs. The feature
Lemma indicates that the verb?s lemma is used as
an additional feature.
Experiment 1: SEEN verbs
The setting of our first experiment follows Siegel
and McKeown (2000). Table 6 reports results for
10-fold cross-validation, with occurrences of all
verbs distributed evenly over the folds. No feature
combination significantly
4
outperforms the base-
line of simply memorizing the most frequent class
4
According to McNemar?s test with Yates? correction for
continuity, p < 0.01.
FEATURES ACCURACY (%)
1 Baseline 72.5
2 Dist 78.3?
3 LingInd 80.4?
4 LingInd+Dist 81.9*?
Table 7: Experiment 2: UNSEEN verb types, Lo-
gistic regression, Asp-MASC. Baseline labels ev-
erything with the most frequent class in the train-
ing set (DYNAMIC). *Significantly
4
different from
line 1. ?Significantly
4
different from line 3.
DATA FEATURES ACC. (%)
one-label Baseline 92.8
verbs LingInd 92.8
Dist 92.6
(1966 inst.) Inst+Lemma 91.4?
LingInd+Inst+Lemma 92.4
multi-label Baseline 78.9
verbs LingInd 79.0
Dist 79.0
(4195 inst.) Inst 67.4?
Inst+Lemma 79.9
LingInd+Inst+Lemma 80.9*
LingInd+Inst+Lemma+Dist 80.2*
Table 8: Experiment 3: ?ONE- VS. MULTI-
LABEL? verbs, Asp-MASC. Baseline as in Table
6. *Indicates that result is significantly
4
different
from the respective baseline.
of a verb type in the respective training folds.
Experiment 2: UNSEEN verbs
This experiment shows a successful case of semi-
supervised learning: while type-based feature val-
ues can be estimated from large corpora in an un-
supervised way, some labeled training data is nec-
essary to learn their best combination. This exper-
iment specifically examines performance on verbs
not seen in labeled training data. We use 10-fold
cross validation but ensure that all occurrences of
a verb type appear in the same fold: verb types
in each test fold have not been seen in the re-
spective training data, ruling out the Lemma fea-
ture. A Logistic regression classifier (Hall et al,
2009) works better here (using only numeric fea-
tures), and we present results in Table 7. Both the
LingInd and Dist features generalize across verb
types, and their combination works best.
Experiment 3: ONE- vs. MULTI-LABEL verbs
For this experiment, we compute results sepa-
rately for one-label verbs (those for which all in-
stances in Asp-MASC have the same label) and
520
SYSTEM CLASS ACC. P R F
baseline micro-avg. 78.9 0.75 0.79 0.76
LingInd DYNAMIC 0.84 0.95 0.89
+Inst STATIVE 0.76 0.69 0.72
+Lemma BOTH 0.51 0.24 0.33
micro-avg. 80.9* 0.78 0.81 0.79
Table 9: Experiment 3: ?MULTI-LABEL?, preci-
sion, recall and F-measure, detailed class statistics
for the best-performing system from Table 8.
for multi-label verbs (instances have differing la-
bels in Asp-MASC). We expect one-label verbs
to have a strong predominant aspectual class, and
multi-label verbs to be more flexible. Otherwise,
the experimental setup is as in experiment 1. Re-
sults appear in Table 8. In each case, the linguistic
indicator features again perform on par with the
baseline. For multi-label verbs, the feature combi-
nation Lemma+LingInd+Inst leads to significant
4
improvement of 2% gain in accuracy over the
baseline; Table 9 reports detailed class statistics
and reveals a gain in F-measure of 3 points over
the baseline. To sum up, Inst features are essential
for classifying multi-label verbs, and the LingInd
features provide some useful prior. These results
motivate the need for an instance-based approach.
Experiment 4: INSTANCE-BASED classification
For verbs with ambiguous aspectual class, type-
based classification is not sufficient, as this ap-
proach selects a dominant sense for any given verb
and then always assigns that. Therefore we pro-
pose handling ambiguous verbs separately. As
Asp-MASC contains only few instances of each of
the ambiguous verbs, we turn to the Asp-Ambig
dataset. We perform a Leave-One-Out (LOO)
cross validation evaluation, with results reported
in Table 10.
5
Using the Inst features alone (not
shown in Table 10) results in a micro-average ac-
curacy of only 58.1%: these features are only use-
ful when combined with the feature Lemma. For
classifying verbs whose most frequent class oc-
curs less than 56% of the time, Lemma+Inst fea-
tures are essential. Whether or not performance
is improved by adding LingInd/Dist features, with
their bias towards one aspectual class, depends
on the verb type. It is an open research question
which verb types should be treated in which way.
5
The third column also shows the outcome of using ei-
ther only the Lemma, only LingInd or only Dist in LOO; all
have almost the same outcome as using the majority class,
numbers differ only after the decimal point.
I
n
s
t
+
L
e
m
m
a
I
n
s
t
+
L
e
m
m
a
+
L
i
n
g
I
n
d
+
D
i
s
t
# OF MAJORITY
VERB INST. CLASS
5
feel 128 96.1 STAT 93.0 93.8
say 138 94.9 DYN 93.5 93.5
make 136 91.9 DYN 91.9 91.2
come 133 88.0 DYN 87.2 87.2
take 137 85.4 DYN 85.4 85.4
meet 130 83.9 DYN 86.2 87.7
stand 130 80.0 STAT 79.2 83.1
find 137 74.5 DYN 69.3 68.8
accept 134 70.9 DYN 64.9 65.7
hold 134 56.0 BOTH 43.3 49.3
carry 136 55.9 DYN 55.9 58.1
look 138 55.8 DYN 72.5 74.6
show 133 54.9 DYN 69.2 68.4
appear 136 52.2 STAT 64.7 61.0
follow 122 51.6 BOTH 69.7 65.6
consider 138 50.7 DYN 61.6 70.3
cover 123 50.4 STAT 46.3 54.5
fill 134 47.8 DYN 66.4 62.7
bear 135 47.4 DYN 70.4 67.4
allow 135 37.8 DYN 48.9 51.9
micro-avg. 2667 66.3 71.0* 72.0*
Table 10: Experiment 4: INSTANCE-BASED.
Accuracy (in %) on Asp-Ambig. *Differs
significantly
4
from the majority class baseline.
6 Discussion and conclusions
We have described a new, context-aware approach
to automatically predicting aspectual class, includ-
ing a new set of distributional features. We have
also introduced two new data sets of clauses la-
beled for aspectual class. Our experiments show
that in any setting where labeled training data
is available, improvement over the most frequent
class baseline can only be reached by integrating
instance-based features, though type-based fea-
tures (LingInd, Dist) may provide useful priors
for some verbs and successfully predict predom-
inant aspectual class for unseen verb types. In or-
der to arrive at a globally well-performing system,
we envision a multi-stage approach, treating verbs
differently according to whether training data is
available and whether or not the verb?s aspectual
class distribution is highly skewed.
Acknowledgments We thank the anonymous
reviewers, Omri Abend, Mike Lewis, Manfred
Pinkal, Mark Steedman, Stefan Thater and Bonnie
Webber for helpful comments, and our annotators
A. Kirkland and R. K?uhn. This research was sup-
ported in part by the MMCI Cluster of Excellence,
and the first author is supported by an IBM PhD
Fellowship.
521
References
Emmon Bach. 1986. The algebra of events. Linguis-
tics and philosophy, 9(1):5?16.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In Pro-
ceedings of the Workshop on Human Judgements in
Computational Linguistics, pages 2?7. Association
for Computational Linguistics.
Steven Bethard. 2013. ClearTK-TimeML: A minimal-
ist approach to TempEval 2013. In Second Joint
Conference on Lexical and Computational Seman-
tics (* SEM), volume 2, pages 10?14.
Leo Breiman. 2001. Random forests. Machine Learn-
ing, 45(1):5?32.
Michael R. Brent. 1991. Automatic semantic classifi-
cation of verbs from their syntactic contexts: an im-
plemented classifier for stativity. In Proceedings of
the fifth conference on European chapter of the As-
sociation for Computational Linguistics, pages 222?
226. Association for Computational Linguistics.
Nathanael Chambers. 2013. Navytime: Event and
time ordering from raw text. In Second Joint Con-
ference on Lexical and Computational Semantics (*
SEM), volume 2, pages 73?77.
Francisco Costa and Ant?onio Branco. 2012. Aspec-
tual type and temporal relation classification. In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 266?275. Association for Computa-
tional Linguistics.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al 2006. Generat-
ing typed dependency parses from phrase structure
parses. In Proceedings of LREC, volume 6, pages
449?454.
Bonnie J. Dorr and Mari Broman Olsen. 1997. De-
riving verbal and compositional lexical aspect for
NLP applications. In Proceedings of the eighth con-
ference on European chapter of the Association for
Computational Linguistics, pages 151?158. Associ-
ation for Computational Linguistics.
Bonnie J. Dorr. 1992. A two-level knowledge repre-
sentation for machine translation: lexical semantics
and tense/aspect. In Lexical Semantics and Knowl-
edge Representation, pages 269?287. Springer.
Bonnie J. Dorr. 2001. LCS verb database. Online
software database of Lexical Conceptual Structures,
University of Maryland, College Park, MD.
David Dowty. 1979. Word Meaning and Montague
Grammar. Reidel, Dordrecht.
David Graff and Christopher Cieri. 2003. English gi-
gaword.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
ACM SIGKDD explorations newsletter, 11(1):10?
18.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-
becca Passonneau. 2010. The manually annotated
sub-corpus: a community resource for and by the
people. In Proceedings of the ACL 2010 Conference
Short Papers, pages 68?73. Association for Compu-
tational Linguistics.
Hyuckchul Jung and Amanda Stent. 2013. ATT1:
Temporal annotation using big windows and rich
syntactic and semantic features. In Second Joint
Conference on Lexical and Computational Seman-
tics (* SEM), volume 2, pages 20?24.
Graham Katz. 2003. On the stativity of the english
perfect. Perfect explorations, pages 205?234.
Judith L. Klavans and Martin Chodorow. 1992. De-
grees of stativity: the lexical representation of verb
aspect. In Proceedings of the 14th conference on
Computational Linguistics, pages 1126?1131. Asso-
ciation for Computational Linguistics.
Sharid Loaiciga, Thomas Meyer, and Andrei Popescu-
Belis. 2014. English-French Verb Phrase Align-
ment in Europarl for Tense Translation Modeling.
In Language Resources and Evaluation Conference
(LREC), Reykjavik, Iceland.
Michael C. McCord. 1990. Slot Grammar. Springer.
Marc Moens and Mark J. Steedman. 1988. Tempo-
ral ontology and temporal reference. Computational
Linguistics, 14(2):15?28.
Alexander P.D. Mourelatos. 1978. Events, processes,
and states. Linguistics and philosophy, 2(3):415?
434.
Alexander Nakhimovsky. 1988. Aspect, aspectual
class, and the temporal structure of narrative. Com-
putational Linguistics, 14(2):29?43.
Rebecca Passonneau. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, Spring 1988.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, et al 2003. The timebank corpus. In Corpus
linguistics, volume 2003, page 40.
Eric V. Siegel and Kathleen R. McKeown. 2000.
Learning methods to combine linguistic indica-
tors: Improving aspectual classification and reveal-
ing linguistic insights. Computational Linguistics,
26(4):595?628.
522
Eric V. Siegel. 1998a. Disambiguating verbs with the
WordNet category of the direct object. In Proceed-
ings of Workshop on Usage of WordNet in Natural
Language Processing Systems, Universite de Mon-
treal.
Eric V. Siegel. 1998b. Linguistic Indicators for
Language Understanding: Using machine learn-
ing methods to combine corpus-based indicators for
aspectual classification of clauses. Ph.D. thesis,
Columbia University.
Carlota S. Smith. 1991. The Parameter of Aspect.
Kluwer, Dordrecht.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149?156. Association
for Computational Linguistics.
Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In IJCNLP, pages 1134?1143.
Naushad UzZaman, Hector Llorens, Leon Derczyn-
ski, Marc Verhagen, James Allen, and James Puste-
jovsky. 2013. Semeval-2013 task 1: Tempeval-3:
Evaluating time expressions, events, and temporal
relations. In Second joint conference on lexical and
computational semantics (* SEM), volume 2, pages
1?9.
Zeno Vendler, 1957. Linguistics in Philosophy, chapter
Verbs and Times, pages 97?121. Cornell University
Press, Ithaca, New York.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. Semeval-2007 task 15: Tempeval temporal
relation identification. In Proceedings of the 4th
International Workshop on Semantic Evaluations,
pages 75?80. Association for Computational Lin-
guistics.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and
James Pustejovsky. 2010. SemEval-2010 task 13:
TempEval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 57?
62. Association for Computational Linguistics.
523
LAW VIII - The 8th Linguistic Annotation Workshop, pages 149?158,
Dublin, Ireland, August 23-24 2014.
Situation entity annotation
Annemarie Friedrich Alexis Palmer
Department of Computational Linguistics
Saarland University, Saarbr?ucken, Germany
{afried,apalmer}@coli.uni-saarland.de
Abstract
This paper presents an annotation scheme for a new semantic annotation task with relevance for
analysis and computation at both the clause level and the discourse level. More specifically, we
label the finite clauses of texts with the type of situation entity (e.g., eventualities, statements
about kinds, or statements of belief) they introduce to the discourse, following and extending
work by Smith (2003). We take a feature-driven approach to annotation, with the result that
each clause is also annotated with fundamental aspectual class, whether the main NP referent is
specific or generic, and whether the situation evoked is episodic or habitual. This annotation is
performed (so far) on three sections of the MASC corpus, with each clause labeled by at least
two annotators. In this paper we present the annotation scheme, statistics of the corpus in its
current version, and analyses of both inter-annotator agreement and intra-annotator consistency.
1 Introduction
Linguistic expressions form patterns in discourse. Passages of text can be analyzed in terms of the
individuals, concepts, times and situations that they introduce to the discourse. In this paper we intro-
duce a new semantic annotation task which focuses on the latter and in particular their aspectual nature.
Situations are expressed at the clause level; situation entity (SE) annotation is the task of associating
individual clauses of text with the type of SE introduced to the discourse by the clause. Following Smith
(2003), we distinguish the following SE types (see Sec. 3.1): EVENTS, STATES, GENERALIZING SEN-
TENCES, GENERIC SENTENCES, FACTS, PROPOSITIONS, QUESTIONS and IMPERATIVES. Although
these categories are clearly distinct from one another on theoretical grounds, in practice it can be difficult
to cleanly draw boundaries between them. We improve annotation consistency by defining the SE types
in terms of features whose values are easier for annotators to identify, and which provide guidance for
distinguishing the more complex SE types.
As with most complex annotation tasks, multiple interpretations are often possible, and we cannot
expect agreement on all instances. The feature-driven approach (see Sec. 3.2) is a valuable source of
information for investigating annotator disagreements, as the features indicate precisely how annotators
differ in their interpretation of the situation. Analysis of intra-annotator consistency shows that personal
preferences of annotators play a role, and we conclude that disagreements often highlight cases where
multiple interpretations are possible. We further argue that such cases should be handled carefully in
supervised learning approaches targeting methods to automatically classify situation entity types.
As the first phase of the SE annotation project, we are in the process of annotating the written portion
of MASC (Ide et al., 2010), the manually-annotated subcorpus of the Open American National Corpus.
MASC provides texts from 20 different genres and has already been annotated with various linguistic
and semantic phenomena.
1
MASC offers several benefits: it includes text from a wide variety of genres,
it facilitates study of interactions between various levels of analysis, and the data is freely available
with straightforward mechanisms for distribution. In this paper we report results for three of the MASC
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.americannationalcorpus.org/MASC/Full_MASC.html
149
genres: news, letters, and jokes. Once a larger portion of MASC has been labeled with SEs and their
associated features, we will add our annotations to those currently available for MASC. We mark the SE
types of clauses with the aim of providing a large corpus of annotated text for the following purposes:
(1) To assess the applicability of SE type classification as described by Smith (2003): to what extent
can situations be classified easily, which borderline cases occur, and how do humans perform on this
task? (see Sec. 4)
(2) Training, development and evaluation of automatic systems classifying situation entities, as well
as sub-tasks which have (partially) been studied by the NLP community, but for which no large
annotated corpora are available (for example, automatically predicting the fundamental aspectual
class of verbs in context (Friedrich and Palmer, 2014) or the genericity of clauses and noun phrases).
(3) To provide a foundation for analysis of the theory of Discourse Modes (Smith, 2003), which we
explain next (Sec. 2).
2 Background and related work
Within a text, one recognizes stretches that are intuitively of different types and can be clustered by their
characteristic linguistic features and interpretations. Smith (2003) posits five discourse modes: Narrative,
Report, Description, Informative and Argument/Commentary. Texts of almost all genre categories have
passages of different modes. The discourse modes are characterized by (a) the type of situations (also
called situation entities) introduced in a text passage, and (b) the principle of text progression in the
mode (temporal or atemporal, and different manners of both temporal and atemporal progression). This
annotation project directly addresses the first of these characteristics, the situation entity types (SE types).
Some previous work has addressed the task of classifying SE types at the clause level. Palmer et al.
(2004) enrich LFG parses with lexical information from both a database of lexical conceptual structures
(Dorr, 2001) and hand-collected groups of predicates associated with particular SE types. The enriched
parses are then fed to an ordered set of transfer rules which encode linguistic features indicative of SE
types. The system is evaluated on roughly 200 manually-labeled clauses. Palmer et al. (2007) investigate
various types of linguistic features in a maximum entropy model for SE type classification. The best
results are still below 50% accuracy (with a most-frequent-class baseline of 38%), and incorporating
features from neighboring clauses is shown to increase performance. Palmer et al. (2007) annotate data
from one section of the Brown corpus and a small amount of newswire text, with two annotators and
no clear set annotation guidelines. In addition, work by Cocco (2012) classifies clauses of French text
according to a six-way scheme that falls somewhere between the SE level and the level of discourse
modes. The types are: narrative, argumentative, descriptive, explicative, dialogal, and injunctive.
Other related works address tasks related to the features we annotate. One strand of work is in auto-
matic classification of aspectual class (Siegel and McKeown, 2000; Siegel, 1999; Siegel, 1998; Klavans
and Chodorow, 1992; Friedrich and Palmer, 2014) and its determination as part of temporal classification
(UzZaman et al., 2013; Bethard, 2013; Costa and Branco, 2012). A second aims to distinguish generic
vs. specific clauses (Louis and Nenkova, 2011) or to identify generic noun phrases (Reiter and Frank,
2010). The latter work leverages data with noun phrases annotated as either generic and specific from
the ACE-2 corpus (Mitchell et al., 2003); their definitions of these two types match ours (see Sec. 3.2.1).
3 Annotation Scheme and Process
In this section, we first present the inventory of SE types (Sec. 3.1). We then describe our feature-
driven approach to annotation (Sec. 3.2) and define the SE types with respect to three situation-related
features: main referent type, fundamental aspectual class, and habituality. Some situation entity types
are easier to recognize than others. While some can be identified on the basis of surface structure and
clear linguistic indicators, others depend on internal temporal (and other) properties of the verb and its
arguments. Annotators take the following approach: first, easily-identifiable SE types (Speech Acts and
Abstract Entities) are marked. If the clause?s SE type is not one of these, values for the three features are
determined, and the final determination of SE type is based on the features.
150
3.1 Situation entity types
Following Smith (2003), we distinguish the following SE types:
Eventualities. These types describe particular situations such as STATES (1a) or EVENTS (2). The type
REPORT, a subtype of EVENT, is used for situations introduced by verbs of speech (1b).
(1) (a) ?Carl is a tenacious fellow?, (STATE)
(b) said a source close to USAir. (EVENT ? REPORT)
(2) The lobster won the quadrille. (EVENT)
General Statives. This class includes GENERALIZING SENTENCES (3), which report regularities re-
lated to specific main referents, and GENERIC SENTENCES (4), which make statements about kinds.
(3) Mary often feeds my cats. (GENERALIZING)
(4) The lion has a bushy tail. (GENERIC)
Abstract Entities are the third class of SE types, and comprise FACTS (5) and PROPOSITIONS (6).
These situations differ from the other types in how they relate to the world: Eventualities and General
Statives are located spatially and temporally in the world, but Abstract Entities are not. FACTS are objects
of knowledge and PROPOSITIONS are objects of belief from the respective speaker?s point of view.
(5) I know that Mary refused the offer. (FACT)
(6) I believe that Mary refused the offer . (PROPOSITION)
We limit the annotation of Abstract Entities to the clausal complements of certain licensing predicates,
as well as clauses modified by a certain class of adverbs, as it is not always possible to identify sentences
directly expressing Facts or Propositions on linguistic grounds (Smith, 2003). In (6), believe is the
licensing predicate, and Mary refused the offer is a situation that is introduced as not being in the world,
but about the world (Smith, 2003). Annotators are asked to additionally label the embedded SE type
when possible. For example, that Mary refused the offer in (5) and (6) would be labeled as EVENT.
Speech Acts. This class comprises QUESTIONS and IMPERATIVE clauses (Searle, 1969).
Derived SE types. In some cases, the SE type of a clause changes based on the addition of some
linguistic indication of uncertainty about the status of the situation described. We refer to these as derived
SE types. More specifically, clauses that would otherwise be marked as EVENT may be coerced to the
type STATE due to negation, modality, future tense, conditionality, and sometimes subjectivity: e.g. John
did not win the lottery, a negated event, introduces a STATE to the discourse.
3.2 Features for distinguishing situation entity types
In this section, we describe three features that allow for the clear expression of differences between SE
types. Fleshing out the descriptions of SE types with these underlying features is useful to convey the
annotation scheme to new annotators, to get partial information when an annotator has trouble making a
decision on SE type, and to analyze disagreements between annotators.
3.2.1 Main referent type: specific or generic
This feature indicates the type of the most central entity mentioned in the clause as a noun phrase. We
refer to this entity as the clause?s main referent. This referent can be found by asking the question: What
is this clause about? Usually, but not always, the main referent of a clause is realized as its grammatical
subject. We appeal to the annotator?s intuitions in order to determine the main referent of a clause. In
case the main referent does not coincide with the grammatical subject as in example (7), this is to be
indicated during annotation.
(7) There are two books on the table. (specific main referent, STATE)
151
Some SE types (STATES, GENERALIZING SENTENCES and GENERIC SENTENCES, for details see
Table 1) are distinguished by whether they make a statement about some specific main referent or about
a generic main referent. Specific main referents are particular entities (8), particular groups of entities (9),
organizations (10), particular situations (11) or particular instantiations of a concept (12).
(8) Mary likes popcorn. (particular entity ? specific, STATE)
(9) The students met at the cafeteria. (a particular group ? specific, STATE)
(10) IBM was a very popular company in the 80s. (organization ? specific, STATE)
(11) That she didn?t answer her phone really upset me. (particular situation ? specific, EVENT)
(12) Today?s weather was really nice. (particular instantiation of a concept ? specific, STATE)
The majority of generic main referents are noun phrases referring to a kind rather than to a particular
entity, and generic mentions of concepts or notions (14). Definite NPs and bare plural NPs (13) are the
main kind-referring NP types (Smith, 2003).
(13) The lion has a bushy tail. / Dinosaurs are extinct. (generic, GENERIC SENTENCE)
(14) Security is an important issue in US electoral campaigns. (generic, GENERIC SENTENCE)
While some NPs clearly make reference to a well-established kind, other cases are not so clear cut,
as humans tend to make up a context in which an NP describes some kind (Krifka et al., 1995). Sen-
tence (15) gives an example for such a case: while lions in captivity are not a generally well-established
kind, this term describes a class of entities rather than a specific group of lions in this context.
(15) Lions in captivity have trouble producing offspring. (generic, GENERIC SENTENCE)
Gerunds may occur as the subject in English sentences. When they describe a specific process as in
(16a), we mark them as specific. If they instead describe a kind of process as in (16b), we mark them as
generic.
(16) (a) Knitting this scarf took me 3 days. (specific, EVENT)
(b) Knitting a scarf is generally fun. (generic, GENERIC SENTENCE)
We also give annotators the option to explicitly mark the main referent as expletive, as in (17).
(17) It seemed like (expletive = no main referent, STATE)
he would win. (specific, STATE)
3.2.2 Fundamental aspectual class: stative or dynamic
Following Siegel and McKeown (2000), we determine the fundamental aspectual class of a clause. This
notion is the extension of lexical aspect or aktionsart, which describe the ?real life shape? of situations
denoted by verbs, to the level of clauses. More specifically, aspectual class is a feature of the main verb
and a select group of modifiers, which may differ per verb. The stative/dynamic distinction is the most
fundamental distinction in taxonomies of aspectual class (Vendler, 1967; Bach, 1986; Mourelatos, 1978).
We allow three labels for this feature: dynamic for cases where the verb and its arguments describe
some event (something happens), stative for cases where they introduce some properties of the main
referent to the discourse, or both for cases where annotators see both interpretations.
It is important to note that the fundamental aspectual class of a verb can be different from the type
of situation entity introduced by the clause as a whole. The basic situation type of building a house is
dynamic, and in the examples below we see this fundamental aspectual class appearing in clauses with
different situation entity types. Example (18) describes an EVENT. Clause (19), on the other hand, is a
GENERALIZING SENTENCE, as it describes a pattern of events; this is a situation with a derived type.
The same is true for example (20), which is a STATE because of its future tense.
(18) John built a house. (EVENT, dynamic fundamental aspectual class)
(19) John builds houses. (GENERALIZING SENTENCE, dynamic fundamental aspectual class)
(20) John is going to build a house. (STATE, dynamic fundamental aspectual class)
152
3.2.3 Habituality
Another dimension along which situations can be distinguished is whether they describe a static state, a
one-time (episodic) event (21) or some regularity of an event (22) or a state (23), which is labeled ha-
bitual. The term habitual as used in this annotation project covers more than what is usually considered
a matter of habit, extending to any clauses describing regularities (24). The discussion related to this
linguistic feature in this section follows Carlson (2005). If one can add a frequency adverbial such as
typically/usually to the clause and the meaning of the resulting sentence differs at most slightly from the
meaning of the original sentence, or the sentence contains a frequency adverbial such as never, the sen-
tence expresses a regularity, i.e., is habitual. Another property of habituals is that they are generalizations
and hence have the property of tolerating exceptions. If we learn that Mary eats oatmeal for breakfast, it
does not necessarily need to be true that she eats oatmeal at every breakfast. It is important to note that
unlike fundamental aspectual class, habituality is an attribute of the entire situation.
(21) Mary ate oatmeal for breakfast this morning. (episodic, EVENT)
(22) Mary eats oatmeal for breakfast. (habitual, GENERALIZING SENTENCE)
(23) I often feel as if I only get half the story. (habitual, stative fundamental aspectual class, GENER-
ALIZING SENTENCE)
(24) Glass breaks easily. (habitual, GENERIC SENTENCE)
3.3 SE types and their features
The feature-driven approach to annotation taken here is defined such that, ideally, each unique combina-
tion of values for the three features leads to one SE type. Table 1 shows the assignment of SE types to
various combinations of feature values. This table covers all SE types except ABSTRACT ENTITIES and
SPEECH ACTS, which are more easily identifiable based on lexical and/or syntactic grounds. Annotators
are also provided with information about linguistic tests for some SE types and feature values, both for
making feature value determinations and to support selection of clause-level SE type labels.
SE type main referent aspectual class habituality
EVENT
specific
eventive episodic
generic
STATE specific stative static
GENERIC SENTENCE generic
eventive habitual
stative static, habitual
GENERALIZING
specific
eventive
habitual
SENTENCE stative
General Stative
specific
eventive habitual
generic
Table 1: Situation entity types and their features.
4 Annotator agreement and consistency
This section presents analyses of inter-annotator agreement and intra-annotator consistency, looking at
agreement for individual feature values as well as clause-level SE type.
4.1 Data and annotators
The current version of our corpus consists of three sections (news, letters and jokes) of MASC corpus
(Ide et al., 2010). We hired three annotators, all either native or highly-skilled speakers of English, and
had a training phase of 3 weeks using several Wikipedia documents. Afterwards, annotation of the texts
began and annotators had no further communication with each other. Two annotators (A and B) each
marked the complete data set, and one additional annotator (C) marked the news section only.
153
ANNOTATORS NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPE
SEGMENTS REFERENT CLASS (REP=EVT)
A:B 2563 0.35 0.81 0.77 0.56 0.66
A:C 2524 0.29 0.77 0.76 0.55 0.65
B:C 2556 0.45 0.73 0.76 0.76 0.74
average 2545 0.36 0.77 0.76 0.62 0.68
Table 2: Cohen?s ?, for pairs of annotators on the MASC news section.
GENRE NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPE
SEGMENTS REFERENT CLASS (REP=EVT)
jokes 3455 0.57 0.85 0.81 0.74 0.73
news 2563 0.35 0.81 0.77 0.56 0.66
letters 1851 0.41 0.71 0.65 0.56 0.56
all 7869 0.47 0.80 0.77 0.64 0.68
Table 3: Cohen?s ?, for two annotators on three different sections of MASC.
4.2 Segmentation into clauses
We segment the texts into finite clauses using the SPADE discourse parser (Soricut and Marcu, 2003),
applying some heuristic post-processing and allowing annotators to mark segments that do not contain
a situation (for instance, headlines or by-lines) or that should be merged with another segment in order
to describe a complete situation. We filter out all segments marked by any annotator as having a seg-
mentation problem. Of the 2823 segments automatically created for the news section, 4% were marked
as containing no situation by at least one of the three annotators, and 7% were merged to a different
segment by at least one annotator. All three annotators agree on the remaining 2515 segments (89%). Of
the 9428 automatically-created segments in the full data set, 11.5% were marked as no-situation by at
least one of two annotators, and a further 5% were merged to other segments by at least one annotator.
7869 segments remain for studying agreement between two annotators on the full data set.
The three genres vary as to the average segment length. Segments in the letters texts have the longest
average length (11.1 tokens), segments in jokes are the shortest (6.9 tokens on average), and segments in
news fall in the middle with an average length of 9.9 tokens.
4.3 Inter-annotator agreement
As we allow annotators to mark a segment as Speech Acts or Abstract Entities and in addition mark the
SE type of the embedded situation with a non-surface type, we compute agreement for Eventualities and
General Statives in the following, and present the results for Speech Acts and Abstract Entities separately.
news section, 3 annotators. We compute Cohen?s unweighted ? between all three pairs of annotators
for the news section, as shown in Table 2. We compute agreement for the segments where both respective
annotators agree on the segmention, i.e., that the segment describes a situation. For aspectual class, we
compute agreement over the three labels stative, dynamic and both; for main referents, we compute
agreement over the three labels specific, dynamic and expletive; for habituality, we compute agreement
over the three labels episodic, habitual and static. In each case, we omit segments for which one of the
annotators did not give a label, which in each case are fewer than 26 segments.
We observe good agreement for the features aspectual class and habituality, and for SE type between
annotators B and C. Pairs involving annotator A reach lower agreement; we identify two causes. Anno-
tator A marks many segments marked as REPORT by the others as the corresponding supertype EVENT.
This shows up in Table 2 as higher values of ? when considering REPORT to match its supertype EVENT.
The second cause is A?s different preference for marking main referents, causing lower ? scores for agree-
ment on the main referent type and also influencing agreement for situation entity types. In more than
92% of the 183 clauses on which annotators B and C agree with each other, but disagree with A, B
and C assigned the value specific while A marked the main referent as generic. Early in the annotation
project, a revision was made to the scheme for labeling main referents ? one hypothesis is that A might
not have updated her way of labeling these. We estimate that roughly 40% of these cases were due to
154
A?s misunderstanding of feature value definitions, but around 30% of these cases do allow for both inter-
pretations. In the following sentence, the main referent of the second segment could either refer to the
specific set of all kids in New York, or to the class of children in New York: As governor, I?ll make sure
// that every kid in New York has the same opportunity. Another frequent case is the main referent you,
which can be interpreted in a generic way or as specifically addressing the reader (e.g. of a letter). Such
disagreements at the level of feature annotations allow us to detect cases where several interpretations
are possible. Having annotators with different preferences on difficult cases can actually be a valuable
source of information for identifying such cases.
The distribution of labels for main referents is highly skewed towards specific main referents for the
news section; when comparing B and C, they agree on 2358 segments to have a specific main referent.
However, only 122 segments are labeled as having a generic main referent by at least one annotator, and
they agree only on 43 of them. A further 49 are labeled generic by B but specific by C and a further 30
vice versa. In order to collect more reliable data and agreement numbers for the task of labeling main
referent types, we plan to conduct a focused study with a carefully-balanced data set.
news, jokes, letters: 2 annotators. We report agreement for three sections, corresponding to three
genres, for two annotators (A and B) in Table 3. We observe higher agreement for jokes than for news,
and higher agreement for news than for letters. Figure 1 shows the distribution of situation entity types
per genre. The numbers express averages of percentages of label types assigned to the clauses of one
genre by the two annotators. The letters genre is different in that it has more STATES, far fewer EVENTS,
which are usually easy to detect, and more General Statives. Most cases of confusion between annotators
occur between General Statives and STATES, so the more EVENTS texts have, the higher the agreement.
letters news jokes
0%
20%
40%
60% STATE
EVENT
GENERALIZING SENTENCE
GENERIC SENTENCE
Figure 1: Distribution of situation entity types in three different genres.
Speech Acts and Abstract Entities. Figure 2 shows the percentage of segments of each genre that
were marked as a Speech Act or an Abstract Entity by at least one annotator. QUESTIONS are most
frequent in the jokes genre, but about half of them are just marked by one annotator, which has to do with
how consistently indirect questions are marked. The two annotators agree on almost all segments labeled
as imperatives; while there are only very few IMPERATIVES in the news section, there are more in the
jokes and letters sections. The letters are mainly fund-raising letters, which explains the high percentage
of IMPERATIVES (Please help Goodwill. // Use the enclosed card // and give a generous gift today.).
FACTS and PROPOSITIONS, on the other hand, are rather infrequent in any genre, and annotators tend to
mark them inconsistently. We take from this analysis that we need to offer some help to the annotators in
detecting Abstract Entities. We plan to compile a list of verbs that may introduce Abstract Entities and
specifically highlight potential licensing constructions in order to increase recall for these types.
4.4 Intra-annotator consistency
After the first round of annotation, we identified 11 documents with low inter-annotator agreement on
SE type (5 news, 5 letters, 1 jokes) and presented them to two annotators for re-annotation. For each
annotator, the elapsed time between the first and second rounds was at least 3 weeks. We observe that in
general, the agreement of each annotator with herself is greater than agreement with the other annotator.
This shows that the disagreements are not pure random noise, but that annotators have different prefer-
ences for certain difficult decisions. It is interesting to note that annotator B apparently changed how
155
letrs nrws jroorks
0%02
4%02
6%02
S%02
TA%02 EVNENGRLRNI
Z ktrC??? ?enr
Z ktrC??? ? ?eo?
letrs nrws jroorks0%02
4%02
6%02
S%02
TA%02 ???L
letrs nrws jroorks
0%02
4%02
6%02
S%02
TA%02 R?E?V?LR??
letrs nrws jroorks
0%02
4%02
6%02
S%02
TA%02 ???GLRNI
Figure 2: Percentage of segments marked as Speech Act or Abstract Entity by at least one annotator.
GENRE NUMBER OF MAIN ASPECTUAL HABITUALITY SE TYPE SE TYPE
SEGMENTS REFERENT CLASS (REP=EVT)
A1:B1 636 0.15 0.79 0.64 0.40 0.45
A2:B2 599 0.12 0.78 0.70 0.42 0.48
A1:A2 596 0.79 0.88 0.78 0.75 0.75
B1:B2 620 0.55 0.84 0.78 0.75 0.75
Table 4: Consistency study: Cohen?s ?, for two annotators, comparing against each other and against
themselves (re-annotated data). A1 = annotator A in first pass, B2 = annotator B in second pass etc.
she annotates main referents; possibly this is also due to the above mentioned revision to the annotation
scheme. On the other hand, B annotated very few segments as generic (only 61 segments were marked
as having a generic main referent in either the first or second pass, 27 of them in both passes), which
may also have led to the low ? value. The fact that annotators do disagree with themselves indicates that
there are noisy cases in our data set, where multiple interpretations are possible. However, we want to
point out that the level of noise estimated by this intra-annotator consistency study is an upper bound as
we chose the most difficult documents for re-annotation; the overall level of noise in the data set can be
assumed to be much lower.
5 Conclusion
We have presented an annotation scheme for labeling clauses with their situation entity type along with
features indicating the type of main referent, fundamental aspectual class and habituality. The feature-
driven approach allows for a detailed analysis of annotator disagreements, showing in which way the
annotators? understandings of a clause differ. The analysis in the previous chapter showed that while
good inter-annotator agreement can be reached for most decisions required by our annotation schema,
there remain hard cases, on which annotators disagree with each other or with their own first round
of annotations. We do not yet observe satisfying agreement for main referent types or for identifying
abstract entities. In both cases, data sparseness is a problem; there are only very few generic main
referents and abstract entities in our current corpus. We plan to conduct case studies on data that is
specifically selected for these phenomena.
However, in many of the hard cases, several readings are possible. Rather than using an adjudicated
data set for training and evaluation of supervised classifiers for labeling clauses with situation entities,
we plan to leverage such disagreements for training, following proposals by Beigman Klebanov and
Beigman (2009) and Plank et al. (2014).
The annotation reported here is ongoing; our next goal is to extend annotation to additional genres
within MASC, starting with essays, journal, fiction, and travel guides. Following SE annotation, we will
extend the project to annotation of discourse modes. Finally, we are very interested in exploring and
annotating SEs in other languages, as we expect a similar inventory but different linguistic realizations.
Acknowledgments We thank the anonymous reviewers, Bonnie Webber and Andreas Peldszus for
helpful comments, and our annotators Ambika Kirkland, Ruth K?uhn and Fernando Ardente. This re-
search was supported in part by the MMCI Cluster of Excellence, and the first author is supported by an
IBM PhD Fellowship.
156
References
Emmon Bach. 1986. The algebra of events. Linguistics and philosophy, 9(1):5?16.
Beata Beigman Klebanov and Eyal Beigman. 2009. From annotator agreement to noise models. Computational
Linguistics, 35(4):495?503.
Steven Bethard. 2013. ClearTK-TimeML: A minimalist approach to TempEval 2013. In Second Joint Conference
on Lexical and Computational Semantics (* SEM), volume 2, pages 10?14.
Greg Carlson. 2005. Generics, habituals and iteratives. The Encyclopedia of Language and Linguistics.
Christelle Cocco. 2012. Discourse type clustering using pos n-gram profiles and high-dimensional embeddings.
In Proceedings of the Student Research Workshop at the 13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, EACL 2012.
Francisco Costa and Ant?onio Branco. 2012. Aspectual type and temporal relation classification. In Proceedings of
the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), pages
266?275.
Bonnie J. Dorr. 2001. LCS verb database. Online software database of Lexical Conceptual Structures, University
of Maryland, College Park, MD.
Annemarie Friedrich and Alexis Palmer. 2014. Automatic prediction of aspectual class of verbs in context. In
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL). Baltimore,
USA.
Nancy Ide, Christiane Fellbaum, Collin Baker, and Rebecca Passonneau. 2010. The manually annotated sub-
corpus: A community resource for and by the people. In Proceedings of the ACL 2010 conference short papers,
pages 68?73.
Judith L. Klavans and Martin S. Chodorow. 1992. Degrees of stativity: The lexical representation of verb aspect.
In Proceedings of the 14th COLING, Nantes, France.
Manfred Krifka, Francis Jeffry Pelletier, Gregory Carlson, Alice ter Meulen, Gennaro Chierchia, and Godehard
Link. 1995. Genericity: an introduction. The Generic Book, pages 1?124.
Annie Louis and Ani Nenkova. 2011. Automatic identification of general and specific sentences by leveraging
discourse annotations. In Proceedings of IJCNLP 2011.
Alexis Mitchell, Stephanie Strassel, Mark Przybocki, JK Davis, George Doddington, Ralph Grishman, Adam Mey-
ers, Ada Brunstein, Lisa Ferro, and Beth Sundheim. 2003. ACE-2 Version 1.0. Linguistic Data Consortium,
Philadelphia.
Alexander PD Mourelatos. 1978. Events, processes, and states. Linguistics and philosophy, 2(3):415?434.
Alexis Palmer, Jonas Kuhn, and Carlota Smith. 2004. Utilization of multiple language resources for robust
grammar-based tense and aspect classification. In Proceedings of LREC 2004.
Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith. 2007. A sequencing model for situation entity
classification. Proceedings of ACL 2007.
Barbara Plank, Dirk Hovy, and Anders S?gaard. 2014. Learning part-of-speech taggers with inter-annotator
agreement loss. In Proceedings of EACL 2014.
Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL).
John Searle. 1969. Speech Acts. Cambridge University Press.
Eric V Siegel and Kathleen R McKeown. 2000. Learning methods to combine linguistic indicators: Improving
aspectual classification and revealing linguistic insights. Computational Linguistics, 26(4):595?628.
Eric V. Siegel. 1998. Disambiguating verbs with the WordNet category of the direct object. In Proceedings of
Workshop on Usage of WordNet in Natural Language Processing Systems, Universite de Montreal.
Eric V. Siegel. 1999. Corpus-based linguistic indicators for aspectual classification. In Proceedings of ACL37,
University of Maryland, College Park.
157
Carlota S Smith. 2003. Modes of discourse: The local structure of texts. Cambridge University Press.
Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational
Linguistics on Human Language Technology-Volume 1, pages 149?156. Association for Computational Linguis-
tics.
Naushad UzZaman, Hector Llorens, Leon Derczynski, Marc Verhagen, James Allen, and James Pustejovsky. 2013.
Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In Second joint
conference on lexical and computational semantics (* SEM), volume 2, pages 1?9.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter Verbs and Times, pages 97?121. Cornell University Press,
Ithaca, New York.
158
